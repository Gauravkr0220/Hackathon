Effective Generation of Feasible Solutions for Integer
Programming via Guided Diffusion
Hao Zeng
zenghao.zeng@cainiao.com
Cainiao Network
Hangzhou, ChinaJiaqi Wang
tangqiao.wjq@cainiao.com
Cainiao Network
Hangzhou, ChinaAvirup Das
avirup.das@postgrad.manchester.ac.uk
University of Manchester
Manchester, United Kingdom
Junying He
junying.hjy@cainiao.com
Cainiao Network
Hangzhou, ChinaKunpeng Han
Haoyuan Hu
kunpeng.hkp@cainiao.com
haoyuan.huhy@cainiao.com
Cainiao Network
Hangzhou, ChinaMingfei Sunâˆ—
mingfei.sun@manchester.ac.uk
University of Manchester
Manchester, United Kingdom
ABSTRACT
Feasible solutions are crucial for Integer Programming (IP) since
they can substantially speed up the solving process. In many applica-
tions, similar IP instances often exhibit similar structures and shared
solution distributions, which can be potentially modeled by deep
learning methods. Unfortunately, existing deep-learning-based al-
gorithms, such as Neural Diving [ 21] and Predict-and-search frame-
work [ 8], are limited to generating only partial feasible solutions,
and they must rely on solvers like SCIP and Gurobi to complete the
solutions for a given IP problem. In this paper, we propose a novel
framework that generates complete feasible solutions end-to-end.
Our framework leverages contrastive learning to characterize the
relationship between IP instances and solutions, and learns latent
embeddings for both IP instances and their solutions. Further, the
framework employs diffusion models to learn the distribution of
solution embeddings conditioned on IP representations, with a ded-
icated guided sampling strategy that accounts for both constraints
and objectives. We empirically evaluate our framework on four
typical datasets of IP problems, and show that it effectively gen-
erates complete feasible solutions with a high probability (> 89.7
%) without the reliance of Solvers and the quality of solutions is
comparable to the best heuristic solutions from Gurobi. Further-
more, by integrating our methodâ€™s sampled partial solutions with
the CompleteSol heuristic from SCIP [ 19], the resulting feasible
solutions outperform those from state-of-the-art methods across
all datasets, exhibiting a 3.7 to 33.7% improvement in the gap to
optimal values, and maintaining a feasible ratio of over 99.7% for
all datasets.
âˆ—Corresponding author.
This work is licensed under a Creative Commons Attribution-
NoDerivs International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671783CCS CONCEPTS
â€¢Computing methodologies â†’Neural networks ;Learning in
probabilistic graphical models ;Machine learning approaches.
KEYWORDS
Integer Programming, Diffusion Models
ACM Reference Format:
Hao Zeng, Jiaqi Wang, Avirup Das, Junying He, Kunpeng Han, Haoyuan
Hu, and Mingfei Sun. 2024. Effective Generation of Feasible Solutions for
Integer Programming via Guided Diffusion . In Proceedings of the 30th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™24),
August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages.
https://doi.org/10.1145/3637528.3671783
1 Introduction
Integer Programming (IP) in the field of operation research is a class
of optimization problems where some or all of the decision variables
are constrained to be integers [ 36]. Despite their importance in a
wide range of applications such as production planning [ 25,30],
resource allocation [ 12], and scheduling [ 23,29,35], IP is known
to be NP-hard and in general very difficult to solve. For decades,
a significant effort has been made to develop sophisticated algo-
rithms and efficient solvers, e.g., branch-and-bound [ 16], cutting
plane method [ 13] and large neighborhood search algorithms [ 24].
These methods, however, can be computationally expensive be-
cause the search space for large-scale problems can be exponentially
large. Moreover, these algorithms rely heavily on a feasible solu-
tion input that will crucially determine the whole search process.
Consequently, existing solvers, such as SCIP [ 19] and Gurobi [ 7],
firstly employ heuristic algorithms to identify high-quality feasible
solutions that serve as initial starting points for the optimization
process. However, the heuristic algorithms usually fail to capture
the similar structure among different IP instances and the quality of
initial solutions is usually low. Hence, having a data-driven method
that produces high-quality feasible solutions for any IP instances is
desirable for many real-world applications.
To generate feasible solutions, prior works [ 8,21,38] have em-
ployed the advantage of deep learning to capture similarity of the
IP instances from the same domain in order to expedite solving. For
4107
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hao Zeng et al.
presenting IP instances via neural network, Neural Diving [ 21,38]
adopt the methodology delineated by Gasse et al . [5], which models
the formulation of IP instances as bipartite graphs and subsequently
leveraging Graph Neural Networks (GNN) to derive variable fea-
tures from these graph representations. Subsequently, a solution
prediction task is employed to learn the relationship between IP
instances and their solutions, with the aim of directly predicting
those solutions. However, it is difficult to produce complete feasible
solutions as it fails to explicitly integrate objective and constraint
information during the sampling process. Neural Diving thus focus
on generating partial solutions by GNN, where only a subset of
variables is assigned values using neural networks. Importantly,
in many cases, the proportion of variables predicted by the neural
network is set at a relatively low ratio (less than 50%) to ensure
feasibility. Furthermore, such methods tend to be inefficient, primar-
ily due to the introduction of auxiliary problems for filling in the
remaining variables. For instance, the Completesol heuristic [ 19], a
classical approach, solves an auxiliary integer programming model
which is constructed by adding constraints to fix the variables
from partial solutions. Nonetheless, infeasibility can arise in auxil-
iary problems due to the potential for partial assignments to clash
with the initial constraints. To deal with infeasible assignments,
in another approach, Han et al . [8] proposes a predict-and-search
algorithm which constructs a trust region based on predicted partial
solutions and then search for high-quality feasible solutions via
a solver. In summary, these methods require the construction of
auxiliary problems to obtain feasible solutions and fail to utilize the
complete information from the IP instance, as only partial variables
are assigned. This situation highlights the necessity for creating an
end-to-end deep learning framework capable of generating com-
plete and feasible solutions for IP problems.
Recently, diffusion models [ 9,31] have exhibited notable advan-
tages in various generative tasks, primarily owing to their supe-
rior mode-coverage and diversity [ 2]. Notable applications include
high-fidelity image generation [ 4], image-segmentation [ 1], and
text-to-image synthesis [ 28]. These successes motivates the launch
of an investigation into harnessing the generative capability of
diffusion models for acquiring feasible solutions of IP problems.
To this end, we introduce a comprehensive end-to-end gener-
ative framework presented in Figure 1 to produce high-quality
feasible solutions for IP problems. First of all, stemming inspira-
tion from DALL.E-2 [28] for text-to-image translation, we employ
a multimodal contrastive learning approach, akin to the CLIP Al-
gorithm [ 27], to obtain embeddings for an IP instance ğ‘–, denoted
aszğ‘–, and solution embeddings zxfor solutions x(Section 3). Sub-
sequently, we employ DDPM [ 9] to model the distribution of zx
conditioned on zğ‘–(Section 3). During this phrase, a decoder is con-
currently trained with the task of solution reconstruction (Section
3). Finally, to enhance the quality of the feasible solutions during the
sampling process, we propose the IP-guided sampling approaches
tailored for both DDPM and DDIM [ 33] which explicitly consider
both constraints and objectives during sampling. Our experimental
results shown in Section 5 substantiate the efficacy of this approach
in generating complete and feasible solutions for a given IP in-
stance with a higher probability. Besides, by combining with the
CompleteSol heuristic, the solutions from our methods have better
quality than the state-of-the-art. Importantly, to the best of ourknowledge, our approach is the first to have the ability to gener-
ate complete and feasible solutions using pure neural techniques,
without relying on any solvers.
2 Background
Integer Programming and Its Representations. Integer pro-
gramming (IP) is a class of NP-hard problems where the goal is to
optimize a linear objective function, subject to linear and integer
constraints. Without loss of generality, we focus on minimization
which can be formulated as follows,
minxcâŠ¤x subject to Axâ‰¤b, xâˆˆZğ‘›(1)
where câˆˆRğ‘›denotes the objective coefficient, A=[aâŠ¤
1,aâŠ¤
2,...,aâŠ¤ğ‘š]âˆˆ
Rğ‘šÃ—ğ‘›is the coefficient matrix of constraints and b=[ğ‘1,ğ‘2,...,ğ‘ğ‘š]âŠ¤âˆˆ
Rğ‘šrepresents the right-hand-side vector. For simplicity, we fo-
cus on binary integer variables, where xtakes values in{0,1}ğ‘›.
Throughout this paper, we adopt the term IP instance to denote a
specific instance within the domain of some Integer Programming
(IP) problem.
Bipartite graph representation, proposed by Gasse et al . [5], is a
commonly used and useful way to extract features of an IP instance
for machine learning purposes. This representation, see the left part
of Figure 1 (a) for an example, divides the constraints and variables
into two different sets of nodes, and uses a Graph Convolution
Network (GCN) to learn the representation of nodes. Recently, Nair
et al. [21] proposed several changes to the architecture of GCN
for performance improvements. Therefore, in this work, we use
the bipartite graph structure combined with GCN to extract the
embeddings of IP instances (see [5, 21] for more details).
DDPM and DDIM. Diffusion models learn a data distribution
by reversing a gradual noising process. In the DDPM method [ 9],
when presented with a data point sampled from an actual data
distribution, denoted as z(0)
xâˆ¼ğ‘(zx), a diffusion model, as de-
scribed in [ 9,31], typically involves two distinct phases. In the
forward process, a sequence of Gaussian noise is incrementally
added to the initial sample over a span of ğ‘‡steps, guided by a
variance schedule denoted as ğ›½1,ğ›½2,...,ğ›½ğ‘‡. This process yields a
sequence of noisy samples z(1)
x,z(2)
x,...,z(ğ‘‡)
x. Subsequently, the tran-
sition for the forward process can be described as: ğ‘(z(ğ‘¡)
x|z(ğ‘¡âˆ’1)
x)=
N(z(ğ‘¡)
x;âˆšï¸
1âˆ’ğ›½ğ‘¡z(ğ‘¡âˆ’1)
x,ğ›½ğ‘¡I). In fact, z(ğ‘¡)
xcan be sampled at any time
stepğ‘¡in a closed form employing the notations ğ›¼ğ‘¡:=1âˆ’ğ›½ğ‘¡and
Â¯ğ›¼:=Ãğ‘¡
ğ‘ =1ğ›¼ğ‘ ,z(ğ‘¡)
x=âˆšÂ¯ğ›¼ğ‘¡z(0)
x+âˆš1âˆ’Â¯ğ›¼ğ‘¡ğ, where ğâˆ¼N( 0,I). In
the reverse process (denoising process), we need to model the dis-
tribution of z(ğ‘¡âˆ’1)
x given z(ğ‘¡)
xas a Gaussian distribution, which
implies that ğ‘ğœƒ(z(ğ‘¡âˆ’1)
x|z(ğ‘¡)
x)=N
z(ğ‘¡âˆ’1)
x ;ğğœƒ(z(ğ‘¡)
x,ğ‘¡),Î£ğœƒ(z(ğ‘¡)
x,ğ‘¡)
,
where the variance Î£ğœƒ(z(ğ‘¡)
x,ğ‘¡)can be fixed to a known constant [ 9]
or learned with a separate neural network [ 22], while the mean
can be approximately computed by adding z(0)
xas a condition,
ğğœƒ(z(ğ‘¡)
x,ğ‘¡)=âˆšğ›¼ğ‘¡(1âˆ’Â¯ğ›¼ğ‘¡âˆ’1)
1âˆ’Â¯ğ›¼ğ‘¡z(ğ‘¡)
x+âˆšÂ¯ğ›¼ğ‘¡âˆ’1ğ›½ğ‘¡
1âˆ’Â¯ğ›¼ğ‘¡z(0)
x.
To improve the efficiency of sampling of DDPM, DDIM [ 33]
formulates an alternative non-Markovian noising process with the
same forward marginals as DDPM, but rewrites the probability
ğ‘ğœƒ(z(ğ‘¡âˆ’1)
x|z(ğ‘¡)
x)in reverse process as a desired standard deviation
4108Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 1: Our method first trains the IP Encoder and Solution Encoder to acquire the IP embedding (z ğ‘–) and Solution embedding
(zx) using CISP. We then jointly train diffusion models and the solution decoder to capture the distribution of solutions given a
specific IP instance. In the sampling stage, we employ an IP guided diffusion sampling to account for both the objective and
constraints.
ğœğ‘¡. DDIM them derives the following distribution in the reverse
process,
ğ‘ğœ(z(ğ‘¡âˆ’1)
x|z(ğ‘¡)
x,z(0)
x)
=N
z(ğ‘¡âˆ’1)
x ;âˆšÂ¯ğ›¼ğ‘¡z(0)
x+âˆšï¸ƒ
1âˆ’Â¯ğ›¼ğ‘¡âˆ’1âˆ’ğœ2
ğ‘¡ğ(ğ‘¡),ğœ2
ğ‘¡I
,(2)
where ğ(ğ‘¡)=(z(ğ‘¡)
xâˆ’âˆšÂ¯ğ›¼z(0)
x)/(1âˆ’Â¯ğ›¼)shows the direction to z(ğ‘¡)
x.
3 Model Architecture
Our training dataset consists of pairs (ğ‘–,x)of IP instance and their
corresponding one feasible solution x. Given an instance ğ‘–, let
zğ‘–âˆˆRğ‘›Ã—ğ‘‘andzxâˆˆRğ‘›Ã—ğ‘‘be the embeddings of the IP instance ğ‘–
and the solution xrespectively, where ğ‘›is the number of variables
andğ‘‘is the embedding dimension. It is worth noting that a IP
instance can have multiple different feasible solutions, meaning
that model need to learn the distribution of feasible solutions by
conditioning on a given IP instance. Our methods do not directly ap-
ply a diffusion model to learn the distribution of solutions. Instead,
we use an encoder to transform the solutions xâˆˆ{0,1}ğ‘›from a
discrete space to a continuous embedding space, e.g. zxâˆˆRğ‘›Ã—ğ‘‘.
We then construct a diffusion model to learn the distribution of the
solution embeddings given an IP embedding zğ‘–. Finally, a decoder is
trained to recover the predicted solution Ë†xfrom the embedding zx.
To effectively build the connection between the IP instance ğ‘–and so-
lution x, we first apply Contrastive IP-Solution Pre-training (CISP)
module, motivated by CLIP [ 27] which is used for text-to-image
generation, to produce IP embedding zğ‘–and solution embedding
zx. Overall, our model consists of three key components:
â€¢aContrastive IP-Solution Pre-training (CISP) module that pro-
duces IP embeddings zğ‘–and solution embeddings zx;
â€¢adiffusion module ğ‘(zx|zğ‘–)that generates solution embed-
ding zxconditioned on IP embedding zğ‘–;
â€¢and a decoder module ğ‘(x|zx,zğ‘–)that recovers solution x
from embedding zxconditioned on IP embedding zğ‘–.
We provide more details on each module in the following sections.Contrastive IP-Solution Pre-training. Previous works [ 8,21]
show the crucial importance of establishing the connection be-
tween the IP instances and the solutions, and propose to implicitly
learn this connection through the task of predicting feasible so-
lutions. These approaches may not exhibit strong generalization
capabilities on new instances because they only utilize the collected
solutions in dataset without considering feasibility explicitly during
training. To more effectively capture this relationship, we propose
to employ a contrastive learning task to learn representations for
IP instances and embeddings for solutions by constructing feasi-
ble and infeasible solutions. The intuition behind is to ensure that
the IP embeddings stay close to the embeddings of their feasible
solutions, and away from the embeddings of the infeasible ones.
To avoid explicitly constructing infeasible solutions, we proposed
Contrastive IP-Solution Pre-training (CISP) algorithm to train IP
encoder and solution encoder. Specifically, for IP encoder, we ex-
tract the representation of IP instances via a bipartite graph as [ 5],
and use the structure of GCNs from Neural Diving [ 21] to generate
all variablesâ€™ embeddings as IP embeddings zğ‘–. For solution encoder,
we use the encoder of the transformer to obtain the representa-
tions of each variable as solution embeddings zx. Both zxandzğ‘–
have the same dimension to compute pairwise cosine similarities
later. Since the number of variables in different IP instances may
vary, we perform zero-padding on zğ‘–and dummy-padding on x(i.e.
padding 2 for 0-1 integer programming) to align the dimensions.
The zero-padding for zğ‘–is done to ensure that the cosine similarity
remains unaffected. CISP algorithm then learns to maximize the
similarity between embeddings of IP and corresponding solution
pairs, and to minimize the similarity between the embeddings of
incorrect pairs, which is achieved through optimizing a symmetric
cross-entropy loss, as detailed in Appendix A.1.
Diffusion Generation. To leverage diffusion models for gen-
erating feasible solutions (discrete variables), we use the solution
embedding zxâˆˆRğ‘›Ã—ğ‘‘from the aforementioned CISP as the objec-
tive of generation. In addition, zğ‘–is considered as a condition for
4109KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hao Zeng et al.
generating high-quality results. According to [ 9], we parameterize
ğ‘ğœƒ(z(ğ‘¡âˆ’1)
x|z(ğ‘¡)
x,zğ‘–)
=N
z(ğ‘¡âˆ’1)
x ;ğğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡),Î£ğœƒ(z(ğ‘¡âˆ’1)
x,zğ‘–,ğ‘¡)
,(3)
âˆ€ğ‘¡âˆˆ[ğ‘‡,ğ‘‡âˆ’1,...,1]in reverse process, where z(0)
x=zx. Different
from predicting the noise of each step in a general diffusion training
phase, we predict zxdirectly as it empirically performs better. The
training loss is defined as follows,
LMSEâ‰œEğ‘¡,z(ğ‘¡)
xh
âˆ¥fğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡)âˆ’zxâˆ¥2i
, (4)
where fğœƒis an encoder-based transformer model with specific struc-
ture shown in Figure 2 and z(ğ‘¡)
x=âˆšÂ¯ğ›¼ğ‘¡zx+âˆš1âˆ’Â¯ğ›¼ğ‘¡ğ(ğ‘¡),ğ(ğ‘¡)âˆ¼
N(0,I).
T ransformer
Encoder+QK
Sinusoidal
Embedding+V
Figure 2: Diffusion model fğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡)
Solution Decoding. The decoder dğœ™plays a crucial role in recon-
structing the solution xfrom the solution embedding zx. To enhance
the robustness of the solution recovery, we jointly train the decoder
dğœ™with the diffusion model. Specifically, we concatenate the solu-
tion embedding Ë†zx=fğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡)generated by the diffusion model
with the IP embedding zğ‘–, and use the concatenated vector as in-
put to a transformer encoder to obtain the reconstructed solution
Ë†x=dğœ™(Ë†zx,zğ‘–). This process is associated with the cross-entropy
loss defined as:LCEâ‰œâˆ’Ex[logË†x]=âˆ’Ex
logdğœ™(Ë†zx,zğ‘–). To ex-
plicitly account for constraints in the training process, we introduce
a penalty term to measure the degree of constraint violation. More
specifically, let ağ‘‡
ğ‘˜be theğ‘˜th row of matrix Ain(1), the constraint
violation (CV) loss is defined as LCVâ‰œ1
ğ‘šÃğ‘š
ğ‘˜=1max(ağ‘‡
ğ‘˜Ë†xâˆ’ğ‘ğ‘˜,0),
whereğ‘šis the number of constraints. The total loss for training
diffusion and decoder therefore consists of the three parts:
L=LMSE+L CE+ğœ†LCV, (5)
whereğœ†is a hyper-parameter to regulate the penalty. The full
training procedure is given in Algorithm 1 and the training details
can be found in Appendix A.4.
4 IP Guided Sampling
Once the models have been trained, we can then sample variable
assignments by running the sampling algorithm of DDPM or DDIM
from a random Gaussian noise z(ğ‘‡)
xâˆ¼N( 0,I). Interestingly we
find that, without suitable guidance, diffusion model is prone to
generate inaccurate distributions, e.g. violating constraints for a
given IP instance as shown in section 5.1. We thus consider theAlgorithm 1 Training diffusion and solution decoder
Input: IP instance embedding zğ‘–from CISP, solution embedding
zxfrom CISP
Require: diffusion model fğœƒ, and solution decoder dğœ™.
1:repeat
2:ğ‘¡âˆ¼Uniform({1,...,ğ‘‡})
3: ğâˆ¼N( 0,I)
4: Ë†zxâ†fğœƒ âˆšÂ¯ğ›¼ğ‘¡zx+âˆš1âˆ’Â¯ğ›¼ğ‘¡ğ,zğ‘–,ğ‘¡
// predicted Ë†zx
5: Ë†xâ†dğœ™(Ë†zx,zğ‘–)// reconstructed solution Ë†x
6: Take gradient descent step to minimize total loss in (5)
7:until Reaches a fixed number epochs or satisfies an early stop-
ping criteria
constraints information (A,b)and objective coefficient cduring
sampling. We present the IP guided diffusion sampling for both
DDPM and DDIM, of which the latter is faster and better in terms
of the quality and feasibility, as shown in Section 5.
4.1 IP Guided Diffusion Sampling
Consider a conditional diffusion model ğ‘ğœƒ(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–), we first
introduce constraint guidance by designing each transition proba-
bility as
ğ‘ğœƒ,ğœ™(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–,A,b)=ğ‘ğ‘ğœƒ(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–)ğ‘’âˆ’ğ‘ ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b),
(6)
whereğ‘ is the gradient scale, ğ‘is a normalizing constant and
ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)=Ãğ‘š
ğ‘˜=1max(ağ‘‡
ğ‘˜dğœ™(z(ğ‘¡)
x,zğ‘–)âˆ’ğ‘ğ‘˜,0)measures the
violation of constraints. Let ğandÎ£be the mean and variance of
the Gaussian distribution representing ğ‘ğœƒ,ğœ™(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–). Then,
logğ‘ğœƒ(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–)=âˆ’1
2(z(ğ‘¡)
xâˆ’ğ)ğ‘‡Î£âˆ’1(z(ğ‘¡)
xâˆ’ğ)+ğ¶, whereğ¶
is a constant. Consider the Taylor expansion for ğ‘ğœ™atz(ğ‘¡)
x=ğ,
ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)
â‰ˆğ‘ğœ™(ğ,zğ‘–,A,b)+( z(ğ‘¡)
xâˆ’ğ)âˆ‡z(ğ‘¡)
xğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)|z(ğ‘¡)
x=ğ
=(z(ğ‘¡)
xâˆ’ğ)g+ğ¶1,
where g=âˆ‡z(ğ‘¡)
xğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)|z(ğ‘¡)
x=ğandğ¶1is a constant. Similar
to Classifier Guidance [ 4], we assume that ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)has low
curvature compared to Î£âˆ’1and thus have the following,
log(ğ‘ğœƒ,ğœ™(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–,A,b))
â‰ˆâˆ’1
2(z(ğ‘¡)
xâˆ’ğ)ğ‘‡Î£âˆ’1(z(ğ‘¡)
xâˆ’ğ)âˆ’ğ‘ (z(ğ‘¡)
xâˆ’ğ)g+ğ‘ ğ¶1+ğ¶2
=âˆ’1
2(z(ğ‘¡)
xâˆ’ğ+ğ‘ Î£g)ğ‘‡Î£âˆ’1(z(ğ‘¡)
xâˆ’ğ+ğ‘ Î£g)+ğ¶3,
whereğ¶3is a constant and can be safely ignored. Therefore, the
denoising transition ğ‘ğœƒ,ğœ™(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–,A,b)can be approximated
by a Gaussian distribution with a mean shifted by âˆ’ğ‘ Î£g. We can
further inject the objective guidance to transition probability for
acquiring high-quality solutions,
ğ‘ğœƒ,ğœ™(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–,A,b,c)
=ğ‘ğ‘ğœƒ(z(ğ‘¡)
x|z(ğ‘¡+1)
x,zğ‘–)ğ‘’âˆ’ğ‘ 
(1âˆ’ğ›¾)ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)+ğ›¾ğ‘œğœ™(z(ğ‘¡)
x,zğ‘–,c)
,(7)
4110Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
whereğ‘œğœ™(z(ğ‘¡)
x,zğ‘–,c)=cğ‘‡dğœ™(z(ğ‘¡)
x,zğ‘–),cis the coefficient of objective
from (1), andğ›¾âˆˆ[0,1]is the leverage factor for balancing constraint
and objective. The corresponding sampling method is called IP
Guided Diffusion Sampling, as presented in Algorithm 2.
Algorithm 2 IP Guided Diffusion Sampling
Input: gradient scale ğ‘ , leverage factor ğ›¾, constraint information
(A,b)and objective coefficient c
Require: diffusion model fğœƒand solution decoder dğœƒ.
1:z(ğ‘‡)
xâˆ¼N( 0,I)
2:forğ‘¡fromğ‘‡to1do
3: ğâ†âˆšğ›¼ğ‘¡(1âˆ’Â¯ğ›¼ğ‘¡âˆ’1)
1âˆ’Â¯ğ›¼ğ‘¡z(ğ‘¡)
x+âˆšÂ¯ğ›¼ğ‘¡âˆ’1ğ›½ğ‘¡
1âˆ’Â¯ğ›¼ğ‘¡fğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡)
4: Î£â†Î£ğœƒ(z(ğ‘¡âˆ’1)
x|z(ğ‘¡)
x,zğ‘–)
5: ğâ†ğâˆ’ğ‘ Î£âˆ‡z(ğ‘¡)
x
(1âˆ’ğ›¾)ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)+ğ›¾ğ‘œğœ™(z(ğ‘¡)
x,zğ‘–,c)
6: z(ğ‘¡âˆ’1)
xâˆ¼N( ğ,Î£)
7:end for
8:return dğœ™(z(0)
x,zğ‘–).
4.2 Non-Markovian IP Guided Sampling
For the non-Markovian sampling scheme as used in DDIM, the
method for conditional sampling is no longer invalid. To guide
the sampling process, existing studies [ 4,10] used the score-based
conditioning trick from Song et al . [34] to construct a new epsilon
prediction. We found that this trick turns out ineffective in our prob-
lem setting, possibly due to unique difficulties of finding feasible
solutions in IP instances.
Instead, we find that adding a direction that guides the pre-
dicted solution to constraint region in each step of reverse pro-
cess helps generate more reasonable solutions. Specifically, we
first generate the predicted noise according to Ë†z(0)
x=ğ‘“ğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡),
ğ(ğ‘¡)
ğœƒ=z(ğ‘¡)
xâˆ’âˆšÂ¯ğ›¼ğ‘¡Ë†z(0)
xâˆš1âˆ’Â¯ğ›¼ğ‘¡. According to (2), the transition equation of
DDIM for z(ğ‘¡âˆ’1)
x from a sample z(ğ‘¡)
xcan be written as
z(ğ‘¡âˆ’1)
x =âˆšÂ¯ğ›¼ğ‘¡fğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡)+âˆšï¸ƒ
1âˆ’Â¯ğ›¼ğ‘¡âˆ’1âˆ’ğœ2
ğ‘¡ğ(ğ‘¡)
ğœƒ+ğœğ‘¡ğğ‘¡.(8)
where the first term is the prediction of z(0)
xand the second term
is the direction pointing to z(ğ‘¡)
x. To consider constraints in (1), we
modify ğ(ğ‘¡)
ğœƒby adding the direction of minimizing sum of constraint
violation, that is
Ë†ğ(ğ‘¡)
ğœƒ=ğ(ğ‘¡)
ğœƒâˆ’ğ‘ âˆ‡z(ğ‘¡)
xğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b), (9)
whereğ‘ is gradient scale. By replacing ğ(ğ‘¡)
ğœƒin(8)with Ë†ğ(ğ‘¡)
ğœƒ, we
obtain Non-Makovian Constraint Guided Sampling, which guides
the solution generated in each transition to approach constraint
region. Equivalently, it is to perform a gradient descent step with
a step-size shrinking to zero as ğ‘¡â†’0whenğœğ‘¡â†’0. To further
consider the objective function together with the constraint, we
can update ğ(ğ‘¡)
ğœƒas follows:
Ë†ğ(ğ‘¡)
ğœƒ=ğ(ğ‘¡)
ğœƒâˆ’ğ‘ âˆ‡z(ğ‘¡)
x
(1âˆ’ğ›¾)ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)+ğ›¾ğ‘œğœ™(z(ğ‘¡)
x,zğ‘–,c)
,(10)whereğ›¾âˆˆ[0,1]is the leverage factor for balancing constraint and
objective. This method is called Non-Markovian IP Guided Diffusion
Sampling, as presented in Algorithm 3 .
Algorithm 3 Non-Markovian IP Guided Diffusion Sampling
Input: gradient scale ğ‘ , leverage factor ğ›¾, constraint information
(A,b)and objective coefficient c
Require: diffusion model fğœƒ, solution decoder dğœƒ
1:z(ğ‘‡)
xâˆ¼N( 0,I)
2:forğ‘¡fromğ‘‡to1do
3: ğ(ğ‘¡)
ğœƒâ†(z(ğ‘¡)
xâˆ’âˆšÂ¯ğ›¼ğ‘¡fğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡))/âˆš1âˆ’Â¯ğ›¼ğ‘¡.
4: Ë†ğ(ğ‘¡)
ğœƒâ†ğ(ğ‘¡)
ğœƒâˆ’ğ‘ âˆ‡z(ğ‘¡)
x
(1âˆ’ğ›¾)ğ‘ğœ™(z(ğ‘¡)
x,zğ‘–,A,b)+ğ›¾ğ‘œğœ™(z(ğ‘¡)
x,zğ‘–,c)
5: z(ğ‘¡âˆ’1)
xâ†âˆšÂ¯ğ›¼ğ‘¡fğœƒ(z(ğ‘¡)
x,zğ‘–,ğ‘¡)+âˆšï¸ƒ
1âˆ’Â¯ğ›¼ğ‘¡âˆ’1âˆ’ğœ2
ğ‘¡Ë†ğ(ğ‘¡)
ğœƒ+ğœğ‘¡ğğ‘¡
6:end for
7:return dğœ™(z(0)
x,zğ‘–).
5 Experiments
This section empirically investigates the effectiveness of our method
in solving IP instances. The efficacy is evaluated with two metrics:
feasible ratio andobjective value. The feasible ratio measures the pro-
portion of feasible solutions among all sampled solutions and the
objective value obtained from the generated feasible solutions mea-
sures the solution quality. Additionally, we also present gap between
feasible solution ğ‘¥and optimal solution ğ‘¥âˆ—via|cğ‘‡(xâˆ’xâˆ—)|
max(|cğ‘‡x|,|cğ‘‡xâˆ—|)for
comparing the performance more clearly.
We evaluate our methods on four IP datasets generated by the
Ecole library [26]:
â€¢Set Cover (SC) is to find the least number of subsets that cover a
given universal set.
â€¢Capacitated Facility Location (CF) is to locate a number of facilities
to serve the sites with a given demand and the aim is minimize
the total cost.
â€¢Combinatorial Auction (CA) is to help bidders place unrestricted
bids for bundles of goods and the aim is to maximize the revenue.
â€¢Independent Set (IS) is to find the maximum subset of nodes of
an undirected graph such that no pair of nodes are connected.
We compare the performance of our approach with state-of-the-art
methods including Neural Diving (ND) [ 21], the Predict-and-Search
algorithm (PS) [ 8], and heuristic solutions from SCIP 8.0.1 [ 3] and
Gurobi 9.5.2 [ 7]. Detailed descriptions of the datasets and baseline
methods are provided in Appendix A.3. Moreover, we incorporate
the best objective values, which serve as the ground truth, acquired
by executing Gurobi on each instance for a duration of 100 seconds.
To ensure clarity, we use IP Guided DDPM to denote the (Markovian)
IP Guided Diffusion sampling in Section 4.1, and IP Guided DDIM
to represent the Non-Markovian IP Guided Diffusion sampling in
Section 4.2.
In the following, we first illustrate the guided diffusion sampling
and emphasize its distinctions to Neural Diving and vanilla genera-
tion process in diffusion models in Section 5.1. Further, we evaluate
the feasibility and quality of solutions generated by IP guided DDIM
across all four datasets in Section 5.2. Furthermore, we conduct
4111KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hao Zeng et al.
0
1
2345
67
89
10
1112
13
14predicted solution
0
1
2345
67
89
10
1112
13
14random sampling
0
1
2345
67
89
10
1112
13
14partial solution
0
1
2345
67
89
10
1112
13
14complete solution
0.00.20.40.60.81.0
Variable Value
0
1
2345
67
89
10
1112
13
14step 0
0
1
2345
67
89
10
1112
13
14step 10
0
1
2345
67
89
10
1112
13
14step 50
0
1
2345
67
89
10
1112
13
14step 100
0.00.20.40.60.81.0
Variable Value
0
1
2345
67
89
10
1112
13
14step 0
0
1
2345
67
89
10
1112
13
14step 10
0
1
2345
67
89
10
1112
13
14step 50
0
1
2345
67
89
10
1112
13
14step 100
0.00.20.40.60.81.0
Variable ValueNeural Diving
Unguided DDIM
IP Guided DDIMNeural Diving
Unguided DDIM
IP Guided DDIMNeural Diving
Unguided DDIM
IP Guided DDIM
Figure 3: The sampling results from different methods. For Neural Diving, we present the predicted solution from GCN, random
sampling according to the predicted solution, the partial solution obtained via SelectiveNet (only node 1, 9 and 13 are assigned
to 0), and the completing result by calling CompleteSol heuristic. For DDIM and IP Guided DDIM, we present the results from
different time steps (transformed to solution space by a decoder) during sampling.
an ablation study to investigate the impact of different guided ap-
proaches and contrastive learning in Section 5.3. In Section 5.4, we
demonstrate the scalability of our approach by applying it to larger
instances and present the outcomes of qualitative analysis of solu-
tions. All experiments are performed in a workstation with two In-
tel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz, 176GB ram and two
Nvidia V100 GPUs. We also provide the total training and inference
time in Appendix A.5. The detailed hyper-parameters for IP guided
sampling can be found in Appendix A.6. The codes can be found
in: https://github.com/agent-lab/diffusion-integer-programming.
5.1 Illustrative experiments
The maximal independent set problem involves finding the largest
subset of nodes in an undirected graph where no two nodes are con-
nected. We focus on an illustrative example: a graph consisting of
15 nodes and 22 edges. This graph can be transformed into an inte-
ger programming (IP) instance with 15 variables and 22 constraints.
The graph is depicted in Figure 3, and we present the results from
Neural Diving, and from the different time steps of unguided DDIM
and IP Guided DDIM. Among the three algorithms, Neural Diving
fixes only three node values and uses the Completesol heuristic to
find an independent set containing seven nodes. However, the ran-
dom sampling solution based on predicted probability from Neural
Diving is infeasible. Unguided DDIM is unable to find a feasible
solution (there is an edge between node 7 and node 8). In contrast,
IP Guided DDIM is able to fetch the optimal solution by finding
an independent set containing eight nodes during the sampling
process, where no two nodes are connected. Notably, the qualityof the solution improves as the sampling process progresses. The
independent set contains 5 nodes at step 10, 7 nodes at step 50, and
finally 8 nodes (the optimal solution) at step 100. These indicate
that IP Guided DDIM outperforms Neural Diving and Unguided
DDIM in finding the optimal solution for this illustrative example.
5.2 Performance Evaluation
In this section, we evaluate the performance of different methods by
comparing their average feasible ratios, gaps and average objective
values across four datasets mentioned earlier. Each dataset contains
100 instances. For each instance, we sample 30 solutions and cal-
culate the corresponding metrics, which allows us to assess the
performance of each method in terms of both solution feasibility
and objective value across the different datasets.
We first compare the feasibility ratio of the complete solutions
generated by IP Guided Diffusion with the partial solutions gen-
erated by Neural Diving, due to the inability of Neural Diving to
produce complete solutions. In Neural Diving, the expected pro-
portion of variables assigned by the model is controlled by the
hyper-parameter "Coverage" ğ¶. A higherğ¶results in more vari-
ables being assigned by the model, which generally leads to a lower
feasibility ratio. Therefore, a carefully prescribed coverage ğ¶is cru-
cial in Neural Diving to ensure the feasibility of partial solutions.
In this experiment, we trained two variants of Neural Diving with
different coverage thresholds for each dataset. For the SC, CA, and
IS datasets, the coverage are set to 0.2 (low coverage) and 0.3 (high
coverage) respectively. However, for the CF dataset, the coverage
4112Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 20 40 60 80 100
Instance0.20.40.60.81.0Average Feasible RatioSC
0 20 40 60 80 100
InstanceCA
0 20 40 60 80 100
InstanceCF
0 20 40 60 80 100
InstanceIS
ND (low coverage) ND (high coverage) IP Guided DDIM IP Guided DDPM
Figure 4: The feasible ratio in 100 instances, with each instance sampled 30 complete or partial solutions. For diffusion model,
we measure the feasible ratio of complete solutions. Two versions of Neural Diving are trained with distinct coverage thresholds,
referred to as ND (low coverage) and ND (high coverage). The feasibility ratio is evaluated only for partial solutions from
Neural Diving, as the complete solutions from this method yield a 0% feasibility ratio.
Table 1: The average objective value (obj.), gap and feasible ratio (fea.) for 100 instances on 4 datasets. Optimal, heuristic, complete
and partial denote optimal solutions from Gurobi, heuristic solutions from solvers, complete solutions from models, and
partial solutions from models. CompleteSol and search indicates the CompleteSol heuristic and predict-and-search algorithm
for completing the partial solutions, respectively.
Algorithm Solutionsâ€™
TypeSC
(min) CF
(min) CA
(max) IS
(max)
obj.
gap fea. obj.
gap fea. obj.
gap fea. obj.
gap fea.
Gur
obi (100s) Optimal 168.3
0.0% 100% 11405.5
0.0% 100% 36102.6
0.0% 100% 685.3
0.0% 100%
SCIP Heuristic 1967.0
91.4% 100% 84748.4
86.5% 100% 28007.4
22.4% 100% 447.8
34.7% 100%
Gurobi Heuristic 522.4
67.8% 100% 50397.3
77.4% 100% 30052.0 16.8% 100% 415.5
39.4% 100%
Neural Diving Complete -
- 0.0% -
- 0.0% -
- 0.0% -
- 0.0%
IP Guided DDPM (Ours) Complete 577.9
70.8% 95.7% 58488.1
80.5% 44.0% 800.3
97.8% 87.3% 129.9
81.0% 100%
IP Guided DDIM (Ours) Complete 533.5
68.5% 99.8% 25119.2
54.6% 89.7% 26916.9
25.4% 97.1% 455.6
33.5% 99.7%
Neural
Diving Partial
+ CompleteSol 849.0
80.2% 100% 14259.8
20.0% 81.3% 30143.6
16.5% 87.0% 484.1
29.4% 90.4%
PS Partial
+ Search 593.7
71.7% 100% 32119.8
64.5% 100% 31159.5
13.7% 100% 587.9
14.2% 100%
IP Guided DDIM (Ours) Partial
+ CompleteSol 255.5
34.1% 100% 14224.1
19.8% 100% 32491.1
10.0% 99.7% 639.4
6.7% 100%
Table 2: Ablation study for 100 instances on 4 datasets with different guidances.
Unguide
d
DDIMConstraint Guided
DDIMObjective Guided
DDIMIP Guided DDIM
w/o CISPIP Guided
DDIMdatasetobj.
fea. obj. fea. obj. fea. obj. fea. obj. fea.
SC
(min) - 0.0% 63046.9 99.8% - 0.0% 763.4 99.8% 533.5 99.8%
CF (min) - 0.0% 53311.2 74.1 % - 0.0% 31319.8 41.7% 25119.2 89.7%
CA (max) - 0.0% 5157.2 99.7% - 0.0% 23383.3 57.7% 26916.9 97.1 %
IS (max) - 0.0% 386.5 100 % - 0.0% 479.1 68.9% 455.6 99.7%
thresholds are set to 0.1 and 0.2. Figure 4 presents the average feasi-
ble ratio for 100 instances. In this comparison, the feasible ratio of
complete solutions from IP guided DDIM outperforms the solutions
of Neural Diving in almost all instances.
To more comprehensively evaluate the performance of the dif-
fusion model, we compare it against different baselines in Table 1.
For SCIP, we adopt the first solution obtained through non-trivial
heuristic algorithms during the solving phase. For Gurobi, we use
the best heuristic solution for each instance as a benchmark. Addi-
tionally, we include the optimal objective values (obtained through
running Gurobi for 100 seconds on each instance) as ground-truth.
For Neural Diving, complete solutions are frequently infeasible. As
such, we employ a low-coverage model that prioritizes the feasi-
bility of partial solutions. Subsequently, a CompleteSol heuristic isutilized to finalize these partial solutions. In the case of the Predict
and Search algorithm (PS), we construct a trust region using partial
solutions with the same proportion of assigned variables as Neural
Diving and use Gurobi as the Solver to search the best heuristic
solutions found as a benchmark. To ensure a fair comparison, we
report not only the quality of the complete solutions generated by
our methods (IP Guided DDIM/DDPM), but also the quality of par-
tial solutions. For this, we randomly select the same proportion of
variables from DDIMâ€™s complete solutions as the ratio from Neural
Diving and PS as our partial solutions. We then use the CompleteSol
heuristic to finalize these partial solutions.
The results are presented in Table 1. It is evident that the com-
plete solutions produced using the IP Guided DDIM method have a
4113KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hao Zeng et al.
Table 3: The average objective value (obj.), gap and the feasible ratio (fea.) for 100 instances in 3 different size SC datasets.
Optimal, heuristic, complete and partial denote optimal solutions from Gurobi, heuristic solutions from solvers, complete
solutions from models, and partial solutions from models. CompleteSol and search indicates CompleteSol heuristic and
predict-and-search algorithm for completing the partial solutions, respectively.
Algorithm Solutionsâ€™
TypeSC
(2000) SC
(3000) SC
(4000)
obj.
gap fea. obj.
gap fea. obj.
gap fea.
Gur
obi (100s) Optimal 168.3
0.0% 100% 140.4
0.0% 100% 126.9
0.0% 100%
SCIP Heuristic 1977.0
91.4% 100% 2236.2
93.7% 100% 2386.0
94.7% 100%
Gurobi Heuristic 522.4
67.8% 100% 718.8
80.5% 100% 1454.5
91.3% 100%
Neural Diving Complete -
- 0.0% -
- 0.0% -
- 0.0%
IP Guided DDPM (Ours) Complete 594.7
70.8% 96.5% 451.8
68.9% 83.7% 440.7
71.2% 77.9%
IP Guided DDIM (Ours) Complete 533.5
68.5% 99.8% 486.8
71.2% 99.9% 464.9
72.7% 100%
Neural
Diving Partial
+ CompleteSol 849.0
80.2% 100% 1145.8
87.7% 100% 1465.6
91.3% 100%
PS Partial
+ Search 593.7
71.7% 100% 737.0
80.9% 100% 994.9
87.2% 100%
IP Guided DDIM (Ours) Partial
+ CompleteSol 255.5
34.1% 100% 217.4
35.4% 100% 195.9
35.2% 100%
feasible ratio of at least 89.7%, and their objective values are com-
parable to the best heuristic solutions from Gurobi. In contrast, the
complete solutions obtained solely through Neural Diving are con-
sistently infeasible. Moreover, the integration of partial solutions
from IP Guided DDIM with the CompleteSol heuristic outstrips
that of all methods in terms of objective values. This improvement
is demonstrated by a 3.7 to 33.7% reduction in the gap to optimal
values, while the feasibility ratio for all datasets approaches 100%.
5.3 Ablation Study
We ablate on unguided DDIM (with ğ‘ =0), constraint guided DDIM
(withğ›¾=1), objective guided DDIM (with ğ›¾=0) models and
IP guided DDIM on four datasets. We also include an experiment
where we train IP and solution embeddings directly via algorithm
1 without CISP, in order to assess the advantages of contrastive
learning, i.e. IP Guided DDIP w/o CISP in Table 2. The results are
presented in Table 2. Evidently, the constraint guidance is crucial
in generating feasible solutions, and the objective guidance fur-
ther enhances the quality of solutions. Moreover, the experiments
demonstrate that CISP plays a crucial role in ensuring that the
solutions produced by our methods are more feasible. Therefore,
combining both constraint and objective guidance achieves good
quality solutions with high probability.
5.4 Scalability test and qualitative analysis
Practitioners often aim to apply the models learned to solve prob-
lems of larger scales than the ones used for data collection and
training. To estimate how well a model can generalize to bigger
instances, we evaluate its performance on datasets of varying sizes
from the Set Cover problem (minimization problem). We utilize
three size categories:
â€¢SC (2000): 2000 variables, 1000 constraints
â€¢SC (3000): 3000 variables, 1500 constraints
â€¢SC (4000): 4000 variables, 2000 constraints
It is worth noting that all models were trained using the SC (2000)
dataset. The results in table 3 demonstrate that IP guided DDPM
consistently performs well across all three different-sized datasets,
indicating that our framework possesses strong generalization ca-
pabilities.
200 300 400 500 600 700 800
Objectives0.0000.0020.0040.0060.0080.0100.0120.0140.016FrequencyGurobi Heuristic
Optimal
IP Guided DDIM
IP Guided DDIM + CompleteSol(a) SC instance (minimization).
400 450 500 550 600 650 700
Objectives0.000.010.020.030.040.050.060.07FrequencyGurobi Heuristic
Optimal
IP Guided DDIM
IP Guided DDIM + CompleteSol
(b) IS instance (maximization).
Figure 5: The objective distribution of 1000 solutions sampled
from a single instance.
Diffusion models are generative models that capture the distri-
bution of a dataset. In this experiment, we focus on the distribution
of solutions generated by our methods. We take a single instance
from the SC dataset and IS dataset and use the IP Guided DDIM
algorithm to generate 1000 complete feasible solutions. We also
randomly sample 20% of variable values from each solution and
4114Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
use the CompleteSol heuristic to fill in the remaining variables.
We analyze the distribution of objective values for these complete
solutions and compare them with the optimal objective values and
the best heuristic solutions found by the Gurobi heuristic. The re-
sults are shown in Figure 5. Clearly, the complete solutions directly
generated by the IP Guided DDIM algorithm are superior to the
best solutions from the Gurobi heuristic. Furthermore, the objective
values from the partial solutions completed by the CompleteSol
heuristic are closer to the optimal value.
6 Related Work
We start the related work section with a summary of deep learning
techniques used in the construction of feasible solutions for Inte-
ger Programming (IP) problems. Gasse et al . [5] propose a method
that combines a bipartite graph with a Graph Convolutional Net-
work (GCN) to extract representations of Integer Programming
(IP) instances. Although this approach is primarily employed to
learn the branching policy in the branch and bound algorithm, it
is worth noting that this modeling method can also be utilized for
the prediction of solutions for IP instances. However, the solutions
produced by GCN directly are often infeasible or sub-optimal. To
address this, Neural Diving [ 21] leverages the SelectiveNet [ 6] to
assign values to only a subset of the variables based on a coverage
threshold, with the rest of the variables being completed via an IP
Solver. To further improve the feasibility of generated solutions,
Han et al . [8] proposes a predict-search framework that combines
the predictions from GNN model with trust region method. How-
ever, this method still relies on IP solver to solve a modified instance
(adding neighborhood constraints to origin instance) in order to
get complete solutions. Similar to Nair et al . [21] , Khalil et al . [14]
integrate GNN into integer programming solvers and apply it to
construct a partial solutions through a prescribed rounding thresh-
old, which is then completed using SCIP. In contrast, our method
aims to learn the latent structure of IP instances by diffusion models,
and obtains complete feasible solutions through guided diffusion
sampling, without any reliance on the IP solver.
Another set of related works to our paper is diffusion mod-
els [9,32]. As the latest state-of-the-art family of deep generative
models, diffusion models have demonstrated their ability to en-
hance performance across various generative tasks [ 37]. In this
paper, we focus our discussion specifically on conditional diffusion
models. Unlike unconditional generation, conditional generation
emphasizes application-level contents as a condition to control the
generated results based on predefined intentions. To enable this
conditioning, Dhariwal and Nichol [4]introduce the concept of clas-
sifier guidance, which enhances sample quality by conditioning the
generative process on an additional trained classifier. In the same
vein, Ho and Salimans [10]propose a joint training strategy for both
conditional and unconditional diffusion models, i.e., classifier-free
guidance. This approach combines the resulting conditional and un-
conditional scores to achieve a balance between sample quality and
diversity. This idea has also found its effectiveness in Topology Op-
timization [ 20]. We thus take a similar derivation from the classifier
guidance and devise IP-guided diffusion sampling by incorporating
the objectives and constraints into the transition probability.7 Discussions
7.1 The Importance of Solution Generation
Integer Programming solvers typically rely on branch-and-bound or
branch-and-cut algorithms to tackle problems, often starting with
a primal heuristic to find a good initial feasible solution. A robust
starting solution generally accelerates the entire solving process.
Our method demonstrates that diffusion models are capable of gen-
erating complete feasible solutions that satisfy all constraints. These
generated solutions exhibit competitive performance compared
to the heuristics employed in optimization tools such as Gurobi.
Notably, the ability to generate complete solutions from scratch
distinguishes our model from other neural-based approaches like
Neural Diving [ 21] and the Predict-Search algorithm [ 8]. Moreover,
generating feasible solutions end-to-end is especially important, as
it represents a significant leap toward developing a purely neural-
based method for integer programming. The paper shows that
diffusion models can produce solutions that satisfy all constraints,
which lays the foundation for future research in which neural net-
works could potentially solve such problems without relying on
traditional solvers or heuristics.
7.2 Sampling Efficiency
The sampling time is a notable limitation of current diffusion models.
Specifically, our method requires 100 iterative denoising steps for
DDIM and 1000 for DDPM, resulting in longer times to generate a
single solution compared to other methods. To address this issue,
there have been active studies focused on accelerating diffusion
sampling, such as model quantization (Li et al . [17] ) and distillation
(HUANG et al . [11] ). These techniques can be readily integrated
into our methods. Moving forward, we will enhance our method
by incorporating these acceleration techniques.
8 Conclusion
In this paper, we presented a comprehensive framework for gen-
erating feasible solutions for Integer Programming (IP) problems.
We utilized the CISP approach to establish a link between IP in-
stances and their corresponding solutions, allowing to obtain IP
embeddings and solution embeddings. To effectively capture the
distribution of feasible solutions, we leveraged diffusion models,
which are known for their powerful learning capabilities, to learn
the distribution of solution embeddings. We further employed a so-
lution decoder to reconstruct the solutions from their embeddings.
Importantly, we proposed an IP guided sampling algorithm that
explicitly incorporates the objective and constraint information to
generate high-quality solutions. The experimental results on four
distinct datasets demonstrate the superiority of our approach to
the state-of-the-art.
ACKNOWLEDGMENTS
Mingfei Sun is affiliated with Centre for AI Fundamentals. He is
also a member of AI Hub in Generative Models, funded by Engi-
neering and Physical Sciences Research Council (EPSRC), part of
UK Research and Innovation (UKRI),
4115KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hao Zeng et al.
REFERENCES
[1]Tomer Amit, Eliya Nachmani, Tal Shaharabany, and Lior Wolf. 2021. SegDiff:
Image Segmentation with Diffusion Probabilistic Models. CoRR abs/2112.00390
(2021). arXiv:2112.00390 https://arxiv.org/abs/2112.00390
[2]Reza Bayat. 2023. A Study on Sample Diversity in Generative Models: GANs vs.
Diffusion Models. https://openreview.net/forum?id=BQpCuJoMykZ
[3]Ksenia Bestuzheva, Mathieu BesanÃ§on, Wei-Kun Chen, Antonia Chmiela, Tim
Donkiewicz, Jasper van Doornmalen, Leon Eifler, Oliver Gaul, Gerald Gamrath,
Ambros Gleixner, et al .2023. The SCIP optimization suite 8.0. ACM Trans. Math.
Softw. 49, 2 (2023), 22:1â€“22:21. https://doi.org/10.1145/3585516
[4]Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on
image synthesis. Advances in neural information processing systems 34 (2021),
8780â€“8794.
[5]Maxime Gasse, Didier ChÃ©telat, Nicola Ferroni, Laurent Charlin, and Andrea
Lodi. 2019. Exact combinatorial optimization with graph convolutional neural
networks. Advances in Neural Information Processing Systems 32 (2019).
[6]Yonatan Geifman and Ran El-Yaniv. 2019. Selectivenet: A deep neural network
with an integrated reject option. In International conference on machine learning.
PMLR, 2151â€“2159.
[7] LLC Gurobi Optimization. 2021. Gurobi optimizer reference manual.
[8]Qingyu Han, Linxin Yang, Qian Chen, Xiang Zhou, Dong Zhang, Akang Wang,
Ruoyu Sun, and Xiaodong Luo. 2023. A GNN-Guided Predict-and-Search Frame-
work for Mixed-Integer Linear Programming. In International Conference on
Learning Representations. https://openreview.net/forum?id=pHMpgT5xWaE
[9]Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in neural information processing systems 33 (2020), 6840â€“6851.
[10] Jonathan Ho and Tim Salimans. 2022. Classifier-Free Diffusion Guid-
ance. CoRR abs/2207.12598 (2022). https://doi.org/10.48550/arXiv.2207.12598
arXiv:2207.12598
[11] JUNWEI HUANG, Zhiqing Sun, and Yiming Yang. 2023. Accelerating Diffusion-
based Combinatorial Optimization Solvers by Progressive Distillation. In ICML
2023 Workshop: Sampling and Optimization in Discrete Space. https://openreview.
net/forum?id=AbMj31okE4
[12] Naoki Katoh and Toshihide Ibaraki. 1998. Resource allocation problems. Hand-
book of Combinatorial Optimization: Volume1â€“3 (1998), 905â€“1006.
[13] James E Kelley, Jr. 1960. The cutting-plane method for solving convex programs.
Journal of the society for Industrial and Applied Mathematics 8, 4 (1960), 703â€“712.
[14] Elias B Khalil, Christopher Morris, and Andrea Lodi. 2022. Mip-gnn: A data-
driven framework for guiding combinatorial solvers. In Proceedings of the AAAI
Conference on Artificial Intelligence, Vol. 36. 10219â€“10227.
[15] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti-
mization. In 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio
and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980
[16] Eugene L Lawler and David E Wood. 1966. Branch-and-bound methods: A survey.
Operations research 14, 4 (1966), 699â€“719.
[17] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shang-
hang Zhang, and Kurt Keutzer. 2023. Q-Diffusion: Quantizing Diffusion Models.
InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).
17535â€“17545.
[18] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.
In7th International Conference on Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?
id=Bkg6RiCqY7
[19] Stephen J Maher, Tobias Fischer, Tristan Gally, Gerald Gamrath, Ambros Gleixner,
Robert Lion Gottwald, Gregor Hendel, Thorsten Koch, Marco LÃ¼bbecke, Matthias
Miltenberger, et al. 2017. The SCIP optimization suite 4.0. (2017).
[20] FranÃ§ois MazÃ© and Faez Ahmed. 2023. Diffusion models beat gans on topology
optimization. In Proceedings of the AAAI Conference on Artificial Intelligence
(AAAI), Washington, DC.
[21] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid Von Glehn, Pawel Lichocki,
Ivan Lobov, Brendan Oâ€™Donoghue, Nicolas Sonnerat, Christian Tjandraatmadja,
Pengming Wang, et al .2020. Solving mixed integer programs using neural
networks. arXiv preprint arXiv:2012.13349 (2020).
[22] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffu-
sion probabilistic models. In International Conference on Machine Learning. PMLR,
8162â€“8171.
[23] CC Pantelides, MJ Realff, and N Shah. 1995. Short-term scheduling of pipeless
batch plants. Chemical engineering research & design 73, 4 (1995), 431â€“444.
[24] David Pisinger and Stefan Ropke. 2019. Large neighborhood search. Handbook
of metaheuristics (2019), 99â€“127.
[25] Yves Pochet and Laurence A Wolsey. 2006. Production planning by mixed integer
programming. Vol. 149. Springer.
[26] Antoine Prouvost, Justin Dumouchelle, Lara Scavuzzo, Maxime Gasse, Didier
ChÃ©telat, and Andrea Lodi. 2020. Ecole: A gym-like library for machine learning
in combinatorial optimization solvers. arXiv preprint arXiv:2011.06069 (2020).[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.2021. Learning transferable visual models from natural language supervision.
InInternational conference on machine learning. PMLR, 8748â€“8763.
[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
2022. Hierarchical text-conditional image generation with clip latents. arXiv
preprint arXiv:2204.06125 1, 2 (2022), 3.
[29] Tadeusz Sawik. 2011. Scheduling in supply chains using mixed integer program-
ming. John Wiley & Sons.
[30] Edward Allen Silver, David F Pyke, Rein Peterson, et al .1998. Inventory manage-
ment and production planning and scheduling. Vol. 3. Wiley New York.
[31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
2015. Deep unsupervised learning using nonequilibrium thermodynamics. In
International conference on machine learning. PMLR, 2256â€“2265.
[32] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output
representation using deep conditional generative models. Advances in neural
information processing systems 28 (2015).
[33] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising Diffusion
Implicit Models. In 9th International Conference on Learning Representations, ICLR
2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net. https://openreview.
net/forum?id=St1giarCHLP
[34] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Ste-
fano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through
Stochastic Differential Equations. In 9th International Conference on Learning
Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
https://openreview.net/forum?id=PxTIG12RRHS
[35] Paolo Toth and Daniele Vigo. 2002. The vehicle routing problem. SIAM.
[36] L.A. Wolsey. 1998. Integer Programming. Wiley. https://books.google.co.uk/
books?id=x7RvQgAACAAJ
[37] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue
Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. 2022.
Diffusion Models: A Comprehensive Survey of Methods and Applications.
CoRR abs/2209.00796 (2022). https://doi.org/10.48550/arXiv.2209.00796
arXiv:2209.00796
[38] Taehyun Yoon. 2022. Confidence Threshold Neural Diving. CoRR abs/2202.07506
(2022). arXiv:2202.07506 https://arxiv.org/abs/2202.07506
A Appendix
A.1 CISP Algorithm
The study conducted by [ 27] underscores the substantial efficacy
of contrasting pre-training in capturing multi-modal data, with
particular emphasis on its application in the text-to-image transfer
domain. Drawing inspiration from this seminal work, we introduce
the CISP algorithm. The primary objective of CISP is to facilitate
the learning of the IP Encoder Eğ¼and the Solution Encoder EX, as
illustrated in Algorithm 4. Within the scope of our investigation,
we work with a mini-batch of data comprising instances denoted
asğ¼and their corresponding solutions denoted as X. The batchâ€™s
bipartite graph representing instances ğ¼is denoted as G. We use
zğ¼andzXto denote the embeddings of instances and solutions,
respectively. Notably, both zğ¼andzXpossess identical dimensions,
enabling us to compute their cosine similarity. Furthermore, within
the mini-batch,z ğ¼,ğ‘—andzX,ğ‘˜denote to the ğ‘—th sample in zğ¼and
theğ‘˜th sample in zX, respectively. Within this conceptual frame-
work, we leverage the matrix sâˆˆRğ‘Ã—ğ‘to represent the similarity
betweenğ‘instances and ğ‘solutions. Each element sğ‘—,ğ‘˜, where
ğ‘—,ğ‘˜âˆˆ{1,...,ğ‘}, corresponds to the logit employed in computation
of the symmetric cross-entropy loss.
A.2 Feature Descriptions For Variables Nodes,
Constraint Nodes And Edges
In Table 4, we provide a description of the features that are ex-
tracted using the Ecole library [ 26] and used as IP bipartite graph
representations for training the GCN model.
4116Effective Generation of Feasible Solutions for Integer Programming via Guided Diffusion KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 4: Description of the variable, constraint and edge features in our bipartite graph representations.
Feature Description
Variabletype Type(binary, integer, impl. integer, continuous) as a one-hot encoding.
coef Objective coefficient, normalized.
has_lb Lower bound indicator.
has_ub Upper bound indicator.
sol_is_at_lb Solution value equals lower bound.
sol_is_at_ub Solution value equals upper bound.
sol_frac Solution value fractionality.
basis_status Simplex basis status(lower, basic, upper, zero) as a one-hot encoding.
reduced_cost Reduced cost, normalized.
age LP age, normalized.
sol_val Solution value.
Constraintobj_cos_sim Cosine similarity with objective.
bias Bias value, normalized with constraint coefficients.
is_tight Tightness indicator in LP solution.
dualsol_val Dual solution value, normalized.
age LP age, normalized with total number of LPs.
Edge coef Constraint coefficient, normalized per constraint.
Algorithm 4 Contrastive IP-Solution Pre-Training (CISP)
Input: The mini-batch size ğ‘, the mini-batch bipartite graph repre-
sentations of IP instance set ğ¼, denoted by G, and corresponding
mini-batch solutions X
Require: IP Encoder Eğ¼, Solution Encoder EX, temperature param-
eterğœ
1:Get IP and solution embeddings zğ¼,zX=Eğ¼(G),EX(X)//ğ‘Ã—
ğ‘›Ã—ğ‘‘, whereğ‘›is the padding length of variables and ğ‘‘is the embedding
size.
2:forğ‘—âˆˆ{1,2,...,ğ‘}andğ‘˜âˆˆ{1,2,...,ğ‘}do
3: Flatten zğ¼,ğ‘—andzX,ğ‘˜into vectors Â¯zğ¼,ğ‘—andÂ¯zX,ğ‘˜
4: sğ‘—,ğ‘˜=ğ‘’ğœÂ·Â¯zğ‘‡
ğ¼,ğ‘—Â¯zX,ğ‘˜/(âˆ¥Â¯zğ¼,ğ‘—âˆ¥âˆ¥Â¯zX,ğ‘˜âˆ¥)// compute similarity for IP
and solution embeddings
5:end for
6:Set labels y=(1,2,...,ğ‘)
7:Compute cross-entropy loss Lğ¼by utilizing sğ‘—,âˆ—andy.
8:Compute cross-entropy loss LXby utilizing sâˆ—,ğ‘˜andy.
9:Compute the symmetric loss L=(Lğ¼+L X)/2
10:returnL
A.3 Datasets and Baselines
A.3.1 Datasets. For all four datasets, we randomly generate 1000
instances (800 for training, 100 for validation and 100 for testing).
Table 5 summarizes the numbers of constraints, variables and prob-
lem type of each dataset. We then collect feasible solutions and
their objective values for each instance by running the Gurobi [7]
or SCIP [ 3], where the time limit is set to 1000s for each instance.
For those instances with a large number of feasible solutions, we
only keep 500 best solutions. We adopt the same features exacted
via the Ecole library as in [ 5] and exclude those related to feasible
solutions. The specific features are shown in Appendix A.2.
A.3.2 Baselines. We compared our method with the following
baselines:â€¢SCIP [3] (an open source solver): SCIP is currently one of
the fastest non-commercial solvers for mixed integer pro-
gramming (MIP) and mixed integer nonlinear programming
(MINLP). Here, we focus on comparing the quality of initial
solutions with SCIP. Instead of relying on the first feasible
solution generated by SCIP, which are often of low quality
due to the use of trivial heuristics, we employ the first so-
lution produced via non-trivial heuristic algorithms during
the solving process of SCIP [ 3] (i.e. the first feasible solution
after the pre-solving stage of SCIP).
â€¢Gurobi [7] (the powerful commercial solver): Gurobi is a
highly efficient commercial mathematical optimization solver.
As our focus is on comparing feasible solutions, we consider
the best solutions obtained through Gurobiâ€™s default heuris-
tic algorithms as a benchmark.
â€¢Neural Diving (ND) [21]: Neural Diving adopts a solution pre-
diction approach, training a GCN to predict the value of each
variable. It then incorporates SelectiveNet [ 6] to generate
partial solutions with a predefined coverage threshold ğ¶(e.g.,
a coverage threshold ğ¶=0.2means that the expectation of
the number of assigned variables by the neural network is
20%). This threshold is typically set to be low ( <0.5) to en-
sure the feasibility of partial solutions. In our experiments,
to evaluate the feasibility of solutions, we set two different
coverage levels, namely low coverage (which usually indi-
cates higher feasibility) and high coverage (which usually
indicates lower feasibility), for each dataset. We then com-
pare the feasibility of the partial solutions with the complete
solutions generated by our methods. To assess the quality
of solutions, we employ the Completesol heuristic in SCIP
(Algorithm 4 in [ 19]) to enhance the partial solutions. This
heuristic involves solving auxiliary IP instances by fixing the
4117KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Hao Zeng et al.
variables from the partial solutions. By utilizing this heuris-
tic, we can obtain more complete solutions and evaluate their
quality.
â€¢Predict-and-search algorithm (PS) [8]: The PS algorithm, sim-
ilar to Neural Diving, utilizes graph neural networks to pre-
dict the value of each variable. It then searches for the best
feasible solution within a trust region constructed by the
predicted partial solutions. This method requires setting pa-
rameters(ğ‘˜0,ğ‘˜1)to represent the numbers of 0â€™s and 1â€™s in
a partial solution, and Î”to define the range of the neigh-
borhood region of the partial solution. To search a high-
quality feasible solution, this method adds neighborhood
constraints to origin instance, which produces modified IP
instance. Therefore, an IP solver such as SCIP or Gurobi is
required to solve the modified instance and obtain feasible
solutions. In our experiments, we use Gurobi as the solver
and control the parameters Î”to ensure that the modified
instance is 100% feasible. We considered the best heuristic
solutions from the modified instance found by Gurobi as our
baseline.
Table 5: Instance size of each dataset
Dataset Constraints Variables Problem Type
SC 1000 2000 minimize
CF 5051 5050 minimize
CA 786 1500 maximize
IS 6396 1500 maximize
A.4 Training Details
We trained the CISP and diffusion model on four IP datasets. Each
dataset contained 800 training instances, with 500 solutions col-
lected for each instance. In each batch, we sampled 64instances,
and for each instance, We sample one solution from 500 solutions
in proportion to the probability of the objective value as a possi-
ble label. This implies that solutions with better objective values
had a higher probability of being sampled. We iterated through all
instances (with one solution per instance) in each epoch.
For the Solution Encoder, we utilized a single transformer encode
layer with a width of 128. The IP encoder adopted the architecture
described in [ 21], using GCN to obtain embeddings for all vari-
ables as IP embeddings. Both models transformed the features of
the solution and IP into latent variables with a dimension of 128,
enabling convenient computation of cosine similarity in CISP. The
CISP was trained using the AdamW Optimizer [ 18]. We employed
a decreasing learning rate strategy, starting with a learning rate of
0.001 and linearly decaying it by a factor of 0.9 every 100 epochs
until reaching 800 epochs. The model training was performed with
a batch size of 64.
For the diffusion model, we utilized a single-layer Transformer
encoder with a width of 128 to predict zxand adjusted the number
of time steps to 1000. The forward process variances were set as
constants, increasing linearly from ğ›½1=10âˆ’4toğ›½ğ‘‡=0.02, follow-
ing the default setting of DDPM [ 9]. The solution decoder modelwas jointly trained with the diffusion model and consisted of two
Transformer encode layers with a width of 128. The loss function
was defined as the sum of the diffusion loss, decoder loss, and the
penalty for violating constraints, as shown in (5). Here,ğœ†is set
to be the number of variables in the instances from the training
set, excluding the IS dataset, where ğœ†=0. We trained diffusion
and decoder model for 100 epochs with batch size of 32via Adam
Optimizer [15].
A.5 Training and Inference Time
In this section, we report the training time (including CISP pretrain-
ing and Diffusion model) for each dataset, which takes 100 epochs
for both CISP and Diffusion model to converge. Additionally, we
provide the total inference time for sampling 3000 solutions by us-
ing IP Guided DDIM and DDPM. From Table 6, we observe that our
method requires a reasonable amount of time for model training.
During the inference phase, IP Guided DDIM demonstrates faster
performance compared to IP Guided DDPM with average time of
0.46s-1.68s for sampling each solution. Moreover, as shown in the
experiment results from Section 5, IP Guided DDIM also achieves
better performance than DDPM, making it suitable for practical
applications.
Table 6: Total training time and inference time for sampling
3000 solutions for each dataset
Dataset
Training (CISP + Diffusion) IP Guided DDIM IP Guided DDPM
SC
24.4m 37.5m 374m
CF 71.7m 84m 805m
CA 9.3m 23m 233.5m
IS 11.1m 23m 234m
A.6 Hyperparameters for IP Guided Sampling
During the sampling process, we configured the number of steps to
be 1000 for IP Guided Diffusion sampling (IP Guided DDPM) and
100 for Non-Markovian IP Guided Diffusion sampling (IP Guided
DDIM). In Table 7, we provide the specific values for the gradient
scaleğ‘ and leverage factor ğ›¾.
Table 7:ğ‘ andğ›¾settings in different dataset
datasetIP Guided DDIM IP Guided DDPM
ğ‘  ğ›¾ ğ‘  ğ›¾
SC (2000) 100,000 0.9 15,000 0.1
SC (3000) 150,000 0.9 22,500 0.1
SC (4000) 200,000 0.9 30,000 0.1
CF 1,000 0.7 500,000 0.1
CA 20,000 0.7 10,000 0.3
IS 20,000 0.5 10,000 0.1
4118