Mining of Switching Sparse Networks for Missing Value
Imputation in Multivariate Time Series
Kohei Obata
SANKEN, Osaka University
Osaka, Japan
obata88@sanken.osaka-u.ac.jpKoki Kawabata
SANKEN, Osaka University
Osaka, Japan
koki@sanken.osaka-u.ac.jp
Yasuko Matsubara
SANKEN, Osaka University
Osaka, Japan
yasuko@sanken.osaka-u.ac.jpYasushi Sakurai
SANKEN, Osaka University
Osaka, Japan
yasushi@sanken.osaka-u.ac.jp
Abstract
Multivariate time series data suffer from the problem of missing
values, which hinders the application of many analytical methods.
To achieve the accurate imputation of these missing values, ex-
ploiting inter-correlation by employing the relationships between
sequences (i.e., a network) is as important as the use of temporal
dependency, since a sequence normally correlates with other se-
quences. Moreover, exploiting an adequate network depending on
time is also necessary since the network varies over time. However,
in real-world scenarios, we normally know neither the network
structure nor when the network changes beforehand. Here, we pro-
pose a missing value imputation method for multivariate time series,
namely MissNet, that is designed to exploit temporal dependency
with a state-space model and inter-correlation by switching sparse
networks. The network encodes conditional independence between
features, which helps us understand the important relationships
for imputation visually. Our algorithm, which scales linearly with
reference to the length of the data, alternatively infers networks
and fills in missing values using the networks while discovering
the switching of the networks. Extensive experiments demonstrate
thatMissNet outperforms the state-of-the-art algorithms for mul-
tivariate time series imputation and provides interpretable results.
CCS Concepts
â€¢Information systems â†’Data mining; â€¢Mathematics of
computingâ†’Time series analysis.
Keywords
Multivariate time series, Missing value imputation, Network infer-
ence, State-space model, Graphical lasso
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671760ACM Reference Format:
Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai. 2024.
Mining of Switching Sparse Networks for Missing Value Imputation in
Multivariate Time Series . In Proceedings of the 30th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining (KDD â€™24), August 25â€“29, 2024,
Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10.
1145/3637528.3671760
1 Introduction
With the development of the Internet of Things (IoT), multivariate
time series are generated in many real-world applications, such as
motion capture [ 37], and health monitoring [ 8]. However, there
are inevitably many values missing from these data, and this has
many possible causes (e.g., sensor malfunction). As most algorithms
assume an intact input when building models, missing value impu-
tation is indispensable for real-world applications [3, 40].
In time series data, missing values often occur consecutively,
leading to a missing block in a sequence, and can happen simultane-
ously to multiple sequences [ 27]. To effectively reconstruct missing
values from such partially observed data, we must exploit both tem-
poral dependency, by taking account of past and future values in the
sequence, and inter-correlation, by using the relationship between
different sequences (i.e., a network) [ 10,52]. Here, a network does
not necessarily mean the spatial proximity of sensors but rather
underlying connectivity (e.g., Pearson correlation or partial correla-
tion). Moreover, as time series data are normally non-stationary, so
is the network; an adequate network must be exploited depending
on time [ 45,46]. We collectively refer to a group of time points
with the same network as a â€œregimeâ€. Fig. 1 shows an illustrative
example where missing blocks randomly exist in a multivariate
time series consisting of three features (i.e., A,B, and C). Each time
point belongs to either of two regimes with different networks (i.e.,
#1and#2), where the thickness of the edge indicates the strength
of the interplay between features. It is appropriate to use the values
of feature Cto impute the block missing from feature Bin regime
#1since the network has an edge between BandC. On the other
hand, in regime #2, it is preferable to use feature A, as the network
suggests. Nevertheless, in real-world scenarios, we often have no
information about the data; that is, we do not know the structure
of the network, let alone when the network changes. Thus, given
a partially observed multivariate time series, how can we infer
networks and allocate each time point to the correct regime? How
 
2296
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
Figure 1: An illustrative example of a multivariate time series
including missing blocks, where each time point of the data
is allocated to two regimes, each with a distinct network,
where the edges show relationships between features.
can we achieve an accurate imputation exploiting both temporal
dependency and inter-correlation?
There have been many studies on time series missing value
imputation with machine learning and deep learning models [ 23].
Many of them employ time-variant latent/hidden states, as used in
a State-Space Model (SSM) or a Recurrent Neural Network (RNN),
to learn the dynamic patterns of time series [ 7,9,25,54]. However,
they do not make full use of inter-correlation, while the sequences of
a multivariate time series usually interact. Although they implicitly
capture inter-correlation in latent space, they potentially capture
spurious correlations under the presence of missing values, leading
to inaccurate imputation. To tackle this problem, several studies
explicitly utilize the network [ 5,6,10,17,21,50]. However, they
require a predefined network, even though we rarely know it in
advance.
In this work, we propose MissNet1,Mining of switching sparse
Networks for multivariate time series imputation, which repeat-
edly infers sparse networks from imputed data and fills in miss-
ing values using the networks while allocating each time point to
regimes. Specifically, our model has three components: (1) a regime-
switching model based on a discrete Markov process for detecting
the change point of the network. (2) An imputation model based on
an SSM for exploiting temporal dependency and inter-correlation
by latent space and the networks. (3) A network inference model for
inferring sparse networks via graphical lasso, where each network
encodes pairwise conditional independencies among features, and
the lasso penalty helps avoid capturing spurious correlations. Our
proposed algorithm maximizes the joint probability distribution
over the above components.
Our method has the following desired properties:
â€¢Effective :MissNet, which exploits both temporal depen-
dency and inter-correlation by inferring switching sparse
networks, outperforms the state-of-the-art algorithms for
missing value imputation for multivariate time series.
â€¢Scalable : Our proposed algorithm scales linear with regard
to the length of the time series and is thus applicable to a
long-range time series.
â€¢Interpretable :MissNet discovers regimes with distinct sparse
networks, which help us interpret data, e.g., what is the key
relationship in the data, and why does a particular regime
distinguish itself from others?
1Our source code and datasets are publicly available:
https://github.com/KoheiObata/MissNet.Table 1: Capabilities of MissNet, Matrix Factorization (MF),
State-Space Model (SSM), Deep Learning (DL), and Graphical
Lasso (GL)-based approaches.
MF SSM DL GLSoftImpute
[30]
SoRe
c [28]
TRMF
[55]
SLDS
[36]
D
ynaMMo [25]
DCMF
[5]
BRI
TS [7]
POGEV
ON [50]
TICC
[19]
MMGL
[48]
MissNet
T
emporal dependency - -âœ“âœ“âœ“âœ“âœ“âœ“ - -âœ“
Inter-corr
elation -âœ“ - -
-âœ“ -âœ“ -âœ“âœ“
Time-var
ying inter-correlation - - - - - - -âœ“ -âœ“âœ“
Missing
value imputation âœ“
âœ“ âœ“ -âœ“
âœ“âœ“
âœ“ -âœ“âœ“
Segmentation - - -âœ“ - - - -âœ“ -âœ“
Sparse
network inference -
- - -
- - -
-âœ“
âœ“âœ“
2 Related work
We review previous studies closely related to our work. Table 1
shows the relative advantages of MissNet. Current approaches fall
short with respect to at least one of these desired characteristics.
Time series missing value imputation. Missing value imputa-
tion for time series is a very rich topic [ 23]. We roughly classify
missing value imputation methods as Matrix Factorization (MF)-
based, SSM-based, and Deep Learning (DL)-based approaches.
MF-based methods, such as SoftImpute [ 30] based on Singu-
lar Value Decomposition (SVD), recover missing values from low-
dimensional embedding matrices of partially observed data [ 22,
35,52,56]. For example, SoRec [ 28], proposed as a recommenda-
tion system, constrains MF with a predefined network to exploit
inter-correlation. Since MF is limited in capturing temporal depen-
dency, TRMF [ 55] uses an Auto-Regressive (AR) model and imposes
temporal smoothness on MF.
SSMs, such as Linear Dynamical Systems (LDS) [ 16], use latent
space to capture temporal dependency, where the data point de-
pends on all past data points [ 9,11,17,39]. To fit more complex time
series, Switching LDS (SLDS) [ 14,36] switches multiple LDS models.
SSM-based methods, such as DynaMMo [ 25], focus on capturing
the dynamic patterns in time series rather than inter-correlation
implicitly captured through the latent space. To use the underlying
connectivity in multivariate time series, DCMF [ 5], and its tensor
extension Facets [ 6] use SSM constrained with a predefined net-
work, which is effective, especially when the missing rate is high.
However, they assume that the network is accurately known and
fixed, while it is usually unknown and may change over time in
real-world scenarios.
Recently, extensive research has focused on DL-based methods,
employing techniques including graph neural networks [ 10,13,
21,38], self-attention [ 12,41], and, most recently, diffusion mod-
els [1,44,51], to harness their high model capacity [ 2,53]. For
example, BRITS [ 7] and M-RNN [ 54] impute missing values accord-
ing to hidden states from bidirectional RNN. To utilize dynamic
inter-correlation in time series, POGEVON [ 50] requires a sequence
of networks and imputes missing values in time series and missing
edges in the networks, assuming that the network varies over time.
Although DL-based methods can handle complex data, the impu-
tation quality depends heavily on the size and the selection of the
training dataset.
 
2297Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Sparse network inference. From another perspective, our method
infers sparse networks from time series containing missing values
and discovers regimes (i.e., clusters) based on networks. Inferring a
sparse inverse covariance matrix (i.e., network) from data helps us
to understand feature dependency in a statistical way. Graphical
lasso [ 15], which maximizes the Gaussian log-likelihood impos-
ing anâ„“1-norm penalty, is one of the most commonly used tech-
niques for estimating a sparse network from static data. StÃ¤dler and
BÃ¼hlmann [ 42] have tackled inferring a sparse network from par-
tially observed data according to conditional probability. However,
the network varies over time [ 29,32,33]; thus, TVGL [ 18] infers
time-varying networks by considering the time similarity with a
network belonging to neighboring segments. To infer time-varying
networks in the presence of missing values, MMGL [ 48], which
employs TVGL, uses the expectation-maximization (EM) algorithm
to repeat the inference of time-varying networks and missing value
imputation based on conditional probability under the condition
that each segment has the same observed features. Discovering
clusters based on networks [ 34], such as TICC [ 19] and TAGM [ 47],
provides interpretable results that other traditional clustering meth-
ods cannot find. However, they cannot handle missing values.
As a consequence, none of the previous studies have addressed
missing value imputation for multivariate time series by employing
sparse network inference and segmentation based on the network.
3 Preliminaries
3.1 Problem definition
In this paper, we focus on the task of multivariate time series miss-
ing value imputation. We use a multivariate time series with ğ‘
features and ğ‘‡timesteps ğ‘¿={ğ’™1,ğ’™2,...,ğ’™ğ‘‡}âˆˆRğ‘Ã—ğ‘‡. To repre-
sent the missing values in ğ‘¿, we introduce the indicator matrix
ğ‘¾âˆˆRğ‘Ã—ğ‘‡, where ğ‘¾ğ‘–,ğ‘¡indicates the availability of feature ğ‘–at
timestepğ‘¡:ğ‘¾ğ‘–,ğ‘¡being 0or1indicates whether ğ‘¿ğ‘–,ğ‘¡is missing or
observed. Thus, the observed entries can be described as Ëœğ‘¿=ğ‘¾â—¦ğ‘¿,
whereâ—¦is a Hadamard product. Our problem is formally written
as follows:
Problem 1 (Multivariate Time Series Missing Value Impu-
tation). Given: a partially observed multivariate time series Ëœğ‘¿;
Recover: its missing parts indicated by ğ‘¾.
3.2 Graphical lasso
We use graphical lasso [ 15] to infer the network for each regime.
Given ğ‘¿, graphical lasso estimates the sparse Gaussian inverse
covariance matrix (i.e., network) ğš¯âˆˆRğ‘Ã—ğ‘, also known as the
precision matrix. The network encodes pairwise conditional inde-
pendencies among ğ‘features, e.g., if ğš¯ğ‘–,ğ‘—=0, then features ğ‘–and
ğ‘—are conditionally independent given the values of all the other
features. The optimization problem is expressed as follows:
minimizeğš¯âˆˆğ‘†ğ‘
++ğœ†||ğš¯||ğ‘œğ‘‘,1âˆ’ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘™ğ‘™(ğ’™ğ‘¡,,ğš¯), (1)
ğ‘™ğ‘™(ğ’™ğ‘¡,ğš¯)=âˆ’1
2(ğ’™ğ‘¡âˆ’ğ†)â€²ğš¯(ğ’™ğ‘¡âˆ’ğ†)
+1
2logdet(ğš¯)âˆ’ğ‘
2log(2ğœ‹), (2)where ğš¯must be symmetric positive definite ( ğ‘†ğ‘
++).ğ‘™ğ‘™(ğ’™ğ‘¡,ğš¯)is
the log-likelihood and ğ†âˆˆRğ‘is the empirical mean of ğ‘¿.ğœ†â‰¥
0is a hyperparameter for determining the sparsity level of the
network, andâˆ¥Â·âˆ¥ğ‘œğ‘‘,1indicates the off-diagonal â„“1-norm. This is a
convex optimization problem that can be solved via the alternating
direction method of multipliers (ADMM) [4].
4 Proposed MissNet
In this section, we present our proposed MissNet for missing value
imputation. We give the formal formulation of the model and then
provide the detailed algorithm to learn the model.
4.1 Optimization model
MissNet infers sparse networks and fills in missing values using
the networks while discovering regimes. We first introduce three
interacting components of our model: a regime-switching model,
an imputation model, and a network inference model. Then, we
define the optimization formulation.
Regime-switching model. MissNet describes the change of net-
works by regime-switching. Let ğ¾be the number of regimes, ğ‘­=
{ğ’‡1,...ğ’‡ğ‘‡}âˆˆRğ¾Ã—ğ‘‡be regime assignments, and ğ’‡ğ‘¡be a one-hot
vector2that indicates Ëœğ’™ğ‘¡âˆˆRğ‘belongs to ğ’‡ğ‘¡â„
ğ‘¡-regime. We assume
regime-switching to be a discrete first-order Markov process:
ğ‘(ğ’‡1)=ğ…0, ğ‘(ğ’‡ğ‘¡+1|ğ’‡ğ‘¡)=ğš·ğ’‡ğ‘¡+1,ğ’‡ğ‘¡, (3)
where ğš·âˆˆRğ¾Ã—ğ¾is the Markov transition matrix and ğ…0âˆˆRğ¾is
the initial state distribution.
Imputation model. MissNet imputes missing values exploiting
temporal dependency and inter-correlation indicated by the net-
works. We assume that the temporal dependency is consistent
throughout all regimes and captured in the latent space of an SSM,
which allows us to consider long-term dependency. We define the la-
tent states ğ’={ğ’›1,ğ’›2,..., ğ’›ğ‘‡}âˆˆRğ¿Ã—ğ‘‡corresponding to Ëœğ‘¿, where
ğ¿is the number of latent dimensions, and ğ’›ğ‘¡+1âˆˆRğ¿is linear to ğ’›ğ‘¡
through the transition matrix ğ‘©âˆˆRğ¿Ã—ğ¿with the covariance ğšºğ’,
shown in Eq. (5). As defined in Eq. (4), the first timestep of latent
state ğ’›1is defined by the initial state ğ’›0and the covariance ğš¿0.
ğ’›1âˆ¼N(ğ’›0,ğš¿0), (4)
ğ’›ğ‘¡+1|ğ’›ğ‘¡âˆ¼N(ğ‘©ğ’›ğ‘¡,ğšºğ’). (5)
We define the observation Ëœğ’™ğ‘¡assigned to ğ’‡ğ‘¡â„
ğ‘¡-regime as being
linear to the latent state ğ’›ğ‘¡through the observation matrix of ğ’‡ğ‘¡â„
ğ‘¡-
regime ğ‘¼(ğ’‡ğ‘¡)âˆˆRğ‘Ã—ğ¿with the covariance ğšºğ‘¿(ğ’‡ğ‘¡):
Ëœğ’™ğ‘¡|ğ’›ğ‘¡,ğ‘¼(ğ’‡ğ‘¡),ğ’‡ğ‘¡âˆ¼N( ğ‘¼(ğ’‡ğ‘¡)ğ’›ğ‘¡,ğšºğ‘¿(ğ’‡ğ‘¡)). (6)
MissNet captures inter-correlation by adding a constraint on
ğ‘¼(ğ‘˜). Let it be assumed that the contextual matrix of the ğ‘˜ğ‘¡â„-regime
ğ‘º(ğ‘˜)âˆˆRğ‘Ã—ğ‘encodes the inter-correlation of ğ‘¿belonging to the
ğ‘˜ğ‘¡â„-regime ( 1â‰¤ğ‘˜â‰¤ğ¾). We define the contextual latent factor of
theğ‘˜ğ‘¡â„-regime ğ‘½(ğ‘˜)âˆˆRğ¿Ã—ğ‘, and theğ‘—-th column (1â‰¤ğ‘—â‰¤ğ‘) of
the contextual matrix ğ’”(ğ‘˜)
ğ‘—as linear to ğ’—(ğ‘˜)
ğ‘—through the observation
matrix ğ‘¼(ğ‘˜)with the covariance ğšºğ‘º(ğ‘˜), formulated in Eq. (7). In
2We use it interchangeably with the index of the regime (i.e., ğ’‡ğ‘¡â„
ğ‘¡-regime).
 
2298KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
Eq. (8), we place zero-mean spherical Gaussian priors on ğ’—(ğ‘˜)
ğ‘—under
the assumption that âˆ’1â‰¤ğ‘º(ğ‘˜)
ğ‘–,ğ‘—â‰¤1.
ğ’”(ğ‘˜)
ğ‘—|ğ’—(ğ‘˜)
ğ‘—,ğ‘¼(ğ‘˜)âˆ¼N( ğ‘¼(ğ‘˜)ğ’—(ğ‘˜)
ğ‘—,ğšºğ‘º(ğ‘˜)), (7)
ğ’—(ğ‘˜)
ğ‘—âˆ¼N( 0,ğšºğ‘½(ğ‘˜)). (8)
To avoid our model overfitting the observed entries since many
entries are missing, we simplify the covariances by assuming that all
noises are independent and identically distributed (i.i.d.). Thus, the
parameters ğšºğ’,ğšºğ‘¿(ğ‘˜),ğšºğ‘º(ğ‘˜),ğšºğ‘½(ğ‘˜)are reduced to ğœ2
ğ’,ğœ2
ğ‘¿(ğ‘˜),ğœ2
ğ‘º(ğ‘˜),
ğœ2
ğ‘½(ğ‘˜), respectively (i.e., ğšºğ’=ğœ2
ğ’ğ‘°,ğšºğ‘¿(ğ‘˜)=ğœ2
ğ‘¿(ğ‘˜)ğ‘°,ğšºğ‘º(ğ‘˜)=ğœ2
ğ‘º(ğ‘˜)ğ‘°,
andğšºğ‘½(ğ‘˜)=ğœ2
ğ‘½(ğ‘˜)ğ‘°).
With the above imputation model, the imputed time series Ë†ğ’™ğ‘¡at
timestepğ‘¡is recovered as follows:
Ë†ğ’™ğ‘¡=ğ‘¾:,ğ‘¡â—¦Ëœğ’™ğ‘¡+(1âˆ’ğ‘¾:,ğ‘¡)â—¦ğ‘¼(ğ’‡ğ‘¡)ğ’›ğ‘¡. (9)
Network inference model. MissNet infers the network for each
regime to exploit inter-correlation. We define a Gaussian distribu-
tion andâ„“1-norm for the imputed data belonging to each regime to
allow us to estimate networks accurately:
Ë†ğ’™ğ‘¡|ğ’‡ğ‘¡âˆ¼N( ğ†(ğ’‡ğ‘¡),ğš¯(ğ’‡ğ‘¡)âˆ’1), ğ‘ .ğ‘¡.,||ğš¯(ğ’‡ğ‘¡)||ğ‘œğ‘‘,1â‰¤ğ‘’
ğœ†, (10)
whereğ‘’is any constant value for convenience, and ğœ†is a hyper-
parameter that controls the sparsity of the network (i.e., inverse
covariance matrix) ğš¯(ğ’‡ğ‘¡)withâ„“1-norm, which helps avoid capturing
spurious correlations.
Optimazation formulation. Our goal is to estimate the model
parametersğœƒ={ğ‘©,ğ’›0,ğš¿0,ğœğ’,Â¤ğœğ‘¿,Â¤ğœğ‘º,Â¤ğœğ‘½,Â¤ğ†,Â¤ğš¯,ğ…0,ğš·}and find the
latent factors ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­, where the letters with a dot indicate a set
ofğ¾vectors/matrices/scalers (e.g., Â¤ğ‘º={ğ‘º(ğ‘˜)}ğ¾
ğ‘˜=1), that maximizes
the following joint probability distribution:
arg maxğ‘(Ëœğ‘¿,ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­)=ğ‘(ğ’›1)ğ‘‡Ã–
ğ‘¡=2ğ‘(ğ’›ğ‘¡|ğ’›ğ‘¡âˆ’1)
|                    {z                    }
temporal dependency
ğ‘‡Ã–
ğ‘¡=1ğ‘(Ëœğ’™ğ‘¡|ğ’›ğ‘¡âˆ’1,ğ‘¼(ğ’‡ğ‘¡),ğ’‡ğ‘¡)
|                         {z                         }
time seriesğ¾Ã–
ğ‘˜=1(ğ‘Ã–
ğ‘—=1ğ‘(ğ’—(ğ‘˜)
ğ‘—)ğ‘Ã–
ğ‘—=1ğ‘(ğ’”(ğ‘˜)
ğ‘—|ğ‘¼(ğ‘˜),ğ’—(ğ‘˜)
ğ‘—))
|                                                 {z                                                 }
network constraint
ğ‘(ğ’‡1)ğ‘‡Ã–
ğ‘¡=2ğ‘(ğ’‡ğ‘¡|ğ’‡ğ‘¡âˆ’1)
|                   {z                   }
regime-switchingğ‘‡Ã–
ğ‘¡=1ğ‘(Ë†ğ’™ğ‘¡|ğ†(ğ’‡ğ‘¡),ğš¯(ğ’‡ğ‘¡),ğ’‡ğ‘¡)
|                         {z                         }
network inference,(ğ‘ .ğ‘¡.||ğš¯(ğ‘˜)||ğ‘œğ‘‘,1â‰¤ğ‘’
ğœ†)
|                      {z                      }
network sparsity.
(11)
4.2 Algorithm
It is difficult to find the global optimal solution of Eq. (11), for the
following reasons: (i)As a constraint for Â¤ğ‘¼,Â¤ğ‘ºhas to be fixed and
encode inter-correlation; (ii)Â¤ğ‘¼,ğ’andğ‘­jointly determine Ëœğ‘¿, andÂ¤ğ‘¼
andÂ¤ğ‘½jointly determineÂ¤ğ‘º;(iii)Calculating the correct ğ‘­is NP-hard.
Hence, we aim to find its local optimum instead, following the EM
algorithm, where the graphical model for each iteration is shown
in Fig. 2. Specifically, to address the aforementioned difficulties, we
Figure 2: Graphical model of MissNet at each iteration.
employ the following steps: (i)we considerÂ¤ğ‘ºis observed in each
iteration, and we update it at the end of the iteration using Â¤ğš¯;(ii)we
regardÂ¤ğ‘¼as a model parameter, thus {Ëœğ‘¿,ğ’,ğ‘­}are independent with
{Â¤ğ‘º,Â¤ğ‘½}. We alternate the inference of {ğ’,ğ‘­}andÂ¤ğ‘½and the update
of the model parameters; (iii)we employ a Viterbi approximation
and infer the most likely ğ‘­.
4.2.1 E-step. GivenÂ¤ğ‘¼, we can infer{ğ’,ğ‘­}andÂ¤ğ‘½independently.
Letğ’ğ‘¡denote the indices of the observed entries of Ëœğ’™ğ‘¡. The
observed-only data Â¯ğ’™ğ‘¡and the corresponding observed-only obser-
vation matrix Â¯ğ‘¼(ğ‘˜)
ğ‘¡are defined as follows:
ğ’ğ‘¡={ğ‘–|ğ‘¾ğ‘–,ğ‘¡>0,ğ‘–=1,...,ğ‘},
Â¯ğ’™ğ‘¡=Ëœğ’™ğ‘¡(ğ’ğ‘¡),Â¯ğ‘¼(ğ‘˜)
ğ‘¡=ğ‘¼(ğ‘˜)(ğ’ğ‘¡,:). (12)
Inferring ğ‘­andğ’.ğ‘­andğ’are coupled, and so must be jointly
determined. We first use a Viterbi approximation to find the most
likely regime assignments ğ‘­that maximize the log-likelihood Eq. (11).
The likelihood term obtained during the calculation of ğ‘­also acts
as a Kalman Filter (forward algorithm). Then, we infer ğ’with a
Rauch-Tung-Streibel (RTS) smoother (backward algorithm).
In a Viterbi approximation, finding ğ‘­requires the partial cost
ğ½ğ‘¡,ğ‘¡âˆ’1,ğ‘˜,ğ‘™when the switch is to regime ğ‘˜at timeğ‘¡from regime ğ‘™at
timeğ‘¡âˆ’1. To calculate the partial cost, we define the following LDS
state and variance terms:
ğğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™=ğ‘©ğğ‘¡âˆ’1|ğ‘¡âˆ’1,ğ‘˜,ğ‘™,
ğğ‘¡|ğ‘¡,ğ‘˜,ğ‘™=ğğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™+ğ‘«ğ‘¡,ğ‘˜,ğ‘™(Â¯ğ’™ğ‘¡âˆ’Â¯ğ‘¼(ğ‘˜)
ğ‘¡ğ‘©ğğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™),
ğš¿ğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™=ğ‘©ğš¿ğ‘¡âˆ’1|ğ‘¡âˆ’1,ğ‘˜,ğ‘™ğ‘©â€²+ğœ2
ğ’ğ‘°,
ğš¿ğ‘¡|ğ‘¡,ğ‘˜,ğ‘™=(ğ‘°âˆ’ğ‘«ğ‘¡,ğ‘˜,ğ‘™Â¯ğ‘¼(ğ‘˜)
ğ‘¡)ğš¿ğ‘¡âˆ’1|ğ‘¡âˆ’1,ğ‘˜,ğ‘™,
ğ‘«ğ‘¡,ğ‘˜,ğ‘™=ğš¿ğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™Â¯ğ‘¼(ğ‘˜)â€²
ğ‘¡(Â¯ğ‘¼(ğ‘˜)
ğ‘¡ğš¿ğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™Â¯ğ‘¼(ğ‘˜)â€²
ğ‘¡+ğœ2
ğ‘¿(ğ‘˜)ğ‘°)âˆ’1,
(13)
with the initial state:
ğ1|1,ğ‘˜=ğ’›0+ğ‘«1,ğ‘˜(Â¯ğ’™1âˆ’Â¯ğ‘¼(ğ‘˜)
1ğ’›0),
ğš¿1|1,ğ‘˜=(ğ‘°âˆ’ğ‘«1,ğ‘˜Â¯ğ‘¼(ğ‘˜)
1)ğš¿0,
ğ‘«1,ğ‘˜=ğš¿0Â¯ğ‘¼(ğ‘˜)â€²
1(Â¯ğ‘¼(ğ‘˜)
1ğš¿0Â¯ğ‘¼(ğ‘˜)â€²
1+ğœ2
ğ‘¿(ğ‘˜)ğ‘°)âˆ’1, (14)
where ğğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™andğğ‘¡|ğ‘¡,ğ‘˜,ğ‘™are the one-step predicted LDS state
and the best-filtered state estimates at ğ‘¡, respectively, given the
switch is in regime ğ‘˜at timeğ‘¡and in regime ğ‘™at timeğ‘¡âˆ’1and only
theğ‘¡âˆ’1measurements are known. Similar definitions are used for
ğš¿ğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™andğš¿ğ‘¡|ğ‘¡,ğ‘˜,ğ‘™.
 
2299Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
The partial cost is obtained by calculating the logarithm of
Eq. (11) related to ğ’‡ğ‘¡:
ğ½ğ‘¡,ğ‘¡âˆ’1,ğ‘˜,ğ‘™=1
2(Â¯ğ’™ğ‘¡âˆ’Â¯ğ‘¼(ğ‘˜)
ğ‘¡ğğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™)â€²ğ‘¹(Â¯ğ’™ğ‘¡âˆ’Â¯ğ‘¼(ğ‘˜)
ğ‘¡ğğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™)
âˆ’1
2logdet(ğ‘¹)+ğ¿
2ğ‘™ğ‘œğ‘”(2ğœ‹)
+1
2(Ë†ğ’™ğ‘¡âˆ’ğ†(ğ‘˜))â€²ğš¯(ğ‘˜)(Ë†ğ’™ğ‘¡âˆ’ğ†(ğ‘˜))
âˆ’1
2logdet(ğš¯(ğ‘˜))+ğ‘
2log(2ğœ‹)âˆ’log(ğš·ğ‘˜,ğ‘™),(15)
ğ‘¹=(Â¯ğ‘¼(ğ‘˜)
ğ‘¡ğš¿ğ‘¡|ğ‘¡âˆ’1,ğ‘˜,ğ‘™Â¯ğ‘¼(ğ‘˜)â€²
ğ‘¡+ğœ2
ğ‘¿(ğ‘˜)ğ‘°)âˆ’1.
Once all partial costs are obtained, it is well-known how to apply
a Viterbi inference to a discrete Markov process to obtain the most
likely regime assignments ğ‘­[49].
Then, we infer ğ’. Let the posteriors of ğ’›ğ‘¡be as follows:
ğ‘(ğ’›ğ‘¡|Ëœğ’™1,..., Ëœğ’™ğ‘¡)=N(ğ’›ğ‘¡|ğğ‘¡,ğš¿ğ‘¡),
ğ‘(ğ’›ğ‘¡|Ëœğ’™1,..., Ëœğ’™ğ‘‡)=N(ğ’›ğ‘¡|Ë†ğğ‘¡,Ë†ğš¿ğ‘¡). (16)
We now obtain ğğ‘¡,ğš¿ğ‘¡,ğš¿ğ‘¡|ğ‘¡âˆ’1using ğ‘­; thus, in practice, we have
conducted a Kalman Filter. We apply the RTS smoother to infer ğ’.
Ë†ğğ‘¡=ğğ‘¡+ğ‘ªğ‘¡(Ë†ğğ‘¡+1âˆ’ğ‘©ğğ‘¡),
Ë†ğš¿ğ‘¡=ğš¿ğ‘¡+ğ‘ªğ‘¡(Ë†ğš¿ğ‘¡+1âˆ’ğš¿ğ‘¡|ğ‘¡âˆ’1)ğ‘ªâ€²
ğ‘¡,
ğ‘ªğ‘¡=ğš¿ğ‘¡ğ‘©â€²(ğš¿ğ‘¡|ğ‘¡âˆ’1)âˆ’1, (17)
E[ğ’›ğ‘¡]=Ë†ğğ‘¡,
E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡âˆ’1]=Ë†ğš¿ğ‘¡ğ‘ªâ€²
ğ‘¡+Ë†ğğ‘¡Ë†ğâ€²
ğ‘¡âˆ’1,
E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡]=Ë†ğš¿ğ‘¡+Ë†ğğ‘¡Ë†ğâ€²
ğ‘¡âˆ’1. (18)
InferringÂ¤ğ‘½.We apply Bayesâ€™ theorem to Eq. (7) and (8) to obtain
the posteriors ğ‘(ğ’—(ğ‘˜)
ğ‘—|ğ’”(ğ‘˜)
ğ‘—)=N(ğ’—(ğ‘˜)
ğ‘—|ğ‚(ğ‘˜)
ğ‘—,ğš¼(ğ‘˜)):
ğ‚(ğ‘˜)
ğ‘—=ğ‘´(ğ‘˜)âˆ’1ğ‘¼(ğ‘˜)â€²ğ’”(ğ‘˜)
ğ‘—,
ğš¼(ğ‘˜)=ğœ2
ğ‘º(ğ‘˜)ğ‘´(ğ‘˜)âˆ’1,
ğ‘´(ğ‘˜)=ğ‘¼(ğ‘˜)â€²ğ‘¼(ğ‘˜)+ğœâˆ’2
ğ‘½(ğ‘˜)ğœ2
ğ‘º(ğ‘˜)ğ‘°,
E[ğ’—(ğ‘˜)
ğ‘—]=ğ‚(ğ‘˜)
ğ‘—,
E[ğ’—(ğ‘˜)
ğ‘—ğ’—(ğ‘˜)â€²
ğ‘—]=ğš¼(ğ‘˜)+ğ‚(ğ‘˜)
ğ‘—ğ‚(ğ‘˜)â€²
ğ‘—. (19)
4.2.2 M-step. After obtaining{ğ’,ğ‘­}andÂ¤ğ‘½, we update the model
parameters to maximize the expectation of the log-likelihood:
ğœƒğ‘›ğ‘’ğ‘¤=arg max
ğœƒğ‘„(ğœƒ,ğœƒğ‘œğ‘™ğ‘‘), (20)
ğ‘„(ğœƒ,ğœƒğ‘œğ‘™ğ‘‘)=Eğ’,ğ‘­,Â¤ğ‘½|ğœƒğ‘œğ‘™ğ‘‘[logğ‘(Ëœğ‘¿,ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­|ğœƒ)]âˆ’ğ¾âˆ‘ï¸
ğ‘˜=1ğœ†||ğš¯(ğ‘˜)||ğ‘œğ‘‘,1,
where we incorporate the â„“1-norm constraint.
Parameters for the imputation model, Â¤ğ‘¼and{ğ‘©,ğ’›0,ğš¿0,ğœğ’,Â¤ğœğ‘¿,Â¤ğœğ‘º,Â¤ğœğ‘½},
can be obtained by taking the derivative of ğ‘„(ğœƒ,ğœƒğ‘œğ‘™ğ‘‘). For ğ‘¼(ğ‘˜),we update each row ğ‘¼(ğ‘˜)
ğ‘–,:individually:
(ğ‘¼(ğ‘˜)
ğ‘–,:)ğ‘›ğ‘’ğ‘¤=ğ‘¨(ğ‘˜)
1ğ‘¨(ğ‘˜)âˆ’1
2, (21)
ğ‘¨(ğ‘˜)
1=ğ›¼ğœâˆ’2
ğ‘º(ğ‘˜)ğ‘âˆ‘ï¸
ğ‘—=1ğ‘º(ğ‘˜)
ğ‘–,ğ‘—E[ğ’—(ğ‘˜)â€²
ğ‘—]+(1âˆ’ğ›¼)ğœâˆ’2
ğ‘¿(ğ‘˜)ğ‘‡âˆ‘ï¸
ğ‘¡=1,ğ’‡ğ‘¡âˆˆğ‘˜ğ‘¾ğ‘–,ğ‘¡Ëœğ‘¿ğ‘–,ğ‘¡E[ğ’›â€²
ğ‘¡],
ğ‘¨(ğ‘˜)
2=ğ›¼ğœâˆ’2
ğ‘º(ğ‘˜)ğ‘âˆ‘ï¸
ğ‘—=1E[ğ’—(ğ‘˜)
ğ‘—ğ’—(ğ‘˜)â€²
ğ‘—]+(1âˆ’ğ›¼)ğœâˆ’2
ğ‘¿(ğ‘˜)ğ‘‡âˆ‘ï¸
ğ‘¡=1,ğ’‡ğ‘¡âˆˆğ‘˜ğ‘¾ğ‘–,ğ‘¡E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡],
where 0â‰¤ğ›¼â‰¤1is a hyperparameter employed as a trade-off for
the contributions of inter-correlation and temporal dependency.
The details of updating {ğ‘©,ğ’›0,ğš¿0,ğœğ’,Â¤ğœğ‘¿,Â¤ğœğ‘º,Â¤ğœğ‘½}are presented in
Appendix A.3.
For the network inference model, we calculate Ë†ğ‘¿with Eq. (22)
and then update ğ†(ğ‘˜)by calculating the empirical mean of Ë†ğ‘¿be-
longing to the ğ‘˜ğ‘¡â„-regime and ğš¯(ğ‘˜)by solving the graphical lasso
problem shown in Eq. (23) via ADMM.
Ë†ğ’™ğ‘¡=ğ‘¾:,ğ‘¡â—¦Ëœğ’™ğ‘¡+(1âˆ’ğ‘¾:,ğ‘¡)â—¦(ğ‘¼(ğ’‡ğ‘¡))ğ‘›ğ‘’ğ‘¤Ë†ğğ‘¡, (22)
minimizeğš¯(ğ‘˜)âˆˆğ‘†ğ‘
++ğœ†||ğš¯(ğ‘˜)||ğ‘œğ‘‘,1âˆ’ğ‘‡âˆ‘ï¸
ğ‘¡=1,ğ’‡ğ‘¡âˆˆğ‘˜ğ‘™ğ‘™(Ë†ğ’™ğ‘¡,ğš¯(ğ‘˜)). (23)
For the regime-switching model, the initial state distribution and
the Markov transition matrix are updated as follows:
ğ…0=ğ’‡1,ğš·=ğ‘‡âˆ‘ï¸
ğ‘¡=2ğ’‡ğ‘¡ğ’‡â€²
ğ‘¡âˆ’1
diagğ‘‡âˆ‘ï¸
ğ‘¡=2ğ’‡ğ‘¡âˆ’1
. (24)
4.2.3 UpdateÂ¤ğ‘º.We update ğ‘º(ğ‘˜)at the end of each iteration. As
shown in Eq. (25), we define the off-diagonal elements of ğ‘º(ğ‘˜)as
partial correlations calculated from the network ğš¯(ğ‘˜)to encode
the inter-correlation.
ğ‘º(ğ‘˜)
ğ‘–,ğ‘—=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³1(ğ‘–=ğ‘—)
âˆ’(ğš¯(ğ‘˜)
ğ‘–,ğ‘—âˆšï¸ƒ
ğš¯(ğ‘˜)
ğ‘–,ğ‘–âˆšï¸ƒ
ğš¯(ğ‘˜)
ğ‘—,ğ‘—) (ğ‘– â‰ ğ‘—). (25)
4.2.4 Overall algorithm. We have the overall algorithm shown as
Alg. 1 to obtain a local optimal solution of Eq. (11). Given a partially
observed multivariate time series Ëœğ‘¿, an indicator matrix ğ‘¾, the
dimension of latent state ğ¿, the number of regimes ğ¾, the network
parameterğ›¼, and the sparse parameter ğœ†, our algorithm aims to
find the latent factors ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­, other model parameters in ğœƒ,
and imputed time series Ë†ğ‘¿.
TheMissNet algorithm starts by initializing Ë†ğ‘¿with a linear in-
terpolation, and by randomly initializing ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­, andğœƒ(Line 3).
Then, it alternately updates the latent factors and parameters until
they converge. In each iteration, we consider Â¤ğ‘ºto be given andÂ¤ğ‘¼
to be a model parameter. In an iteration, we first conduct a Viterbi
approximation to calculate the most likely regime assignments ğ‘­
(Line 5-12). Then, we infer the expectations of ğ’andÂ¤ğ‘½(Line 13-15
and Line 16-18), and we update Â¤ğ‘¼and model parameters ğœƒ(Line 20),
and at the end of the iteration, we update Â¤ğ‘º(Line 21).
4.3 Complexity analysis
Lemma 1. The time complexity of MissNet is
ğ‘‚(#ğ‘–ğ‘¡ğ‘’ğ‘ŸÂ·(ğ¾2Ãğ‘‡
ğ‘¡=1(ğ¿3+ğ¿2ğ‘ğ‘¡+ğ¿ğ‘2
ğ‘¡+ğ‘3
ğ‘¡)+ğ¾ğ¿2ğ‘2+ğ¾ğ‘‡ğ¿2ğ‘+ğ¾ğ‘3)).
 
2300KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
Algorithm 1 MissNet ( Ëœğ‘¿,ğ‘¾,ğ¿,ğ¾,ğ›¼,ğœ† )
1:Input: (a) partially observed multivariate time series Ëœğ‘¿,
(b) indicator matrix ğ‘¾,
(c) and hyperparameters ğ¿,ğ¾,ğ›¼,ğœ†
2:Output: latent factors ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­model parameters ğœƒ, and Ë†ğ‘¿
3:Initialize Ë†ğ‘¿with linear interpolation, ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­, andğœƒ;
4:repeat
5:forğ‘¡=1 :ğ‘‡do
6: forğ‘˜=1 :ğ¾do
7: forğ‘™=1 :ğ¾do
8: Calculate partial cost ğ½ğ‘¡,ğ‘¡âˆ’1,ğ‘˜,ğ‘™using Eq. (15)
9: end for
10: end for
11: end for
12: Infer ğ‘­and obtain ğğ‘¡,ğš¿ğ‘¡based on Eq. (13), (14)
13: forğ‘¡=ğ‘‡: 1do
14: Infer Ë†ğğ‘¡,Ë†ğš¿ğ‘¡,E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡âˆ’1],E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡]by Eq. (17), (18)
15: end for
16: forğ‘—=1 :ğ‘do
17: InferE[ğ’—(ğ‘˜)
ğ‘—],E[ğ’—(ğ‘˜)
ğ‘—ğ’—(ğ‘˜)â€²
ğ‘—]using Eq. (19)
18: end for
19: Setğ’={Ë†ğ1,..., Ë†ğğ‘‡},{ğ‘½(ğ‘˜)={ğ‚(ğ‘˜)
1,...,ğ‚(ğ‘˜)
ğ‘}}ğ¾
ğ‘˜=1
20: UpdateÂ¤ğ‘¼,ğœƒand Ë†ğ‘¿by Eq. (21) - (24)
21: UpdateÂ¤ğ‘ºbased on Eq. (25)
22:until convergence;
23:return{ğ’,Â¤ğ‘½,Â¤ğ‘¼,Â¤ğ‘º,ğ‘­,ğœƒ,Ë†ğ‘¿};
Proof. See Appendix A.2. â–¡
ğ‘ğ‘¡represents the number of observed features of Â¯ğ’™ğ‘¡. Note that
ğ¾2Ãğ‘‡
ğ‘¡=1(ğ¿3+ğ¿2ğ‘ğ‘¡+ğ¿ğ‘2
ğ‘¡+ğ‘3
ğ‘¡)is upper bounded by ğ¾2ğ‘‡ğ‘3. In
practice, the length of the time series ( ğ‘‡) is often orders of mag-
nitude greater than the number of features ( ğ‘). Hence, the actual
running time of MissNet is dominated by the term related to ğ‘‡,
which is linear in ğ‘‡.
Lemma 2. The space complexity of MissNet is
ğ‘‚(ğ‘‡ğ‘+ğ¾2ğ‘‡ğ¿2+ğ¾ğ¿2ğ‘+ğ¾ğ‘2).
Proof. See Appendix A.2. â–¡
5 Experiments
In this section, we empirically evaluate our approach against state-
of-the-art baselines on 12 datasets. We present experimental results
for the following questions:
Q1. Effectiveness: How accurate is the proposed MissNet for
recovering missing values?
Q2. Scalability: How does the proposed algorithm scale?
Q3. Interpretability: How can MissNet help us understand the
data?
5.1 Experimental setup
5.1.1 Datasets. We use the following datasets.
Synthetic. We generate two types of synthetic data, PatternA and
PatternB, five times each, by defining ğ’andÂ¤ğ‘¼. We setğ‘‡=1000,ğ‘=50andğ¿=10(Appendix B.1). PatternA has one regime ( ğ¾=1),
and in PatternB ( ğ¾=2), two regimes switch at every 200timesteps.
MotionCapture. This dataset contains nine types of full body
motions of MotionCapture database3. Each motion measures the
positions of 41bones in the human body, resulting in a total of 123
features (X, Y, and Z coordinates).
Motes. This dataset consists of temperature measurements from
the54sensors deployed in the Intel Berkeley Research Lab4. We
use hourly data for the first two weeks (03-01 âˆ¼03-14). Originally,
9.6%of the data is missing, including a blackout from 03-10 to 03-11
where all the values are missing.
5.1.2 Data preprocessing. We generate a synthetic missing block at
a length of 0âˆ¼5%of the data length and place it randomly until the
total missing rate reaches {10,20,...80%}. Thus, a missing block
can be longer than 0.05ğ‘‡when it overlaps. An additional 10%of
missing values are added for hyperparameter tuning. Each dataset
feature is normalized independently using a z-score so that each
dataset has a zero mean and a unit variance.
5.1.3 Comparison methods. We compare our method with state-of-
the-art imputation methods ranging from classical baselines (Linear
and Quadratic), MF-based methods (SoftImpute, CDRec and TRMF),
SSM-based methods (DynaMMo and DCMF), to DL-based methods
(BRITS, SAITS and TIDER).
â€¢Linear/Quadratic5use linear/quadratic equations to inter-
polate missing values.
â€¢SoftImpute [ 30] first fills in missing values with zero, then
alternates between recovering the missing values and updat-
ing the SVD using the recovered matrix.
â€¢CDRec [ 22] is a memory-efficient algorithm that iterates
centroid decomposition (CD) and missing value imputation
until they converge.
â€¢TRMF [ 55] is based on MF that imposes temporal dependency
among the data points with the AR model.
â€¢DynaMMo [ 25] first fills in missing values using linear in-
terpolation and then uses the EM algorithm to iteratively
recover missing values and update the LDS model.
â€¢DCMF [ 5] adds a contextual constraint to SSM and captures
inter-correlation by a predefined network. As suggested in
the original paper, we give the cosine similarity between
each pair of time series calculated after linear interpolation
as a predefined network. This method is similar to MissNet
if we setğ¾=1, employ a predefined network that is fixed
throughout the algorithm, and eliminate the effect of regime-
switching and network inference models from MissNet.
â€¢BRITS [ 7] imputes missing values according to hidden states
from bidirectional RNN.
â€¢SAITS [12] is a self-attention-based model that jointly opti-
mizes imputation and reconstruction to perform the missing
value imputation of multivariate time series.
â€¢TIDER [ 26] learns disentangled seasonal and trend repre-
sentations by employing a neural network combined with
MF.
3http://mocap.cs.cmu.edu
4https://db.csail.mit.edu/labdata/labdata.html
5https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html
 
2301Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) PatternA
(ğ‘‡=1000)
(b) PatternB
(ğ‘‡=1000)
(c) Run
(ğ‘‡=145)
(d) Bouncy walk
(ğ‘‡=644)
(e) Swing shoulder
(ğ‘‡=804)
(f) Walk slow
(ğ‘‡=1223)
(g) Mawashigeri
(ğ‘‡=1472)
(h) Jump distance
(ğ‘‡=547)
(i) Wave hello
(ğ‘‡=299)
(j) Football throw
(ğ‘‡=1091)
(k) Boxing
(ğ‘‡=773)
(l) Motes
(ğ‘‡=336)
Figure 3: RMSE of (a), (b) Synthetic ( ğ‘=50), (c)âˆ¼(k) MotionCapture ( ğ‘=123) and (l) Motes ( ğ‘=54) datasets.
Figure 4: Critical difference diagram of real-world datasets.
5.1.4 Hyperparameter setting. ForMissNet, we use the latent di-
mensions of 10,30and15for Synthetic, MotionCapture and Motes,
respectively, and we set ğœ†=1.0,ğ›¼=0.5for all datasets. We set
the correct number of regimes on Synthetic datasets; we vary
ğ¾={1,2,3}for other datasets. We list the detailed hyperparameter
settings for the baselines in Appendix B.2.
5.1.5 Evaluation metric. To evaluate the effectiveness, we use Root
Mean Square Error (RMSE) of the observed time series [5].
5.2 Results
5.2.1 Q1. Effectiveness. We show the effectiveness of MissNet
over baselines in missing value imputation.
Synthetic. Fig. 3 (a) and (b) show the results obtained with Syn-
thetic datasets. SSM and MF-based methods perform worse with
PatternB than with PatternA due to the increased complexity of
data. DL-based methods, especially BRITS, are less affected thanks
to their high modeling power. MissNet significantly outperforms
DCMF for PatternB although it produces similar results for Pat-
ternA. This is because DCMF fails to capture inter-correlation with
PatternB since it can only use one predefined network and cannot
afford a change of network. Meanwhile, MissNet can capture the
inter-correlation for two different regimes thanks to our regime-
switching model. However, MissNet fails to discover the correct
transition when the missing rate exceeds 70%, and RMSE becomes
similar to DCMF.
MotionCapture and Motes. The results for MotionCapture and
Motes datasets are shown in Fig. 3 (c) âˆ¼(l). We can see that MissNetand DCMF constantly outperform other baselines thanks to their
ability to exploit inter-correlation explicitly.
Fig. 4 shows the corresponding critical difference diagram for
all missing rates based on the Wilcoxon-Holm method [ 20], where
methods that are not connected by a bold line are significantly
different in average rank. This confirms that MissNet significantly
outperforms other methods, including DCMF, in average rank. Our
algorithm for repeatedly inferring networks and the use of â„“1-norm
enables the inference of adequate networks for imputation, con-
tributing to better results than DCMF, which uses cosine similarity
as a predefined network that may contain spurious correlations
in the presence of missing values. Note that MissNet and DCMF
exhibit only minor differences when the missing rate is low (10%)
because a plausible predefined network can be calculated from
observed data.
Classical Linear and Quadratic baselines are unsuitable for im-
puting missing blocks since they impute missing values mostly
from neighboring observed points and cannot capture temporal
patterns when there are large gaps. DL-based methods lack suffi-
cient training data and are not suitable for the data we use here,
making them perform particularly poorly at a high missing rate, as
also noted in [ 23]. MF-based methods, SoftImpute and CDRec, have
a higher RMSE than SSM-based methods since they do not model
the temporal dynamics of the data. TRMF utilizes temporal depen-
dency with the AR model, however, it can only capture certain
lags specified on the hyperparameter of the AR model. SSM-based
methods are superior in imputation to other groups owing to their
ability to capture temporal dependency in latent space. DynaMMo
implicitly captures inter-correlation in latent space. Therefore, it is
no match for MissNet or DCMF.
Fig. 5 demonstrates the results for the MotionCapture Run dataset
(missing rate =60%). We compare the imputation result for the sen-
sor at RKNE provided by the top five methods in terms of average
rank, including MissNet, in Fig. 5 (a). BRITS and SoftImpute fail
to capture the dynamics of time series while providing a good fit
to observed values. The imputation of DynaMMo is smooth, but
 
2302KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
(a) Imputation results at RKNE
 (b) Y-network of MissNet
Figure 5: Case study on MotionCapture Run dataset.
(a) Impact of ğ¿
 (b) Impact of ğ›¼
 (c) Impact of ğœ†
Figure 6: Hyperparameter sensitivity results.
some parts are imprecise since it cannot explicitly exploit inter-
correlation. MissNet and DCMF can effectively exploit other ob-
served features associated with RKNE, thereby accurately imputing
missing values where other methods fail (e.g., ğ‘‡=20âˆ¼40,60âˆ¼80).
Fig. 5 (b) shows the sensor network of Y-coordinate values obtained
byMissNet plotted on the human body, where a green/yellow dot
(node) indicates a sensor placed on the front/back of the body and
the thickness and color (blue/red) of the edges are the value and
sign (positive/negative) of partial correlations, respectively. We can
see that the sensors located close together have edges, meaning they
are conditionally dependent given all other sensor values. For ex-
ample, the sensor at RKNE has edges between RTHI, RSHN, LKNE,
and LTHI. They are located to RKNE nearby and show similar dy-
namics, thus it is reasonable to consider that they are connected.
Since MissNet can infer such a meaningful network from partially
observed data, the imputation of MissNet is more accurate than
that of DCMF.
5.2.2 Hyperparameter sensitivity. We take the Motes dataset and
show the impact of hyperparameters: the latent dimension ğ¿, the
network parameter ğ›¼, and the sparse parameter ğœ†. We show the
mean RMSE of all missing rates.
Latent dimension. Fig. 6 (a) shows the impact of ğ¿. Asğ¿becomes
larger, the modelâ€™s fitting against the observed data increases. As
we can see, the RMSE is constantly decreasing as ğ¿increases and
stabilizes after 15. This shows that MissNet does not overfit the
observed data even for a large ğ¿.
Network parameter. ğ›¼determines the contributions of inter-
correlation and temporal dependency to learning Â¤ğ‘¼. Ifğ›¼=0, the
Figure 7: Scalability of MissNet.
(a) Results of regime assignments
(b) Regime #1,#of edges: 116
 (c) Regime #2,#of edges: 134
Figure 8: Case study on Motes dataset.
contextual matrix Â¤ğ‘ºis ignored. If ğ›¼=1, onlyÂ¤ğ‘ºis considered for
learningÂ¤ğ‘¼. Fig. 6 (b) shows the results of varying ğ›¼and they are
robust except when ğ›¼=1(RMSE =0.76). We can see that ğ›¼=0.4
shows the best result, indicating that both temporal dependency
and inter-correlation are important for precise imputation.
Sparse parameter. ğœ†controls the sparsity of the networks Â¤ğš¯
throughâ„“1-norm. The bigger ğœ†becomes, the more sparse the net-
works become, resulting in MissNet considering only strong inter-
play. By contrast, when ğœ†is small, MissNet considers insignificant
interplays. Fig. 6 (c) shows the impact of ğœ†. We can see that the
sparsity of the networks affects the accuracy, and the best ğœ†exists
between 0.1and10. Thus, the â„“1-norm constraint helps MissNet
to exploit important relationships.
5.2.3 Q2. Scalability. We test the scalability of the MissNet algo-
rithm by changing the number of the data length ( ğ‘‡) in PatternA.
Fig. 7 shows the computation time for one iteration plotted with the
data length. As it shows, our proposed MissNet algorithm scales
linearly with regard to the data length ğ‘‡.
5.2.4 Q3. Interpretability. We demonstrate how MissNet helps us
understand data. We have shown an example with the MotionCap-
ture Run dataset in Fig. 5 (b) where MissNet provides an inter-
pretable network. Here, we demonstrate the results on the Motes
dataset (missing rate =30%) of MissNet (ğ¾=2). Fig. 8 (a) shows
the regime assignments ğ‘­, and MissNet mostly assigns night hours
to regime #1and working hours (about 9am. to 10pm.) to regime
#2, suggesting that they have different networks. Fig. 8 (b) and (c)
show the networks for regimes #1and#2obtained by MissNet
plotted on the building layout. The sensor numbers in the figure
are plotted on the actual sensor deployments. As we can see, the
two regimes have different networks, and a common feature is that
the neighboring sensors tend to form edges, which aligns with our
expectations, considering that the sensors measure temperature
and, thus, neighboring sensors correlate. The network of regime #2
has more edges than that of #1, and the edges are 1.2times longer
 
2303Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
on average, which might be caused by air convection due to the
movement of people during working hours. Consequently, MissNet
provides regime assignments and sparse networks, which help us
understand how data can be separated and important relationships
for imputation.
6 Conclusion
In this paper, we proposed an effective missing value imputation
method for multivariate time series, namely MissNet, which cap-
tures temporal dependency based on latent space and inter-correlation
by the inferred networks while discovering regimes. Our proposed
method has the following properties: (a) Effective : it outperforms
the state-of-the-art algorithms for multivariate time series imputa-
tion. (b) Scalable : the computation time of MissNet scales linearly
with regard to the length of the data. (c) Interpretable : it provides
sparse networks and regime assignments, which help us understand
the important relationships for imputation visually. Our extensive
experiments demonstrated the above properties of MissNet.
Acknowledgments
The authors would like to thank the anonymous referees for their
valuable comments and helpful suggestions. This work was sup-
ported by JSPS KAKENHI Grant-in-Aid for Scientific Research Num-
ber JP21H03446, JP22K17896, NICT JPJ012368C03501, JST-AIP JP-
MJCR21U4, JST CREST JPMJCR23M3.
References
[1]Juan Miguel Lopez Alcaraz and Nils Strodthoff. 2022. Diffusion-based time series
imputation and forecasting with structured state space models. arXiv preprint
arXiv:2208.09399 (2022).
[2]Parikshit Bansal, Prathamesh Deshpande, and Sunita Sarawagi. 2021. Missing
Value Imputation on Multidimensional Time Series. Proc. VLDB Endow. 14, 11
(jul 2021), 2533â€“2545.
[3]Jeroen Berrevoets, Fergus Imrie, Trent Kyono, James Jordon, and Mihaela van der
Schaar. 2023. To impute or not to impute? missing data in treatment effect
estimation. In International Conference on Artificial Intelligence and Statistics.
PMLR, 3568â€“3590.
[4]Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.
2011. Distributed Optimization and Statistical Learning via the Alternating
Direction Method of Multipliers. Found. Trends Mach. Learn. 3, 1 (2011), 1â€“122.
[5]Yongjie Cai, Hanghang Tong, Wei Fan, and Ping Ji. 2015. Fast mining of a network
of coevolving time series. In Proceedings of the 2015 SIAM International Conference
on Data Mining. SIAM, 298â€“306.
[6]Yongjie Cai, Hanghang Tong, Wei Fan, Ping Ji, and Qing He. 2015. Facets: Fast
comprehensive mining of coevolving high-order time series. In KDD. 79â€“88.
[7]Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018. Brits:
Bidirectional recurrent imputation for time series. Advances in neural information
processing systems 31 (2018).
[8]Stanislas Chambon, Mathieu N Galtier, Pierrick J Arnal, Gilles Wainrib, and
Alexandre Gramfort. 2018. A deep learning architecture for temporal sleep stage
classification using multivariate and multimodal time series. IEEE Transactions
on Neural Systems and Rehabilitation Engineering 26, 4 (2018), 758â€“769.
[9]Xinyu Chen and Lijun Sun. 2021. Bayesian temporal factorization for multi-
dimensional time series prediction. IEEE Transactions on Pattern Analysis and
Machine Intelligence 44, 9 (2021), 4659â€“4673.
[10] Andrea Cini, Ivan Marisca, and Cesare Alippi. 2021. Filling the g_ap_s: Mul-
tivariate time series imputation by graph neural networks. arXiv preprint
arXiv:2108.00298 (2021).
[11] Joel Janek Dabrowski, Ashfaqur Rahman, Andrew George, Stuart Arnold, and
John McCulloch. 2018. State Space Models for Forecasting Water Quality Vari-
ables: An Application in Aquaculture Prawn Farming (KDD â€™18). Association for
Computing Machinery, New York, NY, USA, 177â€“185.
[12] Wenjie Du, David CÃ´tÃ©, and Yan Liu. 2023. Saits: Self-attention-based imputation
for time series. Expert Systems with Applications 219 (2023), 119619.
[13] Yangxin Fan, Xuanji Yu, Raymond Wieser, David Meakin, Avishai Shaton, Jean-
Nicolas Jaubert, Robert Flottemesch, Michael Howell, Jennifer Braid, Laura Bruck-
man, Roger French, and Yinghui Wu. 2023. Spatio-Temporal Denoising Graph Au-
toencoders with Data Augmentation for Photovoltaic Data Imputation. Proc. ACMManag. Data 1, 1, Article 50 (may 2023), 19 pages. https://doi.org/10.1145/3588730
[14] Emily Fox, Erik Sudderth, Michael Jordan, and Alan Willsky. 2008. Nonparametric
Bayesian learning of switching linear dynamical systems. Advances in neural
information processing systems 21 (2008).
[15] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2008. Sparse inverse
covariance estimation with the graphical lasso. Biostatistics 9, 3 (2008), 432â€“441.
[16] Zoubin Ghahramani. 1998. Learning dynamic Bayesian networks. Springer Berlin
Heidelberg, Berlin, Heidelberg, 168â€“197.
[17] N Hairi, Hanghang Tong, and Lei Ying. 2019. NetDyna: mining networked
coevolving time series with missing values. In 2019 IEEE International Conference
on Big Data (Big Data). IEEE, 503â€“512.
[18] David Hallac, Youngsuk Park, Stephen P. Boyd, and Jure Leskovec. 2017. Network
Inference via the Time-Varying Graphical Lasso. In KDD. 205â€“213.
[19] David Hallac, Sagar Vare, Stephen P. Boyd, and Jure Leskovec. 2017. Toeplitz
Inverse Covariance-Based Clustering of Multivariate Time Series Data. In KDD.
215â€“223.
[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,
and Pierre-Alain Muller. 2019. Deep learning for time series classification: a
review. Data mining and knowledge discovery 33, 4 (2019), 917â€“963.
[21] Baoyu Jing, Hanghang Tong, and Yada Zhu. 2021. Network of tensor time series.
InProceedings of the Web Conference 2021. 2425â€“2437.
[22] Mourad Khayati, Michael BÃ¶hlen, and Johann Gamper. 2014. Memory-efficient
centroid decomposition for long time series. In 2014 IEEE 30th International
Conference on Data Engineering. IEEE, 100â€“111.
[23] Mourad Khayati, Alberto Lerner, Zakhar Tymchenko, and Philippe CudrÃ©-
Mauroux. 2020. Mind the gap: an experimental evaluation of imputation of
missing values techniques in time series. In Proceedings of the VLDB Endowment,
Vol. 13. 768â€“782.
[24] Mladen Kolar and Eric P Xing. 2012. Estimating sparse precision matrices from
data with missing values. In International Conference on Machine Learning. 635â€“
642.
[25] Lei Li, James McCann, Nancy S Pollard, and Christos Faloutsos. 2009. Dynammo:
Mining and summarization of coevolving sequences with missing values. In KDD.
507â€“516.
[26] SHUAI LIU, Xiucheng Li, Gao Cong, Yile Chen, and YUE JIANG. 2022. Multivari-
ate Time-series Imputation with Disentangled Temporal Representations. In The
Eleventh International Conference on Learning Representations.
[27] Yuehua Liu, Tharam Dillon, Wenjin Yu, Wenny Rahayu, and Fahed Mostafa. 2020.
Missing value imputation for industrial IoT sensor data with large gaps. IEEE
Internet of Things Journal 7, 8 (2020), 6855â€“6867.
[28] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social
recommendation using probabilistic matrix factorization. In Proceedings of the
17th ACM conference on Information and knowledge management. 931â€“940.
[29] Solt KovÃ¡cs Malte Londschien and Peter BÃ¼hlmann. 2021. Change-Point Detection
for Graphical Models in the Presence of Missing Values. Journal of Computational
and Graphical Statistics 30, 3 (2021), 768â€“779.
[30] Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. 2010. Spectral regulariza-
tion algorithms for learning large incomplete matrices. The Journal of Machine
Learning Research 11 (2010), 2287â€“2322.
[31] Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, and Su-In Lee.
2014. Node-Based Learning of Multiple Gaussian Graphical Models. J. Mach.
Learn. Res. 15, 1 (jan 2014), 445â€“488.
[32] Ricardo Pio Monti, Peter Hellyer, David Sharp, Robert Leech, Christoforos Anag-
nostopoulos, and Giovanni Montana. 2014. Estimating time-varying brain con-
nectivity networks from functional MRI time series. NeuroImage 103 (2014),
427â€“443.
[33] A. Namaki, A.H. Shirazi, R. Raei, and G.R. Jafari. 2011. Network analysis of a
financial market based on genuine correlation and threshold method. Physica A:
Statistical Mechanics and its Applications 390, 21 (2011), 3835â€“3841.
[34] Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai. 2024.
Dynamic Multi-Network Mining of Tensor Time Series. In Proceedings of the
ACM on Web Conference 2024 (WWW â€™24). 4117â€“4127.
[35] Spiros Papadimitriou, Jimeng Sun, and Christos Faloutsos. 2005. Streaming
pattern discovery in multiple time-series. (2005).
[36] Vladimir Pavlovic, James M Rehg, Tat-Jen Cham, and Kevin P Murphy. 1999. A
dynamic Bayesian network approach to figure tracking using learned dynamic
models. In Proceedings of the seventh IEEE international conference on computer
vision, Vol. 1. IEEE, 94â€“101.
[37] Zhen Qin, Yibo Zhang, Shuyu Meng, Zhiguang Qin, and Kim-Kwang Raymond
Choo. 2020. Imaging and fusing time series for wearable sensor-based human
activity recognition. Information Fusion 53 (2020), 80â€“87.
[38] Xiaobin Ren, Kaiqi Zhao, Patricia Riddle, Katerina TaÅ¡kova, Lianyan Li, and
Qingyi Pan. 2023. DAMR: Dynamic Adjacency Matrix Representation Learning
for Multivariate Time Series Imputation. SIGMOD (2023). https://doi.org/10.
1145/3589333
[39] Mark Rogers, Lei Li, and Stuart J Russell. 2013. Multilinear dynamical systems for
tensor time series. Advances in Neural Information Processing Systems 26 (2013).
 
2304KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Kohei Obata, Koki Kawabata, Yasuko Matsubara, and Yasushi Sakurai
[40] Tolou Shadbahr, Michael Roberts, Jan Stanczuk, Julian Gilbey, Philip Teare, SÃ¶ren
Dittmer, Matthew Thorpe, Ramon ViÃ±as TornÃ©, Evis Sala, Pietro LiÃ³, Mishal Patel,
Jacobus Preller, Ian Selby, Anna Breger, Jonathan R. Weir-McCall, Effrossyni
Gkrania-Klotsas, Anna Korhonen, Emily Jefferson, Georg Langs, Guang Yang,
Helmut Prosch, Judith Babar, Lorena Escudero SÃ¡nchez, Marcel Wassin, Markus
Holzer, Nicholas Walton, Pietro LiÃ³, James H. F. Rudd, Tuomas Mirtti, Antti Sakari
Rannikko, John A. D. Aston, Jing Tang, and Carola-Bibiane SchÃ¶nlieb. 2023. The
impact of imputation quality on machine learning classifiers for datasets with
missing values. Communications Medicine 3, 1 (2023).
[41] Satya Narayan Shukla and Benjamin M Marlin. 2021. Multi-time attention
networks for irregularly sampled time series. arXiv preprint arXiv:2101.10318
(2021).
[42] Nicolas StÃ¤dler and Peter BÃ¼hlmann. 2012. Missing values: sparse inverse covari-
ance estimation and an extension to sparse regression. Statistics and Computing
22 (2012), 219â€“235.
[43] Nicolas StÃ¤dler and Peter BÃ¼hlmann. 2012. Missing values: sparse inverse covari-
ance estimation and an extension to sparse regression. Statistics and Computing
22 (2012), 219â€“235.
[44] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. Csdi: Con-
ditional score-based diffusion models for probabilistic time series imputation.
Advances in Neural Information Processing Systems 34 (2021), 24804â€“24816.
[45] Federico Tomasi, Veronica Tozzo, and Annalisa Barla. 2021. Temporal Pattern
Detection in Time-Varying Graphical Models. In ICPR. 4481â€“4488.
[46] Federico Tomasi, Veronica Tozzo, Saverio Salzo, and Alessandro Verri. 2018.
Latent Variable Time-varying Network Inference. In KDD. 2338â€“2346. https:
//doi.org/10.1145/3219819.3220121
[47] Veronica Tozzo, Federico Ciech, Davide Garbarino, and Alessandro Verri. 2021.
Statistical Models Coupling Allows for Complex Local Multivariate Time Series
Analysis. In KDD. 1593â€“1603.
[48] Veronica Tozzo, Davide Garbarino, and Annalisa Barla. 2020. Missing Values
in Multiple Joint Inference of Gaussian Graphical Models. In Proceedings of the
10th International Conference on Probabilistic Graphical Models (Proceedings of
Machine Learning Research, Vol. 138), Manfred Jaeger and Thomas Dyhre Nielsen
(Eds.). PMLR, 497â€“508.
[49] Andrew Viterbi. 1967. Error bounds for convolutional codes and an asymptotically
optimum decoding algorithm. IEEE transactions on Information Theory 13, 2 (1967),
260â€“269.
[50] Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew
Margenot, and Hanghang Tong. 2023. Networked time series imputation via
position-aware graph enhanced variational autoencoders. In KDD. 2256â€“2268.
[51] Xu Wang, Hongbo Zhang, Pengkun Wang, Yudong Zhang, Binwu Wang,
Zhengyang Zhou, and Yang Wang. 2023. An Observed Value Consistent Diffu-
sion Model for Imputing Missing Values in Multivariate Time Series. In KDD.
2409â€“2418.
[52] Xiuwen Yi, Yu Zheng, Junbo Zhang, and Tianrui Li. 2016. ST-MVL: filling missing
values in geo-sensory time series data. In Proceedings of the 25th International
Joint Conference on Artificial Intelligence.
[53] Jinsung Yoon, James Jordon, and Mihaela Schaar. 2018. Gain: Missing data impu-
tation using generative adversarial nets. In International conference on machine
learning. PMLR, 5689â€“5698.
[54] Jinsung Yoon, William R Zame, and Mihaela van der Schaar. 2018. Estimating
missing data in temporal data streams using multi-directional recurrent neural
networks. IEEE Transactions on Biomedical Engineering 66, 5 (2018), 1477â€“1490.
[55] Hsiang-Fu Yu, Nikhil Rao, and Inderjit S. Dhillon. 2016. Temporal Regularized
Matrix Factoriztion for High-dimensional Time Series Prediction. In Advances in
Neural Information Processing Systems 28.
[56] Dejiao Zhang and Laura Balzano. 2016. Global convergence of a grassmannian
gradient descent algorithm for subspace estimation. In Artificial Intelligence and
Statistics. PMLR, 1460â€“1468.
A Proposed model
A.1 Symbols
Table 2 lists the main symbols we use throughout this paper.
A.2 Complexity analysis
Proof of Lemma 1.Proof. The overall time complexity is composed of four parts
by taking the most time-consuming part of equations for each
iteration considering ğ‘>ğ‘ğ‘¡,ğ¿: the complexity for the inference
ofğ’andğ‘­isğ‘‚(ğ¾2Ãğ‘‡
ğ‘¡=1(ğ¿3+ğ¿2ğ‘ğ‘¡+ğ¿ğ‘2
ğ‘¡+ğ‘3
ğ‘¡))related to Eq. (13)
and Eq. (15); the inference of Â¤ğ‘½isğ‘‚(ğ¾ğ¿2ğ‘2)(Eq. (19)); M step is
ğ‘‚(ğ¾ğ‘‡ğ¿2ğ‘)related to the calculation of Â¤ğ‘¼(Eq. (21)); and the update
ofÂ¤ğš¯isğ‘‚(ğ¾ğ‘3)(Eq. (23)). Thus, the overall time complexity is
ğ‘‚(#ğ‘–ğ‘¡ğ‘’ğ‘ŸÂ·(ğ¾2Ãğ‘‡
ğ‘¡=1(ğ¿3+ğ¿2ğ‘ğ‘¡+ğ¿ğ‘2
ğ‘¡+ğ‘3
ğ‘¡)+ğ¾ğ¿2ğ‘2+ğ¾ğ‘‡ğ¿2ğ‘+
ğ¾ğ‘3)). â–¡
Proof of Lemma 2.
Proof. The space complexity is composed of three parts: stor-
ing input dataset ğ‘¿isğ‘‚(ğ‘‡ğ‘); intermediate values in E step are
ğ‘‚(ğ¾2ğ‘‡ğ¿2+ğ¾ğ¿2ğ‘); and storing parameter set is ğ‘‚(ğ¾ğ‘2). Thus, the
overall space complexity is ğ‘‚(ğ‘‡ğ‘+ğ¾2ğ‘‡ğ¿2+ğ¾ğ¿2ğ‘+ğ¾ğ‘2).â–¡
Table 2: Symbols and definitions.
Symbol Definition
ğ‘¿ Multivariate time series ğ‘¿={ğ’™1,ğ’™2,...,ğ’™ğ‘‡} âˆˆ
Rğ‘Ã—ğ‘‡
ğ‘¾ Indicator matrix ğ‘¾âˆˆRğ‘Ã—ğ‘‡
Ëœğ‘¿ Partially observed multivariate time series Ëœğ‘¿=ğ‘¾â—¦ğ‘¿
Ë†ğ‘¿ Inputed multivariate time series
ğ’ Time series latent states ğ’âˆˆRğ¿Ã—ğ‘‡
ğ‘½(ğ‘˜)Contextual latent factor of ğ‘˜ğ‘¡â„-regime ğ‘½(ğ‘˜)âˆˆRğ¿Ã—ğ‘
ğ‘º(ğ‘˜)Contextual matrix of ğ‘˜ğ‘¡â„-regime ğ‘º(ğ‘˜)âˆˆRğ‘Ã—ğ‘
ğ‘© Transition matrix ğ‘©âˆˆRğ¿Ã—ğ¿
ğ‘¼(ğ‘˜)Observation matrix of ğ‘˜ğ‘¡â„-regime ğ‘¼(ğ‘˜)âˆˆRğ‘Ã—ğ¿
ğ‘­ Regime assignments ğ‘­âˆˆRğ¾Ã—ğ‘‡
ğ†(ğ‘˜)Mean vector of ğ‘˜ğ‘¡â„-regime ğ†(ğ‘˜)âˆˆRğ‘
ğš¯(ğ‘˜)Inverse covariance matrix (i.e., network) of ğ‘˜ğ‘¡â„-
regime ğš¯(ğ‘˜)âˆˆRğ‘Ã—ğ‘
ğ‘ Number of features
ğ‘‡ Number of timesteps
ğ¿ Number of latent dimensions
ğ¾ Number of regimes
ğ›¼ Trade-off between temporal dependency and inter-
correlation
ğœ† Parameter for â„“1-norm that regulates network sparsity
A.3 Updating parameters
The parameters are updated as follows:
ğ‘©ğ‘›ğ‘’ğ‘¤=ğ‘‡âˆ‘ï¸
ğ‘¡=2E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡âˆ’1]ğ‘‡âˆ‘ï¸
ğ‘¡=2E[ğ’›ğ‘¡âˆ’1ğ’›â€²
ğ‘¡âˆ’1]âˆ’1
,
ğ’›ğ‘›ğ‘’ğ‘¤
0=E[ğ’›1],
ğš¿ğ‘›ğ‘’ğ‘¤
0=E[ğ’›1ğ’›â€²
1]âˆ’E[ğ’›1]E[ğ’›â€²
1],
 
2305Mining of Switching Sparse Networks for Missing Value Imputation in Multivariate Time Series KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(ğœ2
ğ’)ğ‘›ğ‘’ğ‘¤=1
(ğ‘‡âˆ’1)ğ¿trğ‘‡âˆ‘ï¸
ğ‘¡=2E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡]âˆ’ğ‘©ğ‘‡âˆ‘ï¸
ğ‘¡=2E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡]
,
(ğœ2
ğ‘¿(ğ‘˜))ğ‘›ğ‘’ğ‘¤=1
Ãğ‘‡
ğ‘¡=1(ğ’‡ğ‘¡=ğ‘˜)Ãğ‘
ğ‘–=1ğ‘¾ğ‘–,ğ‘¡
ğ‘‡âˆ‘ï¸
ğ‘¡=1(ğ’‡ğ‘¡=ğ‘˜)ğ‘¡ğ‘Ÿ
(Â¯ğ‘¼(ğ‘˜)
ğ‘¡)ğ‘›ğ‘’ğ‘¤E[ğ’›ğ‘¡ğ’›â€²
ğ‘¡](Â¯ğ‘¼(ğ‘˜)
ğ‘¡)ğ‘›ğ‘’ğ‘¤â€²
+ğ‘‡âˆ‘ï¸
ğ‘¡=1(ğ’‡ğ‘¡=ğ‘˜)
(Â¯ğ’™ğ‘¡)â€²Â¯ğ’™ğ‘¡âˆ’2(Â¯ğ’™ğ‘¡)â€²((Â¯ğ‘¼(ğ‘˜)
ğ‘¡)ğ‘›ğ‘’ğ‘¤E[ğ’›ğ‘¡])
,
(ğœ2
ğ‘º(ğ‘˜))ğ‘›ğ‘’ğ‘¤=1
ğ‘2ğ‘âˆ‘ï¸
ğ‘—=1
ğ’”(ğ‘˜)â€²
ğ‘—ğ’”(ğ‘˜)
ğ‘—âˆ’2ğ’”(ğ‘˜)â€²
ğ‘—((ğ‘¼(ğ‘˜))ğ‘›ğ‘’ğ‘¤E[ğ’—(ğ‘˜)
ğ‘—])
+ğ‘¡ğ‘Ÿ
(ğ‘¼(ğ‘˜))ğ‘›ğ‘’ğ‘¤(ğ‘âˆ‘ï¸
ğ‘—=1E[ğ’—(ğ‘˜)
ğ‘—ğ’—(ğ‘˜)â€²
ğ‘—])(ğ‘¼(ğ‘˜)ğ‘›ğ‘’ğ‘¤)â€²
,
(ğœ2
ğ‘½(ğ‘˜))ğ‘›ğ‘’ğ‘¤=1
ğ‘ğ¿ğ‘âˆ‘ï¸
ğ‘—=1tr(E[ğ’—(ğ‘˜)
ğ‘—ğ’—(ğ‘˜)â€²
ğ‘—]). (26)
B Experiments
B.1 Synthetic data generation
We first generate a latent factor containing a linear trend, a sinu-
soidal seasonal pattern, and a noise, ğ’ğ‘–,ğ‘¡=sin(2ğœ‹ğ›½
ğ‘‡ğ‘¡)+ğ›¾ğ‘¡+ğœ‚,
s.t.1<ğ›½<20,0.3<|ğ›¾|<1,ğœ‚âˆ¼N( 0,0.3), where ğ’âˆˆRğ¿Ã—ğ‘‡.
We then project the latent factor with object matrix ğ‘¿=ğ‘¼(ğ‘˜)ğ’,
where ğ‘¿âˆˆRğ‘Ã—ğ‘‡, and ğ‘¼(ğ‘˜)âˆˆRğ‘Ã—ğ¿is a random graph created as
follows [31]:
(1)Setğ‘¼(ğ‘˜)equal to the adjacency matrix of an ErdÅ‘s-RÃ©nyi
directed random graph, where every edge has a 20%chance
of being selected.
(2)Set selected edge ğ‘¼(ğ‘˜)
ğ‘–,ğ‘—âˆ¼Uniform([âˆ’0.6,âˆ’0.3]âˆª[ 0.3,0.6]),
where ğ‘¼(ğ‘˜)
ğ‘–,ğ‘—denotes the weight between variables ğ‘–andğ‘—.
We setğ‘‡=1000,ğ‘=50,ğ¿=10. We generate two types of syn-
thetic data, PatternA and PatternB, five times each, where ğ¾=1,2,
respectively. In PatternB, the regime switches every 200timesteps.B.2 Hyperparameters
We describe the hyperparameters of the baselines. For Synthetic
datasets, we give a latent dimension of 10for all baselines. For
a fair comparison, we set the latent dimension of the SSM-based
methods at the same value as MissNet. For the MF-based methods,
we vary the latent dimension {3,5,10,15,20,30,40}. We vary the AR
parameter for TRMF {[1,2,3,4,5],[1,24]}. To learn the DL-based
methods, we add 10%of the data as missing values for training the
model. We vary the window size {16,32,ğ‘‡}. Other hyperparameters
are the same as the original codes.
C Discussion
While MissNet achieved superior performance against state-of-
the-art baselines in missing value imputation, here, we mention
two limitations of MissNet in terms of sparse network inference
and data size.
As mentioned in Section 5.2.1, MissNet fails to discover the
correct transition when the missing rate exceeds 70%. However,
we claim that MissNet failing to discover the correct transition
when the missing rate exceeds 70%is reasonable; rather, correctly
discovering transition up to 60% is valuable. Several studies [ 24,
43,48] tackled the sparse network inference under the existence
of missing values. They aim to infer the correct network and, thus,
only utilize the observed value for the network inference. Since
observing a complete pair at a high missing rate is rare, it is difficult
to infer the correct network. Therefore, the maximum missing rate
in their experiments is 30%. Although the experimental settings
are different from ours, we can say that the task of sparse network
inference in the presence of missing values itself is challenging.
As shown in the experiments, MissNet performs well even when
a relatively small number of samples ( ğ‘‡) and a large number of
features (ğ‘) since MissNet is a parametric model and we assume
the sparse networks to capture inter-correlation. This cannot be
achieved by DL models, which contain a massive number of param-
eters that require a large amount of ğ‘‡, especially when ğ‘is large
since all the relationships between features need to be learned. How-
ever, unlike DL models, the increased number of samples may not
greatly improve MissNetâ€™s performance as it has a much smaller
number of parameters than DL models, even though switching
sparse networks increases the modelâ€™s flexibility.
 
2306