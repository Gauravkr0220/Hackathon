RHiOTS: A Framework for Evaluating Hierarchical Time Series
Forecasting Algorithms
Luis Roque
luis_roque@live.com
LIACC/Faculty of Engineering,
University of Porto
Porto, PortugalCarlos Soares
LIACC/Faculty of Engineering,
University of Porto
Porto, Portugal
Fraunhofer AICOS Portugal
Porto, PortugalLuÃ­s Torgo
Dalhousie University
Halifax, Canada
ABSTRACT
We introduce the Robustness of Hierarchically Organized Time
Series (RHiOTS) framework, designed to assess the robustness of
hierarchical time series forecasting models and algorithms on real-
world datasets. Hierarchical time series, where lower-level forecasts
must sum to upper-level ones, are prevalent in various contexts,
such as retail sales across countries. Current empirical evaluations
of forecasting methods are often limited to a small set of benchmark
datasets, offering a narrow view of algorithm behavior. RHiOTS
addresses this gap by systematically altering existing datasets and
modifying the characteristics of individual series and their interre-
lations. It uses a set of parameterizable transformations to simulate
those changes in the data distribution. Additionally, RHiOTS incor-
porates an innovative visualization component, turning complex,
multidimensional robustness evaluation results into intuitive, easily
interpretable visuals. This approach allows an in-depth analysis of
algorithm and model behavior under diverse conditions. We illus-
trate the use of RHiOTS by analyzing the predictive performance
of several algorithms. Our findings show that traditional statistical
methods are more robust than state-of-the-art deep learning algo-
rithms, except when the transformation effect is highly disruptive.
Furthermore, we found no significant differences in the robustness
of the algorithms when applying specific reconciliation methods,
such as MinT. RHiOTS provides researchers with a comprehen-
sive tool for understanding the nuanced behavior of forecasting
algorithms, offering a more reliable basis for selecting the most
appropriate method for a given problem.
CCS CONCEPTS
â€¢Computing methodologies â†’Machine learning.
KEYWORDS
Hierarchical time series, Forecasting algorithms, Evaluation meth-
ods, Time series forecasting
ACM Reference Format:
Luis Roque, Carlos Soares, and LuÃ­s Torgo. 2024. RHiOTS: A Framework for
Evaluating Hierarchical Time Series Forecasting Algorithms. In Proceedings
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672062of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York,
NY, USA, 9 pages. https://doi.org/10.1145/3637528.3672062
1 INTRODUCTION
In time series forecasting, modeling inter-temporal dependencies
between observations is a crucial task to capture the dynamics of the
underlying process. Furthermore, considering cross-series informa-
tion, i.e., the relationship between multiple related time series, can
enhance the performance of traditional univariate algorithms [ 22].
Such relationships occur in many real-world situations and often
represent hierarchical structures, such as in data concerning Gross
Domestic Product [ 2], epidemics [ 10], and sales demand [ 19]. Co-
herent forecasts [ 22], which satisfy these hierarchical relations,
are often required in such applications. To improve performance,
state-of-the-art algorithms for hierarchical time series (HTS) fore-
casting rely on both the autocorrelation of each time series and the
cross-series correlations [16, 33].
An important characteristic of HTS forecasting algorithms is
their robustness to changes in time series relationships, such as
variations in seasonality, trends, cross-series dependencies, and
volatility. These phenomena significantly impact predictive per-
formance and may lead to inaccurate forecasts and suboptimal
decision-making. Nevertheless, conventional evaluation method-
ologies, typically based on a small number of datasets and metrics,
cannot assess the robustness of HTS forecasting algorithms in
dynamic scenarios where relationships between time series and
temporal dependencies change over time. We note that the con-
cerns about the adequacy of evaluation methods are not limited
to HTS forecasting. In fact, there is a growing discussion on the
limitations of empirical evaluations in time series analysis (e.g.,
anomaly detection [ 35]). We argue that similar issues can be raised
for time series forecasting since both suffer the exact root cause:
most papers evaluate algorithms on one or more of a handful of
popular benchmark datasets (e.g. [29, 30]).
This work introduces the Robustness of Hierarchically Orga-
nized Time Series (RHiOTS) framework. RHiOTS is designed to
evaluate the robustness of HTS forecasting models and algorithms
facing real-world data distribution changes. Our framework system-
atically quantifies the robustness by imposing controlled realistic
transformations, mimicking common patterns observed in actual
datasets. These semi-synthetic datasets retain essential characteris-
tics of the original data while introducing new dynamics, serving
as a robust baseline to assess algorithmic performance against data
variations. RHiOTS has an innovative visualization component,
 
2491
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Luis Roque, Carlos Soares, and LuÃ­s Torgo
which transforms the complex, multidimensional results of the
robustness evaluation into intuitive, easily interpretable visual rep-
resentations. It focuses on aggregating the main findings on the
performance of different algorithms and understanding the impact
of the various data transformations. It enables practitioners to make
more informed data-driven decisions in selecting and applying HTS
forecasting algorithms.
To illustrate the usefulness of RHiOTS, we evaluated five fore-
casting algorithms across three real-world time series datasets. Our
approach involved creating variations of these datasets using four
time series transformations, often used in the context of time se-
ries augmentation. For each transformation, we defined a set of
parameter variants and then generated several samples of these
transformed datasets to reduce the variability of the results.
The results indicate that traditional statistical algorithms, such
as Exponential Smoothing (ETS), show considerable robustness
and ranked best using the RHiOTS framework. Also, we found no
meaningful differences in the robustness of the algorithms when ap-
plying specific reconciliation methods, such as MinT. An additional
observation is that, while generally less consistent, deep learning
counterparts showed more robustness when dealing with disruptive
transformations, such as high-intensity magnitude warping. We
also demonstrated that these insights could not have been obtained
through the limited benchmark-based methodologies currently in
use.
In summary, the contributions of this paper are the following:
â€¢We introduce RHiOTS, a novel framework for evaluating
the robustness of HTS forecasting models and algorithms
on real-world datasets. It provides a more systematic and
comprehensive comparison than the existing benchmark-
based methods;
â€¢We propose a visual representation of the results of RHiOTS.
It converts complex multidimensional results into clear, con-
cise plots, enabling easier comparison of performance and
the effects of data transformations;
â€¢We present empirical insights into the robustness of five dis-
tinct HTS models and algorithms. It illustrates the usefulness
of RHiOTS as a new evaluation framework.
All experiments are fully reproducible, and the methods and time
series data are available in a public code repository.1
2 BACKGROUND AND NOTATION
We are working with a collection of ğ‘†related univariate time series,
Z={zğ‘–
ğ‘¡,ğ‘¡âˆˆN,ğ‘–=1,...,ğ‘†}. The training values can be written
aszğ‘–
1:ğ‘‡=[ğ‘§ğ‘–
1,ğ‘§ğ‘–
2,Â·Â·Â·,ğ‘§ğ‘–
ğ‘‡]whereğ‘§ğ‘–
ğ‘¡âˆˆRdenotes the value of time
seriesğ‘–at timeğ‘¡andğ‘‡represents the last training point. When the
interpretation is unambiguous and to simplify the notation in spe-
cific sections, we refer to zğ‘–as the observed time series. The training
range is denoted by {1,2,...,ğ‘‡}, while{ğ‘‡+1,ğ‘‡+2,...,ğ‘‡+ğœ}is the
prediction range and ğœis the forecast horizon. Point predictions are
defined as Ë†zğ‘–
ğ‘‡+1:ğ‘‡+ğœ.
HTS organizes a collection of time series, represented as zğ‘–,
within a hierarchical structure denoted by ğ». The hierarchy ğ»is
a tree-like structure, where each node represents a group ğºğ‘™or a
1https://github.com/luisroque
/robustness_hierarchical_time_series_forecasting_algorithms
Figure 1: Simple example of a hierarchically organized time
series dataset that comprises sales data from a retailer in the
US.
time series zğ‘–. The root of the tree represents the total aggregate of
all the time series in the dataset, denoted as zğ‘¡. The leaf nodes of
the tree represent individual time series zğ‘–. Each time series zğ‘–is
associated with one or more groups ğºğ‘™in the hierarchy. Thus, we
can writeğºğ‘™âŠ†1,...,ğ‘† where zğºğ‘™=Ã
ğ‘–âˆˆğºğ‘™zğ‘–. The subset of groups
a time series zğ‘–belongs to is denoted by ğ¿ğ‘–={ğ‘™:ğ‘–âˆˆğºğ‘™}.ğºdenotes
the set of all groups, and its cardinality is denoted by |ğº|.
We illustrate this concept in Figure 1 using an example from a
retailer operating in the United States. It consists of two groups:
The first, denoted by ğ‘ˆ, represents the states in the United States
(US), with elements ğ‘andğ‘, representing California (CA) and New
York (NY), respectively. The second group, denoted by ğ‘ƒ, represents
product categories and has elements ğ‘¥andğ‘¦, corresponding to
trousers and t-shirts, respectively. At the bottom level, the hierar-
chy would generate four distinct time series: zğ‘ğ‘¥
ğ‘¡,zğ‘ğ‘¦
ğ‘¡,zğ‘ğ‘¥
ğ‘¡, and zğ‘ğ‘¦
ğ‘¡.
Each non-leaf node (group ğºğ‘™) in the hierarchy represents the ag-
gregation of the time series associated with its children nodes. For
instance, ifğ‘ˆrepresents the group of all stores in different states of
the US, and zğ‘–represents the sales data of an individual product in
a specific store, then the time series at node ğ‘ˆiszğ‘ˆ,ğ‘¡=Ã
zğ‘–âˆˆğ‘ˆzğ‘–
ğ‘¡.
Similarly, the time series for each element ğ‘ofğ‘ˆiszğ‘
ğ‘¡=Ã
ğ‘–âˆˆğ‘ˆğ‘zğ‘–
ğ‘¡.
The objective of HTS forecasting is twofold: firstly, to minimize
the forecast error for each series within the hierarchy, and sec-
ondly, to ensure that forecasts at all levels of the hierarchy are
consistent with each other. Formally, the forecast error for a se-
riesğ‘–over a prediction horizon ğœis defined as ğ¸ğ‘–
ğ‘‡+1:ğ‘‡+ğœ={ğ‘’ğ‘–
ğ‘¡=
Ë†ğ‘§ğ‘–
ğ‘¡âˆ’ğ‘§ğ‘–
ğ‘¡|ğ‘¡âˆˆ{ğ‘‡+1,ğ‘‡+2,...,ğ‘‡+ğœ}}, where Ë†ğ‘§ğ‘–
ğ‘¡denotes the fore-
casted value and ğ‘§ğ‘–
ğ‘¡denotes the actual value at time ğ‘¡. The con-
sistency constraint requires that for any aggregated level within
the hierarchy, represented by a group ğºğ‘™, the aggregated forecasts
Ë†zğºğ‘™
ğ‘‡+1:ğ‘‡+ğœ=Ã
ğ‘–âˆˆğºğ‘™Ë†zğ‘–
ğ‘‡+1:ğ‘‡+ğœ, should equal the sum of forecasted
values for all time series ğ‘–belonging to that group. This ensures
that the forecasts respect the hierarchical structure, maintaining
integrity and coherence across all levels of aggregation within the
dataset.
 
2492RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
In terms of evaluation and using our previous example, we start
by computing the Mean Absolute Scaled Error (MASE) ([ 14]) for
each time series at the bottom level, such as ğ‘€ğ´ğ‘†ğ¸ğ‘ğ‘¥for the time
series zğ‘ğ‘¥. The formula for MASE is defined as follows: ğ‘€ğ´ğ‘†ğ¸ğ‘–=
1
ğœâˆ’ğ‘‡+1Ãğœ
ğ‘¡=ğ‘‡+1|ğ‘’ğ‘–
ğ‘¡|
1
(ğ‘‡âˆ’1)Ãğ‘‡
ğ‘¡=2|ğ‘§ğ‘–
ğ‘¡âˆ’ğ‘§ğ‘–
ğ‘¡âˆ’1|.
It is equally important to assess the forecasting performance at
aggregated levels. Hence, we also compute the MASE metric for
every group element ğ‘™. This is achieved by aggregating the time
series at the bottom level that belongs to a specific group element.
For example, for group element ğ‘in our previous example, the ag-
gregated time series zğ‘
ğ‘¡is calculated as zğ‘
ğ‘¡=zğ‘ğ‘¥
ğ‘¡+zğ‘ğ‘¦
ğ‘¡. Subsequently,
the MASE metric for zğ‘
ğ‘¡is computed.
To evaluate the forecast accuracy at the group level, we calculate
the average MASE across all elements ğ‘™that belong to the group. For
example, for group ğ‘ˆ, we would consider zğ‘
ğ‘¡andzğ‘
ğ‘¡. First, we define
the aggregated time series for each group ğ‘™aszğºğ‘™=Ã
ğ‘–:ğ‘–âˆˆğºğ‘™zğ‘–.
Then, we compute the MASE for each of these aggregate time
series zğºğ‘™and finally take the average across all ğ‘™in the group ğº,
ğ‘€ğ´ğ‘†ğ¸ğº=1
|ğº|Ã
ğ‘™âˆˆğºğ‘€ğ´ğ‘†ğ¸ğºğ‘™.
At the most aggregated level (the top of the hierarchy), we sum
the values of all the bottom-level time series and evaluate the pre-
dictive performance based on these values.
Analyzing the distance between time series is not a trivial task.
We adopt Dynamic Time Warping (DTW) following the approach
recommended by [ 20] for short time series. DTW evaluates se-
quence similarity by computing a matrix of distances between
elements and determining the optimal alignment path that mini-
mizes the total distance (Eq. 1). This alignment, or warp path, is
mathematically formulated as:
ğ·ğ‘‡ğ‘Šğ‘(z,zâ€²)=min
ğœ‹âˆˆğ´(z,zâ€²)âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆğœ‹ğ‘‘(zğ‘–,zâ€²
ğ‘—)ğ‘1
ğ‘
(1)
where an alignment path ğœ‹of lengthğ¾is a sequence of ğ¾index pairs
((ğ‘–0,ğ‘—0),...,(ğ‘–ğ¾âˆ’1,ğ‘—ğ¾âˆ’1))andğ´(z,zâ€²)is the set of all admissible
paths.
In terms of time series transformations, we denote them by the
functionT, to the original time series zğ‘–
ğ‘¡. Such transformations can
modify individual components of a time series and alter the inter-
relations among multiple time series within the dataset. Different
transformation functions will have different sets of parameters. For
simplicity of notation, we introduce the concept of a parameter set
ğ‘£, representing a specific parameter set used in the transformation.
Formally, we denote the resulting transformed series as zâ€²ğ‘–
ğ‘£,ğ‘¡, where
zâ€²ğ‘–
ğ‘£,ğ‘¡=T(zğ‘–
ğ‘¡,ğœ‚ğ‘£), andğœ‚ğ‘£symbolizes the particular parameter set ğ‘£.
We denote the resulting dataset as Zğ›¾,ğ‘£,ğ‘—, whereğ›¾âˆˆ{1,2,..., Î“}
denotes the transformation applied, ğ‘£is the set of parameters used,
andğ‘—represents the number of samples generated from the same
transformation and set of parameters.
3 RELATED WORK
Hierarchical Time Series Forecasting Methods. Univariate
methods like ARIMA and ETS are common for individual time
series analysis and can be used for HTS when using simple rec-
onciliation strategies [ 14]. General approaches like those foundin [9,23,26,32] are methods used to forecast a set of time series
from the same domain. It means that they are capable of learning
relationships among time series. However, these approaches do not
incorporate any hierarchical structure, missing out on leveraging it
to improve forecasting accuracy.
Alternatively, several methods explicitly model the hierarchi-
cal nature of HTS datasets. Early approaches by [ 15], followed
by refinements in [ 1,22], involve fitting and reconciling indepen-
dent forecasts at all hierarchy levels. Non-linear models (e.g., [ 13])
improve their ability to capture complex patterns in the data by
employing optimization and regularization techniques during the
model training process. Another approach uses Gaussian Processes
and does not require any reconciliation since the hierarchical struc-
ture is an input to the model itself [31].
Evaluating Hierarchical Time Series Forecasting. The eval-
uation of HTS forecasting models has received limited attention in
recent literature. Exceptions. For example, [ 13] and [ 3], compare the
performance of state-of-the-art reconciliation models on a limited
set of real-world datasets. Despite considering various forecasting
models, the fact that the experimental setup does is so limited and
does not provide rich data variations, our ability to generalize is
limited. Recently, a practical guide addressing concepts like scale,
units, sparsity, forecast horizon, multiple evaluation windows, and
decision context has been proposed [ 5]. However, the guide falls
short in objectively quantifying performance differences between
models beyond basic accuracy assessment on benchmarks.
Time Series Augmentation. Data augmentation involves gen-
erating synthetic data that covers unexplored input space while
preserving correct labels [ 34]. This technique has proven effective in
domains such as computer vision, where methods like AlexNet [ 21]
have leveraged augmented data for image classification. On the
other side, the unique properties of time series data, such as its
temporal dependencies and intricate dynamics, create a set of chal-
lenges.
Autoregressive models, while effective for linear, stationary time
series, face limitations with complex data and computational de-
mands [ 18]. Generative models, including Generative Adversar-
ial Networks (GANs) and Variational Autoencoders (VAEs), have
emerged as powerful tools for augmenting time series data. GANs,
particularly TimeGAN, and extensions of this work, create realis-
tic synthetic samples by learning temporal dynamics, albeit with
training challenges [ 25,34,36]. VAEs and their conditional variants
(CVAEs) generate new data from a latent space, offering potential de-
spite some control issues over the generated samples [ 8,11,12,34].
Sliding window methods risk overfitting by focusing too nar-
rowly on local patterns, while decomposition methods generate
limited variety from extracted dataset features like trends [6].
Simpler transformations like jittering, scaling, magnitude, and
time warping have been shown to help in specific tasks and do-
mains [ 7,28]. They are simple to implement and increase data
variety. While they may not capture complex patterns, they provide
some control over the transformations since they are parametric
transformations.
 
2493KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Luis Roque, Carlos Soares, and LuÃ­s Torgo
Figure 2: Ridge plot that shows the DTW distribution between time series in the dataset for each dataset (columns), transforma-
tion (rows), and parameter set. For the original DTW distance we use orange and for the transformed ones we use shades of
blue: as we increase the magnitude of the transformation the color gets lighter.
4 RHIOTS
We introduce RHiOTS, a framework designed to assess the ro-
bustness of HTS forecasting algorithms. Traditional evaluations,
which primarily focus on predictive accuracy using a limited set
of benchmark problems, fail to adequately assess the resilience
of an algorithm to minor variations in individual time series or
their interrelations. RHiOTS addresses these issues by offering a
nuanced analysis of the stability of forecasting algorithms against
such changes, providing deeper insights into their robustness and
reliability.
4.1 Framework
RHiOTS serves two primary objectives. The first goal is to provide a
comprehensive understanding of the behavior of models. We do this
by systematically applying various transformations to the data and
then evaluating the performance of models. This process facilitates
a detailed assessment of the robustness of each model, controlled
by transformation and intensity. It enables practitioners to identify
the most effective model for the unique aspects of their problem
and potential variations in data properties.
For the second goal, RHiOTS aims to support the reliable se-
lection of an appropriate algorithm for a given forecasting task.
By analyzing how different algorithms perform across a range of
transformed datasets, the framework offers insights into which
algorithms are most adaptable and effective under varying condi-
tions. This allows for a more effective and informed comparison of
algorithms and improves generalization.
RHiOTS applies transformations to each individual time series,
zâ€²ğ‘–=Tğ‘–(zğ‘–,ğœ‚ğ‘–). The transformations are applied to the time series
in the leaf nodes of the hierarchy only. The aggregated levels of the
hierarchy are recomputed after the transformation, i.e., the total
sum of the observations of a specific group (or for the top-level
series) is computed based on the transformed individual series,
zâ€™ğº=Ã
ğ‘™:zâ€™ğ‘–âˆˆğºğ‘™zâ€™ğºğ‘™.
The next step in RHiOTS is to assess the robustness of HTS
forecasting models by connecting the variations introduced by
time series transformations with the variation in forecasting per-
formance. To quantify the variation in forecasting performance, we
compute the forecasting error of the model on the original datasetand the various transformed versions. Given that each transforma-
tion represents a type of variation (e.g., jittering represents noise
in the measurement of the values), the analysis of the variation of
forecasting performance for different versions of a transformation
gives a systematic perspective on the robustness of a method to
that type of variation (e.g., the robustness of the method to noise).
4.2 Time Series Transformations
We apply random-based transformations to the original time se-
ries, represented by zâ€²ğ‘–
ğ‘£,ğ‘¡=T(zğ‘–
ğ‘£,ğ‘¡,ğœ‚), whereTdenotes a transfor-
mation function and ğœ‚indicates its governing parameters. These
transformations can affect individual time series components and
relationships between series in a dataset. However, they should be
smooth and continuous to preserve a meaningful relationship be-
tween the original and transformed series. This ensures that similar
parameters produce closely related transformed series.
Jittering is a magnitude domain transformation that can be de-
fined as the addition of a random noise component to the values of a
time seriesğ‘§â€²ğ‘–
ğ‘£,ğ‘¡=ğ‘§ğ‘–
ğ‘¡+ğœ–ğ‘–
ğ‘£,ğ‘¡whereğœ–ğ‘–
ğ‘£,ğ‘¡âˆ¼N( 0,ğœ2ğ‘£). the standard devia-
tionğœğ‘£,ğ‘–of the added noise is a transformation parameter. Applying
jittering to the time series using the addition of i.i.d. Gaussian noise
is widely used in the literature to simulate realistic sources of mea-
surement error or irregularity in time series data [ 17]. For example,
consider a retail store where sales data suddenly becomes more
erratic due to unexpected external factors such as local construction
affecting customer traffic.
Another transformation used is scaling, which involves modify-
ing the amplitude of the series by a random scalar value ğ‘§â€²ğ‘–
ğ‘£,ğ‘¡=ğ›¼ğ‘–ğ‘£ğ‘§ğ‘–
ğ‘¡
whereğ›¼ğ‘–ğ‘£âˆ¼N( 0,ğœ2
ğ‘£,ğ‘–). Once again, ğœğ‘£,ğ‘–defines the standard devia-
tion of the multiplicative effect for each parameter set of the trans-
formation, dataset, and series. Scaling the time series data using a
multiplicative factor simulates realistic changes in the magnitude
of the time series. For example, consider a retail store experiencing
increased variability in sales due to various promotional campaigns
executed by the store itself and its competitors
We also applied magnitude warping [ 17]. This method causes a
smooth, continuous, nonlinear transformation of time series data.
It can be written as ğ‘§â€²ğ‘–
ğ‘£,ğ‘¡=ğ‘†ğ‘–
ğ‘¢ğ‘–
ğ‘˜(ğ‘§ğ‘–
ğ‘£,ğ‘¡)whereğ‘¢ğ‘–
ğ‘˜âˆ¼N( 1,ğœ2
ğ‘£,ğ‘–). Note
 
2494RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Figure 3: Model performance across various data transformations in hierarchical time series forecasting, assessed using MASE.
Each panel represents a different forecasting model subjected to transformations such as jitter, magnitude warping, scaling,
and time warping, with the transformation intensity increasing from â€˜origâ€˜ to â€˜v5â€˜. The lines within each panel correspond to
different hierarchical levels of the data, providing insight into the robustness of each model at various granularities.
thatğ‘†ğ‘–
ğ‘¢ğ‘–
ğ‘˜interpolates a cubic spline with knots u=ğ‘¢1,...,ğ‘¢ğ‘˜. Each
knotğ‘¢ğ‘˜comes from a distribution N(1,ğœ2
ğ‘£,ğ‘–), with the number of
knotsğ‘˜and the standard deviation ğœğ‘£,ğ‘–as parameters. The cubic
spline is fitted to the original data points, and the transformed data
is obtained by scaling the original magnitude using the evaluated
cubic spline function values. The proposed transformation method
modulates the magnitude of the time series, maintaining its overall
structure and smoothness. For example, the summer season sales
time series of a store near a beach area during unusually high
temperatures could experience magnitude warping.
Finally, the last transformation considered was time warping.
Time warping is a process that stretches or compresses the time
axis of a time series. Similarly to magnitude warping, we define
time warping using a cubic spline to interpolate the values of the
time series at a set of evenly spaced time points. The time points
can be chosen such that they are spaced more closely together in
regions where the time series is stretched and more widely spaced
in regions where it is compressed. This will effectively stretch or
compress the time axis of the time series while preserving the shape
of the time series itself. Time warping is defined as ğ‘§â€²ğ‘–
ğ‘£,ğ‘¡=ğ‘§ğ‘–
ğ‘£,ğ‘†ğ‘¢ğ‘–
ğ‘˜(ğ‘¡)
whereğ‘¢ğ‘–
ğ‘˜âˆ¼N( 1,ğœ2
ğ‘£,ğ‘–). Now,ğ‘†ğ‘–
ğ‘¢ğ‘–
ğ‘˜(ğ‘¡)is applied on the time steps.
Time warping can potentially impact the seasonality of a time seriesby stretching or compressing the time axis of the series. This can
cause the periodic patterns in the series, such as seasonal cycles or
trends, to become more or less prominent, depending on how the
time axis is altered. For example, changes in weather cycles could
significantly impact the seasonality of specific stores and products.
5 EXPERIMENTS
The primary objective of our experimental setup is to illustrate how
RHiOTS can be used to analyze the robustness of both models and
algorithms within the context of Hierarchical Time Series (HTS)
forecasting.
The first step we are interested in is how different transforma-
tions affect the distance between time series in the dataset (Q1).
HTS algorithms rely on these dependencies to improve their uni-
variate estimates.
(1)Models: In selecting models for a specific problem, stan-
dard evaluation methods test those models on the available
data. The assumption is that existing data is representative
of new, unseen data. RHiOTS can be used to test the robust-
ness of those models to variations in the data (e.g., noise).
We investigate the effects of different types of perturbations
on prediction error (Q2) and how the prediction error of
HTS forecasting models changes with manipulation of de-
pendencies across time and between series (Q3). Then, we
 
2495KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Luis Roque, Carlos Soares, and LuÃ­s Torgo
use RHiOTS to systematically compare the impact of the
data transformations on ranking the different algorithms
(Q4). This kind of analysis could support the data scientist in
choosing the best model not only in terms of standard evalua-
tion methods (e.g., average forecasting performance) but also
in terms of their robustness (e.g., robustness of forecasting
performance to noise in the data).
(2)Algorithms: In developing new algorithms, typical bench-
marking methods test those algorithms on public datasets
available for research. The results of the new algorithm are
compared to the results obtained by simple baselines and
state-of-the-art algorithms (e.g., the average forecasting ac-
curacy). Thus, we start by comparing the performance of
the algorithms using the benchmark method of assessing
predictive performance (Q5). Then, we use RHiOTS to as-
sess the relevance of dependencies between time series for
algorithm performance (Q6) and to identify the most robust
algorithm suitable for a given application domain (Q7). This
evaluation process enables researchers to make informed
decisions when developing new HTS algorithms and dealing
with diverse datasets and application requirements.
5.1 Datasets
The empirical evaluation uses three public datasets: the Tourism [ 4]
dataset from the Australian Bureau of Statistics, the M5 [ 24] dataset
based on Walmart sales, and the Police [ 27] dataset from Houston
police criminal reports. These datasets encompass various time
granularities, frequencies, lengths, and hierarchical structures, il-
lustrating the usefulness of RHiOTS in evaluating the robustness
of time series forecasting models in different scenarios.
To efficiently conduct the experiments across all datasets and
models, we downsampled the M5 dataset by reducing its frequency
from daily to weekly. Additionally, for the M5 and Houston datasets,
we selected a subset of 500 time series with high count levels. Pre-
liminary experiments indicate that this selection has no significant
impact on evaluating the different algorithms using RHiOTS. In the
interest of space, we do not discuss them here.
5.2 Transformations and Algorithms
We utilized four different transformations described in Section 4.2.
These transformations were applied to each dataset, and the param-
eters of each transformation were defined to increase linearly with
a slope of 1.
The robustness of the following methods was analyzed:
â€¢ETS + BU: the ETS method was applied to the bottom time
series, and then the naive Bottom-Up (BU) [ 14] strategy was
followed to aggregate the forecasts to the upper hierarchical
levels.
â€¢ETS + MinT: The ETS method was used as the base fore-
caster, but this time for all the time series (including the
upper levels). To ensure the coherence of the forecasts, we
used the Minimum Trace (MinT) reconciliation method pro-
posed by [22].
â€¢DeepAR + BU: DeepAR produces probabilistic forecasts
based on training an auto-regressive recurrent network model
on related time series [ 9]. The hierarchy of the dataset is
Figure 4: Ranking of the performance of forecasting methods
under magnitude warping transformation for the Tourism
dataset, from original data (â€˜origâ€˜) to the most intense trans-
formation (â€˜v5â€˜). Performance rank is indicated by proximity
to the center, with 0 being the best. It shows that the perfor-
mance of all algorithms deteriorates with increased transfor-
mation intensity, highlighted by the significant reordering
of ranks and crossing of lines.
handled by representing it as multiple static categorical fea-
tures to the model. We then use a naive BU reconciliation
strategy.
â€¢TFT + BU: Temporal Fusion Transformer (TFT) combines
recurrent and transformer architectures for complex time
series forecasting. TFT uses recurrent layers for local pro-
cessing and interpretable self-attention layers for long-term
dependencies [ 23]. We also use the naive BU reconciliation
strategy after fitting the bottom series with TFT.
â€¢GPHF: HTS forecasting model using Gaussian Processes [ 31].
No reconciliation strategy is needed with this approach as
the model already takes into account the hierarchical struc-
ture of the data.
5.3 Results and Discussion
We start by addressing Q1by evaluating the impact of our pro-
posed transformations (Section 4.2) in the distance distributions
between the generated and original datasets. Remember that HTS
algorithms rely on the dependencies between time series to improve
the univariate estimates. Thus, we are interested in measuring how
these relations are disrupted when applying each transformation
controlled by its magnitude. Looking at Figure 2, we are systemati-
cally generating rich variations of the original datasets. Also, the
behavior is consistent across all datasets.
If we look closer to Figure 2, for every transformation, the effect is
limited and increases with the magnitude of the transformation, as
expected. The transformation with the largest effect on the distance
is magnitude warping, potentially having a greater impact on the
performance of the algorithms that rely on the relations between
 
2496RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Dataset Method Bottom Purpose Region State Zone Top
Tourism ETS + BU 0.86 1.08 0.84 1.06 0.95 1.50
ETS + MinT 0.88 1.03 0.85 1.04 0.93 1.43
DeepAR + BU 0.87 1.24 0.84 1.17 0.94 1.78
TFT + BU 0.82 1.74 0.87 1.47 1.08 2.59
GPHF 0.95 1.02 0.89 1.03 0.96 1.12
Dataset Method Bottom Category Depart. Item State Store Top
M5 ETS + BU 0.82 0.61 0.71 0.75 0.75 0.80 0.73
ETS + MinT 0.84 0.59 0.71 0.76 0.77 0.80 0.73
DeepAR + BU 0.75 0.71 0.77 0.70 0.95 0.92 1.06
TFT + BU 0.75 0.89 0.80 0.70 1.35 1.14 1.55
GPHF 0.90 0.66 0.70 0.84 0.63 0.79 0.60
Dataset Method Bottom Crime Beat Street Zip Top
Police ETS + BU 0.98 0.82 0.84 0.77 0.83 0.85
ETS + MinT 0.95 0.85 0.83 0.84 0.82 1.34
DeepAR + BU 1.03 0.85 0.88 0.77 0.86 0.92
TFT + BU 0.65 1.64 0.95 2.01 1.00 7.09
GPHF 0.99 0.87 0.84 0.76 0.85 0.77
Table 1: Results (MASE) for the original datasets considered in the experiments. Bold values represent the lowest error across
models. The errors are calculated for each group, for bottom and top-level series.
the series. The jittering transformation reduces the spread of the
distribution, i.e., it pushes the number of series that are very similar
or dissimilar to each other to be closer to the original mean distance.
Finally, as expected, the time warping transformation does not
produce a relevant impact in the distance. We are using DTW
to measure the distance, which already accounts for time-based
warping effects. In this case, we are interested in the fact that
although the distance does not change, there are warping effects in
the time dimension that could impact performance.
Regarding Q2, Figure 3 suggests that predictive performance
varies significantly by transformation applied. Also, and answer-
ingQ3, increasing the magnitude of transformations has not only
different effects but, in some cases, opposite ones. Figure 3 lays
out a comparative analysis of different HTS forecasting models
(columns) in terms of their sensitivity to transformations (rows) in
the time series data. We control by different levels of the hierarchy
within the Tourism dataset â€” such as â€˜bottomâ€˜, â€˜purposeâ€˜, â€˜regionâ€˜,
and so on. Note that as the x-axis increases, the magnitude of the
transformation applied also increases. Thus, a flatter line would sug-
gest that a model is less sensitive to that particular transformation,
maintaining a consistent performance despite the increasing inten-
sity of data alteration. As anticipated, magnitude warping is the
most disruptive transformation, and its impact increases with the
magnitude. For all models, we can see that there is a positive slope,
i.e., consistently worse results when the parameters of the transfor-
mation increase. Interestingly, for jitter, scaling, and time warping,
the performance of all models stays flat or actually improves as we
increase the magnitude of the transformation (especially for jitter).
Comparing the robustness of the different models, we observe that
GPHF is the model that shows more consistent behavior, which
means less impact in terms of performance.
Regarding Q4, Figure 4 and the left radar chart on Figure 5 help
us compare the robustness of the different models. The classical
methods yield better results than the more complex counterparts in
most cases. The only exception is when the transformation is toodisruptive, such as magnitude warping, where we cannot spot any
meaningful difference between the predictive performance of the
different models. The radar chart in Figure 4 provides a visualization
of the ranking of different forecasting models in handling the most
disruptive transformation â€” magnitude warping â€” across various
magnitudes of this transformation. Each axis represents a different
set of parameters of the magnitude warping transformation, starting
from the original data (â€˜origâ€˜) and progressing through increasingly
intense transformations (â€˜v0â€˜ through â€˜v5â€˜). The distance from the
center indicates the rank of the method performance, with a rank of
1 being the closest to the center (best performance) and higher ranks
being further away (worse performance). The extensive crossing
of lines as the magnitude increases shows the disruptive effect of
the transformation. Thus, we cannot say that there is a model that
would be more robust to this transformation when applied to this
dataset in particular. We then expanded the visualization to all
transformations to have a global perspective on the robustness of
the models (see the left radar chart on Figure 5). ETS + MinT method
appears to be the most stable model across most transformations
(except for magnitude warping, as already discussed). It also seems
to suggest that using hierarchical information brings value to the
model performance since ETS-BU is consistently worse than ETS
+ MinT. Most interestingly, classical methods seem to be the most
robust when applied to this dataset.
When answering Q5, we start by applying a standard method-
ology to estimate the forecasting accuracy, which is often used to
test algorithms (e.g., to compare a newly proposed algorithm with
existing ones). In this analysis, researchers compare the predictive
performance of algorithms on a small number of datasets. We re-
produce what this analysis would look like, as shown in Table 1. We
can see that the results are somewhat consistent for the Tourism
and Police datasets. If M5 was not considered, this could lead us to
generalize on unreliable information. The main outcome is that we
get mixed results, which are hard to generalize. Furthermore, the
 
2497KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Luis Roque, Carlos Soares, and LuÃ­s Torgo
Figure 5: The chart on the left shows the performance of forecasting algorithms against multiple transformations for the
Tourism dataset. The chart on the right averages the performance ranks of forecasting algorithms for all datasets.
metrics presented in the table do not assess the robustness of the
different algorithms.
Still, there are some insights that we can extract from this type
of standard comparison. First, and contrary to our expectations,
the ETS model does not show meaningful differences when using
the naive reconciliation strategy or MinT. The global deep learning
models performed well on the bottom series, especially TFT, but not
on the aggregate-level ones. The GPHF model consistently produces
good predictions for aggregated levels, but it is less accurate on the
bottom level ones. Once again, can we generalize these results, or
are they the result of our experimental setup?
After using RHiOTS across all datasets and averaging the results,
we can more confidently answer Q6andQ7for all the algorithms.
We donâ€™t see relevant differences between ETS-BU and ETS-MinT,
and thus, using hierarchical information does not seem to produce
increased robustness. On the predictive performance of algorithms,
we consistently observe simpler algorithms (classical ones) yielding
better results across transformations than more complex ones. The
radar chart on the right in Figure 5 presents the average ranks of
various forecasting algorithms across all datasets and controlled
by transformation. It serves as a generalization of the chart on
the left of the same figure. The chart shows that deep learning
algorithms employing BU strategies, such as DeepAR and TFT,
often underperform other approaches. The only exception is when
faced with high-intensity magnitude warping. This is expected, as
complex algorithms are better equipped to handle such behavior.
The underperformance of these algorithms is not simply due to their
naive reconciliation strategies, as evidenced by the resilience of the
ETS + BU algorithm. GPHF ranks between deep learning algorithms
and classical ones. Classical approaches, ETS + BU and ETS + MinTyield the best robustness, making them an appealing choice for use
as a baseline when developing any new HTS algorithm.
6 CONCLUSIONS AND FUTURE WORK
We propose RHiOTS, a novel framework for generating semi-synthetic
time series datasets with controlled dependencies. We demonstrate
its effectiveness in evaluating the performance of HTS models and
algorithms under various conditions. Our empirical study using
RHiOTS confirms that model and algorithm performance varies
depending on the perturbations applied to the data and provides
insights into the expected effects based on the type of perturbation.
First, we show that RHiOTS creates rich variations of the origi-
nal datasets regarding the correlations between time series. When
evaluating models, the predictive performance varies substantially
based on the transformation applied. As we increase the magnitude,
performance degrades quickly in transformations like magnitude
warping, while it improves in cases such as jitter. When evaluat-
ing algorithms, we start by applying the conventional benchmark
analysis of comparing predictive performance across three datasets.
As the results were inconclusive, we extended the approach to use
RHiOTS. One takeaway is that we do not see meaningful differences
in robustness when applying specific reconciliation methods, such
as MinT. The second is that classical algorithms are more robust
than more complex deep learning counterparts. Deep learning algo-
rithms only showed more robustness when the transformation was
highly disruptive, such as high-intensity magnitude warping. The
aggregated findings from our visualizations and analyses suggest
that if a single model must be chosen without prior knowledge of
potential distortions in a dataset, the ETS model stands out as the
most robust option.
 
2498RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting Algorithms KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Future research can focus on developing finer transformation
controls, enabling a more targeted analysis of their impact on perfor-
mance. Meta-learning techniques can help more effectively identify
relationships between transformation parameters and model perfor-
mance. RHiOTS, combined with these research directions, will help
build a more comprehensive evaluation framework for hierarchical
time series forecasting models.
Acknowledgements. This work was partially funded by projects
AISym4Med (101095387) supported by Horizon Europe Cluster 1:
Health, ConnectedHealth (n.Âº 46858), supported by Competitive-
ness and Internationalisation Operational Programme (POCI) and
Lisbon Regional Operational Programme (LISBOA 2020), under
the PORTUGAL 2020 Partnership Agreement, through the Euro-
pean Regional Development Fund (ERDF) and NextGenAI - Cen-
ter for Responsible AI (C645008882-00000055), supported by IAP-
MEI, and also by FCT plurianual funding for 2020-2023 of LIACC
(UIDB/00027/2020_UIDP/00027/2020)
REFERENCES
[1]George Athanasopoulos, Roman A. Ahmed, and Rob J. Hyndman. 2009. Hierarchi-
cal forecasts for Australian domestic tourism. International Journal of Forecasting
25, 1 (2009), 146â€“166. https://doi.org/10.1016/j.ijforecast.2008.07.004
[2]George Athanasopoulos, Puwasala Gamakumara, Anastasios Panagiotelis, Rob J.
Hyndman, and Mohamed Affan. 2020. Hierarchical Forecasting. In Macroeconomic
Forecasting in the Era of Big Data: Theory and Practice, Peter Fuleky (Ed.). Springer
International Publishing, Cham, 689â€“719. https://doi.org/10.1007/978-3-030-
31150-6_21
[3]George Athanasopoulos, Puwasala Gamakumara, Anastasios Panagiotelis, Rob J.
Hyndman, and Mohamed Affan. 2020. Hierarchical Forecasting. In Macroeconomic
Forecasting in the Era of Big Data (1st ed.), Peter Fuleky (Ed.). Springer, 689â€“719.
https://doi.org/10.1007/978-3-030-31150-6_21
[4]George Athanasopoulos and Rob Hyndman. 2006. Modeling and forecasting
Australian domestic tourism. Monash University, Department of Econometrics and
Business Statistics, Monash Econometrics and Business Statistics Working Papers 29
(01 2006). https://doi.org/10.1016/j.tourman.2007.04.009
[5]George Athanasopoulos and Nikolaos Kourentzes. 2020. On the Evaluation of
Hierarchical Forecasts. Monash Econometrics and Business Statistics Working
Papers 2/20. Monash University, Department of Econometrics and Business
Statistics. https://EconPapers.repec.org/RePEc:msh:ebswps:2020-2
[6]Christoph Bergmeir, Rob J. Hyndman, and JosÃ© M. BenÃ­tez. 2016. Bagging ex-
ponential smoothing methods using STL decomposition and Boxâ€“Cox trans-
formation. International Journal of Forecasting 32, 2 (2016), 303â€“312. https:
//doi.org/10.1016/j.ijforecast.2015.07.002
[7]Hong Cao, Vincent Tan, and John Pang. 2014. A Parsimonious Mixture of
Gaussian Trees Model for Oversampling in Imbalanced and Multimodal Time-
Series Classification. IEEE transactions on neural networks and learning systems
25 (12 2014), 2226â€“2239. https://doi.org/10.1109/TNNLS.2014.2308321
[8]Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron Courville,
and Y. Bengio. 2015. A Recurrent Latent Variable Model for Sequential Data. 8.
[9]Valentin Flunkert, David Salinas, and Jan Gasthaus. 2017. DeepAR: Probabilistic
Forecasting with Autoregressive Recurrent Networks. CoRR abs/1704.04110
(2017). arXiv:1704.04110 http://arxiv.org/abs/1704.04110
[10] Simone Gitto, Carmela Di Mauro, Alessandro Ancarani, and Paolo Mancuso. 2021.
Forecasting national and regional level intensive care unit bed demand during
COVID-19: The case of Italy. Plos one 16, 2 (2021), e0247726.
[11] Maxime Goubeaud, Philipp JouÃŸen, Nicolla Gmyrek, Farzin Ghorban, Lucas
Schelkes, and Anton Kummert. 2021. Using Variational Autoencoder to augment
Sparse Time series Datasets. In 2021 7th International Conference on Optimization
and Applications (ICOA). 1â€“6. https://doi.org/10.1109/ICOA51614.2021.9442619
[12] Maxime Goubeaud, Philipp JouÃŸen, Nicolla Gmyrek, Farzin Ghorban, Lucas
Schelkes, and Anton Kummert. 2021. Using Variational Autoencoder to augment
Sparse Time series Datasets. In 2021 7th International Conference on Optimization
and Applications (ICOA). 1â€“6. https://doi.org/10.1109/ICOA51614.2021.9442619
[13] Xing Han, Sambarta Dasgupta, and Joydeep Ghosh. 2021. Simultaneously
Reconciled Quantile Forecasting of Hierarchically Related Time Series. CoRR
abs/2102.12612 (2021). arXiv:2102.12612 https://arxiv.org/abs/2102.12612
[14] Robin John Hyndman and George Athanasopoulos. 2021. Forecasting: Principles
and Practice (3rd ed.). OTexts, Australia.
[15] Rob J Hyndman, Roman A Ahmed, George Athanasopoulos, and Han Lin Shang.
2011. Optimal combination forecasts for hierarchical time series. ComputationalStatistics & Data Analysis 55, 9 (2011), 2579â€“2589.
[16] Rob J Hyndman and George Athanasopoulos. 2018. Forecasting: principles and
practice. OTexts.
[17] Brian Kenji Iwana and Seiichi Uchida. 2021. An empirical survey of data aug-
mentation for time series classification with neural networks. PLOS ONE 16, 7
(jul 2021), e0254841. https://doi.org/10.1371/journal.pone.0254841
[18] Yanfei Kang, Rob Hyndman, and Feng Li. 2020. GRATIS: GeneRAting TIme Series
with diverse and controllable characteristics. Statistical Analysis and Data Mining:
The ASA Data Science Journal 13 (05 2020). https://doi.org/10.1002/sam.11461
[19] Juan Pablo Karmy and SebastiÃ¡n Maldonado. 2019. Hierarchical time series
forecasting via Support Vector Regression in the European Travel Retail Industry.
Expert Systems with Applications 137 (2019), 59â€“73. https://doi.org/10.1016/j.
eswa.2019.06.060
[20] Eamonn J. Keogh and Shruti Kasetty. 2004. On the Need for Time Series Data
Mining Benchmarks: A Survey and Empirical Demonstration. Data Mining and
Knowledge Discovery 7 (2004), 349â€“371.
[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-
sification with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-
berger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/
paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf
[22] Shanika L Wickramasuriya, George Athanasopoulos, and Rob Hyndman. 2018.
Optimal Forecast Reconciliation for Hierarchical and Grouped Time Series
Through Trace Minimization. J. Amer. Statist. Assoc. (03 2018), 1â€“45. https:
//doi.org/10.1080/01621459.2018.1448825
[23] Bryan Lim, Sercan O. Arik, Nicolas Loeff, and Tomas Pfister. 2020. Temporal
Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting.
arXiv:1912.09363 [stat.ML]
[24] Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. 2021. The
M5 competition: Background, organization, and implementation. International
Journal of Forecasting (2021). https://doi.org/10.1016/j.ijforecast.2021.07.007
[25] Hao Ni, Lukasz Szpruch, Marc Sabate-Vidales, Baoren Xiao, Magnus Wiese,
and Shujian Liao. 2021. Sig-Wasserstein GANs for Time Series Generation.
arXiv:2111.01207 [cs.LG]
[26] Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. 2019. N-
BEATS: Neural basis expansion analysis for interpretable time series forecasting.
https://doi.org/10.48550/ARXIV.1905.10437
[27] Houston Police. 2022. Open data from the Houston Police Department for criminal
reports. https://www.houstontx.gov/police/public_information.htm. Accessed:
2022-03-01.
[28] Khandakar M. Rashid and Joseph Louis. 2019. Time-Warping: A Time Series Data
Augmentation of IMU Data for Construction Equipment Activity Identification.
InProceedings of the 36th International Symposium on Automation and Robotics
in Construction (ISARC), Mohamed Al-Hussein (Ed.). International Association
for Automation and Robotics in Construction (IAARC), Banff, Canada, 651â€“657.
https://doi.org/10.22260/ISARC2019/0087
[29] UCI Machine Learning Repository. year of dataset publication. Elec-
tricity Load Diagrams 2011-2014. https://archive.ics.uci.edu/dataset/321/
electricityloaddiagrams20112014. Accessed: date of access.
[30] UCI Machine Learning Repository. year of dataset publication. PEMS-SF. https:
//archive.ics.uci.edu/dataset/204/pems+sf. Accessed: date of access.
[31] Luis Roque, Luis Torgo, and Carlos Soares. 2022. Gaussian Processes for Hierar-
chical Time Series Forecasting. 8th ACM 8th SIGKDD International Workshop on
Mining and Learning from Time Series. https://kdd-milets.github.io/milets2022/
papers/MILETS_2022_paper_9727.pdf
[32] Slawek Smyl. 2019. A hybrid method of exponential smoothing and recurrent
neural networks for time series forecasting. International Journal of Forecasting
36 (07 2019). https://doi.org/10.1016/j.ijforecast.2019.03.017
[33] Souhaib Ben Taieb, James W. Taylor, and Rob J. Hyndman. 2021. Hierarchical
Probabilistic Forecasting of Electricity Demand With Smart Meter Data. J. Amer.
Statist. Assoc. 116, 533 (2021), 27â€“43. https://doi.org/10.1080/01621459.2020.
1736081 arXiv:https://doi.org/10.1080/01621459.2020.1736081
[34] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun Gao, Xue Wang,
and Huan Xu. 2021. Time Series Data Augmentation for Deep Learning: A
Survey. In Proceedings of the Thirtieth International Joint Conference on Artificial
Intelligence (IJCAI-2021). International Joint Conferences on Artificial Intelligence
Organization. https://doi.org/10.24963/ijcai.2021/631
[35] Renjie Wu and Eamonn J. Keogh. 2023. Current Time Series Anomaly De-
tection Benchmarks are Flawed and are Creating the Illusion of Progress.
IEEE Transactions on Knowledge and Data Engineering 35, 3 (2023), 2421â€“2429.
https://doi.org/10.1109/TKDE.2021.3112126
[36] Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. 2019. Time-series
Generative Adversarial Networks. In Advances in Neural Information Processing
Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and
R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/
paper_files/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf
 
2499