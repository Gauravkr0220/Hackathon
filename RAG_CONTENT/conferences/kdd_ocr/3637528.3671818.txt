Efficient Exploration of the Rashomon Set of Rule-Set Models
Martino Ciaperoniâ€ 
martino.ciaperoni@aalto.fi
Aalto University
Espoo, FinlandHan Xiaoâ€ 
xiaohan2012@gmail.com
The Upright Project
Helsinki, FinlandAristides Gionis
argioni@kth.se
KTH Royal Institute of Technology
Stockholm, Sweden
ABSTRACT
Today, as increasingly complex predictive models are developed,
simple rule sets remain a crucial tool to obtain interpretable pre-
dictions and drive high-stakes decision making. However, a single
rule set provides a partial representation of a learning task. An
emerging paradigm in interpretable machine learning aims at ex-
ploring the Rashomon set of all models exhibiting near-optimal
performance. Existing work on Rashomon-set exploration focuses
on exhaustive search of the Rashomon set for particular classes of
models, which can be a computationally challenging task. On the
other hand, exhaustive enumeration leads to redundancy that often
is not necessary, and a representative sample or an estimate of the
size of the Rashomon set is sufficient for many applications. In this
work, we propose, for the first time, efficient methods to explore
the Rashomon set of rule-set models with or without exhaustive
search. Extensive experiments demonstrate the effectiveness of the
proposed methods in a variety of scenarios.
CCS CONCEPTS
â€¢Computing methodologies â†’Supervised learning; Rule
learning.
KEYWORDS
Interpretable machine learning, Rashomon set, Rule-based classifi-
cation, Scalable algorithms
ACM Reference Format:
Martino Ciaperoniâ€ , Han Xiaoâ€ , and Aristides Gionis. 2024. Efficient Ex-
ploration of the Rashomon Set of Rule-Set Models. In Proceedings of the
30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3671818
1 INTRODUCTION
Following the impressive results achieved by modern machine-
learning methods, automated decision making is used in consequen-
tial domains, such as health care, education, and criminal justice.
However, many state-of-the-art models are opaque, and as such,
they are difficult to interpret, understand, and trust. In other cases,
they may hide harmful biases [ 34]. Thus, the research community
â€ Both authors contributed equally to this work.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671818
0.04 0.06 0.08 0.10 0.12 0.14 0.16 0.18
SP [race]0.00.10.20.30.40.50.60.7SP [gender]Rule sets in a Rashomon set for the COMPAS dataset
0.560.570.580.590.600.610.620.630.64
AccuracyA
CC: 0.65, SP[race]: 0.19
IfPrior-Crimes>3âˆ¨Age=18-25
ğ‘¦=1
Elseğ‘¦=0A
CC: 0.62, SP[race]: 0.09
IfPrior-Crimes>3âˆ¨
(Gender=Maleâˆ§Prior-Crimes=1-3)
ğ‘¦=1
Elseğ‘¦=0
Figure 1: A Rashomon set of rule sets in the Compas dataset.
For each rule set, we show accuracy (colour) and statistical
parity (SP) [ 9] on race (ğ‘¥-axis) and gender ( ğ‘¦-axis). Two ex-
ample rule sets with similar accuracy, but highly different
statistical parity on race, are additionally presented.
has become increasingly aware of the importance of inherently-
interpretable machine-learning algorithms, and there is a pressing
need for models that can be understood and trusted by humans.
Logical models, based on â€œ if-then â€ rules, are intrinsically inter-
pretable models for predictive tasks. Among popular logical models,
in this work we focus on rule sets, which are particularly easy to
interpret [ 25]. Extension to more structured logical models, such
asrule lists ordecision trees, is left to future work.
Another significant aspect of interpretable machine learning is
that, often, a single model does not offer an adequate representa-
tion of reality since there is a large set of models with near-optimal
predictive performance. In the literature, such a set is referred to as
Rashomon set. Rashomon sets have been shown to have applications
in multiple domains, including credit-score estimation, natural-
language processing, health-record analysis, recidivism prediction,
and more [ 23,34,37,43]. Considering the entire Rashomon set
rather than a single model provides a wealth of actionable informa-
tion. For instance, computing the proportion of models belonging
to the Rashomon set allows us to characterize the complexity of a
learning task [ 37]. Additionally, Rashomon sets allow us to study im-
portant properties of machine-learning models, such as fairness [ 15]
and feature importance [ 43]. As a concrete example, Figure 1 depicts
a Rashomon set of rule sets for the Compas dataset used for recidi-
vism prediction. Although the rule sets in the Rashomon set have
similar accuracy scores (ranging from 0.56to0.65), two important
measures of fairness vary significantly.
Due to the combinatorial explosion of the search space, exhaus-
tive enumeration or storage of the rule sets in the Rashomon set
poses significant computational challenges, and may not always be
feasible. In this paper, we propose, for the first time, methods to
efficiently explore the Rashomon set with or without exhaustive
 
478
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
enumeration. As demonstrated in Section 7, the proposed methods
accurately reveal the complexity inherent in tackling a learning
task based on rule sets, as well as other key properties of rule sets
including feature importance and fairness.
All the methods we propose rely on a highly-optimized branch-
and-bound algorithm for exhaustive enumeration of the rule sets
in the Rashomon set. To scale up, the branch-and-bound algorithm
leverages (ğ‘–) pruning bounds that effectively restrict the search
space, and (ğ‘–ğ‘–) incremental computation to re-use previously com-
puted results. Building on our branch-and-bound algorithm for
exhaustive enumeration, we introduce two alternative approaches
for non-exhaustive exploration of the Rashomon set by generating
representative samples and estimating its size. The first approach
partitions the solution space into random cells and enumerates
the solutions in one randomly selected cell. The second approach
instead simply visits subsets of the search space and constructs sam-
ples during the process. The samples generated by both approaches
are supported by guarantees of near uniformity.
In summary, we make the following contributions.
â€¢We formally describe exact and approximate variants of the prob-
lems of exhaustive and non-exhaustive enumeration of rule sets
in the Rashomon set.
â€¢We propose a branch-and-bound algorithm, named BBenum , for
efficient exhaustive enumeration.
â€¢AsBBenum may incur high cost, we develop ApproxSample and
ApproxCount , two highly-optimized algorithms with strong
quality guarantees, which allow for non-exhaustive exploration
of the Rashomon set by approximate uniform sampling and esti-
mation of the size of the Rashomon set.
â€¢We additionally devise BBsts , a faster, but generally less accurate
alternative to ApproxSample andApproxCount.
â€¢We evaluate the proposed algorithms in a thorough experimental
evaluation and through cases studies.
The rest of this paper is organized as follows. Section 2 discusses
related work. Section 3 introduces our notation and problem formu-
lations. Section 4 presents the proposed method for exhaustive enu-
meration of the Rashomon set, while Sections 5 and 6 describe the
proposed methods for non-exhaustive exploration of the Rashomon
set. Section 7 presents our experimental evaluation, and finally,
Section 8 is a short conclusion.
2 RELATED WORK
Interpretable machine learning . The study of interpretable mod-
els to address machine-learning tasks is a fast-growing field. The
topic is related to explainable machine learning [6], which aims at
explaining the predictions of opaque models [ 8]. However, there is
evidence that explaining opaque algorithms may provide mislead-
ing and even false characterizations [ 26,33]. Therefore, there is a
need for novel inherently interpretable models.
Optimal logical models. Logical models (including rule sets, rule
lists, and decision trees ) are prominent examples of interpretable
models that have been successfully used in a variety of applica-
tions [ 34,40,45]. Over the years, due to the complexity inherent in
the optimization, approximate algorithms and heuristic approaches
have been employed to find logical models with good predictionperformance. Recent advances in computing power and algorith-
mic techniques, however, motivate the search for globally optimal
models for different classes of logical models. For finding optimal
rule lists [ 5] and decision trees [ 22], ad hoc branch-and-bound
algorithms have been proposed, while existing work on finding
optimal rule sets relies on off-the-shelf SAT solvers [ 41] or integer
programming solvers [28].
The Rashomon set. In recent years, research in interpretable ma-
chine learning has emphasized the importance of going beyond
a single model. The Rashomon effect [7] expresses the idea that a
real-world phenomenon can be explained equally well by multiple
models. Such a set of models is referred to as the Rashomon set [34],
and finds a number of interesting applications, such as measuring
the complexity of a learning task [ 36], analyzing feature impor-
tance [18, 19] and investigating fairness in machine learning [29].
Recently, work has been carried out to develop techniques to
exhaustively enumerate the Rashomon set for particular classes of
models, including decision lists [30] and decision trees [43].
In a similar vein, Hara and Ishihata [21] consider approximate
and exact enumeration of rule sets and lists sorted by an objective
value, representing the quality of the model. Although this enumer-
ation problem is similar to the problem studied in this work, there
are crucial differences. Hara and Ishihata [21] consider a simplistic
formulation of the rule-set learning problem, which neglects false
positives. Further, the methods they propose hinge on particular
assumptions and the output rule models are required to be sorted
by their quality. In view of the mentioned differences, our proposed
methods and those by Hara and Ishihata [21] are not directly com-
parable. Additionally, the methods of Hara and Ishihata [21] are
not competitive with ours, in terms of runtime. For instance, for the
Compas dataset, we observe that the time required by our methods
to enumerate 50rule sets is on par with the time required by the
methods of Hara and Ishihata [21] to find a single rule set.
Rule sets can be regarded as less constrained extensions of de-
cision lists and trees. The problem of enumerating the Rashomon
set for rule sets is more challenging since the additional structure
imposed by decision lists and trees allows for pruning additional
portions of the search space. This computational challenge calls for
novel ideas: we can effectively explore the Rashomon set for rule
sets without exhaustive enumeration. Non-exhaustive enumeration,
which is the main focus of this work, is a largely unexplored topic.
Constrained counting and sampling. Constrained (or model)
sampling and counting is a fundamental problem in artificial in-
telligence involving sampling and counting the satisfying assign-
ments of a propositional formula. The problem is known to be
computationally hard [ 39]. Thus, approximate solutions have been
investigated. Chakraborty et al . [11,12]leverage hash functions to
randomly partition the space of possible models into small cells,
and satisfying assignments are sampled via calls to SAT solvers. In
this work, we leverage this idea to design efficient sampling and
counting algorithms that do not require exhaustive enumeration.
Ermon et al . [17] propose an alternative approach for approximate
model sampling. The algorithm leverages a SAT solver whilst en-
forcing a uniform exploration of the search space. We also build on
this idea to design alternative efficient algorithms for sampling and
counting without the need of exhaustive enumeration.
 
479Efficient Exploration of the Rashomon Set of Rule-Set Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
3 PROBLEM FORMULATION
We use boldface uppercase letters to denote matrices, e.g., ğ‘¨, and
boldface lowercase letters to denote vectors, e.g., ğ’™andğ’ƒ. For a
matrix ğ‘¨, we use ğ‘¨ğ‘–to denote its ğ‘–-th row, ğ‘¨:ğ‘–to denote its first ğ‘–
rows, and ğ‘¨ğ‘–,ğ‘—to denote the ğ‘—-th element of ğ‘¨ğ‘–. Similarly, for a
vector ğ’ƒ, we use ğ’ƒğ‘–andğ’ƒ:ğ‘–to denote the ğ‘–-th element and the
firstğ‘–elements of ğ’ƒ, respectively. Given a positive integer ğ‘€and
a sequence of positive integers ğ‘†with values in the set {1,...,ğ‘€},
we use 1ğ‘†âˆˆ{0,1}ğ‘€to denote the indicator vector of ğ‘†, i.e.,1ğ‘†,ğ‘–=1
ifğ‘–âˆˆğ‘†and1ğ‘†,ğ‘–=0otherwise.
3.1 Preliminaries
We restrict our setting to binary classification with binary-valued
features. More general settings can be mapped to the setting we
study via preprocessing, although the performance of the resulting
methods will depend on the preprocessing methodology. Extending
our methods to more general settings is left for future work.
We denote the training data as D=[(ğ’™ğ‘›,ğ‘¦ğ‘›)]ğ‘
ğ‘›=1, where ğ’™ğ‘›âˆˆ
{0,1}ğ½are binary features and ğ‘¦ğ‘›âˆˆ{0,1}is the label. Let ğ’™ğ‘›,ğ‘—
denote the value of the ğ‘—-th feature of the observation vector ğ’™ğ‘›.
A rule setğ‘†=(ğ‘Ÿ1,...,ğ‘Ÿğ¿)of sizeğ¿consists ofğ¿distinct decision
rules. A decision rule (or simply, rule) ğ‘Ÿ=ğ‘â†’ğ‘is a logical
implication â€œif ğ‘thenğ‘â€. An antecedent ğ‘is a clause consisting of a
conjunction of features. For data point ğ’™ğ‘›, antecedent ğ‘evaluates to
trueif all features of ğ‘have value 1, i.e., ğ’™ğ‘›,ğ‘—=1for all features ğ‘—inğ‘,
and it evaluates to false otherwise. A consequent ğ‘is the predicted
label. For instance, the rule (ğ’™ğ‘›,2=1)âˆ§( ğ’™ğ‘›,5=1)â†’ğ‘¦ğ‘›=1
predictsğ‘¦ğ‘›=1for any data point ğ’™ğ‘›with ğ’™ğ‘›,2=1andğ’™ğ‘›,5=1.
We say that a rule ğ‘Ÿ=ğ‘â†’ğ‘captures a data point ğ’™ğ‘›, writ-
ten as cap(ğ’™ğ‘›,ğ‘Ÿ)=1, ifğ‘evaluates ğ’™ğ‘›to true. We say that the
rule setğ‘†captures ğ’™ğ‘›, written as cap(ğ’™ğ‘›,ğ‘†)=1, if at least one
rule inğ‘†captures ğ’™ğ‘›. Ifğ’™ğ‘›is not captured by any rule in ğ‘†, we
write cap(ğ’™ğ‘›,ğ‘†)=0. As it is common [ 13,42], to prioritize inter-
pretability, we consider rule sets consisting of positive rules only,
i.e.,ğ‘=(ğ‘¦ğ‘›=1).1In other words, if cap(ğ’™ğ‘›,ğ‘†)=1, then the pre-
diction isğ‘¦ğ‘›=1, while if cap(ğ’™ğ‘›,ğ‘†)=0, the prediction is ğ‘¦ğ‘›=0.
We assume that a set of candidate decision rules U={ğ‘Ÿ1,...,ğ‘Ÿğ‘€}
is provided.2We further assume that the rules in Uare ordered,
e.g., lexicographically, indicated by a subscript index. Hence, we say
that ruleğ‘Ÿğ‘˜isbefore (orafter ) ruleğ‘Ÿâ„“ifğ‘˜<â„“(orğ‘˜>â„“). We assume
that the rules of a rule set ğ‘†are sorted in ascending order. We say
that a rule set ğ‘†â€²starts with rule setğ‘†ifğ‘†âŠ†ğ‘†â€²and all rules in ğ‘†â€²\ğ‘†
are after the last rule in ğ‘†. We denote by ğ‘†max=arg maxğ‘–{ğ‘Ÿğ‘–âˆˆğ‘†}
the largest rule index in a given rule set ğ‘†âŠ†U .
For a ruleğ‘Ÿğ‘˜âˆˆğ‘†, we define cap(ğ’™ğ‘›,ğ‘Ÿğ‘˜|ğ‘†)=1ifğ’™ğ‘›is captured
byğ‘Ÿğ‘˜, but not by rules in ğ‘†that are before ğ‘Ÿğ‘˜, i.e.,
cap(ğ’™ğ‘›,ğ‘Ÿğ‘˜|ğ‘†)=cap(ğ’™ğ‘›,ğ‘Ÿğ‘˜)âˆ§âˆ§
ğ‘Ÿâ„“âˆˆğ‘†|â„“<ğ‘˜(Â¬cap(ğ’™ğ‘›,ğ‘Ÿâ„“)).
When the context is clear, we use rules (e.g., ğ‘Ÿğ‘–) and their indices
(e.g.,ğ‘–) interchangeably. As a result, a rule set ğ‘†can be represented
as a sorted list of integers and 1ğ‘†âˆˆ{0,1}ğ‘€represents the indicator
vector of the rule indices in ğ‘†.
1Negative rules, i.e. ğ‘=(ğ‘¦ğ‘›=0), when used together with positive rules, may hinder
interpretability by simultaneously predicting labels as 0and1.
2For instance, the set of rules can be obtained via some association rule-mining algo-
rithm [24], like the FP-growth algorithm [20].3.2 Objective function
To assess a rule set ğ‘†in terms of accuracy and interpretability, we
consider the following objective function:
ğ‘“(ğ‘†;ğœ†)=â„“(ğ‘†)+ğœ†|ğ‘†|, (1)
which consists of a misclassification error term â„“(ğ‘†)and a penalty
term|ğ‘†|for model complexity. The intuition is that, for a given
level of accuracy, shorter rule sets are preferred as they are easier
to interpret and are less prone to overfitting. The regularization
parameterğœ†>0controls the relative importance of the two terms.
The loss term â„“can be decomposed into:
â„“(ğ‘†)=â„“ğ‘(ğ‘†)+â„“0(ğ‘†), (2)
where
â„“ğ‘(ğ‘†)=1
ğ‘ğ‘âˆ‘ï¸
ğ‘›=1ğ¾âˆ‘ï¸
ğ‘˜=1(cap(ğ’™ğ‘›,ğ‘Ÿğ‘˜|ğ‘†)âˆ§ 1[ğ‘¦ğ‘›â‰ 1])and (3)
â„“0(ğ‘†)=1
ğ‘ğ‘âˆ‘ï¸
ğ‘›=1(Â¬cap(ğ’™ğ‘›,ğ‘†)âˆ§ 1[ğ‘¦ğ‘›=1]). (4)
The termâ„“ğ‘is the proportion of false positives of the rule set ğ‘†,
while the term â„“0is the proportion of false negatives.
3.3 The Rashomon set of decision sets
Given a set of candidate decision rules U, an objective function ğ‘“(Â·;ğœ†)
for evaluating rule sets, and a parameter ğœƒâˆˆR+, we define the
Rashomon set of rule sets for Uwith respect to ğœ†andğœƒas:
R(U,ğœ†,ğœƒ)={ğ‘†âŠ†U|ğ‘“(ğ‘†;ğœ†)â‰¤ğœƒ}. (5)
When the context is clear, we use R(U)instead ofR(U,ğœ†,ğœƒ).
In the literature, the Rashomon set is sometimes alternatively
defined with ğœƒ=ğ‘“âˆ—+ğ›¼, whereğ‘“âˆ—is the optimal objective value.
3.4 Problem formulation
We consider a set of candidate rules U={ğ‘Ÿ1,...,ğ‘Ÿğ‘€}, each of
which passes a given threshold on the number of captured training
points. This definition of Uis common in the literature [ 5,24,25].
To constructU, we resort to the popular FP-growth algorithm [ 20].
We first consider the problem of exhaustively enumerating R(U).
Problem 1 (Enumeration). Given a set of candidate rules U, and
parametersğœ†>0andğœƒ>0, enumerate all rule sets in R(U,ğœ†,ğœƒ).
Solving this problem allows us to compute |R(U,ğœ†,ğœƒ)|and draw
uniform samples from R(U,ğœ†,ğœƒ). The number|R(U,ğœ†,ğœƒ)|can be
further used to compute the Rashomon ratio [37], which is defined
as the ratio between |R(U)|and the total number of models.3This
ratio is a measure of complexity of a learning problem. The larger
the ratio, the more likely that a simple-yet-accurate model exists.
Problem 1 is #P-hard and the problems of almost-uniform sam-
pling and approximate counting, defined next, are also hard, as they
can be shown to generalize similar problems whose complexity has
been established in the literature [44].
We define as a sampling algorithm S(or sampler) any algorithm
that, given as input the set of candidate rules U, the objective
functionğ‘“and the value of the upper bound ğœƒ, returns a random
3In our case, the Rashomon ratio is computed as |R(U)|/(2ğ‘€âˆ’1).
 
480KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
Algorithm 1 BBenum , a branch-and-bound algorithm to enumer-
ate all rule sets inR(U,ğœ†,ğœƒ).
1:ğ‘„â†Queue([âˆ…])
2:whileğ‘„is not empty do
3:ğ‘†â†ğ‘„.pop()
4: forğ‘–in{ğ‘†max+1,...,ğ‘€}do
5:ğ‘†â€²â†ğ‘†âˆª{ğ‘–}
6: ifğ‘(ğ‘†â€²)â‰¤ğœƒ{Hierarchical lower bound} then
7: ifğ‘(ğ‘†â€²)+ğœ†â‰¤ğœƒ{Look-ahead bound} then
8: if|ğ‘†â€²|â‰¤âŒŠğœƒâˆ’ğ‘(ğ‘†â€²)
ğœ†âŒ‹{Size bound} then
9: ğ‘„.push(ğ‘†â€²)
10: ifğ‘“(ğ‘†â€²)â‰¤ğœƒthen
11: yieldğ‘†â€²{Yield a feasible solution}
element fromR(U). Similarly, a counting algorithm Creceives the
same inputs and estimates |R(U)|.
Problem 2 (Almost-uniform sampling). Given objective func-
tionğ‘“, find a samplerS, such that for a bound ğœƒâˆˆR+, tolerance
parameterğœ–âˆˆR+, andğ‘†âˆˆR(U), it holds
1
(1+ğœ–)1
|R(U)|â‰¤Pr(S(U,ğ‘“,ğœƒ,ğœ–)=ğ‘†)â‰¤(1+ğœ–)1
|R(U)|.(6)
We similarly define the approximate counting problem.
Problem 3 (Approximate counting). Find a counting algo-
rithmC, such that for a tolerance parameter ğœ–âˆˆR+and a confidence
parameterğ›¿âˆˆ[0,1], it holds
Pr|R(U)|
1+ğœ–â‰¤C(U,ğ‘“,ğœƒ,ğœ–,ğ›¿)â‰¤(1+ğœ–)|R(U)|
â‰¥1âˆ’ğ›¿. (7)
4 AN EXACT ALGORITHM VIA COMPLETE
ENUMERATION
In this section, we describe our solution for Problem 1, a branch-
and-bound algorithm equipped with effective pruning bounds and
incremental computation techniques, which enumerates efficiently
all rule sets inR(U,ğœ†,ğœƒ). Similar enumeration problems have been
studied for other types of logical models, such as decision lists [ 30]
and decision trees [43], but new ideas are required for rule sets.
4.1 A branch-and-bound algorithm
In order to find the set of feasible solutions, the algorithm we
propose, referred to as BBenum and presented in Algorithm 1, visits
rule sets in a breadth-first fashion with the help of a queue and
leverages a hierarchy among the rule sets to prune away the rule
setsğ‘†â€²that start with a rule set ğ‘†if certain criteria on ğ‘†are met.
In particular, at each iteration, the rule set at the front of the
queue is popped and extended with an additional rule, whose index
is in the range[ğ‘†max+1,...,ğ‘€], to formğ‘†â€². Next, we check using
bounds (described shortly) whether rule set ğ‘†â€²and any rule set
starting with ğ‘†â€²can be pruned. If ğ‘†â€²is not pruned, we enqueue it.
If in addition the objective value achieved by ğ‘†â€²is below the upper
boundğœƒ, we addğ‘†â€²to the Rashomon set.
The proposed pruning bounds are based on two key observations:
(ğ‘–)rule sets form a hierarchy under prefix relations, i.e., ğ‘†â€²âŠ†U is a
descendant of ğ‘†âŠ†U in the hierarchy if ğ‘†â€²starts withğ‘†;(ğ‘–ğ‘–)certain
characteristics of a given rule set can determine the feasibility ofits descendants in the hierarchy. We next illustrate the details of
the pruning bounds. Proofs of all non-trivial results presented in
the following sections are provided in an extended version of this
paper available online [14].
Hierarchical objective lower bound. For a rule set ğ‘†, we define:
ğ‘(ğ‘†)=â„“ğ‘(ğ‘†)+ğœ†|ğ‘†|. (8)
Then, for any ğ‘†â€²that starts with ğ‘†, the quantity ğ‘(ğ‘†)serves as a
lower bound for ğ‘“(ğ‘†â€²), as formalized next.
Theorem 1 (Hierarchical objective lower bound). For any
rule setğ‘†âŠ†U and anyğ‘†â€²âŠ†U that starts with ğ‘†, it isğ‘“(ğ‘†â€²)â‰¥ğ‘(ğ‘†).
In other words, all rule sets ğ‘†â€²starting with a rule set ğ‘†for which
ğ‘(ğ‘†)>ğœƒare infeasible.
Look-ahead lower bound. The next bound takes Theorem 1 one
step further by observing that any superset of ğ‘†must include at
least an additional rule.
Theorem 2 (Look-ahead lower bound). For a given rule set
ğ‘†âŠ†U , ifğ‘(ğ‘†)+ğœ†>ğœƒ, then for any rule set ğ‘†â€²âŠ†U that starts with ğ‘†
and is a proper superset of ğ‘†(i.e.,ğ‘†â€²â‰ ğ‘†), it holds that ğ‘“(ğ‘†â€²)>ğœƒ.
Rule set size bound. Finally, we use the lower bound ğ‘(ğ‘†)to
bound the size of any rule set that can be part of the Rashomon set.
Theorem 3 (Rule set size bound). For a given rule set ğ‘†âŠ†U
and any rule set ğ‘†â€²âŠ†U that starts with ğ‘†, if|ğ‘†|>âŒŠ(ğœƒâˆ’ğ‘(ğ‘†))/ğœ†âŒ‹,
thenğ‘“(ğ‘†â€²)>ğœƒ.
We empirically find that the look-ahead and the rule-set-size
bounds are remarkably effective in pruning. Details are presented
in the extended version of this paper [14].
4.2 Incremental computation
To further speed up BBenum , we updateğ‘(Â·)andğ‘“(Â·)incrementally.
The update formulas are stated below.
Theorem 4 (Lower bound update). For any rule set ğ‘†âŠ†U and
anyğ‘†â€²âŠ†U that starts with ğ‘†and has exactly one more rule ğ‘Ÿ, i.e.,
ğ‘†â€²=ğ‘†âˆª{ğ‘Ÿ}, the following holds:
ğ‘ ğ‘†â€²=ğ‘(ğ‘†)+ğœ†+1
ğ‘ğ‘âˆ‘ï¸
ğ‘›=1(cap(ğ’™ğ‘›,ğ‘Ÿ|ğ‘†)âˆ§ 1[ğ‘¦ğ‘›â‰ 1]).
Thus, provided that ğ‘(ğ‘†)is computed already, computing ğ‘(ğ‘†â€²)
requires evaluating only the last term in the above sum.
Theorem 5 (Objective update). For any rule set ğ‘†âŠ†U and
anyğ‘†â€²âŠ†U that starts with ğ‘†and has exactly one more rule ğ‘Ÿ, i.e.,
ğ‘†â€²=ğ‘†âˆª{ğ‘Ÿ}, the following holds:
ğ‘“ ğ‘†â€²=ğ‘ ğ‘†â€²+1
ğ‘ğ‘âˆ‘ï¸
ğ‘›=1(Â¬cap(ğ’™ğ‘›,ğ‘†)âˆ§Â¬cap(ğ’™ğ‘›,ğ‘Ÿ)âˆ§ 1[ğ‘¦ğ‘›=1]).
The details of the branch-and-bound algorithm with incremental
computation are provided in the extended version of this paper [ 14].
5APPROXIMATION ALGORITHMS BASED ON
RANDOM PARTITIONING
In this section, we address Problems 2 and 3. We develop efficient
methods with theoretical quality guarantees. To achieve this objec-
tive, we leverage the SAT-based framework proposed by Meel [31].
 
481Efficient Exploration of the Rashomon Set of Rule-Set Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
However, since this framework scales poorly for our purposes, we
propose novel methods to improve scalability.
5.1 An algorithmic framework based on
random parity constraints
We illustrate the proposed framework by first discussing our algo-
rithm for the counting problem, i.e., Problem 3.
Approximate counting. Algorithm ApproxCount , shown as Al-
gorithm 2 in the extended version of this paper [ 14], generates ran-
dom parity constraints to partition the solution space into â€œsmall
cells.â€ It then measures the size of a random cell (i.e., the number
of solutions in the cell) and computes an estimate of |R(U)|by
multiplying that cell size by the number of cells.4To achieve the
desired confidence, the estimation is repeated on sufficiently many
random cells, and the median is returned as the final estimate.
To achieve the desired estimation quality, the algorithm deter-
mines an upper bound ğµon cell sizes based on a tolerance para-
meterğœ–. Each evaluation, carried out by ApproxCountCore (Al-
gorithm 3 in the extended version of this paper [ 14]) first generates
ğ‘€âˆ’1random parity constraints. Then, it searches for the number
of constraints that produce a cell of size closest to, but below, ğµ.
Finally, all the solutions in that cell are enumerated to obtain the
cell size.
Solution space partitioning via parity constraints. A system of
parity constraints is imposed on the original enumeration problem
(Problem 1). The system consists of ğ‘˜linear equations in the finite
field of 2 and can be written as ğ‘¨ğ’™=ğ’ƒ, where ğ‘¨âˆˆ{0,1}ğ‘˜Ã—ğ‘€,
ğ’ƒâˆˆ{0,1}ğ‘˜andğ’™âˆˆ{0,1}ğ‘€(the solution variable). The system
ğ‘¨ğ’™=ğ’ƒlocates a specific cell among the 2ğ‘˜counterparts (each corre-
sponding to a different value in {0,1}ğ‘˜). The set of feasible solutions
in that cell is denoted by R(U;ğ‘¨,ğ’ƒ)={ğ‘†âˆˆR(U)|ğ‘¨1ğ‘†=ğ’ƒ}.
Searching for the desired ğ‘˜.Given constraints ğ‘¨ğ’™=ğ’ƒ, where
ğ‘¨âˆˆ{0,1}(ğ‘€âˆ’1)Ã—ğ‘€andğ’ƒâˆˆ{0,1}ğ‘€, procedure LogSearch (Al-
gorithm 7 in the extended version of this paper [ 14]) finds the
value ofğ‘˜such that the cell size |R(U|ğ‘¨:ğ‘˜,ğ’ƒ:ğ‘˜)|is closest to,
but below,ğµ. For each attempted ğ‘˜,LogSearch invokes an ora-
cleParityConsEnum , which enumerates at mostğµsolutions in
R(U|ğ‘¨:ğ‘˜,ğ’ƒ:ğ‘˜).
Near-uniform sampling. ApproxSample (Algorithm 8 in the
extended version of this paper [ 14]) relies on a similar idea as
ApproxCount ; the solution space is partitioned into cells and sam-
ples are drawn from random cells. The algorithm accepts a tolerance
parameterğœ–to determine a range of the desired cell sizes (to guar-
antee closeness to uniformity). To find the appropriate value of ğ‘˜,
it first obtains an estimate Ë†ğ‘of|R(U)|using ApproxCount . Then,
different values of ğ‘˜(determined by Ë†ğ‘) are attempted until the re-
sulting cell size falls within the desired range. Finally, a sample is
drawn uniformly at random from that cell.
Statistical guarantee. Meel [31] proves that, provided that the
oracle ParityConsEnum exists, the counting and sampling algo-
rithms (Algorithm 2 and Algorithm 8 in the extended version of this
paper [ 14]) indeed address the approximate sampling and counting
problems (Problems 2 and 3), respectively.
4The size of a cell is the number of feasible solutions in it.5.2 Parity constrained enumeration
The effectiveness of the above approach heavily depends on the im-
plementation of the oracle ParityConsEnum . In the work of Meel
[31], SAT-based solvers are used since the work deals with the
general problem of constrained programming. In our setting, we
rely on BBenum and linear algebra to design a novel algorithm
tailored for our problems for better scalability. Formally, the oracle
ParityConsEnum addresses the following problem.
Problem 4 (Partial enumeration under parity constraints).
Given a set of candidate rules U, an objective function ğ‘“, an upper
boundğœƒ, a parity constraint system characterized by ğ‘¨ğ’™=ğ’ƒ, and an
integerğµ, find a collection of rule sets Ssuch that|S|â‰¤ğµ,ğ‘“(ğ‘†)â‰¤ğœƒ,
andğ‘¨1ğ‘†=ğ’ƒ, for allğ‘†âˆˆS.
Compared to Problem 1, the above problem asks to enumerate
at mostğµsolutions and further imposes parity constraints on the
solution. Note that Problem 4 is at least as hard as Problem 1, since
the latter is a special case.
Without loss of generality, we assume the matrix ğ‘¨is in its
reduced row echelon form ğ‘¨âˆ’, resulting in the system ğ‘¨âˆ’ğ’™=ğ’ƒâˆ’.5
The reason is that for any ğ‘†, it is ğ‘¨âˆ’ğ’™=ğ’ƒâˆ’if and only if ğ‘¨ğ’™=ğ’ƒ, so
that replacing the constraint ğ‘¨ğ’™=ğ’ƒin Problem 4 with ğ‘¨âˆ’ğ’™=ğ’ƒâˆ’
results in an equivalent problem. Further, important properties
revealed by ğ‘¨âˆ’, such as the rank and pivot positions, turn out to
be essential for the subsequent algorithmic developments. Finally,
we assume there is at least one feasible solution to ğ‘¨ğ’™=ğ’ƒ.
LetğœŒbe the rank of ğ‘¨and let pivotğ‘¨:[ğœŒ]â†’[ğœŒ]denote the
pivot table ofğ‘¨, where pivotğ‘¨[ğ‘–]is the column index of the pivot po-
sition in the ğ‘–-th row. We definePğ‘¨=
pivotğ‘¨[ğ‘–]|ğ‘–âˆˆ[ğœŒ]	
, i.e., the
indices of columns corresponding to pivot variables in ğ‘¨. Similarly,
we defineFğ‘¨={0,...,ğ‘€âˆ’1}\Pğ‘¨, i.e., the indices of columns
corresponding to free columns. When context is clear, for brevity,
we drop the subscript ğ‘¨and use pivot[ğ‘–],PandF.
We relate the rules to the pivot positions. We call the ğ‘—-th rule
apivot rule if theğ‘—-th column in ğ‘¨corresponds to some pivot
position, i.e., exists ğ‘–âˆˆ[ğœŒğ‘¨]such that pivot[ğ‘–]=ğ‘—. Otherwise, the
rule is called a free rule. For rule set ğ‘†, we denoteP(ğ‘†)=Pâˆ©ğ‘†the
set of pivot rules in ğ‘†andF(ğ‘†)=Fâˆ©ğ‘†the set of free rules in ğ‘†.
5.3 A branch-and-bound algorithm
The proposed algorithm builds upon a technique for enumerating
solutions to a linear system ğ‘¨ğ’™=ğ’ƒin finite field of 2. During
the enumeration process, solutions are pruned using the bounds
(Section 4) to satisfy ğ‘“(ğ‘†)â‰¤ğœƒ.
Enumerating feasible solutions to ğ‘¨ğ’™=ğ’ƒ.We first consider
the problem of enumerating all feasible solutions to ğ‘¨ğ’™=ğ’ƒalone.
A straightforward way is by considering the reduced row eche-
lon form of ğ‘¨, identifying the pivot variables and free ones, and
considering all possible assignments of the free variables .
We give a toy example of 3 constraints and 5 variables: the re-
duced row echelon form is shown on the left, while the formula for
the feasible solutions on the right. The pivot columns (correspond-
ing toğ‘¥1,ğ‘¥2, andğ‘¥4) are highlighted in bold. The set of feasible
solutions can be enumerated by substituting [ğ‘¥3,ğ‘¥5]âˆˆ{0,1}2in
the equation below.
5ğ’ƒâˆ’is obtained via the same operations done on ğ‘¨.
 
482KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
ï£®ï£¯ï£¯ï£¯ï£¯ï£°1 0 000
0 1 100
0 0 011ï£¹ï£ºï£ºï£ºï£ºï£»ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ğ‘¥1
ğ‘¥2
ğ‘¥3
ğ‘¥4
ğ‘¥5ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»=ï£®ï£¯ï£¯ï£¯ï£¯ï£°1
0
1ï£¹ï£ºï£ºï£ºï£ºï£»â†’ğ‘¥=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1
0
0
1
0ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»+ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0
1
1
0
0ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ğ‘¥3+ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0
0
0
1
1ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ğ‘¥5
Main idea of the proposed algorithm. Algorithm 2 integrates
the above ideas into the search process in Algorithm 1. The main
changes are:
(1)In the for loop of Algorithm 1, we only check the free rules.
In the example above, only ğ‘¥3andğ‘¥5are checked.
(2)While adding a rule ğ‘—to a given rule set, the procedure
EnsMinNonViolation checks if the satisfiability of some
parity constraints can already be determined. If this is the
case, the associated pivot rules are added.
(3)When checking the look-ahead bound, the pivot rules added
byEnsMinNonViolation are considered.
(4)Before yielding a solution, EnsSatisfaction adds relevant
pivot rules to guarantee ğ‘¨ğ’™=ğ’ƒis satisfied.
(5)The algorithm terminates when either ğµsolutions or all
feasible solutions (at most ğµ) are found.
(6)Finally, a priority queue is used to guide the search process,
where the priority of a rule set ğ‘†equalsâˆ’ğ‘(ğ‘†).
Ensuring minimal non-violation. To describe the procedure
EnsMinNonViolation , we need some additional definitions. Given
a matrix ğ‘¨, itsboundary table, denoted by ğµğ‘¨:[ğœŒğ‘¨]â†’[ğ‘€], maps
a row index to the largest non-zero non-pivot column index of that
row in ğ‘¨. That is,ğµğ‘¨[ğ‘–]=max
ğ‘—|ğ‘¨ğ‘–,ğ‘—=1andğ‘—â‰ pivotğ‘¨[ğ‘–]	ifÃ
ğ‘—ğ‘¨ğ‘–,ğ‘—>1, otherwise ğµğ‘¨[ğ‘–]=âˆ’1, for every ğ‘–âˆˆ[ğœŒğ‘¨]. In our
example,ğµğ‘¨=[âˆ’1,2,4].
We use the boundary table to check if the satisfiability of con-
straints in ğ‘¨ğ’™=ğ’ƒcan be determined by a given ğ‘†. Given a con-
straint ğ‘¨:ğ‘–ğ’™=ğ’ƒ:ğ‘–, we say its satisfiability is determined byğ‘†if
ğ‘†maxâ‰¥ğµğ‘¨[ğ‘–]. In other words, adding any rule after ğ‘†maxdoes not
affect its satisfiability. In our example, the satisfiability of ğ‘¥2+ğ‘¥3=1
is determined by{1,4}and{4}but not by{1}.
Given a rule set ğ‘†, we say that ğ‘†isnon-violating if the constraints
inğ‘¨ğ’™=ğ’ƒthat are determined by ğ‘†are all satisfied. For instance,
{1,4}and{1}are non-violating, while {4}is not. Further, we say ğ‘†
isminimally non-violating ifğ‘†is non-violating and removing any
ruleP(ğ‘†)fromğ‘†violates at least one constraint. For such ğ‘†, we
call each rule inP(ğ‘†)anecessary pivot forF(ğ‘†). In our example,
ğ‘†={1,2,3}is minimally non-violating.
We rely on minimal non-violation to determine the addition of
a minimal set of pivot rules to ensure non-violation. Minimality
ensures no redundant rules are added, thus the algorithm does not
incorrectly prune feasible rule sets.
The procedure EnsMinNonViolation (Algorithm 4 in the ex-
tended version of this paper [ 14]) returns the set of pivot rules to
ensure minimal non-violation of a given ğ‘†. For each constraint, the
process checks if it is determined and unsatisfied, and if yes, adds
the associated pivot rule. Formally: let ğ’“=ğ’ƒâˆ’ğ‘¨Â·1ğ‘†. For each
ğ‘–âˆˆ[ğœŒ(ğ‘¨)], ifğ’“ğ‘–=1andğ‘†maxâ‰¥ğµğ‘¨[ğ‘–], then add the pivotğ‘¨[ğ‘–]-th
pivot rule.
Theorem 6. Given a parity constraint system ğ‘¨ğ’™=ğ’ƒ, for any rule
setğ‘†with free rules only, it follows that EnsMinNonViolation (ğ‘†,ğ‘¨,ğ’ƒ)Algorithm 2 A branch-and-bound algorithm to solve Problem 4.
1:ğ‘›â†0
2:ğ‘†ğ‘ â†EnsSatisfaction(âˆ…,ğ‘¨,ğ’ƒ)
3:ifğ‘…(ğ‘†ğ‘ )â‰¤ğœƒthen
4: Incrementğ‘›and yieldğ‘†ğ‘ 
5:ğ‘†ğ‘â†EnsMinNonViolation (âˆ…,ğ‘¨,ğ’ƒ)
6:ğ‘„â†PriorityQueue  ğ‘†ğ‘,ğ‘ ğ‘†ğ‘
7:whileğ‘„is not empty and ğ‘›<ğµdo
8:ğ‘†â†ğ‘„.ğ‘ğ‘œğ‘()
9: forğ‘—=(F(ğ‘†)ğ‘šğ‘ğ‘¥+1),...,ğ‘€ andğ‘—is free do
10:ğ‘†â€²â†ğ‘†âˆª{ğ‘—}
11: ifğ‘(ğ‘†â€²)â‰¤ğœƒthen
12:ğ¸ğ‘â†EnsMinNonViolation (ğ‘†â€²,ğ‘¨,ğ’ƒ)
13: ifğ‘(ğ‘†â€²âˆªğ¸ğ‘)+ğœ†â‰¤ğœƒthen
14: ğ‘„.push(ğ‘†â€²,ğ‘(ğ‘†â€²âˆªğ¸ğ‘))
15:ğ¸ğ‘ â†EnsSatisfaction(ğ‘†â€²,ğ‘¨,ğ’ƒ)
16:ğ‘†ğ‘ â†ğ‘†â€²âˆªğ¸ğ‘ 
17: ifğ‘…(ğ‘†ğ‘ )â‰¤ğœƒthen
18: Incrementğ‘›and yieldğ‘†ğ‘ 
returns a set of pivot rules ğ¸such thatğ‘†âˆªğ¸is minimally non-violating
with respect to ğ‘¨ğ’™=ğ’ƒ.
Ensuring satisfiability. Satisfiability to ğ‘¨ğ’™=ğ’ƒis guaranteed
byEnsSatisfaction (Algorithm 5 in the extended version of this
paper [ 14]), which works as follows: let ğ’“=ğ’ƒâˆ’ğ‘¨Â·1ğ‘†, for each
ğ‘–âˆˆ[ğœŒ(ğ‘¨)], add the pivotğ‘¨[ğ‘–]-th pivot rule if ğ’“ğ‘–=1.
Proposition 1. Given a parity constraint system ğ‘¨ğ’™=ğ’ƒ, for any
rule setğ‘†with free rules only, it follows that EnsSatisfaction(ğ‘†,ğ‘¨,ğ’ƒ)
returns a set of pivot rules ğ¸such that ğ‘¨1ğ‘†âˆªğ¸=ğ’ƒ.
Extended look-ahead bound. Finally, we extend the look-ahead
bound (Theorem 2) to account for the addition of necessary pivots.
Theorem 7 (Extended look-ahead bound). Given a parity
constraint system ğ‘¨ğ’™=ğ’ƒ, letğ‘†be a rule set with free rules only and
letğ¸be the set of necessary pivots associated with ğ‘†with respect to
ğ‘¨ğ’™=ğ’ƒ. Ifğ‘(ğ‘†âˆªğ¸)+ğœ†>ğœƒ, then for any ğ‘†â€²that starts with ğ‘†and
ğ‘†â€²â‰ ğ‘†, it follows that ğ‘“(ğ‘†â€²âˆªğ¸â€²)>ğœƒ, whereğ¸â€²is the set of necessary
pivots forğ‘†â€²with respect to ğ‘¨ğ’™=ğ’ƒ.
5.4 Incremental computation
We achieve further speed up by incrementally adding the pivots
to ensure minimal non-violation and satisfaction. For instance, we
address the following question: given a minimally non-violating
rule setğ‘†, if ruleğ‘—is added toğ‘†, which pivot rules should be added
to maintain minimal non-violation of the new rule set?
Two arrays are used to represent the parity and satisfiability
states of a rule set ğ‘†. The parity states array ğ’›âˆˆ{0,1}ğ‘˜stores the
difference between ğ‘¨ğ‘–1ğ‘†andğ’ƒğ‘–, for eachğ‘–. The satisfiability array
ğ’”âˆˆ{0,1}ğ‘˜stores whether the satisfiability of each constraint is
guaranteed (meaning determined and satisfied) by ğ‘†. Computations
are saved by ( ğ‘–) skipping the check of constraints whose satisfiability
is already guaranteed and ( ğ‘–ğ‘–) determining the addition of pivots
based only on the value of ğ’›andğ’ƒ. Further, both ğ’›andğ’”are updated
incrementally. Details are provided in Appendixes C.5 and C.6 of
the extended version of this paper [14].
 
483Efficient Exploration of the Rashomon Set of Rule-Set Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
5.5 Implementation details
We also propose a few implementation-level enhancements (details
in Appendix C.8 of the extended version of this paper [ 14]) to speed
up even more the above algorithms.
â€¢The columns of ğ‘¨and the rules are permutated to increase the
chances that IncEnsNoViolation returns a non-empty pivot
sets, leading to more pruning of the search space.
â€¢ApproxCountCore executions are parallelized in ApproxCount .
â€¢We use a fast routine to compute the number of pivot rules
required for satisfiability, before calling the more expensive
IncEnsSatisfaction . This number is used to check the rule set
size bound.
6 SEARCH-TREE-BASED APPROXIMATION
ALGORITHMS
In this section we introduce BBsts , a fast alternative to
ApproxCount , which draws approximately uniform samples and
approximates the size of the Rashomon set. BBsts leverages ideas
from the SearchTreeSampler method by Ermon et al . [17] for
approximately uniform sampling of solutions (i.e., satisfying assign-
ments) of a set of hard constraints in a combinatorial space.
BBsts assumes that rule sets are organized in a search tree. The
root of the search tree is the empty rule set. All rule sets that are ğ‘-
hops away from the tree root contain exactly ğ‘rules. BBsts explores
the search tree in a breadth-first fashion. While exploring the tree,
BBsts generates partial rule sets , which are progressively extended
(by adding additional rules) to form the final solutions. Partial rule
sets of levelâ„are associated with the first â„rules inU.
BBsts does not traverse the search tree exhaustively. Given an
input parameter â„“, the search tree is partitioned into ğ¿=âŒˆğ‘€
â„“âŒ‰
depth levels. The parameter â„“controls the approximation level, the
smallerâ„“, the larger runtime and expected solution quality. At depth
ğ‘–of the search tree, BBsts generates partial rule sets of level ğ‘–â„“.
The steps of BBsts are summarized in Algorithm 3 and visu-
ally in Figure 2 of the extended version of this paper [ 14].BBsts
starts from the tree root which corresponds to the empty rule set
being the partial solution ğ‘ƒ0at depth and level 0. Then, at the ğ‘–-th
iteration, partial rule sets ğ‘ƒğ‘–âˆ’1at depthğ‘–âˆ’1(of level(ğ‘–âˆ’1)â„“) are
uniformly sub-sampled without replacement, and for each sampled
partial solution ğ‘†,BBsts finds all the partial rule sets ğ‘†â€²at depthğ‘–
(of levelğ‘–â„“) that start with ğ‘†. The set of all such partial rule sets
at depthğ‘–that start with ğ‘†is denoted by{ğ‘†â€²}ğ‘†
ğ‘–. To find all the par-
tial solutions{ğ‘†â€²}ğ‘†
ğ‘–,BBsts starts fromğ‘†and invokes a variant of
BBenum(ğ‘†,â„“)which considers â„“additional rules. BBenum(ğ‘†,â„“)
is identical to BBenum , as described in Algorithm 1, except that
it starts by enqueueing set ğ‘†instead of the empty set âˆ…, and the
main loop only iterates from (ğ‘–âˆ’1)â„“toğ‘–â„“, instead of from ğ‘†ğ‘šğ‘ğ‘¥+1
toğ‘€. The process of drawing a uniform sample ğ‘†fromğ‘ƒğ‘–âˆ’1and
finding the associated set {ğ‘†â€²}ğ‘†
ğ‘–is repeated min(ğœ…,|ğ‘ƒğ‘–âˆ’1|)times,
for a user-specified parameter ğœ…, which trades quality for efficiency.
The largerğœ…is, the longer runtime but higher solution quality.
Eventually, BBsts yields approximate uniform samples from the
Rashomon set by generating partial rule sets at depth ğ¿(of levelğ‘€),
and filtering out the rule sets that do not belong to the Rashomon set.
In particular, Ermon et al . [17] show that, for any partial solutionsAlgorithm 3 BBsts algorithm for Problem 2.
1:ğ‘ƒ0â†âˆ… .
2:ğ¿â†âŒˆğ‘€
â„“âŒ‰.
3:forğ‘–in1,...ğ¿ do
4:ğ‘ƒğ‘–â†âˆ…
5: forğ‘—in1,...min(ğœ…,|ğ‘ƒğ‘–âˆ’1|)do
6: drawğ‘†âˆ¼U(ğ‘ƒğ‘–âˆ’1)without replacement
7:{ğ‘†â€²}ğ‘†
ğ‘–â†BBenum(ğ‘†,â„“)
8:ğ‘ƒğ‘–â†ğ‘ƒğ‘–âˆª{ğ‘†â€²}ğ‘†
ğ‘–
9:return{ğ‘ƒğ‘–}âˆ©R(U)
ğ‘†andğ‘†â€², it holds:
ğœ…
2â„“+ğœ…âˆ’1â‰¤Pr(ğ‘†)
Pr(ğ‘†â€²)â‰¤2â„“+ğœ…âˆ’1
ğœ…, (9)
where Pr(ğ‘†)denotes the probability of sampling ğ‘†. Equation (9)
bounds the uniformity of the samples returned by BBsts , but it only
holds for large ğœ…. For values of ğœ…used in practice, the uniformity
guarantee in Equation (9) may not hold, and a rule set ğ‘†may be
arbitrarily more likely to be sampled than another rule set ğ‘†â€².
The use of a BBenum -like search is the main difference between
BBsts andSearchTreeSampler [17], which, instead, uses expen-
sive calls to SAT solvers. This difference leads to a drastic reduction
in runtime, because, as shown in Section 7.2, BBenum outperforms
a SAT-based solver in runtime by orders of magnitude.
Not only BBsts efficiently draws samples from the Rashomon
set, but, as suggested by Ermon et al . [17] , the partial rule sets
constructed while executing BBsts pave the way for estimation of
|R(U)|via the following formula:
|R(U)|â‰ˆ|ğ‘ƒğ¿|=|ğ‘ƒğ¿|
|ğ‘ƒğ¿âˆ’1||ğ‘ƒğ¿âˆ’1|
|ğ‘ƒğ¿âˆ’2||ğ‘ƒğ¿âˆ’2|
|ğ‘ƒğ¿âˆ’3|...|ğ‘ƒ1|
1, (10)
where|ğ‘ƒğ‘–|
|ğ‘ƒğ‘–âˆ’1|=1
|ğ‘ƒğ‘–âˆ’1|Ã
ğ‘†âˆˆğ‘ƒğ‘–âˆ’1|{ğ‘†â€²}ğ‘†
ğ‘–|.Note that Equation (10) does
not provide any accuracy guarantee.
7 EXPERIMENTS
In this section, we present an empirical evaluation of our methods.
The main goal of the evaluation is to demonstrate the effective-
ness and scalability of the proposed methods for exploring the
Rashoomon set of rule sets.
As our methods come with guarantees of near uniformity for
sampling, we focus on demonstrating the accuracy of our methods
in estimating the size of the Rashomon set (counting). Accurate
counts obtained by ApproxCount andBBsts are also good indica-
tions of uniform output samples.
We describe the experimental setup in Section 7.1, present a per-
formance comparison in Section 7.2, and describe two case studies
in Section 7.3 and 7.4, respectively. We also provide more experi-
ment results in the extended version of this paper [14].
7.1 Experimental setting
We describe our datasets, performance metrics, parameter configu-
rations, and the choices of baselines.
Data. We consider four real-world datasets (whose summary sta-
tistics are presented in Table 1) from various domains where inter-
pretability is of primary importance.
 
484KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
Table 1: Summary statistics for the datasets used in the exper-
iments. We report the number of data records ğ‘, the number
of attributes ğ½, the density in the feature space and the im-
balance ratio(Ãğ‘
ğ‘›=11[ğ‘¦ğ‘›=0])/(Ãğ‘
ğ‘›=11[ğ‘¦ğ‘›=1]).
Name ğ‘ ğ½ Feature density Imbalance ratio
Compas 6 489 15 0.256 1.232
Mushrooms 8 124 117 0.188 1.074
Voting 435 48 0.333 1.589
Credit 690 566 0.019 1.248
â€¢Compas dataset for two-year recidivism prediction [27].
â€¢Mushrooms dataset for classification of mushrooms into the
categories poisonous and edible [3].
â€¢Credit dataset for credit scoring [1].
â€¢Voting dataset for classification of american voters as republi-
cans or democrats [2].
Baselines. We compare BBenum ,ApproxCount and BBsts
against three baselines, NaÃ¯ve-BB-Enum, a naive variant of
BBenum , which does not use pruning and thereby mirrors the
theoretical worst-case behaviour of BBenum ,CP-sat, a constraint
programming solver, and IS, an importance sampler. Details of the
baselines are given in Appendix A.
Metrics. Since the main goal in our experimental evaluation is
to show that our methods efficiently and effectively explore the
Rashomon set of near-optimal rule sets, we report runtime (in
seconds) and the estimated Rashomon set size |R(U)|.
Parameters. For fixed value of ğœ†, the choice of upper bound ğœƒ
affects the most the computational requirements of the proposed
algorithms. Hence, we focus on showing runtime and accuracy
of counts as a function of ğœƒ. Unless specified otherwise, we set
ğœ†=0.1. This choice of ğœ†shifts the Rashomon set towards concise
rule sets prioritizing interpretability over performance. If instead
performance is of primary importance, a smaller value of ğœ†(e.g.,
ğœ†=0.01) is preferable. We vary the value of ğœƒin arithmetic pro-
gression. For instance, ğœƒâˆˆ[0.5,0.7,0.9,1.1]in the Compas dataset.
We construct the universe of rules Uby considering the 50rules
capturing the most data records. In addition, in the extended ver-
sion of this paper [ 14], we investigate the performance of BBenum ,
ApproxCount andBBsts as a function of|U|(see Figure 4).
When comparing with the baselines, NaÃ¯ve-BB-Enum, CP-sat
andIS, which do not scale well, we use only the 30rules captur-
ing the most data records and set ğœƒ=0.3,0.5,0.8,and0.8for the
Compas, Mushrooms, Voting andCredit datasets, respectively.
ForApproxCount , we fixğœ–=0.2andğ›¿=0.9since varying ğœ–
andğ›¿does not significantly affect the accuracy of ApproxCount .
On the other hand, for BBsts , the parameters â„“andğœ…affect accuracy
greatly. We consider â„“âˆˆ{2,4,8}andğœ…âˆˆ{50,225,506,1138,5760}
and we average results over different values of â„“andğœ….
Computing environment and source code. Experiments are
executed on a machine with 2Ã—10core Xeon E5 processor and
256 GB memory. The source code is available at https://github.com/
xiaohan2012/efficient-rashomon-rule-set.
7.2 Performance comparison
Comparison among the proposed algorithms. Figure 2 demon-
strates how ğœƒaffects runtime (top row) and accuracy in estimating|R(U)|(bottom row), on all datasets. Note that BBenum always
returns the correct value for |R(U)|.BBsts is the fastest algorithm,
although it can be rather inaccurate in estimating |R(U)|. Instead,
ApproxCount strikes the best balance between scalability and ac-
curacy. For large values of ğœƒboth BBsts andApproxCount are
drastically more scalable than BBenum , while for small values of ğœƒ,
BBenum is typically the preferred algorithm.
Comparison against the baselines. The runtime and estimated
|R(U)|for the proposed algorithms and the baselines are provided
in Table 2. CP-sat andNaÃ¯ve-BB-Enum yield exact counts, but they
take remarkably longer time than the proposed methods, mean-
while ISdelivers estimates that are too inaccurate.
7.3 Case study: feature importance in Compas
We illustrate the application of the approximate sampling algorithm
for the task of feature importance analysis. For a feature ğ‘—in a
rule setğ‘†, we use model reliance [18] to measure the importance
of the feature for ğ‘†. To show the variation of feature importance
across the rule sets in the Rashomon set, we compute mcrâˆ’(ğ‘—)and
mcr+(ğ‘—), the minimum and maximum model reliance values for
each feature ğ‘—. More details are provided in Appendix B.
We obtain the ground truth based on allmodels inR(U). We also
estimate mcrâˆ’andmcr+using samples of 400rule sets extracted
fromR(U). In our experiment, we use the Compas dataset and
consider a Rashomon set of 2 003 rule sets. The sampling process
is repeated 48times and the mean is reported. Sample estimates
ofmcrâˆ’andmcr+as well as the corresponding ground-truth are
shown in Figure 3. Sample estimates are close to the ground-truth,
suggesting that exhaustive enumeration of the Rashomon set may
not be needed to investigate feature importance. In Section 7.4 we
reach similar conclusions regarding a use case on fairness.
7.4 Case study: fairness in Compas
Fairness has emerged as a central topic in machine learning since
the influential work of [ 16]. Exploring the Rashomon set allows
to address fairness considerations, which may arise in tackling
classification tasks. The goal of this case study is two-fold. Focusing
on the Compas dataset, we first show that samples drawn from the
Rashomon set by the proposed algorithms yield reliable estimates
of popular fairness metrics. Second, we show that the samples can
be used to find a model satisfying specific fairness constraints.
Investigating fairness measures by sampling. Figure 4 shows
that samples of increasing size provide an increasingly accurate rep-
resentation of the distribution of fairness measures in the Rashomon
set. While there can be some variability in the estimates of the mini-
mum, the maximum, median, first and third quartiles are accurately
estimated even in the smallest sample. All details on the fairness
measures and experiment settings are given in Appendix C.
Finding accurate-yet-fair models by sampling. In Appendix C
we also demonstrate that the algorithms we propose can be used
to find an accurate model satisfying certain fairness constraints.
8 CONCLUSIONS
We study the problems of sampling from the Rashomon set of ac-
curate rule set models and computing the size of the Rashomon
 
485Efficient Exploration of the Rashomon Set of Rule-Set Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
bbEnum ApproxCount bbSTS
0.5 0.7 0.9 1.1
 Upper Bound 05001000150020002500 Runtime (s) 
0.6 0.9 1.2 1.5
 Upper Bound 050100150200250300350400 Runtime (s) 
1.0 1.2 1.4 1.6
 Upper Bound 0500100015002000 Runtime (s) 
1.0 1.2 1.4 1.6
 Upper Bound 02000400060008000100001200014000 Runtime (s) 
0.5 0.7 0.9 1.1
 Upper Bound 104105106107108 Counts 
0.6 0.9 1.2 1.5
 Upper Bound 102103104105106107108109 Counts 
1.0 1.2 1.4 1.6
 Upper Bound 105106107108109 Counts 
1.0 1.2 1.4 1.6
 Upper Bound 1051061071081091010 Counts 
Compas Mushrooms Voting Credit
Figure 2: Runtime (in seconds, top row) and estimated |R(U)|(in log scale, bottom row) against objective upper bound ğœƒ.
Table 2: Performance on small problem instances. We report NA if runtime exceeds 12 hours. Aâˆ—indicates exact counts.
Compas Mushrooms Voting Credit
Runtime (s) Count Runtime (s) Count Runtime (s) Count Runtime (s) Count
ApproxCount 0.007 21 0.026 15 0.027 364 1.558 2 810
BBenum 0.010âˆ—21 0.006âˆ—15 0.022âˆ—364 0.068âˆ—2 807
BBsts 0.039 21 0.006 16 0.044 289 0.765 2 465
NaÃ¯ve-BB-Enum NA NA 30 381.500âˆ—15 29 981.500âˆ—364 31 161.300âˆ—2 807
CP-sat 10.277âˆ—21 26.478âˆ—15 2.072âˆ—364 18.789âˆ—2 807
IS 11.128 0 13.348 2 17.445 701 13.919 3 641
MCR+(True) MCR+(Sample) MCR (True) MCR (Sample)
0.8 0.9 1.0 1.1
ValuePriors (>3)
Juvenile crimes
Juvenile misdemeanors
Age (21-22)
Juvenile felonies
Sex
Age (18-20)
Age (26-45)
Age (23-25)
Priors (2-3)
No priors
Misdemeanor
Age (>45)
Priors (1) 
Figure 3: Estimated feature importance against the ground-
truth in the Compas dataset. 95% confidence intervals are
shown as black lines.
set. Unlike in related work, we consider both exhaustive and non-
exhaustive enumeration. For the former, we propose an efficient
branch-and-bound algorithm, optimized with pruning and incre-
mental computation. For the latter, we devise two algorithms: one
based on the random partitioning of the solution space and another
based on subsampling partial solutions during the branch-and-
bound exploration of the search tree of rule sets.
Our work opens interesting questions for future research. For
example, (ğ‘–) can we make ApproxCount even faster by exploiting
the parity constraint further? ( ğ‘–ğ‘–) Can we improve the accuracy
ofBBsts without sacrificing efficiency? ( ğ‘–ğ‘–ğ‘–) Can we design algo-
rithms for non-exhaustive exploration of the (possibly continuous)
10% 20% 40% 100%
104
103
102
101
100101102
Measure ValueEqual OpportunityPredictive EqualityPredictive ParityStatistical ParityFigure 4: Distribution of four fairness measures in the entire
Rashomon set (100%) as well as in samples of increasing size
(10%, 20%and 40%). Theğ‘¥-axis is on log scale.
Rashomon set for other classes of interpretable models? And finally
(ğ‘–ğ‘£) can we showcase algorithms for non-exhaustive exploration of
the Rashomon set in unexplored application scenarios?
9 ACKNOWLEDGEMENTS
This research is supported by the ERC Advanced Grant REBOUND
(834862), the EC H2020 RIA project SoBigData++ (871042), and
the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation.
 
486KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
REFERENCES
[1][n. d.]. Credit Score Classification task. http://kaggle.com/datasets/parisrohan/
credit-score-classification. Accessed: 2023-10-01.
[2]1987. Congressional Voting Records. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C5C01P.
[3]1987. Mushroom. UCI Machine Learning Repository. DOI:
https://doi.org/10.24432/C5959T.
[4]Ulrich AÃ¯vodji, Julien Ferry, SÃ©bastien Gambs, Marie-JosÃ© Huguet, and Mohamed
Siala. 2021. Faircorels, an open-source library for learning fair rule lists. In
Proceedings of the 30th ACM International Conference on Information & Knowledge
Management. 4665â€“4669.
[5]Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia
Rudin. 2017. Learning certifiably optimal rule lists. In Proceedings of the 23rd
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
35â€“44.
[6]Vaishak Belle and Ioannis Papantonis. 2021. Principles and practice of explainable
machine learning. Frontiers in big Data (2021), 39.
[7]Leo Breiman. 2001. Statistical modeling: The two cultures (with comments and a
rejoinder by the author). Statistical science 16, 3 (2001), 199â€“231.
[8]Nadia Burkart and Marco F Huber. 2021. A survey on the explainability of
supervised machine learning. Journal of Artificial Intelligence Research 70 (2021),
245â€“317.
[9]Toon Calders, Faisal Kamiran, and Mykola Pechenizkiy. 2009. Building classifiers
with independency constraints. In 2009 IEEE international conference on data
mining workshops. IEEE, 13â€“18.
[10] FrÃ©dÃ©ric CÃ©rou, Pierre Del Moral, Teddy Furon, and Arnaud Guyader. 2012. Se-
quential Monte Carlo for rare event estimation. Statistics and computing 22, 3
(2012), 795â€“808.
[11] Supratik Chakraborty, Kuldeep S Meel, and Moshe Y Vardi. 2013. A scalable
and nearly uniform generator of SAT witnesses. In Computer Aided Verification:
25th International Conference, CAV 2013, Saint Petersburg, Russia, July 13-19, 2013.
Proceedings 25. Springer, 608â€“623.
[12] Supratik Chakraborty, Kuldeep S Meel, and Moshe Y Vardi. 2014. Balancing
scalability and uniformity in SAT witness generator. In Proceedings of the 51st
Annual Design Automation Conference. 1â€“6.
[13] M. Ciaperoni, H. Xiao, and A. Gionis. 2022. Concise and interpretable multi-label
rule sets. In 2022 IEEE International Conference on Data Mining (ICDM). IEEE Com-
puter Society, Los Alamitos, CA, USA, 71â€“80. https://doi.ieeecomputersociety.
org/10.1109/ICDM54844.2022.00017
[14] Martino Ciaperoni, Han Xiao, and Aristides Gionis. 2024. Efficient Exploration
of the Rashomon Set of Rule Set Models. arXiv:2406.03059 [cs.LG]
[15] Amanda Coston, Ashesh Rambachan, and Alexandra Chouldechova. 2021. Char-
acterizing fairness over the set of good models under selective labels. In Interna-
tional Conference on Machine Learning. PMLR, 2144â€“2155.
[16] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. 214â€“226.
[17] Stefano Ermon, Carla Pedro Gomes, and Bart Selman. 2012. Uniform Solution
Sampling Using a Constraint Solver As an Oracle. In Conference on Uncertainty
in Artificial Intelligence. https://api.semanticscholar.org/CorpusID:16218653
[18] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. 2019. All Models are
Wrong, but Many are Useful: Learning a Variableâ€™s Importance by Studying an
Entire Class of Prediction Models Simultaneously. J. Mach. Learn. Res. 20, 177
(2019), 1â€“81.
[19] Aristides Gionis, Theodoros Lappas, and Evimaria Terzi. 2012. Estimating entity
importance via counting set covers. In Proceedings of the 18th ACM SIGKDD
international conference on Knowledge discovery and data mining. 687â€“695.
[20] Jiawei Han, Jian Pei, and Yiwen Yin. 2000. Mining frequent patterns without
candidate generation. ACM sigmod record 29, 2 (2000), 1â€“12.
[21] Satoshi Hara and Masakazu Ishihata. 2018. Approximate and exact enumeration
of rule models. In Proceedings of the AAAI Conference on Artificial Intelligence,
Vol. 32.
[22] Xiyang Hu, Cynthia Rudin, and Margo Seltzer. 2019. Optimal sparse decision
trees. Advances in Neural Information Processing Systems 32 (2019).
[23] Katarzyna KobyliÅ„ska, Mateusz KrzyziÅ„ski, RafaÅ‚ Machowicz, Mariusz Adamek,
and PrzemysÅ‚aw Biecek. 2023. Exploration of Rashomon Set Assists Explanations
for Medical Data. arXiv preprint arXiv:2308.11446 (2023).
[24] Trupti A Kumbhare and Santosh V Chobe. 2014. An overview of association rule
mining algorithms. International Journal of Computer Science and Information
Technologies 5, 1 (2014), 927â€“930.
[25] Himabindu Lakkaraju, Stephen H Bach, and Jure Leskovec. 2016. Interpretable
decision sets: A joint framework for description and prediction. In Proceedings of
the 22nd ACM SIGKDD international conference on knowledge discovery and data
mining. 1675â€“1684.
[26] Himabindu Lakkaraju and Osbert Bastani. 2020. " How do I fool you?" Manipu-
lating User Trust via Misleading Black Box Explanations. In Proceedings of the
AAAI/ACM Conference on AI, Ethics, and Society. 79â€“85.[27] Jeff Larson, Surya Mattu, Lauren Kirchner, and Julia Angwin. 2016. How we
analyzed the COMPAS recidivism algorithm. ProPublica (5 2016) 9, 1 (2016), 3â€“3.
[28] Dmitry Malioutov and Kuldeep S Meel. 2018. MLIC: A MaxSAT-based framework
for learning interpretable classification rules. In International Conference on
Principles and Practice of Constraint Programming. Springer, 312â€“327.
[29] Charles Marx, Flavio Calmon, and Berk Ustun. 2020. Predictive multiplicity in
classification. In International Conference on Machine Learning. PMLR, 6765â€“6774.
[30] Kota Mata, Kentaro Kanamori, and Hiroki Arimura. 2022. Computing the Collec-
tion of Good Models for Rule Lists. arXiv preprint arXiv:2204.11285 (2022).
[31] Kuldeep Singh Meel. 2017. Constrained counting and sampling: bridging the gap
between theory and practice. Ph. D. Dissertation. Rice University.
[32] Laurent Perron and FrÃ©dÃ©ric Didier. [n. d.]. CP-SAT. Google. https://developers.
google.com/optimization/cp/cp_solver/
[33] Cynthia Rudin. 2019. Stop explaining black box machine learning models for
high stakes decisions and use interpretable models instead. Nature machine
intelligence 1, 5 (2019), 206â€“215.
[34] Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, and
Chudi Zhong. 2022. Interpretable machine learning: Fundamental principles and
10 grand challenges. Statistic Surveys 16 (2022), 1â€“85.
[35] Mirka Saarela and Susanne Jauhiainen. 2021. Comparison of feature importance
measures as explanations for classification models. SN Applied Sciences 3 (2021),
1â€“12.
[36] Lesia Semenova, Cynthia Rudin, and Ronald Parr. 2019. A study in Rashomon
curves and volumes: A new perspective on generalization and model simplicity
in machine learning. arXiv preprint arXiv:1908.01755 (2019).
[37] Lesia Semenova, Cynthia Rudin, and Ronald Parr. 2022. On the existence of
simpler machine learning models. In Proceedings of the 2022 ACM Conference on
Fairness, Accountability, and Transparency. 1827â€“1858.
[38] Surya T Tokdar and Robert E Kass. 2010. Importance sampling: a review. Wiley
Interdisciplinary Reviews: Computational Statistics 2, 1 (2010), 54â€“60.
[39] Leslie G Valiant. 1979. The complexity of enumeration and reliability problems.
siam Journal on Computing 8, 3 (1979), 410â€“421.
[40] Srishti Vashishtha and Seba Susan. 2019. Fuzzy rule based unsupervised sentiment
analysis from social media posts. Expert Systems with Applications 138 (2019),
112834.
[41] Tong Wang and Cynthia Rudin. 2015. Learning optimized Orâ€™s of Andâ€™s. arXiv
preprint arXiv:1511.02210 (2015).
[42] Tong Wang, Cynthia Rudin, Finale Doshi-Velez, Yimin Liu, Erica Klampfl, and
Perry MacNeille. 2017. A bayesian framework for learning rule sets for inter-
pretable classification. The Journal of Machine Learning Research 18, 1 (2017),
2357â€“2393.
[43] Rui Xin, Chudi Zhong, Zhi Chen, Takuya Takagi, Margo Seltzer, and Cynthia
Rudin. 2022. Exploring the whole rashomon set of sparse decision trees. Advances
in Neural Information Processing Systems 35 (2022), 14071â€“14084.
[44] Guangyi Zhang and Aristides Gionis. 2020. Diverse rule sets. In Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining. 1532â€“1541.
[45] Guangyi Zhang and Aristides Gionis. 2023. Regularized impurity reduction:
accurate decision trees with complexity guarantees. Data mining and knowledge
discovery 37, 1 (2023), 434â€“475.
Appendices
A ADDITIONAL BASELINES
In order to offer a more complete assessment of the performance
of the methods we propose, we compare BBenum ,BBsts and
ApproxCount against three simpler alternative approaches. The
details of such simple baselines are given next.
â€¢NaÃ¯ve-BB-Enum: a naÃ¯ve search algorithm that does not
enforce any pruning of the search space. The NaÃ¯ve-BB-
Enum algorithm is analogous to BBenum , but it exhaustively
considers all rule sets and tests them for inclusion into the
Rashomon set.
â€¢CP-sat: a constraint programming solver that uses a highly
optimized SAT (satisfiability) solver. In order to leverage the
solver, we encode the problem as follows.
A data record(ğ’™ğ‘›,ğ‘¦ğ‘›)is said to be positive if ğ‘¦ğ‘›=1and
negative otherwise. The numbers of negative and positive
data records are denoted by |D|âˆ’and|D|+, respectively.
 
487Efficient Exploration of the Rashomon Set of Rule-Set Models KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Givenğ‘€input rules, let Nğ¼be an indicator matrix such that
Nğ¼ğ‘–,ğ‘—=1if theğ‘–-th negative data record is covered by the
ğ‘—-th rule. Similarly, let Pğ¼be an indicator matrix such that
Pğ¼ğ‘–,ğ‘—=1if theğ‘–-th positive data record is covered by the
ğ‘—-th rule. We encode the counting problem as the problem
of finding all xâˆˆ{0,1}ğ‘€, such that:
ğ‘§ğ¹ğ‘ƒ
ğ‘+ğ‘§ğ¹ğ‘
ğ‘+ğœ†ğ‘€âˆ‘ï¸
ğ‘—=1x[ğ‘—]â‰¤ğœƒ, (11)
whereğ‘§ğ¹ğ‘ƒ=Ã|D|âˆ’
ğ‘–âˆˆ1min(Nğ¼ğ‘–x,1)andğ‘§ğ¹ğ‘=Ã|D|+
ğ‘–âˆˆ1max(1âˆ’
Pğ¼ğ‘–x,0).Here, x[ğ‘—]is theğ‘—-th entry of x,Nğ¼ğ‘–xdenotes the
dot product between the ğ‘–-th row of Nğ¼and the vector x.
Similarly, Pğ¼ğ‘–xdenotes the dot product between the ğ‘–-th row
ofPğ¼andx. AsNğ¼ğ‘–,Pğ¼ğ‘–andxare all binary vectors, the dot
product corresponds to a set intersection.
Given the set of constraints described in Equations 11, we
find all rule sets by resorting to a state-of-the-art solver for
constraint programming [32].
â€¢IS: a method based on Monte Carlo simulation, where we
simulate a large number of rule sets and evaluate the pro-
portion of rule sets that belong to the Rashomon set. The
proportion can then be mapped to the corresponding count
by multiplying by the total number of rule-set models, that
can be easily computed. Plain Monte Carlo sampling is ex-
tremely inefficient for very rare events [ 10]. As suggested
by Semenova et al . [36] , in order to estimate the size of the
Rashomon set, it is preferable to use the Monte Carlo method
known as importance sampling [ 38], where the training data
are used to bias the sampling towards the Rashomon set. In
particular, the importance sampler is designed as follows.
â€“Given the set of pre-mined rules U, compute the normal-
ized reciprocal individual contribution of each rule ğ‘Ÿto
the loss, namely Î”â„“(ğ‘Ÿ)=1
â„“ğ‘(ğ‘Ÿ)+â„“0(ğ‘Ÿ)/Ã
ğ‘Ÿâ€²âˆˆUÎ”â„“(ğ‘Ÿâ€²).
â€“Sampleğ‘ğ‘Ÿğ‘’ğ‘(1,000,000by default) integers ğ‘¡uniformly
at random in the interval [1,|U|] and rule sets ğ‘†of sizeğ‘¡
with probability ğ‘(ğ‘†)=Î”â„“â€²(ğ‘Ÿ1)Î”â„“â€²(ğ‘Ÿ2)...Î”â„“â€²(ğ‘Ÿğ‘›).
â€“Compute the importance sampling estimate
1
ğ‘Ãğ‘
ğ‘–=1ğ‘“ğ¼(ğ‘†)ğ‘¢(ğ‘†)
ğ‘(ğ‘†)whereğ‘“ğ¼(ğ‘†)is an indicator func-
tion for the event that ğ‘†belongs to the Rashomon set, and
ğ‘¢(Â·)is the uniform (target) distribution.
In practice, to enhance the performance of IS, instead of
sampling rule sets of length up to |U|, we sample rule sets
of length up to the upper bound obtained by setting ğ‘†=âˆ…
in Theorem 3.
B CASE STUDY ON FEATURE IMPORTANCE
As a simple case study, we show how the proposed methods allow
to efficiently estimate feature importance and, more specifically, we
show that reliable estimates of feature importance can be derived
from samples of rule sets in the Rashomon set. The main results are
summarized in Section 7.3. In this section, we provide more details.
Different measures of feature importance have been pro-
posed [ 35]. Recent work focuses on model reliance [18,43]. Model
reliance captures the extent to which a model relies on a given
feature to achieve its predictive performance. For our purposes,given rule set ğ‘†and feature ğ‘£, we define model reliance as follows:
ğ‘€ğ‘…(ğ‘†,ğ‘£)=ğ‘“(ğ‘†;ğ‘£â€²,ğœ†)
ğ‘“(ğ‘†;ğ‘£,ğœ†), (12)
whereğ‘“(ğ‘†;ğ‘£,ğœ†)is the objective achieved by ğ‘†in the original
dataset, and ğ‘“(ğ‘†;ğ‘£â€²,ğœ†)is identical to ğ‘“(ğ‘†;ğ‘£,ğœ†)except that ğ‘£is re-
placed by its uninformative counterpart ğ‘£â€². Featureğ‘£â€²is obtained
by swapping the first and second halves of the feature values of ğ‘£,
thereby retaining the marginal distribution of ğ‘£, while destroying
its predictive power. This measure is similar to the model reliance
measure used by Xin et al . [43] . Model reliance evaluates how im-
portant a variable is for a given rule set. In particular, the higher
model reliance, the more important feature ğ‘£. If we have a single
rule setğ‘†, we would simply estimate the importance of feature ğ‘£
byğ‘€ğ‘…(ğ‘†,ğ‘£). However, if we have access to the Rashomon set of
all near-optimal rule sets, it is more informative to investigate the
variation of ğ‘€ğ‘…(ğ‘†,ğ‘£)across rule sets ğ‘†in the Rashomon set. Hence,
we compute ğ‘€ğ¶ğ‘…âˆ’(ğ‘£)andğ‘€ğ¶ğ‘…+(ğ‘£), the minimum and maximum
model reliance for feature ğ‘£across rule sets in the Rashomon set.
In Figure 3 (Section 7.3), we compare ğ‘€ğ¶ğ‘…âˆ’(ğ‘£)andğ‘€ğ¶ğ‘…+(ğ‘£)
computed in the entire Rashomon set and in samples of rule sets
drawn from the Rashomon set by ApproxSample , and we conclude
that the sample estimates are consistently close to the measures
computed in the entire Rashomon set, suggesting that exhaustive
enumeration may be redundant when the goal is to investigate
feature importance.
In addition, while ğ‘€ğ¶ğ‘…âˆ’(ğ‘£)andğ‘€ğ¶ğ‘…+(ğ‘£)are adequate mea-
sures of the importance of features in rule sets, they fail to capture
the idea that some features are more frequent than others in the
Rashomon set. Inuitively, at parity model reliance, the more fre-
quent a feature is in the Rashomon set, the more important. Hence,
to provide a more complete assessment of feature importance, Fig-
ure 5 shows the proportion of rule sets including a given variable
in the entire Rashomon set or in a sample of 400rule sets obtained
using ApproxSample . The reported sample estimates are obtained
as averages over 10repetitions of the sampling process. The relative
frequency of the features estimated in the sample and in the entire
Rashomon set are remarkably similar, corroborating the findings
presented in Section 7.3 with respect to model reliance.
Finally, we mention that the results do not correspond exactly to
the similar results presented by Xin et al . [43] because we consider
a different class of models and a different loss. However, there are
interesting commonalities. For instance, the variable ğ‘ƒğ‘Ÿğ‘–ğ‘œğ‘Ÿ >3has
the highest ğ‘€ğ¶ğ‘…+(ğ‘£)in both studies.
C CASE STUDY ON FAIRNESS
The Rashomon set offers a novel perspective on fairness of machine
learning models. Although all models in the Rashomon set achieve
near-optimal predictive performance, they may exhibit different
fairness characteristics. The Rashomon set allows to identify the
range of predictive bias produced by the models and to search for
models that are both accurate and fair.
We carry out a case study focusing on the Compas dataset, which
has fueled intense debate and research in fair machine learning [ 4,
34], and fairness constraints are specified with respect to the sex
attribute, which partitions the dataset into two groups, males (M)
andfemales (F).
 
488KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Martino Ciaperoni, Han Xiao & Aristides Gionis
0.0 0.1 0.2
Relative FrequencyAge (18-20)Priors (1)Age (21-22)No priorsAge (>45)Juvenile feloniesPriors (2-3)Age (23-25)Juvenile misdemeanorsMisdemeanorAge (26-45)Juvenile crimesPriors (>3)Sex
0.0 0.1 0.2
Sample Relative FrequencyAge (18-20)Priors (1)Age (21-22)No priorsAge (>45)Juvenile feloniesPriors (2-3)Age (23-25)Juvenile misdemeanorsAge (26-45)MisdemeanorPriors (>3)Juvenile crimesSex
Figure 5: Relative frequencies of features in the Rashomon set (left) and associated sample estimates (right).
10% 20% 40% 100%
0.0 0.2 0.4 0.6
T est Set LossEqual OpportunityPredictive EqualityPredictive ParityStatistical Parity
Figure 6: Objective ğ‘“on the test set obtained by the optimal
fair rule set found in the train set with respect to different
fairness measures in the entire Rashomon set (100%) as well
as in samples of increasing size (10%, 20%and 40%).
A concise summary of the case study is given in Section 7.4.
Here, we discuss the details.
First, we introduce the considered measures of fairness.
Fairness measures. LetË†ğ‘¦=1denote the event that the data
record ğ’™is predicted as positive (i.e. cap(ğ’™,ğ‘†)). Moreover, let ğ‘¥ğ‘ 
denote the sex of data record ğ’™. We consider the following fairness
measures [4].
â€¢Statistical parity measures the absolute difference of rate of
positive predictions between the groups:
|Pr(Ë†ğ‘¦=1|ğ‘¥ğ‘ =F)âˆ’Pr(Ë†ğ‘¦=1|ğ‘¥ğ‘ =M)|.
â€¢Predictive parity measures the absolute difference of preci-
sion between the groups:
|Pr(ğ‘¦=1|Ë†ğ‘¦=1,ğ‘¥ğ‘ =F)âˆ’Pr(ğ‘¦=1|Ë†ğ‘¦=1,ğ‘¥ğ‘ =M)|.
â€¢Predictive equality measures the absolute difference of false
positive rate between the groups:
|Pr(Ë†ğ‘¦=1|ğ‘¦=0,ğ‘¥ğ‘ =F)âˆ’Pr(Ë†ğ‘¦=1|ğ‘¦=0,ğ‘¥ğ‘ =M)|.
â€¢Equal opportunity measures the absolute difference of true
positive rate between the groups:
|Pr(Ë†ğ‘¦=1|ğ‘¦=1,ğ‘¥ğ‘ =F)âˆ’Pr(Ë†ğ‘¦=1|ğ‘¦=1,ğ‘¥ğ‘ =M)|.
For all four measures, the larger the value, the more unfair the
model is.Investigating fairness measures by sampling. Figure 4 in Sec-
tion 7.4 shows the distribution of the above fairness measures in
the entire Rashomon set and in samples of increasing size. For
each measure, we show the range (minimum and maximum), the
interquartile range (first and third quartiles) and the median. All
such statistics describing the distributions of the fairness measures
of interest in the samples of rule sets are obtained as average over
10repetitions of the random sampling process.
The Rashomon set consists of |R(U)|=1409 rule sets and we
useApproxSample to draw samples of sizes 10%, 20%and40%of
|R(U)|.
Finding accurate-yet-fair models by sampling. To demonstrate
that the proposed sampling strategy can be used to find an accurate
model while satisfying particular fairness constraints, we set up a
simple two-step experiment.
First, given a sample of rule sets from the Rashomon set, we
consider any of the four fairness measures described above, say
ğ‘€ğ‘“ğ‘ğ‘–ğ‘Ÿ, and we exclude all models with value of ğ‘€ğ‘“ğ‘ğ‘–ğ‘Ÿbeyond the
first quartile of the distribution of ğ‘€ğ‘“ğ‘ğ‘–ğ‘Ÿin the entire Rashomon
set.
The remaining models are referred to as fair models (with respect
to the chosen ğ‘€ğ‘“ğ‘ğ‘–ğ‘Ÿ). Second, among the remaining (fair) models,
we pick the model ğ‘†âˆ—which minimizes the objective ğ‘“.
Figure 6 reports the value of ğ‘“in the test set for the chosen rule
setğ‘†âˆ—in the entire Rashomon set and in samples of rule sets of
increasing size. Again, the Rashomon set consists of |R(U)|=1,409
rule sets and we draw samples of sizes 10%, 20%and40%of|R(U)|
using ApproxSample . The reported losses are obtained as average
over 10repetitions of the sampling process. The performance of the
optimal fairrule set chosen from the samples is not far from the
performance of the optimal fairrule set chosen in the entire R(U),
and the gap between the performance of the optimal fairrule set
in the entire Rashomon set and in samples drawn from it quickly
shrinks as the sample size increases. In the case of statistical parity,
no significant difference is observed across different sample sizes,
suggesting that even the smallest sample is enough to find a rule set
which is fair with respect to statistical parity and exhibits predictive
performance indistinguishable from the predictive performance of
the fair rule set that would be chosen in the entire Rashomon set.
Thus, in view of the results reported in this section, we conclude
that exhaustive enumeration of the Rashomon set may be redundant
when the goal is to investigate fairness or find a model that is both
accurate and fair. A representative sample may suffice.
 
489