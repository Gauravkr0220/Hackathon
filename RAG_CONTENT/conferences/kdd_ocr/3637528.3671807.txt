Graph Bottlenecked Social Recommendation
Yonghui Yang
Hefei University of Technology
Hefei, China
yyh.hfut@gmail.comLe Wuâˆ—
Hefei University of Technology
Hefei, China
lewu.ustc@gmail.comZihan Wang
Hefei University of Technology
Hefei, China
zhwang.hfut@gmail.com
Zhuangzhuang He
Hefei University of Technology
Hefei, China
hyicheng223@gmail.comRichang Hong
Hefei University of Technology
Hefei, China
hongrc.hfut@gmail.comMeng Wang
Hefei University of Technology
Hefei, China
eric.mengwang@gmail.com
ABSTRACT
With the emergence of social networks, social recommendation has
become an essential technique for personalized services. Recently,
graph-based social recommendations have shown promising results
by capturing the high-order social influence. Most empirical studies
of graph-based social recommendations directly take the observed
social networks into formulation, and produce user preferences
based on social homogeneity. Despite the effectiveness, we argue
that social networks in the real-world are inevitably noisy (existing
redundant social relations), which may obstruct precise user pref-
erence characterization. Nevertheless, identifying and removing
redundant social relations is challenging due to a lack of labels. In
this paper, we focus on learning the denoised social structure to
facilitate recommendation tasks from an information bottleneck
perspective. Specifically, we propose a novel Graph Bottlenecked
Social Recommendation (GBSR) framework to tackle the social noise
issue. GBSR is a model-agnostic social denoising framework, that
aims to maximize the mutual information between the denoised
social graph and recommendation labels, meanwhile minimizing it
between the denoised social graph and the original one. This enables
GBSR to learn the minimal yet sufficient social structure, effectively
reducing redundant social relations and enhancing social recom-
mendations. Technically, GBSR consists of two elaborate compo-
nents, preference-guided social graph refinement, and HSIC-based
bottleneck learning. Extensive experimental results demonstrate
the superiority of the proposed GBSR , including high performances
and good generality combined with various backbones. Our code
is available at: https://github.com/yimutianyang/KDD24-GBSR.
CCS CONCEPTS
â€¢Human-centered computing â†’Collaborative and social
computing.
Le Wu is the Corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671807KEYWORDS
Robust Social Recommendation, Social Denoising, Information Bot-
tleneck
ACM Reference Format:
Yonghui Yang, Le Wu, Zihan Wang, Zhuangzhuang He, Richang Hong,
and Meng Wang. 2024. Graph Bottlenecked Social Recommendation. In
Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/3637528.3671807
1 INTRODUCTION
Learning informative user and item representations is the key to
building modern recommender systems. Classic collaborative filter-
ing paradigm factorizes user-item interaction matrix to learn user
and item representations, which is widely researched but usually
limited by sparse interactions. With the proliferation of social me-
dia, social recommendation has become an important technique
to provide personalized suggestions [ 44]. Both user-item interac-
tions [ 38,39] and user-user social relations [ 54,55] are available
on social platforms, prompting the development of various social
recommendation methods designed to exploit these behavior pat-
terns [23, 26].
Following the social homophily [ 30] and social influence the-
ory [ 29], many efforts are devoted to characterizing social relation
effects on user preferences. Early works mainly focus on exploiting
first-order social relations, i.e., social regularization that assumes
socially connected users share similar preference [ 19], and social
enhancement that incorporates user-trusted friendsâ€™ feedback as
auxiliary for the target user [ 15]. Recently, witnessed the power of
graph neural networks (GNNs) on machine learning [ 5,6,22,57,60],
graph-based recommendations have attracted more and more atten-
tion [ 4,16,56,63]. Graph-based social recommendations [ 8,55,58]
achieve impressive progress in improving recommendation perfor-
mances by formulating usersâ€™ high-order interest propagation and
social influence diffusion with GNNs.
Despite the effectiveness, current graph-based social recommen-
dations rarely notice the social noise problem, i.e., social graphs
are inevitably noisy with redundant social relations. Those redun-
dant relations are caused by unreliable social relations and low
preference-affinity social relations [ 35,43]. Consequently, directly
using the observed social graph may hinder precise user preference
characterization, leading to sub-optimal recommendation results.
We conduct an empirical study to illustrate the social noise problem.
As shown in Figure 1, we compare LightGCN with current SOTA
3853
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yonghui Yang et al.
(a) Douban-Book
 (b) Yelp
Figure 1: Performance comparisons between LightGCN and
SOTA graph-based social recommendation methods.
graph-based recommendation methods, including SocialLGCN [ 25]
and DiffNet++ [ 54]. To avoid the effect of the message-passing
mechanism of different methods, we additionally implement the
extension of LightGCN, called LightGCN-S which additionally per-
forms social neighbor aggregation for user representation learning.
We can find that compared with LightGCN, graph-based social rec-
ommendations do not present significant strength on both metrics,
even worse on the Douban-Book dataset. This indicates that social
networks are usually noisy, itâ€™s necessary to filter redundant social
relations to enhance the robustness of social recommendations.
However, identifying and removing redundant social relations is
non-trivial due to a lack of ground-truth labels. Besides, how can
guarantee the recommendation accuracy while removing social
relations?
In this paper, we focus on learning the denoised social graph
structure to facilitate recommendation tasks from an information
bottleneck perspective. Specifically, we propose a novel Graph Bot-
tlenecked Social Recommendation (GBSR) framework to tackle the
social noise problem. Let Gğ‘†={ğ‘ˆ,S}denote the user-user social
graph and Rdenote the user-item interaction matrix, where ğ‘ˆis
userset and Sis social structure matrix. The optimal denoised social
graph structure Sâ€²should satisfy: the minimal from Syet efficient
for infer R.To achieve this goal, we first introduce user preference
signals to guide the social graph denoising process, then optimize
the learning process via the Information Bottleneck (IB) principle.
Specifically, GBSR maximize the mutual information between the
denoised social graph structure Sâ€²and interaction matrix R, mean-
while minimizing it between the denoised social graph structure Sâ€²
and the original S. Therefore, the learning objective is formulated
as:ğ‘€ğ‘ğ‘¥ :ğ¼(R;Sâ€²)âˆ’ğ›½ğ¼(Sâ€²;S).
Nevertheless, optimizing the objective of GBSR for social recom-
mendation is still challenging due to the following two challenges.
For the maximization of ğ¼(R;Sâ€²), social graph and sparse interac-
tion matrix are two non-Euclidean data, which are hard to com-
pare directly. For the minimization of ğ¼(Sâ€²;S), estimating the upper
bound of MI is an intractable problem. Although some works [ 1,7]
leverage variational techniques to estimate the upper bound, they
heavily rely on the prior assumption. To address the above two chal-
lenges, GBSR is implemented as follows. First, regarding the hard-
comparable issue of ğ¼(R;Sâ€²), we take all nodes into intermediary
and derive the lower bound of ğ¼(R;Sâ€²)for maximization. Second, we
introduce the Hilbert-Schmidt independence criterion (HSIC) [ 28]to replace the minimization of ğ¼(Sâ€²;S). HSIC [ 12] is a statistic mea-
sure of variable dependency, minimizing HSIC approximate the
minimization of mutual information. Our contributions are sum-
marized as follows:
â€¢In this paper, we revisit the social denoising recommendation
from an information theory perspective, and propose a novel
Graph Bottlenecked Social Recommendation (GBSR) frame-
work to tackle the noise issue.
â€¢Technically, we derive the lower bound of ğ¼(R;Sâ€²)for maxi-
mization, and introduce the Hilbert-Schmidt independence
criterion (HSIC) to approximate the minimization of ğ¼(Sâ€²;S).
â€¢Empirical studies on three benchmarks clearly demonstrate
the effectiveness and generality of the proposed GBSR , i.e.,
GBSR achives over 17.06%, 10%, and 11.27%improvements
of NDCG@20 compared with the strongest baseline.
2 PRELIMINARIES
2.1 Problem Statement
There are two kinds of entities in fundamental social recommenda-
tion scenarios: a userset ğ‘ˆ(|ğ‘ˆ|=ğ‘€) and an itemset ğ‘‰(|ğ‘‰|=ğ‘).
Users have two kinds of behaviors, user-user social relations and
user-item interactions. We use matrix SâˆˆRğ‘€Ã—ğ‘€to describe user-
user social structure, where each element ğ‘ ğ‘ğ‘=1if userğ‘follows
userğ‘, otherwise ğ‘ ğ‘ğ‘=0. Similar, we use matrix RâˆˆRğ‘€Ã—ğ‘to
describe user-item interactions, where each element rğ‘ğ‘–=1if user
ğ‘interacted with item ğ‘–, otherwise rğ‘ğ‘–=0. Given user ğ‘, itemğ‘–,
and social relation matrix Sas input, graph-based social recom-
menders aim to infer the probability user ğ‘will interact with item
ğ‘–:Ë†ğ‘Ÿğ‘ğ‘–=Gğœƒ(ğ‘,ğ‘–,S), whereGğœƒdenotes GNN formulation. Thus, the
optimization objective of graph-based social recommendation is
defined as follows:
ğœƒâˆ—=arg min
ğœƒE(ğ‘,ğ‘–,ğ‘Ÿğ‘ğ‘–)âˆ¼PLğ‘Ÿ(ğ‘Ÿğ‘ğ‘–;Gğœƒ(ğ‘,ğ‘–,S)), (1)
wherePdenote distribution of training data, and ğœƒdenote GNN
parameters. However, user social networks are usually noisy with
redundant relations [ 35], directly using Sto infer interaction prob-
ability may decrease the recommendation accuracy. In this work,
we focus on learning robust social structure Sâ€²to facilitate recom-
mendation performance:
Sâ€²=Fğœ™(ğ‘ˆ,S), (2)
whereFğœ™denotes social denoising function with the parameters ğœ™.
Consequently, the final optimization of graph-noised social recom-
mendation is described as follows:
ğœƒâˆ—,ğœ™âˆ—=arg min
ğœƒ,ğœ™E(ğ‘,ğ‘–,ğ‘Ÿğ‘ğ‘–)âˆ¼PLğ‘Ÿ(ğ‘Ÿğ‘ğ‘–;Gğœƒ(ğ‘,ğ‘–,Fğœ™(ğ‘ˆ,S))).(3)
2.2 Information Bottleneck Principle
Information Bottleneck (IB) is a representation learning principle
in machine learning, which seeks a trade-off between data fit and
reducing irrelevant information [ 45,46]. Given input data ğ‘‹,ğ‘
is the hidden representation, and ğ‘Œis the downstream task label,
which follows the Markov Chain <ğ‘‹â†’ğ‘â†’ğ‘Œ>. IB principle de-
scribes that an optimal representation should maintain the minimal
3854Graph Bottlenecked Social Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
9
ğ‘† Graph Denoising ğ‘… ğ‘†â€²Graph -based social
recommendation
ğ‘€ğ‘ğ‘¥:ğ¼ğ‘…;ğ‘†â€²âˆ’ğ›½ğ¼(ğ‘†â€²;ğ‘†)
Social Graph Information BottleneckSocial graph (original) Denoised social graph User-item interactionsïï
ïï
ïïUser preference s
Figure 2: Overview of our proposed GBSR framework.
sufficient information for the downstream tasks [37, 46]:
ğ‘âˆ—=arg max
ğ‘ğ¼(ğ‘Œ;ğ‘)âˆ’ğ›½ğ¼(ğ‘‹;ğ‘), (4)
whereğ¼(ğ‘Œ;ğ‘)denotes the mutual information between the hidden
representation ğ‘and labelğ‘Œ,ğ¼(ğ‘‹;ğ‘)denotes the mutual informa-
tion between the hidden representation ğ‘and input data ğ‘‹two
variables,ğ›½is the coefficient to balance these two parts. IB prin-
ciple has been widely applied in machine learning tasks, such as
model robustness [ 53,59], fairness [ 13], and explainability [ 2]. In
this work, we introduce the IB principle to robust social denois-
ing learning, which aims to seek the minimal yet sufficient social
structure for recommendation tasks.
3 THE PROPOSED GBSR FRAMEWORK
In this section, we introduce our proposed Graph Bottlenecked So-
cial Recommendation (GBSR) framework for social denoising based
recommendation. Essentially, GBSR aims to learn the minimal yet
efficient social structure to facilitate recommendation tasks, which
is guaranteed by the information bottleneck principle. Next, we
first give the overall optimization objective of GBSR , then introduce
how to implement each component of GBSR in detail. Finally, we
instantiate GBSR with LightGCN-S backbone.
3.1 Overview of GBSR
As shown in Figure 2, we present the overall objective of our pro-
posed GBSR framework for the social recommendation. Instead
of directly using the original social structure S, we aim to learn a
denoised yet informative social structure Sâ€²to enhance recommen-
dation. Due to the lack of available prior for social denoising, we
introduce user preference signals to guide social graph denoising.
To guarantee the trade-off between social denoising and recommen-
dation tasks, we optimize GBSR via graph information bottleneck
principle. Thus, the goal of GBSR is:ğ‘€ğ‘ğ‘¥ :ğ¼(R;Sâ€²)âˆ’ğ›½ğ¼(Sâ€²;S). Due
to the intractability of ğ¼(R;Sâ€²), we take all nodes into an intermedi-
ary for calculation. Thus, we obtain the final optimization objective
ofGBSR :
ğ‘€ğ‘ğ‘¥ :ğ¼(R;ğ‘ˆ,ğ‘‰, Sâ€²)âˆ’ğ›½ğ¼(Sâ€²;S), (5)
where the first term is encouraging that the denoised social graph
preserves the essential information to facilitate recommendation
tasks. The second term is the compression of the original social
graph, aiming to filter redundant social relations.3.2 Preference-guided Social Denoising
To achieve the above objective of GBSR , we first need to refine
the denoised social graph. The challenge is that although the social
graph has noisy relations, there are no available labels to guide the
denoising process. Based on social homogeneity social-connected
individuals have more similar behavior similarity, we inject user
preference signals into the social denoising process, i.e., users with
similar preferences are more likely to have social relations.
Formally, we formulate the social denoising process as a graph
edge dropout problem. Given the original social graph structure S,
the denoised one is defined as:
Sâ€²=Fğœ™(ğ‘ˆ,S)={ğ‘ ğ‘ğ‘âŠ™ğœŒğ‘ğ‘}, (6)
in whichğœŒğ‘ğ‘âˆ¼ğµğ‘’ğ‘Ÿğ‘›(ğ‘¤ğ‘ğ‘)+ğœ–denotes that each edge <ğ‘¢ğ‘,ğ‘¢ğ‘>
will be dropped with the probability 1âˆ’ğ‘¤ğ‘ğ‘+ğœ–. Here, we add the
parameterğœ–>0to represent the observation bias. Due to lacking
prior information, we introduce task-relevant user preferences to
refine social structure. Let Eğ‘ˆâˆˆRğ‘€Ã—ğ‘‘denote user preference
representations learned from the observed interactions, such as
Matrix Factorization [ 31] and LightGCN [ 16]. For each observed
social relation <ğ‘,ğ‘>, the link confidence is calculated as:
ğ‘¤ğ‘ğ‘=(ğ‘”(eğ‘,eğ‘)), (7)
where eğ‘andeğ‘denotes user ğ‘and userğ‘preference representa-
tions, respectively. ğ‘”()is the fusion function, we employ MLPs to
realize it. However, Sâ€²is not differentiable with the parameter ğœŒof
Bernoulli distribution, so we use the popular concrete relaxation
method [20] to replace:
ğµğ‘’ğ‘Ÿğ‘›(ğ‘¤ğ‘ğ‘)=ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ‘™ğ‘œğ‘”(ğ›¿/(1âˆ’ğ›¿)+ğ‘¤ğ‘ğ‘)/ğ‘¡), (8)
whereğ›¿âˆ¼ğ‘ˆ(0,1), andğ‘¡âˆˆR+is the temperature parameter (we set
t=0.2 in our experiments). After re-parameterization, the discrete
Bernoulli distribution is transferred to a differentiable function.
3.3 Maximization of ğ¼(R;ğ‘ˆ,ğ‘‰, Sâ€²)
Given the denoised social graph Sâ€², we first present how to maxi-
mize the mutual information ğ¼(R;ğ‘ˆ,ğ‘‰, Sâ€²), which ensures the de-
noised social graph satisfy recommendation tasks. Specifically, we
derivate the lower bound of ğ¼(R;ğ‘ˆ,ğ‘‰, Sâ€²)as follows:
ğ¼(R;ğ‘ˆ,ğ‘‰, Sâ€²)(ğ‘)=ğ»(R)âˆ’ğ»(R|ğ‘ˆ,ğ‘‰, Sâ€²)
(ğ‘)
â‰¥ğ‘€âˆ’1âˆ‘ï¸
ğ‘=0ğ‘âˆ’1âˆ‘ï¸
ğ‘–=01âˆ‘ï¸
ğ‘Ÿ=0ğ‘(ğ‘Ÿ,ğ‘,ğ‘–, Sâ€²)ğ‘™ğ‘œğ‘”(ğ‘(ğ‘Ÿ|ğ‘,ğ‘–,Sâ€²)
(ğ‘)
â‰¥âˆ‘ï¸
(ğ‘,ğ‘–,ğ‘—)âˆˆDğ‘™ğ‘œğ‘”(ğ‘(ğ‘Ÿğ‘ğ‘–=1|ğ‘,ğ‘–,Sâ€²))+ğ‘™ğ‘œğ‘”(ğ‘(ğ‘Ÿğ‘ğ‘–=0|ğ‘,ğ‘—,Sâ€²))
(ğ‘‘)=âˆ‘ï¸
(ğ‘,ğ‘–,ğ‘—)âˆˆDğ‘™ğ‘œğ‘”(ğœ(G(ğ‘,ğ‘–,Sâ€²)))âˆ’ğ‘™ğ‘œğ‘”(ğœ(G(ğ‘,ğ‘—,Sâ€²)))
(ğ‘’)
â‰¥âˆ‘ï¸
(ğ‘,ğ‘–,ğ‘—)âˆˆDğ‘™ğ‘œğ‘”(ğœ(G(ğ‘,ğ‘–,Sâ€²)âˆ’G(ğ‘,ğ‘—,Sâ€²))),
(9)
whereG(Â·) is any graph-based social recommender as we men-
tioned in the preliminaries, ğœ(Â·)is the sigmoid activation, D=
{(ğ‘,ğ‘–,ğ‘—)|ğ‘Ÿğ‘ğ‘–=1âˆ§ğ‘Ÿğ‘ğ‘—=0}is all training data. Next, we intro-
duce each derivation step as follows: (a) is the definition of mu-
tual information; (b) is the non-negative property of ğ»(R); (c)
3855KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yonghui Yang et al.
is thatğ‘(ğ‘Ÿ|ğ‘,ğ‘–,Sâ€²) â‰¤ 1, and we split all samples into observed
interactions and non-observed interactions; (d) ğœ(G(ğ‘,ğ‘–,Sâ€²))is
the variational approximation of ğ‘(ğ‘Ÿğ‘ğ‘–=1|ğ‘,ğ‘–,Sâ€²); (e) is due to
ğ‘™ğ‘œğ‘”(ğœ(ğ‘¥))âˆ’ğ‘™ğ‘œğ‘”(ğœ(ğ‘¦))â‰¥ğ‘™ğ‘œğ‘”(ğœ(ğ‘¥âˆ’ğ‘¦)).
According to the above derivation, we can find that the popular
BPR ranking loss [ 36] is the lower bound of mutual information
ğ¼(R;ğ‘ˆ,ğ‘‰, Sâ€²). Therefore, we employ BPR loss as the objective of
mutual information maximization.
3.4 Minimization of ğ¼(Sâ€²;S)
Next, we introduce how to minimize ğ¼(Sâ€²,S), which aims to reduce
the redundant social relations in the original graph. Estimating
the upper bound of mutual information is an intractable problem.
Although some works [ 1,7] leverage variational techniques to
estimate the upper bound, but heavily rely on the prior assump-
tion. Therefore, we introduce Hilbert-Schmidt Independence Cri-
terion (HSIC [ 12]) as the approximation of the minimization of
ğ¼(R;Sâ€²).
HSIC brief. HSIC serves as a statistical measure of dependency [ 12],
which is formulated as the Hilbert-Schmidt norm, assessing the
cross-covariance operator between distributions within the Repro-
ducing Kernel Hilbert Space (RKHS). Mathematically, given two
variablesğ‘‹andğ‘Œ, HSIC(ğ‘‹,ğ‘Œ)is defined as follows:
ğ»ğ‘†ğ¼ğ¶(ğ‘‹,ğ‘Œ)=||ğ¶ğ‘‹ğ‘Œ||2
â„ğ‘ 
=Eğ‘‹,ğ‘‹â€²,ğ‘Œ,ğ‘Œâ€²[ğ¾ğ‘‹(ğ‘‹,ğ‘‹â€²)ğ¾ğ‘Œ(ğ‘Œ,ğ‘Œâ€²)]
+Eğ‘‹,ğ‘‹â€²[ğ¾ğ‘‹(ğ‘‹,ğ‘‹â€²)]Eğ‘Œ,ğ‘Œâ€²[ğ¾ğ‘Œ(ğ‘Œ,ğ‘Œâ€²)]
âˆ’2Eğ‘‹ğ‘Œ[Eğ‘‹â€²[ğ¾ğ‘‹(ğ‘‹,ğ‘‹â€²)]Eğ‘Œâ€²[ğ¾ğ‘Œ(ğ‘Œ,ğ‘Œâ€²)]],(10)
whereğ¾ğ‘‹andğ¾ğ‘Œare two kernel functions for variables ğ‘‹andğ‘Œ,ğ‘‹â€²
andğ‘Œâ€²are two independent copies of ğ‘‹andğ‘Œ. Given the sampled
instances(ğ‘¥ğ‘–,ğ‘¦ğ‘–)ğ‘›
ğ‘–=1from the batch training data, the ğ»ğ‘†ğ¼ğ¶(ğ‘‹,ğ‘Œ)
can be estimated as:
Ë†ğ»ğ‘†ğ¼ğ¶(ğ‘‹,ğ‘Œ)=(ğ‘›âˆ’1)âˆ’2ğ‘‡ğ‘Ÿ(ğ¾ğ‘‹ğ»ğ¾ğ‘Œğ»), (11)
whereğ¾ğ‘‹andğ¾ğ‘Œare used kernel matrices [ 12], with elements
ğ¾ğ‘‹ğ‘–ğ‘—=ğ¾ğ‘‹(ğ‘¥ğ‘–,ğ‘¥ğ‘—)andğ¾ğ‘Œğ‘–ğ‘—=ğ¾ğ‘Œ(ğ‘¦ğ‘–,ğ‘¦ğ‘—),ğ»=Iâˆ’1
ğ‘›11ğ‘‡is the
centering matrix, and ğ‘‡ğ‘Ÿ(Â·)denotes the trace of matrix. In practice,
we adopt the widely used radial basis function (RBF) [ 48] as the
kernel function:
ğ¾(ğ‘¥ğ‘–,ğ‘¥ğ‘—)=ğ‘’ğ‘¥ğ‘(âˆ’||ğ‘¥ğ‘–âˆ’ğ‘¥ğ‘—||2
2ğœ2), (12)
whereğœis the parameter that controls the sharpness of RBF.
HSIC-based bottleneck learning. Given the original and de-
noised social graph structures SandSâ€², we minimize ğ»ğ‘†ğ¼ğ¶(Sâ€²;S)
to replace the minimization of ğ¼(Sâ€²;S). However, social graphs are
non-Euclidean data, making it difficult to measure dependency.
In practice, we adopt Monte Carlo sampling [ 40] on all the node
representations for calculation:
ğ‘€ğ‘–ğ‘› :ğ»ğ‘†ğ¼ğ¶(Sâ€²,S)=Ë†ğ»ğ‘†ğ¼ğ¶(Eâ€²B,EB), (13)
whereBdenotes the batch sampling users, Eâ€²andEdenote node
representations, which are learned from recommenders Gğœƒ,ğœ™(ğ‘ˆ,ğ‘‰, Sâ€²)
andGğœƒ(ğ‘ˆ,ğ‘‰, S). Thus, we can reduce the redundant social relations
via the HSIC-based bottleneck regularization:
Lğ‘–ğ‘=Ë†ğ»ğ‘†ğ¼ğ¶(Eâ€²B,EB). (14)3.5 Instantiating the GBSR Framework
In this section, we instantiate our proposed GBSR with specific
graph-based social recommender Gğœƒ(ğ‘ˆ,ğ‘‰, S). To avoid the effect of
different message-passing mechanisms, we implement LightGCN-S
as the backbone model (we also realize GBSR with other backbones,
refer to the generality analysis). Firstly, we formulate the available
data and denoised social structure as a graph G={ğ‘ˆâˆªğ‘‰,A}, where
ğ‘ˆâˆªğ‘‰denotes the set of nodes, and Ais the adjacent matrix defined
as follows:
A=S R
Rğ‘‡0ğ‘Ã—ğ‘
. (15)
Given the initialized node embeddings E0âˆˆR(ğ‘€+ğ‘)Ã—ğ‘‘, LightGCN-
S updates node embeddings through multiple graph convolutions:
Eğ‘™+1=Dâˆ’1
2ADâˆ’1
2Eğ‘™, (16)
where Dis the degree matrix of graph G,Eğ‘™+1andEğ‘™denote node
embeddings in ğ‘™+1ğ‘¡â„andğ‘™ğ‘¡â„graph convolution layer, respectively.
When stacking ğ¿graph convolution layers, the final node represen-
tations can be obtained with a readout operation:
E=ğ‘…ğ‘’ğ‘ğ‘‘ğ‘œğ‘¢ğ‘¡(E0,E1,...,Eğ¿). (17)
After obtaining the learned node representations through GCNs,
LightGCN-S infers the propensity that user ğ‘interacts with item
ğ‘–by an inner product: Ë†ğ‘Ÿğ‘ğ‘–=<ğ‘’ğ‘,ğ‘’ğ‘–>. All the above process are
summarized as Ë†ğ‘Ÿğ‘ğ‘–=Gğœƒ(ğ‘,ğ‘–,S).
Next, we give the illustration of graph-denoised social recommen-
dation. We first use the initialized node embeddings to obtain user
preference representations P=E0[:ğ‘€], then achieve the denoised
social structure Sâ€²based on preference-guided social structure learn-
ing (section 3.2). Given the learned denoised social structure Sâ€², we
establish graph-denoised social recommender Ë†ğ‘Ÿğ‘ğ‘–=Gğœƒ,ğœ™(ğ‘,ğ‘–,Sâ€²).
Then, we select the pairwise ranking loss [ 36] to optimize model
parameters:
Lğ‘Ÿğ‘’ğ‘=ğ‘€âˆ’1âˆ‘ï¸
ğ‘=0âˆ‘ï¸
(ğ‘–,ğ‘—)âˆˆğ·ğ‘âˆ’ğ‘™ğ‘œğ‘”ğœ(Ë†ğ‘Ÿğ‘ğ‘–âˆ’Ë†ğ‘Ÿğ‘ğ‘—)+ğœ†||E0||2, (18)
whereğœ(Â·)is the sigmoid activation function, ğœ†is the regularization
coefficient. ğ·ğ‘={(ğ‘–,ğ‘—)|ğ‘–âˆˆğ‘…ğ‘âˆ§ğ‘—âˆ‰ğ‘…ğ‘}denotes the pairwise training
data for user ğ‘.ğ‘…ğ‘represents user ğ‘interacted items on the training
data. Combined with the HSIC-based bottleneck regularizer, we
obtain the final optimization objective:
arg min
ğœƒ,ğœ™L=Lğ‘Ÿğ‘’ğ‘+ğ›½Lğ‘–ğ‘, (19)
The overall learning process of GBSR is illustrated in Algorithm 1.
3.6 Model Discussion
In this section, we analyze the proposed GBSR from model com-
plexity and model generalization.
3.6.1 Space Complexity. As illustrated in Algorithm 1, the parame-
ters of GBSR are composed of two parts: graph-based social recom-
mender parameters ğœƒand social denoising parameters ğœ™. Among
them,ğœƒ=E0are the general parameters equipped for backbone
models (such as LightGCN-S). ğœ™are the parameters of MLPs, which
are used to calculate the social edge confidence. Because ğœ™are the
3856Graph Bottlenecked Social Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1: The algorithm of GBSR
Data: Usersetğ‘ˆ, Itemsetğ‘‰, User-item interactions R,
User-user social relations S, and observation bias ğœ–
Result: Optimal graph-denoised social recommender
Gâˆ—
ğœƒ,ğœ™(Â·)
1Initialize recommender Gğœƒ,ğœ™with random weights;
2 while not converged do
3 Sample a batch training data D;
4 Compute social edge dropout probability via
Eq.(7)-Eq.(8);
5 Refine the denoised social structure Sâ€²via Eq.(6);
6 Obtain node representations Eâ€²viaGğœƒ,ğœ™(ğ‘ˆ,ğ‘‰, Sâ€²);
7 Obtain node representations EviaGğœƒ(ğ‘ˆ,ğ‘‰, S);
8 Compute recommendation task loss Lğ‘Ÿğ‘’ğ‘via Eq.(18);
9 Compute HSIC bottleneck loss Lğ‘–ğ‘via Eq.(14);
10 Update model parameters according to Eq.(19);
11 end
12Return the optimal Gâˆ—
ğœƒ,ğœ™(Â·);
shared parameters for all social edges, the additional parameters of
GBSR are ignorable compared with backbone models.
3.6.2 Time Complexity. Compared with the backbone model (such
as LightGCN-S), the additional time cost is social graph denoising
and HSIC-bottleneck optimization. Social graph denoising is con-
ducted on the observed social relations, which performs a sparse
matrix. Besides, the time complexity of the HSIC-bottleneck regu-
larizer lies in the number of the sampled nodes (refer to Eq. (13)). In
practice, we adopt a mini-batch training strategy to reduce the time
cost of bottleneck learning, and the additional time cost of GBSR is
affordable. Besides, as we remove redundant social relations, the
denoised yet informative social graph makes GBSR convergence
much faster than the backbone model. Experiments also verify the
efficiency of GBSR.
3.6.3 Model Generalization. The proposed GBSR is designed for so-
cial denoising under graph-based social recommendation scenarios.
It does not depend on specific graph-based social recommenders,
such as DiffNet++ [ 54] and SocialLGN [ 25]. Our proposed GBSR is
a flexible denoising framework to enhance social recommendations,
we also conduct experiments on four backbones to demonstrate the
generalization. Besides the backbone model, the idea of introducing
the information bottleneck principle to graph denoising can also
be generalized for different recommendation scenarios.
4 EXPERIMENTS
In this section, we conduct extensive experiments on three real-
world datasets to validate the effectiveness of our proposed GBSR .
We first introduce experimental settings, followed by recommenda-
tion performance comparisons. Finally, we give a detailed model
investigation, including training efficiency, visualization of the de-
noised social graph, and parameter sensitivities.Table 1: The statistics of three datasets.
Dataset Douban-Book Yelp Epinions
Users 13,024 19,593 18,202
Items 22,347 21,266 47,449
Interactions 792,062 450,884 298,173
Social Relations 169,150 864,157 381,559
Interaction Density 0.048% 0.034% 0.035%
Relation Density 0.268% 0.206% 0.115%
4.1 Experimental Settings
4.1.1 Datasets. We conduct empirical studies on three public datasets
to verify the effectiveness of our proposed GBSR , including Yelp,
Epinions, and Dianping [ 62]. All datasets involve user-user social
links and user-item interactions. For the Yelp dataset, we follow
the released version in [ 65]. For epinions and dianping datasets, we
filter ratings less than 3 and keep the remaining ratings as positive
feedback. After that, we sample 80% interactions as training data,
and the remaining 20% as test data. The detailed statistics of all
datasets are summarized in Table 1.
4.1.2 Baselines and Evaluation Metrics. To evaluate the effective-
ness of our proposed GBSR , we select state-of-the-art baselines for
comparisons. Specifically, these baselines can be divided into two
groups: graph-based social recommendation methods [ 8,16,54]
and social graph denoising methods [ 35,66,69], which are list as
follows:
â€¢LightGCN [16]: is the SOTA graph-based collaborative fil-
tering method, which simplifies GCNs by removing the re-
dundant feature transformation and non-linear activation
components for ID-based recommendation.
â€¢LightGCN-S: We extend LightGCN to graph-based social
recommendation, that each userâ€™s neighbors include their in-
teracted items and linked social users. LightGCN-S is a basic
and lightweight model, considering our proposed GBSR is
a model-agnostic social graph denoising method, we select
LightGCN-S as the backbone model.
â€¢GraphRec [8]: is a classic graph-based social recommen-
dation method, it incorporates user opinions and user two
kinds of graphs for preference learning.
â€¢DiffNet++ [54]: is the SOTA graph-based social recommen-
dation method, it recursively formulates user interest propa-
gation and social influence diffusion process with a hierar-
chical attention mechanism.
â€¢SocialLGN [25]: propagates user representations on both
user-item interactions graph and user-user social graph with
light graph convolutional layers, and fuses them for recom-
mendation.
â€¢Rule-based: We follow [ 35] and remove unreliable social
relations based on the similarity of the user-interacted items.
â€¢ESRF [66]: generates alternative social neighbors and further
performs neighbor denoising with adversarial training.
â€¢GDMSR [35]: designs the robust preference-guided social
denoising to enhance graph-based social recommendation,
it only remains the informative social relations according to
preference confidences.
3857KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yonghui Yang et al.
Table 2: Overall performance comparisons on three benchmarks. The best performance is highlighted in bold and the second is
highlighted by underlines . Impro. indicates the relative improvement of our proposed GBSR compared to the best baseline.
Douban-Bo
ok Y
elp EpinionsMo
delsR@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20 R@10 N@10 R@20 N@20
LightGCN 0.1039 0.1195 0.1526 0.1283 0.0698 0.0507 0.1081 0.0623 0.0432 0.0314 0.0675 0.0385
GraphRe
c 0.0971 0.1145 0.1453 0.1237 0.0672 0.0485 0.1077 0.0607 0.0436 0.0315 0.0681 0.0387
DiffNet++ 0.1010 0.1184 0.1489 0.1270 0.0707 0.0516 0.1114 0.0640 0.0468 0.0329 0.0727 0.0406
So
cialLGN 0.1034 0.1182 0.1527 0.1274 0.0681 0.0507 0.1059 0.0620 0.0416 0.0307 0.0634 0.0371
LightGCN-S 0.1021 0.1187 0.1506 0.1281 0.0714 0.0529 0.1126 0.0651 0.0477 0.0347 0.0716 0.0417
Rule-base
d 0.1033 0.1192 0.1518 0.1289 0.0705 0.0526 0.1126 0.0652 0.0465 0.0340 0.0716 0.0414
ESRF 0.1042 0.1199 0.1534 0.1301 0.0718 0.0526 0.1123 0.0645 0.0462 0.0329 0.0727 0.0406
GDMSR 0.1026 0.1001 0.1538 0.1245 0.0739 0.0535 0.1148 0.0658 0.0461 0.0326 0.0721 0.0414
GBSR 0.1189 0.1451 0.1694 0.1523 0.0805 0.0592 0.1243 0.0724 0.0529 0.0385 0.0793 0.0464
Impr
o. 14.11% 21.02% 10.14% 17.06% 8.93% 10.65% 8.28% 10.00% 10.90% 10.95% 9.08% 11.27%
As we focus on implicit recommendation scenarios, we employ
two widely used ranking metrics: Recall@N and NDCG@N [ 14,41].
Specifically, Recall@N measures the percentage of the recalled pos-
itive samples for the Top-N ranking lists. Furthermore, NDCG@N
assigns higher scores for those items in the top-ranked positions. In
the evaluation stage, we adopt a full-ranking strategy that views all
non-interacted items as candidates to avoid biased evaluation [ 24,
68]. For each model, we repeat experiments in 5 times and report
the average values.
4.1.3 Parameter Settings. We implement our proposed GBSR and
backbone with Tensorflow1. For all baselines, we follow the origi-
nal settings and carefully fine-tune parameters for fair comparisons.
For latent embedding based methods, we initialize their embeddings
with a Gaussian distribution with a mean value of 0 and a standard
variance of 0.01, and fix the embedding size to 64. For model opti-
mization, we use Adam optimizer with a learning rate of 0.001 and
a batch size of 2048. We follow the mainstream ranking-based meth-
ods [ 36], and randomly select 1 non-interacted item as the negative
sample for pairwise ranking optimization. We search the GCN layer
in[1,2,3,4], the regularization parameter ğœ†in[0.0001,0.001,0.01].
For the observation bias, we set ğœ–=0.5for all datasets. For infor-
mation bottleneck constraint coefficient ğ›½, we use grid-search with
different scales over three datasets, and report detailed analysis in
experiments.
4.2 Recommendation Performances
4.2.1 Overall Comparisons with Baselines. As shown in Table 2,
we compare our proposed GBSR with state-of-the-art methods on
three benchmarks. For a fair comparison, all denoising methods
are conducted on the LightGCN-S backbone. Given the empirical
studies, we have the following observations:
â€¢Compared with LightGCN, graph-based social recommenda-
tion methods present slight improvements under most of the
datasets, i.e., DiffNet++ obtains a 2.24% improvement on the
NDCG@20 metric for Yelp dataset. However, this is not al-
ways the case, all social graph recommendations show a per-
formance degradation on the Douban-Book dataset. While
1https://www.tensorflow.orgsupported by social graphs, it is noteworthy that graph-based
social recommendation methods do not consistently outper-
form LightGCN in terms of performance. These demonstrate
that directly using social graphs may decrease recommenda-
tion performance, itâ€™s necessary to remove redundant social
relations to enhance recommendation.
â€¢Compared with directly using original social graphs, so-
cial denoising methods present better performances in most
cases. This indicates that social noise is ubiquitous in real-
world recommendation scenarios. All social denoising meth-
ods are implemented on LightGCN-S backbone, we find
that GDMSR is the strongest baseline, which benefits from
preference-guided social denoising and self-correcting cur-
riculum learning. However, these social denoising methods
donâ€™t present large-margin improvements compared with
the backbone model. The reason is that simple rule or as-
sumption based denoising methods lack of theoretical guar-
antee, itâ€™s hard to seek an effective trade-off between social
denoising and recommendation accuracy.
â€¢Our proposed GBSR consistently outperforms all baselines
under all experimental settings. Specifically, GBSR improves
the strongest baseline ğ‘¤.ğ‘Ÿ.ğ‘¡ NDCG@20 by 17.06%, 10% and
11.27% on Douban-Book, Yelp, and Epinions datasets, respec-
tively. Compared with the backbone model, GBSR achieves
impressive superiority over three benchmarks. These in-
dicate that our proposed GBSR can significantly improve
graph-based social recommendations, demonstrating the ef-
fectiveness of graph bottleneck learning to reduce redun-
dant social relations. Compared with other social denoising
methods, our GBSR can better obtain the trade-off between
removing social relations and recommendation tasks.
4.2.2 Ablation study. We conduct ablation studies on three datasets
to explore the effectiveness of each component of the proposed
GBSR framework. As shown in Table 4, we compare GBSR with
corresponding variants on Top-20 recommendation performances.
GBSR -w/o HSIC denotes that remove the HSIC-based bottleneck
regularization of GBSR , we only keep preference-guided social
denoising module. From Table 4, we can find that GBSR-w/o HSIC
performs worse in all cases, even worse than the backbone model.
3858Graph Bottlenecked Social Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 3: Performance comparisons of GBSR on different backbones.
Douban-Bo
ok Y
elp EpinionsMo
delsRe
call@20 NDCG@20 Re
call@20 NDCG@20 Re
call@20 NDCG@20
GraphRe
c 0.1453 0.1237 0.1077 0.0607 0.0681 0.0387
+GBSR 0.1510(
+3.92%) 0.1306(
+5.58%) 0.1136(
+5.48%) 0.0662(
+9.06%) 0.0706(
+3.67%) 0.0403(
+4.13%)
DiffNet++ 0.1489 0.1270 0.1114 0.0640 0.0727 0.0406
+GBSR 0.1545(
+3.76%) 0.1334(
+5.04%) 0.1224(
+9.87%) 0.0721(
+12.66%) 0.0790+(
+8.67%) 0.0451(
+11.08%)
So
cialLGN 0.1537 0.1274 0.1059 0.0620 0.0634 0.0371
+GBSR 0.1591(
+3.51%) 0.1353(
+6.20%) 0.1152(
+8.78%) 0.0675(
+8.87%) 0.0675(
+6.47%) 0.0399(
+7.55%)
LightGCN-S 0.1506 0.1281 0.1126 0.0651 0.0716 0.0417
+GBSR 0.1694(
+12.48%) 0.1523(
+18.89%) 0.1243(
+10.39%) 0.0724(
+11.21%) 0.0793(
+10.75%) 0.0464(
+11.27%)
Table 4: Ablation study on three datasets.
Douban-Bo
ok Y
elp EpinionsMo
delsR@20 N@20 R@20 N@20 R@20 N@20
LightGCN-S 0.1506 0.1281 0.1126 0.0651 0.0716 0.0417
GBSR -w/o
HSIC 0.1482 0.1259 0.1119 0.0644 0.0688 0.0388
GBSR 0.1694 0.1523 0.1243 0.0724 0.0793 0.0464
This indicates that simple social structure learning without HSIC-
based bottleneck regularization is useless for recommendation tasks.
Furthermore, under the constraint of the information bottleneck
principle, the learned social structure is meaningful, which can
effectively improve social recommendations on three datasets.
4.2.3 Generality study of GBSR .As we mentioned in the model
discussion, the proposed GBSR is a model-agnostic social denois-
ing framework. To better illustrate the generality of GBSR , we
conduct experiments of GBSR on several graph-based social rec-
ommendation backbones. As shown in Table 3, we implement
GBSR under four backbones, including GraphRec [ 8], DiffNet++[ 54],
SocialLGN [ 25], and LightGCN-S, and report their performances
of Top-20 recommendation task. From Table 3, we observe that
GBSR consistently outperforms each backbone by a large margin.
For example, on the Yelp dataset, GBSR achieves 9.06%, 12.66%,
8.87%, and 11.21% improvements of NDCG@20 compared with
GraphRec, DiffNet++, SocialLGN, and LightGCN-S, respectively.
Similarly, GBSR also obtains 5.48%, 9.87%, 8.78%, and 10.39% im-
provements on the Recall@20 metric. Extensive experimental re-
sults show that our proposed GBSR has a good generalization ability,
which can easily coupled with current graph-based social recom-
mendation methods and further enhancement.
4.3 Investigation of GBSR
In this section, we further analyze GBSR from the following aspects:
training efficiency, visualization of the denoised social graphs, and
hyper-parameter sensitivity analysis.
4.3.1 Training efficiency of GBSR. To analyze the training effi-
ciency of GBSR , we compare the convergence speed of GBSR and
corresponding backbone (LightGCN-S). As shown in Figure 3, we
compare the convergence process of both models. As the space
limit, we only present the convergence process on Douban-Book
and Yelp datasets. We set gcn layer to 3 and keep all experimental
settings the same. According to these figures, we can observe that
0 100 200 300 400 500 600
Epoch0.040.060.080.100.120.140.16NDCG@20
Douban-Book
LightGCN-S
GBSR6.97 (s)/epoch
3.61 (s)/epoch
0 50 100 150 200 250 300
Epoch0.020.040.060.080.10NDCG@20
Yelp
LightGCN-S
GBSR6.84 (s)/epoch
3.08 (s)/epochFigure 3: Convergence curves of GBSR and LightGCN-S on
Douban-Book and Yelp datasets.
GBSR converges much faster than the backbone model. Particularly,
GBSR reaches the best performances at the 82ğ‘¡â„, the 67ğ‘¡â„epoch on
Douban-Book and Yelp datasets. In contrast, LightGCN-S obtains
the best results on 509ğ‘¡â„, and 261ğ‘¡â„epoch, respectively. Empiri-
cal evidence shows that GBSR convergence 2-3 times faster than
LightGCN-S.
4.3.2 Visualization and statistics of the denoised social graphs. Here
we first present the visualization of the denoised social graph. As
shown in Figure 4(a), we present the sampled ego-network from
Douban-Book datasets. The red node denotes the center user of this
ego-network, and the blue nodes denote social neighbors. The depth
of the node color denotes the probability of edge dropping, where
the darker the color, the lower the dropping probability. We can
observe that user social neighbors perform different confidences of
social relations. Besides, we analyze the statistics of the denoised
social graphs. As shown in Figure 4(b), we plot the mean and vari-
ance values of social relation confidence on three datasets. We can
observe that Douban-Book presents the lowest mean value of so-
cial confidence, which means that it has the most social noise over
the three datasets. This also explains the results of Figure 1 that
graph-based social recommendations show a performance decrease
compared with LightGCN on the Douban-Book dataset. These re-
sults demonstrate that the proposed GBSR can effectively refine the
observed social graph via information bottleneck, which provides
informative social structures to enhance social recommendations.
4.3.3 Parameter Sensitivity Analysis. In this part, we analyze the
impact of different hyper-parameters of GBSR . There are two key
parameters, bottleneck loss coefficient ğ›½and RBF sharpness pa-
rameterğœ2. As both parameters determine the scale of bottleneck
3859KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yonghui Yang et al.
(a) Ego-network (from Douban-
Book)
Douban-Book Yelp Epinions0.60.81.01.21.4Social confidence(b) Errorbar of social relation confi-
dence
Figure 4: Visualization and statistics of the denoised social
graphs.
loss, we combine them to analyze the influence of recommenda-
tion results. As shown in Figure 5, we conduct careful grid-search
of(ğ›½,ğœ2)on three datasets. We can observe that GBSR reaches
the best performance when ğ›½=40,ğœ2=2.5on Douban-Book,
ğ›½=2.0,ğœ2=0.25on Yelp, and ğ›½=3.0,ğœ2=0.25, respectively.
5 RELATED WORKS
5.1 Graph-based Social Recommendation
With the emergence of social media, social recommendation has
been an important technique and has attracted more and more
research attention [ 19,23,26,27,44]. Following the social ho-
mophily [ 30] and social influence theory [ 29], social recommenda-
tions are devoted to characterizing social relation effects on user
preferences. Early efforts exploit social relations in a shallow form,
such as co-factorization methods [ 23,26] and regularization-based
methods [ 19,21,27]. For example, SoRec [ 26] jointly co-factorize the
interaction and social matrices and then project interaction and so-
cial contexts into the same semantic space. [ 27] designs a social reg-
ularization term that assumes two socially connected users should
be closer in preference space. Recently, with the great success of
graph neural networks [ 22,47], graph-based social recommenda-
tions have been widely researched and achieved impressive pro-
cess [ 8,25,54,55,62,67]. By formulating user-user social relations
as a graph, graph-based social recommendations inject high-order
social influences into user preference learning, vibrant the repre-
sentation ability. For example, DiffNet models the high-order social
influence diffusion process to enhance user representation [ 55], and
DiffNet++ further improves it by combining both social influence
diffusion and user-item interest propagation with a hierarchical at-
tention mechanism [ 54]. Inspired by the architecture of LightGCN,
[25] proposes SocialLGN to model user interaction and social be-
haviors. Instead of learning social graphs on Euclidean space, some
works attempt to introduce hyperbolic learning for graph-based
social recommendations [ 50,62]. Despite the effectiveness of mod-
eling high-order social influence to improve recommendation, these
works are built on the clean social relation assumption. However,
social graphs are inevitably noisy with redundant relations, and
these graph-based social recommendation methods are usually far
from satisfactory. Instead of directly using the original social graph,in this work, we propose a graph noising framework to improve
social recommendation.
5.2 Recommendation Denoising
Recommendation denoising works mainly focus on implicit feed-
back, which aims to refine implicit feedback to build robust rec-
ommender systems [ 11,17,51,52,61]. Most efforts are devoted
to removing noise feedback, which is easily vulnerable to usersâ€™
unconscious behaviors and various biases. For example, [ 51] pro-
poses to drop noisy feedback based on the observation that noisy
feedback has higher training loss, [ 52] devises a bi-level optimiza-
tion method to implement recommendation denoising. Besides,
graph augmentation methods are proposed to realize recommenda-
tion denoising [ 9,61]. Different from the above feedback-based
denoising works, we focus on social denoising for recommen-
dations. Social graphs are inevitably noisy with redundant rela-
tions, including unreliable relations and low preference-affinity
relations [ 35,43]. Early works employ statistics to identify unstable
social relations [ 27,33], or model different user influences with
attention mechanism [ 42,54]. Besides, fine-grained social leverag-
ing [ 10] and adversarial learning based methods have been pro-
posed [ 64,66]. Recently, GDMSR [ 35] proposes a distilled social
graph based on progressive preference-guided social denoising.
Nevertheless, the above methods still face the challenge of lack-
ing ground-truth. Whether rule-based or assumption-based social
denoising is hard to guarantee the trade-off between social denois-
ing and social recommendation. Distinguished by these denoising
methods, we address the social denoising recommendation from a
novel information bottleneck perspective, which seeks the denoised
yet informative social structure to enhance recommendations.
5.3 Information Bottleneck and Applications
Information Bottleneck (IB) is an effective representation learning
principle in machine learning tasks, that the optimal representation
should satisfy the minimal yet efficient manner [ 45,46]. In the era
of deep learning, calculating high-dimensional variablesâ€™ mutual
information (MI) is the key challenge for IB. The general solution is
estimating the upper/lower bounds instead of directly calculating
mutual information [ 1,7]. Specifically, VIB [ 1] leverages the vari-
ational technique to estimate the bounds of mutual information.
Besides, MINE [ 3], InfoNCE [ 32] are proposed to estimate the lower
bound of MI. In contrast, a few attempts propose to estimate the
upper bound of MI [ 7,18]. Besides optimizing the bounds of MI,
HSIC-based methods [ 28,53] are proposed to implement IB learning,
which employs the Hilbert-Schmidt Independence Criterion (HSIC)
to replace mutual information for optimization. HSIC measures the
independence of two variables, which can approximate the mutual
information objective [ 12]. IB principle has been successfully ap-
plied to many applications, such as image classification [ 49], text
understanding [ 34], and graph learning [ 59]. In this work, we in-
troduce the HSIC-based bottleneck to the graph-denoised social
recommendation, aiming to filtering redundant social relations for
robust recommendation.
3860Graph Bottlenecked Social Recommendation KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
25 30 35 40 45
1.52.02.53.03.52
0.14970.15080.15090.15010.1491
0.15020.15140.15180.15110.1500
0.15020.15110.15220.15200.1506
0.15000.15160.15230.15170.1512
0.14940.15150.15190.15140.1512
0.14950.15000.15050.15100.15150.1520
(a) Douban-Book dataset
1.0 2.0 3.0 4.0 5.0
0.200.250.300.350.402
0.06980.07050.07230.07160.0717
0.07160.07240.07170.07150.0714
0.07190.07170.07120.07100.0699
0.07160.07130.07080.07010.0698
0.07070.07060.07000.07000.0694
0.06950.07000.07050.07100.07150.0720 (b) Yelp dataset
/uni00000014/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000013 /uni00000016/uni00000011/uni00000013 /uni00000017/uni00000011/uni00000013 /uni00000018/uni00000011/uni00000013
/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000013/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000011/uni00000017/uni000000132
/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000019/uni00000013/uni00000011/uni00000013/uni00000017/uni00000019/uni00000013/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni0000001c/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni0000001b
/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni0000001b/uni00000013/uni00000011/uni00000013/uni00000017/uni00000019/uni00000014/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni0000001a/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000017/uni00000019/uni00000013/uni00000013/uni00000011/uni00000013/uni00000017/uni00000019/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni0000001c/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000016
/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000019/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni0000001b/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000017/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000014/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000016/uni0000001c
/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni00000013/uni00000013/uni00000011/uni00000013/uni00000017/uni00000017/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000011/uni00000013/uni00000017/uni00000019/uni00000013 (c) Epinions dataset
Figure 5: Performance comparisons under different parameters (ğ›½,ğœ2).
6 CONCLUSION
In this paper, we investigate graph-denoised social recommenda-
tions and propose a novel Graph Bottlenecked Social Recommen-
dation (GBSR) framework. Specifically, GBSR aims to learn the de-
noised yet informative social structure for recommendation tasks.
To achieve this goal, we first design preference-guided social de-
noising, then optimize the denoising process via the information
bottleneck principle. Particularly, we derive the lower bound of
mutual information maximization and introduce HSIC regulariza-
tion to replace mutual information minimization. Extensive experi-
ments conducted on three benchmarks demonstrate the effective-
ness of our proposed GBSR framework, i.e., over 10% improvements
on Top-20 Recommendation. Moreover, GBSR is a model-agnostic
framework, which can be flexibly coupled with various graph-based
social recommenders. In the future, we will explore more potential
of leveraging the IB principle to recommendation tasks, i.e., self-
supervised recommendation, fairness-aware recommendation, and
LLM-enhanced recommendation.
ACKNOWLEDGMENTS
This work was supported in part by grants from the National Key Re-
search and Development Program of China( Grant No.2021ZD0111802),
and the National Natural Science Foundation of China( Grant No.
U23B2031, 721881011).
REFERENCES
[1]Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. 2017. Deep
variational information bottleneck. In ICLR.
[2]Seojin Bang, Pengtao Xie, Heewook Lee, Wei Wu, and Eric Xing. 2021. Explaining
a black-box by using a deep variational information bottleneck approach. In AAAI,
Vol. 35. 11396â€“11404.
[3]Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua
Bengio, Aaron Courville, and Devon Hjelm. 2018. Mutual information neural
estimation. In ICML. PMLR, 531â€“540.
[4]Miaomiao Cai, Lei Chen, Yifan Wang, Haoyue Bai, Peijie Sun, Le Wu, Min Zhang,
and Meng Wang. 2024. Popularity-Aware Alignment and Contrast for Mitigating
Popularity Bias. arXiv preprint arXiv:2405.20718 (2024).
[5]Miaomiao Cai, Min Hou, Lei Chen, Le Wu, Haoyue Bai, Yong Li, and Meng Wang.
2024. Mitigating Recommendation Biases via Group-Alignment and Global-
Uniformity in Representation Learning. ACM Transactions on Intelligent Systems
and Technology (2024).
[6]Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun
Zhou, and Meng Wang. 2023. Improving recommendation fairness via data
augmentation. In WWW. 1012â€“1020.[7]Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and Lawrence
Carin. 2020. Club: A contrastive log-ratio upper bound of mutual information. In
ICML. PMLR, 1779â€“1788.
[8]Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin.
2019. Graph neural networks for social recommendation. In WWW. 417â€“426.
[9]Ziwei Fan, Ke Xu, Zhang Dong, Hao Peng, Jiawei Zhang, and Philip S Yu. 2023.
Graph collaborative signals denoising and augmentation for recommendation.
InSIGIR. 2037â€“2041.
[10] Bairan Fu, Wenming Zhang, Guangneng Hu, Xinyu Dai, Shujian Huang, and
Jiajun Chen. 2021. Dual side deep context-aware modulation for social recom-
mendation. In WWW. 2524â€“2534.
[11] Yunjun Gao, Yuntao Du, Yujia Hu, Lu Chen, Xinjun Zhu, Ziquan Fang, and Baihua
Zheng. 2022. Self-guided learning to denoise for robust recommendation. In
SIGIR. 1412â€“1422.
[12] Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard SchÃ¶lkopf. 2005.
Measuring statistical dependence with Hilbert-Schmidt norms. In International
conference on algorithmic learning theory. Springer, 63â€“77.
[13] Adam Gronowski, William Paul, Fady Alajaji, Bahman Gharesifard, and Philippe
Burlina. 2023. Classification utility, fairness, and compactness via tunable in-
formation bottleneck and RÃ©nyi measures. IEEE Transactions on Information
Forensics and Security (2023).
[14] Asela Gunawardana and Guy Shani. 2009. A survey of accuracy evaluation
metrics of recommendation tasks. Journal of Machine Learning Research 10, 12
(2009).
[15] Guibing Guo, Jie Zhang, and Neil Yorke-Smith. 2015. Trustsvd: Collaborative
filtering with both the explicit and implicit influence of user trust and of item
ratings. In AAAI, Vol. 29.
[16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng
Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for
recommendation. In SIGIR. 639â€“648.
[17] Zhuangzhuang He, Yifan Wang, Yonghui Yang, Peijie Sun, Le Wu, Haoyue Bai,
Jinqi Gong, Richang Hong, and Min Zhang. 2024. Double Correction Framework
for Denoising Recommendation. arXiv preprint arXiv:2405.11272 (2024).
[18] Michal HledÃ­k, Thomas R Sokolowski, and GaÅ¡per TkaÄik. 2019. A tight upper
bound on mutual information. In 2019 IEEE ITW. IEEE, 1â€“5.
[19] Mohsen Jamali and Martin Ester. 2010. A matrix factorization technique with
trust propagation for recommendation in social networks. In Recsys. 135â€“142.
[20] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization
with gumbel-softmax. ICLR (2017).
[21] Meng Jiang, Peng Cui, Fei Wang, Wenwu Zhu, and Shiqiang Yang. 2014. Scalable
recommendation with social contextual information. IEEE TKDE 26, 11 (2014),
2789â€“2802.
[22] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph
convolutional networks. In ICLR.
[23] Ioannis Konstas, Vassilios Stathopoulos, and Joemon M Jose. 2009. On social
networks and collaborative recommendation. In SIGIR. 195â€“202.
[24] Walid Krichene and Steffen Rendle. 2020. On sampled metrics for item recom-
mendation. In SIGKDD. 1748â€“1757.
[25] Jie Liao, Wei Zhou, Fengji Luo, Junhao Wen, Min Gao, Xiuhua Li, and Jun Zeng.
2022. SocialLGN: Light graph convolution network for social recommendation.
Information Sciences 589 (2022), 595â€“607.
[26] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. 2008. Sorec: social
recommendation using probabilistic matrix factorization. In CIKM. 931â€“940.
[27] Hao Ma, Dengyong Zhou, Chao Liu, Michael R Lyu, and Irwin King. 2011. Rec-
ommender systems with social regularization. In WSDM. 287â€“296.
3861KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Yonghui Yang et al.
[28] Wan-Duo Kurt Ma, JP Lewis, and W Bastiaan Kleijn. 2020. The HSIC bottleneck:
Deep learning without back-propagation. In AAAI, Vol. 34. 5085â€“5092.
[29] Peter V Marsden and Noah E Friedkin. 1993. Network studies of social influence.
Sociological Methods & Research 22, 1 (1993), 127â€“151.
[30] Miller McPherson, Lynn Smith-Lovin, and James M Cook. 2001. Birds of a feather:
Homophily in social networks. Annual review of sociology 27, 1 (2001), 415â€“444.
[31] Andriy Mnih and Russ R Salakhutdinov. 2007. Probabilistic matrix factorization.
NeurIPS 20 (2007).
[32] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[33] Yiteng Pan, Fazhi He, and Haiping Yu. 2020. A correlative denoising autoencoder
to model social influence for top-N recommender system. Frontiers of Computer
science 14 (2020), 1â€“13.
[34] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Haupt-
mann, Joao Henriques, and Andrea Vedaldi. 2021. Support-set bottlenecks for
video-text representation learning. ICLR (2021).
[35] Yuhan Quan, Jingtao Ding, Chen Gao, Lingling Yi, Depeng Jin, and Yong Li. 2023.
Robust Preference-Guided Denoising for Graph based Social Recommendation.
InWWW. 1097â€“1108.
[36] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI. 452â€“
461.
[37] Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky,
Brendan D Tracey, and David D Cox. 2019. On the information bottleneck theory
of deep learning. Journal of Statistical Mechanics: Theory and Experiment 2019,
12 (2019), 124020.
[38] Pengyang Shao, Le Wu, Lei Chen, Kun Zhang, and Meng Wang. 2022. FairCF:
Fairness-aware collaborative filtering. Science China Information Sciences 65, 12
(2022), 222102.
[39] Pengyang Shao, Le Wu, Kun Zhang, Defu Lian, Richang Hong, Yong Li, and
Meng Wang. 2024. Average User-side Counterfactual Fairness for Collaborative
Filtering. ACM TOIS (2024).
[40] Alexander Shapiro. 2003. Monte Carlo sampling methods. Handbooks in operations
research and management science 10 (2003), 353â€“425.
[41] Harald Steck. 2013. Evaluation of recommendations: rating-prediction and rank-
ing. In RecSys. 213â€“220.
[42] Peijie Sun, Le Wu, and Meng Wang. 2018. Attentive recurrent social recommen-
dation. In SIGIR. 185â€“194.
[43] Youchen Sun. 2023. Denoising Explicit Social Signals for Robust Recommendation.
InRecSys. 1344â€“1348.
[44] Jiliang Tang, Xia Hu, and Huan Liu. 2013. Social recommendation: a review.
Social Network Analysis and Mining 3 (2013), 1113â€“1133.
[45] Naftali Tishby, Fernando C Pereira, and William Bialek. 2000. The information
bottleneck method. arXiv preprint physics/0004057 (2000).
[46] Naftali Tishby and Noga Zaslavsky. 2015. Deep learning and the information
bottleneck principle. In 2015 IEEE ITW. IEEE, 1â€“5.
[47] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2018. Graph attention networks. In ICLR.
[48] Jean-Philippe Vert, Koji Tsuda, and Bernhard SchÃ¶lkopf. 2004. A primer on kernel
methods. Kernel methods in computational biology 47 (2004), 35â€“70.
[49] Bowen Wang, Liangzhi Li, Yuta Nakashima, and Hajime Nagahara. 2023. Learning
Bottleneck Concepts in Image Classification. In CVPR. 10962â€“10971.
[50] Hao Wang, Defu Lian, Hanghang Tong, Qi Liu, Zhenya Huang, and Enhong
Chen. 2021. Hypersorec: Exploiting hyperbolic user and item representationswith multiple aspects for social-aware recommendation. TOIS 40, 2 (2021), 1â€“28.
[51] Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021.
Denoising implicit feedback for recommendation. In WSDM. 373â€“381.
[52] Zongwei Wang, Min Gao, Wentao Li, Junliang Yu, Linxin Guo, and Hongzhi Yin.
2023. Efficient bi-level optimization for recommendation denoising. In SIGKDD.
2502â€“2511.
[53] Zifeng Wang, Tong Jian, Aria Masoomi, Stratis Ioannidis, and Jennifer Dy. 2021.
Revisiting hilbert-schmidt information bottleneck for adversarial robustness.
NeurIPS 34 (2021), 586â€“597.
[54] Le Wu, Junwei Li, Peijie Sun, Richang Hong, Yong Ge, and Meng Wang. 2020.
Diffnet++: A neural influence and interest diffusion network for social recom-
mendation. TKDE (2020).
[55] Le Wu, Peijie Sun, Yanjie Fu, Richang Hong, Xiting Wang, and Meng Wang. 2019.
A neural influence diffusion model for social recommendation. In SIGIR. 235â€“244.
[56] Le Wu, Yonghui Yang, Lei Chen, Defu Lian, Richang Hong, and Meng Wang.
2020. Learning to transfer graph embeddings for inductive graph based recom-
mendation. In SIGIR. 1211â€“1220.
[57] Le Wu, Yonghui Yang, Kun Zhang, Richang Hong, Yanjie Fu, and Meng Wang.
2020. Joint item recommendation and attribute inference: An adaptive graph
convolutional network approach. In SIGIR. 679â€“688.
[58] Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2022. Graph neural
networks in recommender systems: a survey. Comput. Surveys 55, 5 (2022), 1â€“37.
[59] Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. 2020. Graph information
bottleneck. NeurIPS 33 (2020), 20437â€“20448.
[60] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and
S Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE
TNNLS 32, 1 (2020), 4â€“24.
[61] Yonghui Yang, Le Wu, Richang Hong, Kun Zhang, and Meng Wang. 2021. En-
hanced graph learning for collaborative filtering via mutual information maxi-
mization. In SIGIR. 71â€“80.
[62] Yonghui Yang, Le Wu, Kun Zhang, Richang Hong, Hailin Zhou, Zhiqiang Zhang,
Jun Zhou, and Meng Wang. 2023. Hyperbolic Graph Learning for Social Recom-
mendation. IEEE TKDE (2023).
[63] Yonghui Yang, Zhengwei Wu, Le Wu, Kun Zhang, Richang Hong, Zhiqiang Zhang,
Jun Zhou, and Meng Wang. 2023. Generative-contrastive graph learning for
recommendation. In SIGIR. 1117â€“1126.
[64] Junliang Yu, Min Gao, Hongzhi Yin, Jundong Li, Chongming Gao, and Qinyong
Wang. 2019. Generating reliable friends via adversarial training to improve social
recommendation. In ICDE. IEEE, 768â€“777.
[65] Junliang Yu, Hongzhi Yin, Min Gao, Xin Xia, Xiangliang Zhang, and Nguyen Quoc
Viet Hung. 2021. Socially-aware self-supervised tri-training for recommendation.
InSIGKDD. 2084â€“2092.
[66] Junliang Yu, Hongzhi Yin, Jundong Li, Min Gao, Zi Huang, and Lizhen Cui.
2020. Enhancing social recommendation with adversarial graph convolutional
networks. IEEE TKDE 34, 8 (2020), 3727â€“3739.
[67] Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet Hung,
and Xiangliang Zhang. 2021. Self-supervised multi-channel hypergraph convolu-
tional network for social recommendation. In WWW. 413â€“424.
[68] Wayne Xin Zhao, Junhua Chen, Pengfei Wang, Qi Gu, and Ji-Rong Wen. 2020.
Revisiting alternative experimental settings for evaluating top-n item recommen-
dation algorithms. In CIKM. 2329â€“2332.
[69] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu,
Haifeng Chen, and Wei Wang. 2020. Robust graph representation learning via
neural sparsification. In ICML. PMLR, 11458â€“11468.
3862