TrajRecovery: An Efficient Vehicle Trajectory Recovery
Framework based on Urban-Scale Traffic Camera Records
Dongen Wu
Zhejiang University
Hangzhou, China
wude@zju.edu.cnZiquan Fang
Zhejiang University
Hangzhou, China
zqfang@zju.edu.cnQichen Sun
Zhejiang University
Hangzhou, China
sunqichen@zju.edu.cn
Lu Chen
Zhejiang University
Hangzhou, China
luchen@zju.edu.cnHaiyang Hu
Huawei Cloud Computing
Technologies Co., Ltd
Hangzhou, China
huhaiyang2@huawei.comFei Wang
Huawei Cloud Computing
Technologies Co., Ltd
Hangzhou, China
wangfei1@huawei.com
Yunjun Gao
Zhejiang University
Hangzhou, China
gaoyj@zju.edu.cn
ABSTRACT
Accurate vehicle trajectory recovery enables providing indispens-
able data foundations in intelligent urban transportation. However,
existing methods face two challenges: i) the inability to process
city-wide vehicle trajectories, and ii) the dependence on a substan-
tial amount of accurate GPS trajectories for model training, leading
to poor generalization ability. To address these issues, we propose
a novel trajectory recovery system based on vehicle snapshots
captured by traffic cameras, named TrajRecovery. TrajRecovery
consists of three main components: i) Preprocessor processes traf-
fic cameras and vehicle snapshots to provide necessary data for
trajectory recovery; ii) Spatial Transfer Probabilistic Model (STPM)
integrates road conditions and driver behavior to compute turning
probability at intersections; iii) Trajectory Generator utilizes the
output probabilities from STPM to recover a continuous and most
likely complete trajectory. We evaluate TrajRecovery on two real
datasets from a city in China, demonstrating substantial perfor-
mance gains compared to state-of-the-art methods. Furthermore,
our system is deployed in practical applications at Huawei Com-
pany, achieving extraordinary profits in business scenarios.
CCS CONCEPTS
â€¢Information systems â†’Spatial-temporal systems .
KEYWORDS
vehicle trajectory recovery, deep learning, mobile sensing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671558ACM Reference Format:
Dongen Wu, Ziquan Fang, Qichen Sun, Lu Chen, Haiyang Hu, Fei Wang,
and Yunjun Gao. 2024. TrajRecovery: An Efficient Vehicle Trajectory Recov-
ery Framework based on Urban-Scale Traffic Camera Records. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Min-
ing (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3637528.3671558
1 INTRODUCTION
The proliferation of positioning devices enables the generation
of vehicle trajectories, which helps in understanding traffic sys-
tems [ 41]. Based on the collected trajectories, researchers perform
various traffic analyses such as traffic prediction [ 3,19], travel time
estimation [ 4,37,40], movement pattern mining [ 5,6,22,36,43],
etc. However, in real-world applications, the raw trajectory data
typically have low sampling rates [38]. For example, empty taxis
tend to lower the sampling rates of GPS devices to save battery
consumption. Complex urban environments like tall buildings or
network transmission interruptions can also result in low-sampling
trajectories. To provide a good-quality data foundation for trajec-
tory analytics, trajectory recovery is widely studied, where the
general goal is to recover accurate trajectory data based on the raw
and low-sampling trajectory data.
Due to its great benefits, the data mining community has pro-
posed many trajectory recovery methods [2, 15, 24, 30, 33], which
can refer to Section 6. Specifically, they target GPS-based trajec-
tory recovery. However, GPS devices and that-based trajectory
data are only available to public transportation such as taxis and
buses, rather than all vehicles [ 1]. Considering the relatively low
installation rate of GPS devices [ 1,34] and user privacy protection
laws, obtaining GPS-based trajectories for most private vehicles is
scarcely possible. Consequently, most of the existing studies are
limited to general vehicle trajectory recovery.
In this paper, we aim to propose a new trajectory recovery
method that does not rely on GPS trajectories but enables general
vehicle trajectory recovery. We found that, in urban areas, traffic
5979
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dongen Wu, et al.
Time: 2023/05/02 19:12:39
Camera ID: A1
Time: 2023/05/02 19:21:16Camera ID: C1Time: 2023/05/02 20:57:42
Camera ID: D1
Time: 2023/05/02 19:26:17
Camera ID: A2ID:  A1Locations: (25.55001,119.75324)Name: zhong shan da dao north
ID:  C1Locations: (0, 0)
Name: hai tan lu yuan eastID:  B1
Locations: (0, 0)
Name: None
ID:  A2Locations: (27.46163,120.30440)
Name: None
Time: 2023/05/02 19:32:36
Camera ID: C2ID:  C2Locations: (25.50079,119.77782)Name: None
Mismatch
Out of scope
network 
malfunctions
Time: 2023/05/02 19:12:39
Camera ID: A1
Time: 2023/05/02 19:21:16
Camera ID: C1Time: 2023/05/02 20:57:42
Camera ID: D1
Time: 2023/05/02 19:26:17
Camera ID: A2ID:  A1Locations: (25.55001,119.75324)Name: zhong shan da dao north
ID:  C1Locations: (0, 0)
Name: hai tan lu yuan eastID:  B1
Locations: (0, 0)
Name: None
ID:  A2
Locations: (27.46163,120.30440)
Name: None
Time: 2023/05/02 19:32:36
Camera ID: C2ID:  C2Locations: (25.50079,119.77782)Name: None
ID:  A1
Locations: (25.55001,119.75324)Name: Zhong Shan Avenue north
Time: 2023/05/02 19:12:39Camera ID: A1
ID:  A2Locations: (25.53000,119.74810)
Name: None
Time: 2023/05/02 20:57:42
Camera ID: A2
ID:  C1
Locations: (0, 0)
Name: Hai Tan Lu Yuan East
Time: 2023/05/02 19:21:16
Camera ID: C1
ID:  B1
Locations: (0, 0)
Name: None
Time: 2023/05/02 19:32:36
Camera ID: B1
ID:  C2Locations: (25.50079,119.77782)Name: None
Time: 2023/05/02 19:32:36Camera ID: C2
network malfunctions
Mismatch1
2
3
4
5
ID:  A1
Locations:  (27.55001,130.75324)
Name:  City Avenue North
Time:  2023/05/02 14:12:39
Camera ID:  A1
ID:  A2
Locations:  (27.53000,130.74810)
Name:  None
Time:  2023/05/02 18:57:42
Camera ID:  A2
ID:  B1
Locations:  (0, 0)
Name:  None
Time:  2023/05/02 14:32:36
Camera ID:  B1
ID:  C2
Locations:  (27.50079,130.77782)
Name:  None
Time:  2023/05/02 14:32:36
Camera ID:  C2
1
2
3
4
Mismatch
Wrong RecordRight Record
Figure 1: An Example of Sparse Vehicle Passing Records.
cameras operate 24/7, continuously capturing passing records of
all vehicles [ 12]. Combined with vehicle re-identification technolo-
gies [ 8,10,17,20,26], these traffic cameras present a promising
way for general vehicle recovery based on their passing records. In
contrast to GPS-based trajectory recovery studies, the passing (cam-
era) records-based methods [ 16,34] involve two main tasks, i.e.,
vehicle re-identification and trajectory recovery. However, it is non-
trivial to design such a passing record-based trajectory recovery
framework due to the following three challenges.
Challenge 1: How to utilize sparse vehicle passing records
for effective trajectory recovery? Although surveillance cameras
operate 24/7, the observed vehicle passing records are sparse due
to three aspects. i) First, not all road segments are covered by cam-
eras [ 1,28], making it challenging to predict vehicle path choices
in unobserved road segments. We have collected over 20 million
vehicle snapshots recorded by 491 traffic cameras in a city of China,
covering a continuous period of five days. Upon analyzing these
data, we find that traffic cameras only cover 7% of the road seg-
ments, leaving the remaining 93% unobservable. ii) Second, network
malfunctions, image deterioration, and camera mismatch may re-
sult in the partial failure of vehicle detection, further exacerbating
the sparsity problem. Fig. 1 shows an example. As observed, camera
B1 failed to be accurately matched to the road network, attributed
to the lack of location information and device names. Consequently,
the vehicle passing records captured by these cameras were invalid.
Furthermore, network malfunctions in camera A2 led to a temporal
deviation in the occurrence time of the second passing record, ren-
dering it invalid. It is important to note that, due to non-technical
reasons, all coordinates and road names in this paper have under-
gone offsetting or obfuscation. iii) Last but not least, despite the
broad distribution of traffic cameras, the distances between these
devices can still be long, making it difficult for trajectory recovery.
Overall, constructing an effective trajectory model using sparse
vehicle passing records is challenging.
Challenge 2: How to generalize learned knowledge to un-
observed road segments? Most existing GPS-trajectory-oriented
recovery methods typically require massive observed trajectory
data for model training. When the model encounters road segmentsnot encompassed in the training data, it encounters difficulties in re-
covery. However, in real-life scenarios, a notable proportion of road
segments, particularly in suburban areas, lack camera coverage. In
that case, how to generalize learned knowledge from observed road
segments to unobserved road segments is challenging.
Challenge 3: How to achieve personalized trajectory recovery
that considers the mobility preferences of different vehicles?
When performing trajectory recovery, previous methods tend to
treat all drivers equally from a coarse-grained perspective and thus
neglect the preference diversity in driving habits. This oversight
led to the recovery of identical trajectories for different vehicles
under similar contextual conditions. Therefore, we are inspired to
learn the driving habits of drivers from their historical behavior,
thereby achieving fine-grained trajectory recovery. Consequently,
how to learn the driverâ€™s driving habits to achieve personalized
trajectory recovery is challenging.
In this paper, we propose TrajRecovery, an efficient framework
for vehicle trajectory recovery utilizing traffic camera records. To
address Challenge 1, we construct a comprehensive preprocessing
pipeline to ensure the precise alignment of traffic cameras with road
networks. Furthermore, we introduce a simple yet highly effective
Historical Passing Record Retriever (HPRR) to retrieve comparable
passing records from the historical passing records, thereby enhanc-
ing the sampling rate of sparse passing records. To address Chal-
lenge 2 , we design a Spatial Transfer Probability Model (STPM) to
capture driver turning probability in camera-covered road segments
through vehicle snapshots. To extend this knowledge to areas with-
out camera coverage, we develop a Motion-GAT Module to capture
the intrinsic connections and features between road segments by
aggregating the characteristics of surrounding road segments. By
incorporating the Motion-GAT Module into STPM, we ensures that
STPM emphasizes the impact of road conditions on turning at the
urban level, rather than specific turns between road segments, thus
enabling more effective knowledge transfer. To address Challenge
3, we introduce the OD-Attention Module in STPM, which incorpo-
rates attention mechanisms into the retrieval of historical driving
behaviors. By utilizing driving habits, the OD-Attention Module
enables vehicle-level personalized trajectory recovery, avoiding the
same results for identical OD scenarios. Overall, this paper makes
the following contributions.
â€¢To the best of our knowledge, we present the first framework
for trajectory recovery that solely utilizes traffic camera
records instead of GPS data, offering comprehensive support
for a wide range of vehiclesâ€™ trajectory recovery.
â€¢To effectively generalize learned knowledge to unobserved
road segments, we develop a novel Motion-GAT Module to
emphasize the road conditions on turning at the urban level,
instead of specific turns between camera-covered segments.
â€¢To enable personalized trajectory recovery, we introduce a
novel OD-Attention to learn driversâ€™ different habits from
their historical driving behaviors.
â€¢Extensive experiments using two real datasets show that
TrajRecovery significantly outperforms state-of-the-art base-
lines, e.g., 2.6% - 12.2% improvement in recovery quality.
5980TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢Finally, the deployment of TrajRecovery in practical applica-
tions at HuaWei Company, China, showcasing outstanding
performance in real-world business scenarios.
The paper is organized as follows. Section 2 presents preliminar-
ies and a problem statement, and the proposed system is detailed
in Section 3. Experimental results are reported in Section 4, and the
practical deployment of TrajRecovery is introduced in Section 5.
Section 6 reviews related work and Section 7 concludes this paper.
2 PRELIMINARIES
2.1 Definitions
Definition 1 (Traffic Camera). The traffic camera set is denoted as
ğ¶={ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘}, whereğ‘is the total number of traffic cameras,
and each traffic camera is represented as ğ‘ğ‘–=<ğ¿ğ‘ğ‘¡,ğ¿ğ‘œğ‘›,ğ‘ğ‘ğ‘šğ‘’,ğ·ğ‘–ğ‘Ÿğ‘’ğ‘ >,
whereğ¿ğ‘ğ‘¡,ğ¿ğ‘œğ‘› andğ‘ğ‘ğ‘šğ‘’ represent the latitude, longitude and
name of the traffic camera respectively, and ğ·ğ‘–ğ‘Ÿğ‘’ğ‘ is the shoot-
ing direction of traffic cameras (e.g., south to north). Note that, as
mentioned in Challenge 1, these information is unrealiable.
Definition 2 (Road Network). The road network is an essential
component for trajectory recovery. A road network is a directed
graphğº=(ğ‘‰,ğ¸), whereğ‘‰={ğ‘£1,ğ‘£2,Â·Â·Â·,ğ‘£ğ‘€}refers to the set of
vertices representing road intersections, and ğ¸={ğ‘’1,ğ‘’2,Â·Â·Â·,ğ‘’ğ¿}
refers to the set of edges representing the road segment between
intersections. Each road segment ğ‘’âˆˆğ¸contains three types of
properties: 1) length, width, number of lanes, number of in-degrees
and out-degrees; 2) level, such as rural, provincial way, national
way, etc.; 3) type, such as tunnel, pedestrian, side road, etc.
Definition 3 (Vehicle Trajectory). We define a vehicle trajectory
as a sequence of time-ordered passing records Jğ‘=[ğ‘Ÿ1,ğ‘Ÿ2,Â·Â·Â·,ğ‘Ÿğ‘›]
for a vehicle over a continuous period. Each ğ‘Ÿğ‘–=<ğ‘,ğ‘¡,ğ‘’ >rep-
resents a passing record, indicating that vehicle ğ‘is detected by
camerağ‘Ÿğ‘–.ğ‘at timeğ‘Ÿğ‘–.ğ‘¡on road segment ğ‘Ÿğ‘–.ğ‘’, where camera ğ‘Ÿğ‘–.ğ‘is
located. Further details of camera matching can be found in Sec-
tion 3.1. Each passing record corresponds to a vehicle snapshot, we
denoteğ‘…as the set of all vehicle snapshots.
Definition 4 (OD-Pair). For each vehicle passing record ğ‘Ÿğ‘–, the
corresponding road segment ğ‘Ÿğ‘–.ğ‘’is denoted as ğ‘’ğ‘œ, and the adjacent
road segment the vehicle is approaching is designated as ğ‘’ğ‘‘. Notably,
ğ‘’ğ‘‘is inferred based on the identified vehicleâ€™s lane position and
turn information from snapshots and the road network ğº. Then, we
identify the corresponding OD-pairs for each passing record, and
form a OD-pair trajectory Jğ‘‚ğ·ğ‘=[ğ‘‚ğ·J
1,ğ‘‚ğ·J
2,Â·Â·Â·,ğ‘‚ğ·J
ğ‘›], where
ğ‘‚ğ·J
ğ‘–=<ğ‘¡ğ‘œ,ğ‘’ğ‘œ,ğ‘’ğ‘‘>represents a OD-pair, indicating that vehicle
ğ‘traveled from road segment ğ‘‚ğ·J
ğ‘–.ğ‘’ğ‘œto adjacent road segment
ğ‘‚ğ·J
ğ‘–.ğ‘’ğ‘‘at timeğ‘‚ğ·J
ğ‘–.ğ‘¡ğ‘œ, whereğ‘‚ğ·J
ğ‘–.ğ‘’ğ‘œisğ‘Ÿğ‘–.ğ‘’andğ‘‚ğ·J
ğ‘–.ğ‘¡ğ‘œisğ‘Ÿğ‘–.ğ‘¡.
Problem Statement. Given a traffic camera set ğ¶and the vehicle
snapshot data ğ‘…fromğ‘traffic cameras, our task is to recover the
sparse passing record trajectory into a continuous and complete
trajectory based on the road network ğº.
3 TRAJECTORY RECOVERY SYSTEM
To address the aforementioned challenges, as shown in Fig. 2, we in-
troduce a novel system named TrajRecovery, comprising three main
components: 1) the Preprocessor, 2) Spatial Transfer Probability
Model (STPM), and 3) Trajectory Generator.
Vehicle Clustering
Historical Passing 
Record Retriever
Spatial Temporal
Camera 
Map 
MatchingCamera Info.
 Road network
1
24
3Feature ExtractorMotion-GATOD-AttentionSpatial Transfer ProbabilitiesPostprocess
Trajectory Search
Dijkstra 
SearchGreedy 
Search
Recovery Trajectory
1
243
Preprocessor STPM Trajecotry Generator
àµ†log  ( Transfer Probabilities  )
Camera System
Vehicle Snapshots
Camera 
Map-Matching
Historical Passing 
Record Retriever
Historical Passing Record Retriever
ClusteringSTPM
Feature Extractor
Motion-GAT
OD-Attention
Historical Passing 
Record Retriever
Spatial Temporal
Camera 
Map 
MatchingCamera Info.Postprocess
Trajectory Search
Dijkstra 
SearchGreedy 
Search
Recovery TrajectoryPreprocessor Trajecotry Generator
àµ†ğ¥ğ¨ğ    ( Transfer Probabilities  )Vehicle Snapshots
Clustering
Road network
Feature ExtractorMotion-GATOD-AttentionSpatial Transfer ProbabilitiesSTPM
1
24
3
1
243
Figure 2: System architecture.
The Preprocessor consists of three steps. Initially, we match
traffic cameras with road segments on the road network based on
their latitude, longitude, name and direction. Drawing inspiration
from previous studies [ 34,35], we extract visual features and license
plates from vehiclesâ€™ passing snapshot records. Subsequently, we
cluster vehicles based on the visual feature similarity, obtaining all
passing records for each vehicle. Finally, we introduce the Historical
Passing Record Retriever (HPRR) module, enhancing the sampling
rate of sparse trajectories by retrieving historical passing records
for the current vehicle and other vehicles.
Additionally, we devise the Spatial Transfer Probability Model
(STPM) to calculate the probability of a vehicle transitioning from
one road segment to its subsequent segment. The Feature Extractor
generates deep representations based on road segment properties
such as length, width, type, and level for each input road segment.
Following that, the Motion-GAT module characterizes the traf-
fic development and vehicle suitability of a road segment based
on its relationships with surrounding segments. Considering that
even with the same origin-destination (OD), different driving habits
may generate distinct trajectories, we introduce the OD-Attention
module to mine driving habits from a vehicleâ€™s historical driving
behavior, achieving vehicle-level personalized trajectory recovery.
Finally, in the Trajectory Generator, we leverage the inference
probabilities from the STPM to recovery a continuous and proba-
bilistically optimal trajectory based on the road network.
3.1 Preprocessor
The Preprocessor encompasses traffic camera map matching, vehi-
cle snapshot processing and clustering, and upsampling.
Traffic camera map matching. Matching traffic cameras to
road segments on the road network is the foundation for trajectory
recovery from vehicle passing records. Initially, we categorize all
cameras into four classes based on their credibility derived from lat-
itude, longitude, and name information. Subsequently, we employ
distinct strategies to match cameras to corresponding intersections
under different situations: (1) Correct location and incorrect name:
match to the nearest intersection based on location; (2) Incorrect
location and correct name: utilize edit distance to identify the in-
tersection with the highest text similarity; (3) Correct location and
correct name: identify the top-k nearest intersections and calculate
text similarity with edit distance, selecting the intersection with the
highest text similarity; (4) Incorrect location and incorrect name:
5981KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dongen Wu, et al.
unmatchable. Once the corresponding intersection is identified,
we use the cameraâ€™s ğ·ğ‘–ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘–ğ‘œğ‘› to locate the road segment in the
current direction of the intersection. Consequently, the camera is
then associated with this road segment, represented as ğ‘ğ‘–â†’ğ‘’ğ‘—,
indicating the binding of camera ğ‘ğ‘–to road segment ğ‘’ğ‘—. This match-
ing strategy successfully matches 491 out of 602 cameras, covering
96% of passing records, ensuring the availability of these records.
Vehicle snapshot processing and clustering. When a vehicle
passes by, the traffic camera will capture all the content within
its field of view, including noise. Consequently, preprocessing the
snapshot is essential for accurate vehicle identification. Inspired
by advancements in object detection algorithm [ 29], we first lo-
cate the vehicle and lane (e.g. left-turn lane) from the snapshot
by the excellent detection ability of YOLOV7 [ 29], and extracting
256-dimensional vehicle appearance features ğ‘“ğ‘and vehicle plate
featuresğ‘“ğ‘. Inspired by [ 34], we then calculate the total similarity
between each pair of vehicle snapshots as:
ğ‘†ğ‘–,ğ‘—=ğ‘¤ğ‘ğ‘“ğ‘–ğ‘Â·ğ‘“ğ‘—
ğ‘+ğ‘¤ğ‘ğ‘“ğ‘–ğ‘Â·ğ‘“ğ‘—
ğ‘
ğ‘¤ğ‘+ğ‘¤ğ‘(1)
where the hyper-parameters ğ‘¤ğ‘,ğ‘¤ğ‘represent the weight of vehicle
appearance features and plate features respectively. Considering
that the plate is more recognizable than the appearance of the
vehicle, we assign ğ‘¤ğ‘to be larger than ğ‘¤ğ‘. Then we use the total
similarity of each pair of vehicle snapshots for clustering. If the
total similarity between them is greater than a predefined threshold
ğ‘‡â„, then they belong to the same cluster.
Upsampling. As mentioned in Section 1, the camera may oc-
casionally fail, resulting in vehicles passing by without obtaining
passing records. To address this, considering that peopleâ€™s travel
often has repeatability [ 33], we devised a Historical Passing Record
Retriever to improve the sampling rate of passing records by re-
trieving the vehicleâ€™s historical passing records before performing
trajectory recovery. Specifically, in order to find a trajectory similar
to the current trajectory from the historical passing records, we
consider both spatial and temporal aspects. From a spatial perspec-
tive, for any two consecutive passing records ğ‘Ÿğ‘–,ğ‘Ÿğ‘–+1in the vehi-
cle trajectoryJğ‘, we calculate the longest common subsequence
(LCSS) between the subtrajectory with a context window of size ğ‘˜
[ğ‘Ÿğ‘–âˆ’ğ‘˜.ğ‘,Â·Â·Â·,ğ‘Ÿğ‘–.ğ‘,ğ‘Ÿğ‘–+1.ğ‘,Â·Â·Â·,ğ‘Ÿğ‘–+1+ğ‘˜.ğ‘]and any sub-trajectory in his-
torical trajectory with a context window of size ğ‘˜. By this way, we
can obtain the top- ğ‘historical sub-trajectories with the highest spa-
tial similarity. From a temporal perspective, we sort the top- ğ‘histori-
cal subtrajectories in ascending order by |(ğ‘Ÿğ‘.ğ‘¡âˆ’ğ‘Ÿğ‘.ğ‘¡)âˆ’(ğ‘Ÿğ‘–+1.ğ‘¡âˆ’ğ‘Ÿğ‘–.ğ‘¡)|,
whereğ‘Ÿğ‘,ğ‘Ÿğ‘represent the historical passing records of cameras ğ‘Ÿğ‘–.ğ‘
andğ‘Ÿğ‘–+1.ğ‘in the historical subtrajectory, and ğ‘Ÿğ‘.ğ‘¡âˆ’ğ‘Ÿğ‘.ğ‘¡>0. Then
we obtained the top- ğ‘ subtrajectories with the highest temporal
similarity from the top- ğ‘. Finally, we identify the most frequent
camerağ‘ğ‘šand average travel time ğ‘¡ğ‘šfromğ‘ğ‘štoğ‘Ÿğ‘–+1.ğ‘between
camerağ‘Ÿğ‘–.ğ‘and camera ğ‘Ÿğ‘–+1.ğ‘in the subtrajectories in top- ğ‘ , and
insert the passing record <ğ‘ğ‘š,ğ‘Ÿğ‘–.ğ‘¡+ğ‘¡ğ‘š>betweenğ‘Ÿğ‘–andğ‘Ÿğ‘–+1to
fill the missing passing record. By retrieving historical vehicle pass-
ing records, we can further increase the density of vehicle passing
records, which is more conducive to subsequent trajectory recovery.
Figure 3: Spatial Transfer Probability Model.
3.2 Spatial Transfer Probability Model
To estimate the probability of a road segment transitioning to its
successor, referred to as â€™tuning probabilityâ€™, employing probabil-
ity models is a widely accepted approach in trajectory recovery
tasks [ 11,16,32,35]. Considering that the traffic camera is diffi-
cult to record the continuous trajectory, previous studies rely on
high sampling rate GPS trajectories to establish a transfer prob-
ability model. However, as discussed in Section 1, obtaining GPS
trajectories with high sampling rates is challenging, and such data
often exhibits information bias. To address it, we propose a Spatial
Transfer Probability Model (STPM) to recover the most probable
trajectory between consecutive passing records based on vehicle
passing records.
Problem Optimization. To begin with, we refine the trajec-
tory recovery problem into a trajectory probability calculation
problem, which quantifies the probability of a certain trajectory as
the recovery result, and the recovered trajectory is the trajectory
with the highest probability. Traditionally, for a vehicle ğ‘, given
a start passing record ğ‘Ÿğ‘ with its road segment ğ‘Ÿğ‘ .ğ‘’and an end
passing record ğ‘Ÿğ‘’with its road segment ğ‘Ÿğ‘’.ğ‘’and timestamp ğ‘Ÿğ‘’.ğ‘¡, the
set of historical passing record H, the probability of a trajectory
R={R.ğ‘’0,Â·Â·Â·,R.ğ‘’|R|}betweenğ‘Ÿğ‘ andğ‘Ÿğ‘’can be expressed as:
ğ‘ƒğ‘Ÿ(R|ğ‘Ÿğ‘ ,ğ‘Ÿğ‘’,ğ‘,H)=ğ‘ƒğ‘Ÿ(R|ğ‘Ÿğ‘ .ğ‘’,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H)
=ğ‘ƒğ‘Ÿ(R.ğ‘’0,Â·Â·Â·,R.ğ‘’|R||ğ‘Ÿğ‘ .ğ‘’,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H)
=|R|Ã–
ğ‘–=1ğ‘ƒğ‘Ÿ(R.ğ‘’ğ‘–|R.ğ‘’0Â·Â·Â·R.ğ‘’ğ‘–âˆ’1,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H)
(2)
Considering that many studies have shown that human mobility
patterns follow Markovâ€™s assumptions [ 16,31,32], we can further
convert formula (2) to:
ğ‘ƒğ‘Ÿ(R|ğ‘Ÿğ‘ ,ğ‘Ÿğ‘’,ğ‘,H)=|R|Ã–
ğ‘–=1ğ‘ƒğ‘Ÿ(R.ğ‘’ğ‘–|R.ğ‘’ğ‘–âˆ’1,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H) (3)
5982TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Accordingly, the trajectory recovery problem can be conceptualized
as finding a trajectory Râˆ—with the highest conditional probability:
Râˆ—=arg max
âˆ€Râˆˆğºğ‘ƒğ‘Ÿ(R|ğ‘Ÿğ‘ ,ğ‘Ÿğ‘’,ğ‘,H)
=arg max
âˆ€Râˆˆğº|R|Ã–
ğ‘–=1ğ‘ƒğ‘Ÿ(R.ğ‘’ğ‘–|R.ğ‘’ğ‘–âˆ’1,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H)(4)
According to the guidance of Formula (4), we designed the
STPM to calculate ğ‘ƒğ‘Ÿ(R.ğ‘’ğ‘–|R.ğ‘’ğ‘–âˆ’1,ğ‘Ÿğ‘ .ğ‘’,ğ‘Ÿğ‘ .ğ‘¡,ğ‘,H), i.e. for vehicle
ğ‘, given the destination ğ‘Ÿğ‘’.ğ‘’, the arrival time ğ‘Ÿğ‘’.ğ‘¡, and the set of
historical passing record H, we aims to calculate the probability
of any road segment R.ğ‘’ğ‘–âˆ’1of the road network to its successor
road segmentR.ğ‘’ğ‘–. For eachRâˆˆğº, we generate the correspond-
ing OD-pair trajectory Rğ‘‚ğ·=[ğ‘‚ğ·R
1,ğ‘‚ğ·R
2,Â·Â·Â·,ğ‘‚ğ·R
|R|], where
ğ‘‚ğ·R
ğ‘–=<ğ‘Ÿğ‘’.ğ‘¡,R.ğ‘’ğ‘–âˆ’1,R.ğ‘’ğ‘–>,âˆ€ğ‘–,1â‰¤ğ‘–â‰¤|R| . SinceRcan represent
any trajectory from origin road segment ğ‘Ÿğ‘ .ğ‘’to destination road
segmentğ‘Ÿğ‘’.ğ‘’, we can also denote ğ‘‚ğ·R
ğ‘–=<ğ‘Ÿğ‘’.ğ‘¡,ğ‘’ğ‘,ğ‘’ğ‘>, whereğ‘’ğ‘
is the successor road segment of road segment ğ‘’ğ‘. Therefore, we
can simplify ğ‘‚ğ·R
ğ‘–asğ‘‚ğ·ğ‘ğ‘.
3.2.1 Feature Extractor. To obtain dense representations of each
road segment in the road network, we devise a road segment fea-
ture extractor, as show in Fig. 3. First, to represent whether the
road segment is suitable for vehicle traffic and the level of road
development, we jointly embed the road segmentâ€™s properties and
time into dense representations as the input of following modules.
Specifically, for properties of category types (e.g., type, level), we
set up two trainable embedding matrix ğ¸ğ‘šğ‘ğ‘¡ğ‘¦ğ‘ğ‘’âˆˆR|#ğ‘¡ğ‘¦ğ‘ğ‘’ğ‘ |Ã—ğ‘‘
andğ¸ğ‘šğ‘ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆˆR|#ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™ğ‘ |Ã—ğ‘‘, therefore, the road type embedding
vector and road level embedding vector of road segment ğ‘’ğ‘–can
be represented as ğ‘‹ğ‘’ğ‘–
ğ‘¡ğ‘¦ğ‘ğ‘’âˆˆRğ‘‘andğ‘‹ğ‘’ğ‘–
ğ‘™ğ‘’ğ‘£ğ‘’ğ‘™âˆˆRğ‘‘respectively. And
for numerical types (e.g., length, width, the distance close to the
destination), we use linear transformation to obtain ğ‘‘dimension
vectorsğ‘‹ğ‘’ğ‘–ğ‘›ğ‘¢ğ‘šâˆˆRğ‘‘. We concatenate the above vectors to obtain
road segment-level hidden-state vector ğ‘‹ğ‘’ğ‘–
ğ‘Ÿğ‘œğ‘ğ‘‘âˆˆR3ğ‘‘, followed by
linear transformation to obtain static representation of road seg-
mentğ‘‹ğ‘’ğ‘–ğ‘ âˆˆRğ‘‘. Besides, consider that the traffic conditions on road
segments may vary during different time periods (such as morning
and evening rush hours), we divide a day into 48 time periods and
set up a trainable embedding matrix ğ¸ğ‘šğ‘ğ‘¡ğ‘–ğ‘šğ‘’âˆˆR48Ã—ğ‘‘. Finally, the
spatio-temporal representation of road segment ğ‘’ğ‘–at timeğ‘¡ğ‘—can
be represented as:
ğ‘‹ğ‘’ğ‘–,ğ‘¡ğ‘—=ğ‘‹ğ‘’ğ‘–ğ‘ +ğ‘‹ğ‘¡ğ‘—
ğ‘¡(5)
3.2.2 Motion-GAT. The previously extracted road segment spatial-
temporal features focus primarily on the individual road segments.
However, it is insufficient for effectively generalize learned knowl-
edge from camera-covered segments to unobserved road segments.
To this end, considering that the traffic development level of the road
segment is influenced by its adjacent road segments, we propose
the Motion-GAT module. This module first extracts the subgraph
related to road segments from the road network ğºand then uses the
self-attention mechanism to capture the interaction information
between adjacent road segments and the current road segment.
Firstly, we extract subgraphs for each road segment ğ‘’ğ‘–from the
road network. For each road segment ğ‘’ğ‘–, its adjacent road segmentis denoted as ğ‘’ğ‘âˆˆğ´ğ‘‘ğ‘—ğ‘–,ğ´ğ‘‘ğ‘—ğ‘–={ğ´ğ‘‘ğ‘—ğ‘–,ğ‘’ğ‘–}. We represented the
motion relationship between ğ‘’ğ‘andğ‘’ğ‘–by concatenating their turn
information (e.g., left turn, right turn, straight, U-turn) and transfer
relationship (e.g., ğ‘’ğ‘–â†’ğ‘’ğ‘orğ‘’ğ‘â†’ğ‘’ğ‘–), resulting 8 distinct mo-
tion relationships. Subsequently, We set a trainable ğ‘‘-dimensional
motion vector ğ¸ğ‘šğ‘ğ‘šâˆˆR8Ã—ğ‘‘for each motion relationship. Thus,
the motion vector for ğ‘’ğ‘is denoted as ğ‘€ğ‘’ğ‘,ğ‘¡ğ‘—âˆˆRğ‘‘. For clarity of
formulas, we omit the time variable ğ‘¡ğ‘—in the following discussion.
Moreover, we add the motion vector to the spatial-temporal feature
and obtain the final road segment representation ğ‘‹ğ‘€ğ‘’ğ‘:
ğ‘‹ğ‘€
ğ‘’ğ‘=ğ‘‹ğ‘’ğ‘+ğ‘€ğ‘’ğ‘ (6)
whereğ‘‹ğ‘’ğ‘represents the spatio-temporal feature of road segment
ğ‘’ğ‘andğ‘€ğ‘’ğ‘denotes the motion vector of ğ‘’ğ‘. This approach further
strengthens the relationship between ğ‘’ğ‘andğ‘’ğ‘–through the motion
vector. Then we use a multi-head self-attention mechanism to focus
on different motions and the impact of different adjacent road
segments on the current road segment from multiple perspectives.
Specifically, we define the feature correlation between the adja-
cent road segment ğ‘’ğ‘that has been fused with the motion vector
and the current road segment ğ‘’ğ‘–under theâ„-head attention as:
ğ›¼(â„)(ğ‘’ğ‘–,ğ‘’ğ‘)=ğ‘’ğ‘¥ğ‘(ğœ“(ğ‘‹ğ‘’ğ‘–,ğ‘‹ğ‘€ğ‘’ğ‘))
Ã|ğ´ğ‘‘ğ‘—ğ‘–|
ğ‘â€²=1ğ‘’ğ‘¥ğ‘(ğœ“(ğ‘‹ğ‘’ğ‘–,ğ‘‹ğ‘€ğ‘’ğ‘â€²))
ğœ“(ğ‘‹ğ‘’ğ‘–,ğ‘‹ğ‘€
ğ‘’ğ‘)=<ğ‘Š(â„)
ğ‘„ğ‘‹ğ‘’ğ‘–,ğ‘Š(â„)
ğ¾ğ‘‹ğ‘€ğ‘’ğ‘>
âˆš
ğ‘‘â€²(7)
whereğ‘Š(â„)
ğ‘„,ğ‘Š(â„)
ğ¾âˆˆRğ‘‘â€²Ã—ğ‘‘are learnable parameters, <,>is the
inner product function, and ğ‘‘â€²=ğ‘‘/ğ». Next, we generate the aggre-
gated representation of road segment ğ‘’ğ‘–by correlation ğ›¼(â„)(ğ‘’ğ‘–,ğ‘’ğ‘):
Ëœğ‘‹ğ‘’ğ‘–=Ëœğ‘‹(1)
ğ‘’ğ‘–âŠ•Ëœğ‘‹(2)
ğ‘’ğ‘–âŠ•Â·Â·Â·âŠ• Ëœğ‘‹(ğ»)
ğ‘’ğ‘–
Ëœğ‘‹â„
ğ‘’ğ‘–=|ğ´ğ‘‘ğ‘—ğ‘–|âˆ‘ï¸
ğ‘â€²=1ğ›¼(â„)(ğ‘’ğ‘–,ğ‘’ğ‘â€²)(ğ‘Š(â„)
ğ‘‰ğ‘‹ğ‘€
ğ‘’ğ‘â€²)(8)
whereğ‘Š(â„)
ğ‘‰âˆˆRğ‘‘â€²Ã—ğ‘‘is learnable parameters, and âŠ•is the con-
catenation operator, and ğ»is the number of total heads. As such,
we can integrate the structural information of the road network
into the representation of road segments, and in the same way, we
can obtain the representation of each road segment in the road
network.
3.2.3 OD-Attention. As mentioned in Challenge 3, different vehi-
cles may produce different driving trajectories even at the same
time and with the same OD. Thus, we develop OD-Attention to
extract the driving habits of drivers.
Assuming that the vehicle ğ‘is currently located in road segment
ğ‘’ğ‘, whereğ‘’ğ‘represents a road segment in a trajectory between
passing record ğ‘Ÿğ‘ andğ‘Ÿğ‘’. The first step is to retrieval the historical
OD-pairs before time ğ‘Ÿğ‘’.ğ‘¡, and obtain the representation for each
OD-pair. Specifically, for OD-pair ğ‘‚ğ·ğ‘ğ‘âˆˆRğ‘‚ğ·, itsğ‘‘ğ‘‚ğ·dimension
representation Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘âˆˆRğ‘‘ğ‘‚ğ·can be calculated as:
Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘=M( Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘.ğ‘’ğ‘œâŠ•Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘.ğ‘’ğ‘‘) (9)
5983KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dongen Wu, et al.
whereMdenote two-layer fully-connected neural network func-
tion. As such, we further obtain the representation sequence of vehi-
cle OD-pair trajectory Jğ‘‚ğ·ğ‘asËœğ‘‹ğ‘‚ğ·J=[Ëœğ‘‹ğ‘‚ğ·J
1,Ëœğ‘‹ğ‘‚ğ·J
2,Â·Â·Â·,Ëœğ‘‹ğ‘‚ğ·J
ğœ‚],
whereğœ‚is the number of total historical OD-pairs. The next step is
to extract the driving habits among historical OD-pairs. Intuitively,
we believe that driving habits can be seen as similar choices that
drivers habitually make under similar road conditions. Therefore,
we calculate the similarity between the OD-pair representation
Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘of the current OD-pair ğ‘‚ğ·ğ‘ğ‘and the OD-pair representa-
tions Ëœğ‘‹ğ‘‚ğ·J
ğ‘—of the historical OD-pairs, and then selectively incorpo-
rate the historical OD-pair features Ëœğ‘‹ğ‘‚ğ·J
ğ‘—into the current OD-pair
features Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘by multi-head attention mechanism:
Ëœğ‘‹A
ğ‘‚ğ·ğ‘ğ‘=Ëœğ‘‹A(1)
ğ‘‚ğ·ğ‘ğ‘âŠ•Ëœğ‘‹A(2)
ğ‘‚ğ·ğ‘ğ‘âŠ•Â·Â·Â·âŠ• Ëœğ‘‹A(ğ»)
ğ‘‚ğ·ğ‘ğ‘
Ëœğ‘‹A(â„)
ğ‘‚ğ·ğ‘ğ‘=ğ‘›âˆ‘ï¸
ğ‘—â€²=1ğ›¼(â„)(ğ‘‚ğ·ğ‘ğ‘,ğ‘‚ğ·J
ğ‘—)(ğ‘Š(â„)
ğ‘‰â€²Ëœğ‘‹ğ‘‚ğ·J
ğ‘—â€²)(10)
ğ›¼(â„)(ğ‘‚ğ·ğ‘ğ‘,ğ‘‚ğ·J
ğ‘—)=ğ‘’ğ‘¥ğ‘(ğœ“(Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘,Ëœğ‘‹ğ‘‚ğ·J
ğ‘—))
Ãğ‘›
ğ‘—â€²=1ğ‘’ğ‘¥ğ‘(ğœ“(Ëœğ‘‹ğ‘‚ğ·R
ğ‘–,Ëœğ‘‹ğ‘‚ğ·J
ğ‘—â€²))
ğœ“(Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘,Ëœğ‘‹ğ‘‚ğ·J
ğ‘—)=<ğ‘Š(â„)
ğ‘„â€²Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘,ğ‘Š(â„)
ğ¾â€²Ëœğ‘‹ğ‘‚ğ·J
ğ‘—>
âˆšï¸ƒ
ğ‘‘â€²
ğ‘‚ğ·(11)
whereğ‘Š(â„)
ğ‘„â€²,ğ‘Š(â„)
ğ¾â€²,ğ‘Š(â„)
ğ‘‰â€²âˆˆRğ‘‘â€²
ğ‘‚ğ·Ã—ğ‘‘ğ‘‚ğ·are learnable parameters,
andğ‘‘â€²
ğ‘‚ğ·=ğ‘‘ğ‘‚ğ·/ğ». To preserve the representation of the original
OD-pair, we further use standard residual connections:
ğ‘‹ğ‘‚ğ·ğ‘ğ‘=ğœ(Ëœğ‘‹A
ğ‘‚ğ·ğ‘ğ‘+ğ‘Šğ‘ŸËœğ‘‹ğ‘‚ğ·ğ‘ğ‘) (12)
whereğ‘Šğ‘ŸâˆˆRğ‘‘ğ‘‚ğ·Ã—ğ‘‘ğ‘‚ğ·is learnable projection matrix, and ğœis
non-linear activation function e.g. ğ‘…ğ‘’ğ¿ğ‘ˆ function.
With the OD-attention layer, the representation of OD-pair
Ëœğ‘‹ğ‘‚ğ·ğ‘ğ‘can be updated into a more informative representation
ğ‘‹ğ‘‚ğ·ğ‘ğ‘, incorporating the driving habit features from historical
passing records.
3.2.4 Transfer Probabilistic Generator. Assuming that the vehicle
is currently located in road segment ğ‘’ğ‘, it can then travel to any
adjacent road segment ğ‘’ğ‘âˆˆğ´ğ‘‘ğ‘—ğ‘. Thus, we obtain the transfer
probability of ğ‘’ğ‘â†’ğ‘’ğ‘by:
ğ‘ƒğ‘Ÿ(ğ‘’ğ‘|ğ‘’ğ‘,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H)=ğ‘’ğ‘¥ğ‘(F(ğ‘‹ğ‘‚ğ·ğ‘ğ‘))
Ã|ğ´ğ‘‘ğ‘—ğ‘|
ğ‘â€²=1ğ‘’ğ‘¥ğ‘(F(ğ‘‹ğ‘‚ğ·ğ‘ğ‘â€²))(13)
whereFdenote two-layer fully-connected neural network function:
F(ğ‘‹)=ğœ(ğ‘‹ğ‘Š 1+ğ‘1)ğ‘Š2 (14)
Here,ğ‘Š1âˆˆRğ‘‘ğ‘‚ğ·Ã—ğ‘‘1andğ‘Š2âˆˆRğ‘‘1Ã—1are two learnable weights,
andğ‘1is the learnable bias.
3.3 Trajectory Generator
Given the vehicle trajectory Jğ‘of vehicleğ‘, based on formula (4)
and the transfer probability output of each road segment ğ‘’ğ‘âˆˆğ¸
transfer to its adjacency road segment ğ‘’ğ‘âˆˆğ´ğ‘‘ğ‘—ğ‘by the STPM. We
sequentially recover the trajectory between passing record ğ‘Ÿğ‘–âˆˆJğ‘and passing record ğ‘Ÿğ‘–+1âˆˆJğ‘. Inspired by [ 11], we simplify the
Formula 4 as:
Râˆ—=arg max
âˆ€Râˆˆğº|R|Ã–
ğ‘–=1ğ‘ƒğ‘Ÿ(R.ğ‘’ğ‘–|R.ğ‘’ğ‘–âˆ’1,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H)
=arg min
âˆ€Râˆˆğº|R|âˆ‘ï¸
ğ‘–=1âˆ’ğ‘™ğ‘œğ‘”(ğ‘ƒğ‘Ÿ(R.ğ‘’ğ‘–|R.ğ‘’ğ‘–âˆ’1,ğ‘Ÿğ‘’.ğ‘’,ğ‘Ÿğ‘’.ğ‘¡,ğ‘,H))(15)
Based on that, we can form a road network graph with road seg-
ments as nodes and the connection relationships between road
segments as edges based on the connection relationship of road
segments in the road network ğº. We can regard the transfer proba-
bility as the edge weight between nodes, i.e. road segments, so we
can transform the problem into the shortest path finding problem
and use Dijkstraâ€™s Algorithm to solve it.
3.4 Training
Finally, we elaborate on the end-to-end training process of STPM.
As mentioned in Section 1, we obtain snapshots of vehicles captured
by traffic cameras, extracting lane information and corresponding
turning details. Therefore, we construct training samples using
snapshots with explicit turning information (detailed construction
rules are introduced in Appendix A). Each training sample is repre-
sented as(ğ‘¥ğ‘–,ğ‘¦ğ‘–), whereğ‘¥ğ‘–=<ğ‘’ğ‘,ğ‘’ğ‘,ğ‘Ÿğ‘’,ğ‘,Hğ‘>signifies that the
vehicleğ‘is currently on road segment ğ‘’ğ‘, and given its destination
at road segment ğ‘Ÿğ‘’.ğ‘’and arrive time ğ‘Ÿğ‘’.ğ‘¡, the probability of heading
towards adjacent road segment ğ‘’ğ‘âˆˆğ´ğ‘‘ğ‘—ğ‘isğ‘¦. In a mini-batch of ğ‘
training samples, the training objective is to minimize the modelâ€™s
mean squared error using the following loss function:
ğ¿(ğœƒ)=1
ğ‘ğ‘âˆ‘ï¸
ğ‘–=1(ğ‘¦ğ‘–âˆ’ğ‘“(ğ‘¥ğ‘–,ğœƒ))2+ğœ†||ğœƒ||2(16)
Here,ğ‘“(ğ‘¥,ğœƒ)represents the output of STPM, ğœƒdenotes all train-
able parameters in the model, and ğœ†is a parameter controlling the
regularization strength. Our goal is to train the model and find the
optimal parameters ğœƒthat minimize the loss function ğ¿(ğœƒ).
4 EXPERIMENTS
4.1 Experimental Settings
4.1.1 Datasets. Camera Record Data. To demonstrate the ro-
bustness of TrajRecovery, we use two different-scale real traffic
camera record datasets, which are collected from a region of a city
in China, which encompasses 975 traffic cameras, 5,597 road inter-
sections, and 13,579 road segments. Specifically, the larger dataset
is D5, which comprises over 20 million vehicle snapshots over 5
days. The smaller dataset is D1, which consists of 1.5 million vehicle
snapshots during the morning rush hour on the workday.
Ground Truth Data. To evaluate the recovery performance, we
require ground truth data. Corresponding to the time cover range of
D5 and D1, our volunteers collect 515 and 196 real GPS trajectories,
respectively, while driving on the roads. Then, we perform map-
matching [ 21] to align these GPS trajectories with the road network,
which helps evaluate trajectory recovery quality.
4.1.2 Baselines. We compare TrajRecovery with six baselines.
5984TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Overall Performance Comparison.
MethodD1 D5
LCSS EDR STLC DTW LCSS EDR STLC DTW
SP 0.4716Â±0.0031 30.5164Â±0.1059 0.6128Â±0.0058 1253Â±11 0.5831Â±0.0052 36.1543Â±0.1129 0.5043Â±0.0081 1863Â±30
VeTrac 0.3921Â±0.0108 18.9641Â±0.6018 0.7681Â±0.0130 896Â±38 0.5031Â±0.0126 23.5163Â±0.7158 0.5968Â±0.0146 1653Â±53
MMVC 0.3761Â±0.0124 16.5176Â±0.7426 0.8214Â±0.0105 841Â±40 0.4713Â±0.0135 19.9643Â±0.8201 0.6514Â±0.0117 1568Â±51
CFST 0.3495Â±0.0078 15.1536Â±0.5971 0.8537Â±0.0126 726Â±28 0.4482Â±0.0103 18.7694Â±0.6391 0.6786Â±0.0138 1496Â±42
CFST-C 0.3351Â±0.0071 14.9658Â±0.5820 0.8602Â±0.0107 719Â±29 0.4324Â±0.0098 18.0659Â±0.6018 0.6812Â±0.0118 1406Â±38
TrajRecovery-G 0.3356Â±0.0116 15.1443Â±0.6419 0.8616Â±0.0130 728Â±36 0.4218Â±0.0120 17.5011Â±0.6310 0.6943Â±0.0107 1367Â±28
TrajRecovery 0.3179Â±0.0082 13.8169Â±0.5981 0.8826Â±0.0118 653Â±31 0.4096Â±0.0108 16.6115Â±0.6189 0.7281Â±0.0116 1235Â±47
Gain 5.1% 7.7% 2.6% 9.2% 5.3% 8.1% 6.9% 12.2%
â€¢Shortest Path (SP) [ 39]. Initially, we use the vehicle reidenti-
fication in the Preprocessor module to cluster vehicles. Then,
we employ this method to generate the shortest path.
â€¢VeTrac [ 28]. It utilizes the graph convolutional process to
maintain identity consistency across different camera obser-
vations. Due to memory limitations, following the previous
study [34], we employ the ğ‘˜-means clustering.
â€¢MMVC [ 16]. It jointly optimizes vehicle reidentification and
trajectory recovery. It clusters vehicles using visual features
and employs the Hidden Markov Model (HMM) for trajectory
recovery based on vehicle clusters.
â€¢CFST [ 34]. Similar to MMVC [ 16], it clusters vehicles us-
ing visual features and employs GPS trajectories to train a
uniform Dirichlet prior model and a temporal model. Specif-
ically, we utilize 84,153 trajectories recorded over 24 hours
on a day not included in the dataset for model training.
â€¢CFST-C [ 35]. This is an optimized version of CFST [ 34], divid-
ing the entire map area into multiple blocks and recovering
and concatenating trajectories blockwise.
â€¢TrajRecovery-G. We replace the Dijkstra algorithm used in
the trajectory generator with a greedy algorithm. It selects
the road segment with the highest probability at each step
during the trajectory generation process.
4.1.3 Evaluation Metrics. Referring to previous studies, we uti-
lize four metrics to evaluate the precision of trajectory recovery,
i.e., Longest Common SubSequence (LCSS), Edit Distance on Real se-
quence (EDR), Spatio-Temporal Linear Combine distance (STLC) [ 27],
and Dynamic Time Warping (DTW) [ 25]. The larger the STLC
value, the better the quality of trajectory recovery; the smaller the
LCSS/EDR/DTW value, the better the quality of trajectory recovery.
4.1.4 Parameter Settings. All experiments were conducted on a
Linux server equipped with PyTorch 1.10.1, 2 NVIDIA TESLA T4
GPUs, and 64GB memory. These programs were implemented using
Python. In the Preprocessor module, we set the similarity threshold
ğ‘‡â„=0.8, the context window ğ‘˜=2,ğ‘=50, andğ‘ =20. In the
Feature Extractor module, we configure the dimension ğ‘‘of each
dense representation to be 128. In the OD-Attention module, we set
the dimension ğ‘‘ODto 64. In multi-head attention, we employ 4 heads
to generate attention from different perspectives. We use the Adam
optimizer [ 14] for training, where the initial learning rate is 0.0001
and the mini-batch size is 128. The size of training/validation/testingis set to 0.8/0.1/0.1 for STPM, and select the model that performs best
on the testing set for trajectory recovery. The Xavier initializer [ 7]
is employed to initialize the model parameters, and the associated
parameters adhere to the configurations outlined in the original
paper. We present the replication results for all baselines, using
hyperparameters consistent with those specified in the original
paper.
4.2 Overall Performance Comparison
Table 1 shows the overall performance of all methods on two
datasets, where the best result in each metric (column) is shown in
bold. Overall, we yield the following observations.
The first observation is that our model consistently outperforms
baselines on all four metrics for both datasets, showing great supe-
riority, e.g., up to 12% improvement. Second, SP performs poorly,
as individuals may consider various factors when selecting routes
while driving. Even in similar road conditions, drivers may choose
longer routes due to driving habits. In contrast, our methods com-
prehensively consider factors such as road conditions and driving
habits, making them more aligned with real-world scenarios. An-
other interesting observation is both VeTrac and MMVC require
a high density of traffic cameras to provide sufficient information.
This leads to their suboptimal performance in suburban areas with
lower traffic camera density. Although CFST and CFST-C perform
better than VeTrac and MMVC by utilizing GPS trajectories from
other vehicles in their trajectory recovery, biases in path prefer-
ences may occur, causing the model to fail on road segments not
covered by GPS trajectories. In contrast, our methods employ a uni-
versal method for trajectory recovery based on road conditions and
individual driving habits, even in areas without camera coverage.
We also observe that in the D1 dataset, CFST-C is comparable to the
TrajRecovery-G version. This is because, during the morning rush
hour, peopleâ€™s travel patterns tend to be relatively fixed, resulting
in concentrated driving trajectories. The final observation is that,
when replacing the Dijkstra algorithm with the greedy algorithm
in TrajRecovery, its performance declined. This is mainly because
consistently moving to the road segment with the highest probabil-
ity can result in local optima rather than global optima in certain
situations. It may also lead to the occurrence of trajectory "loops,"
where the road segment with the highest probability forms a loop,
preventing the trajectory from reaching the destination.
5985KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dongen Wu, et al.
Table 2: Impact of each Component (OD: OD-Attention, M:
Motion-GAT, HPRR: Historical Passing Record Retriever).
Ablation LCSS EDR STLC DTW
-OD & M 0.4367Â±0.0128 17.9584Â±0.6241 0.6894Â±0.0125 1437Â±56
-M 0.4208Â±0.0110 17.9123Â±0.6110 0.6953Â±0.0120 1342Â±53
-HPRR 0.4195Â±0.0107 17.7614Â±0.5917 0.6927Â±0.0102 1415Â±43
-OD 0.4164Â±0.0113 17.6615Â±0.6071 0.6913Â±0.0127 1361Â±50
All 0.4096Â±0.0108 16.6115Â±0.6189 0.7281Â±0.0116 1235Â±47
4.3 Ablation Study
We study each module to influence the final results. To achieve this,
we systematically performed ablation experiments by sequentially
removing each module. The results are presented in Table 2, and
for a more comprehensive conclusion, we conducted the analysis
using the information-rich D5 dataset. It is evident from the re-
sults that the performance of the complete version of the system is
optimal, indicating that each module contributes to the overall sys-
tem performance. The most notable performance decline occurred
when the Motion-GAT module was ablated. This is attributed to the
significance of factors such as the developmental level and traffic
conditions of road segments, crucial elements influencing a dri-
verâ€™s decision to choose a particular route. Additionally, the traffic
conditions of surrounding road segments also impact the driverâ€™s
decision on whether to select a particular route.
4.4 Robustness Study
We conducted experiments to assess the robustness of TrajRecovery
in various scenarios. As mentioned in the first chapter, traffic cam-
eras may occasionally malfunction, resulting in instances where
vehicles passing through are not captured. Specifically, we investi-
gated the systemâ€™s recovery performance under different percent-
ages of missing vehicle records. For a particular vehicleâ€™s complete
record, we retained the first and last entries, randomly removing
varying percentages of intermediate entries from the vehicle record
database. The results are presented in Fig. 4, indicating that as the
percentage of missing entries increases, trajectory recovery errors
gradually escalate. However, our model consistently outperforms
the comparison methods in terms of performance.
4.5 Parameter Study
We investigate the sensitivity of three parameters: vehicle snapshot
similarity threshold ğ‘‡â„, the road segment representation dimension
ğ‘‘, and the head number ğ». The results are shown in Appendix B.
4.6 Case Study
We selected a representative trajectory from the test trajectories
to illustrate the effectiveness of TrajRecovery. As shown in Fig. 5,
compared to CFST-C, the results of TraRecovery are closer to the
ground truth. Specifically, we first identified six passing records of
the vehicle through vehicle re-identification. Subsequently, using
the Historical Passing Record Retriever, we retrieved the histor-
ical passing records of the vehicle. The retrievals indicated that
the vehicle passed through the camera at the red coordinate point
TrajRecovery CFST -C MMVC
 VeTrac
 SPFigure 4: The influence of the missing rate.
Figure 5: Visualization of trajectory recovery by TrajRecov-
ery and CFST-C.
under similar contextual conditions multiple times. This prelimi-
nary increase in the sampling rate of passing record trajectories
before trajectory recovery helps reduce uncertainty. Moreover, at
the fifth passing record, the vehicle chose to turn left instead of
going straight, which differs from the choice of the majority. This
decision was influenced by the driverâ€™s historical OD-pair pref-
erences, indicating a preference for roads with better conditions
rather than the closest ones. However, CFST-C relies heavily on a
large volume of GPS trajectories for trajectory recovery, making it
prone to failure when encountering intersections not encountered
during the training phase. In contrast, TrajRecovery, even in road
segments not covered by traffic cameras, can make road selections
based on road conditions and driver habits.
5986TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
5 PRACTICAL DEPLOYMENT
Based on the trajectory recovery results provided by TrajRecovery,
we have developed a web-based system at Huawei Cloud. This
system leverages recovered trajectories for various downstream
applications. For instance, Fig. 6 showcases the traffic flow monitor-
ing interface, allowing administrators to interact with the system in
the following scenarios. (i) Traffic Volume Visualization. Utilizing a
heat map displayed in the upper left corner of Fig. 6, we illustrate
the traffic volume at each intersection. Color variations depict the
congestion or smooth flow of traffic conditions. (ii) Crowded In-
tersection Ranking. As depicted in the lower right part of Fig. 6, it
provides periodic traffic statistics, presenting the number of vehi-
cles during different periods and at specific intersections (e.g., Road
22). (iii) Traffic Volume Statistics. By selecting a specific region,
a traffic volume trend-line for the past 24 hours is plotted in the
lower right part of Fig. 6. Furthermore, administrators can easily
obtain precise volume information by hovering the mouse over a
specific time interval. Additionally, we have successfully deployed
TrajRecovery in a city in China for traffic signal control, bringing in
a noteworthy 12.5% increase in the average speed of intersections.
6 RELATED WORK
6.1 Trajectory Recovery
Depending on the data source of sparse vehicle trajectories, it can
be divided into two categories.
The first category primarily focuses on sparse GPS trajecto-
ries [15, 24, 30, 33], which are not the scope of this paper.
The second category takes sparse snapshots captured by traffic
cameras as input, which often requires vehicle re-identification
processing, facing missed identifications, misidentifications, and
aggravated sparsity. Cao et al. [ 1] utilized high-sampling-rate GPS
trajectories from commercial vehicles to assist in recovering low-
sampling-rate trajectories. Tong et al. [ 28] fused vehicle movement
correlations and visual features to reduce uncertainties in identify-
ing vehicles, requiring a higher camera distribution rate to enhance
trajectory recovery accuracy. Qi et al. [ 23] performed trajectory
recovery on automatic license plate recognition data obtained from
cameras. Lin et al. [ 16] recognized the correlation between vehicle
re-identification and trajectory recovery, proposing a joint optimiza-
tion framework for both tasks. Yu et al. [ 34] optimized previous
efforts by combining vehicle visual features and dynamic spatio-
temporal constraint features for vehicle re-identification. Recently,
Yu et al. [ 35] divided the entire city into multiple regions, performed
trajectory recovery within each region, and then concatenated the
complete trajectories. However, these approaches require a sub-
stantial amount of GPS trajectories for training trajectory recovery
models and may not perform well in areas not covered by cameras.
6.2 Vehicle Re-identification
Vehicle re-identification involves clustering the same vehicle from
a large number of snapshots captured by cameras, serving as an
essential foundation for trajectory recovery. Numerous CNN-based
methods have been proposed. Liu et al. [ 18] introduced a coarse-
to-fine vehicle retrieval framework that utilizes multi-modal fea-
tures, such as visual features, license plate information, camera
1
2
3
4
5
62
3456
1
Rank 1: Road 22 Rank 2: Road 11 Rank 3: Road 715 Rank 4: Road 6 Rank 5: Road 9
Traffic Volume HeatmapFigure 6: TrajRecovery deployment at HuaWei Cloud.
location, and other contextual details. Zhou et al. [ 42] employed
reverse learning and a viewpoint-aware attention model to acquire
viewpoint-aware representations for vehicle re-identification. [ 13]
proposed AAVER, a dual-path model with adaptive attention for
vehicle re-identification, which uses key part features of vehicles
to distinguish local features in different regions and concentrates
attention on the most informative key points to capture local dis-
criminative features. Shen et al. [ 26] introduced a Graph Interactive
Transformer framework that utilizes the interaction between graphs
and transformers to effectively coordinate local and global features.
Meng et al. [20] extracted features from different parts of vehicles
and used commonly visible attention to calculate the similarity
between different parts. He et al. [ 9] proposed a simple yet effective
method of part-regularized discriminative feature preservation, fo-
cusing attention on vehicle-specific features to enhance sensitivity
to subtle differences.
7 CONCLUSION
In this paper, we propose the TrajRecovery framework for re-
covering urban-scale vehicle trajectories using vehicle snapshots
captured by traffic cameras. TrajRecovery generalizes knowledge
learned from camera-covered road segments to uncovered road seg-
ments, and learns driver-specific driving habits based on historical
driving behavior, enabling vehicle-level personalized trajectory re-
covery. Additionally, we develop a web-based system TrajRecovery
at HuaWei Cloud, which leverages recovered trajectories for various
downstream applications. Experiments on two real-world datasets
validate TrajRecoveryâ€™s effectiveness and utility improvement on
the business scenarios.
8 ACKNOWLEDGMENTS
This work was supported in part by the NSFC under Grants No.
(62025206, U23A20296, and 62102351), the Zhejiang Provinceâ€™s
"Lingyan" R&D Project under Grant No 2024C01259, the Ningbo
Science and Technology Special Innovation Projects under Grants
No. (2022Z095 and 2021Z019), and the Hangzhou Technology Bu-
reau AI Research Project under Grant No. 2022AIZD0116. Yunjun
Gao is the corresponding author of the work.
5987KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dongen Wu, et al.
REFERENCES
[1]Zijian Cao, Dong Zhao, Hanxing Song, Haitao Yuan, Qiyue Wang, Huadong Ma,
Jianjun Tong, and Chang Tan. 2023. F 3 VeTrac: Enabling Fine-grained, Fully-
road-covered, and Fully-individual penetrative Vehicle Trajectory Recovery. IEEE
Transactions on Mobile Computing (2023).
[2]Yuqi Chen, Hanyuan Zhang, Weiwei Sun, and Baihua Zheng. 2023. Rntrajrec:
Road network enhanced trajectory recovery with spatial-temporal transformer.
In2023 IEEE 39th International Conference on Data Engineering (ICDE). IEEE,
829â€“842.
[3]Ziquan Fang, Dongen Wu, Lu Pan, et al .2022. When transfer learning meets
cross-city urban flow prediction: spatio-temporal adaptation matters. IJCAIâ€™22
(2022), 2030â€“2036.
[4]Kun Fu, Fanlin Meng, Jieping Ye, and Zheng Wang. 2020. CompactETA: A
fast inference system for travel time prediction. In Proceedings of the 26th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. 3337â€“
3345.
[5]Qiang Gao, Fan Zhou, Kunpeng Zhang, Goce Trajcevski, Xucheng Luo, and Fengli
Zhang. 2017. Identifying Human Mobility via Trajectory Embeddings.. In IJCAI,
Vol. 17. 1689â€“1695.
[6]Fosca Giannotti, Mirco Nanni, Dino Pedreschi, Fabio Pinelli, Chiara Renso, Salva-
tore Rinzivillo, and Roberto Trasarti. 2011. Unveiling the complexity of human
mobility by querying and mining massive trajectory data. The VLDB Journal 20
(2011), 695â€“719.
[7]Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of the thirteenth international
conference on artificial intelligence and statistics. JMLR Workshop and Conference
Proceedings, 249â€“256.
[8]Haiyun Guo, Chaoyang Zhao, Zhiwei Liu, Jinqiao Wang, and Hanqing Lu. 2018.
Learning coarse-to-fine structured feature embedding for vehicle re-identification.
InProceedings of the AAAI Conference on Artificial Intelligence, Vol. 32.
[9]Bing He, Jia Li, Yifan Zhao, and Yonghong Tian. 2019. Part-regularized near-
duplicate vehicle re-identification. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 3997â€“4005.
[10] Mingdi Hu, Long Bai, Jiulun Fan, Sirui Zhao, and Enhong Chen. 2023. Vehicle
color recognition based on smooth modulation neural network with multi-scale
feature fusion. Frontiers of Computer Science 17, 3 (2023), 173321.
[11] Jayant Jain, Vrittika Bagadia, Sahil Manchanda, and Sayan Ranu. 2021. NeuroMLR:
Robust & reliable route recommendation on road networks. Advances in Neural
Information Processing Systems 34 (2021), 22070â€“22082.
[12] Sultan Daud Khan and Habib Ullah. 2019. A survey of advances in vision-based
vehicle re-identification. Computer Vision and Image Understanding 182 (2019),
50â€“63.
[13] Pirazh Khorramshahi, Amit Kumar, Neehar Peri, Sai Saketh Rambhatla, Jun-
Cheng Chen, and Rama Chellappa. 2019. A dual-path model with adaptive
attention for vehicle re-identification. In Proceedings of the IEEE/CVF international
conference on computer vision. 6132â€“6141.
[14] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[15] Dedong Li, Ziyue Li, Zhishuai Li, Lei Bai, Qingyuan Gong, Lijun Sun, Wolfgang
Ketter, and Rui Zhao. 2023. A Critical Perceptual Pre-trained Model for Complex
Trajectory Recovery. In Proceedings of the 2nd ACM SIGSPATIAL International
Workshop on Searching and Mining Large Collections of Geospatial Data. 1â€“8.
[16] Zongyu Lin, Guozhen Zhang, Zhiqun He, Jie Feng, Wei Wu, and Yong Li. 2021.
Vehicle Trajectory Recovery on Road Network Based on Traffic Camera Video
Data. In Proceedings of the 29th International Conference on Advances in Geographic
Information Systems. 389â€“398.
[17] Xinchen Liu, Wu Liu, Huadong Ma, and Huiyuan Fu. 2016. Large-scale vehicle re-
identification in urban surveillance videos. In 2016 IEEE international conference
on multimedia and expo (ICME). IEEE, 1â€“6.
[18] Xinchen Liu, Wu Liu, Tao Mei, and Huadong Ma. 2016. A deep learning-based
approach to progressive vehicle re-identification for urban surveillance. In Com-
puter Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
October 11-14, 2016, Proceedings, Part II 14. Springer, 869â€“884.
[19] Yisheng Lv, Yanjie Duan, Wenwen Kang, Zhengxi Li, and Fei-Yue Wang. 2014.
Traffic flow prediction with big data: A deep learning approach. IEEE Transactions
on Intelligent Transportation Systems 16, 2 (2014), 865â€“873.
[20] Dechao Meng, Liang Li, Xuejing Liu, Yadong Li, Shijie Yang, Zheng-Jun Zha,
Xingyu Gao, Shuhui Wang, and Qingming Huang. 2020. Parsing-based view-
aware embedding network for vehicle re-identification. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition. 7103â€“7112.
[21] Paul Newson and John Krumm. 2009. Hidden Markov map matching through
noise and sparseness. In Proceedings of the 17th ACM SIGSPATIAL international
conference on advances in geographic information systems. 336â€“343.
[22] Xiao Pan, Lei Wu, Fenjie Long, and Ang Ma. 2022. Exploiting user behavior
learning for personalized trajectory recommendations. Frontiers of Computer
Science 16 (2022), 1â€“12.[23] Xinyi Qi, Yanjie Ji, Wenhao Li, and Shuichao Zhang. 2021. Vehicle trajectory
reconstruction on urban traffic network using automatic license plate recognition
data. IEEE Access 9 (2021), 49110â€“49120.
[24] Huimin Ren, Sijie Ruan, Yanhua Li, Jie Bao, Chuishi Meng, Ruiyuan Li, and Yu
Zheng. 2021. Mtrajrec: Map-constrained trajectory recovery via seq2seq multi-
task learning. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge
Discovery & Data Mining. 1410â€“1419.
[25] Stan Salvador and Philip Chan. 2007. Toward accurate dynamic time warping in
linear time and space. Intelligent Data Analysis 11, 5 (2007), 561â€“580.
[26] Fei Shen, Yi Xie, Jianqing Zhu, Xiaobin Zhu, and Huanqiang Zeng. 2023. Git:
Graph interactive transformer for vehicle re-identification. IEEE Transactions on
Image Processing 32 (2023), 1039â€“1051.
[27] Han Su, Shuncheng Liu, Bolong Zheng, Xiaofang Zhou, and Kai Zheng. 2020. A
survey of trajectory distance measures and performance evaluation. The VLDB
Journal 29 (2020), 3â€“32.
[28] Panrong Tong, Mingqian Li, Mo Li, Jianqiang Huang, and Xiansheng Hua. 2021.
Large-scale vehicle trajectory reconstruction with camera sensing network. In
Proceedings of the 27th Annual International Conference on Mobile Computing and
Networking. 188â€“200.
[29] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. 2023. YOLOv7:
Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition. 7464â€“7475.
[30] Jingyuan Wang, Ning Wu, Xinxi Lu, Wayne Xin Zhao, and Kai Feng. 2019. Deep
trajectory recovery with fine-grained calibration using kalman filter. IEEE Trans-
actions on Knowledge and Data Engineering 33, 3 (2019), 921â€“934.
[31] Jingyuan Wang, Ning Wu, Wayne Xin Zhao, Fanzhang Peng, and Xin Lin. 2019.
Empowering A* search algorithms with neural networks for personalized route
recommendation. In Proceedings of the 25th ACM SIGKDD international conference
on knowledge discovery & data mining. 539â€“547.
[32] Hao Wu, Jiangyun Mao, Weiwei Sun, Baihua Zheng, Hanyuan Zhang, Ziyang
Chen, and Wei Wang. 2016. Probabilistic robust route recovery with spatio-
temporal dynamics. In Proceedings of the 22nd ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining. 1915â€“1924.
[33] Tong Xia, Yunhan Qi, Jie Feng, Fengli Xu, Funing Sun, Diansheng Guo, and
Yong Li. 2021. Attnmove: History enhanced trajectory recovery via attentional
network. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35.
4494â€“4502.
[34] Fudan Yu, Wenxuan Ao, Huan Yan, Guozhen Zhang, Wei Wu, and Yong Li.
2022. Spatio-Temporal Vehicle Trajectory Recovery on Road Network Based on
Traffic Camera Video Data. In Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining. 4413â€“4421.
[35] Fudan Yu, Huan Yan, Rui Chen, Guozhen Zhang, Yu Liu, Meng Chen, and Yong
Li. 2023. City-scale vehicle trajectory data from traffic camera videos. Scientific
data 10, 1 (2023), 711.
[36] Hao Yu, Xi Guo, Xiao Luo, Weihao Bian, and Taohong Zhang. 2023. Construct
Trip Graphs by Using Taxi Trajectory Data. Data Science and Engineering 8, 1
(2023), 1â€“22.
[37] Haitao Yuan, Guoliang Li, Zhifeng Bao, and Ling Feng. 2020. Effective travel
time estimation: When historical trajectories over road networks matter. In
Proceedings of the 2020 acm sigmod international conference on management of
data. 2135â€“2149.
[38] Jing Yuan, Yu Zheng, Chengyang Zhang, Xing Xie, and Guang-Zhong Sun. 2010.
An interactive-voting based map matching algorithm. In 2010 Eleventh interna-
tional conference on mobile data management. IEEE, 43â€“52.
[39] F Benjamin Zhan and Charles E Noon. 1998. Shortest path algorithms: an evalu-
ation using real road networks. Transportation science 32, 1 (1998), 65â€“73.
[40] Chuanpan Zheng, Xiaoliang Fan, Cheng Wang, and Jianzhong Qi. 2020. Gman: A
graph multi-attention network for traffic prediction. In Proceedings of the AAAI
conference on artificial intelligence, Vol. 34. 1234â€“1241.
[41] Yu Zheng, Licia Capra, Ouri Wolfson, and Hai Yang. 2014. Urban computing:
concepts, methodologies, and applications. ACM Transactions on Intelligent
Systems and Technology (TIST) 5, 3 (2014), 1â€“55.
[42] Yi Zhou and Ling Shao. 2018. Aware attentive multi-view inference for vehicle
re-identification. In Proceedings of the IEEE conference on computer vision and
pattern recognition. 6489â€“6498.
[43] Chenyi Zhuang, Nicholas Jing Yuan, Ruihua Song, Xing Xie, and Qiang Ma. 2017.
Understanding People Lifestyles: Construction of Urban Movement Knowledge
Graph from GPS Trajectory.. In Ijcai. 3616â€“3623.
5988TrajRecovery: An Efficient Vehicle Trajectory Recovery Framework based on Urban-Scale Traffic Camera Records KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A TRAINING DATA PREPARATION
Algorithm 1: Training Sample Generation
Input: Shortest distance matrix ğ·ğ‘’, current passing record
ğ‘Ÿğ‘ , next passing record ğ‘Ÿğ‘’, possible adjacent road
sectionsğ´ğ‘‘ğ‘—ğ‘‡
ğ‘Ÿğ‘ .ğ‘’, vehicleğ‘, the historical passing
recordsHğ‘.
Output: training samples ğ‘‡ğ‘Ÿ.
1Initğ‘ğ‘œğ‘ ğ‘‘ğ‘–ğ‘ â†{} ;ğ‘›ğ‘’ğ‘”ğ‘‘ğ‘–ğ‘ â†{} ;ğœâ†0;
2forğ‘’ğ‘âˆˆğ´ğ‘‘ğ‘—ğ‘‡
ğ‘Ÿğ‘ .ğ‘’do
3ğ‘‘ğ‘–ğ‘ â†ğ·ğ‘’[ğ‘Ÿğ‘ .ğ‘’][ğ‘Ÿğ‘’.ğ‘’]âˆ’ğ·ğ‘’[ğ‘’ğ‘][ğ‘Ÿğ‘’.ğ‘’];
4 ifğ‘‘ğ‘–ğ‘ >0: // far away from ğ‘Ÿğ‘’
5ğ‘ğ‘œğ‘ ğ‘‘ğ‘–ğ‘ .push(ğ‘‘ğ‘–ğ‘ );
6 else: // close toğ‘Ÿğ‘’
7ğ‘›ğ‘’ğ‘”ğ‘‘ğ‘–ğ‘ .push(|ğ‘‘ğ‘–ğ‘ |);
8end
9if|ğ‘ğ‘œğ‘ ğ‘‘ğ‘–ğ‘ |>0and|ğ‘›ğ‘’ğ‘”ğ‘‘ğ‘–ğ‘ |<0:
10ğœâ†0.2;
11elif|ğ‘ğ‘œğ‘ ğ‘‘ğ‘–ğ‘ |>0:
12ğœâ†1;
13forğ‘’ğ‘âˆˆğ´ğ‘‘ğ‘—ğ‘‡
ğ‘Ÿğ‘ .ğ‘’do
14ğ‘‘ğ‘–ğ‘ â†ğ·ğ‘’[ğ‘Ÿğ‘ .ğ‘’][ğ‘Ÿğ‘’.ğ‘’]âˆ’ğ·ğ‘’[ğ‘’ğ‘][ğ‘Ÿğ‘’.ğ‘’];
15 ifğ‘‘ğ‘–ğ‘ >0:
16ğ‘‡ğ‘Ÿ.push( <ğ‘Ÿğ‘ .ğ‘’,ğ‘’ğ‘,ğ‘Ÿğ‘’,ğ‘,Hğ‘>,ğ‘’ğ‘¥ğ‘(âˆ’ğ‘‘ğ‘–ğ‘ )Ã—ğœÃ
ğ‘‘â€²âˆˆğ‘ğ‘œğ‘ ğ‘‘ğ‘–ğ‘ ğ‘’ğ‘¥ğ‘(ğ‘‘â€²));
17 else:
18ğ‘‡ğ‘Ÿ.push( <ğ‘Ÿğ‘ .ğ‘’,ğ‘’ğ‘,ğ‘Ÿğ‘’,ğ‘,Hğ‘>,ğ‘’ğ‘¥ğ‘(âˆ’ğ‘‘ğ‘–ğ‘ )Ã—(1âˆ’ğœ)Ã
ğ‘‘â€²âˆˆğ‘›ğ‘’ğ‘”ğ‘‘ğ‘–ğ‘ ğ‘’ğ‘¥ğ‘(ğ‘‘â€²));
19end
20returnğ‘‡ğ‘Ÿ;
First, we directly use the popular YOLOv7 [ 29] to locate the
coordinates of vehicles in the images captured by traffic cameras.
Then, we combine such information with the lane boundaries and
turn details (such as straight, left, right, U-turn, etc) defined by the
installer during camera installation. Based on that, we can identify
the lane of the moving vehicles from camera snapshots.
Next, given a road network ğº, we employ the Dijkstra algo-
rithm to obtain the shortest distances between every pair of road
segments, forming the matrix ğ·ğ‘’, whereğ·ğ‘’[ğ‘’ğ‘][ğ‘’ğ‘]denotes the
shortest distance from road segment ğ‘’ğ‘to road segment ğ‘’ğ‘. Note
that, this procedure is pre-processing and does not affect model
design. Based on the turning information of the vehicleâ€™s located
lane and the topological structure of ğº, we obtain the set of possible
adjacent road segments that the vehicle may head towards, denoted
asAdjğ‘‡
ğ‘Ÿğ‘ .ğ‘’.
Based on the above pre-processing, Algorithm 1 shows the train-
ing data/sample generation. The basic idea is that we categorize
potential adjacent road segments into two classes based on the
distance, i.e., either shorten or lengthen the distance to the desti-
nation. Specifically, it first calculates the total distance away from
(or towards) the destination for each class (lines 2â€“7). Note that we
cannot simply assume that a vehicle will necessarily head towards
an adjacent road segment close to the destination, even if the prob-
ability is relatively high. Thus, if there are adjacent road segmentsboth close to and far from the destination, we set a hyperparameter
ğœto 0.2 (lines 9â€“10). This value indicates that there is a certain
likelihood that the vehicle will head towards road segments that
are far from the destination. This approach enables generating di-
verse samples and preventing the model from consistently choosing
adjacent road segments close to the destination, neglecting other
potential factors such as road development levels. Finally, we use
the softmax function to calculate the transition probability labels
for each sample (lines 13â€“19).
B PARAMETER SENSITIVITY
0.76
0.74
0.72
0.70
0.74
0.72
0.700
0
0
0
0
0
0
0
0
0
0
0
0
0
016 1 2 4 8 16 1 2 4 816 32 64 128 256
ğ’…-value  16 32 64 128 256
ğ’…-value  
# of head # of headSimilarity Threshold Similarity ThresholdLCSSLCSS LCSS
STLCSTLC STLC
EDR EDR EDR
DTW DTWDTW
Figure 7: Different Similarity Threshold ğ‘‡â„.
0.76
0.74
0.72
0.70
0.74
0.72
0.700
0
0
0
0
0
0
0
0
0
0
0
0
0
016 1 2 4 8 16 1 2 4 816 32 64 128 256
ğ’…-value  16 32 64 128 256
ğ’…-value  
# of head # of headSimilarity Threshold Similarity ThresholdLCSSLCSS LCSS
STLCSTLC STLC
EDR EDR EDR
DTW DTWDTW
Figure 8: Different Dimension ğ‘‘Value.
0.76
0.74
0.72
0.70
0.74
0.72
0.700
0
0
0
0
0
0
0
0
0
0
0
0
0
016 1 2 4 8 16 1 2 4 816 32 64 128 256
ğ’…-value  16 32 64 128 256
ğ’…-value  
# of head # of headSimilarity Threshold Similarity ThresholdLCSSLCSS LCSS
STLCSTLC STLC
EDR EDR EDR
DTW DTWDTW
Figure 9: Different Number of Head ğ».
We investigate the sensitivity of three key parameters: vehicle
snapshot similarity threshold ğ‘‡â„, the road segment representation
dimensionğ‘‘, and the head number ğ», which determine the rep-
resentation ability of the model. (i) The results as shown in Fig. 7
reveal that low ğ‘‡â„value tends to introduce more noise from other
vehicles, leading to poor performance, where as a high ğ‘‡â„value
may filter out some valid passing records. (ii) Additionally, Fig. 8
presents the performance on different values of dimension ğ‘‘. We
observe a gradual improvement in performance as the dimension
5989KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Dongen Wu, et al.
value increases. Performance stabilizes when the dimension ex-
ceeds 128. This is because when the value of ğ‘‘is too small, the
resulting feature vectors fail to adequately capture the informa-
tion of road segments. Conversely, an excessively large dimension
value introduces redundancy. Therefore, we set ğ‘‘to 128 to strike
a balance between expressive feature representation and avoiding
unnecessary redundancy. (iii) Finally, we conducted experiments
using different numbers of heads ğ», and the results are presented in
Fig. 9. We observed that when ğ»is greater than 1, the performance
is relatively stable across various values. Therefore, we chose a
value of 4 as it offered a relatively better performance.
5990