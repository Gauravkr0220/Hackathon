Capturing Homogeneous Influence among Students: Hypergraph
Cognitive Diagnosis for Intelligent Education Systems
Junhao Shen
shenjh@stu.ecnu.edu.cn
School of Computer Science and
Technology, East China Normal
University
Shanghai, ChinaHong Qianâˆ—
hqian@cs.ecnu.edu.cn
School of Computer Science and
Technology, and Shanghai Institute of
AI Education, East China Normal
University
Shanghai, ChinaShuo Liu
shuoliu@stu.ecnu.edu.cn
School of Computer Science and
Technology, East China Normal
University
Shanghai, China
Wei Zhang
zhangwei.thu2011@gmail.com
School of Computer Science and
Technology, and Shanghai Institute of
AI Education, East China Normal
University
Shanghai, ChinaBo Jiang
bjiang@deit.ecnu.edu.cn
School of Computer Science and
Technology, and Shanghai Institute of
AI Education, East China Normal
University
Shanghai, ChinaAimin Zhou
amzhou@cs.ecnu.edu.cn
School of Computer Science and
Technology, and Shanghai Institute of
AI Education, East China Normal
University
Shanghai, China
Abstract
Cognitive diagnosis is a vital upstream task in intelligent educa-
tion systems. It models the student-exercise interaction, aiming
to infer the studentsâ€™ proficiency levels on each knowledge con-
cept. This paper observes that most existing methods can hardly
effectively capture the homogeneous influence due to its inher-
ent complexity. That is to say, although students exhibit similar
performance on given exercises, their proficiency levels inferred
by these methods vary significantly, resulting in shortcomings in
interpretability and efficacy. Given the complexity of homogeneous
influence, a hypergraph could be a choice due to its flexibility and
capability of modeling high-order similarity which aligns with
the nature of homogeneous influence. However, before incorpo-
rating hypergraph, one at first needs to address the challenges of
distorted homogeneous influence, sparsity of response logs, and
over-smoothing. To this end, this paper proposes a hypergraph
cognitive diagnosis model (HyperCDM) to address these challenges
and effectively capture the homogeneous influence. Specifically, to
avoid distortion, HyperCDM employs a divide-and-conquer strat-
egy to learn student, exercise and knowledge representations in
their own hypergraphs respectively, and interconnects them via
a feature-based interaction function. To construct hypergraphs
based on sparse response logs, the auto-encoder is utilized to pre-
process response logs and ğ¾-means is applied to cluster students.
To mitigate over-smoothing, momentum hypergraph convolution
âˆ—Hong Qian is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3672002networks are designed to partially keep previous representations
during the message propagation. Extensive experiments on both of-
fline and online real-world datasets show that HyperCDM achieves
state-of-the-art performance in terms of interpretability and cap-
turing homogeneous influence effectively, and is competitive in
generalization. The ablation study verifies the efficacy of each com-
ponent, and the case study explicitly showcases the homogeneous
influence captured by HyperCDM.
CCS Concepts
â€¢Applied computing â†’Education; â€¢Computing methodolo-
giesâ†’Machine learning.
Keywords
Student proficiency inference, Cognitive diagnosis, Homogeneous
influence, Hypergraph
ACM Reference Format:
Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, and Aimin Zhou.
2024. Capturing Homogeneous Influence among Students: Hypergraph
Cognitive Diagnosis for Intelligent Education Systems. In Proceedings of
the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining
(KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA,
12 pages. https://doi.org/10.1145/3637528.3672002
1 Introduction
Cognitive diagnosis [ 25,29] is a fundamental upstream task in intel-
ligent education systems [ 2], and extensive applications are found
in areas such as exercise recommendation [ 18] and computerized
adaptive testing [ 46]. An illustrative example is depicted in Figure 1.
Generally, students are tasked with completing some exercises (e.g.,
ğ‘’1,ğ‘’2,ğ‘’3,ğ‘’4), and educators can collect response logs. The main ob-
ject of cognitive diagnosis is to discover the studentsâ€™ proficiency
levels on specific knowledge concepts (e.g., linear processing) and
exercisesâ€™ features (e.g., difficulty) via the analysis of these response
logs, which can be used in various downstream tasks.
 
2628
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, and Aimin Zhou
e1
e2
e3
e4Cognitive DiagnosisBob Alice
Response LogsCalculation, InequalityGeometry, Linear ProcessingGeometry, Triangle
Inequality, Linear Processing
Diagnostic OutcomesBob Alice
Q-matrixLinear
Processing
Calculation
Inequality GeometryTriangle
Bob
Alice
Complete Exercisese1
Geometry: 0.1
Triangle: 0.2Difficulty
Figure 1: An illustrative example of cognitive diagnosis.
(b) NCDM (c) KSCD
(e) RCD (d) KaNCD(a) Sample Response Logs1
1
11
1
11
11
1
11
1
11
1
11
1
11
1
11
1
11
1
11
11
11
00
0
00 0
0 0
0 00
0
0Correct
CommonIncorrect
Figure 2: The motivation example and preliminary study
results. The sampled response logs containing three similar
students whose IDs are ğ‘ 77,ğ‘ 704andğ‘ 3307. Their inferred pro-
ficiency levels obtained by different CDMs on the real-world
dataset Math1 are visualized by t-SNE. Each point represents
a student and is assigned color ranging from red to purple
based on the correctness rates of their answers. Students with
similar correctness rates have similar colors, while with sim-
ilar inferred proficiency levels are positioned close.
In the past decades, considerable efforts have been invested in
th e development of cognitive diagnosis models (CDMs), which
model the student-exercise interaction as a function, i.e., interac-
tion function. For example, traditional CDMs [ 7,30,36] incorporate
manually-designed interaction functions based on expert experi-
ence. In contrast, neural network-based CDMs [ 27,39,40] leverage
the multi-layer perceptron to enhance the generalization and in-
terpretability of diagnostic outcomes. Another approach, symbolic
CDM [ 34], introduces symbolic tree as an explicit representation
of non-linear intricate student-exercise interactions. Besides, some
CDMs utilizes large language model [ 23] to diagnose student based
on ample prior knowledge. Recently, a promising direction has
emerged that utilizes graph neural networks (GNNs) to learn the
structure of student-exercise-knowledge relation graphs, providing
valuable insights for cognitive diagnosis. For example, relation map
driven cognitive diagnosis model (RCD) [ 11] establishes relation
graphs to fully utilize the rich information.
Although aforementioned CDMs have made progress in cogni-
tive diagnosis, we have observed an important issue that is often
ignored by most existing CDMs: the homogeneous influenceamong students. In educational psychology, the homogeneous
influence among students is frequently explained by social learning
theory [ 3], suggesting that a studentâ€™s learning is influenced by
peers who exhibit similar behaviors or characteristics. However,
due to the multifaceted nature of learning and intricate non-linear
interaction among students, the backbones of existing CDMs may
not be flexible and complex enough, making them hard to capture
the homogeneous influence among students. Consequently, this
could lead to shortcoming in interpretability and efficacy of diag-
nosis, thereby impacting downstream tasks and the practical use of
teachers and students in real-world scenarios.
In order to understand this issue intuitively and support the mo-
tivation of this paper, we conduct a preliminary study of cognitive
diagnosis on the real-world dataset Math1 [ 26] to analyze four rep-
resentative CDMs (all tuned to optimum and data split identically).
The studentsâ€™ proficiency levels obtained by them are visualized by
t-SNE [ 37] respectively. As shown in Figure 2, although their re-
sponse logs are similar and correctness rate is the same in (a), their
proficiency levels inferred by these methods separate significantly,
implying an inability to cluster similar students and suboptimal
performance in cognitive diagnosis. The quantitative experimental
results based on our proposed metrics in Section 5.2 also validate
the deficiencies when homogeneous influence is not considered.
Since the backbones of existing CDMs, such as the multi-layer
perceptron or pair-wise attention networks, may be inadequate in
effectively capturing the homogeneous influence among students,
a crucial question is then raised: Is there any more flexible and ef-
fective way to model the high-order and non-linear homogeneous
influence among students? Fortunately, the answer is YES: A hyper-
graph could offer an effective solution since it possesses significant
advantages in modeling high-order correlations [ 12] and is more
complex than pair-wise relations adopted by RCD, and has found
extensive applications in fields such as computer vision [ 19] and
recommendation systems [ 43]. However, directly applying the hy-
pergraph to cognitive diagnosis is not feasible due to the following
challenges: i) Distorted homogeneous influence. The use of
heterogeneous hypergraphs such as relation maps utilized by ex-
isting CDMs may lead to homogeneous influence among students
being distorted by interference from exercises and knowledge con-
cepts, finally reducing the interpretability and efficacy of diagnosis.
ii) Sparsity in response logs. Some work in other domains has
provided construction methods for hypergraphs [ 12]. However, be-
cause students may not complete every exercise, there is a high
degree of sparsity in response logs of some online datasets. Con-
structing a graph directly based on it could lead to a graph with
very few and sparse usable edges, hindering the effective utilization
of homogeneous influence. iii) Over-smoothing problem. The
 
2629Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
GNNs-based CDMs with embedding propagation layers may diffi-
cultly provide distinguishable studentsâ€™ proficiency levels, which
adversely affects the representation quality of diagnostic outcomes.
To this end, this paper proposes a hypergraph cognitive diagnosis
model (HyperCDM) to address these challenges and effectively cap-
ture the homogeneous influence. In particular, to avoid distortion,
HyperCDM employs a divide-and-conquer strategy to learn stu-
dent, exercise and knowledge concept representations in their own
hypergraphs respectively, and interconnects them via a feature-
based interaction function. To construct hypergraphs based on
sparse response logs, the auto-encoder is utilized to preprocess re-
sponse logs and ğ¾-means is applied to cluster students. To mitigate
over-smoothing, momentum hypergraph convolution networks
(MHGCN) are designed to partially keep previous representations
during the message propagation on the networks. In experimental
analysis, we also propose homogeneity index (HI) and consistency
index (CI) as metrics of the capability of capturing homogeneous
influence. Extensive experiments on four real-world datasets show
that HyperCDM achieves state-of-the-art performance in inter-
pretability and capturing the homogeneous influence, and compet-
itively strong performance in generalization. The ablation study
verifies the efficacy of each part in HyperCDM, hyperparameter
analysis provides some insights, and the case study explicitly show-
cases the homogeneous influence captured by HyperCDM.
In the subsequent sections, we respectively recap the related
work, introduce the preliminaries, present the proposed HyperCDM,
analyze the experimental results and finally conclude the paper.
2 Related Work
Cognitive Diagnosis. Cognitive diagnosis is a vital field in educa-
tional psychology. Unlike knowledge tracing [ 1,6,35], it focuses on
the diagnosis of static cognitive states. In recent decades, consider-
able efforts have been invested in the development of cognitive diag-
nosis models (CDMs), such as item response theory (IRT) [ 30], mul-
tidimensional IRT (MIRT) [ 36], deterministic inputs, noisy and gate
model (DINA) [ 7], neural cognitive diagnosis model (NCDM) [ 39],
knowledge-association neural cognitive diagnosis model (KaNCD)
[40], Q-augmented causal cognitive diagnosis model (QCCDM) [ 27],
symbolic cognitive diagnosis model (SCDM) [ 34] and foundation
model enhanced cognitive diagnosis model (FineCD) [ 23]. These
methods model the student-exercise interaction as the interaction
function. Specifically, IRT, MIRT and DINA utilize interaction func-
tions annotated by experts. NCDM, KaNCD and QCCDM leverage
neural networks to capture intricate non-linear interactions, SCDM
utilizes symbolic regression to explicitly capture non-linear inter-
action, and FineCD utilizes LLMs. Recently, some research has
focused on graph-based CDMs since graphs can model more com-
plex relationships than previous ones. For example, relation map
driven cognitive diagnosis model (RCD) [ 11] and supervised graph
learning cognitive diagnosis model (SCD) [ 41] establishes relation
graphs to integrate structural information, and inductive cognitive
diagnosis model (ICDM) [ 28] designs student-centered graph to
aggregate neighborsâ€™ embeddings for fast diagnosis. As aforemen-
tioned, however, we observe that these models may not be flexible
enough to effectively capture the homogeneous influence among
students, sometimes leading to deficiencies in diagnosis.Graph Representation Learning. In the past decades, graphs
have found widespread applications in data mining and machine
learning, whose representation learning, especially based on deep
learning techniques, has become a popular topic. The message-
passing graph neural networks (GNN) is the primary framework of
GNN, including graph convolutional networks (GCN) [ 21], graph at-
tention networks (GAT) [ 38], GraphSAGE [ 14] and others. Recently,
some work has focused on hypergraphs, where the traditional pair-
wise edges are expanded into hyperedges capable of connecting an
arbitrary number of nodes. To name a few, hypergraph convolution
networks (HGCN) [ 10] and hypergraph transformer [ 4,8]. As afore-
mentioned, directly adopting existing architecture is not feasible
due to the challenges of distorted homogeneous influence, sparsity
of response logs and over-smoothing. Note that while transformer-
based GNNs outperform traditional methods in alleviating over-
smoothing and over-squashing [ 24] issues, they require a larger
volume of training data to achieve the optimal performance [ 45]
and lack of interpretability, which is not suitable for this task.
3 Preliminaries
3.1 Cognitive Diagnosis
Letğ‘†={ğ‘ 1,...,ğ‘ |ğ‘†|},ğ¸={ğ‘’1,...,ğ‘’|ğ¸|}, andğ¾={ğ‘˜1,...,ğ‘˜|ğ¾|}
respectively denote the sets of students, exercises and knowledge
concepts, where|Â·|calculates the size of set. Each student is required
to complete some exercises, and the corresponding response logs are
denoted as a set of triplets ğ‘…={ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘–ğ‘—|ğ‘ ğ‘–âˆˆğ‘†,ğ‘’ğ‘—âˆˆğ¸,ğ‘Ÿğ‘–ğ‘—âˆˆ{0,1}},
whereğ‘Ÿğ‘–ğ‘—represents the score (i.e., ğ‘Ÿğ‘–ğ‘—=1means right while ğ‘Ÿğ‘–ğ‘—=0
wrong) obtained by student ğ‘ ğ‘–on exerciseğ‘’ğ‘—. Besides, the Q-matrix,
typically annotated by experts, is denoted as ğ‘¸={ğ‘ğ‘–,ğ‘—}|ğ¸|Ã—|ğ¾|,
which includes the relationship between exercises and knowledge
concepts. Here, ğ‘ğ‘–,ğ‘—=1if exerciseğ‘’ğ‘–is associated with knowledge
conceptğ‘˜ğ‘—, andğ‘ğ‘–,ğ‘—=0otherwise.
Task Definition. Given the observed triplet logs of students
ğ‘…and the labeled Q-matrix ğ‘¸, the goal of the task is to infer the
studentsâ€™ proficiency levels on knowledge concepts.
To ensure the interpretability, recent work also incorporates the
monotonicity assumption [ 32] and it is defined as Assumption 1.
We assess diagnostic outcomes based on it in Section 5.2.
Assumption 1 (Monotonicity Assumption). The probability
of providing a correct response to the exercise consistently rises with
the studentâ€™s proficiency level on relevant knowledge concepts.
Besides, as aforementioned, the homogeneous influence among
students is expected to play a crucial role in cognitive diagnosis [ 3],
but little attention has been devoted to this issue in previous studies.
We define this assumption as Assumption 2 based on social learning
theory [ 3] and assess diagnostic outcomes based on it in Section 5.2.
The similarity can be measured via distances like cosine.
Assumption 2 (Homogeneity Assumption). If students have
similar correctness rates and perform similarly in answering exercises,
their proficiency levels on relevant knowledge concepts are similar.
3.2 Hypergraph
An edge of the simple graph only connects two vertices, but a
hyperedge of the hypergraph establishes connections among two
or more vertices. A hypergraph is defined as G=(V,E), including
 
2630KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, and Aimin Zhou
Q-matrixResponse LogsLatent Response 
Logs FeatureStudent HypergraphStudent
EmbeddingsStudent Feature
Exercise 
Embeddings
Knowledge
EmbeddingsExercise HypergraphOverlap
Clustering
Knowledge HypergraphRelevance ConstructionPretrained
Auto
Encoder
Exercise Feature
Discrimination
Knowledge Feature...
Layer
Layer 2
Layer 1 MHGCN...
Layer
Layer 2
Layer 1...
Layer
Layer 2
Layer 1...MHGCN
MHGCNKnowledge
GeneratorDiscrimination
GeneratorExercise
GeneratorStudent
Generator
Feature FeatureMultilayer Positive Perceptron
Interaction Function w.r.t. the i-th Student and j-th Exercise(a) Hypergraph Construction (b) Hypergraph Convolution (c) Embedding Conversion (d) Unified Diagnosis Layer
Figure 3: An overview of hypergraph cognitive diagnosis model (HyperCDM).
a vertex setVand a hyperedge set E. The hypergraphGcan be
denoted by|V|Ã—|E| incidence matrix ğ‘¯, with entries defined as
â„(ğœˆ,ğœ€)=(
1,ifğœˆâˆˆğœ€;
0,ifğœˆâˆ‰ğœ€.(1)
The degree of vertex ğœˆâˆˆV is defined asÃ
ğœ€âˆˆEâ„(ğœˆ,ğœ€), and the de-
gree of an edge ğœ€âˆˆEisÃ
ğœˆâˆˆVâ„(ğœˆ,ğœ€). Besides, ğ‘«ğœ€andğ‘«ğœˆdenote
the diagonal matrices of the edge degrees and the vertex degrees,
respectively. Compared with the simple graph, a hypergraph in-
herently exhibits the capability to represent complex connections,
enabling the capture of complex homogeneous influence.
4 Hypergraph Cognitive Diagnosis
An overview of the hypergraph cognitive diagnosis model (Hy-
perCDM) is shown in Figure 3. Sequentially, student, exercise and
knowledge hypergraphs are constructed in different strategies, and
their representations are learned by the momentum hypergraph
convolution networks (MHGCN). These embeddings are converted
to diagnostic features, and these features are unified via an inter-
action function. HyperCDM infers studentsâ€™ proficiency levels by
predicting the response of students on some exercises.
4.1 Hypergraph Construction
Firstly, we consider the construction of the exercise hypergraph and
knowledge hypergraph. Unlike studentsâ€™ response logs, exercises
and knowledge concepts inherently exhibit a low-noise relationship
matrix, i.e., Q-matrix ğ‘¸(similar to previous work, we affirm the
expert-annotated ğ‘¸is highly credible and needs no adjustment).
And the construction is shown as Eq. (2).
ğ‘¯ğ¸=ğ‘¸,ğ‘¯ğ¾=ğ‘¸âŠ¤, (2)
where the ğ‘¯ğ¸is the incidence matrix of the exercise hypergraph
andğ‘¯ğ¾is that of the knowledge hypergraph, as shown in Fig-
ure 3 (a). Next, we tackle constructing the student hypergraph.
As mentioned before, students may not complete all exercises. Di-
rectly constructing a graph from their response logs could create
sparse edges, leading to inefficient use of homogeneous influence.
Therefore, inspired by [ 44], the construction method is depicted in
Figure 4. We convert triplet response logs into a more manageable
response matrix ğ‘¹as Eq. (3), where the Ë†ğ’“ğ‘–is theğ‘–-th column of re-
sponse matrix ğ‘¹(if not specified, all mentioned vectors are column
Response Matrix Auto-encoderReconstructing
Response Matrix
Latent Response 
Logs FeatureOverlap Clustering Hypergraph(a)
(b)Figure 4: Details of the student hypergraph construction.
(a) Utilizing auto-encoder to process response matrix. (b) Af-
ter gaining the latent response logs feature, the overlap clus-
tering is applied to construct the hypergraph.
vectors), and also the response logs of student ğ‘ ğ‘–.
ğ‘¹={Ë†ğ‘Ÿğ‘—ğ‘–}|ğ¸|Ã—|ğ‘†|,Ë†ğ‘Ÿğ‘—ğ‘–=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³1,if(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘–ğ‘—)âˆˆğ‘…,ğ‘Ÿğ‘—ğ‘–=1
âˆ’1,if(ğ‘ ğ‘–,ğ‘’ğ‘—,ğ‘Ÿğ‘–ğ‘—)âˆˆğ‘…,ğ‘Ÿğ‘—ğ‘–=0
0,otherwise.(3)
As shown in Figure 4 (a), we obtain clustering-friendly latent repre-
sentations via auto-encoder, i.e.,
Ëœğ’“ğ‘–=A(Ë†ğ’“ğ‘–;Î˜A),A(Â·;Î˜A):R|ğ¸|â†¦â†’RËœ|ğ¸|, (4)
where Ëœğ’“ğ‘–is the latent response logs feature of student ğ‘ ğ‘–,Ais
the non-linear encoder mapping, Ëœ|ğ¸|is the dimension of latent
feature, and Î˜Ais a set of parameters of encoder. To improve
the performance of subsequent clustering, we pre-train the auto-
encoder via reconstruction loss Lrec, i.e.,
Lrec=|ğ‘†|âˆ‘ï¸
ğ‘–=1||Aâˆ’1(A(Ë†ğ’“ğ‘–;Î˜A);Î˜Aâˆ’1)âˆ’Ë†ğ’“ğ‘–||2, (5)
whereAâˆ’1(Â·;Î˜Aâˆ’1):RËœ|ğ¸|â†¦â†’R|ğ¸|is the decoder, Î˜Aâˆ’1is its set
of parameters, and ||Â·|| 2isğ‘™2norm.
After pre-training, we obtain the hypergraph using the overlap
ğ¾-means. Initially, following the classical ğ¾-means [ 15], the op-
timization objective is defined as Eq. (6), where each column of
 
2631Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1 Alternating Clustering
Hyperparameter: Maximum number of epoch ğ‘‡.
1:forğ‘¡=1,...,ğ‘‡ do
2: Update network parameters A,Aâˆ’1viaâˆ‡(A,Aâˆ’1)Lcons
and stochastic gradient descent optimizer (e.g., Adam).
3: forğ‘–=1,...,|ğ‘†|do
4: Update cluster assignment as follows:
ğ‘œğ‘—,ğ‘–â†ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³1,ifğ‘—=arg min
ğ‘˜=1,Â·Â·Â·,K||A(Ë†ğ’“ğ‘–;Î˜A)âˆ’ğ’„ğ‘˜||2;
0,otherwise.
5: Update all centroids as follows:
ğ’„ğ‘˜â†ğ’„ğ‘˜âˆ’ğ›¿ğ‘–
ğ‘˜(ğ’„ğ‘˜âˆ’A( Ë†ğ’“ğ‘–;Î˜A))ğ‘œğ‘˜,ğ‘–.
6: end for
7:end for
centroid matrix ğ‘ª=[ğ’„1,Â·Â·Â·,ğ’„K]âˆˆRËœ|ğ¸|Ã—Kis the feature repre-
sentation ofKcentroids, and ğ’ğ‘–is the one-hot vector to signify
which clustering the ğ‘–-th studentâ€™s latent feature belongs to.
min
ğ‘ª,{ğ’ğ’Š}Jclt=|ğ‘†|âˆ‘ï¸
ğ‘–=1||A(Ë†ğ’“ğ‘–;Î˜A)âˆ’ğ‘ªÂ·ğ’ğ‘–||2
s.t.ğ’ğ’Šâˆˆ{0,1}Ëœ|ğ¸|,1âŠ¤ğ’ğ‘–=1,âˆ€ğ‘–.(6)
And we give the loss function of hypergraph construction,
Lcons=Lrec+Jclt. (7)
However, optimizing Eq. (7)is challenging since both the loss
function and the constraints are non-convex. Besides, scalability
issues should be taken into consideration. Therefore, after the
pre-training process, the clustering algorithm is shown in Algo-
rithm 1. Line 2 shows further optimization based on the pre-trained
auto-encoder using a stochastic gradient descent optimizer (e.g.,
Adam [ 20]). Line 4 entails reassigning the cluster centroids for each
student based on the Euclidean distance. In line 5, an adaptive ap-
proach [ 33] is employed to update the cluster centers, where ğ›¿ğ‘–
ğ‘˜is
the inverse of the frequency that the algorithm assigns students to
clusterğ‘˜before processing the incoming student ğ‘ ğ‘–.
Finally, we construct a student hypergraph based on the Eu-
clidean distance between each studentâ€™s latent feature and centroids.
Initially, we compute the first quartile ğ›¾0.25ğ‘˜of the distances from
students to each clustering ğ‘˜, and fill incidence matrix of student
hypergraph ğ‘¯ğ‘†âˆˆR|ğ‘†|Ã—Kas follows in Figure 4 (b):
ğ‘¯ğ‘†={â„ğ‘†ğ‘–,ğ‘˜}, â„ğ‘†ğ‘–,ğ‘˜=(
1,if||A(Ë†ğ’“ğ‘–;Î˜A)âˆ’ğ’„ğ‘˜||2â‰¤ğ›¾0.25ğ‘˜;
0,otherwise.(8)
4.2 Hypergraph Convolution
Upon constructing hypergraphs, the next step is to learn the graph
representation. Given a hypergraph derived from datasets, the inci-
dent matrix ğ‘¯and vertex embeddings in ğ‘™-th layer ğ‘¿(ğ‘™)are inputted
into hypergraph convolution networks [10, 12] defined as:
ğ‘¿(ğ‘™+1)=ğœ
ğ‘«âˆ’1
2ğœˆğ‘¯ğ‘«âˆ’1
ğœ€ğ‘¯âŠ¤ğ‘«âˆ’1
2ğœˆğ‘¿(ğ‘™)ğš¯(ğ‘™)
, (9)whereğš¯(ğ‘™)is a trainable parameter matrix at the ğ‘™-th layer, and ğœ(Â·)
denotes an non-linear activation function (e.g., Tanh ). However,
He et al . [16] utter that since each node has no concrete semantics
features, applying multi-layer non-linear feature transformation
(e.g., activation function) will increase training difficulty, impede
speed and impair performance. Besides, the over-smoothing prob-
lem is prevalent in message-passing GNNs. To expedite training,
enhance performance, and alleviate over-smoothing, we propose
the momentum hypergraph convolution networks (MHGCN) and
sum up the embeddings of each layer to combine them:
ğ‘¿(ğ‘™+1)=ğ‘«âˆ’1
2ğœˆğ‘¯ğ‘«âˆ’1
ğœ€ğ‘¯âŠ¤ğ‘«âˆ’1
2ğœˆğ‘¿(ğ‘™)+ğœ‚Â·ğ‘¿(ğ‘™),
ğ‘¿=1
1+ğ¿ğ¿âˆ‘ï¸
ğ‘™=0ğ‘¿(ğ‘™),(10)
whereğœ‚is momentum parameter and ğ¿is the number of layers of
MHGCN. The omission of trainable parameter Î˜(ğ‘™)and non-linear
transformations ğœaccelerates training speed and improves perfor-
mance. To mitigate over-smoothing, we integrate embeddings from
previous layers partially during the update. This structural modifi-
cationâ€™s efficacy is confirmed through ablation study in Section 5.2.
Based on MHGCN, as shown in Figure 3 (b), for each hypergraph,
the convolution is defined as Eq. (11).
ï£±ï£´ï£´ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£´ï£´ï£³ğ‘¿(ğ‘™+1)
ğ‘†=ğ‘«âˆ’1
2
ğ‘†ğœˆğ‘¯ğ‘†ğ‘«âˆ’1
ğ‘†ğœ€ğ‘¯âŠ¤
ğ‘†ğ‘«âˆ’1
2
ğ‘†ğœˆğ‘¿(ğ‘™)
ğ‘†+ğœ‚Â·ğ‘¿(ğ‘™)
ğ‘†
ğ‘¿(ğ‘™+1)
ğ¸=ğ‘«âˆ’1
2
ğ¸ğœˆğ‘¯ğ¸ğ‘«âˆ’1
ğ¸ğœ€ğ‘¯âŠ¤
ğ¸ğ‘«âˆ’1
2
ğ¸ğœˆğ‘¿(ğ‘™)
ğ¸+ğœ‚Â·ğ‘¿(ğ‘™)
ğ¸
ğ‘¿(ğ‘™+1)
ğ¾=ğ‘«âˆ’1
2
ğ¾ğœˆğ‘¯ğ¾ğ‘«âˆ’1
ğ¾ğœ€ğ‘¯âŠ¤
ğ¾ğ‘«âˆ’1
2
ğ¾ğœˆğ‘¿(ğ‘™)
ğ¾+ğœ‚Â·ğ‘¿(ğ‘™)
ğ¾,(11)
where subscripts ğ‘†,ğ¸,ğ¾ mean components in the student, exer-
cise and knowledge hypergraph respectively, and ğ‘¿(ğ‘™)
ğ‘†,ğ‘¿(ğ‘™+1)
ğ‘†âˆˆ
R|ğ‘†|Ã—ğ‘‘emb,ğ‘¿(ğ‘™)
ğ¸,ğ‘¿(ğ‘™+1)
ğ¸âˆˆR|ğ¸|Ã—ğ‘‘emb,ğ‘¿(ğ‘™)
ğ¾,ğ‘¿(ğ‘™+1)
ğ¾âˆˆR|ğ¾|Ã—ğ‘‘emb. The
layer combination is defined as Eq. (12).
ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³ğ‘¿ğ‘†=1
1+ğ¿Ãğ¿
ğ‘™=0ğ‘¿(ğ‘™)
ğ‘†
ğ‘¿ğ¸=1
1+ğ¿Ãğ¿
ğ‘™=0ğ‘¿(ğ‘™)
ğ¸
ğ‘¿ğ¾=1
1+ğ¿Ãğ¿
ğ‘™=0ğ‘¿(ğ‘™)
ğ¾, (12)
where ğ‘¿ğ‘†âˆˆR|ğ‘†|Ã—ğ‘‘emb,ğ‘¿ğ¸âˆˆR|ğ¸|Ã—ğ‘‘emb, and ğ‘¿ğ¾âˆˆR|ğ¾|Ã—ğ‘‘emb.
4.3 Embedding Conversion
Some work [ 7,39] usually sets ğ‘‘embas the number of knowledge
concepts|ğ¾|, and inputs them into the interaction functions. How-
ever, forcing the dimension to |ğ¾|may reduce the performance.
This problem is commonly faced in some graph downstream tasks
like recommendation systems [ 16]. To address it, we first embed
students, exercises and knowledge concepts into a low-dimensional
latent space. After convolution, we apply a linear transformation to
increase dimension of embeddings, which is expressed as follows:
ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³Ëœğ‘¿ğ‘†=ğœ(Linearğ‘†(ğ‘¿ğ‘†;Î˜ğ‘†))
Ëœğ‘¿ğ¸=ğœ(Linearğ¸(ğ‘¿ğ¸;Î˜ğ¸))
Ëœğ‘¿ğ¾=ğœ(Linearğ¾(ğ‘¿ğ‘†;Î˜ğ¾)), (13)
where Î˜ğ‘†,Î˜ğ¸,Î˜ğ¾are parameters in each linear transformation,
Linearğ‘†(Â·;Î˜ğ‘†)is the mapping R|ğ‘†|Ã—ğ‘‘embâ†¦â†’R|ğ‘†|Ã—ğ‘‘feat,Linearğ¸(Â·;Î˜ğ¸)
isR|ğ¸|Ã—ğ‘‘embâ†¦â†’R|ğ¸|Ã—ğ‘‘feat,Linearğ¾(Â·;Î˜ğ¾)isR|ğ¾|Ã—ğ‘‘embâ†¦â†’R|ğ¾|Ã—ğ‘‘feat,
 
2632KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, and Aimin Zhou
andğœis the activation function (here is LeakyReLU). It is notewor-
thy thatğ‘‘featmay not always equal |ğ¾|, and typically it satisfies
ğ‘‘featâ‰¥|ğ¾|>ğ‘‘emb. This is because more parameters can result in
a more effective fit [ 17], which is also validated in hyperparame-
ter analysis of Section 5.2. At the same time, discrimination holds
equal importance as another aspect of exercise features (i.e., diffi-
culty) [ 25,29]. We obtain the discrimination ğƒdiscbased on the exer-
cise embeddings, and employ a method similar to Eq. (13), as shown
below. The Linear disc(Â·;Î˜disc)is the mapping R|ğ¸|Ã—ğ‘‘embâ†¦â†’R|ğ¸|Ã—1,
andÎ˜discis a set of parameters in the linear transformation.
ğƒdisc=ğœ(Linear disc(ğ‘¿ğ¸;Î˜disc)). (14)
4.4 Unified Diagnosis Layer
According to the principle of divide-and-conquer, we first model
students, exercises and knowledge concepts respectively using ho-
mogeneous hypergraphs, aiming to prevent interference from dif-
ferent types of nodes in capturing homogeneous influence; then,
we employ a unified interaction function to combine these three
components to simultaneously train these features. As illustrated
in Figure 3 (d), for student ğ‘ ğ‘–and exercise ğ‘’ğ‘—, the corresponding
proficiency level, exercise difficulty, discrimination and relevance
features are obtained as follows:
ğƒprofğ‘–=Ëœğ‘¿ğ¾Ëœğ‘¿âŠ¤
ğ‘†ğ’–ğ‘–,
s.t.ğ’–ğ‘–âˆˆ{0,1}|ğ‘†|,1âŠ¤ğ’–ğ‘–=1,ğ‘¢ğ‘–,ğ‘–=1 ;
ğƒdiffğ‘—=Ëœğ‘¿ğ¾Ëœğ‘¿âŠ¤
ğ¸ğ’›ğ‘—, ğœ‰discğ‘—=ğƒâŠ¤
discğ’›ğ‘—,ğƒreleğ‘—=ğ‘¸âŠ¤ğ’›ğ‘—,
s.t.ğ’›ğ‘—âˆˆ{0,1}|ğ¸|,1âŠ¤ğ’›ğ‘—=1,ğ‘§ğ‘—,ğ‘—=1,(15)
where ğƒprofğ‘–,ğƒdiffğ‘—,ğƒreleğ‘—âˆˆR|ğ¾|,ğœ‰discğ‘—âˆˆR, and ğ’–ğ‘–,ğ’›ğ‘—are one-
hot vectors. In Eq. (15), it is noteworthy that proficiency level and
difficulty are not directly obtained from feature matrices Ëœğ‘¿ğ‘†,Ëœğ‘¿ğ¸.
Instead, they are derived by multiplying Ëœğ‘¿ğ‘†,Ëœğ‘¿ğ¸with the knowl-
edge feature matrix Ëœğ‘¿ğ¾and extracting via one-hot vectors. This
is because: i)Performing a linear transformation via Ëœğ‘¿ğ¾to ensure
their dimensions correspond to |ğ¾|, thereby ensuring interpretabil-
ity (distinguishing it from previous work like MIRT [ 36]).ii)In the
knowledge hypergraph, the implicit associations between knowl-
edge concepts can be learned and kept in Ëœğ‘¿ğ¾. By incorporating
these associations into Ëœğ‘¿ğ‘†,Ëœğ‘¿ğ¸, it is possible to predict proficiency
level even when a student has minimal exposure or zero-shot on a
particular knowledge concept (especially in online datasets where
students are not required to interact with all concepts) and enhances
interpretability and capturing the homogeneous influence.
After obtaining these diagnostic factors, we input them into the
unified diagnostic layers defined as:
ğ‘¦ğ‘–ğ‘—=Sigmoid(MLP(ğœ‰discğ‘—Ã—(ğƒprofğ‘–âˆ’ğƒdiffğ‘—)â—¦ğƒreleğ‘—;Î˜inter)),(16)
whereğ‘¦ğ‘–ğ‘—is the prediction of the ğ‘–-th studentâ€™s response on the
ğ‘—-th exercise, Sigmoid(ğ‘¥)is the activation function1
1+ğ‘’âˆ’ğ‘¥,MLP(Â·):
R|ğ¾|â†¦â†’Ris the multi-layer perceptron, Î˜inter are parameters
whose weights are non-negative to adhere to Assumption 1, Ã—is
the scalar multiplication, and â—¦is the Hadamard product.
To train HyperCDM via prediction, we adopt cross entropy loss
Lbcebetween output ğ‘¦ğ‘–ğ‘—and the true label ğ‘Ÿğ‘–ğ‘—, and incorporate the
ğ‘™2regularization term to avoid over-fitting. The entire loss LisTable 1: Time complexity and space complexity analysis of
each component in HyperCDM. The symbol â€œâ€”â€ means that
this entry is not applicable to the component.
Comp
onent Time
Complexity Space
Complexity
Hyp
ergraph
ConstructionA
uto-encoderO
(ğ‘‡|ğ¸||ğ‘†|(|Î˜A|+|Î˜Aâˆ’1|))O
(|Î˜A|+|Î˜Aâˆ’1|)
O
verlap
ğ¾-meansO
(ğ‘‡K|ğ‘†|Ëœ|ğ¸|) O
(|ğ‘†|Ëœ|ğ¸|)
Repr
esentation
LearningIncidence
Matrixâ€” O
(2|ğ¸||ğ¾|+|ğ‘†|K)
Conv
olutionO
(ğ¿(trace(ğ‘«ğœˆ))O
(ğ‘‘emb(|ğ‘†|+|ğ¸|+|ğ¾|))
Conv
ersionO
(ğ‘‘feat|ğ‘†||Î˜ğ‘ |O
(|Î˜ğ‘†|+|Î˜ğ¸|+|Î˜ğ¾|+|Î˜disc|)
BCE
Loss O
(2|ğ‘…|) O
(2|ğ‘…|)
defined as Eq. (17), where Î˜including all parameters.
Lbce=âˆ’âˆ‘ï¸
ğ‘Ÿğ‘–ğ‘—âˆˆğ‘…(ğ‘Ÿğ‘–ğ‘—logğ‘¦ğ‘–ğ‘—+(1âˆ’ğ‘Ÿğ‘–ğ‘—)log(1âˆ’ğ‘¦ğ‘–ğ‘—)),
L=Lbce+ğœ†||Î˜||2.(17)
4.5 Discussion
We would like to emphasize some points related to HyperCDM.
Complexity. We consider both time complexity and space com-
plexity from two aspects: graph construction and graph learning.
Detailed analysis is shown in Table 1, where the number of stu-
dents|ğ‘†|is significantly larger than |ğ¸|,|ğ¾|, and we have omitted
the terms in the time complexity that only involve |ğ¸|and|ğ¾|.
Flexibility. In Figure 4, if necessary, we can also input additional
side-information to assist in the hypergraph construction, such as
studentsâ€™ background information or the problem context. And in
Figure 3 (d), other types of interaction functions (e.g., MIRT and
DINA) are also applicable, and adjustments to diagnostic factors
can be made based on the specific requirements.
Training. The hypergraph construction, in relation to subse-
quent graph representation learning, serves as a â€œpre-trainingâ€
phase. Specifically, once hypergraphs are constructed, the struc-
ture remains unchanged thereafter, which ensures the hypergraph
construction does not become a bottleneck of the efficiency of subse-
quent representation learning. In essence, hypergraph construction
and graph learning are two mutually independent processes.
5 Experiments
This section conducts experiments on real-world datasets to answer
the following research questions.1
â€¢Q1:Does the proposed HyperCDM successfully capture homoge-
neous influence among students?
â€¢Q2:How does HyperCDM perform in interpretability?
â€¢Q3:How does HyperCDM perform in generalization?
â€¢Q4:Does each component contribute to the performance?
â€¢Q5:How do hyperparameters influence HyperCDM?
â€¢Q6:How efficient is HyperCDM compared to existing methods?
â€¢Q7:Why and how does the diagnostic outcome of HyperCDM
work in real-world educational scenarios?
5.1 Experiment Setup
We explain the datasets, comparison methods, metrics and settings.
1The source code are available at https://github.com/shinkungoo/HyperCDM.
 
2633Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Details of the real-world datasets for experiments.
Dataset NeurIPS20
EdNet-1 Math1 Math2
#Student 3000
1827 4209 3911
#Exercise 6000
11996 15 16
#Knowledge Concepts 268
189 11 16
#Response Logs 215323
556770 63135 62576
Density 0.012
0.025 1.000 1.000
Dataset Description. The experiments are conducted on four
real-world datasets from both online (sparse) scenarios (NeurIPS20 [ 42],
EdNet-1 [ 5]) and offline (dense) scenarios (Math1, Math2 [ 26]).
These datasets cover common learning scenarios and are repre-
sentative, which can show our methodâ€™s versatility in various ed-
ucational scenarios. The density is computed as |ğ‘…|/(|ğ‘†|Ã—|ğ¸|). A
density value nearing 1 indicates that students have completed
nearly all exercises in the datasets, which is often seen in offline
datasets. Conversely, a density value nearing 0 suggests that stu-
dents may only complete a subset of the exercises in the datasets,
which is often seen in online datasets. Details of these datasets are
shown in Table 2, and their sources are explained in Appendix A.1.
Baselines and State-of-the-Art Methods. In recent decades,
various CDMs have been developing, some of which are represen-
tative methods and selected for comparison: MIRT [ 36], DINA [ 7],
NCDM [ 39], KaNCD [ 40], KSCD [ 31] and RCD [ 11]. Since graph-
based CDMs are less explored, we choose the strong model RCD as
a representative graph-based model for comparison. Note that these
methods are all open source and adjusted to optimum according to
their recommended settings in the paper. Details of these methods
and the source are shown in Appendix A.2.
Generalization Metrics. Assessing the performance of CDM
proves challenging due to the inherent difficulty in accurately ob-
serving studentsâ€™ proficiency levels. To address this challenge, a
widely accepted strategy is to evaluate them by predicting studentsâ€™
test scores. Similar to previous methods [ 11,39], we evaluate how
close the model predicts whether a student solves an exercise to
the ground truth in the test set with common classification metrics,
i.e., accuracy (Acc.), area under curve (AUC) and F1-score (F1).
Interpretability Metric. Generalization metrics are only one
facet of assessing the performance of CDMs. In previous work, de-
gree of agreement (DOA) [ 39,40] is usually applied to quantitatively
assess the interpretability of diagnostic outcomes. As Assumption 1,
if studentğ‘ ğ‘shows higher accuracy in responding to exercises re-
lated toğ‘˜ğ‘–than student ğ‘ ğ‘, it implies the proficiency level of ğ‘ ğ‘
on knowledge concept ğ‘˜ğ‘–(ğƒprofğ‘,ğ‘–) may surpass that of student ğ‘ ğ‘
(ğƒprofğ‘,ğ‘–). The DOA of ğ‘˜ğ‘–is defined as Eq. (18).
ğ·
ğ‘‚ğ´(ğ‘–)=1
ğ‘1|ğ‘†|Ã
ğ‘=1|ğ‘†|Ã
ğ‘=1Î›(ğƒpr
ofğ‘,ğ‘–,ğƒprofğ‘,ğ‘–)|ğ¸|Ã
ğ‘—=1ğ‘ğ‘—,ğ‘–ğ¼(ğ‘—,ğ‘,ğ‘)Â·Î›(ğ‘Ÿğ‘ğ‘—,ğ‘Ÿğ‘ğ‘—)
ğ¼(ğ‘—
,ğ‘,ğ‘),(18)
whereğ‘1=Ã|ğ‘†|
ğ‘=1Ã|ğ‘†|
ğ‘=1Î›(ğƒprofğ‘,ğ‘–,ğƒprofğ‘,ğ‘–), and ğƒprofğ‘,ğ‘–is the profi-
ciency level of student ğ‘ ğ‘on knowledge concept ğ‘˜ğ‘–.Î›(ğ‘¥,ğ‘¦)=1if
ğ‘¥>ğ‘¦and otherwise Î›(ğ‘¥,ğ‘¦)=0.ğ‘ğ‘—ğ‘–=1if exerciseğ‘’ğ‘—contains
knowledge concept ğ‘˜ğ‘–and otherwise ğ‘ğ‘—ğ‘–=0.ğ¼(ğ‘—,ğ‘,ğ‘)=1if both
studentğ‘ ğ‘andğ‘ ğ‘complete exercise ğ‘’ğ‘—and otherwise ğ¼(ğ‘—,ğ‘,ğ‘)=0.
For DOA, a greater value indicates stronger interpretability and ad-
herence to Assumption 1. To evaluate each knowledge concept, weaverageğ·ğ‘‚ğ´(ğ‘–)across all knowledge concepts for offline datasets
(i.e., DOA) and the top-10 frequent ones for online datasets (i.e.,
DOA@10), which is consistent with previous work [22, 27].
Homogeneity Metrics. To assess whether the model success-
fully captures the homogeneous influence among students, we
propose two metrics: homogeneity index (HI) and consistency in-
dex (CI). As Assumption 2, if the correctness rate of students ğ‘ ğ‘
andğ‘ ğ‘are equal, it implies that their proficiency levels should be
similar and comparable. Based on it, the ğ»ğ¼is defined as Eq. (19).
ğ»ğ¼=1
ğ‘2|ğ‘†|âˆ‘ï¸
ğ‘=1|ğ‘†|âˆ‘ï¸
ğ‘=1ğƒprofğ‘âˆ’ğƒprofğ‘2Ã—ğ½(ğ‘,ğ‘), (19)
whereğ‘2=Ã|ğ‘†|
ğ‘=1Ã|ğ‘†|
ğ‘=1ğ½(ğ‘,ğ‘). The function ğ½(ğ‘,ğ‘)=1if both
studentsğ‘ ğ‘andğ‘ ğ‘(ğ‘â‰ ğ‘) have an identical correctness rate and
otherwiseğ½(ğ‘,ğ‘)=0. The correctness rate is computed as the ratio
of correctly answered exercises to the total number of attempted
exercises. Besides, Assumption 2 also considers that students with
similar response logs should have similar proficiency levels. There-
fore, we present the formula for calculating CI as Eq. (20).
ğ¶ğ¼=1
ğ‘3|ğ‘†|âˆ‘ï¸
ğ‘=1|ğ‘†|âˆ‘ï¸
ğ‘=1ğƒprofğ‘âˆ’ğƒprofğ‘2Â·Ë†ğ’“âŠ¤ğ‘Â·Ë†ğ’“ğ‘
||Ë†ğ’“ğ‘||2Ã—||Ë†ğ’“ğ‘||2
,(20)
whereğ‘3=|ğ‘†|(|ğ‘†|âˆ’1)andÂ·denotes the inner product. Ë†ğ’“ğ‘is the
ğ‘-th column of the response matrix given in Section 4.1, and also
the student ğ‘ ğ‘â€™s response logs. For both HI and CI, a smaller value
indicates stronger homogeneity and adherence to Assumption 2. To
deeply understand these two metrics, please refer to Appendix A.4.
However, DOA, HI and CI are only applicable for NCDM, KaNCD,
KSCD and RCD since the diagnostic outcomes of MIRT only ex-
hibit vague correspondence between latent traits and knowledge
concepts, and DINA is confined to discrete representation.
Experiment Settings. During training, we initialize parameters
in all networks with Xavier normal initialization [ 13] and use Adam
optimizer [ 20] with a learning rate being 0.0001. The batch size is
64for all datasets and the ğ‘™2-regularization parameter ğ›¼=0.0005.
To evaluate performance, we perform an 80%/20% train/test split of
response logs, and the hypergraph construction is only based on
the train set. All experiments are repeated independently with 10
seeds. More Implementation details are shown in Appendix A.3.
5.2 Experimental Results
We conduct comprehensive experiments to analyze the experimen-
tal results and answer the aforementioned research questions.
Homogeneous Influence (Q1). The performance on homo-
geneity of all methods is shown in Table 3, and the variance of each
metric is less than 0.05. From the table: i)Byğ‘¡-test, HyperCDM
outperforms all baselines and state-of-the-art methods with sig-
nificant level ğ›¼=5%in terms of homogeneity metrics HI and CI,
indicating that our model can effectively capture the homogeneous
influence among students. ii)Based on the HI and CI values, we sort
all comparative methods from low to high and observe that RCD
performs best, while KSCD worst. These findings align with the
preliminary results depicted in Figure 2, that is to say, in Figure 2
(c) (i.e., KSCD), the distance between similar students is farthest,
whereas in (e) (i.e., RCD), the distance is relatively closer. iii)In each
 
2634KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, and Aimin Zhou
Table 3: Performance of baselines, state-of-the-art methods, and HyperCDM in interpretability and capture of the homogeneous
influence (i.e., homogeneity). DOA@10 and DOA are expressed in percentage while HI and CI in decimal form. â€œ â†‘â€ means larger
values are better while â€œ â†“â€ means smaller values are better. In each column, an entry is marked in bold if its mean value is the
best. Byğ‘¡-test, a bold one is significantly better than others on the corresponding metrics with significant level ğ›¼=5%.
NeurIPS20 EdNet-1 Math1 Math2
Metho
ds DO
A@10 (%)â†‘ HIâ†“ CIâ†“ DO
A@10 (%)â†‘ HIâ†“ CIâ†“ DO
A (%)â†‘ HIâ†“ CIâ†“ DO
A (%)â†‘ HIâ†“ CIâ†“
NCDM 71.36
1.4992 1.5963 59.99
1.4564 2.2783 54.16
0.2206 0.2647 54.89
0.5299 0.5732
KaNCD 73.37
3.3682 3.4992 63.72
2.5189 2.3983 58.50
0.0710 0.1366 62.30
0.9455 0.9666
KSCD 57.43
9.9884 9.1289 57.03
8.7416 9.0598 51.69
0.8367 1.2651 48.36
1.9788 2.3911
RCD 58.95
0.1506 0.1584 55.12
0.1599 0.1775 57.93
0.0280 0.0338 61.77
0.0339 0.0431
Hyp
erCDM 74.29
0.1350 0.1486 66.02
0.1103 0.1602 60.42
0.0099 0.0107 64.34
0.0159 0.0278
Rank 1st Rank 2nd Rank 3rd Rank 4th Rank 5th Rank 6th Rank 7th
Ranking Distribution
Methods01234
HyperCDM RCD KSCD KaNCD NCDM DINA MIRTMetrics
Figure 5: Performance rank on generalization. The height of
rectangles of different colors signifies the number of corre-
sponding rank of the given metric (upper x-axis) across all
datasets for the given method (lower x-axis). Detailed statis-
tics are shown in Appendix B.1 and Table 6.
dataset, graph-based methods (HyperCDM and RCD) show simi-
lar magnitudes for HI and CI values, indicating graph structures
improve capturing the homogeneous influence among students.
However, simple pair-wise graphs fall short in effectively capturing
complex relationships, leading to HyperCDM outperforming RCD.
Interpretability (Q2) In Table 3, by ğ‘¡-test, HyperCDM reaches
state-of-the-art performance in terms of interpretability metric
DOA, and its variance is less than 0.05. The relative improvements
(%) over the best baselines are 1.25%, 3.61%, 3.28%and3.27%for
NeurIPS20, EdNet-1, Math1 and Math2 respectively, and those over
the RCD are 26.0%,19.8%,4.30%and4.16%. It demonstrates that: i)
HyperCDM not only effectively captures homogeneous influence
but also enhances the interpretability via hypergraph structure.
ii)Compared to the graph-based method RCD, HyperCDM sig-
nificantly outperforms it, especially in online datasets NeurIPS20
and EnNet-1. This indicates the hypergraph is more flexible and
effective than pair-wise graphs, and validates the effectiveness of
our hypergraph construction strategy in handling sparse online
interaction. Specifically, while RCD directly constructs a relation
map based on sparse response logs [ 11], we construct hypergraphs
via their latent features to avoid this problem.
Generalization (Q3). The performance of baseline methods
and HyperCDM on generalization are illustrated in Figure 5, which
shows the ranking distribution of a model across the four datasets
for a specific metric. The HyperCDM secures all top positions in
both Acc. and F1, while also achieving commendable performanceTable 4: Ablation study of HyperCDM. By ğ‘¡-test, a bold one is
significantly better than others on the corresponding metrics
with significant level ğ›¼=5%. To intuitively understand the
function of each component, we visualize the corresponding
proficiency level by t-SNE (similar to Figure 2).
Metho
ds DatasetsMetricst-SNE
Figure on Math1
A
cc.(%)â†‘AUC(%)â†‘ HIâ†“ DOA(%)â†‘
Hyp
erCDM
w.o.
MonNeurIPS20 71.76
77.32 0.1571 68.90
EdNet-1 71.70
74.41 0.1385 65.87
Math1 68.32
74.30 0.0108 59.97
Math2 69.35
77.76 0.0264 63.46
Hyp
erCDM
w.o.
ConNeurIPS20 71.14
76.99 1.5126 46.90
EdNet-1 71.45
74.40 1.4977 45.72
Math1 67.60
73.84 0.2718 49.42
Math2 69.91
77.21 0.5764 49.95
Hyp
erCDM
w.o.
GNeurIPS20 71.66
76.72 0.9649 72.80
EdNet-1 71.08
73.13 0.8756 62.84
Math1 68.04
74.21 0.1406 51.81
Math2 69.40
77.05 0.3675 52.53
Hyp
erCDM
w.o.
LNeurIPS20 71.48
76.84 0.3547 73.05
EdNet-1 71.50
74.03 0.4152 63.31
Math1 67.76
74.01 0.0514 59.73
Math2 69.99
77.67 0.0739 63.56
Hyp
erCDMNeurIPS20 71.88
77.48 0.1350 74.29
EdNet-1 71.98
74.45 0.1103 66.02
Math1 68.47
74.76 0.0099 60.42
Math2 70.35
78.27 0.0159 64.34
on AUC. This suggests that HyperCDM not only excels in capturing
homogeneous influence and interpretability but also shows strong
competitiveness in terms of generalization. Detailed statistics are
shown in Appendix B.1 and Table 6.
Ablation Study (Q4). We conduct an ablation study to evaluate
several variants including: (a) HyperCDM w.o. Mon that does not
use momentum strategy in MHGCN (i.e., let ğœ‚=0); (b) HyperCDM
w.o. Con that does not convert low-dimensional embeddings to
high-dimensional features; (c) HyperCDM w.o. G that replaces
constructed hypergraphs with random graphs; (d) HyperCDM w.o.
Lthat directly construct hypergraphs based on sparse response logs.
As shown in Table 4, HyperCDM outperforms all variants. Besides,
we observe that utilizing latent features for hypergraph construc-
tion and momentum in MHGCN enhances the distinctiveness and
establishes a more evident sequential relationship among students
in t-SNE figure. We also find that converting embeddings into fea-
tures makes the arrangement of points more compact and orderly,
 
2635Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Acc.
AUCHI CI
F1DOA
Figure 6: Hyperparameter analysis of ğ‘‘embandğ‘‘feat.
Table 5: Comparison of running times between baselines and
HyperCDM, measured in seconds. The measurement inter-
val spans from the graph construction (if any) to achieving
optimum performance on the validation set.
Metho
ds NeurIPS20
EdNet-1 Math1 Math2
MIRT 327
704 295 249
DINA 344
512 366 305
NCDM 683
482 136 117
KaNCD 651
467 121 114
KSCD 1678
1403 322 264
RCD 9144
38524 1531 1318
Hyp
erCDM 3067
3744 511 535
aligning with Assumption 2. Compared with random graphs, stu-
dentsâ€™ proficiency levels obtained by HyperCDM better match their
correctness rate. More details can be found in Appendix B.2.
Hyperparameter Analysis (Q5). We conduct a hyperparame-
ter experiment and the results of ğ‘‘emb,ğ‘‘featon Math1 are shown
in Figure 6. Without loss of generality, we find that: i)Optimal
interpretability is often achieved with high dimension of both fea-
ture space and embeddings because more dimension enriches the
representation. ii)Optimal homogeneity is usually achieved with
low dimension of both feature space and embeddings. iii)Optimal
generalization is typically achieved with a high-dimensional fea-
ture space and a low-dimensional embedding. Thus, they mutually
constrain each other. To optimize performance, we can choose high-
dimensional features, and improve the homogeneity and generaliza-
tion by choosing low-dimensional embeddings (as we mentioned in
Section 4.3). Comprehensive analysis can be found in Appendix B.3.
Scalability Analysis (Q6). Table 5 presents the runtime of base-
line methods and proposed HyperCDM on the same hardware
devices (details of the devices are shown in Section A.3). Graphs
offer significant benefits in modeling complex relationships but at
the cost of increased time requirements. Compared with traditional
and neural network-based cognitive diagnosis models, graph-based
models are slower. To address this, our model uses a lightweight
network to reduce time disadvantages. Specifically, our model oper-
ates significantly faster compared to RCD. And even on large-scale
datasets (i.e., NeurIPS20 and EdNet-1), HyperCDM can still com-
plete diagnosis within the timeframe that static cognitive diagnosis
can tolerate. Therefore, HyperCDM can be applied to large-scale
datasets under the current task setting.
(a) t-SNE Figure of HyperCDM on Math1
(c) Hotmap of Diagnostic Proficiency Levels(b) Clustering of Knowledge ConceptsNo.
1
2
3
4
5
6
7
8
9
10
11Concepts
Set
Inequality
Trigonometric function
Logarithm versus exponential
Plane vector
Property of function
Image of function
Spatial imagination
Abstract summarization
Reasoning and demonstration
Calculation1
10
92
411
36
785
1.0
0.0
Figure 7: Case study: t-SNE figure of HyperCDM, knowledge
concepts clustering and proficiency levels of ğ‘ 77,ğ‘ 704,ğ‘ 3307(re-
sponse logs are mentioned in Figure 2).
Case Study (Q7). Considering an educator conducts cognitive
diagnosis with HyperCDM, and the results are shown in Figure 7. In
(a), compared with other methods of Figure 2, three similar students
are positioned closer, and they all exhibit deficiencies in nearly iden-
tical knowledge concepts ( ğ‘˜4,ğ‘˜5,ğ‘˜11), as shown in (c). This indicates
that HyperCDM can effectively capture homogeneous influence
among students. Moreover, HyperCDM can learn the implicit asso-
ciations among concepts from the knowledge hypergraph. In (b),
we applyğ¾-means on knowledge concepts features ( ğ¾=4) and
observe that semantically related concepts are clustered together.
For example, ğ‘˜3,ğ‘˜6,ğ‘˜7are all related to function, and ğ‘˜5,ğ‘˜8are ge-
ometry. In summary, HyperCDM effectively helps teachers identify
similar students for tailored instruction and discover connections
between knowledge points, enhancing teaching methods.
6 Conclusion
This paper first observes a key issue that is often neglected by previ-
ous work: the homogeneous influence among students. To capture
the homogeneous influence, we propose a hypergraph cognitive
diagnosis model (HyperCDM) to model flexible and high-order rela-
tionships among students. To avoid distortion, HyperCDM employs
a divide-and-conquer strategy to learn different kinds of represen-
tations and unifies them via a feature-based interaction function.
To construct the hypergraph based on sparse response logs, the
auto-encoder is used to learn latent features to cluster students.
To alleviate over-smoothing, momentum hypergraph convolution
networks are designed. Experiment results show that HyperCDM
excels in capturing the homogeneous influence. The future work
of HyperCDM includes further enhancing its generalization and
landing it on more real-world intelligent education scenarios.
Acknowledgments
We would like to thank the anonymous reviewers for their com-
prehensive and constructive reviews. The first author would like
to extend special thanks to Xinyu Shi, whose unwavering support
has enriched his brightest days during the research. This work
is supported by the Natural Science Foundation of Shanghai (No.
21ZR1420300, 23ZR1418500), and Science and Technology Commis-
sion of Shanghai Municipality Grant (No. 22511105901).
 
2636KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, and Aimin Zhou
References
[1]Ghodai Abdelrahman, Qing Wang, and Bernardo Pereira Nunes. 2023. Knowledge
Tracing: A Survey. ACM Computing Survey 55, 11 (2023), 224:1â€“224:37.
[2]Ashton Anderson, Daniel P. Huttenlocher, Jon M. Kleinberg, and Jure Leskovec.
2014. Engaging with massive online courses. In Proceedings of the ACM on Web
Conference 2014. Seoul, Korea.
[3]Albert Bandura and Richard H Walters. 1977. Social Learning Theory. Vol. 1.
Englewood Cliffs Prentice Hall.
[4]Dexiong Chen, Leslie Oâ€™Bray, and Karsten M. Borgwardt. 2022. Structure-Aware
Transformer for Graph Representation Learning. In Proceedings of the 39th Inter-
national Conference on Machine Learning, Vol. 162. Baltimore, MD, 3469â€“3489.
[5]Youngduck Choi, Youngnam Lee, Dongmin Shin, Junghyun Cho, Seoyon Park,
Seewoo Lee, Jineon Baek, Chan Bae, Byungsoo Kim, and Jaewe Heo. 2020. Ed-
Net: A Large-Scale Hierarchical Dataset in Education. In Proceedings of the 21st
International Conference on Artificial Intelligence in Education. Ifrane, Morocco,
69â€“73.
[6]Jiajun Cui, Zeyuan Chen, Aimin Zhou, Jianyong Wang, and Wei Zhang. 2023. Fine-
Grained Interaction Modeling with Multi-Relational Transformer for Knowledge
Tracing. ACM Transactions on Information Systems 41, 4 (2023), 104:1â€“104:26.
[7]Jimmy De La Torre. 2008. An Empirically Based Method of Q-matrix Validation
for the DINA Model: Development and applications. Journal of Educational
Measurement 45, 4 (2008), 343â€“362.
[8]Kaize Ding, Albert Jiongqian Liang, Bryan Perozzi, Ting Chen, Ruoxi Wang,
Lichan Hong, Ed H. Chi, Huan Liu, and Derek Zhiyuan Cheng. 2023. Hyper-
Former: Learning Expressive Sparse Feature Representations via Hypergraph
Transformer. In Proceedings of the 46th International ACM SIGIR Conference on
Research and Development in Information Retrieval. Taipei, Taiwan, 2062â€“2066.
[9]Paszke et al. 2019. PyTorch: An Imperative Style, High-Performance Deep Learn-
ing Library. In Advances in Neural Information Processing Systems 32. British
Columbia, Canada, 8024â€“8035.
[10] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hyper-
graph Neural Networks. In Proceedings of the 33rd AAAI Conference on Artificial
Intelligence. Honolulu, HI, 3558â€“3565.
[11] Weibo Gao, Qi Liu, Zhenya Huang, Yu Yin, Haoyang Bi, Mu-Chun Wang, Jianhui
Ma, Shijin Wang, and Yu Su. 2021. RCD: Relation Map Driven Cognitive Diagnosis
for Intelligent Education Systems. In Proceedings of the 44th International ACM
SIGIR Conference on Research and Development in Information Retrieval. Virtual,
501â€“510.
[12] Yue Gao, Zizhao Zhang, Haojie Lin, Xibin Zhao, Shaoyi Du, and Changqing Zou.
2022. Hypergraph Learning: Methods and Practices. IEEE Transactions on Pattern
Analysis and Machine Intelligence 44, 5 (2022), 2548â€“2566.
[13] Xavier Glorot and Yoshua Bengio. 2010. Understanding the Difficulty of Training
Deep Feedforward Neural Networks. In Proceedings of the 13rd International
Conference on Artificial Intelligence and Statistics. Sardinia, Italy, 249â€“256.
[14] William L. Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive Represen-
tation Learning on Large Graphs. In Advances in Neural Information Processing
Systems 30. Long Beach, CA, 1024â€“1034.
[15] John A Hartigan and Manchek A Wong. 1979. Algorithm AS 136: A k-means
clustering algorithm. Journal of the royal statistical society. series c (applied
statistics) 28, 1 (1979), 100â€“108.
[16] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng
Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Net-
work for Recommendation. In Proceedings of the 43rd International ACM SIGIR
conference on research and development in Information Retrieval. virtual, 639â€“648.
[17] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989. Multilayer Feed-
forward Networks are Universal Approximators. Neural Networks 2, 5 (1989),
359â€“366.
[18] Lu Jiang, Kunpeng Liu, Yibin Wang, Dongjie Wang, Pengyang Wang, Yanjie Fu,
and Minghao Yin. 2023. Reinforced Explainable Knowledge Concept Recommen-
dation in MOOCs. ACM Transactions on Intelligent Systems and Technology 14, 3
(2023), 43:1â€“43:20.
[19] Aisha Urooj Khan, Hilde Kuehne, Bo Wu, Kim Chheu, Walid Bousselham, Chuang
Gan, Niels da Vitoria Lobo, and Mubarak Shah. 2023. Learning Situation Hyper-
Graphs for Video Question Answering. In Proceedings of the 2023 IEEE/CVF
Conference on Computer Vision and Pattern Recognition. Vancouver, Canada.
[20] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimiza-
tion. In Proceedings of the 3rd International Conference on Learning Representations.
San Diego, CA.
[21] Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with
Graph Convolutional Networks. In Proceedings of the 5th International Conference
on Learning Representations. Toulon, France.
[22] Jiatong Li, Fei Wang, Qi Liu, Mengxiao Zhu, Wei Huang, Zhenya Huang, Enhong
Chen, Yu Su, and Shijin Wang. 2022. HierCDF: A Bayesian Network-based
Hierarchical Cognitive Diagnosis Framework. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining. Washington, DC,
904â€“913.
[23] Mingjia Li, Hong Qian, Jinglan Lv, Mengliang He, Wei Zhang, and Aimin Zhou.
2024. Foundation Model Enhanced Derivative-Free Cognitive Diagnosis. Frontiersof Computer Science (2024).
[24] Jiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Mengmei Zhang,
Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi. 2023. Towards
Graph Foundation Models: A Survey and Beyond. CoRR abs/2310.11829 (2023).
arXiv:2310.11829
[25] Qi Liu. 2021. Towards a New Generation of Cognitive Diagnosis. In Proceedings of
the 30th International Joint Conference on Artificial Intelligence. Montreal, Canada,
4961â€“4964.
[26] Qi Liu, Runze Wu, Enhong Chen, Guandong Xu, Yu Su, Zhigang Chen, and Guop-
ing Hu. 2018. Fuzzy Cognitive Diagnosis for Modelling Examinee Performance.
ACM Transactions on Intelligent Systems and Technology 9, 4 (2018), 48:1â€“48:26.
[27] Shuo Liu, Hong Qian, Mingjia Li, and Aimin Zhou. 2023. QCCDM: A Q-
Augmented Causal Cognitive Diagnosis Model for Student Learning. In Pro-
ceedings of the 26th European Conference on Artificial Intelligence. KrakÃ³w, Poland,
1536â€“1543.
[28] Shuo Liu, Junhao Shen, Hong Qian, and Aimin Zhou. 2024. Inductive Cognitive
Diagnosis for Fast Student Learning in Web-Based Intelligent Education Systems.
InProceedings of the ACM on Web Conference 2024. Singapore.
[29] Yingjie Liu, Tiancheng Zhang, Xuecen Wang, Ge Yu, and Tao Li. 2023. New
development of cognitive diagnosis models. Frontiers of Computer Science 17, 1
(2023), 171604.
[30] Frederic Lord. 1952. A Theory of Test Scores. Psychometric Monographs (1952).
[31] Haiping Ma, Manwei Li, Le Wu, Haifeng Zhang, Yunbo Cao, Xingyi Zhang,
and Xuemin Zhao. 2022. Knowledge-Sensed Cognitive Diagnosis for Intelligent
Education Platforms. In Proceedings of the 31st ACM International Conference on
Information and Knowledge Management. Atlanta, GA, 1451â€“1460.
[32] Mark D. Reckase. 2009. Multidimensional Item Response Theory Models. Springer.
79â€“112 pages.
[33] D. Sculley. 2010. Web-scale k-means clustering. In Proceedings of the ACM on
Web Conference 2010. Raleigh, NC, 1177â€“1178.
[34] Junhao Shen, Hong Qian, Wei Zhang, and Aimin Zhou. 2024. Symbolic Cog-
nitive Diagnosis via Hybrid Optimization for Intelligent Education Systems. In
Proceedings of the 38th AAAI Conference on Artificial Intelligence. Vancouver,
Canada.
[35] Jianwen Sun, Fenghua Yu, Sannyuya Liu, Yawei Luo, Ruxia Liang, and Xiaoxuan
Shen. 2023. Adversarial Bootstrapped Question Representation Learning for
Knowledge Tracing. In Proceedings of the 31st ACM International Conference on
Multimedia. Ottawa, Canada, 8016â€“8025.
[36] J. B. Sympson. 1978. A Model for Testing with Multidimensional Items. In
Proceedings of the 1977 Computerized Adaptive Testing Conference. Minneapolis,
MN, 82â€“98.
[37] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of Machine Learning Research 9, 11 (2008).
[38] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
LiÃ², and Yoshua Bengio. 2017. Graph Attention Networks. CoRR abs/1710.10903
(2017). arXiv:1710.10903
[39] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yuying Chen, Yu Yin, Zai Huang,
and Shijin Wang. 2020. Neural Cognitive Diagnosis for Intelligent Education
Systems. In Proceedings of the 34th AAAI Conference on Artificial Intelligence. New
York, NY, 6153â€“6161.
[40] Fei Wang, Qi Liu, Enhong Chen, Zhenya Huang, Yu Yin, Shijin Wang, and Yu
Su. 2022. NeuralCD: A General Framework for Cognitive Diagnosis. IEEE
Transactions on Knowledge and Data Engineering 35, 8 (2022), 8312â€“8327.
[41] Shanshan Wang, Zhen Zeng, Xun Yang, and Xingyi Zhang. 2023. Self-Supervised
Graph Learning for Long-Tailed Cognitive Diagnosis. In Proceedings of the 37th
AAAI Conference on Artificial Intelligence. Washington, DC, 110â€“118.
[42] Zichao Wang, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan Za-
ykov, JosÃ© Miguel HernÃ¡ndez-Lobato, Richard E Turner, Richard G Baraniuk,
Craig Barton, Simon Peyton Jones, Simon Woodhead, and Cheng Zhang. 2020.
Diagnostic Questions: The NeurIPS 2020 Education Challenge. arXiv preprint
arXiv:2007.12061 (2020).
[43] Lianghao Xia, Chao Huang, Yong Xu, Jiashu Zhao, Dawei Yin, and Jimmy Huang.
2022. Hypergraph Contrastive Collaborative Filtering. In Proceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval. New York, NY.
[44] Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos, and Mingyi Hong. 2017. Towards
K-means-friendly Spaces: Simultaneous Deep Learning and Clustering. In Pro-
ceedings of the 34th International Conference on Machine Learning, Vol. 70. Sydney,
Australia, 3861â€“3870.
[45] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He,
Yanming Shen, and Tie-Yan Liu. 2021. Do Transformers Really Perform Badly
for Graph Representation?. In Advances in Neural Information Processing Systems
34. virtual, 28877â€“28888.
[46] Yan Zhuang, Qi Liu, GuanHao Zhao, Zhenya Huang, Weizhe Huang, Zachary
Pardos, Enhong Chen, Jinze Wu, and Xin Li. 2024. A Bounded Ability Estimation
for Computerized Adaptive Testing. Advances in Neural Information Processing
Systems 36.
 
2637Capturing Homogeneous Influence among Students: Hypergraph Cognitive Diagnosis for Intelligent Education Systems KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Appendix
The appendix is organized as follows:
â€¢Appendix A presents the details of datasets, methods, implemen-
tation, and explanation of proposed metrics HI and CI.
â€¢Appendix B presents detailed statistics of performance on gener-
alization, ablation study and other hyperparameter analysis results.
â€¢Appendix C presents the limitations of our method.
A Experiment Details
A.1 Datasets Sources
We conduct experiments on four real-world datasets. Note that
these datasets are open-accessible and do not involve ethical issues
such as data collection. Their sources are introduced below.
Math1 and Math2. The datasets are collected from high school
students who participate in their final exams during their first and
second senior years, respectively. They include both objective and
subjective problems. However, to align with previous work, we
only consider the objective problems.
NeurIPS20. This dataset is used in the NeurIPS 2020 Education
Challenge. It encompasses four semesters (September 2018 to May
2020) of studentsâ€™ answers to mathematics exercises from Eedi, a
leading educational platform which students interact with daily
from all around the world. We only use utilize the dataset offered
for cognitive diagnosis in this competition.
EdNet-1. This dataset contains all student-system interactions
collected over 2 years by Santa, a multi-platform AI tutoring service
in Korea. The 1means the first section of the huge dataset.
In the main paper, we state, â€œThese datasets cover common
learning scenarios and are representative,â€ because they include
both online and offline environments, as well as middle school and
high school levels, and also cover students from different locations.
A.2 Baselines and State-of-the-Art Methods
We compare with six baseline and state-of-the-art methods includ-
ing transitional CDMs, neural CDMs and the graph-based CDM.
All of them are open-source and available at https://github.com/
bigdata-ustc/EduCDM.
â€¢MIRT [36] extends the IRT by adopting multidimensional
vectors to model latent traits of students and exercises. The simple
form of the interaction function is defined as
ğ‘ƒ(ğ‘Ÿğ‘–ğ‘—=1)=1
1+ğ‘’âˆ’1.7(ğ’‚âŠ¤
ğ‘—ğœ½ğ‘–âˆ’ğœ·ğ‘—),
whereğ‘ƒ(ğ‘Ÿğ‘–ğ‘—=1)is the probability that student ğ‘ ğ‘–answers the
exerciseğ‘’ğ‘—correctly, ğ’‚ğ‘—âˆˆRğ‘‘is the discrimination of exercise ğ‘—,
ğœ½ğ‘–âˆˆRğ‘‘is the latent traits of students, and ğœ·ğ‘—âˆˆRğ‘‘is that of
exercises. Note that ğ‘‘â‰ |ğ¾|, i.e., each dimension of traits does not
correspond to the specific knowledge concepts.
â€¢DINA [7] is a conjunctive assumption-based model, where
proficiency levels are denoted by discrete binary values. The inter-
action function is defined as
ğ‘ƒ(ğ‘Ÿğ‘–ğ‘—=1)=ğ‘”1âˆ’ğœ‚ğ‘–ğ‘—
ğ‘—(1âˆ’ğ‘ ğ‘—)ğœ‚ğ‘–ğ‘—, ğœ‚ğ‘–ğ‘—=Ã–
ğ‘˜ğœƒğ›½ğ‘–ğ‘—
ğ‘–ğ‘—,
whereğ‘”ğ‘—is the probability of correctly guessing exercise ğ‘’ğ‘—,ğ‘ ğ‘—is
the probability of making a careless mistake on exercise ğ‘’ğ‘—,ğœƒğ‘–ğ‘—âˆˆ{0,1}denotes whether student ğ‘–masters knowledge concept ğ‘—, and
ğ›½ğ‘–ğ‘—âˆˆ0,1donates whether exercise ğ‘–includes concept ğ‘—.
â€¢NCDM [39] adopts neural networks in replacement of man-
ually designed interaction functions and outperforms traditional
CDMs on most datasets in generalization and interpretability. The
interaction function are multi-layer position perceptron.
â€¢KaNCD [40] builds upon the advancements of NCDM, consid-
ers implicit associations between knowledge concepts, and achieves
state-of-the-art results across most datasets.
â€¢KSCD [31] explores implicit relationships between knowledge
concepts and exercises, employing a novel interaction function.
â€¢RCD [11] explores intricate relationships among students,
exercises, and knowledge attributes, employing a graph attention
network to model these connections.
A.3 Implementation Details
In hypergraph construction, we set the number of clusters K=
0.02|ğ‘†|, the dimension of latent response logs feature Ëœğ¸=64; the net-
work dimension of auto-encoder is [512,256,128], the pre-training
epoch is 100, and the ğ¾-means clustering epoch ğ‘‡is50. In diag-
nostic layers, the network dimension of MLP is[512,256,128]. In
MHGCN, the number of layers ğ¿=4, the momentum ğœ‚=0.8, and
the dimension of embeddings ğ‘‘emb=16. In embedding conversion,
the dimension of feature ğ‘‘feat=512.
When selecting hyperparameters, we adopt the cross-validation
method, which involves a portion of the training set as a validation
set for parameter selection, and utilizing grid search to explore the
parameters. Through the pilot study, we determined the hyperpa-
rameters for HyperCDM.
All models are implemented by Pytorch [ 9] framework and per-
formed on CUDA 11.1. All experiments are conducted on a Linux
server with Intel(R) Xeon(R) Gold 6130 CPU and NVIDIA V100-
32GB GPU.
A.4 Metrics Explanation
To deeply understand HI and CI, we give an example to show how
to evaluate modelsâ€™ performance on homogeneity via our proposed
metrics. Assuming that we have two cognitive diagnostic models,
A and B, and a response matrix formed by students ğ‘ 1,ğ‘ 2,ğ‘ 3and
exercisesğ‘’1,ğ‘’2,ğ‘’3as follows.
ğ‘¹=ï£®ï£¯ï£¯ï£¯ï£¯ï£°1âˆ’1âˆ’1
âˆ’1 1 1
1âˆ’1âˆ’1ï£¹ï£ºï£ºï£ºï£ºï£»,
where each column signifies the response logs of the corresponding
student. The diagnostic outcomes after employing the cognitive
diagnostic models would be as follows, and each column signifies
the proficiency levels of the corresponding student.
ğƒprofğ´=ï£®ï£¯ï£¯ï£¯ï£¯ï£°0.4 0.7 0.7
0.6 0.2 0.3
0.9 0.2 0.1ï£¹ï£ºï£ºï£ºï£ºï£»,ğƒprofğµ=ï£®ï£¯ï£¯ï£¯ï£¯ï£°0.5 0.8 0.6
0.6 0.3 0.1
0.9 0.4 0.2ï£¹ï£ºï£ºï£ºï£ºï£».
We find that A significantly outperforms B. Given the similarity
in studentsâ€™ interaction records, their proficiency levels should be
equivalent.
Then we try to evaluate with proposed metrics. To calculate
HI, we first compute the correctness rate of each student, i.e.,
 
2638KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Junhao Shen, Hong Qian, Shuo Liu, Wei Zhang, Bo Jiang, and Aimin Zhou
Table 6: Performance of baselines, state-of-the-art methods, and HyperCDM in generalization. Acc., AUC and F1 are expressed
in percentage. â€œâ†‘â€ means larger values are better. In each column, an entry is marked in bold if its mean value is the best. By
ğ‘¡-test, a bold one is significantly better than others on the corresponding metrics with significant level ğ›¼=5%.
Datasets NeurIPS2020 EdNet-1 Math1 Math2
Metho
ds A
cc. (%)â†‘AUC (%)â†‘F1 (%)â†‘A
cc. (%)â†‘AUC (%)â†‘F1 (%)â†‘A
cc. (%)â†‘AUC (%)â†‘F1 (%)â†‘A
cc. (%)â†‘AUC (%)â†‘F1 (%)â†‘
MIRT 66.18
69.19 73.76 68.45
68.98 77.60 67.99
74.42 72.06 69.88
77.08 69.38
DINA 39.16
62.16 12.87 42.95
54.73 41.99 47.18
67.51 24.50 50.54
68.90 18.66
NCDM 71.79 77.73 78.59 70.50
72.70 79.83 67.59
74.25 72.03 69.36
76.87 69.99
KaNCD 71.86
77.53 78.75 71.76
74.51 80.35 68.31
75.11 74.24 70.28
78.10 70.32
KSCD 71.51
76.79 78.32 71.38
74.43 80.42 68.34 75.26 71.69 67.75
76.48 67.06
RCD 71.62
77.11 78.64 71.87
74.57 80.60 67.84
73.93 74.37 69.20
77.08 70.21
Hyp
erCDM 71.88 77.32 78.95 71.98
74.75 80.81 68.47 74.76 75.09 70.35
78.27 70.93
Table 7: Ablation study of HyperCDM on F1 and CI. F1 is
expressed in percentage. â€œ â†‘â€ means larger values are better,
while â€œâ†“â€ means smaller values are better. In each column,
an entry is marked in bold if its mean value is the best. By
ğ‘¡-test, a bold one is significantly better than others on the
corresponding metrics with significant level ğ›¼=5%.
Datasets NeurIPS20 EdNet-1 Math1 Math2
Metho
ds F1(%)â†‘ CIâ†“ F1(%)â†‘ CIâ†“ F1(%)â†‘ CIâ†“ F1(%)â†‘ CIâ†“
Hyp
erCDM w.o. Mon 78.07
0.1864 80.69
0.2034 73.01
0.0258 65.07
0.0579
HyperCDM w.o. Con 79.32
1.4363 80.09
2.1768 73.92
0.2541 68.32
0.5032
HyperCDM w.o. G 78.62
0.9631 80.81
1.7631 74.06
0.1325 67.86
0.2655
HyperCDM w.o. L 78.58
0.5064 80.90
0.8152 72.94
0.0832 69.20
0.2315
Hyp
erCDM 78.95
0.1486 80.99
0.1602 75.09
0.0107 70.93
0.0278
Table 8: Hyperparameter analysis on two offline datasets.
When investigating a particular hyperparameter, we main-
tain the settings of other hyperparameters as specified in
Section A.3.
Hyp
erparametersMath1 Math2
A
cc. AUC F1 DOA HI CI A
cc. AUC F1 DOA HI CI
ğ‘‘emb8 67.84
74.34 73.84 56.99 0.0089 0.0097 70.62
78.86 70.96 64.26 0.0140 0.0244
16 68.47
74.76 75.09 60.42 0.0099 0.0107 70.35
78.27 70.93 64.34 0.0159 0.0278
32 67.70
74.33 72.74 60.56 0.0096 0.0104 69.85
77.29 70.86 64.48 0.0174 0.0301
64 67.34
74.13 72.35 60.41 0.0082 0.0101 69.80
77.26 70.19 64.86 0.0150 0.0261
128 64.91
73.36 74.80 60.83 0.0080 0.0149 69.62
76.95 69.98 65.02 0.0165 0.0289
ğ‘‘feat64 67.99
74.44 71.69 55.51 0.0057 0.0074 69.46
77.37 70.11 63.93 0.0127 0.0254
128 67.93
74.41 74.59 58.02 0.0061 0.0085 69.74
77.80 70.51 64.07 0.0134 0.0265
256 68.06
74.68 75.19 60.04 0.0070 0.0089 69.80
77.98 70.87 64.16 0.0142 0.0268
512 68.47
74.76 75.09 60.42 0.0099 0.0107 70.35
78.27 70.93 64.34 0.0159 0.0278
1024 68.22
74.39 75.23 58.13 0.0091 0.0086 70.46
78.34 71.01 64.59 0.0172 0.0288
ğ¿1 67.09
73.62 72.52 59.14 0.0053 0.0077 69.68
77.21 69.49 63.80 0.0115 0.0199
2 67.46
73.65 73.79 59.22 0.0051 0.0082 69.92
77.87 70.40 64.16 0.0126 0.0209
3 68.02
74.36 74.66 59.14 0.0064 0.0096 70.12
78.12 70.35 64.18 0.0149 0.0213
4 68.47
74.76 75.09 60.42 0.0099 0.0107 70.35
78.27 70.93 64.34 0.0159 0.0248
5 67.37
73.73 72.34 58.22 0.0145 0.0279 69.52
77.02 69.68 65.11 0.0166 0.0265
ğœ‚0.0 68.32
74.30 73.01 59.97 0.0108 0.0258 69.35
77.76 65.07 63.46 0.0264 0.0107
0.2 68.38
74.32 74.99 60.90 0.0125 0.0205 70.33
78.24 70.03 64.50 0.0187 0.0302
0.4 68.03
74.16 74.91 60.12 0.0101 0.0164 70.15
78.22 70.49 64.17 0.0178 0.0290
0.6 68.24
74.35 75.08 60.21 0.0092 0.0122 70.07
78.16 70.26 64.26 0.0151 0.0267
0.8 68.47
74.76 75.09 60.42 0.0099 0.0107 70.35
78.27 70.93 64.34 0.0159 0.0278
1 67.53
74.28 75.01 61.17 0.0074 0.0084 70.06
78.15 70.47 64.75 0.0148 0.0277
0.67,0.33,0.33respectively. Then we have
ğ»ğ¼ğ´â‰ˆ0.1414, ğ»ğ¼ğµâ‰ˆ0.3464,
andğ»ğ¼ğ´<ğ»ğ¼ğµ, validating that A outperforms B.
On the other hand, we try to calculate CI. At first we give the
cosine similarity matrix
ğ‘ª=ï£®ï£¯ï£¯ï£¯ï£¯ï£°1âˆ’1âˆ’1
âˆ’1 1 1
âˆ’1 1 1ï£¹ï£ºï£ºï£ºï£ºï£».
And then we have
ğ¶ğ¼ğ´â‰ˆâˆ’0.3610, ğ¶ğ¼ğµâ‰ˆâˆ’0.2612,whereğ¶ğ¼ğ´<ğ¶ğ¼ğµ, also validating that A outperforms B.
B Experiment Statistics
B.1 Generalization
The performance of baselines, state-of-the-art methods, and Hy-
perCDM on generalization is shown in Table 6, and the variance
of each entry is less than 0.05. Byğ‘¡-test, HyperCDM significantly
outperforms other methods on Acc. and F1 with significant level
ğ›¼=5%, and performs competitively with other methods on AUC.
B.2 Ablation Study
In addition to the metrics presented in Table 4 (i.e., Acc., AUC, HI,
and DOA), we also conducted experiments involving F1 and CI,
as depicted in Table 7. The results still indicate that HyperCDM
outperforms all other HyperCDM variants.
B.3 Hyperparameter Analysis
We conducted a hyperparameter experiment on ğ‘‘feat,ğ‘‘embin Math1,
as well as experiments on the number of layers ğ¿and momentum
ğœ‚in MHGCN across two offline datasets (without loss of gener-
ality, and similar to NeurIPS20 and EdNet-1), as depicted in Ta-
ble 8. Theğ‘‘featâˆˆ{64,128,256,512,1024},ğ‘‘embâˆˆ{8,16,32,64,128},
ğ¿âˆˆ {1,2,3,4,5}, andğœ‚âˆˆ {0.0,0.2,0.4,0.6,0.8,1.0}. We find the
trend in hyperparameter analysis of Section 5.2 persists across all
datasets:ğ‘‘emb,ğ‘‘featmutually constrain each other. Thus, to opti-
mize performance, we can choose high-dimensional features, and
improve the homogeneity and generalization by choosing low-
dimensional embeddings (as we mentioned in Section 4.3). We also
observe that a momentum value ğœ‚around 0.8 is most suitable, since
small values fail to mitigate over-smoothing, while overly large val-
ues result in the MHGCN consistently prioritizing representations
of the previous layer. Additionally, ğ¿=4reaches optimal in most
datasets, because shallow GNNs struggle to capture the structure of
the graph, while overly deep ones may encounter over-smoothing.
C Limitations
Our model might occasionally struggle with scenarios involving
millions of students because graph (including hypergraph) repre-
sentation learning faces many challenges in the face of large-scale
data. This paper is focused on how to effectively apply hypergraphs
to capturing homogeneous influence among students, and we hope
it can be addressed in the future work.
 
2639