Your Neighbor Matters: Towards Fair Decisions Under Networked
Interference
Wenjing Yangâˆ—
Department of Intelligent Data
Science, College of Computer,
National University of Defense
Technology
Changsha, China
wenjing.yang@nudt.edu.cnHaotian Wangâ€ 
Department of Intelligent Data
Science, College of Computer,
National University of Defense
Technology
Changsha, China
accwht@hotmail.comHaoxuan Li
Peking University
Beijing, China
hxli@stu.pku.edu.cn
Hao Zou
ZGC laboratory
Beijing, China
ahio@163.comRuochun Jin
Department of Intelligent Data
Science, College of Computer,
National University of Defense
Technology
Changsha, China
jinrc@nudt.edu.cnKun Kuangâ€¡
Institute of Artificial Intelligence,
Zhejiang University
Hangzhou, China
kunkuang@zju.edu.cn
Peng Cui
Tsinghua University
Beijing, China
cuip@tsinghua.edu.cn
Abstract
In the era of big data, decision-making in social networks may
introduce bias due to interconnected individuals. For instance, in
peer-to-peer loan platforms on the Web, considering an individ-
ualâ€™s attributes along with those of their interconnected neighbors,
including sensitive attributes, is vital for loan approval or rejec-
tion downstream. Unfortunately, conventional fairness approaches
often assume independent individuals, overlooking the impact of
one personâ€™s sensitive attribute on othersâ€™ decisions. To fill this
gap, we introduce "Interference-aware Fairness" (IAF) by defining
two forms of discrimination as Self-Fairness (SF) and Peer-Fairness
(PF), leveraging advances in interference analysis within causal
inference. Specifically, SF and PF causally capture and distinguish
discrimination stemming from an individualâ€™s sensitive attributes
(with fixed neighborsâ€™ sensitive attributes) and from neighborsâ€™ sen-
sitive attributes (with fixed selfâ€™s sensitive attributes), separately.
Hence, a network-informed decision model is fair only when SF
and PF are satisfied simultaneously, as interventions in individualsâ€™
âˆ—Wenjing Yang, Haotian Wang and Haoxuan Li contributed equally to this research.
â€ Haotian Wang is the corresponding author.
â€¡Kun Kuang is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671960sensitive attributes or those of their peers both yield equivalent out-
comes. To achieve IAF, we develop a deep doubly robust framework
to estimate and regularize SF and PF metrics for decision models.
Extensive experiments on synthetic and real-world datasets validate
our proposed concepts and methods.
CCS Concepts
â€¢Applied computing â†’Law, social and behavioral sciences;
â€¢Computing methodologies â†’Machine learning.
Keywords
Algorithmic fairness, machine learning, social network
ACM Reference Format:
Wenjing Yang, Haotian Wang, Haoxuan Li, Hao Zou, Ruochun Jin, Kun
Kuang, and Peng Cui. 2024. Your Neighbor Matters: Towards Fair Decisions
Under Networked Interference. In Proceedings of the 30th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD â€™24), August
25â€“29, 2024, Barcelona, Spain. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/3637528.3671960
1 Introduction
The widespread use of machine learning (ML) models raises con-
cerns about ethical and legal implications due to potential biases [ 2,
37]. For example, ML-based credit scoring in loan systems may
yield discriminatory results for individuals with similar financial
profiles but different races [ 19]. To ensure fairness, research has
developed various fairness metrics [ 4,9,11,12,14,19,36,38,40,42].
Earlier work focused on statistical independence between ML deci-
sions and sensitive attributes [ 8,11]. There has been much recent
interest in answering the fairness questions from the perspective
3829
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenjing Yang et al.
of causality [ 19,26,30], aiming to achieve equity by examining
interventions on sensitive attributes, e.g., â€œHow would the decision
changes if we intervened with racial attributes?â€.
Previous fairness criteria, particularly those rooted in causality-
based fairness metrics, have often assumed individual independence,
i.e., one personâ€™s sensitive attributes do not impact othersâ€™ decisions.
This assumption implies that discrimination against individuals,
such as the borrower in our P2P loan example, is solely influenced
by their features, such as credit status and the sensitive attribute of
living location1. However, in the era of big data, our living world
is interconnected and social relationships play a significant role in
decision-making models [ 7,20,31]. For instance, in contemporary
online lending platforms, such as peer-to-peer (P2P) loan systems
on the Web, decision-making increasingly relies on both an indi-
vidualâ€™s attributes and those of their social network neighbors [ 20].
In essence, decisions made by many ML systems now incorporate
sensitive attributes not only from the individual but also from their
neighbors in the social network.
Consequently, when the effects of self-sensitive attributes (self-
effect) and neighborsâ€™ sensitive attributes (peer-effect) intertwine,
a decision model that appears fair under conventional criteria may
exhibit unfairness. Keeping the P2P loan example in mind, a loan
decision model satisfying conventional fairness criteria [ 8,10,11,
14,19,39] entails that the overall correlation/causal effect from
sensitive attribute, i.e., location or race, to the loan decision vanishes.
However, a loan model unfair for minor-group individuals might
seem to be fair with a vanished effect from race to loan decision. The
core reason is that scores assigned by the decision model for minor-
group individuals will be enhanced by considering their social
connections with the major-group neighbors. We term such kinds of
unfairness as interference-specific unfairness throughout our paper.
In general, interference-specific unfairness exists commonly across
many scenarios beyond loan systems. For instance, prestigious
colleges may admit applicants from the minor group (seemingly
fair) with lots of neighbors from the major group, while rejecting
applicants with similar academic qualifications from the minor
group with few neighbors from the major group (indeed unfair) [ 3].
In response to such discrimination specific to interference across
individuals, we advocate for the establishment of more robust
fairness metrics that account for the influence of social relation-
ships on decision-making. Building upon recent advancements in
interference-based causality [ 13,22,27], we introduce the concepts
of "self-fairness (SF) " and "peer-fairness (PF)" to causally evalu-
ate equity among individuals with similar self-sensitive attributes
and distinct neighbor-sensitive attributes, as well as equity among
individuals with dissimilar self-sensitive attributes and analogous
neighbor-sensitive attributes, respectively. Subsequently, the simul-
taneous satisfaction of SF and PF through model regularization can
mitigate interference-specific unfairness.
To the best of our knowledge, we are pioneering the formal
differentiation and mitigation of discrimination stemming from an
individualâ€™s sensitive attributes versus those arising from peersâ€™
sensitive attributes. We summarize our contributions in below:
1.We contribute the Interference-aware Fairness (IAF) metric to
capture such unfair decisions.
1As discussed in the real-world case study by [20].2.To characterize IAF, we introduce causal definitions for Self Fair-
ness (SF) and Peer Fairness (PF) aimed at capturing unfair deci-
sions induced by peer effects.
3.Inspired by networked causal inference [ 27], we devise a deep
doubly robust (DR) framework to regularize unfair decision mod-
els in the presence of interference.
4.Our experiments, conducted on one synthetic data and two real-
world datasets, yield the following key results (a) SF and PF
effectively capture unfair decisions stemming from peer effects,
and (b) our designed DR framework successfully eliminates this
interference-specific unfairness.
2 Preliminaries
2.1 Notations
We formalize definitions of fairness in the essence of interference
across decision subjects. Throughout this paper, uppercase let-
ters denote the Random variables, e.g., ğ‘‹, and lowercase letters
denote their realizations, e.g., ğ‘¥. Let{ğ´ğ‘–,ğ‘‹ğ‘–,ğ‘Œğ‘–}ğ‘›
ğ‘–=1âˆ¼ğ‘ƒbe the
logged dataset with ğ‘›individuals sampled from the joint distri-
butionğ‘ƒ(ğ´,ğ‘‹,ğ‘Œ), whereğ´ğ‘–,ğ‘‹ğ‘–andğ‘Œğ‘–are sensitive attributes, con-
textual features and outcome to be predicted for individual ğ‘–, respec-
tively. A decision model ğ‘€ğœƒ(ğœƒis the parameter of ğ‘€ğœƒ) is learned on
{ğ´ğ‘–,ğ‘‹ğ‘–,ğ‘Œğ‘–}ğ‘›
ğ‘–=1with predictions Ë†ğ‘Œğ‘–for individual ğ‘–. We assume ğ´to
be binary throughout this paper, while our discussion framework
can be easily generalized to categorical sensitive attributes. We
present all causal notions using the language of potential outcome
framework [ 30], i.e., Ë†ğ‘Œ(ğ‘)represents the potential decision if the
sensitive attribute ğ´were set to value ğ‘. Notably,(ğ´,ğ‘‹, Ë†ğ‘Œ)is the
observational data and cannot be intervented arbitrarily, as ğ‘€ğœƒat-
tempts to fit ğ‘ƒ(ğ´,ğ‘‹,ğ‘Œ). Besides,[ğ‘›]represents the set{1,2,Â·Â·Â·,ğ‘›}
andâˆ’ğ‘–represents all elements in [ğ‘›]except forğ‘–.
2.2 Correlation-based Fairness
By accounting for the correlations between sensitive attribute ğ´and
outcomeğ‘Œ, several popular metrics have been proposed to achieve
fairness in the correlation sense [ 8,10,11]. For instance, the Fairness
Through Unawareness (FTU) [ 10] principle proposes to overlook
the sensitive attribute ğ´, while the Demographic Parity (DP) [ 8]
criteria enforces ğ‘€ğœƒto decisions Ë†ğ‘Œindependent from ğ´:ğ´âŠ¥âŠ¥Ë†ğ‘Œ. Be-
sides, two important notions, i.e., Equality of Opportunity (EO) [ 11]
and individualized fairness (IF) [ 9], have generalized the DP metric
on some sub-populations/individuals. To be specific, EO requires
DP on individuals receiving positive decisions: ğ´âŠ¥âŠ¥Ë†ğ‘Œ|Ë†ğ‘Œ=1,
while IF enforces individuals with similar contextual features to re-
ceive similar decisions: ğ·(Ë†ğ‘Œğ‘–,Ë†ğ‘Œğ‘—)â‰¤ğœ–ğ·ğ‘‹(ğ‘‹ğ‘–,ğ‘‹ğ‘—)(ğ·is some metric
andğœ–is some pre-defined threshold).
2.3 Causality-based fairness
While correlation-based fairness notions promote numerous ap-
proaches with compelling simplicity [ 11,23], they may suffer from
bias caused by confounders (explicit ( ğ‘‹) or latent factors). We re-
fer to extensive analysis on such phenomena to previous stud-
ies [19,23]. With the aim of quantifying and migrating the causal
effect ofğ´onË†ğ‘Œvia controlling third factors, the causality-based fair-
ness approaches have emerged in recent years [ 14,19,25,26,39,43].
3830Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
XğŸğŸ
AğŸğŸï¿½ğ’€ğ’€ğŸğŸ
XğŸğŸAğŸğŸï¿½ğ’€ğ’€ğŸğŸAğŸğŸï¿½ğ’€ğ’€ğŸğŸ
AğŸğŸï¿½ğ’€ğ’€ğŸğŸXğŸğŸ
XğŸğŸAğŸğŸï¿½ğ’€ğ’€ğŸğŸ
AğŸğŸï¿½ğ’€ğ’€ğŸğŸDirect Self
(a) Example: DP (c) Causal Graph of PSF (d) Causal Graph of IAFğ’‚ğ’‚â€²
ğ’‚ğ’‚ğ’‚ğ’‚â€²
ï¿½ğ’€ğ’€=ğŸğŸï¿½ğ’€ğ’€=ğŸğŸï¿½ğ’€ğ’€=ğŸğŸ
ğ’‚ğ’‚â€²
ğ’‚ğ’‚ğ’‚ğ’‚â€²
ï¿½ğ’€ğ’€=ğŸğŸï¿½ğ’€ğ’€=ğŸğŸ ï¿½ğ’€ğ’€=ğŸğŸ
(b) Example: IF
Figure 1: Counterexamples and causal graphs we illustrated in Section 4.2. In (b), we merge the node A and Y for the clarity.
In (c), direct (unfair) and indirect (fair) causal paths from ğ´ğ‘–toË†ğ‘Œğ‘–are colored in green, while the red dashed lines represent that
PSF do not consider causal effects across individuals. In (d), self and peer causal effects from ğ´ğ‘–toË†ğ‘Œğ‘–,Ë†ğ‘Œğ‘—are colored in blue.
The counterfactual parity and conditional counterfactual parity [ 25]
has described population-level causality-based fairness by elimi-
nating average and conditional treatment effect: ğ¸[Ë†ğ‘Œ(ğ´=1)]=
ğ¸[Ë†ğ‘Œ(ğ´=0)]andğ¸[Ë†ğ‘Œ(ğ´=1) |ğ‘‹]=ğ¸[Ë†ğ‘Œ(ğ´=0) |ğ‘‹]. The
principle fairness [ 14] and counterfactual equalized odds [ 24] have
developed causality-based fairness specific to some important sub-
populations. The unit-level causality-based fairness, i.e., the coun-
terfactual fairness [ 19], has been proposed based on the Structural
Causal Model (SCM) [ 29]:Ë†ğ‘Œğ‘–(ğ´ğ‘–=1)=Ë†ğ‘Œğ‘–(ğ´ğ‘–=0). The path-
specific causality-based fairness [ 4,26,39] has been developed to
distinguish fair and unfair causal paths from ğ´to the Ë†ğ‘Œwith dif-
ferent mediators. For instance, PSF requires that ğ¸[Ë†ğ‘Œ(ğ´=1,ğ¾(ğ´=
0))]=ğ¸[Ë†ğ‘Œ(ğ´=0,ğ¾(ğ´=0))], whereğ¾is the mediator lying in
the causal path from ğ´toË†ğ‘Œ.
2.4 Fairness on Graphs
Recent research has extensively examined fairness in graph data
across various tasks, such as node classification, link prediction,
and community detection, through graph representation learning
with Graph Neural Networks (GNNs) [ 1,6,7,17,28,31]. Fairness
concepts originally designed for tabular data, including DP, EO, IF,
and sample perturbations, have been integrated into the context of
fair node embedding and classification [ 7]. Notably, prior work in
graph fairness has made significant progress, but none has explicitly
addressed or formalized discrimination arising from peer effects.
3A Real-world Case Study: Unfair Judgment in
Peer-to-Peer Loan
Prior research has highlighted the potential for discrimination stem-
ming from peer effects when incorporating social relationships into
decision-making processes. As depicted in Fig.2, [ 20] constructed a
real-world social network using the Prosper Loans Network Dataset,
encompassing over 1,048,575 Peer-to-peer (P2P) loan data records.
Leveraging this data, our analysis addresses two key objectives (a)
the necessity of considering social relationships when designing
decision-making models, and (b) the identification of fairness con-
cerns arising from the specific modeling of social relationships. Inthis case study, we designate the Location variable2as the sensi-
tive attribute. Therefore, the fairness inquiry centers on whether
the loan decision model exhibits bias against borrowers from less
favorable areas (see Appendix A for details on implementation.)
The first task is to design decision models to judge the individ-
ualâ€™s social score based on their social connections, rating, and
status of corresponding loan records, etc. Meanwhile, the second
task is to design decision models to judge the risk of each loan
record based on features including the credit status, and loan rating
records of the lender and borrower. For the first task, we design and
compare two decision models, i.e., a Graph Neural Network (GNN)
model and a multi-layer MLP model. For the second task, we fol-
low [ 20] and compare the XGBoost classifiers trained with and
without social features [ 20]. This metric quantifies the proportion of
negative judgments on creditworthy individuals with connections
in unfavorable locations and positive judgments on individuals who
lack creditworthiness but have connections in favorable locations3.
In Fig. 2, we observe a significant degradation in model prediction
performance for both tasks in the absence of interference modeling.
However, this improved performance comes at the cost of a con-
siderable increase in Unfair Proportion. This suggests that the bias
introduced by neighboring relationships is concurrently incorpo-
rated into the learning models. Hence, our case study demonstrates
that interference across individuals is a double-edged sword, en-
hancing prediction performance while introducing discrimination
stemming from peer effects.
4 Interference-aware Fairness
4.1 Problem Setting
In the context of interference-aware fairness, we introduce a for-
mal problem. Consider a social network comprising ğ‘›individuals,
denoted as decision subjects, with an adjacency matrix ğºâˆˆRğ‘›âˆ—ğ‘›.
2A binary variable representing the location of each borrower.
3We explicitly encode the feature "creditworthy" for each individual by its social score
ğ‘ ğ‘–, i.e., the true label for the first task. ğ‘ ğ‘–=2refers to creditworthy individuals and
ğ‘ ğ‘–=0refers to individuals who are not creditworthy. In similar, we explicitly encode
the feature "favorable" by the value of the variable "Location". When Location= 1, i.e.,
more low-risk neighbors than high-risk borrowers, the location is "favorable".
3831KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenjing Yang et al.
Location
Loan Status
Credit Status
â€¦Location
Loan Status
Credit Status
â€¦
Location
Loan Status
Credit Status
â€¦B
L
BL
L
B
Figure 2: In the case study of P2P loans on the Web, we present our findings and outcomes. Specifically, on the left-hand side,
â€™Lâ€™ and â€™Bâ€™ denote the lender and borrower in a loan record, while on the right-hand side, we refer to â€™GNNâ€™ as a three-layer
Graph Neural Network, â€™MLPâ€™ as a three-layer fully connected network, â€™Xgb:Sâ€™ as the XGB boost classifier incorporating social
features, and â€™Xgb:nonSâ€™ as the XGB boost classifier excluding social features.
Here,ğºğ‘–ğ‘—=1signifies a causal relationship between individual
ğ´ğ‘–and Ë†ğ‘Œğ‘—, as well as between ğ´ğ‘—and Ë†ğ‘Œğ‘–. Additionally, connections
between individuals ğ‘–andğ‘—imply causal effects from ğ‘‹ğ‘–toğ´ğ‘—and
Ë†ğ‘Œğ‘—, as well as from ğ‘‹ğ‘—toğ´ğ‘–and Ë†ğ‘Œğ‘–. In summary, when individuals ğ‘–
andğ‘—are neighbors, their own covariates influence the sensitive
attribute and decision of the other party, and their sensitive at-
tribute also impacts the decision of the other party. See Fig. 1 (c) for
an illustrative example of this setting. Consequently, the potential
decision of individual ğ‘–can be expressed as Ë†ğ‘Œ(ğ´ğ‘–,ğ‘‹ğ‘–,ğ´âˆ’ğ‘–,ğ‘‹âˆ’ğ‘–)with
ğ´ğ‘–(ğ‘‹ğ‘–,ğ‘‹âˆ’ğ‘–). To address the data inefficiency challenge arising from
the exponential realizations of the high-dimensional treatment vec-
tor (ğ´ğ‘–,ğ´âˆ’ğ‘–), we commonly adopt the following assumptions in the
field of networked interference in prior work [22, 27].
Assumption 1 (Neighborhood interference). The potential
decision for ğ‘–is only decided by its neighborâ€™s sensitive attributes
with covariates: Ë†ğ‘Œ(ğ´ğ‘–,ğ‘‹ğ‘–,ğ´âˆ’ğ‘–,ğ‘‹âˆ’ğ‘–)=Ë†ğ‘Œ(ğ´ğ‘–,ğ‘‹ğ‘–,ğ´Nğ‘–,ğ‘‹Nğ‘–), where
Nğ‘–={ğ‘—|ğºğ‘–ğ‘—=1,ğ‘—â‰ ğ‘–,ğ‘—âˆˆ [ğ‘›]}represents the neighbors of
individualğ‘–on the network. In similar, ğ´ğ‘–(ğ‘‹ğ‘–,ğ‘‹âˆ’ğ‘–)=ğ´ğ‘–(ğ‘‹ğ‘–,ğ‘‹Nğ‘–).
The exposure mapping assumption is further applied to mitigate
the exponential combination of Nğ‘–[22]:
Assumption 2 (Exposure Mapping). There exists Î¦which maps
ANğ‘–to a dense vector Î¦(ANğ‘–)for anyğ‘–âˆˆ [ğ‘›]such that for any
neighboring treatment vectors ğ´1
Nğ‘–andğ´2
Nğ‘–, we have Ë†ğ‘Œ(ğ‘,ğ´1
Nğ‘–)=
Ë†ğ‘Œ(ğ‘,ğ´2
Nğ‘–)ifÎ¦(ğ´1
Nğ‘–)=Î¦(ğ´2
Nğ‘–), where the notations ğ´1
Nğ‘–andğ´2
Nğ‘–
are realization values of ğ´Nğ‘–.
Notably, the above assumption states that we can use a function Î¦
for summarizing the neighbor treatments from a high dimensional
vectorğ´Nğ‘–to a dense vector Î¦(ğ´Nğ‘–). We denote Ëœ0in the exposure
setËœğ´as the no-treatment regime of Nğ‘–, serving as the control group
for treatment effect without interference. Following prior work [ 21,
22,27], we define ğ‘”ğ‘‹as the summary function capturing the in-
fluence of neighboring covariates, i.e., ğ´ğ‘–=ğ‘“ğ´(ğ‘‹ğ‘–,ğ‘”ğ‘‹(ğ‘‹Nğ‘–),ğ‘ˆğ´
ğ‘–),whereğ‘“ğ´is an unspecified function, and ğ‘ˆğ´
ğ‘–represents individ-
ual characteristics. Similarly, ğ‘”ğ´denotes the summary functions
for sensitive attributes, i.e., Ë†ğ‘Œğ‘–=ğ‘“ğ‘Œ(ğ‘‹ğ‘–,ğ‘”ğ‘‹(ğ‘‹Nğ‘–),ğ´ğ‘–,ğ‘”ğ´(ğ´Nğ‘–),ğ‘ˆğ‘Œ
ğ‘–),
whereğ‘“ğ‘Œandğ‘ˆğ‘Œ
ğ‘–are defined analogously. To ensure fairness in the
presence of interference, we introduce the concept of Interference-
Aware Fairness (IAF) to differentiate and mitigate discrimination
arising from sensitive attributes and network neighbors:
Definition 1. A decision model ğ‘€ğœƒsatisfies Self-Fairness (SF),
i.e., eliminates the discrimination from selfâ€™s sensitive attribute, if
the population-level direct (self) effect ofğ´ğ‘–onË†ğ‘Œğ‘–vanishes:
1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆ[ğ‘›]ğ¸[Ë†ğ‘Œğ‘–(ğ´ğ‘–=1,Ëœğ‘)]âˆ’ğ¸[Ë†ğ‘Œğ‘–(ğ´ğ‘–=0,Ëœğ‘)]=0,âˆ€Ëœğ‘âˆˆËœğ´. (1)
Meanwhile, ğ‘€ğœƒsatisfies Peer-Fairness (PF), i.e., eliminates the
discrimination from neighborâ€™s sensitive attribute, if the population-
level peer effect ofğ´Nğ‘–onË†ğ‘Œğ‘–vanishes:
1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆ[ğ‘›]ğ¸[Ë†ğ‘Œğ‘–(ğ´ğ‘–=ğ‘,Ëœğ‘)]âˆ’ğ¸[Ë†ğ‘Œğ‘–(ğ´ğ‘–=ğ‘,Ëœ0)]=0,âˆ€ğ‘âˆˆ{0,1}.(2)
Remark. Ëœğ‘comes from Î¦. By simultaneously satisfying SF and
PF through the enforcement of decision model ğ‘€ğœƒ, we sayğ‘€ğœƒ
satisfies IAF. It is worth noting that conceptualizing SF and PF at the
individual lev (counterfactual) will produce much sharper fairness.
However, the computational challenges of deriving individual-level
IAF from observational data, stemming from the absence of an SCM
model, render this approach practically infeasible. Meanwhile, our
experiments empirically show that regularizing SF and PF already
leads to near-optimal performance. We defer the exploration of
individual-level IAF to future research (refer to the Conclusion for
details).
3832Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Acronym Full Name Definition
DP Demographic Parity ğ´âŠ¥Ë†ğ‘Œ
FTU Fairness Through Unawareness Do not include ğ´to the prediction model
IF Individualized Fairness ğ·
Ë†ğ‘Œğ‘–,Ë†ğ‘Œğ‘—
â‰¤ğœ–ğ·ğ‘‹ ğ‘‹ğ‘–,ğ‘‹ğ‘—
EO Equality of Opportunity ğ´âŠ¥Ë†ğ‘Œ|ğ‘Œ=1
CP Counterfactual Parity ğ¸[Ë†ğ‘Œ(ğ´=1)]=ğ¸[Ë†ğ‘Œ(ğ´=0)]
CCP Conditional Counterfactual Parity ğ¸[Ë†ğ‘Œ(ğ´=1)]=ğ¸[Ë†ğ‘Œ(ğ´=0)|ğ‘‹=ğ‘¥],âˆ€ğ‘¥âˆˆX
CF Counterfactual Fairness Ë†ğ‘Œğ‘–(ğ´ğ‘–=1)=Ë†ğ‘Œğ‘–(ğ´ğ‘–=0),âˆ€ğ‘–âˆˆ[ğ‘›]
PSF Path-specific Fairness ğ¸[Ë†ğ‘Œ(ğ´,ğ¾(ğ´))]=ğ¸Ë†ğ‘Œ(ğ´,ğ¾â€²(ğ´))
,ğ¸[Ë†ğ‘Œ(ğ´,ğ¾(ğ´))]=ğ¸Ë†ğ‘Œ(ğ´â€²,ğ¾(ğ´))
IACP Interference-aware Counterfactual Parity1
ğ‘›Ã
ğ‘–âˆˆ[ğ‘›]ğ¸Ë†ğ‘Œğ‘–(ğ´ğ‘–=1,Ëœğ‘)âˆ’Ë†ğ‘Œ(ğ´ğ‘–=0,0)
=0,âˆ€Ëœğ‘âˆˆËœğ´
IACCP Interference-aware Conditional Counterfactual Parity1
ğ‘›Ã
ğ‘–âˆˆ[ğ‘›]ğ¸Ë†ğ‘Œğ‘–(ğ´ğ‘–=1,Ëœğ‘)âˆ’Ë†ğ‘Œ(ğ´ğ‘–=0,0)|ğ‘‹=ğ‘¥
=0,âˆ€ğ‘¥âˆˆX,Ëœğ‘âˆˆËœğ´
IACF Interference-aware Counterefactual Fairness Ë†ğ‘Œğ‘–(ğ´ğ‘–=1,Ëœğ‘)=Ë†ğ‘Œ(ğ´ğ‘–=0,0),âˆ€ğ‘–âˆˆ[ğ‘›],Ëœğ‘âˆˆËœğ´
SF, PF Self-fairness, Peer-fairness Def. 1
Table 1: Summary of various fairness notions with their capabilities, including whether well-defined, whether can identify
discrimination stemmed from self-effect and peer effects, in our IAF setting.
4.2 Comparison to Other Fairness Notions
In our IAF framework, we assert that prior fairness concepts, such
as correlation-based, causality-based, and graph fairness, fall short
in attaining SF and PF.
Correlation-based Fairness We investigate the limitations of demo-
graphic parity (DP) [ 8], equal opportunity (EO) [ 11], and individual
fairness (IF) [ 9] in achieving statistical fairness (SF) and personal-
ized fairness (PF) through counterexamples. In the context of the
P2P loan case, we modify it by introducing a sensitive attribute,
the borrowerâ€™s race ( ğ´ğ‘–), while keeping other factors constant. We
consider empty contextual features, i.e., ğ‘‹=âˆ…, with the ability to
generalize to non-empty ğ‘‹. In our scenario, two borrowers apply
for a loan, and the decision Ë†ğ‘Œ1,Ë†ğ‘Œ2depends on both ğ´1,ğ´2. We dis-
tinguish between minor and major race groups denoted as ğ‘andğ‘â€².
We setğ‘ƒ(ğ´2=ğ‘â€²)=ğ‘ƒ(ğ´2=ğ‘)=0.5,ğ‘ƒ(ğ´1=ğ‘|ğ´2=ğ‘â€²)=0.99,
andğ‘ƒ(ğ´1=ğ‘|ğ´2=ğ‘)=0.01. We construct an unfair binary
decision model ğ‘€ğœƒwithğ‘ƒ(Ë†ğ‘Œ=1|ğ´1=ğ‘,ğ´ 2=ğ‘)=0.1,
ğ‘ƒ(Ë†ğ‘Œ=1|ğ´1=ğ‘â€²,ğ´2=ğ‘â€²)=0.9,ğ‘ƒ(Ë†ğ‘Œ=1|ğ´1=ğ‘â€²,ğ´2=ğ‘)=
ğ‘ƒ(Ë†ğ‘Œ=1|ğ´1=ğ‘,ğ´ 2=ğ‘â€²)=0.5. Consequently, we calculate that
ğ‘ƒ(Ë†ğ‘Œ1=1|ğ´1=ğ‘â€²)=0.5â‰ˆ0.496=ğ‘ƒ(Ë†ğ‘Œ1=1|ğ´1=ğ‘)andğ‘ƒ(Ë†ğ‘Œ2=
1|ğ´2=ğ‘â€²)=0.504â‰ˆ0.505=ğ‘ƒ(Ë†ğ‘Œ2=1|ğ´2=ğ‘)(see Appendix B
for details). Therefore, both population-level and individual-level
DP fail to identify unfairness in this setting. Similarly, EO, which is
a finer-grained version of DP, also cannot detect unfairness in the
presence of interference. In the context of the IF metric, consider
the loan system example in Fig.1(b) with 6individuals: 4in the
major group ( ğ‘â€²) and 2in the minor group ( ğ‘). The loan decision
model initially exhibits unfairness, rejecting the minor group and
approving the major group with one neighbor. However, when an
applicant from the minor group has two neighbors from the major
group, decisions become positive. Upon utilizing IF for decision
analysis, all subjects (regardless of the same or different ğ´) receive
identical decisions. This remains true even when incorporating
network structure as features through encoding techniques such
as social score in [20] or motifs in [41].
Causality-based Fairness Previous causality-based fairness
approaches, including CP, CCP, CF, and PSF, rely on the Stable
Unit Treatment Value Assumption (SUTVA) [ 15], which assumes
no interference between an individualâ€™s sensitive attribute andneighborsâ€™ decisions. To facilitate a fair comparison, we extend CP,
CCP, and CF to address self-discrimination and peer discrimination
within our proposed IAF problem, demonstrating their limitations:
Definition 2. A decision model ğ‘€ğœƒsatisfies Interference-aware CP
(IACP), Interference-aware CCP (IACCP) and Interference-aware
CF (IACF), if the following criteria are satisfied:
ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³IACP :1
ğ‘›Ã
ğ‘–âˆˆ[ğ‘›]ğ¸[Ë†ğ‘Œğ‘–(ğ´ğ‘–=1,Ëœğ‘)âˆ’Ë†ğ‘Œ(ğ´ğ‘–=0,Ëœ0)]=0âˆ€Ëœğ‘âˆˆËœğ´.
IACCP :1
ğ‘›Ã
ğ‘–âˆˆ[ğ‘›]ğ¸[Ë†ğ‘Œğ‘–(ğ´ğ‘–=1,Ëœğ‘)âˆ’Ë†ğ‘Œ(ğ´ğ‘–=0,Ëœ0)|ğ‘‹]=0âˆ€Ëœğ‘âˆˆËœğ´.
IACF :Ë†ğ‘Œğ‘–(ğ´ğ‘–=1,Ëœğ‘)=Ë†ğ‘Œ(ğ´ğ‘–=0,Ëœ0) âˆ€ğ‘–âˆˆ[ğ‘›],Ëœğ‘âˆˆËœğ´.
In light of the extended concepts mentioned earlier, it becomes
evident that IACP, IACCP, and IACF fail to distinguish between SF
and PF as defined in Def.1. When Ëœğ‘is held fixed in SF and ğ‘=0
in PF, we observe that ğ¼ğ´ğ¶ğ‘ƒ =ğ‘ƒğ¹+ğ‘†ğ¹, implying that a vanished
IACP, representing the complete absence of the combined effects of
ğ´ğ‘–andğ´Nğ‘–, can be decomposed into a negative PF (direct effect of
ğ´ğ‘–) and a positive SF (peer effect of ğ´Nğ‘–). Consequently, a decision
model deemed fair under IACP may exhibit unfairness when viewed
through the lenses of SF and PF. Similarly, decisions considered
fair under IACCP and IACF may appear unfair from the SF and PF
perspectives. In contrast, a fair decision model adhering to SF and
PF criteria is guaranteed to satisfy IACP, as indicated by the decom-
position mentioned above. Finally, we compare PSF and our IAF in
Fig.1 (c) and (d). Specifically, PSF quantifies both direct and indirect
causal effects for the same individual via different causal pathways
fromğ´ğ‘–toË†ğ‘Œğ‘–, such asğ´ğ‘–â†’Ë†ğ‘Œğ‘–andğ´ğ‘–â†’ğ‘€ğ‘–â†’Ë†ğ‘Œğ‘–, where some
causal pathways are deemed fair while others are deemed unfair.
For example, a loan rejection directly caused by an individualâ€™s
race is considered unfair, while a rejection indirectly caused by
inadequate education is considered fair. In contrast, our IAF focuses
on distinguishing and identifying unfair decisions resulting from
self-effect and peer-effects of sensitive attributes.
Graph Fairness. In our analysis, we find that graph fairness
methods, including DP, EO, IF, and graph representation pertur-
bations [ 7,17,28,31], lack the capability to achieve IAF. These
methods (a) typically apply fairness regularization inherited from
tabular data directly to graph models and (b) fail to distinguish
between discrimination originating from an individual and their
3833KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenjing Yang et al.
neighbors, leading to the same limitations we have discussed in the
context of correlation-based fairness comparisons.
Remark. We leave a summary of the boundary capabilities
of various fairness metric in Tab. 1. We observe the potential to
extend PSFâ€™s scope to encompass a broader definition. This involves
analyzing the impact of variable ğ´through Ë†ğ‘Œacross various paths,
both at the feature and individual levels.
Remark. We also provide a summary of different fairness no-
tions and their boundary capabilities in our IAF problem in Tab. 1.
5 A Doubly Robust Debiasing Framework
To achieve IAF, we begin by estimating direct and peer effects
from historical data, following Def.1. We then regularize the ML
modelğ‘€ğœƒto mitigate bias, drawing inspiration from the DragonNet
paradigm [ 32]. We formulate a doubly robust (DR) framework,
named â€œIAF+DRâ€, aligning with the optimization objective and
influence curve of the average causal effect under interference [ 27].
Formally, we aim to constrain the causal effect of the joint variables
ğ´ğ‘–andğ´Nğ‘–onË†ğ‘Œacross the population:
ğœ™(ğ‘âˆ—,Ëœğ‘âˆ—)=1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆ[ğ‘›]ğ¸[Ë†ğ‘Œğ‘–(ğ´ğ‘–=ğ‘âˆ—,ğ´Nğ‘–=Ëœğ‘âˆ—)], (3)
where we use ğ‘âˆ—,Ëœğ‘âˆ—to highlight the interventional values of ğ´ğ‘–
andğ´Nğ‘–and distinguish from observational values. We adapt the
well-known overlap assumption and unconfounded assumption
into our problem:
Assumption 3 (Overlap). For allğ‘¥in the support of ğ‘‹, for all
ğ‘–âˆˆ[ğ‘›]and for all Ëœğ‘âˆˆËœğ´,ğ‘in the support of ğ´, we have:
ğ‘ƒ ğ´ğ‘–=ğ‘,ğ´Nğ‘–=Ëœğ‘|ğ‘”ğ‘‹(ğ‘¥)>0
Assumption 4 (Unconfoundness). All the variables affecting
both the treatments and the potential outcome are observed:
Ë†ğ‘Œğ‘–(ğ´ğ‘–,ğ´Nğ‘–)âŠ¥âŠ¥ğ´ğ‘–,ğ´Nğ‘–|ğ‘”ğ‘‹(ğ‘‹)
We then have the following identification result:
Theorem 1. The target estimand, i.e., ğœ™(ğ‘âˆ—,Ëœğ‘âˆ—)for intervened
valuesğ‘âˆ—,Ëœğ‘âˆ—, can be identified as follows:
ğœ™(ğ‘âˆ—,Ëœğ‘âˆ—)=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1âˆ«
xğ¸[Ë†ğ‘Œğ‘–|ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹(ğ‘¥)]ğ‘ğ‘¥(x)ğ‘‘ğ‘¥.
Proof. We have the following derivation:
ğ¸[ğœ™(ğ‘,Ëœğ‘)]=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğ¸Ë†ğ‘Œğ‘–(ğ‘,Ëœğ‘)
=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1âˆ«
xğ¸Ë†ğ‘Œğ‘–(ğ‘,Ëœğ‘)|ğ‘‹=ğ‘¥
ğ‘ğ‘‹(ğ‘¥)
=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1âˆ«
xğ¸Ë†ğ‘Œğ‘–(ğ‘,Ëœğ‘)|ğ‘,Ëœğ‘,ğ‘‹=ğ‘¥
ğ‘ğ‘‹(ğ‘¥)
=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1âˆ«
xğ¸Ë†ğ‘Œğ‘–|ğ‘,Ëœğ‘,ğ‘‹=ğ‘¥
ğ‘ğ‘‹(ğ‘¥)
=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1âˆ«
xğ¸Ë†ğ‘Œğ‘–|ğ‘,Ëœğ‘,ğ‘”ğ‘‹(ğ‘‹)
ğ‘ğ‘‹(x).
â–¡We begin by parameterizing the model for regression estimation,
denoted asğ¸[Ë†ğ‘Œğ‘–|ğ‘,Ëœğ‘,ğ‘”ğ‘‹(ğ‘¥)], using a deep model, Ë†ğ‘Œğ‘›ğ‘›, which also
serves as the decision model. To model the aggregation of ğ‘”ğ‘‹(ğ‘¥)
from neighboring elements, we opt for a GNN model within Ë†ğ‘Œğ‘›ğ‘›.
Prior research [ 32] has demonstrated the benefit of targeted reg-
ularization for causal inference without interference [ 27,34]. To
improve the robustness and finite-sample efficiency of estimating
ğœ™(ğ‘,Ëœğ‘), we incorporate targeted regularization with interference.
Specifically, we introduce a scoring head Ë†ğ´ğ‘›ğ‘›to the GNN repre-
sentation output for capturing the propensity score, denoted as
ğ‘ƒ ğ´ğ‘–=ğ‘,ğ´Nğ‘–=Ëœğ‘|ğ‘”ğ‘‹(ğ‘¥). We then apply targeted regularization
to ensure that the estimated ğœ™and(Ë†ğ´ğ‘›ğ‘›,Ë†ğ‘Œğ‘›ğ‘›)satisfy the estimation
equation, i.e., ğœ‘(ğ‘Œ,ğ´, Ëœğ´,ğ‘‹;Ë†ğ‘Œğ‘›ğ‘›,Ë†ğ´ğ‘›ğ‘›,ğœ™), defined as follows:
Ë†ğ‘Œğ‘›ğ‘›(ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹(ğ‘¥))âˆ’ğœ™+Ë†ğ´ğ‘›ğ‘›(ğ‘âˆ—,Ëœğ‘âˆ—|ğ‘”ğ‘‹(ğ‘¥))
Ë†ğ´ğ‘›ğ‘›(ğ‘,Ëœğ‘|ğ‘”ğ‘‹(ğ‘¥))[]ğ‘¦ğ‘–âˆ’Ë†ğ‘Œğ‘›ğ‘›(ğ‘,Ëœğ‘,ğ‘”ğ‘‹(ğ‘¥))],
(4)
By regulating that1
ğ‘›Ãğ‘›
ğ‘–=1ğœ‘(ğ‘Œğ‘–,ğ´ğ‘–,Ëœğ´ğ‘–,ğ‘‹ğ‘–;Ë†ğ‘Œğ‘›ğ‘›
ğ‘–,Ë†ğ´ğ‘›ğ‘›
ğ‘–,ğœ™ğ‘–)=0, we de-
signed targeted-regularized outcome estimation as follows:
Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”(ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹(ğ‘¥))=Ë†ğ‘Œğ‘›ğ‘›(ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹(ğ‘¥))+ğœ–âˆ—Ë†ğ´ğ‘›ğ‘›(ğ‘âˆ—,Ëœğ‘âˆ—|ğ‘”ğ‘‹(ğ‘¥))
Ë†ğ´ğ‘›ğ‘›(ğ‘,Ëœğ‘|ğ‘”ğ‘‹(ğ‘¥))
(5)
Hence, the regularization objective is formularized as
Lğ‘Ÿğ‘’ğ‘”=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğ‘âˆ—âˆˆğ´,Ëœğ‘âˆ—âˆˆËœğ´(ğ‘Œğ‘–âˆ’Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”
ğ‘–(ğ‘âˆ—
ğ‘–,Ëœğ‘âˆ—
ğ‘–,ğ‘”ğ‘‹(ğ‘¥ğ‘–)))2
and the estimated ğœ™asğœ™ğ‘Ÿğ‘’ğ‘”(ğ‘âˆ—,Ëœğ‘âˆ—)=1
ğ‘›Ãğ‘›
ğ‘–=1Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”
ğ‘–(ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹(ğ‘¥)).
Based on the above identification of ğœ™(ğ‘âˆ—,Ëœğ‘âˆ—), we are now capable
of designing fairness penalty loss, i.e., Lğ‘“, to remove bias conveyed
in the decision model Ë†ğ‘Œğ‘›ğ‘›:
Lğ‘“=âˆ‘ï¸
ğ‘âˆ—âˆˆğ´VarËœğ‘âˆ—âˆˆËœğ´(ğœ™ğ‘Ÿğ‘’ğ‘”(ğ‘âˆ—,Ëœğ‘âˆ—))+âˆ‘ï¸
Ëœğ‘âˆ—âˆˆËœğ´Varğ‘âˆ—âˆˆğ´(ğœ™ğ‘Ÿğ‘’ğ‘”(ğ‘âˆ—,Ëœğ‘âˆ—)),
(6)
where VarËœğ‘âˆ—âˆˆËœğ´refers to the variance of ğœ™ğ‘Ÿğ‘’ğ‘”(ğ‘âˆ—,Ëœğ‘âˆ—)with differ-
entËœğ‘. This objective can lead to the disappearance of SF and PF,
and attainment of IAF under optimal optimization conditions. Ad-
ditionally, two standard objectives are employed for optimizing
Ë†ğ‘Œğ‘›ğ‘›and Ë†ğ´ğ‘›ğ‘›:Lğ‘Œ=1
ğ‘›Ãğ‘›
ğ‘–=1L(ğ‘Œğ‘–,Ë†ğ‘Œğ‘›ğ‘›
ğ‘–(ğ‘âˆ—
ğ‘–,Ëœğ‘âˆ—
ğ‘–,ğ‘”ğ‘‹(ğ‘¥ğ‘–)))2andLğ´=
1
ğ‘›Ãğ‘›
ğ‘–=1L(ğ´ğ‘–,Ë†ğ´ğ‘›ğ‘›(ğ‘,Ëœğ‘|ğ‘”ğ‘‹(ğ‘¥ğ‘–)))(Lrefers to the classification loss,
i.e., cross-entropy loss). To summarize, the overall objective of our
debiasing framework can be formulated as follows:
Lğ‘ ğ‘¢ğ‘š=Lğ‘Œ+Lğ´+ğ›¼Lğ‘“+ğœ–Lğ‘Ÿğ‘’ğ‘”,
whereğ›¼is the parameter to control the fairness penalty. One obser-
vation hints that minimizing targeted regularization term, e.g., Lğ‘Ÿğ‘’ğ‘”,
force(ğœ™ğ‘Ÿğ‘’ğ‘”,Ë†ğ´ğ‘›ğ‘›,Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”)to satisfy the estimating equation in (4):
0=ğœ•ğœ–(Lğ‘ ğ‘¢ğ‘š)|ğœ–=ğœ–1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1ğœ‘(ğ‘Œğ‘–,ğ´ğ‘–,Ëœğ´ğ‘–,ğ‘‹ğ‘–;Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”
ğ‘–,Ë†ğ´ğ‘›ğ‘›
ğ‘–,ğœ™ğ‘Ÿğ‘’ğ‘”
ğ‘–).
Remark. As our method is formulated using the semi-parametric
analysis, it naturally satisfies the doubly-robustness property. For
ease to understand, we summarize the training &inference proce-
dure as follows:
Step 1. Model training. Learn Ë†ğ‘Œğ‘›ğ‘›as the regression model,
ğ‘”ğ‘‹as the GNN model to capture the aggregation of neighborsâ€™
3834Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
covariates, and Ë†ğ´ğ‘›ğ‘›as the propensity model by minimizing the
training loss:
ğ¿reg=1
ğ‘›ğ‘›âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğ‘âˆ—âˆˆğ´,Ëœğ‘âˆ—âˆˆËœğ´
ğ‘Œğ‘–âˆ’Ë†ğ‘Œreg
ğ‘– ğ‘âˆ—
ğ‘–,Ëœğ‘âˆ—
ğ‘–,ğ‘”ğ‘‹(ğ‘¥ğ‘–)2
,
where
Ë†ğ‘Œğ‘Ÿğ‘’ğ‘” ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹(ğ‘¥)|ğ‘,Ëœğ‘=Ë†ğ‘Œğ‘›ğ‘› ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹(ğ‘¥)+ğœ–ğ´ğ‘›ğ‘›(ğ‘âˆ—,Ëœğ‘âˆ—|ğ‘”ğ‘‹(ğ‘¥))
Ë†ğ´ğ‘›ğ‘›(ğ‘,Ëœğ‘|ğ‘”ğ‘‹(ğ‘¥));
Step 2. Counterfactual outcomes inference. For testing sam-
plesn
ğ‘¥ğ‘¡ğ‘’
ğ‘–,ğ‘ğ‘¡ğ‘’
ğ‘–,Ëœğ‘ğ‘¡ğ‘’
ğ‘–
|ğ‘–=ğ‘›+1,...,ğ‘›+ğ‘šo
, the potential outcome
Ë†ğ‘Œğ‘¡ğ‘’ ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘¥ğ‘¡ğ‘’is estimated by:
Ë†ğ‘Œğ‘¡ğ‘’ ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘¥ğ‘¡ğ‘’=Ë†ğ‘Œğ‘›ğ‘› ğ‘âˆ—,Ëœğ‘âˆ—,ğ‘”ğ‘‹ ğ‘¥ğ‘¡ğ‘’+ğœ–Ë†ğ´ğ‘›ğ‘› ğ‘âˆ—,Ëœğ‘âˆ—|ğ‘”ğ‘‹ ğ‘¥ğ‘¡ğ‘’
Ë†ğ´ğ‘›ğ‘›(ğ‘ğ‘¡ğ‘’,Ëœğ‘ğ‘¡ğ‘’|ğ‘”ğ‘‹(ğ‘¥ğ‘¡ğ‘’))
Step 3. Calculate SF and PF. Then the SF for Ëœğ´=Ëœğ‘can be
calculated as:
SF=1
ğ‘šğ‘›+ğ‘šâˆ‘ï¸
ğ‘–=ğ‘›+1Ë†ğ‘Œğ‘¡ğ‘’
ğ‘–(ğ´ğ‘–=1,Ëœğ‘)âˆ’1
ğ‘šğ‘›+ğ‘šâˆ‘ï¸
ğ‘–=ğ‘›+1Ë†ğ‘Œğ‘¡ğ‘’
ğ‘–(ğ´ğ‘–=0,Ëœğ‘).
We note that the PF can be calculated from a similar argument.
6 Experiments
In this section, we conduct extensive experiments to answer the
following questions:
Q1.Does our PF and SF models effectively identify peer-induced
unfair decisions in reality?
Q2.Does our IAF+DR framework mitigate bias while preserving
high prediction accuracy?
Q3.Does debiasing with IAF impact other fairness metrics?
Q4.What is the impact of the fairness penalty on the trade-off
between prediction accuracy and fairness?
Q5.Can our IAF+DR framework accurately estimate self-effects
and peer-effects?
6.1 Dataset, Baselines, and Metric
Dataset. In this study, we perform experiments on three datasets:
one synthetic and two real-world datasets. The synthetic dataset,
created based on simulations from [ 5], models hiring decisions
for physically demanding jobs. It comprises one binary sensitive
attribute with three covariates and a binary outcome generated
using a predefined SCM. Additionally, we evaluate our methods on
two real-world datasets, namely, the NBA dataset and the Credit
Default Dataset, which include constructed social networkss [ 1,7].
Baselines. We compare our IAF+DR framework with the follow-
ing baselines including (a) two vanilla decision models on graphs: (1)
Graph Convolution Network (GCN) model [ 18] and (2) the Graph
Attention Network (GAT) model [ 35]; (b) Correlation-based fair
GNN methods : (3)CrossWalk [17] achieves fairness by biasing
random walks to cross group boundaries, (4) FairGNN [7] achieves
fairness by incorporating fairness regularization to ensure equi-
table treatment of different groups, (5) NIFTY [1] aims to improve
counterfactual fairness and stability of node representations by
sample perturbations; (6) InFo_GNN [16] adapts the individual
fairness by considering the interconnectedness of nodes on the
network, (7) GEAR [21] proposes graph augmentation by sampleperturbations. (c) Causality-based Fairness Methods : In our study, we
employ the GNN with CP regularization, denoted as IACP (adapted
from Def. 2). We do not consider path-specific fairness or counter-
factual fairness methods, as they rely on prior causal knowledge
and are not well-suited for interference scenarios. Adapting these
methods to our IAF setting would require independent research.
Metric. In prediction, we assess testing accuracy (ACC) using
AUC for each method. Fairness is quantified following established
protocols [ 1,7] through reporting Demographic Parity (DP) and
Equal Opportunity (EO) as: DP=|ğ‘ƒ(Ë†ğ‘Œ=1|ğ´=1)âˆ’ğ‘ƒ(Ë†ğ‘Œ=1|ğ´=
0)|andEO=|ğ‘ƒ(Ë†ğ‘Œ=1|ğ‘Œ=1,ğ´=1)âˆ’ğ‘ƒ(Ë†ğ‘Œ=1|ğ‘Œ=1,ğ´=0)|.
Additionally, we estimate the proposed SF and PF metrics using
IAF+DR on each dataset by reporting the direct effect and peer effect
defined in Assumption. 1. However, recognizing that SF and PF lack
ground truth in real-world data, we introduce a complementary
metric for other baselines to ensure a fair comparison. We extend the
concept of "unfair proportion" used in our case study by employing
matching to mitigate confounding bias from covariates. We term
this metric "Interference-aware Unfairness from Neighbors" (IUFN)
as the quantity measuring the extent of SF and PF. To be spcific,
we first define the variable to reflect the neighboring effect, i.e., ğ‘.
We letğ‘ğ‘–=1if more than half of the individualâ€™s neighbors have
positive sensitive attributes, and ğ‘ğ‘–=0otherwise. Supposing that
the test samples are {(ğ‘‹ğ‘–,ğ‘Œğ‘–,ğ‘ğ‘–,ğ´ğ‘–)}ğ‘›
ğ‘–=1, we then define the overall
metric as the quantity measuring the extent of the unfairness arising
from interference in the network as follows:
IUFN =1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ000âˆ‘ï¸
ğ‘—âˆˆğµ1011 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–+1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ000âˆ‘ï¸
ğ‘—âˆˆğµ0111 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–
+1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ010âˆ‘ï¸
ğ‘—âˆˆğµ1111 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–+1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ100âˆ‘ï¸
ğ‘—âˆˆğµ1111 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–
âˆ’1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ001âˆ‘ï¸
ğ‘—âˆˆğµ1001 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–âˆ’1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ001âˆ‘ï¸
ğ‘—âˆˆğµ0101 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–
âˆ’1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ011âˆ‘ï¸
ğ‘—âˆˆğµ1101 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–âˆ’1
ğ‘›âˆ‘ï¸
ğ‘–âˆˆğµ101âˆ‘ï¸
ğ‘—âˆˆğµ1101 ğ· ğ‘‹ğ‘–,ğ‘‹ğ‘—â‰¤ğœ–,
whereğµğ‘™ğ‘šğ‘˜={ğ‘–âˆˆ[ğ‘›]|ğ´ğ‘–=ğ‘™,ğ‘ğ‘–=ğ‘š,ğ‘Œğ‘–=ğ‘˜},1is the indicator
function, and ğ·(ğ‘‹ğ‘–,ğ‘‹ğ‘—)is the L2 distance between ğ‘‹ğ‘–andğ‘‹ğ‘—, and
the first term is the proportion of units whose outcome would
change from 0 to 1, if we keep ğ‘ğ‘–=0the same but change ğ´ğ‘–from
0 to 1. The rest terms follow a similar argument. We refer to such
metric as â€œInterference-aware Unfairness from Neighborsâ€ ( IUFN),
as IUFN is similar to the matching method [ 33] to account for causal
peer effects.
Implementations. Our approach, IAF+DR, utilizes a 3-layer
GCN as its core. Specifically, we employ GCN as the embedding
model to capture interference across individuals with respect to
covariatesğ‘‹. We assume a prior knowledge of the neighborhood
exposure mapping, denoted as ğ´Nğ‘–âˆˆR|Nğ‘–|, which transforms into
Ëœğ´âˆˆR. Notably, while this assumption may seem restrictive for
general tasks such as estimating causal effects with interference,
we argue that it is justifiable for fairness-related tasks. In fairness
tasks, historical data typically consists of past decision records or
decisions made by previous decision-makers. It is reasonable for the
current decision maker, who is training the model, to possess some
prior knowledge of past decision-making rules. Therefore, following
established protocols in causal inference [ 27,34], we define the
3835KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenjing Yang et al.
Table 2: Comparisons of our proposed IAF+DR with the baselines on ACC, AUC, DP, EO, and IUFN. All experiments are repeated
and averaged with 5 independent random seeds. The best performance is marked in bold.
Dataset Metrics GCN GAT CrossWalk FairGNN NIFTY InFoRM_GNN Gear IACP IAF+DR
SyntheticACC (%) 70.1Â±0.2 71.3Â±0.2 65.5Â±0.6 69.6Â±0.3 65.3Â±0.3 69.1 Â±0.2 74.5Â±0.1 71.1Â±0.2 73.2Â±0.1
AUC (%) 62.7Â±0.2 65.8Â±0.1 61.7Â±0.3 60.6Â±0.2 50.0Â±0.4 69.1 Â±0.1 72.5Â±0.1 68.0Â±0.1 73.1Â±0.2
DP 0.06Â±0.03 0.06Â±0.01 0.04Â±0.01 0.05Â±0.01 0.03Â±0.01 3.68Â±0.4 0.94 Â±0.1 0.16Â±0.03 0.08Â±0.02
EO 0.05Â±0.02 0.05Â±0.01 0.06Â±0.01 0.03Â±0.01 0.03Â±0.02 1.11 Â±0.03 0.95Â±0.08 0.13Â±0.03 0.09Â±0.02
IUFN (%) 17.5Â±0.3 19.6Â±0.4 16.3Â±0.6 18.3Â±0.6 18.5Â±0.2 19.5 Â±0.3 18.5Â±0.4 18.0Â±0.2 0.5Â±0.2
NBAACC (%) 65.9Â±0.2 66.8Â±0.1 46.8Â±0.4 72.3Â±0.3 56.3Â±0.6 68.1 Â±0.5 68.2Â±0.4 70.1Â±0.2 72.3Â±0.1
AUC (%) 65.9Â±0.2 67.2Â±0.1 46.5Â±0.6 72.3Â±0.4 56.3Â±0.6 68.8 Â±0.4 65.9Â±0.3 70.1Â±0.3 71.4Â±0.2
DP 0.31Â±0.06 0.41Â±0.10 10.71Â±1.25 0.14Â±0.06 0.04Â±0.01 2.97Â±0.41 0.19 Â±0.08 0.06Â±0.03 0.05Â±0.02
EO 0.26Â±0.05 0.31Â±0.14 28.69Â±1.68 0.22Â±0.07 0.13Â±0.03 1.20 Â±0.32 0.21Â±0.11 0.26Â±0.03 0.03Â±0.01
IUFN (%) 16.60Â±0.2 22.20Â±0.4 21.1Â±0.3 11.1Â±0.3 17.1Â±0.4 12.2 Â±0.2 18.8Â±0.2 10.0Â±0.1 3.3Â±0.1
CreditACC (%) 69.6Â±0.2 70.4Â±0.1 72.7Â±0.6 66.8Â±0.4 67.6Â±0.3 69.3 Â±0.1 66.2Â±0.2 68.7Â±0.2 70.9Â±0.2
AUC (%) 64.7Â±0.2 66.2Â±0.1 54.3Â±0.5 63.1Â±0.3 64.2Â±0.4 68.1Â±0.1 65.4Â±0.3 67.5Â±0.2 67.9Â±0.2
DP 0.13Â±0.01 0.11Â±0.02 0.03Â±0.01 0.32Â±0.10 0.19Â±0.03 1.49 Â±0.69 0.11Â±0.01 0.06Â±0.03 0.05Â±0.01
EO 0.12Â±0.02 0.12Â±0.04 0.03Â±0.02 0.30Â±0.11 0.19Â±0.02 6.35 Â±1.05 0.11Â±0.06 0.03Â±0.02 0.03Â±0.01
IUFN (%) 5.8Â±0.2 6.7Â±0.2 3.6Â±0.7 4.0Â±0.6 4.0Â±0.7 3.8 Â±0.1 4.6Â±0.2 5.2Â±0.2 0.8Â±0.2
neighborhood exposure mapping from ğ´Nğ‘–toËœğ´as binary-valued:
Ëœğ´=1when half of ğ´Nğ‘–is positive, and Ëœğ´=0otherwise (see
Appendix D.2 for details on baselines).
6.2 Performance Comparison
SF and PF captures unfair decisions raised from peer ef-
fects (Q1). In Fig. 3a, 3b, and 3c, we present the IFUN metric on
synthetic data and two real-world datasets prior to applying our
IAF+DR debiasing method. As previously indicated, the IFUN metric
estimates SF and PF by cross-referencing diverse groups of individ-
uals. Statistical findings regarding IFUN, particularly with NBA and
credit data, reveal that real-world data often exhibits bias attribut-
able to self-effect or peer effects of sensitive attributes. For instance,
over 20% of NBA data records exhibit nationality-based discrimina-
tion originating from either an individualâ€™s own nationality or that
of their neighbors. This reaffirms our stance: decision-making in an
interconnected world is susceptible to bias stemming from self or
neighbor-related sensitive attributes, emphasizing the importance
of identifying and mitigating these forms of discrimination.
Our IAF+DR migrates unfair decisions while maintains pre-
diction performance (Q2). In Table 2, we present the mean and
standard deviations of metrics across baseline models for three
datasets. Our observations are as follows:
â€¢Previous debiasing approaches, including CrossWalk, FairGNN,
NIFTY, InFoRM_GNN, and Gear, achieve satisfactory prediction
performance and reduce bias on standard fairness metrics like DP
and EO. However, their performance on the interference-fairness
metric (IUFN) indicates that they still retain discrimination stem-
ming from neighboring relationships. Notably, the IUFN of Cross-
Walk and Gear exceeds that of GCN by nearly 20% on the NBA
dataset.
â€¢Our compared baseline, IACP, falls short in mitigating interference-
aware discrimination across all datasets. These results confirm
our theoretical analysis in Section 4.2, which asserts that the van-
ished total effect cannot be assumed to eliminate both self-effect
and peer effects.â€¢In contrast, our proposed IAF+DR method effectively reduces the
IUFN metric across all datasets when compared to other baselines,
while maintaining competitive prediction accuracy.
Debias on SF and PF will not conflict with conventional
fairness metric (Q3). As depicted in Table 2, our IAF+DR approach
effectively mitigates bias in IUFN while maintaining low bias in DP
and DP across various datasets. These results support our decompo-
sition, TF=SF+PF, where DP and EO represent correlated versions
of TF for the overall population and specific sub-populations. We
contend that addressing bias in SF and PF aligns with established
fairness criteria.
6.3 In-depth Analysis
Impact of fairness penalty to prediction and fairness (Q4). In
Fig.4, we observe that as the penalty parameter ( ğ›¼) increases, the
test accuracy of our IAF+DR stabilizes in the range [0.0,1.5]while
the IUFN metric sharply declines for ğ›¼>1.25. Asğ›¼approaches 2.0,
both ACC and IUFN reach and maintain lower values. This phenom-
enon highlights the successful debiasing capability of our proposed
IAF+DR while maintaining high prediction accuracy within the
fairness penalty range of ğ›¼âˆˆ[1,1.5]. Itâ€™s worth noting that the re-
duction in prediction performance with increasing fairness penalty
is a common occurrence due to historical decision data bias, as
discussed in Q1. The fairness task seeks to strike a balance between
acceptable prediction accuracy and minimizing decision bias.
Targeted regularization is crucial for estimations of SF
and PF (Q5). We evaluated SF and PF estimation with and without
our targeted regularization term Lğ‘Ÿğ‘’ğ‘”(see Fig. 3d). Specifically,
"withoutLğ‘Ÿğ‘’ğ‘”" denotes the optimization of only Lğ‘ŒandLğ´, with
decision outcomes derived from Ë†ğ‘Œğ‘›ğ‘›rather than Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”Since only
synthetic data possesses ground truth for SF and PF, measuring
estimation error on NBA and Credit is infeasible, necessitating the
introduction of the IUFN metric. Notably, the absence of Lğ‘Ÿğ‘’ğ‘”(left
bars) results in a significant increase in estimation error compared
to its presence (right bars).
3836Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
(a) Debiasing on Synthetic
 (b) Debiasing on NBA
 (c) Debiasing on Credit
 (d) Estimation on Synthetic
Figure 3: Results on: (a,b,c): Debiasing performance of SF and PF across datasets; (d) Estimation performance on SF and PF for
synthetic data. All experiments are repeated and averaged with 5 independent random seeds.
(a) Trade-off on NBA Data
 (b) Trade-off on Credit Data
Figure 4: Trade-off between fairness and prediction accuracy
on NBA and Credit data by tuning the fairness penalty ğ›¼
ranging in[0.0,0.01,0.02,..., 2.00].
(a) AUC
 (b) IUFN
Figure 5: Impact of substitution on the backbone across three
datasets. All experiments are repeated and averaged with 5
independent random seeds.
Impact of different backbones. Questions may arise regarding
the criticality of this choice and the adaptability of our proposed
IAF+DR framework to various backbones. To address these con-
cerns, we present a comparative analysis of AUC and IUFN results
for our IAF+DR model across three datasets using both GCN and
GAT as backbones (see Fig. 5). Our results demonstrate that the
choice of backbone, whether GCN or GAT, does not significantly
impact the prediction performance or debiasing capabilities of our
IAF+DR framework.
Impact of parameter ğœ–.In Fig.6, we observe that as ğœ–increases,
the debias capability of our IAF+DR, i.e., IUFN, and the estimation
error decreases sharply. When ğœ–â‰¥1, all metrics are stabilized. Such
phenomenon shows that the DR regularization effectively estimates
SF and PF, and efficiently migrates bias.
7 Conclusion
In this paper, we introduce Interference-Aware Fairness (IAF), a
novel concept addressing discrimination within interconnected
Figure 6: Impact of the parameter for our DR framework. i.e.,
ğœ–, on estimation and fairness. Results are reported by tuning
ğœ–ranging in[0.0,0.01,0.02,..., 2.00].
individuals on social networks. We establish that achieving IAF
is tantamount to achieving two proposed fairness metrics: Self-
Fairness (SF) and Peer-Fairness (PF). Consequently, we present a
doubly robust framework for end-to-end estimation and mitigation
of SF and PF in model decisions.
We propose several avenues for future research. Firstly, an exten-
sion of IAF to individual-level fairness, specifically addressing coun-
terfactual causal inference, warrants attention. While individual-
level IAF offers sharper fairness compared to population-level IAF,
its identification in the context of SF and PF (individual-level) re-
mains an open challenge. An alternative approach gaining popular-
ity involves establishing a computationally feasible upper bound
for SF and PF based on observational data.
Acknowledgement
This work was supported in part by the National Natural Science
Foundation of China (623B2002,62141607,62376243,62441605,62302
503,KY0402052402), Starry Night Science Fund of Zhejiang Univer-
sity Shanghai Institute for Advanced Study (SN-ZJU-SIAS-0010),
and NUDT Youth Independent Innovation Science Fund Project
(Grant No. ZK23-15).
3837KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenjing Yang et al.
References
[1]Chirag Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. 2021. Towards a uni-
fied framework for fair and stable graph representation learning. In Uncertainty
in Artificial Intelligence. PMLR, 2114â€“2124.
[2]Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. 2016. Machine bias.
(2016).
[3]Hal Berghel. 2020. A Critical Look at the 2019 College Admissions Scandal?
Computer 53, 1 (2020), 72â€“77.
[4]Silvia Chiappa. 2019. Path-specific counterfactual fairness. In Proceedings of the
AAAI conference on artificial intelligence, Vol. 33. 7801â€“7808.
[5]Yoichi Chikahara, Shinsaku Sakaue, Akinori Fujino, and Hisashi Kashima. 2021.
Learning individually fair classifier with path-specific causal-effect constraint. In
International conference on artificial intelligence and statistics. PMLR, 145â€“153.
[6]Manvi Choudhary, Charlotte Laclau, and Christine Largeron. 2022. A survey on
fairness for machine learning on graphs. arXiv preprint arXiv:2205.05396 (2022).
[7] Enyan Dai and Suhang Wang. 2021. Say no to the discrimination: Learning fair
graph neural networks with limited sensitive attribute information. In Proceedings
of the 14th ACM International Conference on Web Search and Data Mining . 680â€“
688.
[8]Richard B Darlington. 1971. Another look at â€œcultural fairnessâ€ 1. Journal of
educational measurement 8, 2 (1971), 71â€“82.
[9]Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
Zemel. 2012. Fairness through awareness. In Proceedings of the 3rd innovations in
theoretical computer science conference. 214â€“226.
[10] Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian
Weller. 2016. The case for process fairness in learning: Feature selection for fair
decision making. In NIPS symposium on machine learning and the law, Vol. 1.
Barcelona, Spain, 11.
[11] Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in
supervised learning. Advances in neural information processing systems 29 (2016).
[12] Yaowei Hu, Yongkai Wu, Lu Zhang, and Xintao Wu. 2020. Fair multiple decision
making through soft interventions. Advances in Neural Information Processing
Systems 33 (2020), 17965â€“17975.
[13] Michael G Hudgens and M Elizabeth Halloran. 2008. Toward causal inference
with interference. J. Amer. Statist. Assoc. 103, 482 (2008), 832â€“842.
[14] Kosuke Imai and Zhichao Jiang. 2023. Principal fairness for human and algorith-
mic decision-making. Statist. Sci. 38, 2 (2023), 317â€“328.
[15] Guido W Imbens and Donald B Rubin. 2010. Rubin causal model. In Microecono-
metrics. Springer, 229â€“241.
[16] Jian Kang, Jingrui He, Ross Maciejewski, and Hanghang Tong. 2020. Inform:
Individual fairness on graph mining. In Proceedings of the 26th ACM SIGKDD
international conference on knowledge discovery & data mining. 379â€“389.
[17] Ahmad Khajehnejad, Moein Khajehnejad, Mahmoudreza Babaei, Krishna P Gum-
madi, Adrian Weller, and Baharan Mirzasoleiman. 2022. Crosswalk: Fairness-
enhanced node representation learning. In Proceedings of the AAAI Conference
on Artificial Intelligence, Vol. 36. 11963â€“11970.
[18] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph
convolutional networks. arXiv preprint arXiv:1609.02907 (2016).
[19] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. 2017. Counterfac-
tual fairness. Advances in neural information processing systems 30 (2017).
[20] Yanying Li, Yue Ning, Rong Liu, Ying Wu, and Wendy Hui Wang. 2020. Fairness
of classification using usersâ€™ social relationships in online peer-to-peer lending.
InCompanion Proceedings of the Web Conference 2020. 733â€“742.
[21] Jing Ma, Ruocheng Guo, Mengting Wan, Longqi Yang, Aidong Zhang, and Jun-
dong Li. 2022. Learning fair node representations with graph counterfactual
fairness. In Proceedings of the Fifteenth ACM International Conference on Web
Search and Data Mining. 695â€“703.
[22] Yunpu Ma and Volker Tresp. 2021. Causal inference under networked interference
and intervention policy enhancement. In International Conference on ArtificialIntelligence and Statistics. PMLR, 3700â€“3708.
[23] Karima Makhlouf, Sami Zhioua, and Catuscia Palamidessi. 2020. Survey on
causal-based machine learning fairness notions. arXiv preprint arXiv:2010.09553
(2020).
[24] Alan Mishler, Edward H Kennedy, and Alexandra Chouldechova. 2021. Fairness in
risk assessment instruments: Post-processing to achieve counterfactual equalized
odds. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and
Transparency. 386â€“400.
[25] Shira Mitchell, Eric Potash, Solon Barocas, Alexander Dâ€™Amour, and Kristian
Lum. 2021. Algorithmic fairness: Choices, assumptions, and definitions. Annual
Review of Statistics and Its Application 8 (2021), 141â€“163.
[26] Razieh Nabi and Ilya Shpitser. 2018. Fair inference on outcomes. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 32.
[27] Elizabeth L Ogburn, Oleg Sofrygin, Ivan Diaz, and Mark J Van der Laan. 2022.
Causal inference for social network data. J. Amer. Statist. Assoc. (2022), 1â€“15.
[28] John Palowitch and Bryan Perozzi. 2019. Monet: Debiasing graph embeddings via
the metadata-orthogonal training unit. arXiv preprint arXiv:1909.11793 (2019).
[29] Judea Pearl. 2009. Causality. Cambridge university press.
[30] Donald B Rubin. 2005. Causal inference using potential outcomes: Design, mod-
eling, decisions. J. Amer. Statist. Assoc. 100, 469 (2005), 322â€“331.
[31] Akrati Saxena, George Fletcher, and Mykola Pechenizkiy. 2022. Fairsna: Al-
gorithmic fairness in social network analysis. arXiv preprint arXiv:2209.01678
(2022).
[32] Claudia Shi, David Blei, and Victor Veitch. 2019. Adapting neural networks for
the estimation of treatment effects. Advances in neural information processing
systems 32 (2019).
[33] Elizabeth A Stuart. 2010. Matching methods for causal inference: A review and a
look forward. Statistical science: a review journal of the Institute of Mathematical
Statistics 25, 1 (2010), 1.
[34] Mark J Van der Laan. 2014. Causal inference for a population of causally connected
units. Journal of Causal Inference 2, 1 (2014), 13â€“74.
[35] Petar VeliÄkoviÄ‡, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint
arXiv:1710.10903 (2017).
[36] Yongkai Wu, Lu Zhang, and Xintao Wu. 2018. On discrimination discovery
and removal in ranked data using causal graph. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining. 2536â€“
2544.
[37] Yongkai Wu, Lu Zhang, and Xintao Wu. 2019. Counterfactual fairness: Unidenti-
fication, bound and algorithm. In Proceedings of the twenty-eighth international
joint conference on Artificial Intelligence.
[38] Yongkai Wu, Lu Zhang, and Xintao Wu. 2019. On convexity and bounds of
fairness-aware classification. In The World Wide Web Conference. 3356â€“3362.
[39] Yongkai Wu, Lu Zhang, Xintao Wu, and Hanghang Tong. 2019. Pc-fairness: A
unified framework for measuring causality-based fairness. Advances in neural
information processing systems 32 (2019).
[40] Depeng Xu, Yongkai Wu, Shuhan Yuan, Lu Zhang, and Xintao Wu. 2019. Achiev-
ing causal fairness through generative adversarial networks. In Proceedings of
the Twenty-Eighth International Joint Conference on Artificial Intelligence.
[41] Yuan Yuan, Kristen Altenburger, and Farshad Kooti. 2021. Causal network motifs:
identifying heterogeneous spillover effects in A/B tests. In Proceedings of the Web
Conference 2021. 3359â€“3370.
[42] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. A causal framework for discovering
and removing direct and indirect discrimination. arXiv preprint arXiv:1611.07509
(2016).
[43] Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, and Mingming Gong.
2022. Counterfactual fairness with partially known causal graph. Advances in
Neural Information Processing Systems 35 (2022), 1238â€“1252.
3838Your Neighbor Matters: Towards Fair Decisions Under Networked Interference KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
A Details on Case Study
The Prosper dataset contains 1,048,575 loan records that occurred
from November 2005 to September 2011. Each record contains
nine features, including the Lender ID, Borrower ID, Timestamp,
Amount, Status (a binary variable representing the risk of each bor-
rower), Lender rate, Borrower rate, and the Rating. We refer readers
to [20] for a more detailed introduction to each feature. The social
network is naturally built for each individual by adding undirected
edges among each borrower and lender [ 20]. However, we only
concerned with the borrower network for individual judgment task,
i.e., the network consists of individuals who has become a borrower
more than once.
The first task, i.e. the individual social score judgment task, is
performed as a prediction task on the borrower network, where
the decision outcome is the social score for each borrower. In our
case study, such variable is interpreted as the â€œLocationâ€ for each
borrower. Of course, a loan decision should be discriminative on
the locations of borrowers. Considering the social network built
by loan relationships, this can be regarded as a node classification
task. Especially, the original social score is set as follows:
ğ‘ ğ‘–=1
1+
ğœ†
1âˆ’ğœ†ğ‘”ğ‘–ğœ†ğ‘+(1âˆ’ğœ†)
ğœ†+(1âˆ’ğœ†)ğ‘ğ¿ğ‘–ğœ†+(1âˆ’ğœ†)ğ‘
ğœ†ğ‘+(1âˆ’ğœ†)ğ»ğ‘–, (7)
whereğ»ğ‘–andğ¿ğ‘–account for the number of high-risk and low-
risk neighboring borrowers, and ğ‘”ğ‘–=I(ğ»ğ‘–>ğ¿ğ‘–). We set hyper-
parameterğœ†=0.46andğ‘=0.5following original implementations
in [20]. Based on the value of ğ‘ ğ‘–âˆˆ[0,1], we create discrete labels in
[0,1,2]by settingğ‘ ğ‘–falling in[0.0,0.33),[0.33,0.66), and[0.66,1.0],
respectively. We then construct two models, i.e., one GNN and one
MLP, for prediction4. The GNN model contains three Graph Con-
volutional Layers (GCL) with reluactivation functions. The MLP
model contains three fully connected layers with reluactivation
functions. All the latent representations are set to 64 neurons. We
optimize both GNN and MLP using the Adam optimizer with an
initial learning rate 0.01and weight-decay as 5ğ‘’âˆ’4. Both the accu-
racy and unfair proportions metric are reported on the testing data,
where we split the whole data in the ratio 8 : 2.
The second task, i.e. the loan risk judgment task, is performed as
a prediction task on the overall loan records, where the decision out-
come is the loan status, i.e., the risk of each loan. Following [ 20], we
use the XGBoost classifier in Sklearn https://scikit-learn.org/stable/
to perform prediction. We here set the ğ‘ ğ‘–into binary variables,
whereğ‘ ğ‘–<0.5corresponds to label 0andğ‘ ğ‘–>=0.5corresponds
to label 1. To test whether the accuracy and unfair proportion will
change with and without modeling the network structure, we con-
struct two XGBoost predictors with and without the social score
as features, i.e., the XGB:S and XGB:non-S. Notably, we interpret
the social feature here as a signal of the living/working location
of each borrower, as the calculation of ğ‘ ğ‘–relies onğ‘ ğ‘–â€™s neighbors.
Both the accuracy and unfair proportions metric are reported on
the testing data, where we split the whole data in the ratio 8 : 2.
The unfair proportion we reported in the case study roughly
characterizes interference-specific unfairness, which reports the
4We note that all GNN-related models are constructed by the Pytorch-geometric
package in https://pytorch-geometric.readthedocs.io/en/latest/proportion of individuals who receive unfair decisions caused by
their neighboring relationships. For the first task, individuals re-
ported by unfair proportion can be divided into two types: (1)
individuals with low observed signals (0) and powerful connec-
tions (ground truth of social score ğ‘ ==2) receive negative credit
judgment (predicted ğ‘ ==0); (2) individuals with high observed sig-
nals (1) and weak connections (ground truth of social score ğ‘ ==0)
receives negative credit judgment (predicted ğ‘ ==2). Similarly,
for the second task, the unfair proportion accounts for two types
of samples: (1) loan record with low loan status (ground truth of
low-risk borrower) and powerful connections (social score of the
borrowerğ‘ >0.9) receive negative evaluation (high risk); (2) loan
record with high loan status (ground truth of high-risk borrower)
and weak connections (social score of the borrower ğ‘ <0.1) receive
positive evaluation (low risk).
B Details on Comparison between our IAF and
Conventional Fairness Notions
We first detail the constructed counterexample to distinguish our
IAF from DP. To be more intuitive, we present the unfair decision
model, i.e.,ğ‘Œ, as follows:
ğ´1ğ´2ğ‘ƒ(Ë†ğ‘Œ=1|ğ´1,ğ´2)
ğ‘ğ‘ 0.1
ğ‘â€²ğ‘â€²0.9
ğ‘ğ‘â€²0.5
ğ‘â€²ğ‘ 0.5(8)
, together with ğ‘ƒ(ğ´1=ğ‘|ğ´2=ğ‘â€²)=0.99,ğ‘ƒ(ğ´1=ğ‘|ğ´2=ğ‘)=
0.01andğ‘ƒ(ğ´2=ğ‘â€²)=0.5=ğ‘ƒ(ğ´2=ğ‘), then we first derive the
DP for individual 2 as follows:
ğ‘ƒ
Ë†ğ‘Œ2=1|ğ´2=ğ‘â€²
=âˆ‘ï¸
ğ‘1âˆˆ{ğ‘,ğ‘}ğ‘ƒ
Ë†ğ‘Œ2=1|ğ´1=ğ‘1,ğ´2=ğ‘â€²
ğ‘ƒ ğ´1=ğ‘1|ğ´2=ğ‘â€²=0.5
ğ‘ƒ
Ë†ğ‘Œ2=1|ğ´2=ğ‘
=âˆ‘ï¸
ğ‘1âˆˆ{ğ‘,ğ‘}ğ‘ƒ
Ë†ğ‘Œ2=1|ğ´1=ğ‘1,ğ´2=ğ‘
ğ‘ƒ(ğ´1=ğ‘1|ğ´2=ğ‘)=0.496.
We then derive the posterior probabilities as follows:
ğ‘ƒ ğ´1=ğ‘â€²
=âˆ‘ï¸
ğ‘2âˆˆ{ğ‘,ğ‘}ğ‘ƒ ğ´1=ğ‘â€²|ğ´2=ğ‘2ğ‘ƒ(ğ´2=ğ‘2)=0.5
ğ‘ƒ(ğ´1=ğ‘)
=âˆ‘ï¸
ğ‘2âˆˆ{ğ‘,ğ‘}ğ‘ƒ(ğ´1=ğ‘|ğ´2=ğ‘2)ğ‘ƒ(ğ´2=ğ‘2)=0.5,
and we have:
ğ‘ƒ ğ´2=ğ‘|ğ´1=ğ‘â€²=ğ‘ƒ(ğ´1=ğ‘â€²|ğ´2=ğ‘)ğ‘ƒ(ğ´2=ğ‘)
ğ‘ƒ(ğ´1=ğ‘â€²)=0.99
ğ‘ƒ(ğ´2=ğ‘|ğ´1=ğ‘)=ğ‘ƒ(ğ´1=ğ‘|ğ´2=ğ‘)ğ‘ƒ(ğ´2=ğ‘)
ğ‘ƒ(ğ´1=ğ‘)=0.01.
Hence, we derive similar results for individual 2 as follows:
ğ‘ƒ
Ë†ğ‘Œ1=1|ğ´1=ğ‘â€²
=0.504
ğ‘ƒ
Ë†ğ‘Œ1=1|ğ´1=ğ‘
=0.505
3839KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Wenjing Yang et al.
Algorithm 1 Training procedure of Our DR Framework
1:Input The datasetD={ğ‘‹ğ‘–,ğ´ğ‘–,ğ‘Œğ‘–}ğ‘›
ğ‘–=1with the adjacent ma-
trixğº, the GNN-based feature encoder ğ‘‰, the neighborhood
exposure mapping Î¦, the hyper-parameter ğœ–.
2:foritr = 1 toIdo
3: Embedding covariates ğ‘‹byEmb X=ğ‘‰(ğ‘‹,ğº);
4: Computing the neighbor exposure mapping for each node
asËœğ´=Î¦(ğ´,ğº);
5: Compute (observed) hashed value ğ´ğ‘ ğ‘¢ğ‘š=Î¦(ğ´,Ëœğ´);
6: Compute the prediction head Ë†ğ‘Œğ‘›ğ‘›(ğ¸ğ‘šğ‘ğ‘‹,ğ´ğ‘ ğ‘¢ğ‘š);
7: Compute the score head Ë†ğ´ğ‘›ğ‘›(ğ´ğ‘ ğ‘¢ğ‘š|ğ¸ğ‘šğ‘ğ‘‹);
8:Lğ‘Ÿğ‘’ğ‘”=0;
9: forAll possible values (ğ‘âˆ—,Ëœğ‘âˆ—)inğ´Ã—Ëœğ´:do
10: Compute hashed value ğ´âˆ—,ğ‘ ğ‘¢ğ‘š=Î¦(ğ‘âˆ—,Ëœğ‘âˆ—);
11: Compute Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”(ğ´âˆ—,ğ‘ ğ‘¢ğ‘š,ğ¸ğ‘šğ‘ğ‘‹);
12: Compute Ë†ğ´ğ‘›ğ‘›(ğ´âˆ—,ğ‘ ğ‘¢ğ‘š|ğ¸ğ‘šğ‘ğ‘‹);
13: Compute Ë†ğ‘Œğ‘Ÿğ‘’ğ‘”(ğ´âˆ—,ğ‘ ğ‘¢ğ‘š,ğ¸ğ‘šğ‘ğ‘‹)based on above terms;
14: AccumulateLğ‘Ÿğ‘’ğ‘”;
15: end for
16: forAll possible values (ğ‘âˆ—,Ëœğ‘âˆ—)inğ´Ã—Ëœğ´:do
17: Computeğœ™ğ‘Ÿğ‘’ğ‘”(ğ‘âˆ—,Ëœğ‘âˆ—);
18: end for
19: ComputeLğ‘“based on{ğœ™ğ‘Ÿğ‘’ğ‘”(ğ‘âˆ—,Ëœğ‘âˆ—)}ğ‘âˆ—âˆˆğ´,Ëœğ‘âˆ—âˆˆËœğ´;
20: ComputeLğ‘Œ=1
ğ‘›Ãğ‘›
ğ‘–=1L(ğ‘Œğ‘–,Ë†ğ‘Œğ‘›ğ‘›
ğ‘–(ğ´ğ‘ ğ‘¢ğ‘š,ğ‘”ğ‘‹(ğ‘¥ğ‘–)))2;
21: ComputeLğ´=1
ğ‘›Ãğ‘›
ğ‘–=1L(ğ´ğ‘–,Ë†ğ´ğ‘›ğ‘›(ğ´ğ‘ ğ‘¢ğ‘š|ğ‘”ğ‘‹(ğ‘¥ğ‘–)));
22: Update overall objective Lğ‘ ğ‘¢ğ‘š;
23:end for
24:Output Using Ë†ğ‘Œğ‘›ğ‘›as the decision model.
C Algorithm Procedure
We detail the procedure of our proposed IAF+DR in Alg. 1.
D Experimental Details
D.1 Dataset Details
Synthetic Dataset Description Following [ 5], we construct the
synthetic data containing the gender ğ´âˆˆ{0,1}, qualification ğ‘„,
number of children ğ·, physical strength ğ‘€ğœƒ, and hiring decision
outcomeğ‘Œâˆˆ{0,1}following a pre-defined SCM. We first randomly
sample the social network by generating the adjacent matrix ğº:
ğºğ‘–ğ‘—âˆ¼Bernoulli(0.5). We then generate each feature attribute as
follows:
ğ‘„=
ğ‘ˆğ‘„
, ğ‘„âˆˆRğ‘›ğ‘ˆğ‘„âˆ¼N
2,52
,
ğ·=ğ´+âŒŠ0.5ğ‘„ğ‘ˆğ·âŒ‹, ğ·âˆˆRğ‘›,ğ‘ˆğ·âˆ¼TrN
2,12,0.1,3.0
,
ğ‘€=3ğ´+0.4ğ‘„ğ‘ˆğ‘€, ğ‘€âˆˆRğ‘›, ğ‘ˆğ‘€âˆ¼TrN
3,22,0.1,3.0
,
ğ´âˆ¼Bernoulli
expit(ğºğ‘‡ğ‘„âˆ’40)
, ğ´âˆˆRğ‘›,(9)
where expit refers to the sigmoid function. We then perform ex-
posure mapping for ğ´Nğ‘–for eachğ‘–ğ‘›[ğ‘›]and thus the mapping of
ğ´:
Ëœğ´ğ‘–=I(ğºğ‘‡ğ´>0.5âˆ—mean(ğºğ‘‡ğ´)),where mean refers to the mean-value of the vector ğºğ‘‡ğ´, andI
refers to the indicator function. Intuitively, our exposure mapping
here implies that Ëœğ´ğ‘–=1if the number of positive neighbors of ğ‘–is
larger than the mean level of the overall population. Furthermore,
we generated hashed ğ´from Ëœğ´ğ‘–andğ´ğ‘–:
ğ´â„ğ‘ğ‘ â„=ï£±ï£´ï£´ï£´ ï£²
ï£´ï£´ï£´ï£³0,ifËœğ´ğ‘–=0&ğ´=1
1,ifËœğ´ğ‘–=1&ğ´=0orËœğ´ğ‘–=0&ğ´=1
2,ifËœğ´ğ‘–=1&ğ´=1(10)
Hence,ğ‘Œare generated as follows:
ğ‘Œâˆ¼Bernoulli(expit(âˆ’20+10ğ´â„ğ‘ğ‘ â„+0.1âˆ—(ğºğ‘‡ğ‘„+ğºğ‘‡ğ·+ğºğ‘‡ğ‘€)))
NBA dataset This is extended from a Kaggle dataset 1 containing
around 400 NBA basketball players. The performance statistics of
players in the 2016-2017 season and other various information e.g.,
nationality, age, and salary are provided. The social network is
constructed by collecting the relationships of the NBA basketball
players on Twitter with its official crawling API 2. The sensitive, i.e.,
the nationality, is binarized into two categories, i.e., U.S. players and
overseas players. The classification, in task is to predict whether
the salary of the player is over the median [7].
Credit Default Dataset Credit Default Dataset is comprised
of 30,000 nodes, where each node represents individuals who are
utilizing some form of credit. Each node contains 13 attributes.
Individuals are connected by their spending and payment behavior.
The classification task is to determine whether an individual will
default on the credit card payment. Age is used as the sensitive
attribute [1].
D.2 Implementation Details
To unify backbone embedding model across all baselines, we
set the GNN model with three GCN layers throughout our exper-
iments (except for the GAT baseline). The activation function is
therelufunction, and the optimizer is set as the Adam optimizer
with initial learning rate 0.001and weight_decay as 5ğ‘’âˆ’4. The
training epoch for all baselines is set to 300. We run experiments
on all baselines using the open-source project, i.e., the pygdebias
package in https://github.com/yushundong/PyGDebias. For the
NIFTY [ 1], we set the penalty parameter as 0.6following original
implementations. For fairGNN method [ 7], we setğ›¼=100and
ğ›½=1following their optimal setting. For CrossWalk in [ 17], the
random walk length is set to ğ‘‘=5, and the number of walks is
set toğ‘Ÿ=500, and their ğ›¼=0.5andğ‘=2. For InfoRM_GNN
in [16], we setğ›¼=10andğœ‚across all datasets by combining
their original suggestions and our cross-fold validation results. For
GEAR [ 21], we follow their original parameter setting, which states
thatğœ†=0.6,ğ¶=2,ğœ†ğ‘ =0.4,ğ›½=10,ğœ‡=1ğ‘’âˆ’5,ğ‘˜=20,ğµ=4. For
our IAF+DR method, we map Ëœğ´andğ´to a scalar value by simply
operations as ğ´â„ğ‘ğ‘ â„=Ëœğ´+2âˆ—ğ´, as we hold the belief that individuals
with lowğ´and low Ëœğ´will receives the worst decisions, individuals
with highğ´and high Ëœğ´will receive best decisions, and individuals
with lowğ´and high Ëœğ´or highğ´but low Ëœğ´will lie between. Besides,
we setğ›¼=1withğœ–=0.1throughout our experiments.
3840