Counterfactual Generative Models for Time-Varying Treatments
Shenghao Wu
shenghaw@andrew.cmu.edu
Carnegie Mellon University
Pittsburgh, PA, USAWenbin Zhou
wenbinz2@andrew.cmu.edu
Carnegie Mellon University
Pittsburgh, PA, USAMinshuo Chen
minshuochen@princeton.edu
Princeton University
Princeton, NJ, USAShixiang Zhu
shixiangzhu@cmu.edu
Carnegie Mellon University
Pittsburgh, PA, USA
ABSTRACT
Estimating the counterfactual outcome of treatment is essential for
decision-making in public health and clinical science, among others.
Often, treatments are administered in a sequential, time-varying
manner, leading to an exponentially increased number of possible
counterfactual outcomes. Furthermore, in modern applications, the
outcomes are high-dimensional and conventional average treat-
ment effect estimation fails to capture disparities in individuals.
To tackle these challenges, we propose a novel conditional gen-
erative framework capable of producing counterfactual samples
under time-varying treatment, without the need for explicit den-
sity estimation. Our method carefully addresses the distribution
mismatch between the observed and counterfactual distributions
via a loss function based on inverse probability re-weighting, and
supports integration with state-of-the-art conditional generative
models such as the guided diffusion and conditional variational au-
toencoder. We present a thorough evaluation of our method using
both synthetic and real-world data. Our results demonstrate that
our method is capable of generating high-quality counterfactual
samples and outperforms the state-of-the-art baselines.
CCS CONCEPTS
â€¢Applied computing â†’Decision analysis; â€¢Computing method-
ologiesâ†’Latent variable models.
KEYWORDS
Policy evaluation, longitudinal causal inference, inverse probability
of treatment weighting, marginal structural models, conditional
generative models, variational auto-encoders, diffusion models.
ACM Reference Format:
Shenghao Wu, Wenbin Zhou, Minshuo Chen, and Shixiang Zhu. 2024. Coun-
terfactual Generative Models for Time-Varying Treatments. In Proceedings
of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Min-
ing (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New York, NY,
USA, 12 pages. https://doi.org/10.1145/3637528.3671950
1 INTRODUCTION
Estimating the time-varying treatment effect from observational
data has garnered significant attention due to the growing preva-
lence of time-series records. One particular relevant field is public
health [ 8,35,77], where researchers and policymakers grapple with
a series of decisions on preemptive measures to control epidemic
This work is licensed under a Creative Commons Attribution
International 4.0 License.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0490-1/24/08.
https://doi.org/10.1145/3637528.3671950outbreaks, ranging from mask mandates to shutdowns. It is vital
to provide accurate and comprehensive outcome estimates under
such diverse time-varying treatments, so that policymakers and
researchers can accumulate sufficient knowledge and make well-
informed decisions with discretion.
In the literature, average treatment effect estimation has received
extensive attention and various methods have been proposed [ 4,6,
17,25,30,38,42,59,66,71]. By estimating the average outcome over
a population that receives a treatment or policy of interest, these
methods evaluate the effectiveness of the treatment via hypothesis
testing. However, solely relying on the average treatment effect
might not capture the full picture, as it may overlook pronounced
disparities in the individual outcomes of the population, especially
when the counterfactual distribution is heterogeneous (Figure 1,
left).
Recent efforts [ 32,33,43] have been made to directly estimate
the counterfactual density function of the outcome. This idea has
demonstrated appealing performance for univariate outcomes.
Nonetheless, for multi-dimensional outcomes, the estimation accu-
racy quickly degrades [ 65]. In modern high-dimensional applica-
tions, for example, predicting COVID-19 cases at the county level
of a state (Figure 1, middle), these methods are hardly scalable and
incur a computational overhead.
Adding another layer of complexity, considering time-varying
treatments causes the capacity of the potential treatment sequences
to expand exponentially. For example, even if the treatment is bi-
nary at a single time step, the total number of different combina-
tions on a time-varying treatment increases as 2ğ‘‘withğ‘‘being the
length of history. More importantly, time-varying treatments lead
to significant distributional discrepancy between the observed and
counterfactual outcomes (Figure 1, right).
In this paper, we provide a whole package of accurately estimat-
ing high-dimensional counterfactual distributions for time-varying
treatments. Instead of a direct density estimation, we implicitly
learn the counterfactual distribution by training a generative model,
capable of generating credible samples of the counterfactual out-
comes given a time-varying treatment. This allows policymakers
to assess a policyâ€™s efficacy by exploring a range of probable out-
comes and deepening their understanding of its counterfactual
result. Here, we summarize the benefits of our proposed method:
(1)Our model is capable of handling high-dimensional outcomes,
surpassing existing top-performing baselines in estimation
accuracy and counterfactual sample quality.
(2)Our modelâ€™s generative capability uncovers the multi-modality
of the high-dimensional counterfactual samples, such as iden-
tifying distinct disease outbreak hotspots across U.S. counties.
(3)Applying our model to real COVID-19 mask mandate data
shows that full mask mandates lead to significantly higher
3402
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shenghao Wu, Wenbin Zhou, Minshuo Chen, and Shixiang Zhu
* Heterogeneous effect* Distributional discrepancy between observed and counterfactual outcomesMean
ğ‘‘=1ğ‘‘=5ğ‘‘=10ObservedCounterfactual
Wasserstein dist: 0.28Wasserstein dist: 1.10Wasserstein dist: 1.51ğ‘Œ(+ğ‘)ğ‘“!"* High-dimensional outcomePennsylvania Covid-19 cases (67 counties) 
HighLowğ‘Œ1(+ğ‘)â€¦â€¦ğ‘Œ2(+ğ‘)ğ‘Œ3(+ğ‘)ğ‘Œ67(+ğ‘)
Figure 1: Challenges in estimating the counterfactual outcomes of time-varying treatments. Left: The mean is incapable of
describing the heterogeneous effect in counterfactual distributions. Middle : In a realistic scenario where the treatment is the
state-level mask mandate, the outcome is a 67-dimensional vector, corresponding to the number of COVID-19 cases of the 67
counties in Pennsylvania. Right : The longer the dependence on the treatment history, the greater the distributional mismatch
tends to be. Here ğ‘‘denotes the length of the history dependence.
variance than not having a mandate, highlighting our methodâ€™s
strong potential in policy-making uncertainty quantification.
To be specific, we develop a conditional generator [ 27,44,67].
This generator, which we choose in a flexible manner, takes into
account the treatment history as input and generates counterfactual
outcomes that align with the underlying distribution of counterfac-
tuals. The key idea behind the scenes is to utilize a â€œproxyâ€ condi-
tional distribution as an approximation of the true counterfactual
distribution. To achieve this, we establish a statistical relationship
between the observed and counterfactual distributions inspired by
the g-formula [ 15,46,54,61]. We learn the conditional generator
by optimizing a novel weighted loss function based on a pseudo
population through Inverse Probability of Treatment Weighting
(IPTW) [ 54] and incorporate the state-of-the-art conditional gener-
ative models such as the guided diffusion model and conditional
variational autoencoder. We evaluate our framework through nu-
merical experiments extensively on both synthetic and real-world
data sets.
1.1 Related work
Our work has connections to causal inference in time series, coun-
terfactual density estimation, and generative models. To our best
knowledge, our work is the first to intersect the three aforemen-
tioned areas. Below we review each of these areas independently.
Causal inference with time-varying treatments. Causal inference
has historically been related to longitudinal data. Classic approaches
to analyzing time-varying treatment effects include the g-computation
formula, structural nested models, and marginal structural models
[15,37,51,53,56,57,61]. These seminal works are typically based
on parametric models with limited flexibility. Recent advancements
in machine learning have significantly accelerated progress in this
area using flexible statistical models [ 10,64] and deep neural net-
works [4, 6, 17, 37, 38, 42, 66, 71] to capture the complex temporal
dependency of the outcome on treatment and covariate history.
These approaches, however, focus on predicting the mean coun-
terfactual outcome instead of the distribution. The performanceof these methods also heavily relies on the specific structures (e.g.,
long short term memory) without more flexible architectures.
Counterfactual distribution estimation. Recently, several studies
have emerged to estimate the entire counterfactual distribution
rather than the means, including estimating quantiles of the cumu-
lative distributional functions (CDFs) [ 11,72], re-weighted kernel
estimations [ 14], and semiparametric methods [ 32]. In particular,
[32] highlights the extra information afforded by estimating the
entire counterfactual distribution and using the distance between
counterfactual densities as a measure of causal effects. [ 43] uses
normalizing flow to estimate the interventional density. However,
these methods are designed to work under static settings with no
time-varying treatments [ 2], and are explicit density estimation
methods that may be difficult to scale to high-dimensional outcomes.
[37] proposes a deep framework based on G-computation which
can be used to simulate outcome trajectories on which one can
estimate the counterfactual distribution. However, this framework
approximates the distribution via empirical estimation of the sample
variance, which may be unable to capture the complex variability
of the (potentially high-dimensional) distributions. Our work, on
the other hand, approximates the counterfactual distribution with a
generative model without explicitly estimating its density. This will
enable a wider range of application scenarios including continuous
treatments and can accommodate more intricate data structures in
the high-dimensional outcome settings.
Counterfactual generative model. Generative models, including a
variety of deep network architectures such as generative adversarial
networks (GAN) and autoencoders, have been recently developed to
perform counterfactual prediction [ 3,19,21,28,36,39,40,50,62,63,
70,76,78]. However, many of these approaches primarily focus on
using representation learning to improve treatment effect estima-
tion rather than obtaining counterfactual samples or approximating
counterfactual distributions. For example, [ 62,76] adopt deep gen-
erative models to improve the estimation of individual treatment
effects (ITEs) under static settings. Some of these approaches focus
on exploring causal relationships between components of an image
[50,63,70]. Furthermore, there has been limited exploration of
3403Counterfactual Generative Models for Time-Varying Treatments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 1: Summary of key notations
Variable Domain Description
ğ‘¡{1,2,Â·Â·Â·,ğ‘‡} Time index
ğ‘‘ Z+Length of the history dependence
ğ‘§ Rğ‘ŸRandom noise vector
â„ Z+Covariate dimension
ğ‘š Z+Outcome dimension
ğ´ğ‘¡{0,1} Treatment at time ğ‘¡
ğ‘‹ğ‘¡ Râ„Covariate at time ğ‘¡
ğ‘Œğ‘¡ Rğ‘šOutcome at time ğ‘¡
ğ´ğ‘¡{0,1}ğ‘‘Treatment history at time ğ‘¡
ğ‘‹ğ‘¡ Rğ‘‘Ã—â„Covariate history at time ğ‘¡
ğ‘“(ğ‘¦,ğ‘,ğ‘¥) R+Joint density
ğ‘“ğ‘(ğ‘¦) R+Counterfactual density
ğ‘”ğœƒ(ğ‘§,ğ‘) Rğ‘šCounterfactual generator
ğ‘¤ğœ™(ğ‘,ğ‘¥) R+IPTW score
applying generative models to time series settings in the existing
literature. A few attempts, including [ 36,40], train autoencoders
to estimate treatment effect using longitudinal data. Nevertheless,
these methods are not intended for drawing counterfactual samples.
In sum, to the best of our knowledge, our work is the first to use
generative models to approximate counterfactual distribution from
data with time-varying treatments, a novel setting not addressed
by prior works.
2 METHODOLOGY
2.1 Problem setup
In this study, we consider the treatment for each discrete time
period (such as day or week) as a random variable ğ´ğ‘¡âˆˆ A =
{0,1}, whereğ‘¡=1,...,ğ‘‡ andğ‘‡is the total number of time points.
Note that our framework also works with categorical and even
continuous treatments. Let ğ‘‹ğ‘¡âˆˆ X âŠ‚ Râ„be the time-varying
covariates, and ğ‘Œğ‘¡âˆˆ Y âŠ‚ Rğ‘šthe subjectâ€™s outcome at time ğ‘¡.
We useğ´ğ‘¡={ğ´ğ‘¡âˆ’ğ‘‘+1,...,ğ´ğ‘¡}to denote the previous treatment
history from time ğ‘¡âˆ’ğ‘‘+1toğ‘¡, whereğ‘‘is the length of history
dependence. Similarly, we use ğ‘‹ğ‘¡={ğ‘‹ğ‘¡âˆ’ğ‘‘+1,...,ğ‘‹ğ‘¡}to denote the
covariate history. We use ğ‘¦ğ‘¡,ğ‘ğ‘¡, andğ‘¥ğ‘¡to represent a realization of
ğ‘Œğ‘¡,ğ´ğ‘¡, andğ‘‹ğ‘¡, respectively, and use ğ‘ğ‘¡=(ğ‘ğ‘¡âˆ’ğ‘‘+1,...,ğ‘ğ‘¡)andğ‘¥ğ‘¡=
(ğ‘¥ğ‘¡âˆ’ğ‘‘+1,...,ğ‘¥ğ‘¡)to denote the history of treatment and covariate
realizations. In the sections below, we will refer to ğ‘Œğ‘¡,ğ´ğ‘¡, andğ‘‹ğ‘¡
as simplyğ‘Œ,ğ´, andğ‘‹, whereğ‘¡will be clear from context. Since
the outcome is independent conditioning on its history, we can
consider the samples across time and individuals as conditionally
independent tuples ( ğ‘¦ğ‘–,ğ‘ğ‘–,ğ‘¥ğ‘–), whereğ‘–denotes the sample index
and the sample size is ğ‘. The notations are summarized in Table 1.
The goal of our study is to obtain realistic samples of the coun-
terfactual outcome for all given time-varying treatment ğ‘, without
estimating its counterfactual density. Let ğ‘Œ(ğ‘)denote the counter-
factual outcome for a subject under a time-varying treatment ğ‘,
and defineğ‘“ğ‘as its counterfactual distribution. We note that ğ‘“ğ‘is
different from the marginal density of ğ‘Œ, as the treatment is fixed
atğ´=ğ‘in the conditioning. It is also not equal to the unadjusted
ğ´!ğ´!"#ğ‘‹!ğ‘Œ!ğ‘‹!"#ğ‘‹!"$%#ğ´!"$%#â€¦â€¦Figure 2: The causal directed acyclic graph (DAG) of the time-
varying treatment.
conditional density ğ‘“(ğ‘¦|ğ‘). Instead,ğ‘“ğ‘is the density of the coun-
terfactual variable ğ‘Œ(ğ‘), which represents the outcome that would
have been observed if treatment were set to ğ´=ğ‘. We assume the
standard assumptions needed for identifying the treatment effects
[15, 29, 38, 64]:
(1)Consistency : Ifğ´ğ‘¡=ğ‘ğ‘¡for a given subject, then the counter-
factual outcome for treatment, ğ‘ğ‘¡, is the same as the observed
(factual) outcome: ğ‘Œ(ğ‘ğ‘¡)=ğ‘Œ.
(2)Positivity : IfP{ğ´ğ‘¡âˆ’1=ğ‘ğ‘¡âˆ’1,ğ‘‹ğ‘¡=ğ‘¥ğ‘¡}â‰ 0, then P{ğ´ğ‘¡=
ğ‘ğ‘¡|ğ´ğ‘¡âˆ’1=ğ‘ğ‘¡âˆ’1,ğ‘‹ğ‘¡=ğ‘¥ğ‘¡}>0for allğ‘ğ‘¡.
(3)Sequential strong ignorability :ğ‘Œ(ğ‘ğ‘¡)âŠ¥âŠ¥ğ´ğ‘¡|ğ´ğ‘¡âˆ’1=ğ‘ğ‘¡âˆ’1,ğ‘‹ğ‘¡=
ğ‘¥ğ‘¡, for allğ‘ğ‘¡andğ‘¡.
Assumption 2 means that, for each timestep, each treatment has a
non-zero probability of being assigned. Assumption 3 (also called
conditional exchangeability) means that there are no unmeasured
confounders, that is, all of the covariates affecting both the treat-
ment assignment and the outcomes are present in the the obser-
vational dataset. Note that while assumption 3 is standard across
all methods for estimating treatment effects, it is not testable in
practice [ 49,56]. We also assume that ğ‘Œ,ğ´, andğ‘‹follows the typi-
cally structural causal relationship as shown in Figure 2 , which is
a classical setting in longitudinal causal inference [51, 56].
2.2 Counterfactual generative framework for
time-varying treatments
This paper proposes a counterfactual generator, denoted as ğ‘”ğœƒ,
to simulate ğ‘Œ(ğ‘)according to the proxy conditional distribution
ğ‘“ğœƒ(ğ‘¦|ğ‘)instead of directly modeling its expectation or specifying
a parametric counterfactual distribution. Here we use ğœƒâˆˆÎ˜to
represent the modelâ€™s parameters, and formally define the generator
as a function:
ğ‘”ğœƒ(ğ‘§,ğ‘):Rğ‘ŸÃ—Ağ‘‘â†’Y. (1)
The generator takes as input a random noise vector ( ğ‘§âˆˆRğ‘Ÿâˆ¼
N(0,ğ¼)) and the time-varying treatment ğ‘. Note that our selection
of the generator ğ‘”is not constrained to a particular function. Instead,
it can represent an arbitrary generative process, such as a diffusion
model, provided it has the capability to produce a sample when
supplied with noise and treatment history. In addition, the noise
dimension, ğ‘Ÿ, depends on the specific generator: for conditional
variational autodencoders, it corresponds to the latent dimension,
whereas for the guided diffusion models, it has the same dimen-
sionality,ğ‘š, as the outcome.
3404KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shenghao Wu, Wenbin Zhou, Minshuo Chen, and Shixiang Zhu
ğ’ˆğœ½"ğ‘ğ‘¦âˆ¼ğ‘“"(â‹…|"ğ‘)ğ‘§Random noiseTime series treatmentCounterfactualoutcomeğğ“Observed populationğ‘¦("ğ‘,Ì…ğ‘¥)ğ‘“Pseudo populationğ‘¦("ğ‘,Ì…ğ‘¥)ğ‘“"(â‹…|"ğ‘)ğ‘“$%ğ‘“$%
Figure 3: The architecture of the proposed counterfactual gen-
erative models. The generator ğ‘”ğœƒis designed to produce sam-
ples of the outcome variable ğ‘Œ(ğ‘)with a given time-varying
treatmentğ‘. The generated samples are expected to conform
to the proxy conditional distribution ğ‘“ğœƒ, which is an approxi-
mate of the underlying counterfactual distribution ğ‘“ğ‘.
The output of the generator is a sample of possible counter-
factual outcomes that follows the proxy conditional distribution
represented by ğœƒ,i.e.,
ğ‘¦âˆ¼ğ‘“ğœƒ(Â·|ğ‘),
which can be viewed as an approximate of the underlying counter-
factual distribution ğ‘“ğ‘. Figure 3 shows an overview of the proposed
generative model architecture.
Marginal structural generative models. The learning objective is
to find the optimal generator that minimizes the distance between
the proxy conditional distribution ğ‘“ğœƒ(Â·|ğ‘)and the true counterfac-
tual distribution ğ‘“ğ‘for any treatment sequence, ğ‘, as illustrated in
Figure 4. For a general distributional distance, ğ·ğ‘“(Â·,Â·), the objec-
tive can be expressed as finding an optimal Ë†ğœƒthat minimizes the
difference between ğ‘“ğœƒ(Â·|ğ‘)andğ‘“ğ‘over all treatments ğ‘(i.e., over a
uniform distribution of ğ´):
Ë†ğœƒ=arg min
ğœƒâˆˆÎ˜E
ğ´
ğ·ğ‘“(ğ‘“ğ‘,ğ‘“ğœƒ(Â·|ğ‘))
. (2)
If the distance metric is Kullback-Leibler (KL) divergence, this objec-
tive can be expressed equivalently by maximizing the log-likelihood
[45]. The proof can be found in Appendix A:
Ë†ğœƒ=arg max
ğœƒâˆˆÎ˜E
ğ´"
E
ğ‘¦âˆ¼ğ‘“ğ‘logğ‘“ğœƒ(Â·|ğ‘)#
. (3)
To obtain samples from the counterfactual distribution ğ‘“ğ‘, we fol-
low the idea of marginal structural models (MSMs) introduced
by [46,54,61] and extended by [ 15] to account for time-varying
treatments. Specifically, we introduce Lemma 1, which follows the
g-formula proposed in [ 54] and establishes a connection between
the counterfactual distribution and the data distribution. The proof
can be found in Appendix B.
Lemma 1. Under unconfoundedness and positivity, we have
ğ‘“ğ‘(ğ‘¦)=âˆ«1Ãğ‘¡
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)ğ‘“(ğ‘¦,ğ‘,ğ‘¥)ğ‘‘ğ‘¥ (4)
ğ‘“ğ‘“!"ğœ”#ğ‘“$ğ‘“$âˆ—Î˜D!"(ğœƒ,ğœ™)Pseudo populationObserved distributionTrue counterfactual distributionProxy conditional distributionÎ¦Figure 4: An illustration of our learning objective. We aim to
minimize the KL-divergence between the proxy distribution
ğ‘“ğœƒ(Â·|ğ‘)and the true counterfactual distribution ğ‘“ğ‘.
whereğ‘“(ğ‘¦,ğ‘,ğ‘¥)denotes the joint distribution and ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)
denotes the propensity score at ğœ.
Now we present a proposition using Lemma 1, allowing us to
substitute the expectation in (3), computed over a counterfactual
distribution, with the sample average over a pseudo-population.
This pseudo-population is constructed by assigning weights to each
data tuple based on their subject-specific IPTW. Figure 4 gives an
illustration of the learning objective. See the proof in Appendix C.
Proposition 1. LetDdenote the set of observed data tuples. The
generative learning objective can be approximated by:
E
ğ´"
E
ğ‘¦âˆ¼ğ‘“ğ‘logğ‘“ğœƒ(ğ‘¦|ğ‘)#
â‰ˆ1
ğ‘âˆ‘ï¸
(ğ‘¦,ğ‘,ğ‘¥)âˆˆDğ‘¤ğœ™(ğ‘,ğ‘¥)logğ‘“ğœƒ(ğ‘¦|ğ‘),(5)
whereğ‘represents the sample size, and ğ‘¤ğœ™(ğ‘,ğ‘¥)denotes the subject-
specific IPTW, parameterized by ğœ™âˆˆÎ¦, which takes the form:
ğ‘¤ğœ™(ğ‘,ğ‘¥)=1Ãğ‘¡
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“ğœ™(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ). (6)
Remark 1. The generative learning objective in Proposition 1 offers
specific benefits when compared to plugin methods using Lemma 1
[7,33] and density estimators [ 43]. We only used IPTW in the training
of generative models, instead of combining an outcome model with
IPTW like the widely used doubly robust method, due to the challenges
of learning an outcome model for high-dimensional counterfactual
distributions. We include a detailed discussion in Appendix D.
Our Proposition 1 intuitively posits that to effectively approxi-
mate the counterfactual distribution using a generative model, one
can adjust the weighting of samples based on their IPTW during
the modelâ€™s training phase. This reweighting process debiases the
observed outcome distribution to match its counterfactual distri-
bution, thereby enhancing the modelâ€™s ability to generate accurate
counterfactual distributions. Here we use another model, denoted
byğœ™âˆˆÎ¦, to represent the propensity score ğ‘“ğœ™(ğ‘ğœ|ğ‘ğœ,ğ‘¥ğœ), which
defines the IPTW ğ‘¤ğœ™and can be learned separately using observed
data. Note that the effectiveness of this method is dependent on a
correct specification of the IPTW ğœ™,i.e., the black dot is inside of the
blue area in Figure 4 [ 15,38]. In [ 38], they use an RNN-like structure
to represent the conditional probability ğ‘“ğœ™without making strong
3405Counterfactual Generative Models for Time-Varying Treatments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Algorithm 1 Learning algorithm for the conditional generator ğœƒ
Input: Training setDdata tuples:D={(ğ‘¦ğ‘–,ğ‘ğ‘–,ğ‘¥ğ‘–)}ğ‘–=1,...,ğ‘
whereğ‘is the total number of training samples; the number of
learning epoch ğ¸.
Initialization: model parameters ğœƒand fitted bğœ™usingD.
whileğ‘’<ğ¸do
foreach sampled batch Bwith sizeğ‘›do
1. Draw samples ğœ–âˆ¼N( 0,ğ¼)from noise distribution, whose
dimensionality is either the outcome dimension (for the
guided diffusion) or the latent dimension (for CVAE); ğ¼is
the identity matrix;
2. Compute the loss for (ğ‘¦,ğ‘,ğ‘¥)âˆˆB givenğœ–andğœƒeither
according to (7) or (8);
3. Re-weight the loss for (ğ‘¦,ğ‘,ğ‘¥)âˆˆB usingğ‘¤bğœ™(ğ‘,ğ‘¥)accord-
ing to (6);
4. Updateğœƒby maximizing (5) with gradient descent.
end for
end while
returnğœƒ
assumptions on the form of the conditional probability. The choice
ofğ‘”ğœƒandğ‘“ğœ™are entirely general and both can be specified using
deep architectures. In this paper, we use fully-connected neural
networks for both ğ‘”ğœƒandğ‘“ğœ™.
To maximize the objective as expressed in (5) and learn the pro-
posed generative model, one needs to compute the conditional
log-likelihood, logğ‘“ğœƒ(Â·|ğ‘)for anyğ‘, which usually has no closed-
form. We can leverage various state-of-the-art generative learning
algorithms that approximate the likelihood. To demonstrate the
flexibility of our proposed marginal structural generative frame-
work, we explore two state-of-the-art models: the guided diffusion
models [ 13,27] and the conditional variational autoencoder (CVAE)
[67]. We summarize our learning procedure in Algorithm 1.
Classifier-free guided diffusion model. Diffusion models have been
commonly used to generate realistic samples in domains such as im-
ages [ 26,75]. Here we adopt the classifier-free guidance framework
[27] by predicting the noise conditioning on the treatment:
logğ‘“ğœƒ(Â·|ğ‘)â‰¥âˆ’Eğ‘ âˆ¼[1,ğ‘†],ğ‘¦âˆ¼ğ‘“(ğ‘¦|ğ‘),ğœ–ğ‘ ||ğœ–ğ‘ âˆ’ğœ–ğœƒ(âˆšï¸ƒ
Â¯ğœ†ğ‘ ğ‘¦+âˆšï¸ƒ
1âˆ’Â¯ğœ†ğ‘ ğœ–ğ‘ ,ğ‘ ,ğ‘)||2,
(7)
whereğ‘ denotes the denoising step, ğ‘†is the total number of steps,
andğœ–ğ‘ is the Gaussian noise at the ğ‘ -th step. Here Â¯ğœ†ğ‘ =Ãğ‘†
ğ‘ =1(1âˆ’ğ›¾),
whereğ›¾is the noise variance at ğ‘ andğœ–ğœƒis the score function
which is parameterized by a neural network to predict the noise
at stepğ‘ . Note that with the classifier-free guidance, ğœ–ğœƒ, is a linear
combination of conditional and unconditional score functions (see
Appendix E).
Conditional variational autoencoder. For the CVAE, we approxi-
mate the logarithm of the proxy conditional probability using its
evidence lower bound represented with an encoder-decoder form:
logğ‘“ğœƒ(Â·|ğ‘)â‰¥âˆ’ğ·KL(ğ‘(ğ‘§|ğ‘¦,ğ‘)||ğ‘ğœƒ(ğ‘§|ğ‘))+Eğ‘ğœƒ(ğ‘§|ğ‘¦,ğ‘)[logğ‘ğœƒ(ğ‘¦|ğ‘§,ğ‘)],
(8)
whereğ‘ğœƒ(ğ‘¦|ğ‘§,ğ‘)is our conditional generator, and ğ‘ğœƒ(ğ‘§|ğ‘¦,ğ‘)and
ğ‘ğœƒ(ğ‘§|ğ‘)are both parameterized by neural networks as encoder andprior. The complete derivation of (8) and implementation details
can be found in Appendix F.
3 EXPERIMENTS
We evaluate our method using numerical examples and demonstrate
the superior performance compared to the state-of-the-art methods.
These are (1) Kernel Density Estimation ( KDE) [60], (2) Marginal
structural model with a fully-connected neural network ( MSM+NN )
[38,55], (3) Conditional Variational Autoencoder ( CVAE ) [67], (4)
Semi-parametric Plug-in method based on pseudo-population
(Plugin+KDE ) [33], and (5) G-Net ( G-Net ) [37]. In the following, we
refer to our proposed conditional generator as marginal structural
conditional variational autoencoder ( MSCVAE ) and marginal struc-
tural diffusion ( MSDiffusion ) to show the flexibility of our gen-
erative framework. Given that the diffusion model with classifier-
free guidance primarily targets high-dimensional data, such as
images [ 27,58,75], we focus the evaluation of MSDiffusion on
semi-synthetic datasets with high outcome dimensionality (Sec-
tion 3.2). Additionally, we introduce an unweighted Diffusion
model [ 26] as our baseline for such comparisons. We also com-
pare to the Counterfactual Recurrent Network ( CRN)[5] on fully
synthetic data. Here, the G-Net is based on G-computation. The
Plugin+KDE is tailored for counterfactual density estimation. The
CVAE andDiffusion act as a reference model, highlighting the sig-
nificance of IPTW reweighting in our approach. See Appendix G.1
for a detailed review of these baseline methods.
Experiment set-up. To learn the model parameter ğœƒ, we use sto-
chastic gradient descent to maximize the weighted log-likelihood
(5). We adopt an Adam optimizer [ 34] with a batch size of 256, a
learning rate of 10âˆ’3forMSCVAE and a learning rate of 10âˆ’4for
MSDiffusion . To ensure learning stability, we follow a commonly-
used practice [38, 74] that involves truncating the subject-specific
IPTW weights at the 0.01-th and 99.99-th percentiles and normaliz-
ing them by their mean. All experiments are performed with 16GB
RAM and a 2.6 GHz 6-Core Intel Core i7 CPU. More details of the
experiment set-up can be found in Appendix G.2.
3.1 Fully synthetic data
We first assess the effectiveness of the MSCVAE using fully synthetic
experiments. The effectiveness of MSDiffusion is evaluated on
several semi-synthetic datasets in the next section for the afore-
mentioned reason. Following the classical experimental setting
described in [ 55], we simulate three synthetic datasets with differ-
ent lengths of history dependence ( ğ‘‘=1,3,5) using linear models.
Each dataset comprises 10,000trajectories, representing recorded
observations of individual subjects. These trajectories consist of
100data tuples, encompassing treatment, covariate, and outcome
values at specific time points. See Appendix G.4 for a detailed de-
scription of the synthetic data generation. The causal dependence
between these variables is visualized in Figure 2. In Figure 5, the
MSCVAE (orange shading) outperforms the baseline methods in accu-
rately capturing the shape of the true counterfactual distributions
(represented by the black line) across all scenarios. It is worth men-
tioning that the learned distribution produced by CVAE deviates
significantly from the desired target, emphasizing the significance
3406KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shenghao Wu, Wenbin Zhou, Minshuo Chen, and Shixiang Zhu
Figure 5: The estimated and true counterfactual distributions across various lengths of history dependence ( ğ‘‘=1,3,5) on the
fully synthetic datasets ( ğ‘š=1). Each sub-panel provides a comparison for a specific treatment combination ğ‘.
of the weighting term in Proposition 1 in accurately approximating
the counterfactual distribution.
Table 2 summarizes the quantitative comparisons across the
baselines. For the fully synthetic datasets, we adopt two metrics:
mean distance and 1-Wasserstein distance [ 18,48], as commonly-
used metrics to measure the discrepancies between the approxi-
mated and counterfactual distributions (see Appendix G.3 for more
details). The MSCVAE not only consistently achieves the smallest
Wasserstein distance in the majority of the experimental settings,but also demonstrates highly competitive accuracy on mean estima-
tion, which is consistent with the result in Figure 5. Note that even
though our goal is not to explicitly estimate the counterfactual dis-
tribution, the results clearly demonstrate that our generative model
can still accurately approximate the underlying counterfactual dis-
tribution, even compared to the unbiased density-based method
such as Plugin+KDE.
To further compare the performance of the algorithms under
extended application scenarios, we looked at two cases: when the
3407Counterfactual Generative Models for Time-Varying Treatments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Table 2: Quantitative performance on fully-synthetic data
ğ‘‘=1 ğ‘‘=3 ğ‘‘=5
Methods Meanâ†“ Wassersteinâ†“ Meanâ†“ Wassersteinâ†“ Meanâ†“ Wassersteinâ†“
MSM+NN 0.001 (0.002) 0.601(0.603) 0.070(0.159)0.689(0.718) 0.198(0.563) 0.600(0.737)
KDE 0.246(0.267) 0.244(0.268) 0.520(1.080) 0.538(1.080) 0.538(1.419) 0.539(1.419)
Plugin+KDE 0.010(0.014) 0.034 (0.036) 0.045 (0.168) 0.132(0.168) 0.147 (0.598) 0.182(0.598)
CRN 0.228(0.280) 0.289(0.331) 0.913(1.753) 1.014(1.757) 1.713(4.080) 1.775(4.080)
G-Net 0.211(0.258) 0.572(0.582) 1.167(2.173) 1.284(2.173) 2.314(5.263) 2.354(5.263)
CVAE 0.250(0.287) 0.253(0.288) 0.517(1.061) 0.553(1.061) 0.539(1.430) 0.613(1.430)
MSCVAE 0.006(0.006)0.055(0.056)0.046(0.150) 0.105 (0.216)0.150(0.633)0.173 (0.633)
MSDiffusion 0.029(0.052) 0.056(0.065) 0.086(0.234) 0.135(0.234) 0.207(0.845) 0.259(0.845)
* Numbers represent the average metric across all treatment combinations and those in the parentheses represent
theworst across treatment combinations. Bold and underlined numbers represent the best and second best results.
Table 3: Quantitative performance on fully-synthetic data
with imbalanced treatment and conditional outcome
Imbalance
d Conditional
ğ‘‘=5
Methods Meanâ†“ Wassersteinâ†“ Meanâ†“ Wassersteinâ†“
MSM+NN 0.173(0.448) 0.502(0.613) 0.164 (0.441) 0.368(0.517)
KDE 0.518(1.504) 0.520(1.504) 0.562(1.590) 0.564(1.590)
Plugin+KDE 0.157 (0.863) 0.211(0.863) 0.170(0.823) 0.196(0.823)
G-Net 2.070(4.794) 2.072(4.794) 0.815(2.238) 0.843(2.238)
CVAE 0.521(1.540) 0.565(1.540) 0.534(1.478) 0.585(1.478)
MSCVAE 0.162(0.832)0.187 (0.832)0.169(0.767)0.186 (0.767)
Results
forğ‘‘=1and3as well as the visualizations are included in Appendix H.
dataset has imbalanced proportions of different treatment combina-
tions and when there is a static baseline covariate for conditional
counterfactual outcome generation. The MSCVAE consistently out-
performed other baselines, as shown in Appendix H.
3.2 Semi-synthetic data
To demonstrate the ability of our generative framework to gen-
erate credible high-dimensional counterfactual samples, we test
both MSCVAE andMSDiffusion on two semi-synthetic datasets. The
benefit of these datasets is that both factual and counterfactual out-
comes are available. Therefore, we can obtain a sample from the
ground-truth counterfactual distribution, which we can then use
for benchmarking. We evaluate the performance by measuring the
quality of generated samples and the true samples from the dataset.
Time-varying MNIST. We create TV-MNIST, a semi-synthetic
dataset using MNIST images [ 12,31] as the outcome variable ( ğ‘š=
784). In this dataset, images are randomly selected, driven by the
result of a latent process defined by a linear autoregressive model,
which takes a 1-dimensional covariate and treatment variable as
inputs and outputs a digit (between 0and9). Here we set the length
of history dependence, ğ‘‘, to3. This setup allows us to evaluate the
performance of the algorithms by visually contrasting the quality
and distribution of generated samples against counterfactual ones.
The full description of the dataset can be found in Appendix G.5.Table 4: Quantitative performance on semi-synthetic data
CO
VID-19 TV-MNIST
ğ‘š=67ğ‘š=784
Methods FID*â†“ FID*â†“
MSM+NN 1.520(2.434) 1.236(3.956)
KDE 1.689(3.067) 1.509(2.557)
Plugin+KDE 1.474(1.584) 1.370(1.799)
G-Net 1.591(2.804) 1.751(6.096)
CVAE 0.916(4.092) 2.149(5.484)
Diffusion 0.703
(2.971) 1.138 (2.891)
MSCVAE 0.462 (0.838) 0.270 (1.004)
MSDiffusion 0.648 (0.918 )
0.734 (1.005 )
Pennsylvania COVID-19 mask mandate. We create another semi-
synthetic dataset to investigate the effectiveness of mask mandates
in Pennsylvania during the COVID-19 pandemic. We collected data
from multiple sources, including the Centers for Disease Control
and Prevention (CDC), the US Census Bureau, and a Facebook
survey [ 9,16,20,22,79,80]. The dataset encompasses variables ag-
gregated on a weekly basis spanning 106weeks from 2020 and2022.
There are four state-level covariates (per 100K people): the number
of deaths, the average retail and recreation mobility, the surveyed
COVID-19 symptoms, and the number of administered COVID-19
vaccine doses. We set the state-level mask mandate policy (with
values of 0indicating no mandate and 1indicating a mandate) as the
treatment variable, and the county-level number of new COVID-19
cases (per 100K) as the outcome variable ( ğ‘š=67). We simulate
2,000trajectories of the (covariate, treatment) tuples of 300time
points (each point corresponding to a week) according to the real
data. The outcome model is structured to exhibit a peak, defined
as the "hotspot", in one of the stateâ€™s two major cities: Pittsburgh
or Philadelphia. The likelihood of these hotspots is contingent on
the covariates. Consequently, the counterfactual and observed dis-
tributions manifest as bimodal, with varying probabilities for the
hotspot locations. To ensure a pertinent analysis window, weâ€™ve
fixed the history dependence length, ğ‘‘, at3, aligning with the typi-
cal duration within which most COVID-19 symptoms recede [ 41].
The full description of the dataset can be found in Appendix G.6.
3408KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shenghao Wu, Wenbin Zhou, Minshuo Chen, and Shixiang Zhu
True
CVAE
Diffusion
G-Net
Plugin+KDE
KDE
MSM+NN
MSCVAE
MSDiffusion
Figure 6: Results on the semi-sythetic TV-MNIST datasets ( ğ‘š=784). We show representative samples generated from different
methods under the treatment combinations ğ‘=(1,1,1).
Pittsburgh PhiladelphiaHotspot 
density
High
low
True
Diffusion KDE MSM+NN CVAEMSCVAE MSDiffusion Plugin+KDE G-Net
Figure 7: Results on the semi-synthetic Pennsylvania COVID-19 mask datasets ( ğ‘š=67) under the treatment combination
ğ‘=(1,1,1). We visualize the distribution of â€œhotspotsâ€ from the generated and true counterfactual distribution. For each model,
we generate 500counterfactual samples. Each sample is a 67-dimensional vector representing the inferred new cases per 100K
for the counties in Pennsylvania. We define the hotspot of each sample as the coordinate of the county with the highest number
of new cases per 100K, and visualize the density of the 500hotspots using kernel density estimation.
Given the high-dimensional outcomes of both semi-synthetic
datasets, straightforward comparisons using means or the Wasser-
stein distance of the distributions tend to be less insightful. As a
result, we use FID* (FrÃ©chet inception distance *), an adaptation of
the commonly-used FID [ 24] to evaluate the quality of the counter-
factual samples. For the TV-MNIST dataset, we utilize a pre-trained
MNIST classifier, and for the COVID-19 dataset, a geographical pro-
jector, to map the samples into a feature space. Subsequently, we
calculate the Wasserstein distance between the projected samples
and counterfactual samples. The details can be found in Appendix
G.3.
From Figure 6, 7, and Table 4, we observe:
(1)Both the MSCVAE andMSDiffusion outperform other baselines
in generating samples that closely resemble the ground truth
with overwhelmingly better FID* scores.
(2)Samples produced by the Plugin+KDE appear blurred in Fig-
ure 6 and exhibit noise in Figure 7. This can be attributed to theinherent complexities of high-dimensional density estimation
[65], underscoring the value of employing a generative model
to craft high-dimensional samples without resorting to precise
density estimation.
(3)The superior results of MSCVAE (vs.CVAE ),MSDiffusion (vs.
Diffusion ), and Plugin+KDE (vs.KDE) emphasize the pivotal
role of IPTW correction during modeling.
(4)Deterministic approaches like MSM+NN fall short in capturing
key features of the counterfactual distribution.
In sum, the semi-synthetic experiments highlights the distinct
benefits of our generative framework, particularly in generating
high-quality counterfactual samples under time-varying treatments
in a high-dimensional causal context.
3409Counterfactual Generative Models for Time-Varying Treatments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
0 101102103104DensityObserved
0 101102103104Counterfactual
Number of cases per 100Ka= (0,0,0)
a= (1,1,1)
Figure 8: Observed distribution and estimated counterfactual
distribution of the number of real COVID-19 cases per 100K
under two mask policies. The vertical dashed lines represent
the mean of the corresponding distributions.
3.3 Real data
We perform a case study using a real COVID-19 mask mandate
dataset across the U.S. from 2020 to2021 spanning 49weeks. We
analyze the same set of variables as the semi-synthetic COVID-
19 dataset except that: (1) We exclude the vaccine dosage from
the covariates due to missing data in some states. (2) All variables
in this dataset, including the treatments, the covariates, and the
outcomes, are real data without simulation. Due to the limitation
on the sample size for state-level observations, we only look at the
county-level data, covering 3,219U.S. counties. This leads to ğ‘š=1
and we utilize MSCVAE as our counterfactual generator. The details
can be found in Appendix G.6.
Figure 8 illustrates a comparative analysis of the distribution of
the observed and generated outcome samples under two different
scenarios: one without a mask mandate ( ğ‘=(0,0,0)) and the other
with a full mask mandate ( ğ‘=(1,1,1)). In the left panel, we ob-
serve that the distributions under both policies appear remarkably
similar, suggesting that the mask mandate has a limited impact on
controlling the spread of the virus. This unexpected outcome chal-
lenges commonly held assumptions. In the right panel, we present
counterfactual distributions estimated using our method, revealing
a noticeable disparity between the mask mandate and no mask
mandate scenarios. The mean of the distribution for the mask man-
date is significantly lower than that of the no-mask mandate. These
findings indicate that implementing a mask mandate consistently
for three consecutive weeks can effectively reduce the number of
future new cases. It aligns with the understanding supported by
health expertsâ€™ suggestions and various studies [ 1,23,47,69,73] re-
garding the effectiveness of wearing masks. Finally, it is important
to note that the implementation of full mask mandates exhibits a
significantly higher variance compared to the absence of a mask
mandate. This implies that the impact of a mask mandate varies
across different data points, specifically counties in our study. This
insight highlights the need for policymakers to carefully assess the
unique characteristics of their respective regions when considering
the implementation of mask mandate policies. It is crucial for poli-
cymakers to understand that the effectiveness of a mask mandate
may yield negative outcomes in certain extreme cases. Therefore,when proposing and implementing such policies, a thorough ex-
amination of the specific circumstances is highly recommended to
avoid any unintended consequences.
4 CONCLUSIONS
We have introduced a powerful conditional generative framework
tailored to generate samples that mirror counterfactual distribu-
tions in scenarios where treatments vary over time. Our model
approximates the true counterfactual distribution by minimizing
the KL-divergence between the true distribution and a proxy con-
ditional distribution, approximated by generated samples. We have
showcased our frameworkâ€™s superior performance against state-of-
the-art methods in both fully-synthetic and real experiments.
Our proposed framework has great potential in generating in-
tricate high-dimensional counterfactual outcomes and can be en-
hanced by adopting cutting-edge generative models and their learn-
ing algorithms, such as diffusion models. Additionally, our genera-
tive approach can be easily adapted to scenarios with continuous
treatments, where the conditional generator enables extrapolation
between unseen treatments under continuity assumptions.
We also recognize potential caveats stemming from breaches in
statistical assumptions. In real-world scenarios, the conditional ex-
changeability condition might be compromised due to unobserved
confounders. Similarly, the positivity assumption could be at risk,
attributed to the escalating number of treatment combinations as
ğ‘‘increases. Hence, meticulous assessment of these assumptions
is imperative for a thorough and accurate statistical interpretation
when employing our framework.
5 ACKNOWLEDGEMENT
We would like to extend our sincere thanks to Liyan Xie for her
valuable insights during our discussions.
REFERENCES
[1]Dhaval Adjodah, Karthik Dinakar, Matteo Chinazzi, Samuel P Fraiberger, Alex
Pentland, Samantha Bates, Kyle Staller, Alessandro Vespignani, and Deepak L
Bhatt. 2021. Association between COVID-19 outcomes and mask mandates,
adherence, and attitudes. PLoS One 16, 6 (2021), e0252315.
[2]Ahmed M Alaa and Mihaela Van Der Schaar. 2017. Bayesian inference of indi-
vidualized treatment effects using multi-task gaussian processes. Advances in
neural information processing systems 30 (2017).
[3]Vahid Balazadeh Meresht, Vasilis Syrgkanis, and Rahul G Krishnan. 2022. Partial
Identification of Treatment Effects with Implicit Generative Models. Advances in
Neural Information Processing Systems 35 (2022), 22816â€“22829.
[4]Jeroen Berrevoets, Alicia Curth, Ioana Bica, Eoin McKinney, and Mihaela van der
Schaar. 2021. Disentangled counterfactual recurrent networks for treatment
effect inference over time. arXiv preprint arXiv:2112.03811 (2021).
[5]Ioana Bica, Ahmed M Alaa, James Jordon, and Mihaela van der Schaar. 2019.
Estimating counterfactual treatment outcomes over time through adversarially
balanced representations. In International Conference on Learning Representations.
[6]Ioana Bica, Ahmed M Alaa, James Jordon, and Mihaela van der Schaar. 2020.
Estimating counterfactual treatment outcomes over time through adversarially
balanced representations. arXiv preprint arXiv:2002.04083 (2020).
[7]Peter J Bickel and Jaimyoung Kwon. 2001. Inference for semiparametric models:
some questions and an answer. Statistica Sinica (2001), 863â€“886.
[8]Matteo Bonvini, Edward Kennedy, Valerie Ventura, and Larry Wasserman. 2021.
Causal inference in the time of Covid-19. arXiv preprint arXiv:2103.04472 (2021).
[9]U.S. Census Bureau. 2022. State Population Totals: 2020-2022. https://www.cens
us.gov/data/tables/time-series/demo/popest/2020s-state-total.html. Accessed:
2022-09-15.
[10] Yehu Chen, Annamaria Prati, Jacob Montgomery, and Roman Garnett. 2023. A
Multi-Task Gaussian Process Model for Inferring Time-Varying Treatment Effects
in Panel Data. In International Conference on Artificial Intelligence and Statistics.
PMLR, 4068â€“4088.
3410KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shenghao Wu, Wenbin Zhou, Minshuo Chen, and Shixiang Zhu
[11] Victor Chernozhukov, IvÃ¡n FernÃ¡ndez-Val, and Blaise Melly. 2013. Inference on
counterfactual distributions. Econometrica 81, 6 (2013), 2205â€“2268.
[12] Li Deng. 2012. The mnist database of handwritten digit images for machine
learning research [best of the web]. IEEE signal processing magazine 29, 6 (2012),
141â€“142.
[13] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on
image synthesis. Advances in neural information processing systems 34 (2021),
8780â€“8794.
[14] John DiNardo, Nicole M. Fortin, and Thomas Lemieux. 1996. Labor Market Insti-
tutions and the Distribution of Wages, 1973-1992: A Semiparametric Approach.
Econometrica 64, 5 (1996), 1001â€“1044. http://www.jstor.org/stable/2171954
[15] Garrett Fitzmaurice, Marie Davidian, Geert Verbeke, and Geert Molenberghs.
2008. Longitudinal data analysis. CRC press.
[16] Centers for Disease Control. 2021. US state and territorial public mask mandates
from April 10, 2020 through August 15, 2021 by county by day. Policy Surveillance.
September 10 (2021).
[17] Dennis Frauen, Tobias Hatt, Valentyn Melnychuk, and Stefan Feuerriegel. 2023.
Estimating average causal effects from patient trajectories. In Proceedings of the
AAAI Conference on Artificial Intelligence. 7586â€“7594.
[18] Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya, and Tomaso A
Poggio. 2015. Learning with a Wasserstein loss. Advances in neural information
processing systems 28 (2015).
[19] Keisuke Fujii, Koh Takeuchi, Atsushi Kuribayashi, Naoya Takeishi, Yoshinobu
Kawahara, and Kazuya Takeda. 2022. Estimating counterfactual treatment out-
comes over time in complex multi-agent scenarios. arXiv preprint arXiv:2206.01900
(2022).
[20] Google. 2022. Community Mobility Reports. https://www.google.com/covid19
/mobility/. Accessed: 2022-09-15.
[21] Olivier Goudet, Diviyan Kalainathan, Philippe Caillou, Isabelle Guyon, David
Lopez-Paz, and MichÃ¨le Sebag. 2017. Causal generative neural networks. arXiv
preprint arXiv:1711.08936 (2017).
[22] CMU DELPHI Group. 2022. COVID-19 Symptom Surveys through Facebook.
https://delphi.cmu.edu/blog/2020/08/26/covid-19-symptom-surveys-through-
facebook/. Accessed: 2022-09-15.
[23] Gery P Guy Jr, Florence C Lee, Gregory Sunshine, Russell McCord, Mara Howard-
Williams, Lyudmyla Kompaniyets, Christopher Dunphy, Maxim Gakh, Regen
Weber, Erin Sauber-Schatz, et al .2021. Association of state-issued mask mandates
and allowing on-premises restaurant dining with county-level COVID-19 case
and death growth ratesâ€”United States, March 1â€“December 31, 2020. Morbidity
and Mortality Weekly Report 70, 10 (2021), 350.
[24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and
Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to
a local nash equilibrium. Advances in neural information processing systems 30
(2017).
[25] Keisuke Hirano, Guido W Imbens, and Geert Ridder. 2003. Efficient estimation
of average treatment effects using the estimated propensity score. Econometrica
71, 4 (2003), 1161â€“1189.
[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851.
[27] Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv
preprint arXiv:2207.12598 (2022).
[28] Daniel Jiwoong Im, Kyunghyun Cho, and Narges Razavian. 2021. Causal effect
variational autoencoder with uniform treatment. arXiv preprint arXiv:2111.08656
(2021).
[29] Kosuke Imai and David A Van Dyk. 2004. Causal inference with general treatment
regimes: Generalizing the propensity score. J. Amer. Statist. Assoc. 99, 467 (2004),
854â€“866.
[30] Guido W Imbens. 2004. Nonparametric estimation of average treatment effects
under exogeneity: A review. Review of Economics and statistics 86, 1 (2004), 4â€“29.
[31] Andrew Jesson, SÃ¶ren Mindermann, Yarin Gal, and Uri Shalit. 2021. Quantifying
ignorance in individual-level causal-effect estimates under hidden confounding.
InInternational Conference on Machine Learning. PMLR, 4829â€“4838.
[32] E H Kennedy, S Balakrishnan, and L A Wasserman. 2023. Semiparametric coun-
terfactual density estimation. Biometrika (03 2023), asad017. https://doi.org/10
.1093/biomet/asad017 arXiv:https://academic.oup.com/biomet/advance-article-
pdf/doi/10.1093/biomet/asad017/50607309/asad017.pdf
[33] Kwangho Kim, Jisu Kim, and Edward H Kennedy. 2018. Causal effects based on
distributional distances. arXiv preprint arXiv:1806.02935 (2018).
[34] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[35] Samantha Kleinberg and George Hripcsak. 2011. A review of causal inference for
biomedical informatics. Journal of biomedical informatics 44, 6 (2011), 1102â€“1112.
[36] Milan Kuzmanovic, Tobias Hatt, and Stefan Feuerriegel. 2021. Deconfound-
ing Temporal Autoencoder: estimating treatment effects over time using noisy
proxies. In Machine Learning for Health. PMLR, 143â€“155.
[37] Rui Li, Stephanie Hu, Mingyu Lu, Yuria Utsumi, Prithwish Chakraborty, Daby M
Sow, Piyush Madan, Jun Li, Mohamed Ghalwash, Zach Shahn, et al .2021. G-net:
a recurrent network approach to g-computation for counterfactual predictionunder a dynamic treatment regime. In Machine Learning for Health. PMLR, 282â€“
299.
[38] Bryan Lim, Ahmed Alaa, and Mihaela van der Schaar. 2018. Forecasting Treatment
Responses Over Time Using Recurrent Marginal Structural Networks. In Advances
in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle,
K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates,
Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/56e6a93212e448
2d99c84a639d254b67-Paper.pdf
[39] Qiao Liu, Zhongren Chen, and Wing Hung Wong. 2022. CausalEGM: a general
causal inference framework by encoding generative modeling. arXiv preprint
arXiv:2212.05925 (2022).
[40] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and
Max Welling. 2017. Causal effect inference with deep latent-variable models.
Advances in neural information processing systems 30 (2017).
[41] Helena C Maltezou, Androula Pavli, and Athanasios Tsakris. 2021. Post-COVID
syndrome: an insight on its pathogenesis. Vaccines 9, 5 (2021), 497.
[42] Valentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. 2022. Causal trans-
former for estimating counterfactual outcomes. In International Conference on
Machine Learning. PMLR, 15293â€“15329.
[43] Valentyn Melnychuk, Dennis Frauen, and Stefan Feuerriegel. 2023. Normaliz-
ing flows for interventional density estimation. In International Conference on
Machine Learning. PMLR, 24361â€“24397.
[44] Mehdi Mirza and Simon Osindero. 2014. Conditional Generative Adversarial
Nets. http://arxiv.org/abs/1411.1784 cite arxiv:1411.1784.
[45] Kevin P Murphy. 2012. Machine learning: a probabilistic perspective. MIT press.
[46] Jersey Neyman. 1923. Sur les applications de la thÃ©orie des probabilitÃ©s aux
experiences agricoles: Essai des principes. Roczniki Nauk Rolniczych 10, 1 (1923),
1â€“51.
[47] My Nguyen. 2021. Mask mandates and COVID-19 related symptoms in the US.
ClinicoEconomics and Outcomes Research (2021), 757â€“766.
[48] Victor M Panaretos and Yoav Zemel. 2019. Statistical aspects of Wasserstein
distances. Annual review of statistics and its application 6 (2019), 405â€“431.
[49] Judea Pearl. 2009. Causal inference in statistics: An overview. (2009).
[50] Hadrien Reynaud, Athanasios Vlontzos, Mischa Dombrowski, CiarÃ¡n Gilligan Lee,
Arian Beqiri, Paul Leeson, and Bernhard Kainz. 2022. Dâ€™artagnan: Counter-
factual video generation. In Medical Image Computing and Computer Assisted
Interventionâ€“MICCAI 2022: 25th International Conference, Singapore, September
18â€“22, 2022, Proceedings, Part VIII. Springer, 599â€“609.
[51] James Robins. 1986. A new approach to causal inference in mortality studies
with a sustained exposure periodâ€”application to control of the healthy worker
survivor effect. Mathematical modelling 7, 9-12 (1986), 1393â€“1512.
[52] James Robins and Miguel Hernan. 2008. Estimation of the causal effects of
time-varying exposures. Chapman & Hall/CRC Handbooks of Modern Statistical
Methods (2008), 553â€“599.
[53] James M Robins. 1994. Correcting for non-compliance in randomized trials using
structural nested mean models. Communications in Statistics-Theory and methods
23, 8 (1994), 2379â€“2412.
[54] James M Robins. 1999. Association, causation, and marginal structural models.
Synthese 121, 1/2 (1999), 151â€“179.
[55] James M Robins, Sander Greenland, and Fu-Chang Hu. 1999. Estimation of the
causal effect of a time-varying exposure on the marginal mean of a repeated
binary outcome. J. Amer. Statist. Assoc. 94, 447 (1999), 687â€“700.
[56] James M Robins, Miguel Angel Hernan, and Babette Brumback. 2000. Marginal
structural models and causal inference in epidemiology. Epidemiology (2000),
550â€“560.
[57] James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. 1994. Estimation of
regression coefficients when some regressors are not always observed. Journal
of the American statistical Association 89, 427 (1994), 846â€“866.
[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn
Ommer. 2022. High-resolution image synthesis with latent diffusion models. In
Proceedings of the IEEE/CVF conference on computer vision and pattern recognition .
10684â€“10695.
[59] Paul R Rosenbaum and Donald B Rubin. 1983. The central role of the propensity
score in observational studies for causal effects. Biometrika 70, 1 (1983), 41â€“55.
[60] Murray Rosenblatt. 1956. Remarks on some nonparametric estimates of a density
function. The annals of mathematical statistics (1956), 832â€“837.
[61] Donald B Rubin. 1978. Bayesian inference for causal effects: The role of random-
ization. The Annals of statistics (1978), 34â€“58.
[62] Shiv Kumar Saini, Sunny Dhamnani, Akil Arif Ibrahim, and Prithviraj Chavan.
2019. Multiple treatment effect estimation using deep generative model with
task embedding. In The World Wide Web Conference. 1601â€“1611.
[63] Axel Sauer and Andreas Geiger. 2021. Counterfactual generative networks. arXiv
preprint arXiv:2101.06046 (2021).
[64] Peter Schulam and Suchi Saria. 2017. Reliable decision support using counterfac-
tual models. Advances in neural information processing systems 30 (2017).
[65] David W Scott and James R Thompson. 1983. Probability density estimation in
higher dimensions. In Computer Science and Statistics: Proceedings of the fifteenth
symposium on the interface, Vol. 528. North-Holland, Amsterdam, 173â€“179.
3411Counterfactual Generative Models for Time-Varying Treatments KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
[66] Nabeel Seedat, Fergus Imrie, Alexis Bellot, Zhaozhi Qian, and Mihaela van der
Schaar. 2022. Continuous-Time Modeling of Counterfactual Outcomes Using
Neural Controlled Differential Equations. arXiv preprint arXiv:2206.08311 (2022).
[67] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output
representation using deep conditional generative models. Advances in neural
information processing systems 28 (2015).
[68] The New York Times. 2021. Coronavirus (Covid-19) Data in the United States.
https://github.com/nytimes/covid-19-data. Accessed: 2022-09-15.
[69] Miriam E Van Dyke, Tia M Rogers, Eric Pevzner, Catherine L Satterwhite, Hina B
Shah, Wyatt J Beckman, Farah Ahmed, D Charles Hunt, and John Rule. 2020.
Trends in county-level COVID-19 incidence in counties with and without a mask
mandateâ€”Kansas, June 1â€“August 23, 2020. Morbidity and Mortality Weekly Report
69, 47 (2020), 1777.
[70] Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, and Oliver Cobb. 2021.
Conditional generative models for counterfactual explanations. arXiv preprint
arXiv:2101.10123 (2021).
[71] Toon Vanderschueren, Alicia Curth, Wouter Verbeke, and Mihaela van der Schaar.
2023. Accounting For Informative Sampling When Learning to Forecast Treat-
ment Outcomes Over Time. arXiv preprint arXiv:2306.04255 (2023).
[72] Lan Wang, Yu Zhou, Rui Song, and Ben Sherwood. 2018. Quantile-optimal
treatment regimes. J. Amer. Statist. Assoc. 113, 523 (2018), 1243â€“1254.
[73] Yuxin Wang, Zicheng Deng, and Donglu Shi. 2021. How effective is a mask in
preventing COVID-19 infection? Medical devices & sensors 4, 1 (2021), e10163.
[74] Yongling Xiao, Michal Abrahamowicz, and Erica EM Moodie. 2010. Accuracy of
conventional and marginal structural Cox model estimators: a simulation study.
The international journal of biostatistics 6, 2 (2010).
[75] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao,
Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2023. Diffusion models: A
comprehensive survey of methods and applications. Comput. Surveys 56, 4 (2023),
1â€“39.
[76] Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. 2018. GANITE: Esti-
mation of individualized treatment effects using generative adversarial nets. In
International conference on learning representations.
[77] Weijia Zhang, Thuc Duy Le, Lin Liu, Zhi-Hua Zhou, and Jiuyong Li. 2017. Mining
heterogeneous causal effects for personalized cancer treatment. Bioinformatics
33, 15 (2017), 2372â€“2378.
[78] YiFan Zhang, Hanlin Zhang, Zachary Chase Lipton, Li Erran Li, and Eric Xing.
2022. Exploring transformer backbones for heterogeneous treatment effect
estimation. In NeurIPS ML Safety Workshop.
[79] Shixiang Zhu, Alexander Bukharin, Liyan Xie, Mauricio Santillana, Shihao Yang,
and Yao Xie. 2021. High-Resolution Spatio-Temporal Model for County-Level
COVID-19 Activity in the U.S. ACM Trans. Manage. Inf. Syst. 12, 4, Article 33
(sep 2021), 20 pages. https://doi.org/10.1145/3468876
[80] Shixiang Zhu, Alexander Bukharin, Liyan Xie, Khurram Yamin, Shihao Yang,
Pinar Keskinocak, and Yao Xie. 2022. Early Detection of COVID-19 Hotspots
Using Spatio-Temporal Data. IEEE Journal of Selected Topics in Signal Processing
16, 2 (2022), 250â€“260. https://doi.org/10.1109/JSTSP.2022.3154972
A PROOF OF EQUATION 3
The proof can be found in Appendix A of the online version1.
B PROOF OF LEMMA 1
Given a probability distribution for (ğ‘Œ,ğ´,ğ‘‹)and a causal directed
acyclic graph (DAG) shown in Figure 2, we can factor ğ‘“(ğ‘¦,ğ‘,ğ‘¥)as:
ğ‘“(ğ‘¦,ğ‘,ğ‘¥)=ğ‘“(ğ‘¦|ğ‘,ğ‘¥)ğ‘¡Ã–
ğœ=ğ‘¡âˆ’ğ‘‘+1h
ğ‘“(ğ‘¥ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœâˆ’1)Â·ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)i
.
(9)
Using the definition of g-formula [54], we have:
ğ‘“ğ‘(ğ‘¦)=âˆ«
ğ‘“(ğ‘¦|ğ‘,ğ‘¥)Â·ğ‘¡Ã–
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“(ğ‘¥ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœâˆ’1)ğ‘‘ğ‘¥
=âˆ«
ğ‘“(ğ‘¦|ğ‘,ğ‘¥)Â·Ãğ‘¡
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)
Ãğ‘¡
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)
Â·ğ‘¡Ã–
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“(ğ‘¥ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœâˆ’1)ğ‘‘ğ‘¥
1https://arxiv.org/pdf/2305.15742(ğ‘–)=âˆ«1Ãğ‘¡
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)ğ‘“(ğ‘¦,ğ‘,ğ‘¥)ğ‘‘ğ‘¥,
where the equation (ğ‘–)holds due to equation 9.
C PROOF OF PROPOSITION 1
We recall our notations for densities: ğ‘“(ğ‘¦,ğ‘,ğ‘¥)denotes the den-
sity of the observed data, ğ‘“ğ‘denotes the counterfactual density
underğ‘, andğ‘“ğœƒ(Â·|ğ‘)denotes the conditional density represented
by our conditional generator. Note that these density notations
should be interpreted in a broad sense to unify discrete and contin-
uous random variables, meaning that when Â¯ğ‘is a discrete random
variable, we allow the density function to be Delta functions. For
example, when Â¯ğ´is distributed as P(Â¯ğ´=Â¯ğ‘1)=P(Â¯ğ´=Â¯ğ‘2)=0.5, its
corresponding density function is ğ‘(Â·)=0.5ğ›¿Â¯ğ‘1(Â·)+ 0.5ğ›¿Â¯ğ‘2(Â·).
We also recall that ğ‘¤(ğ‘,ğ‘¥)=1/Ãğ‘¡
ğœ=ğ‘¡âˆ’ğ‘‘+1ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)where
ğ‘“(ğ‘ğœ|ğ‘ğœâˆ’1,ğ‘¥ğœ)denotes the individual propensity score. Here ğ‘is
the collection of all the history treatment. Using Lemma 1, we have:
E
ğ´"
E
ğ‘¦âˆ¼ğ‘“ğ‘[logğ‘“ğœƒ(ğ‘¦|ğ‘)]#
=E
ğ´âˆ«
logğ‘“ğœƒ(ğ‘¦|ğ‘)ğ‘“ğ‘(ğ‘¦)ğ‘‘ğ‘¦
=E
ğ´âˆ«
logğ‘“ğœƒ(ğ‘¦|ğ‘)âˆ«
ğ‘¤(ğ‘,ğ‘¥)ğ‘“(ğ‘¦,ğ‘,ğ‘¥)ğ‘‘ğ‘¥ğ‘‘ğ‘¦
(by Lemma 1)
=E
ğ´âˆ« âˆ«
logğ‘“ğœƒ(ğ‘¦|ğ‘)ğ‘¤(ğ‘,ğ‘¥)ğ‘“(ğ‘¦,ğ‘,ğ‘¥)ğ‘‘ğ‘¥ğ‘‘ğ‘¦
=E
(ğ‘¦,ğ‘,ğ‘¥)âˆ¼ğ‘“ğ‘¤(ğ‘,ğ‘¥)logğ‘“ğœƒ(ğ‘¦|ğ‘) (ğ´is uniform)
â‰ˆ1
ğ‘âˆ‘ï¸
(ğ‘¦,ğ‘,ğ‘¥)âˆˆDğ‘¤ğœ™(ğ‘,ğ‘¥)logğ‘“ğœƒ(ğ‘¦|ğ‘),
whereğ‘represents the sample size, and ğ‘¤ğœ™(ğ‘,ğ‘¥)denotes the
learned subject-specific IPTW, parameterized by ğœ™âˆˆÎ¦, which
takes the form of equation (equation 6).
D CONNECTION TO RELATED METHODS
Our work is widely connected to a number of density estimation
methods. Detailed discussions can be found in Appendix D of the
online version.
E DERIVATION OF THE MSDIFFUSION
The derivation can be found in Appendix E of the online version.
F DERIVATION OF THE MSCVAE
The derivation can be found in Appendix F of the online version.
G ADDITIONAL EXPERIMENT DETAILS
G.1 Baselines
The details of each baseline method in the paper are included in
Appendix G.1 of the online version.
G.2 Experiment set-up
The details of the neural network architecture for the MSCVAE and
MSDiffusion are included in Appendix G.2 of the online version.
3412KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Shenghao Wu, Wenbin Zhou, Minshuo Chen, and Shixiang Zhu
G.3 Experiment metrics
To quantify the quality of the approximated counterfactual distri-
butions, we used the mean, 1-Wasserstein Distance, as well as FID*
as comparison metrics. The details of each metric are included in
Appendix G.3 of the online version.
G.4 Fully Synthetic data
We generate three synthetic datasets with varying levels of his-
torical dependence denoted as ğ‘‘. Each dataset consists of 10,000
trajectories, which represent recorded observations of individual
subjects. These trajectories comprise 100 data tuples, encompassing
treatment, covariate, and outcome values at specific time points.
The causal relationships between these variables are visually de-
picted in Figure 2. For each time trajectory of length ğ‘‡, the datasets
are generated based on the following equations:
ğ‘‹0âˆ¼uniform(0,1), (10)
ğ‘‹ğ‘¡=ğ›¾0+ğ‘¡âˆ’1âˆ‘ï¸
ğœ=ğ‘¡âˆ’ğ‘‘ğ›¾ğ‘¡âˆ’ğœğ´ğœ+ğ‘¡âˆ’1âˆ‘ï¸
ğœ=ğ‘¡âˆ’ğ‘‘ğ›¾ğ‘‘+ğ‘¡âˆ’ğœğ‘‹ğœ, (11)
P{ğ´ğ‘¡=1}=ğœ(ğ›½0+ğ‘¡âˆ’1âˆ‘ï¸
ğœ=ğ‘¡âˆ’ğ‘‘ğ›½ğ‘¡âˆ’ğœğ´ğœ+ğ‘¡âˆ‘ï¸
ğœ=ğ‘¡âˆ’ğ‘‘ğ›½ğ‘‘+ğ‘¡âˆ’ğœ+1ğ‘‹ğœ), (12)
ğ‘Œğ‘¡=ğ›¼0+ğ‘¡âˆ‘ï¸
ğœ=ğ‘¡âˆ’ğ‘‘ğ›¼ğ‘¡âˆ’ğœ+1ğ´ğœ+ğ‘¡âˆ‘ï¸
ğœ=ğ‘¡âˆ’ğ‘‘ğ›¼ğ‘‘+ğ‘¡âˆ’ğœ+2ğ‘‹ğœ+ğœ–,(13)
whereğœ–âˆ¼N( 0,0.05)is the observation noise and ğœ(Â·)is a Sigmoid
function. Note that here we count from ğ‘¡âˆ’ğ‘‘instead ofğ‘¡âˆ’ğ‘‘+1
to alignğ‘‘with the notation ğ¾in [52]. Once the synthetic data is
generated, we use them to obtain samples from the counterfactual
outcome distribution, ğ‘Œ(ğ‘), for any given treatment combination
ğ‘. This is achieved by iteratively fixing the treatment sequence in
the time series and generating the covariates and response vari-
ables according to equations (11) and (13) for each of the 10,000
trajectories. The specific coefficients and the detailed procedure for
obtaining a single counterfactual outcome sample is summarized
in Appendix G.4 of the online version.
G.5 Semi-synthetic time-varying MNIST data
The TV-MNIST dataset was generated using similar equations as
in the fully synthetic data, while the outcome becomes an image
from the MNIST dataset. The details of the data-generation process
are included in Appendix G.5 of the online version.G.6 COVID-19 data
Both the semi-synthetic Pennsylvania COVID-19 mask data and
the real nationwide COVID-19 mask data are based on the same set
of aggregated sources. The dataset comprises COVID-19-related
demographic statistics collected from 3,219counties across 56
states/affiliated regions of the United States. The data covers a
time period from 2020 to2022. We obtained the data from reputable
sources including the U.S. Census Bureau [ 9], the Center for Dis-
ease Control and Prevention [ 16], Google [ 20], the CMU DELPHI
groupâ€™s Facebook survey [ 22], and the New York Times [ 68]. To
capture a relevant time window for analysis, we set the history
dependence length ğ‘‘to3, as most COVID-19 symptoms tend to
subside within this timeframe [ 41]. the treatment variable ğ´is the
state-wise mask mandate indicator variable. A value of 0indicates
no mask mandate, while a value of 1indicates the enforcement of
a mask mandate. Notably, we observe a pattern in the data where
mask mandates are typically implemented simultaneously across
all counties within a state. This synchronization justifies the use
of the state-wise mask mandate count as the treatment variable.
As for the covariates ğ‘‹, we choose the county-wise incremental
death count, state-wise the average retail and recreation metric,
the state-wise symptom value, and the state-wise vaccine dosage.
Further details are included in Appendix G.6 of the online version.
H ADDITIONAL SYNTHETIC RESULTS
H.1 Imbalanced treatment combinations
We provide an additional fully-synthetic experiment while setting
ğ›½0=âˆ’2(as opposed to ğ›½0=âˆ’0.5,) where the treatment combina-
tions are imbalancedly distributed. The full results can be found in
Appendix H.1 of the online version.
H.2 Conditional counterfactual distribution
A related extension of our work might be to infer conditional coun-
terfactual outcomes (related to the conditional average treatment
effect, CATE). This corresponds to looking at the distribution of
the outcome under a specific subpopulation. The full results can be
found in Appendix H.2 of the online version.
I CODE AVAILABILITY
The implementation code is available on Github2.
2https://github.com/ShenghaoWu/Counterfactual-Generative-Models
3413