Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions
Haoming Li
Shanghai Jiao Tong University
Shanghai, China
wakkkka@sjtu.edu.cnYumou Liuâˆ—
The Chinese University of Hong
Kong, Shenzhen
Shenzhen, China
yumouliu@link.cuhk.edu.cnZhenzhe Zhengâ€ 
Shanghai Jiao Tong University
Shanghai, China
zhengzhenzhe@sjtu.edu.cn
Zhilin Zhang
Alibaba Group
Beijing, China
zhangzhilin.pt@alibaba-inc.comJian Xu
Alibaba Group
Beijing, China
xiyu.xj@alibaba-inc.comFan Wu
Shanghai Jiao Tong University
Shanghai, China
fwu@cs.sjtu.edu.cn
ABSTRACT
Online advertising platforms leverage a two-stage auction architec-
ture to deliver personalized ads to users with low latency. The first
stage efficiently selects a small subset of promising candidates out
of the complete pool of ads. In the second stage, an auction is con-
ducted within the subset to determine the winning ad for display,
using click-through-rate predictions from the second-stage machine
learning model. In this work, we investigate the online learning
process of the first-stage subset selection policy, while ensuring
game-theoretic properties in repeated two-stage ad auctions. Specif-
ically, we model the problem as designing a combinatorial bandit
mechanism with a general reward function, as well as additional
requirements of truthfulness and individual rationality (IR). We es-
tablish an Î©(ğ‘‡)regret lower bound for truthful bandit mechanisms,
which demonstrates the challenge of simultaneously achieving allo-
cation efficiency and truthfulness. To circumvent this impossibility
result, we introduce truthful ğ›¼âˆ’approximation oracles and evaluate
the bandit mechanism through ğ›¼âˆ’approximation regret. Two mech-
anisms are proposed, both of which are ex-post truthful and ex-post
IR. The first mechanism is an explore-then-commit mechanism with
regretğ‘‚(ğ‘‡2/3), and the second mechanism achieves an improved
ğ‘‚(logğ‘‡/Î”2
ğœ™)regret where Î”ğœ™is a distribution-dependent gap, but
requires additional assumptions on the oracles and information
about the strategic bidders.
CCS CONCEPTS
â€¢Theory of computation â†’Algorithmic game theory and
mechanism design; Online learning theory; â€¢Information
systemsâ†’Online advertising.
âˆ—Work was done during visiting Shanghai Jiao Tong University.
â€ Zhenzhe Zheng is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Â©2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0490-1/24/08
https://doi.org/10.1145/3637528.3671813KEYWORDS
Mechanism Design, Online Learning, Multi-Armed Bandit, Online
Advertising
ACM Reference Format:
Haoming Li, Yumou Liu, Zhenzhe Zheng, Zhilin Zhang, Jian Xu, and Fan
Wu. 2024. Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions.
InProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining (KDD â€™24), August 25â€“29, 2024, Barcelona, Spain. ACM, New
York, NY, USA, 11 pages. https://doi.org/10.1145/3637528.3671813
1 INTRODUCTION
Modern online advertising platforms usually serve a vast number
of advertisers, who participate in ad auctions to compete for ad
impressions. In order to select highly relevant ads for users and to
maximize social welfare, the platforms typically rank the ads by
ğ‘ğ‘–ğ‘ğ‘–, whereğ‘ğ‘–is the bid reported by advertiser ğ‘–, andğ‘ğ‘–is the click
through rate (CTR) predicted by machine learning models of the
platform. However, executing complex CTR prediction models [ 6,
33,34] for millions of candidate ads within a limited response time is
often infeasible due to the associated high inference cost. To be able
to deliver highly personalized ads to incoming users in real time, a
widely adopted approach is a two-stage structure [ 13,25,32], which
is also ubiquitous in large-scale online recommendation systems
[7,10,15,20,29]. The first stage focuses on efficiently generating a
subset of candidates that contains enough promising ads. To ensure
low latency, first-stage machine learning models are lightweight and
less accurate [ 21,26]. The selected subset then enters the second
stage, where a sophisticated CTR model provides accurate CTR
predictions. Using those predictions and the submitted bids, an ad
auction is conducted within the subset to determine the winning
ad for display, along with the corresponding payment.
Prior works on two-stage systems has primarily focused on the
performance of subset selection policy in the first stage [ 20,24,
25,32], i.e., how to efficiently select a promising subset of candi-
dates such that the ad allocation performance of the second stage is
guaranteed. Besides, as such two-stage procedures are repeatedly
executed upon sequential arrivals of users, online learning of CTR
in two-stage recommendation systems has also been considered
[16,31]. However, advertising systems differ from recommenda-
tion systems by the involvement of money transfer, and hence the
requirement of game-theoretic properties, e.g., truthfulness and
individual rationality (IR) of auction mechanisms.
 
1565
KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haoming Li et al.
In this work, we address the challenge of simultaneously incor-
porating online learning of the subset selection policy and game-
theoretic properties in repeated two-stage ad auctions. Concretely,
we consider a repeated auction with ğ‘›advertisers and ğ‘‡rounds,
where, in each round, at most ğ‘˜advertisers are selected to enter the
second stage. Advertisers report their bids before the first round
starts, to optimize their cumulative utility over ğ‘‡rounds. In the
first stage of round ğ‘¡, we select a subset of advertisers denoted
asğ¾ğ‘¡to enter the second stage. Then for each advertiser ğ‘–inside
ğ¾ğ‘¡, its second-stage CTR prediction ğ‘ğ‘–ğ‘¡, which we assume to be an
i.i.d. sample from a probability distribution ğ·ğ‘–, is observed. We fix
the second stage to be a second-price auction, a well-known truth-
ful mechanism prevalent in ad auctions. The second-price auction
only involves advertisers within ğ¾ğ‘¡, and determines the advertiser
with the highest ğ‘ğ‘–ğ‘¡ğ‘ğ‘–to be displayed. Our goal is to maximize the
cumulative social welfare without knowing the second-stage CTR
distributions ğ·1,Â·Â·Â·,ğ·ğ‘›beforehand, while preserving the truthful
and IR properties of the ğ‘‡-round mechanism.
If we ignore the strategic behaviours of advertisers, this prob-
lem is equivalent to a combinatorial bandit with a general reward
function [ 5], where advertisers are treated as base arms, and in
each round we choose a super arm ğ¾ğ‘¡subject to the cardinality con-
straint|ğ¾ğ‘¡|â‰¤ğ‘˜, and receive reward maxğ‘–âˆˆğ¾ğ‘¡{ğ‘ğ‘–ğ‘ğ‘–ğ‘¡}1. However, the
additional requirement of truthfulness makes our problem a non-
trivial extension of the original bandit problem. In fact, advertisers
could misreport their values to manipulate the outcome of both
stages in a round. The observed samples of one round will further
influence the bandit algorithmâ€™s behaviour in subsequent rounds.
To reveal the challenge of simultaneously ensuring performance
and truthfulness, we present in Proposition 1 the impossibility to
design a stochastically truthful (please refer to Definition 5) mecha-
nism even if the CTR distributions are known beforehand. Building
upon this result, we establish an Î©(ğ‘‡)regret lower bound on bandit
mechanisms that are stochastically truthful (Theorem 1).
To circumvent the impossibility result, we introduce truthful
approximation oracles, which are constant-factor approximation
algorithms for solving the offline optimization problem in a truth-
ful manner. These oracles allow us to preserve truthfulness at
the cost of sacrificing allocation efficiency. We use the notion of
ğ›¼âˆ’approximation regret to evaluate bandit algorithms which calls
anğ›¼âˆ’approximation oracle. We propose a bandit mechanism with
ğ‘‚(ğ‘‡2/3)regret and achieves ex-post truthfulness and ex-post IR.
The algorithm is based on a straightforward explore-then-commit
(ETC) strategy. The separation of exploration phase and exploita-
tion phase prevents strategic bidders from influencing the data
collection process, thereby ensuring truthfulness. Furthermore, we
discover that truthful approximation oracles often exhibit a typical
structure: scoring the arms by their CTR distributions, then select
the top-ğ‘˜arms according to the product of their bid and score.
By exploiting this structure and making additional assumptions
on prior knowledge of biddersâ€™ private values, we propose an al-
gorithm that achieves an improved regret bound of ğ‘‚(logğ‘‡/Î”2
ğœ™),
1This reward function is "general" in the sense that the expected value
Eğ‘ğ‘–ğ‘¡âˆ¼ğ·ğ‘–[maxğ‘–âˆˆğ¾ğ‘¡{ğ‘ğ‘–ğ‘ğ‘–ğ‘¡}]not depend only on the means of random variables , i.e.,
Eğ·ğ‘–[ğ‘ğ‘–ğ‘¡], but on the entire distributions of these variables.while preserving ex-post truthfulness and ex-post IR, where Î”ğœ™is
a distribution-dependent gap.
To summarize, our major contributions in this work include:
â€¢To the best of our knowledge, this is the first work that
jointly considers online subset selection and game-theoretic
properties in the setting of repeated two-stage auctions.
â€¢We demonstrate the difficulty of the problem by establishing
aÎ©(ğ‘‡)regret lower bound for truthful bandit mechanisms.
â€¢We introduce truthful ğ›¼âˆ’approximation oracles, which al-
low us to design two ex-post truthful and ex-post IR bandit
mechanisms: one has ğ‘‚(ğ‘‡2/3)ğ›¼-approximation regret, and
the other one enjoys a better ğ‘‚(logğ‘‡/Î”2
ğœ™)ğ›¼-approximation
regret but requires additional assumptions on both the oracle
and the bidders.
â€¢We validate the efficiency and truthfulness of our proposed
mechanisms through experiments on both synthetic and real-
world data, with results aligning well with our theoretical
claims.
2 MODEL AND PRELIMINARIES
We consider a repeated single-slot ad auction setting with ğ‘›ad-
vertisers2[ğ‘›]andğ‘‡rounds, where a two-stage auction is con-
ducted in each round.3The advertisers have their private values
v=(ğ‘£1,Â·Â·Â·,ğ‘£ğ‘›)which is unknown to the auctioneer. Before the
first round starts, all advertisers submit their bids b=(ğ‘1,Â·Â·Â·,ğ‘ğ‘›).
We assume that all values and bids are bounded in [0,ğ‘‰], where
ğ‘‰>0. The second-stage CTR prediction of each advertiser ğ‘–follows
distribution ğ·ğ‘–, whereğ·ğ‘–is a probability distribution over [0,1].
We useğ·=(ğ·1,Â·Â·Â·,ğ·ğ‘›)to denote the product distribution of
eachğ·ğ‘–. To characterize the two-stage ad auction in each round,
we first define second-price auction within a subset.
Definition 1 (Second-price auction within a subset). Given
ğ‘›advertisers, a subset ğ¾âŠ†[ğ‘›]with cardinality constraint |ğ¾|â‰¤ğ‘˜,
CTR{ğ‘ğ‘–}ğ‘–âˆˆğ¾, and a bid vector bâˆˆ[0,ğ‘‰]ğ‘›, a second-price auction
withinğ¾determines the following allocation ğ‘¥ğ‘–and payment ğ‘ğ‘–for
eachğ‘–âˆˆ[ğ‘›]:
ğ‘¥ğ‘–=(
1,ifğ‘–âˆˆğ¾andğ‘–=argmaxğ‘—âˆˆğ¾{ğ‘ğ‘—ğ‘ğ‘—}
0,otherwise,
ğ‘ğ‘–=(maxğ‘—âˆˆğ¾,ğ‘—â‰ ğ‘–ğ‘ğ‘—ğ‘ğ‘—
ğ‘ğ‘–,ifğ‘–âˆˆğ¾andğ‘–=argmaxğ‘—âˆˆğ¾{ğ‘ğ‘—ğ‘ğ‘—}
0,otherwise,
where ties are broken consistently. The output of the auction is an
allocation vector x=(ğ‘¥1,ğ‘¥2,Â·Â·Â·,ğ‘¥ğ‘›)and a payment vector p=
(ğ‘1,ğ‘2,Â·Â·Â·,ğ‘ğ‘›). Throughout the paper, we use x(b,{ğ‘ğ‘–}ğ‘–âˆˆğ¾,ğ¾)and
p(b,{ğ‘ğ‘–}ğ‘–âˆˆğ¾,ğ¾)to denote outcomes of second-price auctions4.
In each round ğ‘¡âˆˆ [ğ‘‡], a two-stage auction is conducted as
follows:
2We use the terms advertiser, bidder, and arm interchangeably.
3For simplicity, we assume that the time horizon ğ‘‡is known beforehand, but our
results can be extended to the case with unknown ğ‘‡using a standard "doubling trick"
[2, 19].
4Sometimes we might slightly abuse notations and write x(b,c,ğ¾)andp(b,c,ğ¾),
where c=(ğ‘1,Â·Â·Â·,ğ‘ğ‘›)is the full CTR vector, although xandponly depends on
{ğ‘ğ‘–}ğ‘–âˆˆğ¾.
 
1566Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
â€¢First stage. The auctioneer selects a subset of advertisers
ğ¾ğ‘¡with cardinality constraint |ğ¾ğ‘¡| â‰¤ğ‘˜based on the bid
vector band all observations from previous rounds. Only
the advertisers in ğ¾ğ‘¡enters the second stage.
â€¢Second stage. For each ğ‘–âˆˆğ¾ğ‘¡, the auctioneer observes
the second-stage CTR prediction ğ‘ğ‘–ğ‘¡, which is an i.i.d. sam-
ple from distribution ğ·ğ‘–. Then a second-price auction is
run withinğ¾ğ‘¡(see Definition 1), using the predicted CTRs
{ğ‘ğ‘–ğ‘¡}ğ‘–âˆˆğ¾ğ‘¡and the submitted bid vector b. The allocation to
advertisers in this round is xğ‘¡=x(b,{ğ‘ğ‘–ğ‘¡}ğ‘–âˆˆğ¾ğ‘¡,ğ¾ğ‘¡), with
payment pğ‘¡=p(b,{ğ‘ğ‘–ğ‘¡}ğ‘–âˆˆğ¾ğ‘¡,ğ¾ğ‘¡).
Our goal is to maximize the cumulative social welfare of ğ‘‡rounds.
The reward we obtain in each round ğ‘¡is defined as the social welfare
of the second-price auction in that round, i.e., ğ‘…ğ‘¡=maxğ‘–âˆˆğ¾ğ‘¡{ğ‘ğ‘–ğ‘ğ‘–ğ‘¡}.
Since all bids are in [0,ğ‘‰]and CTRs are in[0,1], the reward ğ‘…ğ‘¡is
in[0,ğ‘‰]. The reward depends on sampled CTR predictions, and we
define its expectation with respect to distribution ğ·asğ‘…ğ·(ğ¾ğ‘¡)=
Eğ·[ğ‘…ğ‘¡]. Thenğ‘…ğ·(ğ¾ğ‘¡)is a scalar that depends on ğ¾ğ‘¡,ğ·,andb,
serving as a measure of how good ğ¾ğ‘¡is, givenğ·andb.
We define the offline optimal reward, i.e. the maximum ex-
pected reward one can achieve if ğ·(and b) is known, as OPTğ·=
maxğ¾ğ‘…ğ·(ğ¾).
To maximize the cumulative social welfare, we should select
proper advertisers to enter the second stage in each round. More-
over, we can only observe the second-stage CTR predictions of
advertisers that enters the second stage. This sequential subset
selection procedure with partial feedback can be viewed as a com-
binatorial (semi-)bandit, where we treat advertisers as base arms
and subsets as super arms. Following the convention of bandit
algorithms, we define regret as the performance measure.
Definition 2. The regret of an algorithm is
ğ‘…ğ‘’ğ‘”(ğ‘‡)=ğ‘‡Â·OPTğ·âˆ’E"ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘…ğ·(ğ¾ğ‘¡)#
.
In our setting, a bandit algorithm also defines a ğ‘‡-round mecha-
nism5, where we consider the total allocation , X=(ğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘›)=Ãğ‘‡
ğ‘¡=1xğ‘¡, and the total payment, P=(ğ‘ƒ1,Â·Â·Â·,ğ‘ƒğ‘›)=Ãğ‘‡
ğ‘¡=1pğ‘¡. We
refer to those mechanisms as bandit mechanisms. When a fixed
bandit algorithm runs on a fixed instance (ğ·,b)for several times,
the resulting allocation Xand payment Pmay be different, because
the CTR predictions in each round are stochastic. We can consider
thisğ‘‡-round interaction process as first drawing CTR predictions
ğ‘ğ‘–ğ‘¡for allğ‘–âˆˆ[ğ‘›]andğ‘¡âˆˆ[ğ‘‡]from the distributions ğ·, and then
running the bandit algorithm on these fixed samples. Specifically,
ağ‘›Ã—ğ‘‡realization table Cwhose(ğ‘–,ğ‘¡)-th entry is the ğ‘ğ‘–ğ‘¡to reveal
when armğ‘–is played in the ğ‘¡-th round. When a table Cis fixed,
there is no stochasticity in the mechanism, so the total allocation
X(C,b)and payment P(C,b)of a mechanism are also fixed. This
allows us to consider the utility function of advertiser ğ‘–with respect
to any table C, which is a deterministic function.
5Since the second-stage auction mechanism is fixed to be second-price (See Defini-
tion 1), the allocation rule of a ğ‘‡-round mechanism is uniquely defined by a combina-
torial bandit algorithm which decides the subset selection ğ¾ğ‘¡in each round.Definition 3. Bidderğ‘–â€™s utility function ğ‘¢ğ‘–with respect to bid ğ‘ğ‘–
and other biddersâ€™ bids bâˆ’ğ‘–=(ğ‘1,Â·Â·Â·,ğ‘ğ‘–âˆ’1,ğ‘ğ‘–+1,Â·Â·Â·,ğ‘ğ‘›)is
ğ‘¢ğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–)=ğ‘£ğ‘–Â·ğ‘¥ğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–)âˆ’ğ‘ğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–).
We expect our mechanisms to be truthful, i.e., reporting the real
value is the utility-maximizing strategy for any bidder. We define
two notions of truthfulness. Ex-post truthfulness requires the mech-
anism to be truthful on any fixed realization table. Stochastically
truthfulness is a weaker notion, which only requires truthfulness
in expectation, i.e., the expected utility function is maximized by
truthful bidding.
Definition 4. A mechanism is ex-post truthful if for any ğ‘–,ğ‘£ğ‘–,ğ‘ğ‘–,
bâˆ’ğ‘–,C,
ğ‘¢ğ‘–(C,ğ‘£ğ‘–,bâˆ’ğ‘–)â‰¥ğ‘¢ğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–).
Definition 5. A mechanism is stochastically truthful if for any
ğ·,ğ‘–,ğ‘£ğ‘–,ğ‘ğ‘–,bâˆ’ğ‘–,
EC[ğ‘¢ğ‘–(C,ğ‘£ğ‘–,bâˆ’ğ‘–)]â‰¥EC[ğ‘¢ğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–)].
The mechanisms should also satisfy IR (Individual Rationality),
which ensures that advertisers have non-negative utility when
participating in the auction.
Definition 6. A mechanism is ex-post IR if for any ğ‘–,ğ‘£ğ‘–,ğ‘ğ‘–,bâˆ’ğ‘–,C,
ğ‘¢ğ‘–(C,ğ‘£ğ‘–,bâˆ’ğ‘–)â‰¥0.
3 IMPOSSIBILITY RESULT
In this section, we recognize the impossibility for any bandit mech-
anism to simultaneously achieve sublinear regret and stochastically
truthfulness, as shown in Theorem 1. This result reveals that in our
two-stage auction setting, it is challenging to design a mechanism
that enjoys both good performance and game-theoretic properties.
Theorem 1. There exists an instance set such that any stochasti-
cally truthful algorithm ğœ‹must incur Î©(ğ‘‡)regret.
The proof of Theorem 1 relies on Myersonâ€™s Lemma.
Lemma 1 (Myersonâ€™s Lemma [ 23]).A mechanism is stochastically
truthful if and only if any bidderâ€™s allocation EC[ğ‘‹ğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–)]is
monotone(i.e. non-decreasing) with respect to her bid ğ‘ğ‘–, and the
payment rule is given by an explicit formula.
Now we construct an offline problem instance such that the
social welfare-maximizing allocation is not monotone with respect
to oneâ€™s bid.
Proposition 1 (Optimal offline allocation is not mono-
tone). In the offline (full-information) setting, there exists ğ·=
(ğ·1,Â·Â·Â·,ğ·ğ‘›), and an advertiser ğ‘–, such that her expected optimal
allocation Erâˆ¼ğ·[ğ‘¥ğ‘–(r,b,ğ¾âˆ—)], whereğ¾âˆ—=argmaxğ¾ğ‘…ğ·(ğ¾,b)is the
optimal super arm, is not monotone.
Proof. We present a counterexample with ğ‘›=3andğ‘˜=2. The
CTR distributions ğ·=(ğ·1,ğ·2,ğ·3)are
ğ‘1=(
0.8 with prob. 0.5
0.001 with prob. 0.5,ğ‘2=(
0with prob. 0.7
1with prob. 0.3,ğ‘3=0.32.
Consider bids b=(1,1,1)andbâ€²=(1.5,1,1), where advertiser 1
raises her bid.
 
1567KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haoming Li et al.
Onğ·andb, on can easily compute that the optimal subset is ğ¾âˆ—=
{1,2}, and advertiser 1â€™s expected allocation E[ğ‘¥1]=Pr[ğ‘1ğ‘1>
ğ‘2ğ‘2]=0.7; while on ğ·andbâ€², the optimal subset shifts to ğ¾âˆ—â€²=
{1,3}and advertiser 1â€™s expected allocation decreases to E[ğ‘¥â€²
1]=
Pr[ğ‘1ğ‘â€²
1>ğ‘3ğ‘â€²
3]=0.5. â–¡
In the proof of Proposition 1, the increase of bidder 1â€™s bid leads
to a change of the optimal subset, which introduces a stronger
competitor (bidder 3) to bidder 1 during the auction within the
subset and finally causes the expected allocation of bidder 1 to
decrease. This example reveals a fundamental difference between
two-stage and one-stage auctions: in two-stage auctions, one bidder
may change her competitors in the second stage by changing her
bid.
Goel et al. [ 13] also proved an impossibility result by a construc-
tion similar to our Proposition 1. They proved that ex-post truth-
fulness is not achievable in two-stage auctions, while we present a
stronger result, i.e. even stochastically truthfulness is impossible.
To finally prove Theorem 1, we still need the following simple
lemma from the bandit literature.
Lemma 2. Letğœ‹be a combinatorial bandit algorithm, and I=
(ğ·,b)be an instance. Assume Ihas a unique optimal super arm
ğ¾âˆ—=argmaxğ¾ğ‘…ğ·(ğ¾,b). Letğœğ¾(ğ‘‡)=Ãğ‘‡
ğ‘¡=1I{ğ¾ğ‘¡=ğ¾}denote the
times ofğœ‹playingğ¾from round 1toğ‘‡. Ifğœ‹achieves sublinear regret
onI, then limğ‘‡â†’âˆE[ğœğ¾âˆ—(ğ‘‡)]/ğ‘‡=1.
Now we are ready to prove Theorem 1.
Proof of Theorem 1. We consider two instances I={ğ·,b}
andIâ€²={ğ·,bâ€²}, whereğ·,b,bâ€²are the constructions in the proof
of Proposition 1.
Since algorithm ğœ‹is stochastically truthful, by Lemma 1, bid-
der 1â€™s expected allocation must be monotone with respect to
ğ‘1. We use shorthand E[ğ‘‹1]forE[ğ‘‹1(C,ğ‘1,bâˆ’1)], andE[ğ‘‹â€²
1]for
E[ğ‘‹1(C,ğ‘â€²
1,bâ€²
âˆ’1)]. Letğœğ¾(ğ‘‡)=Ãğ‘‡
ğ‘¡=1I{ğ¾ğ‘¡=ğ¾}whenğœ‹is running
onI, andğœâ€²
ğ¾(ğ‘‡)be its counterpart on Iâ€².
We decompose the total allocation E[ğ‘‹1]to the times when
different super arms containing arm 1 are pulled.
E[ğ‘‹1]
=E
ğœ{1,2}(ğ‘‡)
Pr[ğ‘1ğ‘1>ğ‘2ğ‘2]+E
ğœ{1,3}(ğ‘‡)
Pr[ğ‘1ğ‘1>ğ‘3ğ‘3]
=0.7E
ğœ{1,2}(ğ‘‡)
+0.5E
ğœ{1,3}(ğ‘‡)
.
(1)
Similarly,
E[ğ‘‹â€²
1]=0.85Eh
ğœâ€²
{1,2}(ğ‘‡)i
+0.5Eh
ğœâ€²
{1,3}(ğ‘‡)i
. (2)
Ifğœ‹achieves sublinear regret on both IandIâ€², by applying
Lemma 2 toIandIâ€², we know that
lim
ğ‘‡â†’âˆE
ğœ{1,2}(ğ‘‡)
/ğ‘‡=1and lim
ğ‘‡â†’âˆEh
ğœâ€²
{1,3}(ğ‘‡)i
/ğ‘‡=1.(3)
Moreover,
lim
ğ‘‡â†’âˆE
ğœ{1,3}(ğ‘‡)
/ğ‘‡=0and lim
ğ‘‡â†’âˆEh
ğœâ€²
{1,2}(ğ‘‡)i
/ğ‘‡=0.(4)
Combining (3), (4) and (1), (2), we have
lim
ğ‘‡â†’âˆE[ğ‘‹1]/ğ‘‡=0.7and lim
ğ‘‡â†’âˆE[ğ‘‹â€²
1]/ğ‘‡=0.5,
which contradicts with E[ğ‘‹1]â‰¤E[ğ‘‹â€²
1]and finishes the proof. â–¡4 TRUTHFUL BANDIT MECHANISMS
In this section, we first introduce truthful approximation oracles,
which allows us to design truthful mechanisms at the cost of sacri-
ficing the performance. Then we present two mechanisms utilizing
truthful approximation oracles, both of which are ex-post truth-
ful and ex-post IR. The first mechanism achieves ğ‘‚(ğ‘‡2/3)regret.
The second mechanism requires additional assumptions on the or-
acle and the biddersâ€™ values, and achieves an improved regret of
ğ‘‚(logğ‘‡/Î”2
ğœ™), where Î”ğœ™is a distribution-dependent gap.
4.1 Truthful Approximation Oracles
Theorem 1 states the impossibly to design a truthful bandit mecha-
nism that approaches the offline optimal allocation. To overcome
the difficulty, we introduce approximation oracles that solves the
offline optimization problem in a truthful manner.
Definition 7 (Truthful Approximation Oracle). An ora-
cle takes distributions ğ·and bid vector bas input, and outputs a
subsetğ¾â†Oracle(ğ·,b),|ğ¾|â‰¤ğ‘˜. Forğ›¼âˆˆ(0,1), an oracle is an
ğ›¼âˆ’approximation if for any ğ·and any b
ğ‘…ğ·(Oracle(ğ·,b))â‰¥ğ›¼OPTğ·.
An oracle is ex-post truthful if for any ğ·,vâˆˆ[0,ğ‘‰]ğ‘›,râˆˆ[0,1]ğ‘›,
the following mechanism is truthful:
â€¢Solicit bid vector b.
â€¢Query the oracle for ğ¾â†Oracle(ğ·,b).
â€¢Run second-price auction within ğ¾. Output x(b,c,ğ¾)and
p(b,c,ğ¾).
Representation of Distributions. One may wonder how to rep-
resent distributions for the oracleâ€™s input. In fact, our proposed
algorithms only call the oracles on empirical distributions, which
are discrete distributions with finite support. We represent such
distributions ğ·={ğ·1,Â·Â·Â·,ğ·ğ‘›}by their CDFs F={ğ¹1,Â·Â·Â·,ğ¹ğ‘›}.
Eachğ¹ğ‘–is a piecewise constant function, which could be further
represented by a vector of supported points and the values of CDF
on those points. Throughout the paper, we may use ğ·andFinter-
changeably.
Note that we actually required truthful approximation oracles to
be ex-post truthful, rather than the weaker notion of stochastically
truthful. The following lemma provides a concrete example of such
an ex-post truthful approximation oracle.
Lemma 3 (Theorem 3 in Goel et al. [ 13]).For each distribution
ğ·ğ‘–ofğ‘ğ‘–, letğœ™ğ‘–(ğœƒ)be the expectation above the quantile function ğ‘ğ‘–(ğœƒ):
ğ‘ğ‘–(ğœƒ)=sup{ğ‘¥|Pr[ğ‘ğ‘–â‰¥ğ‘¥]â‰¥ğœƒ},
ğœ™ğ‘–(ğœƒ)=Eğ‘ğ‘–âˆ¼ğ·ğ‘–[ğ‘ğ‘–I[ğ‘ğ‘–â‰¥ğ‘ğ‘–(ğœƒ)]],
then the following oracle is a truthfulğ‘’âˆ’1
2ğ‘’-approximation oracle: Sort
all bidders by ğ‘ğ‘–ğœ™ğ‘–(1/ğ‘˜), and choose the top ğ‘˜bidders (ties are broken
consistently).
We leverage truthful approximation oracles in our design of
truthful online learning algorithms (mechanisms). In such cases,
it is not fair to compare our algorithm with the optimal algorithm
which always chooses the optimal super arm. Instead, we use the
ğ›¼âˆ’approximation regret to evaluate an algorithm.
 
1568Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Definition 8. Theğ›¼âˆ’approximation regret of an algorithm is
defined as
ğ‘…ğ‘’ğ‘”ğ›¼(ğ‘‡)=ğ›¼ğ‘‡Â·OPTğ·âˆ’E"ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘…ğ·(ğ¾ğ‘¡)#
.
4.2ğ‘‚(ğ‘‡2/3)Mechanism
We present a mechanism that achieves ğ‘‚(ğ‘‡2/3)gap-independent
regret and ex-post truthfulness. The mechanism consists of a fixed-
length exploration phase followed by an exploitation phase. In the
exploration phase, we collect ğ‘šsamples for each arm in a round-
robin manner, and calculate their empirical distributions in terms
of CDFs. With ğ‘ši.i.d. samples ğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘šfrom distribution ğ·, the
empirical CDF is defined as Ë†ğ¹(ğ‘¥)=1
ğ‘šÃğ‘š
ğ‘—=1I(ğ‘‹ğ‘—â‰¤ğ‘¥). Based on
these empirical CDFs, a truthful approximation oracle decides the
super arm that is repeatedly played in the exploitation phase.
Algorithm 1 An ETC mechanism
1:ğ‘šâ†(ğœ‹
2)1
3(1+ğ›¼)2
3ğ‘›2
3ğ‘‡2
3
2:forroundğ‘¡â‰¤ğ‘šğ‘˜do âŠ²Exploration Phase
3:ğ¾ğ‘¡â†{1+ğ‘˜ğ‘¡modğ‘›,2+ğ‘˜ğ‘¡modğ‘›,Â·Â·Â·,ğ‘˜+ğ‘˜ğ‘¡modğ‘›},
run second-price auction within ğ¾ğ‘¡
4: Update Ë†ğ¹ğ‘–for eachğ‘–âˆˆğ¾ğ‘¡
5:end for
6:Query the oracle, get ğ¾â†Oracle(Ë†F,b)
7:foreach remaining round ğ‘¡do âŠ²Exploitation Phase
8:ğ¾ğ‘¡â†ğ¾, run second-price auction within ğ¾ğ‘¡
9:end for
Theorem 2. Algorithm 1 achieves the following ğ›¼âˆ’approximation
regret upper bound:
ğ‘…ğ‘’ğ‘”ğ›¼(ğ‘‡)â‰¤3(1+ğ›¼)2
3ğ‘˜ğ‘‰ğ‘›2
3ğ‘‡2
3.
The proof of Theorem 2 relies on several useful lemmas.
Lemma 4 (Dvoretzky-Kiefer-Wolfowitz ineqality[ 9,22]).
Consider a distribution ğ·, and letğ¹(ğ‘¥)be its CDF. With ğ‘ši.i.d. sam-
plesğ‘‹1,Â·Â·Â·,ğ‘‹ğ‘šfromğ·, the empirical CDF is Ë†ğ¹(ğ‘¥)=1
ğ‘šÃğ‘š
ğ‘—=1I(ğ‘‹ğ‘—â‰¤
ğ‘¥), then for any ğœ–>0, we have
Pr[sup
ğ‘¥âˆˆğ‘…|ğ¹(ğ‘¥)âˆ’Ë†ğ¹(ğ‘¥)|â‰¥ğœ–]â‰¤2ğ‘’âˆ’2ğ‘šğœ–2
Lemma 5. For random variable ğ‘‹with non-negative support,
E[ğ‘‹]=âˆ«âˆ
0Pr[ğ‘‹>ğœ–]dğœ–.
Lemma 6 (Lemma 3 in Chen et al. [ 5]).If for anyğ‘–âˆˆ[ğ‘›],ğ‘¥âˆˆ
[0,1],supğ‘¥|ğ¹ğ‘–(ğ‘¥)âˆ’ğ¹â€²
ğ‘–(ğ‘¥)|â‰¤Î›, then for any super arm ğ¾, we have
|ğ‘…ğ¹(ğ¾)âˆ’ğ‘…ğ¹â€²(ğ¾)|â‰¤ 2ğ‘‰ğ‘˜Î›.
Lemma 4, i.e. the DKW inequality, is on concentration of empir-
ical distributions. Lemma 5 is a simple fact in probability theory,
bridging expectations and CDFs. Lemma 6 characterizes the con-
centration of super armsâ€™ rewards based on the concentration of
base armsâ€™ empirical distributions. Now we are ready to prove
Theorem 2.Proof. Letğ¾be the super arm played in the exploitation phase.
Let Ë†ğ·=(Ë†ğ·1,Â·Â·Â·,Ë†ğ·ğ‘›)be the empirical distributions at round
ğ‘šğ‘˜, with empirical CDFs Ë†F=(Ë†ğ¹1,Â·Â·Â·,Ë†ğ¹ğ‘›), and letğ·be the real
distributions. Since we call an ğ›¼âˆ’approximation oracle, we have
ğ‘…Ë†ğ·(ğ¾)â‰¥ğ›¼OPT Ë†ğ·. Letğ¾âˆ—=argmaxğ¾ğ‘…ğ·(ğ¾)be the real optimal
super arm. Then
ğ‘…Ë†ğ·(ğ¾)â‰¥ğ›¼OPT Ë†ğ·â‰¥ğ›¼ğ‘…Ë†ğ·(ğ¾âˆ—),
where the second inequality follows from the optimality of OPT Ë†ğ·,
i.e.,OPT Ë†ğ·â‰¥ğ‘…Ë†ğ·(ğ¾â€²)for anyğ¾â€². We then bound the probability
Pr[ğ‘…ğ·(ğ¾)<ğ›¼OPTğ·âˆ’ğœ–]for anyğœ–âˆˆR+.
Pr[ğ‘…ğ·(ğ¾)<ğ›¼OPTğ·âˆ’ğœ–]
â‰¤Pr[ğ‘…ğ·(ğ¾)<ğ›¼OPTğ·âˆ’ğœ–+(ğ‘…Ë†ğ·(ğ¾)âˆ’ğ›¼ğ‘…Ë†ğ·(ğ¾âˆ—))]
=Pr[(ğ‘…Ë†ğ·(ğ¾)âˆ’ğ‘…ğ·(ğ¾))âˆ’ğ›¼(ğ‘…Ë†ğ·(ğ¾âˆ—)âˆ’ğ‘…ğ·(ğ¾âˆ—))>ğœ–]
â‰¤Pr[|ğ‘…Ë†ğ·(ğ¾)âˆ’ğ‘…ğ·(ğ¾)|+ğ›¼|ğ‘…Ë†ğ·(ğ¾âˆ—)âˆ’ğ‘…ğ·(ğ¾âˆ—)|>ğœ–]
:=Pr[E],(5)
where the last inequality follows from |ğ‘âˆ’ğ‘|â‰¤|ğ‘|+|ğ‘|for any
ğ‘,ğ‘âˆˆR, and in the last line we define event E={|ğ‘…Ë†ğ·(ğ¾)âˆ’ğ‘…ğ·(ğ¾)|+
ğ›¼|ğ‘…Ë†ğ·(ğ¾âˆ—)âˆ’ğ‘…ğ·(ğ¾âˆ—)|>ğœ–}.
Also, define good event
G=(
âˆ€ğ‘–âˆˆ[ğ‘›],sup
ğ‘¥âˆˆ[0,1]|ğ¹ğ‘–(ğ‘¥)âˆ’Ë†ğ¹ğ‘–(ğ‘¥)|â‰¤ğœ–
2ğ‘‰ğ‘˜(1+ğ›¼))
.
By Lemma 6, ifGhappens, then we have |ğ‘…Ë†ğ·(ğ¾)âˆ’ğ‘…ğ·(ğ¾)|â‰¤
ğœ–
1+ğ›¼and|ğ‘…Ë†ğ·(ğ¾âˆ—)âˆ’ğ‘…ğ·(ğ¾âˆ—)|â‰¤ğœ–
1+ğ›¼, which preventsEto happen.
Therefore,EimpliesÂ¬G, which gives us
Pr[E]
â‰¤Pr[Â¬G]
=Pr"
âˆƒğ‘–âˆˆ[ğ‘›],sup
ğ‘¥âˆˆ[0,1]|ğ¹ğ‘–(ğ‘¥)âˆ’Ë†ğ¹ğ‘–(ğ‘¥)|>ğœ–
2ğ‘‰ğ‘˜(1+ğ›¼)#
â‰¤ğ‘›âˆ‘ï¸
ğ‘–=1Pr"
sup
ğ‘¥âˆˆ[0,1]|ğ¹ğ‘–(ğ‘¥)âˆ’Ë†ğ¹ğ‘–(ğ‘¥)|>ğœ–
2ğ‘‰ğ‘˜(1+ğ›¼)#
â‰¤2ğ‘›exp
âˆ’ğ‘šğœ–2
2ğ‘‰2ğ‘˜2(1+ğ›¼)2
,(6)
where the second inequality is by taking a union bound, and the
third inequality follows from DKW inequality.
Combining (5) and (6) gives us
Pr[ğ‘…ğ·(ğ¾)<ğ›¼OPTğ·âˆ’ğœ–]â‰¤2ğ‘›exp
âˆ’ğ‘šğœ–2
2ğ‘‰2ğ‘˜2(1+ğ›¼)2
.
Split the regret by exploration phase and exploitation phase,
ğ‘…ğ‘’ğ‘”ğ›¼(ğ‘‡)=ğ‘šğ‘˜ğ‘‰+(ğ‘‡âˆ’ğ‘šğ‘˜)E[ğ›¼OPTğ·âˆ’ğ‘…ğ·(ğ¾)]. (7)
Calculate the expectation term with Lemma 5,
E[ğ›¼OPTğ·âˆ’ğ‘…ğ·(ğ¾)]
=âˆ«âˆ
0Pr[ğ›¼OPTğ·âˆ’ğ‘…ğ·(ğ¾)>ğœ–]dğœ–
â‰¤âˆ«âˆ
02ğ‘›exp
âˆ’2ğ‘šğœ–2
4ğ‘‰2ğ‘˜2(1+ğ›¼)2
dğœ–
=âˆš
2ğœ‹ğ‘›ğ‘‰ğ‘˜(1+ğ›¼)âˆšğ‘š.(8)
 
1569KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haoming Li et al.
Plug (8)into(7), and letğ‘š=min(âŒˆ(ğœ‹
2)1
3(1+ğ›¼)2
3ğ‘›2
3ğ‘‡2
3âŒ‰,âŒˆğ‘‡/ğ‘˜âŒ‰),
we have
ğ‘…ğ‘’ğ‘”ğ›¼(ğ‘‡)=ğ‘šğ‘˜ğ‘‰+(ğ‘‡âˆ’ğ‘šğ‘˜)E[ğ›¼OPTğ·âˆ’ğ‘…ğ·(ğ¾)]
â‰¤ğ‘šğ‘˜ğ‘‰+ğ‘‡âˆš
2ğœ‹ğ‘›ğ‘‰ğ‘˜(1+ğ›¼)âˆšğ‘š
=(2âˆ’2
3+2âˆ’1
3)(2ğœ‹)2
3(1+ğ›¼)2
3ğ‘˜ğ‘‰ğ‘›2
3ğ‘‡2
3
â‰¤3(1+ğ›¼)2
3ğ‘˜ğ‘‰ğ‘›2
3ğ‘‡2
3.(9)
â–¡
Beyond sublinear regret, Algorithm 1 also enjoys the following
truthful and IR properties.
Proposition 2. Algorithm 1 is ex-post truthful and ex-post IR.
The truthfulness of Algorithm 1 mainly follows from the prop-
erty of truthful oracles combined with the truthfulness of second-
price auctions. The IR property follows from that of second-price
auctions. The complete proofs are deferred to Appendix A.
4.3ğ‘‚(logğ‘‡/Î”2
ğœ™)Mechanism
Although Algorithm 1 guarantees ex-post truthfulness and ex-post
IR, itsğ‘‚(ğ‘‡2/3)regret is not satisfactory in some scenarios. To design
an algorithm with a better regret bound, we make an additional as-
sumption on the oracle. Beyond truthfulness and ğ›¼âˆ’approximation,
we assume that the oracle determines ğ¾by ranking the bidders
according to some score and choosing the top- ğ‘˜, and the scores
are accessible. The scores provide additional information about the
quality of the bidders, and can be leveraged by the bandit algorithm
to quickly determine the correct subset to exploit.
Definition 9 (Truthful Approximation Scoring Oracle). A
scoring oracle Score(Â·)assigns each bidder a score according to its
distribution, i.e., ğœ™ğ‘–â†Score(ğ·ğ‘–), such that ranking the bidders by
ğ‘ğ‘–ğœ™ğ‘–, and choosing the top- ğ‘˜asğ¾will ensure
ğ‘…ğ·(ğ¾)â‰¥ğ›¼OPTğ·.
It is easy to check that this scoring and ranking procedure is
always ex-post truthful for any score ğœ™=(ğœ™1,Â·Â·Â·,ğœ™ğ‘›).
We further make an assumption on the smoothness of Score(Â·):
There exists a Lipschitz constant ğ¿>0, such that if supğ‘¥|ğ¹(ğ‘¥)âˆ’
ğ¹â€²(ğ‘¥)|â‰¤Î›, then
|Score(ğ¹)âˆ’Score(ğ¹â€²)|â‰¤ğ¿Î›.
The following lemma tells us such scoring oracle exists. In fact,
the oracle presented in Lemma 3 is exactly a truthfulğ‘’âˆ’1
2ğ‘’âˆ’approx-
imation scoring oracle. The proof of its Lipschitz constant ğ¿=1is
deferred to Appendix A.
Lemma 7. There exists a truthfulğ‘’âˆ’1
2ğ‘’-approximation scoring ora-
cle with Lipschitz constant ğ¿=1.
We denote the gap of an instance as Î”ğœ™=minğ‘–,ğ‘—âˆˆ[ğ‘›],ğ‘–â‰ ğ‘—|ğ‘£ğ‘–ğœ™ğ‘–âˆ’
ğ‘£ğ‘—ğœ™ğ‘—|. For the truthful property of the mechanism, we assume that
the valueğ‘£ğ‘–of each arm ğ‘–is within an interval around a known
parameterğ›½ğ‘–âˆˆ[0,ğ‘‰]:
ğ‘£ğ‘–âˆˆ
ğ›½ğ‘–âˆ’Î”ğœ™/2ğœ™ğ‘–,ğ›½ğ‘–+Î”ğœ™/2ğœ™ğ‘–
. (10)This assumption is often satisfied in industrial scenarios where
advertisersâ€™ valuation of a certain impression is static, and the ad
platform can estimate oneâ€™s value from oneâ€™s bidding history.
Based on an ğ›¼âˆ’approximation scoring oracle, we design an ETC
mechanism with adaptive commitment time, i.e., the length of ex-
ploration phase depends on the collected data. The fixed prior
information of value ğ›½ğ‘–, instead of submitted bid ğ‘ğ‘–, is used in de-
ciding of commitment. This prevents the bidders from influencing
the commitment time by strategic bidding.
Algorithm 2 An ETC mechanism with adaptive commitment time
1:Throughout the exploration phase, for each arm ğ‘–âˆˆ[ğ‘›]we
maintain: (i) counter ğ‘‡ğ‘–which stores the times that ğ‘–has been
selected so far (ii) CDF Ë†ğ¹ğ‘–of the empirical distribution of the
observed outcomes of arm ğ‘–so far
2:repeat âŠ²Exploration Phase
3:ğ¾ğ‘¡â†{1+ğ‘˜ğ‘¡modğ‘›,2+ğ‘˜ğ‘¡modğ‘›,Â·Â·Â·,ğ‘˜+ğ‘˜ğ‘¡modğ‘›},
run second-price auction within ğ¾ğ‘¡
4: foreachğ‘–âˆˆğ¾ğ‘¡do
5: Updateğ‘‡ğ‘–and Ë†ğ¹ğ‘–
6: Query the scoring oracle, get Ë†ğœ™ğ‘–â†Score(Ë†ğ¹ğ‘–), compute
ğœ™ğ‘–â†Ë†ğœ™ğ‘–+ğ¿âˆšï¸ƒ
logğ‘‡
ğ‘‡ğ‘–andğœ™ğ‘–â†Ë†ğœ™ğ‘–âˆ’ğ¿âˆšï¸ƒ
logğ‘‡
ğ‘‡ğ‘–
7: end for
8: Rank all the arms by ğ›½ğ‘–Ë†ğœ™ğ‘–, letğ»ğ‘–ğ‘”â„ be the set of top- ğ‘˜arms,
letğ¿ğ‘œğ‘¤ be the other ğ‘›âˆ’ğ‘˜arms
9:ğœ™â„â†minğ‘–âˆˆğ»ğ‘–ğ‘”â„ğ›½ğ‘–ğœ™ğ‘–,ğœ™ğ‘™â†maxğ‘–âˆˆğ¿ğ‘œğ‘¤ğ›½ğ‘–ğœ™ğ‘–
10:untilğœ™â„â‰¥ğœ™ğ‘™
11:foreach remaining round ğ‘¡do âŠ²Exploitation Phase
12: Pullğ¾ğ‘¡â†ğ»ğ‘–ğ‘”â„ , run second-price auction within ğ¾ğ‘¡, using
bidsb
13:end for
Theorem 3. Algorithm 2 achieves the following ğ›¼âˆ’approximation
regret upper bound:
ğ‘…ğ‘’ğ‘”ğ›¼(ğ‘‡)â‰¤2ğ‘›ğ‘‰+4ğ‘›ğ¿2ğ‘‰3
ğ‘˜Î”2
ğœ™logğ‘‡.
Proof. LetË†ğ¹ğ‘–,ğ‘¢(ğ‘¥)be the empirical distribution of arm ğ‘–when
ğ‘¢samples from ğ‘–are observed. Define event
E=(
âˆƒğ‘–âˆˆ[ğ‘›],âˆƒğ‘¢âˆˆ[ğ‘‡],sup
ğ‘¥âˆˆ[0,1]Ë†ğ¹ğ‘–,ğ‘¢(ğ‘¥)âˆ’ğ¹ğ‘–(ğ‘¥)â‰¥âˆšï¸‚
logğ‘‡
ğ‘¢)
.
From the DKW inequality, for âˆ€ğ‘–âˆˆ[ğ‘›],âˆ€ğ‘¢âˆˆ[ğ‘‡],
Pr
sup
ğ‘¥âˆˆ[0,1]|Ë†ğ¹ğ‘–,ğ‘¢(ğ‘¥)âˆ’ğ¹ğ‘–(ğ‘¥)|â‰¥âˆšï¸‚
logğ‘‡
ğ‘¢
â‰¤2ğ‘’âˆ’2ğ‘¢logğ‘‡
ğ‘¢=2
ğ‘‡2.
Taking an union bound gives Pr[E]â‰¤2ğ‘›
ğ‘‡.
LetË†ğœ™ğ‘–,ğ‘¡,ğœ™ğ‘–,ğ‘¡,ğœ™ğ‘–,ğ‘¡be the value of Ë†ğœ™ğ‘–,ğœ™ğ‘–,ğœ™ğ‘–at timeğ‘¡. On eventÂ¬E,
by definition, for any arm ğ‘–, any timeğ‘¡in the exploration phase,
|ğœ™ğ‘–âˆ’Ë†ğœ™ğ‘–,ğ‘¡|â‰¤ğ¿âˆšï¸ƒ
logğ‘‡
ğ‘‡ğ‘–, thereforeğœ™ğ‘–,ğ‘¡â‰¤ğœ™ğ‘–â‰¤ğœ™ğ‘–,ğ‘¡. When condition
ğœ™â„â‰¥ğœ™ğ‘™is satisfied, forâˆ€ğ‘–âˆˆğ»ğ‘–ğ‘”â„ ,âˆ€ğ‘—âˆˆğ¿ğ‘œğ‘¤,ğ›½ğ‘–ğœ™ğ‘–,ğ‘¡â‰¥ğ›½ğ‘—ğœ™ğ‘—,ğ‘¡, thus
ğ›½ğ‘–ğœ™ğ‘–â‰¥ğ›½ğ‘—ğœ™ğ‘—. This implies that ğ»ğ‘–ğ‘”â„ is the top-ğ‘˜subset according
 
1570Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
to{ğ›½ğ‘–ğœ™ğ‘–}. By Equation (10),ğ»ğ‘–ğ‘”â„ is also the top- ğ‘˜subset according
to{ğ‘ğ‘–ğœ™ğ‘–}.
Letğ‘šbe the last round in which ğœ™â„<ğœ™ğ‘™. Letğ‘–=argminğ‘–âˆˆğ»ğ‘–ğ‘”â„ğ›½ğ‘–ğœ™ğ‘–,
ğ‘—=argmaxğ‘—âˆˆğ¿ğ‘œğ‘¤ğ›½ğ‘—ğœ™ğ‘—, we knowğ›½ğ‘–ğœ™ğ‘–<ğ›½ğ‘—ğœ™ğ‘—. By the exploration
rule, till round ğ‘š, we have observedğ‘šğ‘˜
ğ‘›samples for each arm. Since
ğ›½ğ‘–ğœ™ğ‘–<ğ›½ğ‘—ğœ™ğ‘—,
ğ›½ğ‘– 
ğœ™ğ‘–âˆ’ğ¿âˆšï¸‚
ğ‘›logğ‘‡
ğ‘šğ‘˜!
<ğ›½ğ‘— 
ğœ™ğ‘—+ğ¿âˆšï¸‚
ğ‘›logğ‘‡
ğ‘šğ‘˜!
.
Rearranging the terms, we have
Î”ğœ™â‰¤ğ›½ğ‘–ğœ™ğ‘–âˆ’ğ›½ğ‘—ğœ™ğ‘—â‰¤(ğ›½ğ‘–+ğ›½ğ‘—)ğ¿âˆšï¸‚
ğ‘›logğ‘‡
ğ‘šğ‘˜â‰¤2ğ¿ğ‘‰âˆšï¸‚
ğ‘›logğ‘‡
ğ‘šğ‘˜.
Solvingğ‘šfrom the first and last term gives ğ‘šâ‰¤4ğ‘›ğ¿2ğ‘‰2logğ‘‡
ğ‘˜Î”2
ğœ™.
On eventE, each round incurs regret of at most ğ‘‰. On event
Â¬E, we know that ğ»ğ‘–ğ‘”â„ is the top-ğ‘˜subset according to {ğ‘ğ‘–ğœ™ğ‘–}.
By definition of the ğ›¼âˆ’approximation scoring oracle, playing ğ»ğ‘–ğ‘”â„
incurs non-positive regret. Therefore, the regret of the algorithm is
ğ‘…ğ‘’ğ‘”ğ›¼(ğ‘‡)â‰¤Pr[E]ğ‘‡ğ‘‰+Pr[Â¬E]E[ğ‘š|Â¬E]ğ‘‰
â‰¤2ğ‘›
ğ‘‡ğ‘‡ğ‘‰+4ğ‘›ğ¿2ğ‘‰2logğ‘‡
ğ‘˜Î”2
ğœ™ğ‘‰
=2ğ‘›ğ‘‰+4ğ‘›ğ¿2ğ‘‰3
ğ‘˜Î”2
ğœ™logğ‘‡.
â–¡
Algorithm 2 also achieves ex-post truthfulness and ex-post IR.
The proofs are deferred to Appendix A.
Proposition 3. Algorithm 2 is ex-post truthful and ex-post IR.
By the design of adaptive commitment time, Algorithm 2 achieves
a better regret than Algorithm 1 on most instances, except for the
cases when Î”ğœ™is extremely small. A potential way to further im-
prove the regret bound is to adopt UCB-based [ 5] or successive-
elimination style [ 27] algorithms. However, these algorithms are
much more data sensitive than ETC algorithms, thus may be prone
to strategic bids.
5 EXPERIMENTS
In this section, we evaluate our two mechanisms through experi-
ments on both synthetic data and real-world data.
For the experiments, instead of ğ›¼âˆ’approximation regret (Defini-
tion 8), we compare the cumulative rewards achieved by our mech-
anisms against ğ‘‡Â·ğ‘…ğ·(ğ¾Oracle), whereğ¾Oracle is the subset returned
by anğ›¼âˆ’approximation oracle. By definition of ğ›¼âˆ’approximation
oracles (Definition 7), ğ‘…ğ·(ğ¾Oracle)â‰¥ğ›¼OPTğ·, soğ‘…ğ·(ğ¾Oracle)is a
more challenging reference value, and all of our theoretical results
still holds under this notion of regret. This choice is based on two
reasons: (i) computing OPTğ·is often computationally infeasible,
(ii) the value of ğ›¼OPTğ·can be much lower than ğ‘…ğ·(ğ¾Oracle)prac-
tically, and comparing the mechanismsâ€™ cumulative reward against
ğ›¼OPTğ·often leads to negative regret.
Beyond regret, we also test the truthfulness of our mechanisms,
by computing a bidderâ€™s utility with different bids.
(a) Ex-post truthfulness of Algo-
rithm 1.
(b) Ex-post truthfulness of Algo-
rithm 2.
Figure 1: Ex-post truthfulness of our two algorithms evalu-
ated on synthetic data. Each line represents a bidderâ€™s utilities
with respect to different submitted bids on one random seed.
5.1 Experiments with synthetic data
5.1.1 Experiment Setup. We construct an environment with ğ‘›=7
bidders, from which ğ‘˜=3bidders are selected in each round. All
bidders have the same uniform CTR distribution, i.e., ğ‘ğ‘–âˆ¼ğ‘ˆ([0,1]).
The biddersâ€™ values are [1+Î”,1+Î”,1+Î”,1,1,1,1], where Î”is a
gap parameter that we control through the experiments. By the
truthful property of our mechanisms, the input bids are equal to
the values. We run experiments for different time horizons ğ‘‡âˆˆ
{2Ã—104,4Ã—104,6Ã—104,8Ã—104,105}. For each time horizon, we run
experiments for Î”âˆˆ{0.5,1,1.5,2}. The result of each experiment
is averaged over 80 independent runs.
We leverage the oracle presented in 3 for both Algorithm 1 and
Algorithm 2. The difference is that for Algorithm 1, the oracle only
returns a subset it chooses, while for Algorithm 2, it returns all the
scoresğœ™ğ‘–(1/ğ‘˜)forğ‘–âˆˆ[ğ‘›].
To test the ex-post truthfulness of our mechanisms, we fix Î”to
be 1, so biddersâ€™ the values are [2,2,2,1,1,1,1]. Since ex-post truth-
fulness requires the mechanism to be truthful on any random seed,
we pick 100 random seeds for evaluation. For each fixed random
seed, we adjust the bid of bidder 1 to ğ‘1âˆˆ{1.25,1.50,1.75,2.00,
2.25,2.50,2.75,3.00}, while keeping other biddersâ€™ bids unchanged.
We report bidder 1â€™s utility when different bids are reported. If
the mechanism is ex-post truthful, then for any random seed, the
utility-maximizing bid should be equal to the value. During the test
of truthfulness, we fix ğ‘‡=10000.
5.1.2 Results and Discussions. Figure 2a presents a comparison of
regret between Algorithm 1 and Algorithm 2. For any time hori-
zonğ‘‡and gap Î”, the regret of Algorithm 2 is significantly lower
than that of Algorithm 1. The low regret is due to the design of
adaptive commitment time in Algorithm 2. Besides, when the gap
Î”decreases, Algorithm 1 achieves lower regret, while Algorithm 2
suffers from higher regret. This phenomenon is demonstrated more
clearly in Figure 2b and Figure 2c. Figure 2b depicts the regret
of Algorithm 1 with different ğ‘‡andÎ”in a log-log plot. The grey
dashed lines represent ğ‘…ğ‘’ğ‘”=ğ‘ğ‘‡2
3with different values of ğ‘. We
observe that the regret curves are almost parallel with the grey
lines, which indicates Î˜(ğ‘‡2/3)regret, matching Theorem 2. More-
over, Algorithm 1 shows higher regret when Î”gets high. This is
because the regret accumulated in the exploration phase grows
as the gap between optimal and suboptimal super arms expands.
 
1571KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haoming Li et al.
(a) Comparison of the regret of two mecha-
nisms. Regret is displayed on a log scale.
(b) The regret of Algorithm 1. To demon-
strate the order of ğ‘‡2/3, both Regret and T
are displayed on a log scale.
(c) The regret of Algorithm 2. To demon-
strate the order of logğ‘‡, T is displayed on a
log scale.
Figure 2: Regret of our two algorithms evaluated on synthetic data.
Figure 3: Regret of our two algorithms evaluated on Movie-
Lens dataset. Both Regret and T are displayed on a log scale.
Figure 2c shows the regret of Algorithm 2. We observe that the
regret almost grows linearly with respect to logğ‘‡. Besides, the re-
gret grows quadratically with decreasing Î”. Note that in the case
of our experiment, Î”is proportional to Î”ğœ™, as the distributions are
identical and fixed. The dependence of regret on ğ‘‡andÎ”ğœ™nicely
matches our theoretical result (Theorem 3).
In the test of truthfulness, on all 100 random seeds, bidding the
true value achieves highest utility among 8 different bids. Figure 1
shows the utility curve of two mechanisms on five random seeds
{1,2,3,4,5}. The curves on all 100 seeds actually look similar, with
ğ‘=2being their common maximum point, and the fives seeds are
arbitrarily picked only for demonstration.
5.2 Experiments with real-world data
We evaluate the Algorithm 1 and Algorithm 2 on the MovieLens
1M [14] dataset.
5.2.1 Experiment Setup. We treat each movie as an arm and convert
the ratings into a CTR-like metric. The top 7arms, determined by
the highest number of ratings in the original dataset, are selected as
the base arms. From these base arms, a super arm consisting of ğ‘˜=3arms is chosen in each round. To construct the CTR distribution, we
manually set a rating threshold ğ‘Ÿğ‘¡=3.5, such that ratings ğ‘Ÿ>3.5are
converted to 1, and otherwise to 0. Subsequently, we calculate the
mean and variance of the converted ratings for each arm, denoted as
the mean and variance of the CTR distribution, respectively. These
CTR distributions are modeled as truncated Gaussian distributions,
with support restricted to [0,1]. The bid of each arm is sampled
from a uniform distribution ğ‘ˆ([0,5])and remains constant after
the experiment begins. We conduct experiments for different time
horizonsğ‘‡âˆˆ{1250,2500,5000,1Ã—104,2Ã—104,4Ã—104,6Ã—104}.
The setting of the oracle is the same as Section 5.1.1.
5.2.2 Results and Discussions. Figure 3 presents a comparison of
the regret between Algorithm 1 and Algorithm 2. When the horizon
ğ‘‡is small, both algorithms exhibit linear regret, as ğ‘šğ‘˜>ğ‘‡for a
smallğ‘‡, leading to a predominantly exploratory phase within the
limited horizon. As ğ‘‡increases, both algorithms demonstrate im-
provements by incorporating an exploitation phase. In comparison
with the dashed grey line, it is evident that Algorithm 1 achieves
ğ‘‚(ğ‘‡2/3)regret, while Algorithm 2 attains ğ‘‚(logğ‘‡)regret. Notably,
with a large horizon ğ‘‡, Algorithm 2 achieves lower regret compared
to Algorithm 1.
6 RELATED WORKS
Two-stage Advertising Systems. Previous studies on two-stage
advertising systems have primarily focused on two aspects: allo-
cation efficiency and incentives. Both Wang et al. [ 25] and Zhao
et al. [ 32] addressed the learning objectives of machine learning
models in the first stage, in order to align with the second stage and
enhancing the overall ad allocation performance. While Wang et
al. [25] also discussed incentives, they considered a non-standard
value-maximizer utility model. On incentives in two-stage auctions,
Goel et al. [ 13] provided an insightful characterization of first-stage
mechanisms that ensures overall truthfulness when composed with
any truthful second-stage auction mechanism. However, their work
was limited to single-round mechanisms, whereas we considered
incentives in multi-round bandit mechanisms.
 
1572Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Two-stage Recommendation Systems. Research on two-stage struc-
tures in recommendation is rather abundant than advertising. A gen-
eral approach is cooperative training of both stages [ 12,15,17,18]
to improve the overall recommendation performance, and particu-
lar attention has been paid to off-policy correction [ 20] and fairness
issues [ 24]. Closer to our setting is synchronized two-stage explo-
ration, studied under both linear [ 16] and neural [ 31] bandit settings.
Although these studies share some similarities with our setting,
there are significant differences due to the presence of incentives
in ad auctions.
Truthful Bandit Mechanisms. A line of research has focused on
designing truthful mechanisms for multi-round ad auctions, where
a multi-armed bandit algorithm acts as the allocation rule. Babaioff
et al. [ 4] provided a Î©(ğ‘‡2/3)regret lower bound for any determin-
istic truthful multi-armed bandit mechanisms, and provided a ETC
algorithm that matches this lower bound. Devanur and Kakade
[8] obtained a similar Î©(ğ‘‡2/3)lower bound under the revenue-
maximizing setting. Babaioff et al. [ 3] further extended to ran-
domized mechanisms, and provided a black-box reduction from
any monotone bandit algorithm to a truthful mechanism, which
gives rise to ğ‘‚(âˆš
ğ‘‡)regret. Recent works have considered extended
settings in different directions, such as utility models [ 11] and con-
textual information [ 1,28,30]. Our work is based on a novel setting
of two-stage ad auctions which is formulated as designing combi-
natorial bandit mechanisms.
7 CONCLUSION
In this paper, we investigate the problem of designing truthful
bandit mechanisms for two-stage online ad auctions. We prove
anÎ©(ğ‘‡)lower bound for truthful mechanisms, and introduce
truthfulğ›¼âˆ’approximation oracles which give rise to sublinear
ğ›¼âˆ’approximation regret mechanisms.
We leave it as an open problem to potentially design ğ‘‚(âˆš
ğ‘‡)
bandit mechanisms within the approximation regret setting, or
to establish an Î©(ğ‘‡2/3)lower bound. Moreover, the impossibility
result may also be circumvented by other approaches, e.g., relaxing
the notion of truthfulness by considering high-probability truthful
mechanisms.
ACKNOWLEDGMENTS
The authors sincerely thank Shuai Li and Xutong Liu for their help-
ful discussions. This work was supported in part by National Key
R&D Program of China (No. 2022ZD0119100), in part by China
NSF grant No. 62322206, 62132018, U2268204, 62025204, 62272307,
62372296. The opinions, findings, conclusions, and recommenda-
tions expressed in this paper are those of the authors and do not
necessarily reflect the views of the funding agencies or the govern-
ment.
REFERENCES
[1]Kumar Abhishek, Shweta Jain, and Sujit Gujar. 2020. Designing Truthful Contex-
tual Multi-Armed Bandits based Sponsored Search Auctions. In Proceedings of
the 19th International Conference on Autonomous Agents and MultiAgent Systems
(Auckland, New Zealand) (AAMAS â€™20). International Foundation for Autonomous
Agents and Multiagent Systems, Richland, SC, 1732â€“1734.[2]Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 1995.
Gambling in a rigged casino: The adversarial multi-armed bandit problem. In
Proceedings of IEEE 36th annual foundations of computer science. IEEE, 322â€“331.
[3]Moshe Babaioff, Robert D Kleinberg, and Aleksandrs Slivkins. 2015. Truthful
mechanisms with implicit payment computation. Journal of the ACM (JACM) 62,
2 (2015), 1â€“37.
[4]Moshe Babaioff, Yogeshwer Sharma, and Aleksandrs Slivkins. 2014. Character-
izing Truthful Multi-armed Bandit Mechanisms. SIAM J. Comput. 43, 1 (2014),
194â€“230.
[5]Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, and Pinyan Lu. 2016. Combinatorial multi-
armed bandit with general reward functions. Advances in Neural Information
Processing Systems 29 (2016).
[6]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems. 7â€“10.
[7]Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
for youtube recommendations. In Proceedings of the 10th ACM conference on
recommender systems. 191â€“198.
[8]Nikhil R Devanur and Sham M Kakade. 2009. The price of truthfulness for
pay-per-click auctions. In Proceedings of the 10th ACM conference on Electronic
commerce. 99â€“106.
[9]Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz. 1956. Asymptotic minimax
character of the sample distribution function and of the classical multinomial
estimator. The Annals of Mathematical Statistics (1956), 642â€“669.
[10] Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu, Rahul Sharma,
Charles Sugnet, Mark Ulrich, and Jure Leskovec. 2018. Pixie: A system for
recommending 3+ billion items to 200+ million users in real-time. In Proceedings
of the 2018 world wide web conference. 1775â€“1784.
[11] Zhe Feng, Christopher Liaw, and Zixin Zhou. 2023. Improved online learning
algorithms for CTR prediction in ad auctions. In International Conference on
Machine Learning. PMLR, 9921â€“9937.
[12] Luke Gallagher, Ruey-Cheng Chen, Roi Blanco, and J. Shane Culpepper. 2019.
Joint Optimization of Cascade Ranking Models. In Proceedings of the Twelfth ACM
International Conference on Web Search and Data Mining (WSDM â€™19) . 15â€“23.
[13] Gagan Goel, Renato Paes Leme, Jon Schneider, David Thompson, and Hanrui
Zhang. 2023. Eligibility Mechanisms: Auctions Meet Information Retrieval. In
Proceedings of the ACM Web Conference 2023. 3541â€“3549.
[14] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History
and context. Acm transactions on interactive intelligent systems (tiis) 5, 4 (2015),
1â€“19.
[15] Jiri Hron, Karl Krauth, Michael Jordan, and Niki Kilbertus. 2021. On Compo-
nent Interactions in Two-Stage Recommender Systems. In Advances in Neural
Information Processing Systems, Vol. 34. 2744â€“2757.
[16] Jiri Hron, Karl Krauth, Michael I. Jordan, and Niki Kilbertus. 2020. Exploration
in two-stage recommender systems. arXiv:2009.08956 [cs.IR]
[17] Xu Huang, Defu Lian, Jin Chen, Liu Zheng, Xing Xie, and Enhong Chen. 2023.
Cooperative Retriever and Ranker in Deep Recommenders. In Proceedings of the
ACM Web Conference 2023. 1150â€“1161.
[18] Wang-Cheng Kang and Julian McAuley. 2019. Candidate Generation with Binary
Codes for Large-Scale Top-N Recommendation. In Proceedings of the 28th ACM
International Conference on Information and Knowledge Management (CIKM â€™19).
1523â€“1532.
[19] Tor Lattimore and Csaba SzepesvÃ¡ri. 2020. Bandit algorithms. Cambridge Univer-
sity Press, Chapter 6, 97â€“98.
[20] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Ji Yang, Minmin Chen, Jiaxi Tang, Lichan Hong,
and Ed H. Chi. 2020. Off-policy Learning in Two-stage Recommender Systems.
InProceedings of The Web Conference 2020. 463â€“473.
[21] Xu Ma, Pengjie Wang, Hui Zhao, Shaoguo Liu, Chuhan Zhao, Wei Lin, Kuang-
Chih Lee, Jian Xu, and Bo Zheng. 2021. Towards a Better Tradeoff between
Effectiveness and Efficiency in Pre-Ranking: A Learnable Feature Selection based
Approach. 2036â€“2040.
[22] Pascal Massart. 1990. The tight constant in the Dvoretzky-Kiefer-Wolfowitz
inequality. The annals of Probability (1990), 1269â€“1283.
[23] Roger B Myerson. 1981. Optimal auction design. Mathematics of operations
research 6, 1 (1981), 58â€“73.
[24] Lequn Wang and Thorsten Joachims. 2023. Uncertainty Quantification for Fair-
ness in Two-Stage Recommender Systems. In Proceedings of the Sixteenth ACM
International Conference on Web Search and Data Mining. 940â€“948.
[25] Yiqing Wang, Xiangyu Liu, Zhenzhe Zheng, Zhilin Zhang, Miao Xu, Chuan Yu,
and Fan Wu. [n. d.]. On Designing a Two-stage Auction for Online Advertising.
InProceedings of the ACM Web Conference 2022. 90â€“99.
[26] Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, and Kun
Gai. 2020. COLD: Towards the Next Generation of Pre-Ranking System.
arXiv:2007.16122 [cs.IR]
[27] Haike Xu and Jian Li. 2021. Simple combinatorial algorithms for combinatorial
bandits: Corruptions and approximations. In Uncertainty in Artificial Intelligence.
PMLR, 1444â€“1454.
 
1573KDD â€™24, August 25â€“29, 2024, Barcelona, Spain Haoming Li et al.
[28] Yinglun Xu, Bhuvesh Kumar, and Jacob Abernethy. 2023. On the robustness
of epoch-greedy in multi-agent contextual bandit mechanisms. arXiv preprint
arXiv:2307.07675 (2023).
[29] Xinyang Yi, Ji Yang, Lichan Hong, Derek Zhiyuan Cheng, Lukasz Heldt, Aditee
Kumthekar, Zhe Zhao, Li Wei, and Ed Chi. 2019. Sampling-bias-corrected neural
modeling for large corpus item recommendations. In Proceedings of the 13th ACM
Conference on Recommender Systems. 269â€“277.
[30] Mengxiao Zhang and Haipeng Luo. 2023. Online Learning in Contextual Second-
Price Pay-Per-Click Auctions. arXiv preprint arXiv:2310.05047 (2023).
[31] Mengyan Zhang, Thanh Nguyen-Tang, Fangzhao Wu, Zhenyu He, Xing Xie, and
Cheng Soon Ong. 2022. Two-Stage Neural Contextual Bandits for Personalised
News Recommendation. arXiv:2206.14648 [cs.IR]
[32] Zhishan Zhao, Jingyue Gao, Yu Zhang, Shuguang Han, Siyuan Lou, Xiang-
Rong Sheng, Zhe Wang, Han Zhu, Yuning Jiang, Jian Xu, and Bo Zheng.
2023. COPR: Consistency-Oriented Pre-Ranking for Online Advertising.
arXiv:2306.03516 [cs.IR]
[33] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang
Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate
prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33.
5941â€“5948.
[34] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through
rate prediction. In Proceedings of the 24th ACM SIGKDD international conference
on knowledge discovery & data mining. 1059â€“1068.
A PROOFS
Proof of Lemma 2. Decompose the regret to super arms,
ğ‘…ğ‘’ğ‘”(ğ‘‡)=ğ‘‡Â·OPTğ·âˆ’E"ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘…ğ·(ğ¾ğ‘¡)#
=âˆ‘ï¸
ğ¾(ğ‘…ğ·(ğ¾âˆ—)âˆ’ğ‘…ğ·(ğ¾))E[ğœğ¾(ğ‘‡)].
By sublinear regret,
0=lim
ğ‘‡â†’âˆğ‘…ğ‘’ğ‘”(ğ‘‡)
ğ‘‡=âˆ‘ï¸
ğ¾(ğ‘…ğ·(ğ¾âˆ—)âˆ’ğ‘…ğ·(ğ¾))lim
ğ‘‡â†’âˆE[ğœğ¾(ğ‘‡)]
ğ‘‡.
Since the optimal super arm is unique, for every ğ¾â‰ ğ¾âˆ—,ğ‘…ğ·(ğ¾âˆ—)âˆ’
ğ‘…ğ·(ğ¾)>0, thus we have limğ‘‡â†’âˆE[ğœğ¾(ğ‘‡)]/ğ‘‡=0. Note that
ğ‘‡=Ã
ğ¾E[ğœğ¾(ğ‘‡)]. Therefore limğ‘‡â†’âˆE[ğœğ¾âˆ—(ğ‘‡)]/ğ‘‡=1.
â–¡
Proof of Proposition 2. For truthfulness, fix any realization
table C, and consider the cumulative utility ğ‘ˆğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–)of bidder
ğ‘–. We separate ğ‘ˆğ‘–to the cumulative utility in exploration phase and
exploitation phase,
ğ‘ˆğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–)=ğ‘šğ‘˜âˆ‘ï¸
ğ‘¡=1ğ‘¢ğ‘–ğ‘¡(C,ğ‘ğ‘–,bâˆ’ğ‘–)+ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘šğ‘˜+1ğ‘¢ğ‘–ğ‘¡(C,ğ‘ğ‘–,bâˆ’ğ‘–)
whereğ‘¢ğ‘–ğ‘¡(C,ğ‘ğ‘–,bâˆ’ğ‘–)is bidderğ‘–â€™s utility in round ğ‘¡.
In the exploration phase, bidder ğ‘–only participates in the subset
auction in certain fixed rounds, and its competitors in these rounds
is not influenced by ğ‘ğ‘–. For each of these rounds, by the truthfulness
of second-price auctions, bidder ğ‘–optimizes its utility with truthful
bidğ‘£ğ‘–.
For the exploitation phase, the empirical distributions Ë†Fin line 6
only depend on C, and are not influenced by ğ‘ğ‘–. The exploitation
phase consists of repeated second-price auctions on a fixed subset ğ¾,
which is selected by the oracle. From the truthfulness of the oracle,
for fixed Ë†F, the combination of selecting ğ¾and running second-
price auction within ğ¾is truthful, with respect to any realization
c. This implies that the combination of selecting ğ¾and running
any one of the auctions in rounds ğ‘šğ‘˜+1â‰¤ğ‘¡â‰¤ğ‘‡is truthful.Thereforeğ‘£ğ‘–is the maximizer of any of the objectives ğ‘¢ğ‘–ğ‘¡(C,ğ‘ğ‘–,bâˆ’ğ‘–)
forğ‘šğ‘˜+1â‰¤ğ‘¡â‰¤ğ‘‡.
To conclude, the truthful bid ğ‘£ğ‘–maximizes any of ğ‘¢ğ‘–ğ‘¡(C,ğ‘ğ‘–,bâˆ’ğ‘–)
for1â‰¤ğ‘¡â‰¤ğ‘‡, and thus maximizes ğ‘ˆğ‘–(C,ğ‘ğ‘–,bâˆ’ğ‘–).
For IR, the total utility ğ‘ˆğ‘–of bidderğ‘–is defined as the sum of
utility in each round,
ğ‘ˆğ‘–(C,ğ‘£ğ‘–,bâˆ’ğ‘–)=ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘¢ğ‘–ğ‘¡(C,ğ‘£ğ‘–,bâˆ’ğ‘–)
Fix any realization table C. For anyğ‘¡, ifğ‘–âˆˆğ¾ğ‘¡, thenğ‘–participates
a second-price auction in round ğ‘¡. By the IR property of second-
price auctions, ğ‘¢ğ‘–ğ‘¡(C,ğ‘£ğ‘–,bâˆ’ğ‘–)â‰¥0. Ifğ‘–âˆ‰ğ¾ğ‘¡, both allocation ğ‘¥ğ‘–ğ‘¡and
paymentğ‘ğ‘–ğ‘¡are zero, thus ğ‘¢ğ‘–ğ‘¡=0. Since each round produces non-
negative utility when truthful bidding, the total utility ğ‘ˆğ‘–(C,ğ‘£ğ‘–,bâˆ’ğ‘–)
is non-negative, therefore the mechanism is IR. â–¡
Proof of Lemma 7. The oracle in Lemma 3 is a truthfulğ‘’âˆ’1
2ğ‘’-
approximation scoring oracle. Now we prove that its Lipschitz
constant isğ¿=1.
For distributions ğ·andğ·â€², with CDFs ğ¹andğ¹â€²,
Score(ğ¹)=ğœ™1
ğ‘˜
=Eğ‘Ÿâˆ¼ğ·
ğ‘ŸÂ·I
ğ‘Ÿâ‰¥ğ‘(1
ğ‘˜)
=âˆ«1
ğ‘(1
ğ‘˜)ğ‘Ÿdğ¹(ğ‘Ÿ)
=âˆ«ğ‘(1
ğ‘˜)
01
ğ‘˜dğ‘Ÿ+âˆ«1
ğ‘(1
ğ‘˜)(1âˆ’ğ¹(ğ‘Ÿ))dğ‘Ÿ
Define
ğ‘§(ğ‘Ÿ):=ï£±ï£´ï£´ ï£²
ï£´ï£´ï£³1
ğ‘˜0â‰¤ğ‘Ÿ<ğ‘
1
ğ‘˜
1âˆ’ğ¹(ğ‘Ÿ)ğ‘
1
ğ‘˜
â‰¤ğ‘Ÿâ‰¤1,
we have Score(ğ¹)=âˆ«1
0ğ‘§(ğ‘Ÿ)dğ‘Ÿ.
For the difference of scores,
|Score(ğ¹)âˆ’Score(ğ¹â€²)|â‰¤âˆ«1
0|ğ‘§(ğ‘Ÿ)âˆ’ğ‘§â€²(ğ‘Ÿ)|dğ‘Ÿ (11)
Now we prove that for any ğ‘Ÿâˆˆ[0,1],
|ğ‘§(ğ‘Ÿ)âˆ’ğ‘§â€²(ğ‘Ÿ)|â‰¤|ğ¹(ğ‘Ÿ)âˆ’ğ¹â€²(ğ‘Ÿ)|. (12)
Without loss of generality, assume ğ‘(1
ğ‘˜)â‰¤ğ‘â€²(1
ğ‘˜). Consider three
cases:
Case 1.ğ‘Ÿ<ğ‘(1
ğ‘˜). In this case ğ‘§(ğ‘Ÿ)=ğ‘§â€²(ğ‘Ÿ)=1
ğ‘˜,ğ‘§(ğ‘Ÿ)âˆ’ğ‘§â€²(ğ‘Ÿ)=0.
Case 2.ğ‘(1
ğ‘˜)â‰¤ğ‘Ÿ<ğ‘â€²(1
ğ‘˜). Byğ‘(1
ğ‘˜)â‰¤ğ‘â€²(1
ğ‘˜)we knowğ¹â€²(ğ‘Ÿ)<
1âˆ’1
ğ‘˜. Thusğ‘§(ğ‘Ÿ)âˆ’ğ‘§â€²(ğ‘Ÿ)=1âˆ’ğ¹(ğ‘Ÿ)âˆ’1
ğ‘˜â‰¤ğ¹â€²(ğ‘Ÿ)âˆ’ğ¹(ğ‘Ÿ)
Case 3.ğ‘Ÿâ‰¥ğ‘â€²(1
ğ‘˜). In this case ğ‘§(ğ‘Ÿ)âˆ’ğ‘§â€²(ğ‘Ÿ)=ğ¹â€²(ğ‘Ÿ)âˆ’ğ¹(ğ‘Ÿ)
Concluding the three cases finishes the proof of (12). Plugging
(12) into (11) gives us
|Score(ğ¹)âˆ’Score(ğ¹â€²)|â‰¤âˆ«1
0|ğ¹(ğ‘Ÿ)âˆ’ğ¹â€²(ğ‘Ÿ)|dğ‘Ÿâ‰¤âˆ«1
0Î›dğ‘Ÿâ‰¤Î›
â–¡
 
1574Truthful Bandit Mechanisms for Repeated Two-stage Ad Auctions KDD â€™24, August 25â€“29, 2024, Barcelona, Spain
Proof of Proposition 3. For truthfulness, fix any realization
table C. In the exploration phase, the selected subset do not de-
pend on the bids. Moreover, the output super arm ğ»ğ‘–ğ‘”â„ is also not
influenced by the bids. In the exploitation phase, we repeatedly
run second-price auctions within ğ»ğ‘–ğ‘”â„ . Since a bidder cannot influ-
ence the selected subset ğ¾ğ‘¡in any round ğ‘¡, the truthfulness of our
mechanism follows from the truthfulness of second-price auction.For IR, again fix any realization table C. For any round ğ‘¡, and any
bidderğ‘–, ifğ‘–âˆˆğ¾ğ‘¡, thenğ‘–participates a second-price auction in round
ğ‘¡. By the IR property of second-price auctions, ğ‘¢ğ‘–ğ‘¡(C,ğ‘£ğ‘–,bâˆ’ğ‘–)â‰¥0.
Ifğ‘–âˆ‰ğ¾ğ‘¡, both allocation ğ‘¥ğ‘–ğ‘¡and payment ğ‘ğ‘–ğ‘¡are zero, thus ğ‘¢ğ‘–ğ‘¡=
0. Since each round produces non-negative utility when truthful
bidding, the total utility ğ‘ˆğ‘–(C,ğ‘£ğ‘–,bâˆ’ğ‘–)is non-negative. â–¡
 
1575