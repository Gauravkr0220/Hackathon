Wasserstein Distributionally Robust Optimization
Through the Lens of Structural Causal Models and
Individual Fairness
Ahmad-Reza Ehyaei
Max Planck Institute for Intelligent Systems, TÃ¼bingen AI Center, Germany
ahmad.ehyaei@tuebingen.mpg.de
Golnoosh Farnadiâˆ—
Mila QuÃ©bec AI Institute ; McGill University, MontrÃ©al, Canada
farnadig@mila.quebec
Samira Samadi
Max Planck Institute for Intelligent Systems, TÃ¼bingen AI Center, Germany
ssamadi@tuebingen.mpg.de
Abstract
In recent years, Wasserstein Distributionally Robust Optimization (DRO) has
garnered substantial interest for its efficacy in data-driven decision-making under
distributional uncertainty. However, limited research has explored the application
of DRO to address individual fairness concerns, particularly when considering
causal structures and sensitive attributes in learning problems. To address this
gap, we first formulate the DRO problem from causality and individual fairness
perspectives. We then present the DRO dual formulation as an efficient tool to
convert the DRO problem into a more tractable and computationally efficient form.
Next, we characterize the closed form of the approximate worst-case loss quantity
as a regularizer, eliminating the max-step in the min-max DRO problem. We further
estimate the regularizer in more general cases and explore the relationship between
DRO and classical robust optimization. Finally, by removing the assumption of
a known structural causal model, we provide finite sample error bounds when
designing DRO with empirical distributions and estimated causal structures to
ensure efficiency and robust learning.
1 Introduction
Machine learning models must address discrimination because they often reflect and amplify biases
present in their training datasets [ 31]. These biases can significantly influence decisions in domains
such as healthcare [ 30], education [ 3], recruitment [ 18], and lending services [ 6]. Consequently,
these decisions disproportionately affect individuals based on sensitive attributes like race or gender,
perpetuating systemic discrimination.
To address and quantify unfairness, researchers have developed concepts like group fairness and
individual fairness [45,4]. Group fairness aims to achieve equitable outcomes across demographic
groups, while individual fairness ensures that similar individuals receive similar treatment. Formally,
withVas the feature space and Yas the label space, a model h:V â†’ Y ensures individual fairness
âˆ—Lead scientific advisor on the project
38th Conference on Neural Information Processing Systems (NeurIPS 2024).if it satisfies the condition in [20]:
dY(h(v), h(vâ€²))â‰¤LdV(v, vâ€²)for all v, vâ€²âˆˆ V, (1)
where dVanddYare dissimilarity functions, often referred to as fair metrics on the input and output
spaces. These functions capture the proximity of individuals and LâˆˆR+is a Lipschitz constant.
The metric dVreflects the intuition about which instances should be considered similar by the model.
Due to challenges in defining such metrics, group fairness is often prioritized in fairness literature
because it more straightforwardly addresses observable disparities among distinct groups, making
measurement and implementation easier in practice [ 8]. Therefore, it is crucial to study and formulate
individual fairness under different assumptions in machine learning.
Individual fairness can be achieved through robust optimization methods such as Wasserstein DRO ,
which has gained significant attention for its applications in learning and decision-making [ 55,43].
DRO incorporates a regularization term to mitigate overfitting [ 17,26,59]. By using a fair metric
as the transportation cost function in computing the Wasserstein distance, models are designed to
deliver consistent performance across varied data distributions, ensuring similar individuals receive
comparable outcomes, thus satisfying individual fairness.
Incorporating causal structures and sensitive attributes into data models complicates using an indi-
vidual fair metric as a cost function within the DRO framework. The fair metric must account for
perturbations in sensitive attributes based on counterfactuals to ensure counterfactual fairness [ 22].
This can violate the positive-definite property, where d(v, vâ€²) = 0 implies v=vâ€², a key assumption
in many DRO theorems [55, 43].
Although previous works [ 42,67,70,69,57] have attempted to apply DRO to address individual
fairness, they often do not explore the implications when causal structures and sensitive attributes
are present in the learning problem. These studies are typically limited to linear Structural Causal
Model (SCM) with specific metrics and do not discuss the form of the regularizer for other classical
DRO theorems when using a fair metric. To accurately compare our work with related studies, we
will postpone this discussion until after presenting our results in Section 4.1.
1.1 Our Contributions
In this work, we adopt the definition of a fair metric from [ 22] to define a Causally Fair Dissimilarity
Function (CFDF) , which delineates how to establish a fair metric through causality and sensitive
attributes. Using CFDF, we introduce Causally Fair DRO and present a strong duality theorem for
our approach. Under mild assumptions about CFDF and causal structure, we demonstrate that the
DRO regularizer can be estimated, or in some cases can be explicitly solved. This estimation often
leads to being more practical and computationally efficient than solving the min-max problem in (4),
as supported by advancements in algorithms from previous research such as [ 14,15]. Finally, Our
numerical analysis of both real and synthetic data demonstrates the practicality of our theoretical
framework in real-world applications. ( Â§5). In summary, the main contributions of this work are:
â€¢Define a causally fair dissimilarity function, an individual fair metric incorporating causal
structures and sensitive attributes (Def. 1), along with its representation form (Prop. 1).
â€¢ Define a causally fair DRO problem with a causally fair dissimilarity function cost ( Â§4).
â€¢ Present the strong duality theorem for causally fair DRO (Thm. 1).
â€¢Provide the exact regularizer for linear SCM under mild conditions for the loss function in
regression and classification problems (Thm. 2 and Thm. 3).
â€¢ Estimate the first-order causally fair DRO regularizer for non-linear SCM (Thm. 4).
â€¢Provide the relation between classical robust optimization and causally fair DRO (Prop. 2).
â€¢Demonstrate that under unknown SCM assumptions, by estimating the SCM or cost function,
we have finite sample guarantees for convergence of empirical DRO problems (Thm. 5).
2 Preliminaries & Notations
Data Model. LetVâˆˆ V denote a vector of feature space (predictor variables) and let Yâˆˆ Y
represent the response variable, such that Z= (V,Y)comprises the observation variables with an
2underlying probability Pâˆ—. Furthermore, assume that the feature vector V= (A,X)comprises both
sensitive attributes Aâˆˆ A and non-sensitive attributes Xâˆˆ X. Let{zi= (vi, yi)}N
i=1represent
the observations used to construct the empirical distribution PN, defined as PN:=1
NPN
i=1Î´zi,
where Î´zis the Dirac delta function. Given a loss function â„“:Z Ã—Î˜â†’R, the risk function for a
parameter Î¸âˆˆÎ˜and a probability measure PisR(P, Î¸) =EP[â„“(Z, Î¸)]. This leads to the common
empirical risk minimization approach. This method seeks to find the minimizer Î¸erm
Nwithin the
setÎ¸erm
Nâˆˆarg min Î¸âˆˆÎ˜R(PN, Î¸), as an empirical way to obtaining the optimal solution Î¸âˆ—, which is
given by Î¸âˆ—= inf Î¸âˆˆÎ˜R(Pâˆ—, Î¸).
Assume the feature space is represented by a structural causal model (SCM) M =
âŸ¨G,V,U,PUâŸ©[51]. This model includes structural equations {Vi:=fi(VPa(i),Ui)}n
i=1, which
delineate the causal relations among an endogenous variable Vi, its causal predecessors VPa(i), and
an exogenous variable Uirepresenting unobservable factors. The modelâ€™s structure is encapsulated
in a directed acyclic graph G. Exogenous variables are posited as mutually independent, enabling PU
to be expressed asQn
i=1PUi, assuming causal sufficiency and excluding hidden confounders [53].
Counterfactuals. In causal structures, data perturbation is achieved through counterfactuals ,
which are derived from interventions in SCMs. These interventions, conducted using do-calculus,
include both hard andsofttypes [ 51]. Hard interventions fix a subset I âŠ† { 1, . . . , n }of features
VIto a constant Ï„, modifying their causal connections within the causal graph while maintaining the
structural equations of other features [ 51]. This type of intervention is denoted as Mdo(VI:=Ï„)and
its structural equations are obtained by:
{Vi:=Ï„i,âˆ€iâˆˆ I;Vi:=fi(VPa(i),Ui),âˆ€i /âˆˆ I}.
Soft interventions, on the other hand, adjust the functions in the structural equations, such as
through additive interventions, without disrupting existing causal links [ 53]. In an additive (or shift)
intervention, a value âˆ†âˆˆRnis added to each feature within the SCM to enact manipulation:
{Vi:=fi(VPa(i),Ui) + âˆ† i}n
i=1.
In SCMs, counterfactuals are computed by modifying structural equations to reflect hard interventions
on specific variables, thus exploring what would occur if the intervention was applied. Under the
assumption of acyclicity, a unique function F:U â†’ V exists such that F(u) =v. Acyclicity remains
unchanged by either hard or shift interventions, allowing for the existence of modified functions
Fdo(VI:=Ï„)andFdo(VI+=âˆ†)corresponding to these interventions, respectively. The counterfactual
outcome for a hard intervention can thus be calculated using CF(v, Ï„) =Fdo(VI:=Ï„)(Fâˆ’1(v)), and
similarly, for a shift intervention, it is defined as CF(v,âˆ†). These interventions are frequently applied
in this analysis.
Counterfactuals involving the modification of sensitive attributes (termed twins ) are essential for
addressing individual-level fairness [ 40,64]. Twins are generated by altering the sensitive attribute
from atoaâ€²across its domain A. For any instance, vâˆˆ V, a set of counterfactual twins is produced
as{Â¨va=CF(v, a) :aâˆˆ A} , facilitating the analysis of fairness by comparing outcomes under
different sensitive attribute values.
Counterfactual Identifiability. To estimate the effects of interventions from observational data,
counterfactuals must be identifiable within a causal framework. A notable example of such identifi-
able SCMs is the additive noise models (ANMs) , which suggest that structural equations can be
represented as:
{Vi:=fi(VPa(i)) +Ui}n
i=1=â‡’U= (Iâˆ’f)(V) =â‡’V= (Iâˆ’f)âˆ’1(U) (2)
leading to a bijective mapping between UiandVi, ensuring no loss of information from exogenous to
endogenous variables[ 50]. This relationship implies that Vcan be derived from Uthrough a bijective
reduced-form mapping F= (Iâˆ’f)âˆ’1, where I(x) =xis the identity function. Besides ANM,
there are other counterfactually identifiable models such as LSNM [ 34] and PNL [ 71]. However, for
the sake of simplicity, our focus remains on ANM. Linear SCMs is a specific instance of ANMs,
characterized by linear functions fi.
Individual Fairness Through Robustness. In machine learning, individual fairness [ 20] is achieved
through robustness by ensuring that similar individuals receive similar outcomes, regardless of
3variations in their inputs. This concept aligns with the notion of Lipschitz continuity in decision
functions (Eq. 1), where small changes in input should not lead to excessively large changes in output.
Depending on how the uncertainty set is defined, various types of robust optimization can be employed.
Inadversarially robust optimization [44,7], the uncertainty set is defined by introducing a slight
perturbation Î´based on the metric dto the input data. The goal is to find the optimal Î¸that minimizes
risk even under the worst-case perturbation quantity:
Radv
Î´(P, Î¸) =E
vâˆ¼P"
sup
dp(v,v+âˆ†)â‰¤Î´â„“(v+ âˆ†, y, Î¸)#
, (3)
where pâˆˆ[0,âˆž]. This formulation ensures that the optimization considers the maximum potential
loss within the defined perturbation bounds.
Incounterfactually robust optimization [40,37,64,23,24], the uncertainty set is generated by
twins, which are obtained by creating counterfactuals concerning all levels of the sensitive attribute.
In this scenario, the worst-case loss quantity is obtained by calculating the maximum loss over the
twins of the input data:
Rcf
Î´(P, Î¸) =E
vâˆ¼P
sup
aâˆˆAâ„“(Â¨va, y, Î¸)
.
Distributionally Robust Optimization [43,55] is a data-driven approach designed to minimize the
discrepancies between in-sample and out-of-sample expected losses, using ambiguity sets based on
Wasserstein distances. Consider a lower semi-continuous cost function c(Â·,Â·) :Z Ã— Z â†’ [0,âˆž]that
satisfies c(z, z) = 0 for all zâˆˆ Z, serving as a fair metric. The optimal transport cost between two
distributions P,Qâˆˆ P(Z), is represented by:
Wc,p(P,Q)â‰œ min
Ï€âˆˆP(ZÃ—Z )(
E
(z,zâ€²)âˆ¼Ï€[cp(z, zâ€²)]1
p
:Ï€1=P, Ï€2=Q)
,
Here, Ï€âˆˆ P(Z Ã— Z )denotes the set of all joint probability distributions, and Ï€1andÏ€2are the
marginals of Ï€under first and second coordinates [ 54,63]. When c(z, zâ€²)acts as a metric (in
mathematics term) on Z,Wc,pis called the Wasserstein distance [63].
An important ingredient in the DRO formulation is the description of the distributional uncertainty
region BÎ´(P)that is defined by optimal transport cost:
BÎ´(P):={Qâˆˆ P(V) :Wc,p(Q,P)â‰¤Î´}.
DRO problem minimizes worst-case loss quantity:
RÎ´(P, Î¸)â‰œ sup
QâˆˆBÎ´(P)
EQ[â„“(Z, Î¸)]	
, (4)
and obtained the Î¸dro
Nâˆˆarg min Î¸âˆˆÎ˜RÎ´(PN, Î¸). The main tool in DRO is the strong duality
theorem [26,46], which converts an infinite-dimensional problem into a finite optimization problem.
The theorem states that:
sup
QâˆˆBÎ´(P)
E
vâˆ¼Q[Ïˆ(v)]
= inf
Î»â‰¥0n
Î»Î´p+E
vâˆ¼P[ÏˆÎ»(v)]o
, (5)
where ÏˆÎ»(v)is defined as ÏˆÎ»(v):= supvâ€²âˆˆV{Ïˆ(vâ€²)âˆ’Î»dp(v, vâ€²)}.
3 Causally Fair Dissimilarity Function
The key to robust optimization and individual fairness is the metric that measures individual similarity.
This section outlines the properties of such a metric in a causal framework to protect sensitive
attributes. We begin with an illustrative example.
Example 1 LetM1andM2represent two SCMs describing the relationships among the variables
gender ( G), education ( E), and income ( I).M1models these variables as independent, whereas
M2specifies a linear causal relationship:
M1=ï£±
ï£²
ï£³G:=UG,UGâˆ¼ B(0.5)
E:=UE,UEâˆ¼ N(0,1)
I:=UI,UIâˆ¼ N(0,1),M2=ï£±
ï£²
ï£³G:=UG, UGâˆ¼ B(0.5)
E:=G+UE, UEâˆ¼ N(0,1)
I:=G+ 2E+UI,UIâˆ¼ N(0,1),
4Where UGrepresents the population distribution of gender, modeled by a Bernoulli distribution, while
UEandUIare intrinsic talents for academic and income achievements, respectively, modeled by
normal distributions. To compare individuals, letâ€™s consider the L1norm on non-sensitive attributes
(d(v, vâ€²) =|eâˆ’eâ€²|+|iâˆ’iâ€²|). If two individuals have less than a 0.1 unit difference, they are deemed
similar. Now, consider an individual with data v= (M,1,1). Based on experience, we expect
that a perturbation in educational talent by .05 units will not significantly alter this individualâ€™s
status. We model this perturbation with a shift intervention âˆ† = (0 , .05,0). In Model 1, the result
CF(v,âˆ†) = ( M,1.05,1)is considered similar to v. However, in Model 2, CF(v,âˆ†) = ( M,1.05,1.1)
results in a distance of d(v,CF(v,âˆ†)) = 0 .15, indicating dissimilarity. In the presence of causality,
one attribute can be amplified multiple times in the final feature space. Therefore, we need to control
our intuition of dissimilarity between the exogenous variables and the feature space.
To protect against gender bias, we need to ensure that people with the same intrinsic characteristics
but different genders behave similarly. This is modeled by a counterfactual change in gender.
In Model 1, CF(v, F) = ( F,1,1)shows no difference ( d(v,Â¨vF) = 0 ). However, in Model 2,
CF(v, F) = (F,0,âˆ’2)results d(v,Â¨vF) = 4 , which means that they are not similar.
The example 1 demonstrates that in the presence of causality and protected variables, the standard
lp-norm or any metric fails to accurately capture the intuition of similarity. In these scenarios, a
dissimilarity function should incorporate counterfactuals and uniformly control for non-sensitive
perturbations to effectively capture proximity. This approach is further elaborated in the following
definition. Before proceeding, we introduce some notation. For a vector voru, we define PA(Â·)and
PX(Â·)as the projections onto the sensitive and non-sensitive parts, respectively.
Definition 1 (Causally Fair Dissimilarity Function) Letd:V Ã— V â†’ [0,âˆž]be a dissimilarity
function defined on the feature space V, generated by a SCM M. LetAdenote a set of sensitive
attributes, and Irepresent their corresponding index within {1, . . . , n }. The metric is called a
causally fair dissimilarity function or CFDF if it adheres to the following properties:
â€¢Zero Dissimilarity for Twin Pairs: For any vâˆˆ V andaâˆˆA, the dissimilarity d(v,Â¨va)
between an instance and its twins is zero.
â€¢Guaranteed Similarity for Minor Perturbations: For every vâˆˆ V and any Î´ >0, there
exists an Ïµsuch that for any sufficiently small intervention ( âˆ¥âˆ†âˆ¥ â‰¤Ïµ) on the non-sensitive
attributes ( PA(âˆ†) = 0 ), the distance d(v,CF(v,âˆ†))remains less than Î´.
To understand the shape of dunder the assumptions of Def. 1, we must first recognize that the CFDF
needs to be defined on a larger space than Range( M)[22]. This is because, generally, when Mis in-
tervened upon by some sensitive attribute level a, we have Range( M)âŠ†S
aâˆˆARange( Mdo(A:=a)).
The complete space encompassing all counterfactual values can be defined as follows.
Definition 2 (Parent-Free Sensitive Attribute SCM) Consider Mwith sensitive attributes in-
dexed by I. The parent-free sensitive attribute SCM denoted as M0, is derived from Mby removing
the causal effects of parents of sensitive attributes and replacing their exogenous variables with
indigenous ones. The structural equations for M0are as follows:
V0
i:=(
Ui Ui:=Viâˆ¼PVi, iâˆˆ I
fi(V0
pa(i)) +UiUiâˆ¼PUi, i / âˆˆ I
The exogenous space corresponding to M0, denoted by U0, includes the sensitive attributes and the
non-sensitive parts of the exogenous variables of M. This space called the semi-latent space, is
constructed as U0=A Ã— U X, where UXis the non-sensitive part of the exogenous space in M.
If we know the structural equations of M, we can first map the CFDF to the exogenous space. In
this space, the exogenous variables are assumed to be independent. Therefore, we can design a
dissimilarity function for each variable separately and then combine them using product topology
(Â§.2 [48]). Following this intuition, we introduce the bijective map g:V â†’ U 0from the feature space
to the semi-latent space, along with its inverse, defined as follows:
gi(v):=vi iâˆˆ I
Fi(v)i /âˆˆ I, gâˆ’1
i(u):=(
ui iâˆˆ I
fi(gâˆ’1
pa(i)(u)) +uii /âˆˆ I(6)
5If all sensitive attributes have no parents, the semi-latent space is equivalent to the exogenous space,
andg=Fâˆ’1. The counterfactual with respect to M0is denoted by CF0(v,âˆ†). We can now present
the following proposition to determine the shape of d.
Proposition 1 LetMbe an ANM, with gas its corresponding map to the semi-latent space 6 , and
PX(u)the projection of vector uto the non-sensitive part UX. Then:
(i) IfdXis a continuous dissimilarity function on diagonal UXÃ— UX, then the function ddefined as:
d(v, vâ€²) =dX(PX(g(v)), PX(g(vâ€²))) (7)
satisfies the definitions of a CFDF .
(ii) If d:V Ã— V â†’ [0,âˆž]satisfies the CFDF definition and the triangle inequality property, then d
can be represented as a dissimilarity function dXdependent solely on the non-sensitive components
UXi.e.,d(v, vâ€²) =dX(PX(g(v)), PX(g(vâ€²))).
Since dXis defined on independent coordinates, its relation to the components is less complex than
the CFDF d. We assume the dissimilarity function dX(x, xâ€²)is translation-invariant. Therefore, for
simplicity, we assume dX(xâ€², x) =âˆ¥xâ€²âˆ’xâˆ¥. The dual of âˆ¥ Â· âˆ¥ is defined as âˆ¥xâˆ¥âˆ—= supxâ€²{xTxâ€²|
âˆ¥xâ€²âˆ¥ â‰¤1}. Now we establish our assumptions about the SCM and its CFDF.
Assumption 1 (i)Mis an ANM with known structural equations and a semi-latent map g.
(ii) The CFDF is defined as d(v, vâ€²) =âˆ¥PX(g(v))âˆ’PX(g(vâ€²))âˆ¥, where âˆ¥.âˆ¥is a some norm.
(iii) Cost function over Zhas form c((v, y),(vâ€², yâ€²)) =d(v, vâ€²) +âˆž Â· |yâˆ’yâ€²|.
(iv) The ambiguity set is defined as: BÎ´(P) ={Qâˆˆ P(V) :Wc,p(P,Q)â‰¤Î´}, forpâˆˆ[1,âˆž).
Remark 1 All results of this work apply to the homogeneous dissimilarity function (Def. 6), which
includes a broad family of dissimilarity functions, such as norms.
4 Causally Fair Distributionally Robust Optimization
To find out the impact of the CFDF in DRO problems, we first consider the dual form of the worst-case
loss quantity, which simplifies the infinite-dimensional primal problem into a more tractable and
computationally manageable form.
Theorem 1 (Causally Fair Strong Duality) If Assumption 1 is satisfied, then for any reference
probability distribution Pand any function Ïˆ:V â†’Rthat is both upper semi-continuous and
L1-integrable, the following duality holds:
sup
QâˆˆBÎ´(P)
E
vâˆ¼Q[Ïˆ(v)]
= inf
Î»â‰¥0
Î»Î´p+E
vâˆ¼P
sup
aâˆˆAÏˆÎ»(Â¨va)
, (8)
where ÏˆÎ»(v)is defined as
ÏˆÎ»(v):= sup
âˆ†âˆˆX{Ïˆ(CF0(v,âˆ†))âˆ’Î»pd(v,CF0(v,âˆ†))}, (9)
andCF0is counterfactual regarding parent-free SCM M0.
Remark 2 The intuition behind the above formula is as follows: In the case where all features are
independent, let v= (a, x). The CFDF should exhibit no difference between (a, x)and(aâ€², x)for
eacha, aâ€²âˆˆ A. Consequently, the distance metric satisfies d((a, x),(aâ€², xâ€²)) = dX(x, xâ€²). Under
this condition, the classical strong duality theorem (Eq. 5) provides the following relationship:
ÏˆÎ»(v) = sup
(aâ€²,xâ€²)âˆˆV{Ïˆ((aâ€², xâ€²))âˆ’Î»dp
X(x, xâ€²)}= sup
aâˆˆA
sup
âˆ†âˆˆXÏˆ((a, x+ âˆ†)) âˆ’Î»dp
X(x, x+ âˆ†)
When we incorporate causal structure instead of coordinating aandx, the two dimensions Â¨vaand
CF0(v,âˆ†)are replaced accordingly.
6In the DRO formulation, the worst-case loss is expressed in a dual form and can act as a regularizer
for parameter learning. Explicitly solving the dual problem eliminates the need to compute the
worst-case distribution, resulting in faster, more efficient learning algorithms [ 14,16,62,73]. Before
presenting the general theorem, the next two theorems show that, under mild conditions, the dual
formula for specific loss functions in classification and regression problems can be explicitly solved.
Theorem 2 (Higher Order Linear Loss) Given Assumptions 1, let Mbe a linear SCM and the
loss function â„“(z, Î¸)p, where â„“(z, Î¸)is of the form h(yâˆ’ âŸ¨Î¸, vâŸ©)orh(yÂ· âŸ¨Î¸, vâŸ©)for functions h(t)
such as |t|,max(0 , t),|tâˆ’Ï„|, ormax(0 , tâˆ’Ï„)for some Ï„â‰¥0, and pâˆˆ[1,âˆž). Then the DRO
problem 4 can be reduced to:
RÎ´(PN, Î¸) =ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³
Rcf
Î´(PN, Î¸)1
p+Î´PX(MTÎ¸)
âˆ—p
, diam (A)<âˆž

R(PN, Î¸)1
p+Î´PX(MTÎ¸)
âˆ—p
,s.t.PA(MTÎ¸) = 0; diam (A) =âˆž
where Mis the corresponding matrix for the linear map gâˆ’1(see Eq. 6).
Remark 3 In real-world datasets, the sensitive part always satisfies diam (A)<âˆž. According to
the above theorem, RÎ´(PN, Î¸)â‰¥ Rcf
Î´(PN, Î¸). For practical applications, if the worst-case loss must
not exceed a certain value, we can replace âˆžwith some constant in the above theorem.
Example 2 Here are specific examples of the above theorem. We offer a framework to study the
equivalence between the worst-case loss in the DRO problem, with the cost function derived from the
CFDF , and the regularization scheme for classification and regression problems.
Regression Lower Partial Moments
EP[|Yâˆ’ âŸ¨Î¸,VâŸ©|p], pâ‰¥1 E P[(Yâˆ’ âŸ¨Î¸,VâŸ© âˆ’Ï„)p
+], pâ‰¥1, Ï„âˆˆR
Ridge Linear Regression Ï„-Insensitive Regression
EP[(Y+âŸ¨Î¸,VâŸ©)2] E P[(|Yâˆ’ âŸ¨Î¸,VâŸ©| âˆ’Ï„)p
+],pâ‰¥1,Ï„âˆˆR
Hinge Loss Binary Classification Support Vector Machine Classification
EP[(1âˆ’YÂ· âŸ¨Î¸,VâŸ©)p
+],pâ‰¥1 E P[|1âˆ’YÂ· âŸ¨Î¸,VâŸ©|p],pâ‰¥1
The Thm. 2 can be extended to the non-linear regression loss function.
Theorem 3 (Nonlinear Loss) Let assumptions 1 be satisfied, with p= 1,Mlinear with matrix M
corresponding to map gâˆ’1, and a loss function â„“(z, Î¸)of the form h(yâˆ’ âŸ¨Î¸, vâŸ©)for regression and
h(yÂ· âŸ¨Î¸, vâŸ©)for classification, where hhas the following two properties:
(i)his Lipschitz on RwithLhconstant, i.e., |h(t2)âˆ’h(t1)| â‰¤Lh|t2âˆ’t1|,âˆ€t1, t2âˆˆR.
(ii)There exists sequence of {tk}âˆž
k=1goes to âˆžsuch that for each t0âˆˆRwe have
limkâ†’âˆž|h(t0+tk)âˆ’h(t0)|
|tk|=Lh.
By the above assumption, DRO problem 4 can be reduced as:
RÎ´(PN, Î¸) =ï£±
ï£´ï£²
ï£´ï£³Rcf
Î´(PN, Î¸) +Î´LhPX(MTÎ¸)
âˆ—, diam(A)<âˆž
R(PN, Î¸) +Î´LhPX(MTÎ¸)
âˆ—,s.t.PA(MTÎ¸) = 0; diam(A) =âˆž
Example 3 The following forms of the loss function satisfy the conditions of hin Thm. 3:
Now, the first-order estimation of the regularizer for non-linear SCM and loss function is ready to be
stated.
Theorem 4 (First-Order Estimation of DRO Regularizer) Assume Mhas structural equation f,
which fand loss function â„“are both twice continuously differentiable respect to non-sensitive
7Log-cosh Loss Huber Loss
h:t7â†’log(cosh( t)) h:t7â†’1
2t2if|t| â‰¤1,
|t| âˆ’1
2otherwise ;
Quantile Loss Log-exponential Loss
h:t7â†’Î³t iftâ‰¥0,
âˆ’totherwise ,withÎ³âˆˆ(0,1); h:t7â†’log(1 + exp( âˆ’t))
Smooth Hinge Loss Truncated Pinball Loss
h:t7â†’ï£±
ï£²
ï£³0 iftâ‰¥1,
1
2(1âˆ’t)2if0< t < 1,
1
2âˆ’t otherwise ;h:t7â†’ï£±
ï£²
ï£³1âˆ’t iftâ‰¤1,
Ï„1(tâˆ’1) if1< t < Ï„ 2+ 1,
Ï„1Ï„2 otherwise ,
where Ï„1âˆˆ[0,1], Ï„2â‰¥0are two given constants.
attributes, diam (U)<âˆžandcsatisfies the assumption 1 with pâˆˆ[2,âˆž]. The necessary condition
for the existence of a finite DRO solution is that for each vâˆˆ V:
sup
aâˆˆA{â„“(Â¨va, y, Î¸)}<âˆž.
By these conditions, the worst-case loss quantity is equal to:
RÎ´(PN, Î¸) =E
vâˆ¼PN
sup
aâˆˆAâ„“(Â¨va, y, Î¸)
+Î´Â·
E
vâˆ¼PN
sup
aâˆˆAâˆ¥âˆ‡CFâ„“(Â¨va, y, Î¸)âˆ¥q
âˆ—1/q
+O(Î´2),(10)
where the O(Î´2)term is uniform over all Î¸âˆˆÎ˜,qispâ€™s conjugate, and the gradient âˆ‡CFâ„“equals to:
âˆ‡CFâ„“(v, y, Î¸ ) = lim
âˆ†â†’0â„“(CF0(v,âˆ†), y, Î¸)âˆ’â„“(v, y, Î¸ )
âˆ¥âˆ†âˆ¥
where CF0is counterfactual regarding parent-free SCM M0.
By applying Prop. 2 from Gaoâ€™s work [ 28], the next proposition presents the relationship between
classical adversarial optimization 3 and DRO for CFDF.
Proposition 2 (Approximation by Robust Optimization) Suppose Ais a finite set and let
{(vi, yi)}N
i=1be observational data. Under Assumption 1, assume that for the loss function â„“
there exist constants L, Mâ‰¥0such that
|â„“(v, y, Î¸ )âˆ’â„“(vâ€², y, Î¸)|< Ldp(v, vâ€²) +M for all v, vâ€²âˆˆ V andpâˆˆ[1,âˆž).
For an arbitrary KâˆˆN, consider the adversarial loss within the setting:
ËœRadv
Î´(PN) := sup
(wik)i,kâˆˆËœBÎ´(
1
NKNX
i=1KX
k=1sup
aâˆˆAâ„“( Â¨wik
a, yi, Î¸))
,
where the uncertainty set ËœBÎ´is defined as:
ËœBÎ´:=(
(wik)i,k:1
NNX
i=1KX
k=1dp(vi, wik)â‰¤Î´, wikâˆˆ V)
.
Then, the DRO can be approximated by adversarial optimization as follows:
ËœRadv
Î´(PN)â‰¤ R Î´(PN)â‰¤ËœRadv
Î´(PN) +LD+M
NK,
where Dis independent of K.
One of the main challenges in designing DRO for SCMs is that the CFDF depends on the causal
structure. When the functional structure is unknown, it must be estimated from data. This empirical
estimation impacts the DRO learning process. Therefore, it is crucial to control the uniform conver-
gence error of the DRO problem between the true metric and distribution and the DRO estimated
from the data. The following theorem guarantees learning from sample data, but certain assumptions
need to be established first.
8Assumption 2 (i)Mis an unknown ANM, diam (V)<âˆž, and Î˜is a compact subset of Rd.
(ii)The loss function â„“is uniformly bounded: there exists a positive constant Msuch that
0â‰¤â„“(z, Î¸)â‰¤Mfor all Î¸âˆˆÎ˜. Moreover, â„“is Lipschitz with respect to the counterfactual
inM0; that is, there exists a constant Lsuch that:
|â„“(v, y, Î¸ )âˆ’â„“(CF0(v,âˆ†), y, Î¸)| â‰¤ âˆ¥â„“âˆ¥Lip).
(iii) Ë†dis an estimation of the CFDF such that, with probability 1âˆ’Ïµ, there exists Mdsuch that,
at a rate of Nâˆ’Î·, the discrepancy is uniformly bounded by:
âˆ€v, vâ€²âˆˆ V:|d(v, vâ€²)âˆ’Ë†d(v, vâ€²)| â‰¤MdNâˆ’Î·,for some Î· >0.
The following theorem states that the efforts to estimate the metric or causal structures and the
parameter Ë†Î¸dro
N,
Ë†Î¸dro
N:= inf
Î¸âˆˆÎ˜(
sup
Q:WË†c,p(Q,PN)â‰¤Î´E
zâˆ¼Q[â„“(z, Î¸)])
Where Ë†cis the Ë†dcorresponding cost on Z, leading to the estimation of the true parameters of the
DRO problem. To state our result, we need the Dudley entropy integral [ 61], which measures the
complexity of the loss function class.
Theorem 5 (Learning Finite Sample Guarantee) With assumption 1 and 2, then for Ë†Î¸dro
Nwe have:
RÎ´(Pâˆ—,Ë†Î¸dro
N)âˆ’inf
Î¸âˆˆÎ˜RÎ´(Pâˆ—, Î¸)â‰¤Nâˆ’1/2h
c0+c1Î´1âˆ’p+c2Î´1âˆ’pNâˆ’Î·+1/2+c3p
log(2/Ïµ)i
,
With probability at least 1âˆ’2Ïµ. With C(L)denoting the Dudley entropy integral for the function
class{â„“(Â·, Î¸) :Î¸âˆˆÎ˜}, the constants c0,c1andc2are identified as follows:
c0:= 96C(L), c1:= 96LÂ·diam (V)p, c2:= 2pLÂ·diam (V)pâˆ’1Â·Md,andc3:= 2âˆš
2Ã—M.
The final theorem completes our framework, enabling us to perform DRO on real-world datasets
without knowing the SCM structures while providing performance bounds.
4.1 Related Works
Causally Fair Dissimilarity Function. Various studies have addressed the specification and learning
of individual fair metrics, such as [ 33,68,70,47], but their construction based on causal structure
and sensitive attributes remains unclear. Our work adopts and extends the concept of a causal fair
metric, as discussed in the works [23, 24].
DRO and Individual Fairness. Previous works, such as [ 68,70,47], address the DRO problem with
an individual fairness metric but are limited to linear SCMs and p= 2. These studies do not discuss
the duality theorem or regularizers. Additionally, [ 42] studied DRO, but its connection to causality
remains unclear.
Strong Duality Theorem. Various versions of the strong duality theorem have been explored in
prior works. For instance, in [ 59,46,9,14,27,28,66], the cost function must be a metric or [ 74]
has convex property. Additionally, in [ 72,11,58], the distance function dmust be positive-definite,
meaning d(v, vâ€²) = 0 if and only if v=vâ€². However, these conditions are not met for CFDF,
necessitating a new formulation of the duality theorem 1.
DRO as Regularizer. Previous works on using DRO as a regularizer, explicitly solved [ 60,14,16,29]
or through k-order estimation [ 5,9,6,66,27], only consider cases where the cost function is derived
from a metric or a positive-definite dissimilarity function. Therefore, their theorems do not apply
directly to our CFDF. We present new results in Theorems 2, 3, and 4 tailored for our cases.
Finite Sample Guarantee. Various works provide bounds on the performance of DRO solutions
with finite samples [ 41,25,10,12], but these do not apply to our CFDF due to previously mentioned
reasons. The studies [ 68,70,47] offer performance bounds only for the case of linear SCMs with
p= 2. Therefore, we present a general case in Theorem 5.
Optimal Transport and Causality. Recent works [ 39,35,13,32,21,1,2] on causal optimal
transport focus on the causal structure of the transport map or plan, which differs from our problem.
In our case, causality pertains to the transportation cost derived from SCMs.
95 Numerical Studies
In our numerical studies, we evaluate the impact of using causally fair DRO to mitigate individual
unfairness, henceforth referred to as CDRO . We compare CDROâ€™s performance against Empirical
Risk Minimization (ERM), non-causal Adversarial Learning (AL) [ 44], and the Ross method [ 56].
Our experiments employ real-world datasets, namely the Adult [ 38] and COMPAS [ 65] datasets,
pre-processed according to [ 19]. Additionally, we use a synthetic dataset for linear SCM (LIN) with
formulations detailed in Appendix C.1. We first fit a linear structural equation model for both the
AdultCOMPASReal-world Data
AL
CDROERMROSSAL
CDROERMROSS0%10%20%30%Unfair Area (D =0.05)LINSynthetic Data
AL
CDROERMROSS0%10%20%30%40%
AdultCOMPASReal-world Data
AL
CDROERMROSSAL
CDROERMROSS0%20%40%60%80%AccuracyLINSynthetic Data
AL
CDROERMROSS0%20%40%60%
Figure 1: Displays the findings from our numerical experiment, assessing the performance of DRO across
different models and datasets. (left) Bar plot showing the comparison of models based on the unfair area
percentage (lower values are better) for âˆ† =.05. (right) Bar plot comparing methods by prediction accuracy
performance (higher values are better).
Adult and COMPAS datasets. Logistic regression is employed for classification, and performance
is evaluated based on accuracy. Fairness is assessed using the Unfair Area Index (UAI), which is
defined by the following equation:
Uâˆ†:=P 
{vâˆˆ V:âˆƒvâ€²âˆˆ V s.t. d(v, vâ€²)â‰¤âˆ†âˆ§h(v)Ì¸=h(vâ€²)}
.
We evaluate UAI across different âˆ†values, specifically 0.05 and 0.01. Additionally, we calculate
the UAI for scenarios where no sensitive attributes are considered, representing the percentage of
non-robust data. Detailed computational experiment procedures are provided in Section C.1.
Our experiments, conducted using 100 different seeds, are summarized in Table 1. Figures 1, 2 and 3
illustrate that the CDRO method achieves a lower unfair area ( Uâˆ†) forâˆ† =.05, and âˆ† = 0 .01in all
scenarios. Although CDRO shows slightly lower accuracy than ERM, this trade-off is a common
observation in several studies [52]. Additional results can be found in Â§ C.6.
6 Discussion and Limitations
Our study introduces a novel framework for causally fair DRO, integrating causal structures and
sensitive attributes into the DRO paradigm. This framework is supported by several theoretical
advancements, including a strong duality theorem, explicit regularizer formulation, first-order regu-
larizer estimation, and finite sample guarantees with unknown SCMs, enhancing its efficiency and
practicality for real-world applications. Our experimental results demonstrate its effectiveness in
various settings, highlighting its potential for mitigating biases in machine learning models.
Despite the promising results, our study has several limitations that warrant further investigation.
Firstly, the assumption of an additive noise model may not capture the complexity of all real-
world causal relationships, posing challenges in computing additive interventions in general SCMs.
Secondly, while Theorem 2 and Theorem 3 could be extended to more general cases, we omitted these
extensions to avoid complexity. Lastly, further work is needed to explore the relationship between
our method and causal optimal transport [13, 32].
Acknowledgments
The authors thank the Max Planck Institute for Intelligent Systems, TÃ¼bingen AI Center, for supporting
this project. Partial funding support was also provided by the Canada CIFAR AI Chair program.
10References
[1]Beatrice Acciaio, Julio Backhoff-Veraguas, and Anastasiia Zalashko. Causal optimal transport
and its links to enlargement of filtrations and continuous-time stochastic optimization. Stochastic
Processes and their Applications , 130(5):2918â€“2953, 2020.
[2]Julio Backhoff, Mathias Beiglbock, Yiqing Lin, and Anastasiia Zalashko. Causal transport in
discrete time and applications. SIAM Journal on Optimization , 27(4):2528â€“2562, 2017.
[3]Ryan S Baker and Aaron Hawn. Algorithmic bias in education. International Journal of
Artificial Intelligence in Education , pages 1â€“41, 2022.
[4]Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and machine learning: Limita-
tions and opportunities . MIT Press, 2023.
[5]Daniel Bartl, Mathias BeiglbÃ¶ck, and Gudmund Pammer. The wasserstein space of stochastic
processes. arXiv preprint arXiv:2104.14245 , 2021.
[6]Robert Bartlett, Adair Morse, Richard Stanton, and Nancy Wallace. Consumer-lending discrim-
ination in the fintech era. Journal of Financial Economics , 143(1):30â€“56, 2022.
[7]Dimitris Bertsimas, David B Brown, and Constantine Caramanis. Theory and applications of
robust optimization. SIAM review , 53(3):464â€“501, 2011.
[8]Reuben Binns. On the apparent conflict between individual and group fairness. In Proceedings
of the 2020 conference on fairness, accountability, and transparency , pages 514â€“524, 2020.
[9]Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and
applications to machine learning. Journal of Applied Probability , 56(3):830â€“857, 2019.
[10] Jose Blanchet, Jiajin Li, Sirui Lin, and Xuhui Zhang. Distributionally robust optimization and
robust statistics. arXiv preprint arXiv:2401.14655 , 2024.
[11] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.
Mathematics of Operations Research , 44(2):565â€“600, 2019.
[12] Jose Blanchet, Karthyek Murthy, and Nian Si. Confidence regions in wasserstein distributionally
robust estimation. Biometrika , 109(2):295â€“315, 2022.
[13] Patrick Cheridito and Stephan Eckstein. Optimal transport and wasserstein distances for causal
models. arXiv preprint arXiv:2303.14085 , 2023.
[14] Hong Chu, Meixia Lin, and Kim-Chuan Toh. Wasserstein distributionally robust optimization
and its tractable regularization formulations. arXiv preprint arXiv:2402.03942 , 2024.
[15] Hong TM Chu, Kim-Chuan Toh, and Yangjing Zhang. On regularized square-root regression
problems: distributionally robust interpretation and fast computations. The Journal of Machine
Learning Research , 23(1):13885â€“13923, 2022.
[16] Hong TM Chu, Kim-Chuan Toh, and Yangjing Zhang. On regularized square-root regression
problems: distributionally robust interpretation and fast computations. Journal of Machine
Learning Research , 23(308):1â€“39, 2022.
[17] Zac Cranko, Zhan Shi, Xinhua Zhang, Richard Nock, and Simon Kornblith. Generalised Lips-
chitz regularisation equals distributional robustness. In International Conference on Machine
Learning , pages 2178â€“2188. PMLR, 2021.
[18] Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women. In
Ethics of data and analytics , pages 296â€“299. Auerbach Publications, 2022.
[19] Ricardo Dominguez-Olmedo, Amir H Karimi, and Bernhard SchÃ¶lkopf. On the adversarial
robustness of causal algorithmic recourse. In International Conference on Machine Learning ,
pages 5324â€“5342. PMLR, 2022.
11[20] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science
conference , pages 214â€“226, 2012.
[21] Stephan Eckstein and Gudmund Pammer. Computational methods for adapted optimal transport.
The Annals of Applied Probability , 34(1A):675â€“713, 2024.
[22] Ahmad-Reza Ehyaei, Golnoosh Farnadi, and Samira Samadi. Causal fair metric: Bridging
causality, individual fairness, and adversarial robustness. arXiv preprint arXiv:2310.19391 ,
2023.
[23] Ahmad-Reza Ehyaei, Amir-Hossein Karimi, Bernhard SchÃ¶lkopf, and Setareh Maghsudi.
Robustness implies fairness in causal algorithmic recourse. In Proceedings of the 2023 ACM
Conference on Fairness, Accountability, and Transparency , pages 984â€“1001, 2023.
[24] Ahmad-Reza Ehyaei, Kiarash Mohammadi, Amir-Hossein Karimi, Samira Samadi, and Gol-
noosh Farnadi. Causal adversarial perturbations for individual fairness and robustness in
heterogeneous data spaces. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 38, 10, pages 11847â€“11855, 2024.
[25] Rui Gao. Finite-sample guarantees for wasserstein distributionally robust optimization: Break-
ing the curse of dimensionality. Operations Research , 71(6):2291â€“2306, 2023.
[26] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and
variation regularization. arXiv preprint arXiv:1712.06050 , 2017.
[27] Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and
variation regularization. Operations Research , 2022.
[28] Rui Gao and Anton Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. Mathematics of Operations Research , 48(2):603â€“655, 2023.
[29] Camilo AndrÃ©s GarcÃ­a Trillos and NicolÃ¡s GarcÃ­a Trillos. On the regularized risk of distribu-
tionally robust learning over deep neural networks. Research in the Mathematical Sciences ,
9(3):54, 2022.
[30] Milena A Gianfrancesco, Suzanne Tamang, Jinoos Yazdany, and Gabriela Schmajuk. Potential
biases in machine learning algorithms using electronic health record data. JAMA internal
medicine , 178(11):1544â€“1547, 2018.
[31] Melissa Hall, Laurens van der Maaten, Laura Gustafson, Maxwell Jones, and Aaron Adcock. A
systematic study of bias amplification. arXiv preprint arXiv:2201.11706 , 2022.
[32] Bingyan Han. Distributionally robust risk evaluation with a causality constraint and structural
information. arXiv preprint arXiv:2203.10571 , 2022.
[33] Christina Ilvento. Metric learning for individual fairness. In 1st Symposium on Foundations
of Responsible Computing (FORC 2020) . Schloss-Dagstuhl-Leibniz Zentrum fÃ¼r Informatik,
2020.
[34] Alexander Immer, Christoph Schultheiss, Julia E V ogt, Bernhard SchÃ¶lkopf, Peter BÃ¼hlmann,
and Alexander Marx. On the identifiability and estimation of causal location-scale noise models.
arXiv preprint arXiv:2210.09054 , 2022.
[35] Yifan Jiang. Duality of causal distributionally robust optimization: the discrete-time case. arXiv
preprint arXiv:2401.16556 , 2024.
[36] Olav Kallenberg and Olav Kallenberg. Foundations of modern probability , volume 2. Springer,
1997.
[37] Amir-Hossein Karimi, Bernhard SchÃ¶lkopf, and Isabel Valera. Algorithmic recourse: from
counterfactual explanations to interventions. In Proceedings of the 2021 ACM conference on
fairness, accountability, and transparency , pages 353â€“362, 2021.
12[38] Ronny Kohavi and Barry Becker. Uci adult data set. UCI Meachine Learning Repository , 5,
1996.
[39] Daniel KrÅ¡ek and Gudmund Pammer. General duality and dual attainment for adapted transport.
arXiv preprint arXiv:2401.11958 , 2024.
[40] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In
Advances in Neural Information Processing Systems , pages 4069â€“4079, 2017.
[41] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances.
Advances in Neural Information Processing Systems , 31, 2018.
[42] Peizhao Li, Ethan Xia, and Hongfu Liu. Learning antidote data to individual unfairness. In
International Conference on Machine Learning , pages 20168â€“20181. PMLR, 2023.
[43] Fengming Lin, Xiaolei Fang, and Zheming Gao. Distributionally robust optimization: A review
on theory and applications. Numerical Algebra, Control and Optimization , 12(1):159â€“212,
2022.
[44] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,
2017.
[45] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A
survey on bias and fairness in machine learning. ACM computing surveys (CSUR) , 54(6):1â€“35,
2021.
[46] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathemati-
cal Programming , 171(1):115â€“166, 2018.
[47] Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, and Yuekai Sun. Two simple
ways to learn individual fairness metrics from data. In International Conference on Machine
Learning , pages 7097â€“7107. PMLR, 2020.
[48] James R Munkres. Topology, 2nd edn of [mr0464128], 2000.
[49] Razieh Nabi and Ilya Shpitser. Fair inference on outcomes. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 32, 1, 2018.
[50] Arash Nasr-Esfahany, Mohammad Alizadeh, and Devavrat Shah. Counterfactual identifiability
of bijective causal models. In International Conference on Machine Learning , pages 25733â€“
25754. PMLR, 2023.
[51] Judea Pearl. Causality: Models, Reasoning, and Inference . Cambridge University Press, 2009.
[52] Dana Pessach and Erez Shmueli. A review on fairness in machine learning. ACM Computing
Surveys (CSUR) , 55(3):1â€“44, 2022.
[53] Jonas Peters, Dominik Janzing, and Bernhard SchÃ¶lkopf. Elements of causal inference: founda-
tions and learning algorithms . The MIT Press, 2017.
[54] Gabriel PeyrÃ©, Marco Cuturi, et al. Computational optimal transport. Center for Research in
Economics and Statistics Working Papers , 2017-86, 2017.
[55] Hamed Rahimian and Sanjay Mehrotra. Frameworks and results in distributionally robust
optimization. Open Journal of Mathematical Optimization , 3:1â€“85, 2022.
[56] Alexis Ross, Himabindu Lakkaraju, and Osbert Bastani. Learning models for actionable
recourse. Advances in Neural Information Processing Systems , 34:18734â€“18746, 2021.
[57] Anian Ruoss, Mislav Balunovic, Marc Fischer, and Martin Vechev. Learning certified individu-
ally fair representations. In Advances in Neural Information Processing Systems , 2020.
13[58] Soroosh Shafieezadeh-Abadeh, Liviu Aolaritei, Florian DÃ¶rfler, and Daniel Kuhn. New per-
spectives on regularization and computation in optimal transport-based distributionally robust
optimization. arXiv preprint arXiv:2303.03900 , 2023.
[59] Soroosh Shafieezadeh-Abadeh, Daniel Kuhn, and Peyman Mohajerin Esfahani. Regularization
via mass transportation. Journal of Machine Learning Research , 20(103):1â€“68, 2019.
[60] Soroosh Shafieezadeh Abadeh, Peyman M Mohajerin Esfahani, and Daniel Kuhn. Distribu-
tionally robust logistic regression. Advances in Neural Information Processing Systems , 28,
2015.
[61] Michel Talagrand. Upper and lower bounds for stochastic processes , volume 60. Springer,
2014.
[62] Peipei Tang, Chengjing Wang, Defeng Sun, and Kim-Chuan Toh. A sparse semismooth newton
based proximal majorization-minimization algorithm for nonconvex square-root-loss regression
problem. The Journal of Machine Learning Research , 21(1):9253â€“9290, 2020.
[63] CÃ©dric Villani et al. Optimal transport: old and new , volume 338. Springer, 2009.
[64] Julius V on KÃ¼gelgen, Amir-Hossein Karimi, Umang Bhatt, Isabel Valera, Adrian Weller, and
Bernhard SchÃ¶lkopf. On the fairness of causal algorithmic recourse. In Proceedings of the
AAAI conference on artificial intelligence , volume 36, 9, pages 9584â€“9594, 2022.
[65] Anne L Washington. How to argue with an algorithm: Lessons from the compas-propublica
debate. Colo. Tech. LJ , 17:131, 2018.
[66] Qinyu Wu, Jonathan Yu-Meng Li, and Tiantian Mao. On generalization and regularization via
wasserstein distributionally robust optimization. arXiv preprint arXiv:2212.05716 , 2022.
[67] Samuel Yeom and Matt Fredrikson. Individual fairness revisited: transferring techniques
from adversarial robustness. In Proceedings of the Twenty-Ninth International Conference on
International Joint Conferences on Artificial Intelligence , 2021.
[68] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with
sensitive subspace robustness. arXiv preprint arXiv:1907.00020 , 2019.
[69] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with
sensitive subspace robustness. In International Conference on Learning Representations , 2020.
[70] Mikhail Yurochkin and Yuekai Sun. Sensei: Sensitive set invariance for enforcing individual
fairness. In International Conference on Learning Representations , 2021.
[71] K Zhang and A HyvÃ¤rinen. On the identifiability of the post-nonlinear causal model. In 25th
Conference on Uncertainty in Artificial Intelligence (UAI 2009) , pages 647â€“655. AUAI Press,
2009.
[72] Luhao Zhang, Jincheng Yang, and Rui Gao. A simple and general duality proof for wasserstein
distributionally robust optimization. arXiv preprint arXiv:2205.00362 , 2022.
[73] Yangjing Zhang, Ning Zhang, Defeng Sun, and Kim-Chuan Toh. An efficient hessian based
algorithm for solving large-scale sparse group lasso problems. Mathematical Programming ,
179:223â€“263, 2020.
[74] Jianzhe Zhen, Daniel Kuhn, and Wolfram Wiesemann. A unified theory of robust and dis-
tributionally robust optimization via the primal-worst-equals-dual-best principle. Operations
Research , 2023.
14A Supplementary Theoretical Details
Notations. In this work random variables are denoted by bold letters (e.g., V), their corresponding
probability spaces by calligraphic letters (e.g., V), and instances by normal letters (e.g., v). The space
of probability measures on Vis represented by P(V)and probability measures by blackboard bold
letters (e.g., P).
Non-Sensitive Part. LetF:U â†’ V be the reduced-form map of M. The vector vdecomposes
into sensitive and non-sensitive parts, v= (a, x), and we have a corresponding decomposition in the
exogenous space denoted by u= (ua, ux)andUA,UXare corresponding spaces. Using the ANM
model, we can assume that both VandUare equivalent, and therefore, the non-sensitive feature
space is the same as the non-sensitive part of the exogenous space. If Pis a probability measure in
P(V), then (P)Xrefers to the marginal probability over the non-sensitive part. We also refer to Q)X
for marginal probability over the non-sensitive part of the exogenous space.
Definition 3 (Push-forward Measure) LetPâˆˆ P(V),Qâˆˆ P(U)be two probability measures and
T:V â†’ U is map, the measure Qis called the push-forward of Pthrough Tis denoted by T#Pif:
Q(B) =P(Tâˆ’1(B)),âˆ€BâŠ‚ U
Definition 4 (Set of Couplings) The set Î“(P,Q)represents the couplings of probability distributions
Pâˆˆ P(V),Qâˆˆ P(U), comprising distributions over V Ã— U with margins PandQ. A measure Ï€
belongs to Î“(P,Q)if and only if
Ï€(AÃ— U) =P(A)and Ï€(V Ã—B) =Q(B)âˆ€AâŠ‚ V, BâŠ‚ U
By extension, a random pair (X, Y)âˆ¼Ï€, where Ï€âˆˆÎ“(P,Q), will also be called a coupling of P
andQ.
Definition 5 (Diameter of a Set) LetAbe a set in a metric space with a distance function d. The
diameter ofA, denoted diam (A), is defined as:
diam(A) = sup {d(x, y) :x, yâˆˆA}
where suprepresents the supremum of the set of distances d(x, y)for all pairs (x, y)inA.
Definition 6 (Homogeneous dissimilarity function) LetÎ›be an extended-valued function
Î›:X â†’ [0,âˆž]on a real vector space Xwith absolutely homogeneous assumption i.e. Î›(tx) =
|t|Î›(x)for any tâˆˆRandzâˆˆ X. In addition, Î›is proper it means there exists x0âˆˆ X such that
Î›(x0) = 1 . The cost function d:X Ã— X â†’ [0,âˆž]is called Homogeneous dissimilarity function if is
defined as d(xâ€², x):= Î›(xâ€²âˆ’x)for any xâ€², xâˆˆ X.
Lemma 1 IfMis an additive noise model with mutually independent exogenous variables, then the
parent-free sensitive M0attribute model retains both of these properties. Moreover, the map-reduced
form mapping of M0is equivalent to gâˆ’1, where grepresents the mapping to the semi-latent space.
Proof. First, M0is an additive noise model because its structure is derived from the initial equations
ofMby removing those equations related to the sensitive attributes and replacing the exogenous
variable UibyVi.
Regarding the mutual independence of the exogenous variables, in the original model M, the variables
Viforiâˆˆ Iare not independent of Ujforj /âˆˆ Iif they have parents. However, assuming a hard
intervention for each instance of Viâ€” where a do-action is executed for this intervention â€” it implies
that the intervened variable Vican be considered independent from the other variables. Therefore,
since we apply hard interventions to all sensitive variables, we can assume that Viforiâˆˆ I are
mutually independent, and also that Viare independent of Ujfor all j /âˆˆ I.
Finally, by referencing equations 6, it is observable that the map-reduced form mapping of M0is
equivalent to the inverse of the map to the semi-latent space.
Lemma 2 LetMbe an additive noise model with a mapping gto semi-latent space U0. Assume
Mincludes the sensitive attributes Aand other non-sensitive attributes Xthat belong to the vector
15spaceX. Consider v= (a, x)as an instance in M, and let âˆ†âˆˆ X represent a shift intervention
value. Then, the counterfactual corresponding to additive shit is obtained by:
PX(CF(v,âˆ†)) = PX(gâˆ’1(g(v) + (0 ,âˆ†))).
Moreover, if aâ€²âˆˆ A represents another level of sensitive attributes, then the hard intervention
concerning A:=aâ€²is achieved by:
Â¨vaâ€²=CF(v, do(A:=aâ€²)) =gâˆ’1((aâ€², PX(g(v)))).
Proof. In additive noise models, an additive intervention can be conceptualized as adding a value Î´
to the exogenous variables, while all structural equations remain unchanged. Consequently, during
such an intervention, the reduced-form mapping FMâˆ†of the intervened SCM remains unchanged.
Therefore by definition of intervention, it follows that:
CF(v, do(X+=âˆ†)) = FMâˆ†(Fâˆ’1(v) + (0 ,âˆ†)) = F(Fâˆ’1(v) + (0 ,âˆ†)).
Since Fandgâˆ’1are coincide in non-sensitive coordinates then PX(Ïˆ(Fâˆ’1(v) + (0 ,âˆ†))) =
PX(gâˆ’1(g(v) + (0 ,âˆ†))) and it completes the first part. In this case, we denote FasM, which
is an invertible matrix. Consequently, the counterfactual CF(v, do(X+=âˆ†)) can be expressed as
v+Mâˆ’1(0,âˆ†).
To prove the second part, when intervention is performed on sensitive attributes, Mtransforms into a
parent-free sensitive attribute model where the sensitive attribute ais replaced by aâ€². Given that the
map-reduced form of the parent-free sensitive attribute model aligns with gâˆ’1, and since gandFâˆ’1
coincide on the non-sensitive parts, the counterfactual can be expressed as follows:
Â¨vaâ€²=CF(v, do(A:=aâ€²)) =gâˆ’1((aâ€², PX(g(v)))).
B Proof Section
Proof of Proposition 1.
(i) If dadheres to Eq. 7, it means that for each vâˆˆ V , the mapping g(v) = ( a, x). For its
counterfactual Â¨va, we have g(Â¨vaâ€²) = (aâ€², x). Using Eq. 7, we can express:
d(v,Â¨va) =dX(PX(g(v)), PX(g(Â¨va))) = dX(x, x) = 0
This demonstrates that dretains the first property of Def. 1.
Additionally, since dXis continuous, for each xand any Ïµ >0, there exists a Î´ >0such that if
âˆ¥âˆ†âˆ¥< Î´, then dX(x, x+ âˆ†) < Ïµ. Referencing Lemma 2 and the formulation of d, it follows that for
âˆ¥âˆ†âˆ¥< Î´and for each aâˆˆ A:
d(v,CF(v,âˆ†)) = d(PX(g(v)), PX(g(CF(v,âˆ†)))) = dX(x, x+ âˆ†) < Ïµ
Thus, it satisfies property (ii) of the CFDF.
(ii) Letâ€™s consider a CFDF denoted as d:V Ã— V â†’ R, with an embedding g:V â†’ Q that
maps from the feature space to a semi-latent space. We define dâˆ—as the pull-back of dontoQ,
dâˆ—(q1, q2) =d(gâˆ’1(q1), gâˆ’1(q2))where dâˆ—is a dissimilarity function, and we aim to clarify which
properties it inherits from Def. 1. We utilize a decomposition of QintoA Ã— X , where q=gâˆ’1(v)
andvâˆˆ V, denoting qas(a, x). Property (i) of the CFDF ensures:
d(v,Â¨vaâ€²) =dâˆ—((a, x),(aâ€², x)) = 0 âˆ€aâ€²âˆˆ A
This property confirms that dâˆ—is insensitive to changes in the sensitive part A. To demonstrate,
consider any two points q1= (a1, x1)andq2= (a2, x2), with an arbitrary a0âˆˆ A. By triangle
property of dissimilarity function dit can be seen:
dâˆ—((a1, x1),(a2, x2))â‰¤dâˆ—((a1, x1),(a0, x1)) +dâˆ—((a0, x1),(a2, x2)) =â‡’
dâˆ—((a1, x1),(a2, x2))â‰¤dâˆ—((a0, x1),(a2, x2))
Here, dâˆ—((s1, x1),(s0, x1))is zero due to property (i). Similarly, we can argue:
dâˆ—((a0, x1),(a2, x2))â‰¤dâˆ—((a0, x1),(a1, x1)) +dâˆ—((a1, x1),(a2, x2)) =â‡’
dâˆ—((a0, x1),(a2, x2))â‰¤dâˆ—((a1, x1),(a2, x2))
16This results in dâˆ—((a1, x1),(a2, x2)) =dâˆ—((a0, x1),(a2, x2)). With similar reasoning, we have:
dâˆ—((a1, x1),(a2, x2)) =dâˆ—((a1, x1),(a0, x2)) =â‡’dâˆ—((a1, x1),(a2, x2)) =dâˆ—((a0, x1),(a0, x2))
Hence, dâˆ—is invariant to the sensitive subspace. If dXis the dissimilarity function induced by dâˆ—on
X, then dâˆ—((a1, x1),(a2, x2)) =dX(x1, x2). In accordance with Lemma 2, the second property of
Def. 1 states that for each Ïµ >0, there exists a Î´such that if |âˆ†|< Î´, then dX(x, x+ âˆ†) < Ïµ. This
property demonstrates the continuity of dXalong the diagonal.
Finally, dcan be embedded in semi-latent space and described by another dissimilarity function on it
that only depends on the non-sensitive part of exogenous space:
d(v, w) =dX(PX(g(v)), PX(g(w)))
This concludes the proof.
Lemma 3 (Transformation by a Bijective Map) Letg:V â†’ U be an invertible function and let
the transportation cost function cbe constructed by c(v, vâ€²) =d(g(v), g(vâ€²))where dis a metric on
the space U. For every P,Qâˆˆ P(V), the following equation holds:
Wc,p(P,Q) =Wd,p(g#P, g#Q)
where WcandWdrepresent the Wasserstein distances with respect to the metrics candd, respectively.
Proof. By the definition of the Wasserstein distance,
Wc,p(P,Q) = inf
Ï€âˆˆÎ“(P,Q)Z
VÃ—Vcp(v, vâ€²)dÏ€(v, vâ€²).
Substituting c(v, vâ€²) =d(g(v), g(vâ€²))andu=g(v)gives:
Wc,p(P,Q) = inf
Ï€âˆˆÎ“(P,Q)Z
VÃ—Vdp(g(v), g(vâ€²))dÏ€(v, vâ€²).
Consider a coupling Ï€ofPandQ. Define a measure ËœÏ€onUÃ—U byËœÏ€(AÃ—B) =Ï€(gâˆ’1(A)Ã—gâˆ’1(B)).
ËœÏ€is a coupling of g#Pandg#Qbecause:
ËœÏ€(AÃ— U) =Ï€(gâˆ’1(A)Ã— V) =g#P(A); ËœÏ€(U Ã—B) =Ï€(V Ã—gâˆ’1(B)) =g#Q(B).
Therefore, the p-Wasserstein distance for the push-forward measures is
inf
Ï€âˆˆÎ“(P,Q)Z
VÃ—Vdp(g(v), g(vâ€²))dÏ€(v, vâ€²) = inf
ËœÏ€âˆˆÎ“(g#P,g#Q)Z
UÃ—Udp(u, uâ€²)dËœÏ€(u, uâ€²)
=Wd,p(g#P, g#Q).
Since ËœÏ€arises from Ï€viag, and gis invertible and measure-preserving in this context, the values
in the integrals of the definitions of Wc,p(P,Q)andWd,p(g#P, g#Q)match. Thus, we have shown
thatWc,p(P,Q) =Wd,p(g#P, g#Q).
Lemma 4 (Optimal Transportation Cost on Subspace) LetU âŠ†Rnand suppose Uis decom-
posed into two subspaces, U= (A,X), where Acorresponds to the subset of some coordinates and
Xto its complements. Let PXdenote the projection function onto the Xspace, i.e., PX(u)projects
uâˆˆ U ontoXcomponents. Define a cost function c(u, uâ€²) =d(PX(u), PX(uâ€²)), where dis a cost
function on the space X. Consider probability measures P,Qâˆˆ P(U), and define PX=PX#P
andQX=PX#Qas the pushforward measures of PandQunder the projection PX, respectively,
placing them in P(X). LetÏ€âˆ—
Xbe the optimal transport plan concerning the Wasserstein distance
Wd(PX,QX). Then, any transport plan Ï€âˆˆ P(U Ã— U ), whose marginal distribution over X Ã— X
equals Ï€âˆ—
X, should also be an optimal solution for the Wasserstein distance Wc(P,Q)concerning the
cost function c.
Proof. Given any coupling Ï€âˆˆÎ“(P,Q), we consider elements u= (a, x)anduâ€²= (aâ€², xâ€²)in
U=A Ã— X . The cost function cis defined by c((a, x),(aâ€², xâ€²)) =d(x, xâ€²), where dis a metric on
17the space X. By definition of optimal transport cost Wc(P,Q)we have:
sup
Ï€âˆˆÎ“(P,Q)Z
UÃ—Uc((a, x),(aâ€², xâ€²))dÏ€
= sup
Ï€âˆˆÎ“(P,Q)Z
UÃ—Ud(x, xâ€²)dÏ€
=
sup
Ï€âˆˆÎ“(P,Q)Z
XÃ—XZ
AÃ—Ad(x, xâ€²)dÏ€((a, aâ€²)|X=x,Xâ€²=xâ€²)
d(Ï€)XÃ—X
=
sup
Ï€âˆˆÎ“(P,Q)Z
XÃ—Xd(x, xâ€²)d(Ï€)XÃ—X
= sup
Ï€âˆˆÎ“((P)X,(Q)X)Z
XÃ—Xd(x, xâ€²)Ï€
where (P)Xand(Ï€)XÃ—X is marginal distribution over XandX Ã— X respectively. Ï€(.|X=x,Xâ€²=
xâ€²)is conditional distribution of Ï€condition to the first and second Xcomponents equal to xandxâ€².
We observe that this integral effectively only depends on the Xcomponent since the cost function c
does not involve A. Hence, we reduce the expression to:
sup
Ï€âˆˆÎ“((P)X,(Q)X)Z
XÃ—Xd(x, xâ€²)Ï€
(11)
The Eq. 11 shows that the optima cost function of Wc(P,Q)equals to Wc((P)X,(Q)X). Therefore
ifÏ€âˆ—
Xbe the optimal transport plan for PXtoQXwith respect to donX, then any coupling Ï€in
U Ã— U that its marginal distribution (Ï€)XÃ—X equals Ï€âˆ—
Xis the solution of optimal transport. It results
that the conditional distribution Ï€((., .)|X=x,Xâ€²=xâ€²)could be any distribution. This completes
the proof.
Lemma 5 LetXandAbe sets, and let f:XÃ—Aâ†’Rbe a function. Then
sup
xâˆˆXsup
aâˆˆAf(x, a) = sup
aâˆˆAsup
xâˆˆXf(x, a).
Proof. Define:
L= sup
xâˆˆXsup
aâˆˆAf(x, a)and R= sup
aâˆˆAsup
xâˆˆXf(x, a).
To show that L=R, we need to prove that Lâ‰¤RandRâ‰¤L. Consider any xâˆˆXandaâˆˆA. By
definition, f(x, a)â‰¤supaâˆˆAf(x, a)for each fixed x. Therefore,
f(x, a)â‰¤sup
aâˆˆAf(x, a)â‰¤sup
xâˆˆXsup
aâˆˆAf(x, a) =R.
Since f(x, a)was arbitrary, we have:
sup
aâˆˆAf(x, a)â‰¤Rfor all xâˆˆX,
and thus,
L= sup
xâˆˆXsup
aâˆˆAf(x, a)â‰¤R.
Similarly, for any fixed aâˆˆA,f(x, a)â‰¤supxâˆˆXf(x, a). Hence,
f(x, a)â‰¤sup
xâˆˆXf(x, a)â‰¤sup
aâˆˆAsup
xâˆˆXf(x, a) =L.
As before, since f(x, a)was arbitrary, we conclude:
sup
xâˆˆXf(x, a)â‰¤Lfor all aâˆˆA,
and thus,
R= sup
aâˆˆAsup
xâˆˆXf(x, a)â‰¤L.
Since Lâ‰¤RandRâ‰¤L, it follows that L=R. Therefore, we have proven that:
sup
xâˆˆXsup
aâˆˆAf(x, a) = sup
aâˆˆAsup
xâˆˆXf(x, a).
This demonstrates the Principle of the Iterated Suprema.
18B.1 Proof of Theorem 1.
We prove the assertion in two steps: first, we assume that none of the sensitive attributes have parents,
and second, we address and prove the general case. When all sensitive attributes do not have parents,
in this case by definition2 semi-latent space equivalent with exogenous space and therefore g=Fâˆ’1.
First, we show that the worst-case loss quantity can be decomposed into sensitive and non-sensitive
components like as below equation:
sup
QâˆˆBÎ´(P)
E
vâˆ¼Q[Ïˆ(v)]
= sup
QâˆˆBÎ´((Fâˆ’1
#P)X)
E
uxâˆ¼Q
sup
uaâˆˆUA{Ïˆ(F((ua, ux)))}
. (12)
By the assumption, the CFDF has a form d(v, vâ€²) =dX(PX(Fâˆ’1(v)), PX(Fâˆ’1(vâ€²))). By Def. 2 in
a case that sensitive attributes have no parents then the semi-latent space coincides with exogenous
space and the map between feature space and semi-latent space equals g=Fâˆ’1. Therefore in the
following equations, we use ginstead of Fâˆ’1. Moreover since gis invertible by Lemma 3, we can
write:
sup
QâˆˆBÎ´(P)
E
vâ€²âˆ¼Q[Ïˆ(vâ€²)]
= sup
QâˆˆBÎ´(g#P)
E
uâ€²âˆ¼Q[Ïˆ(F(uâ€²))]
=
sup
Ï€âˆˆP(UÃ—U )
E
uâ€²âˆ¼Ï€2[Ïˆ(F(uâ€²))]uâˆ¼g#P,E
(u,uâ€²)âˆ¼Ï€[ËœdX(u, uâ€²)]â‰¤Î´
=
sup
Ï€âˆˆP(UÃ—U )
E
uâ€²âˆ¼Ï€2[Ïˆ(F((uâ€²
a, uâ€²
x)))]uâˆ¼g#P,E
(u,uâ€²)âˆ¼Ï€[dX(ux, uâ€²
x)]â‰¤Î´
=
sup
Ï€âˆˆP(UÃ—U )Z
UÏˆ(F((uâ€²
a, uâ€²
x)))dÏ€2(uâ€²)uâˆ¼g#P, E
(ux,uâ€²x)âˆ¼Ï€XÃ—X[dX(ux, uâ€²
x)]â‰¤Î´
=âˆ—,
where Ëœdbe a cost function on Udefined as Ëœd(u, uâ€²) =dX(PX(u), PX(uâ€²)),Ï€2denotes the marginal
distribution on second part and BÎ´(g#P) ={Qâˆˆ P(U) :WËœd(Q, g#P)â‰¤Î´}. Using the disintegra-
tion theorem ( [ 36] Chapter 3), the joint distribution Ï€2can be decomposed into the product of the
conditional distribution of UAgivenUXand the marginal distribution on UX. Therefore we have
Z
UÏˆ(F((uâ€²
a, uâ€²
x)))dÏ€2(uâ€²) =Z
UXZ
UAÏˆ(F((ua, ux)))dÏ€2(uâ€²
a|UX=uâ€²
x)
dX(Ï€2)X(uâ€²
x),
where (Ï€2)Xis the marginal distribution of Ï€2over the non-sensitive part and Ï€2(uâ€²
a|UX=uâ€²
x)
is a conditional distribution of the sensitive part of exogenous space condition by UX=uâ€²
x. By
disintegration formula, (*) can be rewritten as:
sup
Ï€2âˆˆP(U)Z
UÏˆ(F((uâ€²
a, uâ€²
x)))dÏ€2(uâ€²)Ï€âˆˆ P(U Ã— U ), Ï€1=g#P, E
(ux,uâ€²x)âˆ¼(Ï€)XÃ—X[d(ux, uâ€²
x)]â‰¤Î´
= sup
Ï€2âˆˆP(U)(Z
UXZ
UAÏˆ(F(ua, ux))dÏ€2(uâ€²
a|UX=uâ€²
x)
dX(Ï€2)X(uâ€²
x)
Ï€âˆˆ P(U Ã— U ), Ï€1=g#P, E
(ux,uâ€²x)âˆ¼(Ï€)XÃ—X[d(ux, uâ€²
x)]â‰¤Î´)
= sup
(Ï€2)XâˆˆP(UX)(Z
UXsup
Ï€2(.|UX=uâ€²x)nZ
UAÏˆ(F((uâ€²
a, uâ€²
x)))dÏ€2(uâ€²
a|UX=uâ€²
x)o
d(Ï€2)X(uâ€²
x)
Ï€âˆˆ P(U Ã— U ), Ï€2X= (g#P)X, E
(ux,uâ€²x)âˆ¼(Ï€)XÃ—X[dX(ux, uâ€²
x)]â‰¤Î´)
(13)
Since dXdepends only on the non-sensitive components, it follows from Lemma 4 that Ï€2(.|UX=
uâ€²
x)can achieve any distribution. Moreover, since it does not depend on the Wasserstein distance in
each coupling, the marginal distribution of the sensitive attribute can be considered independent of the
marginal distribution of the non-sensitive attributes. Therefore, the supremum over Ï€2(.|UX=uâ€²
x)of
integral equals the supremum of Ïˆ(F((uâ€²
a, uâ€²
x)))over all values of uâ€²
a. Furthermore, the distribution
19Ï€2(uâ€²
a|uâ€²
x)does not influence the value of the Wasserstein distance. Based on these points, the last
equation can be rewritten as:
sup
Ï€2âˆˆP(UX)(Z
UXsup
uâ€²aâˆˆUAÏˆ(F((uâ€²
a, uâ€²
x)))dÏ€2Ï€âˆˆ P(UXÃ— UX),
Ï€1= (g#P)X,E
(ux,uâ€²x)âˆ¼Ï€[d(ux, uâ€²
x)]â‰¤Î´)
=
sup
Ï€2âˆˆP(UX)(
E
uâ€²xâˆ¼Ï€2"
sup
uâ€²aâˆˆUAÏˆ(F((uâ€²
a, uâ€²
x)))#Ï€âˆˆ P(UXÃ— UX),
Ï€1= (g#P)X,E
(ux,uâ€²x)âˆ¼Ï€[d(ux, uâ€²
x)]â‰¤Î´)
=
sup
QâˆˆBÎ´((g#P)X)(
E
uâ€²xâˆ¼Q"
sup
uâ€²aâˆˆUA{Ïˆ(F((uâ€²
a, uâ€²
x)))}#)
.
The last equation concludes the proof of Eq. 12. Similarly, by altering the order of integration in
Eq. 13, we arrive at the following equation:
sup
QâˆˆBÎ´(P)
E
vâˆ¼Q[Ïˆ(v)]
= sup
uaâˆˆUA(
sup
QâˆˆBÎ´((g#P)X)
E
uxâˆ¼Q[Ïˆ(F((ua, ux)))])
. (14)
To proceed with the proof, we utilize the strong duality theorem. There are various kinds of duality
theorems for DRO, but we apply the one proposed by Blanchet et al. [11].
Strong duality [ 11].Suppose the transportation cost c:Z Ã— Z â†’ [0,âˆž]satisfies c(z, z) = 0 for
allzâˆˆ Z and lower semi-continuous. Then for any reference probability distribution Pand upper
semi-continuous Ïˆ:Z â†’Rsatisfying E
P[f(Z)]<âˆž, we have
sup
QâˆˆBÎ´(P)E
Q[Ïˆ(Z)] = inf
Î»â‰¥0Î»Î´+E
P[ÏˆÎ»(Z)], (15)
where ÏˆÎ»(z):= supzâ€²âˆˆZ{Ïˆ(zâ€²)âˆ’Î»c(z, zâ€²)}.
Based on the assumption about the CFDF, where only d(v,Â¨vaâ€²) = 0 , it follows that d(x, xâ€²) = 0 only
ifx=xâ€². Therefore, we can apply the duality theorem to Eq. 12. According to the duality theorem,
it can be expressed as follows:
sup
uaâˆˆUA(
sup
QâˆˆBÎ´((g#P)X)
E
uxâˆ¼Q[Ïˆ(F((ua, ux)))])
= inf
Î»â‰¥0
Î»Î´+ E
uxâˆ¼(g#P)X[Î·Î»(ux)]
(16)
where Î·Î»(ux) = supuâ€²xâˆˆUX
supuaâˆˆUA{Ïˆ(F((ua, uâ€²
x))} âˆ’Î»dX(ux, uâ€²
x)	
. By using lemma 5
supuâ€²
xâˆˆUX
supuaâˆˆUA{Ïˆ(F((ua, uâ€²
x))}	
= supuaâˆˆUAn
supuâ€²
xâˆˆUX{Ïˆ(F((ua, uâ€²
x))}o
.
Now, since sensitive attributes donâ€™t have parents then two spaces UAandAare equal. By applying
Lemma 2, we can replace the above equation with hard and soft interventions as follows:
Î·Î»(ux) = sup
aâˆˆA(
sup
uâ€²xâˆˆUX{Ïˆ(F((a, uâ€²
x)))âˆ’Î»dX(ux, uâ€²
x)})
=
sup
aâˆˆA
sup
âˆ†âˆˆUX{Ïˆ(F((a, ux+ âˆ†))) âˆ’Î»dX(ux, ux+ âˆ†)}
=
sup
aâˆˆA
sup
âˆ†âˆˆUXÏˆ(F((a, ux+ âˆ†))) âˆ’Î»dX(PX((a, ux)), PX((a, ux+ âˆ†)))
=
sup
aâˆˆA
sup
âˆ†âˆˆUXÏˆ(F((a, ux+ âˆ†))) âˆ’Î»dX(PX(gâˆ’1(g((a, ux)))), PX(gâˆ’1(g((a, ux+ âˆ†)))))
=
sup
aâˆˆA
sup
âˆ†âˆˆUXÏˆ(CF(Â¨va,âˆ†))âˆ’Î»c(Â¨va,CF(Â¨va,âˆ†))
= sup
aâˆˆAn
ËœÏˆÎ»(Â¨va)o
(17)
20Where ËœÏˆÎ»(Â¨va) := supâˆ†âˆˆUXÏˆ(CF(Â¨va,âˆ†))âˆ’Î»d(Â¨va,CF(Â¨va,âˆ†)). The equations F((a, ux+ âˆ†)) =
CF(v,âˆ†)andF((aâ€², ux+âˆ†)) = CF(Â¨vaâ€²,âˆ†)hold true according to Lemma 2. Finally, by substituting
Â¨va=CF(v, a)into the equation, we prove the equation:
sup
QâˆˆBÎ´(P)
E
vâˆ¼Q[Ïˆ(v)]
= inf
Î»â‰¥0
Î»Î´p+E
vâˆ¼P
sup
aâˆˆAÏˆÎ»(Â¨va)
,
where ÏˆÎ»(v)is defined as
ÏˆÎ»(v):= sup
âˆ†âˆˆX{Ïˆ(CF0(v,âˆ†))âˆ’Î»pd(v,CF0(v,âˆ†))},
Since, in this case, CFis equivalent to CF0, this completes the proof for case one.
Now consider the scenario where sensitive attributes have parents. Eq. 17 shows in strong duality
computation it needs to compute function in intervened Mconcerning the sensitive attributes levels.
In this case, instead of using the structural causal model M, it is sufficient to employ the parent-free
sensitive attribute SCM (Def. 2), M0.M0aligns with the semi-latent space and is compatible with
the representation form outlined in Proposition 1. By adopting this strategy, we transform Minto a
model where sensitive attributes do not have parents. The proof procedure for M0remains the same
as forMand completes the proof.
Lemma 6 Let(Z, c)be a space with cost function c,PNan empirical probability measure based on
observations {zi}N
i=1, and define Qas:
Q=PNâˆ’1
NÎ´z1+1
NÎ´zâ€²
1
where Î´z1andÎ´zâ€²
1are Dirac measures at z1andzâ€²
1, respectively. Then, the p-Wasserstein distance
between PNandQis given by:
Wc,p(PN,Q) = (1
N)1
pc(z1, zâ€²
1).
Proof. The definition of the p-Wasserstein distance between two probability measures PNandQis:
Wc,p(PN,Q) =
inf
Ï€âˆˆÎ“(PN,Q)Z
ZÃ—Zc(z, zâ€²)pdÏ€(z, zâ€²)1
p
,
where Î“(PN,Q)represents the set of all couplings of PNandQ. Since Qis obtained by transferring
a mass of1
Nfromz1tozâ€²
1, the optimal transport plan under the constraint that PNandQdiffer only
at two points involves only moving the mass1
Nfrom z1tozâ€²
1. The cost of this transportation is
c(z1, zâ€²
1)p, and because the entire mass1
Nis being moved:
Z
ZÃ—Zc(z, zâ€²)pdÏ€(z, zâ€²) =c(z1, zâ€²
1)pÂ·1
N.
Therefore, substituting this into the formula for Wp, we obtain:
Wc,p(PN,Q) =
c(z1, zâ€²
1)pÂ·1
N1
p
=1
N1
p
c(z1, zâ€²
1),
thus proving the lemma.
Lemma 7 Assume that f(a, x)is convex in xfor each fixed aand continuous in both aandx. Also,
assume fis uniformly continuous in xacross a. IfAis compact, then the function defined by
g(x) = sup
aâˆˆAf(a, x)
is convex and continuous in x.
21Proof. To show that g(x)is convex, consider any x1, x2in the domain and Î»âˆˆ[0,1]. By the
definition of supremum and the convexity of f(a, x)inx,
f(a, Î»x 1+ (1âˆ’Î»)x2)â‰¤Î»f(a, x1) + (1 âˆ’Î»)f(a, x2).
Taking the supremum over ainAon both sides, we get:
sup
aâˆˆAf(a, Î»x 1+ (1âˆ’Î»)x2)â‰¤sup
aâˆˆA(Î»f(a, x1) + (1 âˆ’Î»)f(a, x2)).
Using the properties of supremum,
sup
aâˆˆAf(a, Î»x 1+ (1âˆ’Î»)x2)â‰¤Î»sup
aâˆˆAf(a, x1) + (1 âˆ’Î») sup
aâˆˆAf(a, x2).
Thus,
g(Î»x1+ (1âˆ’Î»)x2)â‰¤Î»g(x1) + (1 âˆ’Î»)g(x2),
proving that g(x)is convex.
To show continuity of g(x)at a point x0, consider any sequence {xn}converging to x0. Since fis
uniformly continuous in x, given Ïµ >0, there exists Î´ >0such that for all x, ywith|xâˆ’y|< Î´,
|f(a, x)âˆ’f(a, y)|< Ïµ for all aâˆˆA.
Thus,
f(a, xn)< f(a, x0) +Ïµand f(a, x0)< f(a, xn) +Ïµfor all aâˆˆAand|xnâˆ’x0|< Î´.
Taking the supremum over ainA,
g(xn)â‰¤g(x0) +Ïµand g(x0)â‰¤g(xn) +Ïµ.
This implies
|g(xn)âˆ’g(x0)| â‰¤Ïµ,
establishing the continuity of g(x)atx0.
Hence, we conclude that supaâˆˆAf(a, x)is convex and continuous in x.
B.2 Proof of Theorem 2.
Letâ€™s consider the â„“(v, y, Î¸ ) =h(Yâˆ’ âŸ¨Î¸,VâŸ©)(or in abbreviation â„“(y)) orh(YÂ· âŸ¨Î¸,VâŸ©)where
h:Râ†’Rhas one of the forms |t|,max(0 , t),|tâˆ’Ï„|, ormax(0 , tâˆ’Ï„)for some Ï„â‰¥0
First consider the case diam (A) =âˆž. By property of CFDF for each Since the distance of vby
its twins Â¨vais zero.Let zâˆˆ {zi}IfPNthe empirical distribution by lemma 6 it can be seen for
observation Z= (v, y)the distribution
Qa=PNâˆ’1
NÎ´z+1
NÎ´(Â¨va,y1)=â‡’Wc,p(Qa,PN) = 0 = â‡’QaâˆˆBÎ´(PN).
This equation results that
RÎ´(PN, Î¸)â‰¥sup
aâˆˆARÎ´(Qa, Î¸)â‰¥ R(PN, Î¸)âˆ’1
Nâ„“(v1) +1
Nsup
aâˆˆAâ„“(Â¨va)| â‰¥1
Nsup
aâˆˆAâ„“(Â¨va)|
Letu= (uA, uX)such that v=Mu. By definition of hard intervention Â¨vais obtained by the
formula
Â¨va= (Mâˆ’Mpa)Ã—(uâˆ’(0, . . . ,:=Î±z}|{
aâˆ’uA, . . . , 0)T)
=MÃ—uâˆ’
 :MAÃ—Î±
MÃ—(0, . . . , Î±, . . . , 0)âˆ’ :CMpaÃ—u+
 :0
MpaÃ—(0, . . . , Î±, . . . , 0)
=vâˆ’MAÃ—Î±âˆ’C
22where Mparefers to the effect of parents of sensitive variables and MAis the columns of matrix M
related to sensitive attributes that show the effects of sensitive attributes on non-sensitive variables.
By substituting that last equation in the loss function we have:
RÎ´(PN, Î¸)â‰¥1
Nsup
aâˆˆAâ„“(vâˆ’MAÃ—Î±âˆ’C)â‰¥1
Nsup
Î±â†’âˆžO(Î¸TMAÃ—Î±)
where Bis some constant value. with the assumptions about loss function, all of them by choosing
proper z1, its behavior when Î±is large enough is linear so can be approximated by its input value.
Now Since the diam (A) =âˆžtherefore the value of Î±goes to the infinity. Therefore to prevent the
value of RÎ´(PN, Î¸)it needs that the expression Î¸TMAÃ—Î±= 0 in the other word the PA(MTÎ¸)
needs to be zero. This condition implies that for all aâˆˆ A we have â„“(v) =â„“(Â¨va). By using strong
duality 8 we have:
sup
QâˆˆBÎ´(P)
E
vâˆ¼Q[â„“(v)]
= inf
Î»â‰¥0
Î»Î´+E
vâˆ¼P
sup
aâˆˆAâ„“Î»(Â¨va)
= inf
Î»â‰¥0n
Î»Î´+E
vâˆ¼P[â„“Î»(v)]o
, (18)
where
â„“Î»(v) = sup
âˆ†âˆˆX{â„“(CF(v,âˆ†))âˆ’Î»d(v,CF(v,âˆ†))}=
sup
âˆ†âˆˆXh(Î¸TM0((ua, ux+ âˆ†))) âˆ’Î»dX(ux, ux+ âˆ†) =
sup
âˆ†âˆˆXh(PX(MTÎ¸)T(ux+ âˆ†)) âˆ’Î»dX(ux, ux+ âˆ†) =
sup
âˆ†âˆˆXh(âŸ¨Î¸0, ux+ âˆ†âŸ©)âˆ’Î»âˆ¥âˆ†âˆ¥=hÎ»(ux) (19)
In the equation 19, M0is reduced-form mapping of the parent-free sensitive attribute M0. By the
definition it is easy in both SCM, the effect of the sensitive attributes is equal therefore PA(MT
0Î¸) =
PA(MTÎ¸) = 0 . Moreover since in M0the structure of non-sensitive attribute has not changed then
PX(MT
0Î¸) =PX(MTÎ¸). Finally, by substituting Î¸0=PX(MTÎ¸), it can be seen that the problem
of finding worst-case loss quantity converts to the regular problem in space X. This problem was
solved previously in works of [ 14,58,25,66]. By using Theorem 3.2 and 3.3 and Proposition 4.1
and 4.2, of work by Chu et al. [14], we can write
inf
Î»â‰¥0
Î»Î´+E
vâˆ¼PN[â„“Î»(v)]
= inf
Î»â‰¥0
Î»Î´+ E
uxâˆ¼(g#PN)X[hÎ»(ux)]
=

R((g#PN)X, Î¸0)1
p+Î´âˆ¥Î¸0âˆ¥âˆ—p
=
R(PN, Î¸)1
p+Î´PX(MTÎ¸)
âˆ—p
The equality R((g#PN)X, Î¸0) =R(PN, Î¸)holds by definition and property PA(MTÎ¸) = 0 . The
last equation completes the proof of the first case.
Now letâ€™s consider the case that diam (A)<âˆž.
Case: pâˆˆ(1,âˆž).Lets consider Eq. 16 it implies that:
RÎ´(P) = inf
Î»â‰¥0
Î»Î´+ E
uxâˆ¼(g#P)Xh
Ëœâ„“Î»(ux)i
(20)
where, Ëœâ„“(ux) = supuaâˆˆUA{â„“(M(ua, uâ€²
x)}andÎ·Î»(ux) = supâˆ†âˆˆUXËœâ„“(ux+âˆ†)âˆ’Î»dX(ux, ux+âˆ†)}.
By assumption, the whole type of loss functions are form h(âŸ¨Î¸, vâŸ©)and convex and continuous. h
can be written by Min the form h(âŸ¨Î¸, M(ua, ux)âŸ©). Then Ëœâ„“(ux) = supuaâˆˆUAh(PA(MTÎ¸)ua+
PX(MTÎ¸)ux). Since all forms of function are uniformly continuous concerning the uxthen lemma 7
implies that the Ëœâ„“(ux)is still continuous and convex. Theorem 6 and 7 of work [66] states that:
Theorem. [ 66]Letâ„“:Râ†’Rbe a non-negative, Lipschitz continuous and convex function. For an
integer pâˆˆ(1,âˆž), suppose that for any Pâˆˆ P(X), and Ïµâ‰¥0, we have:
sup
QâˆˆBÎ´(P)Exâˆ¼Q[â„“p(Î¸Tx)] = 
Exâˆ¼P[â„“p(Î¸Tx)]1/p+Î´âˆ¥Î¸âˆ¥âˆ—p
.
23By applying above theorem in Eq. 20 it can be seen:
RÎ´(P) =
Euxâˆ¼(g#P)X[Ëœâ„“p(ux)]1/p
+Î´PX(MTÎ¸)
âˆ—p
=
 
Evâˆ¼P[sup
aâˆˆAâ„“p(Â¨va)]1/p
+Î´PX(MTÎ¸)
âˆ—!p
Case: p= 1.To complete the proof we use Theorem 2 and Corollary 2 of Gao et al. work [27].
Theorem [ 27]Ifâ„“is Lipschitz |â„“(x1)âˆ’â„“(x2)| â‰¤Lâˆ¥xkâˆ’x0âˆ¥and satisfies tightness at infinity, i.e.
for every v0there exists sequence {vk}âˆž
k=1âˆˆ V such that âˆ¥xkâˆ’x0âˆ¥ â†’ âˆž we have:
lim
âˆ¥xkâˆ’x0âˆ¥â†’âˆž|â„“(xk)âˆ’â„“(x0)|
âˆ¥xkâˆ’x0âˆ¥=L (21)
then we have RÎ´(P) =R(P) +Î´.L.
Now back to the Eq. 20. It is necessary to show that Ëœâ„“satisfies the Gaoâ€™s theorem. Let h(t)be one
of the functions |t|,(tâˆ’Ï„)+,(|t| âˆ’Ï„)+. All of loss function can be written as form h(yâˆ’ âŸ¨Î¸, vâŸ©)
orh(y.âŸ¨Î¸, vâŸ©). It is easy to check that his Lipschitz with constant 1 and there exist tksuch that for
eacht0we have limkâ†’âˆž|h(t0+tk)âˆ’h(t0)|
|tk|= 1.
To use this theorem for Ëœâ„“, we need to prove that Ëœâ„“is Lipschitz and has a tightness condition at infinity.
Since the his Lipschitz we know that:
âˆ€uaâˆˆ UA:|h(PA(MTÎ¸)ua+PX(MTÎ¸)ux)âˆ’h(PA(MTÎ¸)ua+PX(MTÎ¸)uâ€²
x)| â‰¤
|PX(MTÎ¸)(uxâˆ’uâ€²
x)| â‰¤PX(MTÎ¸)
âˆ—âˆ¥uxâˆ’uâ€²
xâˆ¥ â‡’
sup
uaâˆˆUA|h(PA(MTÎ¸)ua+PX(MTÎ¸)ux)âˆ’h(PA(MTÎ¸)ua+PX(MTÎ¸)uâ€²
x)|
sup
uaâˆˆUA|h(PA(MTÎ¸)ua+PX(MTÎ¸)ux)âˆ’h(PA(MTÎ¸)ua+PX(MTÎ¸)uâ€²
x)|=
|sup
uaâˆˆUAh(PA(MTÎ¸)ua+PX(MTÎ¸)ux)âˆ’sup
uaâˆˆUAh(PA(MTÎ¸)ua+PX(MTÎ¸)uâ€²
x)|=
|Ëœâ„“(ux)âˆ’Ëœâ„“(uâ€²
x)| â‰¤PX(MTÎ¸)
âˆ—âˆ¥uxâˆ’uâ€²
xâˆ¥ â‡’ Ëœâ„“is Lipbschitz.
To satisfy the condition of Gaoâ€™s theorem, it remains to show for u0
xthere exists sequence {uk
x}âˆž
k=1
such that lim
âˆ¥ukxâˆ’u0xâˆ¥â†’âˆž|Ëœâ„“(uk
x)âˆ’Ëœâ„“(u0
x)|
âˆ¥ukxâˆ’u0xâˆ¥=LTo prove it we consider that for function hthere exists
sequence such that limkâ†’âˆž|h(t0+tk)âˆ’h(t0)|
|tk|= 1. consider the specific point u0
xanduaâˆˆ UA.
Lets define t0=PA(MTÎ¸)ua+PX(MTÎ¸)u0
x. Then there exists tkthat satisfies infinity tightness.
Therefore there exist âˆ†kâˆˆ UXsuch that PX(MTÎ¸)uk
xâˆ’PX(MTÎ¸)u0
x=tktherefore it can be
written:
1 =limkâ†’âˆž|h(t0+tk)âˆ’h(t0)|
|tk|=
limkâ†’âˆž|h(PA(MTÎ¸)ua+PX(MTÎ¸)uk
x)âˆ’h(PA(MTÎ¸)ua+PX(MTÎ¸)u0
x)|
|PX(MTÎ¸)ukx|â‡’
limkâ†’âˆž|h(MTÎ¸(ua, uk
x))âˆ’h(MTÎ¸(ua, u0
x)|
âˆ¥ukxâˆ¥=PX(MTÎ¸)
âˆ—â‡’
sup
uaâˆˆUA
limkâ†’âˆž|h(MTÎ¸(ua, uk
x))âˆ’h(MTÎ¸(ua, u0
x)|
âˆ¥ukxâˆ¥
=PX(MTÎ¸)
âˆ—â‡’
limkâ†’âˆž sup
uaâˆˆUA|h(MTÎ¸(ua, uk
x))âˆ’h(MTÎ¸(ua, u0
x)|
âˆ¥ukxâˆ¥
=PX(MTÎ¸)
âˆ—â‡’
limkâ†’âˆž|Ëœâ„“(uk
x)âˆ’Ëœâ„“(u0
x)|
âˆ¥ukxâˆ¥=PX(MTÎ¸)
âˆ—
24In the above equation, since we have uniform convergence, we can change the limit and supremum.
The last equation shows that there exits sequence of {uk
x}âˆž
k=1satisfies tightness condition in âˆž. By
applying Gaoâ€™s theorem we have:
RÎ´(P) =Euxâˆ¼(g#P)X[Ëœâ„“(ux)] +Î´PX(MTÎ¸)
âˆ—=Evâˆ¼P[sup
aâˆˆAâ„“p(Â¨va)] +Î´PX(MTÎ¸)
âˆ—
The last equation completes the proof.
B.3 Proof of Theorem 3.
The case diam (A) =âˆžcorresponds exactly to the first part of the proof of Theorem 2, with the only
difference being that in that theorem we have âˆ¥hâˆ¥Lip= 1, while in this case, we have âˆ¥hâˆ¥Lip=Lh.
Therefore we have the below equation:
RÎ´(P) =R(P) +LhPX(MTÎ¸)
âˆ—
Now consider the case diam (A)<âˆž. To prove our assertion, we use Eq. 16 and it implies that:
RÎ´(P) = inf
Î»â‰¥0
Î»Î´+ E
uxâˆ¼(g#P)Xh
Ëœâ„“Î»(ux)i
where, Ëœâ„“(ux) = supuaâˆˆUA{â„“(M(ua, uâ€²
x)}andÎ·Î»(ux) = supâˆ†âˆˆUXËœâ„“(ux+âˆ†)âˆ’Î»dX(ux, ux+âˆ†)}.
To compute the right side of the above equation, we use the theorem 3.2 of work [14] that states.
Theorem [ 14].LetZN:={z1, . . . , z n} âŠ‚ Z be a given dataset and PNbe the corresponding
empirical distribution. In addition, let c(Â·,Â·)be a cost function on Z Ã— Z andÎ´âˆˆ(0,âˆž)be a scalar.
Suppose the loss function â„“:Z Ã—Î˜â†’R, where satisfies the following assumptions:
(A1) â„“is Lipschitz respect o the cost function dat setZNwithLZN
Î¸âˆˆ(0,âˆž);
(A2) for any Ïµâˆˆ(0, LZN
Î¸)and each ziâˆˆ ZN, there exists Ëœziâˆˆ Z such that Î´â‰¤d(Ëœzi, zi)<âˆž
and
â„“(Ëœzi)âˆ’â„“(zi)â‰¥(LZN
Î¸âˆ’Ïµ)c(Ëœzi, zi).
Then we have:
sup
P:Wd,1(Q,PN)â‰¤Î´EQ[â„“(Z, Î¸)] = E PN[â„“(Z, Î¸)] +LZN
Î¸Î´.
To use the above theorem we need that Ëœâ„“satisfies conditions (A1) and (A2).
A1. To prove the Lipschitz condition it can be seen:
âˆ€uaâˆˆ UX:|h(yâˆ’Î¸TM(ua, ux))âˆ’h(yâˆ’Î¸TM(ua, uâ€²
x))|
â‰¤Lh|PX(MTÎ¸)(uaâˆ’uâ€²
a)| â‰¤LhPX(MTÎ¸)
âˆ—âˆ¥uaâˆ’uâ€²
aâˆ¥ â‡’
sup
uaâˆˆUX|h(yâˆ’Î¸TM(ua, ux))âˆ’h(yâˆ’Î¸TM(ua, uâ€²
x))|=
|Ëœâ„“(ua)âˆ’Ëœâ„“(uâ€²
x)| â‰¤LhPX(MTÎ¸)
âˆ—âˆ¥uxâˆ’uâ€²
xâˆ¥
The case h(y.âŸ¨Î¸, yâŸ©)is similar so we omit it.
A2. To check that Ëœâ„“satisfies (A2) condition we show that there exists sequence {âˆ†k}such that the
âˆ¥âˆ†kâˆ¥ â†’ âˆž for every vâˆˆ V and sequence vk=CF(v,âˆ†k)and we have:
limkâ†’âˆž|h(yâˆ’ âŸ¨Î¸, vkâŸ©)âˆ’h(yâˆ’ âŸ¨Î¸, vâŸ©)|
d(vk, v)=Lh.PX(MTÎ¸)
âˆ—(22)
By assumption about hwe have: For each t0âˆˆRthere exists sequence of {tk}âˆž
k=1goes to
âˆžthenlimkâ†’âˆž|h(t0+tk)âˆ’h(t0)|
|tk|=Lh. By changing variable v=M(ua, ux). Let t0=
25yâˆ’Î¸TM(ua, ux)andâˆ†kâˆˆ X such that PX(MTÎ¸)âˆ†k=tkit is clear âˆ†kexist. No if we define
vk=CF(v,âˆ†k)we have:
Lh=limkâ†’âˆž|h(t0+tk)âˆ’h(t0)|
|tk|=
limkâ†’âˆž|h(yâˆ’Î¸TM(ua, ux) +PX(MTÎ¸)âˆ†k)âˆ’h(yâˆ’Î¸TM(ua, ux))|
|PX(MTÎ¸)âˆ†k|=
limkâ†’âˆž|h(yâˆ’Î¸TM(ua, ux+ âˆ† k))âˆ’h(yâˆ’Î¸TM(ua, ux))|
|PX(MTÎ¸)âˆ†k|=
limkâ†’âˆž|h(yâˆ’Î¸Tvk)âˆ’h(yâˆ’Î¸Tv)|
âˆ¥PX(MTÎ¸)âˆ¥âˆ—âˆ¥âˆ†kâˆ¥=limkâ†’âˆž|h(yâˆ’ âŸ¨Î¸, vkâŸ©)âˆ’h(yâˆ’ âŸ¨Î¸, vâŸ©)|
âˆ¥PX(MTÎ¸)âˆ¥âˆ—d(vk, v)
=â‡’limkâ†’âˆž|h(yâˆ’ âŸ¨Î¸, vkâŸ©)âˆ’h(yâˆ’ âŸ¨Î¸, vâŸ©)|
d(vk, v)=PX(MTÎ¸)
âˆ—.Lh
The Last equation is valid because, by Holder inequality, there exists âˆ†such that for all Î»âˆ†the
Holder inequality converts to equality. Now it is sufficient that find proper Î»such that Î»k=
tk/PX(MTÎ¸)âˆ†
âˆ—so by define âˆ†k=Î»kâˆ†we find sequence that holds the assertion.
The case of h(y.âŸ¨Î¸, vâŸ©is similar. By discussion of the first part, we can find âˆ†k. Now since we have
binary classification, so yâˆˆ {âˆ’ 1,1}we define Ëœâˆ†k=sign(y)âˆ†ktherefore for such âˆ†k, we have
y.PX(MTÎ¸)Ëœâˆ†k) =tk. By assumption, it can be written as:
Lh=limkâ†’âˆž|h(t0+tk)âˆ’h(t0)|
|tk|=
limkâ†’âˆž|h(y.Î¸TM(ua, ux) +y.PX(MTÎ¸)Ëœâˆ†k)âˆ’h(y.Î¸TM(ua, ux))|
|PX(MTÎ¸)Ëœâˆ†k|=
limkâ†’âˆž|h(y.Î¸TM(ua, ux) +y.PX(MTÎ¸)âˆ†k)âˆ’h(y.Î¸TM(ua, ux))|
|PX(MTÎ¸)âˆ†k|=
limkâ†’âˆž|h(y.Î¸TM(ua, ux+ âˆ† k))âˆ’h(y.Î¸TM(ua, ux))|
|PX(MTÎ¸)âˆ†k|=
limkâ†’âˆž|h(y.Î¸Tvk)âˆ’h(y.Î¸Tv)|
âˆ¥PX(MTÎ¸)âˆ¥âˆ—âˆ¥âˆ†kâˆ¥=limkâ†’âˆž|h(y.âŸ¨Î¸, vkâŸ©)âˆ’h(yâˆ’ âŸ¨Î¸, vâŸ©)|
âˆ¥PX(MTÎ¸)âˆ¥âˆ—d(vk, v)
=â‡’limkâ†’âˆž|h(yâˆ’ âŸ¨Î¸, vkâŸ©)âˆ’h(yâˆ’ âŸ¨Î¸, vâŸ©)|
d(vk, v)=PX(MTÎ¸)
âˆ—.Lh
Letâ„“(v) =h(yâˆ’PX(MTÎ¸)uxâˆ’PA(MTÎ¸)ua). By assumption his Lipschitz so it â„“is Lipschitz
concerning each uxanduaandUAis bounded so it is compact. Then these properties imply uniformly
continuous so we have:
sup
uaâˆˆUAlimkâ†’âˆž|h(yâˆ’ âŸ¨Î¸, vkâŸ©)âˆ’h(yâˆ’ âŸ¨Î¸, vâŸ©)|
d(vk, v)=
limkâ†’âˆž sup
uaâˆˆUA|h(yâˆ’ âŸ¨Î¸, vkâŸ©)âˆ’h(yâˆ’ âŸ¨Î¸, vâŸ©)|
d(vk, v)=limkâ†’âˆž|Ëœâ„“(ux)âˆ’Ëœâ„“(âˆ†k)|
âˆ¥uxâˆ’âˆ†kâˆ¥=PX(MTÎ¸)
âˆ—.Lh
The last equation satisfies the ( A2) condition because since limkâ†’âˆž|Ëœâ„“(ux)âˆ’Ëœâ„“(âˆ†k)|
âˆ¥uxâˆ’âˆ†kâˆ¥=
PX(MTÎ¸)
âˆ—.Lhin other hand we have |Ëœâ„“(ua)âˆ’Ëœâ„“(uâ€²
x)| â‰¤LhPX(MTÎ¸)
âˆ—âˆ¥uxâˆ’uâ€²
xâˆ¥, then for
eachÏµthere exist âˆ†ksuch that |Ëœâ„“(ux)âˆ’Ëœâ„“(âˆ†k)|>(LhPX(MTÎ¸)
âˆ—âˆ¥uxâˆ’âˆ†kâˆ¥ âˆ’Ïµ)âˆ¥uxâˆ’âˆ†kâˆ¥
Now we can use Chuâ€™s Theorem and it implies that: RÎ´(P) =Rcf(P) +LhPX(MTÎ¸)
âˆ—. The
classification case is the same and it completes the proof.
26B.4 Proof of Theorem 4.
Letâ€™s prove the necessary condition. By first part proof of theorem 2, we have
RÎ´(PN, Î¸)â‰¥1
Nsup
aâˆˆAâ„“(Â¨va, y, Î¸).
Therefore, for a finite solution to exist for the DRO problem, it is necessary that:
sup
aâˆˆAâ„“(Â¨va, y, Î¸)<âˆž.
To prove Eq. 10, we use again some parts of the proof of strong duality theorem1 and idea of proof of
theorem 9.1 of Garciaâ€™s work [29]. It states:
RÎ´(PN) = sup
QâˆˆBÎ´(PN)
E
vâˆ¼Q[â„“(vâ€², y, Î¸)]
= sup
QxâˆˆBÎ´((g#PN)X)
QaâˆˆP(A)ï£±
ï£´ï£²
ï£´ï£³E
uâ€²
xâˆ¼Qx
uâ€²
aâˆ¼Qa
â„“ 
gâˆ’1((uâ€²
a, uâ€²
x)), y, Î¸,ï£¼
ï£´ï£½
ï£´ï£¾
where by discussion in lemma 4, we can suppose that QxandQaare independent of each other.
For simplicity, we define J(ux, ua, Î¸) =â„“(gâˆ’1((ua, ux)), y, Î¸). By assumption, since the fis twice
differentiable, then (Iâˆ’f)âˆ’1is also twice differentiable. Because the function gis obtained by
Iâˆ’fby removing the functional structure of sensitive attributes and is a set identity function instead
of them, there are two functions gandgâˆ’1. These results show that combination â„“(gâˆ’1)is also twice
differentiable, so the gradient of Jexists concerning the ux. By using Taylorâ€™s expansion theorem if
f:Râ†’Ris the function that has gradient then the first order estimation of fequals:
f(x+h) =f(x) +âˆ‡f(x)âŠ¤h+Z1
0(âˆ‡f(x+th)âˆ’ âˆ‡f(x))âŠ¤h dt.
Letuxâˆ¼(g#P)Xby writing Taylorâ€™s expansion around uxwe have:
E[J(uâ€²
x, uâ€²
a, Î¸)] =E[J(ux, uâ€²
a, Î¸) +âˆ‡xJ(ux, uâ€²
a, Î¸)Â·(uâ€²
xâˆ’ux)
+Z1
0{âˆ‡xJ(ux+Î»(uâ€²
xâˆ’ux), uâ€²
a, Î¸)âˆ’ âˆ‡ xJ(ux, uâ€²
a, Î¸)} Â·(uâ€²
xâˆ’ux)dÎ»
.
Since the diam (U)<âˆžwe can suppose that the space Uis compact. By assumption Jis twice
differentiable, so âˆ‡xJ(., ua, Î¸)is Lipschitz with constant âˆ¥âˆ‡xJ(., ua, Î¸)âˆ¥Lipthat there exist L <âˆž
such that âˆ¥âˆ‡xJ(., ua, Î¸)âˆ¥Lipâ‰¤L. By these assumptions, it can be written:
EZ1
0{âˆ‡xJ(ux+Î»(uâ€²
xâˆ’ux), uâ€²
a, Î¸)âˆ’ âˆ‡ xJ(ux, uâ€²
a, Î¸)} Â·(uâ€²
xâˆ’ux)dÎ»
â‰¤
EZ1
0âˆ¥âˆ‡xJ(ux+Î»(uâ€²
xâˆ’ux), uâ€²
a, Î¸)âˆ’ âˆ‡ xJ(ux, uâ€²
a, Î¸)âˆ¥âˆ¥uâ€²
xâˆ’uxâˆ¥2
edÎ»
=
EZ1
0âˆ¥âˆ‡xJ(., uâ€²
a, Î¸)âˆ¥Lipâˆ¥uâ€²
xâˆ’uxâˆ¥2
edÎ»
=E1
2âˆ¥âˆ‡xJ(., uâ€²
a, Î¸)âˆ¥Lipâˆ¥uâ€²
xâˆ’uxâˆ¥2
e
â‰¤
C
2âˆ¥âˆ‡xJ(., uâ€²
a, Î¸)âˆ¥LipE
âˆ¥uâ€²
xâˆ’uxâˆ¥2
â‰¤CL
2E
âˆ¥uâ€²
xâˆ’uxâˆ¥2
â‰¤O(Î´2),
where Cis a constant that arises from the equivalence of norms in Rd, it means that there
exists Câˆ¥.âˆ¥eâ‰¤Câˆ¥.âˆ¥. The inequality E
âˆ¥uâ€²
xâˆ’uxâˆ¥2
â‰¤Î´2is valid because by definition
QxâˆˆBÎ´((g#PN)X)and the cost function in the space UXis expressed by the âˆ¥uâ€²
xâˆ’uxâˆ¥so
by definition of BÎ´((g#PN)X), forpâ‰¥2by applying Jensenâ€™s inequality we have
QxâˆˆBÎ´((g#PN)X)â‡’EQx[âˆ¥uâ€²
xâˆ’uxâˆ¥p]1
pâ‰¤Î´â‡’
EQx
âˆ¥uâ€²
xâˆ’uxâˆ¥2
â‰¤EQx[âˆ¥uâ€²
xâˆ’uxâˆ¥p]2
pâ‰¤Î´2.
Since by assumption âˆ‡Jxis uniformly Lipschitz for different value of Î¸anduatherefore we have:
RÎ´(PN) = sup
QxâˆˆBÎ´((g#PN)X)
QaâˆˆP(A)n
E[J(ux, uâ€²
a, Î¸) +âˆ‡xJ(ux, uâ€²
a, Î¸)Â·(uâ€²
xâˆ’ux)]o
+O(Î´2),
27forO(Î´2)independent of Î¸andua. The first expression of the above equation has the simple form:
sup
QaâˆˆP(UA)ï£±
ï£´ï£²
ï£´ï£³E
uxâˆ¼(g#PN)X
uâ€²
aâˆ¼Qa[J(ux, uâ€²
a, Î¸)]ï£¼
ï£´ï£½
ï£´ï£¾= E
uxâˆ¼(g#PN)X"
sup
QaâˆˆP(UA)
E
uâ€²aâˆ¼Qa[J(ux, uâ€²
a, Î¸)]#
=
E
uxâˆ¼(g#PN)X"
sup
uâ€²
aâˆˆUA{J(ux, uâ€²
a, Î¸)}#
= E
uxâˆ¼(g#PN)X"
sup
uâ€²aâˆˆUA
â„“(gâˆ’1((ux, uâ€²
a)), y, Î¸)	#
=
E
vâˆ¼PN
sup
aâˆˆAâ„“(Â¨va, y, Î¸)
The only term in the above equation that still depends on the Qis that term âˆ‡xJ(ux, uâ€²
a, Î¸)Â·(uâ€²
xâˆ’ux).
To remove this term we use extended HÃ¶lder inequality with the expectation that can be expressed
using the following formula:
E[|XY|]â‰¤(E[|X|p])1
p(E[|Y|q])1
q,
and equality holds if and only if there exist constants câ‰¥0such that:
|Y|=c|X|p
qalmost surely , câ‰¥0.
. By using HÃ¶lder inequality, with the same reasoning we have:
sup
QxâˆˆBÎ´((g#PN)X)
QaâˆˆP(A)ï£±
ï£´ï£²
ï£´ï£³E
uâ€²
xâˆ¼Qx
uâ€²
aâˆ¼Qa[âˆ‡xJ(ux, uâ€²
a, Î¸)Â·(uâ€²
xâˆ’ux)]ï£¼
ï£´ï£½
ï£´ï£¾=
sup
QxâˆˆBÎ´((g#PN)X)(
E
uâ€²
xâˆ¼Qx[ sup
uâ€²aâˆˆUA{âˆ‡xJ(ux, uâ€²
a, Î¸)Â·(uâ€²
xâˆ’ux)}])
=
 
E
uxâˆ¼(g#PN)X[ sup
uâ€²aâˆˆUA{âˆ¥âˆ‡ xJ(ux, uâ€²
a, Î¸)âˆ¥q
âˆ—}]!1
q
sup
QxâˆˆBÎ´((g#PN)X)ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³ï£«
ï£¬ï£­ E
uâ€²
xâˆ¼Qx
uxâˆ¼(g#PN)X[âˆ¥uâ€²
xâˆ’uxâˆ¥p]ï£¶
ï£·ï£¸1
pï£¼
ï£´ï£´ï£½
ï£´ï£´ï£¾
=Î´ 
E
uxâˆ¼(g#PN)X[ sup
uâ€²aâˆˆUA{âˆ¥âˆ‡ xJ(ux, uâ€²
a, Î¸)âˆ¥q
âˆ—}]!1
p
where equality can be attained whenever 1â‰¤pâ‰¤ âˆž for a proper choice of uâ€²
xâˆ’uxwith
(E[âˆ¥uâ€²
xâˆ’uxâˆ¥p])1/p=Î´. Therefore,
RÎ´(PN) =E
vâˆ¼PN
sup
aâˆˆAâ„“(Â¨va, y, Î¸)
+Î´
E
vâˆ¼PN[sup
aâˆˆA{âˆ¥âˆ‡CFâ„“(Â¨v, y, Î¸ )âˆ¥q
âˆ—}]1/q
+O(Î´2)
where
âˆ‡CFâ„“(v, y, Î¸ ) = lim
âˆ†â†’0â„“(CF0(v,âˆ†))âˆ’f(v)
âˆ¥âˆ†âˆ¥
the last equation completes the proofs.
B.5 Proof of Proposition 2.
By Eq. 16 we have:
RÎ´(P) = inf
Î»â‰¥0
Î»Î´+ E
uxâˆ¼(g#P)Xh
Ëœâ„“Î»(ux)i
where, Ëœâ„“(ux) = supuaâˆˆUA{â„“(M(ua, uâ€²
x)}andÎ·Î»(ux) = supâˆ†âˆˆUXËœâ„“(ux+ âˆ†)âˆ’Î»dX(ux, ux+
âˆ†)}. To prove we use Corollary 2 [ 28] for the Ëœâ„“. First, we need to show that Ëœâ„“satisfies the
condition of Corollary 2 [ 28]. By assumption for vâ€²âˆˆ V ,L, M â‰¥0such that |â„“(v, y, Î¸ )âˆ’
28â„“(vâ€², y, Î¸)|< Ldp(v, vâ€²) +M for all vâˆˆ V andpâˆˆ[1,âˆž). By setting v=gâˆ’1((ua, ua))and
vâ€²=gâˆ’1((uâ€²
a, uâ€²
a)), and for simplicity â„“(v) =â„“(v, y, Î¸ )we have:
|â„“(gâˆ’1((ua, ux)))âˆ’â„“(gâˆ’1((uâ€²
a, uâ€²
x)))|< Lâˆ¥uxâˆ’uâ€²
xâˆ¥p+M,âˆ€uxâˆˆ UX, uaâˆˆ UAâ‡’
|Ëœâ„“(ux)âˆ’Ëœâ„“(uâ€²
x)| â‰¤Lâˆ¥uxâˆ’uâ€²
xâˆ¥p+M
where Ëœâ„“(ux) =supuaâˆˆUAâ„“(gâˆ’1((ua, ux))). This equation implies that Ëœâ„“satisfies the condition of
corollary 2. So in consequence of corollary 2 and the equation 16 if we define uncertainty set:
ËœBÎ´=(
(Ï‰ik
x)i,k:1
NNX
i=1KX
k=1ui
xâˆ’Ï‰ikâ‰¤Î´, Ï‰ikâˆˆ UX)
Since the casual fair metric dand loss function Ëœâ„“do not depend on the sensitive part then ËœBÎ´is
equivalent to the below uncertainty set.
BÎ´=(
(wik)i,k:1
NNX
i=1KX
k=1dp(vi, wik)â‰¤Î´, wikâˆˆ V)
.
By applying the uncertainty ËœBÎ´forËœâ„“, the robust optimization problem has a form:
ËœRadv
Î´(PN) =
sup
(Ï‰ik)i,kâˆˆËœBÎ´(
1
NKNX
i=1KX
k=1Ëœâ„“(Ï‰ik))
= sup
(Ï‰ik)i,kâˆˆËœBÎ´(
1
NKNX
i=1KX
k=1max
uaâˆˆUAâ„“(gâˆ’1((ua, Ï‰ik))))
=
sup
(wik)i,kâˆˆBÎ´(
1
NKNX
i=1KX
k=1max
aâˆˆAâ„“( Â¨wik
a))))
and finally for ËœRadv
Î´(PN)we have:
ËœRadv
Î´(PN)â‰¤ Rn
Î´(PN)â‰¤ËœRadv
Î´(PN) +LD+M
NK,
and it completes the proof.
Lemma 8 Assume that the cost functions candË†csatisfy |c(x, xâ€²)âˆ’Ë†c(x, xâ€²)|< Î± for all x, xâ€²âˆˆRn
and for some Î±â‰¥0. Then, for any Î»â‰¥0, the difference between the Î»-conjugates of fwith respect
tocandË†cis bounded by Î»Î±:
|fÎ»(x)âˆ’Ë†fÎ»(x)| â‰¤Î»Î± for all xâˆˆRn.
Proof. To prove the proposition, we consider any xâˆˆRnand examine the definitions of fÎ»(x)and
Ë†fÎ»(x). Begin by expressing the bounds on Ë†c:
Ë†c(x, xâ€²)â‰¤c(x, xâ€²) +Î±and Ë†c(x, xâ€²)â‰¥c(x, xâ€²)âˆ’Î±.
From these inequalities, for any xâ€²âˆˆRn,
f(xâ€²)âˆ’Î»Ë†c(x, xâ€²)â‰¥f(xâ€²)âˆ’Î»(c(x, xâ€²) +Î±) =f(xâ€²)âˆ’Î»c(x, xâ€²)âˆ’Î»Î±,
f(xâ€²)âˆ’Î»Ë†c(x, xâ€²)â‰¤f(xâ€²)âˆ’Î»(c(x, xâ€²)âˆ’Î±) =f(xâ€²)âˆ’Î»c(x, xâ€²) +Î»Î±.
Taking the supremum over all xâ€²in the above expressions, we obtain:
Ë†fÎ»(x)â‰¥fÎ»(x)âˆ’Î»Î± and Ë†fÎ»(x)â‰¤fÎ»(x) +Î»Î±.
These two bounds together imply:
|fÎ»(x)âˆ’Ë†fÎ»(x)| â‰¤Î»Î±.
Thus, the proof is complete, showing that the difference between the Î»-conjugates of fis indeed
bounded by Î»Î±.
Lemma 9 Letc,Ë†c:RnÃ—Rnâ†’Rbe two functions such that |c(x, y)âˆ’Ë†c(x, y)|< Î± for all
x, yâˆˆRnand some Î± >0. For any real number pâ‰¥1, the following inequality holds:
|c(x, y)pâˆ’Ë†c(x, y)p| â‰¤pÂ·Mpâˆ’1Â·Î±,
where Mâ‰¥max{|c(x, y)|,|Ë†c(x, y)|}for all x, y.
29Proof. Consider the functions candË†cand any x, yâˆˆRn. By the hypothesis, we have |c(x, y)âˆ’
Ë†c(x, y)|< Î±. To find a bound on the difference of their powers, apply the mean value theorem to the
function f(t) =tp, which is differentiable over R(or over R+ifpis not an integer). The derivative
offisfâ€²(t) =ptpâˆ’1.
Since fis continuously differentiable, there exists some Î¾between c(x, y)andË†c(x, y)such that
f(c(x, y))âˆ’f(Ë†c(x, y)) =fâ€²(Î¾)Â·(c(x, y)âˆ’Ë†c(x, y)).
Therefore,
|c(x, y)pâˆ’Ë†c(x, y)p|=|pÎ¾pâˆ’1(c(x, y)âˆ’Ë†c(x, y))|.
Using the bound |c(x, y)âˆ’Ë†c(x, y)|< Î± and noting that Î¾must be within the range of values between
c(x, y)andË†c(x, y), we have Î¾pâˆ’1â‰¤Mpâˆ’1. Thus,
|c(x, y)pâˆ’Ë†c(x, y)p| â‰¤pMpâˆ’1|c(x, y)âˆ’Ë†c(x, y)|< pMpâˆ’1Î±.
Lemma 10 ([41]) Fix some Pâˆˆ P(Z),Î¸âˆˆÎ˜andÎ»âˆ—â‰¥0via
Î»âˆ—:= argminÎ»â‰¥0{Î»Î´p+EP[â„“Î»(Z, Î¸)]}.
Then under â„“Lipschitz and diam (Z)<âˆžassumptions, we have Î»âˆ—â‰¤LÎ´âˆ’(pâˆ’1).
Proof. First, note that:
Î»âˆ—Î´pâ‰¤Î»âˆ—Î´p+EP
sup
zâ€²âˆˆZ{â„“(zâ€², Î¸)âˆ’â„“(z, Î¸)âˆ’Î»âˆ—cp(z, zâ€²)}
=âˆ—,
since the left-hand side, is greater than the case where zâ€²=zso it is positive. By the optimality of
Î»âˆ—inâ„“, the right-hand side can be further upper-bounded as follows for any Î»â‰¥0:
âˆ— â‰¤Î»Î´p+EP
sup
zâ€²âˆˆZ{â„“(zâ€², Î¸)âˆ’â„“(z, Î¸)âˆ’Î»cp(z, zâ€²)}
â‰¤Î»Î´p+EP
sup
zâ€²âˆˆZ{Lc(z, zâ€²)âˆ’Î»cp(z, zâ€²)}
â‰¤Î»Î´p+ sup
tâ‰¥0{Ltâˆ’Î»tp},
using the Lipschitz property for the second line and setting t=c(z, zâ€²)in the third line. If p= 1, by
setting Î»=L, we obtain:
Î»âˆ—Î´â‰¤LÎ´+ sup
tâ‰¥0{Ltâˆ’Lt}=LÎ´,
which implies Î»âˆ—â‰¤L. For p >1, using the optimal value t= (L/pÎ» )1/(pâˆ’1), we derive:
Î»âˆ—Î´pâ‰¤Î»Î´p+Lp
pâˆ’1pâˆ’p
pâˆ’1(pâˆ’1)Î»âˆ’1
pâˆ’1.
Minimizing the right-hand side with Î»=L/pÎ´pâˆ’1yields:
Î»âˆ—Î´pâ‰¤LÎ´â‡’Î»âˆ—â‰¤LÎ´âˆ’(pâˆ’1),
resulting in the stated bound on Î»âˆ—.
Lemma 11 Iff:AÃ—Rnâ†’Ris Lipschitz with respect to xuniformly in a, i.e., there exists a
constant Lsuch that for all aâˆˆAand for all x, yâˆˆRn,
|f(a, x)âˆ’f(a, y)| â‰¤Lâˆ¥xâˆ’yâˆ¥,
thensupaâˆˆAf(a, x)is also Lipschitz in x.
30Proof. LetF(x) = supaâˆˆAf(a, x). We aim to show that there exists a constant Lâ€²such that for all
x, yâˆˆRn,
|F(x)âˆ’F(y)| â‰¤Lâ€²âˆ¥xâˆ’yâˆ¥.
Since fis Lipschitz continuous with respect to xuniformly in awith Lipschitz constant L, it holds
for each aâˆˆAand any x, yâˆˆRnthat
|f(a, x)âˆ’f(a, y)| â‰¤Lâˆ¥xâˆ’yâˆ¥.
Consider F(x)andF(y). By definition,
F(x) = sup
aâˆˆAf(a, x)and F(y) = sup
aâˆˆAf(a, y).
For any aâˆˆA, since |f(a, x)âˆ’f(a, y)| â‰¤Lâˆ¥xâˆ’yâˆ¥, we can infer that
f(a, x)â‰¤f(a, y) +Lâˆ¥xâˆ’yâˆ¥.
Taking the supremum over all aâˆˆAon both sides, we obtain
F(x)â‰¤F(y) +Lâˆ¥xâˆ’yâˆ¥.
Similarly,
F(y)â‰¤F(x) +Lâˆ¥xâˆ’yâˆ¥.
Combining these two inequalities, we find
|F(x)âˆ’F(y)| â‰¤Lâˆ¥xâˆ’yâˆ¥.
Therefore, F(x) = supaâˆˆAf(a, x)is Lipschitz continuous with Lipschitz constant L. This completes
the proof.
B.6 Proof of Theorem 5.
Let define define Ë†RÎ´(P, Î¸) := supQ:WË†c,p(Q,P)â‰¤Î´EQ[â„“(Z, Î¸)], the worst-case loss quantity over esti-
mation of the metric dandÎ¸âˆ—:= inf Î¸âˆˆÎ˜n
supQ:Wc,p(Q,Pâˆ—)â‰¤Î´EQ[â„“(Z, Î¸)]o
. Therefore by definition,
we can write:
RÎ´(Pâˆ—,Ë†Î¸dro
N)âˆ’ R Î´(Pâˆ—, Î¸âˆ—)â‰¤ R Î´(Pâˆ—,Ë†Î¸dro
N)âˆ’Ë†RÎ´(PN,Ë†Î¸dro
N)âˆ’(RÎ´(Pâˆ—, Î¸âˆ—)âˆ’Ë†RÎ´(PN, Î¸âˆ—))(23)
because we have Ë†RÎ´(PN,Ë†Î¸dro
N)â‰¤ R Î´(Pâˆ—, Î¸âˆ—).
We estimate two expression |RÎ´(Pâˆ—,Ë†Î¸dro
N)âˆ’Ë†RÎ´(PN,Ë†Î¸dro
N)|and|RÎ´(Pâˆ—, Î¸âˆ—)âˆ’Ë†RÎ´(PN, Î¸âˆ—)|. By the
general strong duality theorem [26] we have:
RÎ´(Pâˆ—, Î¸âˆ—)âˆ’Ë†RÎ´(PN, Î¸âˆ—) = sup
Q:Wc,p(Q,Pâˆ—)â‰¤Î´EQ[â„“(Z, Î¸âˆ—)]âˆ’ sup
Q:WË†c,p(Q,PN)â‰¤Î´EQ[â„“(Z, Î¸âˆ—)] =
inf
Î»â‰¥0{Î»Î´p+EPâˆ—
â„“c
Î»(Z, Î¸âˆ—)	
âˆ’inf
Î»â‰¥0{Î»Î´p+EPN
â„“Ë†c
Î»(Z, Î¸âˆ—)	
â‰¤
Î»NÎ´p+EPâˆ—
â„“c
Î»N(Z, Î¸âˆ—)	
âˆ’(Î»NÎ´p+EPN
â„“Ë†c
Î»N(Z, Î¸âˆ—)
) =
EPâˆ—
â„“c
Î»N(Z, Î¸âˆ—)	
âˆ’EPN
â„“Ë†c
Î»N(Z, Î¸âˆ—)
where the Î»N:= arginfÎ»â‰¥0
Î»Î´p+EPN
â„“Ë†c
Î»(Z, Î¸âˆ—)	
By assumption 2 and lemma 9,
|â„“c
Î»N(z, Î¸)âˆ’â„“Ë†c
Î»N(z, Î¸)|=sup
vâ€²âˆˆVâ„“(zâ€², y, Î¸)âˆ’Î»Ndp(vâ€², v)âˆ’sup
vâ€²âˆˆVâ„“(vâ€², y, Î¸)âˆ’Î»NË†dp(vâ€², v)
â‰¤sup
vâ€²âˆˆVÎ»N|dp(vâ€², v)âˆ’Ë†dp(vâ€², v)| â‰¤Î»Npdiam (V)pâˆ’1MdNâˆ’Î·.
This implies
RÎ´(Pâˆ—, Î¸âˆ—)âˆ’Ë†RÎ´(PN, Î¸âˆ—)â‰¤EPâˆ—
â„“c
Î»N(Z, Î¸)
âˆ’EPN
â„“c
Î»N(Z, Î¸)
+Î»Npdiam (V)pâˆ’1MdNâˆ’Î·.
31If define Î»âˆ—:= arginfÎ»â‰¥0
Î»Î´p+EP
â„“c
Î»(Z, Î¸âˆ—)	
, similarly,
Ë†RÎ´(PN, Î¸âˆ—)âˆ’ R Î´(Pâˆ—, Î¸âˆ—)â‰¤EPN
â„“Ë†c
Î»âˆ—(Z, Î¸)
âˆ’EPâˆ—
â„“c
Î»âˆ—(Z, Î¸)
â‰¤
EPN
â„“c
Î»âˆ—(Z, Î¸)
âˆ’EPâˆ—
â„“c
Î»âˆ—(Z, Î¸)
+Î»âˆ—pdiam (V)pâˆ’1MdNâˆ’Î·.
We need to estimate the Î»NandÎ»âˆ—. By using strong duality theorem by Eq. 16 we have:
RÎ´(P) = inf
Î»â‰¥0
Î»Î´p+ E
uxâˆ¼(g#P)Xh
Ëœâ„“Î»(ux)i
(24)
So instead the solve problem for â„“is it sufficient to prove our result for Ëœâ„“(ux) =
supuaâˆˆUAâ„“(gâˆ’1((ua, ux)). At first, we show that Ëœâ„“is Lipschitz on the space UXconcerning the
normâˆ¥.âˆ¥. By assumption, for each aâˆˆ A the function â„“(gâˆ’1((ua, ux))is also Lipschitz:â„“(gâˆ’1((ua, ux)), y, Î¸)âˆ’â„“(gâˆ’1((ua, ux+ âˆ†)) , y, Î¸)=
âˆ¥â„“(CF(v, a), y, Î¸)âˆ’â„“(CF(v, a,âˆ†), y, Î¸)âˆ¥ â‰¤Ld(CF(v, a),CF(v, a,âˆ†)) = Lâˆ¥âˆ†âˆ¥
Now by using lemma 11 it can be concluded that the function Ëœâ„“(ux) = supuaâˆˆUAâ„“(gâˆ’1((ua, ux))
also has Lipschitz property with constant L. By Applying lemma 10 for equation 24 and (g#PN)X,
it can be seen Î»N, Î»âˆ—â‰¤LÎ´âˆ’(pâˆ’1). Therefore until now, we have two inequalities:
RÎ´(Pâˆ—, Î¸âˆ—)âˆ’Ë†RÎ´(PN, Î¸âˆ—)â‰¤EPâˆ—
â„“c
Î»N(Z, Î¸)
âˆ’EPN
â„“c
Î»N(Z, Î¸)
+Î»Npdiam (V)pâˆ’1MdNâˆ’Î·,
Ë†RÎ´(PN, Î¸âˆ—)âˆ’ R Î´(Pâˆ—, Î¸âˆ—)â‰¤EPN
â„“c
Î»âˆ—(Z, Î¸)
âˆ’EPâˆ—
â„“c
Î»âˆ—(Z, Î¸)
+Î»âˆ—pdiam (V)pâˆ’1MdNâˆ’Î·â‡’
|RÎ´(Pâˆ—, Î¸âˆ—)âˆ’Ë†RÎ´(PN, Î¸âˆ—)| â‰¤sup
fâˆˆLcR
Zf(z)d(PNâˆ’Pâˆ—)(z)+LÎ´1âˆ’ppdiam (V)pâˆ’1MdNâˆ’Î·,
where Lc={â„“c
Î»(Â·, Î¸) :Î»âˆˆ[0, LÎ´1âˆ’p], Î¸âˆˆÎ˜}is the DR loss class.
In the remaining part of the proof, we estimate supfâˆˆLc|R
Zf(z)d(Pâˆ—âˆ’PN)(z)|using conventional
methods from statistical learning theory. According to assumption 2, the functions within Fare
limited as shown:
0â‰¤â„“c
Î»(v, Î¸)â‰¤sup
vâ€²âˆˆVâ„“(vâ€², y, Î¸)âˆ’Î»d(v, vâ€²)â‰¤sup
vâ€²âˆˆVâ„“(vâ€², y, Î¸)â‰¤M.
similar to the proof Theorem 3 [ 41], by utilizing the bounded-differences inequality and symmetriza-
tion, we derive that:
sup
fâˆˆLcR
Zf(z)d(PNâˆ’Pâˆ—)(z)â‰¤2RN(Lc) +Ms
log2
Ïµ
2N
holds with a probability of at least 1âˆ’Ïµ, where Rn(Lc)represents the Rademacher complexity of
Lc:
RN(Lc) =E
sup
fâˆˆLc1
NNX
i=1Ïƒif(Zi)
.
In the proof of Theorem 2 [41], the authors has proved:
RN(Lc)â‰¤24C(L)âˆš
N+24L.diam (V)p
âˆš
NÎ´pâˆ’1.
whereC(L), the entropy integral of of loss class. By applying this result it can be written:
|RÎ´(Pâˆ—, Î¸âˆ—)âˆ’Ë†RÎ´(PN, Î¸âˆ—)| â‰¤Ms
log2
Ïµ
2N+48C(L)âˆšn+48L.diam (V)p
âˆšnÎ´pâˆ’1+LÎ´1âˆ’ppdiam (V)pâˆ’1MdNâˆ’Î·
Since the prove does depend on value of Î¸, then|RÎ´(Pâˆ—,Ë†Î¸dro
N)âˆ’Ë†RÎ´(PN,Ë†Î¸dro
N)|also satisfies in the
above inequality. By combining two terms with probability 1âˆ’Ïµwe have,
RÎ´(Pâˆ—,Ë†Î¸dro
N)âˆ’RÎ´(Pâˆ—, Î¸âˆ—)â‰¤Ms
2 log2
Ïµ
N+96C(L)âˆšn+96L.diam (V)p
âˆšnÎ´pâˆ’1+2LÎ´1âˆ’ppdiam (V)pâˆ’1MdNâˆ’Î·
Since by assumption, with probability 1âˆ’Ïµwe have inequality
âˆ€z, zâ€²âˆˆ |c(z, zâ€²)âˆ’Ë†c(z, zâ€²)| â‰¤MdNâˆ’Î·, Î· > 0
therefore by probability 1âˆ’2Ïµthe main inequality is true and it completes the proof.
32C Numerical Analysis Supplementary
C.1 Synthetic Data Models
The structural equations used to generate the SCMs in Â§ 5 are listed below. For the LIN SCM, we
generate the protected feature Aand variables Xiaccording to the following structural equations:
â€¢ linear SCM (LIN):
ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³A:=UA, UAâˆ¼ B(0.5)
X1:= 2A+U1, U 1âˆ¼ N(0,1)
X2:=Aâˆ’X1+U2, U2âˆ¼ N(0,1)
Yâˆ¼ B((1 + exp(âˆ’(X1+X2))âˆ’1)
Here,B(p)represents Bernoulli random variables with probability p, andN(Âµ, Ïƒ2)represents normal
random variables with mean Âµand variance Ïƒ2. To generate the ground truth h(A, X1, X2), we
use a linear model for the LIN method. In all the synthetic models considered, we treat Aas a
binary-sensitive attribute.
C.2 Real-World Data
In our research, we have utilized the Adult dataset [ 38] and the COMPAS dataset [ 65] for our
experimental analysis. To employ these datasets, we initially constructed an SCM based on the
causal graph proposed by Nabi et al. [ 49]. For the Adult dataset, we incorporate features such as sex,
age,native-country ,marital-status ,education-num ,hours-per-week , and consider gender as a
sensitive attribute. In the case of the COMPAS dataset, the utilized features comprise age,race,sex,
andpriors count , which function as variables. Additionally, sex is considered a sensitive attribute.
For classification purposes, we apply data standardization before the learning process.
Adult =ï£±
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³A=UA Sex
X2=U2 Age
X3=U3 Country
X4=U4 Marital Status
X5=Î²51X1+Î²52X2+Î²53X3+Î²54X4+U5 Education Level
X6=Î²61X1+Î²62X2+Î²63X3+Î²64X4+Î²65X5+U6Hours per Week
COMPAS =ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³X1=U1 Sex
X2=U2 Age
X3=U3 Race
X4=Î²41X1+Î²42X2+Î²43X3+U4Priors Count
In these above equations, Î²ijare the coefficients for the linear combinations of the Xvariables, and
Uiare the exogenous variables.
C.3 Training Methods
In our study, we utilize various training objectives to train decision-making classifiers, with loss
function â„“(v). The training objectives are as follows:
â€¢Empirical Risk Minimization (ERM) : This approach minimizes the expected risk con-
cerning the classifier parameters Ïˆ, represented by
inf
Î¸âˆˆÎ˜Ezâˆ¼P[â„“(z, Î¸)]
33â€¢Adversarial Learning (AL) : This method trains the model to withstand or defend against
adversarial perturbations, represented by
inf
Î¸âˆˆÎ˜(
Ezâˆ¼P"
sup
âˆ¥âˆ†âˆ¥â‰¤Î´â„“(z+ âˆ†, Î¸)#)
â€¢ROSS : Based on the work of Ross et al. [ 56], this method minimizes the expected risk along
with an adversarial perturbation term, represented by
inf
Î¸âˆˆÎ˜
E(v,y)âˆ¼P
â„“(v, y, Î¸ ) + inf
âˆ¥âˆ†âˆ¥â‰¤Î´â„“(v+ âˆ†,1, Î¸)
â€¢CDRO : Our approach, as described in this paper, is formulated as follows:
inf
Î¸âˆˆÎ˜(
sup
QâˆˆBÎ´(P)Ezâˆ¼Q[â„“(z, Î¸)])
For our loss function â„“, we use the binary cross-entropy loss.
C.4 Hyperparameter Tuning
The majority of the experimental setup is based on the work of Ehyaei et al. [ 22]. For each dataset
and its respective label, we use a generalized linear model (GLM). Each training objective is applied
to four different datasets, using 100 different random seeds. The optimization process is performed
using the Adam optimizer with a learning rate of 10âˆ’3and a batch size of 100. After optimizing
the benchmark time and considering the training rate, we set the number of epochs to 10 to ensure
comparability in benchmarking.
C.5 Metrics
To assess the performance of various training methods concerning accuracy, unfair area, counterfactual
fairness, and adversarial robustness, we employ seven distinct metrics as outlined below:
â€¢Acc : The accuracy of the classifier, is represented as a percentage.
â€¢UÎ´: The proportion of data points within the unfair area with a radius of Î´.
UÎ´:=P 
{vâˆˆ V:âˆƒvâ€²âˆˆ V s.t. d(v, vâ€²)â‰¤Î´âˆ§h(v)Ì¸=h(vâ€²)}
.
â€¢RÎ´: The fraction of data points that are vulnerable to adversarial perturbations within a
radius of Î´. This metric coincides with the unfair area in cases where no sensitive attribute
is considered.
RÎ´:=P 
{vâˆˆ V:âˆƒâˆ†âˆˆ V s.t. d(v,CF(v,âˆ†))â‰¤Î´âˆ§h(v)Ì¸=h(CF(v,âˆ†))}
.
â€¢CF: The percentage of data points that exhibit counterfactual unfairness. This metric aligns
with the unfair area when the perturbation radius is zero.
CF := P 
{vâˆˆ V:âˆƒaâˆˆ A s.t. h(v)Ì¸=h(Â¨va)}
.
C.6 Additional Results
In this section, we present additional simulation results. CDRO performs well across all datasets
except for the RÎ´measure in the Adult dataset, likely because the linear model does not fit the SCM
well. Nevertheless, CDRO demonstrates robustness and counterfactual fairness, as shown in Table 1,
making it the preferred model when balancing both accuracy and fairness.
34AdultCOMPASReal-world Data
AL
CDROERMROSSAL
CDROERMROSS0%10%20%30%Counterfactual Unfair AreaLINSynthetic Data
AL
CDROERMROSS0%10%20%30%40%
AdultCOMPASReal-world Data
AL
CDROERMROSSAL
CDROERMROSS0%1%2%3%4%Non-robust Area (D =0.05)LINSynthetic Data
AL
CDROERMROSS0%5%10%15%Figure 2: Displays the findings from our numerical experiment, assessing the performance of DRO across
different models and datasets. (left) Counterfactual unfair area percentage (lower values are better). (right)
Non-robust area performance of classifier (higher values are better) for âˆ† =.05.
AdultCOMPASReal-world Data
AL
CDROERMROSSAL
CDROERMROSS0%10%20%30%Unfair Area (D =0.01)LINSynthetic Data
AL
CDROERMROSS0%10%20%30%40%
AdultCOMPASReal-world Data
AL
CDROERMROSSAL
CDROERMROSS0.00%0.25%0.50%0.75%Non-robust (D =0.01)LINSynthetic Data
AL
CDROERMROSS0%5%10%15%
Figure 3: Displays the findings from our numerical experiment, assessing the performance of DRO across
different models and datasets. (Left) Bar plot showing the comparison of models based on the unfair area
percentage U(Î´)(lower values are better) at âˆ† =.01. (Right) Bar plot showing the comparison of models based
on the robustness area percentage R(Î´)(lower values are better) at âˆ† =.01.
Broader Impact Statement
Our theoretical framework bridges adversarial robustness, distributional robustness, individual fair-
ness, and causality, aligning with the core pillars of responsible AI. By demonstrating the connection
between these areas, we aim to inspire further research at their intersection and contribute to the
development of safer, more equitable AI models for society. This approach holds the promise of
improving decision-making under uncertainty while ensuring fairness and mitigating the impact of
adversarial perturbations.
However, we acknowledge several limitations and ethical implications inherent in our approach.
While our method produces fair and robust predictions under specific conditions, it is important
to note that it fundamentally relies on a machine learning model, which may inherit the same
vulnerabilities as the original model in areas not explicitly addressed in this work such as multiplicity,
Real-World Data Synthetic Data
Adult COMPAS LIN
Trainer AccU0.5U0.1CFR.05R.01 AccU0.5U0.1CFR.05R.01 AccU0.5U0.1CFR.05R.01
AL 0.79 0.06 0.04 0.04 0.03 0.01 0.67 0.2 0.17 0.16 0.04 0.01 0.67 0.2 0.19 0.19 0.11 0.1
CDRO 0.73 0.04 0.01 0 0.04 0.01 0.66 0.05 0.02 0.01 0.04 0.01 0.66 0.04 0.03 0.03 0.02 0.02
ERM 0.79 0.05 0.02 0.02 0.03 0 0.68 0.26 0.23 0.22 0.04 0.01 0.69 0.4 0.39 0.38 0.15 0.13
ROSS 0.78 0.06 0.04 0.03 0.03 0.01 0.67 0.33 0.3 0.3 0.04 0.01 0.69 0.44 0.43 0.43 0.18 0.17
Table 1: The table presents the results of our numerical experiment, comparing various trainers
based on their input sets in terms of accuracy (Acc, higher values are better), unfairness areas ( U.05,
lower values are better), unfairness areas ( U.01, lower values are better), Counterfactual Unfair area
(CF, lower values are better), the non-robust percentage concerning adversarial perturbation with
radii 0.05(R.05, lower values are better), and the non-robust percentage concerning adversarial
perturbation with radii 0.01(R.01, lower values are better). The top-performing techniques for each
trainer, dataset, and metric are highlighted in bold. The findings demonstrate that CDRO excels in
reducing unfair areas. The average standard deviation for CDRO is .029, while for the other methods,
it is .031.
35privacy breaches, lack of explainability, and safety/security concerns. Additionally, our work operates
under simplifying assumptions regarding the fairness notion. In real-world applications, fairness is a
complex and context-dependent concept. Therefore, it is essential to define fairness carefully and
consider multiple dimensions of fairness when applying our approach. We emphasize that this work
is a proof of concept, and we strongly recommend involving diverse stakeholders, including ethicists,
domain experts, and affected communities, before applying our approach to high-risk application
domains. Itâ€™s important for users to be aware of these limitations and potential biases that might not
be fully addressed by our framework.
36D NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: Our claims in the abstract and the introduction match the body of the text, the
provided proofs, and the reported experimental results.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss the main limitations of our work in the conclusion.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We provide general assumptions in the beginning, and all the lemmas and
theorems have either a full proof or a general intuition in the main body of the paper, with
the remainder of proofs in the appendix.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We mention the used datasets and models, the random seeds, and details about
the data splits in the main body of the paper and the appendix. Information about libraries
and used algorithms is also provided.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We mention the used datasets and models, the random seeds, and details about
the data splits in the main body of the paper and the appendix. Information about libraries
and used algorithms is also provided.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We give detailed results in the form of tables (with mean and std), while we
only plot the mean values to not overcrowd the figures and just report the standard deviation
in the table caption.
8.Experiments Compute Resources
37Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: The experiments are hardware agnostic and use fairly small models and
datasets.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We include a broader impact statement in the appendix.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our paper does not create new data or models.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We properly credit the used datasets.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We documented the code.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
38