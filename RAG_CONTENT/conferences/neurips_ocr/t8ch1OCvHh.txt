Mitigating Quantization Errors Due to Activation
Spikes in GLU-Based LLMs
Anonymous Author(s)
Affiliation
Address
email
Abstract
Modern large language models (LLMs) have established state-of-the-art perfor- 1
mance through architectural improvements, but still require significant computa- 2
tional cost for inference. In an effort to reduce the inference cost, post-training 3
quantization (PTQ) has become a popular approach, quantizing weights and acti- 4
vations to lower precision, such as INT8. In this paper, we reveal the challenges 5
of activation quantization in GLU variants [ 40], which are widely used in feed- 6
forward network (FFN) of modern LLMs, such as LLaMA family. The problem is 7
that severe local quantization errors, caused by excessive magnitudes of activation 8
in GLU variants, significantly degrade the performance of the quantized LLM. We 9
denote these activations as activation spikes . Our further observations provide a 10
systematic pattern of activation spikes: 1) The activation spikes occur in the FFN of 11
specific layers, particularly in the early and late layers, 2) The activation spikes are 12
dedicated to a couple of tokens, rather than being shared across a sequence. Based 13
on our observations, we propose two empirical methods, Quantization-free Module 14
(QFeM) and Quantization-free Prefix (QFeP), to isolate the activation spikes during 15
quantization. Our extensive experiments validate the effectiveness of the proposed 16
methods for the activation quantization, especially with coarse-grained scheme, of 17
latest LLMs with GLU variants, including LLaMA-2/3, Mistral, Mixtral, SOLAR, 18
and Gemma. In particular, our methods enhance the current alleviation techniques 19
(e.g., SmoothQuant) that fail to control the activation spikes.120
1 Introduction 21
Large language models (LLMs) have become a key paradigm in natural language processing, acceler- 22
ating the release of variations within the community [ 49,58]. Furthermore, latest LLMs establish 23
state-of-the-art performance by training with increased scale, as well as by adopting architectural 24
improvements such as GLU [ 40], RoPE [ 41], GQA [ 2], and MoE [ 21]. Especially, GLU (Gated 25
Linear Unit) variants (e.g., SwiGLU, GeGLU) has been adopted in the most of modern LLM archi- 26
tectures (e.g., LLaMA family [ 46]), due to training efficiency [ 31,40]. Although LLMs broaden 27
foundational capabilities in natural language tasks and potential for various applications, billions of 28
parameters in the large models impose considerable computational costs on end users in practice. To 29
reduce GPU memory requirements and accelerate inference speed, post-training quantization (PTQ) 30
offers an affordable solution by quantizing weights and activations into a lower precision (e.g., INT8) 31
without a need for expensive retraining steps [ 17,19,30]. However, recent studies have revealed that 32
large magnitude values at certain coordinates exist in the activations of LLMs, which are often called 33
outliers, posing a key challenge in activation quantization [ 1,12,50,51]. Another line of works 34
attempts to explain the role of outlier values in the attention mechanism [ 9,42]. Nevertheless, current 35
research on the impact of evolving LLM architectures on the outliers remains insufficient. 36
1Code is available at https://anonymous.4open.science/r/activation-spikes-EDF0 .
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.In this paper, we present our discovery that the GLU architecture in the feed-forward network (FFN) 37
generates excessively large activation values, which are responsible for significant local quantization 38
errors. Specifically, we observe that these problematic activation values occur in specific linear 39
layers and are dedicated to a couple of tokens, which will be discussed in Section 3. To distinguish 40
the excessive GLU activations from the outliers, we refer to them as activation spikes . In light of 41
our observations, we propose two empirical methods to mitigate the impact of activation spikes 42
on quantization: Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP). QFeM 43
aims to partially exclude quantization for linear layers (or modules) where large quantization errors 44
occur, instead of quantizing the entire linear modules in the LLM. By scoring the extent of scale 45
disparity, QFeM selects linear modules to exclude. On the other hand, QFeP identifies the prefix that 46
triggers activation spikes and preserves its context as a key-value (KV) cache, thereby preventing the 47
recurrence of activation spikes in subsequent tokens. It is noteworthy that both QFeM and QFeP rely 48
on calibration results to capture activation spikes in advance, without any modifications to the target 49
LLM. This indicates that our methods can be integrated into any existing quantization methods. 50
In our comprehensive experiments, we demonstrate that recently released LLMs incorporating 51
GLU variants struggle with activation spikes when applying activation quantization. Consequently, 52
the proposed methods, QFeM and QFeP, substantially enhance the performance of the primitive 53
quantization method, the round-to-nearest (RTN) method. Furthermore, we observe that current 54
outlier alleviation methods [ 50,51] are exposed to the activation spikes and benefit from our proposed 55
methods. Compared to the strong baseline of fine-grained activation quantization [ 55], our methods 56
show competitive performance, achieving reduced latency and memory footprint. 57
In summary, the contributions of our work are as follows: 58
‚Ä¢We find that the GLU architecture in modern LLMs systematically generates excessive activation 59
values, which are responsible for significant performance degradation in activation quantization. 60
‚Ä¢Based on our observations, we propose two empirical methods, QFeM and QFeP, which effectively 61
exclude the activation spikes during quantization, with negligible computational overhead and 62
compatibility with any existing quantization techniques. 63
‚Ä¢Our extensive experimental results validate the detrimental impact of the activation spikes on activa- 64
tion quantization, while our proposed methods consistently enhance the quantization performance. 65
2 Related Works 66
Outlier Values in LLMs. Previously, outlier values have been observed in the transformer-based 67
language models such as BERT [ 14] and early GPT [ 36] models through numerous studies [ 8,24, 68
27,35,45]. Since the advent of LLMs [ 10,57] rooted in the GPT, recent studies by [ 1,12,51] have 69
tackled the existence of outlier values in LLMs. According to them, these outliers exhibit a large 70
magnitude of values at the shared dimensions of hidden states across tokens. More recently, [ 9,42] 71
explain that the outliers attribute to the vertical pattern in the attention mechanism [ 25,52], which 72
influences the performance of LLMs. In particular, [ 42] claims a different type of outlier existing in 73
the hidden states of specific tokens. However, prior studies merely focus on the superficial hidden 74
states between the decoder layers. Our work provides a module-level investigation where quantization 75
is applied practically, focusing on different LLM architectures. 76
Post-training Quantization for LLMs. Post-training quantization (PTQ) refers to the quantization 77
of a neural network model to low precision, such as INT8, without additional parameter updates [ 17, 78
19]. Especially for LLMs, this approach cost-effectively achieves inference with low memory usage 79
and faster inference latency by quantizing the weights and activations used in matrix multiplication 80
(e.g., linear layer). However, because of the challenges in activation quantization of LLMs, many 81
recent works are mainly focused on the weight-only quantization [ 11,13,15,23,26,39,54]. 82
Otherwise, the activation quantization faces inherent outliers, which hinder accurate quantization 83
by reducing representation resolution. To address this challenge, [ 12] proposes a mixed-precision 84
quantization method where the outlier dimensions are computed in high precision. [ 50,51] approach 85
migration of scale from activation to weights to alleviate the scale of outlier activations. Along this 86
line of research, we propose to enhance the activation quantization based on our observations. 87
2ActivationSpikesActivationSpikes(a) GLU-implemented LLMs(b) Non GLU-implemented LLMsFigure 1: Calibration results on GLU-implemented and non GLU-implemented LLMs. We present
the maximum magnitudes of input activations for each linear modules and layer-wise hidden states.
For more results on different LLMs, see Appendix A.2, A.3.
3 Activation Spikes: Excessive Magnitude of GLU Activations 88
For clarity, "hidden states" refer to the output tensor of a transformer layer (or block), while "input 89
activations" or "activations" denote the input tensor of a linear layer (or module) in the remain of this 90
paper. Recent work [ 42] has investigated a novel type of outlier existing in the hidden states across 91
modern LLMs. Although these outliers of hidden states play a crucial role in the attention mechanism 92
[9,42,52], their relationship with input activations for quantization has not been fully explored. 93
Importantly, because recent LLMs adopt Pre-LN [ 4,53], which normalizes hidden states before self- 94
attention and feed-forward network (FFN) blocks, the scale of hidden states does not reflect the scale 95
of input activations within the transformer block. Therefore, we focus on the input activations fed into 96
each linear module within the transformer block to connect to activation quantization. Specifically, we 97
examine the four linear (projection) layers: query (parallel to keyandvalue ),out,up(parallel to 98
gate ), and down modules. For detailed illustration of Pre-LN transformer, please see Appendix D.1. 99
3.1 Existence of Activation Spikes in GLU Variants 100
To analyze the input activations, we employ a calibration method, which is used to estimate the 101
quantization factors such as scale and zero-point. For the calibration data, we use 512 samples 102
randomly collected from the C4 [ 37] training dataset. Afterwards, we feed each sample into the LLM 103
and monitor each hidden state and input activation through the decoder layers. To estimate the scale 104
factor, we use absolute maximum value. The tested LLMs are listed in Appendix A.1. 105
GLU-implemented LLMs exhibit activation spikes at specific layers. In Figure 1a, we display 106
the calibrated scale factors for the LLMs that implement GLU variants (e.g., SwiGLU, GeGLU). 107
Across models, we observe a shared pattern of scale from the results. Within the early and late 108
layers, the down modules in the FFN show noticeable magnitudes of input activations. Note that 109
these input activations are derived from the Hadamard Product within GLU. Thus, the GLU variants 110
generate activation spikes at the specific layers. Interestingly, we notice a high correlation between the 111
emergence of activation spikes and intermediate hidden states of large scale. This indicates that the 112
FFN contributes to amplifying the hidden states via the addition operation in the residual connection 113
[18]. Once the magnitude of the hidden states is exploded, it persists through layers until encounter 114
the activation spikes at late layers. 115
Non GLU-implemented LLMs show modest scale distribution. Figure 1b illustrates the cali- 116
bration results for LLMs with the original feed-forward implementation in Transformer [ 48]. We 117
observe that the LLMs continue to generate the large-scale hidden states, regardless of the GLU 118
implementation. This corresponds to the observations in [ 42]. More importantly, our module-level 119
results elaborate that the scale of hidden states is not transferable to the input activations of inner 120
linear modules. Instead, we reveal that GLU variants are associated with the hidden states and 121
generate activation spikes. This clarifies the quantization challenge of the GLU-implemented LLMs 122
concentrated in the early and late layers. Because excessive scales of activation spikes have the 123
potential to hinder the accurate quantization, we conduct an in-depth analysis to better understand 124
these activation spikes in the following sections. 125
3BOS It ' ssnow \n It ' ssnow \n05001k1.5k
LLaMA-2-7B Layer 2 feed-forward.down
BOS It ' ssnow \n It ' ssnow \n02k4k6k
LLaMA-2-70B Layer 9 feed-forward.down
Per-token Scale
Per-tensor ScaleActivation MagnitudeFigure 2: Token-wise scales in a specific layer with an activation spike. When quantizing the input
activations using a per-tensor scale, the scale of the activation spike dominates the scales of the other
tokens. For more examples, see Appendix D.2.
3.2 Token-level Scale Analysis within Activation Spikes 126
In the previous section, we observed the excessive scale of the input activations derived from GLU 127
activation. When quantizing the input activations, the variance of input activation scales for each 128
token affects the quantization performance [ 55]. To delve into the disparity between token-wise 129
scales in the activation spikes, we unroll them through the sequence of tokens. Figure 2 illustrates 130
the individual input activation scales where the activation spike appears. Given a token sequence, 131
the large magnitudes of input activations are observed in a couple of tokens, such as the BOS token, 132
newline ( \n), and apostrophe ( '). These specific tokens coincide with the observations of [ 42], which 133
suggests that such tokens exhibit massive values in the hidden states. Thus, the activation spike is 134
associated with the process of assigning a special role to these tokens in later transformer layers. 135
However, the excessive scale of specific token hinders the estimation of scale factor for the other 136
tokens, such as in per-tensor quantization. Additionally, the largest scale is dedicated to the first 137
instance of the specified token, while the following usage exhibits a modest scale. This phenomenon 138
makes the quantization more complicated, as the activation spikes dynamically occur depending on 139
the current input sequence. 140
3.3 Effect of Quantization on Activation Spikes 141
We explore the impact of local quantization errors caused by activation spikes on LLM outputs. To 142
identify the layers where activation spikes occur, we utilize a ratio between the maximum and median 143
values of the token-wise input activation scales, instead of using the maximum scale value alone. 144
The max-median ratio for linear layer mcan be formulated as r(m)=max( S(m))
median( S(m)), where S(m)145
represents the token-wise input activation scales incoming to module m‚ààM. This max-median 146
ratio captures the extent to which maximum scale dominate the other token scales. For comparison, 147
we choose the activation quantization targets as the top-4, middle-4, and bottom-4 modules, based on 148
the max-median ratio in descending order. Then, we evaluate the perplexity and mean-squared error 149
(MSE) using the calibration dataset. Here, the MSE is calculated for the last hidden states between 150
the original (FP16) and partially quantized LLM. As shown in Table 1, quantization on the top-4 rated 151
modules solely degrades the LLM performance by significant margins, while the other cases exhibit 152
negligible performance changes. We consider these quantization-sensitive input activations ( inter alia 153
activation spikes) to be the quantization bottleneck, which, in this paper, refers to the quantization 154
error caused by outliers. 155
Furthermore, the activation spikes are conditioned on the specific context of the input sequence as 156
discussed in Section 3.2. Altogether, such dynamic bottlenecks must be handled with caution to 157
enhance the quantization performance of LLMs. 158
Table 1: Perplexity and MSE of partial activation quantization of LLMs
ModelPerplexity (‚Üì) MSE (‚Üì)
FP16 Top 4 Middle 4 Bottom 4 Top 4 Middle 4 Bottom 4
LLaMA-2-7B 7.37 11.77 7.38 7.40 1908.80 1.03 12.90
LLaMA-2-13B 6.84 15.09 6.84 6.84 4762.11 0.91 10.38
Mistral-7B 8.35 69.45 8.35 8.36 218.60 0.02 0.18
Gemma-7B 10.85 85.83 10.94 10.87 213.93 1.60 1.07
4Offline Inference
ùë•!ùë•"ùë•#ùë•$ùë•%Quantization-free Module (QFeM)ùëü!139320.93320.94136321.3Quantization-free Prefix (QFeP)Layer ùëñLayer ùëóLayer ùëò
KV CacheINT8quantization-free modulequantization-free module
ùë•!ùë•"ùë•#FP16ùë•$ùë•%ùë•&INT8ùëû/ùëò/ùë£ùëúùë¢ùë°ùë¢ùëù/ùëîùëéùë°ùëíùëëùëúùë§ùëõ
ùëû/ùëò/ùë£ùëúùë¢ùë°ùë¢ùëù/ùëîùëéùë°ùëíùëëùëúùë§ùëõ
ùëû/ùëò/ùë£ùëúùë¢ùë°ùë¢ùëù/ùëîùëéùë°ùëíùëëùëúùë§ùëõùëü!139320.93320.94136321.3ùëü!2320.93320.92321.3Input Sequence:quantization-free prefixFigure 3: Overview of QFeM and QFeP. (Left): QFeM excludes the modules whose r(m)is larger than
the hyperparameter Œ±from quantization. (Right): QFeP computes in advance the prefix of activation
spikes and utilizes solely their KV cache during the quantization phase, effectively preventing further
activation spikes in subsequent sequences.
4 Mitigating Quantization Quality Degradation Based on the Observation 159
To address the quantization bottleneck, our approach is based on the deterministic occurrence patterns 160
of activation spikes. First, we utilize the observation that bottlenecks occur at a few specific layers. 161
This implies that naive full quantization of LLMs is affected by these bottlenecks. Second, we exploit 162
the phenomenon that the activation spike is derived from the first occurrence of specific tokens. Thus, 163
the planned occurrence prevents recurrence in the subsequent and possibly future tokens. In the 164
following sections, we propose two methods inspired the above insights. 165
4.1 Quantization-free Module (QFeM) 166
In the full quantization of LLM, all linear layers within the LLM are quantized. Among these 167
linear layers, we propose omitting the quantization of input activations for linear layers where 168
significant quantization errors are caused by activation spikes. To be noted, increasing the number of 169
unquantized modules exhibits a trade-off between the inference latency and the model performance. 170
Thus, determining which module should be quantized (or left unquantized) is crucial to retain the 171
efficacy of quantization. Here, we use the max-median ratio r(m)and define a set of unquantized 172
modules, denoted as Munq, where the ratio r(m)of each linear layer is larger than threshold Œ±. For 173
instance, all linear layers in Mare quantized if Œ±=‚àû. For clarity, we treat sibling linear layers, 174
such as query-key-value, as a single linear layer. To control the impact of activation quantization only, 175
we leave the weight parameters in unquantized linear layers as INT8 and dequantize them into FP16 176
during matrix multiplication with the incoming activations, operating as weight-only quantization. 177
1248163264128256inf
Threshold 
050100150
101520
|Munq|
Perplexity
Figure 4: Trade-off between perplex-
ity (stands for performance) and |Munq|
(stands for latency) according to the
threshold Œ±for LLaMA-2-13B model.Optimizing the threshold Œ±.To calculate the activation 178
scale ratio for each linear layer, we first gather token-wise 179
input activation scales from the calibration examples dis- 180
cussed in Section 3.1. Exceptionally, for FFN experts in 181
the mixture of experts (MoE) architectures like the Mix- 182
tral model [ 21], calibration is performed separately. After 183
determining these ratios, we use binary search to set the 184
threshold value Œ±, balancing inference latency and perfor- 185
mance degradation. As a metric, we assess performance 186
through perplexity measured on the same calibration ex- 187
amples. For example, the relationship between threshold 188
value Œ±and its impact on performance is depicted in Fig- 189
ure 4, demonstrating how full quantization can degrade 190
performance. Rather than fully quantizing, we identify an 191
optimal threshold by finding the intersection of two performance curves; in Figure 4, this threshold is 192
approximately 16. Details on the QFeM implementation are provided in Table 2. 193
54.2 Quantization-free Prefix (QFeP) 194
Orthogonal to the QFeM, we propose Quantization-free Prefix (QFeP) that mitigates the quantization 195
errors by precomputing the prefix (or short prompt) corresponding to activation spikes. This method 196
is based on the observations presented in Section 3.2, which indicate that significant quantization 197
errors result from the overestimated scale factor of the first instance within the restricted token 198
set. Inspired by this occurrence pattern of activation spikes, we aim to construct a prefix which 199
stabilizes the quantization scale factor of the tokens that come after the prefix. In other words, 200
once the prefix is fixed at the beginning, the activation spikes consistently occur within the prefix. 201
Afterward, we employ key-value (KV) caching mechanism to process the activation spikes in advance. 202
In practice, KV cache is utilized to optimize the decoding speed of causal language models by storing 203
precomputed key and value states of the previous tokens [ 32,34]. This approach provides a bypass 204
of the quantization including activation spikes, while preserving the context of prefix through the 205
KV cache. The KV cache for the prefix is precomputed once through the offline inference of LLM 206
without quantization. Then, this KV cache is exploited in the quantization phases, such as calibration 207
or dynamic quantization, even for quantized inference. The process of QFeP is illustrated in Figure 3. 208
Prefix Search. To form a prefix of explicit activation spike, we first identify candidate token that 209
represent the activation spike at the linear layer with the highest max-median ratio r(m). For instance, 210
the candidate token can be apostrophe ( ') token for LLaMA-2-70B model, as highlighted in red in 211
Figure 2. Once the candidate token is identified, we search the middle context token for between 212
the BOS token and the candidate token in the prefix. This middle context provides dummy context, 213
which is required to activate the candidate token. To find the middle context, we design a template 214
[B, T 1, C1, T2, C2]where B,Ti, andCidenote the BOS token, context token, and candidate token in 215
the vocabulary V, respectively. Then, we select the context token Twhere C1triggers an activation 216
spikes, while later instance of the same token C2does not. When the context token for the activation 217
spikes is varied, we choose the token that maximizes the activation scale ratio between the C1and 218
C2. Finally, we prepare the KV cache for searched prefix of [B, T, C ]. Note that the latter sequence 219
in the template can be replaced with sequences from dataset instead of repetition. 220
Table 2: Specifications for QFeM and QFeP used
in experiments. |M|denotes the total number of
linear layers in the LLM, and |Munq|represents
the number of unquantized layers for QFeM.
Model Prefix Œ± |Munq|/|M|
LLaMA-2-7B [BOS] all . 6.68 17 / 128
LLaMA-2-13B [BOS] then , 12.91 6 / 160
LLaMA-2-70B [BOS] I ‚Äô 9.16 25 / 320
Mistral-7B [BOS] how \n 49.00 3 / 128
Mixtral-8x7B [BOS] ). \n 4.03 191 / 608
SOLAR-10.7B [BOS] a 1 6.48 11 / 192
Gemma-7B [BOS] . Pi√π 10.65 5 / 112
LLaMA-3-8B [BOS] - nd 6.64 6 / 128
LLaMA-3-70B [BOS] and , 78.37 3 / 320Implementation Details. During the prefix 221
search phase, we exploit the calibration dataset 222
used in Section 3.1. For the candidate tokens, we 223
consider the tokens with the top three largest in- 224
put activation magnitudes. Then, we search for 225
the middle context token among top 200 most fre- 226
quent tokens in the calibration dataset, which is 227
the subset of the vocabulary V. Finally, with the 228
search result, we prepare the KV cache for the 229
target model in FP16 precision. Exceptionally, for 230
the Mixtral [ 21] model, we use the scale of output 231
hidden states instead of input activations, as the 232
tokens are divided sparsely in a mixture of experts 233
architecture. Table 2 presents the searched prefix. 234
5 Experiments 235
5.1 Experimental Setup 236
Models. Our proposed methods, QFeM and QFeP, aim to mitigate the quantization bottleneck, 237
which is discussed in Section 3.3, caused by the activation spikes, especially in the GLU variants. To 238
validate the efficiency proposed methods, we tested publicly released LLMs that were implemented 239
with GLU, according to their paper and source code. We recognize recent LLMs, including LLAMA- 240
2-{7B, 13B, 70B} [ 47], LLaMA-3-{7B, 70B}, Mistral-7B [ 20], Mixtral-8x7B [ 21], SOLAR-10.7B 241
[22], and Gemma-7B [ 43], utilize the GLU architecture. The LLMs with original FFN are not 242
covered, as they suffer from the existing outliers rather than activation spikes. All models are sourced 243
from the huggingface-hub2repository. 244
2https://huggingface.co/models
6Table 3: Perplexity and zero-shot evaluation for the quantization on LLaMA-2 models. FP16 denotes
the original model precision, and W8A8 denotes the model quantized to INT8 for both weights and
activations.
MethodWikiText-2
(ppl‚Üì)PIQA
(acc‚Üë)LAMBADA
(acc‚Üë)HellaSwag
(acc‚Üë)WinoGrande
(acc‚Üë)Avg
(acc‚Üë)
LLaMA-2-7B
FP16 5.268 78.18% 73.67% 57.13% 69.46% 69.61%
W8A8 8.634 72.80% 62.27% 49.57% 63.69% 62.08%
+QFeM 5.758[ -2.876 ] 78.02% 73.86% 56.32% 68.35% 69.14%[ +7.06 ]
+QFeP 5.758[ -2.876 ] 76.44% 73.57% 55.55% 69.22% 68.69%[ +6.61 ]
+QFeM+QFeP 5.573[ -3.061 ] 77.86% 74.58% 56.05% 69.38% 69.47%[ +7.39 ]
LLaMA-2-13B
FP16 4.789 79.49% 76.54% 60.20% 72.38% 72.15%
W8A8 34.089 70.13% 49.66% 42.65% 58.72% 55.29%
+QFeM 5.241[ -28.848 ]77.58% 75.68% 59.13% 72.61% 71.25%[ +15.96 ]
+QFeP 6.000[ -28.089 ]77.53% 73.94% 57.23% 70.96% 69.91%[ +14.62 ]
+QFeM+QFeP 5.126[ -28.963 ]78.51% 75.86% 59.44% 72.61% 71.61%[ +16.32 ]
LLaMA-2-70B
FP16 3.218 81.45% 79.45% 65.29% 80.43% 76.65%
W8A8 8.055 74.05% 70.27% 55.21% 67.96% 66.87%
+QFeM 3.830[ -4.225 ] 81.23% 77.66% 64.15% 78.14% 75.30%[ +8.43 ]
+QFeP 6.007[ -2.048 ] 77.64% 73.26% 63.40% 76.16% 72.62%[ +5.75 ]
+QFeM+QFeP 3.708[ -4.347 ] 81.23% 77.82% 64.65% 77.11% 75.20%[ +8.33 ]
506070Accuracy(%)Mistral-7B
737475Mixtral-8x7B
65.067.570.072.5SOLAR-10.7B
40506070Gemma-7B
55606570LLaMA-3-8B
304050607080LLaMA-3-70BW8A8 QFeM QFeP QFeM+QFeP FP16
Figure 5: The average accuracy of zero-shot evaluation on other GLU-implemented LLMs. Most
models recover significantly compared to W8A8, with performance close to FP16.
Quantization. In the experiments, we quantize both the input activations and the weights of linear 245
layers for INT8 matrix multiplication operations. Note that in Table 2, |M|denotes the total number 246
of linear modules targeted for quantization. In these linear layers, we opt for dynamic per-tensor 247
quantization as the quantization scheme of input activations, and per-channel quantization for weights, 248
respectively. Regarding both input activations and weights, we symmetrically quantize the range 249
using the absolute maximum value as the scale estimation function. For comparison, we use FP16 250
and per-token activation quantization [ 55] as baselines. We refer the reader to Appendix B for Batch 251
Matrix-Multiplication (BMM) quantization, which involves quantizing tensors in the self-attention. 252
Evaluations. We evaluate the quantized LLMs with two metrics: zero-shot evaluation accuracy 253
and perplexity. For zero-shot evaluation, we use the four datasets: PIQA [ 7], LAMBADA [ 33], 254
HellaSwag [ 56], and WinoGrande [ 38]. We utilize the lm-evaluation-harness library [ 16] to evaluate 255
zero-shot tasks. To measure perplexity, we use the WikiText-2 [ 28] dataset. In all cases, we use the 256
[BOS] token as the starting token for each input sequence by default. 257
5.2 Main Results 258
LLaMA-2 Models. We report the evaluation results of quantization on LLaMA-2 models in Table 3. 259
Compared to FP16 precision, quantizing both weights and activations (W8A8) degrades the overall 260
performance. The results demonstrate that our proposed methods resolve the activation spikes 261
and, surprisingly, restore the performance of the W8A8 close to that of FP16. For example, the 262
LLaMA-2 7B model achieves less than a 1% performance drop from FP16. It is worth noting that the 263
7Table 4: Evaluation of outlier alleviation methods with QFeM and QFeP. We report perplexity on
WikiText-2 and averaged accuracy of four zero-shot tasks. The same quantization scheme for used
on both SQ and OSP. Per-tensor weight quantization results are provided in Appendix C.1.
MethodLLaMA-2-7B LLaMA-2-13B LLaMA-2-70B
ppl(‚Üì) acc( ‚Üë) ppl( ‚Üì) acc( ‚Üë) ppl( ‚Üì) acc( ‚Üë)
SQ [51] 9.907 61.08% 34.869 59.45% 8.800 70.25%
+QFeM 5.534 69.65% 5.118 71.23% 3.599 75.93%
+QFeP 5.715 68.66% 6.551 69.33% 5.228 74.07%
OSP [50] 38.490 59.90% 5.148 71.29% 3.827 75.52%
+QFeM 5.493 69.37% 5.099 71.37% 3.559 75.92%
+QFeP 5.642 68.95% 5.144 71.05% 3.752 75.36%
proposed QFeM and QFeP improve at comparable levels. This indicates that the activation spikes 264
present a direct cause of the significant decrease in quantization performance. Because the proposed 265
methods are orthogonal, the performance slightly increases when incorporating both QFeM and QFeP 266
compared to applying them individually. 267
Other GLU-implemented LLMs. For other LLMs that incorporate GLU, we investigated the 268
effectiveness of our methods in mitigating the quantization bottleneck. As can be seen in Figure 5, 269
our methods consistently remedy the performance drop caused by activation spikes. Noticeably, 270
the Mixtral model demonstrates robustness towards the performance degradation. This indicates 271
that the mixture of experts architecture, which divides the MLP experts by tokens, helps to alleviate 272
the impact of the activation spikes. Meanwhile, addressing the activation spikes is not a sufficient 273
complement for the Gemma model compared to other models. We attribute this to the choice of 274
activation function among GLU variants; specifically, Gemma uses GeGLU, while other models 275
employ SwiGLU. 276
5.3 Combining Outlier Alleviation Methods 277
While our method focuses on the activation spikes, the inherent outlier values in the input activations 278
remain. Here, we combine the prior outlier alleviation methods, such as SmoothQuant (SQ) [ 51] 279
and OutlierSuppressionPlus (OSP) [ 50], to further improve the quantization error. In practice, our 280
methods are utilized during the scale calibration phase of alleviation methods to mitigate the impact 281
of activation spikes on scale migration between activations and weights. Table 4 demonstrates the 282
evaluation results of applying the outlier alleviation methods solely and combining them with our 283
methods. We find that there are cases where the alleviation method fails to recover the performance 284
when quantizing the activations with per-tensor scheme.3This indicates that alleviating the outlier 285
scales, including the activation spikes, is challenging. With the QFeM, the activation spikes are 286
excluded, and the accurate alleviation is enabled. In addition, the QFeP also benefits from the SQ 287
method, as seen in the case of LLaMA-2 70B. Exceptionally, the OSP successfully addresses the 288
activation spikes in the 13B and 70B cases. 289
5.4 Ablation Study 290
6570Accuracy(%)LLaMA-2-7B
506070Mistral-7B
6570SOLAR-10.7BRandom
BOSQFeP (w/o context) QFeP (w/ context)
Figure 6: Prefix ablation. Y-axis represents
averaged accuracy of four zero-shot tasks.For the QFeP, we designed a length-three prefix for 291
the KV cache, including the BOS token, context to- 292
ken, and extra token for activation spike. Because the 293
KV cache consumes the capacity of the pretrained se- 294
quence position, it raises a question about the length 295
of the prefix. Therefore, we conduct ablation study 296
for different prefixes for the KV cache. For the pre- 297
fixes, we prepare random, BOS only, and both QFeP without and with the context token. We illustrate 298
the results of ablation study in Figure 6. In all cases, the random prefix showcases the lowest perfor- 299
mance. While the KV cache with the BOS token demonstrates inconsistent performance, our QFeP 300
3In their papers, the activations of LLaMA models are quantized using only a per-token scheme.
8270 280 290 300 310 320
Latency (ms)3040506070Accuracy (%)LLaMA-2-13B
(GPU=RTX4090, token length=2000)
QauntizationScheme
FP16
AQ1
AQ2
AQ2 + QFeP
AQ2 + QFeM
AQ3
AQ3 + QFeP
AQ3 + QFeM
1350 1400 1450 1500
Latency (ms)505560657075LLaMA-2-70B
(GPU=A100, token length=2000)
QauntizationScheme
FP16
AQ1
AQ2
AQ2 + QFeP
AQ2 + QFeM
AQ3
AQ3 + QFeP
AQ3 + QFeMFigure 7: Accuracy-latency comparison of different activation quan-
tization schemes: dynamic per-token (AQ1), dynamic per-tensor
(AQ2), and static per-tensor (AQ3).Table 5: Memory footprint.
MethodSeqLen
1K 2K
LLaMA-2-7B
AQ1 8185MiB 9516MiB
AQ2 8148MiB 9474MiB
+QFeP 8149MiB 9478MiB
+QFeM 8148MiB 9474MiB
LLaMA-2-70B
AQ1 67756MiB 69037MiB
AQ2 67648MiB 68820MiB
+QFeP 67651MiB 68822MiB
+QFeM 67838MiB 68819MiB
consistently shows significant improvement. Importantly, the results imply that the sufficient prefix 301
for the models exhibits differences. However, we emphasize that our KV design for QFeP shows 302
improvements by large margins across all models. 303
5.5 Computational Cost Analysis 304
The proposed methods require additional resources to evict the activation spikes. Therefore, we ana- 305
lyze the computational costs of the methods and compare them in various schemes. For comparison, 306
we evaluate different activation quantization schemes: dynamic per-token, dynamic per-tensor, and 307
static per-tensor, denoted as AQ1, AQ2, and AQ3, respectively. This distinction establishes strong 308
baselines and demonstrates the potential of the methods. To calibrate the static scales, we estimate 309
the absolute maximum value using the calibration dataset, which is used in Section 3.1. 310
Inference Latency. For each setting, we present the accuracy of the zero-shot tasks and inference 311
latency of the fixed token sequence, as shown in Figure 7. While the fine-grained scheme (AQ1) shows 312
a negligible accuracy drop, the counterparts (AQ2, AQ3) degrade with the quantization bottleneck. 313
However, by applying our methods, the coarse-grained schemes achieve a competitive performance 314
gain. For example, the combination of AQ2 and QFeM demonstrates the performance close to 315
the AQ1 but with faster latency. The results signify that addressing the quantization bottleneck 316
is important to accelerate the inference latency with coarser granularity. Specifically, the naive 317
static quantization (AQ3), the fastest scheme, exhibits a significant decline. We hope that our work 318
contributes to the future works, which address the remaining challenges in static quantization. 319
Memory Footprint. In Table 5, we record the maximum memory footprint of our methods. For 320
QFeP, the additional memory is consistently required for the preserved KV cache. However, this 321
memory overhead is much smaller than that used in the fine-grained quantization (AQ1), as QFeM 322
utilizes only three tokens for the cache. Contrary to QFeP, QFeM shows inconsistent memory 323
utilization. For example, the 7B model with QFeM exhibits memory usage similar to AQ2, while the 324
70B model with QFeM incur additional consumption for a sequence length of 1K. This is attributed to 325
the use of W8A16 for the unquantization modules in QFeM. To tailor the memory usage or inference 326
speed, an alternative strategy can be utilized for QFeM, such as applying fine-grained activation 327
quantization to the unquantization modules instead of using W8A16. 328
6 Conclusion 329
We explore the quantization challenge of GLU activations for modern LLMs. We find that the GLU 330
variants generates excessive activation scales, which cause significant quantization bottlenecks at 331
the specific layers. Based on the systematic generation pattern of the activation spikes, we propose 332
methods that address the spikes in a layer-wise (QFeM) and token-wise manner (QFeP). In the 333
experiments, we confirm that the proposed methods effectively resolve the quantization bottlenecks 334
and result in a large performance gain. We expect that our work sheds light on the potential challenges 335
in future studies regarding quantization and facilitates the development of efficient LLM systems. 336
9References 337
[1]Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh, Zhen Stephen Gou, Phil 338
Blunsom, Ahmet √úst√ºn, and Sara Hooker. Intriguing properties of quantization at scale. 339
Advances in Neural Information Processing Systems , 36:34278‚Äì34294, 2023. 340
[2]Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebr√≥n, and 341
Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head 342
checkpoints. arXiv preprint arXiv:2305.13245 , 2023. 343
[3]Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxan- 344
dra Cojocaru, M√©rouane Debbah, √âtienne Goffinet, Daniel Hesslow, Julien Launay, Quentin 345
Malartic, et al. The falcon series of open language models. arXiv preprint arXiv:2311.16867 , 346
2023. 347
[4]Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. 348
InInternational Conference on Learning Representations , 2018. 349
[5]Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth 350
Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 b 351
technical report. arXiv preprint arXiv:2402.17834 , 2024. 352
[6]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O‚ÄôBrien, 353
Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward 354
Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In 355
International Conference on Machine Learning , pages 2397‚Äì2430. PMLR, 2023. 356
[7]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys- 357
ical commonsense in natural language. In Proceedings of the AAAI conference on artificial 358
intelligence , volume 34, pages 7432‚Äì7439, 2020. 359
[8]Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Understanding and overcoming 360
the challenges of efficient transformer quantization. In Marie-Francine Moens, Xuanjing 361
Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on 362
Empirical Methods in Natural Language Processing , pages 7947‚Äì7969, Online and Punta Cana, 363
Dominican Republic, November 2021. Association for Computational Linguistics. 364
[9]Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: 365
Removing outliers by helping attention heads do nothing. Advances in Neural Information 366
Processing Systems , 36, 2024. 367
[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, 368
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are 369
few-shot learners. Advances in neural information processing systems , 33:1877‚Äì1901, 2020. 370
[11] Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantiza- 371
tion of large language models with guarantees. Advances in Neural Information Processing 372
Systems , 36, 2024. 373
[12] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix 374
multiplication for transformers at scale. Advances in Neural Information Processing Systems , 375
35:30318‚Äì30332, 2022. 376
[13] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh 377
Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. Spqr: A sparse-quantized 378
representation for near-lossless llm weight compression. arXiv preprint arXiv:2306.03078 , 379
2023. 380
[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of 381
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 , 382
2018. 383
[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training 384
quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022. 385
10[16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles 386
Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac‚Äôh, Haonan Li, Kyle McDonell, Niklas 387
Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, 388
Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework 389
for few-shot language model evaluation, 12 2023. 390
[17] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. 391
A survey of quantization methods for efficient neural network inference. In Low-Power Com- 392
puter Vision , pages 291‚Äì326. Chapman and Hall/CRC, 2022. 393
[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual 394
networks. In Computer Vision‚ÄìECCV 2016: 14th European Conference, Amsterdam, The 395
Netherlands, October 11‚Äì14, 2016, Proceedings, Part IV 14 , pages 630‚Äì645. Springer, 2016. 396
[19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, 397
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for 398
efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer 399
vision and pattern recognition , pages 2704‚Äì2713, 2018. 400
[20] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh 401
Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile 402
Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825 , 2023. 403
[21] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris 404
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, 405
et al. Mixtral of experts. arXiv preprint arXiv:2401.04088 , 2024. 406
[22] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeon- 407
woo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, et al. Solar 10.7 b: Scaling large language 408
models with simple yet effective depth up-scaling. arXiv preprint arXiv:2312.15166 , 2023. 409
[23] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W 410
Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization. arXiv preprint 411
arXiv:2306.07629 , 2023. 412
[24] Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and Anna Rumshisky. BERT busters: Out- 413
lier dimensions that disrupt transformers. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto 414
Navigli, editors, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 , 415
pages 3392‚Äì3405, Online, August 2021. Association for Computational Linguistics. 416
[25] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark 417
secrets of BERT. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings 418
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th 419
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 420
4365‚Äì4374, Hong Kong, China, November 2019. Association for Computational Linguistics. 421
[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: 422
Activation-aware weight quantization for llm compression and acceleration. arXiv preprint 423
arXiv:2306.00978 , 2023. 424
[27] Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. Positional artefacts propagate through masked 425
language model embeddings. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, edi- 426
tors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics 427
and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long 428
Papers) , pages 5312‚Äì5327, Online, August 2021. Association for Computational Linguistics. 429
[28] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture 430
models. arXiv preprint arXiv:1609.07843 , 2016. 431
[29] Javaheripi Mojan and Bubeck S√©bastien. Phi-2: The surprising power of small language models, 432
2023. 433
11[30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, 434
and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint 435
arXiv:2106.08295 , 2021. 436
[31] Sharan Narang, Hyung Won Chung, Yi Tay, Liam Fedus, Thibault Fevry, Michael Matena, 437
Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, 438
Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modifications transfer across 439
implementations and applications? In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, 440
and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in 441
Natural Language Processing , pages 5758‚Äì5773, Online and Punta Cana, Dominican Republic, 442
November 2021. Association for Computational Linguistics. 443
[32] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, 444
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint 445
arXiv:1904.01038 , 2019. 446
[33] Denis Paperno, Germ√°n Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, 447
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern√°ndez. The LAMBADA 448
dataset: Word prediction requiring a broad discourse context. In Katrin Erk and Noah A. Smith, 449
editors, Proceedings of the 54th Annual Meeting of the Association for Computational Linguis- 450
tics (Volume 1: Long Papers) , pages 1525‚Äì1534, Berlin, Germany, August 2016. Association 451
for Computational Linguistics. 452
[34] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan 453
Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. 454
Proceedings of Machine Learning and Systems , 5, 2023. 455
[35] Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice Dell‚ÄôOrletta. Outlier dimensions 456
that disrupt transformers are driven by frequency. In Yoav Goldberg, Zornitsa Kozareva, and Yue 457
Zhang, editors, Findings of the Association for Computational Linguistics: EMNLP 2022 , pages 458
1286‚Äì1304, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational 459
Linguistics. 460
[36] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 461
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. 462
[37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, 463
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified 464
text-to-text transformer. Journal of machine learning research , 21(140):1‚Äì67, 2020. 465
[38] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An 466
adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641 , 2019. 467
[39] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng 468
Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally calibrated quantiza- 469
tion for large language models. arXiv preprint arXiv:2308.13137 , 2023. 470
[40] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020. 471
[41] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: 472
Enhanced transformer with rotary position embedding. Neurocomputing , 568:127063, 2024. 473
[42] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language 474
models. arXiv preprint arXiv:2402.17762 , 2024. 475
[43] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya 476
Pathak, Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open 477
models based on gemini research and technology. arXiv preprint arXiv:2403.08295 , 2024. 478
[44] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially 479
usable llms, 2023. Accessed: 2023-05-05. 480
12[45] William Timkey and Marten van Schijndel. All bark and no bite: Rogue dimensions in trans- 481
former language models obscure representational quality. In Marie-Francine Moens, Xuanjing 482
Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on 483
Empirical Methods in Natural Language Processing , pages 4527‚Äì4546, Online and Punta Cana, 484
Dominican Republic, November 2021. Association for Computational Linguistics. 485
[46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo- 486
th√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open 487
and efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023. 488
[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, 489
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open 490
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023. 491
[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, 492
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information 493
processing systems , 30, 2017. 494
[49] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani 495
Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large 496
language models. arXiv preprint arXiv:2206.07682 , 2022. 497
[50] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, 498
and Xianglong Liu. Outlier suppression+: Accurate quantization of large language models 499
by equivalent and effective shifting and scaling. In Houda Bouamor, Juan Pino, and Kalika 500
Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language 501
Processing , pages 1648‚Äì1665, Singapore, December 2023. Association for Computational 502
Linguistics. 503
[51] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 504
SmoothQuant: Accurate and efficient post-training quantization for large language models. In 505
Proceedings of the 40th International Conference on Machine Learning , 2023. 506
[52] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming 507
language models with attention sinks. arXiv preprint arXiv:2309.17453 , 2023. 508
[53] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, 509
Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. 510
InInternational Conference on Machine Learning , pages 10524‚Äì10533. PMLR, 2020. 511
[54] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive study 512
on post-training quantization for large language models. arXiv preprint arXiv:2303.08302 , 513
2023. 514
[55] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong 515
He. Zeroquant: Efficient and affordable post-training quantization for large-scale transformers. 516
Advances in Neural Information Processing Systems , 35:27168‚Äì27183, 2022. 517
[56] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a 518
machine really finish your sentence? In Anna Korhonen, David Traum, and Llu√≠s M√†rquez, edi- 519
tors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 520
pages 4791‚Äì4800, Florence, Italy, July 2019. Association for Computational Linguistics. 521
[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, 522
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained 523
transformer language models. arXiv preprint arXiv:2205.01068 , 2022. 524
[58] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, 525
Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv 526
preprint arXiv:2303.18223 , 2023. 527
13A Additional Calibration Results 528
In this section, we provide details of LLMs when performing calibration, which is the step during 529
quantization where the FP16 ranges are computed (Appendix A.1), and additional calibration results 530
(Appendix A.2, A.3). 531
A.1 Detailed Specification of LLMs 532
In Section 3.1, we have performed the calibration method on various LLMs. We observe the 533
calibration results by categorizing based on the presence of GLU in the LLMs. Table 6 shows the 534
detailed structures of the LLMs. We refer notations for feed-forward implementiation from [ 40]. In 535
the case of GLU-implemented LLMs, which is LLaMA-2, LLaMA-3, Mistral, Mixtral, SOLAR, 536
StableLM-2, and Gemma, most models have SwiGLU for FFN activation, while only Gemma has 537
GeGLU. On the other hand, in non GLU-implemented LLMs, most of them utilize GeLU for FFN 538
activation, with the exception of OPT, which uses ReLU. 539
Table 6: Architecture specification of LLMs. We categorize them into two groups depending on
whether GLU is implemented in the FFN. All LLMs in the table use Pre-LN for the LayerNorm
position.
Model Size FFN Activation Normalization PE Vocabulary Size
GLU-implemented LLMs:
LLaMA-2 [47] 7B, 13B, 70B SwiGLU RMSNorm RoPE 32000
LLaMA-3 8B, 70B SwiGLU RMSNorm RoPE 128256
Mistral [20] 7B SwiGLU RMSNorm RoPE 32000
Mixtral [21] 8x7B SwiGLU RMSNorm RoPE 32000
SOLAR [22] 10.7B SwiGLU RMSNorm RoPE 32000
StableLM-2 [5] 12B SwiGLU LayerNorm RoPE 100352
Gemma [43] 7B GeGLU RMSNorm RoPE 256000
Non GLU-implemented LLMs:
OPT [57] 6.7B, 13B, 30B, 66B ReLU LayerNorm Learned 50272
MPT [44] 7B, 30B GeLU LayerNorm ALiBi 50432
Pythia [6] 6.9B, 12B GeLU LayerNorm RoPE 50432, 50688
Falcon [3] 7B, 40B GeLU LayerNorm RoPE 65024
Phi-2 [29] 2.7B GeLU LayerNorm RoPE 51200
A.2 Other Calibration Results on GLU-implementation 540
Figure 8, 9 show the calibration result examples for various GLU-implemented LLMs that are not 541
shown in the models in Figure 1a. In most GLU-implemented LLMs, we observe that the input 542
activations have large values near the first and last layers. Unlike the typical GLU-implemented LLM 543
architecture, Mixtral is composed of 8 feed-forward blocks in the single FFN, containing multiple 544
gate linear units [ 21]. According to this structure, we can observe that one of the gates spikes in value 545
in Figure 8. 546
1 8 16 24 32
Layers03k6kActivation MagnitudeMixtral-8x7B
q_proj
o_proj
experts.0.w2
experts.0.w3
experts.1.w2
experts.1.w3
experts.2.w2
experts.2.w3
experts.3.w2
experts.3.w3experts.4.w2
experts.4.w3
experts.5.w2
experts.5.w3
experts.6.w2
experts.6.w3
experts.7.w2
experts.7.w3
hidden_states
Figure 8: Calibration results on GLU-implemented LLMs (Mixtral-8x7B).
14110 20 30 40
Layers05001kActivation MagnitudeLLaMA-2-13B
120 40 60 80
Layers05k10k15kLLaMA-2-70B
1 8 16 24 32
Layers0300600LLaMA-3-8B
120 40 60 80
Layers0300600LLaMA-3-70B
112 24 36 48
Layers01k2kActivation MagnitudeSOLAR-10.7B
110 20 30 40
Layers01k2kStableLM-2-12B
1 7 14 21 28
Layers01k2kGemma-7Bself-attn.q/k/v self-attn.out feed-forward.up/gate feed-forward.down hidden statesFigure 9: Calibration results on GLU-implemented LLMs.
Figure 10: Calibration results on Non GLU-implemented LLMs.
15A.3 Other Calibration Results on Non GLU-implementation 547
Figure 10 shows the calibration result examples for various non GLU-implemented LLMs that were 548
not shown in the models in Figure 1b. There are no activation spikes on non GLU-implemented 549
LLMs. 550
B BMM Quantization 551
To achieve faster inference latency, BMM operations in the self-attention also can be computed as 552
INT8 operation [ 51]. This requires a quantization on the query, key, and value states including the 553
cached context. Because activation spikes produce a large magnitude of latent values, it is important 554
to confirm the extent of quantization errors from KV quantization. This confirmation is necessary to 555
gain advantages from BMM quantization. In Table 7, we examine the impact of BMM quantization on 556
the W8A8 and QFeM. Regardless of the BMM quantization, the QFeM method consistently improves 557
the quantization bottleneck. For example, the 13B and 70B models maintain their performance, 558
while the 7B model shows a slight decrease. However, this decrease appears to be due to inherent 559
quantization errors rather than a quantization bottleneck from activation spikes. As a result, we 560
confirm that our QFeM method effectively improves the overall performance even in the BMM 561
quantization scenario. 562
Table 7: BMM quantization results.
Model MethodBMM Quantization
No Yes
7BW8A8 62.08% 61.66%
+QFeP 68.69% 68.30%
13BW8A8 55.29% 55.43%
+QFeP 69.91% 69.77%
70BW8A8 66.87% 66.75%
+QFeP 72.62% 72.69%
C Supplementary Experiment Results 563
C.1 Additional Results for Combining Outlier Alleviation Methods 564
In Table 8, we provide additional results for Section 5.3 with coarse-grained quantization (i.e., 565
per-tensor quantization) scheme for weight quantization. Compared to the results obtained with per- 566
channel weight quantization in Table 4, these results elucidate the negative impact of activation spikes 567
on the performance of outlier alleviation methods. Furthermore, this suggests that the performance of 568
OSP method resort to the weight quantization scheme. Nevertheless, the proposed methods, QFeM 569
and QFeP, consistently improve the effectiveness of outlier alleviation methods by mitigating the 570
impact of activation spikes. 571
Table 8: Evaluation of outlier alleviation methods with QFeM and QFeP. We report perplexity on
WikiText-2 and averaged accuracy of four zero-shot tasks. Compared to Table 4, per-tensor weight
quantization and dynamic per-tensor activation quantization are used.
MethodLLaMA-2-7B LLaMA-2-13B LLaMA-2-70B
ppl(‚Üì) acc( ‚Üë) ppl( ‚Üì) acc( ‚Üë) ppl( ‚Üì) acc( ‚Üë)
SQ [51] 24.661 56.87% 120.966 53.06% 8.435 67.08%
+QFeM 6.016 67.74% 5.464 70.04% 4.015 74.18%
+QFeP 6.122 67.22% 10.473 68.17% 5.998 72.54%
OSP [50] 9.131 63.61% 8.997 64.03% 6.492 71.13%
+QFeM 5.951 68.65% 5.284 70.67% 4.434 73.30%
+QFeP 5.821 68.25% 5.868 67.96% 4.976 73.57%
16D Miscellaneous 572
D.1 Transformer Architecture. 573
In Figure 11, we illustrate the Pre-LN transformer architecture and each sub-modules. We highlight 574
with the same color the linear modules that accept identical input activations. Note that the hidden 575
states are normalized before forwarding into the query anduplinear modules. 576
BMMSoftmaxBMMùëúùë¢ùë°ùëûùëòùë£Hadamard Productùëëùëúùë§ùëõùëîùëéùë°ùëíùë¢ùëùùúéùëëùëúùë§ùëõùë¢ùëùùúé(a)Transformer Block(Pre-LN)(b)Self-Attention(c)Feed-Forward(GLU)LayerNormLayerNormSelf-AttentionFeed-ForwardAddAdd(d)Feed-Forward(Non-GLU)
Figure 11: An illustration of Pre-LN transformer block and its sub-modules. Two feed-forward
implementation, GLU and Non-GLU, are visualized in (c) and (d) respectively. In feed-forward
network, œÉdenotes non-linear activation function, such as GeLU. We highlight the linear modules
where input activations are quantized.
D.2 Additional Results for Token-level Scale Analysis 577
We provide additional results for token-level scale analysis (Section 3.2). In Figure 12 and Figure 13, 578
the token for the activation spikes behind the BOS token does not exhibit the excessive activation 579
scale. 580
BOS \n It ' ssnow \n It ' scold \n05001k1.5kActivation Magnitude
LLaMA-2-7B Layer 2 feed-forward.down
Figure 12: Token-wise scales analysis for LLaMA-2-7B. The newline token behind the BOS token
does not exhibit the activation spikes.
BOS ' It ' ssnow \n It ' scold \n02k4k6kActivation Magnitude
LLaMA-2-7B Layer 2 feed-forward.down
Figure 13: Token-wise scales from the unrolled activation spike of LLaMA-2-70B. The newline
token behind the BOS token does not exhibit the activation spikes.
17NeurIPS Paper Checklist 581
1.Claims 582
Question: Do the main claims made in the abstract and introduction accurately reflect the 583
paper‚Äôs contributions and scope? 584
Answer: [Yes] 585
Justification: We clarify our research scope and contributions in abstract and introduction. 586
Guidelines: 587
‚Ä¢The answer NA means that the abstract and introduction do not include the claims 588
made in the paper. 589
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the 590
contributions made in the paper and important assumptions and limitations. A No or 591
NA answer to this question will not be perceived well by the reviewers. 592
‚Ä¢The claims made should match theoretical and experimental results, and reflect how 593
much the results can be expected to generalize to other settings. 594
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals 595
are not attained by the paper. 596
2.Limitations 597
Question: Does the paper discuss the limitations of the work performed by the authors? 598
Answer: [No] 599
Justification: The limitation of our work is that our methods are based on the observations 600
without theoretical validation. However, our extensive experimental results validate the 601
effectiveness of our methods. 602
Guidelines: 603
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that 604
the paper has limitations, but those are not discussed in the paper. 605
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper. 606
‚Ä¢The paper should point out any strong assumptions and how robust the results are to 607
violations of these assumptions (e.g., independence assumptions, noiseless settings, 608
model well-specification, asymptotic approximations only holding locally). The authors 609
should reflect on how these assumptions might be violated in practice and what the 610
implications would be. 611
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was 612
only tested on a few datasets or with a few runs. In general, empirical results often 613
depend on implicit assumptions, which should be articulated. 614
‚Ä¢The authors should reflect on the factors that influence the performance of the approach. 615
For example, a facial recognition algorithm may perform poorly when image resolution 616
is low or images are taken in low lighting. Or a speech-to-text system might not be 617
used reliably to provide closed captions for online lectures because it fails to handle 618
technical jargon. 619
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms 620
and how they scale with dataset size. 621
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to 622
address problems of privacy and fairness. 623
‚Ä¢While the authors might fear that complete honesty about limitations might be used by 624
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 625
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best 626
judgment and recognize that individual actions in favor of transparency play an impor- 627
tant role in developing norms that preserve the integrity of the community. Reviewers 628
will be specifically instructed to not penalize honesty concerning limitations. 629
3.Theory Assumptions and Proofs 630
Question: For each theoretical result, does the paper provide the full set of assumptions and 631
a complete (and correct) proof? 632
18Answer: [NA] 633
Justification: We propose empirical methods based on our observation, rather than theoretical 634
analysis. 635
Guidelines: 636
‚Ä¢ The answer NA means that the paper does not include theoretical results. 637
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross- 638
referenced. 639
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems. 640
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if 641
they appear in the supplemental material, the authors are encouraged to provide a short 642
proof sketch to provide intuition. 643
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented 644
by formal proofs provided in appendix or supplemental material. 645
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 646
4.Experimental Result Reproducibility 647
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 648
perimental results of the paper to the extent that it affects the main claims and/or conclusions 649
of the paper (regardless of whether the code and data are provided or not)? 650
Answer: [Yes] 651
Justification: We precisely describe the process of the proposed methods in their respec- 652
tive subsections. The models (LLMs) and datasets used in the experiments are publicly 653
accessible. 654
Guidelines: 655
‚Ä¢ The answer NA means that the paper does not include experiments. 656
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived 657
well by the reviewers: Making the paper reproducible is important, regardless of 658
whether the code and data are provided or not. 659
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken 660
to make their results reproducible or verifiable. 661
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways. 662
For example, if the contribution is a novel architecture, describing the architecture fully 663
might suffice, or if the contribution is a specific model and empirical evaluation, it may 664
be necessary to either make it possible for others to replicate the model with the same 665
dataset, or provide access to the model. In general. releasing code and data is often 666
one good way to accomplish this, but reproducibility can also be provided via detailed 667
instructions for how to replicate the results, access to a hosted model (e.g., in the case 668
of a large language model), releasing of a model checkpoint, or other means that are 669
appropriate to the research performed. 670
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis- 671
sions to provide some reasonable avenue for reproducibility, which may depend on the 672
nature of the contribution. For example 673
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 674
to reproduce that algorithm. 675
(b)If the contribution is primarily a new model architecture, the paper should describe 676
the architecture clearly and fully. 677
(c)If the contribution is a new model (e.g., a large language model), then there should 678
either be a way to access this model for reproducing the results or a way to reproduce 679
the model (e.g., with an open-source dataset or instructions for how to construct 680
the dataset). 681
(d)We recognize that reproducibility may be tricky in some cases, in which case 682
authors are welcome to describe the particular way they provide for reproducibility. 683
In the case of closed-source models, it may be that access to the model is limited in 684
some way (e.g., to registered users), but it should be possible for other researchers 685
to have some path to reproducing or verifying the results. 686
195.Open access to data and code 687
Question: Does the paper provide open access to the data and code, with sufficient instruc- 688
tions to faithfully reproduce the main experimental results, as described in supplemental 689
material? 690
Answer: [Yes] 691
Justification: We provide accessible URL in the abstract. 692
Guidelines: 693
‚Ä¢ The answer NA means that paper does not include experiments requiring code. 694
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 695
public/guides/CodeSubmissionPolicy ) for more details. 696
‚Ä¢While we encourage the release of code and data, we understand that this might not be 697
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not 698
including code, unless this is central to the contribution (e.g., for a new open-source 699
benchmark). 700
‚Ä¢The instructions should contain the exact command and environment needed to run to 701
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 702
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 703
‚Ä¢The authors should provide instructions on data access and preparation, including how 704
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 705
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new 706
proposed method and baselines. If only a subset of experiments are reproducible, they 707
should state which ones are omitted from the script and why. 708
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized 709
versions (if applicable). 710
‚Ä¢Providing as much information as possible in supplemental material (appended to the 711
paper) is recommended, but including URLs to data and code is permitted. 712
6.Experimental Setting/Details 713
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 714
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 715
results? 716
Answer: [Yes] 717
Justification: We provide the hyperparameter settings in Table 2. 718
Guidelines: 719
‚Ä¢ The answer NA means that the paper does not include experiments. 720
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail 721
that is necessary to appreciate the results and make sense of them. 722
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental 723
material. 724
7.Experiment Statistical Significance 725
Question: Does the paper report error bars suitably and correctly defined or other appropriate 726
information about the statistical significance of the experiments? 727
Answer: [No] 728
Justification: The proposed methods rely on the sample size of the calibration dataset. 729
Nevertheless, we are convinced that the sample size used in the experiments is sufficient for 730
achieving reliable and consistent calibration results. 731
Guidelines: 732
‚Ä¢ The answer NA means that the paper does not include experiments. 733
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi- 734
dence intervals, or statistical significance tests, at least for the experiments that support 735
the main claims of the paper. 736
20‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for 737
example, train/test split, initialization, random drawing of some parameter, or overall 738
run with given experimental conditions). 739
‚Ä¢The method for calculating the error bars should be explained (closed form formula, 740
call to a library function, bootstrap, etc.) 741
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). 742
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error 743
of the mean. 744
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should 745
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 746
of Normality of errors is not verified. 747
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or 748
figures symmetric error bars that would yield results that are out of range (e.g. negative 749
error rates). 750
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how 751
they were calculated and reference the corresponding figures or tables in the text. 752
8.Experiments Compute Resources 753
Question: For each experiment, does the paper provide sufficient information on the com- 754
puter resources (type of compute workers, memory, time of execution) needed to reproduce 755
the experiments? 756
Answer: [Yes] 757
Justification: We provide a computational cost analysis in Section 5.5. 758
Guidelines: 759
‚Ä¢ The answer NA means that the paper does not include experiments. 760
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster, 761
or cloud provider, including relevant memory and storage. 762
‚Ä¢The paper should provide the amount of compute required for each of the individual 763
experimental runs as well as estimate the total compute. 764
‚Ä¢The paper should disclose whether the full research project required more compute 765
than the experiments reported in the paper (e.g., preliminary or failed experiments that 766
didn‚Äôt make it into the paper). 767
9.Code Of Ethics 768
Question: Does the research conducted in the paper conform, in every respect, with the 769
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 770
Answer: [Yes] 771
Justification: We have reviewed the code of ethics. 772
Guidelines: 773
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 774
‚Ä¢If the authors answer No, they should explain the special circumstances that require a 775
deviation from the Code of Ethics. 776
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid- 777
eration due to laws or regulations in their jurisdiction). 778
10.Broader Impacts 779
Question: Does the paper discuss both potential positive societal impacts and negative 780
societal impacts of the work performed? 781
Answer: [NA] 782
Justification: 783
Guidelines: 784
‚Ä¢ The answer NA means that there is no societal impact of the work performed. 785
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal 786
impact or why the paper does not address societal impact. 787
21‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses 788
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 789
(e.g., deployment of technologies that could make decisions that unfairly impact specific 790
groups), privacy considerations, and security considerations. 791
‚Ä¢The conference expects that many papers will be foundational research and not tied 792
to particular applications, let alone deployments. However, if there is a direct path to 793
any negative applications, the authors should point it out. For example, it is legitimate 794
to point out that an improvement in the quality of generative models could be used to 795
generate deepfakes for disinformation. On the other hand, it is not needed to point out 796
that a generic algorithm for optimizing neural networks could enable people to train 797
models that generate Deepfakes faster. 798
‚Ä¢The authors should consider possible harms that could arise when the technology is 799
being used as intended and functioning correctly, harms that could arise when the 800
technology is being used as intended but gives incorrect results, and harms following 801
from (intentional or unintentional) misuse of the technology. 802
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation 803
strategies (e.g., gated release of models, providing defenses in addition to attacks, 804
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 805
feedback over time, improving the efficiency and accessibility of ML). 806
11.Safeguards 807
Question: Does the paper describe safeguards that have been put in place for responsible 808
release of data or models that have a high risk for misuse (e.g., pretrained language models, 809
image generators, or scraped datasets)? 810
Answer: [NA] 811
Justification: 812
Guidelines: 813
‚Ä¢ The answer NA means that the paper poses no such risks. 814
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with 815
necessary safeguards to allow for controlled use of the model, for example by requiring 816
that users adhere to usage guidelines or restrictions to access the model or implementing 817
safety filters. 818
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors 819
should describe how they avoided releasing unsafe images. 820
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do 821
not require this, but we encourage authors to take this into account and make a best 822
faith effort. 823
12.Licenses for existing assets 824
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 825
the paper, properly credited and are the license and terms of use explicitly mentioned and 826
properly respected? 827
Answer: [Yes] 828
Justification: We cite the models and dataset used in the experiments (see Section 5.1). 829
Guidelines: 830
‚Ä¢ The answer NA means that the paper does not use existing assets. 831
‚Ä¢ The authors should cite the original paper that produced the code package or dataset. 832
‚Ä¢The authors should state which version of the asset is used and, if possible, include a 833
URL. 834
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. 835
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of 836
service of that source should be provided. 837
‚Ä¢If assets are released, the license, copyright information, and terms of use in the 838
package should be provided. For popular datasets, paperswithcode.com/datasets 839
has curated licenses for some datasets. Their licensing guide can help determine the 840
license of a dataset. 841
22‚Ä¢For existing datasets that are re-packaged, both the original license and the license of 842
the derived asset (if it has changed) should be provided. 843
‚Ä¢If this information is not available online, the authors are encouraged to reach out to 844
the asset‚Äôs creators. 845
13.New Assets 846
Question: Are new assets introduced in the paper well documented and is the documentation 847
provided alongside the assets? 848
Answer: [NA] 849
Justification: 850
Guidelines: 851
‚Ä¢ The answer NA means that the paper does not release new assets. 852
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their 853
submissions via structured templates. This includes details about training, license, 854
limitations, etc. 855
‚Ä¢The paper should discuss whether and how consent was obtained from people whose 856
asset is used. 857
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either 858
create an anonymized URL or include an anonymized zip file. 859
14.Crowdsourcing and Research with Human Subjects 860
Question: For crowdsourcing experiments and research with human subjects, does the paper 861
include the full text of instructions given to participants and screenshots, if applicable, as 862
well as details about compensation (if any)? 863
Answer: [NA] 864
Justification: 865
Guidelines: 866
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 867
human subjects. 868
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu- 869
tion of the paper involves human subjects, then as much detail as possible should be 870
included in the main paper. 871
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 872
or other labor should be paid at least the minimum wage in the country of the data 873
collector. 874
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 875
Subjects 876
Question: Does the paper describe potential risks incurred by study participants, whether 877
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 878
approvals (or an equivalent approval/review based on the requirements of your country or 879
institution) were obtained? 880
Answer: [NA] 881
Justification: 882
Guidelines: 883
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 884
human subjects. 885
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent) 886
may be required for any human subjects research. If you obtained IRB approval, you 887
should clearly state this in the paper. 888
‚Ä¢We recognize that the procedures for this may vary significantly between institutions 889
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 890
guidelines for their institution. 891
‚Ä¢For initial submissions, do not include any information that would break anonymity (if 892
applicable), such as the institution conducting the review. 893
23