Unified Mechanism-Specific Amplification by
Subsampling and Group Privacy Amplification
Jan Schuchardt1, Mihail Stoian2âˆ—, Arthur Kosmala1âˆ—, Stephan GÃ¼nnemann1
{j.schuchardt, a.kosmala, s.guennemann}@tum.de, mihail.stoian@utn.de
1Dept. of Computer Science & Munich Data Science Institute, Technical University of Munich
2Dept. of Engineering, University of Technology Nuremberg
Abstract
Amplification by subsampling is one of the main primitives in machine learning
with differential privacy (DP): Training a model on random batches instead of
complete datasets results in stronger privacy. This is traditionally formalized via
mechanism-agnostic subsampling guarantees that express the privacy parameters
of a subsampled mechanism as a function of the original mechanismâ€™s privacy
parameters. We propose the first general framework for deriving mechanism-
specific guarantees, which leverage additional information beyond these parameters
to more tightly characterize the subsampled mechanismâ€™s privacy. Such guarantees
are of particular importance for privacy accounting , i.e., tracking privacy over
multiple iterations. Overall, our framework based on conditional optimal transport
lets us derive existing and novel guarantees for approximate DP, accounting with
RÃ©nyi DP, and accounting with dominating pairs in a unified, principled manner.
As an application, we analyze how subsampling affects the privacy of groups of
multiple users. Our tight mechanism-specific bounds outperform tight mechanism-
agnostic bounds and classic group privacy results.
1 Introduction
Composability and amplification by subsampling are two key properties of Differential Privacy
(DP) [ 1â€“3] that make it possible to provide provable privacy guarantees for machine learning
algorithms that iteratively learn from batches of data.
Composability means that privacy gracefully deteriorates when iteratively applying a differentially
private mechanism to a dataset [ 2]. Kairouz et al. [4], Murtagh and Vadhan [5]ultimately derived
tight composition theorems that optimally characterize the privacy parameters (Îµâ€², Î´â€²)of a composed
mechanism as a function of the component mechanismsâ€™ parameters (Îµ, Î´). However, later work
demonstrated that these guarantees are only tight in a mechanism-agnostic sense [ 6]. Stronger
mechanism-specific composition guarantees can often be obtained by using additional information
beyond the fact that the mechanisms satisfies some notion of DP. Methods for tracking privacy over
multiple iterations (e.g. [6â€“12]) are refererred to as privacy accountants .
Amplification by subsampling means that privacy can be strenghtened by applying a differentially
private mechanism to randomly sampled batches of a dataset [ 13,14]. Similar to Kairouz et al.
[4], Balle et al. [15] ultimately proposed a framework for deriving tight subsampling theorems that
optimally characterize the privacy parameters (Îµâ€², Î´â€²)of a subsampled mechanism as a function of
the underlying base mechanism â€™s parameters (Îµ, Î´).
However, their framework for deriving subsampling theorems has two key limitations. Firstly, as
we shall demonstrate, the resultant guarantees are generally only tight in a mechanism-agnostic
âˆ—Equal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).sense: Given (Îµ, Î´), one can construct some worst-case (Îµ, Î´)-DP mechanism that is (Îµâ€², Î´â€²)-DP under
subsampling. The specific mechanism at hand may however be significantly more private. Secondly,
their framework does not explain how to derive subsampling guarantees for privacy accountants. In
fact, there is so far no unified framework for deriving subsampling guarantees for privacy accountants.
To address these two limitations, we propose a novel framework for unified mechanism-specific
amplification by subsampling analysis, using conditional optimal transport. Our proposed framework
(1) lets us derive mechanism-specific subsampling guarantees, which can be stronger than mechanism-
agnostic guarantees, (2) lets us recover mechanism-agnostic guarantees via a pessimistic upper bound
â€“ essentially subsuming the approach from [ 15] â€“ and (3) lets us derive guarantees for approximate
differential privacy [ 2], moments accounting [ 6] (i.e., RÃ©nyi differential privacy [ 7]), and accounting
with dominating pairs [ 10] (e.g., numerical accounting [ 16,8,17â€“21]) in a unified, principled manner.
ï¿½ï¿½ï¿½ ğ‘¥ğ‘¥1ğ‘¥ğ‘¥2 ğ‘¥ğ‘¥ğ‘ğ‘ğ‘¥ğ‘¥ğ‘¥1ğ‘¥ğ‘¥ğ‘¥2
Base mechanism ğµğµSubsampling scheme ğ‘†ğ‘†
ğ‘¥ğ‘¥1ğ‘¥ğ‘¥2ğ‘¥ğ‘¥6
ğ‘¥ğ‘¥2ğ‘¥ğ‘¥10ğ‘¥ğ‘¥6ğ‘¥ğ‘¥5ğ‘¥ğ‘¥5ğ‘¥ğ‘¥9ğ‘¥ğ‘¥ğ‘¥1
ğ‘¥ğ‘¥7ğ‘¥ğ‘¥10ğ‘¥ğ‘¥9ğ‘¥ğ‘¥ğ‘¥2ğ‘¥ğ‘¥9ğ‘¥ğ‘¥ğ‘¥1ğ‘¥ğ‘¥ğ‘¥2
ğ‘¥ğ‘¥17ğ‘¥ğ‘¥25ğ‘¥ğ‘¥ğ‘¥1ğ‘¥ğ‘¥ğ‘¥2Pr=(1âˆ’ğ‘Ÿğ‘Ÿ)2Pr=2ğ‘Ÿğ‘Ÿ(1âˆ’ğ‘Ÿğ‘Ÿ) Pr=ğ‘Ÿğ‘Ÿ2
ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ğ‘¥ğ‘¥ğ‘¥3
Figure 1: Group members xâ€²
1, xâ€²
2contribute to a
dataset, while group member xâ€²
3does not. For
small subsampling rates r, it is unlikely to access a
single ( Pr = 2 r(1âˆ’r)) or even both ( Pr = r2) in-
serted elements when applying a base mechanism
Bto a subsampled batch (e.g., the yellow one).
This further obfuscates which data was contributed
by members of group {xâ€²
1, xâ€²
2, xâ€²
3}.As a practical application, we consider the prob-
lem of tightly analyzing group privacy under
subsampling (see Fig. 1). Assume a scenario
where Kindividuals may independently decide
to contribute to a dataset. Further assume that
this dataset is processed by randomly sampling
a batch and applying an (Îµ, Î´)-DPbase mecha-
nism . A special case of this setting is discussed
in a recent technical note [ 22], which assumes
that the Kindividuals collaboratively agree to
either contribute or not contribute all their data
and that the base mechanism is Gaussian. In
the general setting we consider, the best known
method for providing privacy guarantees for the
entire group is to (1) use existing bounds to
show that the subsampled mechanism guaran-
tees(Îµâ€², Î´â€²)-DP for individuals and (2) use the
group privacy property[ 23] to show that this im-
plies(KÂ·Îµâ€², KÂ·eKÂ·Îµâ€²Â·Î´â€²)-DP for the group.
Our proposed framework lets us derive stronger, tight group privacy amplification guarantees. By
analyzing group privacy and subsampling jointly, these guarantees accurately capture that it is unlikely
for a large fraction of the groupâ€™s data to simultaneously appear in a batch. Our framework further
lets us derive dominating pairs [ 10], which enables tight tracking of group privacy under composition.
Overall, our main contributions are that we
â€¢demonstrate for the first time that there is a qualitative difference between mechanism-
specific and mechanism-agnostic tightness in privacy amplification,
â€¢propose a general framework for deriving mechanism-specific amplification by subsampling
guarantees, subsuming prior work on mechanism-agnostic amplification,
â€¢develop the first unified method for deriving subsampling guarantees for privacy accounting,
â€¢ and derive the first tight subsampling guarantees for general group privacy.
As part of our group privacy analysis, we also significantly generalize recent results derived for
subsampled matrix mechanisms [ 24], which is of independent interest. Experimental evaluation
demonstrates that our tight mechanism-specific guarantees outperform both tight mechanism-agnostic
bounds and post-hoc use of the group privacy property.
2 Background and preliminaries
Problem setting. We consider the same setting as Balle et al. [15], but with an additional privacy
accounting and group privacy perspective. We assume a space of datasets X, a space of batches
Ythat can be constructed from these datasets, and an output space Z. For the sake of exposition,
we assume that Xis the powerset P(A)of some finite discrete set A, thatY={yâŠ†x|xâˆˆX},
and that Z=RD. We further consider a random subsampling scheme S:Xâ†’Ythat generates
batches from datasets, and a random base mechanism B:Yâ†’RDthat maps batches to outputs.
We write sx(y)for the pmf of S(X)andby(z)for the pdf of B(y). In Appendix D, we introduce a
more general setting that admits arbitrary spaces, for which we derive all our results. Our goal is to
2provide formal privacy guarantees for the subsampled mechanism M=Bâ—¦S, which takes a dataset,
subsamples a batch from it, and then applies the random base mechanism to this batch.
Dataset and batch relations. Specifically, we want to prove privacy under neighboring relations
â‰ƒXâŠ†X2between datasets. For example, the insertion/removal relation xâ‰ƒÂ±,Xxâ€²implies that
xâ€²=xâˆª {a}orxâ€²=x\ {a}for some a. The substitution relation xâ‰ƒâˆ†,Xxâ€²implies that
xâ€²=x\ {a} âˆª { aâ€²}for some a, aâ€². In addition, we assume that Yis equipped with a batch
neighboring relation â‰ƒYâŠ†Y2. The batch relation â‰ƒYcan be distinct from the dataset relation â‰ƒX.
Subsampling schemes. While we consider arbitrary subsampling schemes S:Xâ†’Y, we shall
later apply our framework to three particularly common ones: Subsampling without replacement
andwith replacement sample sets and multisets of fixed size |y|=quniformly at random. Poisson
subsampling includes each element of xwith independent probability (â€œrateâ€) râˆˆ[0,1].
Approximate differential privacy. A mechanism M:Xâ†’RDis privacy-preserving under
symmetric neighboring relation â‰ƒXwhen the distributions of M(x), M(xâ€²)with densities mx, mâ€²
x
are almost indistinguishable for all xâ‰ƒXxâ€². Approximate differential privacy (ADP) [ 2] quantifies
this via hockey stick divergences [25]:
Definition 2.1. ForÎµâ‰¥0, a mechanism M:Xâ†’RDis(Îµ, Î´)-DP under symmetric relation â‰ƒXiff
âˆ€xâ‰ƒXxâ€²:HeÎµ(mx||mxâ€²)â‰¤Î´withHÎ±(mx||mxâ€²) =R
RDmax{mx(z)/mxâ€²(z)âˆ’Î±,0}Â·mxâ€²(z) dz.
Note that this divergence-based definition of approximate differential privacy is equivalent to requiring
for all datasets xâ‰ƒXxâ€²and all events SâŠ†RDthatPr[M(x)âˆˆS]â‰¤eÎµÂ·Pr[M(xâ€²)âˆˆS] +Î´.
RÃ©nyi differential privacy. Privacy accounting was first popularized by methods that used moment
bounds instead of (Îµ, Î´)pairs to better track privacy under composition [ 6,26,27]. These methods
were later developed into a moments-based notion of privacy â€“ RÃ©nyi differential privacy (RDP) [ 7]:
Definition 2.2. ForÎ±â‰¥1, a mechanism M:Xâ†’RDis(Î±, Ï)-RDP under symmetric relation â‰ƒX
iffâˆ€xâ‰ƒXxâ€²: log(Î› Î±(mx||mxâ€²))/(Î±âˆ’1)â‰¤ÏwithÎ›Î±(mx||mxâ€²) =R
RDmx(z)Î±Â·mxâ€²(z)1âˆ’Î±dz.
Note that Î›Î±is not the RÃ©nyi divergence, but its Î±th moment, i.e., a scaled and exponentiated RÃ©nyi
divergence. We use this notation to eliminate exponential terms that arise in amplification for RDP
(see [ 28â€“30]). If a mechanism is (Î±, Ï)-RDP, its T-fold self-composition is (Î±, TÂ·Ï)-RDP. These
RDP parameters can then be converted to ADP parameters ( Îµ, Î´), albeit in a lossy manner [ 7,31,32].
Dominating pairs. Later work proposed other numerical [ 16,8,17â€“21] or analytical accounting
techniques [ 9,33,28,34,35]. Zhu et al. [10] developed a unified view on these works by introducing
the notion of dominating pairs :
Definition 2.3. A pair of distributions (P, Q)with densities (p, q)is a dominating pair for mechanism
Munder neighboring relation â‰ƒX, ifHÎ±(mx||mxâ€²)â‰¤HÎ±(p||q)for all xâ‰ƒXxâ€²and all Î±â‰¥0.
Given a dominating pair (P, Q), one can use various numerical or analytic techniques, such as
convolution of privacy loss distributions [ 16] or central limit theorems of tradeoff functions [ 9], to
track the privacy of mechanism Munder composition (see Fig. 2 in [10]).
Group privacy. The group privacy property is the graceful decay of privacy when considering
multiple userâ€™s data. This is normally formalized via the notion of induced distance (see [ 7,15,23]):
Definition 2.4. The distance dX(x, xâ€²)induced by relation â‰ƒXis the length of the shortest sequence
(x1, . . . , x Kâˆ’1)âˆˆXKâˆ’1such that xâ‰ƒXx1,âˆ€k:xkâ‰ƒXxk+1, and xKâˆ’1â‰ƒXxâ€².
Example 2.5.Letâ‰ƒXbe the insertion/removal relation â‰ƒÂ±. Then the induced distance dXbetween
x={1,2}andxâ€²={2,3}is2, because {1,2} â‰ƒÂ±{1,2,3} â‰ƒÂ±{2,3}.
Proposition 2.6 (Vadhan [23]).If mechanism M:Xâ†’RDis(Îµ, Î´)-DP under relation â‰ƒX, then it
is(KÂ·Îµ, KÂ·eKÂ·ÎµÂ·Î´)-DP under group relation {(x, xâ€²)âˆˆX2|dX(x, xâ€²) =K}.
That is, for sufficiently small Îµthe ADP parameters deteriorate appproximately linearly with induced
distance. As baselines for our experiments, we use even tighter bounds (see Appendix C.1.4).
3 Unified mechanism-specific amplification by subsampling
Our goal is to develop a general procedure for (tightly) deriving ADP parameters (Îµâ€², Î´â€²), RDP
parameters (Î±â€², Ïâ€²), or dominating pairs (Pâ€², Qâ€²)for the subsampled mechanism M=Bâ—¦Swith
3subsampling scheme Sand base mechanism B. Based on Definitions 2.1 to 2.3, this requires that we
evaluate or bound the hockey stick divergence HÎ±or the scaled and exponentiated RÃ©nyi divergence
Î›Î±between the distributions of M(x)andM(xâ€²)withxâ‰ƒXxâ€². To discuss both simultaneously, let
us write Î¨Î±for either HÎ±orÎ›Î±and assume that Î±â‰¥0orÎ±â‰¥1, respectively.
There are two challenges: Firstly, the distribution of M(x)is high-dimensional mixture distribution
with one component per possible batch yâˆˆYand weights given by batch probabilitites sx(y), i.e.,
mx(z) =P
yâˆˆYby(z)Â·sx(y).Secondly, there may be no simple analytic formula for the component
densities. For instance, evaluating the density of perturbed model gradients in noisy SGD [ 36]
requires backpropagation. Both challenges make it hard to evaluate or bound Î¨Î±(mx||mxâ€²).
A useful property that will help us in addressing the first problem of having a large number of mixture
components is the joint convexity of Î¨Î±in the space of probability density functions [37, 38]:
Lemma 3.1. Consider arbitrary densities f(1)
1, f(1)
2, f(2)
1, f(2)
2:RDâ†’R+and weight wâˆˆ[0,1].
Then, Î¨Î±(wf(1)
1+(1âˆ’w)f(1)
2||wf(2)
1+(1âˆ’w)f(2)
2)â‰¤wÎ¨Î±(f(1)
1||f(2)
1)+(1âˆ’w)Î¨Î±(f(1)
2||f(2)
2).
In our case, the fs can be base mechanism densities bywith different yâˆˆY, and the weights
can be given by the subsampling distribution. We could thus upper-bound the mixture divergence
Î¨Î±(mx||mxâ€²)in terms of component divergences â€“ if the mixtures had identical weights. This is
generally not the case, since subsampling mass functions sx(Â·)andsxâ€²(Â·)depend on datasets xÌ¸=xâ€².
To still leverage the joint convexity of Î¨Î±(mx||mxâ€²), we want to rewrite mxandmxâ€²as mixtures with
identical weights. This is exactly what is offered by couplings between probability mass functions.
Intuitively, a coupling between two pmfs p1, p2:Yâ†’[0,1]specifies for every y1, y2âˆˆYhow much
probability should be transported from y1toy2to transform p1intop2. Couplings can be formalized
and generalized to multiple pmfs as follows (for a more thorough introduction, see [39]):
Definition 3.2. A coupling between Npmfs p1, . . . , p N:Yâ†’[0,1]on batch space Yis a joint pmf
Î³:YNâ†’[0,1]where the nth marginal is pn, i.e., pn(yâˆ—
n) =P
yâˆˆYN 1[yn=yâˆ—
n]Â·Î³(y).
Given a valid coupling Î³between subsampling mass functions sxandsxâ€², we can use marginalization
to rewrite mx(z)asP
yâˆˆY2by1(z)Î³(y). Similarly, we can rewrite mxâ€²(z)asP
yâˆˆY2by2(z)Î³(y).
Now, both mxandmxâ€²are mixtures with identical weights and one component per pair of batches in
Y2. We can thus recursively apply Lemma 3.1 to show (full proof in Appendix E):
Theorem 3.3. Consider a subsampled mechanism M=Bâ—¦S, and an arbitrary coupling Î³between
subsampling mass functions sx(Â·)andsxâ€²(Â·). Then,
Î¨Î±(mx||mxâ€²)â‰¤X
yâˆˆY2cÎ±(y(1), y(2))Â·Î³(y(1), y(2)) (1)
with cost function cÎ±:YÃ—Yâ†’R+defined by cÎ±(y(1), y(2)) = Î¨ Î±(by(1)||by(2)).
We write y(1)instead of y1to simplify later notations. While every coupling Î³yields a valid upper
bound, the guarantees can be tightened by finding a coupling Î³âˆ—that minimizes the r.h.s. of Eq. (1).
We thus have an optimal transport problem , where the cost cÎ±of transporting probability from batch
y(1)to batch y(2)depends on the divergence Î¨Î±of base mechanism densities by(1)andby(2).
Unfortunately, experimental evaluation in Appendix B.1.4 shows that the resultant guarantees can be
much weaker than those from prior work â€“ even with an optimal coupling Î³âˆ—. This is because Theo-
rem 3.3 results from recursively applying the joint convexity property (Lemma 3.1) to Î¨Î±(mx||mxâ€²).
Each recursive step splits each mixture into two smaller mixtures and further upper-bounds the
divergence that is achieved by our specific subsampled mechanism Mon our specific pair of datasets
x, xâ€². Upon fully decomposing the overall divergence into divergences between single-mixture com-
ponents, this sequence of bounds is in fact larger than even the divergence Î¨Î±( ËœmËœx||ËœmËœxâ€²)achieved
bya worst-case subsampled mechanism ËœMon a worst-case pair of datasets Ëœx,Ëœxâ€². To overcome
this limitation, we propose to limit the recursion depth in order to obtain a tighter upper bound that
matches our specific subsampled mechanism Mon a worst-case pair of datasets Ë†x,Ë†xâ€².
Limiting the recursion depth to which Lemma 3.1 is applied means upper-bounding Î¨Î±(mx||mxâ€²)
in terms of mixture divergences that have not been fully decomposed into their individual compo-
nents. Specifically, we propose to do so by defining an optimal transport problem between multiple
subsampling mass functions conditioned on different events (proof in Appendix E):
4Theorem 3.4. Consider a subsampled mechanism M=Bâ—¦S. Further consider two disjoint
partitioningsSI
i=1Ai=YandSJ
j=1Ej=Yof batch space Ysuch that all AiandEjhave
non-zero probability under the distribution of SxandSxâ€², respectively. Let Î³be an arbitrary coupling
between conditional mass functions sx(Â· |A1), . . . , s x(Â· |AI), sxâ€²(Â· |E1), . . . , s xâ€²(Â· |EJ). Then,
Î¨Î±(mx||mxâ€²)â‰¤X
yâˆˆYI+JcÎ±(y(1),y(2))Â·Î³((y(1),y(2))),
with cost function cÎ±:YIÃ—YJâ†’R+defined by
cÎ±(y(1),y(2)) = Î¨ Î±ï£«
ï£­IX
i=1by(1)
iÂ·Pr[S(x)âˆˆAi]||JX
j=1by(2)
jÂ·Pr[S(xâ€²)âˆˆEj]ï£¶
ï£¸. (2)
In other words: We now have an optimal transport problem between I+Jprobability mass functions
coupled by Î³. The transport cost cÎ±is a divergence between two mixtures. The components of the
first mixture are base mechanisms densities given batches y(1)
iâˆˆYfrom batch tuple y(1)âˆˆYI. The
weights of the first mixture are probabilities of events Ai. The second mixture is defined analogously.
Note that we can recover Theorem 3.3 by conditioning on a single event, i.e., A1=Y, E1=Y. As
we shall demonstrate, a more fine-grained partitioning lets us obtain tighter bounds that match the
divergence Î¨Î±(mËœx||mËœxâ€²)of our specific subsampled mechanism Mon a worst-case pair of datasets
Ë†x,Ë†xâ€². In the extreme case of defining event per possible batch, Theorem 3.4 holds with equality.
Overall, Theorem 3.4 reduces the broad problem of bounding mixture divergences to the canonical
problem of optimal transport between conditional distributions. But before we can apply it, we need
to address two open problems: Evaluating the cost function and designing optimal couplings.
Cost function bound. Not having an analytic expression for every base mechanism density by(z)
may make it intractable to evaluate cost function cÎ±. We thus propose to bound it via an approach
that is inherent to differential privacy: Considering worst-case inputs (proof in Appendix E).
Proposition 3.5. Consider y(1)âˆˆYI,y(2)âˆˆYJ, and cost function cdefined in Eq. (2). LetdYbe
the distance induced by â‰ƒY(see Definition 2.4). Then, cÎ±(y(1),y(2))â‰¤Ë†cÎ±(y(1),y(2)), with
Ë†cÎ±(y(1),y(2)) = max
Ë†y(1),Ë†y(2)cÎ±(Ë†y(1),Ë†y(2)) (3)
subject to âˆ€k, lâˆˆ {1,2},âˆ€t, u:dY(Ë†y(k)
t,Ë†y(l)
u)â‰¤dY(y(k)
t, y(l)
u)andË†y(1)âˆˆYI,Ë†y(2)âˆˆYJ.
Put differently: We construct two new mixtures with components that are adversarially chosen to
maximize divergence while retaining the pairwise distances between batches in y(1)andy(2). Note
that, for the special case of Î¨Î±=HÎ±andI=J= 1, this bound corresponds to the â€œgroup privacy
profileâ€ in [ 15]. As we shall demonstrate in the next sections, this bound Ë†ccan often be evaluated
using high-level information about the base mechanism B:Yâ†’RD, such as global sensitivity.
Sufficient optimality condition. While every coupling Î³yields a valid upper bound in Theorem 3.4,
this bound can be tightened by designing an optimal coupling Î³âˆ—. To inform this design, we generalize
the notion of distance-compatible couplings from [15] to an arbitrary number of distributions:
Definition 3.6. A coupling Î³between mass functions p1, . . . , p Non batch space YisdY-compatible
when Î³(y)>0only if âˆ€u >1 :dY(y1, yu) =dY({y1},supp( pu))andâˆ€u > t > 1 :dY(yu, yt) =
dY(supp( pt),supp( pu)), where supp( pu)is the support of suand the distance between two sets is
the minimum distance of their elements.
Essentially, a dY-compatible coupling only assigns probability to a tuple of batches ywhen all pairs
yi, yjhave the smallest possible distance to y1and each other while still being in the support of their
distributions. In Appendix F, we prove that dY-compatibility is sufficient for optimality. We further
show that the optimal value has a canonical form whenever a dY-compatible couplings exists.
Summary. To summarize, we propose the following three-step procedure for deriving subsampling
guarantees: (1) Define two partitions of the batch space into IandJevents. (2) Define a simultaneous
coupling between the corresponding I+Jconditional distributions to obtain a bound in terms of
divergences between small mixtures with IandJcomponents. (3) Bound these mixture divergences
by considering I+Jworst-case mixture components under pairwise batch distance constraints.
53.1 Tight mechanism-specific group privacy amplification
As a concrete example, and to illustrate the difference between mechanism-specific and -agnostic
(see Section 3.2) bounds, let us consider the group privacy setting from Fig. 1. We have a group of
Kusers that can independently decide to contribute or not contribute their data, and the resulting
dataset is Poisson subsampled. To provide formal privacy guarantees to the group, we need to prove
that the distributions of M(x), M(xâ€²)are almost indistinguishable for x, xâ€²with induced distance
dÂ±,X(x, xâ€²)â‰¤K. Letxâ‰ƒK+,Kâˆ’,Xxâ€²be the relation that implies that K+records are inserted and
Kâˆ’records are removed to construct xâ€²from x. We can then show (full proof in Appendix M):
Theorem 3.7. LetM=Bâ—¦S, where Sis Poisson subsampling with rate r. Letâ‰ƒYbe the
insertion/removal batch relation â‰ƒÂ±,Y. Then, for all xâ‰ƒK+,Kâˆ’,Xxâ€²,Î¨Î±(mx||mxâ€²)is l.e.q.
max
yÎ¨Î±ï£«
ï£­Kâˆ’+1X
i=1by(1)
iÂ·Binom( iâˆ’1|Kâˆ’, r)||K++1X
j=1by(2)
jÂ·Binom( jâˆ’1|K+, r)ï£¶
ï£¸,(4)
subject to constraints yâˆˆYKâˆ’+K++2, as well as âˆ€lâˆˆ {1,2},âˆ€t, u:dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|, and
âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤(tâˆ’1) + ( uâˆ’1).
Proof sketch. We let AiandEjbe the events that S(x)contains iâˆ’1deleted elements and S(xâ€²)
contains jâˆ’1inserted elements, respectively. Our coupling defines the following generative process:
We sample y(1)
1from sx(Â· |A1)and let y(2)
1â†y(1)
1. We then iteratively generate the other y(1)
iby
sampling a permutation in which we add the Kâˆ’deleted elements to y(1)
1. We then generate the
other y(2)
jby sampling a permutation in which we add the K+inserted elements to y(2)
1. The result
then follows from the cost function bound in Proposition 3.5 and dY-compatibility of the coupling.
Batches y(1)
uandy(1)
tand have a distance bounded by |uâˆ’t|, because one can be obtained from
the other by removing/inserting |uâˆ’t|elements. The constraints for y(2)
uandy(2)
tare analogous.
Batches y(1)
uandy(2)
thave a distance bounded by (tâˆ’i) + (uâˆ’1)because we need to remove tâˆ’1
elements and insert uâˆ’1elements to construct one from the other.
Next, we can solve the constrained optimization problem in Theorem 3.7 â€“ i.e., determine worst-case
components â€“ to obtain mechanism-specific guarantees. For instance (proof in Appendix M.2.1):
Theorem 3.8. LetM=Bâ—¦S, where Sis Poisson subsampling with rate r, and Bis the Gaussian
mechanism h+Vwithh:Yâ†’RDandVâˆ¼ N (0, Ïƒ2ID). Define the â„“2-sensitivity L2=
max yâ‰ƒÂ±,Yyâ€²||f(y)âˆ’f(yâ€²)||2. Then for all xâ‰ƒK+,Kâˆ’,Xxâ€²,Î¨Î±(mx||mxâ€²)is l.e.q.
Î¨Î±ï£«
ï£­Kâˆ’+1X
i=1f(1)
iÂ·Binom( iâˆ’1|Kâˆ’, r)||K++1X
j=1f(2)
jÂ·Binom( jâˆ’1|K+, r)ï£¶
ï£¸,
with univariate normal densities f(1)
i=N(Â· |(iâˆ’1), Ïƒ / L 2),f(2)
j=N(Â· | âˆ’(jâˆ’1), Ïƒ / L 2).
Note that this bound is mechanism-specific in that it explictly depends on Bbeing a Gaussian
mechanism with standard deviation Ïƒand sensitivity L2. The bound can be numerically evaluated to
arbitrary precision using standard techniques from privacy accounting literature (see Appendix M.4).
In Appendix M.2 we derive similar guarantees for Laplace and randomized response mechanisms.
Tightness. These bounds are tight in a mechanism-specific sense: One cannot derive stronger ADP
or RDP guarantees without additional information about the datasets x, xâ€²âˆˆXor the underlying
function h(proofs in Appendix M.3).
Asymptotic bounds. Our focus is on tight bounds that can be explicitly computed. However, some
early works on RDP accounting (e.g.[ 6,28]) also provided asymptotic versions of their bounds, e.g.,
as a function of divergence parameter Î±. For completeness, we use Theorem 3.8 to generalize the
asymptotic bounds of Abadi et al. [6] to the group privacy setting in Appendix N.5.
Other contributions. Our solutions to the optimization problem in Theorem 3.7 are of independent
interest: The special case of Î¨Î±=HÎ±,Kâˆ’= 0 orK+= 0, and Gaussian mechanisms was
6ğ‘†ğ‘†(ğ‘‹ğ‘‹)|ğ´ğ´1ğ‘†ğ‘†(ğ‘‹ğ‘‹)|ğ´ğ´2ğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹)|ğ¸ğ¸1ğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹)|ğ¸ğ¸2ğ‘†ğ‘†(ğ‘‹ğ‘‹)|ğ´ğ´1, ,ğ‘†ğ‘†(ğ‘‹ğ‘‹)|ğ´ğ´ğ¼ğ¼ ,ğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹)|ğ¸ğ¸ğ½ğ½
OT OT OTï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹)
ğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹)|ğ¸ğ¸1,ğ‘†ğ‘†(ğ‘‹ğ‘‹) ğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹) ğ‘†ğ‘†(ğ‘‹ğ‘‹)ğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹)
OTğ‘†ğ‘†(ğ‘‹ğ‘‹ğ‘‹) ğ‘†ğ‘†(ğ‘‹ğ‘‹)
JOINTCONVEXITYğ‘†ğ‘†(ğ‘‹ğ‘‹)|ğ´ğ´1ğ‘†ğ‘†(ğ‘‹ğ‘‹)|ğ´ğ´2ğ‘†ğ‘†(ğ‘‹ğ‘‹)
JOINTCONVEXITY(a) (b) (c) (d)
Figure 2: Mechanism-agnostic guarantees for (a) graph modification [ 40â€“42] (b) inser-
tion/removal [ 14,15,29,30] (c) substitution [ 43,44,15,28] can be derived from (d) our proposed
framework. In (b-c), events AiandEjindicate the presence of inserted or substituted elements.
derived to analyze matrix mechanisms in [ 24]. We significantly generalize it via an entirely different
proof strategy that admits Î¨Î±âˆˆ {HÎ±,Î›Î±}, arbitrary K+, Kâˆ’âˆˆN0, and non-Gaussian mechanisms
(see Appendix O).
3.2 Tight mechanism-agnostic group privacy amplification
To further illustrate the difference between mechanism-specific and -agnostic bounds, let us apply
the framework of Balle et al. [15] to the same setting: For Kâˆ’= 0 orK+= 0 andÎµâ‰¥0, their
ansatz shows that the subsampled mechanism M=Bâ—¦Sis(Îµâ€², Î´â€²)-DP with Îµâ€²= log(1 + (1 âˆ’
Binom(0 |K, r))Â·(eÎµâˆ’1))andÎ´â€²=PK
k=1Binom( k|K, r)Â·Î´k, with group privacy parameters
Î´k= max y,yâ€²Hexp(Îµ)(by||byâ€²)s.t.dY(y, yâ€²)â‰¤k(see Appendix N).
Tightness. This bound is tight in a mechanism-agnostic sense, i.e., for every Îµâ‰¥0, one can construct
some worst-case mechanism that exactly attains the bound (see Appendix N.2).
Mechanism-specific vs mechanism-agnostic tightness. An apparent advantage of this result is that
it expresses (Îµâ€², Î´â€²)as a simple analytic formula of base mechanism DP parameters Îµ, Î´1, . . . , Î´ K.
However, this simplicity comes at a cost: As we show in Appendix N.3, this guarantee implicitly
upper-bounds the tight mixture divergence bound from Theorem 3.7 in terms of its component
divergences using joint convexity.2As we experimentally demonstrate in Section 4, this relaxation
leads to weaker privacy guarantees for group size Kâ‰¥2. This gap has gone unnoticed in earlier work,
because Theorem 3.8 happens to be identical to the bound of Balle et al. [15] for the special case of
K= 1(see proof of Proposition 30 in [ 10]). By studying the more complex group privacy setting,
we demonstrate for the first time that there is a qualitative difference between mechanism-agnostic
and mechanism-specific tightness in privacy amplification .
3.3 From mechanism-specific to mechanism-agnostic guarantees
The observed relation between tight mechanism-specific and mechanism-agnostic bounds does in fact
extend beyond group privacy (see Fig. 2): As we demonstrate in Appendices G and H, mechanism-
agnostic subsampling guarantees from prior work can equivalently be derived by (1) conditioning on
at most 4events indicating the presence of inserted / deleted / substituted elements, (2) defining a
simultaneous coupling, and (3) using joint convexity to upper-bound the resultant mechanism-specific
guarantee by component divergences. This includes the ADP guarantees of Balle et al. [15] (and thus
prior work [14, 43, 44]) and the RDP guarantees from [28â€“30, 40].
Subsumption of [ 15].Going even further, we show in Appendix G.2 that our approach subsumes the
entire framework of Balle et al. [15]: Any mechanism-agnostic guarantee derived via their ansatz can
equivalently be derived by defining a (potentially suboptimal) simultaneous coupling between four
subsampling distributions and upper-bounding the resultant guarantee via (advanced) joint convexity.
Contribution. Importantly, our contribution is not in the bounds themselves, but in the identification
of this implicitly underlying pattern that unifies prior work. We further generalize this pattern through
our proposed framework to enable tight analysis for more challenging scenarios like group privacy.
Novel RDP accounting bounds. In Appendix I, we use this observation to derive a variety of
novel mechanism-agnostic RDP bounds for different combinations of â‰ƒX,â‰ƒY, and subsampling
schemes, such as insertion/removal and subsampling without replacement. We further derive a simple
but tight mechanism-specific guarantee for subsampled randomized response under substitution
(see Theorem I.3) that outperforms the best known mechanism-agnostic bound (see Section 4).
2In combination with â€œadvanced joint convexityâ€ [15].
73.4 From mechanism-specific guarantees to dominating pairs.
Because the mechanism-specific guarantees derived via our framework do not obfuscate the un-
derlying distributions, they can be used to identify dominating pairs. These dominating pairs can
then be combined with arbitrary (numerical) accountants to track privacy under composition. For
example, we can immediately read off from Theorem 3.8 that the two Gaussian mixtures are a
dominating pair for the â€œinsert- K+-remove- Kâˆ’â€œ relation. In Appendix J, we describe a procedure for
constructing dominating pairs when the bound is a weighted sum of multiple mixture divergences. We
can thus determine dominating pairs for any bound derived via optimal transport. As we demonstrate
in Appendix K, the dominating pairs for subsampled mechanisms derived by Zhu et al. [10] (special
cases of which appear in [8, 17â€“19, 11]) can equivalently be proven via our framework.
Subsampling with replacement. As a novel contribution, we derive for the first time dominating
pairs for subsampling with replacement (see Theorem L.5), which were posited but not proven in [ 8]
(see discussion in Appendix L). This is enabled by our solution to the problem in Theorem 3.7. As we
experimentally show in Appendix B.1.5, these bounds can be much stronger than those derived via
the framework of Balle et al. [15]. These results thus demonstrate that there is a qualitative difference
between mechanism-specific and mechanism-agnostic tightness even for group size K= 1.
3.5 Limitations and future work
Limitations. While our proposed framework can be applied to arbitrary subsampling distributions,
conditioning on a finite set of events may be too restrictive in certain settings (e.g., continuous batch
spaces). Also, while we found conditioning on the number of modified elements to be sufficient for
all considered scenarios, an automated procedure for selecting events would be desirable. Maximal
couplings fulfill this purpose in [15], but only yield pairs of distributions.
Future work. A natural direction is applying our ansatz to novel settings other than group privacy. In
particular, it could potentially be used to provide epoch-level guarantees for correlated subsampling
(e.g., batching via shuffling), similar to [ 38]. We present a preliminary result for 2-fold non-adaptive
composition in Appendix P. Future work could also use our solutions to the problem in Theorem 3.7 to
generalize the matrix mechanism analysis from [ 24] to substitutions and non-Gaussian distributions.
4 Experimental evaluation
The purpose of the following experiments is to verify that there can be a benefit to using mechanism-
specific over mechanism-agnostic subsampling bounds, and that a joint analysis of subsampling and
group privacy can offer stronger guarantees than post-hoc application of the generic group privacy
property. In all figures, â€œspecificâ€œ refers to our proposed mechanism-specific bounds, â€œagnosticâ€œ
refers to (tight) mechanism-agnostic bounds, and â€œpost-hocâ€œ refers to applying the generic group
privacy property to tight mechanism-specific bounds derived for group size 1. For all experiments,
we assume â„“psensitivities of 1. Further details on the experimental setup are provided in Appendix C.
An implementation will be made available at https://cs.cit.tum.de/daml/group-amplification.
101102103104
RDPÎ±10âˆ’610âˆ’410âˆ’2RDPÏ(Î±)
Î¸
0.9
0.75
0.6Agnostic
Specific
Figure 3: Randomized response with WOR sub-
sampling ( q / N = 0.001), group size 1, and
varying true response probability Î¸.0 1 2 3 4 5 6
ADP Îµ0.00.20.40.6ADP Î´(Îµ)Group size
16
8
4
2
1Agnostic
Specific
Figure 4: Laplace mechanisms with scale Î»=
1, Poisson subsampling ( r= 0.2), and varying
group size.
84.1 Mechanism-agnostic and mechanism-specific guarantees
Randomized response and RDP. One potential source of looseness in mechanism-agnostic bounds
is that they bound mixture divergences in terms of component divergences that can be summarized by
a single privacy parameter. As an example, let us consider the best known guarantee for substitution,
subsampling without replacement, and RDP from [ 28]. As discussed by the authors, it is only tight
up to factors that are constant in Î±. In Fig. 3 we compare it to our tight mechanism-specific bound for
randomized response (see Theorem I.3) with batch-to-dataset ratio q / N = 0.001and true response
probability Î¸âˆˆ {0.6,0.75,0.9}. In Appendix B.1.1 we consider other ratios. The tight guarantee
eliminates the constant factors and thus achieves much smaller Ïfor a wide range of Î±âˆˆ(1,104].
Group privacy and ADP. Another potential source of looseness in mechanism-agnostic bounds
is that they stem from a binary partitioning of the batch space (recall Fig. 2), which may not be
sufficient when there are multiple possible levels of privacy leakage (e.g. number of sampled group
elements). We demonstrate this in Fig. 4, by comparing the tight mechanism-agnostic group privacy
bound derived via the framework from [ 15] to our tight mechanism-specific bound for Laplace
mechanisms (see Theorem M.2), ADP, scale Î»= 1, subsampling rate r= 0.2, and varying group
size. In Appendix B.1.2 we repeat the experiment with other mechanisms and parameter values, and
also consider RDP. As discussed in Section 3.2, the mechanism-agnostic bound is identical to the
mechanisms-specific bound for group size K= 1. For group sizes Kâ‰¥2, however, the fine-grained
partitioning underlying the mechanism-specific bound yields stronger privacy guarantees. These
results confirm that we need to distinguish between mechanism-agnostic and mechanism-specific
tightness when analyzing complex subsampling settings.
4.2 Post-hoc and mechanism-specific group privacy analysis
Single-iteration group privacy. Our next goal is to demonstrate the benefit of analyzing group
privacy and subsampling jointly. In Fig. 5, we evaluate our tight guarantee for Gaussian mechanisms
(see Theorem 3.8) with Ïƒ= 2, rate r= 0.2, and varying group size. As a baseline, we evaluate the
same tight guarantee with K++Kâˆ’= 1 and apply the generic group privacy property (see Ap-
pendix C.1.4) in a post-hoc manner. As can be seen, our tight analysis can lead to much stronger
privacy guarantees. However, we interestingly observe in Appendix B.2.1 that the generic group
privacy properties of ADP and RDP can serve as increasingly tight upper bounds when considering
more private base mechanisms (e.g., Ïƒ= 5) and much smaller subsampling rates (e.g., r= 10âˆ’3).
Composed group privacy. Nevertheless, even the gaps in tightness for very private mechanisms may
quickly accumulate when repeatedly applying these mechanisms to a dataset. In Fig. 6, we use the
dominating pairs derived via our framework to conduct tight PLD accounting [ 16,8] with â€œconnect
the dotsâ€ [ 20] quantization using Googleâ€™s dp_accounting library [ 45]. For Ïƒ= 5,r= 10âˆ’3, and
fixed Îµ= 2 (for other pararameters and mechanisms, see Appendix B.2.3), the post-hoc analysis
quickly diverges with increasing group size and number of iterations. For example, with group size
16and privacy budget Î´= 10âˆ’6, the post-hoc analysis allows less than 100iterations of DP-SGD
training [ 36,6], whereas our tight analysis enables training for over 1000 iterations. In Appendix B.2.5
we train an image classification model on MNIST with PLD accounting to demonstrate that this
increased number of training iterations can translate to much higher model utility.
0 1 2 3 4 5 6
ADP Îµ0.00.20.40.6ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
Specific
Figure 5: Gaussian mechanisms with standard
deviation Ïƒ= 2, Poisson subsampling ( r= 0.2),
and varying group size.0 250 500 750 1000
Iteration t10âˆ’2210âˆ’1610âˆ’1010âˆ’4ADP Î´(Îµ)
Group size
16
84
21Post-hoc
Specific
Figure 6: PLD accounting for Gaussian mecha-
nisms ( Ïƒ= 5), Poisson subsampling ( r= 10âˆ’3),
and varying group size at Îµ= 2.
95 Related Work
Amplification by subsampling for ADP. Using subsampling to strenghten (Îµ, Î´)-DP guarantees
has a long history [ 13,14] particularly in privacy-preserving machine learning [ 46,47,6]. For a
thorough introduction to subsampling and its interaction with composition, we refer the reader to [ 48].
Balle et al. [15] ultimately proposed a framework for deriving tight mechanism-agnostic subsampling
guarantees. Our work subsumes theirs and enables the derivation of mechanism-specific guarantees.
Amplification for privacy accountants. Abadi et al. [6]â€™s seminal work on moments accounting
already considered subsampling. Similiar results were derived for the more general notion of RDP [ 7]
in [28â€“30]. More recent works on accounting generally include a discussion of either Poisson
subsampling or subsampling without replacement [ 17â€“19,21,11]. Subsampling with replacement is
discussed in [ 8], albeit without complete proofs, which we provide in Appendix L. We demonstrate
that amplification guarantees for the central notions of RDP [ 7] and dominating pairs [ 10] can â€“ just
like guarantees for ADP â€“ be derived via optimal transport. Our novel approach not only unifies prior,
but also enables a principled analysis of challenging scenarios like group privacy.
Group privacy amplification. A special case of group privacy amplification (which we use as an
example to demonstrate the utility of our general framework) with hockey stick divergences, Gaussian
mechanisms, and groups that collaboratively agree to contribute all their data was posited in [ 24]
and proven in a recent follow-up note [ 22]. However, their result (and proof strategy) does not cover
RÃ©nyi divergences, other mechanisms (Laplace, randomized response), and the standard notion of
group privacy under which group members are not forced to collaborate (see, e.g., [3, 7, 9, 15, 23]).
Hierarchical randomized smoothing. Randomized smoothing [ 49â€“51] is a technique for construct-
ing provably robust models via DP achieved through input perturbations (e.g. [ 52â€“68]). Similar to
subsampling, Scholten et al. [69] proposed to only apply perturbations to randomly sampled parts of
an input. We repurpose their technique of treating subsampling indicators as observable random vari-
ables in order to construct dominating pairs from weighted sums of divergences (see Appendix J.1).
Unified amplification for f-DP. Wang et al. [70] proved a form of joint concavity for the tradeoff
functions underlying f-DP accounting [ 9,34]. This enables them to analyze mixtures induced by
random initialization and shuffling in a unified manner. However, they do notconsider amplification
by subsampling, and explicitly state that they need to address this problem in future work. Note that
dominating pairs, which can be derived via our proposed framework, can be used in f-DP accounting
due to duality of privacy profiles and tradeoff functions [9, 10].
6 Conclusion
The main purpose of this work is to provide a unified, principled framework for mechanism-specific
subsampling analysis and subsampling analysis for privacy accountants. To this end, we proposed
a three-step procedure based on optimal transport between conditional subsampling distributions.
Beyond recovering known guarantees for RÃ©nyi DP and dominating pairs, this procedure lets us
derive novel results that were previously only available for approximate DP, such as non-standard
combinations of subsampling schemes and neighboring relations, or subsampling with replacement.
We then applied this procedure to the problem of analyzing group privacy under subsampling.
Our experimental evaluation demonstrates that our mechanism-specific group privacy amplification
bounds are not only tight, but can also significantly outperform tight mechanism-agnostic bounds and
traditional group privacy results â€“ even under composition. On a higher level, these bounds represent
a novel contribution to a larger body of work (e.g., [ 38,71,72]) that demonstrates the benefit of
analyzing multiple properties of differential privacy jointly instead of independently.
7 Acknowledgements
We are grateful to Georgios Kaissis for valuable discussions and feedback on generalizing our work
beyond RÃ©nyi differential privacy, Yan Scholten for pointing out connections between subsampled
mechanisms and hierarchical randomized smoothing, as well as Leo Schwinn and Nicholas Gao for
proofreading our manuscript. This research was funded by the German Research Foundation, grant
GU 1409/4-1, and the Munich Data Science Institute (MDSI) at Technical University of Munich
(TUM) via the Linde/MDSI Doctoral Fellowship program and the MDSI Seed Fund.
10References
[1]Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography , pages 265â€“284. Springer, 2006.
[2]Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor.
Our data, ourselves: Privacy via distributed noise generation. In Advances in Cryptology -
EUROCRYPT , pages 486â€“503, 2006.
[3]Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Founda-
tions and Trends in Theoretical Computer Science , 9(3â€“4):211â€“407, 2014.
[4]Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential
privacy. In International conference on machine learning , pages 1376â€“1385, 2015.
[5]Jack Murtagh and Salil Vadhan. The complexity of computing the optimal composition of
differential privacy. In Theory of Cryptography Conference , pages 157â€“175, 2015.
[6]Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep learning with differential privacy. In Proceedings of the SIGSAC conference
on computer and communications security , pages 308â€“318, 2016.
[7]Ilya Mironov. RÃ©nyi differential privacy. In IEEE 30th computer security foundations symposium
(CSF) , pages 263â€“275, 2017.
[8]Antti Koskela, Joonas JÃ¤lkÃ¶, and Antti Honkela. Computing tight differential privacy guarantees
using fft. In International Conference on Artificial Intelligence and Statistics , pages 2560â€“2569,
2020.
[9]Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. Journal of the Royal
Statistical Society Series B: Statistical Methodology , 84(1):3â€“37, 2022.
[10] Yuqing Zhu, Jinshuo Dong, and Yu-Xiang Wang. Optimal accounting of differential privacy
via characteristic function. In International Conference on Artificial Intelligence and Statistics ,
pages 4782â€“4817, 2022.
[11] Wael Alghamdi, Juan Felipe GÃ³mez, Shahab Asoodeh, FlÃ¡vio P. Calmon, Oliver Kosut, and
Lalitha Sankar. The saddle-point method in differential privacy. In International Conference on
Machine Learning , 2023.
[12] Jiachen Tianhao Wang, Saeed Mahloujifar, Tong Wu, Ruoxi Jia, and Prateek Mittal. A random-
ized approach to tight privacy accounting. Advances in Neural Information Processing Systems ,
36, 2023.
[13] Shiva Prasad Kasiviswanathan, Homin K Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam
Smith. What can we learn privately? SIAM Journal on Computing , 40(3):793â€“826, 2011.
[14] Ninghui Li, Wahbeh Qardaji, and Dong Su. On sampling, anonymization, and differential pri-
vacy or, k-anonymization meets differential privacy. In Proceedings of the 7th ACM Symposium
on Information, Computer and Communications Security , pages 32â€“33, 2012.
[15] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight
analyses via couplings and divergences. Advances in neural information processing systems , 31,
2018.
[16] Sebastian Meiser and Esfandiar Mohammadi. Tight on budget? tight bounds for r-fold approxi-
mate differential privacy. In Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security , pages 247â€“264, 2018.
[17] Antti Koskela and Antti Honkela. Computing differential privacy guarantees for heterogeneous
compositions using fft. In International Conference on Learning Representations , 2021.
[18] Antti Koskela, Joonas JÃ¤lkÃ¶, Lukas Prediger, and Antti Honkela. Tight differential privacy
for discrete-valued mechanisms and for the subsampled gaussian mechanism using fft. In
International Conference on Artificial Intelligence and Statistics , pages 3358â€“3366, 2021.
11[19] Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential
privacy. Advances in Neural Information Processing Systems , 34:11631â€“11642, 2021.
[20] Vadym Doroshenko, Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi. Connect
the dots: Tighter discrete approximations of privacy loss distributions. Proceedings on Privacy
Enhancing Technologies , 2022.
[21] Badih Ghazi, Pritish Kamath, Ravi Kumar, and Pasin Manurangsi. Faster privacy accounting via
evolving discretization. In International Conference on Machine Learning , pages 7470â€“7483,
2022.
[22] Arun Ganesh. Tight group-level dp guarantees for dp-sgd with sampling via mixture of gaussians
mechanisms. arXiv preprint arXiv:2401.10294 , 2024.
[23] Salil Vadhan. The complexity of differential privacy. Tutorials on the Foundations of Cryptog-
raphy , pages 347â€“450, 2017.
[24] Christopher A. Choquette-Choo, Arun Ganesh, Thomas Steinke, and Abhradeep Guha Thakurta.
Privacy amplification for matrix mechanisms. In International Conference on Learning Repre-
sentations , 2024.
[25] Gilles Barthe and Federico Olmedo. Beyond differential privacy: Composition theorems
and relational logic for f-divergences between probabilistic programs. In Fedor V . Fomin,
RÂ¯usin,Å¡ Freivalds, Marta Kwiatkowska, and David Peleg, editors, Automata, Languages, and
Programming , pages 49â€“60, 2013.
[26] Cynthia Dwork and Guy N Rothblum. Concentrated differential privacy. arXiv preprint
arXiv:1603.01887 , 2016.
[27] Mark Bun and Thomas Steinke. Concentrated differential privacy: Simplifications, extensions,
and lower bounds. In Theory of Cryptography Conference , pages 635â€“658, 2016.
[28] Yu-Xiang Wang, Borja Balle, and Shiva Prasad Kasiviswanathan. Subsampled rÃ©nyi differential
privacy and analytical moments accountant. In The 22nd International Conference on Artificial
Intelligence and Statistics , pages 1226â€“1235, 2019.
[29] Ilya Mironov, Kunal Talwar, and Li Zhang. RÃ©nyi differential privacy of the sampled gaussian
mechanism. arXiv preprint arXiv:1908.10530 , 2019.
[30] Yuqing Zhu and Yu-Xiang Wang. Poisson subsampled rÃ©nyi differential privacy. In International
Conference on Machine Learning , pages 7634â€“7642, 2019.
[31] Borja Balle, Gilles Barthe, Marco Gaboardi, Justin Hsu, and Tetsuya Sato. Hypothesis testing
interpretations and renyi differential privacy. In Proceedings of the Twenty Third International
Conference on Artificial Intelligence and Statistics , volume 108, pages 2496â€“2506, 2020.
[32] Shahab Asoodeh, Jiachun Liao, Flavio P Calmon, Oliver Kosut, and Lalitha Sankar. A better
bound gives a hundred rounds: Enhanced privacy guarantees via f-divergences. In IEEE
International Symposium on Information Theory (ISIT) , 2020.
[33] David M. Sommer, Sebastian Meiser, and Esfandiar Mohammadi. Privacy loss classes: The
central limit theorem in differential privacy. Proceedings on Privacy Enhancing Technologies ,
2019(2):245â€“269, 2019.
[34] Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. Deep learning with gaussian differential
privacy. Harvard data science review , 2020(23):10â€“1162, 2020.
[35] Hua Wang, Sheng Gao, Huanyu Zhang, Milan Shen, and Weijie J Su. Analytical composition
of differential privacy via the edgeworth accountant. arXiv preprint arXiv:2206.04236 , 2022.
[36] Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with
differentially private updates. In IEEE global conference on signal and information processing ,
pages 245â€“248, 2013.
12[37] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy profiles and amplification by subsam-
pling. Journal of Privacy and Confidentiality , 10(1), 2020.
[38] Jiayuan Ye and Reza Shokri. Differentially private learning needs hidden state (or much faster
convergence). Advances in Neural Information Processing Systems , 35:703â€“715, 2022.
[39] CÃ©dric Villani. Couplings and changes of variables , pages 5â€“20. 2009. ISBN 978-3-540-71050-
9.
[40] Ameya Daigavane, Gagan Madan, Aditya Sinha, Abhradeep Guha Thakurta, Gaurav Aggarwal,
and Prateek Jain. Node-level differentially private graph neural networks. In ICLR 2022
Workshop on PAIR^2Struct: Privacy, Accountability, Interpretability, Robustness, Reasoning on
Structured Data , 2022.
[41] Morgane Ayle, Jan Schuchardt, Lukas Gosch, Daniel ZÃ¼gner, and Stephan GÃ¼nnemann. Training
differentially private graph neural networks with random walk sampling. In Workshop on
Trustworthy and Socially Responsible Machine Learning, NeurIPS , 2022.
[42] Zihang Xiang, Tianhao Wang, and Di Wang. Preserving Node-level Privacy in Graph Neural
Networks. In IEEE Symposium on Security and Privacy (SP) , pages 4714â€“4732, 2024.
[43] Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil Vadhan. Differentially private release and
learning of threshold functions. In 2015 IEEE 56th Annual Symposium on Foundations of
Computer Science , pages 634â€“649, 2015.
[44] Jonathan Ullman. Cs7880: Rigorous approaches to data privacy. https://www.khoury.
northeastern.edu/home/jullman/cs7880s17/HW1sol.pdf , 2017. Accessed May 21,
2024.
[45] Google Differential Privacy Team. Privacy loss distributions. https://raw.
githubusercontent.com/google/differential-privacy/main/common_docs/
Privacy_Loss_Distributions.pdf , 2024. Accessed May 22, 2024.
[46] Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization:
Efficient algorithms and tight error bounds. In IEEE 55th annual symposium on foundations of
computer science , pages 464â€“473, 2014.
[47] Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for free: Posterior sampling
and stochastic gradient monte carlo. In International Conference on Machine Learning , pages
2493â€“2502, 2015.
[48] Thomas Steinke. Composition of differential privacy & privacy amplification by subsampling.
arXiv preprint arXiv:2210.00597 , 2022.
[49] Mathias LÃ©cuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In IEEE Symposium on Security
and Privacy , pages 656â€“672, 2019.
[50] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness
with additive noise. Advances in neural information processing systems , 2019.
[51] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning , 2019.
[52] Alexander Levine and Soheil Feizi. Robustness certificates for sparse adversarial attacks
by randomized ablation. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 34, pages 4585â€“4593, 2020.
[53] Aleksandar Bojchevski, Johannes Klicpera, and Stephan GÃ¼nnemann. Efficient robustness
certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and
more. In International Conference on Machine Learning , 2020.
[54] Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transfor-
mations via randomized smoothing. Advances in Neural information processing systems ,
2020.
13[55] Yihan Wu, Aleksandar Bojchevski, Aleksei Kuvshinov, and Stephan GÃ¼nnemann. Completing
the picture: Randomized smoothing suffers from curse of dimensionality for a large family of
distributions. In International Conference on Artificial Intelligence and Statistics , 2021.
[56] Aounon Kumar and Tom Goldstein. Center smoothing: Certified robustness for networks with
structured outputs. Advances in Neural Information Processing Systems , 2021.
[57] Jan Schuchardt and Stephan GÃ¼nnemann. Invariance-aware randomized smoothing certificates.
InAdvances in Neural Information Processing Systems , 2022.
[58] Motasem Alfarra, Adel Bibi, Naeemullah Khan, Philip HS Torr, and Bernard Ghanem. De-
formRS: Certifying input deformations with randomized smoothing. In Proceedings of the
AAAI Conference on Artificial Intelligence , number 6, pages 6001â€“6009, 2022.
[59] Yan Scholten, Jan Schuchardt, Simon Geisler, Aleksandar Bojchevski, and Stephan GÃ¼nnemann.
Randomized message-interception smoothing: Gray-box certificates for graph neural networks.
InAdvances in Neural Information Processing Systems , 2022.
[60] Nikita Murarev and Aleksandr Petiushko. Certified robustness via randomized smoothing
over multiplicative parameters of input transformations. In International Joint Conference on
Artificial Intelligence , 2022.
[61] Peter SÃºkenÃ­k, Aleksei Kuvshinov, and Stephan GÃ¼nnemann. Intriguing properties of input-
dependent randomized smoothing. In International conference on machine learning , 2022.
[62] Mikhail Pautov, Olesya Kuznetsova, Nurislam Tursynbek, Aleksandr Petiushko, and Ivan Os-
eledets. Smoothed embeddings for certified few-shot learning. Advances in Neural Information
Processing Systems , 2022.
[63] Jan Schuchardt, Tom WollschlÃ¤ger, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. Localized
randomized smoothing for collective robustness certification. In International Conference on
Learning Representations , 2023.
[64] Taras Rumezhak, Francisco Girbal Eiras, Philip HS Torr, and Adel Bibi. Rancer: Non-axis
aligned anisotropic certification with randomized smoothing. In Proceedings of the IEEE/CVF
Winter Conference on Applications of Computer Vision , pages 4672â€“4680, 2023.
[65] Aman Saxena, Tom WollschlÃ¤ger, Nicola Franco, Jeanette Miriam Lorenz, and Stephan GÃ¼n-
nemann. Randomized smoothing-inspired quantum encoding schemes with formal robustness
guarantees. In Quantum Techniques in Machine Learning , 2023.
[66] Samuel Pfrommer, Brendon G. Anderson, and Somayeh Sojoudi. Projected randomized
smoothing for certified adversarial robustness. Transactions on Machine Learning Research ,
2023. ISSN 2835-8856.
[67] Jan Schuchardt, Yan Scholten, and Stephan GÃ¼nnemann. Provable adversarial robustness for
group equivariant tasks: Graphs, point clouds, molecules, and more. In Advances in Neural
Information Processing Systems , 2023.
[68] Zhuoqun Huang, Neil Marchant, Keane Lucas, Lujo Bauer, Olya Ohrimenko, and Benjamin
I. P. Rubinstein. RS-Del: Edit distance robustness certificates for sequence classifiers via
randomized deletion. In Advances in Neural Information Processing Systems , NeurIPS, 2023.
[69] Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, and Stephan GÃ¼nnemann. Hierarchical
randomized smoothing. Advances in Neural Information Processing Systems , 36, 2023.
[70] Chendi Wang, Buxin Su, Jiayuan Ye, Reza Shokri, and Weijie J Su. Unified enhancement
of privacy bounds for mixture mechanisms via $f$-differential privacy. In Thirty-seventh
Conference on Neural Information Processing Systems , 2023.
[71] Jason Altschuler and Kunal Talwar. Privacy of noisy stochastic gradient descent: More iterations
without more privacy loss. Advances in Neural Information Processing Systems , 35, 2022.
14[72] Antonious Girgis, Deepesh Data, and Suhas Diggavi. RÃ©nyi differential privacy of the sub-
sampled shuffle model in distributed learning. In Advances in Neural Information Processing
Systems , volume 34, pages 29181â€“29192, 2021.
[73] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad,
Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode,
and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. arXiv preprint
arXiv:2109.12298 , 2021.
[74] Borja Balle and Yu-Xiang Wang. Improving the gaussian mechanism for differential privacy:
Analytical calibration and optimal denoising. In International Conference on Machine Learning ,
pages 394â€“403, 2018.
[75] Ãšlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and
Abhradeep Thakurta. Amplification by shuffling: From local to central differential privacy
via anonymity. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms , pages 2468â€“2479, 2019.
[76] Albert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev. Distributed
differential privacy via shuffling. In Advances in Cryptologyâ€“EUROCRYPT , pages 375â€“403,
2019.
15A Table of contents
B Additional experiments 18
B.1 Mechanism-agnostic and mechanism-specific bounds . . . . . . . . . . . . . . . . 18
B.2 Post-hoc and tight group privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
C Experimental setup 36
C.1 Evaluation of privacy guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.2 Computational resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
C.3 Assets and licenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
D General setting and definitions 38
D.1 Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.2 Neighboring relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.3 Mechanisms and subsampling schemes . . . . . . . . . . . . . . . . . . . . . . . . 38
D.4 Differential privacy notions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
D.5 Joint convexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
D.6 Couplings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
E Proof of optimal transport bounds 40
E.1 Proof of Theorem 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
E.2 Proof of Theorem 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
E.3 Proof of Proposition 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
F Distance-compatible couplings of multiple distributions 42
G Recovering known mechanism-agnostic ADP guarantees 45
G.1 Overview of Balle et al. framework . . . . . . . . . . . . . . . . . . . . . . . . . . 45
G.2 Subsumption of Balle et al. framework . . . . . . . . . . . . . . . . . . . . . . . . 46
G.3 Deficiencies of mechanism-agnostic ADP bounds. . . . . . . . . . . . . . . . . . . 48
H Recovering known mechanism-agnostic RDP guarantees 49
H.1 Subsampling without replacement and substitution . . . . . . . . . . . . . . . . . 49
H.2 Poisson subsampling and insertion/removal . . . . . . . . . . . . . . . . . . . . . 52
H.3 Graph subsampling without replacement and node modification . . . . . . . . . . . 54
I Novel RDP guarantees 56
I.1 Hybrid neighboring relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
I.2 Subsampling without replacement under insertion/removal . . . . . . . . . . . . . 57
I.3 Tight mechanism-specific subsampling without replacement for randomized response 59
J From mechanism-specific guarantees to dominating pairs 61
J.1 Step 1: Eliminating weighted sums . . . . . . . . . . . . . . . . . . . . . . . . . . 61
16J.2 Step 2: Resolving non-constancy in dataset pairs . . . . . . . . . . . . . . . . . . 62
J.3 Step 3: Resolving non-constancy in divergence order . . . . . . . . . . . . . . . . 63
K Recovering known dominating pairs 64
K.1 Poisson subsampling and insertion/removal . . . . . . . . . . . . . . . . . . . . . 64
K.2 Subsampling without replacement and insertion/removal . . . . . . . . . . . . . . 65
K.3 Subsampling without replacement and substitution . . . . . . . . . . . . . . . . . 65
L Novel results for dominating pairs 67
L.1 Subsampling without replacement and substitution . . . . . . . . . . . . . . . . . 67
L.2 Subsampling with replacement and substitution . . . . . . . . . . . . . . . . . . . 68
M Tight mechanism-specific group privacy amplification 71
M.1 Proof of Theorem 3.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
M.2 Instantiations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72
M.3 Tightness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
M.4 Numerical evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
N Tight mechanism-agnostic group privacy amplification 78
N.1 ADP guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
N.2 Tightness of ADP guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
N.3 Relation to tight mechanism-specific bound . . . . . . . . . . . . . . . . . . . . . 80
N.4 RDP guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
N.5 Asymptotic RDP guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
O Worst-case mixture components 83
O.1 Gaussian and Laplacian mixtures . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
O.2 Reduction of the divergence to a univariate integral . . . . . . . . . . . . . . . . . 91
O.3 Randomized Response Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92
P Towards epoch-level subsampling analysis 97
P.1 Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
P.2 Optimal transport without conditioning . . . . . . . . . . . . . . . . . . . . . . . . 97
P.3 Optimal transport with conditioning . . . . . . . . . . . . . . . . . . . . . . . . . 98
P.4 Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
Q Broader impact 100
17B Additional experiments
B.1 Mechanism-agnostic and mechanism-specific bounds
B.1.1 Randomized response and RDP
In Fig. 7, we repeat the experiment from Fig. 3 with other ratios of batch and dataset sizes. That
is, we compare our tight mechanism-specific RDP guarantee from Theorem I.3 to the best known
mechanism-agnostic bound from [ 28] to illustrate that mechanism-agnostic bounds may loose privacy
by bounding mixture divergences in terms of component divergences â€“ even outside the group privacy
setting. The tight guarantee eliminates constant factors and thus achieves much smaller Ïfor a wide
range of Î±âˆˆ(1,104], especially for small ratios.
q / N = 0.01
101102103104
RDPÎ±10âˆ’410âˆ’310âˆ’210âˆ’1RDPÏ(Î±)
Î¸
0.9
0.75
0.6Agnostic
Specificq / N = 0.1
101102103104
RDPÎ±10âˆ’210âˆ’1RDPÏ(Î±)
q / N = 0.2
101102103104
RDPÎ±10âˆ’210âˆ’1100RDPÏ(Î±)q / N = 0.5
101102103104
RDPÎ±10âˆ’1100RDPÏ(Î±)
Figure 7: Randomized response under subsampling without replacement, with varying true response
probability Î¸and batch-to-dataset ratio q / N . Theorem I.3 significantly improves upon the baseline
for a wide range of Î±.
B.1.2 Group privacy and ADP
In Figs. 8 and 9 we repeat the experiment from Fig. 4 for other subsampling rates r, standard
deviations Ïƒ, and also with Laplace mechanisms. That is, we compare our tight mechanism-specific
group privacy guarantees to the tight mechanism-agnostic bounds derived via the framework of [ 15].
The results demonstrate that mechanism-specific tightness is a stronger property that can result in
better privacy guarantees â€“ although the difference in significantly more pronounced for Laplace
mechanisms and/or larger group sizes.
In Fig. 10 we repeat the experiment with randomized response mechanisms. Here, both approaches
yield identical guarantees. This verifies that randomized reseponse is indeed the worst-case mecha-
nisms for which the mechanism-agnostic bound is optimal (see [15] and Appendix N.2).
18Gaussian mechanism
r= 0.1, Ïƒ= 1
0 2 4 6 8 10
ADP Îµ0.00.20.4ADP Î´(Îµ)Group size
16
8
4
2
1Agnostic
Specificr= 0.1, Ïƒ= 2
0 1 2 3 4 5
ADP Îµ0.00.10.20.3ADP Î´(Îµ)
r= 0.1, Ïƒ= 5
0.0 0.5 1.0 1.5 2.0
ADP Îµ0.000.050.10ADP Î´(Îµ)r= 0.2, Ïƒ= 2
0 1 2 3 4 5 6
ADP Îµ0.00.20.4ADP Î´(Îµ)
r= 0.2, Ïƒ= 5
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.10.2ADP Î´(Îµ)r= 0.5, Ïƒ= 1
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.5, Ïƒ= 2
0 2 4 6 8 10
ADP Îµ0.00.20.40.60.8ADP Î´(Îµ)r= 0.5, Ïƒ= 5
0 1 2 3 4 5 6
ADP Îµ0.00.20.4ADP Î´(Îµ)
Figure 8: Gaussian mechanisms under Poisson subsampling, with varying standard deviation Ïƒ,
subsampling rate r, and group size. The tight mechanism-specific guarantees are stronger than the
tight mechanism-agnostic bounds, especially for larger group sizes and smaller subsampling rates.
19Laplace mechanism
r= 0.1, Î»= 0.5
0 2 4 6 8 10
ADP Îµ0.00.20.40.6ADP Î´(Îµ)Group size
16
8
4
2
1Agnostic
Specificr= 0.1, Î»= 1
0 1 2 3 4
ADP Îµ0.00.10.20.30.4ADP Î´(Îµ)
r= 0.1, Î»= 2
0.0 0.5 1.0 1.5 2.0
ADP Îµ0.00.10.20.3ADP Î´(Îµ)r= 0.2, Î»= 0.5
0 2 4 6 8 10
ADP Îµ0.00.20.40.60.8ADP Î´(Îµ)
r= 0.2, Î»= 2
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.20.4ADP Î´(Îµ)r= 0.5, Î»= 0.5
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.5, Î»= 1
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)r= 0.5, Î»= 2
0 1 2 3 4 5 6
ADP Îµ0.00.20.40.60.8ADP Î´(Îµ)
Figure 9: Laplace mechanisms under Poisson subsampling, with varying scale Î», subsampling rate
r, and group size. The tight mechanism-specific guarantees are stronger than the tight mechanism-
agnostic bounds, especially for larger group sizes.
20Randomized response mechanism
r= 0.1, Î¸= 0.7
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.10.20.3ADP Î´(Îµ)Group size
16
8
4
2
1Agnostic
Specificr= 0.1, Î¸= 0.8
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.20.4ADP Î´(Îµ)
r= 0.1, Î¸= 2
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.20.40.6ADP Î´(Îµ)r= 0.2, Î¸= 0.7
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.10.20.30.4ADP Î´(Îµ)
r= 0.2, Î¸= 0.8
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.20.40.6ADP Î´(Îµ)r= 0.5, Î¸= 0.7
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.10.20.30.4ADP Î´(Îµ)
r= 0.5, Î¸= 0.8
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.20.40.6ADP Î´(Îµ)r= 0.5, Î¸= 0.9
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.20.40.60.8ADP Î´(Îµ)
Figure 10: Randomized response under Poisson subsampling, with varying true response probability
Î¸, subsampling rate r, and group size. Randomized response is the worst-case mechanism for which
the mechanism-agnostic guarantee is optimal, so both ansÃ¤tze yield identical results.
21B.1.3 Group privacy and RDP
In Fig. 11 we repeat our comparison of mechanism-agnostic (Theorem N.5) and tight mechanism-
specific group privacy amplification (Theorem 3.8) with RDP instead of ADP. We observe that the
tight guarantee delays the phase transition from a high- to a low-privacy regime that was already
observed in [ 30]. In other words, we have small Ïfor larger Î±, which is beneficial for conversion to
ADP after composition (see conversion formulae in [7, 31]).
r= 0.1, Ïƒ= 1
1.1Ã—1002Ã—1004Ã—100
RDPÎ±10âˆ’1101103105RDPÏ(Î±)Group size
8
4
2
1Agnostic
Specificr= 0.001, Ïƒ= 1
101 1.1Ã—1004Ã—100
RDPÎ±10âˆ’410âˆ’1102105RDPÏ(Î±)
r= 0.1, Ïƒ= 2
101 1.1Ã—1004Ã—100
RDPÎ±10âˆ’2100102104RDPÏ(Î±)r= 0.001, Ïƒ= 2
101
RDPÎ±10âˆ’510âˆ’2101104RDPÏ(Î±)
r= 0.001, Ïƒ= 5
101102103104
RDPÎ±10âˆ’310âˆ’1101103RDPÏ(Î±)r= 0.001, Ïƒ= 5
101102103104
RDPÎ±10âˆ’610âˆ’3100103RDPÏ(Î±)
Figure 11: Gaussian mechanisms under Poisson subsampling, with varying standard deviation Ïƒ,
subsampling rate r, and group size. The mechanism-specific guarantee delays the phase transition
from high to low privacy.
22B.1.4 Benefit of conditioning
Next, we demonstrate the benefit of coupling multiple conditional distributions over coupling just
two subsampling distributions in deriving amplification guarantees. Specifically, we evaluate Proposi-
tion H.7, which can be derived via Theorem 3.3, i.e., optimal transport without conditioning. Note
that, unlike with the guarantees of Wang et al. [28], this bound depends on both the dataset size and
the batch size, not just their ratio. We make the following observations: For large Î±Proposition H.7
converges to the baseline. Furthermore, this approach never outperforms the baseline for group size
1. Neither does it outperform the baseline for ratios q / Lâˆˆ {0.01,0.001}. But, for q / L = 0.1,
there is a sweet spot of alphas between 101and102in which it offers stronger guarantees. This
further reinforces our claim that there is a benefit to treating group privacy and amplification jointly.
Furthermore, we observe that in the case of q= 1, Proposition H.7 outperforms the baseline for large
alpha, since it can capture that one cannot possibly include more than one substituted element in a
singleton batch.
Overall, we can conclude that there is a benefit to jointly analyzing group privacy and subsampling
in this manner, but that optimal transport without conditioning is not sufficient for tightly analyzing
such complicated scenarios.
N= 105, q= 104
101102103104
RDPÎ±10âˆ’2100102RDPÏ(Î±)
Group size
4
2
1No conditioning
Post-hocN= 105, q= 102
101102103104
RDPÎ±10âˆ’510âˆ’2101104RDPÏ(Î±)
Group size
4
2
1No conditioning
Post-hoc
N= 104, q= 103
101102103104
RDPÎ±10âˆ’2100102RDPÏ(Î±)
Group size
4
2
1No conditioning
Post-hocN= 104, q= 101
101102103104
RDPÎ±10âˆ’510âˆ’2101104RDPÏ(Î±)
Group size
4
2
1No conditioning
Post-hoc
N= 103, q= 102
101102103104
RDPÎ±10âˆ’2100102RDPÏ(Î±)
Group size
4
2
1No conditioning
Post-hocN= 103, q= 100
101102103104
RDPÎ±10âˆ’510âˆ’2101104RDPÏ(Î±)
Group size
4
2
1No conditioning
Post-hoc
Figure 12: Proposition H.7 derived from Theorem 3.3 applied to Gaussian mechanism ( Ïƒ= 5.0)
under sampling without replacement for varying dataset size N, batch size q, and group size. Optimal
transport without conditioning does not always improve upon the baseline.
23B.1.5 Subsampling with replacement
The previous experiments demonstrated that mechanism-specific analysis can improve upon non-tight
mechanism-agnostic and â€“ in the group privacy setting â€“ tight mechanism-agnostic analysis. In the fol-
lowing, we demonstrate that mechanism-specific bounds can improve upon tight mechanism-agnostic
bounds even for single-element relations, i.e., group size 1. To this end, we consider subsampling
with replacement for Gaussian mechanisms under the substitution relation, comparing Theorem L.5
to Theorem 10 of Balle et al. [15].
Fig. 13 shows the resultant privacy profiles for standard deviation Ïƒ= 1, dataset size N= 100 , and
batch size q= 8. For Îµâ‰¥2, the mechanism-specific bound is more than an order of magnitude
smaller. Intuitively, this gap can be explained similarly to the gap in the group privacy setting: The
single substituted element can be sampled 0,1,2, or up to qtimes, with each case causing different
levels of privacy leakage. Mechanism-agnostic bounds rely on a binary partitioning of the event
space, which is not sufficient for capturing this granular behavior (recall Fig. 2.
Note that due to the relaxations of distance constraints we performed in deriving Theorem L.5, the
mechanism-specific bound is not tight. A tight bound might lead to an even larger gap. Thus, this
experiments further reinforces that there is a qualitative difference between mechanism-specific and
mechanism-agnostic tightness.
100101
ADP Îµ10âˆ’910âˆ’710âˆ’510âˆ’3ADP Î´(Îµ)
Agnostic
Specific
Figure 13: Gaussian mechanism ( Ïƒ= 1) under sampling with replacement for single-element
substitutions, dataset size N= 100 , and batch size q= 8. Using the mechanism-specific bound
results in stronger privacy guarantees.
24B.2 Post-hoc and tight group privacy
B.2.1 Single-iteration ADP
In Figs. 14 to 15 we repeat the experiment from Fig. 5 for other subsampling rates r, standard
deviations Ïƒ, and mechanisms. That is, we compare our tight mechanism-specific guarantees to
post-hoc use of the group privacy property. For all mechanisms, the tight mechanism-specific analysis
yields stronger privacy guarantees than the baseline â€“ particularly for large group sizes. However, we
interestingly see that with increasing base mechanisms noise level and decreasing subsampling rate,
the post-hoc bound converges towards the tight bound.
25Gaussian mechanism
r= 0.1, Ïƒ= 1
0 2 4 6 8 10
ADP Îµ0.00.20.40.6ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
Specificr= 0.1, Ïƒ= 2
0 1 2 3 4 5
ADP Îµ0.00.10.20.3ADP Î´(Îµ)
r= 0.1, Ïƒ= 5
0.0 0.5 1.0 1.5 2.0
ADP Îµ0.000.050.10ADP Î´(Îµ)r= 0.2, Ïƒ= 1
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.2, Ïƒ= 5
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.10.2ADP Î´(Îµ)r= 0.5, Ïƒ= 1
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.5, Ïƒ= 2
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)r= 0.5, Ïƒ= 5
0 1 2 3 4 5 6
ADP Îµ0.00.20.40.6ADP Î´(Îµ)
Figure 14: Gaussian mechanisms under Poisson subsampling, with varying standard deviation Ïƒ,
subsampling rate r, and group size. Analyzing group privacy and subsampling jointly instead of in a
post-hoc manner offers stronger guarantees.
26Laplace mechanism
r= 0.1, Î»= 0.5
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
Specificr= 0.1, Î»= 1
0 1 2 3 4
ADP Îµ0.00.20.40.6ADP Î´(Îµ)
r= 0.1, Î»= 2
0.0 0.5 1.0 1.5 2.0
ADP Îµ0.00.10.20.3ADP Î´(Îµ)r= 0.2, Î»= 0.5
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.2, Î»= 2
0.0 0.5 1.0 1.5 2.0 2.5 3.0
ADP Îµ0.00.20.40.6ADP Î´(Îµ)r= 0.5, Î»= 0.5
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.5, Î»= 1
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)r= 0.5, Î»= 2
0 1 2 3 4 5 6
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
Figure 15: Laplace mechanisms under Poisson subsampling, with varying scale Î», subsampling rate
r, and group size. Analyzing group privacy and subsampling jointly instead of in a post-hoc manner
offers stronger guarantees.
27Randomized response mechanism
r= 0.1, Î¸= 0.7
0.0 0.5 1.0 1.5 2.0
ADP Îµ0.00.20.40.6ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
Specificr= 0.1, Î¸= 0.8
0 1 2 3 4 5 6
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.1, Î¸= 2
0 1 2 3 4 5 6
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)r= 0.2, Î¸= 0.7
0.0 0.5 1.0 1.5 2.0
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.2, Î¸= 0.8
0 1 2 3 4
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)r= 0.5, Î¸= 0.7
0 1 2 3 4 5 6
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
r= 0.5, Î¸= 0.8
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)r= 0.5, Î¸= 0.9
0 2 4 6 8 10
ADP Îµ0.000.250.500.751.00ADP Î´(Îµ)
Figure 16: Randomized response mechanisms under Poisson subsampling, with varying true response
probability Î¸, subsampling rate r, and group size. Analyzing group privacy and subsampling jointly
instead of in a post-hoc manner offers stronger guarantees.
28B.2.2 Single-iteration RDP
In Fig. 17 we repeat our comparison of tight mechanism-specific group privacy amplification and
post-hoc group privacy for RDP instead of ADP. The tight analysis yields stronger guarantees and
delays the phase transition from a high- to a low-privacy regime. However, as with ADP, the post-hoc
bound can be a good upper bound for small subsampling rates and very private base mechanisms.
r= 0.1, Ïƒ= 1
1.1Ã—1002Ã—1004Ã—100
RDPÎ±10âˆ’1101103105RDPÏ(Î±)Group size
8
4
2
1Post-hoc
Specificr= 0.001, Ïƒ= 1
101 1.1Ã—1004Ã—100
RDPÎ±10âˆ’410âˆ’1102105RDPÏ(Î±)
r= 0.1, Ïƒ= 2
101 1.1Ã—1004Ã—100
RDPÎ±10âˆ’2100102104RDPÏ(Î±)r= 0.001, Ïƒ= 2
101
RDPÎ±10âˆ’510âˆ’2101104RDPÏ(Î±)
r= 0.001, Ïƒ= 5
101102103104
RDPÎ±10âˆ’310âˆ’1101103RDPÏ(Î±)r= 0.001, Ïƒ= 5
101102103104
RDPÎ±10âˆ’610âˆ’3100103RDPÏ(Î±)
Figure 17: Gaussian mechanisms under Poisson subsampling, with varying standard deviation Ïƒ,
subsampling rate r, and group size. Analyzing group privacy and subsampling jointly instead of
in a post-hoc manner delays the phase transition from high to low privacy. For very private base
mechanisms and small subsampling rates, the baseline is nevertheless a good upper bound.
29B.2.3 PLD Accounting
In Figs. 18 to 21 we repeat our comparison of tight mechanism-specific and post-hoc group privacy
amplification guarantees under composition from. We consider different combinations of subsampling
rater, standard deviation Ïƒ, privacy parameter Îµ, as well as Laplace mechanisms. Even in high
privacy scenarios, where the mechanism-specific and post-hoc analysis yield similar results on a
single-iteration level, the mechanism-specific guarantees are significantly stronger under composition.
Specifically, the post-hoc analysis diverges to much larger Î´after some number of iterations. In
settings with moderate privacy (e.g. Ïƒ= 1) or large group sizes (e.g. 16), the baseline diverges after
less than 100iterations.
Gaussian mechanism ( r= 0.001)
Îµ= 0.5, Ïƒ= 1
0 250 500 750 1000
Iteration t10âˆ’1110âˆ’810âˆ’510âˆ’2ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
SpecificÎµ= 0.5, Ïƒ= 5
0 250 500 750 1000
Iteration t10âˆ’2210âˆ’1610âˆ’1010âˆ’4ADP Î´(Îµ)
Îµ= 1, Ïƒ= 1
0 250 500 750 1000
Iteration t10âˆ’1310âˆ’910âˆ’510âˆ’1ADP Î´(Îµ)Îµ= 1, Ïƒ= 5
0 250 500 750 1000
Iteration t10âˆ’2210âˆ’1610âˆ’1010âˆ’4ADP Î´(Îµ)
Îµ= 2, Ïƒ= 1
0 250 500 750 1000
Iteration t10âˆ’1710âˆ’1210âˆ’710âˆ’2ADP Î´(Îµ)Îµ= 2, Ïƒ= 5
0 250 500 750 1000
Iteration t10âˆ’2210âˆ’1610âˆ’1010âˆ’4ADP Î´(Îµ)
Figure 18: Self-composed Gaussian mechanisms under Poisson subsampling with subsampling rate
r= 0.001and varying standard deviation Ïƒ, privacy parameter Îµand group size. For sufficiently
large group sizes, the post-hoc analysis divergence from the tight mechanism-specific guarantee
within less than 1000 iterations.
30Gaussian mechanism ( r= 0.01)
Îµ= 0.5, Ïƒ= 1
0 200 400 600 800 1000
Iteration t10âˆ’510âˆ’310âˆ’1ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
SpecificÎµ= 0.5, Ïƒ= 5
0 250 500 750 1000
Iteration t10âˆ’2110âˆ’1510âˆ’910âˆ’3ADP Î´(Îµ)
Îµ= 1, Ïƒ= 1
0 200 400 600 800 1000
Iteration t10âˆ’710âˆ’510âˆ’310âˆ’1ADP Î´(Îµ)Îµ= 1, Ïƒ= 5
0 250 500 750 1000
Iteration t10âˆ’2110âˆ’1510âˆ’910âˆ’3ADP Î´(Îµ)
Îµ= 2, Ïƒ= 1
0 250 500 750 1000
Iteration t10âˆ’1010âˆ’710âˆ’410âˆ’1ADP Î´(Îµ)Îµ= 2, Ïƒ= 5
0 250 500 750 1000
Iteration t10âˆ’2110âˆ’1510âˆ’910âˆ’3ADP Î´(Îµ)
Figure 19: Self-composed Gaussian mechanisms under Poisson subsampling with subsampling rate
r= 0.01and varying standard deviation Ïƒ, privacy parameter Îµand group size. For sufficiently large
group sizes, the post-hoc analysis divergence from the tight mechanism-specific guarantee within
less than 1000 iterations.
31Laplace mechanism ( r= 0.001)
Îµ= 0.5, Î»= 1
0 250 500 750 1000
Iteration t10âˆ’1510âˆ’1110âˆ’710âˆ’3ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
SpecificÎµ= 0.5, Î»= 5
0 250 500 750 1000
Iteration t10âˆ’1410âˆ’1010âˆ’610âˆ’2ADP Î´(Îµ)
Îµ= 1, Î»= 1
0 250 500 750 1000
Iteration t10âˆ’1510âˆ’1110âˆ’710âˆ’3ADP Î´(Îµ)Îµ= 1, Î»= 5
0 250 500 750 1000
Iteration t10âˆ’1410âˆ’1010âˆ’610âˆ’2ADP Î´(Îµ)
Îµ= 2, Î»= 1
0 250 500 750 1000
Iteration t10âˆ’1510âˆ’1110âˆ’710âˆ’3ADP Î´(Îµ)Îµ= 2, Î»= 5
0 250 500 750 1000
Iteration t10âˆ’1410âˆ’1010âˆ’610âˆ’2ADP Î´(Îµ)
Figure 20: Self-composed Laplace mechanisms under Poisson subsampling with subsampling rate
r= 0.001and varying scale Î», privacy parameter Îµand group size. For sufficiently large group sizes,
the post-hoc analysis divergence from the tight mechanism-specific guarantee within less than 1000
iterations.
32Laplace mechanism ( r= 0.01)
Îµ= 0.5, Î»= 1
0 250 500 750 1000
Iteration t10âˆ’1510âˆ’1110âˆ’710âˆ’3ADP Î´(Îµ)Group size
16
8
4
2
1Post-hoc
SpecificÎµ= 0.5, Î»= 5
0 250 500 750 1000
Iteration t10âˆ’1410âˆ’1010âˆ’610âˆ’2ADP Î´(Îµ)
Îµ= 1, Î»= 1
0 250 500 750 1000
Iteration t10âˆ’1510âˆ’1110âˆ’710âˆ’3ADP Î´(Îµ)Îµ= 1, Î»= 5
0 250 500 750 1000
Iteration t10âˆ’1410âˆ’1010âˆ’610âˆ’2ADP Î´(Îµ)
Îµ= 2, Î»= 1
0 250 500 750 1000
Iteration t10âˆ’1510âˆ’1110âˆ’710âˆ’3ADP Î´(Îµ)Îµ= 2, Î»= 5
0 250 500 750 1000
Iteration t10âˆ’1410âˆ’1010âˆ’610âˆ’2ADP Î´(Îµ)
Figure 21: Self-composed Laplace mechanisms under Poisson subsampling with subsampling rate
r= 0.001and varying scale Î», privacy parameter Îµand group size. For sufficiently large group sizes,
the post-hoc analysis divergence from the tight mechanism-specific guarantee within less than 1000
iterations.
33B.2.4 RDP accounting
Finally, we compare tight group privacy amplification to the post-hoc approach for RDP accounting.
We begin with the Gaussian mechanism and subsampling rate r= 0.001in Fig. 22. Our bound offers
stronger group privacy guarantees for Ïƒ= 1.0, but both results are almost identical for very private
base mechanisms with Ïƒ= 5.0. We suspect that this is because the phase transition from high to low
privacy (recall Fig. 17), which the tight guarantee delays, gets shifted to very high Î±regions that are
not useful for conversion to (Îµ, Î´)-DP.
Our observations for randomized response mechanisms (see Fig. 23 are also consistent with earlier
results: For moderate subsampling rates r= 0.1, our guarantees are stronger, particularly for group
size8and large numbers of iterations. But, when decreasing the subsampling rate to 0.001, both
methods are almost identical. Nevertheless, group privacy amplification can demonstrably improve
upon a direct combination of independently derived group privacy and amplification guarantees.
Note that these observations are mostly of theoretic interest. In practice, one would use PLD
accounting, which tightly characterizes the composed mechanismâ€™s privacy leakage, instead of the
looser RDP accounting. As shown in Figs. 18 to 21, the tight mechanisms-specific analysis drastically
outperforms the post-hoc analysis for PLD accounting.
Gaussian mechanism
r= 0.001, Ïƒ= 1
100101102103104105
Iteration t100101ADP ÎµGroup size
8
4
2
1Post-hoc
Specificr= 0.001, Ïƒ= 5
100101102103104105
Iteration t10âˆ’1100ADP Îµ
Figure 22: Self-composed Gaussian mechanisms under Poisson subsampling with privacy parameter
Î´= 10âˆ’8, subsampling rate r= 0.001, and varying standard deviation Ïƒand group size. The tight
mechanism-specific analysis yields better privacy guarantees than the post-hoc baseline, except for
large Ïƒwhere the baseline is a good upper bound.
Randomized response mechanism
r= 0.1, Î¸= 0.6
100101102103104105
Iteration t100102104ADP ÎµGroup size
8
4
2
1Post-hoc
Specificr= 0.001, Ïƒ= 0.6
100101102103104105
Iteration t10âˆ’310âˆ’210âˆ’1100ADP Îµ
Figure 23: Self-composed randomized response mechanisms under Poisson subsampling with privacy
parameter Î´= 10âˆ’8, true response probability Î¸= 0.6, and varying subsampling rate rand group
size. The tight mechanism-specific analysis yields better privacy guarantees than the post-hoc
baseline, except for small rwhere the baseline is a good upper bound.
34B.2.5 Model utility
The previous results demonstrated that handling group privacy via a tight mechanism-specific analysis
allows for a larger number of compositions, i.e., iterations at a given privacy budget than post-hoc
use of the group privacy property. In the following, we demonstrate that this increased number of
iterations can in fact result in increased utility for group-private machine learning models.
We train a convolutional neural network (2 convolution layers with kernel sizes 3and32/64
channels, followed by two linear layers with hidden dimension 128) for image classification on
MNIST ( 55000 training, 5000 validation, 10000 test samples). We set the gradient clipping norm of
DP-SGD [ 6] toC= 10âˆ’4, the Gaussian noise standard deviation to 0.6Â·C, and the subsampling
rate to r= 64 /55000 . The optimizer is ADAM with learning rate 1eâˆ’3. If the privacy budget is
not used up earlier, training is terminated after 8epochs, with âŒˆ55000 /64âŒ‰iterations per epoch.
Even with a large privacy budget of Îµ= 8andÎ´= 1eâˆ’5, training with the post-hoc privacy analysis
needs to terminate after less than 200iterations. With the mechanism-specific analysis, Î´does not
even exceed 1eâˆ’7after8epochs, i.e., the model could potentially be trained for even more iterations.
The resultant validation accuracyâ€™s are 79.6%and91.2%, respectively. This showcases the superior
privacy-utility trade-off that can be achieved by analyzing subsampling and group privacy jointly.
0 2000 4000 6000
Iteration t10âˆ’1010âˆ’710âˆ’4ADP Î´(Îµ= 8)
Post-hoc
Specific
Budget
(a) Privacy0 2 4 6 8
Epoch0.00.20.40.60.81.0Val. accuracyPost-hoc
Specific
(b) Utility
Figure 24: Differentially private training of a 2-layer convolutional network on MNIST with PLD
accounting for group size 2. Our tight mechanism-specific analysis allows us to train for significantly
more epochs or to terminate training after 8epochs with less privacy leakage and higher accuracy.
35C Experimental setup
C.1 Evaluation of privacy guarantees
In all experiments, we assume that all mechanisms have underlying sensitivity 1w.r.t.â„“âˆ(randomized
response), â„“1(Laplace), or â„“2(Gaussian) output norms. We thus only specify noise parameters Ïƒor
Î», rather than the ratio of ÏƒorÎ»and the sensitivities.
C.1.1 Single-iteration approximate differential privacy
Privacy parameters. We evaluate all guarantees for 121equidistant values of Îµin[0,4]and121
logspace-equidistant values in 10âˆ’3,101, i.e.,{10x|xâˆˆ {âˆ’ 3,âˆ’3 +4
120, . . . , 1}}. We clip values
ofÎ´(Îµ)that are larger than 1to[0,1]. For our baselines, we enforce that Î´(Îµ)is monotonically
decreasing by taking a running minimum from larger to smaller Îµ(this may improve but never
worsens the baselines).
Mechanism-agnostic group privacy baseline. As our mechanism-agnostic group privacy baselines,
we use Proposition N.1. We take the maximum over the two cases (K+=K, K âˆ’= 0) and
(K+= 0, Kâˆ’=K), where Kis the group size. For Gaussian and Laplace mechanisms, we evaluate
group privacy profiles Î´k(Îµ)by determining Î´(Îµ)for the same mechanism with sensitivity k. For
randomized response, we let Î´k=Î´1. This is referred to to as â€œwhite-boxâ€ group privacy in [ 15]).
We evaluate this baseline analytically.
Post-hoc group privacy baseline. For our post-hoc baseline, we combine the tight Poisson sub-
sampling guarantee for insertion/removal from [ 15] with the group privacy property of approximate
differential privacy (see Appendix C.1.4 below). We evaluate this baseline analytically.
Tight mechanism-specific group privacy. In all ADP figures â€œspecificâ€ refers to our tight group
privacy guarantees for Gaussian (Theorem 3.8), Laplace (Theorem M.2), or randomized response
mechanisms (Theorem M.3). We take the maximum over all K+, Kâˆ’âˆˆN0withK++Kâˆ’=
K, where Kis the group size. To evaluate the bounds for Gaussian and Laplace mechanisms,
we pessimistically invert the privacy loss of dominating pairs P, Q via binary search (see details
in Appendix M.4), as implemented in the dp_accounting library [ 45]. We set the precision to
10âˆ’6, i.e., find the global optimum over multiples of 10âˆ’6. The bound for the randomized response
mechanism is evaluated analytically in O(1).
C.1.2 Single-iteration RÃ©nyi differential privacy
Privacy parameters . We evaluate all guarantees for Î±âˆˆ {2,3, . . . , 1000}, as well as 121logspace-
equidistant values in [0,104]rounded to the next smallest integer (without 0or1), and 121logspace-
equidistance values in (1,10]. For our baselines, we enforce that Ï(Î±)is monotonically increasing
by taking a running minimum from smaller to larger Î±(this may improve but never worsens the
baselines).
Mechanism-agnostic group privacy baseline. As our mechanism-agnostic group privacy baselines,
we use Proposition N.1. We take the maximum over the two cases (K+=K, K âˆ’= 0) and
(K+= 0, Kâˆ’=K), where Kis the group size. For integer Î±, we use the binomial expansion
in Eq. (33) (without factor 2). For Gaussian and Laplace mechanisms, we evaluate group privacy
profiles Î¶k(Î±)by determining Î¶(Î±)for the same mechanism with sensitivity k. For randomized
response, we let Î¶k=Î¶1. For continuous Î±, we apply tanh -sinh-quadrature with 50digits of decimal
precision to Eq. (33)
Post-hoc group privacy baseline. For our post-hoc baselines, we combine either tight Poisson
subsampling guarantee for insertion/removal from [ 30] or the subsampling without replacement
guarantee for insertion/removal from [ 28] with the group privacy property of approximate differential
privacy (see Appendix C.1.4 below). For Poisson subsampling and integer Î±, we use the binomial
expansion from [ 30] (without factor 2). For continuous Î±, we apply tanh -sinh-quadrature with
50digits of decimal precision. For subsampling without replacement, we use the improved self-
consistency bound (Theorem 27 from [ 28]), which we evaluate via tanh -sinh-quadrature with 50
digits of decimal precision due to the intractability of the nested binomial expansions for larger Î±.
Tight mechanism-specific group privacy. In all RDP figures â€œspecificâ€ refers to our tight group
privacy guarantees for Gaussian (Theorem 3.8), Laplace (Theorem M.2), or randomized response
36mechanisms (Theorem M.3). We take the maximum over all K+, Kâˆ’âˆˆN0withK++Kâˆ’=K,
where Kis the group size. To evaluate the bounds for Gaussian and Laplace mechanisms, we use
tanh -sinh-quadrature with 50digits of decimal precision (see discussion in Appendix M.4). The
bound for the randomized response mechanism is evaluated analytically in O(1).
C.1.3 Privacy accounting
RDP accounting. For RDP accounting, we simply evaluate the single-iteration guarantees as
described in Appendix C.1.2. We then multiply the Ï(Î±)with the number of iterations, apply the
improved RDP-to-ADP formula from [ 31], and take the minimum over all obtained values. For the
post-hoc baseline, we apply the group privacy property before composition (which is equivalent to
applying it after composition, due to additivity of composition).
PLD accounting. For PLD accounting with dominating pairs, we use the implementation of
thedp_accounting library [ 45]. To quantize the privacy loss distribution, we use â€œconnect the
dotsâ€ [ 20] with pessimistic estimates, a discretization interval size of 10âˆ’3, and truncation of eâˆ’50
of the probability mass. During composition, we truncate 10âˆ’15of the tail mass. For the post-hoc
baseline, we apply the group privacy property after composition.
C.1.4 Post-hoc group privacy
ADP. For RDP, we use the following result from the proof of Lemma 2.2 in [ 23], which provides
tigher guarantees than Lemma 2.2 itself: Let Mbe(Îµ, Î´)-DP under neighboring relation â‰ƒX. Then,
Mis(Îµâ€², Î´â€²)-DP with Îµâ€²=ÎµÂ·KandÎ´â€²=PKâˆ’1
k=0ekÂ·ÎµÂ·Î´.
RDP. For RDP, we use the following result from Corollary 4 of [ 7], which provides tighter guarantees
than the upper bound in their Proposition 2. Let DÎ±belog(Î› Î±)/(Î±âˆ’1). This DÎ±fulfills the
following triangle inequality:
DÎ±(p||q)â‰¤Î±âˆ’1
2
Î±âˆ’1D2Î±(p||r) +Î±
Î±âˆ’1D2Î±âˆ’1(r||q).
We recursively apply this bound log2(K)times when evaluating the group privacy of our baselines
for groups of size K.
C.2 Computational resources
We conduct all experiments on a set of Xeon E5-2630 v4 CPUs @ 2.2 GHz .
We use one worker and job per subsampling theorem, base mechanism noise level, subsampling rate,
and combination of group privacy parameters Kâˆ’, K+. Per job, we allocate 2CPU cores, 4 GB and
10minutes of runtime (in practice, most jobs were completed in a few seconds). In total, we ran
13060 such jobs for ADP, 9731 for RDP, and 984for RDP accounting. We estimate that over the
course of the full research project twice as many jobs were executed.
C.3 Assets and licenses
To perform high-precision quadrature for RDP guarantees, we use the tanh -sinh quadrature im-
plementation from the mpmath library (version 1.3.0.), which is available under the BSD- 3-Clause
license at https://github.com/mpmath/mpmath .
For PLD accounting and evaluation of ADP guarantees via bisection, we use and extend
thedp_accounting library [ 45] (commit 0b109e959470c43e9f177d5411603b70a56cdc7a ),
which is available under Apache- 2.0license at https://github.com/google/
differential-privacy
For conversion from RDP to ADP guarantees, we use the get_privacy_spent method implemented
in the Opacus library [ 73] (version 1.4.1), which is available under Apache- 2.0license at https:
//github.com/pytorch/opacus .
An implementation will be made available at https://cs.cit.tum.de/daml/group-amplification.
37D General setting and definitions
In the following, we generalize some of the definitions introduced in Sections 2 and 3 to their
measure-theoretic equivalents. The purpose of this generalization is to handle both continuous and
discrete spaces, base mechanisms, and subsampling schemes, without having to make constant case
distinctions.
We use these more general definitions throughout the remaining appendix sections. The theoretic
results presented in Section 3 follow as special cases.
D.1 Spaces
Unlike before, we assume that dataset space Xis some arbitrary space, whose elements do not have
to be sets. Instead, datasets xâˆˆXcan also be graphs, sequences or any other data collection. We
further assume that the batch space is a measurable space (Y,Y)withÏƒ-algebra Y. For example, Y
can be composed of subsets, subgraphs, or subsequences. We also assume the output space to be a
measurable space (Z,Z). Finally, we assume the existence of some measure Î»on the output space,
such as the Lebesgue measure for continuous Zor the counting measure #for discrete Z.
Whenever we consider Poisson subsampling, we assume that XâŠ† P(A),Y={yâŠ†x|xâˆˆX},
Y=P(Y), where Ais some discrete, finite set and P(Â·)is the powerset.
Whenever we consider subsampling without replacement with batch size q, we assume that XâŠ†
{xâˆˆ P(A)| |x|> q},Y={yâŠ†x|xâˆˆX,|y|=q},Y=P(Y), where Ais some discrete, finite
set and P(Â·)is the powerset.
D.2 Neighboring relations
For set-valued datasets and batches, the insertion/removal relation and the substitution relation are
formally defined as follows:
Definition D.1. Setsx, xâ€²âˆˆXare related by the insertion/removal relation ( xâ‰ƒÂ±xâ€²)ifxÌ¸=xâ€²and
there is some asuch that xâ€²=xâˆª {a}orxâ€²=x\ {a}.
Definition D.2. Setsx, xâ€²âˆˆXare related by the substitution relation ( xâ‰ƒâˆ†xâ€²)if there is some
aâˆˆxandaâ€²/âˆˆxsuch that xâ€²=x\ {a} âˆª {aâ€²}.
In our general setting, we also allow non-symmetric neighboring relations.
D.3 Mechanisms and subsampling schemes
As before, the term â€œmechanismâ€ refers to random functions that map to the output space (Z,Z).
Formally, a random function M:Xâ†’Zis a family of random variables indexed by elements of X.
Definition D.3. A random function M:Xâ†’Zis a function M:XÃ—â„¦â†’Z, where (â„¦,F, P)is
some probability space and all M(x,Â·) :Ï‰7â†’M(x, Ï‰)are measurable.
We write PMx:Z â†’ [0,1]for the distribution of random variable M(x). We further assume that each
PMxis absolutely continuous w.r.t. the aforementioned output measure Î», i.e.,âˆ€xâˆˆX:PMxâ‰ªÎ»,
and write mx:Zâ†’R+for the corresponding Radonâ€“Nikodym derivative dPMx/dÎ». For example,
when the output space (Z,Z)is continuous and Î»is the Lebesgue measure, then mxis the density.
Similarly, we define our base mechanism to be a random function B:Yâ†’Zand write PBy:Z â†’
[0,1]for the distribution of base mechanism outputs given a batch yâˆˆY. We assume that each PBy
is absolutely continuous w.r.t. output measure Î»and write by:Zâ†’R+fordPBy/dÎ».
Finally, we define our subsampling scheme to be a random function S:Xâ†’Zand write PSx:Y â†’
[0,1]for the distribution of batches given a dataset xâˆˆX. We generally do not require PSxto be
absolutely continuous w.r.t. some other measure. When PSxis absolutely continuous w.r.t. counting
measure #, we write sx:Yâ†’[0,1]for the corresponding probability mass function dPSx/d#.
In particular, Poisson subsampling and subsampling without replacement are defined as follows:
Definition D.4. Poisson subsampling with rate râˆˆ[0,1]has probability mass function sx(y) =
r|y|(1âˆ’r)|x|âˆ’|y|for batches yâŠ†x.
38Definition D.5. Subsampling without replacement with batch size qhas probability mass function
sx(y) = |x|
qâˆ’1for batches yâŠ†xwith|y|=q.
As before, our goal is to provide privacy guarantees for subsampled mechanisms M=Bâ—¦S. Similar
to our discussion in Section 2, its distribution PMxgiven a dataset xâˆˆXis a mixture3with
mx(z) =Z
Yby(z) dPSx(y). (5)
There is one component per batch yfrom batch space Y, and the weights depend on subsampling
distribution PSx.
D.4 Differential privacy notions
Since we no longer require the output space to be continuous, we also need to generalize the definitions
of approximate differential privacy, RÃ©nyi differential privacy, dominating pairs, and the divergences
underlying their definition. For this, recall that Î»is our assumed measure on output space (Z,Z).
Definition D.6. ForÎµâ‰¥0, a mechanism M:Xâ†’Zis(Îµ, Î´)-DP under relation â‰ƒXifâˆ€xâ‰ƒXxâ€²:
Hexp(Îµ)(mx||mxâ€²)â‰¤Î´andHexp(Îµ)(mxâ€²||mx)â‰¤Î´with hockey stick divergence
HÎ±(mx||mxâ€²) =Z
Zmax{mx(z)/ mxâ€²(z)âˆ’Î±,0} Â·mxâ€²(z) dÎ»(z). (6)
Definition D.7. A pair of distributions (P, Q)withp= dP /dÎ»andq= dQ /dÎ»is a dominating
pair for mechanism Munder neighboring relation â‰ƒX, ifHÎ±(mx||mxâ€²)â‰¤HÎ±(p||q)for all xâ‰ƒXxâ€²
and all Î±â‰¥0.
Definition D.8. ForÎ±â‰¥1, a mechanism M:Xâ†’RDis(Î±, Ï)-RDP under neighboring relation
â‰ƒXifâˆ€xâ‰ƒXxâ€²: log(Î› Î±(mx||mxâ€²))/(Î±âˆ’1)â‰¤Ïandlog(Î› Î±(mxâ€²||mx))/(Î±âˆ’1)â‰¤Ïwith
Î›Î±(mx||mxâ€²) =Z
RDmx(z)Î±Â·mxâ€²(z)1âˆ’Î±dz. (7)
Importantly, note that Î›Î±is not the RÃ©nyi divergence as used in [ 7]. It is its Î±th moment, i.e., a scaled
and exponentiated RÃ©nyi divergence. This is why a logarithm and quotient appears in Definition D.8.
We use this definition, so that we can simultaneously discuss ADP and RDP without notational clutter.
D.5 Joint convexity
The joint convexity of Î¨Î±âˆˆ {HÎ±,Î›Î±)is not limited to densitites, but also applies to other non-
negative Radonâ€“Nikodym derivatives of probability distributions [37, 38]:
Lemma D.9. Consider arbitrary Radonâ€“Nikodym derivatives f(1)
1, f(1)
2, f(2)
1, f(2)
2:Zâ†’R+and
weight wâˆˆ[0,1]. Then,
Î¨Î±(wf(1)
1+ (1âˆ’w)f(1)
2||wf(2)
1+ (1âˆ’w)f(2)
2)â‰¤wÎ¨Î±(f(1)
1||f(2)
1) + (1 âˆ’w)Î¨Î±(f(1)
2||f(2)
2).
D.6 Couplings
As with the discrete, finite-support subsampling distributions we considered in Section 3, the key tool
we use for analyzing amplification are couplings. However, we no longer assume the subsampling
scheme to always have a mass function. We thus use a more general notion of couplings between
distributions instead of couplings between mass functions:
Definition D.10. A coupling between probability measures P1, . . . , P Non space (Y,Y)is a proba-
bility measure Î“on product space (Y,Y)N, where the nth marginal is Pn, i.e., Î“â—¦Ï€âˆ’1
n=Pnwith
projection Ï€n(y) =yn.
Here,â—¦is the composition operator and Ï€âˆ’1
nis the preimage (not necessarily the inverse) of the
projection function. As before, when considering a coupling between two distributions P1,P2, the
value Î“(T, R)specifies for all events T, Râˆˆ Y how much probability should be transported from
P1(T)toP2(R)to transform P1intoP2.
3assuming that (y, Z)7â†’PBy(Z)is a valid Markov kernel
39E Proof of optimal transport bounds
E.1 Proof of Theorem 3.3
In the following, we show a more general statement for the general setting introduced in Appendix D.
Theorem 3.3 immediately follows in the special case where batch space Yis finite and discrete, and
the subsampling distribution has a probability mass function sx.
Theorem E.1. Consider a subsampled mechanism M=Bâ—¦S, and an arbitrary coupling Î“between
subsampling distributions PSxandPSxâ€². Then
Î¨Î±(mx||mxâ€²)â‰¤Z
Y2cÎ±(y(1), y(2)) dÎ“(( y(1), y(2))) (8)
with cost function cÎ±(y(1), y(2)) = Î¨ Î±(by(1)||by(2)).
Proof. Recall that mxandmxâ€²are mixtures with mx(z) =R
by(z)dPSx(y)andmxâ€²(z) =R
by(z)dPSxâ€²(y). Since Î“is a coupling between PSxandPSxâ€², we can use the projection
Ï€n(y) =ynand change of variables to rewrite these mixtures as
mx(z) =Z
Yby(z)d 
Î“â—¦Ï€âˆ’1
1
(y) =Z
Y2bÏ€1(y)(z)dÎ“(y) =Z
Y2by1(z)dÎ“(y),
mxâ€²(z) =Z
Yby(z)d 
Î“â—¦Ï€âˆ’1
2
(y) =Z
Y2bÏ€2(y)(z)dÎ“(y) =Z
Y2by2(z)dÎ“(y).
Since mx(z)andmxâ€²(z)are now expectations w.r.t. the same measure, we can use the joint convexity
ofÎ¨Î±(Lemma D.9) to show Î¨Î±(mx, mxâ€²)â‰¤R
Y2Î¨Î±(by1||by2) dÎ“(y).
E.2 Proof of Theorem 3.4
As before, we prove a more general statement from which Theorem 3.4 immediately follows. For
this, recall that Yis the Ïƒ-algebra of batch space (Y,Y), and that P(T|R) =P(Tâˆ©R)/ P(R).
Theorem E.2. Consider a subsampled mechanism M=Bâ—¦S. Further consider two dis-
joint partitioningsSI
i=1Ai=YandSJ
j=1Ej=Ysuch that all Ai, Ejare in Yand have
non-zero measure under SxandSxâ€², respectively. Let Î“be an arbitrary coupling between
PSx(Â· |A1), . . . , P Sx(Â· |AI), PSxâ€²(Â· |E1), . . . , P Sxâ€²(Â· |EJ). Then,
Î¨Î±(mx||mxâ€²)â‰¤Z
YI+JcÎ±(y(1),y(2)) dÎ“(( y(1),y(2))),
with cost function c:YIÃ—YJâ†’R+defined by
cÎ±(y(1),y(2)) = Î¨ Î±ï£«
ï£­IX
i=1by(1)
iÂ·PSx(Ai)||JX
j=1by(2)
jÂ·PSxâ€²(Ej)ï£¶
ï£¸. (9)
Proof. Using the law of total expectation, linearity of integration, and change of variables with
projection Ï€n(y) =ynshows that
mx(z) =IX
i=1Z
Yby(z)dPSx(y|Ai)
PSx(Ai)
=Z
YIX
i=1by(z)Â·PSx(Ai)dPSx(y|Ai)
=Z
YIX
i=1by(z)Â·PSx(Ai)d(Î“â—¦Ï€âˆ’1
i)(y)
=Z
YI+J IX
i=1byi(z)Â·PSx(Ai)!
dÎ“(y)
40and
mxâ€²(z) =Z
YI+Jï£«
ï£­JX
j=1by(j+I)(z)Â·PSx(Ej)ï£¶
ï£¸dÎ“(y).
Since mx(z)andmxâ€²(z)are now expectations w.r.t. the same measure, we can use the joint convexity
ofÎ¨Î±(Lemma D.9) to show
Î¨Î±(mx||mxâ€²)â‰¤Z
YI+JÎ¨Î±ï£«
ï£­IX
i=1byi(z)Â·PSx(Ai)||JX
j=1by(j+I)(z)Â·PSx(Ej)ï£¶
ï£¸dÎ“(y).
A change of indexing via y(1)
i=yiandy(2)
j=y(j+I)concludes our proof.
E.3 Proof of Proposition 3.5
Proposition 3.5. Consider y(1)âˆˆYI,y(2)âˆˆYJ, and cost function cdefined in Eq. (2). LetdYbe
the distance induced by â‰ƒY(see Definition 2.4). Then, cÎ±(y(1),y(2))â‰¤Ë†cÎ±(y(1),y(2)), with
Ë†cÎ±(y(1),y(2)) = max
Ë†y(1),Ë†y(2)cÎ±(Ë†y(1),Ë†y(2)) (3)
subject to âˆ€k, lâˆˆ {1,2},âˆ€t, u:dY(Ë†y(k)
t,Ë†y(l)
u)â‰¤dY(y(k)
t, y(l)
u)andË†y(1)âˆˆYI,Ë†y(2)âˆˆYJ.
Proof. The original tuples of batches y(1)âˆˆYIandy(2)âˆˆYJconstitute a feasible solution to the
maximization problem, since they fulfill the constraints with equality, i.e., âˆ€k, l, t, u :dY(y(k)
t, y(l)
u) =
dY(y(k)
t, y(l)
u). The value of any feasible solution to a maximization problem is l.e.q. its optimal
value.
41F Distance-compatible couplings of multiple distributions
d = 2 d = 1
d = 1ğ‘¦1
ğ‘¦2
ğ‘¦3supp(ğ‘ 1)
supp(ğ‘ 2)
supp(ğ‘ 3)
(a) Distance- compatible coupling
d = 2 d > 1
d > 1ğ‘¦1
ğ‘¦2
ğ‘¦3supp(ğ‘ 1)
supp(ğ‘ 2)
supp(ğ‘ 3) (b) Distance- incompatible coupling
Figure 25: Example of a distance-compatible and a distance-incompatible coupling
In the following, we generalize the notion of distance-compatible couplings proposed in [ 15] from two
distributions to an arbitrary number of distributions. This provides a sufficient optimality condition
for Theorem E.2.
Note that we use a more general, measure-theoretic definition of distance-compatibility (see Defini-
tion F.4). Definition 3.6 is a special case for subsampling schemes with finite, discrete support.
As discussed in Section 3, a dY-compatible coupling between (conditional) subsampling distributions
only assigns probability to tuples of batches ywhen all pairs yi, yjhave the smallest possible distance
toy1and to each other, while still being in the support of their respective distributions.
To avoid having to make additional assumptions about the topology of batch space (Y,Y), we will
assume that all subsampling schemes have densities and reason about the support of these densities.
Definition F.1. The support of a function p:Yâ†’R+issupp( p) ={yâˆˆY|p(y)>0}.
Based on this notion of support, we can define a notion of distance between a batch yâˆˆYand the
support of a density:
Definition F.2. Consider a distance dYinduced by a neighboring relation â‰ƒY. The distance between
an element yâˆˆYand the support of a function p:Yâ†’R+is defined as dY(y,supp( p)) =
minyâ€²âˆˆsupp( p)dY(y, yâ€²).
Furthermore, we can define a notion of distance between the support of two different densities:
Definition F.3. Consider a distance dYinduced by a neighboring relation â‰ƒY. The distance between
the support of functions p1, p2:Yâ†’R+is defined as
dY(supp( p1),supp( p2)) = min
y1,y2dY(y1, y2)s.t.âˆ€iâˆˆ {1,2}:yiâˆˆsupp( pi).
Based on these definitions, we can now formally define distance-compatible couplings between
multiple distributions.
Definition F.4. Consider a coupling Î“between probability measures P1, . . . , P Non measurable space
(Y,Y)with symmetric neighboring relation â‰ƒYand induced distance dY. Assume that âˆ€n:Pnâ‰ªÎ½n
for some measures Î½1, . . . , Î½ Nand define pn= dPn/dÎ½n. Further assume that Î“â‰ªQN
n=1Î½nwith
product measureQN
n=1Î½n, and define Î³=dÎ“/dQÎ½n. Then, Î“is adY-compatible coupling if
(yâˆˆsupp(Î“) = â‡’ âˆ€u >1 :dY(y1, yu) =dY(y1,supp( su)))
âˆ§(yâˆˆsupp(Î“) = â‡’ âˆ€u > t > 1 :dY(yu, yt) =dY(supp( st),supp( su))).
That is, Î“only assigns probability to a tuple of batches yif allytandyuare as close as possible to y1
and as close as possible to each other, while still being in the support of their corresponding densities.
Note that our choice of focusing on y1is arbitrary, and dY-compatibility could also be defined for
any other reference index nâˆˆ {1, . . . , N }.
We shall now prove that dY-compatibility is a sufficient optimality condition for our optimal transport
problem. For this proof, we will use the following lemma, which immediately follows from Defini-
tions F.2 and F.3:
42Lemma F.5. Consider a distance dYinduced by a relation â‰ƒYand two functions p1, p2:Yâ†’R+.
Then, for all y1âˆˆsupp( p1), y2âˆˆsupp( p2),
dY(y1, y2)â‰¥dY(y1,supp( p2))â‰¥dY(supp( p1),supp( p2)).
Theorem F.6. Consider a subsampled mechanism M=Bâ—¦S. Further consider two finite partitions
A1, . . . , A Iâˆˆ Y andE1, . . . , E Jâˆˆ Y ofYsuch that all AiandEjhave non-zero measure under Sx
andSxâ€², respectively. Let dYbe the distance induced by a symmetric neighboring relation â‰ƒY. LetÎ“âˆ—
be adY-compatible coupling between PSx(Â· |A1), . . . , P Sx(Â· |AI), PSxâ€²(Â· |E1), . . . , P Sxâ€²(Â· |EJ),
which have Radonâ€“Nikodym derivatives s1, . . . , s I+J. Then, for all Î± >1,
Î“âˆ—âˆˆarg min
Î“âˆˆGâ‰¤Z
YI+JË†cÎ±(y(1),y(2)) dÎ“(( y(1),y(2))). (10)
where Gis the set of valid couplings between the I+Jmeasures, and Ë†cÎ±:YIÃ—YJâ†’R+is the
cost function upper bound defined in Proposition 3.5.
Proof. Consider an arbitrary, not necessarily dY-compatible coupling Î“. By definition of Ë†cand
symmetry of â‰ƒ, we haveZ
YI+JË†cÎ±(y(1),y(2)) dÎ“(( y(1),y(2)))
=Z
YI+J
max
Ë†yâˆˆYI+JcÎ±(Ë†y:I,Ë†yI:)s.t.âˆ€t < u :dY(Ë†yt,Ë†yu)â‰¤dY(yt, yu)
dÎ“(y),
with original cost function cÎ±:YIÃ—YJâ†’R+defined in Theorem E.2.
We can now use Lemma F.5 to tighten the constraints of the optimization problem inside the integrand
and thus lower-bound its optimal value for all yâˆˆsupp(Î“) :
max
Ë†yâˆˆYI+JcÎ±(Ë†y:I,Ë†yI:)s.t.âˆ€t < u :dY(Ë†yt,Ë†yu)â‰¤dY(yt, yu)
â‰¥max
Ë†yâˆˆYI+JcÎ±(Ë†y:I,Ë†yI:)
s.t.âˆ€u >1 :dY(Ë†y1,Ë†yu)â‰¤dY(y1,supp( su)),
âˆ€u > t > 1 :dY(Ë†yt,Ë†yu)â‰¤dY(supp( st),supp( su)).
We notice that the lower bound only depends on y1and shall thus refer to it as ÎºÎ±(y1). Further
note that any y/âˆˆsupp(Î“) does not contribute to the integral. Since Î“is a valid coupling, we can
marginalize out all variables except y1via projection Ï€1(y) =y1to showZ
YI+JË†cÎ±(y(1),y(2)) dÎ“(( y(1),y(2)))
â‰¥Z
YI+JÎºÎ±(Ï€1(y)) dÎ“( y) =Z
YÎºÎ±(y1) d(Î“â—¦Ï€âˆ’1
1)(y1) =Z
YÎºÎ±(y1) dPSx(y1|A1).
By construction of ÎºÎ±, this holds with equality whenever Î“âˆ—is adY-compatible coupling.
Finally, the exact derivations we used for Theorem F.6 can also be used to show that the optimal value
of our transport problem has a simple, canonical form whenever a dY-compatible coupling exists:
Corollary F.7. Consider a subsampled mechanism M=Bâ—¦S. Further consider two finite partitions
A1, . . . , A Iâˆˆ Y andE1, . . . , E Jâˆˆ Y ofYsuch that all AiandEjhave non-zero measure under Sx
andSxâ€², respectively. Let dYbe the distance induced by a symmetric neighboring relation â‰ƒY. Assume
that a dY-compatible coupling between PSx(Â· |A1), . . . , P Sx(Â· |AI), PSxâ€²(Â· |E1), . . . , P Sxâ€²(Â· |EJ)
with Radonâ€“Nikodym derivatives s1, . . . , s I+Jexists. Then, for all Î± >1,
min
Î“âˆˆGZ
YI+JË†cÎ±(y(1),y(2)) dÎ“(( y(1),y(2))) =Z
YÎºÎ±(y1) dPSx(y1|A1), (11)
where Gis the space of valid couplings between the I+Jmeasures,
ÎºÎ±(y1) = max
Ë†yâˆˆYI+JcÎ±(Ë†y:I,Ë†yI:)s.t.âˆ€u >1 :dY(Ë†y1,Ë†yu)â‰¤dY(y1,supp( su)),
âˆ€u > t > 1 :dY(Ë†yt,Ë†yu)â‰¤dY(supp( st),supp( su)),
andcÎ±:YIÃ—YJâ†’R+is the original cost function defined in Theorem E.2.
43Thus, we can focus on constructing distance-compatible couplings when trying to derive existing or
novel amplification by subsampling guarantees. Note that these result also generalize to asymmetric
neighboring relations â‰ƒY. We just wanted to avoid further complicating the indexing. Future work
may want to generalize these results to a more general, topological notion of support that does not
rely on the existence of subsampling densities.
44G Recovering known mechanism-agnostic ADP guarantees
In the following, we first provide an overview of the framework for deriving mechanism-agnostic
ADP guarantees proposed by Balle et al. [15]. We then show that the same guarantees can be derived
via optimal transport between multiple subsampling distributions. Finally, we use observations made
during the proof to explain why the mechanism-agnostic guarantees derived via this approach can be
suboptimal.
G.1 Overview of Balle et al. framework
The framework from [ 15] uses four steps to derive tight mechanism-agnostic guarantees for subsam-
pled mechanisms: (1) Partitioning the subsampling distributions via maximal couplings, (2) applying
advanced joint convexity, (3) applying joint convexity, and (4) defining two couplings involving two
distributions.
Maximal couplings are a construction that makes it possible to partition the subsampling probability
mass functions as
sx(y) = (1 âˆ’w)px(y) +wqx(y)
sxâ€²(y) = (1 âˆ’w)px(y) +wqxâ€²(y),
with probability mass functions px, qx, qxâ€²:Yâ†’[0,1]chosen such that qxandqxâ€²have disjoint
support and wâˆˆ[0,1]is as small as possible.
Note that, when considering typical subsampling schemes (Poisson, without replacement, with
replacement) and single-element neighboring relations (insertion/removal, substitution), this partition
is simply equivalent to
sx(y) = Pr[ SxâˆˆA1]Â·sx(y|A1) + Pr[ SxâˆˆA2]Â·sx(y|A2)
sxâ€²(y) = Pr[ Sxâ€²âˆˆE1]Â·sxâ€²(y|E1) + Pr[ SxâˆˆE2]Â·sxâ€²(y|E2),
where A1andE1are the event that the inserted/removed or substituted element is not sampled, and
A2,E2are their complements (see Appendix B in [15]).
Using this construction, the densitities mxandmxâ€²of subsampled mechanisms M=Bâ—¦Scan be
rewritten as
mx(z) = (1 âˆ’w)X
yâˆˆYby(z)px(y) +wX
yâˆˆYby(z)qx(y) (12)
mxâ€²(z) = (1 âˆ’w)X
yâˆˆYby(z)px(y) +wX
yâˆˆYby(z)qxâ€²(y) (13)
Next, one can rewrite the divergence HÎ±(mx||mxâ€²)via the following property:
Proposition G.1 (Advanced joint convexity [ 15]).Letmx, mâ€²
x:Zâ†’[0,1]be probability mass
functions satisfying mx(z) = (1 âˆ’w)f(z) +wg(z)andmxâ€²(z) = (1 âˆ’w)f(z) +wgâ€²(z)for some
wâˆˆ[0,1],f, g, gâ€²:Zâ†’[0,1]. Given Î±â‰¥1, letÎ±â€²= 1 + w(Î±âˆ’1)andÎ²=Î±â€²/ Î±. Then the
following holds:
DÎ±â€²(mx||mxâ€²) =wDÎ±(g||(1âˆ’Î²)f+Î²gâ€²).
Applying advanced joint convexity, followed by joint convexity to Eqs. (12) and (13) shows that
HÎ±â€²(mx||mxâ€²)â‰¤(1âˆ’Î²)HÎ±ï£«
ï£­X
yâˆˆYbyqx(y)||X
yâˆˆYbypx(y)ï£¶
ï£¸
+Î²HÎ±ï£«
ï£­X
yâˆˆYbyqx(y)||X
yâˆˆYbyqxâ€²(y)ï£¶
ï£¸
Finally, one can construct a coupling Î³:Y2â†’[0,1]between qxandpx, as well as a coupling
Î³â€²:Y2â†’[0,1]between qxandqxâ€². One can then invoke a special case of Theorem 3.3 with
Î¨Î±=HÎ±and the cost function upper bound from Proposition 3.5 to show
HÎ±â€²(mx||mxâ€²)â‰¤(1âˆ’Î²)X
yâˆˆY2Ë†cÎ±(y(1), y(2))Î³(y(1), y(2)) +Î²X
yâˆˆY2Ë†cÎ±(y(1), y(2))Î³â€²(y(1), y(2)).
45Since we are only considering pairs of batches, we have
Ë†cÎ±(y(1), y(2)) = max
Ë†yHÎ±(bË†y(1)||bË†y(1))s.t.dY(y(1), y(2))â‰¤dY(Ë†y(1),Ë†y(2))
with induced distance dYfrom Definition 2.4. This specific cost function bound is referred to as
â€œgroup privacy profileâ€ in [15].
G.2 Subsumption of Balle et al. framework
Next, we show that we can always obtain the same guarantee by defining a (potentially suboptimal)
coupling between four subsampling distributions, and then pessimistically upper-bounding the
guarantee we would obtain through Theorem 3.4:
Theorem G.2. Consider a subsampled mechanism M=Bâ—¦Sand some x, xâ€²âˆˆX. Assume
that subsampling pmfs sx, sxâ€²:Yâ†’[0,1]satisfy sx(z) = (1 âˆ’w)px(z) +wqx(z)andsxâ€²(z) =
(1âˆ’w)px(z) +wqxâ€²(z)for some wâˆˆ[0,1]andpx, qx, qxâ€²:Yâ†’[0,1]. LetÎ³:Y2â†’[0,1]be an
arbitrary coupling of qx, px, and Î³â€²:Y2â†’[0,1]be an arbitrary coupling of qx, pxâ€². Then, there is
a coupling ËœÎ³:Y4â†’[0,1]ofpx, qx, px, qxâ€²such that for all Î±â‰¥1
HÎ±â€²(mx||mxâ€²)â‰¤X
yâˆˆY2+2Ë†cÎ±â€²(y(1),y(2))ËœÎ³(y(1),y(1))
â‰¤(1âˆ’Î²)X
yâˆˆY1+1Ë†cÎ±(y(1), y(2))Î³(y(1), y(2)) +Î²X
yâˆˆY1+1Ë†cÎ±(y(1), y(2))Î³â€²(y(1), y(2)),
with cost function upper bound Ë†cÎ±defined in Proposition 3.5, Î±â€²= 1 + w(Î±âˆ’1), and Î²=Î±â€²/ Î±.
Proof. The main idea is to invoke Theorem 3.4 with a specifically crafted coupling, and then apply
advanced joint convexity and joint convexity.
Specifically, we define a coupling ËœÎ³:Yâ†’[0,1]that corresponds to the following generative process:
We first sample y(1)
2andy(2)
2from the coupling Î³â€²(recall that a coupling is a joint mass function).
This gives us two elements from the support of qxandqxâ€², respectively. Then, we sample y(1)
1from
Î³conditioned on y(1)
2. This gives us an element from the support of px. Finally, we let y(2)
1â†y(1)
1.
Formally, this coupling can be defined as
ËœÎ³(y(1),y(2)) =Î³â€²(y(1)
2, y(2)
2)Â·Î³(y(1)
2, y(1)
1)
qx(y(1)
2)Â· 1h
y(2)
1=y(1)
1i
.
Validity of coupling. Before proceeding, we need to verify that this is a valid coupling, i.e., its four
marginals are px, qx, px, qxâ€². For any y(1)
1âˆˆY, we have
X
y(1)
2,y(2)
1,y(2)
2âˆˆY3ËœÎ³(y(1)
1, y(1)
2, y(2)
1, y(2)
2)
=X
y(1)
2,y(2)
2âˆˆY2Î³â€²(y(1)
2, y(2)
2)Â·Î³(y(1)
2, y(1)
1)
qx(y(1)
2)
=X
y(1)
2âˆˆYqx(y(1)
2)Â·Î³(y(1)
2, y(1)
1)
qx(y(1)
2)
=X
y(1)
2âˆˆYÎ³(y(1)
2, y(1)
1)
=px(y(1)
1).
where the first inequality is due to the indicator function, the second equality follows from marginal-
izing Î³â€², and the last equality follows from marginalizing Î³.
The proof for any y(2)
1âˆˆYis analogous.
46For any y(1)
2âˆˆY, we similarly have
X
y(1)
1,y(2)
1,y(2)
2âˆˆY3ËœÎ³(y(1)
1, y(1)
2, y(2)
1, y(2)
2) (14)
=X
y(1)
1,y(2)
2âˆˆY2Î³â€²(y(1)
2, y(2)
2)Â·Î³(y(1)
2, y(1)
1)
qx(y(1)
2)(15)
=X
y(1)
1âˆˆY1Î³(y(1)
2, y(1)
1) (16)
=qx(y(1)
2), (17)
and for any y(2)
2we have
X
y(1)
1,y(1)
2,y(2)
1âˆˆY3ËœÎ³(y(1)
1, y(1)
2, y(2)
1, y(2)
2) (18)
=X
y(1)
1,y(1)
2âˆˆY2Î³â€²(y(1)
2, y(2)
2)Â·Î³(y(1)
2, y(1)
1)
qx(y(1)
2)(19)
=X
y(1)
2âˆˆYÎ³â€²(y(1)
2, y(2)
2)Â·qx(y(1)
2)
qx(y(1)
2)(20)
=qxâ€²(y(2)
2). (21)
First inequality. Now that we have a valid coupling, we can use the same joint-convexity argument
as in our proof of Theorem 3.4, combined with our cost function bound Ë†cÎ±to show
HÎ±â€²(mx||mxâ€²)â‰¤X
Y2+2max
Ë†yHÎ±â€²
(1âˆ’w)bË†y(1)
1+wbË†y(1)
2]||1âˆ’w)bË†y(2)
1+wbË†y(2)
2
Â·Î³(y(1),y(2)),
with each of the |Y2+2|optimization problems being constrained by âˆ€k, l, t, u :dY(Ë†y(k)
t,Ë†y(l)
u)â‰¤
dY(y(k)
t, y(l)
u)andË†y(1)âˆˆY2,Ë†y(2)âˆˆY2. This corresponds to the first equality in our Theorem.
Second inequality. Due to construction of our coupling, we always have dY(y(1)
1, y(2)
1) = 0 , i.e.,
y(1)
1=y(2)
1. We can thus use advanced joint convexity, joint convexity, and linearity of summation to
obtain a looser bound via
HÎ±â€²(mx||mxâ€²)â‰¤(1âˆ’Î²)X
Y2+2max
Ë†yHÎ±
bË†y(1)
2||bË†y(1)
1
Â·Î³(y(1),y(2))
+Î²X
Y2+2max
Ë†yHÎ±
bË†y(1)
2||bË†y(2)
2
Â·Î³(y(1),y(2)),
with each of the 2Â· |Y2+2|optimization problems being constrained by âˆ€k, l, t, u :dY(Ë†y(k)
t,Ë†y(l)
u)â‰¤
dY(y(k)
t, y(l)
u)andË†y(1)âˆˆY2,Ë†y(2)âˆˆY2.
Next, we can further loosen this bound by dropping all constraints involving y(2)
1andy(2)
2in the
first optimization problem. We can also drop all constraints involving y(1)
1andy(2)
1in the second
optimization problem. Thus, by definition of the group privacy profile, we have
HÎ±â€²(mx||mxâ€²)â‰¤(1âˆ’Î²)X
Y2+2Ë†cÎ±
y(1)
2,y(1)
1
Â·Î³(y(1),y(2))
+Î²X
Y2+2Ë†cÎ±
y(1)
2,y(2)
2
Â·Î³(y(1),y(2)).
Note that the first cost function term does not depend on y(2)
1nory(2)
2, and the second cost function
term does not depend on y(1)
1nory(2)
1. We can thus marginalize out these variables (recall Eq. (16)
and Eq. (20)) to conclude our proof.
47The same argument also applies to the general problem setting defined in Appendix D: Given two
couplings Î“,Î“â€², we can define a product coupling ËœÎ“âˆÎ“Â·Î“â€², invoke Theorem 3.4, and then apply
(advanced) joint convexity to pessimistically upper-bound the guarantee that would be obtained
through our proposed framework.
G.3 Deficiencies of mechanism-agnostic ADP bounds.
Following our discussion, we can identify three potential sources for looseness in this approach
for deriving mechanism-agnostic bounds. Firstly, it only uses a binary partitioning of subsampling
pmfs sxandsxâ€². The resultant mechanism-specific guarantee only depends on divergences between
two-component mixtures, which may not be sufficient to tightly bound the overall divergence in
complicated scenarios like group privacy. Secondly, it neglects the pairwise distances of y(1)
1andy(2)
2,
resulting in potentially very large values of the cost function. Thirdly, the bound might be further
loosened by applying joint convexity once more.
48H Recovering known mechanism-agnostic RDP guarantees
In the following, we demonstrate that existing amplification by subsampling guarantees for RÃ©nyi-DP
can be derived by instantiating our proposed framework (see Fig. 2). Specifically, we demonstrate
that these guarantees can be derived via the procedure discussed in Section 3.3 and shown in Figs. 2b
and 2c: (1) Conditioning on at most 4events indicating the presence of inserted / deleted / substituted
elements, (2) defining a simultaneous coupling, and (3) using joint convexity to upper-bound the
resultant mechanism-specific guarantee by component divergences.
These guarantees are mechanism-agnostic in the sense that they express the subsampled mechanismâ€™s
privacy parameters (Î±, Ï)as a function of the base mechanismâ€™s privacy parameters. We derive
guarantees for the general measure-theoretic problem setting introduced in Appendix D, where the
base mechanism can be either discrete or continuous.
The reader may notice that parts of the proofs in Appendices H.1 and H.2 are very similar to those
in [28,30], safe for the discussion of couplings and dY-compatibility. That is precisely the point:
There is an optimal transport problem that implicitly underlies results from prior work, which we have
identified and can now generalize to more challenging scenarios like group privacy amplification.
For this section, recall that âˆ†Î±is not the RÃ©nyi divergence, but its Î±th moment, i.e., a scaled and
exponentiated RÃ©nyi divergence (see Definition D.8).
H.1 Subsampling without replacement and substitution
For subsampling without replacement and substitution relation â‰ƒâˆ†we first show a more general
result Theorem H.1. We then demonstrate that it can be upper-bounded via joint convexity of
exponentiated RÃ©nyi divergence âˆ†Î±to recover the guarantee from [28].
Theorem H.1. LetM=Bâ—¦Sbe a subsampled mechanism, where Sis subsampling without
replacement with batch size q. Letâ‰ƒYbe the substitution relation â‰ƒâˆ†,Y. Then, for Î± >1and all
xâ‰ƒâˆ†,Xxâ€²of size N,
âˆ†Î±(mx||mxâ€²)â‰¤max
Ë†yâˆ†Î±((1âˆ’w)Â·by(1)
1+wÂ·by(1)
2||(1âˆ’w)Â·by(2)
1+wÂ·by(2)
2)
subject to dâˆ†,Y(y(1)
1, y(1)
2)â‰¤1,dâˆ†,Y(y(1)
1, y(2)
2)â‰¤1,dâˆ†,Y(y(1)
2, y(2)
2)â‰¤1,y(1)
1=y(2)
1, and with
w=q / N .
Proof. Consider arbitrary xâ‰ƒâˆ†,Xxâ€². By definition of â‰ƒâˆ†, there must be some aâˆˆx,aâ€²âˆˆxâ€²such
thatxâ€²=x\ {a} âˆª {aâ€²}. We thus define both A1andE1from Theorem E.2 to be the event that
neither anoraâ€²is sampled, i.e., A1=E1={yâˆˆY|yâˆ© {a, aâ€²}=âˆ…}. We further define A2and
E2to be the event that aoraâ€²is sampled, i.e., A2=A1andE2=E1.
By definition of subsampling without replacement, we have
PSx(A1) =PSxâ€²(E1) = HyperGeom(0 |N,1, q) = 1âˆ’q
N,
PSx(A2) =PSxâ€²(E2) = HyperGeom(1 |N,1, q) =q
N,
which corresponds to the weights (1âˆ’w)andw, respectively. We further have
sx(y|A1) =( |x|âˆ’1|
qâˆ’1ifyâŠ†xâˆ§a /âˆˆy
0 otherwise, s x(y|A2) =( |x|âˆ’1
qâˆ’1âˆ’1ifyâŠ†xâˆ§aâˆˆy
0 otherwise,
and
sxâ€²(y|E1) =( |x|âˆ’1
qâˆ’1ifyâŠ†xâ€²âˆ§aâ€²/âˆˆy
0 otherwise, s x(y|E2) =( |x|âˆ’1
qâˆ’1âˆ’1ifyâŠ†xâ€²âˆ§aâ€²âˆˆy
0 otherwise.
Coupling. We now define a coupling Î³:Y2+2â†’R+that corresponds to the following generative
process: We first generate y(1)
1by sampling a batch that does not contain auniformly at random from
49x. We then let y(2)
1â†y(1)
1Finally, we pick a random element Ëœaofy(1)
1and replace it with aandaâ€²
to generate y(1)
2andy(2)
2, respectively. More formally:
Î³(y(1),y(2)) =(
sx(y(1)
1|A1)Â·1
qify(1),y(2)fulfills H.2
0 otherwise.
Condition H.2. A tuple y(1)âˆˆY2,y(2)âˆˆY2fulfills this condition when y(2)
1=y(1)
1and
âˆƒËœaâˆˆy(1)
1:
y(1)
2=y(1)
1\ {Ëœa} âˆª {a} âˆ§y(2)
2=y(1)
1\ {Ëœa} âˆª {aâ€²}
.
Validity. We now show that this constitutes a valid coupling. Consider y(1)
1. If and only if sx(y(1)
1|
A1)>0, there are exactly qcombinations of y(1)
2, y(2)
1, y(2)
2for which Î³(y)is non-zero. Thus,
X
y(1)
2,y(2)
1,y(2)
2âˆˆY3Î³(y) =qÂ·sx(y(1)
1|A1).
The proof for y(2)
1is analogous.
Next, consider y(1)
2. If and only if sx(y(1)
2|A1)>0, there are exactly |x| âˆ’qelements that could
have been replaced by ato generate y(1)
2fromy(1)
2. Specifically, these elements are all elements that
do not appear in y(1)
2. We thus have
X
y(1)
1,y(2)
1,y(2)
2âˆˆY3Î³(y) = (|x| âˆ’q)Â·(|x| âˆ’1âˆ’q)!Â·q!
|x| âˆ’1Â·1
q=|x| âˆ’1
qâˆ’1âˆ’1
.
The proof for y(2)
2is analogous.
Compatibility. Finally, we show that Î³is adY-compatible coupling (see Appendix F). Whenever
Î³(y)>0, then
dY(y(1)
1, y(1)
2) =dY
y(1)
1,supp( sx(Â· |A2))
= 1,
dY(y(1)
1, y(2)
1) =dY
y(1)
1,supp( sxâ€²(Â· |E1))
= 0,
dY(y(1)
1, y(2)
2) =dY
y(1)
1,supp( sxâ€²(Â· |E2))
= 1.
Similarly, the pairwise distances between all yt, yuwithu > t > 1are identical to the distance
of their respective supports: The batches have a distance of 1because one can transform one into
another using a single substitution. The supports have a distance of 1because one can transition from
one to another using a single substitution.
The result then immediately follows from Corollary F.7.
Next, we can derive the upper bound from [ 28]. For this derivation, we will use the following Lemma,
which is proven in Appendix B of [6]:
Lemma H.3. Consider two probability measures P, Q on output measure space (Z,Z, Î»). Define
p= dP /dÎ»andq= dQ /dÎ». Then
âˆ†Î±(p||q) = 1 +Î±X
l=2Î±
lZ
(p(z)âˆ’q(z))lq(z)1âˆ’ldÎ»(z).
The following proof essentially follows that of [ 28], but skips their Appendix B.2, since we have
already successfully decomposed mixtures mxandmxâ€²into small terms that only involve base
mechanism densities.
50Proposition H.4 (Wang et al. [28]).LetM=Bâ—¦Sbe a subsampled mechanism, where Sis
subsampling without replacement with batch size q. Letâ‰ƒYbe the substitution relation â‰ƒâˆ†,Y. Then,
forÎ± >1and all xâ‰ƒâˆ†,Xxâ€²of size N,âˆ†Î±(mx||mxâ€²)is l.e.q.
1 + 2Î±X
l=2Î±
l
wlmax
yâ‰ƒâˆ†,Yyâ€²âˆ†l(by||byâ€²),
withw=q / N .
Proof. Using the constraint y(1)
1=y(2)
1in Theorem H.1, we can rewrite its objective as
max
y(1)
1,y(1)
2,y(2)
2âˆ†Î±((1âˆ’w)Â·by(1)
1+wÂ·by(1)
2||(1âˆ’w)Â·by(1)
1+wÂ·by(2)
2).
Using Lemma H.3, we can upper-bound its optimal value via
max
y(1)
1,y(1)
2,y(2)
2âˆ†Î±((1âˆ’w)Â·by(1)
1+wÂ·by(1)
2||(1âˆ’w)Â·by(1)
1+wÂ·by(2)
2)
= max
y(1)
1,y(1)
2,y(2)
21 +Î±X
l=2Î±
lZ(wÂ·by(1)
2âˆ’wÂ·by(2)
2)l

(1âˆ’w)Â·by(1)
1+wÂ·by(2)
2lâˆ’1dÎ»(z)
â‰¤ max
y(1)
1,y(1)
2,y(2)
21 +Î±X
l=2Î±
l
wlZ|by(1)
2âˆ’by(2)
2|l

(1âˆ’w)Â·by(1)
1+wÂ·by(2)
2lâˆ’1dÎ»(z)
â‰¤Î±X
l=0Î±
l
wlmax
y(1)
1,y(1)
2,y(2)
2Z|by(1)
2âˆ’by(2)
2|l

(1âˆ’w)Â·by(1)
1+wÂ·by(2)
2lâˆ’1dÎ»(z),
where each of the Î±+ 1optimization problems is independently constrained by dY(y(1)
1, y(1)
2)â‰¤1,
dY(y(1)
1, y(2)
2)â‰¤1, and dY(y(1)
2, y(2)
2)â‰¤1.
Next, we bound the optimal value of each of the Î±+ 1optimization problems. Using the joint
convexity of x, y7â†’xlÂ·y1âˆ’l, which implies convexity in the second component, shows that
max
y(1)
1,y(1)
2,y(2)
2Z|by(1)
2âˆ’by(2)
2|l

(1âˆ’w)Â·by(1)
1+wÂ·by(2)
2lâˆ’1dÎ»(z)
â‰¤ max
y(1)
1,y(1)
2,y(2)
2(1âˆ’w)Â·Z|by(1)
2âˆ’by(2)
2|l
blâˆ’1
y(1)
1dÎ»(z) +wÂ·Z|by(1)
2âˆ’by(2)
2|l
blâˆ’1
y(2)
2dÎ»(z)
â‰¤(1âˆ’w)Â·ï£«
ï£­ max
y(1)
1,y(1)
2,y(2)
2Z|by(1)
2âˆ’by(2)
2|l
blâˆ’1
y(1)
1dÎ»(z)ï£¶
ï£¸+wÂ·ï£«
ï£­ max
y(1)
1,y(1)
2,y(2)
2Z|by(1)
2âˆ’by(2)
2|l
blâˆ’1
y(2)
2dÎ»(z)ï£¶
ï£¸
â‰¤(1âˆ’w)Â·ï£«
ï£­ max
y(1)
1,y(1)
2,y(2)
2Z|by(1)
2âˆ’by(2)
2|l
blâˆ’1
y(1)
1dÎ»(z)ï£¶
ï£¸+wÂ·ï£«
ï£­ max
y(1)
1,y(1)
2,y(2)
2Z|by(1)
2âˆ’by(2)
2|l
blâˆ’1
y(1)
1dÎ»(z)ï£¶
ï£¸
= max
y(1)
1,y(1)
2,y(2)
2Z|by(1)
2âˆ’by(2)
2|l
blâˆ’1
y(1)
1dÎ»(z),
where all optimization problems are independent, with each one being independently constrained by
dY(y(1)
1, y(1)
2)â‰¤1,dY(y(1)
1, y(2)
2)â‰¤1, and dY(y(1)
2, y(2)
2)â‰¤1. Note that, for the last inequality, we
replaced a y(2)
2in the second maximization with a y(1)
1, which essentially adds a degree of freedom
and thus leads to an upper bound.
In [28], the optimal value of the final problem is referred to as the ternary- |Ï‡|l-divergence of b2,by(2)
2,
andby(1)
1. As shown in their Lemma 19, it can be upper bounded via 2Â·max yâ‰ƒâˆ†,Yyâ€²âˆ†l(by||byâ€²),
which concludes our proof.
51Additional terms. Note that [ 28] derive three additional bounds (see their Lemma 17, Lemma 19,
and Theorem 27) on the the ternary- |Ï‡|l-divergence. This introduces additional terms, but does not
change the fact that their result is lower-bounded by Theorem H.1. We use their full theorem as a
baseline in our experiments.
H.2 Poisson subsampling and insertion/removal
Next, we show that the Poisson subsampling guarantees in [ 30,7] follow from another optimal
transport problem. For our proof, we will use the following Lemma, which corresponds to the â€œnovel
alternative decompositionâ€ in Appendix A.1 of [30].
Lemma H.5. Consider K+ 1âˆˆNdistributions P, Q 1, . . . , Q Kon output measure space (Z,Z, Î»),
and define p= dP /dÎ»,qk= dQk/dÎ». Further consider some w1, . . . , w Kâˆˆ[0,1]withPK
k=1wk= 1. Then,
âˆ†Î± 
p||KX
k=1wkÂ·qk!
â‰¤KX
k=1wkÂ·âˆ†Î± 
qk+pâˆ’KX
l=1wlÂ·ql||qk!
.
Proof. Based on the definition of âˆ†Î±, we have
âˆ†Î± 
p||KX
k=1wkÂ·qk!
=Zp(z)Î±
PK
k=1wkÂ·qk(z)Î±âˆ’1dÎ»(z)
=ZPK
k=1wkÂ·qk(z)
+p(z)âˆ’PK
l=1wlÂ·ql(z)Î±
PK
k=1wkÂ·qk(z)Î±âˆ’1dÎ»(z)
=ZPK
k=1wkÂ·
qk(z) +p(z)âˆ’PK
l=1wlÂ·ql(z)Î±
PK
k=1wkÂ·qk(z)Î±âˆ’1dÎ»(z)
The result then follows from joint convexity of âˆ†Î±.
Note that the proof of this lemma is very similar to the proof strategy we used in deriving Propo-
sition H.4 using Lemma H.3: We add 0 =câˆ’cwith some cto the numerator (which leads to the
binomial expansion in Lemma H.3) and then apply joint convexity to obtain an upper bound.
Proposition H.6 (Zhu and Wang [30]).LetM=Bâ—¦Sbe a subsampled mechanism, where Sis
Poisson subsampling with rate r. Letâ‰ƒYbe the insertion/removal relation â‰ƒÂ±,Y. Then, for Î± >1
and all xâ‰ƒÂ±,Xxâ€²,âˆ†Î±(mx||mxâ€²)is l.e.q.
2Â·Î±X
l=0Î±
l
rl(1âˆ’r)Î±âˆ’lmax
yâ‰ƒÂ±,Yyâ€²âˆ†l(by||byâ€²).
Proof. Since we are concerned with insertion/removal, we need to consider two cases:
Case 1: Removal. In this case, there is some aâˆˆxsuch that xâ€²=x\ {a}. We let A1be the event
thatais not sampled, i.e. A1={yâˆˆY|a /âˆˆy}, and let A2=A1. We let E1=Y, i.e., do not
condition on any particular event.
By definition of Poisson subsampling, we have PSx(A1) = 1âˆ’r,PSx(A2) =r, andPSxâ€²(E1) = 1 .
We further have
sx(y|A1) =r|y|(1âˆ’r)|x|âˆ’|y|âˆ’1ifyâŠ†xâˆ§a /âˆˆy
0 otherwise
sx(y|A2) =r|y|âˆ’1(1âˆ’r)|x|âˆ’|y|ifyâŠ†xâˆ§aâˆˆy
0 otherwise,
52and
sxâ€²(y|E1) =r|y|(1âˆ’r)|xâ€²|âˆ’|y|ifyâŠ†xâ€²
0 otherwise=r|y|(1âˆ’r)|x|âˆ’|y|âˆ’1ifyâŠ†xâˆ§a /âˆˆy
0 otherwise.
Note that sx(y|A1) =sxâ€²(y|E1).
Coupling. We now define a coupling Î³:Y2+1â†’R+that corresponds to the following generative
process: We first generate y(1)
1by sampling a batch that does not contain avia Poisson subsampling
from x\ {a}. We then let y(2)
1â†y(1)
1. Finally, we deterministically insert ato generate y(1)
2. This
can be formally defined via
Î³(y(1),y(2)) =(
sx(y(1)
1|A1)ify(1)
1=y(2)
1âˆ§y(1)
2=y(1)
1âˆª {a}
0 otherwise.
Validity. We can verify the validity of this coupling as follows: For every y(1)
1withsx(y(1)
1|A1)>0,
there is exactly one combination y(1)
2, y(2)
1for which Î³(y)>0, namely y(2)
1=y(1)
1andy(1)
2=
y(1)
1âˆª {a}. We thus haveP
y(1)
2,y(2)
1âˆˆY2Î³(y(1)
1, y(1)
2, y(2)
1) =sx(y(1)
1|A1).The proof for y(2)
1is
analogous. For every y(1)
2, we have exactly one combination of y(1)
1, y(2)
1for which Î³(y)>0, namely
y(1)
1=y(2)
1=y(1)
2\ {a}. We thus have
X
y(1)
1,y(2)
1âˆˆY2Î³(y(1)
1, y(1)
2, y(2)
1)
=sx(y(1)
2\ {a} |A1) =r|y(1)
2|âˆ’1(1âˆ’r)|x|âˆ’(|y|âˆ’1)âˆ’1=r|y(1)
2|âˆ’1(1âˆ’r)|x|âˆ’|y|=sx(y(1)
2|A2).
Compatibility. Finally, it can be easily shown that Î³is adY-compatible coupling (see Appendix F).
Whenever Î³(y)>0, then
dY(y(1)
1, y(1)
2) =dY
y(1)
1,supp( sx(Â· |A2))
= 1,
dY(y(1)
1, y(2)
1) =dY
y(1)
1,supp( sxâ€²(Â· |E1))
= 0.
Similarly, the pairwise distance between y(1)
2andy(2)
1is always 1. This is identical to the distance of
their supports, because one can always transition between them via a single insertion/removal.
It thus follows from Corollary F.7 that
âˆ†Î±(mx||mxâ€²)â‰¤ max
y(1)
1,y(1)
2,y(2)
1âˆ†Î±((1âˆ’r)Â·by(1)
1+rÂ·by(1)
2||by(2)
1)
s.t.dÂ±,Y(y(1)
1, y(1)
2)â‰¤1, y(1)
1=y(2)
1, dÂ±,Y(y(1)
2, y(2)
1)â‰¤1
Because y(1)
1=y(2)
1and because dÂ±,Y(y(1)
1, y(1)
2)â‰¤1is equivalent to y(1)
1â‰ƒÂ±,Yy(1)
2, this bound
can be restated as
âˆ†Î±(mx||mxâ€²)â‰¤max
yâ‰ƒÂ±,Yyâ€²âˆ†Î±((1âˆ’r)Â·by+rÂ·byâ€²||by).
Finally, one can use the definition of âˆ†Î±and binomial expansion to show
âˆ†Î±(mx||mxâ€²)â‰¤max
yâ‰ƒÂ±,Yyâ€²Z
Z((1âˆ’r)by(z) +rbyâ€²(z))Î±
by(z)Î±âˆ’1dÎ»(z)
= max
yâ‰ƒÂ±,Yyâ€²Z
Z(1âˆ’r)by(z) +rbyâ€²(z)
by(z)Î±
by(z) dÎ»(z)
= max
yâ‰ƒÂ±,Yyâ€²Z
Z
(1âˆ’r) +rbyâ€²(z)
by(z)Î±
by(z) dÎ»(z)
= max
yâ‰ƒÂ±,Yyâ€²Î±X
l=0Î±
l
rl(1âˆ’r)Î±âˆ’lâˆ†l(by||byâ€²)
â‰¤Î±X
l=0Î±
l
rl(1âˆ’r)Î±âˆ’lmax
yâ‰ƒÂ±,Yyâ€²âˆ†l(by||byâ€²).
53Note that this is smaller than the term in Proposition H.6 by a factor of 2.
Case 2: Insertion In this case, there is some aâˆˆxâ€²such that x=xâ€²\ {a}. Very similar to before,
we let A1=Y, i.e., we do not condition on any particular event. We further let E1be the event that
ais not sampled, i.e. E1={yâˆˆY|a /âˆˆy}, and let E2=E1. We notice that we have exactly the
same conditional distributions as in the first case. We can thus define exactly the same coupling (up
to changes in indexing) to show that
âˆ†Î±(mx||mxâ€²)â‰¤max
yâ‰ƒÂ±,Yyâ€²âˆ†Î±(by||(1âˆ’r)by+rbyâ€²).
Next, applying Lemma H.5 and using the definition of âˆ†Î±shows that
âˆ†Î±(mx||mxâ€²)
â‰¤max
yâ‰ƒÂ±,Yyâ€²âˆ†Î±(by||(1âˆ’r)by+rbyâ€²)
â‰¤max
yâ‰ƒÂ±,Yyâ€²(1âˆ’r)âˆ†Î±(by+byâˆ’((1âˆ’r)by+rbyâ€²)||by) +râˆ†Î±(byâ€²+byâˆ’((1âˆ’r)by+rbyâ€²)||byâ€²)
= max
yâ‰ƒÂ±,Yyâ€²(1âˆ’r)âˆ†Î±((1 + r)byâˆ’rbyâ€²||by) +râˆ†Î±((1âˆ’r)byâ€²+rby||byâ€²)
This corresponds exactly to Eq. 6 of the â€œnovel alternative decompositionâ€ in Appendix A.1 of [ 30].
One can then through the remaining steps in their Appendix A to show that this term is at most two
times larger than the one we derived in the deletion case.
As already mentioned at the beginning of this section, the proof is very similar to that in [ 30],
except for the discussion of couplings and dY-compatibility. We emphasize that the crucial aspect,
which is not discussed in any prior work on RÃ©nyi-DP amplification, is that there is an implicit,
underlying optimal transport problem between conditional subsampling distributions. Now that we
have identified this connection, we can apply this more general optimal transport principle to a much
broader range of amplification by subsampling scenarios for RÃ©nyi-DP.
Further tightening. The factor 2in Proposition H.6 can be eliminated for distributions with particular
symmetries (see Theorem 5 in [ 29]) or bounded Pearson-Vajda Ï‡l-pseudo-divergence (see Theorem
8 in [ 30]). We thus do not include the factor 2when using this amplification guarantee as a baseline
in our experiments.
H.3 Graph subsampling without replacement and node modification
Daigavane et al. [40] consider a setting that differes from the usual insertional/removal into datasets:
Node-level privacy for graphs. There, the dataset space is the set of all directed, attributed graphs
X=S
N,DâˆˆNRNÃ—DÃ— {0,1}NÃ—N, which are composed of a continuous feature matrix and a
discrete adjacency matrix. Two graphs x, xâ€²are related by dataset relation â‰ƒXifxâ€²can be constructed
by inserting a node (including new edges) into x, or removing a node (including its edges) from x.
To analyze this problem setting, they perform a preprocessing step (see their Algorithm 2) to represent
each graph by a set of subgraphs, with each subgraph corresponding to a node in the graph. Via
this construction, they return to the traditional setting where XâŠ† P(A)for some set A, and the
modification of a nodeâ€™s features and edges corresponds to the substitution of Kelements,4i.e.
â‰ƒKâˆ†,X={(x, xâ€²)âˆˆX| âˆƒgâˆˆx\xâ€², gâ€²âˆˆxâ€²\x:xâ€²=x\gâˆªgâ€²âˆ§ |g|=|gâ€²|=K}, (22)
where Kdepends on the maximum considered graph degree.
Irrespective of the graph neural network specific details, this discussion suggests that group privacy
and differentially private learning for graphs are related.
We shall now demonstrate that their result for subsampling without replacement can be derived
from Theorem E.1, i.e. optimal transport without conditioning.
Proposition H.7 (Daigavane et al. [40]).LetM=Bâ—¦Sbe a subsampled mechanism, where Sis
subsampling without replacement with batch size q. Letâ‰ƒYbe the substitution relation â‰ƒâˆ†,Y. Let
4Note that they do not actually consider insertion/removal, because their analysis assumes the number of
subgraphs to remain constant.
54â‰ƒKâˆ†,Xbe the K-fold substitution relation defined in Eq. (22). Then, for Î± >1and all xâ‰ƒKâˆ†,Xxâ€²
of size N,
âˆ†Î±(mx||mxâ€²)â‰¤KX
k=0wkÂ·max
yâ‰ƒkâˆ†,Yyâ€²âˆ†k(by||byâ€²).
withwk= HyperGeom( k|N, K, q ).
Proof. Consider arbitrary xâ‰ƒâˆ†,Xxâ€². By definition of â‰ƒâˆ†, there must be some gâˆˆx\xâ€²,gâ€²âˆˆxâ€²\x
such that xâ€²=x\gâˆªgâ€²and|g|=|gâ€²|=K.
We condition on the events A1=YandE1=Y, meaning
sx(y|A1) =( N
qâˆ’1ifyâŠ†x
0 otherwiseand sxâ€²(y|E1) =( N
qâˆ’1ifyâŠ†xâ€²
0 otherwise.
Coupling. We now define a coupling via Î³:Y1+1â†’R+that define the following generative
process: We first define y(1)by sampling a batch of size qfrom xwithout replacement. We then
remove all elements that are in gand insert an equal number of elements, which are chosen uniformly
at random from gâ€². This coupling can be formally defined as
Î³(y(1), y(2)) =(
sx(y(1)|A1)Â· K
|y(1)âˆ©g|âˆ’1ify(1), y(2)fulfills Condition H.8, .
0 otherwise.
Condition H.8. A tuple y(1)âˆˆY,y(2)âˆˆYfulfills this condition when there exists a ËœgâŠ†gsuch
thaty(2)=y(1)\gâˆªËœgand|y(2)âˆ©Ëœg|=|y(1)âˆ©g|.
Validity. We now show that this constitutes a valid coupling. Consider any y(1)withsx(y(1)|A1)>
0and|y(1)âˆ©g|=k. There are exactly K
k
different y(2)for which Î³(y)is non-zero, and each one
has the same probability. Thus,
X
y(2)âˆˆYÎ³(y) =K
k
Â·sx(y(1)|A1)Â·K
kâˆ’1
=sx(y(1)|A1).|xâˆ’1|
q
.
The other case is analogous.
Compatibility. Evidently, this is a dY-compatible coupling, because (a) whenever y(1)contains k
elements from g, it has a distance of kfrom sxâ€²(y|E1)and (b) whenever y(1)contains kelements
from g, we generate y(2)by substituting exactly kelements.
It thus follows from Corollary F.7 that
âˆ†Î±(mx||mxâ€²)â‰¤X
ysx(y(1)|A1)ÎºÎ±(y(1))
with
ÎºÎ±(y(1)) = max
y,yâ€²âˆ†Î±(by||byâ€²)s.t.dY(y, yâ€²)â‰¤ |y(1)âˆ©g|.
The result then follows from the definition of induced distance, the definition of the K-fold
substitution relation â‰ƒKâˆ†,Xand the fact that |y(1)âˆ©g|is a random variable with distribution
HyperGeom( N, K, q ).
Note that [ 40] instantiate this result with Gaussian mechanisms with sensitivity C, for which
max yâ‰ƒkâˆ†,Yyâ€²âˆ†Î±(by||byâ€²) = exp( Î±Â·(Î±âˆ’1)Â·k2C2
Ïƒ2).
In Appendix B.1.4, we experimentally demonstrate that this result can improve upon the baseline of
combining [ 28] with the traditional group privacy property, but is not sufficiently tight to consistently
outperform it across a wide range of parameters. This emphasizes the benefit of considering optimal
transport between proper conditional distributions when deriving amplification guarantees.
55I Novel RDP guarantees
Now that we have a framework that enables generic subsampling analysis for RÃ©nyi differential pri-
vacy, we can derive a variety of novel guarantees that were previously only available for approximate
differential privacy.
In the following, we first demonstrate that RDP guarantees for a dataset relation â‰ƒXcan be derived
from a base mechanism that is only known to be RDP under a different batch relation â‰ƒY, i.e.,
â€œhybrid neighboring relationsâ€ [ 15]. We then show that we can derive RDP guarantees for non-
standard combinations of subsampling schemes and neighboring relations, such as subsampling
without replacement and insertion/removal. Finally, we derive a simple, tight mechanism-specific
subsampling guarantee for randomized response. We use this guarantee in Section 4 to demonstrate
the limitations of using mechanism-agnostic RDP bounds instead of mechanism-specific RDP bounds.
For this section, recall again that Î›Î±is not the RÃ©nyi divergence, but its Î±th moment, i.e., a scaled
and exponentiated RÃ©nyi divergence (see Definition D.8).
I.1 Hybrid neighboring relations
Hybrid neighboring relations have thus far not been discussed in the context of RÃ©nyi-DP. Analyzing
such scenarios may be particularly useful when the dataset space Xand the batch space (Y,Y)are
different from each other, e.g., when mapping from large text corpora to short sequences of token
embeddings. Note that hybrid relations may also be useful for noisy stochastic gradient descent with
fixed batch sizes (see end of Appendix I.2).
As an example, we consider the following scenario: Subsampling without replacement, with sub-
stitution relation â‰ƒâˆ†,Xfor datasets and insertion/removal relation â‰ƒÂ±,Yfor batches. The proof is
largely identical to that of Theorem H.1, but uses the fact that a substitution can be represented by an
insertion, followed by a removal.
Theorem I.1. LetM=Bâ—¦Sbe a subsampled mechanism, where Sis subsampling without
replacement with batch size q. Letâ‰ƒYbe the insertion/removal relation â‰ƒÂ±,Yandâ‰ƒXbe the
substitution relation â‰ƒâˆ†,X. Then, for Î± >1and all xâ‰ƒâˆ†,Xxâ€²of size N,
Î›Î±(mx||mxâ€²)â‰¤max
yÎ›Î±((1âˆ’w)Â·by(1)
1+wÂ·by(1)
2||(1âˆ’w)Â·by(2)
1+wÂ·by(2)
2)
withyâˆˆY2+2subject to y(1)
1=y(2)
1,dÂ±,Y(y(1)
1, y(1)
2)â‰¤2,dÂ±,Y(y(1)
1, y(2)
2)â‰¤2,dÂ±,Y(y(1)
2, y(2)
2)â‰¤
2, and with w=q / N .
Proof. Consider arbitrary xâ‰ƒâˆ†,Xxâ€². By definition of â‰ƒâˆ†, there must be some aâˆˆx,aâ€²âˆˆxâ€²such
thatxâ€²=x\ {a} âˆª {aâ€²}. We thus define both A1andE1from Theorem E.2 to be the event that
neither anoraâ€²is sampled, i.e., A1=E1={yâˆˆY|yâˆ© {a, aâ€²}=âˆ…}. We further define A2and
E2to be the event that aoraâ€²is sampled, i.e., A2=A1andE2=E1.
By definition of subsampling without replacement, we have
PSx(A1) =PSxâ€²(E1) = HyperGeom(0 |N,1, q) = 1âˆ’q
N,
PSx(A2) =PSxâ€²(E2) = HyperGeom(1 |N,1, q) =q
N,
which corresponds to the weights (1âˆ’w)andw, respectively.
Coupling. We now define a coupling via Î³:Y4â†’R+:
Î³(y(1),y(2)) =(
sx(y(1)
1|A1)Â·1
qify(1),y(2)fulfill Condition H.2,
0 otherwise.
Condition H.2. A tuple y(1)âˆˆY2,y(2)âˆˆY2fulfills this condition when y(2)
1=y(1)
1and
âˆƒËœaâˆˆy(1)
1:
y(1)
2=y(1)
1\ {Ëœa} âˆª {a} âˆ§y(2)
2=y(1)
1\ {Ëœa} âˆª {aâ€²}
.
56As explained before, Î³defines the following generative process: We first generate y(1)
1by sampling a
batch that does not contain auniformly at random from x. We then let y(2)
1â†y(1)
1Finally, we pick a
random element Ëœaofy(1)
1and replace it with aandaâ€²to generate y(1)
2andy(2)
2, respectively. Note
that each of these substitution corresponds to exactly one insertion and one removal.
Validity. The validity of this coupling has already been demonstrated in our proof of Theorem H.1,
since validity of a coupling does not depend on batch neighboring relation â‰ƒY.
Compatibility. We can thus focus on showing that Î³is adY-compatible coupling (see Appendix F).
Whenever Î³(y)>0, then
dY(y(1)
1, y(1)
2) =dY
y(1)
1,supp( sx(Â· |A2))
= 2,
dY(y(1)
1, y(2)
1) =dY
y(1)
1,supp( sxâ€²(Â· |E1))
= 0,
dY(y(1)
1, y(2)
2) =dY
y(1)
1,supp( sxâ€²(Â· |E2))
= 2.
We see that the pairwise distances between all yt, yuwithu > t > 1are identical to the distance
of their respective supports: The batches have a distance of 2because one can transform one into
another using a single substitution, i.e. an insertion and a removal. The supports also have a distance
of2because one can transition from one to another using a single substitution.
The result then immediately follows from Corollary F.7.
If we wanted to express this result in terms of the RÃ©nyi-DP parameters of the base mechanism, we
could go through the same derivations as in our proof of Proposition H.4 to show that
Î›Î±(mx||mxâ€²)â‰¤1 + 2Î±X
l=2Î±
l
wl
max
y,yâ€²Î›l(by||byâ€²)s.t.dÂ±,Y(y, yâ€²)â‰¤2
,
withw=q / N . The inner term can, for instance, be evaluated using the traditional group privacy
property from [31].
I.2 Subsampling without replacement under insertion/removal
To demonstrate that our framework is not limited to analyzing the usually considered combination of
subsampling without replacement and substitution or Poisson subsampling and insertion/removal,
we consider the following non-standard combination: Subsampling without replacement and inser-
tion/removal.
We show that we can derive a guarantee that is qualitatively similar to that of [ 30], while preserving a
fixed batch size. Such results could be useful when implementing noisy stochastic gradient descent in
deep learning frameworks with static computation graphs, where the variable batch sizes resulting
from Poisson subsampling may be problematic. However, note that this guarantee only guarantees
privacy for spaces of datasets whose elements are larger than some QâˆˆNwithQ > q , where q is
the batch size.
For the following proof, we condition on the same events as in our proof of Proposition H.6, but need
to construct a different dY-compatible coupling, since we have a different subsampling distribution.
Theorem I.2. Assume a dataset space and batch space defined by XâŠ† {xâˆˆ P(A)| |x|> Q},
Y={yâŠ†x|xâˆˆX,|y|=q},Y=P(Y)and finite set A, with Q, qâˆˆNandQ > q . Let
M=Bâ—¦Sbe a subsampled mechanism, where Sis subsampling without replacement with batch
size q r. Letâ‰ƒYbe the insertion/removal relation â‰ƒÂ±,Y. Then, for Î± >1and all xâ‰ƒÂ±,Xxâ€²,
Î›Î±(mx||mxâ€²)â‰¤max
NâˆˆN 
2Â·Î±X
l=0Î±
l
wl(1âˆ’w)Î±âˆ’l
max
y,yâ€²Î›l(by||byâ€²)s.t.dY(y, yâ€²)â‰¤2!
subject to N > Q and with w=q / N .
Proof. Like in our proof of Proposition H.6, we need to distinguish between insertion and removal.
57Case 1: Removal. In this case, there is some aâˆˆxsuch that xâ€²=x\ {a}. We define N=|xâ€²|,
which fulfills N > Q by definition of dataset space X.
We let A1be the event that ais not sampled, i.e. A1={yâˆˆY|a /âˆˆy}, and let A2=A1. We let
E1=Y, i.e., do not condition on any particular event.
By definition of subsampling without replacement, we have PSx(A1) = 1âˆ’qN,PSx(A2) =q / N ,
andPSxâ€²(E1) = 1 . We further have
sx(y|A1) =( N
qâˆ’1ifyâŠ†xâˆ§a /âˆˆy
0 otherwise, s x(y|A2) =( N
qâˆ’1âˆ’1ifyâŠ†xâˆ§aâˆˆy
0 otherwise,
and
sxâ€²(y|E1) =( N
qâˆ’1ifyâŠ†xâ€²
0 otherwise.
Note that sx(y|A1) =sxâ€²(y|E1)and that all three conditional distribution are subsampling
without replacement from sets of size N, not of size N+ 1orNâˆ’1. Further note that, for event A1,
we only need to sample qâˆ’1elements, since one element is always fixed to be a.
Coupling. We now define a coupling via Î³:Y2+1â†’R+:
Î³(y(1),y(2)) =(
sx(y(1)
1|A1)Â·1
qify(1)
1=y(2)
1âˆ§y(1)
2=y(1)
1âˆª {a}
0 otherwise.
Simply put, Î³defines the following generative process: We first generate y(1)
1by sampling a batch
that does not contain avia subsampling without replacement from x\ {a}. We then let y(2)
1â†y(1)
1.
We then randomly replace one of the qbatch elements with ato generate y(1)
2.
Validity. We can verify the validity of this coupling as follows: For every y(1)
1withsx(y(1)
1|
A1)>0, there are exactly qcombination y(1)
2, y(2)
1for which Î³(y)>0. We thus haveP
y(1)
2,y(2)
1âˆˆY2Î³(y(1)
1, y(1)
2, y(2)
1) =sx(y(1)
1|A1).The proof for y(2)
1is analogous.
For every y(1)
2, there are exactly Nâˆ’(qâˆ’1)combinations of y(1)
1, y(2)
1for which Î³(y)>0. This is
because amust have replaced one of the Nâˆ’(qâˆ’1)elements of xâ€²that are not in y(2)
1.
We thus have
X
y(1)
1,y(2)
1âˆˆY2Î³(y(1)
1, y(1)
2, y(2)
1)
=(Nâˆ’(qâˆ’1))Â·N
qâˆ’1
Â·1
q=q
Nâˆ’q+ 1Â·N
qâˆ’1
=N
qâˆ’1âˆ’1
.
Compatibility. Finally, it can be easily shown that Î³is adY-compatible coupling. Whenever
Î³(y)>0, then
dY(y(1)
1, y(1)
2) =dY
y(1)
1,supp( sx(Â· |A2))
= 2,
dY(y(1)
1, y(2)
1) =dY
y(1)
1,supp( sxâ€²(Â· |E1))
= 0.
Similarly, the pairwise distance between y(1)
2andy(2)
1is always 2. This is identical to the distance of
their supports, because one can always transition between them via a substitution, i.e., an insertion
and a removal.
It thus follows from Corollary F.7 that
Î›Î±(mx||mxâ€²)â‰¤ max
y(1)
1,y(1)
2,y(2)
1Î›Î±((1âˆ’q / N )Â·by(1)
1+q / NÂ·by(1)
2||by(2)
1)
s.t.dÂ±,Y(y(1)
1, y(1)
2)â‰¤1, y(1)
1=y(2)
1, dÂ±,Y(y(1)
2, y(2)
1)â‰¤2
58As in our proof of Proposition H.6 one can use the definition of Î›Î±and binomial expansion to finally
show that
Î›Î±(mx||mxâ€²)â‰¤Î±X
l=0Î±
l
wl(1âˆ’w)Î±âˆ’l
max
y,yâ€²Î›l(by||byâ€²)s.t.dYy, yâ€²â‰¤2.
withw=q / N . Note that this is smaller than the term in Proposition H.6 by a factor of 2.
Case 2: Insertion In this case, there is some aâˆˆxâ€²such that x=xâ€²\ {a}. We can thus define the
same events and the same coupling in a symmetric manner to show
Î›Î±(mx||mxâ€²)â‰¤max
y,yâ€²(1âˆ’w)Z
Z(1 +w)by(z)âˆ’wbyâ€²(z)
by(z)Î±
by(z) dÎ»(z)
+wZ
Z(1âˆ’w)byâ€²(z) +wby(z)
byâ€²(z)Î±
byâ€²(z) dÎ»(z),
subject to dY(y, yâ€²)â‰¤2. with w=q / N . Again, this corresponds exactly to Eq. 6 of the â€œnovel
alternative decompositionâ€ in Appendix A.1 of [ 30]. One can then through the remaining steps in
their Appendix A to show that this term is at most two times larger than the one we derived in the
deletion case.
Finally, since this bound depends on N, and we want the guarantee to hold for all xâ‰ƒÂ±,Xxâ€², we
need to determine the Nthat maximizes the bound.
Hybrid Relations. Note that we could have also gone through the above derivations with batch
substitution relation â‰ƒâˆ†,Y. Then, each substitution would correspond to an actual substitution, instead
of a pair of insertion and deletion, and we would obtain
Î›Î±(mx||mxâ€²)â‰¤max
NâˆˆN 
2Â·Î±X
l=0Î±
l
wl(1âˆ’w)Î±âˆ’lmax
yâ‰ƒâˆ†,Yyâ€²Î›l(by||byâ€²)!
s.t.N > Q,
withw=q / N .
I.3 Tight mechanism-specific subsampling without replacement for randomized response
As mentioned earlier, the following result is meant as a simple example to demonstrate the benefit of
using mechanism-specific over mechanism-agnostic RDP bounds in Section 4. For the following
discussion, recall Theorem H.1, which we proved in the previous section.
Theorem H.1. LetM=Bâ—¦Sbe a subsampled mechanism, where Sis subsampling without
replacement with batch size q. Letâ‰ƒYbe the substitution relation â‰ƒâˆ†,Y. Then, for Î± >1and all
xâ‰ƒâˆ†,Xxâ€²of size N,
âˆ†Î±(mx||mxâ€²)â‰¤max
Ë†yâˆ†Î±((1âˆ’w)Â·by(1)
1+wÂ·by(1)
2||(1âˆ’w)Â·by(2)
1+wÂ·by(2)
2)
subject to dâˆ†,Y(y(1)
1, y(1)
2)â‰¤1,dâˆ†,Y(y(1)
1, y(2)
2)â‰¤1,dâˆ†,Y(y(1)
2, y(2)
2)â‰¤1,y(1)
1=y(2)
1, and with
w=q / N .
We shall now solve Theorem H.1 for randomized response and prove the tightness of the resultant
guarantee.
Theorem I.3. LetBbe the randomized response mechanism |hâˆ’(1âˆ’V)|withh:Yâ†’ {0,1},
Vâˆ¼Bern( Î¸), and true response probability Î¸âˆˆ[0,1]. LetSbe subsampling without replacement
with batch size q. Then, for all xâ‰ƒâˆ†,Xxâ€²of size N,Î›Î±(mx||mxâ€²)is l.e.q.
max
Ï„Î›Î±((1âˆ’w)Bern( Â· |Î¸) +wBern(Â· |Ï„)||(1âˆ’w)Bern( Â· |Î¸) +wBern(Â· |1âˆ’Ï„))
subject to Ï„âˆˆ {Î¸,1âˆ’Î¸}and with w=q / N .
Proof. Because we have no further information, we must consider the worst possible underlying
function h:Yâ†’ {0,1}.
59Due to the last constraint in Theorem H.1, we must have h(y(1)
1) =h(y(2)
1). Due to symmetry in
Î›Î±(we are summing over zâˆˆ {0,1}), we can assume w.l.o.g. that h(y(1)
1) =h(y(2)
1) = 1 and thus
by(1)
1(z) =by(1)
1(z) = Bern( z|Î¸).
Because the batches y(1)
2andy(1)
2can differ from y(1)
1, we can choose the corresponding function
values h(y(1)
2)andh(y(2)
2)arbitrarily. To make Î›Î±greater than 1, the two mixtures must be different
from each other. Thus, we must choose either h(y(1)
2) = 1 andh(y(2)
2) = 0 , orh(y(1)
2) = 0 and
h(y(2)
2) = 1 . This corresponds to by(1)
2(z) = Bern( z|Ï„)andby(2)
2(z) = Bern( z|1âˆ’Ï„)with
Ï„âˆˆ {Î¸,1âˆ’Î¸}. Maximizing over both options yields our result.
Note that this guarantee can be evaluated in O(1): We need to iterate over two possible values of Ï„,
and evaluating the corresponding Î›Î±requires summing over two values zâˆˆ {0,1}.
Next, we prove the mechanism-specific tightness of this result, meaning it is not possible to de-
rive stronger guarantees without additional information about dataset space Xand the function h
underlying the randomized response mechanism.
Theorem I.4. LetSbe subsampling without replacement with arbitrary batch size qâˆˆNand
Î¸âˆˆ[0,1]be some true response probability. There exists a dataset space Xand a batch space
(Y,Y)fulfilling the constraints in Definition D.5, as well a pair of datasets xâ‰ƒâˆ†,Xof size N, and a
function h:Yâ†’ {0,1}, such that the corresponding subsampled randomized response mechanism
M=Bâ—¦Sfulfills
Î›Î±(mx||mxâ€²)
= max
Ï„Î›Î±((1âˆ’w)Bern( Â· |Î¸) +wBern(Â· |Ï„)||(1âˆ’w)Bern( Â· |Î¸) +wBern(Â· |1âˆ’Ï„))
subject to Ï„âˆˆ {Î¸,1âˆ’Î¸}and with w=q / N .
Proof. LetX={xâŠ†N| |x|> q}. Consider an arbitrary xâˆˆXand select arbitrary elements aâˆˆx,
aâ€²âˆˆN\x. Define xâ€²=x\ {a} âˆª {aâ€²}.
Assume w.l.o.g. that the divergence is maximized by Ï„=Î¸. We now construct an indicator function
h:Yâ†’ {0,1}foraandaâ€²that leads to the largest possible divergence.
h(y) =ï£±
ï£²
ï£³1ifyâˆ© {a, aâ€²}=âˆ…
1ifaâˆˆy
0ifaâ€²âˆˆy
By construction of h, the corresponding base mechanism pmf is
by(z) =ï£±
ï£²
ï£³Bern( z|Î¸) ifyâˆ© {a, aâ€²}=âˆ…
Bern( z|Î¸) ifaâˆˆy
Bern( z|1âˆ’Î¸)ifaâ€²âˆˆy
Under the distribution of S(x), the first case occurs with probability 1âˆ’wand the second case occurs
with probability w. Under the distribution of S(x), the first case occurs with probability 1âˆ’wand
the second case occurs with probability w. We thus have
mx(z) = (1 âˆ’w)Bern( z|Î¸) +wBern( z|Î¸),
mxâ€²(z) = (1 âˆ’w)Bern( z|Î¸) +wBern( z|1âˆ’Î¸),
which exactly attains the desired divergence when Ï„=Î¸is the optimal value.
60J From mechanism-specific guarantees to dominating pairs
Our proposed optimal transport approach lets us derive mechanism-specific guarantees, which upper-
bound the hockey stick divergence between the distribution of M(x)andM(xâ€²)withxâ‰ƒXxâ€²via a
weighted sum5of mixture divergences, i.e.,
HÎ±(mx||mxâ€²)â‰¤KX
k=1w(x,xâ€²)
Î±,kÂ·HÎ±(p(x,xâ€²)
Î±,k||q(x,xâ€²)
Î±,k) (23)
Here, the w(x,xâ€²)
Î±,k are weights withPK
k=1w(x,xâ€²)
Î±,k= 1, which depend on the chosen coupling Î³
and are indexed by Î±â‰¥0,1â‰¤kâ‰¤K, and x, xâ€²âˆˆX. The p(x,xâ€²)
Î±,k andp(x,xâ€²)
Î±,K are densities of
mixture distributions P(x,xâ€²)
Î±,kandq(x,xâ€²)
Î±,K on the output space RD,6which are also indexed by Î±â‰¥0,
1â‰¤kâ‰¤K, and x, xâ€²âˆˆX.
Our goal is to construct dominating pairs from such bounds, so that we can then use them for privacy
accounting. For this discussion, recall the definition of dominating pairs:
Definition 2.3. A pair of distributions (P, Q)with densities (p, q)is a dominating pair for mechanism
Munder neighboring relation â‰ƒX, ifHÎ±(mx||mxâ€²)â‰¤HÎ±(p||q)for all xâ‰ƒXxâ€²and all Î±â‰¥0.
If the densities on the r.h.s. of Eq. (23) are constant in Î±,k,x, andxâ€², i.e.., p(x,xâ€²)
Î±,k= Ëœpandq(x,xâ€²)
Î±,k= Ëœq
with some densitities Ëœp,Ëœq, then we can immediately identify that the corresponding distributions
ËœPandËœQare a dominating pair. For instance, in Section 3.4 we could immediately determine that
the two Gaussian mixtures in Theorem 3.8 are a dominating pair for Poisson subsampling and the
â€œinsert- K+-remove- Kâˆ’â€ relation.
However, this is generally not the case. In the following, we discuss a three-step procedure that
let us (1) reduce the weighted sum of divergences in Eq. (23) to a single divergence (2) resolve
non-constancy in the dataset pairs (x, xâ€²), and (3) resolve non-constancy in the divergence order Î±.
Note that, depending on the considered setting, it may be possible to skip one or multiple of these
steps (like in our group privacy example).
J.1 Step 1: Eliminating weighted sums
Consider some fixed order Î±and fixed datasets xâ‰ƒXxâ€². Let us thus omit the corresponding indexes.
In this step, we construct a pair of distributions ËœP,ËœQwith densitities ËœpandËœqsuch that
KX
k=1wkÂ·HÎ±(pk||qk) =HÎ±(Ëœp||Ëœq).
To achieve this, we notice that there is no requirement for dominating pairs to be distributions on the
same space RD. Taking inspiration from hierarchical randomized smoothing [ 69], we thus construct
ËœpandËœqthat are mixtures of the pkandqkand additionally release their component indices:
Lemma J.1. Consider arbitrary densitities p1, . . . , p K, q1, . . . , q K:RDâ†’R+and arbitrary
weights w1, . . . , w Kâˆˆ[0,1]withPK
k=1wk= 1. Further let r:{1, . . . , K } â†’ [0,1]be the
probability mass function of Categorical( w1, . . . , w K). Define Ëœp,Ëœq:RDÃ— {1, . . . , K } â†’R+as
Ëœp(z, k) =pk(z)Â·r(k)andËœq(z, k) =qk(z)Â·r(k). Then, HÎ±(Ëœp||Ëœq) =PK
k=1wkÂ·HÎ±(pk||qk).
5assuming that the batch space is finite and discrete
6the arguments in this section also apply to the general problem setting from Appendix D, where we allow
arbitrary output spaces
61Proof. By the definition of hockey stick divergence and Ëœp,Ëœq, we have
HÎ±(Ëœp||Ëœq) =KX
k=1Z
RDmaxËœp(z, v)
Ëœq(z, v),0
Â·Ëœq(z, v) dz
=KX
k=1Z
RDmaxpk(z)Â·r(k)
qk(z)Â·r(k),0
Â·qk(z)Â·r(k) dz
=KX
k=1r(k)Â·Z
RDmaxpk(z)
qk(z),0
Â·qk(z) dz
=KX
k=1wkÂ·HÎ±(pk||qk).
Note that one could equivalently construct continuous distributions ËœPandËœQby defining the categori-
cal distribution as a transformation of a uniform distribution. Further note that the distribution of
privacy loss random variable log(Ëœp(Z)/Ëœq)withZâˆ¼ËœPis simply a weighted sum of the componentsâ€™
privacy loss distributions, which can be easily shown via law of total probability. This makes this
construction amenable to numerical privacy accounting.
J.2 Step 2: Resolving non-constancy in dataset pairs
After applying the construction from the previous step, we have bounds of the form HÎ±(mx||mxâ€²)â‰¤
HÎ±(Ëœp(x,xâ€²)
Î±||Ëœq(x,xâ€²)
Î±). If these bounds are not constant in the datasets x, xâ€²âˆˆX, we cannot simply
read off a dominating pair.
However, the ultimate goal behind identifying dominating pairs is to use them in a privacy accounting
method to algorithmically derive guarantees for all possible pair xâ‰ƒXxâ€²under composition.
Providing guarantees for all possible pair does not require that we derive guarantees for all pairs
simultaneously. To formalize this, recall that the neighboring relation â‰ƒXis a set of tuples from X2.
Lemma J.2. Consider any Î±â‰¥0, mechanism MandLdifferent neighboring relations â‰ƒ(1)
, . . . ,â‰ƒ(L)âŠ†X2. Assume that there are Lconstants c(1), . . . , c(L)such that HÎ±(mx||mxâ€²)â‰¤c(l)for
alllâˆˆ {1, . . . , L }and all xâ‰ƒ(l)xâ€². If the neighboring relations partition â‰ƒX, i.e.,â‰ƒX=SL
l=1â‰ƒ(l),
then
âˆ€xâ‰ƒXxâ€²:HÎ±(mx||mxâ€²)â‰¤max
lc(l).
Proof. Consider any xâ‰ƒXxâ€². Because the neighboring relations partition â‰ƒX, there must be some
lâˆ—such that xâ‰ƒ(lâˆ—)xâ€²and thus HÎ±(mx||mxâ€²)â‰¤c(lâˆ—)â‰¤max lc(l).
For the purposes of privacy accounting, it is thus sufficient to determine some sufficiently fine-grained
partition â‰ƒ(1), . . . ,â‰ƒ(L)such that for all lâˆˆ {1, . . . , L }and all Î±â‰¥0
âˆ€xâ‰ƒ(l)xâ€²:HÎ±(Ëœp(x,xâ€²)
Î±||Ëœq(x,xâ€²)
Î±) =HÎ±(Ë†p(l)
Î±||Ë†q(l)
Î±),
where Ë†pl
Î±andË†q(l)
Î±are families of densities indexed by partition index land divergence order Î±. One
can then perform privacy accounting independently for each of the relations.
This goal can always be achieved by having one atomic relation per possible pair of neighboring
elements. In practice, however, much more coarse-grained partitions may be sufficient. Zhu et al.
[10] applied this ansatz to the insertion/removal relation, by deriving dominating pairs for insertion
and removal separately. In our work, we partition the group insertion/removal relation for groups of
sizeKintoK+ 1different â€œinsert- K+-remove- Kâˆ’â€ relations with K++Kâˆ’=K.
62J.3 Step 3: Resolving non-constancy in divergence order
Consider some fixed partition index lâˆˆ {1, . . . , L }, which we omit in the following. We are now left
with a bound of the form âˆ€xâ‰ƒxâ€²:HÎ±(mx||mxâ€²)â‰¤HÎ±(Ë†pÎ±||Ë†qÎ±). If the families of densitities Ë†pÎ±,
Ë†qÎ±is constant in Î±, i.e.
âˆ€Î±:HÎ±(Ë†pÎ±||Ë†qÎ±) =HÎ±(Ë‡p||Ë‡q),
with some Ë‡p,Ë‡q, then the corresponding distributions Ë‡P,Ë‡Qare dominating pairs. Importantly, the
numeric value of the bound can vary with Î±. We just want the numeric value to be identical to the
divergence HÎ±of two specific distributions for all Î±.
If this is not the case, there are two options to construct a dominating pair. The first option uses a
characterization of dominating pairs as a function of privacy profiles. It was applied by Zhu et al.
[10] to determine dominating pairs for subsampling without replacement and the substitution relation.
The second option is enabled by our novel optimal transport and constrained optimization perspective
on subsampling.
J.3.1 Option 1: Convex conjugation of privacy profiles
The following result from [ 10] establishes a correspondence between privacy profiles and dominating
pairs, as well as providing a formula for constructing dominating pairs from privacy profiles:
Proposition J.3 (Zhu et al. [10]).For a given f:R+â†’R, there exists Ë‡P,Ë‡Qsuch that âˆ€Î±â‰¥0 :
H(Î±) =HÎ±(P, Q)if and only if fâˆˆ F where
F={f:R+â†’R|fis convex, decreasing, f(0) = 1 andf(x)â‰¥max{1âˆ’x,0}}. (24)
Moreoever, one can explicitly construct such PandQ:PhasCDF 1 +fâˆ—(xâˆ’1)in[0,1)and
Q= Uniform([0 ,1]), where fâˆ—is the convex conjugate of f.
Next, we show how to construct a function f(Î±)âˆˆ F from our families of densities Ë†qÎ±,Ë†qÎ±indexed
byÎ±that allows us to determine dominating pairs.
Lemma J.4. Consider two families of densities Ë†pÎ±,Ë†qÎ±indexed by Î±â‰¥0such that âˆ€xâ‰ƒxâ€²:
HÎ±(mx||mxâ€²)â‰¤HÎ±(Ë†pÎ±||Ë†qÎ±). Define the function f:R+â†’Rwith
f(Î±) = max
Î²â‰¥0HÎ±(Ë†pÎ²||Ë†qÎ²).
Then fis a valid privacy profile, i.e., fâˆˆ F withHdefined in Eq. (24).
Proof. For every Î²â‰¥0, letfÎ²(Î±) =HÎ±(Ë†pÎ²||Ë†qÎ²)be the privacy profile associated with a specific
pair from the considered families. All these functions are elements of Fas per the first part
of Proposition J.3. We thus know that each fÎ²is convex, decreasing, and has value 1atÎ±= 0.
Naturally, their maximum is also convex, decreasing, and has value 1atÎ±= 0. We further know that,
for all Î²â‰¥0,fÎ²(x)â‰¥max{1âˆ’x,0}. Thus, we have f(x)â‰¥fÎ²(x)â‰¥(1âˆ’x)for any Î²â‰¥0.
We can then invoke the second part of Proposition J.3 to define our dominating pair.
As mentioned earlier, Zhu et al. [10] applied this principle to subsampling without replacement by
taking a maximum over two different pairs of distributions. Lemma J.4 generalizes this principle to
larger families.
J.3.2 Option 2: Relaxation of cost function constraints
A novel solution is enabled by our proposed framework for deriving mechanism-specific subsampling
guarantees: Recall that the densitities Ë†pÎ±andË†qÎ±are the result of identifying worst-case mixture com-
ponents under pairwise batch distance constraints (Proposition 3.5). In Appendix L, we demonstrate
that it is possible to relax some of these constraints to obtain a single pair of densitities that bounds
HÎ±(mx||mxâ€²for all Î±and pairs of datasets xâ‰ƒxâ€². Specifically, we apply this principle to the
substitution relation and subsampling without replacement, as well as subsampling with replacement.
Due to the relaxation, these dominating pairs are not necessarily tight. They may however be easier
to use in practice than convex conjugation, which requires solving an optimization problem.
63K Recovering known dominating pairs
In this section, we demonstrate that known dominating pairs for the standard combinations of Poisson
subsampling under insertion/removal and subsampling without replacement under substitution, as
well as subsampling without replacement under insertion/removal, can also be derived via our
proposed optimal transport framework.
As before, our contribution are not the guarantees themselves. Our contribution is identifying that,
just like mechanism-agnostic guarantees for ADP and RDP, these guarantees can be derived by
defining an (optimal) coupling and then identifying worst-case mixture components under pairwise
batch distance constraints.
K.1 Poisson subsampling and insertion/removal
As discussed in Appendix J, it is beneficial to partition the insertion removal â‰ƒÂ±,Xinto two relations:
The insertion relation â‰ƒ+,X, where xâ‰ƒ+,Xxâ€²implies that there is some a /âˆˆxsuch that xâ€²=xâˆª{a}
and the removal relation â‰ƒâˆ’,X, where xâ‰ƒâˆ’,Ximplies that there is some aâˆˆxsuch that xâ€²=x\{a}.
While we could derive dominating pairs for both relations from scratch, we can use the following
result from [10] to reduce our workload:
Lemma K.1. If distributions (P, Q)are a dominating pair for mechanism Munder the insertion
relation â‰ƒ+, then (Q, P)are a dominating pair under the removal relation â‰ƒâˆ’.
Next, we use our proposed framework to derive the following guarantee in a very natural manner that
does not require reasoning about tradeoff functions and optimal testing rules (c.f. proof of Lemma 29
in [10]).
Proposition K.2 (Zhu et al. [10]).Consider a subsampled mechanism M=Bâ—¦S, where Sis
Poisson subsampling with subsampling rate râˆˆ[0,1]. assume that (P, Q)are a dominating pair
for base mechanism Bunder the batch insertion relation â‰ƒ+,Y. Then, (P,(1âˆ’r)P+rQ)are a
dominating pair for Munder the dataset insertion relation â‰ƒ+,Xand((1âˆ’r)P+rQ, Q )are a
dominating pair for Munder the dataset removal relation â‰ƒâˆ’,X.
Proof. We begin by deriving the dominating pair for the removal relation â‰ƒâˆ’,X. Consider an arbitrary
pairxâ‰ƒâˆ’xâ€²and an arbitrary Î±â‰¥0. Recall from our proof of the mechanism-agnostic Poisson
subsampling guarantee Proposition H.6 that we can construct a dY-compatible coupling to show that
HÎ±(mx||mxâ€²)â‰¤max
yHÎ±((1âˆ’r)by(1)
1+rby(1)
2||by(2)
1)
subject to yâˆˆY2+1,y(1)
1=y(2)
1,dY(y(1)
2, y(1)
1)â‰¤1,dY(y(1)
1, y(1)
2)â‰¤ âˆ .
The last constraint is because there is no sequence of deletions that will result in an insertion. It can
be ignored, meaning our optimization problem is equivalent to
max
yâ‰ƒâˆ’yâ€²HÎ±((1âˆ’r)byâ€²+rby||byâ€²).
Using the definition of hockey stick divergence,7this can be restated as
max
yâ‰ƒâˆ’yâ€²HÎ±((1âˆ’r)byâ€²+rby||byâ€²)
= max
yâ‰ƒâˆ’yâ€²Z
RDmax(1âˆ’r)byâ€²(z) +rby(z)
byâ€²(z)âˆ’Î±,0
Â·byâ€²(z) dz
= max
yâ‰ƒâˆ’yâ€²Z
RDmax
(1âˆ’r) +rby(z)
byâ€²(z)âˆ’Î±,0
Â·byâ€²(z) dz
= max
yâ‰ƒâˆ’yâ€²rÂ·Z
RDmaxby(z)
byâ€²(z)âˆ’
Î±âˆ’1âˆ’r
r
,0
Â·byâ€²(z) dz
7For simplicity, we assume that we have continuous-valued mechanisms, but the same proof strategy can
also be used for the more general definition of hockey stick divergence from Appendix D by calculating
d((1âˆ’r)PByâ€²+rPBy)/dPByâ€².
64We can now distinguish two cases, based on the value of Î±â€˜ =Î±âˆ’(1âˆ’r)/ r.
Case 1: Assume that Î±â€²<0. Then, the integrand is always non-negative, and the objective function
evaluates to a constant r(1âˆ’aâ€²). We can thus choose y, yâ€²arbitrarily, because
max
yâ‰ƒ+yâ€²HÎ±((1âˆ’r)byâ€²+rby||byâ€²) =r(1âˆ’aâ€²) =HÎ±((1âˆ’r)q+rp||q),
where pandqare the densities of dominating pair (P, Q).
Case 2: Assume that Î±â€²â‰¥0. Because (P, Q)is a dominating pair for the batch insertion relation
â‰ƒ+,Y, we know from Lemma K.1 that (Q, P)is a dominating pair for the batch removal relation
â‰ƒâˆ’,Y. Thus
max
yâ‰ƒâˆ’yâ€²HÎ±((1âˆ’r)byâ€²+rby||byâ€²)
= max
yâ‰ƒâˆ’yâ€²rÂ·HÎ±â€²(by||byâ€²)
â‰¤rÂ·HÎ±â€²(q||p)
=HÎ±((1âˆ’r)p+rq||q)
This shows that ((1âˆ’r)P+rQ, Q )is a dominating pair for the removal relation â‰ƒâˆ’,X. It then
follows from Lemma K.1 that (Q,(1âˆ’r)P+rQ)is a dominating pair for the insertion relation
â‰ƒ+,X.
Note that case 1 (but not case 2) of Theorem 11 in [ 10] states that ((1âˆ’r)Q+rP, P )were a
dominating pair for the removal relation. This does however appear to be a typographical error, since
the authors use the same symmetry argument (Lemma K.1) for their proof.
K.2 Subsampling without replacement and insertion/removal
The proof for the following result is virtually identical to the previous one. They only differ in the
coupling which we need to construct, which again highlights the generality and usefulness of our
proposed framework.
Proposition K.3 (Zhu et al. [10]).Assume a dataset space and batch space defined by XâŠ† {xâˆˆ
P(A)| |x| âˆˆ {N, Nâˆ’1},Y={yâŠ†x|xâˆˆX,|y|=q}, and finite set A, with q < N . Consider
a subsampled mechanism M=Bâ—¦S, where Sis subsampling without replacement with batch
sizeq, and define batch-to-dataset ratio w=q / N . assume that (P, Q)are a dominating pair for
base mechanism Bunder the batch substitution relation â‰ƒâˆ†,Y. Then, (P,(1âˆ’w)P+wQ)are a
dominating pair for Munder the dataset insertion relation â‰ƒ+,Xand((1âˆ’w)P+wQ, Q )are a
dominating pair for Munder the dataset removal relation â‰ƒâˆ’,X.
Proof. As in the previous proof, we begin with removal relation â‰ƒâˆ’,X. Consider an arbitrary pair
xâ‰ƒâˆ’xâ€²and an arbitrary Î±â‰¥0. Recall from our novel proof of the mechanism-agnostic RDP
guarantee for subsampling without replacement and insertion/removal (see Theorem I.2) that we can
construct a dY-compatible coupling to show that
HÎ±(mx||mxâ€²)â‰¤max
yHÎ±((1âˆ’w)by(1)
1+wby(1)
2||by(2)
1)
subject to yâˆˆY2+1,y(1)
1=y(2)
1,dY(y(1)
2, y(1)
1)â‰¤1,dY(y(1)
1, y(1)
2)â‰¤1.
By definition of the induced distance, this optimization problem is equivalent to
max
yâ‰ƒâˆ†yâ€²HÎ±((1âˆ’r)byâ€²+rby||byâ€²).
We can now go through exactly the same steps as in our derivation of Proposition K.2, replacing
every occurrence of â€œ â‰ƒâˆ’â€ with â€œ â‰ƒâ€²â€²
âˆ†, to conclude our proof.
K.3 Subsampling without replacement and substitution
In the case of subsampling without replacement and substitution (as well as Poisson subsampling
and insertion/removal without partitioning of the neighboring relation) we do not need to provide a
65new proof. This is because Zhu et al. [10] prove this result by invoking a tight mechanism-agnostic
subsampling guarantee derived by Balle et al. [15] and then applying advanced joint convexity in
reverse order (see proof of Proposition 30 in [10]).
As we discussed in Appendix G.2 and formalized in Theorem G.2, any guarantee that is derived via
the framework from [ 15] can be equivalently derived by defining a (potentially suboptimal) coupling
between multiple conditional subsampling distributions. Thus, we know that the guarantee could
have equivalently been derived via our proposed framework
Proposition K.4 (Zhu et al. [10]).Consider a subsampled mechanism M=Bâ—¦S, where Sis
subsampling without replacement with batch size qâˆˆN. Assume that (P, Q)are a dominating pair
for base mechanism Bunder the batch substitution relation â‰ƒâˆ†,Y. Consider arbitrary xâ‰ƒâˆ†,Xxâ€²of
sizeNand define batch-to-dataset ratio w=q / N . Then
HÎ±(mx||mxâ€²)â‰¤HÎ±((1âˆ’w)q+wp||q) forÎ±â‰¥1
HÎ±(p,(1âˆ’w)p+wq||q)for0< Î± < 1
Note that this result does not immediately specify a dominating pair. However, as shown in [ 10], one
can construct a dominating pair via convex conjugation (recall Appendix J.3.1 and Proposition J.3).
66L Novel results for dominating pairs
In the following, we formally derive dominating pairs for subsampling with replacement under
the substitution relation. We also derive an alternative dominating pair for subsampling without
replacement under substitution which does not require convex conjugation (c.f. Appendix K.3). These
results are novel in three respects.
Novel contribution 1 â€“ Formal guarantees. Using the dominating pairs we shall shortly derive
for privacy accounting was already proposed in [ 8]. However, the authors did not provide a formal
proof for why they should lead to valid privacy guarantees. They only proved that if two pairs of
multivariate Gaussians with colinear means were to be a dominating pair8, then one could use a
change of variable and marginalization to reduce them to a univariate dominating pair (see Appendix
B.1 in [ 8]). They did not prove why this assumption should hold. A formal proof of this assumption
is now enabled by the analysis we conducted for solving the group privacy optimization problem
in Theorem M.6 (see Appendix O).
Novel contribution 2 â€“ Other base mechanisms. The generality of our solution to the group
privacy optimization problem in Theorem M.6 lets us not only derive dominating pairs for Gaussian
mechanisms, but also for Laplace mechanisms and randomized response.
Novel contribution 3 â€“ Resolving non-constancy via constraint relaxation. Perhaps most impor-
tantly, our proposed framework offers a novel perspective on an issue encountered when analyzing
subsampling without replacement: While one can derive a tight mechanism-specific guarantee, this
guarantee depends on different mixture distributions for Î± <1andÎ±â‰¥1(see Proposition K.4 [ 10]
and our more general discussion in Appendix J.3). Thus, one cannot simply read off a dominating
pair from this mechanism-specific guarantee, as we did with our group privacy bounds. However,
as we shall demonstrate, this issue no longer appears when we relax some of the batch distance
constraints in our proposed cost function bound from Proposition 3.5. This suggests that this could
be a more general approach for addressing this non-constancy issue. While the resultant dominating
pairs are not necessarily tight, they may be easier to use in practice than the convex conjugation
construction from [10].
L.1 Subsampling without replacement and substitution
We begin with subsampling without replacement to demonstrate the potential benefit of relaxing
batch distance constraints to determine tractable dominating pairs.
Theorem L.1. LetM=Bâ—¦S, where Sis subsampling without replacement with batch size q, andB
is the Gaussian mechanism h+Vwithh:Yâ†’RDandVâˆ¼ N(0, Ïƒ2ID). Define the â„“2-sensitivity
L2= max yâ‰ƒâˆ†,Yyâ€²||f(y)âˆ’f(yâ€²)||2, where â‰ƒâˆ†is the substitution relation. Consider an arbitrary
dataset size NâˆˆNand define batch-to-dataset ratio w=q
N. Then, for any pair xâ‰ƒâˆ†,Xxâ€²of size
N,
HÎ±(mx||mxâ€²)â‰¤HÎ±
f(1)
1Â·(1âˆ’w) +f(1)
2Â·w||f(2)
1Â·(1âˆ’w) +f(2)
2Â·w
, (25)
with univariate normal densities f(1)
i=N(Â· |(iâˆ’1), Ïƒ / L 2),f(2)
j=N(Â· | âˆ’(jâˆ’1), Ïƒ / L 2).
Proof. Recall from our proof of the mechanism-agnostic subsampling without replacement guaran-
tee Proposition H.4 that we can construct a dY-compatible coupling to show that
HÎ±(mx||mxâ€²)â‰¤max
yHÎ±((1âˆ’w)by(1)
1+wby(1)
2||(1âˆ’w)by(2)
1+wby(2)
2)
subject to yâˆˆY2+1,y(1)
1=y(2)
1,dY(y(1)
1, y(1)
2)â‰¤1,dY(y(1)
1, y(2)
2)â‰¤1,dY(y(2)
1, y(2)
1)â‰¤1.
Next, we relax the constraint between the two components that contain a substituted element:
dY(y(2)
1, y(2)
1)â‰¤2.
We now observe that this optimization problem is identical to the optimization problem from our
group privacy bound (Theorem 3.7) with one insertion ( K+= 1), one deletion (Kâˆ’= 1) , and
subsampling rate r=w. The result thus follows immediately from our mechanism-specific group
privacy amplification bound for Gaussian mechanism (Theorem 3.8).
8Technically, the notion of dominating pairs[ 10] had not been introduced at this point, which is why the
authors discuss a vaguer notion of â€œworst-case distributionsâ€, similar to other early work on numerical accounting.
67We can generalize this result to other mechanisms by using group privacy amplification bounds for
Laplace mechanisms (Theorem M.2) and randomized response mechanisms (Theorem M.2).
We see that the upper bound in Eq. (25) depends on the same pair of mixture distributions for
allÎ±â‰¥0. Thus, this pair of mixture distributions is a dominating pair for subsampling without
replacement under substitution.
L.2 Subsampling with replacement and substitution
Next, we provide the first formal derivation of dominating pairs for subsampling with replacement.
In subsampling with replacement, a single element may appear in a batch multiple times. To formalize
this, we assume a dataset space XâŠ† P(A)that is composed of sets (not multisets) of elements from
an underlying set A. We further assume a batch space YâŠ† P multi(A)that is composed of multisets
of elements from A. Given some yâˆˆY, we write Î¾y(a)for the number of times that aappears in y.
We further define the support of batch yâˆˆYassupp( y) ={aâˆˆY|Î¾y(a)â‰¥0}.
Definition L.2. Subsampling with replacement with batch size qhas probability mass function
sx(y) = |x|+qâˆ’1
qâˆ’1for batches yâˆˆYwithsupp( y)âŠ†xand|y|=q.
Given this subsampling scheme, we can use optimal transport to derive the following constrained
optimization problem, which we shall then relax and solve:
Theorem L.3. LetM=Bâ—¦S, where Sis subsampling without replacement with batch size q, andB
is the Gaussian mechanism h+Vwithh:Yâ†’RDandVâˆ¼ N(0, Ïƒ2ID). Define the â„“2-sensitivity
L2= max yâ‰ƒâˆ†,Yyâ€²||f(y)âˆ’f(yâ€²)||2, where â‰ƒâˆ†is the substitution relation. Consider an arbitrary
dataset size NâˆˆNand define batch-to-dataset ratio w=q
N. Then, for any pair xâ‰ƒâˆ†,Xxâ€²of size
N,
HÎ±(mx||mxâ€²)â‰¤max
yHÎ±ï£«
ï£­q+1X
i=1by(1)
iÂ·wi||q+1X
j=1by(2)
iÂ·wjï£¶
ï£¸,
subject to yâˆˆY(q+1)+( q+1),âˆ€l, t, u :dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|,âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤max{t, u},
and with weights wi= 1
NiÂ· 
1âˆ’1
Nqâˆ’i.
Proof. By definition of the substitution relation â‰ƒâˆ†there is some aâˆˆxwitha /âˆˆxâ€²and some
aâ€²âˆˆxâ€²withaâ€²/âˆˆxsuch that xâ€²=x\ {a} âˆª {aâ€²}.
To simplify indexing, we define zero-based indexed events A0, . . . , A qandE0, . . . , E q. We define
Aito be the event that ais sampled itimes, i.e., Î¾y(a) =i. We define Ejto be the event that aâ€²is
sampled jtimes, i.e., Î¾y(aâ€²) =j.
By definition subsampling with replacement, we have PSx(Ai) =wi, and PSx(Ej) =wjwith
weights wi= 1
NiÂ· 
1âˆ’1
Nqâˆ’i.
For dataset size N, we have
sx(y|Ai) =( (Nâˆ’1)+(qâˆ’i)âˆ’1
qâˆ’iâˆ’1ifsupp( y)âŠ†xâˆ§Î¾y(a) =i
0 otherwise
=(
(1
Nâˆ’1)qâˆ’i(qâˆ’i)!Q
ËœaÌ¸=a1
Î¾y(Ëœa)!ifsupp( y)âŠ†xâˆ§Î¾y(a) =i
0 otherwise
and
sxâ€²(y|Ej) =( (Nâˆ’1)+(qâˆ’i)âˆ’1
qâˆ’iâˆ’1ifsupp( y)âŠ†xâ€²âˆ§Î¾y(aâ€²) =j
0 otherwise
=(
(1
Nâˆ’1)qâˆ’i(qâˆ’i)!Q
ËœaÌ¸=aâ€²1
Î¾y(Ëœa)!ifsupp( y)âŠ†xâ€²âˆ§Î¾y(aâ€²) =j
0 otherwise
Note that sx(y|A0) =sxâ€²(y|E0). Further note that sx(y|Aq) = 1[supp( y) ={a} âˆ§ |y|=q]
andsxâ€²(y|Eq) = 1[supp( y) ={aâ€²} âˆ§ |y|=q].
68Coupling. We now define a coupling Î³:Y(q+1)+( q+1)â†’R+that corresponds to the following
generative process: We first generate y(1)
qby constructing a batch of size qwhose elements are all
a. Beginning at iâ†q, we then iteratively generate y(1)
iâˆ’1from y(1)
iâˆ’1by replacing one instance of a
with an element Ëœaâˆˆx\athat is sampled uniformly at random. Finally, we construct the y(2)
jby
replacing every occurence of ainy(1)
jwithaâ€². More formally,
Î³(y(1),y(2)) =(
1
Nâˆ’1q
ify(1),y(2)fulfills Condition L.4
0 otherwise.
Condition L.4. A tuple y(1)âˆˆYq+1,y(q+1)âˆˆYK++1fulfills this condition when Î¾y(1)
q(a) =
Î¾y(2)
q(aâ€²) =qandâˆ€i,âˆƒËœÎ±âˆˆx\ {a}:y(1)
iâˆ’1=y(1)
iâˆ’1\ {a} âˆª { Ëœa}andâˆ€i,âˆ€Ëœa /âˆˆ {a, aâ€²}) :Î¾y(1)
q(Ëœa) =
Î¾y(2)
q(Ëœa)andÎ¾y(1)
q(a) =Î¾y(2)
q(aâ€²).
Validity. To verify the validity of this coupling, consider an arbitrary iand an arbitrary ysuch that
Î³(y(1),y(2))>0. We know that there are (qâˆ’i)!Q
ËœaÌ¸=a1
Î¾y(Ëœa)!sequences of y(1)
q, y(1)
qâˆ’1, . . . , y(1)
i
that could have generated y(1)
i. We further know that there are
1
Nâˆ’1i
possible sequences of
y(1)
iâˆ’1, . . . , y(1)
0that can be generated from y(1)
i. Ally(2)
jare deterministically determined by y(1)
j. We
thus haveX
yâˆˆY(q+1)+( q+1)1[y(1)
i=y]Î³(y(1),y(2))
=(qâˆ’i)!Y
ËœaÌ¸=a1
Î¾y(Ëœa)!1
Nâˆ’1i1
Nâˆ’1q
=1
Nâˆ’1qâˆ’i
(qâˆ’i)!Y
ËœaÌ¸=a1
Î¾y(Ëœa)!
=sx(y|Ai).
The proof for the y(2)
jis analogous.
Compatibility. Finally, it can be easily shown that Î³is adY-compatible coupling (recall Appendix F).
Whenever Î³(y)>0, then
âˆ€1â‰¤iâ‰¤Kâˆ’:dY(y(1)
0, y(1)
i) =dY
y(1)
0,supp( sx(Â· |Ai))
=i,
âˆ€0â‰¤jâ‰¤K+:dY(y(1)
0, y(2)
i) =dY
y(1)
0,supp( sxâ€²(Â· |Ej))
=j
because we generate the y(1)
ifrom y(1)
0by substituting exactly ielements, and construct the y(2)
jby
substituting exactly jelements. Similarly, we have for the pairwise distances that do not involve y(1)
0
âˆ€0< t < u â‰¤Kâˆ’:dY(y(1)
t, y(1)
u) =dY(supp( sx(Â· |At),supp( sx(Â· |Au))) =|tâˆ’u|,
âˆ€0< t < u â‰¤Kâˆ’:dY(y(2)
t, y(2)
u) =dY(supp( sxâ€²(Â· |Et),supp( sxâ€²(Â· |Eu))) =|tâˆ’u|.
âˆ€0â‰¤t < uâ‰¤Kâˆ’:dY(y(1)
t, y(2)
u) =dY(supp( sx(Â· |At),supp( sxâ€²(Â· |Eu))) = max {t, u}.
The last equality holds for tâ‰¤ubecause to construct y(2)
ufrom y(1)
twe need to substitute all
occurrences of awithaâ€²and then substitute uâˆ’tadditional elements with aâ€²It also holds for t > u ,
because then we need to replace uoccurences of awithaâ€²and then replace another tâˆ’uoccurences
with values other than aâ€².
Applying Corollary F.7, with PSx(Ai) =wi,PSxâ€²(Ej) =wj, shows that
Finally, we can relax and solve Theorem L.3 for specific mechanisms, such as the Gaussian mecha-
nism:
69Theorem L.5. LetM=Bâ—¦S, where Sis subsampling with replacement with batch size q, and Bis
the Gaussian mechanism h+Vwithh:Yâ†’RDandVâˆ¼ N(0, Ïƒ2ID). Define the â„“2-sensitivity
L2= max yâ‰ƒâˆ†,Yyâ€²||f(y)âˆ’f(yâ€²)||2, where â‰ƒâˆ†is the substitution relation. Consider an arbitrary
dataset size NâˆˆN. Then, for any pair xâ‰ƒâˆ†,Xxâ€²of size N,
HÎ±(mx||mxâ€²)â‰¤HÎ±ï£«
ï£­qX
i=1f(1)
iÂ·wi||qX
j=1f(2)
jÂ·wjï£¶
ï£¸, (26)
with univariate normal densities f(1)
i=N(Â· |(iâˆ’1), Ïƒ / L 2),f(2)
j=N(Â· | âˆ’(jâˆ’1), Ïƒ / L 2), and
weights wi= 1
NqÂ· 
1âˆ’1
Nqâˆ’i.
Proof. We first relax the optimization problem in Theorem L.3 by replacing the last constraint
âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤max{t, u}with a new constraint âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤t+u.
This leaves us with
max
yHÎ±ï£«
ï£­q+1X
i=1by(1)
iÂ·wi||q+1X
j=1by(2)
iÂ·wjï£¶
ï£¸,
subject to yâˆˆY(q+1)+( q+1),âˆ€l, t, u :dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|,âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤t+u, and
with weights wi= 1
NiÂ· 
1âˆ’1
Nqâˆ’i.
We now observe that this optimization problem is identical to the optimization problem from our
group privacy bound (Theorem 3.7) with qinsertions ( K+=q),qdeletions (Kâˆ’=q), and a
different set of weights. The result thus follows immediately from our solution to the group privacy
amplification problem from Theorem 3.7, which did not make any specific assumptions about the
weights.
We see that the upper bound in Eq. (25) depends on the same pair of mixture distributions for
allÎ±â‰¥0. Thus, this pair of mixture distributions is a dominating pair for subsampling without
replacement under substitution.
70M Tight mechanism-specific group privacy amplification
In the following, we first prove our generic group privacy amplification guarantee for Poisson
subsampling and insertion/removal, which gives us a constrained optimization problem whose optimal
value bounds Î¨Î±(mx||mxâ€². We then solve this constrained optimization problem for Gaussian,
Laplace, and randomized response mechanisms. After that, we prove the mechanism-specific tightness
for the resultant guarantees. Finally, we discuss how to evaluate these guarantees numerically.
M.1 Proof of Theorem 3.7
Theorem 3.7. LetM=Bâ—¦S, where Sis Poisson subsampling with rate r. Letâ‰ƒYbe the
insertion/removal batch relation â‰ƒÂ±,Y. Then, for all xâ‰ƒK+,Kâˆ’,Xxâ€²,Î¨Î±(mx||mxâ€²)is l.e.q.
max
yÎ¨Î±ï£«
ï£­Kâˆ’+1X
i=1by(1)
iÂ·Binom( iâˆ’1|Kâˆ’, r)||K++1X
j=1by(2)
jÂ·Binom( jâˆ’1|K+, r)ï£¶
ï£¸,(4)
subject to constraints yâˆˆYKâˆ’+K++2, as well as âˆ€lâˆˆ {1,2},âˆ€t, u:dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|, and
âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤(tâˆ’1) + ( uâˆ’1).
Proof. By definition of the â€œinsert K+, remove Kâˆ’â€œ relation â‰ƒK+,Kâˆ’,Xthere is some inserted set
g+âŠ†xâ€²of size K+withg+âˆ©x=âˆ…and some removed set gâˆ’âŠ†xof size Kâˆ’withgâˆ’âˆ©xâ€²=âˆ…
such that xâ€²=x\gâˆ’âˆªg+.
To simplify indexing, we define zero-based indexed events A0, . . . , A Kâˆ’andE0, . . . , E K+. We
define Aito be the event that iremoved elements are sampled, i.e. |yâˆ©gâˆ’|=i. We define Ejto be
the event that jinserted elements are sampled, i.e. |yâˆ©g+|=j.
By definition of Poisson subsampling, we have PSx(Ai) = Binom( i|Kâˆ’, r), and PSx(Ej) =
Binom( j|K+, r). We further have
sx(y|Ai) =r|y|(1âˆ’r)|x|âˆ’|y|Â·Binom( i|Kâˆ’, r)âˆ’1ifyâŠ†xâˆ§ |gâˆ’âˆ©y|=i
0 otherwise
=(
r|y|âˆ’i(1âˆ’r)|x|âˆ’|y|âˆ’(Kâˆ’âˆ’i)Â· Kâˆ’
iâˆ’1ifyâŠ†xâˆ§ |gâˆ’âˆ©y|=i
0 otherwise
and
sxâ€²(y|Ej) =r|y|(1âˆ’r)|x|âˆ’|y|Â·Binom( i|K+, r)âˆ’1ifyâŠ†xâ€²âˆ§ |g+âˆ©y|=i
0 otherwise
=(
r|y|âˆ’j(1âˆ’r)|x|âˆ’|y|âˆ’(K+âˆ’j)Â· K+
jâˆ’1ifyâŠ†xâ€²âˆ§ |g+âˆ©y|=i
0 otherwise
Note that sxâ€²(y|E0) =sx(y|A0).
Coupling. We now define a coupling Î³:Y(K++1)+( Kâˆ’+1)â†’R+that corresponds to the following
generative process: We first generate y(1)
0by sampling a batch that does not contain any inserted or
removed elements via Poisson subsampling from x\gâˆ’. We then let y(2)
0â†y(1)
0. Finally, we sample
uniformly at random an order in which we include removed elements from gâˆ’and inserted elements
from g+to iteratively construct batches from (Ai)i>1and(Ej)j>1, respectively. More formally,
Î³(y(1),y(2)) =(
sx(y(1)
0|A0)Â·(K+!Â·Kâˆ’!)âˆ’1ify(1),y(2)fulfills Condition M.1
0 otherwise.
Condition M.1. A tuple y(1)âˆˆYKâˆ’+1,y(2)âˆˆYK++1fulfills this condition when y(1)
0=y(2)
0,
andâˆ€i:âˆƒaâˆ’âˆˆgâˆ’\y(1)
i:y(1)
i+1=y(1)
iâˆª {aâˆ’}, andâˆ€j:âˆƒa+âˆˆg+\y(2)
j:y(2)
j+1=y(2)
jâˆª {aâˆ’}.
71Validity. We can verify the validity of this coupling as follows:
ForA0and every ywithsx(y|A0)>0, there are exactly K+!Â·Kâˆ’!combinations with y(1)
0=y
for which Î³(y)>0. We thus haveP
yâˆˆYK++Kâˆ’+2 1[y(1)
0=y]Î³(v) =sx(y|A0).The proof for E0
is analogous.
Next, consider an arbitrary Aiwith0< iâ‰¤Kâˆ’. For every ywithsx(y|A0)>0, there are exactly
i!Â·(Kâˆ’âˆ’i)!Â·K+!combinations with y(1)
i=yfor which Î³(y)>0, because there are (a) i!orders
in which the removed elements leading up to icould have been sampled (b) (Kâˆ’âˆ’i!)permutations
for the remaining removed elements (c) K+!permutations for the inserted elements that are only
relevant for y(2). We thus haveX
yâˆˆYK++Kâˆ’+21[y(1)
i=y]Î³(y)
= (i!Â·(Kâˆ’âˆ’i)!Â·K+!)Â·sx(y(1)
0|A0)Â·(K+!Â·Kâˆ’!)âˆ’1
= (i!Â·(Kâˆ’âˆ’i)!Â·K+!)Â·r|y(1)
0|(1âˆ’r)|x|âˆ’|y(1)
0|âˆ’(Kâˆ’)Â·(K+!Â·Kâˆ’!)âˆ’1
=i!(Kâˆ’âˆ’i)!Â·r|y(1)
0|(1âˆ’r)|x|âˆ’|y(1)
0|âˆ’(Kâˆ’)Â·(Kâˆ’!)âˆ’1
=r|y(1)
0|(1âˆ’r)|x|âˆ’|y(1)
0|âˆ’(Kâˆ’)Â·Kâˆ’
iâˆ’1
=r|y|âˆ’i(1âˆ’r)|x|âˆ’|y|âˆ’(Kâˆ’âˆ’i)Â·Kâˆ’
iâˆ’1
For the last step, we have used that y(1)
0contains 0of the removed elements from gâˆ’, whereas y
contains iof the removed elements.
The proof for Ejwith0< jâ‰¤K+is analogous.
Compatibility. Finally, it can be easily shown that Î³is adY-compatible coupling (recall Appendix F).
Whenever Î³(y)>0, then
âˆ€1â‰¤iâ‰¤Kâˆ’:dY(y(1)
0, y(1)
i) =dY
y(1)
0,supp( sx(Â· |Ai))
=i,
âˆ€0â‰¤jâ‰¤K+:dY(y(1)
0, y(2)
i) =dY
y(1)
0,supp( sxâ€²(Â· |Ej))
=j
because we generate the y(1)
ifrom y(1)
0by inserting exactly ielements, and construct the y(2)
jby
inserting exactly jelements. Similarly, we have for the pairwise distances that do not involve y(1)
0
âˆ€0< t < u â‰¤Kâˆ’:dY(y(1)
t, y(1)
u) =dY(supp( sx(Â· |At),supp( sx(Â· |Au))) =|tâˆ’u|,
âˆ€0< t < u â‰¤Kâˆ’:dY(y(2)
t, y(2)
u) =dY(supp( sxâ€²(Â· |Et),supp( sxâ€²(Â· |Eu))) =|tâˆ’u|.
âˆ€0â‰¤t < uâ‰¤Kâˆ’:dY(y(1)
t, y(2)
u) =dY(supp( sx(Â· |At),supp( sxâ€²(Â· |Eu))) = t+u.
The last equality holds because to construct y(2)
ufromy(1)
twe need to remove telements and insert u
elements.
The result then follows from Corollary F.7, PSx(Ai) = Binom( i|Kâˆ’, r),PSxâ€²(Ej) = Binom( j|
K+, r), and reverting back to one-based indexing.
M.2 Instantiations
Next, we can solve the derived optimization problem. In this section, we only provide proof sketches
in which we reduce the optimization problem over batches to an optimization problem over the
means of multiple Gaussian, Laplace, or Bernoulli random variables. Because solving these problems
requires some further exposition, we then refer the reader to Appendix O.
M.2.1 Gaussian mechanism guarantee
Theorem 3.8. LetM=Bâ—¦S, where Sis Poisson subsampling with rate r, and Bis the Gaussian
mechanism h+Vwithh:Yâ†’RDandVâˆ¼ N (0, Ïƒ2ID). Define the â„“2-sensitivity L2=
72max yâ‰ƒÂ±,Yyâ€²||f(y)âˆ’f(yâ€²)||2. Then for all xâ‰ƒK+,Kâˆ’,Xxâ€²,Î¨Î±(mx||mxâ€²)is l.e.q.
Î¨Î±ï£«
ï£­Kâˆ’+1X
i=1f(1)
iÂ·Binom( iâˆ’1|Kâˆ’, r)||K++1X
j=1f(2)
jÂ·Binom( jâˆ’1|K+, r)ï£¶
ï£¸,
with univariate normal densities f(1)
i=N(Â· |(iâˆ’1), Ïƒ / L 2),f(2)
j=N(Â· | âˆ’(jâˆ’1), Ïƒ / L 2).
Proof sketch. Recall from Theorem 3.7 that we need to solve the optimization problem
max
yÎ¨Î±ï£«
ï£­Kâˆ’+1X
i=1w(1)
iÂ·by(1)
i||K++1X
j=1w(2)
jÂ·by(2)
jï£¶
ï£¸,
subject to yâˆˆY(K++1)+( Kâˆ’+1),âˆ€l, t, u :dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|,âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤(tâˆ’1) +
(uâˆ’1), and with w(1)
i= Binom( iâˆ’1|Kâˆ’, r),w(2)
j= Binom( jâˆ’1|K+, r).
LetÂµ(1)
i=h(y(1)
i)andÂµ(2)
j=h(y(2)
j). Since we do not have any additional information about
hbeyond its â„“2-sensitivity, we have to make the worst-case assumption that the Âµ(1)
i,Âµ(2)
jare
arbitrary vectors constrained by âˆ€l, t, u :||Âµ(l)
tâˆ’Âµ(l)
u||2â‰¤L2|tâˆ’u|,âˆ€t, u:||Âµ(1)
tâˆ’Âµ(2)
u||2â‰¤
L2((tâˆ’1) + ( uâˆ’1)),
Thus, the optimization problem is equivalent to
max
Âµ(1),Âµ(2)ï£«
ï£­Î¨Î±(Kâˆ’+1X
i=1w(1)
iN(Â· |Âµ(1)
i, Ïƒ2ID/ L2
2)||K++1X
j=1w(2)
jN(Â· |Âµ(2)
j, Ïƒ2ID/ L2
2)ï£¶
ï£¸
subject to âˆ€l, t, u :||Âµ(l)
tâˆ’Âµ(l)
u||2â‰¤ |tâˆ’u|,âˆ€t, u:||Âµ(1)
tâˆ’Âµ(2)
u||2â‰¤(tâˆ’1) + ( uâˆ’1).
In Appendix O, we rigorously prove that the maximum is attained by co-linear, equidistant means that
fulfill these distance constraints exactly. Thus, we can perform a coordinate transformation such that
Âµ1=0andÂµ(1)
i= (iâˆ’1)e1andÂµ(2)
j=âˆ’(jâˆ’1)e1with first-component indicator vector e1âˆˆRD.
Since the likelihood ratio in Î¨Î±is Gaussian with zero mean in all but the first dimension, we can
marginalize all other dimensions out to obtain our one-dimensional result (cf. Appendix O).
M.2.2 Laplace mechanism guarantee
Theorem M.2. LetM=Bâ—¦S, where Sis Poisson subsampling with rate r, and Bis the Laplacian
mechanism h+Vwithh:Yâ†’RDandVâˆ¼Lap(0, Ïƒ2ID), with location 0âˆˆRDand diagonal
scale matrix Î»IDâˆˆRDÃ—D
+ , which adds independent Laplacian noise to each dimension. Define the
â„“1-sensitivity L1= max yâ‰ƒÂ±,Yyâ€²||f(y)âˆ’f(yâ€²)||1. Then, for all xâ‰ƒK+,Kâˆ’,Xxâ€²,Î¨Î±(mx||mxâ€²)is
l.e.q.
Î¨Î±ï£«
ï£­Kâˆ’+1X
i=1f(1)
iÂ·Binom( iâˆ’1|Kâˆ’, r)||K++1X
j=1f(2)
jÂ·Binom( jâˆ’1|K+, r)ï£¶
ï£¸,
with univariate Laplace densities f(1)
i= Lap( Â· |(iâˆ’1), Î» / L 1),f(2)
j= Lap( Â· | âˆ’(jâˆ’1), Î» / L 1).
Proof sketch. Recall from Theorem 3.7 that we need to solve the optimization problem
Î¨Î±ï£«
ï£­Kâˆ’+1X
i=1by(1)
iÂ·w(1)
i||K++1X
j=1by(2)
jÂ·w(2)
jï£¶
ï£¸,
subject to yâˆˆYK++Kâˆ’,âˆ€l, t, u :dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|,âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤(tâˆ’1) + ( uâˆ’1),
and with w(1)
i= Binom( iâˆ’1|Kâˆ’, r),w(2)
j= Binom( jâˆ’1|K+, r).
73LetÂµ(1)
i=h(y(1)
i)andÂµ(2)
j=h(y(2)
j). Since we do not have any additional information about
hbeyond its â„“1-sensitivity, we have to make the worst-case assumption that the Âµ(1)
i,Âµ(2)
jare
arbitrary vectors constrained by âˆ€l, t, u :||Âµ(l)
tâˆ’Âµ(l)
u||1â‰¤L1|tâˆ’u|,âˆ€t, u:||Âµ(1)
tâˆ’Âµ(2)
u||1â‰¤
L1((tâˆ’1) + ( uâˆ’1)),
Thus, the optimization problem is equivalent to.
max
Âµ(1),Âµ(2)ï£«
ï£­Î¨Î±(Kâˆ’+1X
i=1Lap(Â· |Âµ(1)
i, Î»ID/ L1)Â·w(1)
i||K++1X
j=1Lap(Â· |Âµ(2)
j, Î»ID/ L1)Â·w(2)
jï£¶
ï£¸
subject to âˆ€l, t, u :||Âµ(l)
tâˆ’Âµ(l)
u||1â‰¤ |tâˆ’u|,âˆ€t, u:||Âµ(1)
tâˆ’Âµ(2)
u||1â‰¤(tâˆ’1) + ( uâˆ’1).
In Appendix O, we rigorously show that the maximum is attained by collinear, equidistant vectors
along a single coordinate axis that leave no slack on the pairwise distance constraints. This then
allows us to marginalize out all remaining dimensions to obtain our guarantee in terms of univariate
Laplace densitities (see Appendix O).
M.2.3 Randomized response mechanism guarantee
Theorem M.3. LetBbe the randomized response mechanism |hâˆ’(1âˆ’V)|withh:Yâ†’
{0,1},Vâˆ¼Bernoulli( Î¸), and true response probability Î¸âˆˆ[0,1]. Let M=Bâ—¦Sbe the
corresponding subsampled mechanism, where Sis Poisson subsampling with rate r. Finally, let â‰ƒY
be the insertion/removal relation â‰ƒÂ±,Y. Then, for all xâ‰ƒK+,Kâˆ’,Xxâ€²,Î¨Î±(mx||mxâ€²)is l.e.q.
max
Ï„Î¨Î±((1âˆ’w(1))Bern( Â· |Î¸) +w(1)Bern(Â· |Ï„)||(1âˆ’w(2))Bern( Â· |Î¸) +w(2)Bern(Â· |1âˆ’Ï„))
subject to Ï„âˆˆ {Î¸,1âˆ’Î¸}, and with w(1)= 1âˆ’(1âˆ’r)Kâˆ’andw(2)= 1âˆ’(1âˆ’r)K+.
Proof sketch. Recall from Theorem 3.7 that we need to solve the optimization problem
Î¨Î±ï£«
ï£­Kâˆ’+1X
i=1by(1)
iÂ·w(1)
i||K++1X
j=1by(2)
jÂ·w(2)
jï£¶
ï£¸,
subject to yâˆˆYK++Kâˆ’,âˆ€l, t, u :dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|,âˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤(tâˆ’1) + ( uâˆ’1),
and with w(1)
i= Binom( iâˆ’1|Kâˆ’, r),w(2)
j= Binom( jâˆ’1|K+, r).
LetÂµ(1)
i=h(y(1)
i)andÂµ(2)
j=h(y(2)
j). Since we do not have any additional information about h
beyond it mapping to {0,1}, we have to make the worst-case assumption that the Âµ(1)
i, Âµ(2)
jare only
constrained by Âµ(1)
2=Âµ(2)
1.
Thus, the optimization problem is equivalent to.
max
Âµ(1),Âµ(2)ï£«
ï£­Î¨Î±(Kâˆ’+1X
i=1Bern(Â· |Âµ(1)
i)Â·w(1)
i||K++1X
j=1Bern(Â· |Âµ(2)
j)Â·w(2)
jï£¶
ï£¸
subject to Âµ(1)
1=Âµ(2)
1.
In Appendix O, we rigorously show that the maximum is attained whenever all Âµ(1)
iwithi >1are
simultaneously set to either 0or1, and all Âµ(2)
jwithj >1are set to the opposite value.
Note that this guarantee can be evaluated in O(1): We need to iterate over two possible values of Ï„,
and evaluating the corresponding Î¨Î±requires summing over two values zâˆˆ {0,1}.
M.3 Tightness
Next, we prove tightness by constructing datasets and underlying functions hsuch that the corre-
sponding base mechanism Bexactly attains the different bounds under subsampling scheme S.
74M.3.1 Gaussian mechanism tightness
Theorem M.4. LetSbe Poisson subsampling with rate râˆˆ[0,1]andÎ¸âˆˆ[0,1]be some true
response probability. There exists a dataset space Xand a batch space (Y,Y)fulfilling the constraints
in Definition D.4, as well a pair of datasets xâ‰ƒK+,Kâˆ’,Xxâ€², and a function h:Yâ†’RDwith
â„“2-sensitivity L2, such that the corresponding subsampled Gaussian mechanism M=Bâ—¦Sfulfills
Î¨Î±(mx||mxâ€²) = Î¨ Î±ï£«
ï£­Kâˆ’+1X
i=1f(1)
iÂ·Binom( iâˆ’1|Kâˆ’, r)||K++1X
j=1f(2)
jÂ·Binom( jâˆ’1|K+, r)ï£¶
ï£¸,
with univariate normal densities f(1)
i=N(Â· |(iâˆ’1), Ïƒ / L 2),f(2)
j=N(Â· | âˆ’(jâˆ’1), Ïƒ / L 2).
Proof. LetX=P(N). Consider an arbitrary xâˆˆXand select arbitrary deleted elements gâŠ†x,
with|g|=Kâˆ’and arbitrary inserted elements gâŠ†X\x. Define xâ€²=x\gâˆ’âˆªg+.
We now construct a counting function for hthat leads to the largest possible divergence under the
sensitivity constraint. We define function has follows:
h(y) =(iâˆ’1)Â·e1L2 if|gâˆ’âˆ©y|=iâˆ’1
âˆ’(jâˆ’1)Â·e1L2if|g+âˆ©y|=jâˆ’1
with first-component indicator vector e1âˆˆRD.
By construction, PMxis a mixture distributions with K+ 1components, each corresponding to a size
of a subset of gâˆ’that is included in batch y. These cases each have probability Binom( iâˆ’1|Kâˆ’, r)
under subsampling distributions PSx. Each component has distribution N(Â· |(iâˆ’1),e1L2, Ïƒ2ID).
Analogously, Pmxâ€²is the other desired mixture of Gaussians.
As in our previous proof, we can now notice that the likelihood ratio in Î¨Î±is constant in all but the
first dimension. We can thus marginalize out the remaining dimensions to obtain our result.
M.3.2 Laplace mechanism tightness
Theorem M.5. LetSbe Poisson subsampling with rate râˆˆ[0,1]andÎ¸âˆˆ[0,1]be some true
response probability. There exists a dataset space Xand a batch space (Y,Y)fulfilling the constraints
in Definition D.4, as well a pair of datasets xâ‰ƒK+,Kâˆ’,Xxâ€², and a function h:Yâ†’RDwith
â„“1-sensitivity L1, such that the corresponding subsampled Laplace mechanism M=Bâ—¦Sfulfills
Î¨Î±(mx||mxâ€²) = Î¨ Î±ï£«
ï£­Kâˆ’+1X
i=1f(1)
iÂ·Binom( iâˆ’1|Kâˆ’, r)||K++1X
j=1f(2)
jÂ·Binom( jâˆ’1|K+, r)ï£¶
ï£¸,
with univariate Laplace densities f(1)
i= Lap( Â· |(iâˆ’1), Î» / L 1),f(2)
j= Lap( Â· | âˆ’(jâˆ’1), Î» / L 1).
Proof. The proof is identical to that of the tightness guarantee from Theorem M.4. We can construct
exactly the same counting function that indicates the number of inserted or removed element that
appear in a batch sampled from SxandSxâ€², respectively.
As in our previous proof, we can now notice that the likelihood ratio in Î¨Î±is constant in all but the
first dimension. We can thus marginalize out the remaining dimensions to obtain our result.
M.3.3 Randomized response mechanism tightness
Theorem M.6. LetSbe Poisson subsampling with rate râˆˆ[0,1]andÎ¸âˆˆ[0,1]be some true
response probability. There exists a dataset space Xand a batch space (Y,Y)fulfilling the constraints
in Definition D.4, as well a pair of datasets xâ‰ƒK+,Kâˆ’,Xxâ€², and a function h:Yâ†’0,1, such that
the corresponding subsampled randomized response mechanism M=Bâ—¦Sfulfills
Î¨Î±(mx||mâ€²
x)
= max
Ï„Î¨Î±((1âˆ’w(1))Bern( Â· |Î¸) +w(1)Bern(Â· |Ï„)||(1âˆ’w(2))Bern( Â· |Î¸) +w(2)Bern(Â· |1âˆ’Ï„))
subject to Ï„âˆˆ {Î¸,1âˆ’Î¸}, and with w(1)= 1âˆ’(1âˆ’r)Kâˆ’andw(2)= 1âˆ’(1âˆ’r)K+.
75Proof. The following proof is largely identical to our randomized response tightness proof for
subsampling without replacement from Appendix I.3.
LetX=P(N). Consider an arbitrary xâˆˆXand select arbitrary deleted elements gâŠ†x, with
|g|=Kâˆ’and arbitrary inserted elements gâŠ†X\x. Define xâ€²=x\gâˆ’âˆªg+.
Assume w.l.o.g. that the divergence is maximized by Ï„=Î¸.
We now construct an indicator function h:Yâ†’ {0,1}foraandaâ€²that leads to the largest possible
divergence.
h(y) =ï£±
ï£²
ï£³1ifyâˆ©(gâˆ’âˆªg+) =âˆ…
1ifyâˆ©gâˆ’Ì¸=âˆ…
0otherwise.
By construction of h, the corresponding base mechanism pmf is
by(z) =ï£±
ï£²
ï£³Bern( z|Î¸) ifyâˆ©(gâˆ’âˆªg+) =âˆ…
Bern( z|Î¸) ifyâˆ©gâˆ’Ì¸=âˆ…
Bern( z|1âˆ’Î¸)otherwise.
Under the distribution of S(x), the first case occurs with probability (1âˆ’r)Kâˆ’and the second case
occurs with probability 1âˆ’(1âˆ’r)Kâˆ’. Under the distribution of S(xâ€²), the first case occurs with
probability (1âˆ’r)K+and the second case occurs with probability 1âˆ’(1âˆ’r)K+. We thus have
mx(z) = (1 âˆ’w(1))Bern( z|Î¸) +w(1)Bern( z|Î¸),
mxâ€²(z) = (1 âˆ’w(2))Bern( z|Î¸) +w(2)Bern( z|1âˆ’Î¸),
which exactly attains the desired divergence when Ï„=Î¸is the optimal value.
M.4 Numerical evaluation
Evidently, we do not have a closed-form analytical expression for our tight mechanism-specific group
privacy bounds. However, we can evaluate them to arbitrary precision using standard techniques from
privacy accounting literature.
M.4.1 Gaussian mechanism
ADP. To evaluate the guarantee from Theorem 3.8, we can use Lemma 5from [ 10], which generalizes
an alternative characterization of privacy profiles from [ 74] to dominating pairs. Let P, Q be
the dominating pair of two univariate Gaussian mixtures. Define privacy loss random variables
LP,Q=p(Z)
q(Z)withZâˆ¼PandLQ,P=q(Z)
p(Z)withZâˆ¼Q. Then
HÎ±(p||q)â‰¤Pr[LP,Q>log(Î±)]âˆ’Î±Pr[LQ,P<âˆ’log(Î±)].
Because the privacy loss between the two Gaussian mixtures is monotonically increasing in z[8],
one can perform a change of variables via a binary search for a zâˆ—such that log(p(zâˆ—)
q(zâˆ—))â‰ˆlog(Î±). By
picking one of the two search boundaries, one can either over- or under-approximate the hockey stick
divergence (see, e.g., [8, 24]).
RDP via quadrature. To evaluate the guarantee for RDP, we can simply use numerical quadrature.
This can be done efficiently because we only need to integrate over univariate Gaussians. This
approach was already proposed and used in Abadi et al. [6]â€™s work on moments accounting.
RDP via expansion. For the special case of Kâˆ’=KandK+= 0, one can also use multinomial
expansion (similar to prior work on RDP subsampling from [28â€“30]): We have KX
k=0wkÂ· N(z|Âµk, Ïƒ2I)!Î±
=X
l0+Â·Â·Â·+lK=Î±Î±
l0, . . . , l K KY
k=0wlk
k! KY
k=0N(z|Âµk, Ïƒ2I)lk!
=X
l0+Â·Â·Â·+lK=Î±Î±
l0, . . . , l K KY
k=0wlk
k! KY
k=0N(z|Âµk, Ïƒ2I)lk/Î±!Î±
76Using quadratic expansion, we have
KY
k=0N(z|Âµk, Ïƒ2I)lk/Î±
=N 
z|X
klk
Î±Âµk, Ïƒ2I!
Â·exp 
âˆ’1
2Ïƒ2X
klk
Î±||Âµk||2
2!
Â·exp 
1
2Ïƒ2X
k||(lk
Î±Âµk)||2
2!
Since only the first factor depends on z, our problem reduces to computing the divergence
Î¨Î± 
N 
z|X
klk
Î±Âµk, Ïƒ2I!
||N 
z|0, Ïƒ2I!
for different lk. This can be done in closed form, as shown in [7].
M.4.2 Laplace mechanism
ADP. Because the privacy loss is constant on (âˆ’âˆ,âˆ’K+), monotonically increasing on [âˆ’K+, Kâˆ’]
and constant on (Kâˆ’,âˆ), we can again use the same bisection method.
RDP. As with the Gaussian mechanism, we can evaluate the bound via univariate numerical
quadrature. Because the privacy loss is non-smooth at the component means {âˆ’K+,âˆ’K++
1, . . . , 0,1, . . . , K âˆ’}, we partition Rat these means and integrate over each interval separately.
M.4.3 Randomized response mechanism
The guarantee for randomized smoothing can be evaluated exactly in O(1). We just need to iterate
over the two options Ï„âˆˆ {0,1}, and for each one evaluate the divergence on space {0,1}, which
only requires evaluating two fractions and two sums.
77N Tight mechanism-agnostic group privacy amplification
In this section, we apply the framework from [ 15], which we summarized in Appendix G.1, to
the group privacy setting. We then show the tightness of the resultant guarantees, using the same
proof strategy as in Section 5 of [ 15]. We then demonstrate that it is directly related to our tight
mechanism-specific guarantee through joint convexity. Finally, we derive a qualitatively similar
guarantee for RDP.
For this discussion, we focus on the special case where all of the Kgroup members collaboratively
agree to insert their data (K+=K, K âˆ’= 0) or delete their data (K+= 0, Kâˆ’=K), so that
the partition induced by the maximal coupling remains interpretable. We define the corresponding
neighboring relation as
â‰ƒKÂ±,X={(x, xâ€²)âˆˆX2|xâŠ‚xâ€²âˆ§|xâ€²|=|x|+K}âˆª{(x, xâ€²)âˆˆX2|xâŠƒxâ€²âˆ§|xâ€²|=|x|âˆ’K}.(27)
In our experiments, we only evaluate the baseline for (K+=K, K âˆ’= 0) and(K+= 0, Kâˆ’=K),
while we evaluate our method for all K++Kâˆ’=Kand take the maximum. This evaluation favors
the baseline.
N.1 ADP guarantee
Proposition N.1. LetM=Bâ—¦S, where Sis Poisson subsampling with rate r. Letâ‰ƒYbe the
insertion/removal batch relation â‰ƒÂ±,Y. Then, for all xâ‰ƒKÂ±,Xxâ€²and all Îµâ‰¥0
Hexp(Îµâ€²)(mx||mxâ€²)â‰¤KX
k=1Binom( k|K, r)Â·Î´k
withÎµâ€²= log(1 + (1 âˆ’(1âˆ’r)K)Â·(eÎµâˆ’1))and group privacy parameters
Î´k= max
y,yâ€²Hexp(Îµ)(by||byâ€²)s.t.dY(y, yâ€²)â‰¤k.
Proof. Case 1: Deletion. We first consider the case where Kelements are deleted, i.e., there is some
deleted set gâŠ†xof size Kwithgâˆ’âˆ©xâ€²=âˆ…such that xâ€²=x\g.
The partition induced by the maximal coupling9is
sx(y) = (1 âˆ’w)sx(y|A2) +wsx(y|A1)
sxâ€²(y) = (1 âˆ’w)sx(y|A2) +wsx(y|A2),
where A2={yâˆˆY|yâˆ©g=âˆ…is the event that no element of gis sampled, A1is its complement,
andw= (1âˆ’(1âˆ’r)K) =PSx(A1). We use these indices for A2andA1because advanced joint
convexity will later reverse their order.
Applying advanced joint convexity (see Proposition G.1) and joint convexity shows that
Hexp(Îµâ€²)(mx||mxâ€²)â‰¤wÂ·Hexp(Îµ)ï£«
ï£­X
yâˆˆYbyÂ·sx(y|A1)||X
yâˆˆYbyÂ·sx(y|A2)ï£¶
ï£¸. (28)
Next, we can bound this mixture divergence by constructing a coupling invoking Theorem 3.3 with
the special case of Î¨Î±=Hexp(Îµ). For this, notice that
sx(y|A1) =1
wÂ·r|y|Â·(1âˆ’r)|x|âˆ’|y|ifyâˆˆA1âˆ§yâŠ†x
0 otherwise.
and
sx(y|A2) =sxâ€²(y) =r|y|Â·(1âˆ’r)|xâ€²|âˆ’|y|ifyâˆˆA2âˆ§yâŠ†xâ€²
0 otherwise.
9as can be seen by the fact that wis the total variation distance of sxandsxâ€², and that sx(y|A2)and
sx(y|A1)have disjoint support
78Coupling. We define a coupling Î³:Y2â†’[0,1]that corresponds to the following generative process:
We first sample y(2)from sx(y|A2). We then sample from a truncated binomial distribution how
many elements kfrom group gshould be included. Given this k, we sample uniformly at random a
ËœgâŠ†gfrom all size- ksubsets of gand let y(1)â†y(2)âˆªËœg. Formally this is defined by
Î³(y(1), y(2)) =(
sx(y(2)|A2)Â·1
wÂ·Binom( |y(1)âˆ©g| |K, r)Â· K
|y(1)âˆ©g|âˆ’1under Condition N.2
0 otherwise.
Condition N.2. A tuple y(1)âˆˆY,y(2)âˆˆYfulfills this condition when there exists a ËœgâŠ†gwith
|Ëœg| â‰¥1such that y(1)=y(2)âˆªËœg.
Validity. Next, we verify the validity of this coupling. For every y(1), there is exactly one y(2)such
thatÎ³(y(1), y(2))>0, namely y(2)=y(1)\gâ€². Thus,
X
y(2)Î³(y, y(2)) =sx(y\g|A2)Â·1
wÂ·Binom( |yâˆ©g| |K, r)Â·K
yâˆ©g|âˆ’1
=r|y\g|Â·(1âˆ’r)|xâ€²|âˆ’|y\g|Â·1
wÂ·r|yâˆ©g|Â·(1âˆ’r)Kâˆ’|yâˆ©g|
=sx(y|A1).
For every y(2), there are exactly K
k
batches y(1)such that Î³(y(1), y(2))>0and|y(1)âˆ©g=k|.
Thus,
X
y(1)Î³(y(1), y) =KX
k=1K
k
sx(y|A2)Â·1
wÂ·Binom( k|K, r)Â·K
kâˆ’1
=sx(y|A2)Â·1
wÂ·KX
k=1Binom( k|K, r)
=sx(y|A2).
Now that we have proven the validity of the coupling, we can apply the optimal transport bound
(Theorem 3.3) to Eq. (28) in order to prove
Hexp(Îµâ€²)(mx||mxâ€²)â‰¤wÂ·X
y(1),y(2)Î³(y(1), y(2))Î´dY(y(1),y(2))
=wÂ·KX
k=1K
k1
wBinom( k|K, r)K
kâˆ’1
Â·Î´k
=KX
k=1Binom( k|K, r)Â·Î´k.
Case 2: Insertion. In this case, the partition induced by the maximal coupling is
sx(y) = (1 âˆ’w)sx(y|A1) +wsx(y|A2)
sxâ€²(y) = (1 âˆ’w)sx(y|A1) +wsx(y|A1),
which is identical to the previous partition, up to symmetry. The proof is identical, except for changes
in indexing.
Taking the maximum over both guarantees yields the result.
N.2 Tightness of ADP guarantee
Proposition N.3. LetSbe Poisson subsampling with rate r. Letâ‰ƒYbe the insertion/removal batch
relation â‰ƒÂ±,Y. Then, for all xâ‰ƒKÂ±,Xxâ€²and all Îµâ‰¥0, there exists a worst-case base mechanism B
79such that the corresponding subsampled mechanism M=Bâ—¦Sfulfills
Hexp(Îµâ€²)(mx||mxâ€²) =KX
k=1Binom( k|K, r)Â·Î´k (29)
withÎµâ€²= log(1 + (1 âˆ’(1âˆ’r)K)Â·(eÎµâˆ’1))and group privacy parameters
Î´k= max
y,yâ€²Hexp(Îµ)(by||byâ€²)s.t.dY(y, yâ€²)â‰¤k.
Proof. To show tightness of the bound, we notice that the boundPK
k=1Binom( k|K, r)Â·Î´kis
identical to the bound for subsampling with replacement from [ 15], except for the numeric value of
the weights in the weighted sum. We can thus use exactly the same proof strategy.
Assume w.l.o.g. that we are in the insertion case, i.e., there is some gof size Kwithgâˆ©x=âˆ…
andxâ€²=xâˆªg. Consider an arbitrary Îµand define an arbitrary true response probability Î¸âˆˆ[0,1].
Further define the randomized membership base mechanism
B(y) =| 1[|yâˆ©g|>0]âˆ’(1âˆ’V)|
withVâˆ¼Bern( Î¸). It is easy to verify [ 15] that Î´k=ÏˆÎ¸(Îµ) = max {Î¸âˆ’eÎµ(1âˆ’p),0}. We thus have
for the r.h.s. of Eq. (29)
KX
k=1Binom( k|K, r)Â·Î´k=KX
k=1Binom( k|K, r)Â·ÏˆÎ¸(Îµ) :=wÂ·ÏˆÎ¸(Îµ).
Because Poisson subsampling is a â€œnatural subsamplingâ€[ 15] scheme that only yields elements from
the dataset it is applied to, it follows from Lemma 12 of [15] that also
Hexp(Îµâ€²)(mx||mxâ€²) =wÂ·ÏˆÎ¸(Îµ).
N.3 Relation to tight mechanism-specific bound
In the following, we show that this mechanism-agnostic guarantee implicitly upper bounds our
tight mechanism-specific guarantee via joint convexity. Note that this is qualitatively different from
our discussion in Appendix G. There we showed, that the mechanism-agnostic guarantees can be
derived from mechanism-specific bounds that only use a binary partitioning of the batch space (unlike
the mechanism-specific guarante considered here). We show this result w.l.o.g. for Kâˆ’= 0 and
K+=K.
Theorem N.4. LetB:Yâ†’Zbe an arbtirary base mechanism. Let y(1)âˆˆY1and,y(2)âˆˆYK+1
be arbitrary tuples of batches that fulfill âˆ€l, t, u :dY(y(l)
t, y(l)
u)â‰¤ |tâˆ’u|, andâˆ€t, u:dY(y(1)
t, y(2)
u)â‰¤
(tâˆ’1) + ( uâˆ’1). Then, for Î±â‰¥1
HÎ±â€²ï£«
ï£­by(1)
1||K+1X
j=1by(2)
jÂ·Binom( jâˆ’1|K, r)ï£¶
ï£¸â‰¤KX
k=1Binom( k|K, r)Â·Î´k(Î±)
withw= (1âˆ’(1âˆ’r)K),Î±â€²= 1 + w(Î±âˆ’1)and group privacy parameters
Î´k(Î±) = max
y,yâ€²HÎ±(by||byâ€²)s.t.dY(y, yâ€²)â‰¤k.
80Proof. We can apply joint convexity to show
HÎ±â€²ï£«
ï£­by(1)
1||K+1X
j=1by(2)
jÂ·Binom( jâˆ’1|K, r)ï£¶
ï£¸ (30)
=HÎ±â€²ï£«
ï£­by(1)
1||(1âˆ’w)Â·by(2)
1+K+1X
j=2wÂ·by(2)
jÂ·1
wÂ·Binom( jâˆ’1|K, r)ï£¶
ï£¸ (31)
=HÎ±â€²ï£«
ï£­by(1)
1||K+1X
j=2
(1âˆ’w)Â·by(2)
1+wÂ·by(2)
j
Â·1
wÂ·Binom( jâˆ’1|K, r)ï£¶
ï£¸ (32)
â‰¤K+1X
j=21
wÂ·Binom( jâˆ’1|K, r)Â·HÎ±â€²
by(1)
1||
(1âˆ’w)Â·by(2)
1+wÂ·by(2)
j
(33)
The result then immediately follows from advanced joint convexity (see Proposition G.1) and joint
convexity.
N.4 RDP guarantee
For RDP, we can obtain a qualitatively similar guarantee by simply applying the known mechanism-
agnostic RDP subsampling guarantee for Poisson subsampling from [ 29,30] to each of the K
divergences (although we could also derive this RDP guarantee from scratch via optimal transport)
in Eq. (33) of the previous derivation.
Theorem N.5. LetM=Bâ—¦S, where Sis Poisson subsampling with rate r. Letâ‰ƒYbe the
insertion/removal batch relation â‰ƒÂ±,Y. Then, for all xâ‰ƒKÂ±,Xxâ€²and all Î± >0
Î›Î±(mx||mxâ€²)â‰¤KX
k=11
wÂ·Binom( k|K, r)Â·2Â·Î±X
l=0Î±
l
wl(1âˆ’w)Î±âˆ’lÎ¶k(l)
withw= (1âˆ’(1âˆ’r)K)and group privacy parameters
Î¶k(l) = max
y,yâ€²Î›l(by||byâ€²)s.t.dY(y, yâ€²)â‰¤k.
The factor 2in Theorem N.5 can be eliminated for distributions with particular symmetries (see
Theorem 5 in [ 29]) or bounded Pearson-Vajda Ï‡l-pseudo-divergence (see Theorem 8 in [ 30]). We thus
do not include the factor 2when using this amplification guarantee as a baseline in our experiments.
N.5 Asymptotic RDP guarantees
As mentioned in Section 3.1, our focus is on tight bounds that can be explicitly computed. However,
analyzing the asymptotic behavior of these bounds can provide a potentially useful high-level picture
of their behavior. As discussed in the previous section, and as can be seen from Eq. (33), RÃ©nyi
divergence in the group privacy setting is bounded by a weighted sum of RÃ©nyi divergences between
a single distribution and a mixture of two distributions. For the special case of additive Gaussian
mechanisms with global sensitivity L, this bound is equivalent (see Appendix A of [30]) to
KX
k=11
wÂ·Binom( k|K, r)Â·2Â·Î›Î±(N(0, Ïƒ)||(1âˆ’w)N(0, Ïƒ) +wÂ·N(1, Ïƒ /(kÂ·L))). (34)
Asymptotic bounds on RÃ©nyi divergences with one or two components have been derived in prior
work on privacy accounting [ 6,28,48]. We can apply these asymptotic bounds to each summand.
For instance (see Lemma 5 in [6]):
Proposition N.6. Abadi et al. [6]LetÏƒâ‰¥1andqâ‰¤1
16Ïƒ. Then, for any positive integer Î±â‰¤
Ïƒ2ln
1
qÏƒ
âˆ’1,
Î›Î±(N(0, Ïƒ)||(1âˆ’q)N(0, Ïƒ) +qÂ·N(1, Ïƒ))â‰¤q2Î±(Î±âˆ’1)
(1âˆ’q)Ïƒ2+O(q3Î±3/ Ïƒ3).
81Applying Proposition N.6 to Eq. (34) yields the following asymptotic bound for RDP group privacy:
Theorem N.7. LetM=Bâ—¦S, where Sis Poisson subsampling with rate randBis an additive
gaussian mechanism with global sensitivity Lunder the insertion/removal relation â‰ƒÂ±. Consider
arbitrary datasets xâ‰ƒKÂ±,Xxâ€². Define weight w= (1âˆ’(1âˆ’r)K). IfÏƒâ‰¥kÂ·Landwâ‰¤1
16Ïƒ, then
it holds for any positive integer Î±â‰¤Ïƒ2
k2Â·L2ln 1
wÏƒ
âˆ’1that
Î›Î±(mx||mxâ€²)â‰¤KX
k=11
wÂ·Binom( k|K, r)Â·2Â·k2L2q2Î±(Î±âˆ’1)
(1âˆ’w)Ïƒ2+O(k3L3w3Î±3/ Ïƒ3).
Alternatively, one could apply the asymptotic bound from Theorem 38 from [ 48] to each summand.
If we were to instead consider group privacy under dataset-level substitutions, where each summand
would include a divergence between two mixtures with two components, we could instead use the
asymptotic bounds from Appendix C of [28].
82O Worst-case mixture components
O.1 Gaussian and Laplacian mixtures
We outline a self-contained and extendable proving strategy which we use to find dominat-
ing pairs of Gaussian and Laplacian mixtures given divergences of the form Î¨Î±(P||Q) = R
RDf(P(x),âˆ’Q(x))dx, where fis (not necessarily strictly) convex and increasing in both ar-
guments. Our two main examples of the hockey stick divergence Î¨Î±=HÎ±and (scaled and
exponentiated) RÃ©nyi divergence Î¨Î±= Î›Î±are special cases with f(x, y) = max {x+Î±y,0}and
f(x, y) =xÎ±(âˆ’y)1âˆ’Î±, respectively. Throughout the subsection, we use M,Nto denote the sets of
means of the two mixtures P,Q. We start with two general lemmata before treating the Gaussian and
Laplacian mixture cases in particular. Together, they provide a constructive toolset to connect any
mixture pair with means (M, N )to a dominating mixture pair with means (Mâˆ—, Nâˆ—)via a path of
geometric transformations (M, N )7âˆ’â†’(M(1), N(1))7âˆ’â†’ Â·Â·Â· 7âˆ’â†’ (Mâˆ—, Nâˆ—): concretely, these are
â€¢ mirroring the means of the two mixtures onto opposite sides of a hyperplane, and
â€¢ pushing such hyperplane-separated means further away along the hyperplane normal.
In the first part of this section, we prove that the divergence can only stay equal or grow under
any such transformation. Afterwards, we construct an explicit path of transformations that maps
any Gaussian, as well as any Laplacian mixture onto a dominating pair which is feasible under the
pairwise distance constraints discussed in Appendix M.2.
Lemma O.1. Given pâˆˆ {1,2}, consider two mixtures of the form PM(x) =PK
k=0wkÏ(âˆ¥xâˆ’
Âµkâˆ¥p),QN(x) =PK
Îº=0Ï‰ÎºÏ(âˆ¥xâˆ’Î½Îºâˆ¥p)with means in M:={Âµ0, . . . , Âµ K} âŠ‚RD,N:=
{Î½0, . . . , Î½ K} âŠ‚RDand a decreasing Ï:R+
0â†’[0,1]. Consider all hyperplanes which contain
zero and are normal to L2-unit vectors Ë†nâˆˆ H p, where
(1)H1={ÏƒË†ei|0â‰¤iâ‰¤K, Ïƒâˆˆ {+,âˆ’}}âˆª{1âˆš
2(Ïƒ1Ë†ei+Ïƒ2Ë†ej)|0â‰¤iÌ¸=jâ‰¤K, Ïƒ 1, Ïƒ2âˆˆ {+,âˆ’}},
(2)H2=SD, the unit D-sphere,
and define the lower half-space Râˆ’
Ë†n={xâˆˆRD| 
xTË†n
<0}and upper half-space R+
Ë†n={xâˆˆ
RD| 
xTË†n
>0}.
Consider the map
(Â·)â€²
Ë†n:RDâˆ’â†’RD\Râˆ’
Ë†n,x7âˆ’â†’xâˆ’1Râˆ’
Ë†n(x)(2Ë†nTx)Ë†n
which mirror-reflects the lower into the upper half-space, and its image sets
Mâ€²
Ë†n=RD\Râˆ’
Ë†nandNâ€²
âˆ’Ë†nâŠ‚RD\R+
Ë†n.
The reflected pair of mixtures has equal or greater divergence:
Î¨Î±
PMâ€²
Ë†n||QNâ€²
âˆ’Ë†n
â‰¥Î¨Î±(PM||QN).
Remark O.2.Lemma O.1 applies to Laplacian and Gaussian mixtures with p= 1 andp= 2,
respectively.
Proof. We will need the following lemma.
Lemma O.3. Given pâˆˆ {1,2}, a hyperplane normal to Ë†nâˆˆ H p, and points x1,x2âˆˆRD, we have
âˆ¥(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†nâˆ¥pâ‰¤ âˆ¥x1âˆ’x2âˆ¥pif(x1âˆˆR+
Ë†nâˆ§x2âˆˆRâˆ’
Ë†n)âˆ¨(x1âˆˆRâˆ’
Ë†nâˆ§x2âˆˆR+
Ë†n)
âˆ¥(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†nâˆ¥p=âˆ¥x1âˆ’x2âˆ¥pelse.
Corollary O.4. If the sets M,Nare feasible under pairwise p-norm distance constraints, then so
areMâ€²
Ë†n,Nâ€²
âˆ’Ë†n.
Proof of Lemma O.3. Ifsign(nTx1) =sign(nTx2), then (Â·)â€²
Ë†nacts uniformly on both vectors and
the condition clearly holds. Else, assume w.l.o.g. that x2âˆˆRâˆ’
Ë†n. We start with the case p= 1.
83First, assume Ë†nâˆˆ {ÏƒË†ei|0â‰¤iâ‰¤K, Ïƒ 1âˆˆ {+,âˆ’}}. Since (Â·)â€²
Ë†nnow acts only on vector component
i, we can write âˆ¥(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†nâˆ¥1âˆ’ âˆ¥x1âˆ’x2âˆ¥1=||Ë†eT
ix1| âˆ’ |Ë†eT
ix2|| âˆ’ | Ë†eT
ix1âˆ’Ë†eT
ix2| â‰¤0by
the inverse triangle inequality.
Now assume instead that Ë†nâˆˆ {1âˆš
2(Ïƒ1Ë†ei+Ïƒ2Ë†ej)|1â‰¤iÌ¸=jâ‰¤D, Ïƒ 1, Ïƒ2âˆˆ {+1,âˆ’1}}.
Now, (Â·)â€²
Ë†nacts only on vector components iandj. Thus, âˆ¥(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†nâˆ¥1âˆ’ âˆ¥x1âˆ’x2âˆ¥1=
âˆ¥Ï€i,j[(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†n]âˆ¥1âˆ’ âˆ¥Ï€(i,j)[x1âˆ’x2]âˆ¥1where Ï€(i,j)denotes projection onto the subspace
spanned by the basis vectors Ë†ei,Ë†ej. It is useful to write
âˆ¥Ï€i,j[(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†n]âˆ¥1= max
Ïƒâˆˆ{âˆ’1,+1}D|ÏƒTÏ€i,j[(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†n]|
= max
Ïƒâˆˆ{âˆ’1,+1}D|ÏƒTÏ€i,j
x1âˆ’x2+ 2(Ë†nTx2)Ë†n
|.
The argument of the max function admits two cases. Either, ÏƒT(Ï€i,jË†n) = 0 , in which case
|ÏƒTÏ€i,j[(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†n]|=|ÏƒTÏ€i,j[x1âˆ’x2]|. In the other case, Ï€i,jÏƒ=Â±2âˆš
2Ï€i,jË†n. Then, by
the inverse triangle inequality,
|ÏƒTÏ€i,j[(x1)â€²
Ë†nâˆ’(x2)â€²
Ë†n]|=2âˆš
2Â±Ë†nTx1âˆ“Ë†nTx2Â±2Ë†nTx2=2âˆš
2Â±Ë†nTx1Â±Ë†nTx2
=2âˆš
2Â± 
|Ë†nTx1| âˆ’ |Ë†nTx2|â‰¤2âˆš
2Ë†nT(x1âˆ’x2)=ÏƒTÏ€i,j[x1âˆ’x2].
For completeness, we also prove p= 2. By invariance of the scalar product under mirror reflection,
âˆ¥xâ€²
1âˆ’xâ€²
2âˆ¥2=xT
1x1+xâ€²T
2xâ€²
2âˆ’2xT
1xâ€²
2=xT
1x1+xT
2x2âˆ’2xT
1x2+ 2(nTx2)(nTx1) =
âˆ¥x1âˆ’x2âˆ¥2+ 2(nTx2)(nTx1)â‰¤ âˆ¥x1âˆ’x2âˆ¥2.
We recall the notation introduced at the start of the section, Î¨Î±(P||Q) =R
RDf(P(x),âˆ’Q(x))dx,
where we assume an integrand fthat is convex in both arguments. The statement of Lemma O.1 will
follow from the next lemma, which informally states that at any point xin the upper half-space, the
mirror-reflection of means contained in Mâˆ’
Ë†ncauses an increase of the integrand f(PM(x),âˆ’QN(x))
atxwhich dominates the corresponding decrease at the mirror image point xâ€²in the lower half-space.
Lemma O.5. Given a hyperplane normal to Ë†nâˆˆ H pas defined in Lemma O.1 as well as any point
xâˆˆR+
Ë†nalong with its mirror image xâ€²=xâˆ’2 
xTË†nË†nâˆˆRâˆ’
Ë†n, the change of fsatisfies
h
f
PMâ€²
Ë†n(x),âˆ’QNâ€²
âˆ’Ë†n(x)
âˆ’f(PM(x),âˆ’QN(x))i
+h
f
PMâ€²
Ë†n(xâ€²),âˆ’QNâ€²
âˆ’Ë†n(xâ€²)
âˆ’f(PM(xâ€²),âˆ’QN(xâ€²))i
â‰¥0.
Proof of Lemma O.5. We introduce the following notation,
P0(x):=X
ÂµkâˆˆM\Râˆ’
Ë†nwkÏ(âˆ¥xâˆ’Âµkâˆ¥p), Q 0(x):=X
Î½kâˆˆN\Râˆ’
âˆ’Ë†nÏ‰kÏ(âˆ¥xâˆ’Î½Îºâˆ¥p),
Î´P(x):=X
ÂµkâˆˆMâˆ©Râˆ’
Ë†nwkÏ(âˆ¥xâˆ’Âµkâˆ¥p), Î´Q (x):=X
Î½kâˆˆNâˆ©Râˆ’
âˆ’Ë†nÏ‰kÏ(âˆ¥xâˆ’Î½Îºâˆ¥p),
thus decomposing the densities into a part that stays fixed and a part that gets mirror-reflected. Using
Lemma O.3 and the monotonicity requirement on Ï, we can directly verify that
P0(x)â‰¥P0(xâ€²), Î´P (x)â‰¤Î´P(xâ€²), (35)
âˆ’Q0(x)â‰¥ âˆ’Q0(xâ€²),âˆ’Î´Q(x)â‰¤ âˆ’Î´Q(xâ€²). (36)
Using invariance of distances under simultaneous mirroring of both vectors, we rewrite the expression
of the lemma as
[f(P0(x) +Î´P(xâ€²),âˆ’Q0(x)âˆ’Î´Q(xâ€²))âˆ’f(P0(x) +Î´P(x),âˆ’Q0(x)âˆ’Î´Q(x))]
âˆ’[f(P0(xâ€²) +Î´P(xâ€²),âˆ’Q0(xâ€²)âˆ’Î´Q(xâ€²))âˆ’f(P0(xâ€²) +Î´P(x),âˆ’Q0(xâ€²)âˆ’Î´Q(x))]
The statement of the lemma follows from convexity of fin both arguments by Jensenâ€™s inequality.
84We now proceed to prove Lemma O.1. The difference of divergences can be rewritten as
Î¨Î±
PMâ€²
Ë†n||QNâ€²
âˆ’Ë†n
âˆ’Î¨Î±(PM||QN)
=Z
RDh
f
PMâ€²
Ë†n(x),âˆ’QNâ€²
âˆ’Ë†n(x)
âˆ’f(PM(x),âˆ’QN(x))i
dx
=Z
Râˆ’
Ë†nh
f
PMâ€²
Ë†n(x),âˆ’QNâ€²
âˆ’Ë†n(x)
âˆ’f(PM(x),âˆ’QN(x))i
dx
+Z
R+
Ë†nh
f
PMâ€²
Ë†n(x),âˆ’QNâ€²
âˆ’Ë†n(x)
âˆ’f(PM(x),âˆ’QN(x))i
dx
=Z
R+
Ë†nh
f
PMâ€²
Ë†n(xâ€²),âˆ’QNâ€²
âˆ’Ë†n(xâ€²)
âˆ’f(PM(xâ€²),âˆ’QN(xâ€²))i
+h
f
PMâ€²
Ë†n(x),âˆ’QNâ€²
âˆ’Ë†n(x)
âˆ’f(PM(x),âˆ’QN(x))i
dx
â‰¥0
Apart from Lemma O.5, we used the fact that mirror reflection has a unit absolute Jacobian determi-
nant, and that the hyperplane, a null set, does not contribute to the integral.
Lemma O.1 shows that we can construct a dominating mixture by mirroring all means onto opposite
sides of a suitable hyperplane. This is exactly the condition under which the follow-up transform
discussed in the next lemma yields equal or greater divergence. We slightly change perspective and
consider the means of both mixtures as a joint vector. Given two sets of means M={Âµ0, . . . , Âµ K},
N={Î½0, . . . , Î½ K}we define (in arbitrary ordering), Âµ= (Âµ0, . . . , Âµ K, Î½0, . . . , Î½ K)âˆˆ F âŠ‚
RDÃ—(K+K)where Fdenotes the region feasible under the distance constraints. With this implicit
mapping (M, N )7â†’Âµbetween sets and vectors of means, we can treat the divergence as a function
Î¨Î±:Âµ7â†’R
RDËœf(x,Âµ)dxwith integrand Ëœf(x,Âµ):=f(PM(x),âˆ’Q(x)). Now we can state:
Lemma O.6. Forpâˆˆ {1,2}, letË†nâˆˆ H pbe a normal vector, M,Nwith corresponding vector Âµ
as above be two sets of means for which Mâˆ©Râˆ’
Ë†nandNâˆ©Râˆ’
âˆ’Ë†nare empty (i.e., the hyperplane
separates the two sets), and let Î¨Î±denote either Î›Î±orHÎ±.
Then, the normal directional derivatives of the divergence with respect to the means ÂµlandÎ½Î¹are
nonnegative for all lâˆˆ {1, . . . , K }, Î¹âˆˆ {1, . . . ,K},
d(Âµl)
Ë†nÎ¨Î±(Âµ):= lim
Îµâ†’01
Îµ[Î¨Î±((Âµ0, . . . , Âµ l+ÎµË†n, . . . , Î½ K))âˆ’Î¨Î±(Âµ)]â‰¥0
d(Î½Î¹)
âˆ’Ë†nÎ¨Î±(Âµ):= lim
Îµâ†’01
Îµ[Î¨Î±((Âµ0, . . . , Î½ Î¹âˆ’ÎµË†n, . . . , Î½ K))âˆ’Î¨Î±(Âµ)]â‰¥0.
In the case Î¨Î±=HÎ±, the above expressions are instead defined in the sense of weak derivatives.
The lemma is quite intuitive: once both sets of means are already separated by a hyperplane, pushing
them apart in the normal direction, thereby increasing the margin to the hyperplane, never decreases
the divergence. As we will only ever use the directional derivatives within line integrals, the weak
sense in which the lemma holds for Î¨Î±=HÎ±is sufficient for our purposes.
Proof of Lemma O.6. We first check that for all xâˆˆRDandÂµâˆˆ F, the derivative âˆ‚ÂµËœf(possibly in
the weak sense) exists and is dominated by an integrable function. In the case of Gaussian or Laplacian
mixtures and Î¨Î±= Î›Î±, existence of the derivative is clear. For Gaussian mixtures, the integrability
condition holds since âˆ‚ÂµËœf=O(exp(âˆ’âˆ¥xâˆ¥2
2)), for Laplacian mixtures âˆ‚ÂµËœf=O(exp(âˆ’âˆ¥xâˆ¥1))
since the privacy loss factor in the integrand eventually becomes constant. In the case Î¨Î±=HÎ±,Ëœf
has a weak derivative âˆ‚ÂµËœf=1fâ‰¥0âˆ‚Âµ(Pâˆ’Î±Q), which agrees with its derivative except where the
latter is not defined (i.e., on the null set of means and xat the boundary of exact DP). Integrability
85follows again from âˆ‚ÂµËœf=O(exp(âˆ’âˆ¥xâˆ¥1)). In the case of Î¨Î±= Î›Î±,
d(Âµl)
Ë†nÎ¨Î±(Âµ)
=Z
RDd(Âµl)
Ë†n" KX
k=0wkÏ(âˆ¥xâˆ’Âµkâˆ¥p)!Î±# KX
Îº=0Ï‰ÎºÏ(âˆ¥xâˆ’Î½Îºâˆ¥p)!1âˆ’Î±
dx
=Î±wlZ
RDPM(x)
QN(x)Î±âˆ’1
d(Âµl)
Ë†nÏ(âˆ¥xâˆ’Âµlâˆ¥p)dx
=Î±wlZ
RDPM(x+Âµl)
QN(x+Âµl)Î±âˆ’1
d(Âµl)
Ë†nÏ(âˆ¥xâˆ¥p)dx
Consider xâˆˆR+and its mirror image xâ€²âˆˆRâˆ’. By Lemma O.3 and the assumption that the hyper-
plane separates MandN,PM(x+Âµl)
QN(x+Âµl)=Ï(âˆ¥x+Âµlâˆ¥p)
Ï(âˆ¥xâˆ¥p)PM(x)
Ï(âˆ¥x+Âµlâˆ¥p)
Ï(âˆ¥xâˆ¥p)QN(x)=PM(x)
QN(x)â‰¥PM(xâ€²)
QN(xâ€²)=PM(xâ€²+Âµl)
QN(xâ€²+Âµl). More-
over, also by Lemma O.3, Ï(âˆ¥xâˆ¥p) =Ï(âˆ¥xâ€²âˆ¥p), and therefore d(Âµl)
Ë†nÏ(âˆ¥xâˆ¥p) =d(Âµl)
âˆ’Ë†nÏ(âˆ¥xâ€²âˆ¥p) =
âˆ’d(Âµl)
Ë†nÏ(âˆ¥xâ€²âˆ¥p).
Similarly, we observe how in
d(Î½Î¹)
âˆ’Ë†nÎ¨Î±(Âµ)
=Z
RD KX
k=0wkÏ(âˆ¥xâˆ’Âµkâˆ¥p)!Î±
d(Î½Î¹)
âˆ’Ë†nï£®
ï£° KX
Îº=0Ï‰ÎºÏ(âˆ¥xâˆ’Î½Îºâˆ¥p)!1âˆ’Î±ï£¹
ï£»dx
= (âˆ’1)(1âˆ’Î±)Ï‰Î¹Z
RDPM(x)
QN(x)Î±
d(Î½Î¹)
Ë†nÏ(âˆ¥xâˆ’Î½Î¹âˆ¥p)dx
= (Î±âˆ’1)Ï‰Î¹Z
RDPM(x+Î½Î¹)
QN(x+Î½Î¹)Î±
d(Î½Î¹)
Ë†nÏ(âˆ¥xâˆ¥p)dx
the negative signs of âˆ’Ë†nand(1âˆ’Î±)cancel and apply analogous reasoning thereafter.
The statement for Î¨Î±= Î›Î±now follows in analogy to the proof of Lemma O.1 via Lemma O.5.
ForÎ¨Î±=HÎ±, we find
d(Âµl)
Ë†nÎ¨Î±(Âµ) =wlZ
RD1{P(x)âˆ’Î±Q(x)â‰¥0}d(Âµl)
Ë†nÏ(âˆ¥xâˆ’Âµlâˆ¥p)dx
=wlZ
RD1nPM(x+Âµl)
QN(x+Âµl)â‰¥Î±od(Âµl)
Ë†nÏ(âˆ¥xâˆ¥p)dx
d(Î½Î¹)
Ë†nÎ¨Î±(Âµ) =âˆ’wÎ¹Z
RD1{PM(x)âˆ’Î±QN(x)â‰¥0}
âˆ’d(Âµl)
Ë†nÏ(âˆ¥xâˆ’Î½Î¹âˆ¥p)
dx
=wÎ¹Z
RD1nPM(x+Âµl)
QN(x+Âµl)â‰¥Î±od(Âµl)
Ë†nÏ(âˆ¥xâˆ¥p)dx
and can proceed by analogous reasoning from here on.
We are now equipped to construct the sequence of transformations taking two arbitrary sets M, N of
Gaussian or Laplacian mixture means to the means Mâˆ—, Nâˆ—of a dominating pair. We start with the
Gaussian case.
O.1.1 Dominating pair of Gaussian mixtures
Theorem O.7. LetÎ¨Î±=HÎ±orÎ¨Î±= Î› Î±. Any pair of Gaussian mixtures with means satisfying
distance constraints of the form
Âµ0=Î½0= 0 âˆ¥Âµiâˆ’Âµjâˆ¥2â‰¤ |iâˆ’j| âˆ€ 0â‰¤i, jâ‰¤K, âˆ¥Î½Î¹âˆ’Î½Ï„âˆ¥2â‰¤ |iâˆ’j| âˆ€ 0â‰¤Î¹, Ï„â‰¤ K,
is dominated by a mixture with means
Âµk=kÂ·Ë†eâˆ€kâˆˆ {0, . . . , K }, Î½ Îº=âˆ’ÎºÂ·Ë†eâˆ€Îºâˆˆ {0, . . . ,K}, withË†eâˆˆRD,âˆ¥Ë†eâˆ¥2= 1.
86Proof. We start by proving the following lemma.
Lemma O.8. Any pair of Gaussian mixtures with means at a fixed set of radii,
âˆ¥Âµkâˆ¥2=rk,âˆ¥Î½Îºâˆ¥2=ÏÎº, r 0=Ï0= 0, r k, ÏÎºâˆˆR+
0âˆ€kâˆˆ {1, . . . K }âˆ€Îºâˆˆ {1, . . .K},
and satisfying the constraints of Theorem O.7 is dominated by a feasible pair with means at equal
radii that are collinear on diametral half-lines through zero, i.e.,
Âµk=rkÂ·Ë†e, Î½ Îº=âˆ’ÏÎºÂ·Ë†eâˆ€kâˆˆ {0, . . . , K }âˆ€Îºâˆˆ {0, . . . ,K} withË†eâˆˆRD,âˆ¥Ë†eâˆ¥2= 1.
Proof of Lemma O.8. Without loss of generality, we can pick as the collinearity direction from the
lemma Ë†e=e1, the first canonical basis vector, since the divergence is rotation invariant. Pick any
orthogonal direction, say, the second basis vector e2. By Lemma O.1, we can mirror the two mixtures
onto opposite sides of the hyperplanes with normal vectors Ë†e1andË†e2such that Ë†eT
1Âµkâ‰¥0â‰¥Ë†eT
2Âµk
for all kandË†eT
1Î½Îºâ‰¥0â‰¥Ë†eT
2Î½Îºfor all Îº.
Now, consider a hyperplane with normal vector Ë†n(Î¸) =Ë†e1cos(Î¸) +Ë†e2sin(Î¸),Î¸âˆˆ[0, Ï€/2]. Intu-
itively, we let this hyperplane undergo a full rotation and â€œscoop upâ€ all the means which are not in
the hyperplane normal to Ë†e2, giving a new set of means which are. Formally, we find that by the above
sign condition, there is an angle Î¸kâˆˆ[0, Ï€/2]for every ksuch that sign(Ë†n(Î¸)TË†Âµk) =sign(Î¸kâˆ’Î¸).
Namely, using Ï€12to denote the projection onto the subspace spanned by vectors Ë†e1andË†e2,
Ï€12Âµk=âˆ¥Ï€12Âµkâˆ¥2(sin(Î¸k)Ë†e1âˆ’cos(Î¸k)Ë†e2)). Consider the paths Î³k: [0, Ï€/2]â†’RD, where
Î³k(Î¸) =1{Î¸kâˆ’Î¸â‰¥0}Âµk+1{Î¸kâˆ’Î¸<0}((1âˆ’Ï€12)Âµk+âˆ¥Ï€12Âµkâˆ¥2(sin(Î¸)Ë†e1âˆ’cos(Î¸)Ë†e2))
and a sign-flipped construction with angles Î¸Îºand curves Î¶Îºfor the means Î½Îº. For the derivatives
along the curves, we find Î³â€²
k(Î¸) =âˆ¥Ï€12Âµkâˆ¥2Ë†n(Î¸)andÎ¶â€²
Îº(Î¸) =âˆ’âˆ¥Ï€12Î¶Îºâˆ¥2n(Î¸). Also, by construc-
tion, Ë†n(Î¸)TË†Î³k(Î¸)â‰¥0â‰¥Ë†n(Î¸)TË†Î¶Îº(Î¸). Hence, the prerequisites for Lemma O.6 are fulfilled at every
Î¸âˆˆ[0, Ï€/2]. By evaluating the path integral along these curves and invoking Lemma O.6, we find
that the divergence along the path M, N 7â†’ËœM,ËœNcannot decrease:
Î¦Î±(PËœM||QËœN)âˆ’Î¦Î±(PM||QN)
=ZÏ€/2
0KX
l=1d(Âµl)
Ë†nÎ¨Î±((Î³1, . . . , Î³ K, Î¶1, . . . , Î¶ K)(Î¸))
+KX
Î¹=1d(Î½Î¹)
âˆ’Ë†nÎ¨Î±((Î³1, . . . , Î³ K, Î¶1, . . . , Î¶ K)(Î¸))dÎ¸â‰¥0
Clearly, the paths also preserve the radius of each mean. The final set of means ËœM=
{Î³1(Ï€/2), . . . , Î³ K(Ï€2)},ËœN={Î¶1(Ï€/2), . . . , Î¶ K(Ï€2)}is contained in the hyperplane normal to
Ë†e2. We can now simply repeat this procedure with all basis vectors orthogonal to Ë†e1to find the set
from the lemma. Since it is collinear with the same radii as M, N , and the means M, N are feasible,
the new set is also feasible by the Cauchy-Schwarz inequality.
Since we can map any set of Gaussian mixture means onto one with the same radii that is collinear
without decreasing the divergence, we can from now on study the problem restricted to a single radial
dimension, using
Î¨Î±
RË†e:KY
k=0R+
0Ã—KY
Îº=0R+
0âˆ’â†’R,
(r0, . . . , r K, Ï0, . . . , Ï K)7âˆ’â†’Î¨Î± KX
Îº=0Ï‰ÎºN(âˆ’ÏÎºË†e, Ïƒ2I)||KX
k=0wkN(rkË†e, Ïƒ2I)!
We can now state the next lemma which implies Theorem O.7.
Theorem O.9. (0,1, . . . , K, 0, . . . ,K)is a global maximizer of Î¨Î±
RË†eunder the constraints r0=
Ï0= 0,|riâˆ’rj| â‰¤ |iâˆ’j| âˆ€i, jâˆˆ {0, . . . , K },|ÏÎ¹âˆ’ÏÏ„| â‰¤ |Î¹âˆ’Ï„| âˆ€Î¹, Ï„âˆˆ {0, . . . ,K}.
87Proof of Theorem O.9. To avoid clutter, we constrain ourselves to the case with only means Âµkand
a single Î½0= 0, since the proof involving several Î½Îºis completely analogous. Consider the line
integral of the gradient
âˆ‡Î¨Î±
R+
0Ë†e:KY
k=0R+
0âˆ’â†’RK,(r0, . . . , r K)7âˆ’â†’ 
âˆ‚
âˆ‚r0Î¨Î±
R+
0Ë†e, . . . ,âˆ‚
âˆ‚rKÎ¨Î±
R+
0Ë†e!
between any feasible (r0, . . . , r K)and the point (0, . . . , K )along the connecting path
Î³r0,...,rK: [0,1]âˆ’â†’KY
k=0R+
0,
t7âˆ’â†’(r0, . . . , r K) +tv:= (r0, . . . , r K) +t[(0, . . . , K )âˆ’(r0, . . . , r K)].
First, observe that rkâ‰¤kâˆ€kâˆˆ {0, . . . , K }anywhere in the feasible region, since rk=Pkâˆ’1
j=0(rj+1âˆ’rj)â‰¤Pkâˆ’1
j=0|rj+1âˆ’rj| â‰¤Pkâˆ’1
j=01 =k. This means that vis componentwise-
nonnegative and vanishes only if (r0, . . . , r K) = (0 , . . . , K ). By Lemma O.6, the gradient âˆ‡Î¨Î±
R+
0Ë†e
is componentwise-nonnegative anywhere along Î³r0,...,rK. We may thus conclude that
Î¨Î±
R+
0Ë†e(0, . . . , K )âˆ’Î¨Î±
R+
0Ë†e(r0, . . . , r K)
=Z
Î³(r0,...,rK)âˆ‡Î¨Î±(r)dr=Z1
0âˆ‡Î¨Î±(Î³(t))Tv(t)dtâ‰¥0.
This concludes the proof of Theorem O.7.
O.1.2 Dominating pair of Laplacian mixtures
We also find an analogous result for Laplacian mixtures.
Theorem O.10. LetÎ¨Î±=HÎ±orÎ¨Î±= Î› Î±. Any pair of Laplacian mixtures with means in
M={Âµ0, . . . , Âµ K}, N={Î½0, . . . , Î½ K} âŠ‚RDsatisfying pairwise L1distance constraints
r0=Ï0= 0,âˆ¥Âµiâˆ’Âµjâˆ¥1â‰¤ |iâˆ’j| âˆ€i, jâˆˆ {0, . . . , K },âˆ¥Î½Î¹âˆ’Î½Ï„âˆ¥1â‰¤ |Î¹âˆ’Ï„| âˆ€Î¹, Ï„âˆˆ {0, . . . ,K}.
is dominated by a pair for Mâˆ—, Nâˆ—of the form
Âµâˆ—
k=kÂ·Ë†e, Î½âˆ—
Îº=âˆ’ÎºÂ·Ë†eâˆ€kâˆˆ {0, . . . , K }âˆ€Îºâˆˆ {0, . . . ,K} withË†e=Â±Ë†ei,
where 1â‰¤iâ‰¤Dsuch that Ë†eiis the i-th canonical basis vector of RD.
Proof.
Lemma O.11. There is a dominating pair Mâˆ—,Nâˆ—located in diametral quadrants of RD: given the
component-wise sign function,
âˆƒÏƒâˆˆ {+,âˆ’}D:sign(Âµk) =Ïƒ=âˆ’sign(Î½Îº)âˆ€ÂµkâˆˆMâˆ—, Î½ÎºâˆˆNâˆ—.
Proof. This is an immediate consequence of considering the canonical basis vectors as normal vectors
in Lemma O.1.
Lemma O.12. ForÂµkâˆˆM,Î½ÎºâˆˆN, define the offset vectors Î´Âµk:=Âµkâˆ’Âµkâˆ’1,Î´Î½Îº:=Î½Îºâˆ’Î½Îºâˆ’1
for1â‰¤kâ‰¤K,1â‰¤Îºâ‰¤ K. A dominating pair Mâˆ—,Nâˆ—has offset vectors located on diametral
simplices of the 1-sphere: âˆƒÏƒâˆˆ {+1,âˆ’1}D:Î´Âµâˆ—
kâˆˆSÏƒ,Î´Î½âˆ—
ÎºâˆˆSâˆ’Ïƒ, with SÏƒ:={xâˆˆRD|
ÏƒTx= 1},âˆ€kâˆˆ {1, . . . , K }âˆ€Îºâˆˆ {1, . . . ,K}.
88Proof. For any pair M, N of means not satisfying this condition, we will construct a new pair that
does and has at least equal divergence. By Lemma O.11, we can assume without generality loss
thatâˆƒÏƒ(D)âˆˆ {+1,âˆ’1}D:sign(Âµk) =Ïƒ(D)=âˆ’sign(Î½Îº)âˆ€ÂµkâˆˆM, Î½ ÎºâˆˆN, where we put a
superscript (D)aboveÏƒfor later reasons. At a later stage of the proof, we will invoke that shifting the
means along the basis vector directions with signs prescribed by Ïƒ(D)are positive almost anywhere
as a consequence of Lemma O.6.
It is useful to recast the optimization constraints in terms of the offset vectors: âˆ€kâˆˆ {1, . . . , K }âˆ€Îºâˆˆ
{1, . . . ,K},
Âµ0=Î½0= 0,âˆ¥Î´Âµkâˆ¥1= max
Ïƒâˆˆ{âˆ’1,+1}D 
ÏƒTÎ´Âµk
â‰¤1,âˆ¥Î½Îºâˆ¥1= max
Ïƒâˆˆ{âˆ’1,+1}D 
ÏƒTÎ´Î½Îº
â‰¤1,
where the constraints between non-ascending and non-neighboring index pairs are implied by the
symmetry and triangle inequality of the norm. For 0â‰¤kâ‰¤K, and 0â‰¤Îºâ‰¤ K , setÂµ(0)
k:=Âµk,
Î½(0)
Îº:=Î½Îº, and define (recursively for 1â‰¤iâ‰¤D) the paths
Î³(i)
k: [0,1]âˆ’â†’RD, Î³(i)
k(t) =Âµ(iâˆ’1)
k+tkX
l=1
1âˆ’(Ïƒ(i)
l)TÎ´Âµ(iâˆ’1)
l
(Ïƒ(D))TË†ei
Ë†ei,
Î´Âµ(iâˆ’1)
l=Âµ(iâˆ’1)
lâˆ’Âµ(iâˆ’1)
lâˆ’1,Ïƒ(i)
l= arg max
Ïƒâˆˆ{âˆ’1,+1}D,ÏƒTË†ei=(Ïƒ(D))TË†ei
ÏƒTÎ´Âµ(iâˆ’1)
l
, Âµ(i)
k=Î³(i)
k(1),
Î¶(i)
Îº: [0,1]âˆ’â†’RD, Î¶(i)
Îº(t) =Î½(iâˆ’1)
Îº +tÎºX
Î¹=1
1âˆ’(Ïƒ(i)
Î¹)TÎ´Î½(iâˆ’1)
Î¹
âˆ’(Ïƒ(D))TË†ei
Ë†ei,
Î´Î½(iâˆ’1)
Î¹ =Î½(iâˆ’1)
Î¹âˆ’Î½(iâˆ’1)
Î¹âˆ’1,Ïƒ(i)
Î¹= arg max
Ïƒâˆˆ{âˆ’1,+1}D,ÏƒTË†ei=âˆ’(Ïƒ(D))TË†ei
ÏƒTÎ´Î½(iâˆ’1)
Î¹
, Î½(i)
Îº=Î¶(i)
Îº(1).
By construction,
Î´Âµ(i)
k=Î´Âµ(iâˆ’1)
k+
1âˆ’(Ïƒ(i)
k)TÎ´Âµ(iâˆ’1)
k
(Ïƒ(D))TË†ei
Ë†ei,
Î´Î½(i)
Îº=Î´Î½(iâˆ’1)
Îº +
1âˆ’(Ïƒ(i)
Îº)TÎ´Î½(iâˆ’1)
Îº
âˆ’(Ïƒ(D))TË†ei
Ë†ei.
We can easily check that Î´Âµ(i)
kâˆˆSÏƒ(i)
k,Î´Î½(i)
ÎºâˆˆSÏƒ(i)
Îº, for1â‰¤kâ‰¤K,1â‰¤Îºâ‰¤ K.
We can moreover verify that, based on the definitions of Ïƒ(i)
kandÏƒ(i)
Îº, the feasibility conditions
âˆ¥Î´Âµ(iâˆ’1)
kâˆ¥1â‰¤1andâˆ¥Î´Î½(iâˆ’1)
Îºâˆ¥1â‰¤1imply âˆ¥Î´Âµ(i)
kâˆ¥1= max Ïƒâˆˆ{âˆ’1,+1}D
ÏƒTÎ´Âµ(i)
k
â‰¤1and
âˆ¥Î´Î½(i)
Îºâˆ¥1= max Ïƒâˆˆ{âˆ’1,+1}D
ÏƒTÎ´Î½(i)
Îº
â‰¤1. Since, furthermore, Âµ(i)
0=Âµ0= 0,Î½(i)
0=Î½0= 0for
1â‰¤iâ‰¤D, and M, N are feasible by assumption, all pairs of M(i):={Âµ(i)
0, . . . , Âµ(i)
K}, N(i):=
{Î½(i)
0, . . . , Î½(i)
K}are also feasible by induction.
Finally, the above simplex and feasibility properties together imply that âˆ€kâˆˆ {1, . . . , K },âˆ€Îºâˆˆ
{1, . . .K},sign
Ë†eT
iÎ´Âµ(j)
k
= 
Ïƒ(D)TË†eiandsign
Ë†eT
iÎ´Î½(j)
Îº
=âˆ’ 
Ïƒ(D)TË†eiifjâ‰¥i. But this
means in particular that we can identify Ïƒ(D)
k=âˆ’Ïƒ(D)
Îº=Ïƒ(D)for1â‰¤kâ‰¤Kand1â‰¤Îºâ‰¤ K.
Putting everything together, we conclude that the pair M(D), N(D)obtained from M, N by con-
catenation of all paths is feasible and has offset vectors on diametral simplices, Î´Âµ(D)
kâˆˆSÏƒ(D),
Î´Î½(D)
ÎºâˆˆSâˆ’Ïƒ(D)âˆ€kâˆˆ {1, . . . , K },âˆ€Îºâˆˆ {1, . . .K}. What is left to show is that M(D), N(D)has
at least the same divergence as M, N . To this end, define the product curves
(ËœM(i)Ã—ËœN(i)): [0,1]âˆ’â†’R(K+K)Ã—D, t7âˆ’â†’(Î³(i)
0(t), . . . , Î³(i)
K(t), Î¶(i)
0(t), . . . , Î¶(i)
K(t)),1â‰¤iâ‰¤D
89connecting the endpoints (M(iâˆ’1), N(iâˆ’1))and(M(i), N(i)), and the image sets C(i):= (ËœM(i)Ã—
ËœN(i))([0,1])âŠ‚R(K+K)Ã—D. The divergence difference is a sum of line integrals:
Î¨Î±(M(D)||N(D))âˆ’Î¨Î±(M||N) =DX
i=1Î¨Î±(M(i)||N(i))âˆ’Î¨Î±(M(iâˆ’1)||N(iâˆ’1))
=DX
i=1Z
C(i) KX
k=0d(Âµk)
Ë†niÎ¨Î±(ËœM(i)||ËœN(i)) +KX
Îº=0d(Î½k)
âˆ’Ë†niÎ¨Î±(ËœM(i)||ËœN(i))!
ds
=DX
i=1Zt=1
t=0KX
k=0d(Âµk)
Ë†niÎ¨Î±(ËœM(i)(t)||ËœN(i)(t))kX
l=1
1âˆ’(Ïƒ(i)
l)TÎ´Âµ(iâˆ’1)
l
+KX
Îº=0d(Î½k)
âˆ’Ë†niÎ¨Î±(ËœM(i)(t)||ËœN(i)(t))ÎºX
Î¹=1
1âˆ’(Ïƒ(i)
Î¹)TÎ´Î½(iâˆ’1)
Î¹
dtâ‰¥0 :
At the start of the proof, we made the assumption without generality loss that âˆƒÏƒ(D)âˆˆ
{+1,âˆ’1}D:sign(Âµk) =Ïƒ(D)=âˆ’sign(Î½Îº)âˆ€ÂµkâˆˆM, Î½ ÎºâˆˆN. Therefore, the directional
derivatives d(Âµk)
Ë†ni,d(Î½Îº)
âˆ’Ë†nialong the curve directions Â±Ë†ni=Â± 
(Ïƒ(D))TË†eiË†eiare nonnegative. The
determinant factorsPk
l=1
1âˆ’(Ïƒ(i)
l)TÎ´Âµ(iâˆ’1)
l
and
1âˆ’(Ïƒ(i)
Î¹)TÎ´Î½(iâˆ’1)
Î¹
are nonnegative due to
the feasibility of M(i), N(i)âˆ€iâˆˆ {0, . . . , D }.
Lemma O.13. There is a dominating pair Mâˆ—,Nâˆ—withiâˆˆ {1, . . . , D }andÏƒâˆˆ {âˆ’ ,+}such that
Î´Âµk=ÏƒË†ei,Î´Î½Îº=âˆ’ÏƒË†eiâˆ€kâˆˆ {1, . . . , K }âˆ€Îºâˆˆ {1, . . . ,K}.
Proof. By Lemma O.12, we may assume without generality loss that âˆƒÏƒâˆˆ {+1,âˆ’1}D:Î´ÂµkâˆˆSÏƒ,
Î´Î½ÎºâˆˆSâˆ’Ïƒ, with SÏƒ:={xâˆˆRD|ÏƒTx= 1},âˆ€kâˆˆ {1, . . . , K }âˆ€Îºâˆˆ {1, . . . ,K}.
The following curves will be useful. For 1â‰¤iÌ¸=jâ‰¤D,lâˆˆ {1, . . . , D }define
Î³(ijl)
k: [0,1]âˆ’â†’RD, Î³(ijl)
k(t) =Âµkfork < l
Âµk+t(ÏƒTË†ei)(Ë†eT
iÎ´Âµl)((ÏƒTË†ej)Ë†ejâˆ’(ÏƒTË†ei)Ë†ei)forkâ‰¥l
Î¶(ijÎ¹)
Îº: [0,1]âˆ’â†’RD, Î¶(ijÎ¹)
Îº(t) =Î½ÎºforÎº < Î¹
Î½Îº+t(ÏƒTË†ei)(Ë†eT
iÎ´Î½Î¹)((ÏƒTË†ej)Ë†ejâˆ’(ÏƒTË†ei)Ë†ei)forÎºâ‰¥Î¹.
By construction, Î³(ijl)
k(0) = Âµk,Î¶(ijÎ¹)
Îº(0) = Î½Îº,Î³(ijl)
0(1) = 0 ,Î¶(ijl)
0(1) = 0 ,Î³(ijl)
k(1)âˆ’Î³(ijl)
kâˆ’1(1) =
Î´Âµkfor1â‰¤kÌ¸=lâ‰¤K,Î¶(ijÎ¹)
Îº(1)âˆ’Î¶(ijÎ¹)
Îºâˆ’1(1) = Î´Î½Îºfor1â‰¤ÎºÌ¸=Î¹â‰¤ K,

Î³(ijl)
l(1)âˆ’Î³(ijl)
lâˆ’1(1)T
Ë†em=ï£±
ï£²
ï£³0form=i
(Ë†eT
iÏƒ)Î´ÂµT
lË†ei+ (Ë†eT
jÏƒ)Î´ÂµT
lË†ejform=j
Î´ÂµT
lË†emelse,
and

Î¶(ijÎ¹)
Î¹(1)âˆ’Î¶(ijÎ¹)
Î¹âˆ’1(1)T
Ë†em=ï£±
ï£²
ï£³0form=i
(âˆ’Ë†eT
iÏƒ)Î´Î½T
Î¹Ë†ei+ (âˆ’Ë†eT
jÏƒ)Î´Î½T
Î¹Ë†ejform=j
Î´Î½T
Î¹Ë†emelse.
In particular, this implies that the new pairs of sets (M(l):={Î³(ijl)
0(1), . . . , Î³(ijl)
K(1)}, N),
(M, N(Î¹):={Î¶(ijÎ¹)
0(1), . . . , Î¶(ijÎ¹)
K(1)})are feasible.
Assume that âˆƒlâˆˆ {1, . . . , K }, mâˆˆ {1, . . . , K }, iâˆˆ {1, . . . , D }, jâˆˆ {1, . . . , D }, iÌ¸=
j: (Î´ÂµT
lË†ei)Ì¸= 0âˆ§(Î´ÂµT
mË†ej)} Ì¸= 0. Note that we include the possibility of l=m. We can
then assume without generality loss (by Lemma O.1) that the conditions of Lemma O.6 hold for
either Ë†n+=1âˆš
2(Ë†(ÏƒTË†ej)ejâˆ’(ÏƒTË†ei)Ë†ei)orË†nâˆ’=1âˆš
2((ÏƒTË†ei)Ë†eiâˆ’(ÏƒTË†ej)Ë†ej). In the case of Ë†n+,
consider the product curve
(ËœM(l)Ã—N): [0,1]âˆ’â†’RK+KÃ—D, t7âˆ’â†’(Î³(ijl)
0(t), . . . , Î³(ijl)
K(t), Î½0, . . . , Î½ K)
90and the image set C(l):= (ËœM(l)Ã—N)([0,1]). The divergence difference has the form
Î¨Î±(M(l)||N)âˆ’Î¨Î±(M||N) =Z
C(l) KX
k=0d(Âµk)
Ë†n+Î¨Î±(ËœM(l)||N)!
ds
=Zt=1
t=0 KX
k=ld(Âµk)
Ë†n+Î¨Î±(ËœM(l)(t)||N)!
âˆš
2(ÏƒTË†ei)(Ë†eT
iÎ´Âµl)dtâ‰¥0 :
By construction, the directional derivatives d(Âµk)
Ë†n+Î¨Î±(ËœM(k)(t)||N)are nonnegative. Likewise, the
determinant termâˆš
2(ÏƒTË†ei)(Ë†eT
iÎ´Âµl)is nonnegative by the simplex condition together with feasibility
of(M, N ).
If the separability condition is fulfilled for Ë†nâˆ’instead, we can repeat the argument with indices iâ†”j
swapped and index minstead of l.
An analogous argument holds if âˆƒÎ¹âˆˆ {1, . . . ,K}, Ï€âˆˆ {1, . . . ,K}, iâˆˆ {1, . . . D}, jâˆˆ
{1, . . . , D }, iÌ¸=j: (Î´Î½T
Î¹Ë†ei)Ì¸= 0âˆ§(Î´Î½T
Ï€Ë†ej)} Ì¸= 0.
Theorem O.10 immediately follows from Lemma O.13 by induction.
O.2 Reduction of the divergence to a univariate integral
In the previous subsections, we showed that Î¨Î±(N||M)is maximized by collinear and equidistant sets
of means. This allows to evaluate Î¨Î±(N||M)through a one-dimensional integral via marginalization.
Recall (Theorem O.9). Note furthermore that in both cases, constraining the first element of either N
orMinto the origin incurs no generality loss due to the translation invariance of Î¨Î±.
Being able to constrain the problem to a single dimension, we now show
Lemma O.14. In the collinear case, we may evaluate Î¨Î±as the one-dimensional divergence Î¨1D
Î±
between two mixturesPK
k=0wkN(rk, Ïƒ2),PK
Îº=0Ï‰ÎºN(âˆ’ÏÎº, Ïƒ2)of univariate Gaussians centered
around the means rkandâˆ’ÏÎº,
Î¨Î± KX
Îº=0Ï‰ÎºN(âˆ’ÏÎºË†e, Ïƒ2I)||KX
k=0wkN(rkË†e, Ïƒ2I)!
= Î¨1D
Î± KX
Îº=0Ï‰ÎºN(âˆ’ÏÎº, Ïƒ2)||KX
k=0wkN(rk, Ïƒ2)!
.
Proof. Without loss of generality, we may assume Ë†eto be the indicator vector of the first component
due to rotation invariance of Î¨Î±. Using bracketed superscripts (0)to indicate the first vector
component, and (1 :) to indicate the rest, we can use Fubiniâ€™s theorem to write
Î¨Î± KX
Îº=0Ï‰ÎºN(âˆ’ÏÎºË†e, Ïƒ2I)||KX
k=0wkN(rkË†e, Ïƒ2I)!
=Z
RD"KX
Îº=0Ï‰kN(x| âˆ’ÏÎºË†e, Ïƒ2I)#Î±"KX
k=0wkN(x|rkË†e, Ïƒ2I)#(1âˆ’Î±)
dx(0)dx(1:)
=Z
RDâˆ’1N(x(1:)|0, Ïƒ2I)dx(1:)Z
R"KX
Îº=0Ï‰kN(x(0)| âˆ’ÏÎº, Ïƒ2)#Î±"KX
k=0wkN(x(0)|rk, Ïƒ2)#(1âˆ’Î±)
dx(0)
= 1Â·Î¨1D
Î± KX
Îº=0Ï‰ÎºN(âˆ’ÏÎº, Ïƒ2)||KX
k=0wkN(rk, Ïƒ2)!
.
The proof is analogous for the Laplacian mechanism.
91O.3 Randomized Response Proofs
The following two results (first for the RÃ©nyi divergence, second for the hockey stick divergence)
show that our (group) privacy guarantees for randomized response are given by the maximum of two
terms that can be evaluated in constant time.
Theorem O.15.
arg max
Ï„âˆˆ[0,1](K++1)+( Kâˆ’+1)1X
z=0ï£®
ï£¯ï£°ï£«
ï£­K++1X
i=1w(1)
i(1âˆ’ |zâˆ’Ï„(1)
i|)ï£¶
ï£¸Î±
Â·ï£«
ï£­Kâˆ’+1X
j=1w(2)
j(1âˆ’ |zâˆ’Ï„(2)
j|)ï£¶
ï£¸1âˆ’Î±ï£¹
ï£ºï£»
subject to
Ï„(1)
iâˆˆ {Î¸,1âˆ’Î¸},âˆ€i,
Ï„(1)
jâˆˆ {Î¸,1âˆ’Î¸},âˆ€j,
Ï„(1)
1=Ï„(2)
1,
with
w(1)
i= Binomial( iâˆ’1|Kâˆ’, r),
w(2)
j= Binomial( jâˆ’1|K+, r),
is given by
arg max {Î¨ (mËœÏ„(1)âˆ¥mËœÏ„(2)),Î¨ (mË†Ï„(1)âˆ¥mË†Ï„(2))},
where
ËœÏ„(1)
1= ËœÏ„(2)
1=:Ï„âˆˆ {Î¸,1âˆ’Î¸}(symmetric ),
ËœÏ„(1)
2,...,(K++1)= 1âˆ’Ï„,
ËœÏ„(2)
2,...,(Kâˆ’+1)=Ï„
and
Ë†Ï„(1)
1= Ë†Ï„(2)
1=:Ï„âˆˆ {Î¸,1âˆ’Î¸}(symmetric ),
Ë†Ï„(1)
2,...,(K++1)=Ï„,
Ë†Ï„(2)
2,...,(Kâˆ’+1)= 1âˆ’Ï„
Proof.
The strategy is to show that
(a)Any mixture pair (mÏ„(1), mÏ„(2))where mÏ„(1)has greater mass in (1âˆ’Ï„), i.e.,X
i:Ï„(1)
i=1âˆ’Ï„w(1)
iâ‰¥X
j:Ï„(2)
j=1âˆ’Ï„w(2)
j, is dominated by (mËœÏ„(1), mËœÏ„(2))
(b)Any mixture pair (mÏ„(1), mÏ„(2))where mÏ„(2)has greater mass in (1âˆ’Ï„), i.e.,X
i:Ï„(1)
i=1âˆ’Ï„w(1)
iâ‰¤X
j:Ï„(2)
j=1âˆ’Ï„w(2)
j, is dominated by (mË†Ï„(1), mË†Ï„(2))
Notation:
1X
z=0ï£®
ï£¯ï£°ï£«
ï£­K++1X
i=1w(1)
i(1âˆ’ |zâˆ’Ï„(1)
i|)ï£¶
ï£¸Î±
Â·ï£«
ï£­Kâˆ’+1X
j=1w(2)
j(1âˆ’ |zâˆ’Ï„(2)
j|)ï£¶
ï£¸1âˆ’Î±ï£¹
ï£ºï£»=1X
z=0f(x(z))g(y(z)),
where fstrictly convex, increasing, and gstrictly convex, decreasing.
Trivial case: Î¸=1
2, thus assume Î¸Ì¸=1
2for the rest of the proof.
92ğœƒğœƒ
ğœğœâ€²0
01
1ğ›¿ğ›¿ğ‘¥ğ‘¥
ğ›¿ğ›¿ï¿½ğ‘¥ğ‘¥
(here: ğœğœ)(here: ğœğœ)ğœğœ11=ğœğœ12
ğœğœ
1âˆ’ğœƒğœƒ
ğœƒğœƒ 1âˆ’ğœƒğœƒFigure 26: Visualization of Î´xandÎ´xâ€². Color legend: The color â€œblueâ€ refers to terms with superscript
(1), while â€œredâ€ refers to those with (2).
Proof of (a):
Given: Pair of mixtures (mÏ„(1), mÏ„(2))such that w(1)
1âˆ’Ï„:=X
i:Ï„(1)
i=1âˆ’Ï„w(1)
iâ‰¥X
j:Ï„(2)
j=1âˆ’Ï„w(2)
j=:
w(2)
1âˆ’Ï„.
Further define w(1)
Ï„= 1âˆ’w(1)
1âˆ’Ï„, w(2)
Ï„= 1âˆ’w(2)
1âˆ’Ï„.
Define a newmixture via
Ï„â€²(1)
1=Ï„â€²(2)
1=Ï„,
Ï„â€²(1)
2...,K ++1= 1âˆ’Ï„,
Ï„â€²(2)
i=Ï„(2)
i,âˆ€i.
Assume w.l.o.g. that Ï„0= max {Î¸,1âˆ’Î¸}, else swap roles of 0â†”1in the argument.
Decompose x:x(z) =x0(z) +Î´x(z), where
x0(z) =w(1)
1âˆ’Ï„(1âˆ’ |zâˆ’(1âˆ’Ï„)|) +w(1)
1(1âˆ’ |zâˆ’Ï„|),i.e., â€œpart that stays fixedâ€; see Fig. 26 ,
Î´x(z) = (w(1)
Ï„âˆ’w(1)
1)(1âˆ’ |zâˆ’Ï„|),i.e., â€œpart that relocatesâ€; see Fig. 26 .
Similarly,
xâ€²(z) =xâ€²
0(z) +Î´xâ€²(z),
and by construction,
xâ€²
0(z) =x0(z),
Î´xâ€²(z) =Î´x(1âˆ’z) (â‹†)
We can assume w.l.o.g. Ï„0= max {Î¸,1âˆ’Î¸}=â‡’Î´x(1)> Î´x(0) (â‹†â‹†)(else, swap 1â†”0)
Then, Î¸Ì¸=1
2, hence we can assume w(1)
Ï„> w(1)
1(else, mâ€²â‰¡min the first place).
Î¨Î±(mÏ„â€²(1), mÏ„â€²(2))âˆ’Î¨Î±(mÏ„(1), mÏ„(2))
(â‹†)= [f(x0(0) + Î´x(1))âˆ’f(x0(0) + Î´x(0))]g(y(0)))âˆ’[f(x0(1) + Î´x(1))âˆ’f(x0(1) + Î´x(0))]g(y(1))
Then, using that fis strictly convex, we have
(â‹†â‹†)
> fâ€²(x0(0) + Î´x(0))g(y(0))(Î´x(1)âˆ’Î´x(0))âˆ’fâ€²(x0(1) + Î´x(0))g(y(1))(Î´x(1)âˆ’Î´x(0))
Where fâ€²(x) =xÎ±âˆ’1, and thus fâ€²(x)g(y) = (Î±âˆ’1)
x
yÎ±âˆ’1
= (Î±âˆ’1)"x0(0) + Î´x(0)
y(0)Î±âˆ’1
âˆ’x0(1) + Î´x(0)
y(1)Î±âˆ’1#
(Î´x(1)âˆ’Î´x(0))
= (Î±âˆ’1)ï£®
ï£° 
(1âˆ’w(1)
1âˆ’Ï„(1âˆ’Ï„) +w(1)
1âˆ’Ï„Ï„
(1âˆ’w(2)
1âˆ’Ï„)(1âˆ’Ï„) +w(2)
1âˆ’Ï„Ï„!Î±âˆ’1
âˆ’ 
w(1)
1Ï„+ (1âˆ’w(1)
1)(1âˆ’Ï„)
(1âˆ’w(2)
1âˆ’Ï„)Ï„+w(2)
1âˆ’Ï„(1âˆ’Ï„)!Î±âˆ’1ï£¹
ï£»(Î´x(1)âˆ’Î´x(0))
>0.
93ğœƒğœƒ
Ìƒğœğœ0
01
1ğ›¿ğ›¿ï¿½ğ‘¦ğ‘¦ğ›¿ğ›¿ğ‘¦ğ‘¦â€²
(here: ğœğœ)ğœğœâ€²
1âˆ’ğœƒğœƒ
ğœƒğœƒ 1âˆ’ğœƒğœƒ(here: ğœğœ)Figure 27: Visualization of Î´yandÎ´yâ€². The color â€œblueâ€ refers to terms with superscript (1), while
â€œredâ€ refers to those with (2).
The last inequality follows from the following facts. Since we assume w(1)
Ï„> w(1)
1(else, mâ‰¡mâ€²in
the first place), we can bound the second term by:
 
w(1)
1Ï„+ (1âˆ’w(1)
1)(1âˆ’Ï„)
(1âˆ’w(2)
1âˆ’Ï„)Ï„+w(2)
1âˆ’Ï„(1âˆ’Ï„)!Î±âˆ’1
< 
(1âˆ’w(1)
1âˆ’Ï„)Ï„+w1âˆ’Ï„(1âˆ’Ï„)
(1âˆ’w(2)
1âˆ’Ï„)Ï„+w(1)
1âˆ’Ï„(1âˆ’Ï„)â‰¤1!Î±âˆ’1
(i)
Moreover, we have that the first term is bounded by
 
(1âˆ’w(1)
1âˆ’Ï„)(1âˆ’Ï„) +w(1)
1âˆ’Ï„Ï„
(1âˆ’w(2)
1âˆ’Ï„)(1âˆ’Ï„) +w(2)
1âˆ’Ï„Ï„!Î±âˆ’1
â‰¥1(ii)
(strict >ifw(1)
1âˆ’Ï„> w(2)
1âˆ’Ï„). The conclusion follows from (i),(ii), and since WLOG Ï„ >(1âˆ’Ï„).
We have shown so far:
Î¨Î±(mÏ„â€²(1), mÏ„â€²(2))>Î¨Î±(mÏ„(1), mÏ„(2)).
To complete the proof of (a), we show now that Î¨Î±(mËœÏ„(1), mËœÏ„(2))>Î¨Î±(mÏ„â€²(1), mÏ„â€²(2)).
Idea: Apply symmetric argument to means Ï„(2).
Since 1 = w(1)
Ï„+w(1)
1âˆ’Ï„=w(2)
Ï„+w(2)
1âˆ’Ï„, we have
w(2)
Ï„â€²=w(2)
Ï„â‰¥w(1)
Ï„> w(1)
Ï„â€²,
where w(1)
Ï„â€², w(2)
Ï„â€²are defined like w(1)
Ï„, w(2)
Ï„but for new means Ï„â€².
Decompose yâ€²:yâ€²(z) =yâ€²
0(z) +Î´yâ€²(z), where
yâ€²
0(z) =w(2)
Ï„(1âˆ’ |zâˆ’Ï„|),i.e., â€œpart that stays fixedâ€; see Fig. 27. ,
Î´yâ€²(z) =w(2)
1âˆ’Ï„(1âˆ’ |zâˆ’(1âˆ’Ï„)|),i.e., â€œpart that relocatesâ€; see Fig. 27.
We can assume w.l.o.g. that Ï„= max {Î¸,1âˆ’Î¸}, soÎ´yâ€²(0)> Î´yâ€²(1).
Î¨Î±(mËœÏ„(1), mËœÏ„(2))âˆ’Î¨Î±(mÏ„â€²(1), mÏ„â€²(2))
=f(xâ€²(0)) [g(yâ€²
0(0) + Î´yâ€²(1))âˆ’g(yâ€²
0(0) + Î´yâ€²(0))] + f(xâ€²(1)) [g(yâ€²
0(1) + Î´yâ€²(0))âˆ’g(yâ€²
0(1) + Î´yâ€²(1))]
And using the strict convexity of fwe have
>[f(xâ€²(1))gâ€²(yâ€²
0(1) + Î´yâ€²(1))âˆ’f(xâ€²(0))gâ€²(yâ€²
0(0) + Î´yâ€²(1))] ( Î´yâ€²(0)âˆ’Î´yâ€²(1))
= (Î±âˆ’1)xâ€²(0)
yâ€²
0(0) + Î´yâ€²(1)Î±
âˆ’xâ€²(1)
yâ€²
0(1) + Î´yâ€²(1)Î±
(Î´yâ€²(0)âˆ’Î´yâ€²(1))
= (Î±âˆ’1)" 
w(1)
Ï„(1âˆ’Ï„) + (1 âˆ’w(1)
Ï„)Ï„
1âˆ’Ï„!Î±
âˆ’ 
w(1)
Ï„Ï„+ (1âˆ’w(1)
Ï„)(1âˆ’Ï„)
w(2)
Ï„Ï„+ (1âˆ’w(2)
Ï„)(1âˆ’Ï„)!Î±#
Â·(Î´yâ€²(0)âˆ’Î´yâ€²(1))
>0.
94Proof of (b)is fully analogous.
This proves the theorem statement, since condition (a) or (b) is fulfilled for any mixture pair, and
hence any mixture pair is dominated by the argmax of the two mixture pairs stated in the theorem.
Theorem O.16.
arg max
Ï„âˆˆ[0,1](K++1)+( Kâˆ’+1)1X
z=0ï£®
ï£°ï£«
ï£­K++1X
i=1w(1)
i(1âˆ’ |zâˆ’Ï„(1)
i|)ï£¶
ï£¸âˆ’eÎµï£«
ï£­Kâˆ’+1X
j=1w(2)
j(1âˆ’ |zâˆ’Ï„(2)
j|)ï£¶
ï£¸ï£¹
ï£»
subject to
Ï„(1)
iâˆˆ {Î¸,1âˆ’Î¸},âˆ€i,
Ï„(1)
jâˆˆ {Î¸,1âˆ’Î¸},âˆ€j,
Ï„(1)
1=Ï„(2)
1,
with
w(1)
i= Binomial( iâˆ’1|Kâˆ’, r),
w(2)
j= Binomial( jâˆ’1|K+, r),
is given by
arg max {Î¨ (mËœÏ„(1)âˆ¥mËœÏ„(2)),Î¨ (mË†Ï„(1)âˆ¥mË†Ï„(2))},
where
ËœÏ„(1)
1= ËœÏ„(2)
1=:Ï„âˆˆ {Î¸,1âˆ’Î¸}(symmetric ),
ËœÏ„(1)
2,...,(K++1)= 1âˆ’Ï„,
ËœÏ„(2)
2,...,(Kâˆ’+1)=Ï„
and
Ë†Ï„(1)
1= Ë†Ï„(2)
1=:Ï„âˆˆ {Î¸,1âˆ’Î¸}(symmetric ),
Ë†Ï„(1)
2,...,(K++1)=Ï„,
Ë†Ï„(2)
2,...,(Kâˆ’+1)= 1âˆ’Ï„
Proof sketch.
The strategy is to show that
(a)Any mixture pair (mÏ„(1), mÏ„(2))where mÏ„(1)has greater mass in (1âˆ’Ï„), i.e.,X
i:Ï„(1)
i=1âˆ’Ï„w(1)
iâ‰¥X
j:Ï„(2)
j=1âˆ’Ï„w(2)
j, is dominated by (mËœÏ„(1), mËœÏ„(2))
(b)Any mixture pair (mÏ„(1), mÏ„(2))where mÏ„(2)has greater mass in (1âˆ’Ï„), i.e.,X
i:Ï„(1)
i=1âˆ’Ï„w(1)
iâ‰¤X
j:Ï„(2)
j=1âˆ’Ï„w(2)
j, is dominated by (mË†Ï„(1), mË†Ï„(2))
Notation:
1X
z=0ï£®
ï£°ï£«
ï£­K++1X
i=1w(1)
i(1âˆ’ |zâˆ’Ï„(1)
i|)ï£¶
ï£¸âˆ’eÎµï£«
ï£­Kâˆ’+1X
j=1w(2)
j(1âˆ’ |zâˆ’Ï„(2)
j|)ï£¶
ï£¸ï£¹
ï£»=1X
z=0f(x(z)âˆ’eÎµy(z)),
where f= [Â·]+is convex and differentiable anywhere except at 0.
Trivial case: Î¸=1
2, thus assume Î¸Ì¸=1
2for the rest of the proof.
95Proof of (a):
Given: Pair of mixtures (mÏ„(1), mÏ„(2))such that w(1)
1âˆ’Ï„:=X
i:Ï„(1)
i=1âˆ’Ï„w(1)
iâ‰¥X
j:Ï„(2)
j=1âˆ’Ï„w(2)
j=:
w(2)
1âˆ’Ï„.
Further define w(1)
Ï„= 1âˆ’w(1)
1âˆ’Ï„, w(2)
Ï„= 1âˆ’w(2)
1âˆ’Ï„.
Define a newmixture via
Ï„â€²(1)
1=Ï„â€²(2)
1=Ï„,
Ï„â€²(1)
2...,K ++1= 1âˆ’Ï„,
Ï„â€²(2)
i=Ï„(2)
i,âˆ€i.
Assume w.l.o.g. that Ï„0= max {Î¸,1âˆ’Î¸}, else swap roles of 0â†”1in the argument.
Decompose x:x(z) =x0(z) +Î´x(z), where
x0(z) =w(1)
1âˆ’Ï„(1âˆ’ |zâˆ’(1âˆ’Ï„)|) +w(1)
1(1âˆ’ |zâˆ’Ï„|),i.e., â€œpart that stays fixedâ€; see Fig. 26 ,
Î´x(z) = (w(1)
Ï„âˆ’w(1)
1)(1âˆ’ |zâˆ’Ï„|),i.e., â€œpart that relocatesâ€; see Fig. 26 .
Similarly,
xâ€²(z) =xâ€²
0(z) +Î´xâ€²(z),
and by construction,
xâ€²
0(z) =x0(z),
Î´xâ€²(z) =Î´x(1âˆ’z) (â‹†)
We can assume w.l.o.g. Ï„0= max {Î¸,1âˆ’Î¸}=â‡’Î´x(1)> Î´x(0) (â‹†â‹†)(else, swap 1â†”0)
Then, Î¸Ì¸=1
2, hence we can assume w(1)
Ï„> w(1)
1(else, mâ€²â‰¡min the first place).
Î¨Î±(mÏ„â€²(1), mÏ„â€²(2))âˆ’Î¨Î±(mÏ„(1), mÏ„(2))
(â‹†)= [f(x0(0) + Î´x(1)âˆ’eÎµy(0))âˆ’f(x0(0) + Î´x(0)âˆ’eÎµy(0))]
âˆ’[f(x0(1) + Î´x(1)âˆ’eÎµy(1))âˆ’f(x0(1) + Î´x(0)âˆ’eÎµy(1))]g(y(1))
Assume for now that x0(0) + Î´x(0)âˆ’eÎµy(0)Ì¸= 0Ì¸=x0(1) + Î´x(0)âˆ’eÎµy(1).
(â‹†â‹†)
â‰¥(fâ€²(x0(0) + Î´x(0)âˆ’eÎµy(0))âˆ’fâ€²(x0(1) + Î´x(0)âˆ’eÎµy(1))) ( Î´x(1)âˆ’Î´x(0))
=fâ€²h
(1âˆ’w(1)
1âˆ’Ï„)(1âˆ’Ï„) +w(1)
1âˆ’Ï„Ï„
âˆ’eÎµ
(1âˆ’w(2)
1âˆ’Ï„)(1âˆ’Ï„) +w(2)
1âˆ’Ï„Ï„i
(Î´x(1)âˆ’Î´x(0))
âˆ’fâ€²h
w(1)
1Ï„+ (1âˆ’w(1)
1)(1âˆ’Ï„)
âˆ’eÎµ
(1âˆ’w(2)
1âˆ’Ï„)Ï„+w(2)
1âˆ’Ï„(1âˆ’Ï„)i
(Î´x(1)âˆ’Î´x(0))
â‰¥fâ€²h
(1âˆ’w(1)
1âˆ’Ï„)(1âˆ’Ï„) +w(1)
1âˆ’Ï„Ï„
âˆ’eÎµ
(1âˆ’w(2)
1âˆ’Ï„)(1âˆ’Ï„) +w(2)
1âˆ’Ï„Ï„i
(Î´x(1)âˆ’Î´x(0))
âˆ’fâ€²h
(1âˆ’w(1)
1âˆ’Ï„)Ï„+w1âˆ’Ï„(1âˆ’Ï„)
âˆ’eÎµ
(1âˆ’w(2)
1âˆ’Ï„)Ï„+w(2)
1âˆ’Ï„(1âˆ’Ï„)i
(Î´x(1)âˆ’Î´x(0))
â‰¥0.
The second-to-last inequality follows from w(1)
Ï„> w(1)
1(else, mâ‰¡mâ€²in the first place). The last
inequality follows from w(1)
1âˆ’Ï„> w(2)
1âˆ’Ï„and the prior assumption that, WLOG, Ï„ >1âˆ’Ï„.
Ifx0(0) + Î´x(0)âˆ’eÎµy(0)> x0(1) + Î´x(0)âˆ’eÎµy(1)andx0(1) + Î´x(0)âˆ’eÎµy(1) = 0 , then we
can arrive at the same conclusion by using any subderivative of fin[0,1]atx0(1) + Î´x(0)âˆ’eÎµy(1)
instead.
We have shown so far:
Î¨Î±(mÏ„â€²(1), mÏ„â€²(2))â‰¥Î¨Î±(mÏ„(1), mÏ„(2)).
Continue in analogy to Theorem O.15.
96P Towards epoch-level subsampling analysis
Standard composition theorems assume that the outputs of composed mechanisms are conditionally
independent, meaning each output only depends on the previous outputs and the dataset (see, e.g.,
the proof of Proposition 1 in [ 7] and the proof of Theorem 27 in [ 10]). For this reason, one typically
considers subsampling schemes like Poisson subsampling or subsampling without replacement, which
yield independent batches in each iteration.
However, there may be scenarios where batches are not independent, such as when creating batches
by permuting a dataset and splitting it into equal sized chunks. Such correlated subsampling schemes
are appealing because one can, for instance, limit privacy leakage by ensuring that any element
appears in only one iteration per epoch when training a model. A downside of these corellated
subsampling schemes is that they may brake the conditional independence assumption of standard
compositions theorems.
There already exist approaches to analyzing compositions of correlated mechanisms, such as proba-
bilistically upper-bounding a mechanismâ€™s privacy leakage with uncorellated mechanisms [ 75,24].
Furthermore, permutation-based batching has already been discussed in the context of convergence
guarantees for noisy SGD [ 38]. Nevertheless, this problem setting provides a great opportunity
to demonstrate that the utility of our conditional optimal transport framework extends beyond the
subsampling schemes that are typically discussed in amplification-by-subsampling literature.
P.1 Problem setting
For this example, we consider a non-adaptive composition of two mechanisms, where two batches are
created by permuting a dataset and splitting it in half. We assume that our dataset space is composed
of size 2Nsubsets of some finite, discrete set A, i.e.,XâŠ† {xâˆˆ P(A)| |x|= 2N}, and equipped
with substitution relation â‰ƒâˆ†. We further define a per-iteration batch space Ythat is composed of size
Nsubsets, i.e., Y={yâŠ†x|xâˆˆXâˆ§ |y|=N}, which is also equipped with substitution relation
â‰ƒâˆ†. Finally, we assume a base mechanism B:Yâ†’Zwith conditional density by:Zâ†’R+that
maps from this batch space to some output space.
To apply our framework to epoch-level subsampling distributions, we define the composed batch
spaceY, which consists of equal-sized partitions of datasets in X, i.e.,
Ë†Y={(y1, y2)âˆˆY2
orig| âˆƒxâˆˆX:y1Ë™âˆªy2=x}. (37)
On this composed batch space, we can now define our epoch-level subsampling scheme:
Definition P.1. The permute-and-partition subsampling scheme is the subsampling scheme S:Xâ†’
Ë†Ywith
sx((y1, y2)) =( 2N
Nâˆ’1ify1âˆªy2=x
0 otherwise .
This definition follows from the fact that permuting-and-partitioning is equivalent to sampling a
batch of size Nwithout replacement for the first batch and using the remaining Nelements for the
second batch. We further consider the non-adaptively composed base mechanism Ë†B:Ë†Yâ†’Z2with
Ë†B(y1,y2)= (By1, By2)and resultant joint density Ë†b(y1,y2)=by1Â·by2.
P.2 Optimal transport without conditioning
As a baseline, we use Theorem 3.3, i.e., optimal transport without conditioning, to provide a RÃ©nyi-
DP bound for the composed, subsampled mechanism. This guarantee captures the intuition that by
permuting-and-partitioning, we only leak an elementsâ€™ private information once. Again, note we
prove this result for non-adaptive composition.
Theorem P.2. Assume a dataset space XâŠ† {xâˆˆ P(A)| |x|= 2N}, batch space Y={yâŠ†x|
xâˆˆXâˆ§ |y|=N}, and a base mechanism B:Yâ†’Zthat is (Î±, Îµ)-RÃ©nyi-DP w.r.t. single-element
substitution relation â‰ƒâˆ†,Y. LetS:Xâ†’Ë†Ybe the permute-and-partition subsampling scheme defined
in Definition P .1. Let Ë†Bbe the composed base mechanism with Ë†B(y1,y2)= (By1, By2). Then the
subsampled, composed mechanism Ë†M=Ë†Bâ—¦Sis also (Î±, Îµ)-DP w.r.t. single-element substitution
relation â‰ƒâˆ†,X.
97Proof. Consider an arbitrary pair of datasets xâ‰ƒâˆ†,X. By definition of the substitution relation, there
must be some aâˆˆxand some aâ€²âˆˆxâ€²such that xâ€²=x\ {a} âˆª {aâ€²}.
We can now define the following simple coupling between SxandSxâ€²:
Î³((y(1)
1, y(1)
2),(y(2)
1, y(2)
2)) =ï£±
ï£´ï£²
ï£´ï£³ 2N
Nâˆ’1ifaâˆˆy(1)
1âˆ§y(2)
1=y(1)
1\ {a} âˆª {aâ€²} âˆ§y(1)
2=y(2)
2 2N
Nâˆ’1ifaâˆˆy(1)
2âˆ§y(2)
2=y(1)
2\ {a} âˆª {aâ€²} âˆ§y(1)
1=y(2)
1
0 otherwise .
This coupling generates a pair of batch sequences by first applying permute-and-partition to original
dataset xand then replacing awithaâ€²in the one batch it appears in. Because we only couple pairs
that are identical in one batch and differ by a substitution in the other batch, we have per Theorem 3.3
Î¨( Ë†mx||Ë†mxâ€²)â‰¤ max
(y(1)
1,y(1)
2),(y(2)
1,y(2)
2)âˆˆË†Y2Î¨(by(1)
1Â·by(1)
2||by(2)
1Â·by(2)
2)
subject to (y(1)
1â‰ƒâˆ†,Yy(2)
1)âˆ§(y(1)
2=y(2)
2)âˆ¨(y(1)
2â‰ƒâˆ†,Yy(2)
2)âˆ§(y(1)
1=y(2)
1). Either way, one of
the factors is cancelled out when computing the likelihood ratio in the definition of RÃ©nyi-divergence
(see Eq. (7)). We thus have
Î¨( Ë†mx||Ë†mxâ€²)â‰¤ max
y(1)
1,y(2)
1âˆˆY2Î¨(by(1)
1||by(2)
1)
subject to y(1)
1â‰ƒâˆ†,Yy(2)
1. The result then follows from the definition of RÃ©nyi-DP (see Definition 2.2).
P.3 Optimal transport with conditioning
Next, we use Theorem 3.4, i.e., optimal transport between conditional subsampling distributions, to
derive a stronger guarantee. Again, note we prove this result for non-adaptive composition. Note that
this result is similar in spirit to amplification by shuffling [ 76,75], but considers central differential
privacy of mechanisms operating on batches, instead of locally differentially private mechanisms
that operate on individual elements. Again, note that we do not claim to be the first to consider the
permute-and-partition scheme for composed mechanisms (see, e.g., [ 38]). It is just a nice showcase
for the versatility of our framework in analyzing different subsampling schemes.
For the following result and proof we will use the following indexing convention: y(1)
i,kis a batch
associated with the first mixture Ë†mx, event Ai, and the kth iteration. Similarly, y(2)
j,kis a batch
associated with the second mixture Ë†mxâ€², event Ej, and the kth iteration.
Theorem P.3. Assume a dataset space XâŠ† {xâˆˆ P(A)| |x|= 2N}, and corresponding batch
space Y={yâŠ†x|xâˆˆXâˆ§ |y|=N}, equipped with single-element substitution relation
â‰ƒâˆ†,Y. LetS:Xâ†’Ë†Ybe the permute-and-partition subsampling scheme defined in Definition P .1,
and let B:Yâ†’Zbe some base mechanism. Let Ë†Bbe the composed base mechanism with
Ë†B(y1,y2)= (By1, By2), and Ë†M=Ë†Bâ—¦Sbe the subsampled, coposed mechanism. Then, for all
xâ‰ƒâˆ†,Xxâ€²,
Î¨Î±( Ë†mx||Ë†mxâ€²)
â‰¤ max
y(1)
1,1,y(1)
1,2,y(2)
1,1Î¨Î±
by(1)
1,1Â·by(1)
1,2Â·1
2+by(1)
1,2Â·by(1)
1,1Â·1
2||by(2)
1,1Â·by(1)
1,2Â·1
2+by(1)
1,2Â·by(2)
1,1Â·1
2
subject to y(1)
1,1â‰ƒâˆ†,Yy(2)
1,1.
Proof. By definition of the substitution relation, there must be some aâˆˆxand some aâ€²âˆˆxâ€²such
thatxâ€²=x\ {a} âˆª {aâ€²}.
For the original subsampling scheme Sxwe let A1be the event that aappears in the first batch and
A2be the event that aappears in the second batch. For the modified subsampling scheme Sxâ€²we let
E1be the event that aâ€²appears in the first batch and E2be the event that aâ€²appears in the second
batch. We have PSx(A1) =PSx(A2) =PSâ€²x(E1) =PSâ€²x(E2) =1
2.
98One can sample from the distributions conditioned on A1orE1by first sampling uniformly at random
Nelements from the 2Â·Nâˆ’1elements that are notaoraâ€², and then using the remaining elements
as the first batch. Similarly, one can sample from the distributions conditioned on A2orE2by first
sampling uniformly at random Nelements from the 2Â·Nâˆ’1elements that are notaoraâ€², and then
using the remaining elements as the second batch.
We thus have
sx(y(1)
1|A1) =( 2Nâˆ’1
Nâˆ’1ify(1)
1âˆˆA1
0 otherwisesx(y(1)
2|A2) =( 2Nâˆ’1
Nâˆ’1ify(1)
1âˆˆA2
0 otherwise
and
sxâ€²(y(2)
1|E1) =( 2Nâˆ’1
Nâˆ’1ify(1)
1âˆˆE1
0 otherwisesxâ€²(y(2)
2|E2) =( 2Nâˆ’1
Nâˆ’1ify(1)
1âˆˆE2
0 otherwise
Next, we can define a coupling via
Î³(y(1)
1, y(1)
2, y(2)
1, y(2)
2) =(
sx(y(1)
1|A1)if Condition P.4 is fulfilled
0 otherwise
Condition P.4. Batch tuples y(1)
1âˆˆA1, y(1)
2âˆˆA2, y(2)
1âˆˆE1, y(2)
2âˆˆE2fulfill Condition P.4 when
y(1)
1,1=y(1)
2,2, and y(1)
1,2=y(1)
2,1, and y(1)
1,2=y(2)
1,2, and y(1)
2,1=y(2)
2,1, and y(2)
1,1=y(2)
1,1\ {a} âˆª {aâ€²}, and
y(2)
2,2=y(2)
2,2\ {a} âˆª {aâ€²}.
In short: We sample uniformly at random a batch tuple (y(1)
1,1, y(1)
1,2)from A1. We then generate a
batch tuple from A2by permuting the A1tuple. We then generate a batch tuple from E2by replacing
awithaâ€²in the A2tuple. We finally generate a batch tuple from E1by permuting the E2tuple.
Because for each possible value of a batch tuple there is only one combination of the other three
batch tuples such that Î³(y)>0, and in this case Î³(y) = 2Nâˆ’1
Nâˆ’1, this is a valid coupling.
The result then follows from Theorem 3.4 and the constraints imposed by Condition P.4.
Note that this result could be generalized to Kiterations: We can upper-bound the subsampled,
composed mechanismâ€™s divergence by adversarially choosing a K-fold partition of x, constructing
a mixture with K(notK!) components, with each component corresponding to aappearing in a
specific batch, and finally constructing a modified mixture by replacing every occurence of awithaâ€².
P.4 Experimental Evaluation
Finally, we can compare our epoch-level amplification guarantees obtained via optimal transport
with and without conditioning. As an additional baseline, we use 2-fold composition of subsampling
without replacement with batch size 2. Note that, with the baseline, a modified element may appear in
0,1or2of the iterations. For the sake of this simple example, we consider output space Z=R, and
let base mechanism bbe the Gaussian mechanism f+N(0, Ïƒ)withf:Yâ†’ {0,1}and univariate
standard deviation Ïƒâˆˆ {0.5,1.0,2.0,5.0}. By this construction we do not need to reason about fâ€™s
sensitivity in determining the worst-case mixture components.
We make the following observation: For small Ïƒ, both epoch-level guarantees are almost identical and
outperform the without replacement guarantee. With increasing Ïƒ, subsampling without replacement
outperforms Theorem P.2 for some ranges of Î±. Theorem P.2, which is derived via conditional
optimal transport, however yields smaller Îµ.
The main takeaway of this example should be that (a) there is a benefit to using conditional optimal
transport to bound privacy in terms of mixtures and (b) conditional optimal transport can be used to
reason about schemes other than the typically considered Poisson subsampling, subsampling without
replacement, and subsampling with replacement.
99q / N = 0.01
101102
RDP order Î±101102RDPÎµ(Î±)Without replacement
Theorem P.2
Theorem P.3q / N = 0.1
101102
RDP order Î±100101102RDPÎµ(Î±)Without replacement
Theorem P.2
Theorem P.3
q / N = 0.2
101102
RDP order Î±10âˆ’1100101RDPÎµ(Î±)Without replacement
Theorem P.2
Theorem P.3q / N = 0.5
101102
RDP order Î±10âˆ’210âˆ’1100RDPÎµ(Î±)Without replacement
Theorem P.2
Theorem P.3
Figure 28: Comparison of our epoch-level permute-and-partition guarantees with (Theorem P.3)
and without (Theorem P.2) conditioning, as well as subsampling without replacement, for 2-fold
non-adaptive composition. The base mechanism is a Gaussian mechanism with f:Yâ†’ {0,1}and
varying standard deviations Ïƒ. With increasing Ïƒ, Theorem P.3 and subsampling without replacement
become more similar, while Theorem P.3 consistently yields stronger guarantees.
Q Broader impact
Our work contributes towards provably protecting users, and specifically groups of users, from the
negative societal impact of privacy leakage. However, differential privacy may have negatively impact
other aspects of trustworthy machine learning, such as fairness or robustness. Also, training for
differentially private machine learning is often performed with 1< Îµâ‰¤10. This is useful for relative
comparisons between models, but does not impose any meaningful constraints on absolute privacy
leakage (the probability of any event may increase by a factor of up to â‰ˆ22000 ) and may thus be
used to falsely advertise privacy.
100NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: We verify the generality of our proposed framework by formally deriving
various known and novel subsampling guarantees Appendices G to I and K to M. We
verify the benefit of using mechanism-specific guarantees and analyzing group privacy and
subsampling jointly through our experimental evaluation in Section 4.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss limitations in Section 3.5. Our main contribution is a framework
for proving theoretic results, as such many of the points listed below do not apply.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
101Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Yes, we provide formal proofs for all theoretical results in Appendix E through
Appendix P. All theorems and lemmata state the underlying assumptions.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We fully describe our experimental setup in Appendix C. We further provide
an implementation with the supplementary material, which includes the needed environment,
code, configuration files, and plotting scripts.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
102some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide an implementation that includes the needed environment,
code, configuration files, and plotting scripts for reproducing our experimental results
at https://cs.cit.tum.de/daml/group-amplification.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We fully describe our experimental setup in Appendix C. Note that these
experiments only require numerical evaluation of different divergences, meaning there are
no involved data splits or optimizers.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: Our experiments only require numerical evaluation of different divergences.
There are no typical sources of randomness like data splits or weight initializations that
could be visualized with error bars.
Guidelines:
103â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Yes, we report the type and number of CPUs, as well as the number of workers
and allocated runtime Appendix C.2.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research in this paper conforms, in every respect, with the code of ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
104Justification: Yes, we discuss both positive and negative broader impact in Appendix Q,
which we reference in our â€œLimitations, broader impact, and future workâ€ section Sec-
tion 3.5.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We do not propose any new data or models that could be released.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We list the libraries that we use and/or extend to conduct our numerical
experiments in Appendix C.3, including version numbers and licenses. We cite the original
papers where applicable.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
105â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We do not release new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
106Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
107