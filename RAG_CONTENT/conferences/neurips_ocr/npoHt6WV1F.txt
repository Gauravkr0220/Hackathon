NeuralFuse: Learning to Recover the Accuracy of
Access-Limited Neural Network Inference in
Low-Voltage Regimes
Hao-Lun Sun1, Lei Hsiung2, Nandhini Chandramoorthy3, Pin-Yu Chen3, Tsung-Yi Ho4
1National Tsing Hua University2Dartmouth College3IBM Research
4The Chinese University of Hong Kong
s109062594@m109.nthu.edu.tw
lei.hsiung.gr@dartmouth.edu
{pin-yu.chen, nandhini.chandramoorthy}@ibm.com
tyho@cse.cuhk.edu.hk
Abstract
Deep neural networks (DNNs) have become ubiquitous in machine learning, but
their energy consumption remains problematically high. An effective strategy for
reducing such consumption is supply-voltage reduction, but if done too aggressively,
it can lead to accuracy degradation. This is due to random bit-flips in static random
access memory (SRAM), where model parameters are stored. To address this
challenge, we have developed NeuralFuse, a novel add-on module that handles the
energy-accuracy tradeoff in low-voltage regimes by learning input transformations
and using them to generate error-resistant data representations, thereby protecting
DNN accuracy in both nominal and low-voltage scenarios. As well as being easy to
implement, NeuralFuse can be readily applied to DNNs with limited access, such
cloud-based APIs that are accessed remotely or non-configurable hardware. Our
experimental results demonstrate that, at a 1% bit-error rate, NeuralFuse can reduce
SRAM access energy by up to 24% while recovering accuracy by up to 57%. To the
best of our knowledge, this is the first approach to addressing low-voltage-induced
bit errors that requires no model retraining.
1 Introduction
Energy-efficient computing is of primary importance to the effective deployment of deep neural
networks (DNNs), particularly in edge devices and in on-chip AI systems. Increasing DNN com-
putationâ€™s energy efficiency and lowering its carbon footprint require iterative efforts from both
chip designers and algorithm developers. Processors with specialized hardware accelerators for
AI computing, capable of providing orders of magnitude better performance and energy efficiency
for AI computation, are now ubiquitous. However, alongside reduced precision/quantization and
architectural optimizations, endowing such systems with the capacity for low-voltage operation is a
powerful lever for reducing their power consumption.
The computer engineering literature contains ample evidence of the effects of undervolting and
low-voltage operation on accelerator memories that store weights and activations during computation.
Aggressive scaling-down of static random access memoryâ€™s (SRAMâ€™s) supply voltage to below the
rated value saves power, thanks to the quadratic dependence of dynamic power on voltage. Crucially,
Project Page: https://trustsafeai-neuralfuse.static.hf.space
Code: https://github.com/IBM/NeuralFuse
38th Conference on Neural Information Processing Systems (NeurIPS 2024).NeuralFuse
NeuralFuse DisabledNominal Volt
92.6%
89.8%
38.9%
87.8%Low Volt
Bit ErrorsInput datað‘¿={ð‘¥1,â€¦,ð‘¥ð‘›}On-chip Model Inference
(a) The pipeline of the NeuralFuse framework at inference.
18 22 26 30
Energy Saving Percentage (%)507090Perturbed Accuracy (%)Acc. 20.6%
Sav. 29.1%Acc. 48.8%
Sav. 19.0%Acc. 29.9%
Sav. 27.5%Acc. 49.6%
Sav. 21.2%
Acc. 29.8%
Sav. 28.9%Acc. 45.6%
Sav. 24.0%
ResNet18
(Base Model)
1% BEREnergy/accuracy Tradeoff of CIFAR-10 Pre-trained ResNet18
NeuralFuse Architecture
ConvS
ConvL
DeConvSDeConvL
UNetS
UNetLNeuralFuse Architecture
ConvS
ConvL
DeConvSDeConvL
UNetS
UNetL (b) Energy/accuracy tradeoff example.
Figure 1: (a) At inference, NeuralFuse transforms input samples xinto robust data representations.
Thenominal voltage allows models to work as expected, whereas at low voltage , one would encounter
bit errors (e.g., 1%) that cause incorrect inferences. The percentages reflect the accuracy of a CIFAR-
10 pre-trained ResNet18 with and without NeuralFuse in both those voltage cases. (b) On the same
base model (ResNet18), we illustrate the energy/accuracy tradeoff of six NeuralFuse implementations.
The x-axis represents the percentage reduction in dynamic-memory access energy at low-voltage
settings (base model protected by NeuralFuse), as compared to the bit-error-free (nominal) voltage.
The y-axis represents the perturbed accuracy (evaluated at low voltage) with a 1%bit-error rate.
however, it also leads to an exponential increase in bit failures. Memory bit flips cause errors in the
stored weight and activation values [Chandramoorthy et al., 2019, Ganapathy et al., 2017], leading to
catastrophic accuracy loss.
A recent wave of research has proposed numerous techniques for allowing low-voltage operation of
DNN accelerators while preserving their accuracy. Most of these have been either hardware-based
error-mitigation techniques or error-aware robust training of DNN models. On-chip error mitigation
methods have significant performance and power overheads [Chandramoorthy et al., 2019, Reagen
et al., 2016]. On the other hand, some have proposed to generate models that are robust to bit errors
via a specific learning algorithm [Kim et al., 2018, Koppula et al., 2019, Stutz et al., 2021], thereby
eliminating the need for on-chip error mitigation. However, error-aware robust training to find the
optimal set of robust parameters for each model is time- and energy-intensive and may not be possible
in all access-limited settings.
In this paper, therefore, we propose a novel model-agnostic approach: NeuralFuse . This proof-of-
concept machine-learning module offers trainable input transformation parameterized by a relatively
small DNN; and, by enhancing inputâ€™s robustness, it mitigates bit errors caused by very low-voltage
operation, thus serving the wider goal of more accurate inferencing. The pipeline of NeuralFuse
is illustrated in Figure 1. To protect the deployed models from making wrong predictions under
low-power conditions, NeuralFuse accepts scenarios under access-limited neural networks (e.g.,
non-configurable hardware or cloud-based APIs). Specifically, we consider two access-limited
scenarios that are common in the real world: 1) relaxed access , in which â€˜black boxâ€™ model details are
unknown, but backpropagation through those models is possible; and 2) restricted access , in which
the model details are unknown and backpropagation is disallowed. To enable it to deal with relaxed
access, we trained NeuralFuse via backpropagation, and for restricted-access cases, we trained it on
a white-box surrogate model. To the best of our knowledge, this is the first study that leverages a
learning-based method to address random bit errors as a means of recovering accuracy in low-voltage
and access-limited settings.
We summarize our main contributions as follows:
â€¢We propose NeuralFuse , a novel learning-based input-transformation module aimed at enhancing
the accuracy of DNNs that are subject to random bit errors caused by very low voltage operation.
NeuralFuse is model-agnostic, i.e., operates on a plug-and-play basis at the data-input stage and
does not require any re-training of deployed DNN models.
â€¢We explore two practical limited-access scenarios for neural-network inference: relaxed access and
restricted access. In the former setting, we use gradient-based methods for module training. In the
latter one, we use a white-box surrogate model for training, which is highly transferable to other
types of DNN architecture.
â€¢We report the results of an extensive program of experiments with various combinations of DNN
models (ResNet18, ResNet50, VGG11, VGG16, and VGG19), datasets (CIFAR-10, CIFAR-100,
2GTSRB, and ImageNet-10), and NeuralFuse implementations of different architectures and sizes.
These show that NeuralFuse can consistently increase the perturbed accuracy (accuracy evaluated
under random bit errors in weights) by up to 57%, while simultaneously saving up to 24% of
the energy normally required for SRAM access, based on our realistic characterization of bit-cell
failures for a given memory array in a low-voltage regime inducing a 0.5%/1%bit-error rate.
â€¢We demonstrate NeuralFuseâ€™s transferability (i.e., adaptability to unseen base models), versatility
(i.e., ability to recover low-precision quantization loss), and competitiveness (i.e., state-of-the-art
performance) in various scenarios, establishing it as a promising proof-of-concept for energy-
efficient, resilient DNN inference.
2 Related Work and Background
Software-based Energy-saving Strategies. Various recent studies have proposed software-based
methods of reducing computingâ€™s energy consumption. For instance, quantization techniques have
been reported to reduce the precision required for storing model weights, and thus to decrease total
memory storage [Gong et al., 2014, Rastegari et al., 2016, Wu et al., 2016]. On the other hand,
Yang et al. [2017] - who proposed energy-aware pruning on each layer and fine-tuning of weights
to maximize final accuracy - suggested several ways to reduce DNNsâ€™ energy consumption. For
example, they devised the ECC framework, which compresses DNN models to meet a given energy
constraint [Yang et al., 2019a], and a method of compressing such models via joint pruning and
quantization [Yang et al., 2020]. It is also feasible, during DNN training, to treat energy constraints
as an optimization problem and thereby reduce energy consumption while maximizing training
accuracy [Yang et al., 2019b]. However, unlike ours, all these methods imply changing either model
architectures or model weights.
Hardware-based Energy-saving Strategies. Prior studies have also explored ways of improving
energy efficiency via specially designed hardware. Several of them have focused on the undervolting
of DNN accelerators and proposed methods to maintain accuracy in the presence of bit errors. For in-
stance, Reagen et al. [2016] proposed an SRAM fault-mitigation technique that rounds faulty weights
to zero to avoid degradation of prediction accuracy. Srinivasan et al. [2016] recommended storing
sensitive MSBs (most significant bits) in robust SRAM cells to preserve accuracy. Chandramoorthy
et al. [2019] proposed dynamic supply-voltage boosting to improve the resilience of memory-access
operations; and the learning-based approach proposed by Stutz et al. [2021] aims to find models that
are robust to bit errors. The latter paper discusses several techniques for improving such robustness,
notably quantization, weight-clipping, random bit-error training, and adversarial bit-error training. Its
authors concluded from their experiments that a combination of quantization, weight-clipping, and
adversarial bit-error training will yield excellent performance. However, they also admitted that the
relevant training process was sensitive to hyperparameter settings, and hence, it might come with a
challenging training procedure.
However, we suggest that all the methods mentioned above are difficult to implement and/or unsuitable
for use in real-world access-limited settings. For example, the weights of DNN models packed on
embedded systems may not be configurable or updatable, making model retraining (e.g., Stutz et al.
[2021]) non-viable in that scenario. Moreover, DNN training is already a tedious and time-consuming
task, so adding error-aware training to it may further increase its complexity and, in particular, make
hyperparameter searches more challenging. Ã–zdenizci and Legenstein [2022] also reported that
error-aware training was ineffective for large DNNs with millions of bits. NeuralFuse obviates the
need for model retraining via an add-on trainable input-transformation function parameterized by a
relatively small secondary DNN.
SRAM Bit Errors in DNNs. Low voltage-induced memory bit-cell failures can cause bit-flips
from 0 to 1 and vice versa. In practice, SRAM bit errors increase exponentially when the supply
voltage is scaled below Vmin, i.e., the minimum voltage required to avoid them. This phenomenon
has been studied extensively in the prior literature, including work by Chandramoorthy et al. [2019]
and Ganapathy et al. [2017]. The specific increases in bit errors as voltage scales down, in the
case of an SRAM array of 512Ã—64bits with a 14nm technology node, is illustrated in Figure 2.
The corresponding dynamic energy per SRAM read access, measured at each voltage at a constant
frequency, is shown on the right-hand side of the figure. In this example, accessing the SRAM at
3Figure 2: The bit-error rates (left) and dynamic energy per memory access versus voltage for static
random access memory arrays (right) as reported by Chandramoorthy et al. [2019]. The x-axis shows
voltages normalized with respect to the minimum bit error-free voltage ( Vmin).
0.83Vminleads to a 1%bit-error rate, and at the same time, dynamic energy per access is reduced by
approximately 30%. This can lead to DNNs making inaccurate inferences, particularly when bit-flips
occur at the MSBs. However, improving robustness to bit errors can allow us to lower Vminand
exploit the resulting energy savings.
It has been observed that bit-cell failures for a given memory array are randomly distributed and
independent of each other. That is, the spatial distribution of bit-flips can be assumed to be random,
as it generally differs from one array to another, within as well as between chips. Below, following
Chandramoorthy et al. [2019], we model bit errors in a memory array of a given size by generating a
random distribution of such errors with equal likelihood of 0-to-1 and 1-to-0 bit-flipping. More specifi-
cally, we assume that the model weights are quantized to 8-bit precision (i.e., from 32-bit floats to 8-bit
integers), and generate perturbed models by injecting our randomly distributed bit errors into the twoâ€™s
complement representation of weights. For more implementation details, please refer to Section 4.1.
3 NeuralFuse: Framework and Algorithms
3.1 Error-Resistant Input Transformation
As illustrated in Figure 1, we propose a novel trainable input-transformation module, NeuralFuse,
parametrized by a relatively small DNN, to mitigate the accuracy-energy tradeoff for model inference
and thus overcome the drawback of performance degradation in low-voltage regimes. A specially
designed loss function and training scheme are used to derive NeuralFuse and apply it to the input
data such that the transformed inputs will become robust to low voltage-induced bit errors.
Consider the input xsampled from the data distribution Xand a set of models Mpwithp%random
bit errors on weights (i.e., perturbed models). When it is not manifesting any bit errors (i.e., at
normal-voltage settings), the perturbed model operates as a nominal deterministic one, denoted
byM0. NeuralFuse aims to ensure that a model Mpâˆˆ M pcan make correct inferences on the
transformed inputs while also delivering consistent results in its M0state. To adapt to various data
characteristics, NeuralFuse â€“ designated as Fin Eq. (1), below â€“ is designed to be input-aware. This
characteristic can be formally defined as
F(x) =clip[âˆ’1,1] 
x+G(x)
, (1)
where G(x)is a â€œgeneratorâ€ (i.e., an input-transformation function) that can generate a perturbation
based on input x. As transformed by NeuralFuse, i.e., as F(x), that input is passed to the deployed
model ( M0orMp) for final inference. Without loss of generality, we assume the transformed input
lies within a scaled input range F(Â·)âˆˆ[âˆ’1,1]d, where dis the (flattened) dimension of x.
3.2 Training Objective and Optimizer
To train our generator G(Â·), which ought to be able to ensure the correctness of both the perturbed
model Mpand the clean model M0, we parameterized it with a neural network and apply our training
objective function
arg max
WGlogPM0(y|F(x;WG)) +Î»Â·EMpâˆ¼Mp[logPMp(y|F(x;WG))], (2)
4where WGis the set of trainable parameters for G;yis the ground-truth label of x;PMdenotes the
likelihood of yas computed by a model Mbeing given a transformed input F(x;WG);Mpis the
distribution of the perturbed models inherited from the clean model M0, under a p%random bit-error
rate; and Î»is a hyperparameter that balances the importance of the nominal and perturbed models.
The training objective function can be readily converted to a loss function ( L) that evaluates cross-
entropy between the ground-truth label yand the prediction PM(y|F(x;WG). That is, the total loss
function can be calculated as
LTotal=LM0+Î»Â· LMp. (3)
In particular, optimizing the loss function requires evaluation of the impact of the loss term LMp
on randomly perturbed models. Our training process is inspired by expectation over transformation
(EOT) attacks [Athalye et al., 2018], which aim to produce robust adversarial examples that are
simultaneously adversarial over the entire transformation distribution. Based on that idea, we propose
a new optimizer for solving Eq. (3), which we call expectation over perturbed models (EOPM).
EOPM-trained generators can generate error-resistant input transformations and mitigate inherent bit
errors. However, it would be computationally impossible to enumerate all possible perturbed models
with random bit errors, and the number of realizations for perturbed models is constrained by the
memory size of the GPUs used for training. In practice, therefore, we only use Nperturbed models
per iteration to calculate empirical average loss, i.e.,
LMpâ‰ˆLMp1+Â·Â·Â·+LMpN
N, (4)
where Nis the number of perturbed models {Mp1,Â·Â·Â·, MpN}that are simulated to calculate the loss
caused by random bit errors. Therefore, the gradient used to update the generator can be calculated
as follows:
âˆ‚LTotal
âˆ‚WG=âˆ‚LM0
âˆ‚WG+Î»
Nâˆ‚LMp1
âˆ‚WG+Â·Â·Â·+âˆ‚LMpN
âˆ‚WG
. (5)
Through our implementation, we found that stable performance could be delivered when N= 10 ,
and that there was little to be gained by using a larger value. The results of our ablation study for
different values of Ncan be found in Appendix E.
3.3 Training Algorithm
Algorithm 1 in Appendix A summarizes NeuralFuseâ€™s training steps. Briefly, this involves splitting the
training data XintoBmini-batches for training the generator in each epoch. For each mini-batch, we
first feed these data into F(Â·)to obtain the transformed inputs. Also, we simulate Nperturbed models
using a p%random bit-error rate, denoted by Mp1,Â·Â·Â·, MpN, from Mp. Then, the transformed
inputs are fed into those Nperturbed models as well as into the clean model M0, and their respective
losses and gradients are calculated. Finally, NeuralFuse parameters WGare updated based on the
gradient obtained by EOPM.
4 Experiments
4.1 Experiment Setups
Datasets. We evaluate NeuralFuse on four different datasets: CIFAR-10 [Krizhevsky and Hinton,
2009], CIFAR-100 [Krizhevsky and Hinton, 2009], the German Traffic Sign Recognition Benchmark
(GTSRB) [Stallkamp et al., 2012], and ImageNet-10 [Deng et al., 2009]. CIFAR-10 consists of
10 classes, with 50,000 training images and 10,000 testing images in total. Similarly, CIFAR-100
consists of 100 classes, with 500 training images and 100 testing images in each. The GTSRB
contains 43 classes with a total of 39,209 training images and 12,630 testing images. Similar to
CIFAR-10 and CIFAR-100, we resize GTSRB into 32 Ã—32Ã—3 in our experiment. For ImageNet-10,
we chose the same ten categories as Huang et al. [2022], in which there are 13,000 training images
and 500 test images cropped into 224 Ã—224Ã—3. Due to space limitations, our CIFAR-100 results are
presented in Appendices F and G.
Base Models. We selected several common architectures for our base models: ResNet18, ResNet50
[He et al., 2016], VGG11, VGG16, and VGG19 [Simonyan and Zisserman, 2015]. To replicate the
deployment of models on chips, all our based models were given quantization-aware training that
followed Stutz et al. [2021].
5NeuralFuse Generators. The architecture of the NeuralFuse generator ( G) ) is based on an encoder-
decoder structure. We designed and compared three types of generators, namely convolution-based,
deconvolution-based, and UNet-based. We also considered large(L)/small(S) network sizes for each
type. Further details can be found below and in Appendix B.
â€¢Convolution-based (Conv). Conv uses convolution with MaxPool layers for its encoder and
convolution with UpSample layers for its decoder. This architecture has previously been shown to
be efficient and effective at generating input-aware backdoor triggers [Nguyen and Tran, 2020].
â€¢Deconvolution-based (DeConv). DeConv uses convolution with MaxPool layers for its encoder
anddeconvolution layers for its decoder. We expected this modification both to enhance its
performance and to reduce its energy consumption.
â€¢UNet-based (UNet). UNet uses convolution with MaxPool layers for its encoder, and deconvo-
lution layers for its decoder. UNet is known for its robust performance in image segmentation
[Ronneberger et al., 2015].
Energy-consumption Calculation. The energy consumption reported in Figure 1 is based on the
product of the total number of SRAM memory accesses in a systolic array-based convolution neural
network (CNN) accelerator and the dynamic energy per read access at a given voltage. Research
by Chen et al. [2016] previously showed that energy consumption by SRAM buffers and arrays
accounts for a high proportion of total system energy consumption. We assume that there are no
bit errors on NeuralFuse, given its status as an add-on data preprocessing module whose functions
could also be performed by a general-purpose core. In this work we assume it is implemented on
the accelerator equipped with dynamic voltage scaling and therefore NeuralFuse computation is
performed at nominal error-free voltage. We report a reduction in overall weight-memory energy
consumption (i.e., NeuralFuse + Base Model under a p%bit-error rate) with respect to the unprotected
base model in the regular-voltage mode (i.e., 0%bit-error rate and without NeuralFuse).
To quantify memory accesses, we used the SCALE-SIM simulator [Samajdar et al., 2020], and our
chosen configuration simulated an output-stationary dataflow and a 32 Ã—32 systolic array with 256KB
of weight memory. We collected data on the dynamic energy per read access of the SRAM both at
Vminand at the voltage corresponding to a 1%bit-error rate ( Vberâ‰ˆ0.83Vmin) from Cadence ADE
Spectre simulations, both at the same clock frequency.
Relaxed and Restricted Access Settings. In the first of our experimentsâ€™ two scenarios, relaxed
access, the base-model information was not entirely transparent, but allowed us to obtain gradients
from the black-box model through backpropagation. Therefore, this scenario allowed direct training
of NeuralFuse with the base model using EOPM. In the restricted-access scenario, on the other hand,
only the inference function was allowed for the base model, and we therefore trained NeuralFuse using
a white-box surrogate base model and then transfering the generator to the access-restricted model.
Computing Resources. Our experiments were performed using eight Nvidia Tesla V100 GPUs and
implemented with PyTorch. NeuralFuse was found to generally take 150 epochs to converge, and
its training time was similar to that of the base model it incorporated. On both the CIFAR-10 and
CIFAR-100 datasets, average training times were 17 hours (ResNet18), 50 hours (ResNet50), 9 hours
(VGG11), 13 hours (VGG16), and 15 hours (VGG19). For GTSRB, the average training times were 9
hours (ResNet18), 27 hours (ResNet50), 5 hours (VGG11), 7 hours (VGG16), and 8 hours (VGG19);
and for ImageNet-10, the average training times were 32 hours (ResNet18), 54 hours (ResNet50), 50
hours (VGG11), 90 hours (VGG16), and 102 hours (VGG19).
4.2 Performance Evaluation, Relaxed-access Scenario
Our experimental results pertaining to the relaxed-access scenario are shown in Figure 3. The bit-
error rate (BER) due to low voltage was 1%in the cases of CIFAR-10 and GTSRB, and 0.5%for
ImageNet-10. The BER of ImageNet-10 was lower than that of the other two because, being pre-
trained, it has more parameters than either of them. For each experiment, we sampled and evaluated
N= 10 perturbed models (independent from training), and below, we report the means and standard
deviations of their respective accuracies. Below, clean accuracy (CA) refers to a modelâ€™s accuracy
measured at nominal voltage, and perturbed accuracy (PA) to its accuracy measured at low voltage.
In the cases of CIFAR-10 and the GTSRB, we observed that large generators like ConvL and
UNetL recovered PA considerably, i.e., in the range of 41% to63% on ResNet18, VGG11, VGG16,
6Nominal (w/o NeuralFuse) Low voltage (w/o NeuralFuse) Nominal (w/ NeuralFuse) Low voltage (w/ NeuralFuse)
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.6
Accuracy (%)38       .989  88  89  
82  86  84       .8     .2     .6
     .9     .6     .487  
59  88  
68  84  
68       .8
     .5     .5
     .8     .6
     .8CIFAR-10 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.6
26       .185  85  87  82  86  
77       .5      .2     .4
     .4     .2
     .3
53  
34  63  
42  75  
56       .2
     .6     .3
     .2     .5
     .2CIFAR-10 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS88.4
42       .289  84  89  85  87  85       .6
     .9     .3
     .6     .1     .5 87  
66  87  
68  83  
72       .2
     .3     .2
     .2     .6
     .7CIFAR-10 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS90.3
35       .790  87  89  86  87  87       .1     .4     .7     .8      .4      .4 86  
59  85  
66  83  
71       .0
     .6     .5
     .5     .4
     .2CIFAR-10 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS90.5
36       .089  87  86  86  86  86       .8     .3      .3      .5      .3      .3
77  
52  78  
58  82  
66       .7
     .7     .4
     .2     .1
     .4CIFAR-10 pre-trained VGG19(a) CIFAR-10, 1%Bit-error Rate
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.5
Accuracy (%)36       .995  94  95  95  96  95       .7      .4      .6      .7      .2      .991  
68  91  
78  93  
85       .1
     .6     .3
     .1     .8
     .1GTSRB pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.0
29       .595  94  94  93  94  94       .6      .8      .9     .0     .5      .7
71  
50  71  
56  80  
64       .6
     .5     .6
     .4     .6
     .7GTSRB pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS91.9
34       .994  91  95  92  92  94       .8
     .1     .0     .4      .2     .7
85  
62  84  
67  83  
73       .7
     .2     .6
     .5     .2
     .4GTSRB pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.2
15       .196  94  96  93  95  94       .3     .1     .4     .8     .8     .3
72  
39  72  
50  78  
63       .4
     .8     .0
     .9     .6
     .3GTSRB pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.5
36       .696  93  95  94  95  94       .0     .8     .4      .5      .4      .6
88  
69  87  
73  88  
80       .3
     .0     .2
     .1     .2
     .6GTSRB pre-trained VGG19
(b) GTSRB, 1%Bit-error Rate
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.2
Accuracy (%)72       .394  91  94  92  94  93       .0     .8     .0      .8      .0      .288  83  89  87  88  86       .0
     .6     .2     .5      .1     .4ImageNet-10 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS89.8
39       .492  91  93  93  92  92       .2      .8      .0      .2      .2      .4
80  
65  79  
70  80  
73       .0
     .0     .4
     .9     .5
     .6ImageNet-10 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS91.6
47       .892  89  91  89  92  
86       .0     .4     .0     .0     .4
     .2 86  
66  86  
72  83  
73       .1
     .4     .0
     .5     .0
     .5ImageNet-10 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS94.6
38       .490  90  91  90  90  86       .8      .2      .2      .0      .6
     .4
77  
60  77  
62  81  
72       .1
     .2     .2
     .3     .1
     .3ImageNet-10 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.4
37       .291  88  91  88  89  87       .4     .8     .0     .8      .4     .6
75  
56  75  
64  77  
65       .5
     .5     .9
     .0     .9
     .9ImageNet-10 pre-trained VGG19
(c) ImageNet-10, 0.5%Bit-error Rate
Figure 3: Relaxed-access scenario test accuracies ( %) of various pre-trained models with and without
NeuralFuse, compared at nominal voltage ( 0%bit-error rate) or low voltage (with specified bit-error
rates). The results demonstrate that NeuralFuse consistently recovered perturbation accuracy.
and VGG19. ResNet50â€™s recovery percentage was slightly worse than those of the other base
models, but it nevertheless attained up to 51% recovery on the GTSRB. On the other hand, the
recovery percentages achieved when we used small generators like DeConvS were worse than those
of their larger counterparts. This could be explained by larger-sized networksâ€™ better ability to
learn error-resistant generators (though perhaps at the cost of higher energy consumption). In the
case of ImageNet-10, using larger generators also yielded better PA performance recovery, further
demonstrating NeuralFuseâ€™s ability to work well with large input sizes and varied datasets.
4.3 Performance Evaluation, Restricted-access Scenario (Transferability)
The experimental results of our restricted-access scenario are shown in Table 1. We adopted ResNet18
and VGG19 as our white-box surrogate source models for training the generators under a 1.5%bit-
error rate. We chose ConvL and UNetL as our generators because they performed best out of the six
we tested (see Figure 3).
From Table 1, we can see that transferring from a larger BER such as 1.5%can endow a smaller
one (e.g., 1%) with strong resilience. The table also makes it clear that using VGG19 as a surrogate
model with UNet-based generators like UNetL can yield better recovery performance than other
model/generator combinations. On the other hand, we observed in some cases that if we transferred
between source and target models of the same type (but with different BERs for training and testing),
performance results could exceed those we had obtained during the original relaxed-access scenario.
For instance, when we transferred VGG19 with UNetL under a 1.5%BER to VGG19 or VGG11
under a 0.5%BER, the resulting accuracies were 85.86% (as against 84.99% for original VGG19)
and84.81% (as against 82.42% for original VGG11). We conjecture that generators trained on
relatively large BERs can cover the error patterns of smaller BERs, and even help improve the latterâ€™s
generalization. These findings indicate the considerable promise of access-limited base models in
low-voltage settings to recover accuracy.
4.4 Energy/Accuracy Tradeoff
We report total dynamic energy consumption as the total number of SRAM-access events multiplied
by the dynamic energy of a single such event. Specifically, we used SCALE-SIM to calculate total
7Table 1: Restricted-access scenario: Transfer results on CIFAR-10 with 1.5%bit-error rate
SM TM BER CA PAConvL (1.5%) UNetL (1.5%)
CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet18ResNet181%92.638.9Â±12.489.889.0Â±0.5 50.185.885.2Â±0.5 46.3
0.5% 70.1Â±11.6 89.6Â±0.2 19.5 85.7Â±0.2 15.6
ResNet501%92.626.1Â±9.489.236.1Â±18 10.084.438.9Â±16 12.8
0.5% 61.0Â±10.3 74.1Â±10 13.1 72.7Â±4.6 11.7
VGG111%88.442.2Â±11.686.359.2Â±10 17.082.369.8Â±7.5 27.6
0.5% 63.6Â±9.3 78.9Â±4.9 15.3 77.0Â±4.0 13.4
VGG161%90.335.7Â±7.989.462.2Â±18 26.584.768.9Â±14 33.2
0.5% 66.6Â±8.1 83.4Â±5.5 16.8 80.5Â±5.9 13.9
VGG191%90.536.0Â±12.089.849.9Â±23 13.985.055.1Â±17 19.1
0.5% 64.2Â±12.4 81.8Â±8.5 17.6 78.5Â±6.8 14.3
VGG19ResNet181%92.638.9Â±12.488.962.6Â±13 23.785.072.3Â±11 33.4
0.5% 70.1Â±11.6 84.2Â±7.2 14.1 82.1Â±2.2 12.0
ResNet501%92.626.1Â±9.488.837.9Â±18 11.885.246.7Â±17 20.6
0.5% 61.0Â±10.3 76.6Â±7.8 15.6 78.3Â±3.7 17.3
VGG111%88.442.2Â±11.688.976.0Â±6.1 33.885.581.9Â±3.9 39.7
0.5% 63.6Â±9.3 85.9Â±2.6 22.3 84.8Â±0.5 21.2
VGG161%90.335.7Â±7.989.076.5Â±9.0 40.885.979.2Â±7.5 43.5
0.5% 66.6Â±8.1 87.7Â±0.7 21.1 84.7Â±0.9 18.1
VGG191%90.536.0Â±12.089.180.2Â±12 44.286.384.3Â±1.2 48.3
0.5% 64.2Â±12.4 88.8Â±0.4 24.6 85.9Â±0.3 21.7
Note . SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target
model; CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
Table 2: Energy saving ( %) by NeuralFuse for 30 combinations of base models and generators.
ConvL ConvS DeConvL DeConvS UNetL UNetS
ResNet18 19.0 29.1 21.2 27.5 24.0 28.9
ResNet50 25.4 29.9 26.4 29.2 27.7 29.9
VGG11 6.6 27.5 11.2 24.1 17.1 27.2
VGG16 17.1 28.9 19.7 27.0 23.0 28.7
VGG19 20.3 29.7 22.3 27.8 24.8 29.1
weight-memory access (TWMA), specifics of which can be found in Appendix Câ€™s Table 6. In Table
2, below, we report the percentages of energy saved (ES) at voltages that yield a 1%bit-error rate for
various base-model and generator combinations. The formula for computing ES is
ES=EnergyNVâˆ’ 
EnergyLV+EnergyNeuralFuse at NV
EnergyNVÃ—100% , (6)
where NVdenotes nominal voltage regime, and LV, a low-voltage one.
Our results indicate that when ResNet18 is utilized as a base model, NeuralFuse can recover model
accuracy by 20âˆ’49% while reducing energy use by 19âˆ’29%. In Appendix C, we provide
more results on the tradeoff between energy and accuracy of different NeuralFuse and base-model
combinations. Overall, it would appear that using NeuralFuse can effectively restore model accuracy
when SRAM encounters low-voltage-induced random bit errors.
Runtime and Latency. On the other hand, runtime and its corresponding energy consumption
may also affect overall energy savings. For instance, previous research has shown that multiply-and-
accumulate (MAC) operations account for more than 99% of all operations in state-of-the-art CNNs,
dominating processing runtime and energy consumption alike Yang et al. [2017]. Therefore, we
also report the results of our MAC-based energy-consumption estimation in Appendix C, and of our
latency analysis in Appendix D. Here, it should also be noted that an additional latency overhead is an
inevitable tradeoff for reducing energy consumption in our scenarios. Although neither runtime nor
latency is a major focus of this paper, future researchers could usefully design a lighter-weight version
of the NeuralFuse module, or apply model-compression techniques to it, to reduce these two factors.
4.5 Model Size and NeuralFuse Efficiency
To arrive at a full performance characterization of NeuralFuse, we analyzed the relationship between
the final recovery achieved by each base model in combination with generators of varying parameter
counts. For this purpose, we defined efficiency ratio as the recovery percentage in PA divided by
NeuralFuseâ€™s parameter count. Table 3 compares the efficiency ratios of all NeuralFuse generators
8Table 3: The efficiency ratio for all NeuralFuse generators.
Base
ModelBERNeuralFuse
ConvL ConvS DeConvL DeConvS UNetL UNetS
ResNet181% 67.5 182 76.6 190.7 94.5 245.9
0.5% 24.7 73.3 30.7 62.5 33.6 88.3
ResNet501% 37.4 75.1 57.4 102.7 102.3 248.4
0.5% 35.2 108.7 40.4 92.5 47.4 124.6
VGG111% 62.3 212.9 69.5 165.8 92.0 251.7
0.5% 32.3 96.3 35.8 77.2 38.9 100.7
VGG161% 69.6 211.2 76.9 196.5 98.8 292.9
0.5% 30.3 98.1 33.0 75.3 40.6 113
VGG191% 57.6 147.5 65.5 141.6 95.4 250.8
0.5% 33.0 91.0 37.5 70.2 43.1 106.4
Original Accuracy w/o NeuralFuse NeuralFuse (ConvL) NeuralFuse (UNetL)
8 7 6 5 4 3 2
Bit Quantization Number95.5
Accuracy (%) 95    95    95    95    92   
 67        .5      .5      .4      .4     .6
     .7 93    93    93    93    93    92        .4      .3      .3      .3      .3      .0 96    96    96    96    96    95        .2      .2      .2      .2      .1      .7
3     .83     .83     .8GTSRB pre-trained ResNet18 (Nominal voltage, No Bit Error)
8 7 6 5 4 3 2
Bit Quantization NumberAccuracy (%) 75    69    67   
 48   
 24   
5       .2     .5     .2
     .6
     .6
   .3 89    89    88   
 79   
 52   
9       .5      .9      .9
     .3
     .1
   .7 93    94    93    87   
 62   
 12        .5      .5      .5
     .5
     .2
     .6
3     .83     .83     .8GTSRB pre-trained ResNet18 (Low voltage, 0.5% BER)Figure 4: Reduced-precision accuracy
trained on CIFAR-10. Those results show that UNet-based generators had better efficiency per million
parameters than either convolution-based or deconvolution-based ones.
4.6 NeuralFuseâ€™s Robustness to Reduced-precision Quantization
Lastly, we also explored NeuralFuseâ€™s robustness to low-precision quantization on model weights.
Uniform quantization is the usual method for quantizing model weights [Gholami et al., 2022].
However, it is possible for it to cause an accuracy drop due to lack of precision. Given our aim of
offering protection from bit errors, we hoped to understand whether NeuralFuse could also recover
model-accuracy drops caused by this phenomenon. We therefore uniformly quantized the model
weights to a lower bit precision and measured the resulting accuracy. Specifically, we applied
symmetric uniform quantization to our base models with various numbers of bits to induce precision
loss, and defined the quantized weight Wq(integer) as Wq=âŒŠW
sâŒ‰, where Wdenotes the original
model weight (full precision), s=max|W|
2bâˆ’1âˆ’1is the quantization scale parameter, and bis the precision
(number of bits) used to quantize the models. Bit errors induced by low voltage operation as previously
described, are also applied to low precision weights.
We used the GTSRB pre-trained ResNet18 as our example in an evaluation of two NeuralFuse
generators, i.e., ConvL and UNetL trained with 0.5%BER, and varied precision bfrom 8 bits to
2 bits (integer). The results, shown in Figure 4, indicated that when b >3bits, NeuralFuse could
effectively recover accuracy in both the low-voltage and low-precision scenarios. When b= 3, while
NeuralFuse could still handle the bit-error-free model (Fig. 4 top), it exhibited a limited ability to
recover the random bit-error case (Fig. 4 bottom). We find these results encouraging, insofar as
NeuralFuse was only trained on random bit errors, yet demonstrated high accuracy in dealing with
unseen bit-quantization errors. Further experimental results derived from other base models and
datasets can be found in Appendix H.
4.7 Extended Analysis
Here, we would like to highlight some key findings from the additional results in the Appendices.
In Appendix E, we compare NeuralFuse against a simple baseline of learning a universal input
perturbation. We found that such baseline performed much worse than NeuralFuse at that task,
validating the necessity of adopting input-aware transformation if the goal is to learn error-resistant
data representations in low-voltage scenarios. In Appendix G, we report that ensemble training of
white-box surrogate base models could further improve the transferability of NeuralFuse in restricted-
access scenarios. Appendices K and L present visualization results of NeuralFuseâ€™s data embeddings
and transformed inputs. In Appendix J, we show that NeuralFuse can further recover the accuracy of
a base model trained with adversarial weight perturbation in a low-voltage setting.
5 Conclusion
In this paper, we have proposed NeuralFuse, the first non-intrusive post hoc module that protects
model inference against bit errors induced by low voltage. NeuralFuse is particularly well suited to
9practical machine-deployment cases in which access to the base model is limited or relaxed. The
design of NeuralFuse includes a novel loss function and a new optimizer, EOPM, that enable it to
handle simulated randomness in perturbed models. Our comprehensive experimental results and
analysis show that NeuralFuse can recover test accuracy by up to 57% while simultaneously enjoying
an up to 24% reduction in memory-access energy requirements. Moreover, NeuralFuse demonstrates
high transferability to access-constrained models and high versatility, e.g., robustness to low-precision
quantization. In short, NeuralFuse is a notable advancement in mitigating neural-network inferenceâ€™s
energy/accuracy tradeoff in low-voltage regimes, and points the way to greener future AI technology.
Our future work will include extending this study to other neural-network architectures and modalities,
such as transformer-based language models.
Limitations. We acknowledge the challenge of achieving significant power savings without accuracy
loss and view NeuralFuse as a foundational, proof-of-concept step toward this goal. Future research
could enhance this approach by optimizing the pre-processing module to adapt to specific error
characteristics of low-voltage SRAM or by integrating lightweight hardware modifications to further
improve the energy-accuracy trade-off.
Broader Impacts. We see no ethical or immediate negative societal consequence of our work, and it
holds the potential for positive social impacts, from environmental benefits to improved access to
technology and enhanced safety in critical applications.
Acknowledgments and Disclosure of Funding
We thank the anonymous reviewers for their insightful comments and valuable suggestions. The
research described in this paper was conducted in the JC STEM Lab of Intelligent Design Automation,
which is funded by The Hong Kong Jockey Club Charities Trust in support of Tsung-Yi Ho.
References
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples.
In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine
Learning , volume 80 of Proceedings of Machine Learning Research , pages 284â€“293, Stockholm, Sweden,
10â€“15 Jul 2018. PMLR.
Babak Ehteshami Bejnordi, Tijmen Blankevoort, and Max Welling. Batch-shaping for learning conditional
channel gated networks. In International Conference on Learning Representations , 2020.
Nandhini Chandramoorthy, Karthik Swaminathan, Martin Cochet, Arun Paidimarri, Schuyler Eldridge, Rajiv V .
Joshi, Matthew M. Ziegler, Alper Buyuktosunoglu, and Pradip Bose. Resilient low voltage accelerators for
high energy efficiency. In 2019 IEEE International Symposium on High Performance Computer Architecture
(HPCA) , pages 147â€“158, 2019. doi: 10.1109/HPCA.2019.00034.
Yu-Hsin Chen, Joel Emer, and Vivienne Sze. Eyeriss: A spatial architecture for energy-efficient dataflow for
convolutional neural networks. ACM SIGARCH computer architecture news , 44(3):367â€“379, 2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 248â€“255,
2009. doi: 10.1109/CVPR.2009.5206848.
Shrikanth Ganapathy, John Kalamatianos, Keith Kasprak, and Steven Raasch. On characterizing near-threshold
sram failures in finfet technology. In Proceedings of the 54th Annual Design Automation Conference 2017 ,
pages 1â€“6, 2017.
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer. A survey of
quantization methods for efficient neural network inference. In Low-Power Computer Vision , pages 291â€“326.
Chapman and Hall/CRC, 2022.
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. Compressing deep convolutional networks using
vector quantization. arXiv preprint arXiv:1412.6115 , 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 770â€“778, 2016. doi:
10.1109/CVPR.2016.90.
Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for clustering via
prototype scattering and positive sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
pages 1â€“16, 2022. doi: 10.1109/TPAMI.2022.3216454.
10Sung Kim, Patrick Howe, Thierry Moreau, Armin Alaghi, Luis Ceze, and Visvesh Sathe. Matic: Learning
around errors for efficient low-voltage neural network accelerators. In 2018 Design, Automation & Test in
Europe Conference & Exhibition (DATE) , pages 1â€“6. IEEE, 2018.
Skanda Koppula, Lois Orosa, A Giray Ya Ë˜glÄ±kÃ§Ä±, Roknoddin Azizi, Taha Shahroodi, Konstantinos Kanellopou-
los, and Onur Mutlu. Eden: Enabling energy-efficient, high-performance deep neural network inference
using approximate dram. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on
Microarchitecture , pages 166â€“181, 2019.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report,
University of Toronto, Toronto, Ontario, 2009.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial
perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
pages 1765â€“1773, 2017.
Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. Advances in Neural Information
Processing Systems (NeurIPS) , 33:3454â€“3464, 2020.
Ozan Ã–zdenizci and Robert Legenstein. Improving robustness against stealthy weight bit-flip attacks by output
code matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 13388â€“13397, 2022.
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification
using binary convolutional neural networks. In European Conference on Computer Vision (ECCV) , pages
525â€“542. Springer, 2016.
Brandon Reagen, Paul Whatmough, Robert Adolf, Saketh Rama, Hyunkwang Lee, Sae Kyu Lee, JosÃ© Miguel
HernÃ¡ndez-Lobato, Gu-Yeon Wei, and David Brooks. Minerva: Enabling low-power, highly-accurate deep
neural network accelerators. In 2016 ACM/IEEE 43rd Annual International Symposium on Computer
Architecture (ISCA) , pages 267â€“278, 2016. doi: 10.1109/ISCA.2016.32.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image
segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention ,
pages 234â€“241. Springer, 2015.
Ananda Samajdar, Jan Moritz Joseph, Yuhao Zhu, Paul Whatmough, Matthew Mattina, and Tushar Krishna.
A systematic methodology for characterizing scalability of dnn accelerators using scale-sim. In 2020 IEEE
International Symposium on Performance Analysis of Systems and Software (ISPASS) , pages 58â€“68, 2020.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In
International Conference on Learning Representations (ICLR) , 2015.
Vladislav Sovrasov. ptflops: a flops counting tool for neural networks in pytorch framework, 2018-2023. URL
https://github.com/sovrasov/flops-counter.pytorch . Github Repository.
Gopalakrishnan Srinivasan, Parami Wijesinghe, Syed Shakib Sarwar, Akhilesh Jaiswal, and Kaushik Roy.
Significance driven hybrid 8t-6t sram for energy-efficient synaptic storage in artificial neural networks. In
2016 Design, Automation & Test in Europe Conference & Exhibition (DATE) , pages 151â€“156. IEEE, 2016.
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking
machine learning algorithms for traffic sign recognition. Neural Networks , 32:323â€“332, 2012.
David Stutz, Nandhini Chandramoorthy, Matthias Hein, and Bernt Schiele. Bit error robustness for energy-
efficient dnn accelerators. In A. Smola, A. Dimakis, and I. Stoica, editors, Proceedings of Machine Learning
and Systems , volume 3, pages 569â€“598, 2021.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research , 9(86):2579â€“2605, 2008.
Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems , volume 33, pages 2958â€“2969. Curran Associates, Inc., 2020.
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng. Quantized convolutional neural networks
for mobile devices. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 4820â€“4828, 2016.
Haichuan Yang, Yuhao Zhu, and Ji Liu. Ecc: Platform-independent energy-constrained deep neural network
compression via a bilinear regression model. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 11206â€“11215, 2019a.
Haichuan Yang, Yuhao Zhu, and Ji Liu. Energy-constrained compression for deep neural networks via weighted
sparse projection and layer input masking. In International Conference on Learning Representations (ICLR) ,
2019b.
11Haichuan Yang, Shupeng Gui, Yuhao Zhu, and Ji Liu. Automatic neural network compression by sparsity-
quantization joint learning: A constrained optimization-based approach. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , 2020.
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. Designing energy-efficient convolutional neural networks using
energy-aware pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 5687â€“5695, 2017.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically
principled trade-off between robustness and accuracy. In International Conference on Machine Learning
(ICML) , pages 7472â€“7482. PMLR, 2019.
12Appendix
The appendix provides more implementation details for our method, experimental results on more
datasets and settings, ablation studies, and qualitative analysis. The appendices cover the following:
â€¢Implementation Details: NeuralFuse Training Algorithm (Sec. A), NeuralFuse Generator (Sec.
B), Energy/Accuracy Tradeoff (Sec. C), Latency Reports (Sec. D)
â€¢Experimental Results: Ablation Studies (Sec. E), Relaxed Access (Sec. F), Restricted Access
(Sec. G), Reduced-precision Quantization (Sec. H), Adversarial Training (Sec. I), Adversarial
Weight Perturbation (Sec. J)
â€¢Qualitative Studies: Data Embeddings Visualization (Sec. K), Transformed Inputs Visualization
(Sec. L)
A Training Algorithm of NeuralFuse
Algorithm 1 Training steps for NeuralFuse
Input : Base model M0; Generator G; Training data samples X; Distribution of the perturbed models
Mp; Number of perturbed models N; Total training iterations T
Output : Optimized parameters WGfor the Generator G
1:fort= 0, ..., Tâˆ’1do
2: for all mini-batches {x, y}B
b=1âˆ¼ X do
3: Create transformed inputs xt=F(x) =clip[âˆ’1,1] 
x+G(x)
.
4: Sample Nperturbed models {Mp1, ..., M pN}fromMpunder p%random bit errors.
5: for all Mpiâˆ¼ {Mp1, ..., M pN}do
6: Calculate the loss Lpibased on the output of the perturbed model Mpi. Then calculate
the gradients gpiforWGbased on Lpi.
7: end for
8: Calculate the loss L0based on the output of the clean model M0. Then calculate the
gradients g0forWGbased on L0.
9: Calculate the final gradient gfinal using (5) based on g0andgp1, ..., g pN.
10: Update WGusing gfinal .
11: end for
12:end for
B Implementation Details of NeuralFuse Generator
We consider two main goals in designing the NeuralFuse Generator: 1) efficiency (so the overall
energy overhead is decreased) and 2) robustness (so that it can generate robust patterns on the input
image and overcome the random bit flipping in subsequent models). Accordingly, we choose to utilize
an encode-decoder architecture in implementing the generator. The design of ConvL is inspired
by Nguyen and Tran [2020], in which the authors utilize a similar architecture to design an input-
aware trigger generator, and have demonstrated its efficiency and effectiveness. Furthermore, we
attempted to enhance it by replacing the Upsampling layer with a Deconvolution layer, leading to the
creation of DeConvL. The UNetL-based NeuralFuse draws inspiration from Ronneberger et al. [2015],
known for its robust performance in image segmentation, and thus, we incorporated it as one of our
architectures. Lastly, ConvS, DeConvS, and UNetS are scaled-down versions of the model designed
to reduce computational costs and total parameters. The architectures of Convolutional-based and
Deconvolutional-based are shown in Table 4, and the architecture of UNet-based generators is in
Table 5. For the abbreviation used in the table, ConvBlock means the Convolution block, Conv means
a single Convolution layer, DeConvBlock means the Deconvolution block, DeConv means a single
Deconvolution layer, and BN means a Batch Normalization layer. We use learning rate = 0.001,
Î»= 5, and Adam optimizer. For CIFAR-10, GTSRB, and CIFAR-100, we set batch size b= 25 for
each base model. For ImageNet-10, we set b= 64 for ResNet18, ResNet50 and VGG11, and b= 32
for both VGG16 and VGG19.
13Table 4: Model architecture for both Convolution-based and Deconvolution-based generators. Each
ConvBlock consists of a Convolution ( kernel = 3Ã—3,padding = 1,stride = 1), a Batch Nor-
malization, and a ReLU layer. Each DeConvBlock consists of a Deconvolution ( kernel = 4Ã—4,
padding = 1, stride = 2), a Batch Normalization, and a ReLU layer.
ConvL ConvS DeConvL DeConvS
Layers #CHs Layers #CHs Layers #CHs Layers #CHs
(ConvBlock) Ã—2, MaxPool 32 ConvBlock, Maxpool 32 (ConvBlock) Ã—2, MaxPool 32 ConvBlock, Maxpool 32
(ConvBlock) Ã—2, MaxPool 64 ConvBlock, Maxpool 64 (ConvBlock) Ã—2, MaxPool 64 ConvBlock, Maxpool 64
(ConvBlock) Ã—2, MaxPool 128 ConvBlock, Maxpool 64 (ConvBlock) Ã—2, MaxPool, 128 ConvBlock, Maxpool 64
ConvBlock, UpSample, ConvBlock 128 ConvBlock, UpSample 64 ConvBlock 128 DeConvBlock 64
ConvBlock, UpSample, ConvBlock 64 ConvBlock, UpSample 32 DeConvBlock, ConvBlock 64 DeConvBlock 32
ConvBlock, UpSample, ConvBlock 32 ConvBlock, UpSample 3 DeConvBlock, ConvBlock 32 DeConv, BN, Tanh 3
Conv, BN, Tanh 32 Conv, BN, Tanh 3 Conv, BN, Tanh 3
[Note] #CHs: number of channels .
Table 5: Model architecture for UNet-based generators. Each ConvBlock consists of a Convolution
(kernel = 3Ã—3,padding = 1,stride = 1), a Batch Normalization, and a ReLU layer. Other layers,
such as the Deconvolutional layer ( kernel = 2Ã—2,padding = 1,stride = 2), are used in UNet-based
models. For the final Convolution layer, the kernel size is set to 1.
UNetL UNetS
Layers #Channels Layers #Channels
L1: (ConvBlock) Ã—2 16 L1: (ConvBlock) Ã—2 8
L2: Maxpool, (ConvBlock) Ã—2 32 L2: Maxpool, (ConvBlock) Ã—2 16
L3: Maxpool, (ConvBlock) Ã—2 64 L3: Maxpool, (ConvBlock) Ã—2 32
L4: Maxpool, (ConvBlock) Ã—2 128 L4: Maxpool, (ConvBlock) Ã—2 64
L5: DeConv 64 L5: DeConv 32
L6: Concat[L3, L5] 128 L6: Concat[L3, L5] 64
L7: (ConvBlock) Ã—2 64 L7: (ConvBlock) Ã—2 32
L8: DeConv 32 L8: DeConv 16
L9: Concat[L2, L8] 64 L9: Concat[L2, L8] 32
L10: (ConvBlock) Ã—2 32 L10: (ConvBlock) Ã—2 16
L11: DeConv 16 L11: DeConv 8
L12: Concat[L1, L11] 32 L12: Concat[L1, L11] 16
L13: (ConvBlock) Ã—2 16 L13: (ConvBlock) Ã—2 8
L14: Conv 3 L14: Conv 3
C NeuralFuseâ€™s Energy/Accuracy Tradeoff
SCALE-SIM. SCALE-SIM [Samajdar et al., 2020] is a systolic array-based CNN simulator that can
calculate the number of memory accesses and the total time in execution cycles by giving the specific
model architecture and accelerator architectural configuration as inputs. In this paper, we use SCALE-
SIM to calculate the weights memory access of 5 based models (ResNet18, ResNet50, VGG11,
VGG16, VGG19), and 6 generators (ConvL, ConvS, DeConvL, DeConvS, UNetL, UNetS). While
SCALE-SIM supports both Convolutional and Linear layers, it does not yet support Deconvolution
layers. Instead, we try to approximate the memory costs of Deconvolution layers by Convolution
layers. We change the input and output from Deconvolution into the output and input of the
Convolution layers. Besides, we also change the stride into 1 when we approximate it. We also add
padding for the convolution layers while generating input files for SCALE-SIM. In this paper, we
only consider the energy saving on weights accesses, so we only take the value â€œSRAM Filter Readsâ€
from the output of SCALE-SIM as the total weights memory accesses (T.W.M.A.) for further energy
calculation.
In Table 6, we report the total weight memory access (T.W.M.A.) using SCALE-SIM. We then
showed the energy/accuracy tradeoff between all of the combinations of NeuralFuse and base models
under a 1% of bit error rate in Figure 5.
Parameters and MACs Calculation. In addition to T.W.M.A., the number of parameters and MACs
(multiplyâ€“accumulate operations) are also common metrics in measuring the energy consumption of
machine learning models. Yang et al. [2017] have shown that the energy consumption of computation
14Table 6: The total weights memory access calculated by SCALE-SIM.
Base Model ResNet18 ResNet50 VGG11 VGG16 VGG19 -
T.W.M.A. 2,755,968 6,182,144 1,334,656 2,366,848 3,104,128 -
NeuralFuse ConvL ConvS DeConvL DeConvS UNetL UNetS
T.W.M.A. 320,256 41,508 259,264 86,208 180,894 45,711
[Note] T.W.M.A.: total weight memory access .
Figure 5: The energy/accuracy tradeoff of different NeuralFuse implementations with all CIFAR-10
pre-trained based models. The X-axis represents the percentage reduction in dynamic memory access
energy at low-voltage settings (base model protected by NeuralFuse) compared to the bit-error-free
(nominal) voltage; the Y-axis represents the perturbed accuracy (evaluated at low voltage) with a 1%
bit error rate.
andmemory accesses are both proportional to MACs, allowing us to take computation energy
consumption into account.
Here, we use the open-source package ptflops [Sovrasov, 2018-2023] to calculate the parameters
and MAC values of all the base models and the NeuralFuse generators, in the same units as Bejnordi
et al. [2020] used. The results are shown in Table 7. Note that we modified the base model
architectures for ImageNet-10, as it has larger input sizes. For example, we use a larger kernel size =
7instead of 3in the first Convolution layer in ResNet-based models to enhance the learning abilities.
Therefore, the parameters of base models are different between different datasets. For NeuralFuse
generators, we utilize the same architectures for implementation (including ImageNet-10). As a
result, our proposed NeuralFuse generators are generally smaller than base models, either on total
model parameters or MAC values.
Table 7: Parameter counts and MACs for all base models and generators in this paper.
Base Model
ResNet18 ResNet50 VGG11 VGG16 VGG19 -
ParameterCIFAR-10 11,173,962 23,520,842 9,231,114 14,728,266 20,040,522-ImageNet-10 11,181,642 23,528,522 128,812,810 134,309,962 139,622,218
MACsCIFAR-10 557.14M 1.31G 153.5M 314.43M 399.47M-ImageNet-10 1.82G 4.12G 7.64G 15.53G 19.69G
NeuralFuse
ConvL ConvS DeConvL DeConvS UNetL UNetS
ParameterCIFAR-10
ImageNet-10723,273 113,187 647,785 156,777 482,771 121,195
MACsCIFAR-10 80.5M 10.34M 64.69M 22.44M 41.41M 10.58M
ImageNet-10 3.94G 506.78M 3.17G 1.1G 2.03G 518.47M
15MACs-based Energy Consumption. We can then use the MAC values to further approximate the
end-to-end energy consumption of the whole model. Assume that all values are stored on SRAM
and that a MAC represents single memory access. The corresponding MACs-based energy saving
percentage (MAC-ES, %) can be derived from Eq. 7 (c.f. Sec. 4.4), and results can be found in Table
8. We can observe that most combinations can save a large amount of energy, except that VGG11
with two larger NeuralFuse (ConvL and DeConvL) may increase the total energy. These results
are consistent with the results reported in Table 2. In addition, we also showed the MACs-based
energy/accuracy tradeoff between all of the combinations of NeuralFuse and base models under a 1%
of bit error rate in Figure 6.
MAC-ES =MACs base model Â·Energynominal voltage âˆ’ 
MACs base model Â·Energylow-voltage-regime +MACs NeuralFuse Â·EnergyNeuralFuse at nominal voltage
MACs base model Â·Energynominal voltageÃ—100%
(7)
Table 8: The MACs-Based energy saving percentage ( %) for different combinations of base models
and NeuralFuse.
ConvL ConvS DeConvL DeConvS UNetL UNetS
ResNet18 16.2 28.7 19.0 26.6 23.2 28.7
ResNet50 24.5 29.8 25.7 28.9 27.4 29.8
VGG11 -21.8 23.9 -11.5 16.0 3.6 23.7
VGG16 5.0 27.3 10.0 23.5 17.4 27.2
VGG19 10.4 28.0 14.4 25.0 20.2 28.0
Figure 6: The Mac-based energy/accuracy tradeoff of different NeuralFuse implementations with
all CIFAR-10 pre-trained based models. The X-axis represents the percentage reduction in dynamic
memory access energy at low-voltage settings (base model protected by NeuralFuse), compared to
the bit-error-free (nominal) voltage; the Y-axis represents the perturbed accuracy (evaluated at low
voltage) with a 1% bit error rate.
Although using ConvL or DeConvL along with base model VGG11 for CIFAR-10 implies an increase
in energy consumption, using other smaller-scale generators, we can still save the overall energy and
recover the base modelâ€™s accuracy. That said, developers can always choose smaller generators (with
orders of magnitude fewer MAC operations than the original network) to restore model accuracy,
further demonstrating the flexibility of choosing NeuralFuse generators of different sizes.
D Inference Latency of NeuralFuse
In Table 9, we report the latency (batch_size=1, CIFAR-10/ImageNet-10 testing dataset) of utilizing
the different NeuralFuse generators with two different base models, ResNet18 and VGG19. While
16NeuralFuse contributes some additional latency, we consider this an unavoidable tradeoff necessary
to achieve reduced energy consumption within our framework. Although the primary focus of this
paper is not on latency, we acknowledge its importance. Future research could explore designing a
more lightweight version of the NeuralFuse module or applying model compression techniques to
minimize latency. Additionally, we recognize that running NeuralFuse on a general-purpose CPU
could lead to different latency and energy consumption figures due to various influencing factors like
CPU architecture and manufacturing processes.
Table 9: The Inference Latency of base model and base model with NeuralFuse.
ResNet18 (CIFAR-10) VGG19 (CIFAR-10) ResNet18 (ImageNet-10) VGG19 (ImageNet-10)
Base Model 5.84 ms 5.32 ms 6.21 ms 14.34 ms
+ ConvL 9.37 ms (+3.53) 8.96 ms (+3.64) 10.51 ms (+4.3) 17.66 ms (+3.32)
+ ConvS 7.86 ms (+2.02) 7.40 ms (+2.08) 8.28 ms (+2.07) 16.72 ms (+2.38)
+ DeConvL 9.18 ms (+3.34) 8.59 ms (+3.27) 10.07 ms (+3.86) 17.24 ms (+2.90)
+ DeConvS 7.49 ms (+1.65) 7.04 ms (+1.72) 7.79 ms (+1.58) 15.67 ms (+1.33)
+ UNetL 10.69 ms (+4.85) 10.06 ms (+4.74) 11.14 ms (+4.93) 18.54 ms (+4.20)
+ UNetS 10.63 ms (+4.79) 10.13 ms (+4.81) 11.36 ms (+5.15) 18.60 ms (+4.26)
E Ablation Studies
Study for Nin EOPM. Here, we study the effect of Nused in EOPM (Eq. 5). In Figure 7,
we report the results for ConvL and ConvS on CIFAR-10 pre-trained ResNet18, under a 1% bit
error rate (BER). The results demonstrate that if we apply larger N, the performance increases
until convergence. Specifically, for ConvL (Figure 7a), larger Nempirically has a smaller standard
deviation; this means larger Ngives better stability but at the cost of time-consuming training. In
contrast, for the small generator ConvS (Figure 7b), we can find that the standard deviation is still
large even trained by larger N; the reason might be that small generators are not as capable of learning
representations as larger ones. Therefore, there exists a tradeoff between the stability of the generator
performance and the total training time. In our implementation, choosing N= 5 or10is a good
balance.
(a) Using ConvL
 (b) Using ConvS
Figure 7: The experimental results on using different sizes of Nfor EOPM.
Tradeoff Between Clean Accuracy (CA) and Perturbed Accuracy (PA). We conducted an
experiment to study the effect of different Î»values, which balance the ratio of clean accuracy and
perturbed accuracy. In Table 10, the experimental results showed that a smaller Î»can preserve clean
accuracy, but result in poor perturbed accuracy. On the contrary, larger Î»can deliver higher perturbed
accuracy, but with more clean accuracy drop. This phenomenon has also been observed in adversarial
training [Zhang et al., 2019].
Comparison to Universal Input Perturbation (UIP). Moosavi-Dezfooli et al. [2017] has shown
that there exists a universal adversarial perturbation to the input data such that the model will
make wrong predictions on a majority of the perturbed images. In our NeuralFuse framework, the
universal perturbation is a special case when we set G(x) = tanh ( UIP)for any data sample x.
The transformed data sample then becomes xt=clip[âˆ’1,1] 
x+ tanh ( UIP)
, where xtâˆˆ[âˆ’1,1]d
17Table 10: Experimental results based on Î»value choosing. The results show
thatÎ»= 5can balance the tradeoff between clean accuracy and perturbed
accuracy.
Base
ModelÎ» CA PAConvL
CA (NF) PA (NF) RP
ResNet1810
92.6 38.9 Â±12.490.1 88.0 Â±1.7 49.1
5 89.8 87.8 Â±1.7 48.8
1 90.0 84.2 Â±3.8 45.3
0.1 91.6 65.7 Â±9.3 26.8
0.01 92.2 43.6 Â±13 4.7
VGG1910
90.5 36.0 Â±12.089.6 77.9 Â±19 41.9
5 89.8 77.7 Â±19 41.7
1 89.9 73.1 Â±19 37.1
0.1 89.1 51.2 Â±16 15.2
0.01 90.2 36.8 Â±12 0.8
Note. CA (%): clean accuracy; PA (%): perturbed accuracy; NF: NeuralFuse; and
RP: total recover percentage of PA (NF) vs. PA
andUIP is a trainable universal input perturbation that has the same size as the input data. The
experimental results with the universal input perturbation are shown in Table 11. We observe that
its performance is much worse than our proposed NeuralFuse. This result validates the necessity of
adopting input-aware transformation for learning error-resistant data representations in low-voltage
scenarios.
Table 11: Performance of the universal input perturbation (UIP) trained by EOPM on CIFAR-10
pre-trained ResNet18.
Base Model BER CA PA CA ( UIP) PA ( UIP) RP
ResNet181%92.638.9Â±12.4 91.8 37.9 Â±11 -1.0
0.5% 70.1Â±11.6 92.5 70.6 Â±11 0.5
ResNet501%92.626.1Â±9.4 80.7 21.0 Â±5.9 -5.1
0.5% 61.0Â±10.3 91.9 62.4 Â±12 1.4
VGG111%88.442.2Â±11.6 86.9 43.0 Â±11 0.8
0.5% 63.6Â±9.3 88.2 64.2 Â±8.8 0.6
VGG161%90.335.7Â±7.9 90.1 37.1 Â±8.5 1.4
0.5% 66.6Â±8.1 90.4 67.3 Â±8.1 0.7
VGG191%90.536.0Â±12.0 89.9 35.3 Â±12 -0.7
0.5% 64.2Â±12.4 90.1 64.4 Â±12 0.2
Note . BER: the bit-error rate of the base model; CA ( %): clean accuracy; PA ( %): perturbed accuracy; UIP:
universal input transformation parameter; and RP: total recovery percentage of PA (UIP) vs. PA
18F Additional Experimental Results on Relaxed Access Settings
We conducted more experiments on Relaxed Access settings to show that our NeuralFuse can
protect the models under different BER The results can be found in Sec. F.1 (CIFAR-10), Sec. F.2
(GTSRB), Sec. F.3 (ImageNet-10), and Sec. F.4 (CIFAR-100). For comparison, we also visualize the
experimental results in the figures below each table.
F.1 CIFAR-10
Table 12: Testing accuracy (%) under 1% and 0.5% of random bit error rate on CIFAR-10.
Base
ModelNF CA1% BER 0.5% BER
PA CA (NF) PA (NF) RP PA CA (NF) PA (NF) RP
ResNet18ConvL
92.689.8 87.8 Â±1.7 48.8 90.4 87.9 Â±2.2 17.8
ConvS 88.2 59.5 Â±11 20.6 91.7 78.4 Â±8.3 8.3
DeConvL 38.9 89.6 88.5 Â±0.8 49.6 70.1 90.2 90.0 Â±0.2 19.9
DeConvS Â±12.4 82.9 68.8 Â±6.4 29.9 Â±11.6 84.1 79.9 Â±3.6 9.8
UNetL 86.6 84.6 Â±0.8 45.6 89.7 86.3 Â±2.4 16.2
UNetS 84.4 68.8 Â±6.0 29.8 90.9 80.7 Â±5.8 10.7
ResNet50ConvL
92.685.5 53.2 Â±22 27.1 90.3 86.5 Â±3.2 25.5
ConvS 85.2 34.6 Â±14 8.5 90.8 73.3 Â±8.7 12.3
DeConvL 26.1 87.4 63.3 Â±21 37.2 61.0 89.5 87.2 Â±2.5 26.2
DeConvS Â±9.4 82.4 42.2 Â±17 16.1 Â±10.3 90.3 75.5 Â±8.1 14.5
UNetL 86.2 75.5 Â±12 49.4 89.9 83.9 Â±3.6 22.9
UNetS 77.3 56.2 Â±19 30.1 89.7 76.1 Â±7.2 15.1
VGG11ConvL
88.489.6 87.2 Â±2.9 45.1 89.8 87.0 Â±1.3 23.3
ConvS 84.9 66.3 Â±7.5 24.1 88.2 74.5 Â±5.7 10.9
DeConvL 42.2 89.3 87.2 Â±2.6 45.0 63.6 89.6 86.9 Â±1.1 23.2
DeConvS Â±11.6 85.6 68.2 Â±7.1 26.0 Â±9.3 88.3 75.7 Â±4.6 12.1
UNetL 87.1 83.6 Â±1.3 41.4 88.0 82.4 Â±1.8 18.8
UNetS 85.5 72.7 Â±4.6 30.5 88.1 75.8 Â±4.3 12.2
VGG16ConvL
90.390.1 86.0 Â±6.2 50.3 90.2 88.5 Â±0.9 21.9
ConvS 87.4 59.6 Â±12 23.9 89.9 77.8 Â±4.8 11.1
DeConvL 35.7 89.7 85.5 Â±6.8 49.8 66.6 89.7 88.2 Â±1.0 21.4
DeConvS Â±7.9 86.8 66.5 Â±11 30.8 Â±8.1 90.0 78.4 Â±4.7 11.8
UNetL 87.4 83.4 Â±4.4 47.7 89.0 86.2 Â±1.5 19.6
UNetS 87.4 71.2 Â±8.2 35.5 89.0 80.2 Â±3.5 13.7
VGG19ConvL
90.589.8 77.7 Â±19 41.7 90.4 88.1 Â±1.8 23.9
ConvS 87.3 52.7 Â±17 16.7 89.6 74.5 Â±9.0 10.3
DeConvL 36.0 86.3 78.4 Â±18 42.4 64.2 90.4 88.5 Â±1.4 24.3
DeConvS Â±12.0 86.5 58.2 Â±18 22.2 Â±12.4 89.7 75.2 Â±8.6 11.0
UNetL 86.3 82.1 Â±4.8 46.0 89.1 85.0 Â±2.7 20.8
UNetS 86.3 66.4 Â±13 30.4 89.2 77.1 Â±7.3 12.9
Note . CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
Nominal (w/o NeuralFuse) Low voltage (w/o NeuralFuse) Nominal (w/ NeuralFuse) Low voltage (w/ NeuralFuse)
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.6
Accuracy (%)38       .989  88  89  
82  86  84       .8     .2     .6
     .9     .6     .487  
59  88  
68  84  
68       .8
     .5     .5
     .8     .6
     .8CIFAR-10 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.6
26       .185  85  87  82  86  
77       .5      .2     .4
     .4     .2
     .3
53  
34  63  
42  75  
56       .2
     .6     .3
     .2     .5
     .2CIFAR-10 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS88.4
42       .289  84  89  85  87  85       .6
     .9     .3
     .6     .1     .5 87  
66  87  
68  83  
72       .2
     .3     .2
     .2     .6
     .7CIFAR-10 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS90.3
35       .790  87  89  86  87  87       .1     .4     .7     .8      .4      .4 86  
59  85  
66  83  
71       .0
     .6     .5
     .5     .4
     .2CIFAR-10 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS90.5
36       .089  87  86  86  86  86       .8     .3      .3      .5      .3      .3
77  
52  78  
58  82  
66       .7
     .7     .4
     .2     .1
     .4CIFAR-10 pre-trained VGG19
(a) CIFAR-10, 1%Bit Error Rate
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.6
70       .190  91  90  
84  89  90       .4      .7     .2
     .1     .7      .987  
78  90  
79  86  
80       .9
     .4     .0
     .9     .3
     .7CIFAR-10 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.6
61       .090  90  89  90  89  89       .3      .8      .5      .3      .9      .786  
73  87  
75  83  
76       .5
     .3     .2
     .5     .9
     .1CIFAR-10 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS88.4
63       .689  88  89  88  88  88       .8     .2     .6      .3      .0      .1 87  
74  86  
75  82  
75       .0
     .5     .9
     .7     .4
     .8CIFAR-10 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS90.3
66       .690  89  89  90  89  89       .2      .9      .7      .0      .0      .0 88  
77  88  
78  86  
80       .5
     .8     .2
     .4     .2
     .2CIFAR-10 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS90.5
64       .290  89  90  89  89  89       .4      .6      .4      .7      .1      .2 88  
74  88  
75  85  
77       .1
     .5     .5
     .2     .0
     .1CIFAR-10 pre-trained VGG19
(b) CIFAR-10, 0.5%Bit Error Rate
Figure 8: Experimental results on CIFAR-10
19F.2 GTSRB
Table 13: Testing accuracy (%) under 1% and 0.5% of random bit error rate on GTSRB.
Base
ModelNF CA1% BER 0.5% BER
PA CA (NF) PA (NF) RP PA CA (NF) PA (NF) RP
ResNet18ConvL
95.595.7 91.1 Â±4.7 54.2 93.4 89.5 Â±1.9 14.3
ConvS 94.4 68.6 Â±12 31.7 94.8 87.7 Â±4.2 12.4
DeConvL 36.9 95.6 91.3 Â±4.3 54.4 75.2 95.4 93.4 Â±1.1 18.1
DeConvS Â±16.0 95.7 78.1 Â±9.1 41.2 Â±12.7 95.8 90.1 Â±3.3 14.9
UNetL 96.2 93.8 Â±1.0 56.9 96.2 93.5 Â±1.6 18.3
UNetS 95.9 85.1 Â±6.9 48.2 95.5 91.4 Â±2.8 16.2
ResNet50ConvL
95.095.6 71.6 Â±20 42.1 94.6 90.6 Â±3.7 16.6
ConvS 94.8 50.5 Â±22 21.0 95.4 84.5 Â±8.5 10.5
DeConvL 29.5 94.9 71.6 Â±21 42.0 74.0 94.7 91.6 Â±2.9 17.6
DeConvS Â±16.9 93.0 56.4 Â±17 26.9 Â±13.0 94.6 87.4 Â±5.9 13.5
UNetL 94.5 80.6 Â±15 51.1 96.5 93.7 Â±2.3 19.7
UNetS 94.7 64.7 Â±22 35.2 95.9 90.6 Â±4.8 16.7
VGG11ConvL
91.994.8 85.7 Â±7.2 50.9 93.9 92.6 Â±0.7 27.7
ConvS 91.1 62.2 Â±11 27.3 90.9 80.5 Â±3.5 15.7
DeConvL 34.9 95.0 84.6 Â±7.6 49.7 64.9 93.6 91.9 Â±0.6 27.1
DeConvS Â±12.4 92.4 67.5 Â±11 32.6 Â±10.8 92.3 83.1 Â±3.7 18.2
UNetL 92.2 83.2 Â±6.0 48.3 94.8 90.6 Â±1.7 25.7
UNetS 94.7 73.4 Â±10 38.5 94.6 88.9 Â±2.2 24.1
VGG16ConvL
95.296.3 72.4 Â±12 57.3 95.6 93.2 Â±1.8 34.4
ConvS 94.1 39.8 Â±13 24.6 94.3 82.2 Â±6.2 23.4
DeConvL 15.1 96.4 72.0 Â±12 56.9 58.8 95.6 93.1 Â±2.0 34.3
DeConvS Â±6.8 93.8 50.9 Â±13 35.8 Â±8.9 95.1 84.0 Â±5.3 25.2
UNetL 95.8 78.6 Â±11 63.5 96.0 92.8 Â±2.0 34.0
UNetS 94.3 63.3 Â±14 48.1 95.4 87.8 Â±3.6 29.0
VGG19ConvL
95.596.0 88.3 Â±7.2 51.7 95.6 93.4 Â±2.1 24.2
ConvS 93.8 69.0 Â±14 32.4 94.9 87.0 Â±4.4 17.8
DeConvL 36.6 95.4 87.2 Â±7.5 50.6 69.1 95.5 92.4 Â±2.2 23.3
DeConvS Â±6.8 94.5 73.1 Â±12 36.5 Â±11.1 95.5 88.8 Â±3.7 19.7
UNetL 95.4 88.2 Â±6.7 51.7 94.9 91.7 Â±2.5 22.6
UNetS 94.6 80.6 Â±9.0 44.1 96.5 90.8 Â±3.4 21.6
Note . CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
Nominal (w/o NeuralFuse) Low voltage (w/o NeuralFuse) Nominal (w/ NeuralFuse) Low voltage (w/ NeuralFuse)
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.5
Accuracy (%)36       .995  94  95  95  96  95       .7      .4      .6      .7      .2      .991  
68  91  
78  93  
85       .1
     .6     .3
     .1     .8
     .1GTSRB pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.0
29       .595  94  94  93  94  94       .6      .8      .9     .0     .5      .7
71  
50  71  
56  80  
64       .6
     .5     .6
     .4     .6
     .7GTSRB pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS91.9
34       .994  91  95  92  92  94       .8
     .1     .0     .4      .2     .7
85  
62  84  
67  83  
73       .7
     .2     .6
     .5     .2
     .4GTSRB pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.2
15       .196  94  96  93  95  94       .3     .1     .4     .8     .8     .3
72  
39  72  
50  78  
63       .4
     .8     .0
     .9     .6
     .3GTSRB pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.5
36       .696  93  95  94  95  94       .0     .8     .4      .5      .4      .6
88  
69  87  
73  88  
80       .3
     .0     .2
     .1     .2
     .6GTSRB pre-trained VGG19
(a) GTSRB, 1%Bit Error Rate
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.5
75       .293  94  95  95  96  95       .4     .8      .4      .8      .2      .5
89  87  93  90  93  91       .5     .7     .4     .1     .5     .4GTSRB pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.0
74       .094  95  94  94  96  95       .6      .4      .7      .6     .5      .990  
84  91  87  93  90       .6
     .5     .6
     .4     .7     .6GTSRB pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS91.9
64       .993  90  93  92  94  94       .9     .9     .6      .3     .8      .6 92  
80  91  
83  90  88       .6
     .5     .9
     .1     .6     .9GTSRB pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.2
58       .895  94  95  95  96  95       .6      .3      .6      .1      .0      .493  
82  93  
84  92  87       .2
     .2     .1
     .0     .8
     .8GTSRB pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS95.5
69       .195  94  95  95  94  96       .6      .9      .5      .5      .9     .593  
87  92  88  91  90       .4
     .0     .4     .8     .7      .8GTSRB pre-trained VGG19
(b) GTSRB, 0.5%Bit Error Rate
Figure 9: Experimental results on GTSRB.
20F.3 ImageNet-10
Table 14: Testing accuracy under 0.5% of random bit error rate on ImageNet-10.
Base
ModelNF CA0.5% BER
PA CA (NF) PA (NF) RP
ResNet18ConvL
92.2 72.3Â±7.094.0 88.0 Â±2.0 15.7
ConvS 91.8 83.6 Â±4.1 11.3
DeConvL 94.0 89.2 Â±1.3 16.9
DeConvS 92.8 87.5 Â±2.3 15.2
UNetL 94.0 88.1 Â±1.4 15.8
UNetS 93.2 86.4 Â±2.2 14.1
ResNet50ConvL
89.8 39.4Â±1192.2 80.0 Â±5.8 40.6
ConvS 91.8 65.0 Â±11 25.6
DeConvL 93.0 79.4 Â±5.9 40.0
DeConvS 93.2 70.9 Â±9.1 31.5
UNetL 92.2 80.5 Â±5.8 41.1
UNetS 92.4 73.6 Â±8.9 34.2
VGG11ConvL
91.6 47.8Â±1392.0 86.1 Â±3.7 38.3
ConvS 89.4 66.4 Â±7.1 18.6
DeConvL 91.0 86.0 Â±3.0 38.2
DeConvS 89.0 72.5 Â±7.8 24.7
UNetL 92.4 83.0 Â±3.5 35.2
UNetS 86.2 73.5 Â±6.0 25.7
VGG16ConvL
94.6 38.4Â±1790.8 77.1 Â±11 38.7
ConvS 90.2 60.2 Â±14 21.8
DeConvL 91.2 77.2 Â±11 38.8
DeConvS 90.0 62.3 Â±14 23.9
UNetL 90.6 81.1 Â±5.9 42.7
UNetS 86.4 72.3 Â±8.8 33.9
VGG19ConvL
92.4 37.2Â±1191.4 75.5 Â±8.8 38.3
ConvS 88.8 56.5 Â±13 19.3
DeConvL 91.0 75.9 Â±8.9 38.7
DeConvS 88.8 64.0 Â±11 26.8
UNetL 89.4 77.9 Â±6.1 40.7
UNetS 87.6 65.9 Â±10 28.7
Note . CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP:
total recovery percentage of PA (NF) vs. PA
Nominal (w/o NeuralFuse) Low voltage (w/o NeuralFuse) Nominal (w/ NeuralFuse) Low voltage (w/ NeuralFuse)
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.2
Accuracy (%)72       .394  91  94  92  94  93       .0     .8     .0      .8      .0      .288  83  89  87  88  86       .0
     .6     .2     .5      .1     .4ImageNet-10 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS89.8
39       .492  91  93  93  92  92       .2      .8      .0      .2      .2      .4
80  
65  79  
70  80  
73       .0
     .0     .4
     .9     .5
     .6ImageNet-10 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS91.6
47       .892  89  91  89  92  
86       .0     .4     .0     .0     .4
     .2 86  
66  86  
72  83  
73       .1
     .4     .0
     .5     .0
     .5ImageNet-10 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS94.6
38       .490  90  91  90  90  86       .8      .2      .2      .0      .6
     .4
77  
60  77  
62  81  
72       .1
     .2     .2
     .3     .1
     .3ImageNet-10 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS92.4
37       .291  88  91  88  89  87       .4     .8     .0     .8      .4     .6
75  
56  75  
64  77  
65       .5
     .5     .9
     .0     .9
     .9ImageNet-10 pre-trained VGG19
Figure 10: Experimental results on ImageNet-10, 0.5%Bit Error Rate.
21F.4 CIFAR-100
As mentioned in the previous section, larger generators like ConvL, DeConvL, and UNetL have
better performance than small generators. For CIFAR-100, we find that the gains of utilizing
NeuralFuse are less compared to the other datasets. We believe this is because CIFAR-100 is a more
challenging dataset (more classes) for the generators to learn to protect the base models. Nevertheless,
NeuralFuse can still function to restore some degraded accuracy; these results also demonstrate that
our NeuralFuse is applicable to different datasets. In addition, although the recover percentage is
less obvious on CIFAR-100 (the more difficult dataset), we can still conclude that our NeuralFuse is
applicable to different datasets.
Table 15: Testing accuracy (%) under 1%, 0.5% and 0.35% of random bit error rate on CIFAR-100.
Base
ModelNF C.A.1% BER 0.5% BER 0.35% BER
PA CA (NF) PA (NF) RP PA CA (NF) PA (NF) RP PA CA (NF) PA (NF) RP
ResNet18ConvL
73.754.8 11.0 Â±7.7 6.4 65.2 39.0 Â±7.1 18.1 69.4 42.9 Â±6.2 11.4
ConvS 49.7 4.2 Â±2.2 -0.4 70.0 24.5 Â±7.6 3.6 72.1 35.1 Â±7.3 3.7
DeConvL 4.6 55.2 11.9 Â±8.2 7.3 20.9 66.3 38.2 Â±6.9 17.3 31.4 69.2 42.9 Â±5.5 11.4
DeConvS Â±2.9 32.7 4.0 Â±2.2 -0.6 Â±7.4 68.2 25.9 Â±6.8 5 Â±7.6 71.6 35.8 Â±5.5 4.4
UNetL 50.6 14.5 Â±8.9 10.0 66.2 40.1 Â±6.4 19.2 70.3 46.3 Â±5.5 14.9
UNetS 26.8 4.6 Â±2.5 -0.0 67.1 28.8 Â±6.8 7.9 70.9 38.3 Â±6.4 6.9
ResNet50ConvL
73.563.5 3.2 Â±1.7 0.1 68.4 28.8 Â±6.7 7.6 72.0 40.8 Â±7.5 5.1
ConvS 65.5 3.2 Â±1.6 0.1 71.9 23.1 Â±6.9 1.9 73.0 37.4 Â±8.0 1.7
DeConvL 3.0 59.6 3.2 Â±1.7 0.2 21.3 68.1 28.6 Â±7.0 7.4 35.7 71.7 41.7 Â±7.7 6.1
DeConvS Â±1.8 61.1 3.2 Â±1.7 0.1 Â±7.0 70.3 25.0 Â±6.7 3.7 Â±8.6 72.8 38.9 Â±7.9 3.3
UNetL 39.0 5.0 Â±1.7 1.9 66.6 36.5 Â±6.2 15.3 70.8 45.3 Â±6.7 9.6
UNetS 47.7 3.4 Â±1.8 0.3 69.1 26.1 Â±6.6 4.8 72.6 39.6 Â±7.8 3.9
VGG11ConvL
64.858.3 19.7 Â±11 11.5 63.1 38.8 Â±9.3 15.0 63.9 42.4 Â±9.0 11.1
ConvS 56.6 10.4 Â±7.4 2.2 62.7 27.9 Â±10 4.0 63.9 41.8 Â±8.3 10.5
DeConvL 8.2 60.3 21.2 Â±11 13.0 23.9 63.9 40.0 Â±9.0 16.2 31.3 64.0 42.8 Â±9.1 11.5
DeConvS Â±5.7 58.3 11.8 Â±7.9 3.5 Â±9.4 61.9 29.8 Â±9.9 5.9 Â±10 63.5 36.1 Â±10 4.8
UNetL 51.1 22.1 Â±8.2 13.9 61.8 37.8 Â±9.0 13.9 63.5 40.9 Â±9.3 9.6
UNetS 51.9 13.1 Â±7.9 4.9 61.7 29.8 Â±9.7 6.0 63.8 35.7 Â±9.9 4.5
VGG16ConvL
67.851.4 19.2 Â±6.0 12.6 61.8 41.1 Â±5.6 18.7 64.9 44.9 Â±5.3 13.8
ConvS 44.3 6.7 Â±2.3 0.1 63.8 27.5 Â±6.8 5.1 66.0 36.3 Â±6.1 5.1
DeConvL 7.0 53.1 20.8 Â±6.2 14.2 22.4 62.8 42.1 Â±5.5 19.8 31.1 65.0 46.6 Â±5.2 15.5
DeConvS Â±3.5 23.5 4.8 Â±1.7 -1.8 Â±7.0 62.1 29.9 Â±6.7 7.5 Â±7.2 64.9 38.1 Â±6.3 7.0
UNetL 50.2 25.3 Â±1.7 18.7 61.7 41.3 Â±5.0 18.9 64.8 46.8 Â±4.6 15.7
UNetS 27.7 9.9 Â±2.1 3.3 61.6 31.3 Â±6.3 8.9 65.0 39.8 Â±5.9 8.7
VGG19ConvL
67.859.4 29.2 Â±8.1 18.6 65.6 46.5 Â±6.8 12.5 66.9 49.2 Â±7.4 7.0
ConvS 63.7 14.4 Â±5.1 3.8 66.6 38.3 Â±6.8 4.2 67.7 45.3 Â±8.5 3.2
DeConvL 10.6 60.1 29.6 Â±8.5 19.0 34.0 65.7 46.9 Â±7.1 12.9 42.1 67.3 49.8 Â±7.6 7.6
DeConvS Â±4.3 60.9 16.1 Â±6.0 5.6 Â±9.6 66.5 39.0 Â±3.7 5.0 Â±9.4 67.7 45.7 Â±8.4 3.6
UNetL 58.7 30.2 Â±8.2 19.6 65.5 46.9 Â±6.5 12.9 67.4 50.0 Â±7.5 7.9
UNetS 59.1 18.0 Â±6.2 7.4 66.3 40.1 Â±8.0 6.1 67.5 46.6 Â±8.4 4.5
Note . CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
Nominal (w/o NeuralFuse) Low voltage (w/o NeuralFuse) Nominal (w/ NeuralFuse) Low voltage (w/ NeuralFuse)
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS73.7
Accuracy (%)
4     .654  
49  55  
32  50  
26       .8
     .7     .2
     .7     .6
     .8
11  
4  11  
4  14  
4       .0
   .2     .9
   .0     .5
   .6CIFAR-100 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS73.5
3     .063  65  
59  61  
39  47       .5     .5
     .6     .1
     .0     .7
3  3  3  3  5  3     .2    .2    .2    .2   .0   .4CIFAR-100 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS64.8
8     .258  56  60  58  
51  51       .3     .6     .3     .3
     .1      .9
19  
10  21  
11  22  
13       .7
     .4     .2
     .8     .1
     .1CIFAR-100 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS67.8
7     .051  
44  53  
23  50  
27       .4
     .3     .1
     .5     .2
     .7
19  
6  20  
4  25  
9       .2
   .7     .8
   .8     .3
   .9CIFAR-100 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS67.8
10       .659  63  60  60  58  59       .4     .7
     .1      .9     .7      .1
29  
14  29  
16  30  
18       .2
     .4     .6
     .1     .2
     .0CIFAR-100 pre-trained VGG19
(a) CIFAR-100, 1%Bit Error Rate
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS73.7
20       .965  70  66  68  66  67       .2     .0
     .3     .2     .2      .1
39  
24  38  
25  40  
28       .0
     .5     .2
     .9     .1
     .8CIFAR-100 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS73.5
21       .368  71  68  70  66  69       .4     .9
     .1     .3
     .6     .1
28  
23  28  25  36  
26       .8
     .1     .6
     .0     .5
     .1CIFAR-100 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS64.8
23       .963  62  63  61  61  61       .1      .7     .9     .9      .8      .7
38  
27  40  
29  37  
29       .8
     .9     .0
     .8     .8
     .8CIFAR-100 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS67.8
22       .461  63  62  62  61  61       .8     .8      .8      .1      .7      .6
41  
27  42  
29  41  
31       .1
     .5     .1
     .9     .3
     .3CIFAR-100 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS67.8
34       .065  66  65  66  65  66       .6      .6      .7      .5      .5      .3
46  
38  46  
39  46  
40       .5
     .3     .9
     .0     .9
     .1CIFAR-100 pre-trained VGG19
(b) CIFAR-100, 0.5%Bit Error Rate
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS73.7
31       .469  72  69  71  70  70       .4     .1     .2     .6     .3      .9
42  
35  42  
35  46  
38       .9
     .1     .9
     .8     .3
     .3CIFAR-100 pre-trained ResNet18
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS73.5
35       .772  73  71  72  70  72       .0      .0     .7     .8     .8     .6
40  37  41  38  45  
39       .8
     .4     .7     .9     .3
     .6CIFAR-100 pre-trained ResNet50
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS64.8
31       .363  63  64  63  63  63       .9      .9      .0      .5      .5      .8
42  41  42  
36  40  
35       .4      .8      .8
     .1     .9
     .7CIFAR-100 pre-trained VGG11
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS67.8
31       .164  66  65  64  64  65       .9     .0      .0      .9      .8      .0
44  
36  46  
38  46  
39       .9
     .3     .6
     .1     .8
     .8CIFAR-100 pre-trained VGG16
w/o NF  ConvL ConvSDeConvL DeConvSUNetL UNetS67.8
42       .166  67  67  67  67  67       .9      .7      .3      .7      .4      .5
49  45  49  45  50  46       .2
     .3     .8
     .7     .0
     .6CIFAR-100 pre-trained VGG19
(c) CIFAR-100, 0.35% Bit Error Rate
Figure 11: Experimental results on CIFAR-100.
22G Additional Experimental Results on Restricted Access Settings
(Transferability)
We conduct more experiments with Restricted Access settings to show that our NeuralFuse can be
transferred to protect various black-box models. The experimental results are shown in Sec. G.1
(CIFAR-10), Sec. G.2 (GTSRB), and Sec. G.3 (CIFAR-100).
We find that using VGG19 as a white-box surrogate model has better transferability than ResNet18 for
all datasets. In addition, we can observe that some NeuralFuse generators have downward applicability
if base models have a similar architecture. In other words, if we try to transfer a generator trained on
a large BER (e.g., 1%) to a model with a small BER (e.g., 0.5%), the performance will be better than
that of a generator trained with the original BER (e.g., 0.5%). For example, in Table 16, we could
find that if we use VGG19 as a source model to train the generator ConvL (1%), the generator could
deliver better performance (in terms of PA (NF)) when applied to similar base models (e.g., VGG11,
VGG16, or VGG19) under a 0.5% BER, compared to using itself as a source model (shown in Table
12). We conjecture that this is because the generators trained on a larger BER can also cover the error
patterns of a smaller BER, and thus they have better generalizability across smaller B.E.Rs.
To further improve the transferability to cross-architecture target models, we also conduct an experi-
ment in Sec. G.4 to show that using ensemble-based training can help the generator to achieve this
feature.
G.1 CIFAR-10
The results of CIFAR-10 in which NeuralFuse is trained at 1% BER are shown in Table 16.
Table 16: Transfer results on CIFAR-10: NeuralFuse trained on SM with 1% BER
SM TM BER CA PAConvL (1%) UNetL (1%)
CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet18ResNet18 0.5% 92.6 70.1 Â±11.6 89.8 89.5 Â±0.2 19.4 86.6 86.2 Â±0.3 16.1
ResNet501%92.626.1Â±9.489.536.0Â±19 9.985.238.8Â±19 12.7
0.5% 61.0Â±10.3 75.1Â±10 14.1 77.1Â±5.0 16.1
VGG111%88.442.2Â±11.688.462.5Â±8.4 20.376.861.1Â±8.5 18.9
0.5% 63.6Â±9.3 81.0Â±4.6 17.4 73.7Â±3.0 10.1
VGG161%90.335.7Â±7.989.663.3Â±18 27.685.259.9Â±16 24.2
0.5% 66.6Â±8.1 85.0Â±3.4 18.4 80.2Â±4.5 13.6
VGG191%90.536.0Â±12.089.650.7Â±22 14.785.351.1Â±16 15.1
0.5% 64.2Â±12.4 80.2Â±8.7 16.0 76.5Â±7.8 12.3
VGG19ResNet181%92.638.9Â±12.489.861.0Â±17 22.187.069.7Â±11 30.8
0.5% 70.1Â±11.6 86.1Â±6.9 16.0 84.2Â±3.0 14.1
ResNet501%92.626.1Â±9.489.934.0Â±19 7.987.044.2Â±17 18.1
0.5% 61.0Â±10.3 76.5Â±10 15.5 80.7Â±4.2 19.7
VGG111%88.442.2Â±11.689.776.5Â±7.0 34.387.179.9Â±5.6 37.7
0.5% 63.6Â±9.3 88.0Â±2.1 24.4 85.4Â±0.8 21.8
VGG161%90.335.7Â±7.989.675.5Â±12 39.887.278.9Â±7.8 43.2
0.5% 66.6Â±8.1 88.9Â±0.6 22.3 86.2Â±0.3 19.6
VGG19 0.5% 90.5 64.2 Â±12.4 89.8 89.6 Â±8.7 25.4 87.4 86.8 Â±0.4 22.6
Note . SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target
model; CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
23G.2 GTSRB
In Tables 17 and 18, we show the results on GTSRB in which NeuralFuse is trained at 1.5% and 1%
BER, respectively.
Table 17: Transfer results on GTSRB: NeuralFuse trained on SM with 1.5% BER
SM TM BER CA PAConvL (1.5%) UNetL (1.5%)
CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet18ResNet181%95.536.9Â±16.095.793.9Â±1.9 57.094.994.4Â±0.4 57.5
0.5% 75.2Â±12.7 95.7Â±0.2 20.5 94.8Â±0.2 19.6
ResNet501%95.029.5Â±16.994.437.0Â±22 7.594.447.1Â±23 17.6
0.5% 74.0Â±13.0 77.5Â±13 3.5 84.8Â±9.5 10.8
VGG111%91.934.9Â±12.492.845.2Â±10 10.391.450.5Â±13 15.6
0.5% 64.9Â±10.8 79.4Â±5.8 14.5 83.9Â±4.2 19.0
VGG161%95.215.1Â±6.895.431.1Â±13 15.994.636.8Â±12 21.7
0.5% 58.8Â±8.9 84.5Â±8.3 25.8 86.0Â±8.6 27.2
VGG191%95.536.6Â±6.895.056.4Â±15 19.894.360.8Â±15 24.2
0.5% 69.1Â±11.1 86.9Â±3.4 17.8 87.7Â±3.8 18.6
VGG19ResNet181%95.536.9Â±16.088.450.3Â±12 13.492.863.7Â±16 26.8
0.5% 75.2Â±12.7 77.9Â±7.4 2.7 87.5Â±3.9 12.3
ResNet501%95.029.5Â±16.987.529.7Â±17 0.292.540.4Â±21 10.9
0.5% 74.0Â±13.0 67.9Â±17 -6.1 77.5Â±15 3.5
VGG111%91.934.9Â±12.489.747.1Â±11 12.293.560.0Â±12 25.1
0.5% 64.9Â±10.8 76.3Â±5.1 11.4 86.0Â±3.8 21.1
VGG161%95.215.1Â±6.893.029.2Â±15 14.193.038.5Â±16 23.4
0.5% 58.8Â±8.9 75.7Â±12 16.9 79.9Â±8.3 21.1
VGG191%95.536.6Â±6.895.187.4Â±6.0 50.894.688.7Â±5.0 52.1
0.5% 69.1Â±11.1 92.4Â±2.4 23.3 92.4Â±2.2 23.3
Note . SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target
model; CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
Table 18: Transfer results on GTSRB: NeuralFuse trained on SM with 1% BER
SM TM BER CA PAConvL (1%) UNetL (1%)
CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet18ResNet18 0.5% 95.5 75.2 Â±12.7 95.7 95.3 Â±0.5 20.1 96.2 95.7 Â±0.3 20.5
ResNet501%95.029.5Â±16.994.535.6Â±21 6.195.642.6Â±23 13.1
0.5% 74.0Â±13.0 78.8Â±13 4.8 87.3Â±9.0 13.3
VGG111%91.934.9Â±12.493.145.8Â±11 10.994.047.1Â±14 12.2
0.5% 64.9Â±10.8 81.8Â±5.0 16.9 84.2Â±4.8 19.3
VGG161%95.215.1Â±6.895.526.5Â±12 11.495.532.4Â±11 17.3
0.5% 58.8Â±8.9 82.2Â±9.0 23.4 85.4Â±6.7 26.6
VGG191%95.536.6Â±6.894.953.2Â±14 16.695.660.9Â±15 24.3
0.5% 69.1Â±11.1 85.4Â±4.5 16.3 87.5Â±3.7 18.4
VGG19ResNet181%95.536.9Â±16.093.753.1Â±16 16.295.063.4Â±18 26.5
0.5% 75.2Â±12.7 83.9Â±7.6 8.7 89.7Â±4.8 14.5
ResNet501%95.029.5Â±16.992.830.6Â±18 1.195.438.9Â±22 9.4
0.5% 74.0Â±13.0 74.7Â±18 0.7 81.5Â±16 7.5
VGG111%91.934.9Â±12.493.750.6Â±11 15.795.158.9Â±15 24.0
0.5% 64.9Â±10.8 82.3Â±5.1 17.4 87.5Â±3.7 22.6
VGG161%95.215.1Â±6.895.227.8Â±15 12.795.233.5Â±14 18.4
0.5% 58.8Â±8.9 79.0Â±12 20.2 81.8Â±7.8 23.0
VGG19 0.5% 95.5 69.1 Â±11.1 96.0 94.0 Â±2.2 24.9 95.4 93.9 Â±2.1 24.8
Note . SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target
model; CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
24G.3 CIFAR-100
In Tables 19 and 20, we show results on CIFAR-100 with NeuralFuse trained at 1% and 0.5% BER,
respectively.
Table 19: Transfer results on CIFAR-100: NeuralFuse trained on SM with 1% BER
SM TM BER CA PAConvL (1%) UNetL (1%)
CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet18ResNet180.5%73.720.9Â±7.454.835.8Â±5.2 14.950.639.3Â±2.8 18.4
0.35% 31.4Â±7.6 41.7Â±3.7 10.3 43.3Â±1.4 11.9
ResNet501%
73.53.0Â±1.8
44.92.2Â±2.0 -0.8
41.52.4Â±1.9 -0.6
0.5% 21.3Â±7.0 15.9Â±8.2 -5.4 17.1Â±7.1 -4.2
0.35% 35.7Â±8.6 23.7Â±7.1 -12.0 26.2Â±5.6 -9.5
VGG111%
64.88.2Â±5.7
41.29.8Â±5.6 1.6
37.510.2Â±5.1 2.0
0.5% 23.9Â±9.4 24.2Â±5.9 0.3 24.5Â±4.7 0.6
0.35% 31.3Â±10.0 29.0Â±5.4 -2.3 28.2Â±4.5 -3.1
VGG161%
67.87.0Â±3.5
44.07.9Â±3.7 0.9
39.510.1Â±4.5 3.1
0.5% 22.4Â±7.0 22.4Â±7.6 0.0 26.3Â±5.3 3.9
0.35% 31.1Â±7.2 28.1Â±5.9 -3.0 30.6Â±3.6 -0.5
VGG191%
67.810.6Â±4.3
44.213.5Â±6.1 2.9
40.715.6Â±6.2 5.0
0.5% 34.0Â±9.6 27.9Â±4.8 -6.1 29.3Â±4.6 -4.7
0.35% 42.1Â±9.4 33.2Â±48 -8.9 32.8Â±3.9 -9.3
VGG19ResNet181%
73.74.6Â±2.9
55.55.8Â±3.7 1.2
57.36.8Â±4.4 2.2
0.5% 20.9Â±7.4 24.6Â±6.3 3.7 28.1Â±5.9 7.2
0.35% 31.4Â±7.6 31.1Â±5.0 -0.3 36.4Â±4.5 5.0
ResNet501%
73.53.0Â±1.8
56.12.8Â±2.1 -0.2
56.13.7Â±2.4 0.7
0.5% 21.3Â±7.0 18.9Â±8.6 -2.4 22.8Â±8.5 1.5
0.35% 35.7Â±8.6 28.7Â±8.2 -7.0 33.7Â±7.0 -2.0
VGG111%
64.88.2Â±5.7
52.812.3Â±8.4 4.1
53.915.4Â±9.4 7.2
0.5% 23.9Â±9.4 30.0Â±9.3 6.1 33.3Â±7.2 9.4
0.35% 31.3Â±10.0 36.5Â±7.7 5.2 38.8Â±6.5 7.5
VGG161%
67.87.0Â±3.5
53.611.2Â±4.4 4.2
55.213.6Â±5.2 6.6
0.5% 22.4Â±7.0 32.4Â±7.3 10.0 35.9Â±6.2 13.5
0.35% 31.1Â±7.2 39.4Â±6.3 8.3 42.4Â±4.9 11.3
VGG190.5%67.834.0Â±9.659.450.2Â±3.1 16.258.749.1Â±3.5 15.1
0.35% 42.1Â±9.4 53.1Â±2.3 11.0 52.0Â±3.1 9.9
Note . SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target model; CA ( %):
clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
Table 20: Transfer results on CIFAR-100: NeuralFuse trained on SM with 0.5% BER
SM TM BER CA PAConvL (0.5%) UNetL (0.5%)
CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet18ResNet18 0.35% 73.7 31.4 Â±7.6 65.2 47.7 Â±4.9 16.3 66.2 49.2 Â±4.1 17.8
ResNet500.5%73.521.3Â±7.062.524.0Â±9.9 2.863.526.4Â±9.1 5.1
0.35% 35.7Â±8.6 36.3Â±8.9 0.6 39.4Â±8.1 3.7
VGG110.5%64.823.9Â±9.459.233.0Â±9.8 9.261.134.2Â±9.8 10.3
0.35% 31.3Â±10.0 40.4Â±8.7 9.1 41.4Â±9.0 10.1
VGG160.5%67.822.4Â±7.059.534.7Â±8.0 12.361.437.5Â±6.8 15.2
0.35% 31.1Â±7.2 42.9Â±6.0 11.8 45.3Â±4.9 14.2
VGG190.5%67.834.0Â±9.661.643.7Â±6.2 9.662.045.0Â±6.3 11.0
0.35% 42.1Â±9.4 49.0Â±5.5 6.8 50.5Â±5.3 8.3
VGG19ResNet180.5%73.720.9Â±7.466.124.9Â±6.7 4.067.827.7Â±6.8 6.8
0.35% 31.4Â±7.6 34.4Â±5.4 3.0 38.1Â±5.6 6.7
ResNet500.5%73.521.3Â±7.066.222.7Â±7.8 1.466.725.4Â±8.0 4.2
0.35% 35.7Â±8.6 35.5Â±7.7 -0.2 38.8Â±7.5 3.2
VGG110.5%64.823.9Â±9.459.929.3Â±10 5.461.031.2Â±9.8 7.4
0.35% 31.3Â±10.0 36.6Â±9.5 5.3 38.1Â±9.0 6.8
VGG160.5%67.822.4Â±7.062.530.8Â±7.3 8.462.633.0Â±7.3 10.7
0.35% 31.1Â±7.2 40.0Â±6.5 8.9 42.5Â±5.9 11.3
VGG19 0.35% 67.8 42.1 Â±9.4 65.6 52.0 Â±6.2 9.8 65.5 52.6 Â±6.1 10.4
Note . SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target model; CA ( %):
clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
25G.4 Generator Ensembling
To improve the transferability performance on cross-architecture cases (e.g., using ResNet-based
models as surrogate models to train NeuralFuse and then transfer NeuralFuse to VGG-based target
models), we try to adopt ensemble surrogate models to train our NeuralFuse. The experimental results
are shown in Table 21. We use the same experimental settings mentioned in Table 1 but change
one source model (e.g., ResNet18 or VGG19) into two (ResNet18 with VGG19) for training. The
results show that the overall performance is better than the results shown in Table 1, which means
ensemble-based training can easily solve the performance degradation on cross-architecture target
models.
Table 21: Transfer results on CIFAR-10: NeuralFuse trained on two SM with 1.5% BER
SM TM BER CA PAConvL (1.5%) UNetL (1.5%)
CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet18
+
VGG19ResNet181%92.638.9Â±12.489.488.1Â±1.0 49.286.385.4Â±0.5 46.5
0.5% 70.1Â±11.6 89.2Â±0.2 19.1 86.1Â±0.2 16.0
ResNet501%92.626.1Â±9.489.344.0Â±22 17.986.150.9Â±20 24.8
0.5% 61.0Â±10.3 80.3Â±6.7 19.3 78.6Â±3.9 17.6
VGG111%88.442.2Â±11.689.177.0Â±5.6 34.885.982.3Â±4.1 40.1
0.5% 63.6Â±9.3 87.5Â±1.6 23.9 85.0Â±0.6 21.4
VGG161%90.335.7Â±7.989.180.5Â±8.6 44.885.781.4Â±5.5 45.7
0.5% 66.6Â±8.1 88.2Â±0.7 21.6 85.0Â±0.7 18.4
VGG191%90.536.0Â±12.089.275.1Â±17 39.186.183.0Â±3.4 47.0
0.5% 64.2Â±12.4 89.0Â±0.2 24.8 85.9Â±0.4 21.7
Note . SM: source model, used for training generators; TM: target model, used for testing generators; BER: the bit-error rate of the target
model; CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
H NeuralFuse on Reduced-precision Quantization and Random Bit Errors
As mentioned in Sec. 4.6, we explore the robustness of NeuralFuse to low-precision quantization of
model weights and consider the case of random bit errors. Here, we demonstrate that NeuralFuse can
recover not only the accuracy drop due to reduced precision, but also the drop caused by low-voltage-
induced bit errors (0.5% BER) under low precision. We selected two NeuralFuse generators (ConvL
and UNetL) for our experiments, and these generators were trained with the corresponding base
models (ResNet18 and VGG19) at 1% BER (CIFAR-10, GTSRB) and 0.5% BER (ImageNet-10).
The experimental results are shown as follows: CIFAR-10 (Sec. H.1), GTSRB (Sec. H.2), and
ImageNet-10 (Sec. H.3). Similarly, for ease of comparison, we visualize the experimental results
in the figures below each table. Our results show that NeuralFuse can consistently perform well in
low-precision regimes as well as recover the low-voltage-induced accuracy drop.
H.1 CIFAR-10
Table 22: Reduced-precision Quantization and with 0.5% BER on CIFAR-10 pre-trained models.
Base#Bits CA PAConvL (1%) UNetL (1%)
Model CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet188 92.6 70.1Â±11.6 89.8 89.5 Â±0.2 19.4 86.6 86.2 Â±0.3 16.1
7 92.5 68.8Â±10.4 89.8 89.5 Â±1.7 20.7 86.5 86.0 Â±0.5 17.2
6 92.6 68.4Â±11.2 89.7 89.5 Â±0.2 21.1 86.6 85.9 Â±0.3 17.5
5 92.4 52.7Â±14.1 89.7 90.0 Â±0.7 37.3 86.5 85.5 Â±0.8 32.8
4 91.8 26.3Â±12.7 89.8 58.7 Â±24.5 32.4 86.6 64.9 Â±22.5 38.6
3 84.8 11.3Â±1.8 89.8 12.8 Â±5.8 1.5 86.0 14.8 Â±10.0 3.5
2 10.0 10.0Â±0.0 10.0 10.0 Â±0.0 0.0 10.0 10.0 Â±0.0 0.0
VGG198 90.5 64.2Â±12.4 89.8 89.6 Â±8.7 25.4 87.4 86.8 Â±0.4 22.6
7 90.3 66.5Â±8.5 89.8 89.6 Â±0.2 23.1 87.4 86.7 Â±0.3 20.2
6 90.1 59.8Â±13.2 89.9 89.4 Â±3.8 29.6 87.4 86.4 Â±0.7 26.6
5 90.2 37.7Â±14.1 89.8 78.0 Â±15.8 40.3 87.2 79.8 Â±0.8 42.1
4 87.5 14.7Â±6.0 89.8 27.8 Â±18.9 13.1 87.2 34.4 Â±20.5 19.7
3 78.3 10.5Â±1.5 89.7 10.9 Â±2.6 0.4 86.8 11.0 Â±2.9 0.5
2 10.0 10.0Â±0.0 10.0 10.0 Â±0.0 0.0 10.0 10.0 Â±0.0 0.0
Note. CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs.
PA
26Original Accuracy w/o NeuralFuse NeuralFuse (ConvL) NeuralFuse (UNetL)
8 7 6 5 4 3 2
Bit Quantization Number92.6
Accuracy (%) 92    92    92    92    91    84        .6      .5      .6      .4      .8
     .8 90    90    90    90    90    89        .4      .4      .4      .4      .0      .9  89    89    89    89    89    88        .7      .6      .7      .7      .7      .9
 10        .0 10        .0 10        .0CIFAR-10 pre-trained ResNet18 (Nominal voltage, No Bit Error)(a) Base Model: ResNet18, no bit error.
8 7 6 5 4 3 2
Bit Quantization NumberAccuracy (%) 70    68    68   
 52   
 26   
 11        .1      .8      .4
     .7
     .3
     .3 87    87    87    78   
 40   
 11        .9      .5      .0
     .8
     .0
     .6 88    85    84    77   
 42   
 12        .1     .2      .9
     .4
     .4
     .6 10        .0 10        .0 10        .0CIFAR-10 pre-trained ResNet18 (Low voltage, 0.5% BER) (b) Base Model: ResNet18, 0.5% B.E.R.
8 7 6 5 4 3 2
Bit Quantization Number90.5
Accuracy (%) 90    90    90    90    87   
 78        .5      .3      .1      .2     .5
     .3 90    90    90    90    90    89        .4      .3      .4      .3      .1      .4  89    89    89    88    88    87        .1      .1      .2      .9      .6      .3
 10        .0 10        .0 10        .0CIFAR-10 pre-trained VGG19 (Nominal voltage, No Bit Error)
(c) Base Model: VGG19, no bit error.
8 7 6 5 4 3 2
Bit Quantization NumberAccuracy (%) 64    66    59   
 37   
 14    10        .2     .5
     .8
     .7
     .7     .5 88    88    85   
 60   
 19   
 10        .1      .0     .5
     .1
     .5
     .4 85    84    82   
 60   
 19   
 10        .0      .7     .0
     .2
     .8
     .9 10        .0 10        .0 10        .0CIFAR-10 pre-trained VGG19 (Low voltage, 0.5% BER) (d) Base Model: VGG19, 0.5% B.E.R.
Figure 12: Results of Reduced-precision and bit errors (0.5%) on CIFAR-10 pre-trained base models.
H.2 GTSRB
Table 23: Reduced-precision Quantization and with 0.5% BER on GTSRB pre-trained models.
Base#Bits CA PAConvL (1%) UNetL (1%)
Model CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet188 95.5 75.2Â±12.7 95.7 95.3 Â±0.5 20.1 96.2 95.7 Â±0.3 20.5
7 95.5 69.5Â±10.6 95.7 95.3 Â±0.3 25.8 96.2 95.9 Â±0.3 26.4
6 95.4 67.2Â±14.4 95.7 95.2 Â±0.5 28.0 96.2 95.7 Â±0.5 28.5
5 95.4 48.6Â±18.2 95.8 92.6 Â±5.1 44.0 96.2 94.8 Â±2.5 46.2
4 92.6 24.6Â±9.8 95.9 75.6 Â±16.2 51.0 96.2 86.6 Â±9.5 62.0
3 67.7 5.3Â±3.5 95.4 18.4 Â±15.3 13.1 96.2 25.3 Â±22.5 20.0
2 3.8 3.8Â±0.0 4.1 3.8 Â±0.0 0.0 3.8 3.8 Â±0.0 0.0
VGG198 95.5 69.1Â±11.1 96.0 94.0 Â±2.2 24.9 95.4 93.9 Â±2.1 24.8
7 95.6 66.1Â±14.8 96.0 92.2 Â±5.7 26.1 95.4 92.6 Â±3.7 26.5
6 95.3 64.2Â±8.4 96.0 92.2 Â±5.7 28.0 95.4 92.3 Â±2.3 28.1
5 95.2 48.2Â±14.0 96.0 92.2 Â±5.7 44.0 95.4 86.2 Â±8.4 38.0
4 92.0 18.2Â±14.3 93.0 92.2 Â±5.7 74.0 95.0 49.6 Â±22.8 31.4
3 60.0 2.0Â±0.9 87.3 92.2 Â±5.7 90.2 87.2 1.7 Â±0.9 -0.3
2 5.9 3.8Â±0.0 5.9 3.8 Â±0.0 0.0 5.9 3.8 Â±0.0 0.0
Note . CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
Original Accuracy w/o NeuralFuse NeuralFuse (ConvL) NeuralFuse (UNetL)
8 7 6 5 4 3 2
Bit Quantization Number95.5
Accuracy (%) 95    95    95    95    92   
 67        .5      .5      .4      .4     .6
     .7 93    93    93    93    93    92        .4      .3      .3      .3      .3      .0 96    96    96    96    96    95        .2      .2      .2      .2      .1      .7
3     .83     .83     .8GTSRB pre-trained ResNet18 (Nominal voltage, No Bit Error)
(a) Base Model: ResNet18, no bit error.
8 7 6 5 4 3 2
Bit Quantization NumberAccuracy (%) 75    69    67   
 48   
 24   
5       .2     .5     .2
     .6
     .6
   .3 89    89    88   
 79   
 52   
9       .5      .9      .9
     .3
     .1
   .7 93    94    93    87   
 62   
 12        .5      .5      .5
     .5
     .2
     .6
3     .83     .83     .8GTSRB pre-trained ResNet18 (Low voltage, 0.5% BER) (b) Base Model: ResNet18, 0.5% B.E.R.
8 7 6 5 4 3 2
Bit Quantization Number95.5
Accuracy (%) 95    95    95    95    92   
 60        .5      .6      .3      .2     .0
     .0 95    95    95    95    95    91        .6      .6      .6      .5      .1     .0 94    94    95    94    94    90        .9      .9      .0      .7      .3     .8
5     .95     .95     .9GTSRB pre-trained VGG19 (Nominal voltage, No Bit Error)
(c) Base Model: VGG19, no bit error.
8 7 6 5 4 3 2
Bit Quantization NumberAccuracy (%) 69    66    64   
 48   
 18   
2       .1     .1      .2
     .2
     .2
   .0 93    90    91   
 80   
 37   
1       .4     .4      .3
     .2
     .1
   .8 91    90    89   
 77   
 39   
1       .7      .0      .6
     .6
     .0
   .8 3     .83     .83     .8GTSRB pre-trained VGG19 (Low voltage, 0.5% BER) (d) Base Model: VGG19, 0.5% B.E.R.
Figure 13: Results of Reduced-precision and bit errors (0.5%) on GTSRB pre-trained base models.
27H.3 ImageNet-10
Table 24: Reduced-precision Quantization and with 0.5% BER on ImageNet-10 pre-trained models.
Base#Bits CA PAConvL (0.5%) UNetL (0.5%)
Model CA (NF) PA (NF) RP CA (NF) PA (NF) RP
ResNet188 92.2 72.3Â±7.0 94.0 88.0 Â±2.0 15.7 94.0 88.1 Â±1.4 15.8
7 92.4 70.6Â±13.0 94.2 86.7 Â±4.1 16.1 93.6 87.8 Â±3.5 17.2
6 92.4 68.9Â±9.9 94.2 85.1 Â±4.8 16.2 93.6 86.4 Â±3.7 17.5
5 91.0 60.9Â±13.0 94.2 82.5 Â±6.8 21.6 94.0 83.2 Â±5.9 22.3
4 91.4 47.4Â±9.8 93.8 68.6 Â±9.8 21.2 92.6 68.7 Â±9.2 21.3
3 85.2 28.8Â±11.8 89.2 44.1 Â±14.0 15.3 89.4 42.7 Â±14.2 13.9
2 10.0 10.0Â±0.0 10.0 10.0 Â±0.0 0.0 10.0 10.0 Â±0.0 0.0
VGG198 92.4 37.2Â±11.0 91.4 75.5 Â±8.8 38.3 89.4 77.9 Â±6.1 40.7
7 92.0 27.3Â±6.6 91.2 59.3 Â±13.0 32.0 89.4 65.4 Â±10.0 38.1
6 92.4 27.9Â±6.4 91.0 59.7 Â±11.8 31.8 89.4 64.9 Â±9.9 37.0
5 92.0 15.1Â±4.4 91.6 23.1 Â±0.7 8.0 89.0 27.9 Â±8.8 12.8
4 89.4 12.2Â±2.7 90.8 14.0 Â±4.3 1.8 89.6 14.6 Â±4.9 2.4
3 46.8 9.9Â±0.5 83.2 10.4 Â±0.6 0.5 84.2 9.9 Â±0.7 0.0
2 10.0 10.0Â±0.0 10.0 10.0 Â±0.0 0.0 10.0 10.0 Â±0.0 0.0
Note. CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs.
PA
Original Accuracy w/o NeuralFuse NeuralFuse (ConvL) NeuralFuse (UNetL)
8 7 6 5 4 3 2
Bit Quantization Number92.2
Accuracy (%) 92    92    92    91    91    85        .2      .4      .4      .0      .4
     .2 94    94    94    94    93    89        .0      .2      .2      .2      .8     .2 94    93    93    94    92    89        .0      .6      .6      .0      .6     .4
 10        .0 10        .0 10        .0ImageNet-10 pre-trained ResNet18 (Nominal voltage, No Bit Error)
(a) Base Model: ResNet18, no bit error.
8 7 6 5 4 3 2
Bit Quantization NumberAccuracy (%) 72    70    68    60   
 47   
 28        .3      .6      .9
     .9
     .4
     .8 88    86    85    82   
 68   
 44        .0      .7      .1     .5
     .6
     .1 88    87    86    83   
 68   
 42        .1      .8      .4     .2
     .7
     .7
 10        .0 10        .0 10        .0ImageNet-10 pre-trained ResNet18 (Low voltage, 0.5% BER) (b) Base Model: ResNet18, 0.5% B.E.R.
8 7 6 5 4 3 2
Bit Quantization Number92.4
Accuracy (%) 92    92    92    92    89   
 46        .4      .0      .4      .0     .4
     .8 91    91    91    91    90    83        .4      .2      .0      .6      .8
     .2 89    89    89    89    89    84        .4      .4      .4      .0      .6     .2
 10        .0 10        .0 10        .0ImageNet-10 pre-trained VGG19 (Nominal voltage, No Bit Error)
(c) Base Model: VGG19, no bit error.
8 7 6 5 4 3 2
Bit Quantization NumberAccuracy (%) 37   
 27    27   
 15    12   9       .2
     .3      .9
     .1     .2   .9 75   
 59    59   
 23   
 14    10        .5
     .3      .7
     .1
     .0     .4 77   
 65    64   
 27   
 14   9       .9
     .4      .9
     .9
     .6   .9 10        .0 10        .0 10        .0ImageNet-10 pre-trained VGG19 (Low voltage, 0.5% BER) (d) Base Model: VGG19, 0.5% B.E.R.
Figure 14: Results of Reduced-precision and bit errors (0.5%) on ImageNet-10 pre-trained base
models.
I Additional Experiments on Adversarial Training
Adversarial training is a common strategy to derive a robust neural network against certain pertur-
bations. By training the generator using adversarial training proposed in Stutz et al. [2021], we
report its performance against low voltage-induced bit errors. We use ConvL as the generator and
ResNet18 as the base model, trained on CIFAR-10. Furthermore, we explore different Kflip bits as
the perturbation on weights of the base model during adversarial training, and then for evaluation,
the trained-generator will be applied against 1% of bit errors rate on the base model. The results are
shown in Table 25. After careful tuning of hyperparameters, we find that we are not able to obtain
satisfactory recovery when adopting adversarial training. Empirically, we argue that adversarial
training may not be suitable for training generator-based methods.
JAdditional Experiments on Robust Model Trained with Adversarial Weight
Perturbation with NeuralFuse
Previously, Wu et al. [2020] proposed that one could obtain a more robust model via adversarial
weight perturbation. To seek whether such models could also be robust to random bit errors, we
28Table 25: Performance of the generator trained by adversarial training under K flip bits on ResNet18
with CIFAR-10. The results show that the generator trained by adversarial training cannot achieve
high accuracy against bit errors under a 1%bit error rate.
K-bits CA PA CA (NF) PA (NF) RP
100
92.6 38.9 Â±12.492.4 38.3 Â±12.1 -0.6
500 92.1 38.7 Â±12.5 -0.2
5,000 92.6 38.9 Â±12.5 0
20,000 60.1 23.0 Â±8.1 -16
100,000 71.1 23.6 Â±6.6 -16
Note . CA ( %): clean accuracy; PA ( %): perturbed accuracy; NF: Neural-
Fuse; and RP: total recovery percentage of PA (NF) vs. PA
conducted an experiment on CIFAR-10 with the proposed adversarially trained PreAct ResNet18. The
experimental results are shown in Table 26. We find that the average perturbed accuracy is 23% and
63.2% for PreAct ResNet18 under 1% and 0.5% BER, respectively. This result is lower than 38.9%
and 70.1% from ResNet18 in Table 12, indicating their poor generalization ability against random bit
errors. Nevertheless, when equipped NeuralFuse on the perturbed model, we could still witness a
significant recover percentage under both 1% and 0.5% BER This result further demonstrates that
NeuralFuse could be adapted to various models (i.e., trained in different learning algorithms).
Table 26: Performance of NeuralFuse trained with rubust CIFAR-10 pre-trained PreAct
ResNet18. The results show that NeuralFuse can be used together with a robust model and
further improve perturbed accuracy under both 1% and 0.5% BER
Base Model BER NF CA PA CA (NF) PA (NF) RP
PreAct
ResNet181%ConvL
89.7 23.0 Â±9.387.6 53.7 Â±26 30.7
ConvS 83.1 34.6 Â±15 11.6
DeConvL 87.7 55.4 Â±27 32.4
DeConvS 82.9 32.4 Â±14 9.4
UNetL 86.1 60.4 Â±28 37.4
UNetS 80.4 51.9 Â±24 28.9
0.5%ConvL
89.7 63.2 Â±8.789.2 87.8 Â±1.1 24.6
ConvS 89.2 74.0 Â±6.5 10.8
DeConvL 89.0 87.4 Â±1.1 24.2
DeConvS 89.9 74.4 Â±7.0 11.2
UNetL 87.5 85.9 Â±0.8 22.7
UNetS 88.2 80.4 Â±3.9 17.2
Note. BER: the bit-error rate of the base model; CA ( %): clean accuracy; PA ( %): perturbed accuracy;
NF: NeuralFuse; and RP: total recovery percentage of PA (NF) vs. PA
K Data Embeddings Visualization
To further understand how our proposed NeuralFuse works, we visualize the output distribution
from the final linear layer of the base models and project the results onto the 2D space using t-SNE
[van der Maaten and Hinton, 2008]. Figure 15 shows the output distribution from ResNet18 (trained
on CIFAR-10) under a 1% bit error rate. We chose two generators that have similar architecture:
ConvL and ConvS, for this experiment. We can observe that: (a) The output distribution of the
clean model without NeuralFuse can be grouped into 10 classes denoted by different colors. (b) The
output distribution of the perturbed model under a 1% bit error rate without NeuralFuse shows mixed
representations and therefore degraded accuracy. (c) The output distribution of the clean model with
ConvL shows that applying NeuralFuse will not hurt the prediction of the clean model too much (i.e.,
it retains high accuracy in the regular voltage setting). (d) The output distribution of the perturbed
model with ConvL shows high separability (and therefore high perturbed accuracy) as opposed to (b).
(e)/(f) shows the output distribution of the clean/perturbed model with ConvS. For both (e) and (f),
29we can see nosier clustering when compared to (c) and (d), which means the degraded performance
of ConvS compared to ConvL. The visualization validates that NeuralFuse can help retain good
data representations under random bit errors and that larger generators in NeuralFuse have better
performance than smaller ones.
(a)
 (b)
 (c)
(d)
 (e)
 (f)
Figure 15: t-SNE results for ResNet18 trained by CIFAR-10 under 1%of bit error rate. (a) Clean
model. (b) Perturbed model. (c) Clean model with ConvL. (d) Perturbed model with ConvL. (e)
Clean model with ConvS. (f) Perturbed model with ConvS.
L Qualitative Analysis of Transformed Inputs
In this section, we conduct a qualitative study to visualize the images transformed by NeuralFuse and
present some properties and observations of these images. We utilize six different architectures of
NeuralFuse generators trained with ResNet18 under a 1%bit error rate.
Figure 16 (a) showcases several images from the truck class in CIFAR-10. Notably, images of the
same class, when transformed by the same NeuralFuse, exhibit similar patterns, such as circles
symbolizing the wheels of the trucks.
In Figures 16 (b) and 16 (c), we observe analogous phenomena in the GTSRB and CIFAR-100
datasets. Transformed images of the same class using the same generator consistently display patterns.
On GTSRB, NeuralFuse-generated patterns highlight the signâ€™s shape with a green background, even
if the original images have a dark background and are under different lighting conditions. These
results further underscore the efficacy and efficiency of NeuralFuse.
Figure 17 presents more images from different classes in (a) CIFAR-10, (b) GTSRB, and (c) CIFAR-
100. The transformed images exhibit distinct patterns for each class, suggesting that NeuralFuse
effectively transforms images into class-specific patterns, making associated features robust to random
bit errors and easily recognizable by the base model in low-voltage settings.
30Clean
ConvL
ConvS
DeConvL
DeConvS
UNetL
UNetS
(a)Truck class in CIFAR-10.
 (b)No Passing sign in GTSRB.
 (c)Apple class in CIFAR-100.
Figure 16: Visualization of transformed images from different NeuralFuse generators trained with
ResNet18 at 1%bit error rate.
Clean
ConvL
ConvS
DeConvL
DeConvS
UNetL
UNetS
(a) 10 classes sampled from CIFAR-10
 (b) 10 traffic signs sampled from GTSRB
Clean
ConvL
ConvS
DeConvL
DeConvS
UNetL
UNetS
(c) 20 classes sampled from CIFAR-100
Figure 17: Visualization of transformed images from different NeuralFuse generators trained by
ResNet18 with 1%bit error rate.
31NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: We list our main contributions in the Abstract and Introduction.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discussed the limitation of our work on runtime/latency in Section 4.4.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
32Justification: This paper does not include theoretical results.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The implementation details are presented in Appendices A and B.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
33Answer: [Yes]
Justification: Our code can be found at https://github.com/IBM/NeuralFuse .
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The experimental details are presented in Section 4.1, Appendix and our code.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: For each experimental result, we report the mean and standard deviation and
plot the error bars.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
34â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The computing resources are presented in Section 4.1.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The work stated in this paper conform with the NeurIPS Code of Ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discussed broader impacts in Section 5.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
35â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: This paper poses no such risks.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have cited the original paper that produced the code package or dataset.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
36â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [Yes]
Justification: We described how to run our code in the README.md .
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
37