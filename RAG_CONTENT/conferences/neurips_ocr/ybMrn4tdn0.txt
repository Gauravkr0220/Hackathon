Auditing Local Explanations is Hard
Robi Bhattacharjee
University of TÃ¼bingen and TÃ¼bingen AI Center
robi.bhattacharjee@wsii.uni-tuebingen.de
Ulrike von Luxburg
University of TÃ¼bingen and TÃ¼bingen AI Center
ulrike.luxburg@uni-tuebingen.de
Abstract
In sensitive contexts, providers of machine learning algorithms are increasingly
required to give explanations for their algorithmsâ€™ decisions. However, explanation
receivers might not trust the provider, who potentially could output misleading or
manipulated explanations. In this work, we investigate an auditing framework in
which a third-party auditor or a collective of users attempts to sanity-check expla-
nations: they can query model decisions and the corresponding local explanations,
pool all the information received, and then check for basic consistency properties.
We prove upper and lower bounds on the amount of queries that are needed for
an auditor to succeed within this framework. Our results show that successful
auditing requires a potentially exorbitant number of queries â€“ particularly in high
dimensional cases. Our analysis also reveals that a key property is the â€œlocalityâ€ of
the provided explanations â€” a quantity that so far has not been paid much attention
to in the explainability literature. Looking forward, our results suggest that for
complex high-dimensional settings, merely providing a pointwise prediction and
explanation could be insufficient, as there is no way for the users to verify that the
provided explanations are not completely made-up.
1 Introduction
Machine learning models are increasingly used to support decision making in sensitive contexts
such as credit lending, hiring decisions, admittance to social benefits, crime prevention, and so on.
In all these cases, it would be highly desirable for the customers/applicants/suspects to be able to
judge whether the modelâ€™s predictions or decisions are â€œtrustworthyâ€. New AI regulation such as
the European Unionâ€™s AI Act can even legally require this. One approach that is often held up
as a potential way to achieve transparency and trust is to provide local explanations , where every
prediction/decision comes with a human-understandable explanation for this particular outcome (e.g.,
LIME (Ribeiro et al., 2016), SHAP (Lundberg and Lee, 2017), or Anchors (Ribeiro et al., 2018)).
However, in many real-world scenarios, the explanation receivers may not necessarily trust the
explanation providers (Bordt et al., 2022). Imagine a company that uses machine learning tools to
assist in screening job applications. Because the company is well-advised to demonstrate fair and
equitable hiring, it is plausible that it might bias its explanations towards depicting these properties.
And this is easy to achieve: the company is under full control of the machine learning model and the
setup of the explanation algorithm, and prior literature (Ghorbani et al., 2019; Dombrowski et al.,
2019) has shown that current explainability tools can be manipulated to output desirable explanations.
This motivates the question: what restrictions or procedures could be applied to prevent such
explanation cheating, and more specifically, what are ways to verify that the provided explanations
38th Conference on Neural Information Processing Systems (NeurIPS 2024).(a) Insufficient data for auditing
 (b) Sufficient data for auditing
Figure 1: Local explanations (see Section 2.2 for notation): In both panels, a set of training points x
and their classifications f(x)(red/blue, decision boundary in green) are shown. For three training
points (one centered at each ball), a local linear explanation (gx, Rx)is illustrated where gxis a local
linear classifier (black decision boundary) and Rxis a local ball centered at x.Panel (a) depicts a
regime where there is insufficient data for verifying how accurate the local explanations approximate
the classifier fâ€“ none of the provided regions contain enough points to assess the accuracy of the
linear explanations. Panel (b) depicts a regime with more training points allowing us to validate
the accuracy of the linear explanations based on how closely they align with the points in their
corresponding regions.
are actually trustworthy? One approach is to require that the explanation providers completely
publicize their models, thus allowing users or third-party regulators to verify that the provided
explanations are faithful to the actual model being used. However, such a requirement would likely
face stiff resistance in settings where machine learning models are valuable intellectual property.
In this work, we investigate an alternative approach, where a third-party regulator or a collective of
users attempt to verify the trustworthiness of local explanations, simply based on the predictions and
explanations over a set of examples. The main idea is that by comparing the local explanations with
the actual predictions across enough data one could, in principle, give an assessment on whether the
provided explanations actually adhere to the explained model. The goal of our work is to precisely
understand when this is possible.
1.1 Our contributions: data requirements for auditing.
We begin by providing a general definition for local explainability that encompasses many popular
explainability methods such as Anchors (Ribeiro et al., 2018), Smooth-grad (Smilkov et al., 2017),
and LIME (Ribeiro et al., 2016). We define a local explanation for a classifier fat a point xas a pair
(Rx, gx), where Rxis a local region surrounding x, and gxis a simple local classifier designed to
approximate foverRx. For example, on continuous data, Anchors always output (Rx, gx)where
Rxis a hyper-rectangle around xandgxis a constant classifier; gradient-based explanations such as
Smooth-grad or LIME implicitly approximate the decision function fby a linear function in a local
region around x.
Obviously, any human-accessible explanation that is being derived from such a local approximation
can only be trustworthy if the local function gxindeed approximates the underlying function fon the
local region Rx. Hence, a necessary condition for a local explanation to be trustworthy is that the
function gxis close to fon the region Rx, and this should be the case for most data points xsampled
from the underlying distribution.
To measure how closely a set of local explanations adheres to the original classifier f, we propose an
explainability loss function LÎ³(E, f), which quantifies the frequency with which fdiffers by more
2thanÎ³from the local classifier gxover the local region Rx(see Sec. 2.2 for precise definitions). We
then introduce a formalism for auditing local explanations where an auditor attempts to estimate
the explainability loss LÎ³(E, f). In our formalism, the auditor does so with access to the following
objects:
1. A set of data points X={x1, . . . , x n}drawn i.i.d from the underlying data distribution.
2. The outputs of a classifier on these points, f(X) ={f(x1), . . . , f (xn)}.
3. The provided local explanations for these points E(f, X) ={E(f, x1), . . . , E (f, xn)}
Observe that in our formalism, the auditor has only restricted access to the machine learning model
and the explanations: they can only interact with them through their evaluations at specific data-points.
We have chosen this scenario because we believe it to be the most realistic one in many practical
situations, where explanation providers try to disclose as little information on their underlying
machine learning framework as possible.
In our main result, Theorem 4.1, we provide a lower bound for the amount of data needed for an
auditor to accurately estimate LÎ³(E, f). A key quantity in our analysis is the locality of the provided
explanations. We show that the smaller the provided local regions Rxare, the more difficult it
becomes to audit the explainer. Intuitively, this holds because estimating the explainability loss relies
on observing multiple points within these regions, as illustrated in Panel (b) of Figure 1. By contrast,
if this fails to hold (Panel (a)), then there is no way to validate how accurate the local explanations
are. We also complement our lower bound with an upper bound (Theorem 4.2) that demonstrates that
reasonably large local regions enable auditing within our framework.
Our results imply that the main obstacle to auditing local explanations in this framework is the
locality of the provided explanations. As it turns out, this quantity is often prohibitively small in
practice , making auditing practically impossible . In particular, for high-dimensional applications, the
local regions Rxgiven by the explainer are often exponentially small in the data-dimension. Thus
the explanations cannot be verified in cases where there does not exist any prior trust between the
explanation provider and the explanation receivers.
We stress that estimating the local loss LÎ³(E, f)serves as a first baseline on the path towards
establishing trustworthy explanations. It is very well possible that an explanation provider achieves
a small local loss (meaning that the local classifiers closely match the global classifier f) but
nevertheless provides explanations that are misleading in some other targeted manner. Thus, we
view successful auditing in this setting as a necessary but not sufficient condition for trusting an
explanation provider.
Our results might have far-reaching practical consequences. In cases where explanations are con-
sidered important or might even be required by law, for example by the AI Act, it is a necessary
requirement that explanations can be verified or audited (otherwise, they would be completely use-
less). Our results suggest that in the typical high dimensional setting of modern machine learning,
auditing pointwise explanations is impossible if the auditor only has access to pointwise decisions
and corresponding explanations. In particular, collectives of users, for example coordinated by
non-governmental organizations (NGOs), are never in the position to audit explanations. The only
way forward in auditing explanations would be to appoint a third-party auditor who has more power
andmore access to the machine learning model , be it access to the full specification of the model
function and its parameters, or even to the training data. Such access could potentially break the
fundamental issues posed by small local explainability regions in our restricted framework, and could
potentially enable the third party auditor to act as a moderator to establish trust between explanation
receivers and explanation providers.
1.2 Related Work
Prior work (Yadav et al., 2022; Bhatt et al., 2020; Oala et al., 2020; Poland, 2022) on auditing machine
learning models is often focused on applying explainability methods to audit the models, rather
than the explanations themselves. However, there has also been recent work (Leavitt and Morcos,
2020; Zhou et al., 2021) arguing for more rigorous ways to evaluate the performance of various
explanation methods. There are numerous approaches for doing so: including performance based on
human-evaluation (Jesus et al., 2021; Poursabzi-Sangdeh et al., 2021), and robustness (Alvarez-Melis
and Jaakkola, 2018).
3There has also been a body of work that evaluates explanations based on the general notion of
faithfulness between explanations and the explained predictor. Many approaches (Wolf et al., 2019;
Poppi et al., 2021; Tomsett et al., 2020) examine neural-network specific measures, and typically rely
on access to the neural network that would not be present in our setting. Others are often specialized
to a specific explainability tool â€“ with LIME (Visani et al., 2022; Botari et al., 2020) and Shap (Huang
and Marques-Silva, 2023) being especially popular choices.
By contrast, our work considers a general form of local explanation, and studies the problem of
auditing such explanations in a restricted access setting, where the auditor only interacts with
explanations through queries. To our knowledge, the only previous work in a similar setting is
(Dasgupta et al., 2022), in which local explanations are similarly audited based on collecting them on
a set of data sampled from a data distribution. They consider a quantity called the local sufficiency ,
which directly corresponds to our notion of local loss (Definition 2.3). However, their work is
restricted to a discrete setting where local fidelity is evaluated based on instances that receive
identical explanations. In particular, they attempt to verify that points receiving identical explanations
also receive identical predictions. By contrast, our work lies within a continuous setting, where a
local explanation is said to be faithful if it matches the underlying model over a local region .
A central quantity to our analysis is the locality of an explanation, which is a measure of how large
the local regions are. Prior work has rarely measured or considered this quantity, with a notable
exception being Anchors method (Ribeiro et al., 2018) which utilizes it to assist in optimizing their
constructed explanations. However, that work did not explore this quantity beyond treating it as a
fixed parameter.
Finally, we note that other recent work, such as (Bassan and Katz, 2023), provides avenues for
providing explanations with certifiable correctness , meaning that they provide proof that their
accurate reflect the underlying model. We view our work as complementary to such methods as our
work demonstrates the necessity of such ideas by demonstrating difficulties with using generic local
explanation methods .
2 Local Explanations
2.1 Preliminaries
In this work, we restrict our focus to binary classification â€“ we let Âµdenote a data distribution over
Rd, and f:Rdâ†’ {Â± 1}be a so-called black-box binary classifier that needs to be explained. We
note that lower bounds shown for binary classification directly imply lower bounds in more complex
settings such as multi-class classification or regression.
For any measurable set, MâŠ†Rd, we let Âµ(M)denote the probability mass Âµassigns M.
We will also let supp(Âµ)denote the support ofÂµ, which is the set of all points xsuch that
Âµ({xâ€²:||xâˆ’xâ€²|| â‰¤r})>0for all r >0.
We define a hyper-rectangle inRdas a product of intervals, (a1, b1]Ã— Â·Â·Â· Ã— (ad, bd], and let Hd
denote the set of all hyper-rectangles in Rd. We let Bddenote the set of all L2-balls in Rd, with the
ball of radius rcentered at point xbeing defined as B(x, r) ={xâ€²:||xâˆ’xâ€²|| â‰¤r}.
We will utilize the following two simple hypothesis classes: Cd, which is the set of the two constant
classifiers over Rd, andLd, which is the set of all linear classifiers over Rd. These classes serve as
important examples of simple and interpretable classifiers for constructing local explanations.
2.2 Defining local explanations and explainers
One of the most basic and fundamental concepts in Explainable Machine Learning is the notion of a
local explanation , which, broadly speaking, is an attempt to explain a complex functionâ€™s behavior
at a specific point. In this section, we describe a general form that such explanations can take, and
subsequently demonstrate that two widely used explainability methods, LIME and Anchors, adhere
to it.
We begin by defining a local explanation for a classifier at a given point.
Definition 2.1. ForxâˆˆRd, and f:Rdâ†’ {Â± 1}, alocal explanation forfatxis a pair (Rx, gx)
where RxâŠ†Rdis a region containing x, and gx:Rxâ†’ {Â± 1}is a classifier.
4Here, gxis typically a simple function intended to approximate the behavior of a complex function,
f, over the region Rx. The idea is that the local nature of Rxsimplifies the behavior of fenough to
provide intuitive explanations of the classifierâ€™s local behavior.
Next, we define a local explainer as a map that outputs local explanations.
Definition 2.2. Eis alocal explainer if for any f:Rdâ†’ {Â± 1}and any xâˆˆRd,E(f, x)is a local
explanation for fatx. We denote this as E(f, x) = (Rx, gx).
We categorize local explainers based on the types of explanations they output â€“ if Rdenotes a set of
regions in Rd, andGdenotes a class of classifiers, Rdâ†’ {Â± 1}, then we say Eâˆˆ E(R,G)if for all
f, x,E(f, x)outputs (Rx, gx)withRxâˆˆ R andgxâˆˆ G.
Local explainers are typically constructed for a given classifier fover a given data distribution Âµ.
In practice, different algorithms employ varying amounts of access to both fandÂµâ€“ for example,
SHAP crucially relies on data sampled from Âµwhereas gradient based methods often rely on knowing
the actual parameters of the model, f. To address all of these situations, our work takes a black-box
approach in which we make no assumptions about how a local explainer is constructed from fandÂµ.
Instead we focus on understanding how to evaluate how effective a given explainer is with respect to
a classifier fand a data distribution Âµ.
2.3 Examples of Explainers
We now briefly discuss how various explainability tools in practice fit into our framework of local
explanations.
Anchors: The main idea of Anchors (Ribeiro et al., 2018) is to construct a region the input point in
which the desired classifier to explain remains (mostly) constant. Over continuous data, it outputs a
local explainer, E, such that E(x) = (Rx, gx), where gxis a constant classifier with gx(xâ€²) =f(x)
for all xâ€²âˆˆRd, and Rxis a hyper-rectangle containing x. It follows say that the Anchors method
outputs an explainer in the class, E(Hd,Cd).
Gradient-Based Explanations: Many popular explainability tools (Smilkov et al., 2017; Agarwal
et al., 2021; Ancona et al., 2018) explain a modelâ€™s local behavior by using its gradient. By definition,
gradients have a natural interpretation as a locally linear model. Because of this, we argue that
gradient-based explanations are implicitly giving local explanations of the form (Rx, gx), where
Rx=B(x, r)is a small L2ball centered at x, andgxis a linear classifier with coefficients based on
the gradient. Therefore, while the radius rand the gradient gxbeing used will vary across explanation
methods, the output can be nevertheless interpreted as an explainer in E(Bd,Ld), where Bddenotes
the set of all L2-balls in Rd, andLddenotes the set of all linear classifiers over Rd.
LIME: At a high level, LIME (Ribeiro et al., 2016) also attempts to give local linear approximations
to a complex model. However, unlike gradient-based methods, LIME includes an additional feature-
wise discretization step where points nearby the input point, x, are mapped into a binary representation
in{0,1}dbased on how similar a point is to x. As a consequence, LIME can be construed as
outputting local explanations of a similar form to those outputted by gradient-based methods.
Finally, as an important limitation of our work, although many well-known local explanations fall
within our definitions, this does not hold in all cases. Notably, Shapley-value (Lundberg and Lee,
2017) based techniques do not conform to the format given in Definition 2.1, as it is neither clear
how to construct local regions that they correspond to, nor the precise local classifier being used.
2.4 A measure of how accurate an explainer is
We now formalize what it means for a local classifier, gx, to â€œapproximate" the behavior of finRx.
Definition 2.3. For explainer Eand point x, we let the local loss ,L(E, f, x )be defined as the
fraction of examples drawn from the region Rxsuch that gxandfhave different outputs. More
precisely, we set
L(E, f, x ) = Pr
xâ€²âˆ¼Âµ[gx(xâ€²)Ì¸=f(x)|xâ€²âˆˆRx].
5Âµis implicitly used to evaluate E, and is omitted from the notation for brevity. We emphasize that
this definition is specific to classification , which is the setting of this work. A similar kind of loss can
be constructed for regression tasks based on the mean-squared difference between gxandf.
We contend that maintaining a low local loss across most data points is essential for any reasonable
local explainer. Otherwise, the explanations provided by the tool can be made to support any sort of
explanation as they no longer have any adherence to the original function f.
To measure the overall performance of an explainer over an entire data distribution, it becomes
necessary to aggregate L(E, f, x )over all xâˆ¼Âµ. One plausible way to accomplish this would be to
average L(E, f, x )over the entire distribution. However, this would leave us unable to distinguish
between cases where Egives extremely poor explanations at a small fraction of points as opposed
to giving mediocre explanations over a much larger fraction. To remedy this, we opt for a more
precise approach in which a user first chooses a local error threshold ,0< Î³ < 1, such that local
explanations that incur an explainabiliy loss under Î³are considered acceptable. They then measure the
global loss for Eby determining the fraction of examples, x, drawn from Âµthat incur explainability
loss above Î³.
Definition 2.4. LetÎ³ >0be a user-specified local error threshold. For local explainer E, we define
theexplainability loss LÎ³(E, f)as the fraction of examples drawn from Âµthat incur a local loss
larger than Î³. That is,
LÎ³(E, f) = Pr
xâˆ¼Âµ[L(E, f, x )â‰¥Î³].
We posit that the quantity LÎ³(E, f)serves as an overall measure of how faithfully explainer Eadheres
to classifier f, with lower values of LÎ³(E, f)corresponding to greater degrees of faithfulness.
2.5 A measure of how large local regions are
The outputted local region Rxplays a crucial role in defining the local loss. On one extreme, setting
Rxto consist of a single point, {x}, can lead to a perfect loss of 0, as the explainer only needs to
output a constant classifier that matches fatx. But these explanations would be obviously worthless
as they provide no insight into fbeyond its output f(x). On the other extreme, setting Rx=Rd
would require the explainer to essentially replace fin its entirety with gx, which would defeat the
purpose of explaining f(as we could simply use gxinstead). Motivated by this observation, we
define the local mass of an explainer at a point xas follows:
Definition 2.5. The local mass of explainer Ewith respect to point xand function f, denoted
Î›(E, f, x ), is the probability mass of the local region outputted at x. That is, if E(f, x) = (Rx, gx),
then
Î›(E, f, x ) = Pr
xâ€²âˆ¼Âµ[xâ€²âˆˆRx].
Based on our discussion above, it is unclear what an ideal local mass is. Thus, we treat this quantity
as a property of local explanations rather than a metric for evaluating their validity. As we will later
see, this property is quite useful for characterizing how difficult it is to estimate the explainability
loss of an explainer. We also give a global characterization of the local mass called locality .
Definition 2.6. The locality of explainer Ewith respect to function f, denoted Î›(E, f), is the
minimum local mass it incurs. That is, Î›(E, f) = inf xâˆˆsupp(Âµ)Î›(E, f, x ).
3 The Auditing Framework
Recall that our goal is to determine how explanation receivers can verify provided explanations in
situations where there isnâ€™t mutual trust. To this end, we provide a framework for auditing local
explanations , where an auditor attempts to perform this verification with as little access to the
underlying model and explanations as possible. Our framework proceeds in with the following steps.
1. The auditor fixes a local error threshold Î³.
2. A set of points X={x1, . . . , x n}are sampled i.i.d from data distribution Âµ.
3.A black-box classifier fis applied to these points. We denote these values with f(X) =
{f(x1), . . . , f (xn)}.
64.A local explainer Eoutputs explanations for fat each point. We denote these explanations
withE(f, X) ={E(f, x1), . . . , E (f, xn)}.
5. The Auditor outputs an estimate A(X, f(X), E(f, X))for the explainability loss.
Observe that the auditor can only have access to the the model fand its corresponding explanations
through the set of sampled points. Its only inputs are X,f(X), and E(f, X). In the context of the
job application example discussed in Section 1, this would amount to auditing a company based on
the decisions and explanations they provided over a set of applicants.
In this framework, we can define the sample complexity of an auditor as the amount of data it needs
to guarantee an accurate estimate for LÎ³(E, f). More precisely, fix a data distribution, Âµ, a classifier,
f, and an explainer E. Then we have the following:
Definition 3.1. For tolerance parameters, Ïµ1, Ïµ2, Î´ > 0, and local error threshold, Î³ > 0, we
say that an auditor, A, has sample complexity N(Ïµ1, Ïµ2, Î´, Î³)with respect to Âµ, E, f , if for any
nâ‰¥N(Ïµ1, Ïµ2, Î´, Î³), with probability at least 1âˆ’Î´overX={x1, . . . , x n} âˆ¼Âµn,Aoutputs an
accurate estimate of the explainability loss, LÎ³(E, f). That is,
LÎ³(1+Ïµ1)(E, f)âˆ’Ïµ2â‰¤A(X, f(X), E(f, X))â‰¤LÎ³(1âˆ’Ïµ1)(E, f) +Ïµ2.
Next, observe that our sample complexity is specific to the distribution, Âµ, the classifier, f, and the
explainer, E. We made this choice to understand the challenges that different choices of Âµ,f, and
Epose to an auditor. As we will later see, we will bound the auditing sample complexity using the
locality (Definition 2.5), which is a quantity that depends on Âµ,f, and E.
4 How much data is needed to audit an explainer?
4.1 A lower bound on the sample complexity of auditing
We now give a lower bound on the amount of data needed to successfully audit an explainer. That is,
we show that for any auditor Aand any data distribution Âµwe can find some explainer Eand some
classifier fsuch that Ais highly likely to give an inaccurate estimate of the explainability loss. To
state our theorem we use the following notation and assumptions. Recall that Hddenotes the set of
hyper-rectangles in Rd, and that Cddenotes the set of the two constant binary classifiers over Rd.
Additionally, we will include a couple of mild technical assumptions about the data distribution Âµ.
We defer a detailed discussion of them to Appendices A.3 and A.1. We now state our lower bound.
Theorem 4.1 (lower bound on the sample complexity of auditing ).LetÏµ1, Ïµ2<1
48be tolerance
parameters, and let Î³ <1
3be any local error threshold. Let Âµbe any non-degenerate distribution, and
Î» >0be any desired level of locality. Then for any auditor Athere exists a classifier f:Rdâ†’ {Â± 1}
and an explainer Eâˆˆ E(Hd,Cd)such that the following conditions hold.
1.Ehas locality Î›(E, f) =Î».
2. There exist absolute constants c0, c1>0such that if the auditor receives
nâ‰¤c0
max( Ïµ1, Ïµ2)Î»1âˆ’c1max( Ïµ1,Ïµ2)
many points, then with probability at least1
3overX={x1, . . . , x n} âˆ¼Âµn,Agives an
inaccurate estimate of LÎ³(E, f). That is,
A(X, f(X), E(f, X))/âˆˆ[LÎ³(1+Ïµ1)(E, f)âˆ’Ïµ2, LÎ³(1âˆ’Ïµ1)(E, f) +Ïµ2].
In summary, Theorem 4.1 says that auditing an explainer requires an amount of data that is inversely
proportional to its locality. Notably, this result does not require the data-distribution to be adversarially
chosen, and furthermore applies when the explainer Ecan be guaranteed to have a remarkably simple
form being in E(Hd,Cd).
Proof intuition of Theorem 4.1: The main intuition behind Theorem 4.1 is that estimating the
local explainability loss, L(E, f, x ), requires us to observe samples from the regions Rx. This would
allow us to obtain an empirical estimate of L(E, f, x )by simply evaluating the fraction of points
7fromRxthat the local classifier, gx, misclassifies. This implies that the locality Î»is a limiting factor
as it controls how likely we are to observe data within a region Rx.
However, this idea enough isnâ€™t sufficient to obtain our lower bound. Although the quantity
â„¦ 1
Î»1âˆ’O(Ïµ)
does indeed serve as a lower bound on the amount of data needed to guarantee seeing a
large number of points within a region, Rx, it is unclear what a sufficient number of observations
within Rxis. Even if we donâ€™t have enough points in any single region, Rx, to accurately estimate
L(E, f, x ), it is entirely plausible that aggregating loose estimates of L(E, f, x )over a sufficient
number of points xmight allow us to perform some type of estimation of LÎ³(E, f).
To circumvent this issue, the key technical challenge is constructing a distribution of functions f
and fixing m=O 1
Ïµ
such that observing fewer than mpoints from a given region, Rx, actually
provides zero information about which function was chosen. We include a full proof in Appendix A.
4.2 An upper bound on the sample complexity of auditing.
We now show that if Î»is reasonably large, then auditing the explainability loss LÎ³(E, f)can be
accomplished. As mentioned earlier, we stress that succeeding in our setting is nota sufficient
condition for trusting an explainer â€“ verifying that the local explanations gxmatch the overall
function fis just one property that a good explainer would be expected to have. Thus the purpose
of our upper bound in this section is to complement our lower bound, and further support that the
locality parameter Î»is the main factor controlling the sample complexity of an auditor.
Our auditing algorithm proceeds by splitting the data into two parts, X1andX2. The main idea is to
audit the explanations given for points in X1by utilizing the data from X2. If we have enough data,
then it is highly likely for us to see enough points in each local region to do this. We defer full details
for this procedure to Appendix B.1. We now give the an upper bound on its sample complexity.
Theorem 4.2 (Upper Bound on Sample Complexity of Algorithm 1 ).There exists an auditor, A,
for which the following holds. Let Âµbe a data distribution, fbe a classifier, and Ebe an explainer.
Suppose that Ehas locality Î»with respect to Âµandf. LetÏµ1, Ïµ2, Î´be tolerance parameters and let
Î³ >0be a local error threshold. Then Ahas sample complexity at most
N(Ïµ1, Ïµ2, Î´, Î³) =ËœO1
Ïµ2
2+1
Î»Î³2Ïµ2
1
.
This bound shows that the locality is sufficient for bounding the sample complexity for auditing local
explanations. We defer a full proof to Appendix B. Observe that the dependency on Î»isO(1
Î»)which
matches the dependency in our lower bound provided that Ïµ1, Ïµ2â†’0.
5 The locality of practical explainability methods can be extremely small
Theorems 4.1 and 4.2 demonstrate that the locality Î»characterizes the amount of data needed for an
Auditor to guarantee an accurate estimate of the explainability loss LÎ»(E, f). It follows that if Î»is
extremely small, then auditing could require a prohibitive amount of data. This leads to the following
question: how small is Î»for practical explainability algorithms? To answer this, we will examine
examine several commonly used algorithms that adhere to our framework.
We begin with gradient-based methods , which can be construed as providing an explainer in
the class E(Bd,Ld), where Bddenotes the set of L2balls in Rd, andLddenotes the set of linear
classifiers. To understand the impact of dimension on the locality of such explainers, we begin with a
simple theoretical example.
LetÂµbe the data distribution over Rdthat is a union of three concentric spheres. Specifically, xâˆ¼Âµ
is equally likely to be chosen at uniform from the sets S1={x:||x||= 1âˆ’Î±},S2={x:||x||= 1},
andS3={x:||x||= 1 + Î²}, where Î±, Î² are small d-dependent constants (Defined in Appendix C).
Letf:Rdâ†’ {Â± 1}be any classifier such that f(x) = 1 ifxâˆˆS1âˆªS3andf(x) =âˆ’1ifxâˆˆS2.
Observe that Âµis a particularly simple data distribution over three spherical manifolds, and fis a
simple classifier that distinguishes its two parts. We illustrate this distribution in panel (a) of Figure 2.
Despite its simplicity, locally explaining fwith linear classifiers faces fundamental challenges. We
illustrate this in Figure 2. Choosing a large local neighborhood, as done at point A, leads to issues
8Figure 2: An illustration of Theorem 5.1, with the concentric blue and red circles depicting the
data distribution Âµclassified by f, and with local explanations being depicted at points A and B.
Explanations are forced to either have large local loss (point A) or a low local mass (point B).
posed by the curvature of the data distribution, meaning that it is impossible to create an accurate
local linear classifier. On the other hand, choosing a neighborhood small enough for local linearity, as
done in point B, leads to local regions that are exponentially small with respect to the data dimension .
We formalize this in the following theorem.
Theorem 5.1 (A high dimensional example ).LetÂµ, f, be as described above, and let Ebe any
explainer in E(Bd,Ld). Let xâˆ—be any point chosen on the outer sphere, S3. Then Eoutputs an
explanation at xâˆ—that either has a large local loss, or that has a small local mass. That is, either
L(E, f, xâˆ—)â‰¥1
6, orÎ›(E, f, x )â‰¤31âˆ’d.
Theorem 5.1 demonstrates that if a locally linear explanation achieves even a remotely reasonable
local loss, then it necessarily must have an extremely small local explanation. This suggests that,
gradient based explanations will be exponentially local with respect to the data dimension, d.
We believe that this is also exhibited in practice particularly over image data , where explanations are
often verified based on perceptual validity, rather than relevance to practical training points beyond
the point being explained. For example, the explanations given by SmoothGrad (Smilkov et al., 2017)
are visualized as pixel by pixel saliency maps. These maps often directly correspond to the image
being explained, and are clearly highly specific to the it (see e.g. Figure 3 of (Smilkov et al., 2017)).
As a result, we would hardly expect the implied linear classifier to have much success over almost
any other natural image. This in turn suggest that the locality would be extremely small. We also
remark that a similar argument can be made for Lime , which also tends to validate its explanations
over images perceptually (for example, see Figure 4 of Ribeiro et al. (2016)).
Unlike the previous methods, Anchors (Ribeiro et al., 2018) explicitly seeks to maximize the local
mass of its explanations. However, it abandons this approach for image classifiers, where it instead
maximizes a modified form of locality based on super-imposing pixels from the desired image with
other images. While this gives perceptually valid anchors, the types of other images that fall within
the local region are completely unrealistic (as illustrated in Figure 3 of (Ribeiro et al., 2018)), and
the true locality parameter is consequently extremely small. Thus, although Anchors can provide
useful and auditable explanations in low-dimensional, tabular data setting, we believe that they too
suffer from issues with locality for high-dimensional data. In particular, we note that it is possible
to construct similar examples to Theorem 5.1 that are designed to force highly local Anchors-based
explanations.
6 Conclusion
Our results in Section 4 demonstrate that the locality of a local explainer characterizes how much
data is needed to audit it; smaller local regions lead to larger amounts of data. Meanwhile, our
9discussion in Section 5 shows that typical local explanations are extremely local in high-dimensional
space. It follows that in many cases, auditing solely based on point-wise decisions and explanations
is impossible. Thus, any entity without model access, such as a collective of users, are never in a
position to guarantee trust for a machine learning model.
We believe that the only way forward is through a more powerful third-party auditor that crucially as
more access to the machine learning model, as this could potentially break the fundamental challenges
posed by small explainability regions. We believe that investigating the precise types of access this
would entail as an important direction for future work that might have broad practical consequences.
Finally, although our definition of local explainers encompasses several widely used explanation
methods, we do note that there are notable exceptions such as Shap (Lundberg and Lee, 2017),
which does not fit into our paradigm. As a consequence, one important direction for future work is
expanding our framework to encompass other local explanation methods and examine to what degree
they can be audited.
Acknowledgements
This work has been supported by the German Research Foundation through the Cluster of Excellence
â€œMachine Learning - New Perspectives for Science" (EXC 2064/1 number 390727645) and the Carl
Zeiss Foundation through the CZS Center for AI and Law.
References
Agarwal, S., Jabbari, S., Agarwal, C., Upadhyay, S., Wu, S., and Lakkaraju, H. (2021). Towards
the unification and robustness of perturbation and gradient based explanations. In International
Conference on Machine Learning (ICML) .
Alvarez-Melis, D. and Jaakkola, T. S. (2018). On the robustness of interpretability methods. arxiv
preprint 1806.08049 .
Ancona, M., Ceolini, E., Ã–ztireli, C., and Gross, M. (2018). Towards better understanding of gradient-
based attribution methods for deep neural networks. In International Conference on Learning
Representations (ICLR) .
Bassan, S. and Katz, G. (2023). Towards formal XAI: formally approximate minimal explanations
of neural networks. In Sankaranarayanan, S. and Sharygina, N., editors, Tools and Algorithms
for the Construction and Analysis of Systems - 29th International Conference, TACAS 2023, Held
as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2022,
Paris, France, April 22-27, 2023, Proceedings, Part I , volume 13993 of Lecture Notes in Computer
Science , pages 187â€“207. Springer.
Bhatt, U., Xiang, A., Sharma, S., Weller, A., Taly, A., Jia, Y ., Ghosh, J., Puri, R., Moura, J. M. F., and
Eckersley, P. (2020). Explainable machine learning in deployment. In Conference on Fairness,
Accountability, and Transparency (FAccT) .
Bordt, S., Finck, M., Raidl, E., and von Luxburg, U. (2022). Post-hoc explanations fail to achieve
their purpose in adversarial contexts. In Conference on Fairness, Accountability, and Transparency
(FAccT) .
Botari, T., HvilshÃ¸j, F., Izbicki, R., and de Carvalho, A. C. P. L. F. (2020). Melime: Meaningful local
explanation for machine learning models. arxiv preprint 2009.05818 .
Dasgupta, S., Frost, N., and Moshkovitz, M. (2022). Framework for evaluating faithfulness of local
explanations. In International Conference on Machine Learning (ICML) .
Dombrowski, A., Alber, M., Anders, C. J., Ackermann, M., MÃ¼ller, K., and Kessel, P. (2019).
Explanations can be manipulated and geometry is to blame. In Advances in Neural Information
Processing Systems (NeurIPS) .
Ghorbani, A., Abid, A., and Zou, J. Y . (2019). Interpretation of neural networks is fragile. In AAAI
Conference on Artificial Intelligence .
10Huang, X. and Marques-Silva, J. (2023). The inadequacy of shapley values for explainability. arxiv
preprint 2302.08160 .
Jesus, S. M., BelÃ©m, C., Balayan, V ., Bento, J., Saleiro, P., Bizarro, P., and Gama, J. (2021). How
can I choose an explainer?: An application-grounded evaluation of post-hoc explanations. In â€™21:
2021 ACM Conference on Fairness, Accountability (FAccT) .
Leavitt, M. L. and Morcos, A. S. (2020). Towards falsifiable interpretability research. arxiv preprint
2010.12016 .
Lundberg, S. M. and Lee, S. (2017). A unified approach to interpreting model predictions. In
Advances in Neural Information Processing Systems (NeurIPS) .
Oala, L., Fehr, J., Gilli, L., Balachandran, P., Leite, A. W., RamÃ­rez, S. C., Li, D. X., Nobis, G.,
Alvarado, E. A. M., Jaramillo-Gutierrez, G., Matek, C., Shroff, A., Kherif, F., Sanguinetti, B., and
Wiegand, T. (2020). ML4H auditing: From paper to practice. In Machine Learning for Health
Workshop, (ML4H@NeurIPS) .
Poland, C. M. (2022). The right tool for the job: Open-source auditing tools in machine learning.
arxiv preprint 2206.10613 .
Poppi, S., Cornia, M., Baraldi, L., and Cucchiara, R. (2021). Revisiting the evaluation of class
activation mapping for explainability: A novel metric and experimental analysis. In Workshops of
the Conference on Computer Vision and Pattern Recognition Workshops (CVPR) .
Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Vaughan, J. W., and Wallach, H. M. (2021).
Manipulating and measuring model interpretability. In Conference on Human Factors in Computing
Systems (CHI) .
Ribeiro, M. T., Singh, S., and Guestrin, C. (2016). "why should I trust you?": Explaining the
predictions of any classifier. In Demonstrations Session, Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies .
Ribeiro, M. T., Singh, S., and Guestrin, C. (2018). Anchors: High-precision model-agnostic
explanations. In McIlraith, S. A. and Weinberger, K. Q., editors, AAAI Conference on Artificial
Intelligence .
Smilkov, D., Thorat, N., Kim, B., ViÃ©gas, F. B., and Wattenberg, M. (2017). Smoothgrad: removing
noise by adding noise. arxiv preprint 1706.03825 .
Tomsett, R., Harborne, D., Chakraborty, S., Gurram, P., and Preece, A. D. (2020). Sanity checks for
saliency metrics. In (AAAI) Conference on Artificial Intelligence .
Visani, G., Bagli, E., Chesani, F., Poluzzi, A., and Capuzzo, D. (2022). Statistical stability indices
for LIME: obtaining reliable explanations for machine learning models. J. Oper. Res. Soc. ,
73(1):91â€“101.
Wolf, L., Galanti, T., and Hazan, T. (2019). A formal approach to explainability. In AAAI/ACM
Conference on AI, Ethics, and Society (AIES) .
Yadav, C., Moshkovitz, M., and Chaudhuri, K. (2022). A learning-theoretic framework for certified
auditing of machine learning models. arxiv preprint 2206.04740 .
Zhou, J., Gandomi, A. H., Chen, F., and Holzinger, A. (2021). Evaluating the quality of machine
learning explanations: A survey on methods and metrics. Electronics , 10(5):593.
11A Proof of Theorem 4.1
A.1 An additional assumption
We also include the assumption that the locality parameter be small compared to the tolerance
parameters. More precisely, we assume that Î» < Ïµ2
2.
We believe this to be an extremely mild assumption considering that we typically operate in the
regime where Î»is exponentially small with the dimension, d, whereas the tolerance parameters are
typically between 10âˆ’2and10âˆ’3.
A.2 Main Proof
Proof. FixÏµ1, Ïµ2, Î³, Î» , and Âµ, as given in the theorem statement. Our goal is to show the existence
of classifier fand explainer Eso that the auditor, A, is likely to incorrectly estimate the parameter,
LÎ³(E, f). To do so, our strategy will be instead to consider a distribution over choices of (E, f), and
show that in expectation over this distribution, Aestimates LÎ³(E, f)poorly.
To this end, we define the following quantities:
1. Let Ebe the explainer given in Section A.4.
2. Let fâˆ—be the random classifier defined in Section A.4.
3. Let nbe any integer with
nâ‰¤1
2592Î»(1âˆ’8max(Ïµ1,Ïµ2).
4. Let Xbe a random variable for a set of points {x1, . . . , x n}drawn i.i.d from Âµ.
5.LetY=fâˆ—(X)be a random variable for {fâˆ—(x1), fâˆ—(x2), . . . , fâˆ—(xn)}.Yhas random-
ness over both fâˆ—andXâˆ—.
6. Let âˆ†n= 
RdnÃ— {Â± 1}n, and Ïƒdenote the measure over âˆ†ninduced by (X, Y).
7.By definition Eâ€™s output is independent of function, fâˆ—. Thus, we will abbreviate Aâ€™s output
by writing
A(X, fâˆ—(X), E(fâˆ—, X)) =A(X, Y, E ).
This emphasizes that both Xand the output of Eare independent of fâˆ—.
8. We let Iâˆ—denote the interval that the auditor seeks to output an estimate it. That is,
Iâˆ—=
LÎ³(1 +Ïµ1)(E, fâˆ—)âˆ’Ïµ2, LÎ³(1âˆ’Ïµ1)(E, fâˆ—) +Ïµ2
.
Using this notation, we seek to lower bound that the auditor fails meaning we seek to lower bound,
Pr
fâˆ—,X[A(X, Y, E )/âˆˆIâˆ—].
To do so, let T1denote the event
T1= 11
2âˆ’Ïµ2< LÎ³(1+Ïµ1)(E, fâˆ—)â‰¤LÎ³(1âˆ’Ïµ1)(E, fâˆ—)<1
2+Ïµ2
,
andT2denote the event
T2= 11
2+ 3Ïµ2< LÎ³(1+Ïµ1)(E, fâˆ—)â‰¤LÎ³(1âˆ’Ïµ1)(E, fâˆ—)<1
2+ 5Ïµ2
.
The key observation is that any estimate, A(X, Y, E ), can be inside at most one of the intervals,
[1
2âˆ’Ïµ2,1
2+Ïµ2]and[1
2+ 3Ïµ2,1
2+ 5Ïµ2].Using this, we can re-write our desired probability through
the following integration.
12Let(x, y)denote specific choices of (X, Y). Note that in this context, xrepresents a set of points in
(Rd)n, and yrepresents a set of labels in {Â±1}n. We then have the following:
Pr
fâˆ—,X[A(X, Y, E )/âˆˆIâˆ—] =Z
âˆ†nPr
fâˆ—[A(x, y, E )/âˆˆIâˆ—|X=x, Y=y]dÏƒ(x, y)
â‰¥Z
âˆ†nPr
fâˆ—[T1|X=x, Y=y] 1(A(x, y, E )/âˆˆI1) +
Pr
fâˆ—[T2|X=x, Y=y] 1(A(x, y, E )/âˆˆI1)dÏƒ(x, y)
â‰¥Z
âˆ†nmin
Pr
fâˆ—[T1|X=x, Y=y],Pr
fâˆ—[T2|X=x, Y=y]
dÏƒ(x, y),
where the last equation holds because at least one of the events, A(x, y, E )/âˆˆI1andA(x, y, E )/âˆˆI2,
must hold. To bound this last quantity, we utilize Lemma A.9.
Let
Sâˆ—=
(x, y) : Pr[ T1|X=x, Y=y],Pr[T0|X=x, Y=y]â‰¥2
5
.
By Lemma A.9, we have that Ïƒ(Sâˆ—)â‰¥5
6. It follows that
Pr
fâˆ—,X[A(X, Y, E )/âˆˆIâˆ—]â‰¥Z
âˆ†nmin
Pr
fâˆ—[T1|X=x, Y=y],Pr
fâˆ—[T2|X=x, Y=y]
dÏƒ(x, y)
â‰¥Z
Sâˆ—min
Pr
fâˆ—[T1|X=x, Y=y],Pr
fâˆ—[T2|X=x, Y=y]
dÏƒ(x, y)
â‰¥Z
Sâˆ—2
5dÏƒ(x, y)
=2
5Ïƒ(Sâˆ—)â‰¥1
3,
which completes the proof as this implies with probability at least1
3, the Auditors estimate is not
sufficiently accurate.
A.3 Non-degenerate Distributions
Theorem 4.1 includes the assumption that Âµisnon-degenerate , which is defined as follows.
Definition A.1. We say that data distribution ÂµoverRdisnon-degenerate if for all xâˆˆRd, there
exists 1â‰¤iâ‰¤dsuch that
Âµ({xâ€²:xâ€²
i=xi}) = 0 .
Being non-degenerate essentially means that at any point, x, the data distribution Âµhas a finite
probability density with respect to some feature.
This condition any distribution with a well-defined density over Rd(i.e. such as a Gaussian) and is
also met for most practical data-sets in which any of the features is globally continuously distributed
(i.e. mass in kg over a distribution of patients).
We exclude data distributions with point masses because they can pose particularly simple cases in
which there is a strict lower bound on how small the local region assigned to a given point can be.
For example, in the extreme case where Âµis concentrated on a single point, auditing any model or
explanation over Âµis trivial.
We now show a useful property of non-degenerate distributions.
Lemma A.2. LetÂµbe a non-degenerate distribution and Rbe a hyper-rectangle. Then Rcan be
partitioned into two hyper-rectangles, R1, R2such that Âµ(R1), Âµ(R2)â‰¥Âµ(R)
4.
Proof. LetR= (a1, b1]Ã—(a2, b2]Ã— Â·Â·Â· Ã— (ad, bd]. First, suppose that there exists 1â‰¤iâ‰¤dsuch
that for all râˆˆ(ai, bi],
Âµ({x:x=r} âˆ©R)â‰¤Âµ(R)
4.
13Let
râˆ—= sup
r:Âµ(Râˆ© {x:xiâ‰¤r})â‰¤Âµ(R)
4
.
It follows that setting R1=Râˆ© {x:xiâ‰¤râˆ—}andR2=R\R1will suffice as R1will have
probability mass at leastÂµ(R)
4and probability mass at mostÂµ(R)
2.
Otherwise, suppose that no such iexists. Then, thus, there exists r1, r2, . . . , r d}such that Âµ(Râˆ© {x:
xi=ri})>0. It follows that the point (r1, . . . , r d)violates Definition A.1, which is a contradiction.
Thus some iexists, which allows us to apply the above argument, finishing the proof.
A.4 Constructing fâˆ—andE
We begin by partitioning the support of Âµinto hyper-rectangles such that each rectangle has probability
mass in the interval [Î±
4, Î±]. We then further partition these rectangles into a large number of equal
parts. Formally, we have the following:
Lemma A.3. LetÎ± >0be fixed, and K > 0be any integer. Then for some integer L >0, there
exists a set of hyper-rectangles, {Rj
i: 1â‰¤iâ‰¤L,1â‰¤jâ‰¤K}such that the following hold:
1.R1
i, . . . RK
ipartition rectangle Ri.
2. For all 1â‰¤iâ‰¤L,Î±â‰¤Âµ(Ri)â‰¤4Î±.
3. For all 1â‰¤iâ‰¤Land1â‰¤jâ‰¤K,Âµ(Ri)
4Kâ‰¤Âµ(Rj
i)â‰¤Âµ(Ri)
K.
Proof. First construct R1, . . . , R Lby using the following procedure:
1. Begin with the set A={Râˆ—}where Râˆ—is a large rectangle containing the support of Âµ.
2.IfAcontains a rectangle, R, such that Âµ(R)>4Î±, then apply Lemma A.2 to split Rinto
two rectangles with mass at leastÂµ(R)
4and mass at most3Âµ(R)
4.
3. Repeat step 2 until no such rectangles, R, exist.
This process clearly must terminate in a set of rectangles each of which has mass in the desired range,
and also must terminate as a single rectangle can only be cut at mostlog1
Î±
log3
4times.
Next, to construct R1
i, R2
i, RK
i, we simply utilize an analogous procedure, this time starting with
{Ri}and replacing Î±withÂµ(Ri)
4K.
We now construct a fixed explainer, E.
Definition A.4. LetEdenote the explainer so that for all xâˆˆsupp(Âµ), we have E(x) = (Rx, g+1)
where Rxis the unique hyper-rectangle, Rithat contains x, and g+1is the constant classifier that
always outputs +1.
We now construct a distribution over functions f, and let fâˆ—be a random function that follows this
distribution. We have the following:
Definition A.5. Letfâˆ—be a random classifier mapping Rdto{Â±1}constructed as follows. Let mbe
an integer and 0â‰¤p1, . . . , p 2m, q1, . . . , q 2mâ‰¤1be real numbers that satisfy the conditions set forth
in Lemma A.10. Then fâˆ—is constructed with the following steps:
1. Let Pbe a binary event that occurs with probability1
2.
2. IfPoccurs, then set ri=pifor1â‰¤iâ‰¤p2m. Otherwise set ri=qi.
3. Ifx /âˆˆ âˆªL
i=1Ri, then fâˆ—(x) = +1 .
4. For each rectangle Ri, randomly select 1â‰¤kâ‰¤2mat uniform.
145.For each sub-rectangle Rj
i, with probability rk, setf(x) =âˆ’1for all xâˆˆRj
i, and with
probability 1âˆ’rk, setfâˆ—(x) = +1 for all xâˆˆRj
i.
Note that mis constructed based on Ïµ1, Ïµ2,andÎ³which we assume to be provided as in the statement
of Theorem 4.1.
A.5 Properties of fâˆ—andE
We now prove several useful properties of this construction. To do so, we use the following notation:
1.We let fâˆ—denote the random variable representing the way fis generated. We use fâˆ—=f
to denote the event that fâˆ—equals a specific function f:Rdâ†’ {Â± 1}.
2.We let Pdenote the indicator variable for the binary event used in Section A.4 to construct
f.
3. We let mdenote the integer from Lemma A.10 that is used to construction fâˆ—.
4.We let X= (x1, . . . , x n)âˆ¼Âµndenote a random variable of ni.i.d selected points from Âµ.
We use xto denote a specific instance of X.
5.We let Y= (y1, . . . , y n)be a random variable over labels constructed by setting yi=
fâˆ—(xi). We similarly use yto denote a specific instance of Y.
6. We let Ïƒdenote the measure over 
RdÃ— {Â± 1}nassociated with (X, Y).
7.We let âˆ†ndenote the domain of the pair of random vectors (X, Y)(as done in Section A.2)
We begin with a bounds on the probability that we see any rectangle that has a large number of points
selected from it.
Lemma A.6. LetR1, . . . , R Lbe as defined in section A.4, and mas given. Let Udenote the subset
ofâˆ†nsuch that
U={(x, y) :âˆƒ1â‰¤iâ‰¤z,|Xâˆ©Ri| â‰¥2m}.
Then Ïƒ(U)â‰¤1
180.
Proof. We bound the probability that a single rectangle, Ri, contains at least 2mpoints from X, and
then apply a union bound over all Lrectangles. By construction, Âµ(Ri)â‰¤4Î», which implies that
for each point xjâˆˆXthe probability that Xjfalls within rectangle Riis at most 4Î». Thus, for any
set of 2mdistinct points from X, the probability that they allfall within Riis at most (4Î»)2m. By
taking a union bound over all n
2m
subsets of 2mpoint from X, and substituting our assumed upper
bound for n(point 3. of Section A.2), we have the following
Pr[|Xâˆ©Ri| â‰¥2m]â‰¤n
2m
(4Î»)2m
â‰¤en
2m2m
(4Î»)2m
â‰¤e
2m1
2592 max( Ïµ1, Ïµ2)Î»1âˆ’8 max( Ïµ1,Ïµ2)2m
(4Î»)2m
= (4Î») 
e
2m41âˆ’1
2mÎ»1âˆ’1
2m
2592 max( Ïµ1, Ïµ2)Î»1âˆ’8 max( Ïµ1,Ïµ2)!2m
.
15By definition (see Lemma A.10), mâ‰¥1
16 max( Ïµ1,Ïµ2). Substituting this, and noting that Î»1âˆ’1
2mis
increasing with respect to m(since Î» <1), we have
Pr[|Xâˆ©Ri| â‰¥2m]â‰¤(4Î») 
e
2m41âˆ’1
2mÎ»1âˆ’1
2m
2592 max( Ïµ1, Ïµ2)Î»1âˆ’8 max( Ïµ1,Ïµ2)!2m
â‰¤(4Î»)e8 max( Ïµ1, Ïµ2)
14Î»1âˆ’8 max( Ïµ1,Ïµ2)
2592 max( Ïµ1, Ïµ2)Î»1âˆ’8 max( Ïµ1,Ïµ2)2m
â‰¤(4Î»)96
25922m
<Î»
180.
Finally, we apply a union bound over all rectangles. Observe that there are at most1
Î»such rectangles
because by construction each rectangle has mass at most Î». Thus, our total probability is at most
1
Î»Î»
180which is at most1
180as desired.
Next, we leverage the properties from the construction of fto bound the conditional probability of
P= 1when (x, y)/âˆˆU.
Lemma A.7. Let(x, y)be in the support of Ïƒso that (x, y)/âˆˆU. Then
Pr[P= 1|(X, Y) = (x, y)] = Pr[ P= 0|(X, Y) = (x, y)] =1
2.
Proof. Our main idea will be to use Bayes-rule, and show that Pr[(X, Y) = ( x, y)|P= 1] =
Pr[(X, Y) = ( x, y)|P= 0. This will suffice due to the fact that the prior distribution for Pis
uniform over {0,1}. To do so, we first note that Xis independent from P. For this reason, it suffices
to show that
Pr[Y=y|P= 1, X=x] = Pr[ Y=y|P= 0, X=x].
To do so, we will express these probabilities in terms of the real numbers, p1, . . . , p 2mandq1, . . . , q 2m
from which they were constructed (see Definition A.5).
For each rectangle, Ri(see Lemma A.3), let Yâˆ©Ridenote the function values of all points in the set
Xâˆ©Ri. It follows from step 4 of Definition A.5 that the values in Yâˆ©RiandYâˆ©Rjareindependent
from each other. Thus, we can re-write our desired probability as
Pr[Y=y|P= 1, X=x] =LY
i=1Pr
(Yâˆ©Ri) = (yâˆ©Ri)|P= 1,(Xâˆ©Ri) = (xâˆ©Ri)
.
We now analyze the latter quantity for a rectangle, Ri. For convenience, let us relabel indices so
thatxâˆ©Ri={x1, x2, . . . , x l}andyâˆ©Ri={y1, . . . , y l}for some integer lâ‰¥0. We also let
X1, . . . , X landY1, . . . , Y ldenote the corresponding values for Xâˆ©RiandYâˆ©Ri.
We now further assume that that for all xa, xbâˆˆ {x1, . . . , x l}, that xaandxbare contained within
different sub-rectangles, Ra
i, Rb
i(see Definition A.5). If this isnâ€™t the case, observe that we can simply
remove the pair (xb, yb), as by the construction of fâˆ—, this will be forced to be identical to (xa, ya).
By applying this assumption, we now have that for a given choice of the parameter rk(step 4 of
Definition A.5), the values of y1, . . . , y laremutually independent . Utilizing this, we have
Pr
(Yâˆ©Ri) = (yâˆ©Ri)|P= 1,(Xâˆ©Ri) = (xâˆ©Ri)
=1
2m2mX
j=1lY
k=1yk
2âˆ’ykpj+1
2
=1
2m2mX
j=1F(pj),
Where Fis a polynomial of degree l. Here, the expression,yk
2âˆ’ykpj+1
2simply evaluates to pjif
yk=âˆ’1(aspjis the probability of observing a âˆ’1) and 1âˆ’pjotherwise.
16Next, observe that the only difference when performing this computation for P= 0is that we use the
real numbers, q1, . . . q 2minstead. Thus, we have,
Pr
(Yâˆ©Ri) = (yâˆ©Ri)|P= 0,(Xâˆ©Ri) = (xâˆ©Ri)
=1
2m2mX
j=1lY
k=1yk
2âˆ’ykqj+1
2
=1
2m2mX
j=1F(qj),
To show these two expression are equal, by assumption (x, y)/âˆˆUwhich implies that l <2m.
Furthermore, by Lemma A.10,P2m
k=1pt
k=P2m
k=1qt
k,for all 0â‰¤tâ‰¤l. It follows thatP2m
k=1F(pk) =P2m
k=1F(qk), which implies our desired result.
Next, we bound the probability of events related to the value of LÎ³, the parameter that the Auditor
seeks to estimate.
Lemma A.8. LetT1denote the event that
1
2âˆ’Ïµ2< LÎ³(1+Ïµ1)â‰¤LÎ³(1âˆ’Ïµ1)<1
2+Ïµ2.
LetT0denote the event that
1
2+ 3Ïµ2< LÎ³(1+Ïµ1)< LÎ³(1âˆ’Ïµ1)â‰¤1
2+ 5Ïµ2.
Then taken over the randomness of the entire construction, Pr[T1, P= 1],Pr[T0, P= 0]â‰¥89
180,
where Pis the binary event defined above.
Proof. By definition, Pr[P= 1] = Pr[ P= 0] =1
2. Thus, it suffices to show that Pr[T1|P=
1],Pr[T0|P= 0]â‰¥89
90.
We begin with the case that P= 1(the case for P= 0will be similar). For each rectangle, Ri, let
r(Ri)denote the choice of rkmade for Riin step 5 of Definition A.5. The crucial observation is that
the value of r(Ri)nearly determines the local loss that Epays for points in Riwith respect to fâˆ—. In
particular, if the number of sub-rectangles, K, is sufficiently large, then by the law of large numbers,
we have that with high probability over the choice of fâˆ—, for all rectangles Riand for all xâˆˆRi,
|L(E, fâˆ—, x)âˆ’r(Ri)|<0.01Î³(Ïµ). (1)
Let us fix Kto be any number for which this holds, and assume that this value of Kis set throughout
our construction.
Next, recall by Lemma A.10 that
p1< p2<Â·Â·Â·< pm< Î³(1âˆ’2Ïµ1)< Î³(1 + 2 Ïµ1)< pm+1<Â·Â·Â·< p2m.
Recall that r(Ri)is chosen at uniform among {p1. . . p 2m}(step 5 of Definition A.5). It follows from
Equation 1 that for any xâˆˆRi, and for any Î±âˆˆ {Î³(1âˆ’Ïµ1), Î³(1 +Ïµ1)}that
Pr
fâˆ—[L(E, fâˆ—, x)â‰¥Î±for all xâˆˆRi] =1
2.
Furthermore, because we are conditioning on P= 1, the value of fâˆ—within each rectangle, Ri, is
independent. This implies that we can bound the behavior of LÎ±(E, fâˆ—)by expressing as a sum of
independent variables.
17LetÎ±âˆˆ {Î³(1âˆ’Ïµ1), Î³(1 +Ïµ1)}, we have by Hoeffdingâ€™s inequality that
Pr
LÎ±(E, fâˆ—)âˆˆ1
2âˆ’Ïµ2,1
2+Ïµ2
= Pr" LX
i=1Âµ(Ri) 1(L(E, fâˆ—, x)â‰¥Î±for all xâˆˆRi)!
âˆˆ1
2âˆ’Ïµ2,1
2+Ïµ2#
â‰¥1âˆ’2 exp 
âˆ’2Ïµ2
2PL
i=1Âµ(Ri)2!
â‰¥1âˆ’2 exp
âˆ’2Ïµ2
2
16Î»
â‰¥1âˆ’1
180=179
180;
The penultimate inequality holds since Âµ(Ri)â‰¤4Î»for each Ri, and because there are at most1
Î»
such rectangles. The last inequality holds because Î» < Ïµ2
2by the assumption in Section A.1.
Thus by taking a union bound over both values of Î±, we have that LÎ±(E, fâˆ—)âˆˆ1
2âˆ’Ïµ2,1
2+Ïµ2
with probability at least89
90. This completes our proof for the case P= 1.
ForP= 0, we can follow a nearly identical argument. The only difference is that the values of q(see
Lemma A.10) are selected so that
Pr
fâˆ—[L(E, fâˆ—, x)â‰¥Î±]â‰¥1
2+ 4Ïµ2.
This results in the expected loss falling within a different interval, and an identical analysis using
Hoeffdingâ€™s inequality gives the desired result.
The main idea of proving Theorem 4.1 is to show that for many values of X, Y , the conditional
probabilities of T1andT0occurring are both fairly large. This, in turn, will cause the Auditor to have
difficulty as its estimate will necessarily fail for at least one of these events.
To further assist with proving this, we have the following additional lemma.
Lemma A.9. LetSâˆ—denote the subset of 
RdÃ— {Â± 1}nsuch that
Sâˆ—=
(x, y) : Pr[ T1|(X, Y) = (x, y)],Pr[T0|(X, Y) = (x, y)]â‰¥2
5
.
Then Ïƒ(Sâˆ—)â‰¥5
6.
Proof. LetSâ€²
1={(x, y) : Pr[ T1|(X, Y) = ( x, y)]<2
5}, and similarly Sâ€²
2={(x, y) :
Pr[T2|(X, Y) = ( x, y)]<2
5}. Then Sâˆ—= 
RdÃ— {Â± 1}n\(Sâ€²
1âˆªSâ€²
2). Thus it suffices to up-
per bound the mass of Sâ€²
1andSâ€²
2. To do so, let Ube the set defined in Lemma A.6. Then we
have
89
180â‰¤Pr[T1, P= 1]
=Z
(RdÃ—{Â±1})nPr[T1, P= 1|(X, Y) = (x, y)]dÏƒ
â‰¤Z
Sâ€²
1Pr[T1, P= 1|(X, Y) = (x, y)]dÏƒ+Z
U\Sâ€²
1Pr[T1, P= 1|(X, Y) = (x, y)]dÏƒ
+Z
âˆ†n\(Sâ€²
1âˆªU)Pr[T1, P= 1|(X, Y) = (x, y)]dÏƒ
<2
5Ïƒ(Sâ€²
1) + (Ïƒ(U)âˆ’Ïƒ(Uâˆ©Sâ€²
1)) +1
2(Ïƒ(âˆ†n\U)âˆ’Ïƒ((âˆ†n\U)âˆ©Sâ€²
1))
â‰¤2
5âˆ’1
2
Ïƒ(Sâ€²
1) +1
2Ïƒ(âˆ†n\U) +Ïƒ(U)
â‰¤1
2179
180+1
180âˆ’Ïƒ(Sâ€²
1)
10
18Here we are simply leveraging the fact that Pr[P= 1|X, Y =x, y]is precisely1
2when x, yare not
inU, and consequently that the probability of T1andP= 1is at most2
5,1,and1
2when (x, y)is
in the sets Sâ€²
1, U\Sâ€²
1and(âˆ†n\U)\Sâ€²
1respectively. Finally, simplifying this inequality gives us
Ïƒ(Sâ€²
1)â‰¤1
12.
A completely symmetric argument will similarly give us that Ïƒ(Sâ€²
2)â‰¤1
12. Combining these with a
union bound, it follows that Ïƒ(Sâˆ—)â‰¥5
6, as desired.
A.6 Technical Lemmas
Lemma A.10. For all 0< Î³, Ïµ 1, Ïµ2<1
48, there exists m > 0, and real numbers 0â‰¤
p1, p2, . . . , p 2m, q1, . . . , q 2mâ‰¤1such that the following four conditions hold:
1. For all 0â‰¤tâ‰¤2mâˆ’1,P2m
i=1pt
i=P2m
i=1qt
i.
2.p1â‰¤p2â‰¤ Â·Â·Â· â‰¤ pm< Î³(1âˆ’2Ïµ1)< Î³(1 + 2 Ïµ1)< pm+1â‰¤. . . p 2m.
3.q1â‰¤q2â‰¤ Â·Â·Â· â‰¤ qmâˆ’1< qm=qm+1=Î³(1 + 2 Ïµ1)< qm+2â‰¤. . . q 2m.
4.1
4Ïµ2â‰¥mâ‰¥1
8 max(2 Ïµ1,Ïµ2)+ 1.
Proof. Letldenote the largest integer that is strictly smaller than1
8 max(2 Ïµ1,Ïµ2), and let Ïµ=1
8l. It
follows that Ïµâ‰¥max(2 Ïµ1, Ïµ2). Let PlandQlbe as defined in Lemma A.11. Let m= 2l. Then it
follows from the definitions of m, l that
m= 2lâ‰¤1
4 max(2 Ïµ1,max( Ïµ2)â‰¤1
4Ïµ2,
which proves the first part of property 4 in Lemma A.10. For the second part, by the definition of l,
we have lâ‰¥1
8 max(2 Ïµ1,Ïµ2)âˆ’1. Since Ïµ1, Ïµ2â‰¤1
48, it follows that lâ‰¥2which implies
m= 2lâ‰¥l+ 2â‰¥1
8 max(2 Ïµ1, Ïµ2)+ 1.
Next, let p1, . . . , p 4ldenote the (sorted in increasing order) roots of the polynomial
Pâ€²
l(x) =Plxâˆ’Î³(1 + 2 Ïµ1)
2Î³Ïµ
.
Since the roots of Plare explicitly given in Lemma A.11, it follows that the middle two roots of
Pâ€²
l(x)(which are the values of pmandpm+1) satisfy
pm=Î³(1 + 2 Ïµ1âˆ’2Ïµ), pm+1=Î³(1 + 2 Ïµ1+ 2Ïµ).
Because Ïµâ€²> Ïµ, these values clearly satisfy the inequalities given by point 2 in the Lemma statement.
Next, define q1, . . . , q 4las the (sorted in increasing order) roots of the polynomial,
Qâ€²
l(x) =Qlxâˆ’Î³(1 + 2 Ïµ1)
2Î³Ïµ
.
Again using Lemma A.11, we see that
qm=qm+1=Î³(1 + 2 Ïµ1),
which satisfies point 3.
To see that piandqiare indeed in the desired range, we simply note that by substitution, both p1
andp2must be larger than Î³(1 + 2 Ïµ1)âˆ’4l(2Î³Ïµ). However, by definition, 4l(2Î³Ïµ) =Î³. Thus, this
quantity is larger than 0which implies that p1andq1are both positive. Because Î³ <1
10, a similar
argument implies that p2mandq2mare at most 1.
Finally, point 1 follows from the fact that p1, . . . , p 2mandq1, . . . , q 2mare the complete sets of roots
of two polynomials that have matching coefficients for the first 2mcoefficients. it follows by basic
properties of Newton sums thatPpt
i=Pqt
ifor0â‰¤iâ‰¤2mâˆ’1, and this proves point 1.
19Lemma A.11. For any l >0, let
Pl(x) = (( x+ 1)( x+ 3). . .(x+ 4lâˆ’1)) (( xâˆ’1)(xâˆ’3). . .(xâˆ’4l+ 1)) .
LetQl(x) =Pl(x)âˆ’Pl(0). Then Qlhas2lâˆ’1distinct real roots over the interval (âˆ’4l,âˆ’1),2lâˆ’1
distinct real roots over the interval (1,4l), and a double root at x= 0.
Proof. By symmetry, Pâ€²
l(0) = Qâ€²
l(0) = 0 , and by definition Ql(0) = 0 . It follows that x= 0is a
double root. Next, fix 1â‰¤iâ‰¤lâˆ’1. By definition, we have that Pl(4iâˆ’1) = Pl(4i+ 1) = 0 . We
also have that
Pl(4i) =2(l+i)Y
j=1(2jâˆ’1)2(lâˆ’i)Y
j=1(2jâˆ’1).
Meanwhile, we also have that Pl(0) =Q2
j=1l(2jâˆ’1)2
. By directly comparing terms, it follows
thatPl(i)is strictly larger than Pl(0). Thus, by the intermediate value theorem, Qlmust have at least
one root in both (4iâˆ’1,4i)and(4i,4i+ 1) . Using a similar argument, we can also show that Ql
has at least one root in (4lâˆ’1,4l).
Since Plis an even function, it follows that Qlis as well which means it symmetrically has roots
in the intervals (âˆ’4i,âˆ’4i+ 1) for1â‰¤iâ‰¤land(âˆ’4iâˆ’1,4i)for1â‰¤iâ‰¤lâˆ’1. Taken all
together, we have constructed 2(l+lâˆ’1) = 4 lâˆ’2distinct intervals that each contain a root.
Since Qlalso has a double root at x= 0, it follows that this must account for all of its roots as
deg(Ql) =deg(Pl) = 4 l.
B Proof of Theorem 4.2
B.1 Algorithm description
The main idea of our auditor, simple_audit , is to essentially performs a brute-force auditing where
we choose a set of points, X1, and attempt to assess the accuracy of their local explanations by using
a a wealth of labeled data, X2, to validate it. Our algorithm uses the following steps (pseudocode
given in Algorithm 1).
1.(lines 1 -3) We first partition Xbased on the tolerance parameters, Ïµ1, Ïµ2, Î´.X1will be the
set of points that we validate, and X2will be the set of points we use for validation.
2.(lines 8), For each point xinX1, we check whether there are enough points from X2that
fall within its local region, Rx, to accurate estimate its local loss.
3.(line 9-13) For each point satisfying the criteria in line 8, we evaluate its empirical local loss
and then tally up how many points have a loss that is larger than Î³.
4.(line 17) We output the proportion of points with loss larger than Î³among all points whose
loss we measured.
At a high level, we expect this algorithm to succeed as long as we have enough data in each of the
local regions induced from points in X1.
B.2 Notation
We use the following:
1. Let Î´, Ïµ1, Ïµ2, Î³be the tolerance parameters defined in Definition 3.1.
2. Let Î»= Î›(E, f)denote the locality of E, f w.r.t. data distribution Âµ.
3. Let X1be the set of points that are specifically being audited.
4. Let X2be the set of points being used to audit.
5. Let |X1|=m. By definition, m >log1
Î´
Ïµ2
2.
6. We set |X2|=nâ€²=nâˆ’m. By definition, nâ€²> stuff .
20Algorithm 1 simple _audit (X, f(X), E(f, X), Ïµ1, Ïµ2, Î³, Î´)
1:mâ†61
Ïµ2
2log12
Î´.
2:kâ†1
2Î³2Ïµ2
1log176
Ïµ2Î´.
3:X1â† {x1, . . . , x m},X2â†X\X1.
4:râ€², bâ€²â†0.
5:forxiâˆˆX1do
6: (Rxi, gxi) =E(xi, f)
7: Xi
1=Rxiâˆ©X2.
8: if|Xi
1| â‰¥kthen
9: Ë†L(E, f, x i)â†1
|Xi
1|P
xjâˆˆXi
11(gxi(xj)Ì¸=f(xj))
10: ifË†L(E, f, x i)> Î³ then
11: râ€²=râ€²+ 1.
12: else
13: bâ€²=bâ€²+ 1.
14: end if
15: end if
16:end for
17:Returnrâ€²
râ€²+bâ€².
7.For any xâˆˆRd, we let E(x) = ( Rx, gx)be the local explanation outputted for xby
explainer E.
We also define the following quantities related to estimating how frequently the local loss outputted
by the explainer Eis above the desired threshold, Î³.
1. Let râˆ—= Pr xâˆ¼Âµ[L(E, f, x )â‰¥Î³(1 +Ïµ1)].
2. Let gâˆ—= Pr xâˆ¼Âµ[Î³(1âˆ’Ïµ1)â‰¤L(E, f, x )â‰¤Î³(1 +Ïµ1)].
3. Let bâˆ—= Pr xâˆ¼Âµ[L(E, f, x )â‰¤Î³(1âˆ’Ïµ1)].
Here, râˆ—denotes the probability that a point has a large local error, bâˆ—, the probability a point has a
low local error, and gâˆ—, the probability of an "in-between" case that is nearby the desired threshold,
Î³. By the definition of sample complexity (Definition 3.1), the goal of Algorithm 1 is to output an
estimate that is inside the interval, [râˆ—âˆ’Ïµ2, râˆ—+gâˆ—+Ïµ2].
Next, we define r, g, b as versions of these quantities that are based on the sample, X1.
1. Let r= Pr xâˆ¼X1[L(E, f, x )â‰¥Î³(1 +Ïµ1)].
2. Let g= Pr xâˆ¼X1[Î³(1âˆ’Ïµ1)â‰¤L(E, f, x )â‰¤Î³(1 +Ïµ1)].
3. Let b= Pr xâˆ¼X1[L(E, f, x )â‰¤Î³(1âˆ’Ïµ1)].
Observe that while xis drawn at uniform from X1in these quantities, we still use the true loss with
respect toe Âµ,L(E, f, x ), to determine whether it falls into r, gorb. Because of this, it becomes
necessary to define two more fully empirical quantities that serve as estimates of randb(we ignore g
as it will merely contribute to a "margin" in our estimation terms).
1. Let râ€²= Pr xâˆ¼X1"
(Prxâ€²âˆ¼X2[gx(xâ€²)Ì¸=f(xâ€²)|xâ€²âˆˆRx]> Î³)and|X2âˆ©Rx| â‰¥log176
Ïµ2Î´
2(Î³Ïµ1)2#
.
2. let bâ€²= Pr xâˆ¼X1"
(Prxâ€²âˆ¼X2[gx(xâ€²)Ì¸=f(xâ€²)|xâ€²âˆˆRx]â‰¤Î³)and|X2âˆ©Rx| â‰¥log176
Ïµ2Î´
2(Î³Ïµ1)2#
.
The final estimate outputted by Algorithm 1 is preciselyrâ€²
râ€²+bâ€². Thus, our proof strategy will be to
show that for sufficiently large samples, r, g, b are relatively accurate estimates of râˆ—, gâˆ—, bâˆ—, and in
turnrâ€²andbâ€²are relatively accurate estimates of randb. Together, these will imply that our estimate
is within the desired interval, [râˆ—, bâˆ—].
21B.3 The main proof
Proof. (Theorem 4.2) Let
nâ‰¥61
Ïµ2
2log12
Î´+log176
Ïµ2Î´
2Î»Î³2Ïµ2
1log44 log176
Ïµ2Î´
Ïµ2Î´Î³2Ïµ2
1.
By ignoring log factors, we see that n=ËœO
1
Ïµ2
2+1
Î»Î³2Ïµ2
1
, thus satisfying the desired requirement in
Theorem 4.2.
LetX1andX2be as in Algorithm 1, and let m, nâ€²denote |X1|and|X2respectively. Directly from
Algorithm 1, it follows that ,
m=61
Ïµ2
2log12
Î´, nâ€²â‰¥log176
Ïµ2Î´
2Î»Î³2Ïµ2
1log44 log176
Ïµ2Î´
Ïµ2Î´Î³2Ïµ2
1.
By letting Ïµ=Ïµ2
7, and k=log16
ÏµÎ´
2(Î³Ïµ1)2, we have that mâ‰¥1
2Ïµ2log12
Î´, and that
nâ€²â‰¥log176
Ïµ2Î´
2Î»Î³2Ïµ2
1log44 log16
ÏµÎ´
Ïµ2Î´Î³2Ïµ2
1
=log16
ÏµÎ´
2Î»Î³2Ïµ2
1log4 log16
ÏµÎ´
ÏµÎ´Î³2Ïµ2
1
=klog8k
Î´Ïµ
Î».
Our bounds on m, k andnâ€²allow us to apply Lemmas B.1 and B.4 along with a union bound to get
that the following equations hold simultaneously with probability at least 1âˆ’Î´overXâˆ¼Âµn:
|râˆ’râˆ—|.|gâˆ’gâˆ—|,|bâˆ’bâˆ—| â‰¤Ïµ, (2)
r(1âˆ’2Ïµ)â‰¤râ€²â‰¤r+g+bÏµ (3)
b(1âˆ’2Ïµ)â‰¤bâ€²â‰¤rÏµ+g+b. (4)
Recall that our goal is to show thatrâ€²
râ€²+bâ€²âˆˆ[râˆ—âˆ’Ïµ2, râˆ—+gâˆ—+Ïµ2]holds with probability at least
1âˆ’Î´. Thus, it suffices to show that this a simple algebraic consequence of equations 2, 3, and 4. To
this end, we have
râ€²
râ€²+bâ€²(a)
â‰¥r(1âˆ’2Ïµ)
r(1âˆ’2Ïµ) +rÏµ+g+b
â‰¥r(1âˆ’2Ïµ)
r+b+g
â‰¥r
r+b+gâˆ’2Ïµ
(b)
â‰¥râˆ—âˆ’Ïµ
râˆ—+bâˆ—+gâˆ—+ 3Ïµâˆ’2Ïµ
=râˆ—
1 + 3 Ïµâˆ’Ïµ
1 + 3 Ïµâˆ’2Ïµ
â‰¥râˆ—(1âˆ’3Ïµ)âˆ’Ïµâˆ’2Ïµ
â‰¥râˆ—âˆ’4Ïµâˆ’2Ïµ
(c)
â‰¥râˆ—âˆ’Ïµ2.
Here step (a) follows from Equations 3 and 4, (b) from Equation 2, and (c) from the fact that Ïµ=Ïµ2
11.
22For the other side of the inequality, we have
râ€²
râ€²+bâ€²(a)
â‰¤r+g+bÏµ
r+g+bÏµ+b(1âˆ’2Ïµ)
â‰¤r+g+bÏµ
(r+g+b)(1âˆ’Ïµ)
â‰¤r+g
(r+g+b)(1âˆ’Ïµ)+Ïµ
1âˆ’Ïµ
â‰¤r+g
r+g+b+ 2Ïµ+Ïµ(1 + 2 Ïµ)
(b)
â‰¤râˆ—+gâˆ—+ 2Ïµ
râˆ—+gâˆ—+bâˆ—âˆ’3Ïµ+ 3Ïµ+ 2Ïµ2
=râˆ—+gâˆ—+ 2Ïµ
1âˆ’3Ïµ+ 3Ïµ+ 2Ïµ2
(c)
â‰¤(râˆ—+gâˆ—)(1 + 4 Ïµ) + 2Ïµ(1 + 4 Ïµ) + 3Ïµ+ 2Ïµ2
â‰¤râˆ—+gâˆ—+ 6Ïµ+ 8Ïµ2+ 3Ïµ+ 2Ïµ2
(d)
â‰¤râˆ—+gâˆ—+ 7Ïµ+ 4Ïµ
(e)
â‰¤râˆ—+gâˆ—+Ïµ2
Here step (a) follows from Equations 3 and 4, (b) from Equation 2, (c) from the fact that1
1âˆ’3Ïµâ‰¤1+4Ïµ,
(d) from Ïµ=Ïµ2
11â‰¤1
8,1
2, and (e) from the Ïµ=Ïµ2
11.
B.4 Concentration lemmas
In this section, we show several lemmas that allow us to bound the behavior of the random variables
r, g, b, râ€²andbâ€²(defined in section B.2). We also use mandnâ€²as they are defined in Section B.2 to
be the sizes of |X1|and|X2|respectively. Finally, we also let Ïµ=Ïµ2
11.
We begin by bounding the differences between r, g, b andrâˆ—, gâˆ—, bâˆ—.
Lemma B.1. Suppose that mâ‰¥1
2Ïµ2log12
Î´. Then with probability at least 1âˆ’Î´
2overX1âˆ¼Âµm, the
|râˆ’râˆ—|,|gâˆ’gâˆ—|,|bâˆ’bâˆ—| â‰¤Ïµ.
Proof. Observe that ris the average of mi.i.d binary variables each of which have expected value
râˆ—. It follows by Hoeffdingâ€™s inequality that
Pr[|râˆ’râˆ—|> Ïµ]â‰¤2 expâˆ’2(Ïµm)2
m
â‰¤2 exp
âˆ’2Ïµ21
2Ïµ2log12
Î´
=Î´
6.
By an identical argument, we see that the same holds for Pr[|gâˆ’gâˆ—|> Ïµ]andPr[|bâˆ’bâˆ—|> Ïµ].
Applying a union bound over all three gives us the desired result.
Next, we show that if nâ€²is sufficiently large, then it is highly likely that for any given point x, we
observe a large number of points from X2within the explanation region, Rx.
Lemma B.2. Letxâˆˆsupp(Âµ), and let k >0be an integer. Suppose that nâ€²â‰¥klog8k
Î´Ïµ
Î». Then with
probability at least 1âˆ’Î´Ïµ
8overX2âˆ¼Âµm,|Rxâˆ©X2| â‰¥k.
23Proof. Partition X2intoksets,X1
2, X2
2, . . . , Xk
2each of which contain at leastlog8k
Î´Ïµ
Î»i.i.d points
from Âµ. Because each point is drawn independently, we have that for any 1â‰¤iâ‰¤k,
Pr[Xi
2âˆ©Rx=âˆ…] =
1âˆ’Pr
xâ€²âˆ¼Âµ[xâ€²âˆˆRx]log8k
Î´Ïµ
Î»
â‰¤(1âˆ’Î»)log8k
Î´Ïµ
Î»
â‰¤exp
âˆ’log8k
Î´Ïµ
=Î´Ïµ
8k.
Here we are using the definition of Î»as a lower bound on the probability mass of Rx.
Next, we show that if Rxhas a sufficient number of points, then it is quite likely for the empirical
estimate of the local loss at xto be accurate.
Lemma B.3. Letxâˆˆsupp(Âµ), and let kâ‰¥log16
ÏµÎ´
2(Î³Ïµ1)2. Then conditioning on there being at least k
elements from X2inRx, the empirical local loss at xdiffers from the true local loss by at most Î³Ïµ1
with probability at least 1âˆ’Î´Ïµ
8. That is,
Pr
X2âˆ¼Âµnâ€²"L(E, f, x )âˆ’1
|X2âˆ©Rx|X
xâ€²âˆˆX2âˆ©Rx1(gx(xâ€²)Ì¸=f(xâ€²))> Î³Ïµ 1|X2âˆ©Rx| â‰¥k#
â‰¤Î´Ïµ
8.
Proof. The key idea of this lemma is that the distribution of kpoints drawn from Âµconditioned on
being in Rxisprecisely the marginal distribution over which L(E, f, x )is defined. In particular, this
means that the points in X2âˆ©Rxcan be construed as i.i.d drawn from the marginal distribution of Âµ
overRx. Given this observation, the rest of the proof is a straightforward application of Hoeffdingâ€™s
inequality. Letting Ë†L(E, f, x ) =1
|X2âˆ©Rx|P
xâ€²âˆˆX2âˆ©Rx1(gx(xâ€²)Ì¸=f(xâ€²))andK=|X2âˆ©Rx|, we
have
Pr
X2âˆ¼Âµnâ€²hL(E, f, x )âˆ’Ë†L(E, f, x )> Î³Ïµ 1Kâ‰¥ki
â‰¤2 exp
âˆ’2(KÎ³Ïµ 1)2
K
â‰¤2 exp
âˆ’log16
Î´Ïµ
=Î´Ïµ
8,
as desired.
It follows by a union bound that the probability that least one of the sets in {Xi
2âˆ©Rx: 1â‰¤iâ‰¤k}is
empty is at mostÎ´Ïµ
8. Thus with probability at least 1âˆ’Î´Ïµ
8, all the sets are non-empty which implies
that|Rxâˆ©X2| â‰¥k, completing the proof.
Finally, we use the previous two lemmas to show that râ€²andbâ€²closely approximate randb.
Lemma B.4. Letkâ‰¥log16
ÏµÎ´
2(Î³Ïµ1)2, and suppose that nâ€²â‰¥klog8k
Î´Ïµ
Î». Then with probability at least 1âˆ’Î´
2
overX2âˆ¼Âµnâ€², the following equations holds:
r(1âˆ’2Ïµ)â‰¤râ€²â‰¤r+g+bÏµ,
b(1âˆ’2Ïµ)â‰¤bâ€²â‰¤rÏµ+g+b.
Proof. We begin by defining subsets of X1that correspond to r, g, b, râ€²andbâ€². We have
1. Let R={xâˆˆX1:L(E, f, x )â‰¥Î³(1 +Ïµ1)}.
2. Let G={xâˆˆX1:Î³(1âˆ’Ïµ1)â‰¤L(E, f, x )â‰¤Î³(1 +Ïµ1)}.
243. Let B={xâˆˆX1:L(E, f, x )â‰¤Î³(1âˆ’Ïµ1)]}.
4.LetRâ€²=
xâˆˆX1: 
Prxâ€²âˆ¼X2[gx(xâ€²)Ì¸=f(xâ€²)|xâ€²âˆˆRx]> Î³
and|X2âˆ©Rx| â‰¥log176
Ïµ2Î´
2(Î³Ïµ1)2
.
5.LetBâ€²=
xâˆˆX1: 
Prxâ€²âˆ¼X2[gx(xâ€²)Ì¸=f(xâ€²)|xâ€²âˆˆRx]â‰¤Î³
and|X2âˆ©Rx| â‰¥log176
Ïµ2Î´
2(Î³Ïµ1)2
.
Observe that r, g, b, râ€²,andbâ€²are the probabilities that xâˆ¼X1is in the sets R, G, B, Râ€²,andBâ€²
respectively.
Our strategy will be to use the previous lemmas to bound the sizes of the intersections, Râ€²âˆ©R, Râ€²âˆ©
B, Bâ€²âˆ©R, Bâ€²âˆ©Bâ€². To this end, let xâˆˆRbe an arbitrary point. By Lemma B.2, with probability at
least1âˆ’Î´Ïµ
8overX2âˆ¼Âµnâ€²,xâˆˆRâ€²âˆªBâ€². Furthermore, by Lemma B.3 (along with the definition
ofR), with probability at mostÎ´Ïµ
8,xâˆˆBâ€². Applying linearity of expectation along with Markovâ€™s
inequality, we get the following two bounds:
Pr
X2Râˆ©(X1\(Râ€²âˆ©Bâ€²)>|R|Ïµ
â‰¤EX2[|Râˆ©(X1\(Râ€²âˆ©Bâ€²)|]
|R|Ïµ
â‰¤|R|Î´Ïµ
8
|R|Ïµ=Î´
8,
Pr
X2Râˆ©Bâ€²>|R|Ïµ
â‰¤EX2[|Râˆ©Bâ€²|]
|R|Ïµ
â‰¤|R|Î´Ïµ
8
|R|Ïµ=Î´
8.
Applying an analogous line of reasoning stating with xâˆˆB, we also have
Pr
X2Bâˆ©(X1\(Râ€²âˆ©Bâ€²)>|B|Ïµ
â‰¤Î´
8,
Pr
X2Bâˆ©Râ€²>|B|Ïµ
â‰¤Î´
8.
Applying a union bound, none of these events occur with probability at least 1âˆ’Î´
2overX2âˆ¼Âµnâ€².
Thus, it suffices to show that they algebraically imply the desired inequalities. To this end, suppose
none of them hold. Then we have,
râ€²=|Râ€²|
|X1|
=|Râ€²âˆ©B|+|Râ€²âˆ©G|+|Râ€²âˆ©R|
|X1|
â‰¤|B|Ïµ+|G|+|R|
|X1|
=bÏµ+g+r,
râ€²=|Râ€²|
|X1|
=|Râ€²âˆ©B|+|Râ€²âˆ©G|+|Râ€²âˆ©R|
|X1|
â‰¥0 + 0 + |R| âˆ’ |Râˆ©Bâ€²| âˆ’ |R\(Bâ€²âˆªRâ€²)|
|X1|
â‰¥|R| âˆ’ |R|Ïµâˆ’ |R|Ïµ
|X1|
=r(1âˆ’2Ïµ).
The upper and lower bounds on bâ€²are analogous.
25C Proof of Theorem 5.1
C.1 Definitions and Notation
Definition C.1. LetÎ±=1
3670016 d4andÎ²=1
3584d2.
Definition C.2. LetS1, S2, S3be three (dâˆ’1)-spheres centered at the origin with radii (1âˆ’Î±),1,
and1 +Î²respectively for 0< Î±, Î² .
Definition C.3. LetÂµdenote the data distribution so that xâˆ¼Âµis selected by first selecting
iâˆˆ {1,2,3}at uniform, and then selecting xfrom Siat uniform.
Definition C.4. Letfdenote the classifier Rdâ†’ {Â± 1}such that
f(x) =ï£±
ï£²
ï£³+1||x||2â‰¤1âˆ’Î±
2
âˆ’1 1âˆ’Î±
2<||x||2â‰¤1 +Î²
2
+1||x2||>1 +Î²
2.
Definition C.5. Letxâˆ—be an arbitrary point chosen on S3, and let gbe any linear classifier, and
B(a, r)be any L2-ball that contains xâˆ—.
Lemma C.6. There exists xâˆˆS2and0â‰¤Î¸1, Î¸2, Î¸3â‰¤Ï€such that
S1âˆ©B(a, r) =C(S1, x(1âˆ’Î±), Î¸1),
S2âˆ©B(a, r) =C(S2, x, Î¸ 2),
S3âˆ©B(a, r) =C(S3, x(1 +Î²), Î¸3),
where C(S, x, Î¸ )denotes the spherical cap of angle Î¸centered at xon(dâˆ’1)-sphere S(see
Definition C.18).
C.2 Main Proof
We begin by showing that the structure of the data distribution Âµprovides significant difficulty for
linear classifiers. At a high level, the curvature of the spheres, S1, S2, S3, make separating them
linearly only possible for small portions of the sphere. We formalize this with the following lemma.
Lemma C.7. LetÎ¸â‰¥Ï€
4. Letxbe an arbitrary point on S2, and let T1(x, Î¸), T3(x, Î¸)denote the sets
T1(x, Î¸) =C(S2, x, Î¸)âˆªC(S1, x(1âˆ’Î±), Î¸),
T3(x, Î¸) =C(S2, x, Î¸)âˆªC(S3, x(1 +Î²), Î¸).
Letg:Rdâ†’ {Â± 1}denote any linear classifier. Then gexhibits a loss of at least1
3over the
conditional distribution of Âµrestricted to either T1orT3. That is,
Pr
xâ€²âˆ¼Âµ[g(xâ€²)Ì¸=f(xâ€²)|xâ€²âˆˆT1(x, Î¸)],Pr
xâ€²âˆ¼Âµ[g(xâ€²)Ì¸=f(xâ€²)|xâ€²âˆˆT3(x, Î¸)]â‰¥1
3.
Next, we show that if the local explanation region B(a, r),contains a sufficiently large probability
mass, then it also must include a region that takes the form given by T1orT3from Lemma C.7.
Lemma C.8. Suppose that Âµ(B(a, r))â‰¥31âˆ’d. LetT1andT3be as defined in Lemma C.7. Then
there exist xâˆˆS2andÎ¸â‰¥Ï€
4such that at least one of the following hold:
â€¢T1(x, Î¸)âŠ†B(a, r), andÂµ(T1(x,Î¸))
Âµ(B(a,r))â‰¥1
2.
â€¢T3(x, Î¸)âŠ†B(a, r), andÂµ(T3(x,Î¸))
Âµ(B(a,r))â‰¥1
2.
We are now prepared to prove Theorem 5.1.
Proof. (Theorem 5.1) Suppose B(a, r)â‰¥31âˆ’d. Then by Lemma C.8, there exists Î¸â‰¥Ï€
4such that
either T1(x, Î¸)orT3(x, Î¸)is a subset of B(a, r)and satisfies the conditions outlined above. Suppose
thatT1(x, Î¸)âŠ†B(a, r)(the other case is analogous).
26Letgbe any linear classifier. Then it follows from Lemmas C.7 and C.8 that the loss gincurs over
the conditional distribution of ÂµoverB(a, r)can be bounded as follows:
Pr
zâˆ¼B(a,r)[g(z)Ì¸=f(z)]â‰¥Pr[zâˆˆT1(x, Î¸)] Pr[ g(z)Ì¸=f(z)|zâˆˆT1(x, Î¸)]
â‰¥1
21
3=1
6,
which concludes the proof.
C.3 Proof of Lemma C.7
We will show that the claim holds for T3(x, Î¸)as the proof for T1(x, Î¸)is nearly identical (as Î± < Î² ).
LetwâˆˆRdbe a unit vector and bâˆˆRbe a scalar such that
g(z) =1âŸ¨w, zâŸ© â‰¥b
âˆ’1âŸ¨w, zâŸ©< b.
Our main strategy will be to find a large set of points within T3(x, Î¸)such that g(z) =g(z(1 +Î²))
for all zwithin this set. This will force gto misclassify either zorz(1 +Î²)which will lead to our
desired error bound. To this end, define
Tâˆ—=n
zâˆˆC(S2, x, Î¸) :g(z) =âˆ’1, g(z(1 +Î²)) = +1 ,|âŸ¨x, zâŸ©| â‰¤cosÏ€
8o
.
Lemma C.9.Âµ(Tâˆ—)
Âµ(C(S2,x,Î¸))â‰¤1
10.
Proof. Letzbe selected at uniform from
C(S2, x, Î¸)\
C
S2, x,Ï€
8
âˆªC
S2,âˆ’x,Ï€
8
.
Note that zdefinitionally satisfies that |âŸ¨x, zâŸ©| â‰¤cosÏ€
8. It suffices to upper bound the probability
thatg(z)Ì¸=g(z(1 +Î²)). LetCÏ•={z:âŸ¨z, xâŸ©= cos Ï•}. Our main idea is to condition on zâˆˆCÏ•,
and then integrate over all choices of Ï•. That is, if we let Ï•denote the random variable representing
the angle between xandz, then
Pr
z[g(z) =âˆ’1, g(z(1 +Î²)) = +1] = EÏ•Pr
z|Ï•[g(z) =âˆ’1, g(z(1 +Î²)) = +1] .
We will now bound the latter quantity. Fix any Ï•, and observe that the conditional distribution, z|Ï•
can be written as
z=xcosÏ•+usinÏ•
where uis a random vector in Rdâˆ’1that is uniformly distributed over the unit sphere, Sdâˆ’2âŠ†Rdâˆ’1.
Rewriting the condition that g(z)Ì¸=g(z(1 +Î²))in terms of u, observe that
g(z) =âˆ’1, g(z(1 +Î²)) = +1 = â‡’ âŸ¨w, zâŸ© â‰¤bâ‰¤ âŸ¨w, z(1 +Î²)âŸ©
=â‡’b
1 +Î²â‰¤ âŸ¨w, zâŸ© â‰¤b
=â‡’b
1 +Î²âˆ’ âŸ¨xcosÏ•, wâŸ© â‰¤ âŸ¨w, usinÏ•âŸ© â‰¤bâˆ’ âŸ¨xcosÏ•, wâŸ©
=â‡’ âŸ¨w, uâŸ© âˆˆ
s, s+Î²
sinÏ•
,
where sis a constant that depends solely on b, w, x, andÏ•. Note that we are using the fact that
|b| â‰¤(1 +Î²)as otherwise gwould trivially output the same label over all zâˆ¼Âµ).
By applying Lemma C.17 along with the fact that (by definition of Ï•)
Î²
sinÏ•â‰¤Î²
sinÏ€
8â‰¤1
1370d2,
we have that
Pr
u
uâˆˆ
s, s+Î²
sinÏ•
â‰¤1
10,
which implies the desired result.
27Lemma C.10.Âµ(C(S2,x,Ï€
8)âˆªC(S2,âˆ’x,Ï€
8))
Âµ(C(S2,x,Î¸))â‰¤7
30.
Proof. By symmetry, Âµ 
C 
S2, x,Ï€
8
=Âµ 
C 
S2,âˆ’x,Ï€
8
so it suffices to bound one of them.
Since Î¸â‰¥Ï€
4by assumption, applying Lemma C.20, we have
Âµ 
C 
S2, x,Ï€
8
âˆªC 
S2,âˆ’x,Ï€
8
Âµ(C(S2, x, Î¸))â‰¤2Âµ 
C 
S2, x,Ï€
8
Âµ(C(S2, x, Î¸))
â‰¤2Âµ 
C 
S2, x,Ï€
8
Âµ 
C 
S2, x,Ï€
4
â‰¤21
2sinÏ€
8
sinÏ€
4dâˆ’2
â‰¤7
30,
asdâ‰¥5in the assumption of Theorem 5.1.
We are now prepared to prove the main lemma.
Proof. (Lemma C.7) Let Aâˆ—âŠ†C(S2, x, Î¸)be defined as the set of all points for which gclassifies
both the point and its image in (1 +Î²)S3correctly. That is,
Aâˆ—={zâˆˆC(S2, x, Î¸) :g(z) =âˆ’1, g((1 + Î²)z) = +1 }.
By the previous two lemmas, we have
Âµ(Aâˆ—)
Âµ(C(S2, x, Î¸))â‰¤Âµ(Tâˆ—âˆªC(S2, x,Ï€
8)âˆªC(S2,âˆ’x,Ï€
8))
Âµ(C(S2, x, Î¸))
â‰¤1
10+7
30=1
3
Each zâˆˆAâˆ—is a point for which both zand(1 + Î²)zare correctly classified, and each zâˆˆ
C(S2, x, Î¸)\Aâˆ—corresponds to either zbeing misclassified, or (1 + Î²)zbeing misclassified. It
follows that the overall accuracy of goverT3(x, Î¸)is at most
Pr
zâˆ¼T3(x,Î¸)[g(z) =f(z)]â‰¤ Pr
zâˆ¼C(S2,x,Î¸)[zâˆˆAâˆ—] +1
2Pr
zâˆ¼C(S2,x,Î¸)[z /âˆˆAâˆ—]
â‰¤1
2
1 + Pr
zâˆ¼C(S2,x,Î¸)[zâˆˆAâˆ—]
â‰¤2
3
Thus gmust incur loss at least1
3overT3(x, Î¸), as desired.
C.4 Proof of Lemma C.8
Throughout this section, we assume that Âµ(B(a, r))â‰¥31âˆ’d.
Lemma C.11. max( Î¸1, Î¸2, Î¸3)â‰¥Ï€
3.
Proof. Assume towards a contradiction that this does not hold. Let xbe as in Lemma C.6. Then by
the Definition of Âµ(Definition C.3) and Lemma C.6, it follows that
Âµ(B(a, r)) =Âµ(C(S1, x(1âˆ’Î±), Î¸1)) +Âµ(C(S2, x, Î¸ 2)) +Âµ(C(S3, x(1 +Î²), Î¸3))
=1
3(Î¨(Î¸1) + Î¨( Î¸2) + Î¨( Î¸3))
<Î¨Ï€
3
,
where Î¨is as defined in Section C.6. However, Lemma C.19 implies that Î¨ pi
3
â‰¤31âˆ’dÎ¨(Ï€) =
31âˆ’d. This contradicts our assumption on Âµ(B(a, r))and implies the desired result.
28Lemma C.12. râ‰¥1âˆ’Î±.
Proof. Lemma C.11 implies that B(a, r)must intersect some sphere among S1, S2, S3in a spherical
cap of an angle at leastÏ€
3. Basic geometry implies that râ‰¥min(rad(S1), rad(S2), rad(S3))where
rad(Si)denotes the radius of Si. The desired result follows from the fact that 1âˆ’Î±âˆ’rad(S1)â‰¤
rad(S2), rad(S3).
Lemma C.13. |Î¸2âˆ’max( Î¸1, Î¸3)| â‰¤1
4d.
Proof. We first compute Î¸1, Î¸2, Î¸3in terms of a, r, Î±, andÎ². We begin with Î¸2, and note that the
expressions for Î¸1andÎ¸3can be similarly derived. To this end, we have
S2âˆ©B(a, r) ={x:||x||= 1,||xâˆ’a|| â‰¤r}
=
x:||x||= 1,âŸ¨x, xâŸ© âˆ’2âŸ¨x, aâŸ©+âŸ¨a, aâŸ© â‰¤r2	
=
x:||x||= 1,x
||x||,a
||a||
â‰¥1 +a2âˆ’r2
2a
,
where we use ato denote ||a||in a slight abuse of notation.
It follows from Lemma C.6 that
cosÎ¸2=1 +a2âˆ’r2
2a.
We can similarly show that
cosÎ¸1=(1âˆ’Î±)2+a2âˆ’r2
2(1âˆ’Î±)a,cosÎ¸3=(1 +Î²)2+a2âˆ’r2
2(1 + Î²)a.
Leth:Râ†’Rbe the function defined as h(s) =s2+a2âˆ’r2
2sa. Thus,
cosÎ¸1=h(1âˆ’Î±),cosÎ¸2=h(1),cosÎ¸3=h(1 +Î²).
Note that in cases where his outside of the interval [âˆ’1,1](meaning Î¸iwould not be defined), we
simply set Î¸iequal to Ï€and0respectively, as these quantities still accurately describe the intersection
between B(a, r)and the corresponding sphere, Si.
Case 1: 0â‰¤aâ‰¤Î²
2By definition, B(a, r)contains xâˆ—and therefore intersects S3. It follows from the triangle inequality
thatrâ‰¥1 +Î²
2. However, this implies that B(a, r)must contain the entirety of S2andS1, which
implies that Î¸1=Î¸2= max( Î¸1, Î¸3) =Ï€, thus implying the lemma statement.
Case 2:Î²
2< aâ‰¤1âˆ’Î±
Ifr >1 + 2 Î², then B(a, r)will contain S1, S2andS3, which implies Î¸1=Î¸2=Î¸3=Ï€(implying
the lemma statement). Thus, assume râ‰¤1 + 2 Î².
Differentiating hw.r.t. sgives
hâ€²(s) =1
2a
1 +r2âˆ’a2
s2
.
29By Lemma C.12, r2â‰¥a2, which implies that hâ€²(s)is nonnegative for sâˆˆ[1âˆ’Î±,1+Î²]. Furthermore,
we have that over the interval, [1âˆ’Î±,1 +Î²],
hâ€²(s) =1
2a
1 +r2âˆ’a2
s2
â‰¤1
Î²ï£«
ï£¬ï£­1 +(1 + 2 Î²)2âˆ’
Î²
22
(1âˆ’Î±)2ï£¶
ï£·ï£¸
=1
Î²
1 +1 + 4 Î²+ 3.75Î²2
(1âˆ’Î±)2
â‰¤1
Î²
1 +1 + 4(0 .25) + 3 .75(0.25)2
0.8752
â‰¤4
Î².
This is obtained by substituting appropriate upper and lower bounds for r, a, s, Î±, andÎ². Because
hâ€²(s)is nonnegative over the interval, we must have that h(1âˆ’Î±)â‰¤h(1)â‰¤h(1 +Î²)which implies
Î¸1â‰¥Î¸2â‰¥Î¸3(ascosis a decreasing function). It follows from our upper bound on hâ€²(s)that
|cosÎ¸2âˆ’cos (max( Î¸1, Î¸3))|= cos( Î¸2)âˆ’cos(Î¸1)
=h(1)âˆ’h(1âˆ’Î±)
=Z1
1âˆ’Î±hâ€²(s)ds
â‰¤Z1
1âˆ’Î±4
Î²ds
=4Î±
Î².
Applying Lemma C.14 implies that |Î¸2âˆ’max( Î¸1, Î¸3)| â‰¤8q
Î±
Î²=1
4d, which implies the lemma
statement.
Case 3: a >1âˆ’Î±
First suppose that |aâˆ’r|>3. Ifr > a + 3, then the triangle inequality implies that S1, S2, S3âŠ†
B(a, r)which implies the desired result. On the other hand, if r < aâˆ’3, then we must have a >3,
and that B(a, r)is disjoint from S1, S2, S3which again implies the desired result. Thus, we assume
that|aâˆ’r| â‰¤3.
We now use a similar strategy to the previous case, and bound the derivative, hâ€²(s). By substituting
that|aâˆ’r| â‰¤3, we have, for sâˆˆ[1âˆ’Î±,1 +Î²],
|hâ€²(s)|=1
2a
1 +r2âˆ’a2
s2
=1
2a
1 +(2a+ 3)(3)
s2
=1
2a
1 +(2a+ 3)(3)
(1âˆ’Î±)2
=1
2a
1 +(2a+ 3)(3)
(0.875)2
â‰¤1
2a(1 + 4(2 a+ 3))
â‰¤1
2a(13 + 8 a)
â‰¤4 + 10 = 14 .
30Here we are exploiting the fact that 1âˆ’Î±â‰¥âˆš
3
2,0.65. It follows by the same argument given in Case
2 that|cosÎ¸2âˆ’cos(max( Î¸1, Î¸3))| â‰¤14Î². Applying Lemma C.14 implies |Î¸2âˆ’max( Î¸1, Î¸3)| â‰¤
4âˆš14Î²=1
4d, as desired.
Now we are ready to prove the lemma.
Proof. (Lemma C.8) Let xbe as in Lemma C.6, and let Î¸âˆ—= max( Î¸1, Î¸2, Î¸3). Then by applying
Lemma C.6 to the Definition of Âµ(Definition C.3) gives us
Âµ(B(a, r)) =Âµ(C(S1, x(1âˆ’Î±), Î¸1)) +Âµ(C(S2, x, Î¸ 2)) +Âµ(C(S3, x(1 +Î²), Î¸3))
=1
3Î¨(Î¸1) +1
3Î¨(Î¸2) +1
3Î¨(Î¸3)
â‰¤Î¨(Î¸âˆ—).
Here Î¨denotes the function defined in Section C.6.
Next, let Î¸= min(max( Î¸1, Î¸3), Î¸2). LetT1(x, Î¸)andT3(x, Î¸)be as defined in Lemma C.7. Observe
that if Î¸1â‰¥Î¸3, then
T1(x, Î¸)âŠ†C(S1, x(1âˆ’Î±), Î¸1)âˆªC(S2, x, Î¸ 2)âŠ†B(a, r),
and otherwise,
T3(x, Î¸)âŠ†C(S3, x(1 +Î²), Î¸3)âˆªC(S2, x, Î¸ 2)âŠ†B(a, r).
Thus, at least one of these sets is part of B(a, r). We now show that these sets have the desired mass.
By the definition of Î¸âˆ—, we have
Âµ(T1(x, Î¸))
Âµ(B(a, r)),Âµ(T3(x, Î¸))
Âµ(B(a, r))â‰¥2Âµ(C(S2, x, Î¸))
3Âµ(C(S2, x, Î¸âˆ—)).
Next, Lemma C.11 implies that Î¸âˆ—â‰¥Ï€
3, and Lemma C.13 implies that Î¸âˆ—âˆ’Î¸â‰¤1
4d. It follows that
Î¸â‰¥Î¸âˆ—âˆ’1
4dâ‰¥Î¸âˆ—
1âˆ’1
4d
.
Substituting this, we find that
2Âµ(C(S2, x, Î¸))
3Âµ(C(S2, x, Î¸âˆ—))=2
3Î¨(Î¸)
Î¨(Î¸âˆ—)
â‰¥2
3Î¨ 
Î¸âˆ— 
1âˆ’1
4d
Î¨(Î¸âˆ—)
â‰¥2
3
1âˆ’1
4ddâˆ’1
â‰¥2
31
e1/4
â‰¥1
2,
where the last steps follow from Lemmas C.19 and C.16. This completes the proof.
C.5 Technical Lemmas
Lemma C.14. Suppose Ï•1, Ï•2âˆˆ[0, Ï€]such that |cos(Ï•1)âˆ’cos(Ï•2)| â‰¤c. Then |Ï•1âˆ’Ï•2| â‰¤4âˆšc.
Proof. WLOG, suppose Ï•1â‰¤Ï•2. Letx=Ï•2âˆ’Ï•1.
31Using the sum to product rules, it follows that
Î±â‰¥ |cosÏ•1âˆ’cosÏ•2|
=âˆ’2 sinÏ•1âˆ’Ï•2
2sinÏ•1+Ï•2
2
â‰¥2 sinx
2sinÏ•1+Ï•2
2.
However, observe that Ï€âˆ’Ï•1+Ï•2
2â‰¥Ï•2âˆ’Ï•1+Ï•2
2=x
2and thatÏ•1+Ï•2
2â‰¥0+0+ x
2=x
2. It follows that
Ï•1+Ï•2
2âˆˆ[x
2, Ï€âˆ’x
2], which implies that sinÏ•1+Ï•2
2â‰¥sinx
2. Substituting this, we have
câ‰¥2 sinx
2sinÏ•1+Ï•2
2
â‰¥2 sin2x
2
We now do casework based on x. First suppose that xâ‰¥Ï€
2. Then câ‰¥2 sin2Ï€
4= 1. By definition,
xâ‰¤Ï€, so it follows that xâ‰¤4âˆšÎ±, implying the desired result.
Otherwise, if xâ‰¤Ï€
2, then sinx
2â‰¥x
4, as the function t7â†’sin(t)âˆ’t
2is nonnegative on the interval
[0,Ï€
2]. Substituting this, we see that câ‰¥x2
8. Thus xâ‰¤âˆš
8c <4âˆšc, as desired.
Lemma C.15. For0â‰¤câ‰¤1and0â‰¤Î¸â‰¤Ï€,sin(cÎ¸)â‰¥csin(Î¸).
Proof. Letf(Î¸) = sin( cÎ¸)âˆ’csin(Î¸). Observe that f(0) = 0 . Furthermore, for Î¸âˆˆ[0, Ï€], we
havefâ€²(Î¸) =ccos(cÎ¸)âˆ’ccos(Î¸) =c(cos(cÎ¸)âˆ’cos(Î¸)).Since cosis a decreasing function on the
interval [0, Ï€], it follows that cos(cÎ¸)â‰¥cos(Î¸), which implies fâ€²(Î¸)â‰¥0. Thus fis non-decreasing
on the interval, and the desired inequality holds.
Lemma C.16. For all x >1, 
1âˆ’1
xxâˆ’1â‰¥1
e.
Proof. Letf(x) = 
1âˆ’1
xxâˆ’1. It is well known that limxâ†’âˆžf(x) =1
eandlimxâ†’1+f(x) = 1 .
Thus it suffices to show that f(x)is a non-increasing function. To do so, we will show that lnf(x)is
non-increasing by taking its derivative. We have
d(lnf(x))
dx=d
dx
(xâˆ’1) lnxâˆ’1
x
=d
dx((xâˆ’1) ln( xâˆ’1)âˆ’(xâˆ’1) lnx)
=
ln(xâˆ’1) +xâˆ’1
xâˆ’1
âˆ’
ln(x) +xâˆ’1
x
=1
xâˆ’(ln(x)âˆ’ln(xâˆ’1))
=1
xâˆ’Zx
xâˆ’11
tdt
â‰¤1
xâˆ’Zx
xâˆ’11
xdt
=1
xâˆ’1
x= 0.
Lemma C.17. Letzbe a point chosen at uniform over S2, and let wbe a fixed unit vector. Then if
tâ‰¤1
1370d2, then for any sâˆˆR,
Pr
z[âŸ¨w, zâŸ© âˆˆ[s, s+t]]â‰¤1
10.
32Proof. LetÎ¸denote the random variable that represents the angle between wandz. Applying Lemma
C.14, it follows that for some choice of sâ€²âˆˆRthat
Pr
z[âŸ¨w, zâŸ© âˆˆ[s, s+t]â‰¤Pr
Î¸[Î¸âˆˆ[sâ€², sâ€²+ 4âˆš
t].
We now bound this quantity by utilizing the quantity Î¨(defined in Section C.6). We have,
Pr
Î¸[Î¸âˆˆ[sâ€², sâ€²+ 4âˆš
t] =Rsâ€²+4âˆš
t
sâ€² sin(dâˆ’2)Ï•dÏ•RÏ€
0sin(dâˆ’2)Ï•dÏ•
â‰¤2RÏ€
2
Ï€
2âˆ’2âˆš
tsin(dâˆ’2)Ï•dÏ•
2RÏ€
2
0sin(dâˆ’2)Ï•dÏ•
= 1âˆ’Î¨ Ï€
2âˆ’2âˆš
t
Î¨ Ï€
2.
Here we have simply chosen the interval of length 4âˆš
tthat maximizes the corresponding the integral.
Next, we continue by applying Lemmas C.19 and C.16 to get
Pr
Î¸[Î¸âˆˆ[sâ€², sâ€²+ 4âˆš
t]â‰¤1âˆ’Î¨ Ï€
2âˆ’2âˆš
t
Î¨ Ï€
2
â‰¤1âˆ’
1âˆ’4âˆš
t
Ï€dâˆ’1
â‰¤1âˆ’
1âˆ’1
29ddâˆ’1
= 1âˆ’ 
1âˆ’1
29d29(dâˆ’1)!1/29
â‰¤1âˆ’ 
1âˆ’1
29d29dâˆ’1!1/29
â‰¤1âˆ’1
e1/29
â‰¤1
10,
as desired.
C.6 Spherical Caps
Definition C.18. LetSbe a(dâˆ’1)sphere centered at the origin, let 0â‰¤Î¸â‰¤Ï€be an angle, and
letxâˆˆSbe a point. We let C(S, x, Î¸ )denote the spherical cap with angle Î¸centered at x, and it
consists of all points, xâ€²âˆˆSdâˆ’1, such thatâŸ¨x,xâ€²âŸ©
||x||||xâ€²||â‰¥cosÏ•.
Here we take the convention of associating C(Si, xi,0)with both the empty set and with {xi}. While
these are distinct sets, they both have measure 0under Ï€. We also associate C(Si, xi, Ï€)with the
entirety of Si.
We let Î¨(Î¸)denote the ratio of the (dâˆ’1)-surface area of the region, C(S, x, Î¸ ), to the (dâˆ’1)-surface
area of the entire sphere. Thus, Î¨(Î¸)denotes the fraction of the sphere covered by a spherical cap of
angle Î¸. By standard integration over spherical coordinates, we have
Î¨(Î¸) =RÎ¸
0sin(dâˆ’2)Ï•dÏ•RÏ€
0sin(dâˆ’2)Ï•dÏ•.
Next, we bound Î¨(Î¸)with the following inequality.
33Lemma C.19. Let0â‰¤Î¸â‰¤Ï€and let 0â‰¤câ‰¤1. Then
Î¨(cÎ¸)
Î¨(Î¸)â‰¥cdâˆ’1.
Proof. By applying Lemma C.15 to the definition of Î¨, we have the following manipulations.
Î¨(cÏ•)
Î¨(Ï•)=RcÏ•
0sindâˆ’2Î¸dÎ¸
RÏ•
0sindâˆ’2Î¸dÎ¸
=RÏ•
0sindâˆ’2(cu)(cdu)
RÏ•
0sindâˆ’2Î¸dÎ¸
â‰¥RÏ•
0(csin(u))dâˆ’2(cdu)
RÏ•
0sindâˆ’2Î¸dÎ¸
=cdâˆ’1.
We similarly have an upper bound on this ratio.
Lemma C.20. Let0â‰¤Î¸â‰¤Ï€
2and0â‰¤câ‰¤1. Then
Î¨(cÎ¸)
Î¨(Î¸)â‰¤csincÏ•
sinÏ•dâˆ’2
.
Proof. We similarly have,
Î¨(cÏ•)
Î¨(Ï•)=RcÏ•
0sindâˆ’2Î¸dÎ¸
RÏ•
0sindâˆ’2Î¸dÎ¸
=RÏ•
0sindâˆ’2(cu)(cdu)
RÏ•
0sindâˆ’2Î¸dÎ¸
â‰¤RÏ•
0
sin(u)sincÏ•
sinÏ•dâˆ’2
(cdu)
RÏ•
0sindâˆ’2Î¸dÎ¸
=csincÏ•
sinÏ•dâˆ’2
.
Here we are using the fact that t7â†’sinct
sintis a non-decreasing function for tâˆˆ[0, Ï€].
34NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: we mention all main results in both the abstract and introduction. Furthermore
everything within these sections is revisited in the body.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: all our assumptions are clearly stated, and we use the limitations of our methods
as avenues for future work. We also stress that our framework is by no means comprehensive
for evaluating local explanations.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
35Answer: [Yes]
Justification: this is a theory paper, and doing so is its main focus. our proofs are all included
in the appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: This is a theory paper.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
36Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: This is a theory paper.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: this is a theory paper
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: this is a theory paper
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
37â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: This is a theory paper
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: this paper does not utilize any datasets or human subjects. It also does not
contribute towards any sort of harm.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: we discuss consequences of our results. Because they are theoretical, we donâ€™t
believe our results can be used in a directly harmful manner.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
38â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: no models or data are in this paper.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: no code or models are used.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
39â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: No released assets
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: no human subjects
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: No human subjects
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
40