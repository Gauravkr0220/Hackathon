Fine-grained Analysis of In-context Linear Estimation:
Data, Architecture, and Beyond
Yingcong Li
University of Michigan
yingcong@umich.eduAnkit Singh Rawat
Google Research NYC
ankitsrawat@google.comSamet Oymak
University of Michigan
oymak@umich.edu
Abstract
Recent research has shown that Transformers with linear attention are capable
of in-context learning (ICL) by implementing a linear estimator through gradient
descent steps. However, the existing results on the optimization landscape apply
under stylized settings where task and feature vectors are assumed to be IID and
the attention weights are fully parameterized. In this work, we develop a stronger
characterization of the optimization and generalization landscape of ICL through
contributions on architectures, low-rank parameterization, and correlated designs:
(1) We study the landscape of 1-layer linear attention and 1-layer H3, a state-
space model. Under a suitable correlated design assumption, we prove that both
implement 1-step preconditioned gradient descent. We show that thanks to its native
convolution filters, H3 also has the advantage of implementing sample weighting
and outperforming linear attention in suitable settings. (2) By studying correlated
designs, we provide new risk bounds for retrieval augmented generation (RAG)
and task-feature alignment which reveal how ICL sample complexity benefits from
distributional alignment. (3) We derive the optimal risk for low-rank parameterized
attention weights in terms of covariance spectrum. Through this, we also shed light
on how LoRA can adapt to a new distribution by capturing the shift between task
covariances. Experimental results corroborate our theoretical findings. Overall, this
work explores the optimization and risk landscape of ICL in practically meaningful
settings and contributes to a more thorough understanding of its mechanics.
a) Correlated featuresb) Task-feature alignment:Task and feature vectors areğ›¼-correlatedQueryfeatures
DatabaseRelevantexamplesEx:Retrieval Augmented GenerationATTN(ğ‘„ğ¾!ğ‘‰)Linear AttentionH3 (SSM)Ã—SSMÃ—Distributional AlignmentArchitecture ChoiceLow-rank ParameterizationPretrained Weightsrank ğ‘Ÿ+ğ‘Š!ğ‘Š"a) Low-rank attention weights: rankğ‘Š!,ğ‘Š"â‰¤ğ‘Ÿb) LoRAadaptation
0 10 20 30 40 50
# in-context samples0.30.40.50.60.70.80.9T est risk
Linear Att
H3
Theorem 1
(a) Linear attention =H3
0 10 20 30 40 50
# in-context samples0.20.40.60.8T est risk
=0
=0.2
=0.4
=0.6
 (b) RAG with Î±correlation
0 10 20 30 40 50
# in-context samples0.10.30.50.70.9T est risk
r=1
r=5
r=10
r=20 (c) LoRA
Figure 1: We investigate the optimization landscape of in-context learning from the lens of archi-
tecture choice, the role of distributional alignment, and low-rank parameterization. The empirical
performance (solid curves) are aligned with our theoretical results (dotted curves) from Section 3.
More experimental details and discussion are deferred to Section 4.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).1 Introduction
Modern language models exhibit the remarkable ability to learn novel tasks or solve complex problems
from the demonstrations provided within their context window [Brown et al., 2020, GeminiTeam
et al., 2023, OpenAI, 2023, Touvron et al., 2023]. Such in-context learning (ICL) offers a novel and
effective alternative to traditional fine-tuning techniques and has become an important feature of LLM
with its applications spanning retrieval-augmented generation [Lewis et al., 2020], and reasoning via
advanced prompting techniques, such as chain-of-thought [Wei et al., 2022].
ICL ability presents an important research avenue to develop stronger theoretical and mechanistic
understanding of large language models. To this aim, there has been significant recent interest in
demystifying ICL through the lens of function approximation [Liu et al., 2023a], Bayesian inference
[MÃ¼ller et al., 2021, Xie et al., 2022, Han et al., 2023], and learning and optimization theory [Ahn
et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Duraisamy, 2024]. The latter is concerned
with understanding the optimization landscape of ICL, which is also crucial for understanding the
generalization properties of the model. A notable result in this direction is the observation that
linear attention models [Schlag et al., 2021, V on Oswald et al., 2023, Ahn et al., 2023] implement
preconditioned gradient descent (PGD) during ICL [Ahn et al., 2023, Mahdavi et al., 2024]. While
this line of works provide a fresh perspective to ICL, the existing studies do not address many
questions arising from real-life applications nor provide guiding principles for various ICL setups
motivated by practical considerations.
To this aim, we revisit the theoretical exploration of ICL with linear data model where we feed an
in-context prompt containing nexamples (xi,yi=xâŠ¤
iÎ²+Î¾i)n
i=1âŠ‚RdÃ—Rand a test instance or query
xn+1âˆˆRdto the model, with dbeing the feature dimension, Î²âˆˆRdbeing the task weight vector,
and(Î¾i)n
i=1denoting the noise in individual labels. Given the in-context prompt, the model is tasked
to predict Ë†yn+1â€“ an estimate for yn+1=xâŠ¤
n+1Î²+Î¾n+1. We aim to provide answers to the following
questions by exploring the loss landscape of ICL:
(Q1) Is the ability to implement gradient-based ICL unique to (linear) attention? Can alternative
sequence models implement richer algorithms beyond PGD?
(Q2) In language modeling, ICL often works well with few-shot samples whereas standard linear
estimation typically requires O(d)samples. How can we reconcile this discrepancy between
classical learning and ICL?
(Q3) To our knowledge, existing works assume linear-attention is fully parameterized, i.e., key and
query projections Wk,WqâˆˆRdÃ—d. What happens when they are low-rank? What happens when
there is distribution shift between training and test in-context prompts and we use LoRA [Hu
et al., 2022] for adaptation?
In this work, we conduct a careful investigation of these questions. Specifically, we focus on ICL
with 1-layer models and make the following contributions:
(A1) We jointly investigate the landscape of linear attention and H3 [Fu et al., 2023], a widely popu-
lar state-space model (SSM). We prove that under correlated design, both models implement
1-step PGD (c.f. Proposition 1) and the alignments in Fig. 1a verify that where the dotted
curve represents the theoretical PGD result derived from Theorem 1. Our analysis reveals
that the gating mechanism in H3 imitates attention. We also empirically show that H3 has the
advantage of implementing sample-weighting which allows it to outperform linear attention in
temporally-heterogeneous problem settings in Appendix D.
(A2) Proposition 1 allows for task and features to be correlated to each other as long as odd moments
are zero. Through this, we can assess the impact of distributional alignment on the sample
complexity of ICL. Specifically, we characterize the performance of Retrieval Augmented
Generation (RAG) (c.f. Theorem 2 and Fig. 1b) and Task-Feature Alignment (c.f. Theorem 3),
where the in-context examples are Î±-correlated with either the query or the task vector. For
both settings, we prove that alignment amplifies the effective sample size of ICL by a factor of
Î±2d+1, highlighting that aligned data are crucial for the success of ICL in few-shot settings.
(A3) We show that, under low-rank parameterization, optimal attention-weights still implements
PGD according to the truncated eigenspectrum of the fused task-feature covariance (see
Section 3.2). We similarly derive risk upper bounds for LoRA adaptation (c.f. Eq. (14) and
Fig. 1c), and show that, these bounds accurately predict the empirical performance.
22 Problem Setup and Preliminaries
We begin with a short note on notation. Let bold lowercase and uppercase letters (e.g., xandX)
represent vectors and matrices, respectively. The symbol âŠ™is defined as the element-wise (Hadamard)
product, andâˆ—denotes the convolution operator. 1dand0ddenote the d-dimensional all-ones and
all-zeros vectors, respectively; and Iddenotes the identity matrix of dimension dÃ—d. Additionally,
lettr(W)denote the trace of the square matrix W.
As mentioned earlier, we study the optimization landscapes of 1-layer linear attention [Katharopoulos
et al., 2020, Schlag et al., 2021] and H3 [Fu et al., 2023] models when training with prompts
containing in-context data following a linear model. We construct the input in-context prompt similar
to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024] as follows.
Linear data distribution. Let(x,y)âˆˆRdÃ—Rbe a (feature, label) pair generated by a d-dimensional
linear model parameterized by Î²âˆˆRd, i.e., y=xâŠ¤Î²+Î¾, where xandÎ²are feature and task vectors,
andÎ¾is the label noise. Given demonstrations (xi,yi)n+1
i=1sampled from a single Î², define the input
in-context prompt
Z=[z1...znzn+1]âŠ¤="
x1... xnxn+1
y1... yn 0#âŠ¤
âˆˆR(n+1)Ã—(d+1). (1)
Here, we set zi="
xi
yi#
foriâ‰¤nand the last/query token zn+1="
xn+1
0#
. Then, given Z, the goal of
the model is to predict the correct label yn+1corresponding to xn+1. For cleaner notation, when it
is clear from context, we drop the subscript n+1and set x=xn+1,z=zn+1. Different from the
previous work [Ahn et al., 2023, Mahankali et al., 2024, Zhang et al., 2024, Mahdavi et al., 2024]
where (xi)n+1
i=1andÎ²are assumed to be independent, our analysis focuses on a more general linear
setting that captures the dependency between (xi)n+1
i=1andÎ².
Model architectures. To start with, we first review the architectures of both Transformer and
state-space model (SSM). Similar to the previous work [V on Oswald et al., 2023, Ahn et al., 2023,
Mahankali et al., 2024, Zhang et al., 2024] and to simplify the model structure, we focus on single-
layer models and omit the nonlinearity, e.g., softmax operation and MLP activation, from the
Transformer. Given the input prompt ZâˆˆR(n+1)Ã—(d+1)in(1), which can be treated as a sequence of
(d+1)-dimensional tokens, the single-layer linear attention ATTand H3-like single-layer SSM SSM
are denoted by
ATT(Z)=(ZW qWâŠ¤
kZâŠ¤)ZW v (2a)
SSM(Z)=
(ZW q)âŠ™((ZW kâŠ™ZW v)âˆ—f)
(2b)
where Wk,Wq,WvâˆˆR(d+1)Ã—(d+1)denote the key, query and value weight matrices, respectively.
In(2b), the parameter fâˆˆRn+1is a 1-D convolutional filter that mixes tokens. The Hadamard
productâŠ™is the gating mechanism [Dauphin et al., 2017] between key and query channels, which is
crucial for attention-like feature creation. Thus, (2b) is more generally a gated-convolution layer. For
fonly, we use indexing f=[f0...fn]âŠ¤âˆˆRn+1and given any vector a, denote convolution output
(aâˆ—f)i=Pi
j=1fiâˆ’jaj. Note that our notation slightly differs from the original H3 model [Fu et al.,
2023] in two ways:
1.SSMs provide efficient parameterization of fwhich would otherwise grow with sequence length.
In essence, H3 utilizes a linear state-space model si=Asiâˆ’1+Buiandyi=Csiwith parameters
(AâˆˆRdÃ—d,BâˆˆRdÃ—1,CâˆˆR1Ã—d)from which the filter fis obtained via the impulse response
fi=CAiBforiâ‰¥0. Here dis the state dimension and, in practice, Ais chosen to be diagonal.
Observe that, setting d=1andA=Ï,C=B=1, SSM reduces to the exponential smoothing
fi=Ïiforiâ‰¥0. Thus, H3 also captures the all-ones filter as a special instance. As we show in
Proposition 1, this simple filter is optimal under independent data model and exactly imitates
linear attention. Note that, utilizing a filter fas in (2b) is strictly more expressive than the SSM
as it captures all possible impulse responses.
2.H3 also applies a shift SSM to the key embeddings to enable the retrieval of the local context
around associative recall hits. We opted not to incorporate this shift operator in our model. This
is because unless the features of the neighboring tokens are correlated (which is not the case for
3the typical independent data model), the entry-wise products between values and shifted keys
will have zero mean and be redundant for the final prediction.
We note that we conduct all empirical evaluations with the original H3 model, which displays exact
agreement with our theory formalized for (6b), further validating our modeling choice.
2.1 In-context Linear Estimation
We will next study the algorithms that can be implemented by the single-layer attention and state-
space models. Through this, we will show that training ATTandSSMwith linear ICL data is equivalent
to the prediction obtained from one step of optimally- preconditioned gradient descent (PGD) and
sample-weighted preconditioned gradient descent (WPGD), respectively. We will further show that
under mild assumption, the optimal sample weighting for SSM(e.g., f) is an all-ones vector and
therefore, establishing the equivalence among PGD, ATT, and SSM.
Background: 1-step gradient descent. Consider minimizing squared loss and solving linear
regression using one step of PGD and WPGD. Given nsamples (xi,yi)n
i=1, define
X=[x1Â·Â·Â·xn]âŠ¤âˆˆRnÃ—dand y=[y1Â·Â·Â·yn]âŠ¤âˆˆRn.
Starting from Î²0=0dand letting Î·=1/2be the step size, a single-step GD preconditioned with
weights Wreturns prediction
Ë†y=xâŠ¤WXâŠ¤y:=gPGD(Z), (3)
and a single-step sample-weighted GD given weights Ï‰âˆˆRnandWâˆˆRdÃ—dreturns prediction
Ë†y=xâŠ¤WXâŠ¤(Ï‰âŠ™y) :=gWPGD(Z), (4)
where Zis defined in (1)consisting of X,yandx. Our goal is to find the optimal W, as well asÏ‰in
(4) that minimize the population risks defined as follows.
min
WLPGD(W)whereLPGD(W)=Eh
(yâˆ’gPGD(Z))2i
, (5a)
min
W,Ï‰LWPGD(W)whereLWPGD(W)=Eh
(yâˆ’gWPGD(Z))2i
. (5b)
Here, the expectation is over the randomness in (xi,Î¾i)n+1
i=1andÎ², and we useWto represent the set
of corresponding trainable parameters. The search spaces for Ï‰andWareRnandRdÃ—d, respectively.
As per (2), given input prompt ZâˆˆR(n+1)Ã—(d+1), either of the underlying models outputs a (n+1)-length
sequence. Note that the label for the query x=xn+1is excluded from the prompt Z. Similar to Ahn
et al. [2023], Mahankali et al. [2024], we consider a training objective with a causal mask to ensure
inputs cannot attend to their own labels and training can be parallelized. Let Z0=[z1...zn0]âŠ¤be
the features post-causal masking at time/index n+1. Given weights Wk,Wq,Wvand the filter ffor
SSM, predictions at the query token z="
x
0#
take the following forms following sequence-to-sequence
mappings in (2):
gATT(Z)=(zâŠ¤WqWâŠ¤
kZâŠ¤
0)Z0Wvv,
gSSM(Z)=
(zâŠ¤Wq)âŠ¤âŠ™((Z0WkâŠ™Z0Wv)âˆ—f)n+1
v,
where vâˆˆRd+1is the linear prediction head and ((Z0WkâŠ™Z0Wv)âˆ—f)n+1returns the last row of the
convolution output. Note that SSM can implement the mask by setting f0=0. Now consider the meta
learning setting and select loss function to be the squared loss, same as in (5). Thus, the objectives
for both models take the following forms.
min
Wk,Wq,Wv,vLATT(W)whereLATT(W)=Eh
(yâˆ’gATT(Z))2i
, (6a)
min
Wk,Wq,Wv,v,fLSSM(W)whereLSSM(W)=Eh
(yâˆ’gSSM(Z))2i
. (6b)
Here, similarly, the expectation subsumes the randomness of (xi,Î¾i)n+1
i=1andÎ²andWrepresents the
set of trainable parameters. The search space for matrices Wk,Wq,WvisR(d+1)Ã—(d+1), for head vis
Rd+1, and for fisRn+1.
4Note that for all the optimization methods (c.f. (5),(6)), to simplify the analysis, we train the models
without capturing additional bias terms. Therefore, in the following, we introduce the centralized
data assumptions such that the models are trained to make unbiased predictions.
To begin with, a cross moment of random variables is defined as the expectation of a monomial of
these variables, with the order of the cross moment being the same as order of the monomial. For
example, E[xâŠ¤WÎ²]is a sum of cross-moments of order 2. Then, it motivates the following data
assumptions.
Assumption 1 All cross moments of the entries of (xi)n+1
i=1andÎ²with odd orders are zero.
Assumption 2 The label noise (Î¾i)n+1
i=1are independent of (xi)n+1
i=1andÎ², and their cross moments
with odd orders are zero.
Note that compared to Ahn et al. [2023], Mahankali et al. [2024], Zhang et al. [2024], Assumption 1 is
more general which also subsumes the dependent distribution settings. In this work, we consider the
following three linear models (omitting noise) satisfying Assumption 1. Let Î£Î²,Î£xâˆˆRdÃ—drepresent
the task and feature covariance matrices for independent data, and let 0â‰¤Î±â‰¤1be the correlation
level when considering data dependency. More specific discussions are deferred to Section 3.
â€¢Independent task and data: Î²âˆ¼N(0,Î£Î²),xiâˆ¼N(0,Î£x),for all 1â‰¤iâ‰¤n+1.
â€¢Retrieval augmented generation: Î²,xâˆ¼N(0,Id),xixâˆ¼N(Î±x,(1âˆ’Î±2)Id),for all 1â‰¤iâ‰¤n.
â€¢Task-feature alignment: Î²âˆ¼N(0,Id),xiÎ²âˆ¼N(Î±Î²,Id),for all 1â‰¤iâ‰¤n+1.
Next, we introduce the following result which establishes the equivalence among optimizing 1-layer
linear attention (c.f. (6a)), 1-layer H3 (c.f. (6b)), and 1-step gradient descent (c.f. (5)).
Proposition 1 Suppose Assumptions 1 and 2 hold. Consider the objectives as defined in (5)and(6),
and letLâ‹†
PGD,Lâ‹†
WPGD,Lâ‹†
ATT, andLâ‹†
SSMbe their optimal risks, respectively. Then,
Lâ‹†
PGD=Lâ‹†
ATT andLâ‹†
WPGD=Lâ‹†
SSM.
Additionally, if the examples (xi,yi)n
i=1follow the same distribution and are conditionally independent
given x,Î², then SSM/H3 can achieve the optimal loss using the all-ones filter and Lâ‹†
PGD=Lâ‹†
SSM.
We defer the proof to Appendix A.1. Proposition 1 establishes that analyzing the optimization
landscape of ICL for both single-layer linear attention and the H3 model can be effectively reduced
to examining the behavior of a one-step PGD algorithm. Notably, under the independent, RAG and
task-feature alignment data settings discussed above, examples (xi,yi)n
i=1are independently sampled
given xandÎ², and we therefore conclude that Lâ‹†
PGD=Lâ‹†
ATT=Lâ‹†
SSM. Leveraging this result, the
subsequent section of the paper concentrate on addressing (5a), taking into account various linear
data distributions.
While Proposition 1 demonstrates the equivalence of optimal losses, we also study the uniqueness
and equivalence of optimal prediction functions. To this end, we analyze the strong convexity of
LPGD(W)and derive the subsequent lemmas.
Lemma 1 Suppose Assumption 2 holds and let Î¾=[Î¾1Î¾2Â·Â·Â·Î¾n]âŠ¤. Then the lossLPGD(W)in(5a)
is strongly-convex if and only if E[(xâŠ¤WXâŠ¤XÎ²)2]+E[(xâŠ¤WXâŠ¤Î¾)2]is strongly-convex. Additionally,
letgâ‹†
PGD,gâ‹†
ATTbe the optimal prediction functions of (5a) and(6a). Then under the conditions of
Assumptions 1 and 2, and the strong convexity, gâ‹†
PGD=gâ‹†
ATT.
Lemma 2 Suppose that the label noise (Î¾i)n
i=1are i.i.d., zero-mean, variance Ïƒ2and independent of
everything else, and that there is a decomposition x=x1+x2,X=X1+X2, andÎ²=Î²1+Î²2such
that either of the following holds
â€¢Ïƒ>0, and (x1,X1)have full rank covariance and are independent of each other and (x2,X2).
â€¢(x1,Î²1,X1)have full rank covariance and are independent of each other and (x2,Î²2,X2).
Then, the lossLPGD(W)in(5a)is strongly-convex.
5As mentioned above, in this work, we study three specific linear models: with general independent,
RAG-related, and task-feature alignment data. Note that for all the three cases, according to Proposi-
tion 1, we haveLâ‹†
PGD=Lâ‹†
ATT=Lâ‹†
SSM. Additionally, the second claim in Lemma 2 holds, and LPGD(W)
is strongly convex. Therefore, following Lemma 1, we have gâ‹†
PGD=gâ‹†
ATT. Thanks to the equivalence
among PGD, ATT, and SSM, in the next section, we focus on the solution of objective (5a) under
different scenarios, which will reflect the optimization landscapes of ATTandSSMmodels.
3 Main Results
In light of Proposition 1, optimizing a single layer linear-attention or H3 model is equivalent to
solving the objective (5a). Therefore, in this section, we examine the properties of the one-step PGD
in(5a). To this end, we consider multiple problem settings, including distinct data distributions
and low-rank training. The latter refers to the scenario where the key and query matrices have
rank restrictions, e.g., Wk,WqâˆˆR(d+1)Ã—r, as well as LoRA-tuning when adapting the model under
distribution shift.
3.1 Analysis of Linear Data Models
We first consider the standard independent data setting. We will then examine correlated designs.
Independent data model. LetÎ£xandÎ£Î²be the covariance matrices of the input feature and task
vectors, respectively, and Ïƒâ‰¥0be the noise level. We assume
Î²âˆ¼N(0,Î£Î²),xiâˆ¼N(0,Î£x), Î¾ iâˆ¼N(0,Ïƒ2),1â‰¤iâ‰¤n+1 (7)
and the label is obtained via yi=xâŠ¤
iÎ²+Î¾i. Our following result characterizes the optimal solution of
(5a). Note that the data generated from (7)satisfies the conditions in Proposition 1. Therefore, the
same results can be applied to both linear-attention and H3 models.
Theorem 1 Consider independent linear data as defined in (7), and suppose the covariance matrices
Î£x,Î£Î²are full rank. Recap the objective from (5a) and let Wâ‹†:=arg min WLPGD(W), andLâ‹†=
LPGD(Wâ‹†). Additionally, let Î£=Î£1/2
xÎ£Î²Î£1/2
xandM=tr(Î£)+Ïƒ2. Then Wâ‹†andLâ‹†satisfy
Wâ‹†=Î£âˆ’1/2
xÂ¯Wâ‹†Î£âˆ’1/2
x andLâ‹†=Mâˆ’ntr
Î£Â¯Wâ‹†
, (8)
where we define Â¯Wâ‹†=
(n+1)Id+MÎ£âˆ’1âˆ’1.
Corollary 1 Consider noiseless i.i.d. linear data where Î£x=Î£Î²=IdandÏƒ=0. Then, the objective
in(5a)returns
Wâ‹†=1
n+d+1Id andLâ‹†=dâˆ’nd
n+d+1.
See Appendix B.2 for proofs. Note that Theorem 1 is consistent with prior work [Ahn et al., 2023,
Theorem 1] when specialized to isotropic task covariance, i.e., Î£Î²=Id. However, their result is
limited as the features and task are assumed to be independent. This prompts us to ask: What is the
optimization landscape with correlated in-context samples? Toward this, we consider the following
RAG-inspired and task-feature alignment models, where Assumptions 1 and 2 continue to hold and
Proposition 1 applies.
Retrieval augmented generation. To provide a statistical model of the practical RAG approaches,
given the query vector xn+1=x, we propose to draw ICL demonstrations that are similar to xwith
the same shared task vector Î². Modeling feature similarity through the cosine angle, RAG should
sample the ICL examples xi,iâ‰¤n, from the original feature distribution conditioned on the event
cos(xi,x)â‰¥Î±whereÎ±is the similarity threshold. As an approximate proxy, under the Gaussian
distribution model, we assume that Î²âˆ¼N(0,Id),xâˆ¼N(0,Id)and that RAG samples Î±-correlated
demonstrations (xi,yi)n
i=1as follows:
xixâˆ¼N(Î±x,(1âˆ’Î±2)Id), Î¾ iâˆ¼N(0,Ïƒ2)and yi=xâŠ¤
iÎ²+Î¾i,1â‰¤iâ‰¤n. (9)
Note that the above normalization ensures that the marginal feature distribution remains N(0,Id).
The full analysis of RAG is provides in Appendix B.3. Specifically, when we carry out the analysis
by assuming Î±=O
1/âˆš
d
andd/n=O(1)whereO(Â·)denotes proportionality, our derivation leads
to the following result:
6Theorem 2 Consider linear model as defined in (9). Recap the objective from (5a)and let Wâ‹†:=
arg min WLPGD(W), andLâ‹†=LPGD(Wâ‹†). Additionally, let Îº=Î±2d+1and suppose Î±=O
1/âˆš
d
,
d/n=O(1)anddis sufficiently large. Then Wâ‹†andLâ‹†have approximate forms
Wâ‹†â‰ˆ1
Îºn+d+Ïƒ2Id andLâ‹†â‰ˆd+Ïƒ2âˆ’Îºnd
Îºn+d+Ïƒ2. (10)
Here, (10) is reminiscent of Corollary 1 and has a surprisingly clean message. Observe that, Î±2d+1
is the dominant multiplier ahead of nin both equations. Thus, we deduce that, RAG model follows
the same error bound as the independent data model, however, its sample size is amplified by a factor
ofÎ±2d+1.Î±=0reduces to the result of Corollary 1 whereas we need to set Î±=O
1/âˆš
d
for
constant amplification. When Î±=1, RAG achieves the approximate risk Lâ‹†â‰ˆ2+Ïƒ2, where the
constant bias is due to the higher order moments (e.g., the 4â€™th and 6â€™th moments) of the standard
Gaussian distribution. As dincreases, the normalized loss Lâ‹†/dâ†’0. The full analysis of its optimal
solution Wâ‹†and lossLâ‹†are deferred Theorem 4 in Appendix B.3.
Task-feature alignment. We also consider another dependent data setting where task and feature
vectors are assumed to be correlated. This dataset model has the following motivation: In general, an
LLM can generate any token within the vocabulary. However, once we specify the task (e.g. domain of
the prompt), the LLM output becomes more deterministic and there are much fewer token candidates.
For instance, if the task is â€œCountryâ€, â€œFranceâ€ is a viable output compared to â€œHeliumâ€ and vice
versa when the task is â€œChemistryâ€. Formally speaking, this can be formalized as the input xhaving
a diverse distribution whereas it becomes more predictable conditioned on Î². Therefore, it can be
captured through a linear model by making the conditional covariance of xÎ²to be approximately
low-rank. This formalism can be viewed as a spectral alignment between input and task, which is
also well-established in deep learning both empirically and theoretically [Li et al., 2020, Arora et al.,
2019, Canatar et al., 2021, Cao et al., 2019]. Here, we consider such a setting where the shared task
vector is sampled as standard Gaussian distribution Î²âˆ¼N(0,Id)and lettingÎº=Î±2d+1, we sample
theÎ±-correlated ICL demonstrations (xi,yi)n+1
i=1as follows:
xiÎ²âˆ¼N(Î±Î²,Id), Î¾ iâˆ¼N(0,Ïƒ2)and yi=Îºâˆ’1/2xâŠ¤
iÎ²+Î¾i,1â‰¤iâ‰¤n+1. (11)
Above,Îºâˆ’1/2is a normalization factor to ensure that label variance remains invariant to Î±. To keep
the exposition cleaner, we defer the full analysis of its optimal solution Wâ‹†and lossLâ‹†to Theorem 5
in Appendix B.4. Similar to the RAG setting, by assuming Î±=O
1/âˆš
d
andd/n=O(1), we obtain
the following results for the optimal parameter and risk.
Theorem 3 Consider linear model as defined in (11). Recap the objective from (5a)and let Wâ‹†:=
arg min WLPGD(W), andLâ‹†=LPGD(Wâ‹†). Additionally, given Îº=Î±2d+1and suppose Î±=O
1/âˆš
d
,
d/n=O(1)anddis sufficiently large. Then Wâ‹†andLâ‹†have approximate forms
Wâ‹†â‰ˆ1
Îºn+(d+Ïƒ2)/ÎºId andLâ‹†â‰ˆd+Ïƒ2âˆ’Îºnd
Îºn+(d+Ïƒ2)/Îº. (12)
Similar to (10),(12) containsÎº=Î±2+1multiplier ahead of n, which reduces the in-context sample
complexity and setting Î±=0reduces to the results of Corollary 1.
3.2 Low-rank Parameterization and LoRA
In this section, we investigate training low-rank models, which assume Wk,WqâˆˆR(d+1)Ã—rwhere ris
the rank restriction. Equivalently, we consider objective (5a) under condition rank (W)=r.
Lemma 3 Consider independent linear data as defined in (7). Recap the objective from (5a)and
enforce rank (W)â‰¤randWâŠ¤=W. LetÎ£=Î£1/2
xÎ£Î²Î£1/2
xandM=tr(Î£)+Ïƒ2. DenotingÎ»ito be the
iâ€™th largest eigenvalue of Î£, we have that
min
rank(W)â‰¤r,W=WâŠ¤L(W)=Mâˆ’rX
i=1nÎ»2
i
(n+1)Î»i+M. (13)
70 10 20 30 40 50
# in-context samples0.30.40.50.60.70.80.9T est risk
Linear Att
H3
Theorem 1(a)Î£x=Î£Î²=IdandÏƒ=0
0 10 20 30 40 50
# in-context samples0.40.60.81.01.2T est risk
2/d=0
2/d=0.1
2/d=0.2
2/d=0.3
 (b) Noisy label
0 10 20 30 40 50
# in-context samples0.20.40.60.8T est risk
=0
=0.3
=0.6
=0.9
 (c) Non-isotropic task
Figure 2: Empirical evidence validates Theorem 1 and Proposition 1. We train 1-layer linear attention
and H3 models with prompts containing independent demonstrations following a linear model, and
dotted curves are the theory curves following Eq. (8).(a):We consider noiseless i.i.d. setting where
Î£x=Î£Î²=IdandÏƒ=0, with results presented in red (attention) and blue (H3) solid curves. (b):
We conduct noisy label experiments by choosing Ïƒ,0.(c):Consider non-isotropic task by setting
Î£Î²=Î³11âŠ¤+(1âˆ’Î³)Id. Solid and dashed curves in (b) and (c) represent attention and H3 results,
respectively. The alignments in (a), (b) and (c) show the equivalence between attention and H3,
validating Theorem 1 and Proposition 1. More experimental details are discussed in Section 4.
Note that tr(Î£)=Pd
i=1Î»i. Removing the rank constraint and considering noiseless data setting, this
reduces to the following optimal risk Lâ‹†=Pd
i=1Î»i+M
n+1+M/Î»i. See Appendix C.1 for more details.
Impact of LoRA: Based on the above lemma, we consider the impact of LoRA for adapting the
pretrained model to a new task distribution under jointly-diagonalizable old and new eigenvalues of
Î£,Î£new,(Î»i)d
i=1,(Î»new
i)d
i=1. Consider adapting LoRA matrix to the combined key and value weights
in attention, which reflects minimizing the population loss ËœL(Wlora) :=L(W+Wlora)in(5a)with
fixed W. Suppose tr(Î£)=tr(Î£new)=M,Ïƒ=0andWis jointly diagonalizable with Î£,Î£new, then
LoRAâ€™s risk is upper-bounded by
min
rank(Wlora)â‰¤rËœL(Wlora)â‰¤min
|I|â‰¤r,IâŠ‚[d]ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i<IÎ»i+M
n+1+M/Î»i+X
iâˆˆIÎ»new
i+M
n+1+M/Î»new
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸. (14)
Note that, the right hand side is provided assuming the optimal LoRA-updated model Wlorais also
jointly diagonalizable with covariances Î£,Î£new, and W.
4 Experiments
We now conduct synthetic experiments to support our theoretical findings and further explore the
behavior of different models of interest under different conditions. The experiments are designed to
investigate various scenarios, including independent data, retrieval-augmented generation (RAG),
task-feature alignment, low-rank parameterization, and LoRA adaption.
Experimental setting. We train 1-layer attention and H3 models for solving the linear regression ICL.
As described in Section 2, we consider meta-learning setting where task parameter Î²is randomly
generated for each training sequence. In all experiments, we set the dimension d=20. Depending
on the in-context length ( n), different models are trained to make in-context predictions. We train
each model for 10000 iterations with batch size 128and Adam optimizer with learning rate 10âˆ’3.
Since our study focuses on the optimization landscape, and experiments are implemented via gradient
descent, we repeat 20model trainings from different initialization and results are presented as the
minimal test risk among those 20trails. In all the plots, theoretical predictions are obtained via the
corresponding formulae presented in Section 3 and the test risks are normalized by the dimension d.
â€¢Equivalence among Lâ‹†
PGD,Lâ‹†
ATTandLâ‹†
SSM(Figure 2). To verify Proposition 1 as well as Theorem 1,
we run random linear regression instances where in-context samples are generated obeying (7).
Fig. 2a is identical to Fig. 1a where we set Î£x=Î£Î²=IdandÏƒ=0. In Fig. 2b, set Î£x=Î£Î²=I
and vary noise level Ïƒ2from 0to0.3Ã—d. In Fig. 2c, we consider noiseless labels, Ïƒ=0, isotropic
feature distribution Î£x=Idand set task covariance to be Î£Î²=Î³11âŠ¤+(1âˆ’Î³)Idby choosing Î³in
{0,0.3,0.6,0.9}. Note that in Fig. 2c, we train a sufficient number of models (greater than 20) to
ensure the optimal model is obtained. In all the figures, solid and dashed curves correspond to the
80 10 20 30 40 50
# in-context samples0.20.40.60.8T est risk
=0
=0.2
=0.4
=0.6
(a) RAG
0 10 20 30 40 50
# in-context samples0.20.40.60.8T est risk
=0
=0.2
=0.4
=0.6
 (b) Task-feature alignment
0 10 20 30 40 50
# in-context samples0.30.40.50.60.70.80.9T est risk
r=1
r=5
r=10
r=20 (c) Low-rank attention
0 10 20 30 40 50
# in-context samples0.10.30.50.70.9T est risk
r=1
r=5
r=10
r=20 (d) LoRA adaptation
Figure 3: Distributional alignment and low-rank parameterization experiments. (a)and(b)show
the ICL results using data generated via (9)and(11), respectively, by changing Î±from 0to0.6.
In(c), we train low-rank linear attention models by setting Wk,WqâˆˆR(d+1)Ã—rand in (d), we apply
the low-rank LoRA adaptor, Wlora:=WupWâŠ¤
downwhere Wup,WdownâˆˆR(d+1)Ã—r, to pretrained linear
attention models and adjust the LoRA parameters under different task distribution. Solid and dotted
curves correspond to the linear attention and theoretical results (c.f. Section 3), respectively, and the
alignments validate our theorems in Section 3. More experimental details are discussed in Section 4.
ICL results from training 1-layer ATTandSSMmodels, respectively, and dotted curves are obtained
from (8)in Theorem 1. The alignment of solid, dashed and dotted curves validates our Proposition 1
and Theorem 1.
â€¢Distributional alignment experiments (Figs. 3a&3b). In Figs. 3a and 3b, we generate RAG and
task-feature alignment data following (9)and(11), respectively, by setting Ïƒ=0and varying Î±from
0to0.6. Attention training results are displayed in solid curves, and we generate theory curve (dotted)
via theLâ‹†formula as described in (36) in Appendix B.3 and (42) in Appendix B.4. The empirical
alignments corroborate Theorems 4 and 5, further confirming that Proposition 1 is applicable to a
broader range of real-world distributional alignment data.
â€¢Low-rank (Fig. 3c) and LoRA (Fig. 3d) experiments. We also run simulations to verify our
theoretical findings in Section 3.2. Consider the independent data setting as described in (7). In Fig. 3c,
we set Î£x=Id,Ïƒ=0and task covariance to be diagonal with diagonal entries c[1 2âˆ’1Â·Â·Â·dâˆ’1]âŠ¤for
some normalization constant c=d/Pd
i=1iâˆ’1, and parameterize the attention model using matrices
Wk,WqâˆˆR(d+1)Ã—rand vary racross the set{1,5,10,20}. Results show that empirical (solid) and
theoretical (dotted, c.f. (13)) curves overlap. In Fig. 3d, we implement two phases of training. Phase
1:Setting Î£x=Î£Î²=IdandÏƒ=0, we pretrain the model with full rank parameters and obtain
weights Ë†Wk,Ë†Wq,Ë†WvâˆˆR(d+1)Ã—(d+1).Phase 2: We generate new examples with task covariance Î£Î²
being a diagonal matrix with diagonal entries câ€²[2âˆ’12âˆ’2Â·Â·Â·2âˆ’d]âŠ¤for some normalization constant
câ€²=d/Pd
i=12âˆ’i. Given the rank restriction r, we train additional LoRA parameters Wup,Wdownâˆˆ
R(d+1)Ã—rwhere Wlora:=WupWâŠ¤
downand(2a) becomes ATT(Z)=(Z(Ë†WqË†WâŠ¤
k+WupWâŠ¤
down)ZâŠ¤)ZË†Wv.
Fig. 3d presents the results after two phases of training where dotted curves are drawn from the right
hand side of (14) directly. Here, note that since Î£,Î£neware diagonal, the right hand side of (14)
returns the exact optimal risk of LoRA and the alignments verify it.
5 Related Work
There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu et al.,
2023b, Rae et al., 2021] in LLMs due to its success in continuously enabling novel applications for
LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023]. In the previous work, Garg
et al. [2022] explored ICL ability of Transformers. In particular, they considered in-context prompts
where each in-context example is labeled by a target function from a given function class, including
linear models. A number of works have studied this and related settings to develop a theoretical
understanding of ICL [von Oswald et al., 2023, Gatmiry et al., Collins et al., 2024, Lin and Lee,
2024, Li et al., 2024, Bai et al., 2024, AkyÃ¼rek et al., 2023, Zhang et al., 2023, Du et al., 2023].
AkyÃ¼rek et al. [2023] focus on linear regression and provide a construction of Transformer weights
that can enable a single step of GD based on in-context examples. Along the similar line, V on Oswald
et al. [2023] provide a construction of weights in linear attention-only Transformers that can emulate
GD steps on in-context examples for a linear regression task. Similar to this line of work, Dai et al.
[2023] argue that pre-trained language models act as meta-optimizer which utilize attention to apply
meta-gradients to the original language model based on the in-context examples.
9Building on these primarily empirical studies, Zhang et al. [2024], Mahankali et al. [2024], Ahn
et al. [2023], Duraisamy [2024] focus on developing a theoretical understanding of Transformers
trained to perform ICL. For single-layer linear attention model trained on independent in-context
prompts for random linear regression tasks, Mahankali et al. [2024], Ahn et al. [2023] show that the
resulting model implements a single step of PGD on in-context examples in a test prompt, thereby
corroborating the findings of [V on Oswald et al., 2023]. Zhang et al. [2024] study the optimization
dynamics of gradient flow while training a single-layer linear attention model on in-context prompts
for random linear regression tasks. Similar to Mahankali et al. [2024], Ahn et al. [2023], they show
that the trained model implements a single step of GD and PGD for isotropic and anisotropic Gaussian
features, respectively. In addition, they also characterize the test-time prediction error for the trained
model while highlighting its dependence on train and test prompt lengths.
While our work shares similarities with this line of works, as discussed in our contributions in the
introduction, we expand the theoretical understanding of ICL along multiple novel dimensions,
which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift.
Furthermore, we strive to capture the effect of retrieval augmentation [Lewis et al., 2020, Nakano
et al., 2021] on ICL through our analysis. Retrieval augmentation allows for selecting most relevant
demonstration out of a large collection for a test instance, e.g., via a dense retrieval model [Izacard
et al., 2023], which can significantly outperform the typical ICL setup where fixed task-specific
demonstrations are provided as in-context examples [Wang et al., 2022, Basu et al., 2023]. Through
a careful modeling of retrieval augmentation via correlated design, we show that it indeed has
a desirable amplification effect where the effective number in-context examples becomes larger
with higher correlation which corresponds to preforming a successful retrieval of query-relevant
demonstrations in a practical retrieval augmented setup.
Recently, state space models (SSMs) [Gu et al., 2021b,a, Fu et al., 2023, Gu and Dao, 2023] have
appeared as potential alternatives to Transformer architecture, with more efficient scaling to input
sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple
non-language tasks [Park et al., 2024, Grazzi et al., 2024] as well as complex NLP tasks [Grazzi et al.,
2024]. That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. [2024],
Mahankali et al. [2024], Ahn et al. [2023] is missing from the literature. In this work, we provide the
first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture [Fu et al., 2023], we
highlight its advantages over linear attention in specific ICL settings.
6 Discussion
In this work, we revisited the loss landscape of in-context learning with 1-layer sequence models. We
have established a general connection between ICL and gradient methods that accounts for correlated
data, non-attention architectures (specifically SSMs), and the impact of low-rank parameterization
including LoRA adaptation. Our results elucidate two central findings: (1) The functions learned by
different sequence model architectures exhibit a strong degree of universality and (ii) Dataset and
prompt design , such as RAG, can substantially benefit ICL performance.
Future directions and limitations. The results of this work fall short of being a comprehensive theory
for ICL in LLMs and can be augmented in multiple directions. First, while the exact equivalence
between H3 and linear attention is remarkable, we should examine whether it extends to other
SSMs. Secondly, while empirically predictive, our RAG and LoRA analyses are not precise and
fully formal. Thirdly, it is desirable to develop a deeper understanding of multilayer architectures
and connect to iterative GD methods as in [Ahn et al., 2023, V on Oswald et al., 2023]. Finally, we
have studied the population risk of ICL training whereas one can also explore the sample complexity
of pretraining [Wu et al., 2023, Lu et al., 2024]. Moving beyond the theoretically tractable setup
of this work, our simplified models are trained on in-context prompts from random initialization.
Therefore, this theoretical study doesnâ€™t address more challenging in-context learning tasks, such as
question answering, where both in-context demonstration and general knowledge from pretraining
are required. Future work in this area could also shed light on how certain contexts might elicit
undesirable behaviors acquired by an LLM during pretraining, an aspect not covered in our current
analysis. This work also studies a theoretical model for retrieval augmentation-based ICL. In a
real-life retrieval augmentation-based ICL, one needs to account for the quality of the collection of
the retrievable demonstrations and its (negative) impacts on the final predictions.
10Acknowledgements
This work was supported in part by the National Science Foundation grants CCF-2046816, CCF-
2403075, the Office of Naval Research award N000142412289, an Adobe Data Science Research
award, and a gift by Google Research.
References
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement
preconditioned gradient descent for in-context learning. Advances in Neural Information Processing
Systems , 36, 2023.
Ekin AkyÃ¼rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning
algorithm is in-context learning? investigations with linear models. In The Eleventh International
Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=
0g0X4H8yN4I .
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning , pages 322â€“332. PMLR, 2019.
Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:
Provable in-context learning with in-context algorithm selection. Advances in neural information
processing systems , 36, 2024.
Soumya Basu, Ankit Singh Rawat, and Manzil Zaheer. A statistical perspective on retrieval-based
models. In International Conference on Machine Learning , pages 1852â€“1886. PMLR, 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877â€“1901, 2020.
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model align-
ment explain generalization in kernel regression and infinitely wide neural networks. Nature
communications , 12(1):2914, 2021.
Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning. arXiv preprint arXiv:1912.01198 , 2019.
Liam Collins, Advait Parulekar, Aryan Mokhtari, Sujay Sanghavi, and Sanjay Shakkottai. In-context
learning with transformers: Softmax attention adapts to function lipschitzness. arXiv preprint
arXiv:2402.11639 , 2024.
Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can
GPT learn in-context? language models secretly perform gradient descent as meta-optimizers.
In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Associa-
tion for Computational Linguistics: ACL 2023 , pages 4005â€“4019, Toronto, Canada, July 2023.
Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.247. URL
https://aclanthology.org/2023.findings-acl.247 .
Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutional networks. In International conference on machine learning , pages 933â€“941. PMLR,
2017.
Zhe Du, Haldun Balim, Samet Oymak, and Necmiye Ozay. Can transformers learn optimal filtering
for unknown systems? IEEE Control Systems Letters , 7:3525â€“3530, 2023.
Karthik Duraisamy. Finite sample analysis and bounds of generalization error of gradient descent in
in-context linear regression. arXiv preprint arXiv:2405.02462 , 2024.
Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re.
Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh
International Conference on Learning Representations , 2023. URL https://openreview.net/
forum?id=COZDy0WYGg .
11Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn
in-context? a case study of simple function classes. Advances in Neural Information Processing
Systems , 35:30583â€“30598, 2022.
Khashayar Gatmiry, Nikunj Saunshi, Sashank J Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can
looped transformers learn to implement multi-step gradient descent for in-context learning? In
Forty-first International Conference on Machine Learning .
GeminiTeam, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu
Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, and Frank Hutter. Is mamba capable
of in-context learning? arXiv preprint arXiv:2402.03170 , 2024.
Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv
preprint arXiv:2312.00752 , 2023.
Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured
state spaces. In International Conference on Learning Representations , 2021a.
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher RÃ©.
Combining recurrent, convolutional, and continuous-time models with linear state space layers.
Advances in neural information processing systems , 34:572â€“585, 2021b.
Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. In-context learning of large language models explained
as kernel regression. arXiv preprint arXiv:2305.12766 , 2023.
Noah Hollmann, Samuel MÃ¼ller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer
that solves small tabular classification problems in a second. arXiv preprint arXiv:2207.01848 ,
2022.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International
Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=
nZeVKeeFYf9 .
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning
with retrieval augmented language models. Journal of Machine Learning Research , 24(251):1â€“43,
2023.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. Transformers are rnns:
Fast autoregressive transformers with linear attention. In International conference on machine
learning , pages 5156â€“5165. PMLR, 2020.
Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. Exploring the relationship between model
architecture and in-context learning ability. arXiv preprint arXiv:2310.08049 , 2023.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, et al. Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems , 33:
9459â€“9474, 2020.
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International conference
on artificial intelligence and statistics , pages 4313â€“4324. PMLR, 2020.
Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers
as algorithms: Generalization and stability in in-context learning. In International Conference on
Machine Learning , pages 19565â€“19594. PMLR, 2023.
Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak.
Dissecting chain-of-thought: Compositionality through in-context filtering and learning. Advances
in Neural Information Processing Systems , 36, 2024.
12Ziqian Lin and Kangwook Lee. Dual operating modes of in-context learning. arXiv preprint
arXiv:2402.18819 , 2024.
Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers
learn shortcuts to automata. In The Eleventh International Conference on Learning Representations ,
2023a. URL https://openreview.net/forum?id=De4FYqjFueZ .
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language
processing. ACM Computing Surveys , 55(9):1â€“35, 2023b.
Yue Lu, Mary I. Letey, Jacob A. Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. Asymptotic
theory of in-context learning by linear attention. arXiv preprint arXiv:2310.08391 , 2024.
Arvind V . Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient descent is provably
the optimal in-context learner with one layer of linear self-attention. In The Twelfth International
Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=
8p3fu56lKc .
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis. Revisiting the equivalence of in-context
learning and gradient descent: The impact of data distribution. In ICASSP 2024-2024 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 7410â€“7414.
IEEE, 2024.
Samuel MÃ¼ller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter.
Transformers can do bayesian inference. arXiv preprint arXiv:2112.10510 , 2021.
Samuel MÃ¼ller, Matthias Feurer, Noah Hollmann, and Frank Hutter. Pfns4bo: In-context learning for
bayesian optimization. In International Conference on Machine Learning , pages 25444â€“25470.
PMLR, 2023.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,
Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads.
arXiv preprint arXiv:2209.11895 , 2022.
OpenAI. Gpt-4 technical report. arXiv preprintarXiv:2303.08774 , 2023.
Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kang-
wook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on
in-context learning tasks. International Conference on Machine Learning , 2024.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.
Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. Linear transformers are secretly fast weight
programmers. In International Conference on Machine Learning , pages 9355â€“9366. PMLR, 2021.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e
Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
Johannes V on Oswald, Eyvind Niklasson, Ettore Randazzo, JoÃ£o Sacramento, Alexander Mordvintsev,
Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In
International Conference on Machine Learning , pages 35151â€“35174. PMLR, 2023.
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet,
Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering
mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858 , 2023.
13Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu, and
Michael Zeng. Training data is more valuable than you think: A simple and effective method by
retrieving from training data. arXiv preprint arXiv:2203.08773 , 2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems , 35:24824â€“24837, 2022.
Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett.
How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint
arXiv:2310.08391 , 2023.
Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context
learning as implicit bayesian inference. In International Conference on Learning Representations ,
2022. URL https://openreview.net/forum?id=RdJVFCHjUMI .
Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.
arXiv preprint arXiv:2306.09927 , 2023.
Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context.
Journal of Machine Learning Research , 25(49):1â€“55, 2024.
Nicolas Zucchet, Seijin Kobayashi, Yassir Akram, Johannes V on Oswald, Maxime Larcher, Angelika
Steger, and Joao Sacramento. Gated recurrent neural networks discover attention. arXiv preprint
arXiv:2309.01775 , 2023.
14Appendix
Table of Contents
A Equivalence among Gradient Descent, Attention, and State-Space Models 15
A.1 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.3 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B Analysis of General Data Distribution 21
B.1 Supporting Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2 Independent Data with General Covariance . . . . . . . . . . . . . . . . . . . . 24
B.3 Retrieval Augmented Generation with Î±Correlation . . . . . . . . . . . . . . . . 25
B.4 Task-feature Alignment with Î±Correlation . . . . . . . . . . . . . . . . . . . . . 28
C Analysis of Low-Rank Parameterization 31
C.1 Proof of Lemma 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D Additional Experiments 32
E Extended Related Work 33
A Equivalence among Gradient Descent, Attention, and State-Space Models
In this section, we present the proofs related to Section 2. Recap that given data
X=[x1Â·Â·Â·xn]âŠ¤âˆˆRnÃ—d,
Î¾=[Î¾1Â·Â·Â·Î¾n]âŠ¤âˆˆRn,
y=[y1Â·Â·Â·yn]âŠ¤=XÎ²+Î¾âˆˆRn,
Z0=[z1...zn0d+1]âŠ¤="
x1... xn0d
y1... yn0#âŠ¤
âˆˆR(n+1)Ã—(d+1),
and corresponding prediction functions
gPGD(Z)=xâŠ¤WXâŠ¤y, (15a)
gWPGD(Z)=xâŠ¤WXâŠ¤(Ï‰âŠ™y), (15b)
gATT(Z)=(zâŠ¤WqWâŠ¤
kZâŠ¤
0)Z0Wvv, (15c)
gSSM(Z)=
(zâŠ¤Wq)âŠ¤âŠ™((Z0WkâŠ™Z0Wv)âˆ—f)n+1
v, (15d)
we have objectives
min
WLPGD(W)whereLPGD(W)=Eh
(yâˆ’gPGD(Z))2i
, (16a)
min
W,Ï‰LWPGD(W)whereLWPGD(W)=Eh
(yâˆ’gWPGD(Z))2i
, (16b)
min
Wk,Wq,Wv,vLATT(W)whereLATT(W)=Eh
(yâˆ’gATT(Z))2i
, (16c)
min
Wk,Wq,Wv,v,fLSSM(W)whereLSSM(W)=Eh
(yâˆ’gSSM(Z))2i
. (16d)
Here, the expectation is over the randomness in (xi,Î¾i)n
i=1andÎ², and the search space for WisRdÃ—d,
forÏ‰isRn, forWk,Wq,WvisR(d+1)Ã—(d+1), forvisRd+1, and for fisRn+1.
15A.1 Proof of Proposition 1
Consider the problem setting as discussed in Section 2, Proposition 1 can be proven by the following
two lemmas.
Lemma 4 Suppose Assumptions 1 and 2 hold. Then, given the objectives (16a) and(16c) , we have
min
Wq,Wk,Wv,vLATT(W)=min
WLPGD(W).
Proof. Recap the linear attention estimator from (15c) and denote
WqWâŠ¤
k="Â¯W w 1
wâŠ¤
2w#
and Wvv="
v1
v#
,
where Â¯WâˆˆRdÃ—d,w1,w2,v1âˆˆRd, and w,vâˆˆR. Then we have
gATT(Z)=(zâŠ¤WqWâŠ¤
kZâŠ¤
0)Z0Wvv
=[xâŠ¤0]"Â¯W w 1
wâŠ¤
2w#"
XâŠ¤0d
yâŠ¤0#"
X y
0âŠ¤
d0#"
v1
v#
=(xâŠ¤Â¯WXâŠ¤+xâŠ¤w1yâŠ¤)(Xv1+yv)
=xâŠ¤(vÂ¯W)XâŠ¤y+xâŠ¤w1yâŠ¤Xv1+xâŠ¤Â¯WXâŠ¤Xv1+vâˆ¥yâˆ¥2
â„“2w1
=xâŠ¤(vÂ¯W+w1vâŠ¤
1)XâŠ¤y+xâŠ¤Â¯WXâŠ¤Xv1+vâˆ¥yâˆ¥2
â„“2w1
=xâŠ¤ËœWXâŠ¤y|     {z     }
ËœgATT(Z)+xâŠ¤Â¯WXâŠ¤Xv1+vâˆ¥yâˆ¥2
â„“2w1
|                            {z                            }
Îµ, (17)
where ËœW:=vÂ¯W+w1vâŠ¤
1.
We first show that for any given parameters Wk,Wq,Wv,v,
Eh
(gATT(Z)âˆ’y)2i
â‰¥Eh
(ËœgATT(Z)âˆ’y)2i
. (18)
To this goal, we have
Eh
(gATT(Z)âˆ’y)2i
âˆ’Eh
(ËœgATT(Z)âˆ’y)2i
=Eh
(ËœgATT(Z)+Îµâˆ’y)2i
âˆ’Eh
(ËœgATT(Z)âˆ’y)2i
=E[Îµ2]+2E[(ËœgATT(Z)âˆ’y)Îµ] (19)
where we have decomposition
(ËœgATT(Z)âˆ’y)Îµ=(xâŠ¤ËœWXâŠ¤yâˆ’y)xâŠ¤Â¯WXâŠ¤Xv1+vâˆ¥yâˆ¥2
â„“2w1
=yâŠ¤XËœWâŠ¤xxâŠ¤Â¯WXâŠ¤Xv1+vâˆ¥yâˆ¥2
â„“2w1
âˆ’yxâŠ¤Â¯WXâŠ¤Xv1+vâˆ¥yâˆ¥2
â„“2w1
=yâŠ¤XËœWâŠ¤xxâŠ¤Â¯WXâŠ¤Xv1|                      {z                      }
(a)+vâˆ¥yâˆ¥2
â„“2yâŠ¤XËœWâŠ¤xxâŠ¤w1|                      {z                      }
(b)âˆ’yxâŠ¤Â¯WXâŠ¤Xv1|          {z          }
(c)âˆ’vyâˆ¥yâˆ¥2
â„“2xâŠ¤w1|          {z          }
(d).
In the following, we consider the expectations of (a),(b),(c),(d)sequentially, which return zeros
under Assumptions 1 and 2. Note that since Assumption 1 holds, expectation of any odd order of
monomial of the entries of X,x,Î²returns zero, i.e., order of xâŠ¤Î²xis 3 and therefore E[xâŠ¤Î²x]=0d.
(a) :Eh
yâŠ¤XËœWâŠ¤xxâŠ¤Â¯WXâŠ¤Xv1i
=Eh
(XÎ²+Î¾)âŠ¤XËœWâŠ¤xxâŠ¤Â¯WXâŠ¤Xv1i
=Eh
Î²âŠ¤XâŠ¤XËœWâŠ¤xxâŠ¤Â¯WXâŠ¤Xv1i
+Eh
Î¾âŠ¤XËœWâŠ¤xxâŠ¤Â¯WXâŠ¤Xv1i
=0.
(b) :Eh
vâˆ¥yâˆ¥2
â„“2yâŠ¤XËœWâŠ¤xxâŠ¤w1i
=Eh
v(XÎ²+Î¾)âŠ¤(XÎ²+Î¾)(XÎ²+Î¾)âŠ¤XËœWâŠ¤xxâŠ¤w1i
=Eh
vâˆ¥Î¾âˆ¥2
â„“2Î¾âŠ¤XËœWâŠ¤xxâŠ¤w1i
=0.
16(c) :Eh
yxâŠ¤Â¯WXâŠ¤Xv1i
=Eh
(xâŠ¤Î²+Î¾)xâŠ¤Â¯WXâŠ¤Xv1i
=Eh
Î²âŠ¤xxâŠ¤Â¯WXâŠ¤Xv1i
+Eh
Î¾xâŠ¤Â¯WXâŠ¤Xv1i
=0.
(d) :Eh
vyâˆ¥yâˆ¥2
â„“2xâŠ¤w1i
=vEh
(Î²âŠ¤x+Î¾)(XÎ²+Î¾)âŠ¤(XÎ²+Î¾)xâŠ¤w1i
=vEh
Î¾âˆ¥Î¾âˆ¥2
â„“2xâŠ¤w1i
=0.
Combining the results with (19) returns that
Eh
(gATT(Z)âˆ’y)2i
âˆ’Eh
(ËœgATT(Z)âˆ’y)2i
=E[Îµ2]â‰¥0 (20)
which completes the proof of (18). Therefore, we obtain
min
Wq,Wk,Wv,vEh
(gATT(Z)âˆ’y)2i
â‰¥min
ËœWEh
(ËœgATT(Z)âˆ’y)2i
=min
WEh
(gPGD(Z)âˆ’y)2i
.
We conclude the proof of this lemma by showing that for any WâˆˆRdÃ—dingPGD, there exist
Wk,Wq,Wv,vsuch that gATT(Z)=gPGD(Z). Let
Wk=Wv=Id+1, Wq="
W0d
0âŠ¤
d0#
,and v="
0d
1#
.
Then we obtain
gATT(Z)=xâŠ¤WXâŠ¤y=gPGD(Z), (21)
which completes the proof.
Lemma 5 Suppose Assumptions 1 and 2 hold. Then, given the objectives in (16), we have
min
Wq,Wk,Wv,v,fLSSM(W)=min
W,Ï‰LWPGD(W). (22)
Additionally, if the examples (xi,yi)n
i=1follow the same distribution and are conditionally independent
given xandÎ², then SSM/H3 can achieve the optimal loss using the all-ones filter and
min
W,Ï‰LWPGD(W)=min
WLPGD(W). (23)
Proof. Recap the SSM estimator from (15d) and let
Wq=h
wq1wq2Â·Â·Â· wq,d+1i
,
Wk=h
wk1wk2Â·Â·Â· wk,d+1i
,
Wv=h
wv1wv2Â·Â·Â· wv,d+1i
,
where wq j,wk j,wv jâˆˆRd+1forjâ‰¤d+1, and let
v=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°v1
v2
Â·Â·Â·
vd+1ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»,and f=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°f0
f1
Â·Â·Â·
fnï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£».
Then we have
gSSM(Z)=
(zâŠ¤Wq)âŠ¤âŠ™((Z0WkâŠ™Z0Wv)âˆ—f)n+1
v
=nX
i=1fn+1âˆ’iÂ·vâŠ¤ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°wâŠ¤
q1z
Â·Â·Â·
wâŠ¤
q,d+1zï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»âŠ™ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°wâŠ¤
k1ziwâŠ¤
v1zi
Â·Â·Â·
wâŠ¤
k,d+1ziwâŠ¤
v,d+1ziï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
=nX
i=1fn+1âˆ’iÂ·vâŠ¤ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°wâŠ¤
q1zwâŠ¤
k1ziwâŠ¤
v1zi
Â·Â·Â·
wâŠ¤
q,d+1zwâŠ¤
k,d+1ziwâŠ¤
v,d+1ziï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£».
17Next for all jâ‰¤d+1, let
wq j="
Â¯wq j
wq j#
,wk j="
Â¯wk j
wk j#
,wv j="
Â¯wv j
wv j#
where Â¯wq j,Â¯wk j,Â¯wv jâˆˆRdandwq j,wk j,wv jâˆˆR. Then we have
wâŠ¤
q jzwâŠ¤
k jziwâŠ¤
v jzi=
Â¯wâŠ¤
q jx
Â¯wâŠ¤
k jxi+wk jyi
Â¯wâŠ¤
v jxi+wv jyi
=xâŠ¤Â¯wq j
wv jÂ¯wâŠ¤
k j+wk jÂ¯wâŠ¤
v j
xiyi+
Â¯wâŠ¤
q jx
Â¯wâŠ¤
k jxi
Â¯wâŠ¤
v jxi
+
wk jwv jÂ¯wâŠ¤
q jxy2
i
=xâŠ¤Wâ€²
jxiyi+Î´j(x,xi,xi)+wâ€²
jâŠ¤xy2
i
where
Wâ€²
j:=Â¯wq j
wv jÂ¯wâŠ¤
k j+wk jÂ¯wâŠ¤
v j
âˆˆRdÃ—d,
wâ€²
j:=wk jwv jÂ¯wq jâˆˆRd,
Î´j(x,xi,xi) :=
Â¯wâŠ¤
q jx
Â¯wâŠ¤
k jxi
Â¯wâŠ¤
v jxi
âˆˆR.
Then
gSSM(Z)=nX
i=1fn+1âˆ’iÂ·d+1X
j=1vj
xâŠ¤Wâ€²
jxiyi+Î´j(x,xi,xi)+wâ€²
jâŠ¤xy2
i
=xâŠ¤ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­d+1X
j=1vjWâ€²
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸X(yâŠ™Ëœf)+nX
i=1fn+1âˆ’iÂ·d+1X
j=1vjÂ·Î´j(x,xi,xi)+ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­d+1X
j=1vjwâ€²
jâŠ¤ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸xyâŠ¤(yâŠ™Ëœf)
=xâŠ¤ËœWXËœy|   {z   }
ËœgSSM(Z)+ËœÎ´(x,X,X)|      {z      }
Îµ1+ËœwâŠ¤xyâŠ¤Ëœy|   {z   }
Îµ2.
where
Ëœf:=[fnÂ·Â·Â·f1]âŠ¤âˆˆRn,
Ëœy:=yâŠ™ËœfâˆˆRn,
ËœW:=d+1X
j=1vjWâ€²
jâˆˆRdÃ—d,
Ëœw:=d+1X
j=1vjwâ€²
jâˆˆRd,
ËœÎ´(x,X,X) :=nX
i=1fn+1âˆ’iÂ·d+1X
j=1vjÂ·Î´j(x,xi,xi)âˆˆR.
Next we will show that for any Wk,Wq,Wv,v,
Eh
(gSSM(Z)âˆ’y)2i
â‰¥Eh
(ËœgSSM(Z)âˆ’y)2i
.
To start with, we obtain
Eh
(gSSM(Z)âˆ’y)2i
=Eh
(ËœgSSM(Z)+Îµ1+Îµ2âˆ’y)2i
=Eh
(ËœgSSM(Z)âˆ’y)2i
+Eh
(Îµ1+Îµ2)2i
+2E(ËœgSSM(Z)âˆ’y)(Îµ1+Îµ2)(24)
where there is decomposition
(ËœgSSM(Z)âˆ’y)(Îµ1+Îµ2)=ËœÎ´(x,X,X)Â·xâŠ¤ËœWXËœy|                   {z                   }
(a)âˆ’ËœÎ´(x,X,X)y|       {z       }
(b)+ËœwâŠ¤xyâŠ¤ËœyÂ·xâŠ¤ËœWXËœy|                 {z                 }
(c)âˆ’yÂ·ËœwâŠ¤xyâŠ¤Ëœy|       {z       }
(d).
In the following, similar to the proof of Lemma 4, we consider the expectations of (a),(b),(c),(d)
sequentially, which return zeros under Assumptions 1 and 2. Note that Î´j(x,xi,xi)â€™s and ËœÎ´(x,X,X)
are summation of monomials of entries of (x,X,Î²)with order 3, and entries of yandyare summation
18of monomials of entries of (x,X,Î²)with even orders: e.g., y=xâŠ¤Î²+Î¾whereÎ¾is of oder 0 and xâŠ¤Î²
is of order 2.
(a) :EhËœÎ´(x,X,X)Â·xâŠ¤ËœWXËœyi
=EhËœÎ´(x,X,X)Â·xâŠ¤ËœWX(XÎ²âŠ™Ëœf)i
+EhËœÎ´(x,X,X)Â·xâŠ¤ËœWX(Î¾âŠ™Ëœf)i
=EhËœÎ´(x,X,X)Â·xâŠ¤ËœWXi
Eh
Î¾âŠ™Ëœfi
=0.
(b) :EhËœÎ´(x,X,X)yi
=EhËœÎ´(x,X,X)(xâŠ¤Î²+Î¾)i
=EhËœÎ´(x,X,X)xâŠ¤Î²i
+EhËœÎ´(x,X,X)Î¾i
=0.
(c) :Eh
ËœwâŠ¤xyâŠ¤ËœyÂ·xâŠ¤ËœWXËœyi
=Eh
ËœwâŠ¤x(XÎ²+Î¾)âŠ¤(XÎ²âŠ™Ëœf+Î¾âŠ™Ëœf)Â·xâŠ¤ËœWX(XÎ²âŠ™Ëœf+Î¾âŠ™Ëœf)i
=0.
(d) :Eh
yÂ·ËœwâŠ¤xyâŠ¤Ëœyi
=Eh
(xâŠ¤Î²+Î¾)Â·ËœwâŠ¤x(XÎ²+Î¾)âŠ¤(XÎ²âŠ™Ëœf+Î¾âŠ™Ëœf)i
=0.
Combining the results with (24) results that
Eh
(gSSM(Z)âˆ’y)2i
âˆ’Eh
(ËœgSSM(Z)âˆ’y)2i
=Eh
(Îµ1+Îµ2)2i
â‰¥0.
Therefore we obtain,
min
Wq,Wk,Wv,v,fEh
(gSSM(Z)âˆ’y)2i
â‰¥min
ËœW,ËœfEh
(ËœgSSM(Z)âˆ’y)2i
=min
W,Ï‰Eh
(gWPGD(Z)âˆ’y)2i
.
Next we show that for any choices of WandÏ‰ingWPGD, there are Wq,k,v,v,fsuch that gSSMâ‰¡gWPGD.
To this end, given Ï‰=[Ï‰1... Ï‰ n]âŠ¤, let
Wq=Id+1,Wk="
WâŠ¤0d
0âŠ¤
d0#
,Wv="
0dÃ—d0d
1âŠ¤
d0#
,v="
1d
0#
and f=ï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0
Ï‰n
Â·Â·Â·
Ï‰1ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£».
Then we get
((Z0WkâŠ™Z0Wv)âˆ—f)n+1=  "
XWâŠ¤0n
0d 0#
âŠ™"
y1âŠ¤
d0n
0d0#!
âˆ—f!
n+1
="Pn
i=1Ï‰iÂ·yiWx i
0#
="
WXâŠ¤(yâŠ™Ï‰)
0#
,
and therefore
gSSM(Z)=xâŠ¤WXâŠ¤(yâŠ™Ï‰)=gWPGD(Z),
which completes the proof of (22).
19Next, to show (23), for any WâˆˆRdÃ—d, letL(Ï‰)=Eh xâŠ¤WXâŠ¤(yâŠ™Ï‰)âˆ’y2i
. Then we have
âˆ‚L(Ï‰)
âˆ‚Ï‰i=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°2ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­xâŠ¤WnX
j=1Ï‰jyjxjâˆ’yï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
xâŠ¤Wyixiï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=2nX
j=1Ï‰jEh
(xâŠ¤Wyjxj)(xâŠ¤Wyixi)i
âˆ’2Eh
yxâŠ¤Wyixii
.
Here since (xi,yi)n
i=1follow the same distribution and are conditionally independent given x
andÎ², for any i,j,jâ€²,Eh
(xâŠ¤Wyixi)2i
=Eh
(xâŠ¤Wyjxj)2i
andEh
(xâŠ¤Wyjxj)(xâŠ¤Wyixi)i
=
Eh
(xâŠ¤Wyjâ€²xjâ€²)(xâŠ¤Wyixi)i
. Then let
Eh
(xâŠ¤Wyjxj)(xâŠ¤Wyixi)i
=(c1,i,j
c2,i=jand Eh
yxâŠ¤Wyixii
=c3,
where (c1,c2,c3) :=(c1(W),c2(W),c3(W)). We get
âˆ‚L(Ï‰)
âˆ‚Ï‰i=2c1Ï‰âŠ¤1n+2(c2âˆ’c1)Ï‰iâˆ’2c3.
Ifc2âˆ’c1=0, thenâˆ‚L(Ï‰)
âˆ‚Ï‰iâ‰¡2c1Ï‰âŠ¤1nâˆ’2c3for all iâ‰¤nand anyÏ‰âˆˆRnachieves the same performance.
Ifc2âˆ’c1,0, settingâˆ‚L(Ï‰)
âˆ‚Ï‰i=0returns
Ï‰i=c3âˆ’c1Pn
j=1Ï‰j
c2âˆ’c1:=Cfor all iâ‰¤n.
Therefore the optimal loss is achieved via setting Ï‰=C1n. Without loss of generality, we can update
Wâ†’CW. ThenÏ‰=1n, and we obtain
min
W,Ï‰E
xâŠ¤WXâŠ¤(yâŠ™Ï‰)âˆ’y2
=min
WEh
(xâŠ¤WXâŠ¤yâˆ’y)2i
which completes the proof of (23).
A.2 Proof of Lemma 1
Proof. Recap the lossLPGD(W)in (16a) and prediction gPGD(Z)in (15a), we have
LPGD(W)=E[(yâˆ’gPGD(Z))2]
=E
xâŠ¤Î²+Î¾âˆ’xâŠ¤WXâŠ¤(XÎ²+Î¾)2
=Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2+2(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)+(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)2i
=Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2+(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)2i
+2E[(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)]
=Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2+(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)2i
(25)
=Eh
(xâŠ¤WXâŠ¤XÎ²)2+(xâŠ¤WXâŠ¤Î¾)2i
|                                     {z                                     }
f1(W)âˆ’2E[Î²âŠ¤xxâŠ¤WXâŠ¤XÎ²+Î¾xâŠ¤WXâŠ¤Î¾]|                                         {z                                         }
f2(W)+E[(xâŠ¤Î²)2+Î¾2]|            {z            }
constant
where (25) follows Assumption 2. Since f2(W)is convex,LPGD(W)is strongly-convex if and only if
f1(W)is strongly-convex, which completes the proof of strong convexity.
Next, (20) and(21) in the proof of Lemma 4 demonstrate that the optimal loss is achievable and is
achieved at Îµ=0. Subsequently, (17) indicates that gâ‹†
ATThas the same form as gâ‹†
PGD. Under the strong
convexity assumption, gâ‹†
PGDis unique, which leads to the conclusion that gâ‹†
PGD=gâ‹†
ATT.
20A.3 Proof of Lemma 2
Proof. According to Lemma 1, LPGD(W)is strongly-convex as long as either E[(xâŠ¤WXâŠ¤XÎ²)2]or
E[(xâŠ¤WXâŠ¤Î¾)2]is strongly-convex. Therefore, in this lemma, the two claims correspond to the strong
convexity of E[(xâŠ¤WXâŠ¤Î¾)2]andE[(xâŠ¤WXâŠ¤XÎ²)2]terms, respectively.
Suppose the decomposition claim holds. Without losing generality, we may assume (x1,Î²1,X1)
are zero-mean because we can allocate the mean component to (x2,Î²2,X2)without changing the
covariance.
â€¢Claim 1: LetÂ¯Î£x=E[x1xâŠ¤
1],Â¯Î£Î²=E[Î²1Î²âŠ¤
1], and Â¯Î£X=E[XâŠ¤
1X1]. If the first claim holds, using
independence, observe that we can write
E[(xâŠ¤WXâŠ¤Î¾)2]=E[(xâŠ¤
1WXâŠ¤
1Î¾)2]+E[(xâŠ¤
1WXâŠ¤
2Î¾)2]+E[(xâŠ¤
2WXâŠ¤
1Î¾)2]+E[(xâŠ¤
2WXâŠ¤
2Î¾)2],
where the last three terms of the right hand side are convex and the first term obeys
E[(xâŠ¤
1WXâŠ¤
1Î¾)2]=Ïƒ2E[xâŠ¤
1WXâŠ¤
1X1WâŠ¤x1]
=Ïƒ2tr
E[x1xâŠ¤
1WXâŠ¤
1X1WâŠ¤]
=Ïƒ2trÂ¯Î£xWÂ¯Î£XWâŠ¤
=Ïƒ2q
Â¯Î£xWq
Â¯Î£X2
F.
Since noise level Ïƒ>0, using the full-rankness of covariance matrices Â¯Î£xandÂ¯Î£X, we conclude with
strong convexity of E[(xâŠ¤WXâŠ¤Î¾)2].
â€¢Claim 2: Now recall that Â¯Î£X=E[XâŠ¤
1X1]and set A=XâŠ¤
1X1âˆ’Â¯Î£XandB=XâŠ¤
2X2+Â¯Î£X. Observe
thatE[A]=0. If the second claim holds, E[XâŠ¤X]=E[A+B]. Note that (A,Î²1,x1)are independent
of each other and (B,Î²2,x2). Using independence and E[A]=0, similarly write
E[(xâŠ¤WXâŠ¤XÎ²)2]=E[(xâŠ¤W AÎ²)2]+E[(xâŠ¤WBÎ²)2].
Now using E[Î²1]=E[x1]=0and their independence from rest, these terms obeys
E[(xâŠ¤W AÎ²)2]=E[(xâŠ¤
1W AÎ²1)2]+E[(xâŠ¤
1W AÎ²2)2]+E[(xâŠ¤
2W AÎ²1)2]+E[(xâŠ¤
2W AÎ²2)2]
E[(xâŠ¤WBÎ²)2]=E[(xâŠ¤
1WBÎ²1)2]+E[(xâŠ¤
1WBÎ²2)2]+E[(xâŠ¤
2WBÎ²1)2]+E[(xâŠ¤
2WBÎ²2)2].
In both equations, the last three terms of the right hand side are convex. To proceed, we focus on the
first terms. Using independence and setting Î£X=E[XâŠ¤X]âª°Â¯Î£Xâ‰»0, we note that
E[(xâŠ¤
1W AÎ²1)2]+E[(xâŠ¤
1WBÎ²1)2]=E[(xâŠ¤
1WXâŠ¤XÎ²1)2]
where x1,Î²1,Xare independent and full-rank covariance. To proceed, note that
E[(xâŠ¤
1WXâŠ¤XÎ²1)2]=E[(xâŠ¤
1WÎ£XÎ²1)2]+E[(xâŠ¤
1W(XâŠ¤Xâˆ’Î£X)Î²1)2].
Observing the convexity of the right hand side and focusing on the first term, we get
E[(xâŠ¤
1WÎ£XÎ²1)2]=trÂ¯Î£xWÎ£XÂ¯Î£Î²Î£XWâŠ¤
=q
Â¯Î£xWÎ£Xq
Â¯Î£Î²2
F.
Using the fact that covariance matrices, Â¯Î£x,Î£X,Â¯Î£Î², are full rank concludes the strong convexity proof
ofE[(xâŠ¤WXâŠ¤XÎ²)2].
B Analysis of General Data Distribution
In this section, we provide the proofs in Section 3, which focuses on solving Objective (5a). For the
sake of clean notation, let L(W) :=LPGD(W)andg:=gPGDin this section.
21B.1 Supporting Results
We begin by deriving the even moments of random variables.
â€¢2nâ€™th moment of a normally distributed variable: Letuâˆ¼N(0,Ïƒ2). Then we have
E[u2n]=Ïƒ2n(2nâˆ’1)!!. (26)
â€¢4â€™th moment: Letuâˆ¼N(0,Id). Then for any W,Wâ€²âˆˆRdÃ—d, we have
Eh
(uâŠ¤Wu)(uâŠ¤Wâ€²u)i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1Wi juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1Wâ€²
i juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1Wiiu2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1Wâ€²
iiu2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jWi juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jWâ€²
i juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=dX
i=1WiiWâ€²
iiEh
u4
ii
+X
i,jWiiWâ€²
j jE[u2
i]E[u2
j]+X
i,jWi jWâ€²
i jE[u2
i]E[u2
j]+X
i,jWi jWâ€²
jiE[u2
i]E[u2
j]
=3dX
i=1WiiWâ€²
ii+X
i,jWiiWâ€²
j j+X
i,jWi jWâ€²
i j+X
i,jWi jWâ€²
ji
=dX
i,j=1WiiWâ€²
j j+dX
i,j=1Wi jWâ€²
i j+dX
i,j=1Wi jWâ€²
ji
=tr(W)tr Wâ€²+tr
Wâ€²WâŠ¤
+tr WWâ€². (27)
â€¢4â€™th cross-moment: Letu,vâˆ¼N(0,Id)and for any WâˆˆRdÃ—d, letÎ›W=WâŠ™Id. Then we have
Eh
(uâŠ¤WvvâŠ¤u)2i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1Wi juivjï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸2ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1uiviï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1W2
i ju2
iv2
j+X
i,iâ€²Wi jWiâ€²juiuiâ€²v2
j+X
j,jâ€²Wi jWi jâ€²u2
ivjvjâ€²+X
iâ€²,i,jâ€²,jWi jWiâ€²jâ€²uiuiâ€²vjvjâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u2
iv2
i+X
i,juiujvivjï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1W2
i ju2
iv2
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u2
iv2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸+ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jWi jWjiu2
iu2
jv2
iv2
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1W2
iiu2
iv2
i+X
i,jW2
i ju2
iv2
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u2
iv2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+X
i,jWi jWji
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1W2
iiu4
iv4
i+X
i,jW2
iiu2
iv2
iu2
jv2
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jW2
i ju4
iv2
jv2
i+X
i,jW2
i ju2
iv4
ju2
j+X
i,j,kW2
i ju2
iv2
ju2
kv2
kï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+X
i,jWi jWji
=9dX
i=1W2
ii+(dâˆ’1)dX
i=1W2
ii+6X
i,jW2
i j+(dâˆ’2)X
i,jW2
i j+X
i,jWi jWji
=3dX
i=1W2
ii+(d+4)dX
i,j=1W2
i j+dX
i,j=1Wi jWji
=3tr
Î›2
W
+(d+4)tr
WWâŠ¤
+tr
W2
. (28)
22â€¢6â€™th moment: Letuâˆ¼N(0,Id). Then for any W,Wâ€²âˆˆRdÃ—d, we have
Eh
(uâŠ¤Wu)(uâŠ¤Wâ€²u)âˆ¥uâˆ¥2
â„“2i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1Wi juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1Wâ€²
i juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1Wiiu2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1Wâ€²
iiu2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jWi juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jWâ€²
i juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=dX
i=1WiiWâ€²
iiEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u4
iï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u2
iâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+X
i,jWiiWâ€²
j jEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u2
iâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
+X
i,jWi jWâ€²
i jEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u2
iâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+X
i,jWi jWâ€²
jiEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u2
iâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=(d+4)ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­3dX
i=1WiiWâ€²
ii+X
i,jWiiWâ€²
j j+X
i,jWi jWâ€²
i j+X
i,jWi jWâ€²
jiï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸(29)
=(d+4)ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1WiiWâ€²
j j+dX
i,j=1Wi jWâ€²
i j+dX
i,j=1Wi jWâ€²
jiï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
=(d+4)
tr(W)tr Wâ€²+tr
Wâ€²WâŠ¤
+tr WWâ€²
, (30)
where (29) is obtained by following
Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u4
iï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u2
iâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»=E[u6]+(dâˆ’1)E[u4]E[u2]=3(d+4),
Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u2
iâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»=2E[u4]E[u2]+(dâˆ’2)E[u2]E[u2]E[u2]=d+4.
â€¢8â€™th moment: Letuâˆ¼N(0,Id). Then for any W,Wâ€²âˆˆRdÃ—d, we have
Eh
(uâŠ¤Wu)(uâŠ¤Wâ€²u)âˆ¥uâˆ¥4
â„“2i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1Wi juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1Wâ€²
i juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1u2
iu2
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1Wiiu2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1Wâ€²
iiu2
iï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u4
i+X
i,ju2
iu2
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jWi juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­X
i,jWâ€²
i juiujï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i=1u4
i+X
i,ju2
iu2
jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=dX
i=1WiiWâ€²
iiEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u4
iï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u4
iâ€²+X
iâ€²,jâ€²u2
iâ€²u2
jâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+X
i,jWiiWâ€²
j jEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u4
iâ€²+X
iâ€²,jâ€²u2
iâ€²u2
jâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
+X
i,jWi jWâ€²
i jEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u4
iâ€²+X
iâ€²,jâ€²u2
iâ€²u2
jâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+X
i,jWi jWâ€²
jiEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u4
iâ€²+X
iâ€²,jâ€²u2
iâ€²u2
jâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=(d+4)(d+6)ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­3dX
i=1WiiWâ€²
ii+X
i,jWiiWâ€²
j j+X
i,jWi jWâ€²
i j+X
i,jWi jWâ€²
jiï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸(31)
=(d+4)(d+6)ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
i,j=1WiiWâ€²
j j+dX
i,j=1Wi jWâ€²
i j+dX
i,j=1Wi jWâ€²
jiï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
=(d+4)(d+6)
tr(W)tr Wâ€²+tr
Wâ€²WâŠ¤
+tr WWâ€²
. (32)
23where (31) is obtained by following
Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u4
iï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u4
iâ€²+X
iâ€²,jâ€²u2
iâ€²u2
jâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=E[u8]+(dâˆ’1)E[u4]E[u4]+2(dâˆ’1)E[u6]E[u2]+(dâˆ’1)(dâˆ’2)E[u4]E[u2]E[u2]
=105+9(dâˆ’1)+30(dâˆ’1)+3(dâˆ’1)(dâˆ’2)
=3(d+4)(d+6),
Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°u2
iu2
jï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­dX
iâ€²=1u4
iâ€²+X
iâ€²,jâ€²u2
iâ€²u2
jâ€²ï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=2E[u6]E[u2]+(dâˆ’2)E[u4](E[u2])2+2E[u4]E[u4]+4(dâˆ’2)E[u4](E[u2])2+(dâˆ’2)(dâˆ’3)(E[u2])4
=30+3(dâˆ’2)+18+12(dâˆ’2)+(dâˆ’2)(dâˆ’3)
=(d+4)(d+6).
B.2 Independent Data with General Covariance
Proof of Theorem 1. Consider a general independent linear model as defined in (7)where Î£xand
Î£Î²are full-rank feature and task convariance matrices and
xâˆ¼N(0,Î£x),Î²âˆ¼N(0,Î£Î²), Î¾âˆ¼N(0,Ïƒ2),and y=xâŠ¤Î²+Î¾.
Let
X=[x1Â·Â·Â·xn]âŠ¤,Î¾=[Î¾1Â·Â·Â·Î¾n]âŠ¤,and y=[y1Â·Â·Â·yn]âŠ¤=XÎ²+Î¾.
To simplify and without loss of generality, let Â¯x=Î£âˆ’1/2
xx,Â¯X=XÎ£âˆ’1/2
x,Â¯Î²=Î£1/2
xÎ²where we have
Â¯xâˆ¼N(0,I), Â¯Î²âˆ¼N(0,Î£1/2
xÎ£Î²Î£1/2
x)
and
y=Â¯xâŠ¤Â¯Î²+Î¾, y=Â¯XÂ¯Î²+Î¾.
Then recap the loss from (5a), and we obtain
L(W)=Eh
(yâˆ’g(Z))2i
=E
xâŠ¤Î²+Î¾âˆ’xâŠ¤WXâŠ¤(XÎ²+Î¾)2
=Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2+2(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)+(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)2i
=Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2i
+Eh
(xâŠ¤WXâŠ¤Î¾)2i
+Ïƒ2, (33)
where the last equality comes from the independence of label noise Î¾,Î¾.
We first consider the following term
Eh
(xâŠ¤WXâŠ¤Î¾)2i
=Eh
(Â¯xâŠ¤(Î£1/2
xWÎ£1/2
x)Â¯XâŠ¤Î¾)2i
=nÏƒ2Â·trÂ¯WÂ¯WâŠ¤
where we define Â¯W=Î£1/2
xWÎ£1/2
x. Next, focus on the following
Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2i
=Eh
(Â¯xâŠ¤Â¯Î²âˆ’Â¯xâŠ¤Â¯WÂ¯XâŠ¤Â¯XÂ¯Î²)2i
=E
Â¯xâŠ¤
Iâˆ’Â¯WÂ¯XâŠ¤Â¯XÂ¯Î²2
=tr
E
Iâˆ’Â¯WÂ¯XâŠ¤Â¯X
Î£
Iâˆ’Â¯WÂ¯XâŠ¤Â¯XâŠ¤
=tr(Î£)âˆ’tr
Î£(Â¯W+Â¯WâŠ¤)E[Â¯XâŠ¤Â¯X]
+trÂ¯WâŠ¤Â¯WE[Â¯XâŠ¤Â¯XÎ£Â¯XâŠ¤Â¯X]
=tr(Î£)âˆ’2nÂ·tr
Î£Â¯W
+trÂ¯WâŠ¤Â¯WE[Â¯XâŠ¤Â¯XÎ£Â¯XâŠ¤Â¯X]
,
where Î£:=Î£1/2
xÎ£Î²Î£1/2
x.
24LetÂ¯xiâˆˆRnbe the iâ€™th column of Â¯XandÎ£i jbe the (i,j)â€™th entry of Î£. Then the (i,j)entry of matrix
Â¯XâŠ¤Â¯XÎ£Â¯XâŠ¤Â¯Xis
(Â¯XâŠ¤Â¯XÎ£Â¯XâŠ¤Â¯X)i j=dX
k=1dX
p=1Î£kpÂ¯xâŠ¤
iÂ¯xkÂ¯xâŠ¤
pÂ¯xj.
Then we get
i,j:EÂ¯XâŠ¤Â¯XÎ£Â¯XâŠ¤Â¯X
i j
=Î£i jE[Â¯xâŠ¤
iÂ¯xiÂ¯xâŠ¤
jÂ¯xj]+Î£jiE[Â¯xâŠ¤
iÂ¯xjÂ¯xâŠ¤
iÂ¯xj]=n2Î£i j+nÎ£ji
i=j:EhÂ¯XâŠ¤Â¯XÎ£Â¯XâŠ¤Â¯X
iii
=Î£iiEh
Â¯xâŠ¤
iÂ¯xiÂ¯xâŠ¤
iÂ¯xii
+X
j,iÎ£j jEh
Â¯xâŠ¤
iÂ¯xjÂ¯xâŠ¤
jÂ¯xii
=Î£iiEh
(x2
i1+Â·Â·Â·+x2
in)2i
+nX
j,iÎ£j j
=Î£ii(3n+n(nâˆ’1))+nX
j,iÎ£j j
=nï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­Î£ii(n+1)+dX
j=1Î£j jï£¶ï£·ï£·ï£·ï£·ï£·ï£·ï£¸
=n(Î£ii(n+1)+tr(Î£)).
Therefore
E[Â¯XâŠ¤Â¯XÎ£Â¯XâŠ¤Â¯X]=n(n+1)Î£+nÂ·tr(Î£)I.
Combining all together results in
L(W)=tr(Î£)âˆ’2ntr
Î£Â¯W
+n(n+1)tr
Î£Â¯WâŠ¤Â¯W
+n(tr(Î£)+Ïƒ2)trÂ¯WÂ¯WâŠ¤
+Ïƒ2,
=Mâˆ’2ntr
Î£Â¯W
+n(n+1)tr
Î£Â¯WâŠ¤Â¯W
+nMtrÂ¯WÂ¯WâŠ¤
, (34)
where M:=tr(Î£)+Ïƒ2. Settingâˆ‡Â¯WL(W)=0returns
âˆ’2nÂ·Î£+2n(n+1)Â·Î£Â¯W+2nMÂ¯W=0=â‡’Â¯Wâ‹†=
(n+1)I+MÎ£âˆ’1âˆ’1.
Then we have
Wâ‹†=Î£âˆ’1/2
x
(n+1)I+MÎ£âˆ’1âˆ’1Î£âˆ’1/2
x
and
Lâ‹†=L(Wâ‹†)=Mâˆ’ntr
((n+1)Î£âˆ’1+MÎ£âˆ’2)âˆ’1
.
B.3 Retrieval Augmented Generation with Î±Correlation
In this section, we consider the retrieval augmented generation (RAG) linear model similar to (9),
where we first draw the query vector xand task vector Î²via
xâˆ¼N(0,I)andÎ²âˆ¼N(0,I).
We then draw data (xi)n
i=1to be used in-context according to the rule corr_coef (x,xi)â‰¥Î±â‰¥0. Hence,
foriâ‰¤nwe sample
xixâˆ¼N(Î±x,Î³2I), Î¾ iâˆ¼N(0,Ïƒ2)and yi=xâŠ¤
iÎ²+Î¾i, (35)
which results in (9) by setting Î³2=1âˆ’Î±2.
Theorem 4 (Extended version of Theorem 2) Consider linear model as defined in (35). Recap the
objective from (5a)and let Wâ‹†:=arg min WLPGD(W), andLâ‹†=LPGD(Wâ‹†). Then Wâ‹†andLâ‹†satisfy
Wâ‹†=cI andLâ‹†=d+Ïƒ2âˆ’cnd(Î±2(d+2)+Î³2) (36)
where
c=Î±2(d+2)+Î³2
Î±4n(d+2)(d+4)+Î±2Î³2(d+2)(d+2n+3)+Î³4(d+n+1)+Ïƒ2(Î±2(d+2)+Î³2).
25SupposeÎ±=O
1/âˆš
d
,d/n=O(1)anddis sufficiently large. Let Îº=Î±2d+1andÎ³2=1âˆ’Î±2.
Then Wâ‹†andLâ‹†have approximate forms
Wâ‹†â‰ˆ1
Îºn+d+Ïƒ2I andLâ‹†â‰ˆd+Ïƒ2âˆ’Îºnd
Îºn+d+Ïƒ2. (37)
Proof. Here, for clean notation and without loss of generality, we define and rewrite (35) via
giâˆ¼N(0,I), Î¾ iâˆ¼N(0,Ïƒ2)and xi=Î±x+Î³gi,yi=(Î±x+Î³gi)âŠ¤Î²+Î¾i.
Then we obtain
L(W)=Eh
(yâˆ’g(Z))2i
=E
xâŠ¤Î²+Î¾âˆ’xâŠ¤WXâŠ¤(XÎ²+Î¾)2
=Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2+2(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)+(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)2i
=Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2i
+Eh
(xâŠ¤WXâŠ¤Î¾)2i
+Ïƒ2. (38)
To begin with, let
N1=tr(W)2+tr
WWâŠ¤
+tr
W2
,N2=tr
WWâŠ¤
,and N3=tr(W).
We first focus on the second term in (38)
Eh
(xâŠ¤WXâŠ¤Î¾)2i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î¾ixâŠ¤W(Î±x+Î³gi)ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=nÏƒ2Eh
xâŠ¤W(Î±x+Î³g)(Î±x+Î³g)âŠ¤WâŠ¤xi
=nÏƒ2
Î±2E[xâŠ¤WxxâŠ¤WâŠ¤x]+Î³2E[xâŠ¤W ggâŠ¤WâŠ¤x]
=nÏƒ2
Î±2N1+Î³2N2
. (It follows (27) and independence of x,g.)
Next, the first term in (38) can be decomposed into
Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2i
=Eh
(xâŠ¤Î²)2i
|      {z      }
(a)+Eh
(xâŠ¤WXâŠ¤XÎ²)2i
|                {z                }
(b)âˆ’2Eh
xâŠ¤Î²xâŠ¤WXâŠ¤XÎ²i
|                  {z                  }
(c).
In the following, we consider solving (a)-(c)sequentially.
(a) :Eh
(xâŠ¤Î²)2i
=d.
(b) :Eh
(xâŠ¤WXâŠ¤XÎ²)2i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­xâŠ¤WnX
i=1(Î±x+Î³gi)(Î±x+Î³gi)âŠ¤Î²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1xâŠ¤W(Î±2xxâŠ¤+Î³2gigâŠ¤
i+Î±Î³xgâŠ¤
i+Î±Î³gixâŠ¤)Î²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Î±4n2Eh
(xâŠ¤WxxâŠ¤Î²)2i
+Î³4Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1xâŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Î±2Î³2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1xâŠ¤WxgâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Î±2Î³2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1xâŠ¤W g ixâŠ¤Î²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
+2Î±2Î³2n2Eh
xâŠ¤WxxâŠ¤Î²Î²âŠ¤ggâŠ¤WâŠ¤xi
+2Î±2Î³2nEh
xâŠ¤WxgâŠ¤Î²xâŠ¤W gxâŠ¤Î²i
=
Î±4n2(d+4)N1+Î³4n(d+n+1)N2
+
Î±2Î³2ndN 1+Î±2Î³2n(d+2)N2
+
2Î±2Î³2n2N1+2Î±2Î³2nN1
=
Î±4n2(d+4)+Î±2Î³2n(2n+d+2)
N1+
Î±2Î³2n(d+2)+Î³4n(d+n+1)
N2
=A1N1+A2N2.
26(c) :Eh
xâŠ¤Î²xâŠ¤WXâŠ¤XÎ²i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°nX
i=1xâŠ¤Î²xâŠ¤W(Î±x+Î³gi)(Î±x+Î³gi)âŠ¤Î²ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°nX
i=1xâŠ¤Î²xâŠ¤W(Î±2xxâŠ¤+Î³2gigâŠ¤
i+Î±Î³xgâŠ¤
i+Î±Î³gixâŠ¤)Î²ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
=Î±2nEh
xâŠ¤Î²xâŠ¤WxxâŠ¤Î²i
+Î³2nEh
xâŠ¤Î²xâŠ¤W ggâŠ¤Î²i
=Î±2n(d+2)tr(W)+Î³2ntr(W)
=
Î±2n(d+2)+Î³2n
N3
=A3N3.
Here, (b)utilizes the 4â€™th and 6â€™th moment results (27) and (30) and we define
A1=Î±4n2(d+4)+Î±2Î³2n(2n+d+2)
A2=Î±2Î³2n(d+2)+Î³4n(d+n+1)
A3=Î±2n(d+2)+Î³2n.
Then combining all together results in
L(W)=A1N1+A2N2âˆ’2A3N3+nÏƒ2(Î±2N1+Î³2N2)+d+Ïƒ2.
To find the optimal solution, set âˆ‡L(W)=0and we obtain
A1âˆ‡N1+A2âˆ‡N2âˆ’2A3âˆ‡N3+nÏƒ2(Î±2âˆ‡N1+Î³2âˆ‡N2)=0. (39)
Note that we have
âˆ‡N1=âˆ‡
tr(W)2+tr
WWâŠ¤
+tr
W2
=2tr(W)I+2W+2WâŠ¤
âˆ‡N2=âˆ‡tr
WWâŠ¤
=2W
âˆ‡N3=âˆ‡tr(W)=I.
Therefore, (39) returns
2A1
tr(W)I+W+WâŠ¤
+2A2Wâˆ’2A3+2nÏƒ2(Î±2(tr(W)I+W+WâŠ¤)+Î³2W)I=0,(40)
which implies that the optimal solution Wâ‹†has the form of cIfor some constant c. Then suppose
Wâ‹†=cI, we have tr(W)=cdand (40) returns
2A1(d+2)cI+2A2cIâˆ’2A3I+2nÏƒ2(Î±2(d+2)cI+Î³2cI)=0
=â‡’c=A3
A1(d+2)+A2+nÏƒ2(Î±2(d+2)+Î³2)
=Î±2(d+2)+Î³2
Î±4n(d+2)(d+4)+Î±2Î³2(d+2)(d+2n+3)+Î³4(d+n+1)+Ïƒ2(Î±2(d+2)+Î³2).
Then the optimal loss is obtained by setting Wâ‹†=cIand
Lâ‹†=L(Wâ‹†)=A1c2d(d+2)+A2c2dâˆ’2A3cd+nÏƒ2c2d(Î±2(d+2)+Î³2)+d+Ïƒ2
=c2d
A1(d+2)+A2+nÏƒ2(Î±2(d+2)+Î³2)
âˆ’2A3cd+d+Ïƒ2
=d+Ïƒ2âˆ’A3cd.
It completes the proof of (36). Now if assuming Î±=O
1/âˆš
d
,d/n=O(1)and sufficiently large
dimension d, we have the approximate
câ‰ˆÎ±2d+1
Î±4d2n+Î±2d(d+2n)+(d+n)+Ïƒ2(Î±2d+1)
=Î±2d+1
(Î±2d+1)2n+(Î±2d+1)d+Ïƒ2(Î±2d+1)
=1
(Î±2d+1)n+d+Ïƒ2
and
Lâ‹†â‰ˆd+Ïƒ2âˆ’(Î±2d+1)nd
(Î±2d+1)n+d+Ïƒ2.
27B.4 Task-feature Alignment with Î±Correlation
In this section, we consider the task-feature alignment data model similar to (11), where we first draw
task vectorÎ²via
Î²âˆ¼N(0,I).
Then we generate examples (xi,yi)n+1
i=1according to the rule corr_coef (xi,Î²)â‰¥Î±â‰¥0via
xiÎ²âˆ¼N(Î±Î²,I), Î¾ iâˆ¼N(0,Ïƒ2)and yi=Î³Â·xâŠ¤
iÎ²+Î¾i, (41)
which results in (11) by setting Î³2=1/(Î±2d+1).
Theorem 5 (Extended version of Theorem 3) Consider linear model as defined in (41). Recap the
objective from (5a)and let Wâ‹†:=arg min WLPGD(W), andLâ‹†=LPGD(Wâ‹†). Then Wâ‹†andLâ‹†satisfy
Wâ‹†=cI andLâ‹†=dÎ³2(âˆ†0Î±2+1)+Ïƒ2âˆ’cndÎ³2(âˆ†1Î±4+2âˆ†0Î±2+1) (42)
where
c=âˆ†1Î±4+2âˆ†0Î±2+1
âˆ†2Î±6+ âˆ† 3Î±4+ âˆ† 4Î±2+(d+n+1)+Ïƒ2(âˆ†0Î±4+2Î±2+1)/Î³2
andï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£²ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³âˆ†0=d+2
âˆ†1=(d+2)(d+4)
âˆ†2=(d+2)(d+4)(d+6)n
âˆ†3=(d+2)(d+4)(3n+4)
âˆ†4=(d+2)(3n+d+3)+(d+8).
SupposeÎ±=O
1/âˆš
d
,d/n=O(1)anddis sufficiently large. Let Îº=Î±2d+1andÎ³2=1/Îº. Then
Wâ‹†andLâ‹†have approximate forms
Wâ‹†â‰ˆ1
Îºn+(d+Ïƒ2)/ÎºandLâ‹†â‰ˆd+Ïƒ2âˆ’Îºnd
Îºn+(d+Ïƒ2)/Îº. (43)
Proof. Here, for clean notation and without loss of generality, we define and rewrite (41) via
giâˆ¼N(0,I), Î¾ iâˆ¼N(0,Ïƒ2)and xi=Î±Î²+gi,yi=Î³xâŠ¤
iÎ²+Î¾i=Î³Â·(Î±Î²+gi)âŠ¤Î²+Î¾i.
Recap the loss function from (5a), we obtain
L(W)=Eh
(yâˆ’g(Z))2i
=E
Î³xâŠ¤Î²+Î¾âˆ’xâŠ¤WXâŠ¤(Î³XÎ²+Î¾)2
=Eh
Î³2(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2+2Î³(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)+(Î¾âˆ’xâŠ¤WXâŠ¤Î¾)2i
=Î³2Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2i
+Eh
(xâŠ¤WXâŠ¤Î¾)2i
+Ïƒ2. (44)
Similar to Appendix B.3, to begin with, let
N1=tr(W)2+tr
WWâŠ¤
+tr
W2
,N2=tr
WWâŠ¤
,and N3=tr(W),
and additionally, given Î›W=WâŠ™I, let
N4=3tr
Î›2
W
+(d+4)tr
WWâŠ¤
+tr
W2
.
We first focus on the second term in (44)
Eh
(xâŠ¤WXâŠ¤Î¾)2i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­(Î±Î²+g)âŠ¤WnX
i=1Î¾i(Î±Î²+gi)ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=nÏƒ2E
(Î±Î²+g)âŠ¤W(Î±Î²+gâ€²)2
=nÏƒ2
Î±4Eh
(Î²âŠ¤WÎ²)2i
+2Î±2Eh
(Î²âŠ¤W gâ€²)2i
+Eh
(gâŠ¤W gâ€²)2i
=nÏƒ2
Î±4
tr(W)2+tr
W2
+tr
WWâŠ¤
+(2Î±2+1)tr
WWâŠ¤
=nÏƒ2
Î±4N1+(2Î±2+1)N2
.(It follows (27) and independence of Î²,g,gâ€².)
28Next, the first term of (44) (omitting Î³2) returns the following decomposition:
Eh
(xâŠ¤Î²âˆ’xâŠ¤WXâŠ¤XÎ²)2i
=Eh
((Î±Î²+g)âŠ¤(Î²âˆ’WXâŠ¤XÎ²))2i
=E
Î±Î²âŠ¤Î²âˆ’Î±Î²âŠ¤WXâŠ¤XÎ²+gâŠ¤Î²âˆ’gâŠ¤WXâŠ¤XÎ²2
=Î±2E[(Î²âŠ¤Î²)2]+Î±2E[(Î²âŠ¤WXâŠ¤XÎ²)2]+E[(gâŠ¤Î²)2]+E[(gâŠ¤WXâŠ¤XÎ²)2]
âˆ’2Î±2E[Î²âŠ¤Î²Î²âŠ¤WXâŠ¤XÎ²]âˆ’2E[Î²âŠ¤ggâŠ¤WXâŠ¤XÎ²]
=Î±2d(d+2)+Î±2E[(Î²âŠ¤WXâŠ¤XÎ²)2]|                {z                }
(a)+d+E[(gâŠ¤WXâŠ¤XÎ²)2]|                {z                }
(b)
âˆ’2Î±2E[Î²âŠ¤Î²Î²âŠ¤WXâŠ¤XÎ²]|                  {z                  }
(c)âˆ’2E[Î²âŠ¤ggâŠ¤WXâŠ¤XÎ²]|                  {z                  }
(d).
Consider solving (a)-(d)sequentially as follows:
To begin with, we use the following decomposition for all (a)-(d):
XâŠ¤XÎ²=nX
i=1xixâŠ¤
iÎ²
=nX
i=1(Î±Î²+gi)(Î±Î²+gi)âŠ¤Î²
=nX
i=1Î±2Î²Î²âŠ¤Î²+Î±Î²gâŠ¤
iÎ²+Î±giÎ²âŠ¤Î²+gigâŠ¤
iÎ².
Then, we have
(a) :E[(Î²âŠ¤WXâŠ¤XÎ²)2]
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î±2Î²âŠ¤WÎ²Î²âŠ¤Î²+Î±Î²âŠ¤WÎ²gâŠ¤
iÎ²+Î±Î²âŠ¤W g iÎ²âŠ¤Î²+Î²âŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Î±4n2E
Î²âŠ¤WÎ²Î²âŠ¤Î²2
+Î±2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î²âŠ¤WÎ²gâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Î±2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î²âŠ¤W g iÎ²âŠ¤Î²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î²âŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
+2Î±2nEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°nX
i=1Î²âŠ¤WÎ²Î²âŠ¤Î²Î²âŠ¤W g igâŠ¤
iÎ²ï£¹ï£ºï£ºï£ºï£ºï£ºï£»+2Î±2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°nX
i=1Î²âŠ¤WÎ²gâŠ¤
iÎ²Î²âŠ¤W g iÎ²âŠ¤Î²ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
=Î±4n2E
Î²âŠ¤WÎ²Î²âŠ¤Î²2
+Î±2nE
Î²âŠ¤WÎ²gâ€²âŠ¤Î²2
+Î±2nE
Î²âŠ¤W gâ€²Î²âŠ¤Î²2
+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î²âŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
+2Î±2n2Eh
Î²âŠ¤WÎ²Î²âŠ¤Î²Î²âŠ¤W gâ€²gâ€²âŠ¤Î²i
+2Î±2nEh
Î²âŠ¤WÎ²gâŠ¤
iÎ²Î²âŠ¤W g iÎ²âŠ¤Î²i
=Î±4n2(d+4)(d+6)N1+Î±2n(d+4)N1+Î±2n(d+2)(d+4)N2 (45)
+n(nâˆ’1)N1+nN4 (46)
+2Î±2n2(d+4)N1+2Î±2n(d+4)N1 (47)
=
Î±2n(d+4)(Î±2n(d+6)+2n+3)+n(nâˆ’1)
N1+Î±2n(d+2)(d+4)N2+nN4 (48)
=B1N1+B2N2+nN4,
where (45) and (47) utilize (30) and (32), and (46) is obtained via
Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î²âŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»=nE
Î²âŠ¤W gâ€²gâ€²âŠ¤Î²2
+n(nâˆ’1)Eh
Î²âŠ¤W gâ€²gâ€²âŠ¤Î²Î²âŠ¤W gâ€²â€²gâ€²â€²âŠ¤Î²i
=nN4+n(nâˆ’1)N1,
which follows (27) and (28).
29(b) :Eh
(gâŠ¤WXâŠ¤XÎ²)2i
=Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1Î±2gâŠ¤WÎ²Î²âŠ¤Î²+Î±gâŠ¤WÎ²gâŠ¤
iÎ²+Î±gâŠ¤W g iÎ²âŠ¤Î²+gâŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
=Î±4n2E
gâŠ¤WÎ²Î²âŠ¤Î²2
+Î±2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1gâŠ¤WÎ²gâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Î±2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1gâŠ¤W g iÎ²âŠ¤Î²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1gâŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
+2Î±2nEï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°nX
i=1gâŠ¤WÎ²Î²âŠ¤Î²gâŠ¤W g igâŠ¤
iÎ²ï£¹ï£ºï£ºï£ºï£ºï£ºï£»+2Î±2Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£°nX
i=1gâŠ¤WÎ²gâŠ¤
iÎ²gâŠ¤W g iÎ²âŠ¤Î²ï£¹ï£ºï£ºï£ºï£ºï£ºï£»
=Î±4n2E
gâŠ¤WÎ²Î²âŠ¤Î²2
+Î±2nE
gâŠ¤WÎ²gâ€²âŠ¤Î²2
+Î±2nE
gâŠ¤W gâ€²Î²âŠ¤Î²2
+Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1gâŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»
+2Î±2n2Eh
gâŠ¤WÎ²Î²âŠ¤Î²gâŠ¤W gâ€²gâ€²âŠ¤Î²i
+2Î±2nEh
gâŠ¤WÎ²gâŠ¤
iÎ²gâŠ¤W g iÎ²âŠ¤Î²i
=Î±4n2(d+2)(d+4)N2+Î±2n(d+2)N2+Î±2nd(d+2)N2+n(d+n+1)N2 (49)
+2Î±2n2(d+2)N2+2Î±2n(d+2)N2 (50)
=
Î±2n(d+2)(Î±2n(d+4)+2n+d+3)+n(d+nâˆ’1)
N2
=B3N2,
where (49) and (50) are obtained using (27), (30) and
Eï£®ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°ï£«ï£¬ï£¬ï£¬ï£¬ï£¬ï£­nX
i=1gâŠ¤W g igâŠ¤
iÎ²ï£¶ï£·ï£·ï£·ï£·ï£·ï£¸2ï£¹ï£ºï£ºï£ºï£ºï£ºï£ºï£»=nE
gâŠ¤W gâ€²gâ€²âŠ¤Î²2
+n(nâˆ’1)Eh
gâŠ¤W gâ€²gâ€²âŠ¤Î²gâŠ¤W gâ€²â€²gâ€²â€²âŠ¤Î²i
=n(d+2)N2+n(nâˆ’1)N2=n(n+d+1)N2.
(c) :Eh
Î²âŠ¤Î²Î²âŠ¤WXâŠ¤XÎ²i
=nEh
Î²âŠ¤Î²Î²âŠ¤W(Î±Î²+gâ€²)(Î±Î²+gâ€²)âŠ¤Î²i
=Î±2nEh
Î²âŠ¤Î²Î²âŠ¤WÎ²Î²âŠ¤Î²i
+nEh
Î²âŠ¤Î²Î²âŠ¤W gâ€²gâ€²âŠ¤Î²i
=Î±2n(d+2)(d+4)tr(W)+n(d+2)tr(W)
=
Î±2n(d+2)(d+4)+n(d+2)
N3
=B4N3.
(d) :Eh
Î²âŠ¤ggâŠ¤WXâŠ¤XÎ²i
=nEh
Î²âŠ¤ggâŠ¤W(Î±Î²+gâ€²)(Î±Î²+gâ€²)âŠ¤Î²i
=Î±2nEh
Î²âŠ¤ggâŠ¤WÎ²Î²âŠ¤Î²i
+nEh
Î²âŠ¤ggâŠ¤W gâ€²gâ€²âŠ¤Î²i
=Î±2n(d+2)tr(W)+ntr(W)
=
Î±2n(d+2)+n
N3
=B5N3.
Here we define
B1=Î±2n(d+4)(Î±2n(d+6)+2n+3)+n(nâˆ’1)
B2=Î±2n(d+2)(d+4)
B3=Î±2n(d+2)(Î±2n(d+4)+2n+d+3)+n(d+nâˆ’1)
B4=Î±2n(d+2)(d+4)+n(d+2)
B5=Î±2n(d+2)+n.
Then combining all together results in
L(W)=Î³2
Î±2d(d+2)+d+Î±2(B1N1+B2N2+nN4)+B3N2âˆ’2Î±2B4N3âˆ’2B5N3
+nÏƒ2(Î±4N1+(2Î±2+1)N2)+Ïƒ2
=Î³2
Î±2B1N1+(Î±2B2+B3)N2âˆ’2(Î±2B4+B5)N3+Î±2nN4
+nÏƒ2(Î±4N1+(2Î±2+1)N2)+Î³2d
Î±2(d+2)+1
+Ïƒ2
30and differentiating it results in
âˆ‡L(W)=Î³2
Î±2B1âˆ‡N1+(Î±2B2+B3)âˆ‡N2âˆ’2(Î±2B4+B5)âˆ‡N3+Î±2nâˆ‡N4
+nÏƒ2(Î±4âˆ‡N1+(2Î±2+1)âˆ‡N2).
Similar to the proof in Appendix B.3, Wâ‹†has the form of Wâ‹†=cIand we have
âˆ‡N1=âˆ‡
tr(W)2+tr
WWâŠ¤
+tr
W2
=2tr(W)I+2W+2WâŠ¤=2c(d+2)I
âˆ‡N2=âˆ‡tr
WWâŠ¤
=2W=2cI
âˆ‡N3=âˆ‡tr(W)=I
âˆ‡N4=âˆ‡
3tr
Î›2
W
+(d+4)tr
WWâŠ¤
+tr
W2
=6Â·diag(Î›W)+2(d+4)W+2WâŠ¤
=2c(d+8)I.
Therefore, setting âˆ‡L(W)=0returns
Î³2
2c(d+2)Î±2B1+2c(Î±2B2+B3)âˆ’2(Î±2B4+B5)+2c(d+8)Î±2n
+2cnÏƒ2(Î±4(d+2)+2Î±2+1)=0
=â‡’c=Î±2B4+B5
(d+2)Î±2B1+(Î±2B2+B3)+(d+8)Î±2n+nÏƒ2(Î±4(d+2)+2Î±2+1)/Î³2
=Î±4n(d+2)(d+4)+2Î±2n(d+2)+n
Î±6n2(d+2)(d+4)(d+6)+Î±4n(d+2)(d+4)(3n+4)+Î±2n((d+2)(3n+d+3)+(d+8))+n(d+n+1)+nÏƒ2(Î±4(d+2)+2Î±2+1)/Î³2
=Î±4(d+2)(d+4)+2Î±2(d+2)+1
Î±6n(d+2)(d+4)(d+6)+Î±4(d+2)(d+4)(3n+4)+Î±2((d+2)(3n+d+3)+(d+8))+(d+n+1)+Ïƒ2(Î±4(d+2)+2Î±2+1)/Î³2.
Then the optimal loss is obtained by setting Wâ‹†=cIand
Lâ‹†=L(Wâ‹†)=Î³2d(Î±2(d+2)+1)+Ïƒ2âˆ’Î³2(Î±2B4+B5)cd.
It completes the proof of (42). Now if assuming Î±=O
1/âˆš
d
,d/n=O(1),Î³2=1/(Î±2d+1)and
sufficiently large dimension d, we have the approximate
câ‰ˆÎ±4d2+2Î±2d+1
nÎ±6d3+3nÎ±4d2+(3n+d)Î±2d+d+n+Ïƒ2(Î±4d+2Î±2+1)/Î³2
â‰ˆ(Î±2d+1)2
n(Î±2d+1)3+d(Î±2d+1)+Ïƒ2(Î±2d+1)
â‰ˆ1
(Î±2d+1)n+(d+Ïƒ2)/(Î±2d+1)
and
Lâ‹†â‰ˆÎ³2d(Î±2d+1)+Ïƒ2âˆ’Î³2(Î±2d+1)2nd
(Î±2d+1)n+(d+Ïƒ2)/(Î±2d+1)
=d+Ïƒ2âˆ’(Î±2d+1)nd
(Î±2d+1)n+(d+Ïƒ2)/(Î±2d+1).
C Analysis of Low-Rank Parameterization
C.1 Proof of Lemma 3
Proof. Recall the loss function from (34)
L(W)=Mâˆ’2ntr
Î£Â¯W
+n(n+1)tr
Î£Â¯WâŠ¤Â¯W
+nMtrÂ¯WÂ¯WâŠ¤
where Â¯W=Î£1/2
xWÎ£1/2
x,Î£=Î£1/2
xÎ£Î²Î£1/2
xandM=tr(Î£)+Ïƒ2. For any Â¯W, let us parameterize
Â¯W=UEUâŠ¤where UâˆˆRdÃ—rdenotes the eigenvectors of Â¯WandEâˆˆRrÃ—ris a symmetric square
3101020304050607080
# in-context samples0.30.40.50.60.70.80.91.0T est risknmax=30
nmax=50
nmax=80(a) Linear attention
01020304050607080
# in-context samples0.30.40.50.60.70.80.91.0T est risknmax=30
nmax=50
nmax=80 (b) H3
30 40 50 60 70 80
nmax0.450.500.550.60Averaged test risk
Linear Att
H3 (c) Averaged risk
0 10 20 30 40 50
# in-context samples0.20.30.40.50.60.70.80.9T est risk
Linear Att
H3 (d) Evolving Î²
Figure 4: Further comparison for linear attention and H3. In (a)and(b), given maximum context
lengths nmax, we train linear attention and H3 models to minimize the average loss across all positions
nfrom 1tonmax. Averaged test risks are presented in (c). In(d), the task vector Î²evolves gradually
over the context positions iâ‰¤nviaÎ²i=(i/n)Î²1+(1âˆ’i/n)Î²2. In both scenarios, H3 outperforms
linear attention benefiting from its additional convolutional filter (c.f. fin(2b)). Implementation
details are discussed in Section 4.
matrix. We will first treat Uas fixed and optimize E. We will then optimize U. Fixing U, setting
Â¯Î£=UâŠ¤Î£U, we obtain
L(E)=Mâˆ’2ntrÂ¯Î£E
+n(n+1)trÂ¯Î£E2
+nMtr
E2
.
Differentiating, we obtain
0.5nâˆ’1âˆ‡L(E)=âˆ’Â¯Î£+(n+1)Â¯Î£E+ME.
Settingâˆ‡L(E)=0returns
Eâ‹†=(MI+(n+1)Â¯Î£)âˆ’1Â¯Î£. (51)
LetÂ¯Î»idenote the iâ€™th largest eigenvalue of Â¯Î£. Plugging in this value, we obtain the optimal risk as a
function of Uis given by
Lâ‹†(U)=Mâˆ’nÂ·trÂ¯Î£Eâ‹†
=Mâˆ’nÂ·tr
(MI+(n+1)Â¯Î£)âˆ’1Â¯Î£2
(52)
=Mâˆ’nrX
i=1Â¯Î»2
i
(n+1)Â¯Î»i+M=Mâˆ’nrX
i=1Â¯Î»i
n+1+MÂ¯Î»âˆ’1
i. (53)
Now observe that, the right hand side is strictly decreasing function of the eigenvalues Â¯Î»iofÂ¯Î£=
UâŠ¤Î£U. Thus, to minimize Lâ‹†(U), we need to maximizePr
i=1Â¯Î»i
n+1+MÂ¯Î»âˆ’1
i. It follows from Cauchy
interlacing theorem that Â¯Î»jâ‰¤Î»iwhereÎ»iis the iâ€™th largest eigenvalue of Î£since Â¯Î£is an orthogonal
projection of Î£onU. Consequently, we find the desired bound where
Lâ‹†=Mâˆ’nrX
i=1Î»i
n+1+MÎ»âˆ’1
i.
The equality holds by setting Uto be the top- reigenvectors of Î£andE=Eâ‹†(U)to be the diagonal
matrix according to (51).
D Additional Experiments
In this section, we present additional experiments demonstrating that the H3 model can outperform
the linear attention model under different training or data settings. The implementation details are
consistent with those outlined in Section 4.
â€¢H3 outperforms linear attention (Figure 4). Until now, our analysis has established the equiva-
lence between linear attention and H3 models in solving linear ICL problem. Furthermore, we also
investigate settings where H3 could outperform linear attention due to its sample weighting ability.
In Figs. 4a and 4b, instead of training separate models to fit the different context lengths, we train
a single model with fixed max-length nmaxand loss is evaluated as the average loss given samples
from 1tonmax. Such setting has been wildly studied in the previous ICL work [Garg et al., 2022,
32AkyÃ¼rek et al., 2023, Li et al., 2023]. We generate data according to (7)withÎ£x=Î£Î²=Idand
Ïƒ=0, and train 1-layer linear attention (Fig. 4a) and H3 (Fig. 4b) models with different max-lengths
nmax=30,50,80. Comparison between Fig. 4a and 4b shows that 1-layer attention and H3 implement
different algorithms in solving the averaged linear regression problem and H3 is more consistent
in generalizing to longer context lengths. In Fig. 4c, we plot the averaged risks for each model and
H3 outperforms linear attention. Furthermore, in Fig. 4d, we focus on the setting where in-context
examples are generated using evolving task vector Î². Specifically, consider that each sequence corre-
sponds to two individual task parameters Î²(1)âˆ¼N(0,Id)andÎ²(2)âˆ¼N(0,Id). Then the iâ€™th sample is
generated via xiâˆ¼N(0,Id)andyi=Î²âŠ¤
ixiwhereÎ²i=Î»iÎ²(1)+(1âˆ’Î»i)Î²(2)andÎ»i=i/n. The results
are reported in Fig. 4d which again shows that H3 achieves better performance compared to linear
attention, as H3 may benefit from the additional convolutional filter (c.f. fin(2b)). Here, dotted
curve represent the theoretical results under i.i.d. and noiseless setting, derived from Corollary 1.
E Extended Related Work
There is growing interest in understanding the mechanisms behind ICL [Brown et al., 2020, Liu
et al., 2023b, Rae et al., 2021] in large language models (LLMs) due to its success in continuously
enabling novel applications for LLMs [GeminiTeam et al., 2023, OpenAI, 2023, Touvron et al., 2023].
Towards this, Xie et al. [2022] explain ICL by language modelâ€™s ability to perform implicit Bayesian
inference where, under specific assumptions on the pre-training data distribution, the model infers a
shared latent concept among the in-context examples and leverages the concept to make a prediction.
MÃ¼ller et al. [2021], Hollmann et al. [2022], MÃ¼ller et al. [2023] introduce prior-data fitted network
(PFN) to approximate Bayesian inference on synthetic datasets and use it to perform downstream
tasks such as tabular dataset classification. On the other hand, Olsson et al. [2022] posit induction
heads as the key mechanism enabling ICL in Transformers. Park et al. [2024] study how various
distributional properties of training data aid in the emergence of ICL in Transformers.
In the previous work, Garg et al. [2022] explored ICL ability of Transformers. In particular, they
considered in-context prompts where each in-context example is labeled by a target function from
a given function class, including linear models. A number of works have studied this and related
settings to develop a theoretical understanding of ICL [von Oswald et al., 2023, Gatmiry et al., Collins
et al., 2024, Lin and Lee, 2024, Li et al., 2024, Bai et al., 2024, AkyÃ¼rek et al., 2023, Zhang et al.,
2023, Du et al., 2023]. AkyÃ¼rek et al. [2023] focus on linear regression and provide a construction
of Transformer weights that can enable a single step of GD based on in-context examples. They
further show that Transformers trained on in-context prompts exhibit behaviors similar to the models
recovered via explicit learning algorithm on the in-context examples in a prompt. Along the similar
line, V on Oswald et al. [2023] provide a construction of weights in linear attention-only Transformers
that can emulate GD steps on in-context examples for a linear regression task. Interestingly, they find
similarity between their constructed networks and the networks resulting from training on in-context
prompts corresponding to linear regression tasks. Similar to this line of work, Dai et al. [2023] argue
that pre-trained language models act as meta-optimizer which utilize attention to apply meta-gradients
to the original language model based on the in-context examples. Focusing on various NLP tasks,
they further connect it to a specific form of explicit fine-tuning that performs gradient updates to
the attention-related parameters. Inspired by the connection between linear attention and GD, they
developed a novel attention mechanism that mirrors the behavior of GD with momentum. Beyond
Transformers, existing work [Lee et al., 2023, Zucchet et al., 2023, Grazzi et al., 2024] demonstrate
that other model architectures, such as SSM and RNNs, are also capable of in-context learning (ICL).
Building on these primarily empirical studies, Zhang et al. [2024], Mahankali et al. [2024], Ahn
et al. [2023], Duraisamy [2024] focus on developing a theoretical understanding of Transformers
trained to perform ICL. For single-layer linear attention model trained on in-context prompts for
random linear regression tasks with isotropic Gaussian features and isotropic Gaussian weight vectors,
Mahankali et al. [2024], Ahn et al. [2023] show that the resulting model implements a single step
of GD on in-context examples in a test prompt, thereby corroborating the findings of [V on Oswald
et al., 2023]. They also show that the learned model implements a PGD step, when faced with
anisotropic Gaussian features, with Mahankali et al. [2024] also considering anisotropic Gaussian
weight vectors. Ahn et al. [2023] further study multi-layer model and show that the trained model can
implement a generalization of GD++ algorithm, supporting an empirical observation in V on Oswald
et al. [2023]. On the other hand, Mahankali et al. [2024] extend their single-layer setup to consider
33suitable non-linear target functions, showing that learned Transformer again implements a single
step of GD on lineare regression objective. For a single-layer linear attention model, Zhang et al.
[2024] study the optimization dynamics of gradient flow while training such a model on in-context
prompts for random linear regression tasks. Despite the non-convexity of the underlying problem,
they show the convergence to the global minimum of the population objective. Similar to Mahankali
et al. [2024], Ahn et al. [2023], they show that the trained model implements a single step of GD and
PGD for isotropic and anisotropic Gaussian features, respectively. In addition, they also characterize
the test-time prediction error for the trained model while highlighting its dependence on train and test
prompt lengths. Interestingly, Zhang et al. [2024] further explore the effect of various distributional
shifts, including the shift in task weight vector distributions between train and test time as well
as the covariate shifts among train and test in-context prompts. Interestingly, they find that while
linear-attention models are robust to most shifts, they exhibit brittleness to the covariate shifts.
While our work shares similarities with this line of works, as discussed in our contributions in the
introduction, we expand the theoretical understanding of ICL along multiple novel dimensions,
which includes the first study of LoRA adaptation for ICL in the presence of a distributional shift.
Furthermore, we strive to capture the effect of retrieval augmentation [Lewis et al., 2020, Nakano
et al., 2021] on ICL through our analysis. Retrieval augmentation allows for selecting most relevant
demonstration out of a large collection for a test instance, e.g., via a dense retrieval model [Izacard
et al., 2023], which can significantly outperform the typical ICL setup where fixed task-specific
demonstrations are provided as in-context examples [Wang et al., 2022, Basu et al., 2023]. Through
a careful modeling of retrieval augmentation via correlated design, we show that it indeed has
a desirable amplification effect where the effective number in-context examples becomes larger
with higher correlation which corresponds to preforming a successful retrieval of query-relevant
demonstrations in a practical retrieval augmented setup.
Recently, state space models (SSMs) [Gu et al., 2021b,a, Fu et al., 2023, Gu and Dao, 2023] have
appeared as potential alternatives to Transformer architecture, with more efficient scaling to input
sequence length. Recent studies demonstrate that such SSMs can also perform ICL for simple
non-language tasks [Park et al., 2024, Grazzi et al., 2024] as well as complex NLP tasks [Grazzi et al.,
2024]. That said, a rigorous theoretical understanding of ICL for SSMs akin to Zhang et al. [2024],
Mahankali et al. [2024], Ahn et al. [2023] is missing from the literature. In this work, we provide the
first such theoretical treatment for ICL with SSMs. Focusing on H3 architecture [Fu et al., 2023], we
highlight its advantages over linear attention in specific ICL settings.
34NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: All the theoretical contributions claimed in the abstract and introduction
along with the underlying data model are presented in Section 2 and Section 3.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: This is a theoretical study which (similar to prior studies in the field)
relies on a precise but simplified data model to draw quantitatively precise conclusions.
All the assumptions on the data model are clearly stated in Section 2 and Section 3.
We have also added a paragraph after the conclusion to specifically highlight various
limitations of our work.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
35Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: As discussed above, for all of our theoretical results and proofs we state
the precise setup and assumptions in Section 2 and Section 3. Due to page limit, proofs
are deferred to the supplemental material.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: This is primarily a theoretical work where detailed synthetic experiments
(on the same data model studied in our theoretical analysis) have been conducted to
corroborate our theoretical findings. We provide sufficient details in Section 4 for
reproducing these experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
36(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: As discussed above, this paper conducts small scale synthetic experiments
to corroborate our theoretical findings. We have provided sufficient details to reproduce
these experiments in Section 4.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: All the relevant details for our small scale experiments are provided in
Section 4.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
37Justification: This work is a theoretical work studying the optimization landscape of
linear attention/H3 under population risk. Then our goal of simulations is to find the
optimal solution corresponding to the minimal risks. Therefore, we do not report the
error bars and we have included the discussion in the experiment section.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: Our work only focuses on 1-layer attention/H3 model training with hidden
dimension 21and maximal context length <100, which can be implemented easily.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Yes, the authors confirm that the research conducted in the paper conform
wiht the NeurIPS Code of Ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
38â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: In its current form, we donâ€™t see any specific negative impacts of our
theoretical study. However, we have discussed potential broader impacts of the future
extensions of this work, e.g., the ones tied to eliciting undesirable behavior of LLMs with
in-context learning, while discussing the limitations of the work after the conclusion
section.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The synthetic setup studied in the paper does not pose such risks.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
39Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not rely on existing assets.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer:[NA]
Justification: The paper does not release new assets such as code, data, or models.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human
subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
4015.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human
subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
41