The Many Faces of Optimal Weak-to-Strong Learning
Mikael M√∏ller H√∏gsgaard
Department of Computer Science
Aarhus University
hogsgaards@cs.au.dkKasper Green Larsen
Department of Computer Science
Aarhus University
larsen@cs.au.dk
Markus Engelund Mathiasen
Department of Computer Science
Aarhus University
markusm@cs.au.dk
Abstract
Boosting is an extremely successful idea, allowing one to combine multiple low
accuracy classifiers into a much more accurate voting classifier. In this work, we
present a new and surprisingly simple Boosting algorithm that obtains a provably
optimal sample complexity. Sample optimal Boosting algorithms have only recently
been developed, and our new algorithm has the fastest runtime among all such
algorithms and is the simplest to describe: Partition your training data into 5 disjoint
pieces of equal size, run AdaBoost on each, and combine the resulting classifiers via
a majority vote. In addition to this theoretical contribution, we also perform the first
empirical comparison of the proposed sample optimal Boosting algorithms. Our
pilot empirical study suggests that our new algorithm might outperform previous
algorithms on large data sets.
1 Introduction
Boosting is one of the most powerful machine learning ideas, allowing one to improve the accuracy
of a simple base learning algorithm A. The main idea in Boosting, is to iteratively invoke the base
learning algorithm Aon modified versions of a training data set. Each invocation of Areturns a
classifier, and these classifiers are finally combined via a majority vote or averaging. Variations of
Boosting, including Gradient Boosting [ 10,16,7], are often among the best performing classifiers
in practice, especially when data is tabular. Furthermore, when combined with decision trees or
regressors as the base learning algorithm, these algorithms are independent of scaling of data features
and provides impressive out-of-the-box performance. See the excellent survey [ 22] for further details.
The textbook Boosting algorithm for binary classification, AdaBoost [ 9], works by maintaining a
weighing Dt= (Dt(1), . . . , D t(m))of a training set S= (x1, y1), . . . , (xm, ym)with(xi, yi)‚àà
X √ó {‚àí 1,1}for an input domain Xand labels {‚àí1,1}. In each Boosting iteration t, a classifier
ht:X ‚Üí {‚àí 1,1}is trained to minimize the 0/1-loss on S, but with samples weighed according
toDt. The weights are then updated such that samples (xi, yi)misclassified by hthave a larger
weight under Dt+1and correctly classified samples have a smaller weight. Finally, after a sufficient
number of iterations T, AdaBoost combines the classifiers h1, . . . , h Tinto a voting classifier f(x) =
sign(P
tŒ±tht(x))taking a weighted majority vote among the predictions made by the ht‚Äôs. Here the
Œ±t‚Äôs are real-valued weights depending on the accuracy of htonDt.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Weak-to-Strong Learning. Boosting was originally introduced to address a theoretical question
asked by Kearns and Valiant [ 17,18] onweak-to-strong learning. A learning algorithm Ais called
a weak learner if, for anydistribution DoverX √ó {‚àí 1,1}, when given some m0i.i.d. training
samples SfromD, it produces with probability at least 1‚àíŒ¥0, a classifier/hypothesis hS‚àà H with
erD(hS)‚â§1/2‚àíŒ≥, where erD(h) = Pr (x,y)‚àºD[h(x)Ã∏=y]andH ‚äÜ X ‚Üí {‚àí 1,1}is a predefined
hypothesis set used by A. A weak learner thus produces, for any distribution D, a hypothesis that
performs slightly better than random guessing when given enough samples from D. The parameter Œ≥
is called the advantage of the weak learner and we refer to the weak learner as a Œ≥-weak learner. We
think of Œ¥0andm0as constants that may depend on H, but not D. A strong learner in contrast, is an
algorithm that for any distribution D, and any parameters (Œµ, Œ¥)with0< Œµ, Œ¥ < 1, when given m(Œµ, Œ¥)
i.i.d. samples SfromD, produces with probability at least 1‚àíŒ¥a hypothesis hS:X ‚Üí {‚àí 1,1}
witherD(hS)‚â§Œµ. Here m(Œµ, Œ¥)is the sample complexity of the strong learner. A strong learner
thus obtains arbitrarily high accuracy when given enough training samples. With these definitions in
place, Kearns and Valiant asked whether it is always possible to produce a strong learner from a weak
learner. This was indeed shown to be the case [ 25], and AdaBoost is one among many examples of
algorithms producing a strong learner from a weak learner.
Sample Complexity. Given that weak-to-strong learning is always possible, a natural question is
"what is the best possible sample complexity m(Œµ, Œ¥)of weak-to-strong learning?". This is known
to depend on the VC-dimension dof the hypothesis set Hused by the weak learner, as well as the
advantage Œ≥of the weak learner. In particular, the best known analysis [ 27] of AdaBoost shows that
it achieves a sample complexity mAda(Œµ, Œ¥)of
mAda(Œµ, Œ¥) =Odln(1/(ŒµŒ≥)) ln(d/(ŒµŒ≥))
Œ≥2Œµ+ln(1/Œ¥)
Œµ
. (1)
Larsen and Ritzert [ 20] were the first to give an algorithm improving over AdaBoost. Their algorithm
has a sample complexity mLR(Œµ, Œ¥)of
mLR(Œµ, Œ¥) =Od
Œ≥2Œµ+ln(1/Œ¥)
Œµ
.
They further complemented their algorithm with a lower bound proof showing that any weak-to-strong
learning algorithm must have a sample complexity m(Œµ, Œ¥)of
m(Œµ, Œ¥) = ‚Ñ¶d
Œ≥2Œµ+ln(1/Œ¥)
Œµ
.
The optimal sample complexity for weak-to-strong learning is thus fully understood from a theoretical
point of view.
Other Performance Metrics. Sample complexity is however not the only interesting performance
metric of a weak-to-strong learner. Furthermore, O(¬∑)-notation may hide constants that are too large
for practical purposes. It is thus worthwhile to develop alternative optimal weak-to-strong learners
and compare their empirical performance.
The algorithm of Larsen and Ritzert for instance has a rather slow running time as it invokes the
weak-learner a total of O(mlg43Œ≥‚àí2lnm) =O(m0.8Œ≥‚àí2)times on a training set of msamples.
This should be compared to AdaBoost that only invokes the weak learner O(Œ≥‚àí2lnm)times to
achieve the sample complexity stated in (1).
An alternative sample optimal weak-to-strong learner was given by Larsen [ 19] as a corollary of a
proof that Bagging [ 5] is an optimal PAC learner in the realizable setting. Concretely, his work gives
a weak-to-strong learner with an optimal sample complexity while only invoking the weak-learner a
total of O(Œ≥‚àí2ln(m/Œ¥) lnm)times on a training set of msamples.
A natural question is whether the sample complexity of AdaBoost shown in (1)can be improved
to match the optimal sample complexity by a better analysis. Since AdaBoost only invokes its
weak learner O(Œ≥‚àí2lnm)times on msamples, this would be an even more efficient optimal weak-
to-strong learner. Unfortunately, work by H√∏gsgaard et al. [ 15] shows that AdaBoost‚Äôs sample
complexity is sub-optimal by at least a ln(1/Œµ)factor, i.e.
mAda(Œµ, Œ¥) = ‚Ñ¶dln(1/Œµ)
Œ≥2Œµ+ln(1/Œ¥)
Œµ
. (2)
2It thus remains an intriguing task to design weak-to-strong learners that have an optimal sample
complexity and yet match the runtime guarantees of AdaBoost. Furthermore, do the theoretical
improvements translate to practice? Or are there large hidden constant factors in the O(¬∑)-notation?
And how does it vary among the different weak-to-strong learners?
1.1 Our Contributions
In this work, we first present a new weak-to-strong learner with an optimal sample complexity
(at least in expectation). The algorithm, called MAJORITY -OF-5and shown as Algorithm 1, is
extremely simple: Partition the training set into 5 disjoint pieces of size m/5and run AdaBoost
on each to produce voting classifiers f1, . . . , f 5. Finally, combine them by taking a majority vote
g(x) = sign(P5
t=1ft(x)). This simple algorithm only invokes the weak learner O(Œ≥‚àí2lnm)times,
asymptotically matching AdaBoost and improving over previous optimal weak-to-strong learners.
Furthermore, since each invocation of AdaBoost is on a training set of only m/5samples, it is at
least as fast as AdaBoost, even when considering constant factors. It is even trivial to parallelize the
algorithm among up to 5machines/threads.
Algorithm 1: MAJORITY -OF-5(S,W)
Input: Training set S= (x1, y1), . . . , (xm, ym). Weak learner W.
Result: Hypothesis g:X ‚Üí {‚àí 1,1}.
1Partition Sinto5disjoint pieces S1, . . . , S 5of size m/5.
2fort= 1, . . . , 5do
3 Run AdaBoost on StwithWto obtain ft:X ‚Üí {‚àí 1,1}.
4g‚Üêsign(P
tft).
5return g
The concrete guarantees we give for M AJORITY -OF-5 are as follows
Theorem 1. For any distribution DoverX √ó{‚àí 1,1}and any Œ≥-weak learner Wusing a hypothesis
setHof VC-dimension d, it holds for a training set S‚àº Dmthat running MAJORITY -OF-5onSto
obtain a hypothesis gsatisfies
E[erD(g)] =Od
Œ≥2m
.
In particular, Theorem 1 implies that E[erD(g)]‚â§Œµwhen given m= Œò( d/(Œ≥2Œµ))samples. This is a
slightly weaker guarantee than the alternative optimal weak-to-strong learners in the sense that we do
not provide high probability guarantees (i.e. with probability 1‚àíŒ¥). On the other hand, our algorithm
is extremely simple and has a running time comparable to AdaBoost. Furthermore, the proof that
AdaBoost is sub-optimal (see (2)) shows that even the expected error of AdaBoost is sub-optimal by
a logarithmic factor. It is interesting that combining a constant number of voting classifiers trained by
AdaBoost makes it optimal when a single AdaBoost is provably sub-optimal. Let us also comment
that the analysis of Algorithm 1 is based on recent work by Aden-Ali et al. [ 1] on optimal PAC
learning in the realizable setting, demonstrating new applications of their techniques. Furthermore,
we believe the number 5is an artifact of our proof and we conjecture that it can be replaced with 3
by giving a better generalization bound for large margin voting classifiers. See Section 3 for further
details.
Empirical Comparison. Our second contribution is a pilot empirical study, which gives the first
empirical comparison between the alternative optimal weak-to-strong learners, both the algorithm
of Larsen and Ritzert [ 20], the Bagging+Boosting based algorithm [ 19], our new MAJORITY -OF-5
algorithm, as well as classic AdaBoost. We give the full details of the alternative algorithms in
Section 2. In our experiments, we compare their performance both on real-life data as well as the data
distribution used by H√∏gsgaard et al. [ 15] in their proof that AdaBoost is sub-optimal as shown in (2).
Our pilot empirical study give an indication that our new algorithm MAJORITY -OF-5may outperform
previous algorithms on large data sets, whereas Bagging+Boosting performs best on small data sets.
See Section 4 for further details and the results of these experiments.
32 Previous Optimal Weak-to-Strong Learners
In this section, we present the two previous optimal weak-to-strong learners. The first such algorithm,
by Larsen and Ritzert [ 21], builds on a sub-sampling technique due to Hanneke [ 14] in his seminal
work on optimal PAC learning in the realizable setting. This sub-sampling technique, named
SUBSAMPLE , is shown as Algorithm 2.
Algorithm 2: SUBSAMPLE (S, T)
Input: Training set S, Stash T
Result: List of training sets L.
1if|S|<4then
2 LetLcontain the single training set S‚à™T.
3 return L
4Partition Sinto4disjoint pieces S0, S1, S2, S3of size |S|/4each.
5LetLbe an empty list.
6Append S UBSAMPLE (S0,T‚à™S2‚à™S3) toL.
7Append S UBSAMPLE (S0,T‚à™S1‚à™S3) toL.
8Append S UBSAMPLE (S0,T‚à™S1‚à™S2) toL.
9return L
Given a training set S,SUBSAMPLE generates a list Lof subsets Si‚äÇS. The list Lhas size
mlg43‚âàm0.79when invoking SUBSAMPLE (S,‚àÖ) for a training set Sof size m. Larsen and
Ritzert now give an optimal weak-to-strong learner using SUBSAMPLE as a sub-routine as shown in
Algorithm 3.
Algorithm 3: LARSEN RITZERT (S,W)
Input: Training set S= (x1, y1), . . . , (xm, ym). Weak learner W.
Result: Hypothesis g:X ‚Üí {‚àí 1,1}.
1Invoke S UBSAMPLE (S,‚àÖ) to obtain list L=S1, . . . , S k.
2fort= 1, . . . , k do
3 Run AdaBoost on StwithWto obtain ft:X ‚Üí {‚àí 1,1}.
4g‚Üêsign(P
tft).
5return g
Finally, the algorithm by Larsen [ 19] based on Bagging (a.k.a. Bootstrap Aggregation) by Breiman [ 5],
combines AdaBoost with sampling subsets of the training data with replacement. Unlike the algorithm
above by Larsen and Ritzert, it requires a target failure probability Œ¥as input. The algorithm is shown
as Algorithm 4.
Algorithm 4: BAGGED ADABOOST (S,W, Œ¥)
Input: Training set S= (x1, y1), . . . , (xm, ym). Weak learner W. Failure probability
0< Œ¥ < 1.
Result: Hypothesis g:X ‚Üí {‚àí 1,1}.
1fort= 1, . . . , O (ln(m/Œ¥))do
2 LetStbe a set of mindependent samples with replacement from S.
3 Run AdaBoost on StwithWto obtain ft:X ‚Üí {‚àí 1,1}.
4g‚Üêsign(P
tft).
5return g
3 Analysis of M AJORITY -OF-5
In this section, we give the proof of Theorem 1, showing that our new algorithm MAJORITY -
OF-5has an optimal expected error. Before giving the formal details, we present the main ideas
in our proof. Our analysis is at a high level inspired by recent work of Aden-Ali et al. [ 1] for
4realizable PAC learning. The first ingredient we need is the notion of margins. For a voting classifier
f(x) = sign(P
h‚ààHŒ±hh(x))withŒ±h‚â•0for all h, consider the function f‚Ä≤(x) =P
h‚ààHŒ±‚Ä≤
hh(x)
withŒ±‚Ä≤
h=Œ±h/P
hŒ±h. That is, f‚Ä≤is simply the voting classifier fwithout the sign(¬∑)and normalized
to have coefficients summing to 1. The margin of fon a sample (x, c(x))is then c(x)f‚Ä≤(x)‚àà[‚àí1,1].
The margin is 1if all hypotheses combined by fagree and are correct. It is 0if half of the mass is on
hypothesis that are correct and half of the mass is on the hypothesis that are wrong. We can thus think
of the margin as a confidence of the voting classifier. Margins have been extensively studied in the
context of boosting and were originally introduced to give theoretical justification for the impressive
practical performance of AdaBoost [ 2]. In particular, there are strong generalization bounds for
voting classifiers with large margins [ 2,6,11]. Indeed, the best known sample complexity bound for
AdaBoost, as stated in (1), is derived by showing that AdaBoost produces a voting classifier with
margins ‚Ñ¶(Œ≥).
Returning to our outline of the proof of Theorem 1, recall that the optimal error for weak-to-strong
learning as a function of the number of samples misO(d/(Œ≥2m)). Now assume we can prove that
for a set of mi.i.d. samples from a distribution D, the expected maximum error under Dof any
voting classifier that has margins ‚Ñ¶(Œ≥)on all the samples, is no more than O(p
d/(Œ≥2m)). This fact
follows from previous work on Rademacher complexity. Note that this is sub-optimal compared
to our target error by a polynomial factor since‚àöx‚â•xforxbetween 0and1. We want to argue
that combining 5instantiations of AdaBoost on disjoint training sets reduces this expected error to
optimal O(d/(Œ≥2m)).
For this argument, consider running AdaBoost on n=m/5samples. For any x‚àà X, consider the
probability px= Pr S‚àºDn[fS(x)Ã∏=c(x)]where fSis the hypothesis produced by AdaBoost on S
andc(x)is the correct label of x. Inspired by Aden-Ali et al. [ 1], we now partition the input domain
Xinto sets Ri, such that Ricontains all xfor which px‚àà(2‚àíi,2‚àíi+1]. The crucial observation
is that if we consider kindependently trained AdaBoosts, then the probability they all err on xis
precisely pk
x. Since a majority vote among 5 classifiers only fails when at least 3 of the involved
classifiers fail, combining 5AdaBoosts intuitively reduces the contribution to the expected error from
points x‚ààRitoPrX‚àºD[X‚ààRi]2‚àí3i. What remains is thus to argue that Pr[X‚ààRi]is small.
This last step is done by considering the distribution Di, which is Dconditioned on receiving a sample
from Ri. The expected number of samples we see from Riismi= Pr[ X‚ààRi]m. Furthermore,
since AdaBoost obtains margins ‚Ñ¶(Œ≥)on all its training data, it in particular obtains margins ‚Ñ¶(Œ≥)on
all its samples from Di. This leads to an error probability of pi=O(p
d/(Œ≥2mi))underDi. But the
definition of Riimplies pi‚â•2‚àíi. Hencep
d/(Œ≥2mi) = ‚Ñ¶(2‚àíi)‚áími=O(22id/Œ≥2)‚áíPr[X‚àà
Ri] =O(22id/(Œ≥2m)). By summing over all Ri, the final expected error is hence
‚àûX
i=122id2‚àí3i
Œ≥2m=O(d/(Œ≥2m)).
This completes the proof overview. Let us end by making a few remarks. First, it is worth noting
that the generalization bound O(p
d/(Œ≥2m))seems much worse than the bound in (1)claimed for
AdaBoost and other voting classifiers with large margins. Unfortunately, if we examine (1)carefully
and state Œµas a function of m, we get Œµ=O(dln(m/d) lnm/(Œ≥2m)). The problem is that the two
log-factors are not bounded by a polynomial in d/(Œ≥2m). In particular for m=Cd/Œ≥2withC >0
a constant, any polynomial in d/(Œ≥2m)must be constant. But lnm= ln( Cd/Œ≥2)is not a constant
independent of dandŒ≥. Thus we have to use the generalization bound with‚àö¬∑that fortunately is
within a polynomial factor of optimal for the full range of m. Let us also comment that if the lower
bound for AdaBoost stated in (2)is tight also for Œ≥-margin voting classifiers, i.e. matched by an upper
bound, then it suffices to take a majority of 3 AdaBoosts for optimal sample complexity.
This concludes the description of the high level ideas in our proof.
3.1 Formal Analysis
We now give the formal details of the proof. We start by introducing some notation.
5Preliminaries. For a hypothesis set H, we let ‚àÜ(H)denote the set of linear classifiers using
hypothesis from Hthat is
‚àÜ(H) =(
f‚àà {‚àí 1,1}X:f=X
h‚ààHŒ±f
hh,‚àÄh‚àà H, Œ±f
h‚â•0,X
h‚ààHŒ±f
h= 1)
.
Note that we have termed these linear classifiers rather than voting classifier, to distinguish that we
have not yet applied a sign(¬∑)and insist on normalizing the coefficients so they sum to 1. We define
thesign function as being 1 when the value is non-negative (so also 1when x= 0) and‚àí1when
negative.
We let c‚àà {‚àí 1,1}Xbe a true labeling that we are trying to learn. For a distribution DoverX,
we define the expected loss of f‚àà[‚àí1,1]XaserD(f) :=EX‚àºD[1{sign(f)(X)Ã∏=c(X)}]and we
will use S‚àà Xmto denote a point set of mi.i.d. samples from Di.e.S‚àº Dm(we see the point
set as a vector so we allow repetition of points). Define for any k‚ààNthe majority of klinear
classifiers f1, . . . , f k‚àà[‚àí1,1]XasMaj(f1, . . . , f k)(x) = sign(P
isign(fi(x))). Letsbe a set of
points, i.e. s‚àà ‚à™‚àû
i=1Xiandc(s)‚àà {‚àí 1,1}|s|the labeling of the points that scontains with c, that
isc(s)i=c(si). We define a Œ≥-margin classifier algorithm f:‚à™‚àû
i=1(X √ó {‚àí 1,1})i‚áí[‚àí1,1]Xto
be a mapping that takes as input a point set with labels (s, c(s))‚àà ‚à™‚àû
i=1(X √ó {‚àí 1,1})iand outputs
a function f(s, c(s))(¬∑)fromXto the interval [‚àí1,1], where f(s, c(s))(¬∑)is such that for x‚ààs,
f(s, c(s))(x)c(x)‚â•Œ≥. In the following we will use fsto denote f(s, c(s))and write f‚àà‚àÜ(H)if
for any (s, c(s))‚àà ‚à™‚àû
i=1(X √ó {‚àí 1,1})iwe have that the output of the Œ≥-margin classifier algorithm
f(s, c(s))is in‚àÜ(H).
Analysis. We now prove Theorem 1, which is a direct consequence of the following Corollary 2.
Corollary 2 states that running a Œ≥-margin classifier algorithm on 5disjoint training sets of size m
and forming the majority vote of the produced 5classifiers, has the optimal O(d/(Œ≥2m))expected
error. Since AdaBoost, after O(ln(m)/Œ≥2)iterations, has ‚Ñ¶(Œ≥)margins on all points [ 26] [Section
5.4.1], Corollary 2 gives the claim in Theorem 1. Alternatively one could run AdaBoost‚àó
v[24] instead
of AdaBoost. The proof of Corollary 2 follows the method used in [ 1] where the authors show a
similar bound for PAC learning in the realizable setting. We now state Corollary 2.
Corollary 2. For any distribution DoverX, hypothesis set Hwith VC-dimension d, i.i.d. point sets
S1, . . . , S 5fromDm, margin 0< Œ≥‚â§1andf‚àà‚àÜ(H)being a Œ≥-margin classifier algorithm, we
have that
ES1,...,S 5‚àºDm[erD(Maj(fS1, . . . , f S5))] = Od
Œ≥2m
.
The result in Corollary 2 is primarily a consequence of the following Lemma 3, which says that, in
expectation, the probability that fS1, fS2, fS3all misclassifying a sample from DisO(d/(Œ≥2m)).
We now state Lemma 3 and give the proof of Corollary 2. We postpone the proof of Lemma 3 to later
in this section.
Lemma 3. For any distribution DoverX, hypothesis set Hwith VC-dimension d, i.i.d. point sets
S1, S2, S3fromDm, margin 0< Œ≥‚â§1andf‚àà‚àÜ(H)being a Œ≥-margin classifier algorithm we
have that
ES1,S2,S3‚àºDm
PX‚àºD
‚à©3
i=1{sign(fSi(X))Ã∏=c(X)}
=Od
Œ≥2m
.
Proof of Corollary 2. For the majority of fS1, . . . , f S5to fail on an x‚àà X, it must be the case that at
least3of the trained Œ≥-margin classifiers fSihavesign(fSi(x))Ã∏=c(x). Using this combined with
theSi‚Äôs being i.i.d. we get that
ES1,...,S 5‚àºDm[erD(Maj(fS1, . . . , f S5))]
‚â§X
1‚â§j1<j2<j3‚â§5ESj1,Sj2,Sj3‚àºDm
PX‚àºD
‚à©3
i=1{sign(fSji(X))Ã∏=c(X)}
‚â§ES1,S2,S3‚àºDm
PX‚àºD
‚à©3
i=1{sign(fSi(X))Ã∏=c(X)} X
1‚â§j1<j2<j3‚â§51.
Now using Lemma 3 and 5
3
=O(1), we get that the above is O
d
Œ≥2m
as claimed.
6We now move on to prove Lemma 3. For this we need Lemma 4 which we now state and give the
proof in Appendix A.
Lemma 4. Leta >1denote a universal constant. For Da distribution, Ra subset of Xsuch that
P[R] :=PX‚àºD[X‚ààR]Ã∏= 0, hypothesis set Hwith VC-dimension d, Sa point set of mi.i.d. points
fromD, margin 0< Œ≥‚â§1andf‚àà‚àÜ(H)being a Œ≥-margin classifier algorithm we have that
ES[EX‚àºDR[1{fS(X)Ã∏=c(X)}]] =ES[erDR(fS)]‚â§s
ad
P[R]Œ≥2m.
where we use DRto denote the conditional distribution on the subset R. That is, for any measurable
function g,EX‚àºDR[g(X)] :=EX‚àºD[g(X)1{X‚ààR}]/PX‚àºD[X‚ààR].
Proof of Lemma 3. Forx‚àà X letpx=ES‚àºDm[1{fS(x)Ã∏=c(x)}]and define for i= 1, . . . the
setsRi={x‚àà X :px‚àà 
2‚àíi,2‚àíi+1
}. Now using Tonelli, and that S1, S2, S3are i.i.d. with
distribution Dm, and that px‚â§2‚àíi+1forx‚ààRiwe get that
ES1,...,S 3‚àºDm
PX‚àºD
‚à©3
i=1{fSi(X)Ã∏=c(X)}
=EX‚àºD
ES1,...,S 3‚àºDm
1{‚à©3
i=1{fSi(X)Ã∏=c(X)}}
=EX‚àºD
p3
X
=‚àûX
i=1EX‚àºD
p3
X|X‚ààRi
P[X‚ààRi]‚â§23‚àûX
i=12‚àí3iP[X‚ààRi],
thus if we can show that there exists a universal constant c‚Ä≤>0such that PX‚àºD[X‚ààRi]‚â§c‚Ä≤d22i
Œ≥2m
we get that
ES1,...,S 3
PX‚àºD
‚à©3
i=1{fSi(X)Ã∏=c(X)}
‚â§23c‚Ä≤d
Œ≥2m‚àûX
i=12‚àíi=Od
Œ≥2m
,
and we are done. Thus assume for contradiction that PX‚àºD[X‚ààRi]>c‚Ä≤d22i
Œ≥2m, forc‚Ä≤>1to be
chosen large enough. Using Lemma 4 we have that there exist a universal constant a >1such that
ES‚àºDm
erDRi(fS)
‚â§s
ad
P[Ri]Œ≥2m.
By Tonelli, the definition of pxand that for x‚ààRiwe have px‚àà 
2‚àíi,2‚àíi+1
we get that
ES‚àºDm
erDRi(fS)
=EX‚àºDRi[ES‚àºDm[1{fS(X)Ã∏=c(X)}]] =EX‚àºDRi[pX]‚â•2‚àíi.
Combining the above lower and upper bound on ES‚àºDm
erDRi
andPX‚àºD[X‚ààRi]>c‚Ä≤d22i
Œ≥2mwe
get that
1‚â§2is
ad
P[Ri]Œ≥2m‚â§2is
ad
c‚Ä≤d22i
Œ≥2mŒ≥2m‚â§ra
c‚Ä≤,
which for c‚Ä≤sufficiently large is strictly less than 1, thus we reached a contradiction. Hence it must
be the case that PX‚àºD[X‚ààRi]‚â§c‚Ä≤d22i
Œ≥2m, which concludes the proof of Lemma 3.
4 Experiments
In this section, we present the results of our pilot empirical study between the different sample
optimal weak-to-strong learners. We compare the algorithms on five different data sets. The first four
are standard binary classification data sets and are the same data sets used in [ 12], whereas the last is
a synthetic binary classification data set developed from the lower bound [ 15] showing that AdaBoost
is sub-optimal. For all real world data sets, we have shuffled the samples and randomly set aside 20%
to use as test set. The weak learner we use for these is the scikit-learn DecisionTreeClassifier
with max_depth=1 . This is default for the implementation of AdaBoost in scikit-learn, which is the
implementation used in our experiments. We describe the data sets in greater detail below.
7‚Ä¢Higgs [29]: This data set represents measurements from particle detectors, and the labels
tells whether they come from a process producing Higgs bosons or if they were a background
process. The data set consists of 11 million labeled samples. However, we focus on the first
300,000 samples. Each sample consists of 28 features, where 7 of these are derived from the
other 21.
‚Ä¢Boone [23]: In this data set, we try to distinguish electron neutrinos from muon neutrinos.
The data set consists of 130,065 labeled samples. Each sample consists of 50 features.
‚Ä¢Forest Cover [4]: In this data set, we try to determine the forest cover type of 30 x 30
meter cells. The data set actually has 7 different forest cover types, so we have removed all
samples of the 5 most uncommon to make it into a binary classification problem. This leaves
us with 495,141 samples. Each sample consists of 54 features such as elevation, soil-type
and more.
‚Ä¢Diabetes [28]: In this data set, we try to determine whether a patient has diabetes or not
from features such as BMI, insulin level, age and so on. This is the smallest real-world data
set, consisting of only 768 samples. Each sample consists of 8 features.
‚Ä¢Adversarial [15]: This data set, as well as the weak learner, have been developed using
the lower bound instance in [ 15]. Concretely, the data set consists of 1024 uniform random
samples from the universe X={1, . . . , 350}. Every element of the input domain has the
label 1, but all weak-to-strong learners are run simply by giving them access to a weak
learner. The weak learner is adversarially designed. When it is queried with a weighing
of the training data set, it computes the set Tcontaining the first 20 points from the input
domain that receive zero mass under the query weighing. It then searches through a set of
hypotheses (chosen randomly) and returns the hypothesis with the worst performance on T,
while respecting that it must have error no more than 1/2‚àíŒ≥under the query weighing and
having at least 1/2 +Œ≥error on T. Finally, the hypothesis set contains an additional special
hypothesis h0that is correct (returns 1) on all but the last 20 points of the input domain.
This hypothesis is used to handle queries where none of the randomly chosen hypotheses
have advantage Œ≥. We refer the reader to [ 15] for further details and intuition on why this
construction is hard for AdaBoost.
The data sets represent both large and small training sets. For each data set, we run simple AD-
ABOOST (accuracy shown as a blue horizontal line in the plots), BAGGED ADABOOST (Algorithm 4),
LARSEN RITZERT (Algorithm 3), and our new MAJORITY -OF-X(for X varying from 3 to 29). In our
experiments, we vary the number of AdaBoosts trained by each weak-to-strong learner from 3to29,
instead of merely following the theoretical suggestions. Each of these voting classifiers is then trained
for 300 rounds on its respective input. This has been repeated 5 times with different random seeds,
so the plots indicate the average accuracy across these 5 runs. For Algorithm 3 that creates mlg43
sub-samples, we use the full set of sub-samples on the two small data sets Diabetes and Adversarial.
For the three large data sets Higgs, Boone and Forest Cover, this creates a huge overhead in running
time and we instead randomly sample without replacement from among the sub-samples resulting
from the SUBSAMPLE procedure (Algorithm 2). This is the reason for the non-constant behavior
of this algorithm in the corresponding experiments. For BAGGED ADABOOST , we have chosen to
sample 95% of the samples (with replacement) in our experiments. The results of the experiments on
the three large data sets are shown in Fig. 1.
The results in Fig. 1 gives an initial suggestion that our new algorithm with disjoint training sets
might have an advantage on large data sets. Quite surprisingly, we see that the two other optimal
weak-to-strong learners perform no better, or even worse, than standard AdaBoost. Experiments on
the small Diabetes data set, as well as the Adversarially designed data set, are shown in Fig. 2.
The results in Fig. 2 suggest that our new algorithm may perform poorly on small training sets.
This makes sense, as the training data for each weak learner is extremely small on these data sets.
Instead, we find that the Bagging based variant outperforms classic AdaBoost. Since Bagging has a
relative small overhead compared to simple AdaBoost, this suggests running both our new algorithm
MAJORITY -OF-XandBAGGED ADABOOST and using a validation set to pick the best classifier. We
hope these first experiments may inspire future and more extensive empirical comparisons between
the various weak to strong learners.
8Figure 1: Top is Higgs, Left plot is Boone. Right plot is Forest Cover
Figure 2: Left plot is Diabetes. Right plot is Adversarial
95 Limitations
Our main results are proved under the theoretical assumption of i.i.d. training samples as well as
access to a weak-learner that always obtains an advantage of Œ≥over random guessing. Since these
might not be realistic assumptions, we also performed an empirical evaluation of our algorithm. Due
to computational constraints, we have only been able to run experiments on 5 data sets. We have thus
been careful not to over-emphasize the practical implications of our results. In all circumstances, we
view the theoretical contributions as the main novelty of this work.
Acknowledgment
This research is co-funded by the European Union (ERC, TUCLA, 101125203) and Independent
Research Fund Denmark (DFF) Sapere Aude Research Leader Grant No. 9064-00068B. Views and
opinions expressed are however those of the author(s) only and do not necessarily reflect those of the
European Union or the European Research Council. Neither the European Union nor the granting
authority can be held responsible for them.
References
[1]I. Aden-Ali, M. M. H√∏andgsgaard, K. G. Larsen, and N. Zhivotovskiy. Majority-of-three: The
simplest optimal learner? In S. Agrawal and A. Roth, editors, Proceedings of Thirty Seventh
Conference on Learning Theory , volume 247 of Proceedings of Machine Learning Research ,
pages 22‚Äì45. PMLR, 30 Jun‚Äì03 Jul 2024.
[2]P. Bartlett, Y . Freund, W. S. Lee, and R. E. Schapire. Boosting the margin: a new explanation
for the effectiveness of voting methods. The Annals of Statistics , 26(5):1651 ‚Äì 1686, 1998.
[3]P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. In Journal of machine learning research , 2003.
[4]J. Blackard. Covertype. UCI Machine Learning Repository, 1998. DOI:
https://doi.org/10.24432/C50K5N.
[5] L. Breiman. Bagging predictors. Machine Learning , 24(2):123‚Äì140, 1996.
[6]L. Breiman. Prediction games and arcing algorithms. Neural computation , 11(7):1493‚Äì1517,
1999.
[7]T. Chen and C. Guestrin. Xgboost: A scalable tree boosting system. In KDD , pages 785‚Äì794.
ACM, 2016.
[8]R. M. Dudley. Central Limit Theorems for Empirical Measures. The Annals of Probability ,
6(6):899 ‚Äì 929, 1978.
[9]Y . Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of computer and system sciences , 55(1):119‚Äì139, 1997.
[10] J. H. Friedman. Greedy function approximation: A gradient boosting machine. The Annals of
Statistics , 29(5):1189 ‚Äì 1232, 2001.
[11] W. Gao and Z. Zhou. On the doubt about margin explanation of boosting. Artif. Intell. , 203:1‚Äì18,
2013.
[12] A. Gr√∏nlund, L. Kamma, and K. G. Larsen. Margins are insufficient for explaining gradient
boosting. In Advances in Neural Information Processing Systems 33 (NeurIPS 2020) , 2020.
[13] B. Hajek and M. Raginsky. ECE 543: Statistical Learning Theory. Department of Electrical
and Computer Engineering and the Coordinated Science Laboratory, University of Illinois at
Urbana-Champaign , 2021. Last updated March 18, 2021.
[14] S. Hanneke. The optimal sample complexity of pac learning. The Journal of Machine Learning
Research , 17(1):1319‚Äì1333, 2016.
10[15] M. M. H√∏gsgaard, K. G. Larsen, and M. Ritzert. Adaboost is not an optimal weak to strong
learner. In ICML , volume 202 of Proceedings of Machine Learning Research , pages 13118‚Äì
13140. PMLR, 2023.
[16] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T.-Y . Liu. Lightgbm: A
highly efficient gradient boosting decision tree. In NIPS , 2017.
[17] M. Kearns. Learning boolean formulae or finite automata is as hard as factoring. Technical
Report TR-14-88 Harvard University Aikem Computation Laboratory , 1988.
[18] M. Kearns and L. Valiant. Cryptographic limitations on learning boolean formulae and finite
automata. Journal of the ACM (JACM) , 41(1):67‚Äì95, 1994.
[19] K. G. Larsen. Bagging is an optimal PAC learner. Conference on Learning Theory (COLT
2023) , 195:450‚Äì468, 2023.
[20] K. G. Larsen and M. Ritzert. Optimal weak to strong learning. In S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Process-
ing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS
2022, New Orleans, LA, USA, November 28 - December 9, 2022 , 2022.
[21] K. G. Larsen and M. Ritzert. Optimal weak to strong learning. Advances in Neural Information
Processing Systems (NeurIPS 2022) , 2022.
[22] A. Natekin and A. Knoll. Gradient boosting machines, a tutorial. Frontiers in Neurorobotics , 7,
2013.
[23] B. Roe. MiniBooNE particle identification. UCI Machine Learning Repository, 2010. DOI:
https://doi.org/10.24432/C5QC87.
[24] G. R√§tsch and M. Warmuth. Efficient margin maximizing with boosting. Journal of Machine
Learning Research , 6:2131‚Äì2152, 12 2005.
[25] R. E. Schapire. The strength of weak learnability. Machine learning , 5(2):197‚Äì227, 1990.
[26] R. E. Schapire and Y . Freund. Boosting: Foundations and Algorithms . The MIT Press, 05 2012.
[27] S. Shalev-Shwartz and S. Ben-David. Understanding machine learning: From theory to
algorithms . Cambridge university press, 2014.
[28] J. W. Smith, J. E. Everhart, W. Dickson, W. C. Knowler, and R. S. Johannes. Using the adap
learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the annual
symposium on computer application in medical care , page 261. American Medical Informatics
Association, 1988.
[29] D. Whiteson. HIGGS. UCI Machine Learning Repository, 2014. DOI:
https://doi.org/10.24432/C5V312.
11A Proof of Lemma 4
We now give the proof of Lemma 4. For this we need the following Corollary 5 that gives a high
probability bound on the error for all classifiers in ‚àÜ(H)that have Œ≥margins on a training set S.
Corollary 5. LetC > 1denote a universal constant. For hypothesis set Hwith VC-dimension d,
distribution D, margin 0< Œ≥‚â§1, failure probability 0< Œ¥ < 1, and a point set S‚àº Dm, we have
with probability at least 1‚àíŒ¥overS, that any f‚àà‚àÜ(H)such that f(x)c(x)‚â•Œ≥for all x‚ààS
satisfies
erD(f)‚â§s
2Cd
Œ≥2m+r
2 ln (2 /Œ¥)
m.
Corollary 5 follows for instance by a modification of [ 26] [page 107-111] due to [ 3] and using the
stronger bound on the Rademacher Complexity of O(p
d/m)due to [ 8] [See e.g. theorem 7.2 [ 13] ].
With Corollary 5 in place we now give the proof of Lemma 4.
Proof of Lemma 4. IfP[R]m‚â§d/Œ≥2we have that
s
d
Œ≥2P[R]m‚â•1,
and the claim holds since erDR(fS)is always less than 1. Thus we assume from now on that
P[R]m > d/Œ≥2. We now define the events Ei:={|{S‚à©R}|=i}form‚â•i‚â•P[R]m/2
andE=‚à™i‚â•P[R]m/2Ei={|S‚à©R| ‚â•P[R]m/2}. We further define Xj= 1{Sj‚ààR}for
j= 1, . . . , m and notice that these are i.i.d. {0,1}-random variables where X=P
jXjhas
expectation P[R]mand the eventP
jXj‚â•P[R]m/2is contained in E. Thus by a Chernoff bound
and that exp(‚àíx)‚â§1/xforx >0, we get that
PS[E]‚â•1‚àíexp (‚àíP[R]m/8)‚â•1‚àí8
P[R]m.
We thus have PS¬ØE
‚â§8/(P[R]m). Since we assumed that P[R]m‚â•d/Œ≥2‚â•1and using
0‚â§x‚â§‚àöxforx‚â§1, this further implies that
PS¬ØE
‚â§8¬∑s
d
Œ≥2P[R]m.
We will soon show that ES[erDR(fS)|Ei]‚â§16¬∑q
16Cd
Œ≥2P[R]mform‚â•i‚â•P[R]m/2for a universal
constant C‚â•1. Now using these two relations combined with the law of total expectation on the
partition ¬ØE, E P[R]m/2, . . . , E mand that erDR‚â§1, we get that
ES[erDR(fS)] =X
i‚â•P[R]m/2ES[erDR(fS)|Ei]PS[Ei] +ES
erDR(fS)|¬ØE
PS¬ØE
‚â§16¬∑s
16Cd
Œ≥2P[R]mP[E] + 8¬∑s
d
Œ≥2P[R]m
‚â§24¬∑s
16Cd
Œ≥2P[R]m,
as claimed in Lemma 4 with the universal constant a= 242¬∑16C. Thus we have to show that
ES[erDR(fS)|Ei]‚â§16¬∑q
16Cd
Œ≥2P[R]mform‚â•i‚â•P[R]m/2. So consider such an i. Since erDR(fS)
is a non-negative random variable, we have that
ES[erDR(fS)|Ei] =Z‚àû
0PS[erDR(fS)‚â•x|Ei]dx
12We will thus upper bound this integral. Now conditioned on Ei, we know that Scontains ipoints
that are samples according to DRand that fSon these examples has all margins at least Œ≥. Thus we
have by Corollary 5 that with probability at least 1‚àíŒ¥overS, it holds that
erDR(fS)‚â§s
max8Cd
Œ≥2i,8 ln (2 /Œ¥)
i
(3)
where C‚â•1is a universal constant. For ease of notation let ri=q
8Cd
Œ≥2i. We notice that
ri‚â§q
16Cd
Œ≥2P[R]msince iis assumed to be greater than P[R]m/2. Using this, we get that
Z‚àû
0PS[erDR(fS)‚â•x|Ei]dx‚â§ri+Z‚àû
riPS[erDR(fS)‚â•x]dx (4)
‚â§q
16Cd
Œ≥2P[R]m+Z‚àû
riPS[erDR(fS)‚â•x]dx. (5)
Thus if we can show that
Z‚àû
riPS[erDR(fS)‚â•x]dx‚â§15¬∑q
d
Œ≥2P[R]m,
we get by combining this with Eq. (4) that
ES[erDR(fS)|Ei]‚â§16¬∑q
16Cd
Œ≥2P[R]m
which would conclude the proof. Thus we now showR‚àû
riPS[erDR(fS)‚â•x]dx‚â§15¬∑q
d
Œ≥2P[R]m.
For this, we do the following non-trivial rewriting of xto make it resemble the second term in the
max appearing in (3)
x=vuut8 ln 
2
2 exp ‚àíx2i
8!
i‚àí1.
Now for any x‚â•riwe have that
PS[erDR(fS)‚â•x|Ei] =PS[erDR(fS)‚â•max( ri, x)|Ei],
which combined with the rewriting of xand Eq. (3) with Œ¥= 2 exp
‚àíx2i
8
and noticing riis the
first argument of the max in Eq. (3), we get that
PS[erDR(fS)‚â•x|Ei] =PS[erDR(fS)‚â•max( ri, x)|Ei]‚â§2 exp‚àíx2i
8
,
for any x‚â•ri. Now using the density function of a normal distribution with mean 0and standard
deviation œÉis equal to exp(‚àí1
2(x/œÉ)2)/(œÉ‚àö
2œÄ), and letting N(0, œÉ)denote a normal random
variable with mean 0and standard deviation œÉ, we get that
Z‚àû
riPS[erDR(fS)‚â•x|Ei]dx‚â§Z‚àû
ri2 exp‚àíx2i
8
dx
‚â§2p
8/(2i)‚àö
2œÄZ‚àû
ri1p
8/(2i)‚àö
2œÄexpÔ£´
Ô£≠‚àí1
2 
xp
8/(2i)!2Ô£∂
Ô£∏dx
‚â§2p
8œÄ/i¬∑P"
N 
0,r
8
2i!
‚â•ri#
‚â§8p
œÄ/(P[R]m)‚â§15¬∑s
d
Œ≥2P[R]m,
where the second to last inequality follows from i‚â•P[R]m/2and the last inequality by P[R]m‚â•
d/Œ≥2‚â•1.
13NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: The theoretical claim is stated in the introduction together with a description
of the indications found in the experiments.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have included a Limitations section, discussing limitations of our work.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
14Justification: The main claim is proven in Section 3 together with a sketch proof. The
remaining lemma is proved in Appendix A.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The algorithms have been described in detail, and the parameters for the
experiments are given in the article. Furthermore, the code used for the experiments is
provided.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
15Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We have provided the code, and a README file explaining how to reproduce
the experiments.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The training and test details are explained along with the parameters used for
the algorithms.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: The experiments do not have statistical significance. We would need to run
more iterations of the experiments, which would require more computing power.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
16‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The paper only reports accuracies, and does not discuss concrete running times
in the experiments, so the computer resources are not relevant.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research presented in this paper does not violate the NeurIPS Code of
Ethics.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The result of the paper is foundational research.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
17‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The data used in the experiments are freely available for everyone to use.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All datasets used in the experiments are properly cited.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
18‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
19‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
20