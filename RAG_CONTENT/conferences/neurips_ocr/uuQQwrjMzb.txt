Adaptive Labeling for Efficient Out-of-distribution
Model Evaluation
Daksh Mittalâˆ—, Yuanzhe Maâˆ—, Shalmali Joshi, Hongseok Namkoong
Columbia University
{dm3766, ym2865, sj3261, hn2369 }@columbia.edu
Abstract
Datasets often suffer severe selection bias; clinical labels are only available on
patients for whom doctors ordered medical exams. To assess model performance
outside the support of available data, we present a computational framework for
adaptive labeling, providing cost-efficient model evaluations under severe distri-
bution shifts. We formulate the problem as a Markov Decision Process over states
defined by posterior beliefs on model performance. Each batch of new labels in-
curs a â€œstate transitionâ€ to sharper beliefs, and we choose batches to minimize
uncertainty on model performance at the end of the label collection process. In-
stead of relying on high-variance REINFORCE policy gradient estimators that do
not scale, our adaptive labeling policy is optimized using path-wise policy gra-
dients computed by auto-differentiating through simulated roll-outs. Our frame-
work is agnostic to different uncertainty quantification approaches and highlights
the virtue of planning in adaptive labeling. On synthetic and real datasets, we em-
pirically demonstrate even a one-step lookahead policy substantially outperforms
active learning-inspired heuristics.
1 Introduction
Figure 1: Adaptive labeling to reduce epistemic
uncertainty over model performance. Among the
two clusters of unlabeled examples (left vs. right),
we must learn to prioritize labeling inputs from the
left cluster to better evaluate mean squared error.Engineering progress in AI is predicated on
rigorous empirical evaluation. Evaluating su-
pervised ML models is easy when there is
no distribution shift. However, naive offline
evaluations on observational data cannot reli-
ably assess safety due to selection bias. When
ground-truth outcomes are expensive to col-
lect, existing supervised datasets are often
heavily affected by selection bias. For ex-
ample, AI-based medical diagnosis systems
are typically evaluated using past clinical la-
bels, which are only available for patients
who were initially screened by doctors. This
creates a fundamental limitation: we cannot
even assess the modelâ€™s performance on pa-
tients who were never screened in the first
place. Moreover, due to the absence of positive pathological patterns in the available data, pre-
viously unseen patient types may never get diagnosed by the AI diagnosis system. Evaluating the
modelâ€™s performance on previously unscreened patients is a first step toward mitigating this negative
feedback loop.
âˆ—Equal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Figure 2: Overview of our adaptive sampling framework. At each period, we select batch of
inputs Xtto be labeled, and obtain a new labeled data Dt. We view posterior beliefs Âµt(Â·)on
fâ‹†(Y|X)as â€œstatesâ€, and update it as additional labeled data is collected. Our goal is to minimize
uncertainty on the performance of the prediction model Ïˆ(Â·)at the end of Tperiods.
In this paper, we study model evaluations under severe distribution shifts, where the support of the
available supervised data is different from examples encountered during deployment. Randomly
selecting inputs Xto collect additional outcomes/labels Yincurs prohibitive cost, especially when
the outcome distribution varies over a high-dimensional feature space. To address this, we must
focus our sampling efforts on out-of-distribution (OOD) examples where the modelâ€™s performance
is uncertain. Going beyond the OOD detection literature that focuses on the distribution of input
X[41], we further connect the rarity of Xwith how smooth we believe Y|Xto be. For example,
in the extreme case where we believe Y|Xis linear, it is easy to extrapolate model performance
to regions of Xwhere no data is available; such extrapolation is more challenging for nonlinear
models.
We propose an adaptive sampling framework that focuses sampling effort on inputs Xwhere there
is large epistemic (actionable) uncertainty on the shape of Y|X. We differentiate this with aleatoric
(irreducible) uncertainty, which is due to idiosyncratic noise in the outcome measurement process.
Since updating the sampling policy requires engineering or organizational effort in practice, we
consider a few-horizon setting where at each period, a batch of inputs is selected for labeling.
Our main contribution is the formulation of a planning problem for selecting batches of inputs from
a large pool of unlabeled data (Section 3). We develop adaptive policies that incorporate how beliefs
on model performance get sharper as more labels are collected. We model our current belief on the
data generation process Y|Xas the current state. After observing a new batch of labeled data, we
update our belief on the epistemic uncertainty via a posterior update (â€œstate transitionâ€). Our goal
is to minimize the uncertainty over model performance at the end of label collection, as measured
by the variance under the posterior after the final batch is observed. Modeling each batch as a
time period, we arrive at a Markov Decision Process (MDP) where states are posterior beliefs, and
actions correspond to selecting subsets (batches) from the pool of examples to be labeled (Figure 2).
Notably, our problem is less ambitious than an active learning problem [34] where the goal is to
adaptively label data to improve model performance, over a large number of batches. Narrowing
our focus to model evaluation allows us to address a broad class of evaluation problems including
regression models, and solve the problem more effectively.
Solving our planning problem is computationally challenging due to the combinatorial action space
and continuous states. To address this, we develop a tractable approach by considering a smooth
relaxation of the problem, where we optimize over smoothly parameterized policies that select an
entire batch of inputs [39]. Although the well-known â€œscore trickâ€ enables the computation of policy
gradient estimates based on function evaluations alone (â€œREINFORCEâ€) [36, 5], this method suffers
from high variance and may lead to poor performance in practice.
To compute reliable policy gradients, we develop an auto-differentiable planning framework in Sec-
tion 4. Since the dynamics of the MDP (posterior updates) are known, we perform â€œroll-outsâ€ using
the current posterior beliefs: we simulate potential outcomes that we may obtain if we label a par-
ticular batch, and evaluate the updated uncertainty on model performance based on the imagined
pseudo labels of this new batch. By implementing all the operations within an auto-differentiable
framework, we can backpropagate the observed reward to the policy parameters and calculate the
2pathwise policy gradient. While our original problem is nonsmooth and discrete, auto-differentiating
over this smoothed MDP provides reliable policy optimization methods.
Our approach is agnostic to the uncertainty quantification approach, and allows incorporating latest
advances in Bayesian modeling (e.g., [5, 28]). Our â€œmodel-basedâ€ approach rests on encoding state
transitions (posterior updates) in an auto-differentiation framework. We stress that we do not require
the posterior updates to have a closed form (conjugacy), and allow approximate posterior inference
using gradient-based optimizers. We demonstrate our planning framework over beliefs formed by
Gaussian processes, and Deep Ensembles [18, 26] that enable leveraging the inductive biases of
neural networks in high dimensions and gradient descent methods to update probabilistic beliefs
overY|X. Empirically, we observe that even a single planning step can yield significant practical
benefits (Section 5). On both real and simulated datasets with severe selection bias, we demonstrate
that one-step lookahead policies based on our auto-differentiation framework achieve state-of-the-art
performance, outperforming REINFORCE policy gradients and active learning-based heuristics.
Limitations Our empirical validation highlights several open research directions required to scale
our framework. First, although recent advances in Bayesian modeling have achieved substantial
progress in one-shot uncertainty quantification, we empirically observe that it is often difficult to
maintain accurate beliefs as more data are collected. Second, we detail engineering challenges in
implementing our auto-differentiation framework over multi-period roll-outs, highlighting the need
for efficient implementations of high-order auto-differentiation.
2 Related work
Our work is closely related to active learning [1, 34], where the modeler adaptively collects labels to
improve model performance. Active learning methods typically focus on classification models and
select inputs to label based on uncertainty sampling techniques such as margin-sampling or entropy,
or by leveraging disagreements among different models, exemplified by query-by-committee and
Bayesian Active Learning by Disagreement (BALD) [14]. Although there is no notion of planning,
some query strategies account for the expected reduction in error or model change [34]. Since it is
important to collect labels over a diverse set of inputs, heuristic criteria based on diversity and density
are sometimes incorporated to account for the feature space distribution [34]. Prior work addressing
distribution shifts in active learning [42] uses importance weighting, which is not feasible in our
setting, where the distribution shift is out of support. Batched settings are a long-standing challenge
in active learning [34]. In Section 5, we show that adapting greedy-based heuristics to batched
scenarios yields poor performance. Recent extensions of active learning to batched settings (e.g.,
BatchBALD [17], BatchMIG [37]) continue to rely on myopic greedy algorithms for classification.
On the other hand, we leverage our limited scope on evaluation to formalize a planning problem
and derive a unified computational framework that can handle regression problems. We provide
lookahead policies [2, 6, 7] that plan for the future and flexibly handle different objectives and
uncertainty quantification methodologies.
Our problem can also be viewed as a version of pure exploration bandits [23] where we want to
minimize some given function of the posterior. In contrast to conventional formulations, our central
focus on batching induces combinatorial action spaces and few horizons. Bayesian optimization
methods optimize black-box functions that are computationally challenging to evaluate by modeling
the function as a draw from a Gaussian process [8]. Notably, the knowledge gradient algorithm [9]
maximizes the single period expected increase in the black-box function value and can be viewed as
a one-step lookahead policy. Using auto-differentiation methods, we extend these classical ideas to
model evaluation by incorporating batching (combinatorial action spaces).
While Gaussian processes works well in low dimensions, quantifying epistemic uncertainty on
fâ‹†(Y|X)for high dimensional inputs is an active area of research, including popular techniques
such as dropout [10], Bayes by Backprop [3], Ensembles/ Ensemble +[18, 26], and Epistemic Neu-
ral Networks [28]. Our framework is compatible with deep learning-based uncertainty quantification
models, such as Ensembles, where we can only do approximate posterior updates.
3 Markov decision process over posterior beliefs
Our goal is to evaluate the performance of the model Î¨ :X â†’ Y over the input distribution PXthat
we expect to see during deployment. Given features Xâˆˆ X , outcomes/ labels are generated from
3some unknown function fâ‹†:Y=fâ‹†(X) +Îµwhere Îµâˆ¼N(0, Ïƒ2).When ground truth outcomes
are costly to obtain, previously collected labeled data D0typically suffers selection bias and covers
only a subset of the support of input distribution PXover which we aim to evaluate the model
performance. Assuming we have a pool of data Xpool, we design adaptive sampling algorithms that
iteratively select inputs in Xpoolto be labeled. Since labeling inputs takes time in practice, we model
real-world instances by considering batched settings. Our goal is to sequentially label batches of
data to accurately estimate model performance over PXand therefore we assume we have access to
a set of inputs Xevalâˆ¼PX. We use the squared loss to illustrate our framework, where our goal is
to evaluate EXâˆ¼PX[(Yâˆ’Î¨(X))2]. Under the â€œlikelihoodâ€ function p(y|f, x) =pÎµ(yâˆ’f(x)), let
g(f)be the performance of the AI model Î¨(Â·)under the data generating function f, which we refer
to as our estimand of interest. When we consider the mean squared loss, g(f)is given by
g(f)â‰œEXâˆ¼PXh
EYâˆ¼p(Â·|f,X)h
(Yâˆ’Î¨(X))2i
|fi
. (1)
Our framework is general and can be extended to other settings. For example, a clinically
useful metric is Recall , defined as the fraction of individuals that the model Î¨(Â·)correctly
labels as positive among all the individuals who actually have the positive label g(f)â‰œ
EXâˆ¼PXh
EYâˆ¼p(Â·|f,X)h
1{Î¨(X)>0}|Y= 1ii
.
Since the true function fâ‹†is unknown, we model it from a Bayesian perspective by formulating
a posterior given the available supervised data. We refer to uncertainty over the data generating
function fasepistemic uncertaintyâ€”since we can resolve it with more dataâ€”and that over the
measurement noise Îµasaleatoric uncertainty. Assuming independence given features X, we model
the marginal likelihood of the data via the product p(Y|f,X) =Q
(X,Y)âˆˆDp(Y|f, X)where D=
(X Ã—Y ). Our prior belief Âµover functions freflects our uncertainty about how labels are generated
given features.
To adaptively label inputs from Xpool, we quantify epistemic uncertainty over the labels reliably
using an uncertainty quantification (UQ) method. Roughly speaking, a UQ method provides us
the ability to formulate a posterior belief Âµ(f| D)given any supervised data D. As we detail in
Section 4.1, our framework can leverage both classical Bayesian models like Gaussian processes and
recent advancements in deep learning-based UQ methods. As new batches are labeled, we update
our posterior beliefs about fover time, which we view as â€state transitionsâ€ of a dynamical system.
Recalling the Markov decision process depicted in Figure 2, we sequentially label a batch of inputs
fromXpool(actions), which lead to state transitions (posterior updates). Specifically, our initial state
is given by Âµ0(Â·) =Âµ(Â· | D0)and at each period t, we label a batch of Ktinputs Xt+1âŠ‚ X pool
resulting in labeled data Dt+1= (Xt+1Ã— Yt+1). After acquiring the labels at each step t, we
update the posterior state to Âµt+1(Â·) =Âµt(Â· | Dt+1). Modeling practical instances, we consider
a small horizon problem with limited adaptivity T. Formulating an MDP over posterior states has
long conceptual roots, dating back to the Gittinâ€™s index for multi-armed bandits [12].
We denote by Ï€tthe adaptive labeling policy at period t. We account for randomized policies
Xt+1âˆ¼Ï€t(Âµt)with a flexible batch size |Xt+1|=Kt. We assume Ï€tisFtâˆ’measurable for all
tâ‰¤T, where Ftis the filtration generated by the observations up to the end of step t. Observe that
Âµt+1contains randomness in the policy Ï€tas well as randomness in Yt+1|(Xt+1, Âµt). Letting
Ï€={Ï€0, ...., Ï€ Tâˆ’1}, we minimize the uncertainty over g(f)at the end of data collection
H(Ï€)â‰œED1:Tâˆ¼Ï€[G(ÂµT)]â‰œED1:Tâˆ¼Ï€
G(Âµ(Â· | D0:T))
=ED1:Tâˆ¼Ï€
Varfâˆ¼Âµ(Â·|D0:T)g(f)
,(2)
where G(ÂµT) = Var fâˆ¼ÂµTg(f). In the above objective (2) we assumed that the modeler pays a fixed
and equal cost for each outcome. Our framework can seamlessly accommodate variable labeling
cost as well. Specifically, we can define a cost function c(Â·)applied on the selected subsets and
update the objective accordingly to have a term Î»c(D1:T)where Î»is the penalty factor that controls
the trade-off between minimizing variance and cost of acquiring samples.
In the planning problem (2), states are continuous and the action space is combinatorial. To ad-
dress these issues, we propose a continuous policy parameterization Ï€Î¸(with parameter Î¸) in
the next section. We solve this MDP using policy gradients. Let the gradient be defined as
âˆ‡Î¸H(Î¸)â‰œâˆ‡Î¸EAâˆ¼Ï€Î¸[G(A)]. Here, we slightly abuse notations by letting Aâ‰œD1:Tand
G(A)â‰œG(Âµ(Â· | D0,D1:T)), where the distribution of Adepends on Ï€Î¸. To approximately solve
this MDP using policy gradients, the score function estimator (REINFORCE [38]) uses the fact
4thatâˆ‡Î¸EAâˆ¼Ï€Î¸[G(A)] =EAâˆ¼Ï€Î¸[G(A)âˆ‡Î¸logÏ€Î¸(A)]for any function G(Â·).Despite being unbi-
ased, Monte-Carlo estimation of the above expectation typically suffers from prohibitively large
variance [33] especially when Ï€Î¸(A)is small. Although a stream of work strives to provide variance
reduction techniques for REINFORCE [19, 29], they require value function estimates that are also
challenging to compute in our planning problem (2).
4 Planning using pathwise policy gradients
Our main algorithmic insight is that for systems with known dynamics (posterior updates) or where
the dynamics can be approximated sufficiently well, we can leverage auto-differentiation to directly
estimate approximate pathwise policy gradients (4) instead of relying on high-variance score-based
gradients. The policies derived using our efficient differentiable simulator exploits the system struc-
ture and achieves significantly improved performance compared to policies that do not rely on gra-
dient information, as detailed in Section 5. Intuitively, this is akin to finding a random variable
Zâˆ¼pZdistributed independent of policy Ï€Î¸, such that A=h(Z, Î¸)and
âˆ‡Î¸EAâˆ¼Ï€Î¸[G(A)] =âˆ‡Î¸EZâˆ¼pZ[G(h(Z, Î¸))](a)=EZâˆ¼pZ[âˆ‡Î¸G(h(Z, Î¸))] (3)
However, the equality (a)above holds only if G(h(Z, Î¸))is differentiable w.r.t. Î¸. In our case, as
we will see later, h(Z, Î¸)is non-differentiable w.r.t. Î¸because the actions are discrete and combi-
natorial. To address this, we will use a smoothed approximation, hÏ„(Z, Î¸), such that G(hÏ„(Z, Î¸))
becomes differentiable. Here, Ï„is a temperature parameter that controls the degree of smoothing.
Consequently, we can approximate the gradient as follows â€“
âˆ‡Î¸EAâˆ¼Ï€Î¸[G(A)]â‰ˆ âˆ‡ Î¸E[G(hÏ„(Z, Î¸))]â‰ˆ1
nnX
i=1âˆ‡Î¸G(hÏ„(Zi, Î¸)). (4)
It is important to note that sometimes even G(Â·)is non-differentiable, and in that case we must also
consider a smooth approximation of G(Â·)(e.g., refer to Section D.1 for details on smoothing the
Recall objective discussed earlier in Section 3). Our approach is inspired by the line of work on
differentiable simulators [4, 15, 21, 40, 35]. Theoretical analysis in stochastic optimization [11, 20]
also highlights the benefit of these estimators over zeroth-order gradient estimates of a stochastic
objective such as REINFORCE. Suh et al. [35] notes that first-order gradient estimators typically
perform well when the objective is sufficiently smooth and continuous. In what follows, we em-
pirically demonstrate that through a careful smoothing of the planning objective, it is possible to
trade-off bias and variance through auto-differentiation.
Algorithm 1 Autodiff 1-lookahead
1:Inputs: Labeled batch data D0, horizon T, UQ module, pool data Xpool, batch size K
Returns: Selected batches (Xt,Yt)for1â‰¤tâ‰¤Tand updated estimate of the objective G(ÂµT)
2:t= 0: Compute initial posterior state Âµ0based on UQ module and batch of labeled data D0.
3:for0â‰¤tâ‰¤Tâˆ’1:do
4: Ï€tâ‰œAlgorithm 2 (Inputs: Posterior Âµt, pool (Xt+1
pool), batch size K)
5: Xt+1={Xj:XjâˆˆSortDescending (Xt+1
pool;Ï€t)for1â‰¤jâ‰¤K}
6: Obtain labels Yt+1to create Dt+1
7: Update posterior state: Âµt+1â‰œÂµt(Â· | Dt+1)
8: Estimate the objective G(Âµt+1)
9:end for
We showcase the power of planning using pathwise policy gradients by considering the simplest
possible planning algorithm: one-step lookaheads. Empirically, we demonstrate that even one-step
look-ahead achieves significant improvement in sampling efficiency over other heuristic baselines.
Additionally, we highlight open engineering directions that can enable efficient multi-step planning
methods. At each time step t, our algorithm selects a batch of inputs Xt+1to minimize the uncer-
tainty over the one-step target estimand G(Âµt+1)based on current posterior belief Âµt. After selecting
the batch Xt+1, we observe the corresponding labels Yt+1and update posterior belief over the data-
generating function ftoÂµt+1(see Algorithm 1). The samples Xt+1to query are determined using a
policy Ï€t,Î¸â‰œÏ€Î¸, which we optimize through pathwise policy gradients (Algorithm 2). To optimize
5State Âµt
Policy Ï€t,Î¸
Xt+1
pool
Sample Â¯Yt+1
poolK-subset
samplingSelect batch
Sâ‰œXt+1Pseudo
Posterior
updateÂµS
+Estimate
VarÂµS
+(g(f))
SoftK-subset
samplinga(Î¸)Soft posterior
update
(differentiable)Âµa(Î¸)
+Differentiable
estimate
Figure 3: Differentiable one-step look ahead pipeline for efficient adaptive sampling
the policy Ï€t,Î¸, we simulate â€œroll-outsâ€ of posterior beliefs over fusing imagined pseudo labels
Â¯Ypoolgenerated from current belief Âµt. Algorithm 1 summarizes each of these steps of the overall
procedure. Our conceptual framework is general and can leverage multi-step look-aheads [2, 6, 7].
Our algorithm consists of three major components which we will describe in detail shortly. First, we
parameterize a single-batch policy Ï€Î¸and use K-subset sampling [39] to choose Ksamples. Sec-
ond, we use a UQ module that characterizes posterior beliefs over f. Finally, to reliably optimize Î¸,
we adopt an auto-differentiable â€œroll-outâ€ pipeline through which we approximate policy gradients
by smoothing the pipeline. The overall pipeline (and its differentiable analogue) is briefly described
in Figure 3. We also explain our algorithm graphically in Figure 8 of Section A. Our codebase is
available at https://github.com/namkoong-lab/adaptive-labeling .
4.1 Uncertainty Quantification (UQ) Module
The UQ module maintains an estimate Âµtover posterior beliefs of fover time, enabling our planning
framework. Our method is agnostic to the UQ module, and we illustrate this using two instantiations:
i) Gaussian Processes (GP) that are extensively used in the Bayesian Optimization literature and are
known to work well in low dimensional data and regression setting, and ii) recently developed neural
network-based UQ methods such as Ensembles /Ensemble+ [27].
Gaussian Processes GPsf(Â·)âˆ¼ GP (m(Â·),K(Â·,Â·))are defined by a mean function m(Â·)and ker-
nelK(Â·,Â·), where for any inputs X,f(X)is Gaussian with mean m(X)andCov(f(Xi), f(Xj)) =
K(Xi, Xj). Additionally, the observation consists of (X, Y)with Y=fâ‹†(X) +ÎµandÎµâˆ¼
N(0, Ïƒ2I)for some noise level Ïƒ >0. Posterior updates have a closed form and are differentiable,
as we review in Section B.
Ensembles Ensembles [18] learn an ensemble of neural networks from given data, with each
network having independently initialized weights. Ensemble+ [26] extends this approach by com-
bining ensembles of neural networks with randomized prior functions and bootstrap sampling [25].
The prior function is added to each network in the ensemble, trained using L2 regularization. Re-
cently, Epistemic neural networks (ENNs) [28] have also been shown to be an effective way of
quantifying uncertainty. All these deep learning models are parametrized by some parameter Î·. For
a given sample {(Xj, Yj)}jâˆˆI, the model weights Î·are update through gradient descent under a
loss function â„“(.), with the update rule expressed as: Î·new=Î·âˆ’P
jâˆˆIâˆ‡Î·â„“(Xj, Yj, Î·).
4.2 Sampling Policy
At each time step, a batch of Ksamples is queried to improve the objective. The samples are
selected from a pool Xpoolof size n, based on weights w(Î¸). Specifically, given weights wâ‰¥0and
a batch size K, theK-subset sampling mechanism generates a random vector Sâˆˆ {0,1}n, whose
distribution depends on wsuch thatPn
i=1Si=K. Letejdenote the j-th unit vector, and consider
a sequence of unit vectors [ei1Â·Â·Â·eik]â‰œs. Using a parametrization known as weighted reservoir
sampling ( wrs) [39], the probability of selecting a sequence sis given by:
pwrs([ei1,Â·Â·Â·,eik]|w)â‰œwi1Pn
j=1wjwi2Pn
j=1wjâˆ’wi1...wikPn
j=1wjâˆ’Pkâˆ’1
j=1wij. (5)
The probability of selecting a batch Scan then be defined as: p(S|w)â‰œP
sâˆˆÎ (S)pwrs(s|w),where
Î (S) =n
s:S=Pk
j=1s[j]o
denote all sequences of unit vectors with sum equal to S.
6Algorithm 2 One-step look-ahead policy gradient
1:Inputs: Posterior belief Âµ, pool data (Xpool), batch size K, training epochs: M
Returns: Policy Ï€Î¸
2:Initialize the policy Ï€Î¸(parametrized by Î¸)
3:for1â‰¤mâ‰¤M:do
4: Generate peusdo labels Â¯Ypoolfor all the Xpool, where Â¯Ypoolis drawn from the current poste-
rior state Âµ.
5: Use Algorithm 3 (Input: Ï€Î¸:=w(Î¸)) to generate a random soft vector a(Î¸)corresponding
to policy Ï€Î¸
6: Update (pseudo) the posterior state to Âµa(Î¸)
+ using the pool data and pseudo labels
(Xpool,Â¯Ypool)
7:Estimate the objective G(Âµa(Î¸)
+)
8:SGD update: Î¸â†Î¸âˆ’Ïµtâˆ‡Î¸G(Âµa(Î¸)
+)
9:end for
10:Return: Policy Ï€Î¸
4.3 Differentiable pipeline
We propose a fully differentiable pipeline by ensuring that each component of the pipeline: i) K-
subset sampling, ii) posterior updates (UQ module), and iii) the objective are differentiable. We use
parametrizations Î¸for the policy function Ï€which yields sampling probabilities w(Î¸). Posterior
beliefs Âµare parametrized by Î·(e.g., Ensemble+ weights). In the following, we describe how we
smooth the respective components. The one-step objective parametrized by Î¸is given by
H(Î¸)â‰œEÂ¯Ypoolâˆ¼Âµ;Sâˆ¼p(Â·|w(Î¸))[G 
ÂµS
+
] (6)
where Gwas defined in (2). Here, p(Â·|w(Î¸))is the distribution over subsets governed by was
defined by (5). Further, Âµis the current posterior state and Âµ+is the updated posterior state after
incorporating the the additional batch Sselected from Xpooland the corresponding pseudo-outcomes
from Â¯Ypool(drawn based on current posterior state Âµ). We smooth the objective H(Î¸)to estimate
âˆ‡Î¸H(Î¸)using a soft K-subset sampling.
SoftK-subset sampling We use the smoothed K-subset sampling procedure proposed in Xie and
Ermon [39]. Briefly, the Algorithm 3 introduced in [39] produces a random vector asuch thatPn
i=1ai=K(soft version of S), such that a(Î¸)âˆ¼p(Â·|w(Î¸))â‰¡Ï€Î¸(Â·)(5), and is differentiable. We
defer the details to Section C.
Smoothing the UQ module To enable differentiable posterior updates, we approximate the pos-
terior updates using the soft K-subset samples a(Î¸). In the case of GPs, the soft K-subset selection
variable acan be interpreted as a weighting of samples in a GP. Thus, closed form GP updates can
be analogously used with appropriate weighting, described in the Appendix D. To perform weighted
updates for deep learning based UQ modules ( Ensembles /Ensemble+ ), we approximate previous
gradient updates by incorporating soft sample weights a(Î¸):
Î·1(a)â‰œÎ·0âˆ’X
jaj(Î¸)âˆ‡Î·â„“(Xj,Â¯Yj, Î·), (7)
where â„“is the loss function of the concerned model, with gradients evaluated at Î·. We interpret Î·1as
an approximation of the optimal UQ module parameter with one gradient step. Similarly, we define
a higher-order approximation Î·h+1(a)as
Î·h+1(a)â‰œÎ·h(a(Î¸))âˆ’X
jaj(Î¸)âˆ‡Î·â„“(Xj,Â¯Yj, Î·)|Î·=Î·h(a(Î¸)). (8)
Note here that effectively, the gradient is now of the form âˆ‡Î¸G 
argminÎ·â„“(Î·(a(Î¸)))
â‰¡
âˆ‡Î¸G(Î·+(a(Î¸))).To compute this, we use higher [13] and torchopt [32] which allows us to approx-
imately differentiate through the argmin. In practice, there is a trade-off between the computational
time and the accuracy of the gradient approximation depending on the number of training steps we
do to estimate the argmin.
70 2 4 6 8 10
Time horizon0.000.010.020.030.04Var of 2 loss
REINFORCE
Autodiff 1-lookahead (ours)
Uncertainty sampling - sequential
Random sampling
Uncertainty sampling - staticFigure 4: (Synthetic data) Variance of mean
squared loss evaluated through the posterior be-
liefÂµtat each horizon t. This is the objective
that policy gradient methods like REINFORCE
andAutodiff 1-lookahead optimizes. 1-step
lookaheads are surprisingly effective even in long
horizons.
0 2 4 6 8 10
Time horizon0.10.20.30.40.50.6Error of estimated model 2 loss
REINFORCE
Autodiff 1-lookahead (ours)
Uncertainty sampling - sequential
Random sampling
Uncertainty sampling - staticFigure 5: (Synthetic data) Error between MSE
calculated based on collected data D0:Tvs. pop-
ulation oracle MSE over Devalâˆ¼PX. Reduc-
ing uncertainty over posteriors directly leads to
better OOD evaluations. 1-step lookaheads sig-
nificantly outperform active learning heuristics in
small horizons.
5 Experiments
We empirically demonstrate effectiveness of our planning framework on both synthetic and real
datasets by focusing on the simplest planning algorithm: 1-step lookaheads. Using two uncertainty
quantification modulesâ€”GPs and Ensembles /Ensemble+ â€”we show that planning with path-
wise gradients is a promising approach to adaptive labeling. Throughout this section, we focus
on evaluating the mean squared error of a regression model Î¨, and develop adaptive policies that
minimize uncertainty on this quantity ( g(f)). When GPs provide a valid model of uncertainty, our
experiments show that our auto-differentiable planning framework significantly outperforms other
baselines. We further demonstrate that our conceptual framework extends to deep learning-based
uncertainty quantification methods like Ensemble+ while highlighting computational challenges
that need to be resolved in order to scale our ideas. For simplicity, we assume a naive predictor,
i.e.,Ïˆ(Â·)â‰¡0. However, we emphasize that this problem is just as complex as if we were using a
sophisticated model Ïˆ(.). The performance gap between the algorithms primarily depends on the
level of uncertainty in our prior beliefs.
To evaluate the performance of our algorithm, we benchmark it against several baselines. Our first
set of baselines are from active learning [1]:
Active Learning Heuristics: (1) UNCERTAINTY SAMPLING (STATIC ): In this approach, we query
the samples for which the model is least certain about. Specifically, we estimate the variance of the
latent output f(X)for each Xâˆˆ X poolusing the UQ module and select the top- Kpoints with the
highest uncertainty. (2)UNCERTAINTY SAMPLING (SEQUENTIAL ): This is a greedy heuristic that
sequentially selects the points with the highest uncertainty within a batch, while updating the poste-
rior beliefs using pseudo labels from the current posterior state. Unlike U NCERTAINTY SAMPLING
(STATIC ), this method takes into account the information gained from each point within batch, and
hence tries to diversify the selected points within a batch.
We also compare our approach to solving the planning problem using (3)REINFORCE-based pol-
icy gradients, which implements one-step look ahead policy gradient using the score trick. Finally,
we study (4)RANDOM SAMPLING , which selects each batch uniformly at random from the pool.
We repeat all experiments with 10 random seeds.
5.1 Planning with Gaussian processes
We now briefly describe the data generation process for the GP experiments, deferring a more de-
tailed discussion of the dataset generation to Section E. We use both the synthetic data and the real
data to test our methodology. For the simulated data , we construct a setting where the general pop-
ulation is distributed across 51 non-overlapping clusters while the initial labeled data D0just comes
from one cluster. In contrast, both Dpoolâ‰œ(Xpool, Ypool),Devalâ‰œ(Xeval,Yeval)are generated from
all the clusters. We begin with a low-dimensional scenario, generating a one-dimensional regression
setting using a Gaussian Process (GP). Although the data-generating process is not known to the
80 2 4 6 8 10
Time horizon0.0060.0070.0080.0090.010Var of 2 loss
REINFORCE
Autodiff 1-lookahead (ours)
Uncertainty sampling - sequential
Random sampling
Uncertainty sampling - staticFigure 6: (Real-world eICU data) Variance of
mean squared loss evaluated through the poste-
rior belief Âµtat each horizon t. Even 1-step
lookaheads are extremely effective planners, and
auto-differentiation-based pathwise policy gra-
dients provide a reliable optimization algorithm
based on low-variance gradients.
0 2 4 6 8 10
Time horizon0.7250.7500.7750.8000.8250.8500.875Error of estimated model 2 loss
REINFORCE
Autodiff 1-lookahead (ours)
Uncertainty sampling - sequential
Random sampling
Uncertainty sampling - staticFigure 7: (Real-world eICU data) Error between
MSE calculated based on collected data D0:Tvs.
population oracle MSE over Devalâˆ¼PX. Re-
ducing uncertainty over posteriors directly leads
to better OOD evaluations. Our method signifi-
cantly outperforms active learning heuristics, and
1-lookahead with REINFORCE policy gradients.
algorithms, we assume that the GP hyperparameters are known to all the algorithms to ensure fair
comparisons. This can be viewed as a setting where our prior is well-specified, allowing us to isolate
the effects of different policy optimization approaches without any concerns about the misspecified
priors or in-accurate posteriors. We select 10batches, each of size K= 5 across T= 10 time
horizons.
To examine the robustness of our method against the distributional assumptions made in the sim-
ulated case, we then move to a real dataset where the correct prior is not known. We simulate
selection bias from the eICU dataset [30], which contains real-world patient data with in-hospital
mortality outcomes. We conduct a k-means clustering to generate 51 clusters and then select data
from those clusters. We view this to be a credible replication of practice, as severe distribution shifts
are common due to selection bias in clinical labels. To convert the binary mortality labels into a
regression setting, we train a random forest classifier and fit a GP on predicted scores, which serves
as the UQ module for all the algorithms. As before, the task is to select 10 batches, each consisting
of 5 samples, across 10 time horizons.
In Figures 4 and 5, we present results for the simulated data. Figure 4 shows the variance of â„“2loss,
while Figure 5 presents the error in the estimated â„“2loss using Âµt(relative to true â„“2loss, that is un-
known to the algorithm). As we can see from these plots, our method Autodiff 1-lookahead
gives substantial improvements over active learning baselines. Compared to the other one-step
lookahead planning approach using REINFORCE-based policy gradients, we observe that pathwise
policy gradients provide significantly more robust performance over all horizons.
In Figures 6 and 7, we observe similar findings on the eICU data. We see that planning policies
(REINFORCE and ours) consistently outperform other heuristics by a large margin. Active learning
baselines perform poorly in these small-horizon batched problems and can sometimes be even worse
than the random search baselines. Overall, our results show the importance of careful planning in
adaptive labeling for reliable model evaluation.
We offer some intuition as to why Autodiff 1-lookahead may outperform other heuristic algo-
rithms. First, U NCERTAINTY SAMPLING (STATIC ) while myopically selects the top- Kinputs with
the highest uncertainty, it fails to consider the overlap in information content among the â€œbestâ€ in-
stances; see [1] for more details. In other words, it might acquire points from the same region with
high uncertainty while failing to induce diversity among the batch. Although U NCERTAINTY SAM-
PLING (SEQUENTIAL ) somewhat addresses the issue of information overlap, a significant drawback
of this algorithm is the disconnect between the objective we aim to optimize and the algorithm. For
example, it might sample from a region with high uncertainty but very low density.
5.2 Planning with neural network-based uncertainty quantification methods ( Ensemble+ )
We now provide a proof-of-concept that shows the generalizability of our conceptual framework to
the deep learning based UQ modules, specifically focusing on Ensemble+ due to their previously
9observed superior performance [28]. Recall that implementing our framework with deep learning
based UQ modules requires us to retrain the model across multiple possible random actions a(Î¸)
sampled from the current policy Ï€Î¸. This requires significant computational resources, in sharp con-
trast to the GPs where the posteriors are in closed form and can be readily updated and differentiated.
Due to the computational constraints, we test Ensemble+ on a toy setting to demonstrate the gen-
eralizability of our framework. We consider a setting where the general population consists of four
clusters, while the initial labeled data only comes from one cluster. Again we generate data using
GPs. The task is to select a batch of 2 points in one horizon. We detail the Ensemble+ architec-
ture in the Appendix, and we assume prior uncertainty to be large (depend on the scaling of prior
generating functions). The results are summarized in the Table 1.
Table 1: Performance under Ensemble+ as UQ module
Algorithm Variance of â„“2loss estimate Error of â„“2loss estimate
RANDOM SAMPLING 7129.8 Â±1027.0 136.2Â±8.28
UNCERTAINTY SAMPLING (STATIC ) 10852 Â±0.0 162.156 Â±0.0
UNCERTAINTY SAMPLING (SEQUENTIAL )8585.5 Â±898.9 144Â±6.93
REINFORCE 1697.1 Â±0.0 45.27Â±0.0
Autodiff 1-lookahead 1697.1 Â±0.0 45.27Â±0.0
6 Gradient Estimation - Theoretical Insight
In this section, we present theoretical analysis of the statistical properties (bias-variance trade-
off) of the REINFORCE and AUTODIFF (smoothed-pathwise (4)) gradient estimators. To com-
pare the two estimators, we consider a simplified setting with two actions A={âˆ’1,1}, and the
policy is parametrized by Î¸âˆˆ[0,1]withÏ€Î¸(âˆ’1) = Î¸andÏ€Î¸(1) = 1 âˆ’Î¸, Additionally, we
are interested in the gradient âˆ‡Î¸H(Î¸), where H(Î¸) =EAâˆ¼Ï€Î¸G(A)withG(A) = Afor sim-
plicity. Given Ni.i.d. samples of Aiâˆ¼Ï€Î¸, the REINFORCE estimator is defined as Ë†âˆ‡RF
N=
1
NPN
i=1
G(Ai)âˆ‡Î¸log(Ï€Î¸(Ai))
. The pathwise gradient estimator âˆ‡grad
Ï„,Nis the N-sample approx-
imation of EUâˆ¼Uni[0,1][âˆ‡Î¸G(hÏ„(U, Î¸))], where hÏ„(U, Î¸) = 
exp Uâˆ’Î¸
Ï„
âˆ’1
/ 
exp Uâˆ’Î¸
Ï„
+ 1
(smoothing of h(U, Î¸) := 2 I(U > Î¸ )âˆ’1d=A). Our result (proof in Section F) highlights the
conditions under which Ë†âˆ‡grad
Ï„,Nachieves a lower mean squared error (mse) compared to Ë†âˆ‡RF
N.
Theorem 1. ForÎ¸âˆˆ[0,1]andNâ‰¤1
4Î¸(1âˆ’Î¸)âˆ’1, there exists ËœÏ„depending on (N, Î¸)such that
MSE(Ë†âˆ‡grad
ËœÏ„,N)<4â‰¤MSE(Ë†âˆ‡RF
N).
Additionally, for any N,Î¸=1
k, and kâ†’ âˆž , we have the following MSE(Ë†âˆ‡RF
N) = â„¦( k),
MSE(Ë†âˆ‡grad
ËœÏ„,N)<4. The same statement holds for Î¸= 1âˆ’1
kas well. This implies that the mse
ofË†âˆ‡RF
Nis unbounded, while the mse of gradient estimator is bounded.
7 Conclusion and Future Work
Supervised data often suffers severe selection bias when labels are expensive. We propose a new
framework for adaptive labeling for model evaluation under out-of-support distribution shifts. Fol-
lowing a Bayesian framework, we formulate an MDP over posterior beliefs on model performance.
We show that these planning problems can be efficiently solved with pathwise policy gradients,
computed through a carefully designed auto-differentiable pipeline. We also demonstrate that even
one-step lookahead policies outperform heuristic active learning algorithms. However, this improve-
ment comes at the cost of additional computational resources. Our formulation is thus appropriate
in high-stake settings such as clinical trials where labeling costs far exceed computational costs.
We further highlight some important nuances which we learned from our experiments. There are
some important properties that a UQ module should have for it to perform well in our framework.
The most important feature is that posteriors should be consistent, that is, early stopping of the train-
ing should not cause a significant change across the ranking of the subsets of the pool. Second, the
posterior update should be done efficiently, which can be achieved either by doing parallelization or
having UQ modules that provide readily available posterior updates, such as GPs. Recent advances
in Bayesian transformers [24, 22] will be an interesting direction to explore in this regard.
10References
[1] C. C. Aggarwal, X. Kong, Q. Gu, J. Han, and S. Y . Philip. Active learning: A survey. In Data
classification , pages 599â€“634. Chapman and Hall/CRC, 2014.
[2] D. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming . Athena Scientific, 1996.
[3] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural
network. In Proceedings of the 32nd International Conference on Machine Learning , pages
1613â€“1622. PMLR, 2015.
[4] F. de Avila Belbute-Peres, K. Smith, K. Allen, J. Tenenbaum, and J. Z. Kolter. End-to-end
differentiable physics for learning and control. In Advances in Neural Information Processing
Systems . Curran Associates, Inc., 2018.
[5] T. Du, Y . Li, J. Xu, A. Spielberg, K. Wu, D. Rus, and W. Matusik. D3 {pg}: Deep differentiable
deterministic policy gradients, 2020.
[6] Y . Efroni, G. Dalal, B. Scherrer, and S. Mannor. Multiple-step greedy policies in approximate
and online reinforcement learning. In Advances in Neural Information Processing Systems ,
2018.
[7] Y . Efroni, M. Ghavamzadeh, and S. Mannor. Online planning with lookahead policies. In
Advances in Neural Information Processing Systems , 2020.
[8] P. I. Frazier. Bayesian Optimization , pages 255â€“278. 2018.
[9] P. I. Frazier, W. B. Powell, and S. Dayanik. A knowledge-gradient policy for sequential infor-
mation collection. SIAM Journal on Control and Optimization , 47(5):2410â€“2439, 2008. doi:
10.1137/070693424.
[10] Y . Gal and Z. Ghahramani. Dropout as a Bayesian approximation: Representing model un-
certainty in deep learning. In Proceedings of The 33rd International Conference on Machine
Learning , pages 1050â€“1059, 2016.
[11] S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization , 23(4):2341â€“2368, 2013.
[12] J. C. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical
Society, Series B , 41(2):148â€“177, 1979.
[13] E. Grefenstette, B. Amos, D. Yarats, P. M. Htut, A. Molchanov, F. Meier, D. Kiela, K. Cho, and
S. Chintala. Generalized inner loop meta-learning. arXiv preprint arXiv:1910.01727 , 2019.
[14] N. Houlsby, F. Husz Â´ar, Z. Ghahramani, and M. Lengyel. Bayesian active learning for classifi-
cation and preference learning. arXiv:1112.5745 [cs.CV] , 2011.
[15] Z. Huang, Y . Hu, T. Du, S. Zhou, H. Su, J. B. Tenenbaum, and C. Gan. Plasticinelab: A
soft-body manipulation benchmark with differentiable physics. In International Conference
on Learning Representations , 2021.
[16] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with Gumbel-softmax. In Inter-
national Conference on Learning Representations , 2017.
[17] A. Kirsch, J. van Amersfoort, and Y . Gal. BatchBALD: Efficient and diverse batch acquisition
for deep bayesian active learning. In Advances in Neural Information Processing Systems ,
volume 32, 2019.
[18] B. Lakshminarayanan, A. Pritzel, and C. Blundell. Simple and scalable predictive uncertainty
estimation using deep ensembles. In Proceedings of the 31st International Conference on
Neural Information Processing Systems , page 6405â€“6416, 2017.
[19] A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In Pro-
ceedings of the 31st International Conference on Machine Learning , volume 32 of Proceedings
of Machine Learning Research , pages 1791â€“1799, 2014.
[20] S. Mohamed, M. Rosca, M. Figurnov, and A. Mnih. Monte Carlo gradient estimation in ma-
chine learning. Journal of Machine Learning Research , 21(1), 2020.
[21] M. A. Z. Mora, M. Peychev, S. Ha, M. Vechev, and S. Coros. Pods: Policy optimization via
differentiable simulation. In Proceedings of the 38th International Conference on Machine
Learning , 2021.
[22] S. M Â¨uller, N. Hollmann, S. P. Arango, J. Grabocka, and F. Hutter. Transformers can do
bayesian inference. In Proceedings of the Tenth International Conference on Learning Repre-
sentations , 2022.
[23] R. MUNOS, S. BUBECK, and G. STOLTZ. Pure exploration for multi-armed bandit problems.
Lecture Notes in Computer Science , 2009.
[24] T. Nguyen and A. Grover. Transformer neural processes: Uncertainty-aware meta learning
via sequence modeling. In Proceedings of the 39th International Conference on Machine
11Learning , 2022.
[25] I. Osband and B. Van Roy. Bootstrapped Thompson sampling and deep exploration.
arXiv:1507.00300 [stat.ML] , 2015.
[26] I. Osband, J. Aslanides, and A. Cassirer. Randomized prior functions for deep reinforcement
learning. In Advances in Neural Information Processing Systems 31 , volume 31, 2018.
[27] I. Osband, Z. Wen, S. M. Asghari, V . Dwaracherla, X. Lu, M. Ibrahimi, D. Lawson, B. Hao,
B. Oâ€™Donoghue, and B. V . Roy. The Neural Testbed: Evaluating Joint Predictions. In Advances
in Neural Information Processing Systems , 2022.
[28] I. Osband, Z. Wen, S. M. Asghari, V . Dwaracherla, M. Ibrahimi, X. Lu, and B. V . Roy. Epis-
temic neural networks. In Thirty-seventh Conference on Neural Information Processing Sys-
tems, 2023.
[29] M. Papini, D. Binaghi, G. Canonaco, M. Pirotta, and M. Restelli. Stochastic variance-reduced
policy gradient. In Proceedings of the 35th International Conference on Machine Learning ,
pages 4026â€“4035, 2018.
[30] T. J. Pollard, A. E. Johnson, J. D. Raffa, L. A. Celi, R. G. Mark, and O. Badawi. The eICU
Collaborative Research Database, a freely available multi-center database for critical care re-
search. Scientific data , 5(1):1â€“13, 2018.
[31] C. E. Rasmussen. Gaussian processes for machine learning. pages 63â€“71, 2006.
[32] J. Ren, X. Feng, B. Liu, X. Pan*, Y . Fu, L. Mai, and Y . Yang. TorchOpt: An Efficient Library
for Differentiable Optimization. Journal of Machine Learning Research , 24(367):1â€“14, 2023.
[33] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate
inference in deep generative models. In Proceedings of the 31st International Conference on
Machine Learning , pages 1278â€“1286, 2014.
[34] B. Settles. Active learning literature survey. 2009.
[35] H. J. Suh, M. Simchowitz, K. Zhang, and R. Tedrake. Do differentiable simulators give better
policy gradients? In Proceedings of the 39th International Conference on Machine Learning ,
volume 162, 2022.
[36] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction . MIT press, 2018.
[37] C. Wang, S. Sun, and R. Grosse. Beyond marginal uncertainty: How accurately can Bayesian
regression models estimate posterior predictive correlations? In International Conference on
Artificial Intelligence and Statistics , pages 2476â€“2484, 2021.
[38] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning. Machine Learning , 8:229â€“256, 1992.
[39] S. M. Xie and S. Ermon. Reparameterizable subset sampling via continuous relaxations. In
Proceedings of the 28th International Joint Conference on Artificial Intelligence , IJCAIâ€™19,
page 3919â€“3925, 2019.
[40] J. Xu, V . Makoviychuk, Y . Narang, F. Ramos, W. Matusik, A. Garg, and M. Macklin. Accel-
erated policy learning with parallel differentiable simulation. In International Conference on
Learning Representations , 2021.
[41] J. Yang, K. Zhou, Y . Li, and Z. Liu. Generalized out-of-distribution detection: A survey.
arXiv:2110.11334 [cs.CV] , 2021.
[42] E. Zhao, A. Liu, A. Anandkumar, and Y . Yue. Active learning under label shift. In Proceedings
of The 24th International Conference on Artificial Intelligence and Statistics , pages 3412â€“
3420, 2021.
12A Graphical Representation of Our Algorithm
UQ module (state ðð‘»)
ðœ‡ð‘‡ 
1-step lookahead (Algorithm 2) at time step t
Initialize policy ð…ðœ½ 
Generate  pseudo 
labels (à´¤ð‘Œð‘ð‘œð‘œð‘™) for 
(ð‘‹ð‘ð‘œð‘œð‘™) using UQ 
module (ðœ‡ð‘¡)Pseudo posterior update 
ðœ‡+ð‘Žðœƒ using pseudo 
labels and soft K -subset 
sampling [ð‘Žðœƒ]Estimate variance 
of MSE under 
updated posterior 
[Gðœ‡+ð‘Žðœƒ]
SGD update policy ð…ðœ½ using  end-to-end auto -differentiation
(Repeat N times)Autodiff 1 -Lookahead
ð’‡âˆ— - True data generating function
ðð’• - Posterior state (distribution over ð’‡) at time t 
      [UQ module trained on data ð‘¿ðŸ:ð’•,ð’€ðŸ:ð’•]  
Figure 8: Description of our algorithm â€“ Autodiff 1-lookahead
B Gaussian Process posterior updates
For any inputs X, we assume f(X)is Gaussian with mean m(X)andCov(f(Xi), f(Xj)) =
K(Xi, Xj). In addition, the observation consists of (X,Y)withY=fâ‹†(X) +ÏµandÏµâˆ¼
N(0, Ïƒ2I)for some noise level Ïƒ >0. Given the training data (X,Y)and test points Xâˆ—, closed
form posterior estimates over fâ‹†can be computed
fâˆ—|X,Y,Xâˆ—âˆ¼ N(Â¯f|âˆ—,Cov(fâˆ—)),
where Â¯f|âˆ—â‰œK(Xâˆ—,X)[K(X,X) +Ïƒ2I]âˆ’1Y
Cov(fâˆ—)â‰œK(Xâˆ—,Xâˆ—)âˆ’ K(Xâˆ—,X)[K(X,X) +Ïƒ2I]âˆ’1K(X,Xâˆ—).
Using the above expression, we can update our belief about fâˆ—after observing new data.
C Details of K-subset sampling algorithm
Algorithm 3 for soft K-subset sampling was introduced in [39]. We present this algorithm here and
refer the reader to [39] for further details.
D Details of weighted GP
We present a weighted GP algorithm (Algorithm 4) adapted from Algorithm 2.1 in [31]. Recall that
GPsf(Â·)âˆ¼ GP (m(Â·),K(Â·,Â·))are defined by a mean function m(Â·)and kernel K(Â·,Â·).
Now assume we are given input data (X,Y), we are also given some weights wfor these inputs,
and we aim to compute posterior estimates over fâ‹†for test points Xâˆ—considering the weights w.
Let us define the following terms: Kâ‰œK(X,X), Kâˆ—â‰œK(Xâˆ—,X)andKâˆ—,âˆ—â‰œK(Xâˆ—,Xâˆ—).
Using these definitions, GP Algorithm 4 provides posterior estimates of Â¯fâ‹†for the test samples Xâˆ—,
given that the input data (X,Y)has weights w.
13Algorithm 3 SoftK-subset sampling algorithm
1:Inputs: Weight vector wâˆˆRn
+
2:Sample nindependent standard Gumbel random variables g1, g2, . . . , g n
3:Compute keys Ë†ri=gi+ log( wi)for all i
4:Initialize Îº1
i= Ë†rifor all i= 1, . . . , n
5:Forj= 1, .., K , set
aj
i=exp(Îºj
i/Ï„)Pn
k=1exp(Îºj
k/Ï„)for all i= 1, . . . , n,
Îºj+1
i=Îºj
i+ log(1 âˆ’aj
i)for all i= 1, ..., n .
6:Return: The soft K-hot vector, a=a1+a2+...+aK.
Algorithm 4 Weighted Gaussian process regression
1:Inputs: K, Kâˆ—, Kâˆ—,âˆ—,Y,w
2:Kw=K((1âˆ’I)wwâŠ¤+I)
3:Kw,âˆ—=wâŠ™Kâˆ—
4:L=Cholesky (Kw+Ïƒ2I)
5:Î±=LâŠ¤\(L\Y)
6:Â¯f=KâŠ¤
w,âˆ—Î±
7:v=L\Kw,âˆ—
8:V=Kâˆ—,âˆ—âˆ’vâŠ¤v
9:Return: mean and covariance: (Â¯f, V)
D.1 Smoothing the recall objective
Performance metric, such as the Recall of a model Î¨(.), is not differentiable. Recall
thatH(Î¸)â‰œEÂ¯Ypoolâˆ¼Âµ;Sâˆ¼p(Â·|w(Î¸))[G 
ÂµS
+
], where G(ÂµS
+) = Varfâˆ¼ÂµS
+g(f), and g(f)â‰œ
EXâˆ¼PXh
EYâˆ¼p(Â·|f,X)h
1{Î¨(X)>0}|Y= 1ii
withÂµTdepending on Ï€Î¸. To estimate âˆ‡Î¸H(Î¸)
in a differentiable manner, we can use a smooth approximation of g(f)using the softmax trick [16]
and draw 2ni.i.d. Gumbel (0,1)samples Rl
iwithlâˆˆ {1,2}. Define:
gÏ„(f;R)â‰œPn
i=1YÏ„
i(f;R)Î¨(Xi)Pn
i=1YÏ„
i(f;R),
where YÏ„
i(f;R) =exp 
(logfi) +R1
i)/Ï„
exp ((log fi) +R1
i)/Ï„) + exp ((log(1 âˆ’fi)) +R2
i)/Ï„),
hereÏ„ >0is the temperature hyperparameter that controls the smoothing of gÏ„(f;R). It is easy to
show that ER[YÏ„
i(f;R)]â‰ˆfi, making gÏ„(f;R)a natural approximation of the previously defined
Recall g(f).
E Experimental details
In this section, we provide detailed information about the experiments discussed in Section 5.
E.1 Planning with Gaussian Processes - synthetic data experiments
As mentioned earlier, we generate our data using a Gaussian Process (GP). Specifically, we use a GP
with an RBF kernel: fiâˆ¼ GP (m,K), where m(X) = 0 andK(X, Xâ€²) =Ïƒ2
fexp
âˆ’||Xâˆ’Xâ€²||2
2
2â„“2
,
further Gaussian noise N(0, Ïƒ2)is added to the outputs. We set â„“= 1,Ïƒ2
f= 0.69, and Ïƒ2= 0.01.
Recall that the marginal distribution PXconsists of 51non-overlapping clusters. To achieve this,
we create a polyadic sampler, which first samples 51anchor points spaced at linearly increasing
distances from the center to avoid overlap. These anchor points serve as our cluster centers. We
14sample the points around these anchor points by adding a small Gaussian noise N(0,0.25). The
training points are drawn from a single cluster, while the pool and evaluation points are drawn from
all the 51clusters. The setup includes 100 initial labeled data points, 500 pool points and 285
evaluation points used to estimate the objective.
Autodiff 1-lookahead training and hyperparameters: As mentioned earlier, the underly-
ing uncertainty quantification (UQ) module for these experiments is the GP. Although our algo-
rithm does not know the true data generating function, but it has access to the GP hyperparam-
eters. Therefore we use a GPwith an RBF kernel: fiâˆ¼ GP (m,K), where m(X) = 0 and
K(X, Xâ€²) =Ïƒ2
fexp
âˆ’||Xâˆ’Xâ€²||2
2
2â„“2
, with Gaussian noise N(0, Ïƒ2)added to the outputs. We set
â„“= 1,Ïƒ2
f= 0.69andÏƒ2= 0.01. For soft K-subset sampling (see Algorithm 3), we set Ï„to0.1.
Further for evaluating the objective function Var(g(f))- we take 100samples of f(Xeval)from the
posterior state Âµa(Î¸)
+, see Algorithm 2. For policy optimization in each horizon, we use the Adam
optimizer with a learning rate of 0.1to perform policy gradient steps over 100epochs. The results
presented are averaged over 10different training seeds.
Policy gradient (REINFORCE) training and hyperparameters: As mentioned earlier, all the
algorithms have access to the true to GP hyperparameters. For evaluating the objective Var(g(f))
under a given action, we take 100samples of f(Xeval). To optimize the policy in each horizon,
we use the Adam optimizer with a learning rate of 0.1to perform policy gradient updates over 100
epochs. The results presented are averaged over 10different training seeds.
E.2 Planning with Gaussian Processes - real data (eICU) experiments
The eICU dataset is a healthcare dataset that contains data from various critical care units across the
United States. To create a supervised learning setup from this dataset, we first extracted the 10most
important features using a Random Forest classifier (number of trees = 100 , criterion =â€œginiâ€).
The outcome variable was in-hospital mortality. Some examples of the extracted features include:
Hospital length of stay, Number of days on the ventilator and the Last recorded temperature on Day
1 of ICU admission. We transformed the classification outcome variable (in-hospital mortality) into
a regression task using the probability of in-hospital mortality predicted by the classifier. To adapt
the extracted data to our setting, we introduced selection bias in the data. We generated 51 clusters
through the standard kâˆ’means algorithm. Our initial labeled data comes from only 1cluster, while
the pool and evaluation data comes from all the 51clusters. Our dataset consists 100initial labeled
data points, 500pool points, and 285evaluation points used to estimate the objective.
We then fitted a GP with an RBF kernel to above data using GP-regression. The resulting GP
hyperparameters were: m(x) = 0 .255,â„“= 0.50, Ïƒ2
f= 1.0, Ïƒ2= 0.0001 . ThisGPmodel serves as
our uncertainty quantification (UQ) module.
Autodiff 1-lookahead training and hyperparameters: For soft K-subset sampling (see Algo-
rithm 3), we set Ï„to0.1. Further to evaluate the objective function Var(g(f)), we take 100 samples
off(Xeval)from the posterior state Âµa(Î¸)
+, see Algorithm 2. For policy optimization in each horizon,
we use the Adam optimizer with a learning rate of 0.1to perform policy gradient steps over 1000
epochs. The results presented are averaged over 10different training seeds.
Policy gradient (REINFORCE) training and hyperparameters: To evaluate the objective
Var(g(f))under an action, we take 100samples of f(Xeval). For optimizing the policy in each
horizon, we use an Adam optimizer with learning rate of 0.1, performing policy gradient updates
over1000 epochs. The results presented are averaged over 10different training seeds.
E.3 Planning with Ensemble+ experiments
Once again, we use GPs as our data generating process. Specifically, we use a GP with an RBF
kernel: fiâˆ¼ GP (m,K), where m(X) = 0 andK(X, Xâ€²) =Ïƒ2
fexp
âˆ’||Xâˆ’Xâ€²||2
2
2â„“2
. Additionally,
Gaussian noise N(0, Ïƒ2)is added to the outputs. We set â„“= 1,Ïƒ2
f= 0.69, and Ïƒ2= 0.01. The
marginal distribution PXconsists of 4non-overlapping clusters. We follow the same process as in
the Gaussian process experiments to form the cluster centers and sample points around them. While
the training points are drawn from a single cluster, both the pool and evaluation points are drawn
15from all the 4clusters. Our setup includes 20initial labeled data points, 10pool points, and 252
evaluation points used to evaluate the objective.
Ensemble+ Architecture: We use an ensemble of 10 models. Each model being a 2-hidden layer
MLP with 50 units per hidden layer. In addition, each models includes an additive prior. The additive
prior function for each model is fixed to be a 2-hidden layer MLP with 50 units in each hidden layer.
Specifically, the m-th model for 1â‰¤mâ‰¤10of the Ensemble+ takes the form
fÎ·m(X) =gÎ·m(X) +Î±pm(X),
where gÎ·mis the trainable part of the network while Î±pm(Â·)is the additive prior function. Here Î±
controls our prior belief about the uncertainty â€”higher the Î±, the greater the uncertainty. The prior
functions pm(Â·)differs across models mdue to different initializations. In our setup, we set Î±= 100
to reflect a high level of uncertainty in the prior beliefs.
Ensemble+ training: For a given dataset D= (Xi, Yi)n
i=1, we train the m-th model so as to
minimize the following loss function
â„“(Î·m,D) =1
nnX
i=1(gÎ·m(Xi) +Î±pm(Xi)âˆ’Yi)2+Î»âˆ¥Î·mâˆ¥2
2.
We tune Î»to0.1and use the Adam optimizer with a tuned learning rate of 0.1. Each model is trained
for 50 iterations. Although in standard Ensemble+ each model is trained on a different bootstrapped
subset from the dataset D, in our case we train all the models within Ensemble+ on the entire dataset
available.
Autodiff 1-lookahead training and hyperparameters: Recall that in soft K-subset sampling
there is a hyperparameter Ï„(see Algorithm 3), which we set to 0.1. To differentiate through the
argmin operation, we employ the differentiable optimizer from the torchopt package, specifically
the MetAdam Optimizer with a learning rate of 0.1. For optimizing the sampling policy, we use the
Adam optimizer with a learning rate of 0.05, performing policy gradient updates over 500epochs.
F Proof of Theorem 1
We begin by analyzing the REINFORCE based gradient estimator. With Nsamples, we have:
MSE(Ë†âˆ‡RF
N) = Var( Ë†âˆ‡RF
N) =1
NVarAâˆ¼Ï€Î¸
Aâˆ‡Î¸log(Ï€Î¸(A))
.
Using the definition of A, we derive the following:
VarAâˆ¼Ï€Î¸(Aâˆ‡Î¸log(Ï€Î¸(A))) =1
(1âˆ’Î¸)Î¸âˆ’4.
As a result, we have:
MSE(Ë†âˆ‡RF
N) =1
N1
(1âˆ’Î¸)Î¸âˆ’4
. (9)
Next, we analyze the pathwise gradient estimator Ë†âˆ‡grad
ËœÏ„,N. Let hÏ„(U, Î¸) =exp(Uâˆ’Î¸
Ï„)âˆ’1
exp(Uâˆ’Î¸
Ï„)+1be a random
variable, where Uâˆ¼Uni[0,1], so that:
MSE(Ë†âˆ‡grad
Ï„,N) = [Bias(âˆ‡Î¸hÏ„(U, Î¸))]2+1
NVar(âˆ‡Î¸hÏ„(U, Î¸)). (10)
To compute the bias term in (10), we note that
E(hÏ„(U, Î¸)) =E 
1
1 + exp 
âˆ’Uâˆ’Î¸
Ï„!
âˆ’E 
1
exp Uâˆ’Î¸
Ï„
+ 1!
= 2Ï„log 
exp(1âˆ’Î¸
Ï„) + 1
exp(âˆ’Î¸
Ï„) + 1!
âˆ’1,
âˆ‡Î¸E(hÏ„(U, Î¸)) = 2 
1
1 + exp 1âˆ’Î¸
Ï„âˆ’1
1 + exp âˆ’Î¸
Ï„!
.
16Therefore,
[Bias(âˆ‡Î¸hÏ„(U, Î¸))]2= 4 
1 +1
1 + exp 1âˆ’Î¸
Ï„âˆ’1
1 + exp âˆ’Î¸
Ï„!2
. (11)
Note that the above term goes to 4 as Ï„â†’ âˆž . The other part contributing to the mse (10) is the
variance term and it satisfies
Var(âˆ‡Î¸hÏ„(U, Î¸))â‰¤E[âˆ‡Î¸(hÏ„(U, Î¸))]2=4
Ï„"
3 exp âˆ’Î¸
Ï„
+ 1
6 
exp âˆ’Î¸
Ï„
+ 13âˆ’3 exp 1âˆ’Î¸
Ï„
+ 1
6 
exp 1âˆ’Î¸
Ï„
+ 13#
.
Combining the above equation with (11), we arrive at MSE(Ë†âˆ‡grad
ËœÏ„,N)â‰¤vN(Ï„), where
vN(Ï„)â‰œ4 
1 +1
1 + exp 1âˆ’Î¸
Ï„âˆ’1
1 + exp âˆ’Î¸
Ï„!2
+1
N4
Ï„"
3 exp âˆ’Î¸
Ï„
+ 1
6 
exp âˆ’Î¸
Ï„
+ 13âˆ’3 exp 1âˆ’Î¸
Ï„
+ 1
6 
exp 1âˆ’Î¸
Ï„
+ 13#
For all Nâ‰¥1, we can verify that limÏ„â†’âˆžvN(Ï„) = 4 and for large enough Ï„,vN(Ï„)is increasing
inÏ„. Thus, there exists some ËœÏ„ > 0such that MSE(Ë†âˆ‡grad
ËœÏ„,N)<4. Comparing this with (9) for
MSE(Ë†âˆ‡RF
N)completes the proof.
17NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: Sections 3, 4, and 5
Guidelines:
â€¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Section 1
Guidelines:
â€¢ The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate â€Limitationsâ€ section in their paper.
â€¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢ The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
â€¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢ If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
â€¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
18Justification: Sections 6 and F
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢ All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
â€¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
â€¢ Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Sections 5 and E
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢ If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
â€¢ While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
19Answer: [Yes]
Justification: Section 4
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢ Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢ While we encourage the release of code and data, we understand that this might not
be possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Sections 5 and E
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
â€¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [Yes]
Justification: Sections 5 and E
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ The authors should answer â€Yesâ€ if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
20â€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
â€¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [No]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Our research conform, in every respect, with the NeurIPS Code of Ethics
https://neurips.cc/public/EthicsGuidelines .
Guidelines:
â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Sections 1 and 7
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
21â€¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢ If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
â€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Sections 4 and 5
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢ The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢ If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
â€¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
22â€¢ If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [No]
Justification: We do not have any new assets as of now.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢ At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
â€¢ Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
â€¢ Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
â€¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢ For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
23