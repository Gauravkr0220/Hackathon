Differentially Private Equivalence Testing for
Continuous Distributions and Applications
Daniel Omer
Math. Dept.
Bar-Ilan University
omerdan@biu.ac.ilOr Sheffet
Faculty of Engineering
Bar-Ilan University
or.sheffet@biu.ac.il
Abstract
We present the first algorithm for testing equivalence between two continuous
distributions using differential privacy (DP). Our algorithm is a private version
of the algorithm of Diakonikolas et al [ 16]. The algorithm of [ 16] uses the data
itself to repeatedly discretize the real line so that â€” when the two distributions
are far apart in Ak-norm â€” one of the discretized distributions exhibits large
L2-norm difference; and upon repeated sampling such large gap would be detected.
Designing its private analogue poses two difficulties. First, our DP algorithm can
notresample new datapoints as a change to a single datapoint may lead to a very
large change in the descretization of the real line. In contrast, the (sorted) index of
the discretization point changes only by 1between neighboring instances, and so
we use a novel algorithm that set the discretization points using random Bernoulli
noise, resulting in only a few buckets being affected under the right coupling.
Second, our algorithm, which doesnâ€™t resample data, requires we also revisit the
utility analysis of the original algorithm and prove its correctness w.r.t. the original
sorted data; a problem we tackle using sampling a subset of Poisson-drawn size
from each discretized bin. Lastly, since any distribution can be reduced to a
continuous distribution, our algorithm is successfully carried to multiple other
families of distributions and thus has numerous applications.
1 Introduction
Differential privacy (DP), a mathematically rigorous notion that bounds the effect of any single datum
on the output distribution, is the current (de facto) gold-standard of privacy preserving data analysis.
By now we have a myriad of DP-algorithms for learning and for various tasks of statistical inference .
Indeed, the design of DP-hypothesis testers is crucial for the dissipation of DP into other data-centric
fields â€” such as economics, education and health â€” that analyze sensitive data in massive quantities.
However, by and large the design of DP-hypothesis testing is confined to distributions over finite (and
thus discrete) domains rather than contiunuous distributions.
Hypothesis testing over continuous distributions poses a special challenge due to infinitesimally
small perturbations â€” two continuous distributions can have Total-Variation distance of Î±using,
say, exponentially many intervals each with an exponentially small shift of probability mass, which
clearly cannot be detected with polynomially-size sample. Luckily, this issue was resolved in the
works of [ 10,15,16] who restricted the TV-distance to using k-intervals. Formally, these works
measure distance in Ak-norm : where for any two distributions PandQwe have that âˆ¥P âˆ’ Qâˆ¥ Ak=
sup
IPk
j=1|P(Ij)âˆ’ Q(Ij)|where Iis a partition of the real-line Rintokintervals I1, I2, ..., I k.
But the fact remains that continuous distributions pose a special challenge for DP-algorithm designers.
In fact, releasing even a simple statistics like the median is impossible over infinite domains [ 6,7].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).And yet, as we show in this work, it is possible to compare (samples from) two continuous distributions
while preserving differential privacy and discern whether the two are identical or far-apart in Ak-
distance. This suggests a sharp contrast between the task of learning from a (single) continuous
distribution and the task of statistical inference based on twocontinuous distributions.
Baseline. This sharp contrast may seem striking at first, yet on second thought, it is known that
any statistical inference task that only has two possible outputs (in our case - â€œaccept / identicalâ€ vs
â€œreject / far-apartâ€) can easily be made private using the Subsample-and-Aggregate framework [ 26] â€”
simply run O(1/Ïµ)-times the non-private equivalence tester of Diakonikolas et al [ 16] and return the
most prevalent output using simple noisy count. This gives a simple Ïµ-DP equivalence tester with
O((k4/6Î±âˆ’6/5+k1/2Î±âˆ’2)/Ïµ)sample complexity with two clear drawbacks: (1) its large sample
complexity bound and (2) its decision to accept or reject is explained as â€˜in the majority of the runs
of the tester it decided soâ€™. The algorithm we present in this work improves on both aspects: it has
a sample complexity of ËœO
maxn
k4/5
Î±6/5,k1/2
Î±2,k2/3
Î±Ïµ1/3,k1/3
Î±4/3Ïµ2/3,âˆš
k
Î±Ïµo
and it is capable of producing
numerical estimators directly from the data that explain its accept/reject decision.
Our Algorithm. The difference that makes learning substantially more difficult from equivalence
testing is that when having two distributions we are capable of â€œpitting one against the otherâ€: roughly
speaking, we can partition the real line into intervals based on points from one distribution and see
whether this partition acts as a random partition for the batch of examples that come from the second
distribution. This involves only sorting allspoints from our sample on the real line and dealing
with the order statistics. Our algorithm is based on privatizing the equivalence tester of [ 16], which
operates over the continuous real line. This algorithm works in two stages: In the first stage, it looks
at autocorrelation statistics that involve all (sâˆ’1)pairs of adjacent (post-sorting) data points. In
the second stage, it equipartitions the data using into mbins using a random draw of mpoints and
repeatedly runs the following operation: run a closeness L2-norm based tester on the discretized
m-bins distribution that draws Nnew points, and should it not reject â€“ merge every pair of adjacent
bins to create a m/2-bins discretization and continue.
Our DP-version of this tester also works in similar stages. The first stage is almost trivially privatized
(the statistics estimated in the first stage exhibits small global sensitivity) but numerous difficulties
arise in the privatization of the second stage. Most notably â€” the fact we use datapoints to define
a partition into bins. To that end, we replace it so that we use sorted-indices, implying that a bin is
composed of all datapoints in sorted indices from index Ï€ito index Ï€i+1(not including). This asserts
that any single bin changes by at most two datapoints between neighboring samples. Combining
this with the fact that under the null-hypothesis the difference between the number of points from
PandQin each bin is proportional to the square-root of the bin size, we can bound the sensitivity
in a Propose-Test-Release fashion [ 20] for any single bin. However, the approach of fixing sorted
indices raises two concerns. The first is that we have to revisit the utility analysis of [ 16] because
we no longer use re-sample new points to estimate the L2-norm difference of the two discretized
distributions but rather re-visit the same dataset from which the bin-defining datapoints are taken. We
bypass this difficulty by sampling ourselves Poisson-drawn subsample of each bin, and by focusing
our analysis on a single iteration that should cause us to reject.
The second concern lies in the privacy analysis, and it is the observation that a change to a single
bin isnâ€™t enough to bound the sensitivity of our statistical estimator. The algorithm computes an
L2-approximation of the norm-difference using all bins, and in the extreme case of two neighboring
datasets that differ on the very first (sorted) datapoint that ends up as the very last datapoint in the
neighboring dataset we have that allbins shift. To that end we use the following novel idea: we add a
Ber(1/2)-random variable to each of the bin-defining indices. On the one hand, this possible shift
by 1 changes very little in terms of the analysis; on the other hand, it allows us to argue that within
log4(1/Î´)random shifts we can correlate these Bernoulli random variables so that the bins identify.
Full details of our algorithm and its analysis appear in Section 3.
Applications. Having designed our private equivalence tester for continuous distributions under
Ak-norm, we can now apply it to test equivalence between two distributions from a family Cof
distributions which, under suitable partition, yield large enough Ak-distance. The following fact is
immediate.
2Fact 1. Given a univariate distribution family Cand Î±, let k=k(C, Î±)be
the smallest integer such that for any f1, f2âˆˆ C it holds that âˆ¥f1âˆ’f2âˆ¥1â‰¤
âˆ¥f1âˆ’f2âˆ¥Ak+Î±/2. Then there exists a (Ïµ, Î´)-DP equivalence testing algorithm for Cusing
ËœO 
max
k4/5/Î±6/5,k2/3/Î±Ïµ1/3,k1/2/Î±2,k1/3/Î±4/3Ïµ2/3,âˆš
k/Î±Ïµ	
samples.
In Section 4 we detail, much like [ 16], a variety of such hypothesis-families and the sample complex-
ities we obtain for their respective DP-equivalence testers.
1.1 Related Work
Over the past twenty years, significant progress has been made in distribution property testing.
Initially, research focused on the sample sizes required to assess properties of arbitrary distributions
with a specific support size. Goldreich and Ron [ 23] introduced uniformity testing, proposing an
algorithm using collision statistics with a sample complexity ofâˆšn
Î±4. Batu et al [ 5] studied closeness
testing, evaluating whether two distributions are close in total variation. Paninski [ 27] established
the first lower bound for the sample complexity of uniformity testing at â„¦(âˆšn
Î±2)and proposed a new
uniformity test using unique element statistics. The works of [ 1,10,29] explored optimal bounds for
identity testing, focusing on Ï‡2-based testers. They established a lower bound for closeness testing at
O
n2/3
Î±4/3+âˆšn
Î±2
, with [ 10] providing matching upper bounds for L2closeness testing under certain
conditions. [ 14]. introduced a technique reducing distribution norms to O 1
n
by expanding the
domain size, demonstrating an efficient L2tester for closeness testing. Recent works [ 11,15,16] have
leveraged structural assumptions for more efficient testers in various settings, including continuous
cases. These studies used the Akmetric, aligning with the Kolmogorov distance for k= 2 and
total variation distance for kbeing the domain size. Goldreich [ 22] showed that identity tests could
be reduced to uniformity testing. While earlier testers relied on proxy measures like L2andÏ‡2,
and [ 12,13] demonstrated efficient sample complexity using direct L1metric testers due to low
sensitivity. We refer the interested reader to Cannoneâ€™s excellent survey [9].
Several recent papers have examined hypothesis testing problems under the framework of differ-
ential privacy. Cai et al [ 8] used Ï‡2statistic for identity testing, achieving a sample complexity
ofËœO
maxnâˆšn
Î±2,âˆšn
Î±3/2Ïµ,n1/3
Î±5/3Ïµ2/3o
Â·log(1/Î²)
. Aliakbarpour et al [ 4] proposed three algorithms
for differentially private uniformity and closeness testing. They privatized unique element algo-
rithms, collision-based testers, and Ï‡2tests, each with specific sample complexities and limitations.
Acharya et al [ 2] established a lower bound for private identity testing, suggesting that small expected
Hamming distances might compromise privacy guarantees. They proposed a sample complexity con-
cerning privacy and distance parameters by privatizing an L1statistic-based algorithm. Aliakbarpour
et al [ 3] also examined closeness testing between distributions with unequal sample sizes, introducing
a new technique to privatize the â€˜flatteningâ€™ method through data permutation. Zhang [ 30] derived an
upper bound for closeness testing by privatizing an empirical total variation method, demonstrating
small sensitivity and sample complexity of Oâˆšn
Î±2+n2/3
Î±4/3+âˆšn
Î±âˆšÏµ+n1/3
Î±4/3Ïµ2/3+1
Î±Ïµ
.
Comparison to Existing Lower Bounds. The well-known lower bound for Akcloseness testing in
the non-private regime is given in [ 16]. It is equal to O
k4/5
Î±6/5+âˆš
k
Î±2
. As far as we know, there is no
known lower bound for the private regime in that task. In [ 2], a lower bound for the private regime in
identity testing is presented as Oâˆšn
Î±2+n2/3
Î±4/3+âˆšn
Î±âˆšÏµ+n1/3
Î±4/3Ïµ2/3+1
Î±Ïµ
(where n=kis the domain
size), which is a simpler task than closeness testing. Additionally, [ 30] provided an upper bound equal
to the lower bound of the closeness testing also in the private regime when he crucially relies on the L1
tester that is known with low sensitivity. We have established an upper bound that is asymptotically
close to the lower bound, given by ËœO 
max
k4/5/Î±6/5,k2/3/Î±Ïµ1/3,k1/2/Î±2,k1/3/Î±4/3Ïµ2/3,âˆš
k/Î±Ïµ	
. The
difference between our upper bound and the lower bound lies in two specific terms:âˆš
k
Î±Ïµandk2/3
Î±Ïµ1/3
The first term is a result of the high sensitivity in our algorithm due to the use of L2norm testing,
which was selected to ensure the utility proof. Additionally, we did not focus on optimizing the term
log k
Î±ÏµÎ´
.
32 Preliminaries
Equivalence (Closeness) Testing. We assume oracle access to two distributions P,Qwhich gives
an i.i.d. example from the resp. distribution. We also use P(S)(resp.Q(S)) to denote the total
probability mass assigned by P(resp.Q) to a set S. We assume the two distributions are continuous
with no discrete point mass, which can always be obtained by concatenating each sample with a
uniformly drawn number âˆˆR[0,1]. An equivalence tester between the two distributions should return
NULL w.p.â‰¥2/3if it holds that P=Qand return ALT w.p.â‰¥2/3if it holds that âˆ¥P âˆ’ Qâˆ¥ Akâ‰¥Î±.
Differential Privacy. Two databases DandDâ€²are considered neighboring databases if they differ
by exactly one record, noted as d(D, Dâ€²) = 1 , where d(Â·,Â·)represents the Hamming distance. Given
a domain X, two multi-sets S, Sâ€²âŠ‚ X are called neighbors if they differ on a single entry. An
algorithm (alternatively, mechanism) Mis said to be (Ïµ, Î´)-differentially private (DP) [ 21,19] if
for any two neighboring S, Sâ€²and any set Oof possible outputs we have: Pr[M(S)âˆˆO]â‰¤
eÏµPr[M(Sâ€²)âˆˆO] +Î´. IfÎ´= 0then we say the algorithm MisÏµ-DP.
The Global Sensitivity of a function f:X â†’ Rdis defined as the maximal difference
max S,Sâ€²âˆ¥f(S)âˆ’f(Sâ€²)âˆ¥1. It is known that adding Lap(GS(f)/Ïµ)to each coordinate of f(S)isÏµ-DP;
where Lap(Î»)denotes the Laplace Distribution with parameter Î», whose PDF isPDF(x)âˆeâˆ’|x|/Î».
It is known [ 19] that if A1andA2are(Ïµ1, Î´1)-DP and (Ïµ2, Î´2)-DP resp., then their composition is
(Ïµ1+Ïµ2, Î´1+Î´2)-DP. It is also known [ 18] that the k-fold composition of kalgorithms, each is (Ïµ, Î´)-
DP is (Ïµâˆ—, kÎ´+Î´â€²)-DP for any Î´â€²>0andÏµâˆ—=kÏµ(eÏµâˆ’1)+2 Ïµp
klog(1/Î´â€²). Lastly, it is also known
that if there exists an event Esuch that under Eholding, algorithm Msatisfies that for any two neigh-
boring SandSâ€²and any set of outputs Owe have that Pr[M(S)âˆˆO| E]â‰¤eÏµPr[M(Sâ€²)âˆˆO| E]+Î´
then algorithm Mis(Ïµ, Î´+ Pr[Â¬E])-DP.
Poisson Distribution. The Poisson distribution Poi(Î»)is a discrete distribution over the Naturals
which satisfies Pr[k] =eâˆ’Î»Î»k/k!. It has multiple properties that make it easy to work with.
Proposition 2. â€¢The sum of two ind. Poisson Poi (Î»1)and Poi (Î»2)is Poi (Î»1+Î»2).
â€¢LetX1, X2, ...be i.i.d. Bernoulli r.v.s. with parameter p; then drawing tâˆ¼Poi(Î»), it holds
thatPt
i=1Xiis distributed like Poi (Î»p).
â€¢IfXâˆ¼Poi(Î»x)andYâˆ¼Poi(Î»y)are two ind. Poisson r.v.s, then the distribution of X
conditioned on the event that X+Y=nis Binomial Bin (n,Î»x
Î»x+Î»y).
â€¢IfXiâˆ¼Poi(Î»)thenPr[|Xâˆ’Î»|> k]â‰¤2 exp
âˆ’k2
2(Î»+k)
. So for any Î² > 0, setting
k= 2p
Î»log(2/Î²)we get Pr[|Xâˆ’Î»|>2p
Î»log(2/Î²)]â‰¤Î²provided Î» >4 log( 2/Î²).
Misc. We use ËœO(resp. Ëœâ„¦) to denote big- O(resp. big- â„¦) up to poly-log factors. We made no effort to
minimize constants or the degree of the poly-log.
3 Private Equivalence Testing for Continuous Distributions
In this section, we present our DP-tester for equivalence between two distributions with large Ak
distance and give both its formal privacy guarantee and its sample complexity bounds. The tester is
detailed in Algorithm 1 (which in turn invokes Algorithm 2), where cdkndenotes a large enough
constant detailed in [16].
The algorithm consists of two parts. In terms of the non-private algorithm, both parts function
similarly to the algorithm presented by [ 16]. Our main contribution is the privatization of this
algorithm. In the first part the main objective is to compute the estimator Z(line I.5) which is
privatized using a straightforward approach â€“ since it has low sensitivity we merely add to is some
Laplace noise. The second part is more complex, as it involves discretizing the domain based on the
data itself, resulting in high sensitivity. We addressed this issue by creating the initial partition of
the domain and adding a Bernoulli random variable to each index position (line II.5). Consequently,
the algorithm can iteratively run j0iterations of the L2-tester TestCloseness on based on the
randomized partition, where in each invocation of TestCloseness we sample a Poisson-size batch
4from each partition (line II.11). If the L2-tester doesnâ€™t reject, then we merge adjacent partition
cells (line II.15) and move on to the next iteration. If either invocations of TestCloseness (or the
estimator Zhas too high of a value) then we reject; but if all test pass we return NULL .
Algorithm 1 Private Equivalence Tester
Input: 2 continuous distributions P,Q, distance parameter Î±, privacy parameter Ïµ, Î´.
Output: â€œNULL " ifP=Q; â€œALT" ifâˆ¥P âˆ’ Qâˆ¥ Akâ‰¥Î±
1:Part I:
1. Set mâ†100cdkn
k4
5/Î±6
5+k2
3/Î±Ïµ1
3
2. Draw sâˆ¼Poi(m)points from1
2(P+Q).
3.Label each xidrawn from Pwithâ„“(xi) = 1 and label each xjdrawn from Qasâ„“(xj) =âˆ’1.
4. Sort the sample, denote the outcome as x(1)< x(2)<Â·Â·Â·< x(s).
5.Z=Psâˆ’1
i=1â„“(x(i))â„“(x(i+1))
6. Draw Xâˆ¼Lap(6/Ïµ).
7.ËœZ=Z+X
8.ifËœZ >m3Î±3
2k2then return ALT
2:Part II:
1. Set Nâ†107
k1/3
Î±4/3Ïµ2/3+âˆš
k
Î±Ïµ+âˆš
k
Î±2
log6(k/Î±ÏµÎ´)and assert mdivides N.
2. Draw spâˆ¼Poi N
2
andsqâˆ¼Poi N
2
ind. Set sâ†sp+sq.
3.Sâ†a sample of spi.i.d. examples from Pandsqi.i.d. examples from Q.
4. Sort S. Denote the outcome as x(1)< x(2)â‰¤ Â·Â·Â· < x(s).
5.For each 1â‰¤iâ‰¤msetÏ€iâ†iÂ·N
m+Bifor ind. drawn Biâˆ¼Ber(1/2). Also set
Ï€0â†0, Ï€m+1â†s+ 1.
6.Form the Partition Â¯Î 0={Î 0
1,Î 0
2, ...,Î 0
m}where Î 0
i={x(iâ€²):Ï€i< iâ€²< Ï€i+1}for every
1â‰¤iâ‰¤m.
7. Set j0â†1 +âŒˆlog (m/k)âŒ‰andm0â†m.
8.forj= 0,1, . . . , j 0âˆ’1do:
9. fori= 1,2, . . . , mjdo:
10. Draw NJ
iâˆ¼Poi(N
4mj).
11. Pick a u.a.r. subset CJ
iofNj
ipoints from Î j
i.(IfNj
i>|Î j
i|then use special âŠ¥points.)
12. Set Xj
i(resp. Yj
i) as #points from P(resp.Q) inCj
i.
13. ifTestClosenessP
iNj
i, mj,âŸ¨Xj
iâŸ©,âŸ¨Yj
iâŸ©,Î±
12âˆš2k+1Â·log(1/Î±),Ïµ
8âˆš
j0log(2/Î´),Î´
2j0, Î´
=ALTthen
14. return ALT
15. Merge cells: Set mj+1â† âŒŠmj/2âŒ‹, and for each 1â‰¤iâ‰¤mj+1set:
Î j+1
iâ†Î j
2iâˆ’1âˆª {x(Ï€iâ€²)} âˆªÎ j
2i â–· iâ€²=the separating index of the two bins
3:return NULL
3.1 Privacy Proof
In this subsection, our goal is to proof the following theorem.
Theorem 3. Algorithm 1 is (2Ïµ,2Î´)-DP .
The proof of Theorem 3 shows that Part I of Algorithm 1 is Ïµ-DP â€” which is very straight forward,
whereas Part II of the algorithm is (Ïµ,2Î´)-DP â€” which involves more intricate arguments. In fact, all
that is required regarding the DP of Part I is the following claim. Its proof, as well as most proofs in
this section, is deferred to Appendix A.
Claim 4. The estimator Z=Ps
i=1â„“(x(i))â„“(x(i+1))in Part I of Algorithm 1 (Line 5) has global
senstivity of 6.
5Algorithm 2 TestCloseness -(N, m, Â¯X,Â¯Y , Î±, Ïµâ€², Î´â€², Î´)
1:Setnmaxâ†max i{|Xiâˆ’Yi|}.
2:SetË†nmaxâ†nmax+Lap(8/Ïµ)
3:SetZâ†P
i(Xiâˆ’Yi)2âˆ’Xiâˆ’Yi=P
i|Xiâˆ’Yi|2âˆ’(Xi+Yi).
4:Setnzâ†q
6N
mlog (800 N) +16 ln(1 /Î´â€²)
Ïµâ€² + 1
5:SetË†Zâ†Z+Lap(16 log 4/3(2/Î´)nz/Ïµâ€²)
6:if(Ë†nmax<q
6N
mlog (800 N) +8 ln(1 /Î´â€²)
Ïµâ€² andË†Z <1
2Î±2N2)then return NULL
7:else return ALT
We thus focus for now on Part II of Algorithm 1. In Part II our output is the (2j0)-long tuple
âŸ¨Ë†nj
max, ZjâŸ©j0âˆ’1
j=0which we may return from the j0invocations of TestCloseness . Thus, we denote
Ïµâ€²=Ïµ
8âˆš
j0log(2/Î´),Î´â€²=Î´
2j0and note that each invocation of TestCloseness is with these parameters.
Throughout our analysis of Part II we assume that the Poisson random variables are known to us, but
leave the Bernoulli and the Laplace random variables unknown.
Each invocation of TestCloseness releases two statistics: an approximation of nmaxand approxi-
mation of Z. The next two claims bounds their sensitivity (w.h.p.).
Claim 5. The Global Sensitivity of nmaxis at most 4.
Lemma 6. There exists an event Ewhere Pr[Â¬E]<3Î´/2, and under Eit holds that the sensitivity of
Zat any iteration jis at most 8 log4/3(2/Î´)q
6N
mlog (800) +16 ln(1 /Î´â€²)
Ïµâ€² + 1
.
Proof. Denote E0as the event that there exists an invocation of TestCloseness withmballs and
nbins in which nmaxâ‰¥q
6m
nlog (800 m) +16 ln(1 /Î´â€²)
Ïµâ€² , yetË†nmax<q
6m
nlog (800 m) +8 ln(1 /Î´â€²)
Ïµâ€².
It follows that there must exists an invocation of TestCloseness in which the noise added to nmax
(drawn from Lap(8/Ïµâ€²)) must be smaller than âˆ’8 ln(1 /Î´â€²)
Ïµâ€². This holds w.p. < Î´â€²/2, and from the
Union Bound it follows that Pr[E0] =j0Â·Î´â€²
2=Î´
2.
We now turn to the Bernoulli random variables. Fix SandSâ€²to be two neighboring inputs, and again,
we assume that the changed point appears in place (1)inSand in place (s)inSâ€². (Otherwise, our
analysis is only simpler.) It follows that the changed point falls in Î 0
1inSand in Î 0
minSâ€².
We now specify the coupling of the Bernoulli random variables between SandSâ€², which is the
following. We draw B1,Bâ€²
1.B2,Bâ€²
2and so on, u.a.r. and independently, and apply Bi-s to the
invocation on SandBâ€²
i-s to the invocation on Sâ€²,until the first occurrence of Bi= 0, Bâ€²
i= 1from
which we give Bi+1,Bi+2and so on to both invocation. This is depicted in Figure 1.
Denote Ejas the event that the first occurrence of Bi= 0, Bâ€²
i= 1 is at the j-th draw. Clearly
Pr[Ej] = ( 3/4)jâˆ’1Â·1/4. Thus, from the disjointness of events, it follows that PrhSlog4/3(2/Î´)
j=1 Eji
=
1
4Plog4/3(2/Î´)
j=1 (3/4)jâˆ’1=1
41âˆ’(3/4)log4/3(2/Î´)
1âˆ’3/4= 1âˆ’Î´/2. Symmetrically, we apply the same coupling
Figure 1: Two neighboring inputs that differ on one datapoint, appearing first in Sand last in Sâ€².
In this example, the index defining the first bin, Ï€1, is such that for Swe go B1= 0and for Sâ€²we
haveBâ€²
1= 0; but the index defining the 2nd bin, Ï€2it does hold that B2= 0, Bâ€²
2= 1so the indices
starting from bin 2 onwards align.
6to align the last bins, those affected by x(s), in the last position. We use the coupling that provides
Bm, Bâ€²
m, Bmâˆ’1, Bâ€²
m1, Bmâˆ’2, Bâ€²
mâˆ’2and so on until the first occurrence of (1,0)then it switches to
the same variable Bj. Denoting Eâ€²
jas the event that the first occurrence of Bi= 1, Bâ€²
i= 0is at the
mâˆ’j-th draw, we have that PrhSlog4/3(2/Î´)
j=1 Eâ€²
ji
= 1âˆ’Î´/2.
We thus denote E=E0âˆªlog4/3(2/Î´)S
j=1(Ejâˆª Eâ€²
j)and have that Pr[Â¬E] = 3Î´/2. Note, under Eit follows
that at most 2 log4/3(2/Î´)bins have a change in their |Xiâˆ’Yi|-value, which â€“ due to Claim 5 â€“ is at
most 4. (Observe that in the case where SandSâ€²are such that there are fewer than 2 log4/3(2/Î´)
bins between the locations of the changed point then this statement holds w.p. 1.)
It follows that under Ewe have that the value of Zis affected by at most 2 log4/3(2/Î´)bins,
where for each bin |Xiâˆ’Yi|changes by at most 4. It follows that Zcan change by at most
2 log4/3(2/Î´)Â·4Â·(nmax+ 1)â‰¤8 log4/3(2/Î´)q
6N
mlog (800 N) +16 ln(1 /Î´â€²)
Ïµâ€² + 1
.
Completing the Proof of Theorem 3 is simple, and is deferred to Appendix A.
3.2 Utility Proof
In this subsection, our goal is to proof the following theorems.
Theorem 7. W.p.â‰¥2/3, Algorithm 1 returns NULL whenP=Q.
Theorem 8. W.p.â‰¥2/3, Algorithm 1 returns ALT whenâˆ¥P âˆ’ Qâˆ¥ Akâ‰¥Î±.
The proof of Theorem 7 is fairly simple, but the proof of Theorem 8 requires some preliminaries.
In fact, in order to argue the correctness of theorems we need to argue that both Parts I & II of the
algorithm are correct w.p. â‰¥5/6. Proving that Part I is correct is very straight-forward due to claims
from [ 16]. And so we focus on the correctness of Part II. Its correctness requires that we first assert a
few rudimentary propositions and claims.
3.2.1 Rudimentary Claims
Similarly to the analysis in [ 16] our goal is to compare the two continuous distributions to their
resp. discretizations that were forms under the various Â¯Î j. However, we do not have the luxury
of resampling the points from PandQ, and so our argument diverges from theirs as we fix one
particular jâˆ—and then argue from first principles that under large Ak-difference the specific partition
Â¯Î jâˆ—cause us to reject. (Arguing that when P=Qwe return nullis also fairly straight-forward.) Our
intermediate goal is to argue that the points from P(theX-s) and the points from Q(theY-s) are
distributed like independent Poisson random variables. Thus, for the remainder of the discussion we
fix some particular iteration jand examine solely it, without considering the previous iterations.
Due to space constraint, we defer nearly all the claims in this section to Appendix B, but they all lead
to towards the following main lemma.
Lemma 9. Fix index j. For each index idenote by Iias the interval (x(Ï€j
i), x(Ï€j
i+1))which is the
interval defining bin iin the partition Â¯Î j. Then the estimator Zcomputed by TestCloseness satisfies
thatE[Z] =Pmj
i=1(piâˆ’qi)2where
pi=NP(Ii)
4m(P(Ii) +Q(Ii)), q i=NQ(Ii)
4m(P(Ii) +Q(Ii)).
and that Var [Z] =Pmj
i=14(pi+qi)(piâˆ’qi)2+ 2(pi+qi)2.
In Lemma 9 we established the expectation and variance of our estimator using the notation piwhich
in turn is defined asNP(I)
4m(P(I)+Q(I))(andqisimilarly). The following claim is going to assist us in
bounding the denominators of piandqi.
Claim 10. IfN > 3000mlog(2m), then with a probability of 1âˆ’1
m, any Ithat forms a bin Î 0
iin
Î 0, has that1
1.01mâ‰¤P(I)+Q(I)
2â‰¤1.01
m.
7Unfortunately, due to space constraints, we defer proof of Claim 10 as well as the entire proof of
Theorem 7 to Appendix B.
3.2.2 Proof of Theorem 8
We now turn our attention to proving Theorem 8, however, we need one more technical lemma, whose
proof â€“ like all proofs in this section â€“ is deferred to Appendix B. Then we can proof Theorem 8.
Lemma 11. Fixpâˆˆ(0,1). Suppose there exists nnon-negative random variables X1, X2, . . . , X n,
such that for each iit holds that for some fixed number aiwe have Pr[Xiâ‰¥ai]â‰¥p. Then, given a
constant c >0there exists another constant C >0such that with a probability at least Cit holds
thatPn
i=1Xiâ‰¥cPn
i=1ai.
Proof of Theorem 8. In the alternative case, where âˆ¥P âˆ’ Qâˆ¥ Akâ‰¥Î±, we know that there exists
kintervals Iâˆˆ I such thatP
IâˆˆI|P(I)âˆ’ Q(I)| â‰¥Î±. We denote for each interval Î³(I) =
|P(I)âˆ’ Q(I)|. For the sake of analyze we use two different kinds of interval, small and large.
Definition 12. Interval Iâˆˆ Iis called small if there exists a subinterval JâŠ†Isuch that P(J) +
Q(J)<1/mand|P(J)âˆ’ Q(J)| â‰¥Î³(I)/10, and large otherwise.
Note that Î±â‰¤ âˆ¥P âˆ’ Qâˆ¥ Ak=P
IâˆˆI|P(I)âˆ’ Q(I)|=P
IâˆˆI, IsmallÎ³(I) +P
IâˆˆI, IlargeÎ³(I), so
half of the discrepancy comes from either small or large intervals. We consider first the case where
half of the discrepancy comes from small intervals. In this case, Lemma 9 in [ 14] states that the
expectation of statistic Zin Line 5 of Part I is bounded by E[Z]â‰¥cN3Î±3
k2for some constant c >0.
Just like we did in the soundness case, Proposition 21 gives that the variance Var[Z]â‰¤9m. Therefore
for some large constant cdkn, setting m= 100 cdkn
k4
5
Î±6
5+k2
3
Î±Ïµ1
3
then by Chebyshevâ€™s inequality
we get that with probability5
6it holds that ËœZâ‰¥E[Z]âˆ’10âˆšmâˆ’ |X| â‰¥m3Î±3
2k2(with Xbeing the
random noise sampled in Line 6 of Part I of the algorithm).
Now consider the case whereP
IâˆˆI, Iis large|P(I)âˆ’Q(I)| â‰¥Î±
2. We prove that Part II of Algorithm 1
must return ALT on at least one invocation of TestCloseness . In order to do this, we first need to
show that the discretization of the domain with msamples preserves most of the Ak-distance. Take
any interval Iâˆˆ Ithat gives the Akdistance, and denote I= [a, b]as its boundaries.
Now, consider the total probability mass between aandÏ€i, then the first datapoint selected for the
partition that is greater than a. Since the total number of points is taken from a Poisson distribution,
it is known that the mass from one point to the next has Exponential distribution Exp(N)[28]; and
so the total mass from atoÏ€is distributed like the sum of Exponential random variables, namely, a
Gamma-distribution with mean â‰¤N
mÂ·1
N=1
m, and variance â‰¤N
mÂ·(1
N)2=1
mN<1
3000m2log(m). It
follows that w.p. â‰¤0.01we have that1
2(P[a, Ï€i]+Q[a, Ï€i])>1.01
m. A similar analysis shows that for
Ï€j, the last datapoint selected for the partition before b, we also have P([Ï€j, b]) +Q([Ï€j, b])<2.02
m.1
Note that a large interval must have a total probability mass P(I) +Q(I)â‰¥10
mand so w.p. â‰¥0.98
we have formed the subinterval Iâ€²= [Ï€i, Ï€j]âŠ‚I. Moreover, by the subinterval property of
large intervals, we have that because P([a, Ï€i]) +Q([a, Ï€i]) +P([Ï€j, b]) +Q([Ï€j, b])<4.04
mthen
Î³(Iâ€²)â‰¥Î³(I)âˆ’5Î³(I)
10=Î³(I)/2.
Therefore, denote NIas the indicator of the event that |P(Iâ€²)âˆ’Q(Iâ€²)| â‰¥Î³(I)
2, and we denote PÎ 0and
QÎ 0as the reduced discretized distribution formed by the partition point. We show it preserves most of
the total variation âˆ¥PÎ 0âˆ’ QÎ 0âˆ¥1=P
Iâ€²|P(Iâ€²)âˆ’ Q(Iâ€²)| â‰¥P
IâˆˆINIÎ³(I)
2â‰¥P
IâˆˆI, IlargeNIÎ³(I)
2.
We have established that if Iis large interval then Pr[NI]â‰¥0.98, and so, for the random variables,
NIÎ³(I)
2
-s (one for each large interval) where for each variable with probability 0.98it holds
thatNIÎ³(I)
2â‰¥Î³(I)
2, we apply Lemma 11 with c= 0.36and have that there for the constant
1ifa=âˆ’âˆ orb=âˆour analysis is even simpler, as we take Ï€0andÏ€m+1as the respective partition
point.
8C= 1âˆ’0.98(1âˆ’0.98)
0.982(1âˆ’0.36)2>0.95such that with probability C >0.95
X
IâˆˆI, IlargeNIÎ³(I)
2â‰¥0.36X
IâˆˆI, IlargeÎ³(I)
2>Î±
12
Therefore, with probability >0.95,âˆ¥PÎ 0âˆ’ QÎ 0âˆ¥1â‰¥Î±
12.
Observe that the TV-distance follows from finding suitable subintervals Iâ€²inside large intervals with
large discrepancy. Thus, each of the â‰¤klarge intervals now gives (at most) 2points that form Iâ€², so
theseâ‰¤2kpoints partition the real line to â‰¤2k+ 1intervals where the various Iâ€²-s are part of this
partition. It follows that âˆ¥PÎ 0âˆ’ QÎ 0âˆ¥A2k+1â‰¥Î±
12.
Now, to complete the proof, we need to show that Part I of the algorithm detects the discrepancy. We
deploy the following lemma from [ 16] where for a vector vthe notation âˆ¥vâˆ¥1,krefers to the sum of
the largest kbins/coordinates of v.
Lemma 13. For any two distributions PandQon[m]such that âˆ¥P âˆ’ Qâˆ¥ Ak> Î±, there iteration
jâˆˆ[log( m/k)]such that âˆ¥PÎ jâˆ’ QÎ jâˆ¥1,k> Î±/ log(m/k).
So since âˆ¥PÎ 0âˆ’ QÎ 0âˆ¥A2k+1â‰¥Î±
12, we know that by Lemma 13, there exists some jâˆ—âˆˆ[log(m/k)]
such thatPÎ jâˆ—
âˆ’ QÎ jâˆ—
1,2k+1â‰¥Î±
12 log( m/k)and therefore by Cauchyâ€“Schwarz inequality
PÎ jâˆ—
âˆ’ QÎ jâˆ—
2â‰¥Î±
12âˆš2k+1 log( m/k). We know thatPÎ jâˆ—
âˆ’ QÎ jâˆ—
1,2k+1â‰¥Î±
12 log( m/k). De-
note the set of indices of these 2k+ 1intervals as S, we get from Lemma 9 that
E[Z] =mjP
i=1(piâˆ’qi)2â‰¥P
iâ€²âˆˆS(piâ€²âˆ’qiâ€²)2Cau.Sch.
â‰¥(P
iâ€²âˆˆS|piâ€²âˆ’qiâ€²|)2
2k+1â‰¥1
2k+1P
iâ€²âˆˆSN|P(I)âˆ’Q(I)|
4mj|P(I)+Q(I)|2
.
Also from Lemma 9 we can infer that
Pr
|Zâˆ’E[Z]| â‰¤1
4E[Z]
â‰¤16Var[Z]
E[Z]2â‰¤Pmj
i=164(pi+qi)(piâˆ’qi)2+ 32( pi+qi)2
(Pmj
i=1(piâˆ’qi)2)2
=Pmj
i=164(N(P(I)+Q(I)
4mj(P(I)+Q(I))(piâˆ’qi)2+ 32(N(P(I)+Q(I)
4mj(P(I)+Q(I))2
(Pmj
i=1(piâˆ’qi)2)2=2N2/mj
(Pmj
i=1(piâˆ’qi)2)2+16N/mj
Pmj
i=1(piâˆ’qi)2
mjâ‰¥k
â‰¤6Â·5882(2k+ 1) log2(m/k)
N2Î±4+28224 log( m/k)
NÎ±2<0.01
when Nâ‰¥106âˆš
klog2(m/k)
Î±2 . Now we assert that the noise we added is also proportional to at most
1
4E[Z], and indeed if we draw a random variable Râˆ¼Lap
âˆ†(Z)
Ïµâ€²
withâˆ†(Z)as defined in Line
5 ofTestCloseness , then we get Prh
Râ‰¥N2Î±2
2500(2 k+1) log( m/k)i
â‰¤exp
N2Î±2Ïµâ€²
2500(2 k+1)âˆ†( Z)
â‰¤0.01.
which holds if
N > 106p
kâˆ†(Z)
Î±âˆš
Ïµâ€²â‰¥106
Î±âˆš
Ïµâ€²vuutkÂ·16 log 4/3(2/Î´) max(r
N
2klog (10 N),16 ln(1 /Î´â€²)
Ïµâ€²)
so we get N=Ëœâ„¦
k1/3
Î±4/3Ïµ2/3+âˆš
k
Î±Ïµ
. Combining it all together, we have that w.p. â‰¥5/6Part II of
the algorithm also returns ALT.
4 Applications
Our algorithm is designed for continuous distributions, but it can also be used for discrete distributions.
The process is simple: for a given discrete distribution P, for each example xjâˆ¼ P we draw a
number ijâˆˆR[0,1], and then sort all the examples âŸ¨(xj, ij)âŸ©m
j=1using lexicographic order. This
process gives a simple privatization of the "Flattening method" proposed by Diakonikolas et al [ 14],
9as it does so without looking at the example drawn. Our algorithm method is quite simple: draw m
samples from1
2(P+Q), then calculate the autocorrelation to identify discrepancies within small
intervals. Next, use 2 min( m, n)for flatting to test for total variation distance. Itâ€™s important to
remember that if n=k, where nis the size of the domain, then Akdistance is equal to L1. However,
the sample complexity of the first part, which involves finding discrepancies in small intervals using
the analysis of [ 16], requiresn4/5
Î±6/5. Fact 1 indicates however that the size of the domain is not always
the most suitable parameter for distribution testing. Having knowledge about the structure of the
distribution enables more efficient testing, as we illustrate below. In Table 1, we give a brief summary
of the various statistical inference tasks that can be conducted using our algorithm.
Table 1: Private equivalence testers derived from our algorithm for continuous distributions
Distrib.
FamilyNum of
IntervalsPrivate upper bound
t-
piecewise
constantt ËœO
maxn
t4/5
Î±6/5,t2/3
Î±Ïµ1/3,t1/2
Î±2,t1/3
Î±4/3Ïµ2/3,âˆš
t
Î±Ïµo
t-
piecewise
degree- dt(d+ 1) ËœO
max
(t(d+1))4/5
Î±6/5,(t(d+1))2/3
Î±Ïµ1/3,(t(d+1))1/2
Î±2,(t(d+1))1/3
Î±4/3Ïµ2/3,âˆš
(t(d+1))
Î±Ïµ
log-
concave1âˆšÎ±ËœO
maxn
1
Î±9/5,1
Î±4/3Ïµ1/3,1
Î±3Ïµ2/3,1
Î±5/4Ïµo
k-
mixture
of log-
concavekâˆšÎ±ËœO
maxn
t4/5
Î±8/5,k2/3
Î±4/3Ïµ1/3,k1/2
Î±9/5,k1/3
Î±3Ïµ2/3,âˆš
k
Î±5/4Ïµo
t-model
over[n]tlog(n)
Î±ËœO
max
(tlog(n))4/5
Î±2 ,(tlog(n))2/3
Î±5/2Ïµ1/3,(tlog(n))1/2
Î±5/2,(tlog(n))1/3
Î±5/3Ïµ2/3,âˆš
tlog(n)
Î±3/2Ïµ
MHR
over[n]log(n/Î±)
Î±ËœO
max
(log(n/Î±))4/5
Î±2 ,log(n/Î±)2/3
Î±5/3Ïµ1/3,(log(n/Î±))1/2
Î±5/2,(log(n/Î±))1/3
Î±5/3Ïµ2/3,âˆš
log(n/Î±)
Î±3/2Ïµ
Acknowledgments and Disclosure of Funding
O.S. is supported by the BIU Center for Research in Applied Cryptography and Cyber Security in
conjunction with the Israel National Cyber Bureau in the Prime Ministerâ€™s Office, and by ISF grant no.
2559/20. Both authors thank the anonymous reviewers for their suggestions and advice on improving
this paper.
10References
[1]Jayadev Acharya, Constantinos Daskalakis, and Gautam Kamath. Optimal testing for properties
of distributions. Advances in Neural Information Processing Systems , 28, 2015.
[2]Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private testing of identity and
closeness of discrete distributions. Advances in Neural Information Processing Systems , 31,
2018.
[3]Maryam Aliakbarpour, Ilias Diakonikolas, Daniel Kane, and Ronitt Rubinfeld. Private testing
of distributions via sample permutations. Advances in Neural Information Processing Systems ,
32, 2019.
[4]Maryam Aliakbarpour, Ilias Diakonikolas, and Ronitt Rubinfeld. Differentially private identity
and closeness testing of discrete distributions. CoRR , abs/1707.05497, 2017.
[5]Tugkan Batu, Lance Fortnow, Ronitt Rubinfeld, Warren D Smith, and Patrick White. Testing
that distributions are close. In Proceedings 41st Annual Symposium on Foundations of Computer
Science , pages 259â€“269. IEEE, 2000.
[6]Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of
private learners. In ITCS , pages 97â€“110. ACM, 2013.
[7]Mark Bun, Kobbi Nissim, Uri Stemmer, and Salil P. Vadhan. Differentially private release and
learning of threshold functions. In FOCS , pages 634â€“649. IEEE Computer Society, 2015.
[8]Bryan Cai, Constantinos Daskalakis, and Gautam Kamath. Privâ€™it: Private and sample efficient
identity testing. In International Conference on Machine Learning , pages 635â€“644. PMLR,
2017.
[9]ClÃ©ment L Canonne et al. Topics and techniques in distribution testing: A biased but rep-
resentative sample. Foundations and Trends Â®in Communications and Information Theory ,
19(6):1032â€“1198, 2022.
[10] Siu-On Chan, Ilias Diakonikolas, Paul Valiant, and Gregory Valiant. Optimal algorithms for
testing closeness of discrete distributions. In Proceedings of the twenty-fifth annual ACM-SIAM
symposium on Discrete algorithms , pages 1193â€“1203. SIAM, 2014.
[11] Constantinos Daskalakis, Ilias Diakonikolas, Rocco A Servedio, Gregory Valiant, and Paul
Valiant. Testing k-modal distributions: Optimal algorithms via reductions. In Proceedings of
the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms , pages 1833â€“1852.
SIAM, 2013.
[12] Ilias Diakonikolas, Themis Gouleakis, Daniel M Kane, John Peebles, and Eric Price. Optimal
testing of discrete distributions with high probability. In Proceedings of the 53rd Annual ACM
SIGACT Symposium on Theory of Computing , pages 542â€“555, 2021.
[13] Ilias Diakonikolas, Themis Gouleakis, John Peebles, and Eric Price. Sample-optimal identity
testing with high probability. In Ioannis Chatzigiannakis, Christos Kaklamanis, DÃ¡niel Marx,
and Donald Sannella, editors, 45th International Colloquium on Automata, Languages, and
Programming, ICALP 2018, July 9-13, 2018, Prague, Czech Republic , volume 107 of LIPIcs ,
pages 41:1â€“41:14. Schloss Dagstuhl - Leibniz-Zentrum fÃ¼r Informatik, 2018.
[14] Ilias Diakonikolas and Daniel M Kane. A new approach for testing properties of discrete
distributions. In 2016 IEEE 57th Annual Symposium on Foundations of Computer Science
(FOCS) , pages 685â€“694. IEEE, 2016.
[15] Ilias Diakonikolas, Daniel M Kane, and Vladimir Nikishkin. Testing identity of structured
distributions. In Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete
algorithms , pages 1841â€“1854. SIAM, 2014.
11[16] Ilias Diakonikolas, Daniel M Kane, and Vladimir Nikishkin. Optimal algorithms and lower
bounds for testing closeness of structured distributions. In 2015 IEEE 56th Annual Symposium
on Foundations of Computer Science , pages 1183â€“1202. IEEE, 2015.
[17] Ilias Diakonikolas, Daniel M. Kane, and Vladimir Nikishkin. Near-optimal closeness testing of
discrete histogram distributions. In ICALP , volume 80 of LIPIcs , pages 8:1â€“8:15, 2017.
[18] C. Dwork, G. Rothblum, and S. Vadhan. Boosting and differential privacy. In FOCS , 2010.
[19] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our
data, ourselves: Privacy via distributed noise generation. In EUROCRYPT , 2006.
[20] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In STOC , pages 371â€“380.
ACM, 2009.
[21] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryptography
Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3 , pages 265â€“284.
Springer, 2006.
[22] Oded Goldreich. The uniform distribution is complete with respect to testing identity to a fixed
distribution. In Electron. Colloquium Comput. Complex. , volume 23, page 15, 2016.
[23] Oded Goldreich and Dana Ron. On testing expansion in bounded-degree graphs. Studies
in Complexity and Cryptography. Miscellanea on the Interplay between Randomness and
Computation: In Collaboration with Lidor Avigad, Mihir Bellare, Zvika Brakerski, Shafi
Goldwasser, Shai Halevi, Tali Kaufman, Leonid Levin, Noam Nisan, Dana Ron, Madhu Sudan,
Luca Trevisan, Salil Vadhan, Avi Wigderson, David Zuckerman , pages 68â€“75, 2011.
[24] Svante Janson. Tail bounds for sums of geometric and exponential variables. Statistics &
Probability Letters , 135:1â€“6, 2018.
[25] S. R. Mane. Moments of the poisson distribution of order k, 2023.
[26] Kobbi Nissim, Sofya Raskhodnikova, and Adam D. Smith. Smooth sensitivity and sampling in
private data analysis. In STOC , pages 75â€“84. ACM, 2007.
[27] Liam Paninski. A coincidence-based test for uniformity given very sparsely sampled discrete
data. IEEE Transactions on Information Theory , 54(10):4750â€“4755, 2008.
[28] H. Pishro-Nik. Introduction to Probability, Statistics, and Random Processes . Kappa Research,
LLC, 2014.
[29] Gregory Valiant and Paul Valiant. An automatic inequality prover and instance optimal identity
testing. SIAM Journal on Computing , 46(1):429â€“455, 2017.
[30] Huanyu Zhang. Statistical inference in the differential privacy model. CoRR , abs/2108.05000,
2021.
12A Missing Proofs â€” Privacy
Claim 14 (Claim 4 restated) .The estimator Z=Ps
i=1â„“(x(i))â„“(x(i+1))in Part I of Algorithm 1
(Line 5) has global senstivity of 6.
Proof. Consider omitting a single datapoint, x(i). It is easy to check cases and see that when
â„“(x(iâˆ’1)) =â„“(x(i+1)) =âˆ’â„“(x(i))then before omitting x(i)the three entries contributed âˆ’2toZ
andafter the omittance they contribute 1toZ. Thus the change is 3. Considering SandSâ€²that are
different on a single entry, we have that this change to a triplet of consecutive datapoints occurs twice,
hence the global senstivity of Zis2Â·3 = 6 .
Claim 15 (Claim 5 restated) .The Global Sensitivity of nmaxis at most 4.
Proof. FixSandSâ€²to be two neighboring inputs that differ on a single point, and assume that the
changed point appears in first place in Sand the last ( s-th) place in Sâ€². Now, it is simple to see that
each bin changes by at most two points (its first and last), as shown in Figure 1. However, since we
may not know the beginning-points and end-points of each bin due to the Bernoulli random noise
added to each, then in the extreme case the bin from Shas addition more point and from Sâ€²missed
a point. This sums to 4points that may belong to one bin and not the other, shifting the value of
Xiâˆ’Yiby at most 4.
Proof of Theorem 3. First, we discuss Part I. In this part we only release the noisy estimator  Ps
i=1â„“(x(i))â„“(x(i+1))
+X. Due to Claim 4 we know that adding Lap(6/Ïµ)to this estimator
isÏµ-DP. We now move to Part II of the algorithm.
In this part we release j0-times a pair of outputs: (i) a noisy estimator of nmax and (ii)
a noisy estimator of Z. Assume the event from Lemma 6 holds. Adding Lap(8/Ïµâ€²)-
noise to nmax which has global sensitivity of 4(Claim 5) is Ïµâ€²/2-DP. Adding noise of
Lap
16 log4/3(2/Î´)q
6m
nlog (800 m) +16 ln(1 /Î´â€²)
Ïµâ€² + 1
/Ïµâ€²
isÏµâ€²/2-DP. It follows that under E,
each pair of outputs preserves Ïµâ€²-DP. Note that we set Ïµâ€²=Ïµ
8âˆš
j0log(2/Î´), so it follows from Advanced
Composition, that overall we preserve (Ïµ, Î´/2)-DP under E.
Summing both Parts together, Algorithm 1 preserves (2Ïµ,Î´/2)-DP under an event that holds w.p.
1âˆ’3Î´/2; so overall Algorithm 1 is (2Ïµ,2Î´)-DP.
B Missing Proofs â€” Utility
B.1 Proof of Utility - Rudimentary Claims
Recall that in Part II we partition the domain into bins by using a list of sorted samples, and see if the
examples in those bins pass our L2test â€“ TestCloseness . Moreover should TestCloseness return
NULL we unify bins, using the same sets of points. Throughout the analysis we assume that we know
the endpoint of each bin, namely that the result of the Bernoulli r.v.s have been disclosed. Now that
each bin Î j
ihastipoints we argue that the distribution of the number of points following a binomial
distribution from P(resp.Q).
Claim 16. Fix index j. Fix index i. Denote Ias the interval (x(Ï€j
i), x(Ï€j
i+1))which is some interval
defining a bin in the partition Î j
i, which contains tipoints overall. Then the number of points in an
interval Î j
icoming from Pfollow Bin
ti,P(I)
P(I)+Q(I)
. Moreover, for any two disjoint bins Î j
iand
Î j
iâ€²it holds that the number of points from P(resp.Q) in each bin is independent of the other.
Proof. We draw sfrom a Poisson distribution with parameter N, representing the number of samples
we take from1
2(P+Q). As a result, we can determine that within any fixed interval, including
the interval I, the number of points we denote as ËœXIfollows a Poisson distribution with parameter
1
2(P(I)). However, we know that the number of points within interval Iis exactly ti.
13So now, conduct a thought experiment, where we re-sample Suntil precisely tipoints from Sfall in I.
Replacing our original tpoints in Î 0
iwith the new points is indistinguishable as they are distributed
precisely the same. Denote ËœXi(resp. ËœYi) as the number of points from P(resp.Q) in our repeated
thought experiment that fall in I, and define XI(resp. YI) as the number of points from P(resp.
Q) that fall within interval Iin the draw where precisely tipoints fall in Ifrom both distributions
together. Therefore
XIâˆ¼ËœXI|
ËœXI+ËœYI=ti
We can recall that by the definition of our sampling from Poisson distribution, we get that ËœXIâˆ¼
Poi N
2P(I)
,ËœYIâˆ¼Poi N
2Q(I)
, hence by Proposition 2 it follows that XIâˆ¼Bin(ti,P(I)
P(I)+Q(I)).
Note also that XIandXIâ€²are independent for every IandIâ€². This is due to the fact that the number
of points in any given interval, taken from the set P, follows a Poisson distribution. As a result, the
number of points in any two disjoint intervals IandIâ€²are independent.
We now prove that our new selection of points in each bin follows a Poisson distribution.
Claim 17. For each interval Idefining a bin Î j
i, as in Claim 16, it holds that Xj
iâˆ¼
Poi
NP(I)
4m(P(I)+Q(I))
andYj
iâˆ¼Poi
NQ(I)
4m(P(I)+Q(I))
Proof. Using the same notation as in Claim 16 â€” we know that for each interval Idefining a bin Î j
i,
the following holds: XIis distributed according to a binomial distribution with parameters tiand
P(I)
P(I)+Q(I). Now, let us take a subset of points Cj
iof size Nj
ifromI, where Nj
iis a Poisson random
variable with parameter Î»=N
4mj, and denote Xj
ias the sum of Bernoulli i.i.d. random variables br,
such that each bris distributed according to a Bernoulli distribution Ber(P(I)
P(I)+Q(I)). In other words,
Xj
i=PNi
r=1br. Based on Proposition 2 in the preliminaries, it holds that Xj
iis distributed like a
Poisson r.v. with parameter Î»Â·P(I)
P(I)+Q(I). The proof for Yj
iis symmetrical. To prove independence
ofXj
i=PNj
i
i=1Ber(p)andYj
i=PNj
i
i=1Ber(1âˆ’p), and we have
Pr[Xi=a , Y i=b] = Pr[ Xi=a , X i+Yi=a+b]
= Pr[ Xi=a|Xi+Yi=a+b]Â·Pr[Xi+Yi=a+b]
=a+b
a
pa(1âˆ’p)bÂ·eâˆ’(N
4m) N
4ma+b
(a+b)!
=1
a!b!pa(1âˆ’p)bÂ·eâˆ’(N
4m)p+(N
4m)(1âˆ’p) N
4ma+b
(a+b)!
= N
4mpaeâˆ’(N
4m)p
a!Â· N
4m(1âˆ’p)beâˆ’(N
4m)(1âˆ’p)
b!= Pr[ Xi=a]Â·Pr[Yi=b]
Moreover, we need to ensure that each selection of a subset of points does not require more points
than we already have, which is no more thanN
mfor each bin, meaning no bin holds any âŠ¥points.
Claim 18. IfN > 32/3Â·mln(2m), then with probability 1âˆ’1
2mit holds that for each bin i, the size
of the chosen subset Ciis no greater thanN
mâˆ’1.
Proof. The proof follows from standard tail bounds of the Poisson distribution. Given a random
variable Xâˆ¼Poi(Î»), we have
Pr[X > Î» + 3Î»]â‰¤exp
âˆ’(3Î»)2
3Â·(3 + 1) Î»
â‰¤exp
âˆ’3Î»
4
14Therefore, if we want each of the m0+m1+m2+...+mjâ‰¤2mbins to have a subset size that
doesnâ€™t exceedN
mj, we get
Pr
âˆ€i,âˆ€j,|Cj
i|<N
mj
â‰¥1âˆ’X
i,jPr[Nj
i<4N
4mj]â‰¥1âˆ’X
jmjexp
âˆ’3N
16mj
â‰¥1âˆ’2mexp
âˆ’3N
16m0
= 1âˆ’2meâˆ’2 ln(2 m)= 1âˆ’1
2m
Lemma 19 (Lemma 9 restated) .Fix index j. For each index idenote by Iias the interval
(x(Ï€j
i), x(Ï€j
i+1))which is the interval defining bin iin the partition Â¯Î j. Then the estimator Z
computed by TestCloseness satisfies that E[Z] =Pmj
i=1(piâˆ’qi)2where
pi=NP(Ii)
4m(P(Ii) +Q(Ii)), q i=NQ(Ii)
4m(P(Ii) +Q(Ii)).
and that Var [Z] =Pmj
i=14(pi+qi)(piâˆ’qi)2+ 2(pi+qi)2.
Proof. For brevity, we denote Xj
iasXiand the same for Yi. We want to calculate the expectation of
the statistic Z=P
i(Xiâˆ’Yi)2âˆ’Xiâˆ’Yi. Therefore
E[Z] =Eï£®
ï£°mjX
i=1(Xiâˆ’Yi)2âˆ’Xiâˆ’Yiï£¹
ï£»=mjX
i=1E[X2
iâˆ’2XiYi+Y2
iâˆ’Xiâˆ’Yi]
=mjX
i=1pi+p2
iâˆ’2piqi+qi+q2
iâˆ’piâˆ’qi=mjX
i=1(piâˆ’qi)2
We now want to calculate the variance of Z=Pnj
i=1(Xiâˆ’Yi)2âˆ’Xiâˆ’Yi. Therefore
Var[Z] =Varï£®
ï£°njX
i=1(Xiâˆ’Yi)2âˆ’Xiâˆ’Yiï£¹
ï£»=njX
i=1Var
(Xiâˆ’Yi)2âˆ’Xiâˆ’Yi
Letâ€™s denote Zi= (Xiâˆ’Yi)2âˆ’Xiâˆ’Yiand calculate E
Z2
i
:
E
Z2
i
=Eh 
(Xiâˆ’Yi)2âˆ’Xiâˆ’Yi2i
=E
(Xiâˆ’Yi)4âˆ’2(Xi+Yi)(Xiâˆ’Yi)2+ (Xi+Yi)2
We analyze the expectations of each term separately, based on known first to fourth moments of the
Poisson distribution [25] and the independence of XiandYi, we know that
E
(Xiâˆ’Yi)4
=E[X4
iâˆ’4X3
iYi+ 6X2
iY2
iâˆ’4XiY3+Y4
i]
=p4
i+ 6p3
i+ 7p2
i+piâˆ’4pi(q3
i+ 3q2
i+qi) + 6( pi+p2
i)(qi+q2
i)
âˆ’4qi(p3
i+ 3p2
i+pi) +q4
i+ 6q3
i+ 7q2
i+qi
= (piâˆ’qi)4+ 6p3
i+ 7p2
i+piâˆ’4pi(3q2
i+qi) + 6piqi+ 6piq2
i+ 6p2
iqi
âˆ’4qi(3p2
i+pi) + 6q3
i+ 7q2
i+qi
= (piâˆ’qi)4+ 7(piâˆ’qi)2+ 6(p3
iâˆ’p2
iqiâˆ’piq2
i+q3
i) +pi+qi+ 12piqi
It is to see that E[(Xi+Yi)2] =pi+p2
i+qi+q2
i+ 2piqi= (pi+qi)2+pi+qi, so now
E[(Xi+Yi)(Xiâˆ’Yi)2] =E[X3
iâˆ’2X2
iYi+XiY2
i+YiX2
iâˆ’2XiY2
i+Y3
i]
=p3
i+ 3p2
i+piâˆ’qi(pi+p2
i)âˆ’pi(qi+q2
i) +q3
i+ 3q2
i+qi
=p3
iâˆ’qip2
iâˆ’piq2
i+q3
i+ 3(piâˆ’qi)2+pi+qi+ 4piqi
15Combining all three terms, we get
E
Z2
i
= (piâˆ’qi)4+ 7(piâˆ’qi)2+ 6(p3
iâˆ’p2
iqiâˆ’piq2
i+q3
i) +pi+qi+ 12piqi
âˆ’2(p3
iâˆ’qip2
iâˆ’piq2
i+q3
i+ 3(piâˆ’qi)2+piâˆ’qi+ 4piqi) + (pi+qi)2+pi+qi
= (piâˆ’qi)4+ 4(p3
iâˆ’p2
iqiâˆ’piq2
i+q3
i) + (piâˆ’qi)2+ (pi+qi)2+ 4piqi
= (piâˆ’qi)4+ 4(pi+qi)(piâˆ’qi)2+ 2(pi+qi)2
And so we get that
Var[Zi] =E[Z2
i]âˆ’E[Zi]2= (piâˆ’qi)4+ 4(pi+qi)(piâˆ’qi)2+ 2(pi+qi)2âˆ’(pi+qi)4
= 4(pi+qi)(piâˆ’qi)2+ 2(pi+qi)2
So overall by independence we get
Var[Z] =mjX
i=1Var[Zi] =mjX
i=14(pi+qi)(piâˆ’qi)2+ 2(pi+qi)2
Claim 20 (Claim 10 restated) .IfN > 3000mlog(2m), then with a probability of 1âˆ’1
m, anyIthat
forms a bin Î 0
iinÎ 0, has that1
1.01mâ‰¤P(I)+Q(I)
2â‰¤1.01
m.
Proof. LetIbe an interval determining a bin Î 0
i, and denote SI=1
2(P(I) +Q(I)). From our
process of partitioning the domain and creating the intervals, we know that SIâˆ¼PN/m
i=1Exp(N).
Therefore, as the sum of independent Exponential r.v.s we can conclude that SIâˆ¼Gamma 
N/m,1
N
.
Using the tail bound of the sum of the exponential distribution [ 24], we get the following inequality:
âˆ€t >0,Pr[SI> tE[SI]]â‰¤min1
t,1
exp(âˆ’Î±(tâˆ’1âˆ’log(t)))
where Î±=E[SI]
1/N. In our case, we have E[SI] =N/m
N, soÎ±=N
m. By setting t= 1.01, we arrive at
the following result:
Pr
SI>1.01
m
= Pr
SI>1.01N
mÎ±Â·1
m
â‰¤1
1.01eâˆ’N
m(1.01âˆ’1âˆ’log(1.01))< eâˆ’0.00049 N
mâ‰¤1
2m2
And now if t=1
1.01â‰¤1, then we get
Pr
SIâ‰¤1
1.01m
â‰¤eâˆ’N
m(1/1.01âˆ’1âˆ’log(1/1.01))â‰¤eâˆ’0.00049 N
mâ‰¤1
2m2
So by the Union-Bound, we get that Pr[âˆƒI, S I>1.01/morSI<1/1.01m]â‰¤m(1
2m2+1
2m2) =1
m.
B.1.1 Proof of Theorem 7
Having acquired the rudimentary tools, we can now prove Theorem 7.
Proof of Theorem 7. We can see that in the Part I of the algorithm, we have sâˆ¼Poi(m)samples
such that for all 1â‰¤iâ‰¤s â„“(xi)âˆˆ {1,âˆ’1}with equal probability and independent from each
other, therefore, by linearity of expectation we get that for the estimator Zfrom Line 5 of Part I of
Algorithm 1 we have E[Z] =P
iE[â„“(xi)â„“(xi+1)] =P
iE[â„“(xi)]E[â„“(xi+1)] = 0 . We can bound the
variance of Zusing O(m)by the Effron Stein inequality / Jackknife principle.
Proposition 21. Var[Z]â‰¤9m.
Proof of Proposition 21. Denote our points X= (x1, x2, . . . , x i, . . . , x s)âˆˆ {1,âˆ’1}s, and it is clear
to see that if we change one of the points independently X(i)= (x1, x2, . . . , xâ€²
i, . . . , x s)then
Var[Z|s]â‰¤1
2sX
i=1E
Z(X)âˆ’Z
X(i)2
â‰¤1
2sX
i=1Eh
(â„“(xiâˆ’1)â„“(xi) +â„“(xi)â„“(xi+1)âˆ’â„“(xiâˆ’1)â„“(xâ€²
i)âˆ’â„“(xâ€²
i)â„“(xi+1))2i
â‰¤1
2Â·16s= 8s
16And we know that Var [E[Z|s]]â‰¤Var[s] =m. Therefore we get
Var[Z] =E[Var[Z|s]] +Var[E[Z|s]]â‰¤E[8s] +m= 9m
Therefore by Chebyshevâ€™s inequality, it follows that
Pr[Zâ‰¤10âˆšm]â‰¥1âˆ’Pr[|Zâˆ’E[Z]|>10âˆšm]â‰¥1âˆ’9m
100mâ‰¥0.91.
Now we prove that our Laplace noise does not exceedm3Î±3
8k2, provided mâ‰¥100k2
3/Î±Ïµ1
3.
Pr[|ËœZâˆ’Z| â‰¥1
4m3Î±3
2k2] = Pr
Lap6
Ïµ
â‰¥1
4m3Î±3
2k2
â‰¤exp
âˆ’m3Î±3Ïµ
48k2
â‰¤0.01 (1)
And so , with probability â‰¥0.9it holds that ËœZâ‰¤10âˆšm+1
4m3Î±3
2k2<m3Î±3
2k2, provided mâ‰¥
40k4
5
Î±6
5+ 100k2
3
Î±Ïµ1
3. It follows that Part I of Algorithm 1 returns NULL .
In Part II, we also need to argue that all invocations of TestCloseness return NULL . Fix an iteration
jand observe that for any iit holds that Xj
iandYj
iare distributed like Poi(N
8mj)as the labeling
of points as coming from PorQis completely random Ber(1/2). We next bound |Xj
iâˆ’Yj
i|andP
i(Xj
iâˆ’Yj
i))2using known tail bounds on Poisson random variables (Proposition2).
Proposition 22. Assume that for all jwe have N/8mj>4 log(800 m). Then with probability >0.99
it holds that for any jand any iwe have
|Xj
iâˆ’Yj
i| â‰¤r
6N
mjlog (800 m)
Proof. We have that Nj
iâˆ¼Poi(N
4mj),Poi(N
8mj)andYi=Nj
iâˆ’Xj
i. Therefore, |Xj
iâˆ’Yj
i|=
|2Xj
iâˆ’Nj
I| â‰¤2|Xj
iâˆ’E[Xj
i]|+|Nj
iâˆ’E[Nj
i]|+|2E[Xj
i]âˆ’E[Nj
i]|. Seeing as 2E[Xj
i]âˆ’E[Nj
i] = 0
we bound the other two terms using known tail bounds on the Poisson distribution from Proposition 2.
Pr[|Xj
iâˆ’E[Xj
i]|>2p
N/8mjlog(800 m)]<1
400m
Pr[|Nj
iâˆ’E[Nj
i]|>2p
N/4mjlog(800 m)]<1
400m
Provided N/8mj>4 log(800 m). And so, w.p. â‰¥1âˆ’1
200mit follows that |Xj
iâˆ’Yj
i| â‰¤
4p
N/8mjlog(800 m) + 2p
N/4mjlog(800 m)â‰¤p
6N/mjlog(800 m). Applying the Union Bound
on all m0+m1+...+mj0â‰¤2mbins, we have the required.
We now can complete the proof that Part II also returns NULL . Note that in each invocation of
TestCloseness we use m=P
jNj
ipoints over n=mjbins. It was already established in the proof
of Lemma 6 that the probability that there exists an invocation where nmaxâ‰¤q
6m
nlog (800 m)
yet due to the Laplace noise Ë†nmax>q
6m
nlog (800 m) +8 log( 2/j0Î´)
Ïµâ€² is upper bounded byÎ´
2. We
show a similar result for the difference of Ë†Zâˆ’Z, which we denote as a random variable Râˆ¼
Lap(16 log 4/3(2/Î´)nz/Ïµâ€²). Standard properties of the Laplace distribution give that the probability that
exists even a single invocation of TestCloseness where Ris greater than log(10 j0)16 log 4/3(2/Î´)nz/Ïµâ€²
is at most 0.01. Lastly, we argue that Zisnâ€™t too large. Based on Lemma 9 we know that at any
iteration jwe have E[Z] =Pmj
i=1(N
8mâˆ’N
8m) = 0 andVar[Z]â‰¤2Â·(N/8mj+N/8mj)2mj=N2
8mj.
Using the Chebyshev Inequality and the Union Bound that Pr[âˆƒj, Z > 10Np
log(1/Î±)/8mj]â‰¤
j0Â·1
100 log( 1/Î±)<0.01.
17And so, w.p. â‰¥0.97âˆ’Î´â‰¥0.96we get that
Ë†Z=Z+Râ‰¤10Np
log(1/Î±)/8mj+16 log(10 j0) log 4/3(2/Î´)nz
Ïµâ€²
(âˆ—)
â‰¤10Np
log(1/Î±)/8mj+(Î±
12âˆš2k+1Â·log(1/Î±))2N2
16
(âˆ—âˆ—)
â‰¤(Î±
12âˆš2k+1Â·log(1/Î±))2N2
16+Î±2N2
16=(Î±
12âˆšk+1Â·log(1/Î±))2N2
8(âˆ—âˆ—âˆ—)
â‰¤1
2Î±2
TestCloseness (X
iNj
i)
Where inequality (âˆ—)holds when
16 log(10 j0) log4/3(2/Î´)q
6m
nlog (800 m) +256âˆš
j0log(2/Î´) ln(2 j0/Î´)
Ïµ
Ïµ
8âˆš
j0log(2/Î´)<(Î±
12âˆš2k+1Â·log(1/Î±))2N2
16â‡”
144âˆš
log(1/Î±) log( 2/Î´) log(10 log( 1/Î±)) log4/3(2/Î´)q
12N
mjlog(800 m)+256âˆš
log(1/Î±) log( 2/Î´) ln(2 j0/Î´)
Ïµ
Ïµ<Î±2N2
6912klog2(1/Î±)
which in turn holds when Nâ‰¥Ëœâ„¦(k1/3
Ïµ2/3Î±4/3)andNâ‰¥Ëœâ„¦(k1/2
ÏµÎ±);
inequality (âˆ—âˆ—)holds when
10Nr
log(1/Î±)
8mjâ‰¤10Nr
log(1/Î±)
8kâ‰¤Î±2N2
6912klog2(1/Î±)
which in turn holds when Nâ‰¥â„¦(k1/2log(1/Î±)5/2
Î±2 ); and inequality (âˆ— âˆ— âˆ—)holds due to Proposition 22
when X
iNj
iâ‰¥Nâˆ’mjÂ·2p
N/4mjlog(800 m)â‰¥N/2
when Nâ‰¥3000mlog(800 m).
B.2 Proof of Theorem 8
Lemma 23 (Lemma 11 restated) .Fixpâˆˆ(0,1). Suppose there exists nnon-negative random
variables X1, X2, . . . , X n, such that for each iit holds that for some fixed number aiwe have
Pr[Xiâ‰¥ai]â‰¥p. Then, given a constant c >0there exists another constant C >0such that with a
probability at least Cit holds thatPn
i=1Xiâ‰¥cPn
i=1ai.
Proof. Define the random variables Yi=aiwith probability p
0with probability 1âˆ’p. We can see that
E[Pn
i=1Yi] = pPn
i=1aiand that the variance is Var[Pn
i=1Yi] =Pn
i=1Var[Yi] =
p(1âˆ’p)Pn
i=1a2
i. Fix c >0. Using Chebyshevâ€™s inequality, we can bound the probability of
being far from their expectation.
Pr"nX
i=1Yiâ‰¤cÂ·E"nX
i=1Yi##
â‰¤Pr"nX
i=1Yiâˆ’E"nX
i=1Yi#â‰¥(1âˆ’c)E"nX
i=1Yi##
â‰¤Var[Pn
i=1Yi]
(1âˆ’c)2Â·E[Pn
i=1Yi]2â‰¤p(1âˆ’p)Pn
i=1a2
i
(1âˆ’c)2p2(Pn
i=1a2
i)2â‰¤p(1âˆ’p)
p2(1âˆ’c)2
Now, since we have Pr[Xiâ‰¥ai]â‰¥p= Pr[ Yi=ai]andPr[Xiâˆˆ(0, ai)]â‰¤1âˆ’p= Pr[ Yi= 0]
for each i, it is easy to see that Pr[Pn
iXiâ‰¥a]â‰¥Pr[Pn
iYiâ‰¥a]for any a. Hence
Pr"nX
i=1Xiâ‰¥cnX
i=1ai#
â‰¥Pr"nX
iYiâ‰¥cnX
i=1ai#
â‰¥1âˆ’p(1âˆ’p)
p2(1âˆ’c)2=C
Lemma 24. [17] For any two distributions PandQon[m], letPâ€²andQâ€²be the merged distributions,
Then,
âˆ¥P âˆ’ Qâˆ¥ Akâ‰¤ âˆ¥Pâ€²âˆ’ Qâ€²âˆ¥Ak+ 2âˆ¥P âˆ’ Qâˆ¥ 1,k
18Proof. LetIbe the partition of [m]intokintervals so that âˆ¥P âˆ’Qâˆ¥ Ak=P
iâˆˆI|P(I)âˆ’Q(I)|. Let
Iâ€²be obtained from Iby rounding each upper endpoint of each interval (except for the last) down
to the nearest even integer, and rounding the lower endpoint of each interval up to the nearest odd
integer. Note that
X
IâˆˆIâ€²|P(I)âˆ’ Q(I)|=X
IâˆˆIâ€²|Pâ€²(I/2)âˆ’ Qâ€²(I/2)| â‰¤ âˆ¥Pâ€²âˆ’ Qâ€²âˆ¥Ak
seeing as the partition Iâ€²is obtained from Iby taking at most kpoints and moving them from
one interval to another. Therefore, the differenceP
IâˆˆI|P(I)âˆ’ Q(I)| âˆ’P
IâˆˆIâ€²|P(I)âˆ’ Q(I)|
is at most twice the sum of |P(i)âˆ’ Q(i)|over these kpoints, and therefore at most 2âˆ¥P âˆ’ Qâˆ¥ 1,k.
Combining this with the above gives our result.
Lemma 25 (Lemma 13 restated) .[17] For any two distributions PandQon[m]such that
âˆ¥P âˆ’ Qâˆ¥ Ak> Î±, there iteration jâˆˆ[log( m/k)]such that âˆ¥PÎ jâˆ’ QÎ jâˆ¥1,k> Î±/ log(m/k).
Proof. Lemma 24 asserts that
Î± <âˆ¥P âˆ’ Qâˆ¥ Akâ‰¤ âˆ¥PÎ 1âˆ’ QÎ 1âˆ¥Ak+ 2âˆ¥PÎ 0âˆ’ QÎ 0âˆ¥1,k
We apply this recursively when we know that in the final level j0= log( m/k), we get that âˆ¥PÎ jâˆ’
QÎ jâˆ¥Ak=âˆ¥PÎ jâˆ’ QÎ jâˆ¥1,kbecause the distribution there has at most kbins. Thus
log(m/k)X
j=1âˆ¥PÎ jâˆ’ QÎ jâˆ¥1,kâ‰¥Î±
Therefore, by the average principle one of the jâˆˆ[log( m/k)]must satisfy âˆ¥PÎ jâˆ’ QÎ jâˆ¥1,kâ‰¥
Î±/log(m/k); which, by Cauchyâ€“Schwarz, gives âˆ¥PÎ jâˆ’ QÎ jâˆ¥2â‰¥Î±/âˆš
klog(m/k)
.
191.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction ensure that the paper focuses on adapting the
algorithm tester provided by [ 16] to preserve differential privacy. It also discusses the
difficulties that arise and how they will be addressed.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: see [1]
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
20Justification: We briefly state the positive importance of using our tester as a way for
differential privacy to dissipate into other fields in the first paragraph.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
21