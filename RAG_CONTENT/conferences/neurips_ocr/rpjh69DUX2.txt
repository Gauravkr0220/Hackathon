Meta-Reinforcement Learning with Universal Policy
Adaptation: Provable Near-Optimality under All-task
Optimum Comparator
Siyuan Xu & Minghui Zhu
School of Electrical Engineering and Computer Science
The Pennsylvania State University
University Park, PA 16801
{spx5032, muz16}@psu.edu
Abstract
Meta-reinforcement learning (Meta-RL) has attracted attention due to its capability
to enhance reinforcement learning (RL) algorithms, in terms of data efficiency and
generalizability. In this paper, we develop a bilevel optimization framework for
meta-RL (BO-MRL) to learn the meta-prior for task-specific policy adaptation,
which implements multiple-step policy optimization on one-time data collection.
Beyond existing meta-RL analyses, we provide upper bounds of the expected
optimality gap over the task distribution. This metric measures the distance of
the policy adaptation from the learned meta-prior to the task-specific optimum,
and quantifies the modelâ€™s generalizability to the task distribution. We empirically
validate the correctness of the derived upper bounds and demonstrate the superior
effectiveness of the proposed algorithm over benchmarks.
1 Introduction
Meta-learning [ 58,15,25] aims to extract the shared prior knowledge, known as meta-prior, from
the similarities and interdependencies of multiple existing learning tasks, in order to accelerate the
learning process, increase the efficiency of data usage, and improve the overall learning performance
in new tasks. Meta-learning has been extended to solve RL problems, known as meta-RL [ 15,5],
and shows its promise to overcome the challenges of traditional RL algorithms, including scarce
real-world data [3, 44, 65], limited computing resources, and slow learning speed [54, 63].
Meta-learning methods can be generally categorized into optimization-based, model-based (black box
methods), and metric-based methods [ 27,5]. The optimization-based meta-learning approach [ 25] is
compatible with any model trained by an optimization algorithm, such as gradient descent, and thus
is applicable to a vast range of learning problems, including RL problems. Specifically, it formulates
meta-learning as a bilevel optimization problem. At the lower-level optimization, the task-specific
model is adapted from a shared meta-parameter by an optimization algorithm. At the upper-level
optimization, the meta-parameter is to maximize the meta-objective, i.e., the performance of the
model adapted from the meta-parameter over training tasks. The existing methods, including MAML
and its variants [ 15,38,12], take a one-step gradient ascent as the lower-level policy optimization
algorithm, which limits its data inefficiency and leads to sub-optimality.
During the meta-test, MAML conducts one-time data collection, i.e., collecting data using one policy
(the meta-policy), and adapts the policy by one step of policy gradient to the new task. However,
the collected data is only used in one policy gradient step, which may not sufficiently leverage the
data and potentially fail to achieve a good performance. To mitigate the issue, a typical practice is to
implement the data collection and the policy gradient alternately multiple times [ 15]. However, the
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: Solved theoretical challenges of meta-RL
Convergence
of meta-objectiveOptimality of
meta-objectiveNear-optimality
under all-task optimum
[12, 57] âœ“ Ã— Ã—
[60] Ã— âœ“When assuming convergence Ã—
[42] Ã— Ã—âœ“Under optimal
expert policy supervision
This paper âœ“ Immediate result from [60] âœ“
environment exploration is usually costly and time-consuming during the meta-test in applications of
meta-RL [ 44,6,36]. As a result, the low data efficiency limits the optimality of task-specific policies.
In contrast, in this paper, we collect data by meta-policy for one time and utilize multiple policy
optimization steps to improve the data efficiency.
The optimality analysis of MAML is studied in [ 12,60] with a metric of optimality on the meta-
objective , where the error of the meta-objective is defined by the expectation of the optimality gap
between the task-specific policy adapted from the learned meta-parameter and the policy adapted
from the best meta-parameter [ 60,14,26]. However, the best meta-parameter is shared for all tasks.
Even if the meta-objective error is close to zero, i.e., the learned meta-parameter is close to the best
one, the model adapted from the learned meta-parameter might be far from task-specific optimum for
some tasks. In contrast, we aim to design a meta-RL algorithm that can fit a stronger optimality metric,
called near-optimality under all-task optimum , where the comparator, i.e., the policy adapted from
the best meta-parameter, is replaced by the task-specific optimal policy for each task. This metric
offers a more strict comparator for the model adapted from the learned meta-parameter, i.e., when
the metric achieves zero, the policy adaptation produces the optimal policy for every task. A similar
metric is studied by [ 42]. It assumes that the task-specific optimal expert policy for each task is
accessible and serves the supervision for policy adaptation during meta-training, which alleviates the
analysis difficulty caused by the optimal policy comparator. However, the expert policy supervision
is not accessible in a standard meta-RL problem. The metric under all-task optimum is also studied
by [9, 10, 65] in the context of supervised meta-learning.
Main contribution. We develop a bilevel optimization framework for meta-RL, which implements
multiple-step policy optimization on one-time data collection during task-specific policy adaptation.
The overall contributions are summarized as follows. (i) We develop a universal policy optimization
algorithm, which performs multiple optimization steps to maximize a surrogate of the accumulated
reward function. The surrogate is developed only using one-time data collection. It includes various
widely used policy optimization algorithms, including the policy gradient, the natural policy gradient
(NPG) [ 30], and the proximal policy optimization (PPO) [ 52] as the special cases. Then, to learn
the mete-prior, we formulate the meta-RL problem as a bilevel optimization problem, where the
lower-level optimization is the universal policy optimization algorithm from the meta-policy and
the upper-level optimization is to maximize the meta-objective function, i.e., the total reward of the
models adapted from the meta-policy. (ii) We derive the implicit differentiation for both unconstrained
and constrained lower-level optimization problems to compute the hypergradient, i.e., the gradient of
the meta-objective, and propose the meta-training algorithm. In contrast to [ 60], we do not require
to know the closed-form solution of the lower-level optimization. (iii) We derive upper bounds that
quantify (a) the optimality gap between the adapted policy and the optimal task-specific policy for any
task, and (b) the expected optimality gap over the task distribution. Since the proposed framework
incorporates several existing meta-RL methods, such as MAML, as a special case, the analysis also
provides the theoretical motivation for them. (iv) We conduct experiments to validate the theoretical
bounds and verify the efficacy of the proposed algorithm on meta-RL benchmarks.
Table 1 compares the solved theoretical challenges of meta-RL between this paper and previous
works [ 12,57,60,42]. Specifically, paper [ 60] derives the optimality on the meta-objective under the
assumption of bounded hypergradient. Papers [ 12,57] consider the convergence of the meta-objective.
The near-optimality under all-task optimum is considered in [ 42]. However, it assumes the optimal
expert policies of the training tasks are available in meta-training, such that it can learn to approach
the expert policies, while the other methods do not require the expert policies and learn from the
explorations of the environments. In this paper, we show the convergence and optimality guarantee
on the meta-objective, and, more importantly, the optimality guarantee under the all-task optimum
comparator. It is noted that the optimality on the meta-objective is an immediate result from [60].
22 Related works.
Categorization of meta-RL. Meta-RL methods can be generally categorized into (i) optimization-
based meta-RL, (ii) black-box (also called context-based) meta-RL. Optimization-based meta-RL
approaches, such as MAML [ 15] and its variants [ 55,38], usually include a policy adaptation
algorithm and a meta-algorithm. During the meta-training, the meta-algorithm aims to learn a
meta-policy, such that the policy adaptation algorithm can achieve good performance starting from
the meta-policy. The learned meta-policy parameter is adapted to the new task using the policy
adaptation algorithm during the meta-test. Black-box meta-RL [ 11,59,49,47,68] aims to learn an
end-to-end neural network model. The model has fixed parameters for the policy adaptation during
the meta-test, and generates the task-specific policy using the trajectories of the new task takes. In
optimization-based meta-RL, the task-specific policy is adapted from a shared meta-policy over the
task distribution. The learned meta-knowledge is not specialized for each task, and its meta-test
performance on a task depends on a general policy optimization algorithm applied to new data from
that task. In contrast, the end-to-end model in black-box meta-RL typically includes specialized
knowledge for any task within the task distribution, and uses the new data merely as an indicator to
identify the task within the distribution. As a result, the optimality of optimization-based methods is
usually worse than black-box methods, especially when the task distribution is heterogeneous and
the data scale for adaptation is extremely small. On the other hand, the policy adaptation algorithms
in the meta-test of optimization-based methods can generally improve the policy starting from any
initial policy, not only the learned meta-policy. Therefore, it is robust to sub-optimal meta-policy
and can deal with tasks that are out of the training task distribution [ 16,62]. In contrast, due to the
specialization of the learned model, black-box methods cannot be generalized outside of the training
task distribution. In this paper, we focus on the category of optimization-based meta-RL and compare
the proposed algorithm with the existing optimization-based meta-RL approaches in terms of both
experimental results and theory.
Bilevel optimization in meta-RL. Bilevel optimization has been widely studied empirically [ 45,21,
17,18,53,29] and theoretically [ 20,22,29]. It has been applied to many machine learning problems,
including meta-learning [ 35,48], hyperparameter optimization [ 45,17,18], RL [ 24,34], and inverse
RL [39,40,41]. Since the overall objective function in bilevel optimization is generally non-convex,
theoretical analyses of bilevel optimization mainly focus on the algorithm convergence [20, 29, 64],
rarely on the optimality. This paper formulates meta-RL as a bilevel optimization problem. The key
theoretical contribution of this paper is to derive upper bounds on the near-optimality under all-task
optimum, i.e., the expected optimality of the solutions of the lower-level optimization compared with
that of the task-specific optimal policies. The near-optimality under all-task optimum is unique to
meta-learning and has not been studied in the literature on bilevel optimization.
3 Problem statement
MDP. A Markov decision process (MDP) Mâ‰œ{S,A, Î³, Ï, P, r }is defined by the bounded state
spaceS, the discrete or bounded continuous action space A, the discount factor Î³, the initial state
distribution ÏoverS, the transition probability P(sâ€²|s, a) :S Ã— A Ã— S â†’ [0,1], and the reward
function r:S Ã— A Ã— S â†’ [0, rmax].
Policy and value function. A stochastic policy Ï€:S â†’P(A)is a map from states to probability
distributions over actions, and Ï€(a|s)denotes the probability of selecting action ain state s. For
a policy Ï€, the value function is defined as VÏ€(s)â‰œE[Pâˆž
t=0Î³tr(st, at, st+1)|s0=s, Ï€]. The
action-value function is defined as QÏ€(s, a)â‰œE[Pâˆž
t=0Î³tr(st, at, st+1)|s0=s, a0=a, Ï€]. The
advantage function is defined as AÏ€(s, a)â‰œQÏ€(s, a)âˆ’VÏ€(s). The accumulated reward func-
tion is J(Ï€)â‰œEsâˆ¼Ï[VÏ€(s)]. Define the discounted state visitation distribution of a policy Ï€as
Î½Ï€(s)â‰œEs0âˆ¼Ï[(1âˆ’Î³)Pâˆž
t=0Î³tP(st=s|Ï€)]. In this paper, we consider parametric policy Ï€Î¸,
parameterized by Î¸. The optimal parameter Î¸âˆ—can maximize the accumulated reward function, i.e.,
Î¸âˆ—â‰œargmaxÎ¸J(Ï€Î¸). IfÎ¸âˆ—is not unique, denote the set of the optimal solutions by Î˜âˆ—.
Meta-reinforcement learning. Meta-RL aims to solve multiple RL tasks. Consider a space of RL
tasks Î“, where each task Ï„âˆˆÎ“is modeled by a MDP MÏ„â‰œ{S,A, Î³, Ï Ï„, PÏ„, rÏ„}. Correspondingly,
the notations VÏ€
Ï„,QÏ€
Ï„,AÏ€
Ï„,Î½Ï€
Ï„,Î¸âˆ—
Ï„,Î˜âˆ—
Ï„andJÏ„are defined for task Ï„. The RL tasks follow a probability
distribution P(Î“). Meta-RL aims to learn a meta-policy Ï€Ï•parameterized by a meta parameter Ï•,
3such that it can adapt to an unseen task Ï„newâˆ¼P(Î“)with a few iterations and a small number of new
environment explorations. In specific, during the meta-training, several tasks can be i.i.d. sampled
fromP(Î“), i.e.,{Ï„j}T
j=1âˆ¼P(Î“), and the tasksâ€™ MDPs {MÏ„j}T
j=1can be explored. The meta-learner
applies a meta-algorithm to update the meta parameter Ï•by using the data collected from the sampled
tasks. During the meta-test, a new task Ï„newis given, one time of a within-task algorithm Algwith
data collected from Ï„newis applied, the meta-parameter Ï•is adapted to the task-specific parameter
Î¸â€²
Ï„newand the task-specific policy Ï€Î¸â€²Ï„newis tested on the task Ï„new.
Optimality Metric. Consider a meta-RL algorithm that produces a meta-parameter Ï•, and the
take-specific parameter Ï€Î¸â€²
Ï„is adapted from the meta-parameter Ï•on a task Ï„, denoted as Ï€Î¸â€²
Ï„=
Alg(Ï€Ï•, Ï„). We define the task-expected optimality gap (TEOG) as the metric to evaluate the
algorithm, i.e., EÏ„âˆ¼P(Î“)[JÏ„(Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(Ï€Ï•, Ï„))], where Î¸âˆ—
Ï„is the optimal parameter for task
Ï„. First, the TEOG considers the expected error over the task distribution P(Î“), reflecting the
generalizability of the produced meta-parameter. Second, the TEOG adopts the comparator of the
optimal task-specific policy Ï€Î¸âˆ—Ï„for any task Ï„(all-task optimum comparator), and evaluates the
optimality gap JÏ„(Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(Ï€Ï•, Ï„)). In contrast, [ 60,14,26] adopts the comparator of the policy
adapted from the optimal meta-parameter Ï€Ï•âˆ—, and evaluates the optimality gap JÏ„(Alg(Ï€Ï•âˆ—, Ï„))âˆ’
JÏ„(Alg(Ï€Ï•, Ï„)). The latter only considers the optimality on the meta-objective, i.e., how well the
trained meta-objective can approach the optimal meta-objective. However, even if the error of the
meta-objective is approaching zero, i.e., the learned meta-policy is close to the best candidate, the
performance of the model adapted from the optimal meta-policy might still be lacking. This is
because policy optimization usually requires thousands of value/policy iterations to converge; when
tasks are heterogeneous, even if it starts from the best meta-policy, one time of Algwith one time of
value estimate may not be sufficient. In contrast, if our metric is zero, the policy adapted from the
meta-parameter to any task is optimal for the task.
Policy distance and task variance. To find the solution for a new task within a few iterations of
policy optimization, it is crucial that the meta-policy Ï€Ï•can benefit from learning on correlated tasks.
Similar to [ 4,9,31], we measure the correlation of tasks in the task distribution P(Î“)by its variance,
defined by the minimal mean square of the distances among the optimal task-specific policies, i.e.,
Var(P(Î“))â‰œminÎ¸minÎ¸âˆ—Ï„âˆˆÎ˜âˆ—Ï„EÏ„âˆ¼P(Î“)[D2
Ï„(Ï€Î¸, Ï€Î¸âˆ—Ï„)]. Here, DÏ„(Ï€Î¸, Ï€Î¸âˆ—Ï„)is the distance metric
between Ï€Î¸andÏ€Î¸âˆ—Ï„on the task Ï„and is defined by DÏ„(Ï€Î¸, Ï€Î¸â€²)â‰œq
Esâˆ¼Î½Ï€Î¸Ï„[d2(Ï€Î¸(Â·|s), Ï€Î¸â€²(Â·|s))],
where d(Ï€Î¸(Â·|s), Ï€Î¸â€²(Â·|s))is the distance of the policies Ï€Î¸andÏ€Î¸â€²on the state s.
Note that the distance metrics DÏ„(Â·,Â·)andd(Â·,Â·, s)can be custom-defined, leading to multiple
policy update algorithms, as shown in Section 4. Here, we introduce several examples of d(Â·,Â·, s)
andDÏ„(Â·,Â·), which are commonly used as the distance metrics in RL literature [ 51,30,37]. For
policies Ï€Î¸andÏ€Î¸â€², we apply (i) the KL-divergence of the action probability distribution, i.e.,
d2
1(Ï€Î¸, Ï€Î¸â€², s)â‰œDKL(Ï€Î¸(Â·|s)âˆ¥Ï€Î¸â€²(Â·|s)), which is similar to the definition in [ 31]; (ii) The KL-
divergence with the other order, i.e., d2
2(Ï€Î¸, Ï€Î¸â€², s)â‰œDKL(Ï€Î¸â€²(Â·|s)âˆ¥Ï€Î¸(Â·|s)); (iii) the Euclidean
distance of the parameters, i.e., d2
3(Ï€Î¸, Ï€Î¸â€², s)â‰œâˆ¥Î¸âˆ’Î¸â€²âˆ¥2. Correspondingly, for i= 1,2, and 3,
we define DÏ„,i(Ï€Î¸, Ï€Î¸â€²)â‰œq
Esâˆ¼Î½Ï€Î¸Ï„[d2
i(Ï€Î¸, Ï€Î¸â€², s)]. Note that the distance metrics (i)(ii) are not
symmetric, i.e., DÏ„(Ï€Î¸â€², Ï€Î¸â€²â€²)Ì¸=DÏ„(Ï€Î¸â€²â€², Ï€Î¸â€²), and (iii) is symmetric.
In the subsequent sections, we present algorithms based on the generalized distance definitions of
DÏ„(Â·,Â·)andd(Â·,Â·, s). Moreover, we conduct analyses for the introduced distance metrics, from DÏ„,1
toDÏ„,3, to provide comprehensive insights into their respective performances.
4 Meta-Reinforcement Learning Framework
In this section, we develop a meta-RL algorithm by bilevel optimization, where the lower-level
optimization is the within-task algorithm that adapts the parameter from the meta-parameter and
the upper-level optimization is the meta-algorithm that obtains the meta-parameter. The proposed
algorithm has two distinctions compared with existing algorithms. First, it uses one time of a
universal policy optimization algorithm as the lower-level within-task algorithm. Second, we derive
the hypergradient by the implicit differentiation, where the closed-form solution of the lower-level
optimization is not required.
4Within-task algorithm. Consider the policy optimization from the meta policy as the within-task
algorithm Alg. Specifically, given the meta-parameter Ï•and a task Ï„, the task-specific policy
Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„)is defined by Î¸â€²
Ï„= argmaxÎ¸Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Î¸(Â·|s)
QÏ€Ï•Ï„(s, a)
âˆ’Î»D2
Ï„(Ï€Ï•, Ï€Î¸).
When the action space Ais discretized and the policy is tabular, i.e., the probabilities of actions are
independent between different states, the above problem can be solved by Ï€Î¸â€²Ï„(Â·|s) =
Alg(Ï€Ï•, Î», Ï„)(Â·|s) = argmax
Ï€Î¸(Â·|s)X
aâˆˆAÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)âˆ’Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s)), (1)
for all states sâˆˆ S. When the policy is parameterized by an approximation function, in both
continuous and discrete action space A,Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„)is computed by Î¸â€²
Ï„=
argmaxÎ¸Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)
âˆ’Î»D2
Ï„(Ï€Ï•, Ï€Î¸). (2)
In (1) and (2), Î» > 0is a tuning hyperparameter and the distance metric DÏ„can be arbitrarily
chosen. Considering the explorations for the task Ï„are limited, Algonly needs to evaluate the QÏ€Ï•Ï„
by Monte-Carlo sampling on a single policy Ï€Ï•, where the data sampling complexity is exactly the
same as the one-step gradient descent in MAML [ 15]. Therefore, we denote Alg, i.e., collecting data
on the meta-policy and solving the optimal solution of (1) and (2) as the one-time policy adaptation.
More details about the data sample complexity and the computational complexity of (1) and (2) are
clarified in Appendix F. On the other hand, one gradient step is usually not sufficient to identify a
good policy. Therefore, Algis to solve the optimal solution of (1) or (2). As shown in Section 5.4,
the objective function of (1) or (2) is an approximation of the true objective function JÏ„(Ï€).
Note that the objective function in (1) and (2) can reduce to that of multiple widely used policy
optimization approaches: (i) PPO in [ 51,52] when DÏ„=DÏ„,2; (ii) a variant of the PPO [ 60,37],
when DÏ„=DÏ„,1; (iii) the proximally regularized policy update, i.e., the policy optimization
regularized by Euclidean distance of the policy parameter [ 51], when DÏ„=DÏ„,3. Moreover, (iv) if
we approximate the expectation in (2) by its first-order approximation and also select DÏ„=DÏ„,3, the
within-task algorithm (2) also can be reduced to one-step policy gradient, as shown in Appendix H;
(v) if we use the first-order approximation of the expectation in (2), the second-order approximation
of the term D2
Ï„(Ï€Ï•, Ï€Î¸), and select DÏ„=DÏ„,2, the within-task algorithm (2) is reduced to the natural
policy gradient (NPG).
Meta-algorithm. The performance of the meta-parameter Ï•is evaluated by the meta-objective
function, which is defined as the expected accumulated reward after the parameter is adapted by the
within-task algorithm, i.e., EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ï€Ï•, Î», Ï„))]. In the meta-algorithm, we maximize the
meta-objective to obtain the optimal meta-parameter Ï•âˆ—, i.e.,
Ï•âˆ—= argmax
Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ï€Ï•, Î», Ï„))]. (3)
As (1) and (2) provide multiple choices of the within-task algorithms when selecting different DÏ„,
the meta-algorithm (3) provides the algorithms to learn the corresponding meta-priors. For example,
(3) takes on the role of the meta-PPO algorithm when DÏ„=DÏ„,1orDÏ„,2, i.e., (3) learns the
meta-initialization for PPO. It is a meta-NPG algorithm with the corresponding approximation and
DÏ„. Moreover, when Alg(Ï€Ï•, Î», Ï„)in (2) reduces to the one-step policy gradient shown in (iv) of
the last paragraph, (3) represents a precise formulation of MAML in [ 15]. More details about the
formulation and its relations with MAML are shown in Appendix G and H.
Hypergradient computation. Simlar to [ 29,64], the meta-algorithm in (3) aims to solve a bilevel
optimization problem. In previous works [ 60], they apply the policy optimizations that have known
closed-form solutions as the lower-level within-task algorithms. As a result, the bilevel optimization
problem is reduced to a single-level problem. In contrast, in this paper, as we consider a universal pol-
icy optimization, its closed-form solution cannot be obtained. To address the challenge, we compute
âˆ‡Ï•Alg(Ï€Ï•, Î», Ï„)and the hypergradient by deriving the implicit differentiation on Alg(Ï€Ï•, Î», Ï„). As
shown in Section 4, the optimization problem Alg(Ï€Ï•, Î», Ï„)is unconstrained in (2), but is constrained
in (1) due toP
aâˆˆAÏ€(a|s) = 1 . Therefore, we derive the implicit differentiation for both uncon-
strained and constrained optimization problems. The following proposition shows the hypergradient
computation for the tabular policy. Its proof is shown in Appendix J.1.
Proposition 1 (Hypergradient for the tabular policy ).For the tabular policy in the discrete
state-action space, consider any meta-parameter Ï•and the within-task algorithm (1). Let
5Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„). If M(s)â‰œÎ»âˆ‡2
Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))is non-singular for each sâˆˆ
S, we have âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„hP
aâˆˆAâˆ‡Ï•Ï€Î¸â€²Ï„(a|s)QÏ€Î¸â€²Ï„Ï„(s, a)i
, where âˆ‡âŠ¤
Ï•Ï€Î¸â€²Ï„(Â·|s) =

M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11
âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s,Â·)âˆ’Î»âˆ‡âŠ¤
Ï•âˆ‡Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))
|Ï€=Ï€Î¸â€²Ï„.
The computation of âˆ‡Ï•QÏ€Ï•Ï„(s,Â·)is shown in Appendix C. A sufficient condition of M(s)being
non-singular is that dis locally strongly-convex at Ï€=Ï€Î¸â€²Ï„, shown in Appendix J.1. Moreover,
when d=d1ord=d2(correspondingly, DÏ„=DÏ„,1orDÏ„=DÏ„,2in (1)), the matrix M(s) =
Î»âˆ‡2
Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))is always non-singular for any Ï•andM(s)is always diagonal, and thus
it is easy to compute Mâˆ’1(s). The hypergradient computation âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)forDÏ„=DÏ„,1andDÏ„,2
is shown in Appendix K.1 and L.1.
The following proposition shows the hypergradient computation for the policy with function approxi-
mation. Its proof is shown in Appendix J.2.
Proposition 2 (Hypergradient for the policy with function approximation ).When a policy
is represented by a function approximation, in both the discrete and continuous action spaces,
for any meta-parameter Ï•and the within-task algorithm in (2). Let Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„).
Ifâˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)exists, âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³âˆ‡Ï•Î¸â€²
Ï„E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„,aâˆ¼Ï€Î¸â€²Ï„(Â·|s)hâˆ‡Î¸â€²Ï„Ï€Î¸â€²Ï„(a|s)
Ï€Î¸â€²Ï„(a|s)QÏ€Î¸â€²Ï„Ï„(s, a)i
, and
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„=âˆ’Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡2
Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))âˆ’âˆ‡2
Î¸Ï€Î¸(a|s)
Î»Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)]âˆ’1Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)
[âˆ‡âŠ¤
Ï•âˆ‡Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))âˆ’âˆ‡Î¸Ï€Î¸(a|s)
Î»Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a)]|Î¸=Î¸â€²Ï„.
A sufficient condition of âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)being existent is the objective function of (2)is locally strongly
concave at Î¸=Î¸â€²
Ï„, as proven in Appendix J.2. The computation of âˆ‡Ï•QÏ€Ï•Ï„(s,Â·)is shown in Appendix
C. Note that we need to compute the inverse of the Hessian when computing the hypergradient in
Proposition 2. Similar to several widely used RL algorithms, such as TRPO [ 51] and CPO [ 1],
we apply the conjugate gradient algorithm [ 23] to compute the inverse of the Hessian, which has
demonstrated high efficiency across a wide range of applications of RL and meta-learning [ 51,29,15].
More clarifications about the computation efficiency of the Hessian inverse are shown in Appendix E.
Algorithm 1 Meta-Training for BO-MRL
Require: Regularization weight Î» >0; Initial meta-parameter Ï•0; learning rate Î±
1:fort= 0,Â·Â·Â·, Tdo
2: Sample a task Ï„âˆ¼P(Î“)with the MDP MÏ„i.i.d.
3: Evaluate QÏ€Ï•tÏ„(Â·,Â·)for current meta-policy Ï€Ï•tby Monte-Carlo sampling
4: Adapt the task-specific policy Ï€Î¸â€²Ï„from the meta-policy Ï€Ï•tby solving Ï€Î¸â€²Ï„=Alg(Î», Ï•t, Ï„)defined in
(1) or (2).
5: Evaluate QÏ€Î¸â€²Ï„Ï„(Â·,Â·)for adapted policy Ï€Î¸â€²Ï„Monte-Carlo sampling
6: Compute the hypergradient âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)in Proposition 1 or 2 by conjugate gradient method
7: Update meta-parameter Ï•t+1=Ï•t+Î±âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)
8:end for
9: Return Ï•T
With the hypergradient computations in Proposition 1 and Proposition 2, we apply the stochastic
gradient ascent (SGD) to solve the optimization problem in (3). The meta-training of the bilevel
optimization framework for meta-RL (BO-MRL) is formally stated in Algorithm 1. The state-action
value function in lines 3 and 5 can be estimated by many approaches, including Monte-Carlo sampling
used in MAML [ 15] and vine in [ 51]. We also propose a practical algorithm of Algorithm 1, as shown
in Algorithm 2 in Appendix D, which includes more implementation details of the algorithm and
several mechanisms to improve Algorithm 1.
5 Theoretical Results
In this section, we quantify the performance of Algorithm 1, where the softmax policies and several
distance metrics introduced in Section 3 are adopted. For convenience, we denote Alg(1)asAlgin
(1) and (2) when DÏ„=DÏ„,1, and denote Alg(2)andAlg(3)in an analogous way. In Section 5.1, we
6introduce the softmax policy and necessary assumptions. In the following three sections, we consider
two cases of Algorithm 1, including (i) Algorithm 1 with the within-task algorithm Alg(1)andAlg(2)
for the tabular softmax policy; and (ii) Algorithm 1 with the within-task algorithm Alg(3)for the
softmax policy with function approximation. For the algorithms in (i) and (ii), we study the existence
of hypergradient in Section 5.2, derive the convergence guarantees in Section 5.3, and derive the
near-optimality under the all-task optimum, i.e., derive the upper bounds of TEOG, in Section 5.4.
5.1 Softmax policy and assumptions
We apply the softmax policies, which are commonly applied in [ 66,37,60], and use the following
assumptions on the task Ï„.
Softmax policies. Consider the softmax policies Ë†Ï€Î¸parameterized by Î¸for (i) the tabular policy and
(ii) the policy with function approximation. In particular, the tabular policy in a discrete state-action
space is defined by Ë†Ï€Î¸(Â·|s)âˆexp(Î¸(s,Â·)), where Î¸âˆˆR|S|Ã—|A|is a tabular map. The policy with
function approximation is defined by Ë†Ï€Î¸(Â·|s)âˆexp(fÎ¸(s,Â·)), where fÎ¸is a function approximation
model S Ã— A â†’ Rwith the parameter Î¸âˆˆRn.
Assumption 1 (Upper bound of advantage function) .For any task Ï„âˆˆÎ“and any softmax policy Ë†Ï€Î¸,
|AË†Ï€Î¸Ï„(s, a)| â‰¤Amax for any aâˆˆ A and any sâˆˆ S.
Since the reward rÏ„â‰¤rmax is bounded, it is easy to show that |AË†Ï€Î¸Ï„(s, a)| â‰¤rmax
1âˆ’Î³and Assumption
1 always holds. But we still keep Assumption 1 here, since there usually exist Amax such that
Amaxâ‰ªrmax
1âˆ’Î³. We also have the following assumption and show its remark.
Assumption 2 (Sufficient state visit) .For any task Ï„âˆˆÎ“, there exists a constant Ïµ >0, such that for
all bounded parameters Ï•,Î½Ë†Ï€Ï•Ï„(s)â‰¥Ïµfor all sâˆˆ S.
Remark 1. Here are two sufficient conditions for Assumption 2: (i) For any task Ï„âˆˆÎ“, the MDP
MÏ„is ergodic [43, 56]; or (ii) the initial state distribution ÏÏ„hasÏÏ„(s)>0for any sâˆˆ S.
The proof of Remark 1 is shown in Appendix O. Note that (i) of Remark 1 is a mild condition and is
assumed in recent studies on RL algorithm analysis [61, 46].
For the policy with function approximation, we require the following additional assumptions on the
approximate function fÎ¸, which are standard or weaker than those in the analysis of meta-learning
and meta-RL problems [9, 12, 13, 14].
Assumption 3 (Property of the approximate function) .For any state-action pair (s, a)âˆˆ S Ã— A ,
(i) the approximate function fÎ¸(s, a)are cubic differentiable. (ii) fÎ¸(s, a)isL1-Lipschitz, i.e.,
âˆ¥fÎ¸1(s, a)âˆ’fÎ¸2(s, a)âˆ¥ â‰¤L1âˆ¥Î¸1âˆ’Î¸2âˆ¥for any Î¸1, Î¸2âˆˆRn. (iii)âˆ‡Î¸fÎ¸(s, a)isL2-Lipschitz, i.e.,
âˆ¥âˆ‡Î¸fÎ¸1(s, a)âˆ’ âˆ‡ Î¸fÎ¸2(s, a)âˆ¥ â‰¤L2âˆ¥Î¸1âˆ’Î¸2âˆ¥for any Î¸1, Î¸2âˆˆRn, (iv)âˆ‡2
Î¸fÎ¸(s, a)isL3-Lipschitz,
i.e.,âˆ‡2
Î¸fÎ¸1(s, a)âˆ’ âˆ‡2
Î¸fÎ¸2(s, a)â‰¤L3âˆ¥Î¸1âˆ’Î¸2âˆ¥for any Î¸1, Î¸2âˆˆRn.
5.2 Existence of hypergradient.
An essential prerequisite for using Algorithm 1 is that the hypergradients in Propositions 1 and
2 exist. As shown in Section 4, for the tabular policy, when i= 1 or2, the hypergradient
âˆ‡Ï•JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))exists for any Ï•. For the policy with function approximation, we derive the
following sufficient condition of the hypergradient being existent. Its proof is shown in Appendix M.
Proposition 3 (Existence of hypergradient for the policy with function approximation) .In both
discrete and continuous action space, consider the softmax policy with function approximation
shown in Section 5.1. Suppose that Assumptions 1 and 3 hold. If Î» > (6L2
1+ 2L2)Amax,
âˆ‡Ï•JÏ„(Alg(3)(Ë†Ï€Ï•, Î», Ï„))always for any Ï•.
5.3 Convergence guarantee
We begin with the convergence guarantee of Algorithm 1 for the tabular policy. The following
notations are used in the theorem: Bi,Ci,Gi,Ki,Mi(i= 1and2), where Kiâ‰œ2(Bi+2C2
i)r2
max
(1âˆ’Î³)4 ,
Miâ‰œ(Bi+2C2
i)Girmax
(1âˆ’Î³)4 fori= 1and2.B1â‰œ16rmax
Î»(1âˆ’Î³)3+24
1âˆ’Î³+12
Î»,C1â‰œ6
1âˆ’Î³, and G1â‰œ4Amax
(1âˆ’Î³)2.
B2â‰œ16rmax
Î»(1âˆ’Î³)3+18
(1âˆ’Î³)2,C2â‰œ4
1âˆ’Î³, and G2â‰œ2Amax
(1âˆ’Î³)2.
7Theorem 1 (Convergence guarantee for tabular softmax policy ).Consider the tabular
softmax policy in the discrete action space. Suppose that Assumptions 1 and 2 hold.
Let{Ï•t}T
t=1be the sequence of meta-parameters generated by Algorithm 1 with Î»â‰¥
2Amax and the step size Î±= min
rmaxBi
(1âˆ’Î³)2+2Î³rmaxC2
i
(1âˆ’Î³)3âˆ’1
,1
Giâˆš
T
.Then, the bound:
1
TPT
t=1E[âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2]â‰¤Ki
T+Miâˆš
T.holds for i= 1or2.
The first expectation comes from the random sampling in line 2 of Algorithm 1. The proofs of
Theorem 1 are shown in Appendices K.2 and L.2.
The following theorem shows the convergence guarantee for the policy with function approxima-
tion. The notations are used in the theorem: B3,C3,G3,K3,M3, where K3â‰œ2(B3+2C2
3)r2
max
(1âˆ’Î³)4 ,
M3â‰œ(B3+2C2
3)G3rmax
(1âˆ’Î³)4 ,G3â‰œL1Amax(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+2L2)Amax),C3â‰œ2L1(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+2L2)Amax), and
B3â‰œ(160L3
1+56L1L2+4L3)(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)3(Î»âˆ’(6L2
1+2L2)Amax)2 .
Theorem 2 (Convergence guarantee for softmax policy with function approximation ).In both dis-
crete and continuous action space, consider the softmax policy with function approximation. Suppose
that Assumptions 1, 2, and 3 hold. Let {Ï•t}T
t=1be the sequence of meta-parameters generated by Algo-
rithm 1 with Î» >(6L2
1+2L2)Amaxand the step size Î±= min
rmaxB3
(1âˆ’Î³)2+2Î³rmaxC2
3
(1âˆ’Î³)3âˆ’1
,1
G3âˆš
T
.
Then, the bound1
TPT
t=1E
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(3)(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤K3
T+M3âˆš
T.holds.
The first expectation arises from the random sampling in line 2 of Algorithm 1. The proof of Theorem
2 is shown in Appendix M. Theorems 1 and 2 show that the convergence rate of Algorithm 1 is
O(1âˆš
T)and the constants in the notation Oare only related to the discount factor Î³, the reward bound
rmax, the bound of the advantage function Amax, and the Lipschitz constants of fÎ¸.
5.4 Near-optimality under all-task optimum
Before the derivation of the optimality analysis, we first introduce two intermediate Lemmas.
Lemma 1. Suppose that Assumptions 1, 2 hold. For any task Ï„, any bounded parameters Î¸andÎ¸â€²,
andi= 1or2, we have JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¥Esâˆ¼Î½Ë†Ï€Ï„,aâˆ¼Ë†Ï€â€²(Â·|s)h
AË†Ï€Î¸Ï„(s,a)
1âˆ’Î³i
âˆ’2Î³Amax
(1âˆ’Î³)2ÏµD2
Ï„,i(Ë†Ï€Î¸,Ë†Ï€Î¸â€²).
Lemma 2. Consider the softmax policy with function approximation shown in Section 5.1. Suppose
that Assumptions 1, 2, and 3 hold. For any task Ï„, and any softmax policies parameterized by bounded
Î¸andÎ¸â€², we have JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¥Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸â€²(Â·|s)h
AË†Ï€Î¸Ï„(s,a)
1âˆ’Î³i
âˆ’4Î³AmaxL2
1
(1âˆ’Î³)2ÏµD2
Ï„.3(Ë†Ï€Î¸,Ë†Ï€Î¸â€²).
The proofs of Lemmas 1 and 2 are shown in Appendix N.1. Given Lemma 1, when Î»=2Î³Amax
(1âˆ’Î³)Ïµ, the
within-task algorithm Alg(1,2)(Ë†Ï€, Î», Ï„ )in (1) is actually designed to maximize the right-hand side of
the inequality, where Ë†Ï€â€²is the decision variable. Similarly, Given Lemma 2, when Î»=4Î³AmaxL2
1
(1âˆ’Î³)Ïµ,
Alg(3)(Ë†Ï€Î¸, Î», Ï„)in (2) maximizes the right-hand side of the inequality, where Ë†Ï€Î¸â€²is the decision
variable. In other words, for each i= 1,2,and3, the within-task algorithm Alg(i)is to maximize a
lower bound of JÏ„(Ë†Ï€Î¸), denoted as Â¯JÏ„(Ë†Ï€Î¸). This idea, referred to as the minorization-maximization
(MM) [ 28], is widely used in [ 51,33]. The design of Alg(i)enables us to connect the accumulated
reward of the policy after the policy adaptation with that of the optimal policy Ë†Ï€Î¸âˆ—Ï„for task Ï„, i.e.,
Â¯JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))â‰¥Â¯JÏ„(Ë†Ï€Î¸âˆ—Ï„), which is a key intermediate result for the optimality analysis.
The final preparatory step is that we borrow the analysis of the meta-training error from [ 60]. In
particular, its theoretical result is encapsulated in the following assumption.
Assumption 4. (Bounding error of meta-objective using gradient) Let F(i)(Ï•)â‰œ
EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))]. For both the tabular policy and the policy with functional approxi-
mation, there exists a concave positive non-decreasing function hi: [0,+âˆž)â†’[0,+âˆž), such that
max Ï•â€²F(i)(Ï•â€²)âˆ’F(i)(Ï•)â‰¤hi(âˆ¥âˆ‡Ï•F(i)(Ï•)âˆ¥2).
8Assumption 4 assumes the optimality gap of Ë†Ï€Ï•on the meta-objective is upper bounded by an
increasing function of its gradient. A sufficient condition of Assumption 4 is provided by [ 60].
Combine the Assumption 4 and the convergence analysis in Theorems 1 and 2, we can bound the
error of the meta-objective, i.e., max Ï•F(i)(Ï•)âˆ’F(i)(Ï•t). This result is referred to as the optimality
of the meta-objective shown in Table 1. Finally, we derive the upper bounds of the TEOG for both
the tabular policy and the policy with function approximation.
Theorem 3 (Optimality guarantee for softmax tabular policy ).Consider the tabular soft-
max policy for the discrete state-action space. Suppose that Assumptions 1,2 and 4
hold. Let {Ï•t}T
t=1be the sequence of meta-parameters generated by Algorithm 1 with
Î»=2Amax
(1âˆ’Î³)Ïµand the step size Î±shown in Theorem 1. Then, the following holds for
i= 1 or2:1
TPT
t=1Et
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•t, Î», Ï„))]
â‰¤hi
Ki
T+Miâˆš
T
+
2(1+Î³)Amax
(1âˆ’Î³)2ÏµVari(P(Î“)), where Ë†Ï€Î¸âˆ—Ï„is the optimal softmax policy for task Ï„and the constants Kiand
Miare shown in Theorem 1.
Theorem 4 (Optimality guarantee for softmax policy with function approximation ).In both
discrete and continuous action space, consider the softmax policy with function approximation.
Suppose that Assumptions 1,2, 3 and 4 hold. Let {Ï•t}T
t=1be the sequence of meta-parameters
generated by Algorithm 1 with Î»=(6L2
1+2L2)Amax
(1âˆ’Î³)Ïµand the step size Î±shown in Theorem 2.
The following holds:1
TPT
t=1Et
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(3)(Ë†Ï€Ï•t, Î», Ï„))]
â‰¤h3
K3
T+M3âˆš
T
+
((6+4 Î³)L2
1+2L2)Amax
(1âˆ’Î³)2ÏµVar3(P(Î“)), where Ë†Ï€Î¸âˆ—Ï„is the optimal softmax policy for task Ï„and the constants
K3andM3are the same as Theorem 2.
The proofs of Theorems 3 and 4, as well as the selection of the hyperparameter Î»in these two
theorems, are shown in Appendix N.2. The theorems derive the upper bounds of the TEOGs
between the parameter adapted by one-time policy adaptation from the produced meta-parameter
Ï•tand the task-specific optimal parameter Î¸âˆ—
Ï„. It is shown that, with at most Titerations, we can
achieve the upper bounds in the order of O(hi(1âˆš
T) +Var(P(Î“))) . In other words, there exists a
tâ‰¤TwithEÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]â‰¥EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)]âˆ’ O(hi(1âˆš
T) +Var(P(Î“))) . As the
number of iterations Tincreases, or the variance of the task distribution Var(P(Î“)) reduces, the
optimality of the meat-parameter Ï•timproves. The second term Var(P(Î“)) in the upper bounds
of Theorems 3 and 4 corresponds the intuition of meta-learning, which is that, if the variance of
a task distribution is smaller, the meta-policy learned from the task distribution is more helpful
for new tasks in the task distribution, then the performance is better. Moreover, this term shows
that the learned meta-policy achieves a better performance than the meta-policy Ï•centerdefined by
arg min Ï•EÏ„âˆ¼P(Î“)[D2
Ï„,i(Ï€Ï•, Ï€Î¸âˆ—Ï„)], which is the center of all the task-specific optimal policies Ï€Î¸âˆ—Ï„.
The order of our upper bounds are comparable to O(Tâˆ’1
4+Var(P(Î“)) that is shown in [ 31]. On
the other hand, compared with [ 31], in this paper, the constants in the notation Oonly consist of Î³,
rmax,Amax, and the Lipschitz constants of f, and do not rely on |A|and|S|. As a result, our upper
bounds are tighter when handling high-dimensional problems or continuous spaces.
Monotonic improvement of the within-task algorithm. Another benefit from Lemmas 1 and 2
and the idea of MM used by the within-task algorithm is that, the policy update by the within-task
algorithm monotonically improves, i.e., JÏ„(Alg(i)(Ë†Ï€Î¸, Î», Ï„))â‰¥JÏ„(Ë†Ï€Î¸)fori= 1,2and3and any Î¸
and any task Ï„. Therefore, multiple times of Algalways perform better than one-time Alg.
6 Experiments
6.1 Verification of theoretical results
We conduct an experiment to verify the optimality bounds of Algorithm 1 shown in Theorems 3 and
4. We consider two scenarios of the Frozen Lake environment in Gym: two task distributions with
a high task variance and a low task variance. More details of the setting and the hyperparameter
selection are shown in Appendix A. We consider the within-task algorithm Alg(i)for all i= 1,2and
3, where the results of i= 2and3are shown in Appendix A.
90 1 2 3 4
Number of policy adaptation steps-0.50.00.51.01.5Expected Accumulated reward
High task variance (îˆ­(1) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
0 1 2 3 4
Number of policy adaptation steps0.00.51.01.5Expected Accumulated reward
Low task variance (îˆ­(1) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
No
 adaptationOne-time
 of îˆ­(1)
One-step of
 policy gradient0.00.20.40.60.81.01.2Expected Optimality GapUpper bound for
 one-time îˆ­(1)
High task variance (îˆ­(1) applied)
No
 adaptationOne-time
 of îˆ­(1)
One-step of
 policy gradient0.000.050.100.15Expected Optimality GapUpper bound for
 one-time îˆ­(1)
Low task variance (îˆ­(1) applied)
Figure 1: Results of the meta-test on Frozen Lake, where Alg(1)is applied. Left: Average accumulated reward
across all test tasks v.s. number of policy adaptation steps; Right : Comparing the expected optimality gap by the
BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(1).
0 1 2 3
Number of policy adaptation steps-160-140-120-100-80-60-40Accumulated reward
Half-cheetah, goal velocity
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
ProMP
E-MAML
0 1 2 3
Number of policy adaptation steps0100200300400500600Accumulated reward
Half-cheetah, moving direction
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
ProMP
E-MAML
0 1 2 3
Number of policy adaptation steps406080100120Accumulated reward
Ant, goal velocity
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
0 1 2 3
Number of policy adaptation steps0100200300400500600700Accumulated reward
Ant, moving direction
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
ProMP
E-MAML
Figure 2: Average accumulated reward across all test tasks during the meta-test under the practical algorithm of
BO-MRL on the locomotion tasks.
We compare our algorithm with MAML [ 15] and the random initialization. Figure 1 shows that,
for Algorithm 1 with the within-task algorithm Alg(1), it outperforms the baseline methods. For all
scenarios, the expected optimality gap of the one-time policy adaptation is smaller than the upper
bounds shown in Theorems 3 and 4, which verify our theoretical analysis. Moreover, in Figure 1,
the expected optimality gap of the policy adaptation is better (smaller) but close to the upper bound,
while that of the other policy adaptation approach, the policy gradient, is worse (larger) than the
upper bound. It shows that the derived upper bound is tight.
6.2 High-dimensional Experiment
To evaluate the proposed practical algorithm, Algorithm 2 in Appendix D, we conduct experiments
on high-dimensional locomotion settings in the MuJoCo simulator, including Half-Cheetah with goal
directions and goal velocities, Ant with goal directions and goal velocities. We compare the proposed
algorithm with several optimization-based meta-RL algorithms, including MAML, E-MAML [ 55],
and ProMP [ 50]. For the fairness of the comparison, all the methods share the same data requirement
and task setting. More details of the task setting, the hyperparameter selection, and the supplemental
results are shown in Appendix B.
Figure 2 shows that the proposed algorithm with the within-task algorithms Alg(i)outperforms
the baseline methods in all four experimental settings. For example, we achieve about 25% of
performance improvement in Half-cheetah direction and Ant direction experiments. Moreover,
compared with the baseline methods, the proposed algorithm achieves more policy improvement
when more policy optimization steps are given. For example, our approach achieves about 10% of
performance improvement in the second policy optimization step, while those of baseline methods
are almost 0%.
7 Conclusion
This paper develops a bilevel optimization framework for meta-RL, which implements multiple-step
policy optimization on one-time data collection during task-specific policy adaptation. Beyond
existing meta-RL analyses, we provide upper bounds of the expected optimality gap over the task
distribution. Our experiments validate the bounds derived from our theoretical analysis and show the
superior effectiveness of the proposed framework.
10Acknowledgments and Disclosure of Funding
This work is partially supported by the National Science Foundation through grants ECCS 1846706
and ECCS 2140175. We would like to thank the reviewers for their constructive and insightful
suggestions.
References
[1]Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization.
InInternational conference on machine learning , pages 22â€“31. PMLR, 2017.
[2]Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy
gradient methods: Optimality, approximation, and distribution shift. The Journal of Machine
Learning Research , 22(1):4431â€“4506, 2021.
[3]Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. Meta reinforcement learning
for sim-to-real domain adaptation. In 2020 IEEE International Conference on Robotics and
Automation , pages 2725â€“2731. IEEE, 2020.
[4]Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-
based meta-learning. In International Conference on Machine Learning , pages 424â€“433. PMLR,
2019.
[5]Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and
Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028 ,
2023.
[6]Suneel Belkhale, Rachel Li, Gregory Kahn, Rowan McAllister, Roberto Calandra, and Sergey
Levine. Model-based meta-reinforcement learning for flight with suspended payloads. IEEE
Robotics and Automation Letters , 6(2):1471â€“1478, 2021.
[7]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016.
[8]Imre CsiszÃ¡r and JÃ¡nos KÃ¶rner. Information theory: coding theorems for discrete memoryless
systems . Cambridge University Press, 2011.
[9]Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In International Conference on Machine
Learning , pages 1566â€“1575. PMLR, 2019.
[10] Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online
meta-learning. Advances in Neural Information Processing Systems , 32, 2019.
[11] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. RL2:
Fast reinforcement learning via slow reinforcement learning. In International Conference on
Learning Representations , 2017.
[12] Alireza Fallah, Kristian Georgiev, Aryan Mokhtari, and Asuman Ozdaglar. On the conver-
gence theory of debiased model-agnostic meta-reinforcement learning. Advances in Neural
Information Processing Systems , 34:3096â€“3107, 2021.
[13] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-
based model-agnostic meta-learning algorithms. In International Conference on Artificial
Intelligence and Statistics , pages 1082â€“1092. PMLR, 2020.
[14] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Generalization of model-agnostic meta-
learning algorithms: Recurring and unseen tasks. Advances in Neural Information Processing
Systems , 34:5469â€“5480, 2021.
[15] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adap-
tation of deep networks. In International conference on machine learning , pages 1126â€“1135.
PMLR, 2017.
11[16] Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and
gradient descent can approximate any learning algorithm. In International Conference on
Learning Representations , 2018.
[17] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and
reverse gradient-based hyperparameter optimization. In International Conference on Machine
Learning , pages 1165â€“1173. PMLR, 2017.
[18] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. In International
Conference on Machine Learning , pages 1568â€“1577. PMLR, 2018.
[19] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex
stochastic programming. SIAM Journal on Optimization , 23(4):2341â€“2368, 2013.
[20] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv
preprint arXiv:1802.02246 , 2018.
[21] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and
Edison Guo. On differentiating parameterized argmin and argmax problems with application to
bi-level optimization. arXiv preprint arXiv:1607.05447 , 2016.
[22] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration
complexity of hypergradient computation. In International Conference on Machine Learning ,
pages 3748â€“3758. PMLR, 2020.
[23] Magnus R Hestenes and Eduard Stiefel. Methods of conjugate gradients for solving. Journal of
research of the National Bureau of Standards , 49(6):409, 1952.
[24] Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic
algorithm framework for bilevel optimization: Complexity analysis and application to actor-
critic. SIAM Journal on Optimization , 33(1):147â€“180, 2023.
[25] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in
neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence ,
44(9):5149â€“5169, 2021.
[26] Yu Huang, Yingbin Liang, and Longbo Huang. Provable generalization of overparameterized
meta-learning trained with SGD. Advances in Neural Information Processing Systems , 35:16563â€“
16576, 2022.
[27] Mike Huisman, Jan N Van Rijn, and Aske Plaat. A survey of deep meta-learning. Artificial
Intelligence Review , 54(6):4483â€“4541, 2021.
[28] David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician ,
58(1):30â€“37, 2004.
[29] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and
enhanced design. In International Conference on Machine Learning , pages 4882â€“4892. PMLR,
2021.
[30] Sham M Kakade. A natural policy gradient. Advances in neural information processing systems ,
14, 2001.
[31] Vanshaj Khattar, Yuhao Ding, Javad Lavaei, and Ming Jin. A CMDP-within-online framework
for meta-safe reinforcement learning. In International Conference on Learning Representations ,
2023.
[32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[33] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 , 2013.
12[34] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in Neural Information
Processing Systems , pages 1008â€“1014, 2000.
[35] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning
with differentiable convex optimization. In 2019 IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 10649â€“10657, 2019.
[36] Thomas Lew, Apoorva Sharma, James Harrison, Andrew Bylard, and Marco Pavone. Safe
active dynamics learning and control: A sequential explorationâ€“exploitation framework. IEEE
Transactions on Robotics , 2022.
[37] Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy
optimization attains globally optimal policy. Advances in neural information processing systems ,
32, 2019.
[38] Hao Liu, Richard Socher, and Caiming Xiong. Taming MAML: Efficient unbiased meta-
reinforcement learning. In International conference on machine learning , pages 4061â€“4071.
PMLR, 2019.
[39] Shicheng Liu and Minghui Zhu. Distributed inverse constrained reinforcement learning for
multi-agent systems. Advances in Neural Information Processing Systems , 35:33444â€“33456,
2022.
[40] Shicheng Liu and Minghui Zhu. Learning multi-agent behaviors from distributed and streaming
demonstrations. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.
[41] Shicheng Liu and Minghui Zhu. Meta inverse constrained reinforcement learning: Convergence
guarantee and generalization analysis. In The Twelfth International Conference on Learning
Representations , 2023.
[42] Russell Mendonca, Abhishek Gupta, Rosen Kralev, Pieter Abbeel, Sergey Levine, and Chelsea
Finn. Guided meta-policy search. Advances in Neural Information Processing Systems , 32,
2019.
[43] Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes.
InProceedings of the 29th International Coference on International Conference on Machine
Learning , pages 1451â€“1458, 2012.
[44] Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine,
and Chelsea Finn. Learning to adapt in dynamic, real-world environments through meta-
reinforcement learning. In International Conference on Learning Representations , 2018.
[45] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International
Conference on Machine Learning , pages 737â€“746. PMLR, 2016.
[46] Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On finite-time convergence of
actor-critic algorithm. IEEE Journal on Selected Areas in Information Theory , 2(2):652â€“664,
2021.
[47] Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. Fast adaptation to new
environments via policy-dynamics value functions. In Proceedings of International Conference
on Machine Learning , pages 7920â€“7931, 2020.
[48] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with
implicit gradients. Advances in neural information processing systems , 32, 2019.
[49] Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efficient
off-policy meta-reinforcement learning via probabilistic context variables. In International
Conference on Machine Learning , pages 5331â€“5340. PMLR, 2019.
[50] Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel. Promp: Proximal
meta-policy search. In International Conference on Learning Representations , 2019.
13[51] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust
region policy optimization. In International conference on machine learning , pages 1889â€“1897.
PMLR, 2015.
[52] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
[53] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-
propagation for bilevel optimization. In The 22nd International Conference on Artificial
Intelligence and Statistics , pages 1723â€“1732. PMLR, 2019.
[54] Xingyou Song, Yuxiang Yang, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea
Finn, and Jie Tan. Rapidly adaptable legged robots via evolutionary meta-learning. In 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems , pages 3769â€“3776. IEEE,
2020.
[55] Bradly C Stadie, Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and
Ilya Sutskever. Some considerations on learning to explore via meta-reinforcement learning.
arXiv preprint arXiv:1803.01118 , 2018.
[56] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press,
2018.
[57] Yunhao Tang. Biased gradient estimate with drastic variance reduction for meta reinforcement
learning. In Proceedings of the 39th International Conference on Machine Learning , volume
162 of Proceedings of Machine Learning Research , pages 21050â€“21075. PMLR, 17â€“23 Jul
2022.
[58] Sebastian Thrun and Lorien Pratt. Learning to learn . Springer Science & Business Media,
2012.
[59] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXiv preprint arXiv:1611.05763 , 2016.
[60] Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. On the global optimality of model-
agnostic meta-learning. In International conference on machine learning , pages 9837â€“9846.
PMLR, 2020.
[61] Yue Frank Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two
time-scale actor-critic methods. Advances in Neural Information Processing Systems , 33:17617â€“
17628, 2020.
[62] Zheng Xiong, Luisa M Zintgraf, Jacob Austin Beck, Risto Vuorio, and Shimon Whiteson. On
the practical consistency of meta-reinforcement learning algorithms. In Fifth Workshop on
Meta-Learning at the Conference on Neural Information Processing Systems , 2021.
[63] Siyuan Xu and Minghui Zhu. Meta value learning for fast policy-centric optimal motion
planning. Robotics Science and Systems , 2022.
[64] Siyuan Xu and Minghui Zhu. Efficient gradient approximation method for constrained bilevel
optimization. Proceedings of the AAAI Conference on Artificial Intelligence , 37(10):12509â€“
12517, 2023.
[65] Siyuan Xu and Minghui Zhu. Online constrained meta-learning: Provable guarantees for
generalization. In Thirty-seventh Conference on Neural Information Processing Systems , 2023.
[66] Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement
learning with convergence guarantee. In International Conference on Machine Learning , pages
11480â€“11491. PMLR, 2021.
[67] L Zintgraf, K Shiarlis, M Igl, S Schulze, Y Gal, K Hofmann, and S Whiteson. Varibad: a very
good method for bayes-adaptive deep rl via meta-learning. Proceedings of ICLR 2020 , 2020.
[68] Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann,
and Shimon Whiteson. Varibad: A very good method for bayes-adaptive deep rl via meta-
learning. In International Conference on Learning Representations , 2019.
14Appendix for "Meta-Reinforcement Learning with Universal
Policy Adaptation: Provable Near-Optimality under All-task
Optimum Comparator"
Experimental Supplements
All experiments are executed on a computer with a 5.20 GHz Intel Core i12 CPU.
A Experimental Supplements of Verification of Theoretical Results.
Experimental settings. In Section 6, we use the Frozen Lake environment in Gym [ 7] and consider
a task distribution P(Î“)with high task variance and a task distribution P(Î“)with low task variance.
In each distribution, there are 20tasks. The tasks are characterized by the different settings of holes
in the lake, where the holes are generated by random sampling. In the task distribution with high
variance, the probability of the appearing hole in each grid is 0.3; in the task distribution with low
variance, its probability is 0.1. We set Î³= 0.8, the reward is 1when reaching the goal, and the
reward is âˆ’1when reaching the holes. When deriving the upper bound in Theorems 3 and 4, we
approximately regard Tbe sufficiently large, and O(hi(1âˆš
T))be close to 0. The Lipschitz of the
tabular policy is 1, i.e., L1= 1; the Lipschitz of the derivative and the second-order derivative of the
tabular policy are both 0, i.e., L2= 0andL3= 0.
Selection of hyper-parameters. We consider the tabular softmax policy and use Monte Carlo
sampling to evaluate the Q-value. For the task distribution with high task variance, we set Î»= 0.5
forAlg(1),Î»= 0.5forAlg(2), and Î»= 0.04forAlg(3). For the task distribution with low task
variance, we set Î»= 0.25forAlg(1),Î»= 0.25forAlg(2), and Î»= 0.02forAlg(3). There is a
clarification about the hyper-parameter selection and the verified bound shown in Appendix N.3.
Supplemental results. Figures 3 and 4 show the results of the proposed algorithm with Alg(2)and
Alg(3). It shows that, for all scenarios, the expected optimality gap of the policy adaptation Alg(2)
orAlg(3)is smaller than the upper bound shown in Theorems 3 and 4, which verify our theoretical
analysis.
0 1 2 3 4
Number of policy adaptation steps0.00.51.01.5Expected Accumulated reward
High task variance (îˆ­(2) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
No
 adaptationOne-time
 of îˆ­(2)
One-step of
 policy gradient0.000.250.500.751.001.25Expected Optimality GapUpper bound for
 one-time îˆ­(2)
High task variance (îˆ­(2) applied)
0 1 2 3 4
Number of policy adaptation steps0.00.51.01.5Expected Accumulated reward
Low task variance (îˆ­(2) applied)
BO-MRL (ours)
Random initialization
MAML
Optimal task-specific policies
No
 adaptationOne-time
 of îˆ­(2)
One-step of
 policy gradient0.000.050.100.15Expected Optimality GapUpper bound for
 one-time îˆ­(2)
Low task variance (îˆ­(2) applied)
Figure 3: Results of the meta-test of BO-MRL on Frozen Lake, where Alg(2)is applied. Left: Average
accumulated reward across all test tasks v.s. number of policy adaptation steps; Right : Comparing the expected
optimality gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time
Alg(2).
No
 adaptationOne-time
 of îˆ­(3)
One-step of
 policy gradient0.00.51.01.5Expected Optimality GapUpper bound for
 one-time îˆ­(3)
High task variance (îˆ­(3) applied)
No
 adaptationOne-time
 of îˆ­(3)
One-step of
 policy gradient0.00.20.40.6Expected Optimality GapUpper bound for
 one-time îˆ­(3)
Low task variance (îˆ­(3) applied)
Figure 4: Results of BO-MRL on Frozen Lake, where Alg(3)is applied. Comparing the expected optimality
gap by the BO-MRL and baselines with the upper bound of the accumulated reward of one-time Alg(3).
15B Experimental Supplements of Locomotion.
Experimental settings. We consider locomotion tasks HalfCheetah with goal directions and goal
velocities, Ant with goal directions and goal velocities. We follow the problem setups of [ 67,15]. In
the goal velocity experiments, the moving reward is the negative absolute value between the agentâ€™s
current velocity and a goal velocity, which is chosen uniformly at random between 0.0and2.0for the
cheetah and between 0.0and3.0for the ant. In the goal direction experiments, the moving reward is
the magnitude of the velocity in either the forward or backward direction, chosen at random for each
taskÏ„inP. For the Half-cheetah, the total reward = moving reward - ctrl cost. For the ant, the total
reward = healthy reward + moving reward - ctrl cost - contact cost. The horizon is H= 200 , with 20
rollouts per policy adaption step for all problems except the ant direction task, which used 40rollouts
per step.
Selection of hyper-parameters. We apply the proposed practical algorithm of Algorithm 1, Algo-
rithm 2 in Appendix D. We consider the policy as a Gaussian distribution, where the neural network
produces the means and variances of the actions. The neural network policy has two hidden layers
of size 64, with tanh nonlinearities. We use Monte Carlo sampling to evaluate the Q-value. At the
lower-level task-specific policy adaptation, the optimization number by Adam is 50. The models
are trained for up to 500 meta-iterations. For the TRPO in meta-parameter optimization, we use the
KL-divergence constraint as Î´= 1eâˆ’3.
For the experiment of Half-Cheetah with goal velocities, we set Î»= 0.5forAlg(1),Î»= 0.4for
Alg(2). For the experiment of Half-Cheetah with goal directions, we set Î»= 0.5forAlg(1),Î»= 0.5
forAlg(2). For the experiment of Ant with goal velocities, we set Î»= 0.5forAlg(1),Î»= 0.5for
Alg(2). For the experiment of Ant with goal directions, we set Î»= 0.5forAlg(1),Î»= 0.5for
Alg(2).
Comparison setting. We compare the proposed algorithm with several optimization-based meta-RL
algorithms, including MAML, E-MAML [ 55], and ProMP [ 50]. The experiment results of E-MAML,
ProMP, and MAML-TRPO come from [ 67,15]. We do not compare the proposed algorithm with
black-box meta-RL algorithms, as they are based on the task context and even can achieve good
performance without adaptation.
Supplemental results. Figure 5 shows that the proposed algorithm with both within-task algorithms
Alg(i)outperform the baseline methods in four experimental settings. The accumulated rewards
of proposed algorithms increase fast and stop at points with better performance than the baseline
methods.
0 60 120 180 240 300 360 420 480
Number of meta-training iterations-250-225-200-175-150-125-100-75Accumulated rewardHalf-cheetah, goal velocity
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
0 60 120 180 240 300 360 420 480
Number of meta-training iterations0100200300400500600Accumulated rewardHalf-cheetah, goal direction
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
0 60 120 180 240 300 360 420 480
Number of meta-training iterations-50-250255075100125Accumulated rewardAnt, goal velocity
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
0 50 100 150 200 250 300 350 400
Number of meta-training iterations100200300400500600Accumulated rewardAnt, goal direction
BO-MRL with îˆ­(1)
BO-MRL with îˆ­(2)
BO-MRL with îˆ­(3)
MAML-TRPO
Figure 5: Accumulated rewards during the meta-training under the practical algorithm of BO-MRL on the
locomotion tasks.
Algorithm supplement
C Computation of âˆ‡Ï•QÏ€Ï•
Ï„(s, a)
In the computation of meta-objective shown in Propositions 1 and 2, we need to compute âˆ‡Ï•QÏ€Ï•Ï„(s, a)
âˆ‡Ï•QÏ€Ï•Ï„(s, a) =Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ï€Ï•
âˆ‡Ï•lnÏ€Ï•(aâ€²|sâ€²)QÏ€Ï•Ï„(sâ€², aâ€²)
.
where the state-action visitation probability Ïƒ(s,a)
Ï„,Ï€Î¸initialized at (s, a)âˆˆ S Ã— A is defined by
Ïƒ(s,a)
Ï„,Ï€Ï•(sâ€², aâ€²) = (1 âˆ’Î³)âˆžX
t=0Î³tP(st=sâ€², at=aâ€²|Ï€Ï•, s0âˆ¼PÏ„(Â·|s, a)).
16For the tabular softmax policy in discrete state-action space shown in Section 5.1,
âˆ‡Ï•(sâ€²,Â·)QË†Ï€Ï•Ï„(s, a) =Î³
1âˆ’Î³Â·Ïƒ(s,a)
Ï„,Ë†Ï€Ï•(sâ€²)Â·Ë†Ï€Ï•(Â·|sâ€²)âŠ™AË†Ï€Ï•Ï„(sâ€²,Â·), (4)
where âŠ™is the element-wise product, Ï•(sâ€²,Â·)is the vector which includes Ï•(sâ€², aâ€²)for all aâ€²âˆˆ A
as the elements, and AË†Ï€Ï•Ï„(s,Â·)is the vector which includes AË†Ï€Ï•Ï„(s, a)for all aâˆˆ A as the elements.
Equivalently,
âˆ‡Ï•(sâ€²,aâ€²)QË†Ï€Ï•Ï„(s, a) =Î³
1âˆ’Î³Â·Ïƒ(s,a)
Ï„,Ë†Ï€Ï•(sâ€²)Ë†Ï€Ï•(aâ€²|sâ€²)AË†Ï€Ï•Ï„(sâ€², aâ€²). (5)
For the softmax policy with the function approximation,
âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) =Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ë†Ï€Ï•âˆ‡Ï•Ë†Ï€Ï•(aâ€²|sâ€²)
Ë†Ï€Ï•(aâ€²|sâ€²)QË†Ï€Ï•Ï„(sâ€², aâ€²)
=Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ë†Ï€Ï•h
âˆ‡Ï•fÏ•(sâ€², aâ€²)QË†Ï€Ï•Ï„(sâ€², aâ€²)i
=Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ë†Ï€Ï•h
âˆ‡Ï•fÏ•(sâ€², aâ€²)AË†Ï€Ï•Ï„(sâ€², aâ€²)i(6)
Proof. As shown in [60],
âˆ‡Ï•QÏ€Ï•Ï„(s, a) =âˆ‡Ï• 
(1âˆ’Î³)Â·rÏ„(s, a) +Î³Â·Esâ€²âˆ¼PÏ„(Â·|s,a)
VÏ€Ï•Ï„(sâ€²)
=Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ï€Ï•
âˆ‡Ï•lnÏ€Ï•(aâ€²|sâ€²)Â·QÏ€Ï•Ï„(sâ€², aâ€²)
=Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ï€Ï•
âˆ‡Ï•lnÏ€Ï•(aâ€²|sâ€²)Â·AÏ€Ï•Ï„(sâ€², aâ€²)
.
=Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ï€Ï•âˆ‡Ï•Ï€Ï•(aâ€²|sâ€²)
Ï€Ï•(aâ€²|sâ€²)Â·AÏ€Ï•Ï„(sâ€², aâ€²)
.
By Lemma 4, from (12), we can obtain (4); from (14), we can obtain (6).
D Practical algorithm
In Sections 4 and 5, we develop a theoretically guaranteed algorithm with Assumptions 1, 2, and 3.
In this section, we develop a practical instantiation of Algorithm 1 and evaluate its performance in
high-dimensional experiments in Section 6.
Algorithm 2 states the practical algorithm of Algorithm 1. Compared with Algorithm 1, Algorithm 2
considers and overcomes the following limitations of Algorithm 1: (a) evaluating the exact expectation
in (1) and (2) is costly and the approximation error could influence the task-specific policy adaptation
if using sampling, especially in the meta-RL problem where the sampling data is limited; (b) the
optimization problems in (1) and (2) have no closed-form solution; (c) the computation of the gradients
of the meta-objectives shown in Propositions 1 and 2 is time-consuming; (d) the gradient-based
approach to optimize the meta-objective is not stable in RL problems.
In the beginning of Algorithm 2, we first sample a batch of tasks {Ï„i}N
i=1âˆ¼P(Î“). On each task Ï„i,
we sample the trajectories of the meta-policy Ï€Ï•tasBÏ„i, and evaluate the state-action value function
QÏ€Ï•tÏ„i(Â·,Â·)for each Ï„i. Next, since the number of the sampling state-action pairs in BÏ„iis limited, if
we directly use the sampling average to approximate the expectation in (2), the approximation error
will be very large when Ï€Ï•(a|s)is small. Therefore, we solve the following optimization problem as
the within-task algorithm instead of (2):
Ï€Î¸â€²Ï„=Alg(Î», Ï•t, Ï„) = arg min
Î¸1
|BÏ„|X
(a,s)âˆˆBÏ„hÏ€Î¸(a|s)
Ï€Ï•(a|s)
QÏ€Ï•Ï„(s, a)âˆ’Î»D2
Ï„(Ï€Ï•, Ï€Î¸),(7)
where h(x) =2
1+eâˆ’2(xâˆ’1). The function havoids the termÏ€Î¸(a|s)
Ï€Ï•(a|s)is optimized to very large. We
use Adam [ 32] to solve the problem in (7). Next, the computation of the gradients of the meta-
objectives shown in Proposition 2 is time-consuming, since the computation complexity of the term
17Algorithm 2 Practical Algorithm of BO-MRL
Require: Regularization weight Î» >0; initial meta-parameter Ï•0; learning rate Î±.
1:fort= 1,Â·Â·Â·, Tdo
2: Sample a batch of tasks {Ï„i}N
i=1âˆ¼P(Î“)with the MDP MÏ„ii.i.d.
3: On each task Ï„i, sample the trajectories of the meta-policy Ï€Ï•tasBÏ„i.
4: Evaluate the state-action value function QÏ€Ï•tÏ„i(Â·,Â·)for each Ï„i.
5: For each task Ï„i, compute the task-specific policy Ï€Î¸â€²Ï„iby solving Alg(Î», Ï•t, Ï„i)defined in (7)
by Adam.
6: Compute âˆ‡Ï•JÏ„i(Ï€Î¸â€²Ï„i)in (8) by conjugate gradient method
7: Update meta-parameter by the TRPO with the gradient1
NP
iâˆ‡Ï•JÏ„i(Ï€Î¸â€²Ï„i)and the sampling
trajectories {BÏ„i}N
i=1.
8:end for
9:Return Ï•T
âˆ’âˆ‡Î¸Ï€Î¸(a|s)
Î»Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a)is very high. So, we omit the term, and compute âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)as
1
1âˆ’Î³âˆ‡Ï•Î¸â€²
Ï„Â·E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„
aâˆ¼Ï€Î¸â€²Ï„(Â·|s)âˆ‡Î¸â€²Ï„Ï€Î¸â€²Ï„(a|s)
Ï€Î¸â€²Ï„(a|s)QÏ€Î¸â€²Ï„Ï„(s, a)
,(8)
where
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„â‰ˆ âˆ’ E
sâˆ¼Î½Ï€Ï•
Ï„
aâˆ¼Ï€Ï•(Â·|s)
âˆ‡2
Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))âˆ’âˆ‡2
Î¸Ï€Î¸(a|s)
Î»Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)âˆ’1
E
sâˆ¼Î½Ï€Ï•
Ï„
aâˆ¼Ï€Ï•(Â·|s)
âˆ‡âŠ¤
Ï•âˆ‡Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
|Î¸=Î¸â€²Ï„.
Finally, since the gradient-based approach is not stable in RL problems, we optimize meta-parameter
by the TRPO with the gradient1
NP
iâˆ‡Ï•JÏ„i(Ï€Î¸â€²
Ï„i)and the sampling trajectories {BÏ„i}N
i=1, similar
to [15].
E Discussion about computational complexity of hyper-gradient
In Algorithms 1 and 2, we compute the inverse of the Hessian matrix when computing the hyper-
gradient by Proposition 2 and (8). The computation of the inverse of the Hessian matrix is not
time-consuming and does not increase the processing time much. Here are the two reasons.
First, we apply the conjugate gradient algorithm to compute the inverse of the Hessian and its
computation complexity is not high. According to our experiment of Half-cheetah, the computation
time of the hyper-gradient with the inverse of Hessian for a three-layer neural network is about 0.3
second in each meta-parameter update, where we use only the CPU to compute the hyper-gradient.
This approach has demonstrated high efficiency across a wide range of applications, including several
widely used RL algorithms, such as TRPO [ 51] and CPO [ 1], which compute the inverse of the
Hessian in each policy update iteration. The detail is shown in Appendix C of [ 51]. They usually
compute thousands times of the Hessian inverse for a single RL task. In the simplest meta-RL method,
MAML [ 15], the authors use the TRPO to update the meta-parameter, as shown in Section 5.3 of
[15], the inverse of the Hessian is also computed. Therefore, the computational complexity of the
hyper-gradient in our proposed method is comparable to many existing RL and meta-RL approaches,
which are shown efficient.
Second, the biggest computational bottleneck in the meta-RL framework is not the hyper-gradient
computation. According to our experiment, the percentage of the computation time in the meta-
parameter update, including the computation time of the hyper-gradient computation, is less than
5%, where we use only the CPU to compute the hyper-gradient. The percentage of computation
time in the data collection and the Q value computation by Monte-Carlo sampling is more than 70%,
although the state-action data points are collected in the MDP simulator Gym and the data collection
is very fast. In real-world applications, the state-action data points are even harder to collect and
18data collection consumes a longer time. Therefore, the computational time of the hyper-gradient
computation has a relatively small impact on the mete-RL framework.
FData sampling complexity and computational complexity of one-time policy
adaptation
The one-time policy adaptation in our algorithm is defined as solving the optimal solution of the
optimization problem in (1)or(2)by multiple optimization iterations. The definition of the one-time
policy adaptation follows many widely used RL algorithms, such as TRPO [ 51] and CPO [ 1], which
evaluate the Q-values for the current policy and solve the optimal solution for an optimization
problem to obtain the next policy in each policy optimization iteration. For example, TRPO solves
the optimization problem in (14) of [51] in each iteration.
In the one-time policy adaptation, we only need to evaluate the Q-function for one policy Ï€Ï•by
Monte-Carlo sampling, which requires the agent to explore the MDP using one policy Ï€Ï•, then solve
the optimization problem in (1)or(2)by multiple optimization iterations with the fixed Q-function.
The data sampling complexity is exactly the same as the one-step gradient descent in MAML, which
uses Monte-Carlo sampling to evaluate the Q-function and compute the policy gradient based on the
Q-function.
The multiple optimization steps in the one-time policy adaptation are different from the multi-step
policy gradient update in MAML. In our algorithm, the multiple optimization steps in a one-time
policy adaptation only need to evaluate the Q-function for one policy Ï€Ï•, which requires the agent to
explore the MDP using only Ï€Ï•. In MAML, the Q-function for a new policy needs to be evaluated
in each policy gradient update, and then multiple Q-functions are evaluated for multiple policies,
which requires the agent to explore the MDP using multiple policies. Instead, the one-time policy
adaptation in our algorithm corresponds to a one-step policy gradient update in MAML, as they use
the same number of data points.
Moreover, we would like to claim that the computation complexity for the one-time policy adaptation
in our algorithm and that of the one-step policy gradient update in MAML is comparable, although our
algorithm requires multiple optimization iterations. As mentioned in Appendix E, the computation
time in the data collection and the Q value computation takes more than 70% of total computation
time, which is much longer than other parts of the algorithm, including the multiple optimization
iterations in police adaptation ( 15% of total computation time). This happens although the state-action
data points are collected in the MDP simulator Gym and the data collection is very fast. In real-world
applications, the state-action data points are even harder to collect and the consuming time of data
collection is much longer. Therefore, the computational time of the multiple optimization iterations
has a relatively small impact on the mete-RL framework. Therefore, the computation time of our
algorithm and that of MAML is comparable.
From the statement in the above paragraphs, both the data sampling complexity and computational
complexity of the one-time policy adaptation in our algorithm and the one-step policy gradient update
in MAML are similar. Thus, we define solving the optimal solution of the optimization problem in
(1) or (2) as a single policy adaptation step.
G Algorithm details with the first-order approximation
As we mentioned in Section 4, we can approximate the first term Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)]
in (2) by its first-order approximation as the within-task algorithm, similar to the implementations
in TRPO [ 51] and PPO [ 52]. In particular, the within-task algorithm is reduced to the following
formulation,
Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„)â‰œargmin
Ï€Î¸âˆ’1
Î»G(Ï•)âŠ¤Î¸+D2
Ï„(Ï€Ï•, Ï€Î¸). (9)
Here, we use the first-order approximation to replace the first term of (2). In particular,
G(Ï•)âŠ¤(Î¸âˆ’Ï•)is the first order approximation of Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)], where
G(Ï•) =âˆ‡Î¸Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Î¸Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)]|Î¸=Ï•=Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)].
Under the simplified within-task algorithm Alg, the hypergradient of the meta-objective function
19âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)can be computed by
âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³âˆ‡Ï•Î¸â€²
Ï„Â·E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„
aâˆ¼Ï€Î¸â€²Ï„(Â·|s)âˆ‡Î¸â€²Ï„Ï€Î¸â€²Ï„(a|s)
Ï€Î¸â€²Ï„(a|s)QÏ€Î¸â€²Ï„Ï„(s, a)
,
where
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„=âˆ‡2
Î¸â€²
Ï„D2
Ï„(Ï€Ï•, Ï€Î¸â€²Ï„)âˆ’1( E
sâˆ¼Î½Ï€Ï•
Ï„aâˆ¼Ï€Ï•(Â·|s)[1
Î»âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a)+
1
Î»âˆ‡2
Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)]âˆ’ âˆ‡âŠ¤
Ï•âˆ‡Î¸â€²Ï„D2
Ï„(Ï€Ï•, Ï€Î¸â€²Ï„)).(10)
The computation of âˆ‡âŠ¤
Ï•Î¸â€²
Ï„is derived in Section J.3.
H Connection between the proposed algorithm and MAML
As we claim in Section 4, when we approximate the first term Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)]
in (2) by its first-order approximation and also select DÏ„=DÏ„,3, the within-task algorithm (2)
is reduced to the policy gradient ascent. In particular, the term Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)]
is approximated by (Î¸âˆ’Ï•)âŠ¤Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)], then the within-task algorithm
Alg(Ï€Ï•, Î», Ï„)becomes to
Î¸â€²
Ï„=Alg(Ï€Ï•, Î», Ï„)â‰œargmax
Î¸âˆ’Î»âˆ¥Î¸âˆ’Ï•âˆ¥2+Î¸âŠ¤Â·E
sâˆ¼Î½Ï€Ï•
Ï„
aâˆ¼Ï€Ï•(Â·|s)âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)
.(11)
Solve the optimization problem, we have
Î¸â€²
Ï„=Ï•+1
Î»E
sâˆ¼Î½Ï€Ï•
Ï„
aâˆ¼Ï€Ï•(Â·|s)âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)
=Ï•+1âˆ’Î³
Î»âˆ‡Ï•JÏ„(Ï•),
which is policy gradient ascent. Thus, when we select (11) as the within-task algorithm, the meta-
algorithm (3) is reduced to the algorithm that can learn the initialization parameter for the policy
gradient ascent.
As shown in [ 15], MAML also learns the initialization parameter Ï•for the policy gradient ascent.
However, MAML ignores that the sampled trajectories with policy Ï€Ï•also depend on Ï•. Specifically,
MAML first uses the sampled trajectories to approximate QÏ€Ï•Ï„(s, a)by (Monte Carlo sampling on
the REINFORCE algorithm), then computes the policy gradient and does one step of gradient ascent
for the task-specific adaptation. Next, it computes âˆ‡Ï•JÏ„(Î¸â€²
Ï„)to update the meta-parameter Ï•. When
it computes âˆ‡Ï•Î¸â€²
Ï„, it treats QÏ€Ï•Ï„(s, a)as a given data point that is independent with Ï•, and then
ignore the âˆ‡Ï•QÏ€Ï•Ï„(s, a). In contrast, our reduced meta-algorithm takes it into account and provides a
precise formulation to learn the meta-initialization for the policy gradient algorithm.
Since the proposed meta-RL framework can include MAML as a special case, our analysis in Section
5 also provides the theoretical motivation for MAML.
Analysis and Proof
I Auxiliary Results
Lemma 3 (Policy gradient [ 56,2]).LetÏ€Î¸be the parameterized policy with the parameter Î¸. It
holds that
âˆ‡Î¸JÏ„(Ï€Î¸) =1
1âˆ’Î³Esâˆ¼Î½Ï€Î¸Ï„,aâˆ¼Ï€Î¸(Â·|s)[âˆ‡Î¸lnÏ€Î¸(a|s)QÏ€Î¸Ï„(s, a)]
=1
1âˆ’Î³Esâˆ¼Î½Ï€Î¸Ï„,aâˆ¼Ï€Î¸(Â·|s)[âˆ‡Î¸lnÏ€Î¸(a|s)AÏ€Î¸Ï„(s, a)].
20Lemma 4 (Policy gradient of the softmax policy) .Consider the softmax policy Ë†Ï€Î¸parameterized by
Î¸. For a discrete state-action space and the tabular policy, Ë†Ï€Î¸(a|s) =exp(Î¸(s,a))P
aâ€²âˆˆAexp(Î¸(s,aâ€²)),âˆ€(s, a)âˆˆ
S Ã— A . It holds that
âˆ‡Î¸(s,Â·)JÏ„(Ë†Ï€Î¸) =1
1âˆ’Î³Î½Ë†Ï€Î¸Ï„(s)Â·Ë†Ï€Î¸(Â·|s)âŠ™AË†Ï€Î¸Ï„(s,Â·), (12)
whereâŠ™is the element-wise product, Î¸(s,Â·)is the vector which includes Î¸(s, a)for all aâˆˆ A as the
elements, AË†Ï€Î¸Ï„(s,Â·)is the vector which includes AË†Ï€Î¸Ï„(s, a)for all aâˆˆ A as the elements. Equivalently,
âˆ‡Î¸(s,a)JÏ„(Ë†Ï€Î¸) =1
1âˆ’Î³Î½Ë†Ï€Î¸Ï„(s)Ë†Ï€Î¸(a|s)AË†Ï€Î¸Ï„(s, a), (13)
For the softmax policy with function approximation, the policy Ï€Î¸is defined by Ï€Î¸(a|s) =
exp(fÎ¸(s,a))R
Aexp(fÎ¸(s,aâ€²))daâ€²,âˆ€(s, a)âˆˆ S Ã— A . It holds that
âˆ‡Î¸JÏ„(Ë†Ï€Î¸) =1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸(Â·|s)
âˆ‡Î¸fÎ¸(s, a)AË†Ï€Î¸Ï„(s, a)
. (14)
Proof. For the discrete state-action space and the tabular policy, (12) is shown in Lemma C.1 of [ 2].
For the softmax policy with function approximation, from Lemma 3, we have
âˆ‡Î¸JÏ„(Ë†Ï€Î¸) =1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸(Â·|s)
âˆ‡Î¸ln Ë†Ï€Î¸(a|s)AË†Ï€Î¸Ï„(s, a)
=1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸(Â·|s)
âˆ‡Î¸lnexp(fÎ¸(s, a))R
Aexp(fÎ¸(s, aâ€²))daâ€²
AË†Ï€Î¸Ï„(s, a)
=1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸(Â·|s)
âˆ‡Î¸fÎ¸(s, a)âˆ’ âˆ‡ Î¸lnZ
Aexp(fÎ¸(s, aâ€²))daâ€²
AË†Ï€Î¸Ï„(s, a)
Here,âˆ‡Î¸ln R
Aexp(fÎ¸(s, aâ€²))daâ€²
is independent with a, thenâˆ‡Î¸JÏ„(Ë†Ï€Î¸)
=1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸(Â·|s)
âˆ‡Î¸fÎ¸(s, a)âˆ’ âˆ‡ Î¸lnZ
Aexp(fÎ¸(s, aâ€²))daâ€²
AË†Ï€Î¸Ï„(s, a)
=1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸(Â·|s)
âˆ‡Î¸fÎ¸(s, a)AË†Ï€Î¸Ï„(s, a)
âˆ’
1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„
âˆ‡Î¸lnZ
Aexp(fÎ¸(s, aâ€²))daâ€²
Eaâˆ¼Ë†Ï€Î¸(Â·|s)AË†Ï€Î¸Ï„(s, a)
.
SinceEaâˆ¼Ë†Ï€Î¸(Â·|s)AË†Ï€Î¸Ï„(s, a) =Eaâˆ¼Ë†Ï€Î¸(Â·|s)[QË†Ï€Î¸Ï„(s, a)]âˆ’VË†Ï€Î¸Ï„(s) = 0 . Then,
âˆ‡Î¸JÏ„(Ë†Ï€Î¸) =1
1âˆ’Î³Esâˆ¼Î½Ë†Ï€Î¸Ï„,aâˆ¼Ë†Ï€Î¸(Â·|s)
âˆ‡Î¸fÎ¸(s, a)AË†Ï€Î¸Ï„(s, a)
.
J Proofs of the computation of hypergradient
J.1 Proofs of Propositions 1
Proofs of Propositions 1. Consider the within-task algorithm in discrete space:
Alg(Ï€Ï•, Î», Ï„) = argmax
Ï€Esâˆ¼Î½Ï€Ï•
Ï„"X
aâˆˆAÏ€(a|s)QÏ€Ï•Ï„(s, a)#
âˆ’Î»D2
Ï„(Ï€Ï•, Ï€)
= argmax
Ï€Esâˆ¼Î½Ï€Ï•
Ï„"X
aâˆˆAÏ€(a|s)QÏ€Ï•Ï„(s, a)âˆ’Î»d2(Ï€Ï•(Â·|s), Ï€(Â·|s))#
.
Here, dcan be selected from d1tod3defined in Section 3, corresponding to the selection of DÏ„from
DÏ„,1toDÏ„,3.
21The above optimization problem is formally defined by the following problem,
Alg(Ï€Ï•, Î», Ï„) = argmax
Ï€Esâˆ¼Î½Ï€Ï•
Ï„"X
aâˆˆAÏ€(a|s)QÏ€Ï•Ï„(s, a)âˆ’Î»d2(Ï€Ï•(Â·|s), Ï€(Â·|s), s)#
,
subject toX
aâˆˆAÏ€(a|s) = 1 ,for any sâˆˆ S.(15)
With Assumption 2, the problem is equivalent to that, for any sâˆˆ S,
Alg(Ï€Ï•, Î», Ï„)(Â·|s) = argmax
Ï€(Â·|s)X
aâˆˆAÏ€(a|s)QÏ€Ï•Ï„(s, a)âˆ’Î»d2(Ï€Ï•(Â·|s), Ï€(Â·|s)),
subject toX
aâˆˆAÏ€(a|s) = 1 .(16)
Consider a sâˆˆ S, the Lagrangian of the above maximization problem is
âˆ’X
aâˆˆAÏ€(a|s)QÏ€Ï•Ï„(s, a) +Î»d2(Ï€Ï•(Â·|s), Ï€(Â·|s)) +Âµ(X
aâˆˆAÏ€(a|s)âˆ’1),
where Âµis the Lagrangian multiplier. The optimality condition of Ï€(Â·|s)is that,
âˆ’QÏ€Ï•Ï„(s,Â·) +Î»âˆ‡Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s)) +Âµ[1,Â·Â·Â·,1]âŠ¤= 0.
Here, QÏ€Ï•Ï„(s,Â·)denotes a vector include QÏ€Ï•Ï„(s, a)for each aâˆˆ A, and Ï€(Â·|s)denotes a vector
include Ï€(a|s)for each aâˆˆ A.
Then, we have
âˆ’QÏ€Ï•Ï„(s,Â·) +Î»âˆ‡Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))|Ï€=Alg(Ï€Ï•,Î»,Ï„)+Âµ[1,Â·Â·Â·,1]âŠ¤= 0. (17)
Note that the optimization problem (15) depends on Ï•, andÏ€=Alg(Ï€Ï•, Î», Ï„)is a function of Ï•, we
have
âˆ’QÏ€Ï•Ï„(s,Â·) +Î»âˆ‡Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))|Ï€=Alg(Ï€Ï•,Î»,Ï„)+Âµ(Ï•)[1,Â·Â·Â·,1]âŠ¤= 0,
i.e.,Âµis a function of Ï•.
Also, we have
Âµ(Ï•)(X
aâˆˆAAlg(Ï€Ï•, Î», Ï„)(a|s)âˆ’1) = 0 . (18)
With (17) and (18), we can compute âˆ‡Ï•Alg(Ï€Ï•, Î», Ï„), where Alg(Ï€Ï•, Î», Ï„)is continuously dif-
ferentiable as shown in [ 64]. We do derivative of (17) and (18) with respect to Ï•, we have
[âˆ‡Ï•Alg(Ï€Ï•, Î», Ï„),âˆ‡Ï•Âµ(Ï•)]âŠ¤=
âˆ’Î»âˆ‡2
Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))1
1âŠ¤0âˆ’1
âˆ’âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s,Â·) +Î»âˆ‡âŠ¤
Ï•âˆ‡Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))
0
where Ï€=Alg(Ï€Ï•, Î», Ï„).
Solve the equation, we have
âˆ‡âŠ¤
Ï•Alg(Ï€Ï•, Î», Ï„)(Â·|s) =
M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11
 
âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s,Â·)âˆ’Î»âˆ‡âŠ¤
Ï•âˆ‡Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))
,(19)
where M(s) =Î»âˆ‡2
Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s)). It is easy to show that âˆ‡2
Ï€(Â·|s)d2(Ï€Ï•(Â·|s), Ï€(Â·|s))is
non-singular for any Ï•for any selected d=d1,d=d2, ord=d3.
22From the policy gradient theorem in Lemma 3,
âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„,aâˆ¼Ï€Î¸â€²Ï„(Â·|s)[âˆ‡Ï•lnÏ€Î¸â€²Ï„(a|s)AÏ€Î¸â€²Ï„Ï„(s, a)]|Ï€Î¸â€²Ï„=Alg(Ï€Ï•,Î»,Ï„)
=1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„,aâˆ¼Ï€Î¸â€²Ï„(Â·|s)âˆ‡Ï•Ï€Î¸â€²Ï„(a|s)
Ï€Î¸â€²Ï„(a|s)AÏ€Î¸â€²Ï„Ï„(s, a)
|Ï€Î¸â€²Ï„=Alg(Ï€Ï•,Î»,Ï„),
=1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„"X
aâˆˆAâˆ‡Ï•Ï€Î¸â€²Ï„(a|s)AÏ€Î¸â€²Ï„Ï„(s, a)#
|Ï€Î¸â€²Ï„=Alg(Ï€Ï•,Î»,Ï„).
where âˆ‡Ï•Ï€Î¸â€²Ï„(Â·|s) =âˆ‡Ï•Alg(Ï€Ï•, Î», Ï„)(Â·|s)is shown in (19).
J.2 Proofs of Propositions 2
Proofs of Propositions 2. First, we have
âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =âˆ‡Ï•Î¸â€²
Ï„âˆ‡Î¸â€²Ï„JÏ„(Ï€Î¸â€²Ï„)
From the policy gradient theorem in Lemma 3,
âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³âˆ‡Ï•Î¸â€²
Ï„âŠ¤E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„,aâˆ¼Ï€Î¸â€²Ï„(Â·|s)h
âˆ‡Î¸â€²Ï„lnÏ€Î¸â€²Ï„(a|s)AÏ€Î¸â€²Ï„Ï„(s, a)i
|Î¸â€²Ï„=Alg(Ï€Ï•,Î»,Ï„).
We have
âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³âˆ‡Ï•Î¸â€²
Ï„âŠ¤E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„,aâˆ¼Ï€Î¸â€²Ï„(Â·|s)âˆ‡Î¸â€²Ï„Ï€Î¸â€²Ï„(a|s)
Ï€Î¸â€²Ï„(a|s)AÏ€Î¸â€²Ï„Ï„(s, a)
|Î¸â€²Ï„=Alg(Ï€Ï•,Î»,Ï„).
Next, we compute âˆ‡Ï•Î¸â€²
Ï„, where
Î¸â€²
Ï„=Alg(Ï€Ï•, Î», Ï„)â‰œargmax
Î¸Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)
âˆ’Î»D2
Ï„(Ï€Ï•, Ï€Î¸).
The optimization problem is equivalent to
Î¸â€²
Ï„â‰œargmax
Î¸Esâˆ¼Î½Ï€Ï•
Ï„Z
AÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)daâˆ’Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
= argmin
Î¸Esâˆ¼Î½Ï€Ï•
Ï„
âˆ’Z
AÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
= argmin
Î¸X
sâˆˆSÎ½Ï€Ï•Ï„(s)
âˆ’Z
AÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
.
Similar to the derivation from (15) to (16) with Assumption 2, we have that, when Î¸=Alg(Ï€Ï•, Î», Ï„),
âˆ‡Î¸
âˆ’Z
AÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
= 0.
Then, we have
X
sâˆˆSâˆ‡Ï•Î½Ï€Ï•Ï„(s)âˆ‡Î¸
âˆ’Z
AÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
= 0.
By using implicit differentiation, if the matrix Esâˆ¼Î½Ï€Ï•
Ï„
âˆ’R
Aâˆ‡2
Î¸Ï€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»âˆ‡2
Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
is invertible, i.e., Esâˆ¼Î½Ï€Ï•
Ï„
âˆ’R
AÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
is strongly convex at
Î¸=Î¸â€²
Ï„, we have
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„=âˆ’
Esâˆ¼Î½Ï€Ï•
Ï„
âˆ’Z
Aâˆ‡2
Î¸Ï€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»âˆ‡2
Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))âˆ’1

âˆ’Esâˆ¼Î½Ï€Ï•
Ï„Z
Aâˆ‡Î¸Ï€Î¸(a|s)âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a)da
+Esâˆ¼Î½Ï€Ï•
Ï„
Î»âˆ‡âŠ¤
Ï•âˆ‡Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
+
X
sâˆˆSâˆ‡Ï•Î½Ï€Ï•Ï„(s)âˆ‡Î¸
âˆ’Z
AÏ€Î¸(a|s)QÏ€Ï•Ï„(s, a)da+Î»d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))!
|Î¸=Î¸â€²Ï„,
23This is equivalent to
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„=âˆ’
Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)
âˆ’âˆ‡2
Î¸Ï€Î¸(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a) +Î»âˆ‡2
Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))âˆ’1
Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)
âˆ’âˆ‡Î¸Ï€Î¸(a|s)
Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a) +Î»âˆ‡âŠ¤
Ï•âˆ‡Î¸d2(Ï€Ï•(Â·|s), Ï€Î¸(Â·|s))
|Î¸=Î¸â€²Ï„.
J.3 Proofs of hypergradient of the algorithm in Section G
Deviation of (10). AsÎ¸â€²
Ï„= argmin
Î¸âˆ’1
Î»Î¸âŠ¤Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)AÏ€Ï•Ï„(s, a)] +D2
Ï„(Ï€Ï•, Ï€Î¸), by
the implicit differentiation theorem in bilevel optimization analysis,
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„=âˆ’âˆ‡2
Î¸
âˆ’1
Î»Î¸âŠ¤Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)] +D2
Ï„(Ï€Ï•, Ï€Î¸)âˆ’1
âˆ‡Ï•âˆ‡Î¸
âˆ’1
Î»Î¸âŠ¤Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)] +D2
Ï„(Ï€Ï•, Ï€Î¸)
|Î¸=Î¸â€²Ï„
Also, we have
âˆ‡2
Î¸(1
Î»Î¸âŠ¤Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)]) = 0 ,
and
âˆ‡Ï•âˆ‡Î¸(1
Î»Î¸âŠ¤Esâˆ¼Î½Ï€Ï•
Ï„,aâˆ¼Ï€Ï•(Â·|s)[âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)])
= E
sâˆ¼Î½Ï€Ï•
Ï„aâˆ¼Ï€Ï•(Â·|s)[1
Î»âˆ‡Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a) +1
Î»âˆ‡2
Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s)QÏ€Ï•Ï„(s, a)].
Then, we can get âˆ‡âŠ¤
Ï•Î¸â€²
Ï„.
K Proofs of convergence when DÏ„=DÏ„,1
K.1 Gradients of âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)when DÏ„=DÏ„,1
From Proposition 1,
âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„"X
aâˆˆAâˆ‡Ï•Ï€Î¸â€²Ï„(a|s)AÏ€Î¸â€²Ï„Ï„(s, a)#
=1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„h
âˆ‡Ï•Ï€Î¸â€²Ï„(Â·|s)Â·AÏ€Î¸â€²Ï„Ï„(s,Â·)i
,(20)
where
âˆ‡âŠ¤
Ï•Ï€Î¸â€²Ï„(Â·|s) =
M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11 
âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s,Â·)âˆ’Î»âˆ‡âŠ¤
Ï•âˆ‡Ï€(Â·|s)d2
1(Ï€Ï•, Ï€, s)
|Ï€=Ï€Î¸â€²Ï„,
where
M(s) =Î»âˆ‡2
Ï€(Â·|s)d2
1(Ï€Ï•, Ï€, s) =Î»ï£®
ï£¯ï£¯ï£¯ï£°Ï€Ï•(a1|s)
Ï€Î¸â€²Ï„(a1|s)2
...
Ï€Ï•(an|s)
Ï€Î¸â€²Ï„(an|s)2ï£¹
ï£ºï£ºï£ºï£».
Then,
M(s)âˆ’1=1
Î»ï£®
ï£¯ï£¯ï£¯ï£°Ï€Î¸â€²Ï„(a1|s)2
Ï€Ï•(a1|s)
...
Ï€Î¸â€²Ï„(an|s)2
Ï€Ï•(an|s)ï£¹
ï£ºï£ºï£ºï£», (21)
24and
M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11=1
Î»P
aâˆˆAÏ€Î¸â€²Ï„(a|s)2
Ï€Ï•(a|s)ï£®
ï£¯ï£¯ï£¯ï£°Ï€Î¸â€²Ï„(a1|s)2
Ï€Ï•(a1|s)
...
Ï€Î¸â€²Ï„(an|s)2
Ï€Ï•(an|s)ï£¹
ï£ºï£ºï£ºï£»hÏ€Î¸â€²Ï„(a1|s)2
Ï€Ï•(a1|s)Â·Â·Â·Ï€Î¸â€²Ï„(an|s)2
Ï€Ï•(an|s)i
.
Also,
âˆ‡âŠ¤
Ï•âˆ‡Ï€(Â·|s)d2
1(Ï€Ï•, Ï€, s)|Ï€=Ï€Î¸â€²Ï„=âˆ‡âŠ¤
Ï•ï£®
ï£¯ï£¯ï£¯ï£°âˆ’Ï€Ï•(a1|s)
Ï€Î¸â€²Ï„(a1|s)
...
âˆ’Ï€Ï•(an|s)
Ï€Î¸â€²Ï„(an|s)ï£¹
ï£ºï£ºï£ºï£»=ï£®
ï£¯ï£¯ï£¯ï£°âˆ’âˆ‡âŠ¤
Ï•Ï€Ï•(a1|s)
Ï€Î¸â€²Ï„(a1|s)
...
âˆ’âˆ‡âŠ¤
Ï•Ï€Ï•(an|s)
Ï€Î¸â€²Ï„(an|s)ï£¹
ï£ºï£ºï£ºï£». (22)
Then, plugging these equations into (20), we have
âˆ‡âŠ¤
Ï•JÏ„(Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„h
AÏ€Î¸â€²Ï„Ï„(s,Â·)âŠ¤âˆ‡âŠ¤
Ï•Ï€Î¸â€²Ï„(Â·|s)i
,
=1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„ï£®
ï£¯ï£¯ï£¯ï£°AÏ€Î¸â€²Ï„Ï„(s,Â·)âŠ¤
M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11ï£®
ï£¯ï£¯ï£¯ï£°1
Î»âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a1) +âˆ‡âŠ¤
Ï•Ï€Ï•(a1|s)
Ï€Î¸â€²Ï„(a1|s)
...
1
Î»âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, an) +âˆ‡âŠ¤
Ï•Ï€Ï•(an|s)
Ï€Î¸â€²Ï„(an|s)ï£¹
ï£ºï£ºï£ºï£»ï£¹
ï£ºï£ºï£ºï£»
=1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„hh
(AÏ€Î¸â€²Ï„Ï„(s, a1)âˆ’cÏ„(s))Ï€Î¸â€²Ï„(a1|s)2
Ï€Ï•(a1|s)Â·Â·Â· (AÏ€Î¸â€²Ï„Ï„(s, an)âˆ’cÏ„(s))Ï€Î¸â€²Ï„(an|s)2
Ï€Ï•(an|s)i
ï£®
ï£¯ï£¯ï£¯ï£°1
Î»âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a1) +âˆ‡âŠ¤
Ï•Ï€Ï•(a1|s)
Ï€Î¸â€²Ï„(a1|s)
...
1
Î»âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, an) +âˆ‡âŠ¤
Ï•Ï€Ï•(an|s)
Ï€Î¸â€²Ï„(an|s)ï£¹
ï£ºï£ºï£ºï£»ï£¹
ï£ºï£ºï£ºï£»,
where
cÏ„(s) =P
aâˆˆAAÏ€Î¸â€²Ï„Ï„(s, a)Ï€Î¸â€²Ï„(a|s)2
Ï€Ï•(a|s)
P
aâˆˆAÏ€Î¸â€²Ï„(a|s)2
Ï€Ï•(a|s). (23)
Then, we simplify the computation of âˆ‡âŠ¤
Ï•JÏ„(Ï€Î¸â€²Ï„), we have âˆ‡âŠ¤
Ï•JÏ„(Ï€Î¸â€²Ï„) =
1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„"X
aâˆˆA(AÏ€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))Ï€Î¸â€²Ï„(a|s)2
Ï€Ï•(a|s)(1
Î»âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a) +âˆ‡âŠ¤
Ï•Ï€Ï•(a|s)
Ï€Î¸â€²Ï„(a|s))#
=1
1âˆ’Î³E
sâˆ¼Î½Ï€Î¸â€²Ï„Ï„"X
aâˆˆAÏ€Î¸â€²
Ï„(a|s)(AÏ€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))(Ï€Î¸â€²Ï„(a|s)
Î»Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QÏ€Ï•Ï„(s, a) +âˆ‡âŠ¤
Ï•Ï€Ï•(a|s)
Ï€Ï•(a|s))#
.
When the tabular policy is the softmax policy, we have Ë†Ï€Ï•(a|s) =exp(Ï•(s,a))P
aâ€²âˆˆAexp(Ï•(s,aâ€²)), then
âˆ‡âŠ¤
Ï•Ë†Ï€Ï•(a|s)
Ë†Ï€Ï•(a|s)=âˆ‡âŠ¤
Ï•ln Ë†Ï€Ï•(a|s) =âˆ‡âŠ¤
Ï•Ï•(s, a)âˆ’ âˆ‡âŠ¤
Ï•lnX
aâ€²âˆˆAexp (Ï•(s, aâ€²))
=1(s, a)âˆ’Ë†Ï€Ï•(Â·|s).(24)
Here, 1(sâ€², aâ€²)denote the column vector where the element is 1ifs=sâ€²anda=aâ€², otherwise is 0,
for each pair (s, a)âˆˆ S Ã— A ;Ë†Ï€Ï•(Â·|sâ€²)is the column vector, where the element is Ë†Ï€Ï•(a|sâ€²)ifs=sâ€²,
0ifsÌ¸=sâ€², for each pair (s.a)âˆˆ S Ã— A .
25So, we have
âˆ‡âŠ¤
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„"X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)(AË†Ï€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))
(Ë†Ï€Î¸â€²Ï„(a|s)
Î»Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +âˆ‡âŠ¤
Ï•Ë†Ï€Ï•(a|s)
Ë†Ï€Ï•(a|s))#
=1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„"X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)(AË†Ï€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))
(Ë†Ï€Î¸â€²Ï„(a|s)
Î»Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âŠ¤)
=1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„,aâˆ¼Ë†Ï€Î¸â€²Ï„h
(AË†Ï€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))
(Ë†Ï€Î¸â€²Ï„(a|s)
Î»Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âŠ¤)
.(25)
K.2 Convergence guarantee when DÏ„=DÏ„,1
K.2.1 Auxiliary lemmas
Lemma 5. Suppose that Assumption 2 holds. Let Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,1, for any
sâˆˆ S andaâˆˆ A, we have
Î»
Î»+ max s,a|AÏ€Ï•Ï„(s, a)|â‰¤Ï€Î¸â€²Ï„(a|s)
Ï€Ï•(a|s)â‰¤Î»
Î»âˆ’max s,a|AÏ€Ï•Ï„(s, a)|.
Proof. From (16), when DÏ„=DÏ„,1, we have Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„)and
Ï€Î¸â€²Ï„(Â·|s) = argmax
(Â·|s)X
aâˆˆAÏ€(a|s)QÏ€Ï•Ï„(s, a)âˆ’Î»d2
1(Ï€Ï•, Ï€, s),
subject toX
aâˆˆAÏ€(a|s) = 1 .
For any sâˆˆ S, the Lagrangian of the above maximization problem is
âˆ’X
aâˆˆAÏ€(a|s)QÏ€Ï•Ï„(s, a) +Î»d2(Ï€Ï•(Â·|s), Ï€(Â·|s)) +Âµ(s)(X
aâˆˆAÏ€(a|s)âˆ’1),
where Âµis the Lagrangian multiplier. The optimality condition of Ï€(Â·|s)is that,
âˆ’QÏ€Ï•Ï„(s,Â·) +Î»âˆ‡Ï€(Â·|s)d2
1(Ï€Ï•, Ï€, s) +Âµ(s)[1,Â·Â·Â·,1]âŠ¤= 0.
Solve the equation,
âˆ’QÏ€Ï•Ï„(s, a)âˆ’Î»Ï€Ï•(a|s)
Ï€Î¸â€²Ï„(a|s)+Âµ(s) = 0 .
LetÂµ1(s) =âˆ’VÏ€Ï•Ï„(s) +Âµ(s), we have
âˆ’AÏ€Ï•Ï„(s, a)âˆ’Î»Ï€Ï•(a|s)
Ï€Î¸â€²Ï„(a|s)+Âµ1(s) = 0 . (26)
Then,
âˆ’Ï€Î¸â€²Ï„(a|s)AÏ€Ï•Ï„(s, a)âˆ’Î»Ï€Ï•(a|s) +Ï€Î¸â€²Ï„(a|s)Âµ1(s) = 0 .
We derive the summation of all aâˆˆ A,
X
aâˆˆAâˆ’Ï€Î¸â€²Ï„(a|s)AÏ€Ï•Ï„(s, a)âˆ’Î»+Âµ1(s) = 0 .
26We have
Âµ1(s) =X
aâˆˆAÏ€Î¸â€²Ï„(a|s)AÏ€Ï•Ï„(s, a) +Î».
From (26), we have
Ï€Ï•(a|s)
Ï€Î¸â€²Ï„(a|s)=Âµ1(s)âˆ’AÏ€Ï•Ï„(s, a)
Î»
=Î»+P
aâ€²âˆˆAÏ€Î¸â€²Ï„(aâ€²|s)AÏ€Ï•Ï„(s, aâ€²)âˆ’AÏ€Ï•Ï„(s, a)
Î»
So, we have
Ï€Î¸â€²Ï„(a|s)
Ï€Ï•(a|s)=Î»
Î»+P
aâ€²âˆˆAÏ€Î¸â€²Ï„(aâ€²|s)AÏ€Ï•Ï„(s, aâ€²)âˆ’AÏ€Ï•Ï„(s, a),
then
Î»
Î»+ max s,a|AÏ€Ï•Ï„(s, a)|â‰¤Ï€Î¸â€²Ï„(a|s)
Ï€Ï•(a|s)â‰¤Î»
Î»âˆ’max s,a|AÏ€Ï•Ï„(s, a)|.
Lemma 6. Suppose that Assumption 2 holds. Let Ï€Î¸â€²Ï„=Alg(Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,1, we have
âˆ¥âˆ‡Ï•JÏ„(Ï€Î¸â€²Ï„)âˆ¥ â‰¤max s,a|AÏ€Î¸â€²Ï„Ï„(s, a)|
1âˆ’Î³(max s,a|AÏ€Î¸â€²Ï„Ï„(s, a)|
Î»âˆ’max s,a|AÏ€Ï•Ï„(s, a)|Î³
1âˆ’Î³+ 2).
Proof. As shown in (25),
âˆ‡âŠ¤
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„"X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)(AË†Ï€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))
(Ë†Ï€Î¸â€²Ï„(a|s)
Î»Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âŠ¤)
=1
1âˆ’Î³X
sâˆˆSX
aâˆˆAÎ½Ë†Ï€Î¸â€²Ï„Ï„(s)Ë†Ï€Î¸â€²Ï„(a|s)(AË†Ï€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))
(Ë†Ï€Î¸â€²Ï„(a|s)
Î»Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âŠ¤).
SinceP
sâˆˆSÎ½Ë†Ï€Î¸â€²Ï„Ï„(s) = 1 andP
aâˆˆAË†Ï€Î¸â€²Ï„(a|s) = 1 for all sâˆˆ S, we have âˆ¥âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤
1
1âˆ’Î³max
a,sâˆ¥(AË†Ï€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s))(Ë†Ï€Î¸â€²Ï„(a|s)
Î»Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âŠ¤)âˆ¥.
From (23), for any sâˆˆ S andaâˆˆ A, we have
|AË†Ï€Î¸â€²Ï„Ï„(s, a)âˆ’cÏ„(s)| â‰¤max s,a|AË†Ï€Î¸â€²Ï„Ï„(s, a)|.
Also, for any sâˆˆ S andaâˆˆ A,
âˆ¥1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))âˆ¥ â‰¤1 +âˆ¥Ë†Ï€(Â·|s)âˆ¥ â‰¤2.
From Lemma 5,
Ë†Ï€Î¸â€²Ï„(a|s)
Î»Ë†Ï€Ï•(a|s)â‰¤1
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|.
From the computation of âˆ‡Ï•QË†Ï€Ï•Ï„(s, a)shown in (5) of Appendix C,
|âˆ‡Ï•(sâ€²,aâ€²)QË†Ï€Ï•Ï„(s, a)|=|Î³
1âˆ’Î³Â·Ïƒ(s,a)
Ï„,Ë†Ï€Ï•(sâ€²)Ë†Ï€Ï•(aâ€²|sâ€²)AË†Ï€Ï•Ï„(s, a)|
â‰¤Î³
1âˆ’Î³Ïƒ(s,a)
Ï„,Ë†Ï€Ï•(sâ€²)Ë†Ï€Ï•(aâ€²|sâ€²)|max
a,sAË†Ï€Ï•Ï„(s, a)|.
27Also, sinceP
aâˆˆA,sâˆˆSÏƒ(s,a)
Ï„,Ë†Ï€Ï•(sâ€²)Ë†Ï€Ï•(aâ€²|sâ€²) = 1 , we have
âˆ¥âˆ‡Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤Î³
1âˆ’Î³max
a,s|AË†Ï€Ï•Ï„(s, a)|. (27)
Therefore, we have
âˆ¥âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤max s,a|AË†Ï€Î¸â€²Ï„Ï„(s, a)|
1âˆ’Î³(max s,a|AË†Ï€Î¸â€²Ï„Ï„(s, a)|
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|Î³
1âˆ’Î³+ 2).
Lemma 7. Suppose that Assumption 2 holds. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,1, for any
sâˆˆ S we have
X
aâˆˆAâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤1
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|(Î³max a,s|AË†Ï€Ï•Ï„(s, a)|
1âˆ’Î³+ 2(Î»+ max
a,s|AË†Ï€Ï•Ï„(s, a)|))
and
X
aâˆˆAâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤1
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|(8rmax
(1âˆ’Î³)3
+Î»+ max s,a|AË†Ï€Ï•Ï„(s, a)|
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|((2âˆ’Î³) max a,s|AË†Ï€Ï•Ï„(s, a)|
1âˆ’Î³+ 2Î»+ 2)) .
Proof. From (19), for any sâˆˆ S,
âˆ‡âŠ¤
Ï•Ë†Ï€Î¸â€²Ï„(Â·|s) =
M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11
âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s,Â·)âˆ’Î»âˆ‡âŠ¤
Ï•âˆ‡Ë†Ï€(Â·|s)d2
1(Ë†Ï€Ï•,Ë†Ï€, s)
|Ë†Ï€=Ë†Ï€Î¸â€²Ï„.
From the computations of M(s)âˆ’1,âˆ‡âŠ¤
Ï•âˆ‡Ë†Ï€(Â·|s)d2
1(Ë†Ï€Ï•,Ë†Ï€, s)|Ë†Ï€=Ë†Ï€Î¸â€²Ï„, andâˆ‡Ï•QË†Ï€Ï•Ï„(s,Â·)in (21) (27), we
have
M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11
jâ‰¤Ë†Ï€Î¸â€²Ï„(a|s)
Î»max
a,sË†Ï€Î¸â€²Ï„(a|s)
Ë†Ï€Ï•(a|s)â‰¤Ë†Ï€Î¸â€²Ï„(a|s)
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|,
and
âˆ¥âˆ‡Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤max
aâˆ¥âˆ‡Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤Î³
1âˆ’Î³max
a,s|AË†Ï€Ï•Ï„(s, a)|.
From (22)(24)
âˆ¥Î»âˆ‡âŠ¤
Ï•âˆ‡Ë†Ï€(a|s)d2
1(Ë†Ï€Ï•,Ë†Ï€, s)âˆ¥=âˆ¥Î»(1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))Ë†Ï€Ï•(a|s)
Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤2(Î»+ max
a,s|AË†Ï€Ï•Ï„(s, a)|).
The last inequality comes from Lemma 5. So, we have
âˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|(Î³max a,s|AË†Ï€Ï•Ï„(s, a)|
1âˆ’Î³+ 2(Î»+ max
a,s|AË†Ï€Ï•Ï„(s, a)|)).
Therefore,
X
aâˆˆAâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤1
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|(Î³max a,s|AË†Ï€Ï•Ï„(s, a)|
1âˆ’Î³+ 2(Î»+ max
a,s|AË†Ï€Ï•Ï„(s, a)|)).
Also, we have
âˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|(âˆ¥âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥+Î»âˆ¥âˆ‡2
Ï•âˆ‡Ë†Ï€(a|s)d2
1(Ë†Ï€Ï•,Ë†Ï€, s)âˆ¥).
From Lemma D.4 in [2], we have
âˆ¥âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤8rmax
(1âˆ’Î³)3.
28Moreover, we have
Î»âˆ¥âˆ‡2
Ï•âˆ‡Ë†Ï€(a|s)d2
1(Ë†Ï€Ï•,Ë†Ï€, s)âˆ¥
=Î»âˆ¥âˆ‡Ï•((1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))Ë†Ï€Ï•(a|s)
Ë†Ï€Î¸â€²Ï„(a|s))âˆ¥
â‰¤Î»+ max s,a|AË†Ï€Ï•Ï„(s, a)|
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|(Î³max a,s|AË†Ï€Ï•Ï„(s, a)|
1âˆ’Î³+ 2(Î»+ max
a,s|AË†Ï€Ï•Ï„(s, a)|) + 2)
=Î»+ max s,a|AË†Ï€Ï•Ï„(s, a)|
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|((2âˆ’Î³) max a,s|AË†Ï€Ï•Ï„(s, a)|
1âˆ’Î³+ 2Î»+ 2)
So,
X
aâˆˆAâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤1
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|(8rmax
(1âˆ’Î³)3
+Î»+ max s,a|AË†Ï€Ï•Ï„(s, a)|
Î»âˆ’max s,a|AË†Ï€Ï•Ï„(s, a)|((2âˆ’Î³) max a,s|AË†Ï€Ï•Ï„(s, a)|
1âˆ’Î³+ 2Î»+ 2)) .
Lemma 8. Suppose that Assumptions 1 and 2 hold. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,1, we
have
âˆ¥âˆ‡2
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3, (28)
where C =1
Î»âˆ’Amax(Î³Amax
1âˆ’Î³+ 2Î»+ 2Amax)and B =1
Î»âˆ’Amax(8rmax
(1âˆ’Î³)3+
Î»+Amax
Î»âˆ’Amax((2âˆ’Î³)Amax
1âˆ’Î³+ 2Î»+ 2)) .
Proof. From Lemma 7, we have boundedP
aâˆˆAâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥andP
aâˆˆAâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥. Borrow
the result from Lemma D.2 in [2].
K.2.2 Convergence guarantee
Theorem 5. Consider the tabular softmax policy for the discrete state-action space shown in Section
5.1, and the within-task algorithm Algin (1). Suppose that Assumptions 1 and 2 hold. Let {Ï•t}T
t=1
be the sequence generated by Algorithm 1 with DÏ„=DÏ„,1,Î» > A max, and the step size selected as
Î±= min(rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3âˆ’1
,1
Gâˆš
T)
.
Then,
1
TTX
t=1Et
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤2r2
maxB
(1âˆ’Î³)3+4Î³r2
maxC2
(1âˆ’Î³)41
T+2rmax
1âˆ’Î³+rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3Gâˆš
T,
where
G=2Amax
1âˆ’Î³(Amax
Î»âˆ’AmaxÎ³
1âˆ’Î³+ 2),
C=1
Î»âˆ’Amax(Î³Amax
1âˆ’Î³+ 2Î»+ 2Amax),
and
B=1
Î»âˆ’Amax(8rmax
(1âˆ’Î³)3+Î»+Amax
Î»âˆ’Amax((2âˆ’Î³)Amax
1âˆ’Î³+ 2Î»+ 2)) .
29Proof. As the smoothness constant of JÏ„(Ë†Ï€Î¸â€²Ï„), i.e., JÏ„(Alg(Ë†Ï€Ï•, Î», Ï„)is obtained in (8), the smooth-
ness constant of EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•, Î», Ï„))]is the same, i.e.,
âˆ¥âˆ‡2
Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•, Î», Ï„))]âˆ¥ â‰¤Brmax
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3.
Moreover, from Lemma 6, we have
âˆ¥âˆ‡Ï•JÏ„(Alg(Ë†Ï€Ï•, Î», Ï„))âˆ¥ â‰¤Amax
1âˆ’Î³(Amax
Î»âˆ’AmaxÎ³
1âˆ’Î³+ 2).
From the convergence theorem of SDG with smoothness and bounded gradient shown in [ 19], let the
step size
Î±= min(rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3âˆ’1
,1
Gâˆš
T)
,
we have
1
TTX
t=1Et
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤2rmaxB
(1âˆ’Î³)2+4Î³rmaxC2
(1âˆ’Î³)3
EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•T, Î», Ï„))âˆ’JÏ„(Alg(Ë†Ï€Ï•0, Î», Ï„))]1
T
+
2EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•T, Î», Ï„))âˆ’JÏ„(Alg(Ë†Ï€Ï•0, Î», Ï„))] +rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3
SinceEÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•T, Î», Ï„))âˆ’JÏ„(Alg(Ë†Ï€Ï•0, Î», Ï„))]â‰¤rmax
1âˆ’Î³, we have
1
TTX
t=1Et
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤2r2
maxB
(1âˆ’Î³)3+4Î³r2
maxC2
(1âˆ’Î³)41
T+2rmax
1âˆ’Î³+rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3Gâˆš
T,
where
G=2Amax
1âˆ’Î³(Amax
Î»âˆ’AmaxÎ³
1âˆ’Î³+ 2),
C=1
Î»âˆ’Amax(Î³Amax
1âˆ’Î³+ 2Î»+ 2Amax),
and
B=1
Î»âˆ’Amax(8rmax
(1âˆ’Î³)3+Î»+Amax
Î»âˆ’Amax((2âˆ’Î³)Amax
1âˆ’Î³+ 2Î»+ 2)) .
Corollary 1. Suppose all assumptions and conditions in Theorem 5 hold, and we set Î»â‰¥2Amax,
then
1
TTX
t=1Et
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤(B+ 2C2)rmax
(1âˆ’Î³)4(2rmax
T+Gâˆš
T),
where Bâ‰œ16rmax
Î»(1âˆ’Î³)3+24
1âˆ’Î³+12
Î»,Câ‰œ6
1âˆ’Î³, and Gâ‰œ4Amax
(1âˆ’Î³)2.
Proof. Since Î»â‰¥2Amax, we have1
Î»âˆ’Amaxâ‰¤1
Amaxand1
Î»âˆ’Amaxâ‰¤2
Î». Then, simplify the
inequality in Theorem 5.
30L Proofs of convergence when DÏ„=DÏ„,2
L.1 Gradients of âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)when DÏ„=DÏ„,2
From Proposition 1, we have
âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„h
âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(Â·|s)Â·AË†Ï€Î¸â€²Ï„Ï„(s,Â·)i
, (29)
where
âˆ‡âŠ¤
Ï•Ë†Ï€Î¸â€²Ï„(Â·|s) =
M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11

âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s,Â·)âˆ’Î»âˆ‡âŠ¤
Ï•âˆ‡Ë†Ï€(Â·|s)d2
2(Ë†Ï€Ï•,Ë†Ï€, s)
|Ë†Ï€=Ë†Ï€Î¸â€²Ï„,(30)
where
M(s) =Î»âˆ‡2
Ë†Ï€(Â·|s)d2
2(Ë†Ï€Ï•,Ë†Ï€, s) =Î»ï£®
ï£¯ï£¯ï£°1
Ë†Ï€Î¸â€²Ï„(a1|s)
...
1
Ë†Ï€Î¸â€²Ï„(an|s)ï£¹
ï£ºï£ºï£».
Then,
M(s)âˆ’1=1
Î»ï£®
ï£¯ï£°Ë†Ï€Î¸â€²Ï„(a1|s)
...
Ë†Ï€Î¸â€²Ï„(an|s)ï£¹
ï£ºï£». (31)
Also,
âˆ‡âŠ¤
Ï•âˆ‡Ë†Ï€(Â·|s)d2
2(Ë†Ï€Ï•,Ë†Ï€, s)|Ë†Ï€=Ë†Ï€Î¸â€²Ï„=ï£®
ï£¯ï£¯ï£¯ï£°âˆ’âˆ‡âŠ¤
Ï•Ë†Ï€Ï•(a1|s)
Ë†Ï€Ï•(a1|s)
...
âˆ’âˆ‡âŠ¤
Ï•Ë†Ï€Ï•(an|s)
Ë†Ï€Ï•(an|s)ï£¹
ï£ºï£ºï£ºï£». (32)
Specially, AË†Ï€Î¸â€²Ï„Ï„(s,Â·)âŠ¤M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤Mâˆ’11= 0, because we have
AË†Ï€Î¸â€²Ï„Ï„(s,Â·)âŠ¤M(s)âˆ’11=X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)AË†Ï€Î¸â€²Ï„Ï„(s, a) = 0 .
Then,
âˆ‡âŠ¤
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„,aâˆ¼Ë†Ï€Î¸â€²Ï„
AË†Ï€Î¸â€²Ï„Ï„(s, a)(1
Î»âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âŠ¤)
.
SinceP
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)AË†Ï€Î¸â€²Ï„Ï„(s, a) = 0 , thenP
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)AË†Ï€Î¸â€²Ï„Ï„(s, a)Ë†Ï€Ï•(Â·|s)âŠ¤= 0. We have
âˆ‡âŠ¤
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„,aâˆ¼Ë†Ï€Î¸â€²Ï„
AË†Ï€Î¸â€²Ï„Ï„(s, a)(1
Î»âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a))
. (33)
Here, 1(sâ€², aâ€²)denote the column vector where the element is 1ifs=sâ€²anda=aâ€², otherwise is 0,
for each pair (s, a)âˆˆ S Ã— A .
L.2 Convergence guarantee when DÏ„=DÏ„,2
L.2.1 Auxiliary lemmas
Lemma 9. Suppose that Assumption 2 holds. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,2, we have
âˆ¥âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤max s,a|AË†Ï€Î¸â€²Ï„Ï„(s, a)|
1âˆ’Î³(max s,a|AË†Ï€Î¸â€²Ï„Ï„(s, a)|
Î»Î³
1âˆ’Î³+ 1).
31Proof. As shown in (33)
âˆ‡âŠ¤
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„) =1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„,aâˆ¼Ë†Ï€Î¸â€²Ï„
AË†Ï€Î¸â€²Ï„Ï„(s, a)(1
Î»âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +1âŠ¤(s, a))
.
As shown in proof of Lemma 6 in (27),
âˆ¥âˆ‡Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤Î³
1âˆ’Î³max
a,s|AË†Ï€Ï•Ï„(s, a)|,
we have that
âˆ¥âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤max s,a|AË†Ï€Î¸â€²Ï„Ï„(s, a)|
1âˆ’Î³(max s,a|AË†Ï€Î¸â€²Ï„Ï„(s, a)|
Î»Î³
1âˆ’Î³+ 1).
Lemma 10. Suppose that Assumption 2 holds. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,2, for any
sâˆˆ S,we have
X
aâˆˆAâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤2Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 4
and
X
aâˆˆAâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤(2Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 4)2+16rmax
Î»(1âˆ’Î³)3+ 2.
Proof. As shown in 30, we have âˆ‡âŠ¤
Ï•Ë†Ï€Î¸â€²Ï„(Â·|s) =

M(s)âˆ’1âˆ’M(s)âˆ’11 1âŠ¤M(s)âˆ’1
1âŠ¤M(s)âˆ’11
âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s,Â·)âˆ’Î»âˆ‡âŠ¤
Ï•âˆ‡Ë†Ï€(Â·|s)d2
2(Ë†Ï€Ï•,Ë†Ï€, s)
|Ë†Ï€=Ë†Ï€Î¸â€²Ï„,
where the computations of M(s)âˆ’1andâˆ‡âŠ¤
Ï•âˆ‡Ë†Ï€(Â·|s)d2
2(Ë†Ï€Ï•,Ë†Ï€, s)are shown in (31) (32) and (24), then
âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s) = Ë†Ï€Î¸â€²Ï„(a|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))
âˆ’Ë†Ï€Î¸â€²Ï„(a|s)X
aâ€²âˆˆAË†Ï€Î¸â€²Ï„(aâ€²|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, aâ€²) +1(s, aâ€²)âˆ’Ë†Ï€Ï•(Â·|s)).
Therefore,
âˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))
+Ë†Ï€Î¸â€²Ï„(a|s)X
aâ€²âˆˆAË†Ï€Î¸â€²Ï„(aâ€²|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, aâ€²) +1(s, aâ€²)âˆ’Ë†Ï€Ï•(Â·|s)).
Then,
X
aâˆˆAâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))
+X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)X
aâ€²âˆˆAË†Ï€Î¸â€²Ï„(aâ€²|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, aâ€²) +1(s, aâ€²)âˆ’Ë†Ï€Ï•(Â·|s)).
From (27), we have
âˆ¥âˆ‡Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤Î³
1âˆ’Î³max
a,s|AË†Ï€Ï•Ï„(s, a)|.
32Then,
X
aâˆˆAâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s)
+X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)X
aâ€²âˆˆAË†Ï€Î¸â€²Ï„(aâ€²|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, aâ€²) +1(s, aâ€²)âˆ’Ë†Ï€Ï•(Â·|s))
â‰¤X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)(Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 2)
+X
aâˆˆAË†Ï€Î¸â€²Ï„(a|s)X
aâ€²âˆˆAË†Ï€Î¸â€²Ï„(aâ€²|s)(Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 2)
â‰¤2Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 4,
And
âˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)(2Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 4) (34)
Moreover, since
âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s) = Ë†Ï€Î¸â€²Ï„(a|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))
âˆ’Ë†Ï€Î¸â€²Ï„(a|s)X
aâ€²âˆˆAË†Ï€Î¸â€²Ï„(aâ€²|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, aâ€²) +1(s, aâ€²)âˆ’Ë†Ï€Ï•(Â·|s)).
we have
âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s) =âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s))
+ Ë†Ï€Î¸â€²Ï„(a|s)(1
Î»âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a) +âˆ’âˆ‡Ï•Ë†Ï€Ï•(Â·|s))
âˆ’ âˆ‡ Ï• 
Ë†Ï€Î¸â€²Ï„(a|s)X
aâ€²âˆˆAË†Ï€Î¸â€²Ï„(aâ€²|s)(1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, aâ€²) +1(s, aâ€²)âˆ’Ë†Ï€Ï•(Â·|s))!
.
Then,
âˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤2âˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ¥1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âˆ¥
+ 2Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥1
Î»âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)âˆ’ âˆ‡ Ï•Ë†Ï€Ï•(Â·|s)âˆ¥.
From (34),
âˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)(2Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 4).
From (27)
âˆ¥1
Î»âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) +1(s, a)âˆ’Ë†Ï€Ï•(Â·|s)âˆ¥ â‰¤Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 2
From Lemma D.4 in [2], we have
âˆ¥âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤8rmax
(1âˆ’Î³)3,
then
âˆ¥1
Î»âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)âˆ’ âˆ‡ Ï•Ë†Ï€Ï•(Â·|s)âˆ¥ â‰¤8rmax
Î»(1âˆ’Î³)3+ 1.
Therefore,
âˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)(2Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 4)2+ 2Ë†Ï€Î¸â€²Ï„(a|s)(8rmax
Î»(1âˆ’Î³)3+ 1).
So,X
aâˆˆAâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤(2Î³
Î»(1âˆ’Î³)max
a,s|AË†Ï€Ï•Ï„(s, a)|+ 4)2+16rmax
Î»(1âˆ’Î³)3+ 2.
33Lemma 11. Suppose that Assumptions 1 and 2 hold. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,2,
we have
âˆ¥âˆ‡2
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3, (35)
where C=2Î³
Î»(1âˆ’Î³)Amax+ 4andB= (2Î³
Î»(1âˆ’Î³)Amax+ 4)2+16rmax
Î»(1âˆ’Î³)3+ 2.
Proof. Similar to the proof of Lemma 8 by using Lemma 10.
L.2.2 Convergence guarantee
Theorem 6. Consider the tabular softmax policy for the discrete state-action space shown in Section
5.1, and the within-task algorithm Algin (1). Suppose that Assumptions 1 and 2 hold. Let {Ï•t}T
t=1
be the sequence generated by Algorithm 1 with DÏ„=DÏ„,2and the step size selected as
Î±= min(rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3âˆ’1
,1
Gâˆš
T)
.
Then,
1
TTX
t=1Et
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤2r2
maxB
(1âˆ’Î³)3+4Î³r2
maxC2
(1âˆ’Î³)41
T+2rmax
1âˆ’Î³+rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3Gâˆš
T,
where
G=2Amax
1âˆ’Î³(Amax
Î»Î³
1âˆ’Î³+ 1),
C=2Î³
Î»(1âˆ’Î³)Amax+ 4,
and
B= (2Î³Amax
Î»(1âˆ’Î³)+ 4)2+16rmax
Î»(1âˆ’Î³)3+ 2.
Proof. Similar to the proof of Theorem 5, by using the gradient bound in Lemma 9 and the smoothness
in Lemma 11.
Corollary 2. Suppose all assumptions and conditions in Theorem 6 hold, and we set Î»â‰¥2Amax,
then
1
TTX
t=1Et
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤(B+ 2C2)rmax
(1âˆ’Î³)4(2rmax
T+Gâˆš
T),
where Bâ‰œ16rmax
Î»(1âˆ’Î³)3+18
(1âˆ’Î³)2,Câ‰œ4
1âˆ’Î³, and Gâ‰œ2Amax
(1âˆ’Î³)2).
Proof. Since Î»â‰¥2Amax, we have1
Î»â‰¤1
2Amax. Then, simplify the inequality in Theorem 5.
M Proofs of convergence when DÏ„=DÏ„,3
Lemma 12. Suppose that Assumptions 1, 2, and 3 hold. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,3.
IfÎ» >(6L2
1+ 2L2)Amax, thenâˆ‡Ï•JÏ„(Alg(3)(Ë†Ï€Ï•, Î», Ï„))exists for any Ï•, and
âˆ¥âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤L1Amax(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax).
34Proof. From Proposition 2, we have
âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„) =1
1âˆ’Î³âˆ‡Ï•Î¸â€²
Ï„Â·E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Â·|s)âˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)
Ë†Ï€Î¸â€²Ï„(a|s)AË†Ï€Î¸â€²Ï„Ï„(s, a)
,
where
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„=âˆ’E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Ï•(Â·|s)
âˆ’âˆ‡2
Î¸Ë†Ï€Î¸(a|s)
Ë†Ï€Ï•(a|s)QË†Ï€Ï•Ï„(s, a) +Î»âˆ‡2
Î¸d2(Ë†Ï€Ï•(Â·|s),Ë†Ï€Î¸(Â·|s))âˆ’1
E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Ï•(Â·|s)
âˆ’âˆ‡Î¸Ë†Ï€Î¸(a|s)
Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +Î»âˆ‡âŠ¤
Ï•âˆ‡Î¸d2(Ë†Ï€Ï•(Â·|s),Ë†Ï€Î¸(Â·|s))
|Î¸=Î¸â€²Ï„.
When DÏ„=DÏ„,3, and the policy with function approximation is defined by Ë†Ï€Î¸(a|s)â‰œ
exp(fÎ¸(s,a))R
Aexp(fÎ¸(s,aâ€²))daâ€²,âˆ€(s, a)âˆˆ S Ã— A , from Lemma 4,
âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²
Ï„) =1
1âˆ’Î³âˆ‡Ï•Î¸â€²
Ï„Â·E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Â·|s)h
âˆ‡Î¸â€²Ï„fË†Ï€Î¸â€²Ï„(s, a)AË†Ï€Î¸â€²Ï„Ï„(s, a)i
,
where âˆ‡âŠ¤
Ï•Î¸â€²
Ï„=
E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Ï•(Â·|s)"
âˆ’âˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)
Ë†Ï€Ï•(a|s)QË†Ï€Ï•Ï„(s, a) +Î»I#âˆ’1
E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Ï•(Â·|s)âˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)
Ë†Ï€Ï•(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +Î»I
=Esâˆ¼Î½Ë†Ï€Ï•
Ï„
âˆ’Z
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)QË†Ï€Ï•Ï„(s, a)da+Î»Iâˆ’1
Esâˆ¼Î½Ë†Ï€Ï•
Ï„Z
Aâˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a)da+Î»I
=Esâˆ¼Î½Ë†Ï€Ï•
Ï„
âˆ’Z
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)AË†Ï€Ï•Ï„(s, a)da+Î»Iâˆ’1
Esâˆ¼Î½Ë†Ï€Ï•
Ï„Z
Aâˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a)da+Î»I
First, we have
âˆ¥âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥=1
1âˆ’Î³âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥âˆ¥E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Â·|s)h
âˆ‡Î¸fÎ¸(s, a)AË†Ï€Î¸â€²Ï„Ï„(s, a)i
âˆ¥,
and
âˆ¥E
sâˆ¼Î½Ë†Ï€Î¸â€²Ï„Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Â·|s)h
âˆ‡Î¸fÎ¸(s, a)AË†Ï€Î¸â€²Ï„Ï„(s, a)i
âˆ¥ â‰¤ âˆ¥ max
a,sâˆ‡Î¸fÎ¸(s, a)âˆ¥max
a,s|AË†Ï€Î¸â€²Ï„Ï„(s, a)| â‰¤L1Amax.
For the term âˆ‡Ï•Î¸â€²
Ï„, consider âˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)andâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s), we have
âˆ‡Î¸Ë†Ï€Î¸(a|s) = Ë†Ï€Î¸(a|s)âˆ‡Î¸fÎ¸(s, a)âˆ’Ë†Ï€Î¸(a|s)R
Aâˆ‡Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²
R
Aexp (fÎ¸(s, aâ€²))daâ€². (36)
Then,
âˆ¥âˆ‡Î¸Ë†Ï€Î¸(a|s)âˆ¥ â‰¤Ë†Ï€Î¸(a|s)âˆ¥âˆ‡Î¸fÎ¸(s, a)âˆ¥+ Ë†Ï€Î¸(a|s)R
Aâˆ‡Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²
R
Aexp (fÎ¸(s, aâ€²))daâ€²
â‰¤2Ë†Ï€Î¸(a|s)L1(37)
We also have âˆ‡2
Î¸Ë†Ï€Î¸(a|s) =
âˆ‡Î¸Ë†Ï€Î¸(a|s)âˆ‡âŠ¤
Î¸fÎ¸(s, a) + Ë†Ï€Î¸(a|s)âˆ‡2
Î¸fÎ¸(s, a)âˆ’ âˆ‡Ë†Ï€Î¸(a|s)R
Aâˆ‡âŠ¤
Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²
R
Aexp (fÎ¸(s, aâ€²))daâ€²
âˆ’Ë†Ï€Î¸(a|s)R
Aâˆ‡2
Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²+âˆ‡Î¸fÎ¸(s, aâ€²)âˆ‡âŠ¤
Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²
R
Aexp (fÎ¸(s, aâ€²))daâ€²
+ Ë†Ï€Î¸(a|s)R
Aâˆ‡Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²R
Aâˆ‡âŠ¤
Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²
(R
Aexp (fÎ¸(s, aâ€²))daâ€²)2.
35Then,
âˆ¥âˆ‡2
Î¸Ë†Ï€Î¸(a|s)âˆ¥ â‰¤2Ë†Ï€Î¸(a|s)L2
1+ Ë†Ï€Î¸(a|s)L2+ 2Ë†Ï€Î¸(a|s)L2
1+ Ë†Ï€Î¸(a|s)L2+ 2Ë†Ï€Î¸(a|s)L2
1
= 6Ë†Ï€Î¸(a|s)L2
1+ 2Ë†Ï€Î¸(a|s)L2.(38)
So, Esâˆ¼Î½Ë†Ï€Ï•
Ï„Z
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)AË†Ï€Ï•Ï„(s, a)daâ‰¤(6L2
1+ 2L2)Amax.
Since Esâˆ¼Î½Ë†Ï€Ï•
Ï„hR
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)AË†Ï€Ï•Ï„(s, a)dai
is a diagonal matrix, the above shown its largest
absolute eigenvalue is smaller than (6L2
1+ 2L2)Amax. Then, the smallest eigenvalue of
Esâˆ¼Î½Ë†Ï€Ï•
Ï„h
âˆ’R
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²
Ï„(a|s)AË†Ï€Ï•Ï„(s, a)da+Î»Ii
is larger than Î»âˆ’(6L2
1+ 2L2)Amax. Therefore, if
Î» >(6L2
1+ 2L2)Amax,
Esâˆ¼Î½Ë†Ï€Ï•
Ï„
âˆ’Z
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)AË†Ï€Ï•Ï„(s, a)da+Î»Iâˆ’1â‰¤1
Î»âˆ’(6L2
1+ 2L2)Amax. (39)
Moreover, if Î» >(6L2
1+2L2)Amax, the objective function in the optimization problem Alg(Ë†Ï€Ï•, Î», Ï„)
is strongly concave. Then, from [64], the solution is unique and âˆ‡Ï•JÏ„(Alg(3)(Ë†Ï€Ï•, Î», Ï„))exists.
From (6),
âˆ‡Ï•QË†Ï€Ï•Ï„(s, a) =Î³
1âˆ’Î³Â·E(sâ€²,aâ€²)âˆ¼Ïƒ(s,a)
Ï„,Ë†Ï€Ï•h
âˆ‡Ï•fÏ•(sâ€², aâ€²)AË†Ï€Ï•Ï„(sâ€², aâ€²)i
.
Then,
âˆ¥âˆ‡Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤Î³
1âˆ’Î³L1Amax.
Combine (37), we have
Esâˆ¼Î½Ë†Ï€Ï•
Ï„Z
Aâˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a)da+Î»Iâ‰¤Î»+2Î³
1âˆ’Î³L2
1Amax.
So we have
âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥ â‰¤Î»+2Î³
1âˆ’Î³L2
1Amax
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax). (40)
Therefore, we have
âˆ¥âˆ‡Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤L1Amax(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax).
Lemma 13. For a softmax policy parameterized by Ï•,
âˆ¥âˆ‡2
Ï•JÏ„(Ë†Ï€Ï•)âˆ¥ â‰¤(6L2
1+ 2L2)rmax
(1âˆ’Î³)2+8Î³L2
1rmax
(1âˆ’Î³)3
âˆ¥âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤8Î³2L2
1rmax
(1âˆ’Î³)3+Î³(6L2
1+ 2L2)rmax
(1âˆ’Î³)2. (41)
Proof. From 37,Z
Aâˆ¥âˆ‡Ï•Ë†Ï€Ï•(a|s)âˆ¥daâ‰¤2L1.
From 38, Z
Aâˆ¥âˆ‡Ï•Ë†Ï€Ï•(a|s)âˆ¥daâ‰¤6L2
1+ 2L2.
Borrow the result from Lemma D.2 in [2],
âˆ¥âˆ‡2
Ï•JÏ„(Ë†Ï€Ï•)âˆ¥ â‰¤(6L2
1+ 2L2)rmax
(1âˆ’Î³)2+8Î³L2
1rmax
(1âˆ’Î³)3
36âˆ¥âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)âˆ¥ â‰¤8Î³2L2
1rmax
(1âˆ’Î³)3+Î³(6L2
1+ 2L2)rmax
(1âˆ’Î³)2.
Lemma 14. Suppose that Assumption 2 holds. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,3, for any
sâˆˆ S,we haveZ
Aâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥daâ‰¤2L1(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax)
andZ
Aâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥daâ‰¤(160L3
1+ 56L1L2+ 4L3)(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)3(Î»âˆ’(6L2
1+ 2L2)Amax)2.
Proof. First consider âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s), we have
âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s) = Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡Ï•Î¸â€²
Ï„âˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ’Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡Ï•Î¸â€²
Ï„R
Aâˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²) exp ( fÎ¸â€²Ï„(s, aâ€²))daâ€²
R
Aexp (fÎ¸â€²Ï„(s, aâ€²))daâ€²,
(42)
Then,
âˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥âˆ¥âˆ‡ Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥+
Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥R
Aâˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²) exp ( fÎ¸â€²Ï„(s, aâ€²))daâ€²
R
Aexp (fÎ¸â€²Ï„(s, aâ€²))daâ€²
â‰¤2Ë†Ï€Î¸â€²Ï„(a|s)(Î»+2Î³
1âˆ’Î³L2
1Amax)L1
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax).(43)
Then,Z
Aâˆ¥âˆ‡Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥daâ‰¤2L1(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax).
Next, we consider âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s). From (42), we have
âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s) =âˆ‡Ï•Î¸â€²
Ï„âˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ‡âŠ¤
Ï•Ë†Ï€Î¸â€²Ï„(a|s) + Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡2
Ï•Î¸â€²
Ï„âˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, a)
+ Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡Ï•Î¸â€²
Ï„âˆ‡2
Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ‡âŠ¤
Ï•Î¸â€²
Ï„âˆ’ âˆ‡ Ï•Î¸â€²
Ï„R
Aâˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²) exp ( fÎ¸â€²Ï„(s, aâ€²))daâ€²
R
Aexp (fÎ¸â€²Ï„(s, aâ€²))daâ€²âˆ‡âŠ¤
Ï•Ë†Ï€Î¸â€²Ï„(a|s)
âˆ’Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡2
Ï•Î¸â€²
Ï„R
Aâˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²) exp ( fÎ¸â€²Ï„(s, aâ€²))daâ€²
R
Aexp (fÎ¸â€²Ï„(s, aâ€²))daâ€²âˆ’Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡Ï•Î¸â€²
Ï„
R
A(âˆ‡2
Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²) exp ( fÎ¸â€²Ï„(s, aâ€²)) +âˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²)âˆ‡âŠ¤
Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²) exp ( fÎ¸â€²Ï„(s, aâ€²)))daâ€²
R
Aexp (fÎ¸â€²Ï„(s, aâ€²))daâ€²âˆ‡âŠ¤
Ï•Î¸â€²
Ï„
+ Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡Ï•Î¸â€²
Ï„R
Aâˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, aâ€²) exp ( fÎ¸â€²Ï„(s, aâ€²))daâ€²
R
Aexp (fÎ¸â€²Ï„(s, aâ€²))daâ€²2
âˆ‡âŠ¤
Ï•Î¸â€²
Ï„.
Therefore,
âˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤ âˆ¥âˆ‡ Ï•Î¸â€²
Ï„âˆ¥âˆ¥âˆ‡ Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥âˆ¥âˆ‡ Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥+ Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡2
Ï•Î¸â€²
Ï„âˆ¥âˆ¥âˆ‡ Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥
+ Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥2âˆ¥âˆ‡2
Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥+âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥âˆ¥âˆ‡ Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥âˆ¥âˆ‡ Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥
+ Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡2
Ï•Î¸â€²
Ï„âˆ¥âˆ¥âˆ‡ Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥+ Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥2(âˆ¥âˆ‡2
Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥+âˆ¥âˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥2)
+ Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥2âˆ¥âˆ‡Î¸â€²Ï„fÎ¸â€²Ï„(s, a)âˆ¥2
â‰¤2L1âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥âˆ¥âˆ‡ Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥+ 2Ë†Ï€Î¸â€²Ï„(a|s)L1âˆ¥âˆ‡2
Ï•Î¸â€²
Ï„âˆ¥+ 2Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥âˆ‡Ï•Î¸â€²
Ï„âˆ¥2(L2+L2
1).
From (40) and (43)
âˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s) 
2L2+ 6L2
1(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)2(Î»âˆ’(6L2
1+ 2L2)Amax)2+ 2L1âˆ¥âˆ‡2
Ï•Î¸â€²
Ï„âˆ¥!
.
37Then,Z
Aâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥daâ‰¤2L2+ 6L2
1(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)2(Î»âˆ’(6L2
1+ 2L2)Amax)2+ 2L1âˆ¥âˆ‡2
Ï•Î¸â€²
Ï„âˆ¥.
Next, we consider âˆ‡2
Ï•Î¸â€²
Ï„. We have
âˆ‡2
Ï•Î¸â€²
Ï„=Esâˆ¼Î½Ë†Ï€Ï•
Ï„
âˆ’Z
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²
Ï„(a|s)QË†Ï€Ï•Ï„(s, a)da+Î»Iâˆ’1
Esâˆ¼Î½Ë†Ï€Ï•
Ï„Z
A
âˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡âŠ¤
Ï•Î¸â€²
Ï„âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a) +âˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡2
Ï•QË†Ï€Ï•Ï„(s, a)
da
âˆ’
MEsâˆ¼Î½Ë†Ï€Ï•
Ï„
âˆ’Z
A
âˆ‡3
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡Ï•Î¸â€²
Ï„AË†Ï€Ï•Ï„(s, a) +âˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a)
da
Mâˆ’1N
where M = Esâˆ¼Î½Ë†Ï€Ï•
Ï„h
âˆ’R
Aâˆ‡2
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)AË†Ï€Ï•Ï„(s, a)da+Î»Ii
and N =
Esâˆ¼Î½Ë†Ï€Ï•
Ï„hR
Aâˆ‡Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ‡âŠ¤
Ï•QË†Ï€Ï•Ï„(s, a)da+Î»Ii
. Also, we have Mâˆ’1N=âˆ‡Ï•Î¸â€²
Ï„.
Similar to (37)(38), we can derive the upper bound of âˆ¥âˆ‡3
Ï•Ë†Ï€Ï•âˆ¥, then
âˆ¥âˆ‡3
Î¸â€²Ï„Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥ â‰¤Ë†Ï€Î¸â€²Ï„(a|s)(40L3
1+ 16L1L2+ 2L3).
So, from (38)(39)(40)(41), we have
âˆ¥âˆ‡2
Ï•Î¸â€²
Ï„âˆ¥ â‰¤2Î³L2
1Amax(6L2
1+ 2L2)(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)2(Î»âˆ’(6L2
1+ 2L2)Amax)2
+8Î³2L2
1rmax
(1âˆ’Î³)3+Î³(6L2
1+ 2L2)rmax
(1âˆ’Î³)21
Î»âˆ’(6L2
1+ 2L2)Amax
+Î»+2Î³
1âˆ’Î³L2
1Amax
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax)2((40L3
1+ 16L1L2+ 2L3)(Î»+2Î³
1âˆ’Î³L2
1Amax)Amax
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax)
+2Î³
1âˆ’Î³L1(6L2
1+ 2L2)Amax).
Simplify the inequality by Î³ <1and1âˆ’Î³ <0,
âˆ¥âˆ‡2
Ï•Î¸â€²
Ï„âˆ¥ â‰¤(80L3
1+ 28L1L2+ 2L3)(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)3(Î»âˆ’(6L2
1+ 2L2)Amax)2
Then,Z
Aâˆ¥âˆ‡2
Ï•Ë†Ï€Î¸â€²Ï„(a|s)âˆ¥daâ‰¤(160L3
1+ 56L1L2+ 4L3)(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)3(Î»âˆ’(6L2
1+ 2L2)Amax)2.
Lemma 15. Suppose that Assumptions 1, 2, and 3 hold. Let Ë†Ï€Î¸â€²Ï„=Alg(Ë†Ï€Ï•, Î», Ï„)where DÏ„=DÏ„,3,
we have
âˆ¥âˆ‡2
Ï•JÏ„(Ë†Ï€Î¸â€²Ï„)âˆ¥ â‰¤rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3, (44)
where C=2L1(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+2L2)Amax)andB=(160L3
1+56L1L2+4L3)(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)3(Î»âˆ’(6L2
1+2L2)Amax)2 .
Proof. Similar to the proofs of Lemma 8 and Lemma 11 by using Lemma 14.
Theorem 7. In both discrete and continuous action space, consider the softmax policy with func-
tion approximation shown in Section 5.1, and the within-task algorithm Algis defined in (2)
withDÏ„=DÏ„,3. Suppose that Assumptions 1, 2, and 3 hold. If Î» > (6L2
1+ 2L2)Amax, then
âˆ‡Ï•JÏ„(Alg(3)(Ë†Ï€Ï•, Î», Ï„))exists for any Ï•.
38Let{Ï•t}T
t=1be the sequence generated by Algorithm 1 with Î» >(6L2
1+ 2L2)Amax and the step size
Î±= min(rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3âˆ’1
,1
Gâˆš
T)
.
Then, the following bound holds:
1
TTX
t=1Et
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2
â‰¤2r2
maxB
(1âˆ’Î³)3+4Î³r2
maxC2
(1âˆ’Î³)41
T+2rmax
1âˆ’Î³+rmaxB
(1âˆ’Î³)2+2Î³rmaxC2
(1âˆ’Î³)3Gâˆš
T,
where
G=L1Amax(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax),
C=2L1(Î»+2Î³
1âˆ’Î³L2
1Amax)
(1âˆ’Î³)(Î»âˆ’(6L2
1+ 2L2)Amax),
and
B=(160L3
1+ 56L1L2+ 4L3)(Î»+2Î³
1âˆ’Î³L2
1Amax)2
(1âˆ’Î³)3(Î»âˆ’(6L2
1+ 2L2)Amax)2.
Proof. Similar to the proof of Theorem 5, by using the gradient bound in Lemma 12 and the
smoothness in Lemma 15.
N Optimality of one-time policy adaptation
N.1 Important Lemmas
Lemma 16. Suppose that Assumptions 1, 2 hold. For any task Ï„, and any policies Ï€andÏ€â€², the
following bound holds:
1
1âˆ’Î³E
sâˆ¼Î½Ï€
Ï„
aâˆ¼Ï€â€²(Â·|s)[AÏ€
Ï„(s, a)]âˆ’CÏ€
Ï„(Ï€â€²)â‰¤JÏ„(Ï€â€²)âˆ’JÏ„(Ï€)â‰¤1
1âˆ’Î³E
sâˆ¼Î½Ï€
Ï„
aâˆ¼Ï€â€²(Â·|s)[AÏ€
Ï„(s, a)] +CÏ€
Ï„(Ï€â€²)
where
CÏ€
Ï„(Ï€â€²) =4Î³Amax
(1âˆ’Î³)2Dmax
TV(Ï€||Ï€â€²)Esâˆ¼Î½Ï€Ï„[DTV(Ï€(Â·|s)||Ï€â€²(Â·|s))].
Here, we define DTV(Ï€(Â·|s)||Ï€â€²(Â·|s))â‰œ1
2P
aâˆˆA|Ï€(a|s)âˆ’Ï€â€²(a|s)|in a discrete action space
orDTV(Ï€(Â·|s)||Ï€â€²(Â·|s))â‰œ1
2R
aâˆˆA|Ï€(a|s)âˆ’Ï€â€²(a|s)|dain a continuous action space, and
Dmax
TV(Ï€||Ï€â€²)â‰œmax sâˆˆSDTV(Ï€(Â·|s)||Ï€â€²(Â·|s)).
Proof. LetPÏ€
Ï„is a matrix where PÏ€
Ï„(i, j) =Eaâˆ¼Ï€(Â·|si)PÏ„(sj|si, a)andPÏ€â€²
Ï„is a matrix where
PÏ€â€²
Ï„(i, j) =Eaâˆ¼Ï€â€²(Â·|si)PÏ„(sj|si, a). Let G= (1 + Î³PÏ€
Ï„+ (Î³PÏ€
Ï„)2+. . .) = (1 âˆ’Î³PÏ€
Ï„)âˆ’1, and
similarly ËœG= (1 + Î³PÏ€â€²
Ï„+ (Î³PÏ€â€²
Ï„)2+. . .) = (1 âˆ’Î³PÏ€â€²
Ï„)âˆ’1. Let Ïbe a density vector on state
space and rÏ„is a reward function vector on state space, thus râŠ¤
Ï„Ïis a scalar meaning the expected
reward under density Ï. Note that JÏ„(Ï€) =râŠ¤
Ï„GÏÏ„, and JÏ„(Ï€â€²) =râŠ¤
Ï„ËœGÏÏ„. Here, ÏÏ„is the initial
state distribution for task Ï„. Letâˆ† =PÏ€â€²
Ï„âˆ’PÏ€
Ï„.
Follow the proof in Appendix B in [51], we have
Gâˆ’1âˆ’ËœGâˆ’1= (1âˆ’Î³PÏ€)âˆ’(1âˆ’Î³PËœÏ€) =Î³âˆ†.
Left multiply by ËœGand right multiply by G,
ËœG=Î³ËœGâˆ†G+G. (45)
39Left multiply by Gand right multiply by ËœG,
ËœG=Î³Gâˆ†ËœG+G. (46)
Substituting the right-hand side in (45) into ËœGin (46), then
ËœG=G+Î³Gâˆ†G+Î³2Gâˆ†ËœGâˆ†G.
So we have
JÏ„(Ï€â€²)âˆ’JÏ„(Ï€) =râŠ¤
Ï„(ËœGâˆ’G)ÏÏ„=Î³râŠ¤
Ï„Gâˆ†GÏÏ„+Î³2râŠ¤
Ï„Gâˆ†ËœGâˆ†GÏÏ„. (47)
Note that râŠ¤
Ï„G=vÏ€
Ï„âŠ¤, where vis the value function on the state space. We also have GÏÏ„=1
1âˆ’Î³Î½Ï€
Ï„,
where Î½Ï€
Ï„is the state visitation distribution vector. So,
JÏ„(ËœÏ€)âˆ’JÏ„(Ï€) =râŠ¤
Ï„(ËœGâˆ’G)ÏÏ„=Î³
1âˆ’Î³vÏ€
Ï„âŠ¤âˆ†Î½Ï€
Ï„+Î³2
1âˆ’Î³vÏ€
Ï„âŠ¤âˆ†ËœGâˆ†Î½Ï€
Ï„.
Consider the first termÎ³
1âˆ’Î³vÏ€
Ï„âŠ¤âˆ†Î½Ï€
Ï„, similar to Equation (50) in [51], we have
Î³vÏ€
Ï„âŠ¤âˆ†Î½Ï€
Ï„=vÏ€
Ï„âŠ¤(PÏ€â€²
Ï„âˆ’PÏ€
Ï„)Î½Ï€
Ï„
=X
sÎ½Ï€
Ï„(s)X
sâ€²X
a(Ï€â€²(a|s)âˆ’Ï€(a|s))PÏ„(sâ€²|s, a)Î³vÏ€
Ï„(sâ€²)
=X
sÎ½Ï€
Ï„(s)X
a(Ï€â€²(a|s)âˆ’Ï€(a|s))"
r(s) +X
sâ€²PÏ„(sâ€²|s, a)Î³vÏ€
Ï„(sâ€²)âˆ’v(s)#
=X
sÎ½Ï€
Ï„(s)X
a(Ï€â€²(a|s)âˆ’Ï€(a|s))AÏ€
Ï„(s, a)(48)
Since we haveP
aÏ€(a|s)AÏ€
Ï„(s, a) = 0 , we have
Î³vÏ€
Ï„âŠ¤âˆ†Î½Ï€
Ï„=X
sÎ½Ï€
Ï„(s)X
aÏ€â€²(a|s)AÏ€
Ï„(s, a) = E
sâˆ¼Î½Ï€
Ï„
aâˆ¼Ï€â€²(Â·|s)[AÏ€
Ï„(s, a)].
Combine (47) and the above equation, we have the following for the second term:
Î³2
1âˆ’Î³vÏ€
Ï„âŠ¤âˆ†ËœGâˆ†Î½Ï€
Ï„=JÏ„(Ï€â€²)âˆ’JÏ„(Ï€)âˆ’1
1âˆ’Î³E
sâˆ¼Î½Ï€
Ï„
aâˆ¼Ï€â€²(Â·|s)[AÏ€
Ï„(s, a)].
Then we need to show Î³2
1âˆ’Î³vÏ€
Ï„âŠ¤âˆ†ËœGâˆ†Î½Ï€
Ï„â‰¤CÏ€
Ï„(Ï€â€²).
First, by HÃ¶lderâ€™s inequality,
Î³2
1âˆ’Î³vÏ€
Ï„âŠ¤âˆ†ËœGâˆ†Î½Ï€
Ï„â‰¤Î³
1âˆ’Î³âˆ¥Î³vÏ€
Ï„âŠ¤âˆ†âˆ¥âˆžâˆ¥ËœGâˆ†Î½Ï€
Ï„âˆ¥1.
Similar to (48), each element in the vector Î³vÏ€
Ï„âŠ¤âˆ†isP
a(Ï€â€²(a|s)âˆ’Ï€(a|s))AÏ€
Ï„(s, a), then we have
âˆ¥Î³vÏ€
Ï„âŠ¤âˆ†âˆ¥âˆžâ‰¤X
a|Ï€â€²(a|s)âˆ’Ï€(a|s)|AÏ€
Ï„(s, a)â‰¤2AmaxDmax
TV(Ï€||Ï€â€²).
From the Lemma 3 of [1], we have
âˆ¥ËœGâˆ†Î½Ï€
Ï„âˆ¥1â‰¤2
1âˆ’Î³Esâˆ¼Î½Ï€Ï„[DTV(Ï€(Â·|s)||Ï€â€²(Â·|s))].
Therefore, we have
Î³2
1âˆ’Î³vÏ€
Ï„âŠ¤âˆ†ËœGâˆ†Î½Ï€
Ï„â‰¤CÏ€
Ï„(Ï€â€²) =4Î³Amax
(1âˆ’Î³)2Dmax
TV(Ï€||Ï€â€²)Esâˆ¼Î½Ï€Ï„[DTV(Ï€(Â·|s)||Ï€â€²(Â·|s))].
Then the bounds hold.
40Lemma 17. Suppose that Assumptions 1, 2 hold. For any task Ï„, any bounded parameters Î¸andÎ¸â€²,
andi= 1or2, the following bound holds for both i= 1and2:
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¤1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
+2Î³Amax
(1âˆ’Î³)2ÏµD2
Ï„,i(Ë†Ï€Î¸,Ë†Ï€Î¸â€²)
and
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¥1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
âˆ’2Î³Amax
(1âˆ’Î³)2ÏµD2
Ï„,i(Ë†Ï€Î¸,Ë†Ï€Î¸â€²).
Proof. The proof follows similar lines of Theorem 1 in [ 51] and Corollary 1 and 2 in [ 1]. For the
sake of self-containedness, we provide the complete proof.
We show the first inequality. The second inequality follows a similar way. From Lemma 16,
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)âˆ’1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
â‰¤4Î³Amax
(1âˆ’Î³)2Dmax
TV(Ë†Ï€Î¸||Ë†Ï€Î¸â€²)Esâˆ¼Î½Ë†Ï€Î¸Ï„[DTV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))].
From Assumption 2, Î½Ë†Ï€Î¸(s)â‰¥Ïµfor any sâˆˆ A. Also, DTV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))â‰¥0for any sâˆˆ A.
Then, we have
ÏµDmax
TV(Ë†Ï€Î¸||Ë†Ï€Î¸â€²)â‰¤Esâˆ¼Î½Ë†Ï€Î¸Ï„[DTV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))].
From Jensenâ€™s inequality, we have
Esâˆ¼Î½Ë†Ï€Î¸Ï„[DTV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))]2â‰¤Esâˆ¼Î½Ë†Ï€Î¸Ï„
D2
TV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))
.
From the above three inequalities, we have
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)âˆ’1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
â‰¤4Î³Amax
(1âˆ’Î³)2ÏµEsâˆ¼Î½Ë†Ï€Î¸Ï„
D2
TV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))
.
(49)
From [8], we have
D2
TV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))â‰¤1
2DKL(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s)),
and
D2
TV(Ë†Ï€Î¸(Â·|s)||Ë†Ï€Î¸â€²(Â·|s))â‰¤1
2DKL(Ë†Ï€Î¸â€²(Â·|s)||Ë†Ï€Î¸(Â·|s)).
Therefore,
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¤1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
+2Î³Amax
(1âˆ’Î³)2ÏµD2
Ï„,1(Ë†Ï€Î¸,Ë†Ï€Î¸â€²),
and
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¤1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
+2Î³Amax
(1âˆ’Î³)2ÏµD2
Ï„,2(Ë†Ï€Î¸,Ë†Ï€Î¸â€²).
Lemma 18. Consider the softmax policy with function approximation shown in Section 5.1. Suppose
that Assumptions 1, 2, and 3 hold. For any task Ï„, and any softmax policies parameterized by bounded
Î¸andÎ¸â€², the following bound holds:
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¤1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
+4Î³AmaxL2
1
(1âˆ’Î³)2Ïµâˆ¥Î¸âˆ’Î¸â€²âˆ¥2
and
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)â‰¥1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
âˆ’4Î³AmaxL2
1
(1âˆ’Î³)2Ïµâˆ¥Î¸âˆ’Î¸â€²âˆ¥2.
41Proof. From (36), for any Î¸âˆˆRn,
âˆ‡Î¸Ë†Ï€Î¸(a|s) = Ë†Ï€Î¸(a|s)âˆ‡Î¸fÎ¸(s, a)âˆ’Ë†Ï€Î¸(a|s)R
Aâˆ‡Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²
R
Aexp (fÎ¸(s, aâ€²))daâ€².
Then,
âˆ¥âˆ‡Î¸Ë†Ï€Î¸(a|s)âˆ¥ â‰¤Ë†Ï€Î¸(a|s)âˆ¥âˆ‡Î¸fÎ¸(s, a)âˆ¥+ Ë†Ï€Î¸(a|s)R
Aâˆ‡Î¸fÎ¸(s, aâ€²) exp ( fÎ¸(s, aâ€²))daâ€²
R
Aexp (fÎ¸(s, aâ€²))daâ€²
â‰¤2Ë†Ï€Î¸(a|s)L1
From the mean value theorem, we have
|Ë†Ï€Î¸(a|s)âˆ’Ë†Ï€Î¸â€²(a|s)| â‰¤2Ë†Ï€Ï•(a)(a|s)L1âˆ¥Î¸âˆ’Î¸â€²âˆ¥,
where Ï•(a) =Î´(a)Î¸+ (1âˆ’Î´(a))Î¸â€²and0â‰¤Î´(a)â‰¤1. So,
1
2X
aâˆˆA|Ë†Ï€Î¸(a|s)âˆ’Ë†Ï€Î¸â€²(a|s)| â‰¤L1âˆ¥Î¸âˆ’Î¸â€²âˆ¥.
From (49), we have
JÏ„(Ë†Ï€Î¸â€²)âˆ’JÏ„(Ë†Ï€Î¸)âˆ’1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Î¸Ï„
aâˆ¼Ë†Ï€Î¸â€²(Â·|s)
AË†Ï€Î¸Ï„(s, a)
â‰¤4Î³AmaxL2
1
(1âˆ’Î³)2Ïµâˆ¥Î¸âˆ’Î¸â€²âˆ¥2.
We use the same way to show another inequality.
N.2 Proof of Theorems 3 and 4
Proof of Theorem 3. When the requirement of Theorem 1, Î»â‰¥2Amax, is satisfied, From Assump-
tion 4 and Theorem 1, for both i= 1and2,
1
TTX
t=1Et
max
Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))âˆ’EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•t, Î», Ï„))]]
â‰¤1
TTX
t=1Eth
hi
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2i
â‰¤hi 
1
TTX
t=1Eth
âˆ¥âˆ‡Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•t, Î», Ï„))]âˆ¥2i!
â‰¤hiKi
T+Miâˆš
T(50)
where the constants KiandMiare shown in Theorem 1. The last inequality sign comes from that hi
is a concave function and Jensenâ€™s inequality.
LetË†Ï€Î¸â€²Ï„(Ï•) =Alg(i)(Ë†Ï€Ï•, Î», Ï„)for any meta-parameter Ï•. From the definition of the within-task
algorithm, we have
E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Ï•)(Â·|s)h
QË†Ï€Ï•Ï„(s, a)i
âˆ’Î»D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸â€²Ï„(Ï•))â‰¥ E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸âˆ—Ï„(Â·|s)h
QË†Ï€Ï•Ï„(s, a)i
âˆ’Î»D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„).
This is equivalent to
E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Ï•)(Â·|s)h
AË†Ï€Ï•Ï„(s, a)i
âˆ’Î»D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸â€²
Ï„(Ï•))â‰¥ E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸âˆ—Ï„(Â·|s)h
AË†Ï€Ï•Ï„(s, a)i
âˆ’Î»D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—
Ï„).
42when Î»â‰¥2Î³Amax
(1âˆ’Î³)Ïµ, from the second inequality in Lemma 17 and the above inequality,
JÏ„(Ë†Ï€Î¸â€²Ï„(Ï•))âˆ’JÏ„(Ë†Ï€Ï•)â‰¥1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Ï•)(Â·|s)h
AË†Ï€Ï•Ï„(s, a)i
âˆ’2Î³Amax
(1âˆ’Î³)2ÏµD2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸â€²Ï„(Ï•))
â‰¥1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸â€²Ï„(Ï•)(Â·|s)h
AË†Ï€Ï•Ï„(s, a)i
âˆ’Î»
1âˆ’Î³D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸â€²Ï„(Ï•))
â‰¥1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸âˆ—Ï„(Â·|s)h
AË†Ï€Ï•Ï„(s, a)i
âˆ’Î»
1âˆ’Î³D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„).
From the second inequality in Lemma 17,
JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Ë†Ï€Ï•)â‰¤1
1âˆ’Î³E
sâˆ¼Î½Ë†Ï€Ï•
Ï„
aâˆ¼Ë†Ï€Î¸âˆ—Ï„(Â·|s)h
AË†Ï€Ï•Ï„(s, a)i
+2Î³Amax
(1âˆ’Î³)2ÏµD2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„).
From the last two inequalities,
JÏ„(Ë†Ï€Î¸â€²Ï„(Ï•))âˆ’JÏ„(Ë†Ï€Î¸âˆ—Ï„)â‰¥ âˆ’(2Î³Amax
(1âˆ’Î³)2Ïµ+Î»
1âˆ’Î³)D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„),
i.e.,
JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))â‰¤(2Î³Amax
(1âˆ’Î³)2Ïµ+Î»
1âˆ’Î³)D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„).
Then,
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))]â‰¤(2Î³Amax
(1âˆ’Î³)2Ïµ+Î»
1âˆ’Î³)EÏ„âˆ¼P(Î“)[D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„)].
LetÏ•âˆ—= arg max Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))], we have
EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•âˆ—, Î», Ï„))]â‰¥max
Ï•EÏ„âˆ¼P(Î“)[JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))].
Therefore,
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•âˆ—, Î», Ï„))]â‰¤min
Ï•EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•, Î», Ï„))]
â‰¤min
Ï•(2Î³Amax
(1âˆ’Î³)2Ïµ+Î»
1âˆ’Î³)EÏ„âˆ¼P(Î“)[D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„)]
Since
min
Ï•EÏ„âˆ¼P(Î“)[D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„)] =Vari(P(Î“)),
we have
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•âˆ—, Î», Ï„))]â‰¤(2Î³Amax
(1âˆ’Î³)2Ïµ+Î»
1âˆ’Î³)Vari(P(Î“)).
Note that in the above analysis, we need Î»â‰¥2Amax and also Î»â‰¥2Î³Amax
(1âˆ’Î³)Ïµ. So, we select we select
Î»=2Amax
(1âˆ’Î³)Ïµto satisfy the requirement. When Î»=2Amax
(1âˆ’Î³)Ïµ, we have
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•âˆ—, Î», Ï„))]â‰¤2(1 + Î³)Amax
(1âˆ’Î³)2ÏµVari(P(Î“)). (51)
From (50) and (51) we have
1
TTX
t=1Eth
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(i)(Ë†Ï€Ï•t, Î», Ï„))]i
â‰¤hiKi
T+Miâˆš
T
+2(1 + Î³)Amax
(1âˆ’Î³)2ÏµVari(P(Î“)).
43Proof of Theorem 4. Similar to the above proof of Theorem 3. The difference is using two inequalities
in Lemma 18 instead of those in Lemma 17 and using Theorem 2 for convergence instead of Theorem
1.
The requirement of Theorem 2 is Î» >(6L2
1+ 2L2)Amax, and the requirement of Lemma 18 is
Î»â‰¥4Î³AmaxL2
1
(1âˆ’Î³)Ïµ. Therefore, we select Î»=(6L2
1+2L2)Amax
(1âˆ’Î³)Ïµ. Then, the bound is
1
TTX
t=1Eth
EÏ„âˆ¼P(Î“)[JÏ„(Ë†Ï€Î¸âˆ—Ï„)âˆ’JÏ„(Alg(3)(Ë†Ï€Ï•t, Î», Ï„))]i
â‰¤h3K3
T+M3âˆš
T
+4Î³L2
1Amax
(1âˆ’Î³)2Ïµ+Î»
1âˆ’Î³
Var3(P(Î“)),
â‰¤h3K3
T+M3âˆš
T
+((6 + 4 Î³)L2
1+ 2L2)Amax
(1âˆ’Î³)2ÏµVar3(P(Î“)),
N.3 Clarification of Amax
In all the proofs in Sections N.1 and N.1, we can replace as Amax toAâ€²
max, where Aâ€²
max is defined by
the maximum advantage function value of policy Ë†Ï€Ï•â€², where Ï•â€²= arg min Ï•EÏ„âˆ¼P(Î“)[D2
Ï„,i(Ë†Ï€Ï•,Ë†Ï€Î¸âˆ—Ï„)].
It is easy to see Aâ€²
maxâ‰¤Amax. For simplification of the assumption statements, theorem statements,
and convenience of the proofs, we keep Amax in the proofs and Theorems 3 and 4. We actually
can make the bound in Theorems 3 and 4 tighter by replacing Amax toAâ€²
max. In the verification of
the theoretical results of Section 6, we select Î»based on Aâ€²
max and verify the tighter bounds by the
experiments.
O Proofs of Remarks
Proof of part (i) of Remark 1 . If the MDP MÏ„is ergodic, there exists a policy Ë†Ï€such that Î½Ë†Ï€
Ï„(s)â‰¥
Ïµ0. AsÏ•is bounded, the probability (or probability density) of each action of the softmax policy is
larger than 0and lower bounded by a Ïµ1>0. Therefore, the action probability of the policy Ë†Ï€(a|s)
can be upper bounded by Ë†Ï€Ï•(a|s)/Ïµ1for any a. Therefore, Î½Ë†Ï€Ï•Ï„(s)â‰¥Ïµ0/Ïµ1.
Proof of part (ii) of Remark 1 . If the initial state distribution ÏÏ„hasÏÏ„(s)>0for any sâˆˆ S. Since
Sis bounded, ÏÏ„(s)â‰¥Ïµ2for any sâˆˆ S. Then, Î½Ë†Ï€Ï•Ï„(s)â‰¥(1âˆ’Î³)Ïµ2.
P Limitations
In this paper, we provide several theorems, where the hyper-parameter selection, e.g., Î», is provided
by the theorems. The theoretical analysis usually chooses hyper-parameters, which are sometimes
conservative. In practice, we can tune them to improve the performance.
44NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction, including the main contribution statement and
related works, accurately reflect the paperâ€™s contributions and scope.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide the limitations in the Appendix.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
45Justification: All proofs are provided in Appendix
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide all details of the information needed to reproduce the main
experimental results in the experiment section and in Appendix A and B.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
46Answer: [Yes]
Justification: We provide open access to the data and code with sufficient instructions in the
supplemental material.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide all training details in Appendix A and B.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide it in the section of the experiment.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
47â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We provide the information in the beginning of the Appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: It is followed.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This paper presents work whose goal is to advance the field of Machine
Learning. There is no potential societal consequence.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
48â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
49â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
50