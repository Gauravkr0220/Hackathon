Accelerated Regularized Learning
in Finiteğ‘-Person Games
Kyriakos Lotidis
Stanford University
klotidis@stanford.eduAngeliki Giannou
University of Wisconsinâ€“Madison
giannou@wisc.edu
Panayotis Mertikopoulos
Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP
LIG 38000 Grenoble, France
panayotis.mertikopoulos@imag.frNicholas Bambos
Stanford University
bambos@stanford.edu
Abstract
Motivated by the success of Nesterovâ€™s accelerated gradient algorithm for con-
vex minimization problems, we examine whether it is possible to achieve similar
performance gains in the context of online learning in games. To that end, we
introduce a family of accelerated learning methods, which we call â€œfollow the
accelerated leaderâ€ (FTXL ), and which incorporates the use of momentum within
the general framework of regularized learning â€“ and, in particular, the exponen-
tial / multiplicative weights algorithm and its variants. Drawing inspiration and
techniques from the continuous-time analysis of Nesterovâ€™s algorithm, we show
thatFTXL converges locally to strict Nash equilibria at a superlinear rate, achiev-
ing in this way an exponential speed-up over vanilla regularized learning methods
(which, by comparison, converge to strict equilibria at a geometric , linear rate).
Importantly, FTXL maintains its superlinear convergence rate in a broad range
of feedback structures, from deterministic, full information models to stochastic,
realization-based ones, and even when run with bandit, payoff-based information,
where players are only able to observe their individual realized payoffs.
1 Introduction
One of the most important milestones in convex optimization was Nesterovâ€™s accelerated gradient
(NAG ) algorithm, as proposed by Nesterov [38] in 1983. The groundbreaking achievement of
Nesterovâ€™s algorithm was that it attained an O(1/ğ‘‡2)rate of convergence in Lipschitz smooth convex
minimization problems, thus bridging a decades-old gap between the O(1/ğ‘‡)convergence rate of
ordinary gradient descent and the corresponding Î©(1/ğ‘‡2)lower bound for said class [ 37]. In this way,
Nesterovâ€™s accelerated gradient algorithm opened the door to acceleration in optimization, leading in
turn to a wide range of other, likewise influential schemes â€“ such as FISTA and its variants [ 3] â€“ and
jumpstarting a vigorous field of research that remains extremely active to this day.
Somewhat peculiarly, despite the great success that NAG has enjoyed in all fields where optimization
plays a major role â€“ and, in particular, machine learning and data science â€“ its use has not percolated to
the adjoining field of game theory as a suitable algorithm for learning Nash equilibria. Historically, the
reasons for this are easy to explain: despite intense scrutiny by the community and an extensive corpus
of literature dedicated to deconstructing the algorithmâ€™s guarantees, NAG â€™s update structure remains
quite opaque â€“ and, to a certain extent, mysterious. Because of this, Nesterovâ€™s algorithm could not
be considered as a plausible learning scheme that could be employed by boundedly rational human
agents involved in a repeated game. Given that this was the predominant tenet in economic thought
38th Conference on Neural Information Processing Systems (NeurIPS 2024).at the time, the use of Nesterovâ€™s algorithm in a game-theoretic context has not been extensively
explored, to the best of our knowledge.
On the other hand, as far as applications to machine learning and artificial intelligence are concerned,
the focus on human agents is no longer a limiting factor. In most current and emerging applications of
game-theoretic learning â€“ from multi-agent reinforcement learning to adversarial models in machine
learning â€“ the learning agents are algorithms whose computational capacity is only limited by the
device on which they are deployed. In view of this, our paper seeks to answer the following question:
Can Nesterovâ€™s accelerated gradient scheme be deployed in a game-theoretic setting?
And, if so, is it possible to achieve similar performance gains as in convex optimization?
Our contributions in the context of related work. The answer to the above questions is not easy to
guess. On the one hand, given that game theory and convex optimization are fundamentally different
fields, a reasonable guess would be â€œnoâ€ â€“ after all, finding a Nash equilibrium is a PPAD -complete
problem [ 9], whereas convex minimization problems are solvable in polynomial time [ 7]. On the
other, since in the context of online learning each player would have every incentive to use the most
efficient unilateral optimization algorithm at their disposal, the use of NAG methods cannot be easily
discarded from an algorithmic viewpoint.
Our paper examines if it is possible to obtain even a partially positive answer to the above question
concerning the application of Nesterovâ€™s accelerated gradients techniques to learning in games. We
focus throughout on the class of finite ğ‘-person games where, due to the individual concavity of
the playersâ€™ payoff functions, the convergence landscape of online learning in games is relatively
well-understood â€“ at least, compared to non-concave games. In particular, it is known that regularized
learning algorithms â€“ such as â€œfollow the regularized leaderâ€ ( FTRL ) and its variants â€“ converge
locally to strict Nash equilibria at a geometric rate [ 18], and strict equilibria are the only locally stable
and attracting limit points of regularized learning in the presence of randomness and/or uncertainty
[11,17,23]. In this regard, we pose the question of ( i) whether regularized learning schemes like
FTRL can be accelerated; and ( ii) whether the above properties are enhanced by this upgrade.
We answer both questions in the positive. First, we introduce an accelerated regularized scheme,
in both continuous and discrete time, which we call â€œfollow the accelerated leaderâ€ (FTXL ). In
continuous time, our scheme can be seen as a fusion of the continuous-time analogue of NAG proposed
by Su, Boyd, and CandÃ¨s [41] and the dynamics of regularized learning studied by Mertikopoulos
& Sandholm [31] â€“ see also [ 5,6,19,24,29,32â€“36,42] and references therein. We show that the
resulting dynamics exhibit the same qualitative equilibrium convergence properties as the replicator
dynamics of Taylor & Jonker [43] (the most widely studied instance of FTRL in continuous time).
However, whereas the replicator dynamics converge to strict Nash equilibria at a linear rate, the
FTXL dynamics converge superlinearly .
In discrete time, we likewise propose an algorithmic implementation of FTXL which can be applied
in various information context: ( i)full information , that is, when players observe their entire mixed
payoff vector; ( ii)realization-based feedback , i.e., when players get to learn the â€œwhat-ifâ€ payoff of
actions that they did not choose; and ( iii)bandit, payoff-based feedback , where players only observe
their realized, in-game payoff, and must rely on statistical estimation techniques to reconstruct
their payoff vectors. In all cases, we show that FTXL maintains the exponential speedup described
above, and converges to strict Nash equilibria at a superlinear rate (though the subleading term in
the algorithmâ€™s convergence rate becomes increasingly worse as less information is available). We
find this feature of FTXL particularly intriguing as superlinear convergence rates are often associated
to methods that are second-order in space , not time; the fact that this is achieved even with bandit
feedback is quite surprising in this context.
Closest to our work is the continuous-time, second-order replicator equation studied by Laraki
& Mertikopoulos [25] in the context of evolutionary game theory, and derived through a model
of pairwise proportional imitation of â€œlong-term successâ€. The dynamics of [ 25] correspond to
the undamped, continuous-time version of FTXL with entropic regularization, and the equilibrium
convergence rate obtained by [ 25] agrees with our analysis. Other than that, the dynamics of FlÃ¥m
& Morgan [12] also attempted to exploit a Newtonian structure, but they do not yield favorable
convergence properties in a general setting. The inertial dynamics proposed in [ 26] likewise sought to
leverage an inertial structure combined with the Hessianâ€“Riemannian underpinnings of the replicator
2dynamics, but the resulting replicator equation was not even well-posed (in the sense that its solutions
exploded in finite time).
More recently, Gao & Pavel [15,16]considered a second-order, inertial version of the dynamics of
mirror descent in continuous games, and examined their convergence in the context of variational
stability [ 34]. Albeit related at a high level to our work (given the link between mirror descent and
regularized learning), the dynamics of Gao & Pavel [15,16]are actually incomparable to our own,
and there is no overlap in our techniques or results. Other than that, second-order dynamics in games
have also been studied in continuous time within the context of control-theoretic passivity, yielding
promising results in circumventing the impossibility results of Hart & Mas-Colell [21], cf. Gao &
Pavel [13,14], Mabrok & Shamma [30], Toonsi & Shamma [44], and references therein. However,
the resulting dynamics are also different, and we do not see a way of obtaining comparable rates in
our setting.
2 Preliminaries
In this section, we outline some notions and definitions required for our analysis. Specifically,
we introduce the framework of finite ğ‘-player games, we discuss the solution concept of a Nash
equilibrium, and we present the main ideas of regularized learning in games.
2.1. Finite games. In this work, we focus exclusively with finite games in normal form. Such games
consist of a finite set of players N={1,...,ğ‘}, each of whom has a finite set of actions â€“ orpure
strategies â€“ğ›¼ğ‘–âˆˆAğ‘–and a payoff function ğ‘¢ğ‘–:Aâ†’â„, where A:=Q
ğ‘–âˆˆNAğ‘–denotes the set of all
possible action profiles ğ›¼=(ğ›¼1,...,ğ›¼ğ‘). To keep track of all this, a finite game with the above
primitives will be denoted as Î“â‰¡Î“(N,A,ğ‘¢).
In addition to pure strategies, players may also randomize their choices by employing mixed strategies ,
that is, by choosing probability distributions ğ‘¥ğ‘–âˆˆXğ‘–:= Î”(Ağ‘–)over their pure strategies, where
Î”(Ağ‘–)denotes the probability simplex over Ağ‘–. Now, given a strategy profile ğ‘¥=(ğ‘¥1,...,ğ‘¥ğ‘)âˆˆ
X:=Q
ğ‘–âˆˆNXğ‘–, we will use the standard shorthand ğ‘¥=(ğ‘¥ğ‘–;ğ‘¥âˆ’ğ‘–)to highlight the mixed strategy ğ‘¥ğ‘–of
playerğ‘–against the mixed strategy profile ğ‘¥âˆ’ğ‘–âˆˆXâˆ’ğ‘–:=Q
ğ‘—â‰ ğ‘–Xğ‘—of all other players. We also define:
1. The mixed payoff of playerğ‘–underğ‘¥as
ğ‘¢ğ‘–(ğ‘¥)=ğ‘¢ğ‘–(ğ‘¥ğ‘–;ğ‘¥âˆ’ğ‘–)=âˆ‘ï¸
ğ›¼1âˆˆA1Â·Â·Â·âˆ‘ï¸
ğ›¼ğ‘âˆˆAğ‘ğ‘¥1ğ›¼1...ğ‘¥ğ‘ğ›¼ğ‘ğ‘¢ğ‘–(ğ›¼1,...,ğ›¼ğ‘) (1)
2. The mixed payoff vector of playerğ‘–underğ‘¥as
ğ‘£ğ‘–(ğ‘¥)=âˆ‡ğ‘¥ğ‘–ğ‘¢ğ‘–(ğ‘¥)=(ğ‘¢ğ‘–(ğ›¼ğ‘–;ğ‘¥âˆ’ğ‘–))ğ›¼ğ‘–âˆˆAğ‘– (2)
In words,ğ‘£ğ‘–(ğ‘¥)collects the expected rewards ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘¥):=ğ‘¢ğ‘–(ğ›¼ğ‘–;ğ‘¥âˆ’ğ‘–)of each action ğ›¼ğ‘–âˆˆAğ‘–of
playerğ‘–âˆˆNagainst the mixed strategy profile ğ‘¥âˆ’ğ‘–of all other players. Finally, we write ğ‘£(ğ‘¥)=
(ğ‘£1(ğ‘¥),...,ğ‘£ğ‘(ğ‘¥))for the concatenation of the playersâ€™ mixed payoff vectors.
In terms of solution concepts, we will say that ğ‘¥âˆ—is aNash equilibrium (NE) if no player can benefit
by unilaterally deviating from their strategy, that is
ğ‘¢ğ‘–(ğ‘¥âˆ—)â‰¥ğ‘¢ğ‘–(ğ‘¥ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)for allğ‘¥ğ‘–âˆˆXğ‘–and allğ‘–âˆˆN. (NE)
Moreover, we say that ğ‘¥âˆ—is astrict Nash equilibrium if(NE) holds as a strict inequality for all ğ‘¥ğ‘–â‰ ğ‘¥âˆ—
ğ‘–,
ğ‘–âˆˆN, i.e., if any deviation from ğ‘¥âˆ—
ğ‘–results in a strictly worse payoff for the deviating player ğ‘–âˆˆN.
It is straightforward to verify that a strict equilibrium ğ‘¥âˆ—âˆˆXis also pure in the sense that each player
assigns positive probability only to a single pure strategy ğ›¼âˆ—
ğ‘–âˆˆAğ‘–. Finally, we denote the support of
a strategyğ‘¥as the set of actions with non-zero probability mass, i.e., supp(ğ‘¥)={ğ›¼âˆˆA:ğ‘¥ğ›¼>0}.
2.2. Regularized learning in games. In the general context of finite games, the most widely used
learning scheme is the family of algorithms and dynamics known as â€œfollow the regularized leaderâ€
(FTRL ). In a nutshell, the main idea behind FTRL is that each player ğ‘–âˆˆNplays a â€œregularizedâ€
best response to their cumulative payoff over time, leading to the continuous-time dynamics
Â¤ğ‘¦ğ‘–(ğ‘¡)=ğ‘£ğ‘–(ğ‘¥(ğ‘¡))ğ‘¥ğ‘–(ğ‘¡)=ğ‘„ğ‘–(ğ‘¦ğ‘–(ğ‘¡)) (FTRL-D)
3where
ğ‘„ğ‘–(ğ‘¦ğ‘–)=arg maxğ‘¥ğ‘–âˆˆXğ‘–{âŸ¨ğ‘¦ğ‘–,ğ‘¥ğ‘–âŸ©âˆ’â„ğ‘–(ğ‘¥ğ‘–)} (3)
denotes the regularized best response â€“ or mirror â€“ map of player ğ‘–âˆˆN, andâ„ğ‘–:Xğ‘–â†’â„is a
strongly convex function known as the methodâ€™s regularizer . Accordingly, in discrete time, this leads
to the algorithm
ğ‘¦ğ‘–,ğ‘›+1=ğ‘¦ğ‘–,ğ‘›+ğ›¾Ë†ğ‘£ğ‘–,ğ‘›ğ‘¥ğ‘–,ğ‘›=ğ‘„ğ‘–(ğ‘¦ğ‘–,ğ‘›) (FTRL)
whereğ›¾>0is a hyperparameter known as the algorithmâ€™s learning rate (orstep-size ) and Ë†ğ‘£ğ‘–,ğ‘›is a
black-box â€œpayoff signalâ€ that carries information about ğ‘£ğ‘–(ğ‘¥ğ‘›). In the simplest case, when players
have full information about the game being played and the actions taken by their opponents, we have
Ë†ğ‘£ğ‘–,ğ‘›=ğ‘£ğ‘–(ğ‘¥ğ‘›); in more information-depleted environments (such as learning with payoff-based, bandit
feedback), Ë†ğ‘£ğ‘–,ğ‘›is a reconstruction of ğ‘£ğ‘–(ğ‘¥ğ‘›)based on whatever information is at hand.
For concreteness, we close this section with the prototypical example of FTRL methods, the exponen-
tial / multiplicative weights (EW) algorithm. Going back to [ 2,28,45], this method is generated by
the negentropy regularizer â„ğ‘–(ğ‘¥ğ‘–)=P
ğ›¼ğ‘–âˆˆAğ‘–ğ‘¥ğ‘–ğ›¼ğ‘–logğ‘¥ğ‘–ğ›¼ğ‘–, which yields the EW update rule
ğ‘¦ğ‘–,ğ‘›+1=ğ‘¦ğ‘–,ğ‘›+ğ›¾Ë†ğ‘£ğ‘–,ğ‘›ğ‘¥ğ‘–,ğ‘›=Î›ğ‘–(ğ‘¦ğ‘–,ğ‘›):=exp(ğ‘¦ğ‘–,ğ‘›)
âˆ¥exp(ğ‘¦ğ‘–,ğ‘›)âˆ¥1(EW)
and, in the continuous-time limit ğ›¾â†’0, the exponential weights dynamics
Â¤ğ‘¦ğ‘–(ğ‘¡)=ğ‘£ğ‘–(ğ‘¥(ğ‘¡))ğ‘¥ğ‘–(ğ‘¡)=Î›ğ‘–(ğ‘¦ğ‘–(ğ‘¡)). (EWD)
In the above, Î›ğ‘–denotes the regularized best response induced by the methodâ€™s entropic regularizer,
which is known colloquially as a logit best response â€“ or, even more simply, as the logit map . To
make the notation more compact in the sequel, we will write ğ‘„=(ğ‘„ğ‘–)ğ‘–âˆˆNandÎ›=(Î›ğ‘–)ğ‘–âˆˆNfor the
ensemble of the playersâ€™ regularized / logit best response maps.
Remark 1.To streamline our presentation, in the main part of the paper, quantitative results will
be stated for the special case of the EWsetup above. In Appendix A, we discuss more general
decomposable regularizers of the form â„ğ‘–(ğ‘¥ğ‘–)=P
ğ›¼ğ‘–âˆˆAğ‘–ğœƒğ‘–(ğ‘¥ğ‘–)whereğœƒğ‘–:[0,1]â†’â„is continuous
on[0,1], and hasğœƒâ€²â€²(ğ‘¥)>0for allğ‘¥âˆˆ (0,1]andlimğ‘¥â†’0+ğœƒâ€²(ğ‘¥)=âˆ’âˆ. Although this set of
assumptions can be relaxed, it leads to the clearest presentation of our results, so it will suffice for us.
Remark 2.Throughout the paper, we will interchangeably use Â¤ğ‘”(ğ‘¡)andğ‘‘ğ‘”/ğ‘‘ğ‘¡to denote the time
derivative of ğ‘”(ğ‘¡). This dual notation allows us to adopt whichever form is most convenient in the
given context. Moreover, for a process ğ‘”, we will use the notation ğ‘”(ğ‘¡)forğ‘¡â‰¥0if it evolves in
continuous time, and ğ‘”ğ‘›forğ‘›âˆˆâ„•if it evolves in discrete time steps, omitting the time-index when it
is clear from context.
3 Combining acceleration with regularization: First insights and results
In this section, we proceed to illustrate how Nesterovâ€™s accelerated gradient ( NAG ) method can be
combined with FTRL . To keep things as simple as possible, we focus on the continuous-time limit, so
we do not have to worry about the choice of hyperparameters, the construction of black-box models
for the playersâ€™ payoff vectors, etc.
3.1. Nesterovâ€™s accelerated gradient algorithm. We begin by discussing Nesterovâ€™s accelerated
gradient algorithm as presented in Nesterovâ€™s seminal paper [ 38] in the context of unconstrained
smooth convex minimization. Specifically, given a Lipschitz smooth convex function ğ‘“:â„ğ‘‘â†’â„,
the algorithm unfolds iteratively as
ğ‘¥ğ‘›+1=ğ‘¤ğ‘›âˆ’ğ›¾âˆ‡ğ‘“(ğ‘¤ğ‘›)
ğ‘¤ğ‘›+1=ğ‘¥ğ‘›+1+ğ‘›
ğ‘›+3(ğ‘¥ğ‘›+1âˆ’ğ‘¥ğ‘›)(NAG)
whereğ‘¤1=ğ‘¥1is initialized arbitrarily and ğ›¾ > 0is a step-size parameter (typically chosen as
ğ›¾â†1/ğ¿whereğ¿is the Lipschitz smoothness modulus of ğ‘“). The specific iterative structure of
(NAG) â€“ and, in particular the â€œ 3â€ in the denominator â€“ can appear quite mysterious; nevertheless,
(NAG) otherwise offers remarkable perfomance gains, improving in particular the rate of convergence
4of gradient methods from O(1/ğ‘‡)toO(1/ğ‘‡2)[38], and matching in this way the corresponding
Î©(1/ğ‘‡2)lower bound for the minimization of smooth convex functions [37].1
This groundbreaking result has since become the cornerstone of a vast and diverse literature expanding
on the properties of (NAG) and trying to gain a deeper understanding of the â€œhowâ€ and â€œwhyâ€ of its
update structure. One perspective that has gained significant traction in this regard is the continuous-
time approach of Su et al. [40, 41]; combining the two equations in (NAG) into
ğ‘¥ğ‘›+1âˆ’2ğ‘¥ğ‘›+ğ‘¥ğ‘›âˆ’1âˆšğ›¾=âˆ’âˆšğ›¾âˆ‡ğ‘“(ğ‘¤ğ‘›)âˆ’3
ğ‘›+2ğ‘¥ğ‘›âˆ’ğ‘¥ğ‘›âˆ’1âˆšğ›¾, (4)
they modeled (NAG) as a heavy ball with vanishing friction system of the form
ğ‘‘2ğ‘¥
ğ‘‘ğ‘¡2=âˆ’âˆ‡ğ‘“(ğ‘¥)âˆ’3
ğ‘¡ğ‘‘ğ‘¥
ğ‘‘ğ‘¡(HBVF)
The choice of terminology alludes to the fact that (HBVF) describes the dynamics of a heavy ball
descending the landscape of ğ‘“under the potential field ğ¹(ğ‘¥)=âˆ’âˆ‡ğ‘“(ğ‘¥)with a vanishing kinetic
friction coefficient (the 3/ğ‘¡factor in front of the momentum term ğ‘‘ğ‘¥/ğ‘‘ğ‘¡). In this interpretation, the
mass of the ball accelerates the system, the friction term dissipates energy to enable convergence,
and the vanishing friction coefficient quenches the impact of friction over time in order to avoid
decelerating the system too much (so the system is, in a sense, â€œcritically underdampedâ€).
As was shown by Su et al. [41], an explicit Euler discretization of (HBVF) yields (NAG) with exactly
the right momentum coefficient ğ‘›/(ğ‘›+3); moreover, the rate of convergence of the continuous-time
dynamics (HBVF) is the same as that of the discrete-time algorithm (NAG) , and the energy function
and Lyapunov analysis used to derive the former can also be used to derive the latter. For all these
reasons, (HBVF) is universally considered as the de facto continuous-time analogue of (NAG) , and
we will treat it as such in the sequel.
3.2. NAG meets FTRL .To move from unconstrained convex minimization problems to finite
ğ‘-person games â€“ a constrained, non-convex, multi-agent, multi-objective setting â€“ it will be more
transparent to start with the continuous-time formulation (HBVF) . Indeed, applying the logic behind
(HBVF) to the (unconstrained) state variables ğ‘¦of(FTRL-D) , we obtain the â€œfollow the accelerated
leaderâ€ dynamics
ğ‘‘2ğ‘¦
ğ‘‘ğ‘¡2=ğ‘£(ğ‘„(ğ‘¦))âˆ’ğ‘Ÿ
ğ‘¡ğ‘‘ğ‘¦
ğ‘‘ğ‘¡(FTXL-D)
where the dynamicsâ€™ driving force ğ¹(ğ‘¦)=ğ‘£(ğ‘„(ğ‘¦))is now given by the payoff field of the game,
and the factor ğ‘Ÿ/ğ‘¡,ğ‘Ÿâ‰¥0, plays again the role of a vanishing friction coefficient. To avoid confusion,
we highlight that in the case of regularized learning, the algorithmâ€™s variable that determines the
evolution of the system in an autonomous way is the â€œscore variableâ€ ğ‘¦, not the â€œstrategy variableâ€ ğ‘¥
(which is an ancillary variable obtained from ğ‘¦via the regularized choice map ğ‘„).
In contrast to (EWD) , the accelerated dynamics (FTXL-D) are second-order in time, a fact with
fundamental ramifications, not only from a conceptual, but also from an operational viewpoint.
Focusing on the latter, we first note that (FTXL-D) requires two sets of initial conditions, ğ‘¦(0)and
Â¤ğ‘¦(0), the latter having no analogue in the first-order setting of (FTRL-D) . In general, the evolution of
the system depends on both ğ‘¦(0)andÂ¤ğ‘¦(0), but since this would introduce an artificial bias toward a
certain direction, we will take Â¤ğ‘¦(0)=0, in tune with standard practice for (NAG) [41].
We also note that (FTXL-D) can be mapped to an equivalent autonomous first-order system with
double the variables: specifically, letting ğ‘=Â¤ğ‘¦denote the playersâ€™ ( payoff )momentum ,(FTXL-D)
can be rewritten asğ‘‘ğ‘¦
ğ‘‘ğ‘¡=ğ‘ğ‘‘ğ‘
ğ‘‘ğ‘¡=ğ‘£(ğ‘„(ğ‘¦))âˆ’ğ‘Ÿ
ğ‘¡ğ‘ (5)
withğ‘¦(0)initialized arbitrarily and ğ‘(0)=Â¤ğ‘¦(0). In turn, (5)yieldsğ‘(ğ‘¡)=ğ‘¡âˆ’ğ‘Ÿâˆ«ğ‘¡
0ğ‘¡ğ‘Ÿğ‘£(ğ‘„(ğ‘¦(ğœ)))ğ‘‘ğœ,
soğ‘(ğ‘¡)can be seen as a weighted aggregate of the playersâ€™ payoffs up to time ğ‘¡: ifğ‘Ÿ=0(the
undamped regime), all information enters ğ‘(ğ‘¡)with the same weight; if ğ‘Ÿ >0, past information is
discounted relative to more recent observations; and, in the overdamped limit ğ‘Ÿâ†’âˆ , all weight is
assigned to the current point in time, emulating in this way the first-order system (FTRL-D).
1There are, of course, many other approaches to acceleration, that we cannot cover here; for a discussion of
the popular â€œlinear couplingâ€ approach of Allen-Zhu & Orecchia [1], see Appendix F.
53.3. First insights and results. From an operational standpoint, the main question of interest is to
specify the equilibrium convergence properties of (FTXL-D) â€“ and, later in the paper, its discrete-time
analogue. To establish a baseline, the principal equilibrium properties of its first-order counterpart can
be summarized as follows: ( i) strict Nash equilibria are locally stable and attracting under (FTRL-D)
[23,31];2(ii) the dynamics do not admit any other such points (that is, stable and attracting) [ 11];
and ( iii) quantitively, in the case of (EWD) , the dynamics converge locally to strict Nash equilibria at
ageometric rate of the formâˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥=O(exp(âˆ’ğ‘ğ‘¡))for someğ‘>0[31].
Our first result below shows that the accelerated dynamics (FTXL-D) exhibit an exponential speed-up
relative to (FTRL-D), and the playersâ€™ orbits converge to strict Nash equilibria at a superlinear rate:
Theorem 1. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, and letğ‘¥(ğ‘¡)=ğ‘„(ğ‘¦(ğ‘¡))be a solution orbit of
(FTXL-D) . Ifğ‘¥(0)is sufficiently close to ğ‘¥âˆ—, thenğ‘¥(ğ‘¡)converges to ğ‘¥âˆ—; in particular, if (FTXL-D) is
run with logit best responses (that is,ğ‘„â†Î›), we have
âˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ‘¡2
2(ğ‘Ÿ+1)
(6)
whereğ¶ >0is a constant that depends only on the initialization of (FTXL-D) and
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]>0 (7)
is the minimum payoff difference at equilibrium.
Theorem 1 (which we prove in Appendix B) is representative of the analysis to come, so some
remarks are in order. First, we should note that the explicit rate estimate (6)is derived for the special
case of logit best responses, which underlie all exponential / multiplicative weights algorithms. To
the best of our knowledge, the only comparable result in the literature is the similar rate provided in
[25] for the case ğ‘Ÿ=0. In the case of a general regularizer, an analogous speed-up is observed, but
the exact expressions are more involved, so we defer them to Appendix B. A second important point
concerns whether the rate estimate (6)is tight or not. Finally, the neighborhood of initial conditions
aroundğ‘¥âˆ—is determined by the minimum payoff difference at equilibrium and is roughly O(ğ‘)in
diameter; we defer the relevant details of this discussion to Appendix B.
To answer this question â€“ and, at the same time get a glimpse of the proof strategy for Theorem 1 â€“ it
will be instructive to consider a single-player game with two actions. Albeit simple, this toy example
is not simplistic, as it provides an incisive look into the problem, and will be used to motivate our
design choices in the sequel.
Example 3.1. Consider a single-player game Î“with actions AandBsuch thatğ‘¢(A)âˆ’ğ‘¢(B)=1, so
the (dominant) strategy ğ‘¥âˆ—=(1,0)is a strict Nash equilibrium. Then, letting ğ‘§=ğ‘¦Aâˆ’ğ‘¦B,(FTXL-D)
readily yields
ğ‘‘2ğ‘§
ğ‘‘ğ‘¡2=ğ‘‘2ğ‘¦A
ğ‘‘ğ‘¡2âˆ’ğ‘‘2ğ‘¦B
ğ‘‘ğ‘¡2=ğ‘¢(A)âˆ’ğ‘¢(B)âˆ’ğ‘Ÿ
ğ‘¡ğ‘‘ğ‘¦A
ğ‘‘ğ‘¡âˆ’ğ‘‘ğ‘¦B
ğ‘‘ğ‘¡
=1âˆ’ğ‘Ÿ
ğ‘¡ğ‘‘ğ‘§
ğ‘‘ğ‘¡. (8)
As we show in Appendix B, this non-autonomous differential equation can be solved exactly to yield
ğ‘§(ğ‘¡)=ğ‘§(0)+ğ‘¡2/[2(ğ‘Ÿ+1)], and hence
âˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥âˆ=1
1+exp(ğ‘§(ğ‘¡))âˆ¼exp
âˆ’ğ‘§(0)âˆ’ğ‘¡2
2(ğ‘Ÿ+1)
. (9)
Sinceğ‘=ğ‘¢(A)âˆ’ğ‘¢(B)=1, the rate (9)coincides with that of Theorem 1 up to a factor of 1/2. This
factor is an artifact of the analysis and, in fact, it can be tightened to (1âˆ’ğœ€)for arbitrarily small
ğœ€>0; we did not provide this more precise expression to lighten notation. By contrast, the factor
2(ğ‘Ÿ+1)in (6) cannot be lifted; this has important ramifications which we discuss below. â™¦
The first conclusion that can be drawn from Example 3.1 is that the rate estimate of Theorem 1 is
tight and cannot be improved in general. In addition, and in stark contrast to (NAG) , Example 3.1
2Recall here that ğ‘¥âˆ—âˆˆXis said to be ( i)Lyapunov stable (or simply stable ) if every orbit ğ‘¥(ğ‘¡)of the dynamics
that starts close enough to ğ‘¥âˆ—remains close enough to ğ‘¥âˆ—for allğ‘¡â‰¥0; (ii)attracting iflimğ‘¡â†’âˆğ‘¥(ğ‘¡)=ğ‘¥âˆ—for
every orbitğ‘¥(ğ‘¡)that starts close enough to ğ‘¥âˆ—; and ( iii)asymptotically stable if it is both stable and attracting.
For an introduction to the theory of dynamical systems, cf. Shub [39] and Hirsch et al. [22].
6shows that the optimal value for the friction parameter is ğ‘Ÿ=0(at least from a min-max viewpoint,
as this value yields the best possible lower bound for the rate). Of course, this raises the question as
to whether this is due to the continuous-time character of the policy;3however, as we show in detail
in Appendix C, this is notthe case: the direct handover of (NAG) to Example 3.1 yields the exact
same rate (though the proof relies on a significantly more opaque generating function calculation).
In view of all this, it becomes apparent that friction only hinders the equilibrium convergence
properties of accelerated FTRL schemes in our game-theoretic setting. On that account, we will
continue our analysis in the undamped regime ğ‘Ÿ=0.
4 Accelerated learning: Analysis and results
4.1. The algorithm. To obtain a bona fide, algorithmic implementation of the continuous-time
dynamics (FTXL-D) , we will proceed with the same explicit, finite-difference scheme leading to
the discrete-time algorithm (NAG) from the continuous-time dynamics (HBVF) of Su et al. [41].
Specifically, taking a discretization step ğ›¾ > 0in(FTXL-D) and setting the schemeâ€™s friction
parameterğ‘Ÿto zero (which, as we discussed at length in the previous section, is the optimal choice in
our setting), a straightforward derivation yields the basic update rule
[ğ‘¦ğ‘–,ğ‘›+1âˆ’2ğ‘¦ğ‘–,ğ‘›+ğ‘¦ğ‘–,ğ‘›âˆ’1]/ğ›¾2=Ë†ğ‘£ğ‘–,ğ‘› for allğ‘–âˆˆNand allğ‘›=1,2,... (10)
In the above, just as in the case of (FTRL) ,Ë†ğ‘£ğ‘–,ğ‘›âˆˆâ„Ağ‘–denotes a black-box â€œpayoff signalâ€ that
carries information about the mixed payoff vector ğ‘£ğ‘–(ğ‘¥ğ‘›)of playerğ‘–at the current strategy profile ğ‘¥ğ‘›
(we provide more details on this below).
Alternatively, to obtain an equivalent first-order iterative rule (which is easier to handle and discuss),
it will be convenient to introduce the momentum variables ğ‘ğ‘›=(ğ‘¦ğ‘›âˆ’ğ‘¦ğ‘›âˆ’1)/ğ›¾. Doing just that, a
simple rearrangement of (10) yields the â€œfollow the accelerated leaderâ€ scheme
ğ‘¦ğ‘–,ğ‘›+1=ğ‘¦ğ‘–,ğ‘›+ğ›¾ğ‘ğ‘–,ğ‘›+1ğ‘ğ‘–,ğ‘›+1=ğ‘ğ‘–,ğ‘›+ğ›¾Ë†ğ‘£ğ‘–,ğ‘›ğ‘¥ğ‘–,ğ‘›=ğ‘„ğ‘–(ğ‘¦ğ‘–,ğ‘›). (FTXL)
The algorithm (FTXL) will be our main object of study in the sequel, and we will examine its
convergence properties under three differerent models for Ë†ğ‘£ğ‘›:
1.Full information , i.e., players get to access their full, mixed payoff vectors:
Ë†ğ‘£ğ‘–,ğ‘›=ğ‘£ğ‘–(ğ‘¥ğ‘›) for allğ‘–âˆˆN,ğ‘›=1,2,... (11a)
2.Realization-based feedback , i.e., after choosing an action profile ğ›¼ğ‘›âˆ¼ğ‘¥ğ‘›, each player ğ‘–âˆˆN
observes (or otherwise calculates) the vector of their counterfactual, â€œwhat-ifâ€ rewards, namely
Ë†ğ‘£ğ‘–,ğ‘›=ğ‘£ğ‘–(ğ›¼ğ‘›) for allğ‘–âˆˆN,ğ‘›=1,2,... (11b)
3.Bandit / Payoff-based feedback , i.e., each player only observes their current reward, and must rely
on statistical estimation techniques to reconstruct an estimate of ğ‘£ğ‘–(ğ‘¥ğ‘›). For concreteness, we will
consider the case where players employ a version of the so-called importance-weighted estimator
Ë†ğ‘£ğ‘–,ğ‘›=IWE(ğ‘¥ğ‘–,ğ‘›;ğ›¼ğ‘–,ğ‘›) for allğ‘–âˆˆN,ğ‘›=1,2,... (11c)
which we describe in detail later in this section.
Of course, this list of information models is not exhaustive, but it is a faithful representation of most
scenarios that arise in practice, so it will suffice for our purposes.
Now before moving forward with the analysis, it will be useful to keep some high-level remarks
in mind. The first is that (FTXL) shares many similarities with (FTRL) , but also several notable
differences. At the most basic level, (FTRL) and(FTXL) are both â€œstimulus-responseâ€ schemes in
the spirit of Erev & Roth [10], that is, players â€œrespondâ€ with a strategy ğ‘¥ğ‘–,ğ‘›=ğ‘„ğ‘–(ğ‘¦ğ‘–,ğ‘›)to a â€œstimulusâ€
ğ‘¦ğ‘–,ğ‘›generated by the observed payoff signals Ë†ğ‘£ğ‘–,ğ‘›. In this regard, both methods adhere to the online
learning setting (and, in particular, to the regularized learning paradigm).
3The reader might also wonder if the use of a non-vanishing friction coefficient â€“ ğ‘ŸÂ¤ğ‘¦instead of(ğ‘Ÿ/ğ‘¡)Â¤ğ‘¦â€“
could be beneficial to the convergence rate of (FTXL-D) . As we show in Appendices B and C, this leads to
significantly worse convergence rates of the form âˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥âˆâˆ¼exp(âˆ’Î˜(ğ‘¡))for allğ‘Ÿ >0.
7However, unlike (FTRL) , where players respond to the aggregate of their payoff signals â€“ the process
ğ‘¦ğ‘›in(FTRL) â€“ the accelerated algorithm (FTXL) introduces an additional aggregation layer, which
expresses how players â€œbuild momentumâ€ based on the same payoff signals â€“ the process ğ‘ğ‘›in
(FTXL) . Intuitively, we can think of these two processes as the â€œpositionâ€ and â€œmomentumâ€ variables
of a classical inertial system, not unlike the heavy-ball dynamics of Su et al. [41]. The only conceptual
difference is that, instead of rolling along the landscape of a (convex) function, the players now track
the â€œmirroredâ€ payoff field Ë†ğ‘£(ğ‘¦):=ğ‘£(ğ‘„(ğ‘¦)).
In the rest of this section, we proceed to examine in detail the equilibrium convergence properties of
(FTXL) under each of the three models detailed in Eqs. (11a)â€“(11c) in order.
4.2. Accelerated learning with full information. We begin with the full information model (11a) .
This is the most straightforward model (due to the absence of randomness and uncertainty) but,
admittedly, also the least realistic one. Nevertheless, it will serve as a useful benchmark for the rest,
and it will allow us to introduce several important notions.
Before we state our result, it is important to note that a finite game can have multiple strict Nash
equilibria, so global convergence results are, in general, unattainable; for this reason, we analyze the
algorithmâ€™s local convergence landscape. In this regard, Theorem 2 below shows that (FTXL) with
full information achieves a superlinear local convergence rate to strict Nash equilibria:
Theorem 2. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, and letğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)be the sequence of play
generated by (FTXL) with full information feedback of the form (11a) . Ifğ‘¥1is initialized sufficiently
close toğ‘¥âˆ—, thenğ‘¥ğ‘›converges to ğ‘¥âˆ—; in particular, if (FTXL) is run with logit best responses (that is,
ğ‘„â†Î›), we have
âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2
=exp âˆ’Î˜(ğ‘‡2)(12)
whereğ¶ >0is a constant that depends only on the initialization of (FTXL) and
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]>0 (13)
is the minimum payoff difference at equilibrium.
To maintain the flow of our discussion, we defer the proof of Theorem 2 to Appendix C. Instead,
we only note here that, just as in the case of (HBVF) and(NAG) , Theorem 2 provides essentially
the same rate of convergence as its continuous-time counterpart, Theorem 1, modulo a subleading
term which has an exponentially small impact on the rate of convergence. In particular, we should
stress that the superlinear convergence rate of (FTXL) exhibits an exponential speedup relative to
(FTRL) , which is known to converge at a geometric rate âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆ=exp(âˆ’Î˜(ğ‘‡)). This is in direct
correspondence to what we observe in continuous time, showing in particular that the continuous-time
dynamics (FTXL-D) are a faithful representation of (FTXL).
We should also stress here that superlinear convergence rates are typically associated with methods
that are second-order in space , in the sense that they employ Hessian-like information â€“ like Newtonâ€™s
algorithm â€“ not second-order in time â€“ like (NAG) and(FTXL) . We find this observation particularly
intriguing as it suggests that accelerated rates can be observed in the context of learning in games
without having to pay the excessively high compute cost of second-order methods in optimization.
4.3. Accelerated learning with realization-based feedback. We now turn to the realization-based
model (11b) , where players can only assess the rewards of their pure actions in response to the
realized actions of all other players. In words, Ë†ğ‘£ğ‘–,ğ‘›=ğ‘£ğ‘–(ğ›¼ğ‘›)collects the payoffs that player ğ‘–âˆˆN
would have obtained by playing each of their pure actions ğ›¼ğ‘–âˆˆAğ‘–against the action profile ğ›¼âˆ’ğ‘–,ğ‘›
adopted by the rest of the players.
In contrast to the full information model (11a) , the realization-based model is stochastic in nature, so
our convergence results will likewise be stochastic. Nevertheless, despite the added layer of uncer-
tainty, we show that (FTXL) with realization-based feedback maintains a superlinear convergence
rate with high probability:
Theorem 3. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, fix some confidence level ğ›¿ > 0, and let
ğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)be the sequence of play generated by (FTXL) with realization-based feedback as per
80 200 400 600 800 1000
Iteration (n)10âˆ’1410âˆ’1110âˆ’810âˆ’510âˆ’2101||xnâˆ’xâˆ—||1Feedback Model: Realization-based
FTXL
EW(a)Zero-sum game: Realization-based feedback
0 200 400 600 800 1000
Iteration (n)10âˆ’710âˆ’510âˆ’310âˆ’1||xnâˆ’xâˆ—||1Feedback Model: Bandit
FTXL
EW (b)Zero-sum game: Bandit feedback
0 200 400 600 800 1000
Iteration (n)10âˆ’410âˆ’310âˆ’210âˆ’1100101102||xnâˆ’xâˆ—||1Feedback Model: Realization-based
FTXL
EW
(c)Congestion game: Realization-based feedback
0 200 400 600 800 1000
Iteration (n)10âˆ’310âˆ’210âˆ’1100101102||xnâˆ’xâˆ—||1Feedback Model: Bandit
FTXL
EW (d)Congestion game: Bandit feedback
Figure 1: Performance evaluation of (FTXL) in a zero-sum and a congestion game under realization-based and
bandit feedback. Solid lines represent average values, while shaded regions enclose Â±1standard deviation. The
plots are in logarithmic scale.
(11b) and a sufficiently small step-size ğ›¾>0. Then there exists a neighborhood Uofğ‘¥âˆ—such that
â„™(ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ)â‰¥1âˆ’ğ›¿ ifğ‘¥1âˆˆU. (14)
In particular, if (FTXL) is run with logit best responses (that is,ğ‘„â†Î›), there exist positive
constantsğ¶,ğ‘> 0as in Theorem 2 such that on the event {ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ}:
âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+3
5ğ‘ğ›¾5/3ğ‘‡5/3
=exp âˆ’Î˜(ğ‘‡2). (15)
What is particularly surprising in Theorem 3 is that, (FTXL) maintains the accelerated superlinear rate
of Theorem 2 â€“ and, likewise, the exponential speedup relative to (FTRL) â€“despite the randomness
and uncertainty involved in the realization-based model (11b) . The salient point enabling this feature
of (FTXL) is that Ë†ğ‘£ğ‘›can be expressed as
Ë†ğ‘£ğ‘›=ğ‘£(ğ‘¥ğ‘›)+ğ‘ˆğ‘› (16)
whereğ‘ˆğ‘›âˆˆQ
ğ‘–â„Ağ‘–is an almost surely bounded conditionally zero-mean stochastic perturbation,
that is,ğ”¼[ğ‘ˆğ‘›|Fğ‘›]=0, where Fğ‘›:=ğœ(ğ‘¥1,...,ğ‘¥ğ‘›)denotes the history of play up to (and including)
timeğ‘›. Thanks to the boundedness of (16), we are able to derive a series of probabilistic estimates
showing that, with high probability (and, in particular, greater than 1âˆ’ğ›¿), the contribution of the
noise in the algorithmâ€™s rate becomes subleading, thus allowing the superlinear rate of Theorem 2 to
emerge. As in the case of Theorem 2, we defer the proof of Theorem 3 to the appendix.
4.4. Bandit feedback. The last framework we consider is the bandit model where players only
observe their realized rewards, a scalar from which they must reconstruct their entire payoff vector.
To do so, a standard technique from the multi-armed bandit literature is the so-called importance
weighted estimator (IWE) [8, 27], defined in our setting as
Ë†ğ‘£ğ‘–ğ›¼ğ‘–,ğ‘›=1{ğ›¼ğ‘–,ğ‘›=ğ›¼ğ‘–}
Ë†ğ‘¥ğ‘–ğ›¼ğ‘–,ğ‘›ğ‘¢ğ‘–(ğ›¼ğ‘–;ğ›¼âˆ’ğ‘–,ğ‘›) (IWE)
9where Ë†ğ‘¥ğ‘–,ğ‘›=(1âˆ’ğœ€ğ‘›)ğ‘¥ğ‘–,ğ‘›+ğœ€ğ‘›unifAğ‘–is a mixture of ğ‘¥ğ‘–,ğ‘›and the uniform distribution on Ağ‘–(a
mechanism known in the literature as explicit exploration ). Importantly, this estimator is unbiased
relative to the perturbed strategy Ë†ğ‘¥ğ‘¥ğ‘›, which thus incurs an O(ğœ€ğ‘›)non-zero-sum error to the estimation
ofğ‘£ğ‘–(ğ‘¥ğ‘›). This error can be made arbitrarily small by taking ğœ€ğ‘›â†’0but, in doing so, the variance of
Ë†ğ‘£ğ‘–,ğ‘›diverges, leading to a bias-variance trade-off that is difficult to tame.
Despite these added difficulties, we show below that (FTXL) maintains its superlinear convergence
rate even with bandit, payoff-based feedback:
Theorem 4. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, fix some confidence level ğ›¿ > 0, and let
ğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)be the sequence of play generated by (FTXL) with bandit feedback of the form (11c) ,
anIWE exploration parameter ğœ€ğ‘›âˆ1/ğ‘›â„“ğœ€for someâ„“ğœ€âˆˆ(0,1/2), and a sufficiently small step-size
ğ›¾>0. Then there exists a neighborhood Uofğ‘¥âˆ—inXsuch that
â„™(ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ)â‰¥1âˆ’ğ›¿ ifğ‘¥1âˆˆU. (17)
In particular, if (FTXL) is run with logit best responses (that is,ğ‘„â†Î›), there exist positive
constantsğ¶,ğ‘> 0as in Theorem 2 such that on the event {ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ}
âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+5
9ğ‘ğ›¾9/5ğ‘‡9/5
=exp âˆ’Î˜(ğ‘‡2). (18)
Theorem 4 (which we prove in Appendix D shows that, despite the degradation of the subleading
term, (FTXL) retains its superlinear convergence rate even with bandit, payoff-based feedback (for a
numerical demonstration, see Fig. 1 above). We find this feature of (FTXL) particularly important as
it shows that the algorithm remains exceptionally robust in the face of randomness and uncertainty,
even as we move toward increasingly information-starved environments â€“ from full information, to
realization-based observations and, ultimately, to bandit feedback. This has important ramifications
from an operational standpoint, which we intend to examine further in future work.
4.5. Numerical Experiments. We conclude this section with a series of numerical simulations to
validate the performance of (FTXL) . To this end, we consider two game paradigms, (i) a 2-player
zero-sum game, and (ii) a congestion game.
Zero-sum Games. First, we consider a 2-player zero-sum game with actions {ğ›¼1,ğ›¼2,ğ›¼3}and
{ğ›½1,ğ›½2,ğ›½3}, and payoff matrix
ğ‘ƒ= (2,âˆ’2) (1,âˆ’1) (2,âˆ’2)
(âˆ’2,2) (âˆ’ 1,1) (âˆ’ 2,2)
(âˆ’2,2) (âˆ’ 1,1) (âˆ’ 2,2)!
Here, the rows of ğ‘ƒcorrespond to the actions of player ğ´and the columns to the actions of player
ğµ, while the first item of each entry of ğ‘ƒcorresponds to the payoff of ğ´, and the second one to the
payoff ofğµ. Clearly, the action profile (ğ›¼1,ğ›½2)is a strict Nash equilibrium.
Congestion Games. As a second example, we consider a congestion game with ğ‘=100and2
roads,ğ‘Ÿ1andğ‘Ÿ2, with costsğ‘1=1.1andğ‘2=ğ‘‘/ğ‘whereğ‘‘is the number of drivers on ğ‘Ÿ2. In words,
ğ‘Ÿ1has a fixed delay equal to 1.1, whileğ‘Ÿ2has a delay proportional to the drivers using it. Note, that
the strategy profile where all players are using ğ‘Ÿ2is a strict Nash equilibrium.
In Fig. 1, we assess the convergence of (FTXL) with logit best responses, under realization-based
and bandit feedback, and compare it to the standard (EW) with the same level of information. The
figures verify that (FTXL) outperforms (EW) regarding the convergence to a strict Nash equilibrium
both for the realization-based and the bandit feedback, as expected from the theoretical findings.
Specifically, they validate the faster convergence rate of (FTXL) compared to that of the (EW)
algorithm. Moreover, we observe that both algorithms perform worse under bandit feedback than
under realization-based feedback. This behavior is expected as less information becomes available.
More details can be found in Appendix E.
10Acknowledgments and Disclosure of Funding
This research was supported in part by the French National Research Agency (ANR) in the frame-
work of the PEPR IA FOUNDRY project (ANR-23-PEIA-0003), the â€œInvestissements dâ€™avenir
programâ€ (ANR-15-IDEX-02), the LabEx PERSYV AL (ANR-11-LABX-0025-01), MIAI@Grenoble
Alpes (ANR-19-P3IA-0003), the project IRGA2024-SPICE-G7H-IRG24E90. PM is also with the
Archimedes Research Unit â€“ Athena RC â€“ Department of Mathematics, National & Kapodistrian
University of Athens, and his research was partially funded by project MIS 5154714 of the National
Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU
Program.
References
[1]Allen-Zhu, Z. and Orecchia, L. Linear coupling: An ultimate unification of gradient and mirror descent. In
ITCS â€™17: Proceedings of the 8th Conference on Innovations in Theoretical Computer Science , 2017.
[2]Auer, P., Cesa-Bianchi, N., Freund, Y ., and Schapire, R. E. Gambling in a rigged casino: The adversarial
multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer
Science , 1995.
[3]Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences , 2(1):183â€“202, March 2009.
[4]BenaÃ¯m, M. Dynamics of stochastic approximation algorithms. In AzÃ©ma, J., Ã‰mery, M., Ledoux, M., and
Yor, M. (eds.), SÃ©minaire de ProbabilitÃ©s XXXIII , volume 1709 of Lecture Notes in Mathematics , pp. 1â€“68.
Springer Berlin Heidelberg, 1999.
[5]Boone, V . and Mertikopoulos, P. The equivalence of dynamic and strategic stability under regularized
learning in games. In NeurIPS â€™23: Proceedings of the 37th International Conference on Neural Information
Processing Systems , 2023.
[6]Bravo, M. and Mertikopoulos, P. On the robustness of learning in games with stochastically perturbed
payoff observations. Games and Economic Behavior , 103(John Nash Memorial issue):41â€“66, May 2017.
[7]Bubeck, S. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine
Learning , 8(3-4):231â€“358, 2015.
[8]Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochastic and nonstochastic multi-armed bandit
problems. Foundations and Trends in Machine Learning , 5(1):1â€“122, 2012.
[9]Daskalakis, C., Goldberg, P. W., and Papadimitriou, C. H. The complexity of computing a Nash equilibrium.
Communications of the ACM , 52(2):89â€“97, 2009.
[10] Erev, I. and Roth, A. E. Predicting how people play games: Reinforcement learning in experimental games
with unique, mixed strategy equilibria. American Economic Review , 88:848â€“881, 1998.
[11] Flokas, L., Vlatakis-Gkaragkounis, E. V ., Lianeas, T., Mertikopoulos, P., and Piliouras, G. No-regret
learning and mixed Nash equilibria: They do not mix. In NeurIPS â€™20: Proceedings of the 34th International
Conference on Neural Information Processing Systems , 2020.
[12] FlÃ¥m, S. D. and Morgan, J. Newtonian mechanics and Nash play. International Game Theory Review , 6
(2):181â€“194, 2004.
[13] Gao, B. and Pavel, L. On passivity and reinforcement learning in finite games. In 2018 IEEE Conference
on Decision and Control (CDC) , pp. 340â€“345, 2018. doi: 10.1109/CDC.2018.8619157.
[14] Gao, B. and Pavel, L. On passivity, reinforcement learning, and higher order learning in multiagent finite
games. IEEE Transactions on Automatic Control , 66(1):121â€“136, 2021. doi: 10.1109/TAC.2020.2978037.
[15] Gao, B. and Pavel, L. Second-order mirror descent: exact convergence beyond strictly stable equilibria in
concave games. In 2021 60th IEEE Conference on Decision and Control (CDC) , pp. 948â€“953, 2021. doi:
10.1109/CDC45484.2021.9683223.
[16] Gao, B. and Pavel, L. Second-order mirror descent: Convergence in games beyond averaging and
discounting. IEEE Transactions on Automatic Control , 69(4):2143â€“2157, 2024. doi: 10.1109/TAC.2023.
3291953.
[17] Giannou, A., Vlatakis-Gkaragkounis, E. V ., and Mertikopoulos, P. Survival of the strictest: Stable and
unstable equilibria under regularized learning with partial information. In COLT â€™21: Proceedings of the
34th Annual Conference on Learning Theory , 2021.
[18] Giannou, A., Vlatakis-Gkaragkounis, E. V ., and Mertikopoulos, P. The convergence rate of regularized
learning in games: From bandits and uncertainty to optimism and beyond. In NeurIPS â€™21: Proceedings of
the 35th International Conference on Neural Information Processing Systems , 2021.
11[19] Hadikhanloo, S., Laraki, R., Mertikopoulos, P., and Sorin, S. Learning in nonatomic games, Part I: Finite
action spaces and population games. Journal of Dynamics and Games , 9(4, William H. Sandholm memorial
issue):433â€“460, October 2022.
[20] Hall, P. and Heyde, C. C. Martingale Limit Theory and Its Application . Probability and Mathematical
Statistics. Academic Press, New York, 1980.
[21] Hart, S. and Mas-Colell, A. Uncoupled dynamics do not lead to Nash equilibrium. American Economic
Review , 93(5):1830â€“1836, 2003.
[22] Hirsch, M. W., Smale, S., and Devaney, R. L. Differential Equations, Dynamical Systems, and an
Introduction to Chaos . Elsevier, London, UK, 2 edition, 2004.
[23] Hofbauer, J. and Sigmund, K. Evolutionary game dynamics. Bulletin of the American Mathematical
Society , 40(4), July 2003.
[24] Hsieh, Y .-G., Antonakopoulos, K., and Mertikopoulos, P. Adaptive learning in continuous games: Optimal
regret bounds and convergence to Nash equilibrium. In COLT â€™21: Proceedings of the 34th Annual
Conference on Learning Theory , 2021.
[25] Laraki, R. and Mertikopoulos, P. Higher order game dynamics. Journal of Economic Theory , 148(6):
2666â€“2695, November 2013.
[26] Laraki, R. and Mertikopoulos, P. Inertial game dynamics and applications to constrained optimization.
SIAM Journal on Control and Optimization , 53(5):3141â€“3170, October 2015.
[27] Lattimore, T. and SzepesvÃ¡ri, C. Bandit Algorithms . Cambridge University Press, Cambridge, UK, 2020.
[28] Littlestone, N. and Warmuth, M. K. The weighted majority algorithm. Information and Computation , 108
(2):212â€“261, 1994.
[29] Lotidis, K., Mertikopoulos, P., and Bambos, N. Learning in games with quantized payoff observations. In
CDC â€™22: Proceedings of the 61st IEEE Annual Conference on Decision and Control , 2022.
[30] Mabrok, M. A. and Shamma, J. S. Passivity analysis of higher order evolutionary dynamics and population
games. In 2016 IEEE 55th Conference on Decision and Control (CDC) , pp. 6129â€“6134, 2016. doi:
10.1109/CDC.2016.7799211.
[31] Mertikopoulos, P. and Sandholm, W. H. Learning in games via reinforcement and regularization. Mathe-
matics of Operations Research , 41(4):1297â€“1324, November 2016.
[32] Mertikopoulos, P. and Sandholm, W. H. Riemannian game dynamics. Journal of Economic Theory , 177:
315â€“364, September 2018.
[33] Mertikopoulos, P. and Staudigl, M. Equilibrium tracking and convergence in dynamic games. In CDC â€™21:
Proceedings of the 60th IEEE Annual Conference on Decision and Control , 2021.
[34] Mertikopoulos, P. and Zhou, Z. Learning in games with continuous action sets and unknown payoff
functions. Mathematical Programming , 173(1-2):465â€“507, January 2019.
[35] Mertikopoulos, P., Papadimitriou, C. H., and Piliouras, G. Cycles in adversarial regularized learning. In
SODA â€™18: Proceedings of the 29th annual ACM-SIAM Symposium on Discrete Algorithms , 2018.
[36] Mertikopoulos, P., Hsieh, Y .-P., and Cevher, V . A unified stochastic approximation framework for learning
in games. Mathematical Programming , 203:559â€“609, January 2024.
[37] Nemirovski, A. S. and Yudin, D. B. Problem Complexity and Method Efficiency in Optimization . Wiley,
New York, NY , 1983.
[38] Nesterov, Y . A method for unconstrained convex minimization problem with the rate of convergence
O(1/ğ‘˜2).Proceedings of the USSR Academy of Sciences , 269(543-547), 1983.
[39] Shub, M. Global Stability of Dynamical Systems . Springer-Verlag, Berlin, 1987.
[40] Su, W., Boyd, S. P., and CandÃ¨s, E. J. A differential equation for modeling Nesterovâ€™s accelerated gradient
method: Theory and insights. In NIPS â€™14: Proceedings of the 28th International Conference on Neural
Information Processing Systems , pp. 2510â€“2518, 2014.
[41] Su, W., Boyd, S., and CandÃ¨s, E. J. A differential equation for modeling Nesterovâ€™s accelerated gradient
method: Theory and insights. Journal of Machine Learning Research , 17(153):1â€“43, 2016.
[42] Syrgkanis, V ., Agarwal, A., Luo, H., and Schapire, R. E. Fast convergence of regularized learning in games.
InNIPS â€™15: Proceedings of the 29th International Conference on Neural Information Processing Systems ,
pp. 2989â€“2997, 2015.
[43] Taylor, P. D. and Jonker, L. B. Evolutionary stable strategies and game dynamics. Mathematical Biosciences ,
40(1-2), 1978.
[44] Toonsi, S. and Shamma, J. S. Higher-order uncoupled dynamics do not lead to Nash equilibrium - except
when they do. In NeurIPS â€™23: Proceedings of the 37th International Conference on Neural Information
Processing Systems , 2023.
[45] V ovk, V . G. Aggregating strategies. In COLT â€™90: Proceedings of the 3rd Workshop on Computational
Learning Theory , pp. 371â€“383, 1990.
12Appendix
In Appendix A, we discuss how our findings can be extended to general regularizers. Subsequently,
Appendices B and C contain the technical proofs for the continuous and discrete time algorithms,
respectively. Following this, Appendix D provides the convergence results of (FTXL) under partial
information, specifically under realization-based and bandit feedback. We conclude this section with
Appendix E, which presents the details of the numerical experiments.
A Auxiliary results for general regularizers
In this appendix, we briefly discuss how to obtain the convergence of (FTXL) for mirror maps ğ‘„
beyond the logit map Î›. Namely, we consider regularizers that are decomposable, i.e., â„ğ‘–(ğ‘¥ğ‘–)=P
ğ›¼ğ‘–âˆˆAğ‘–ğœƒğ‘–(ğ‘¥ğ›¼ğ‘–)such thatğœƒğ‘–:[0,1]â†’â„is continuous on[0,1], twice differentiable on (0,1]and
strongly convex with ğœƒâ€²
ğ‘–(0+)=âˆ’âˆ.
Lemma A.1. Suppose that ğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)and for allğ›¼âˆˆA,ğ›¼â‰ ğ›¼âˆ—, it holds that ğ‘¦ğ›¼,ğ‘›âˆ’ğ‘¦ğ›¼âˆ—,ğ‘›â†’âˆ’âˆ
asğ‘›â†’âˆ . Then,ğ‘¥ğ‘›converges to ğ‘¥âˆ—, whereğ‘¥âˆ—is a point mass at ğ›¼âˆ—. Moreover, it holds that:
âˆ¥ğ‘¥ğ‘›âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤âˆ‘ï¸
ğ›¼â‰ ğ›¼âˆ—(ğœƒâ€²)âˆ’1 ğœƒâ€²(1)+ğ‘¦ğ›¼,ğ‘›âˆ’ğ‘¦ğ›¼âˆ—,ğ‘›(A.1)
Proof. First, note that for ğ‘¥=ğ‘„(ğ‘¦), we have that ğ‘¥is the solution of the following optimization
problem
ğ‘„(ğ‘¦)=arg max(
âˆ‘ï¸
ğ›¼âˆˆAğ‘¦ğ›¼ğ‘¥ğ›¼âˆ’â„(ğ‘¥):âˆ‘ï¸
ğ›¼âˆˆAğ‘¥ğ›¼=1andâˆ€ğ›¼âˆˆA:ğ‘¥ğ›¼â‰¥0)
By solving the Karushâ€“Kuhnâ€“Tucker ( KKT ) conditions to this optimization problem we readily get
thatğ‘¥lies in the interior of X, sinceğœƒ(0+)=âˆ’âˆ, and thus we obtain that at the solution, it holds
ğ‘¦ğ›¼=ğœƒâ€²(ğ‘¥ğ›¼)+ğœ†forğœ†âˆˆâ„. Therefore, we have:
ğ‘¦ğ›¼,ğ‘›âˆ’ğ‘¦ğ›¼âˆ—,ğ‘›=ğœƒâ€²(ğ‘¥ğ›¼,ğ‘›)âˆ’ğœƒâ€²(ğ‘¥ğ›¼âˆ—,ğ‘›) (A.2)
or equivalently:
ğœƒâ€²(ğ‘¥ğ›¼,ğ‘›)=ğœƒâ€²(ğ‘¥ğ›¼âˆ—,ğ‘›)+ğ‘¦ğ›¼,ğ‘›âˆ’ğ‘¦ğ›¼âˆ—,ğ‘›â‰¤ğœƒâ€²(1)+ğ‘¦ğ›¼,ğ‘›âˆ’ğ‘¦ğ›¼âˆ—,ğ‘› (A.3)
Now, assume that there exists ğ›¼âˆˆAsuch thatğ‘¥ğ›¼,ğ‘›does not converge to 0, that is, lim supğ‘›ğ‘¥ğ›¼,ğ‘›>ğœ€
for someğœ€>0. Then, since ğœƒis strongly convex, ğœƒâ€²is strictly increasing, and thus ğœƒâ€²(ğ‘¥ğ›¼,ğ‘›)â‰¥ğœƒâ€²(ğœ€)
infinitely often. However, by taking ğ‘›â†’âˆ in(A.3) , it implies that ğœƒâ€²(ğ‘¥ğ›¼,ğ‘›)â†’âˆ’âˆ , which is a
contradiction. Therefore, we conclude that for all ğ›¼â‰ ğ›¼âˆ—, it holds that limğ‘›â†’âˆğ‘¥ğ›¼,ğ‘›=0, and the
convergence result follows.
Finally, note that since ğœƒâ€²is strictly increasing, it is invertible and its inverse is strictly increasing as
well. Thus, for each ğ›¼â‰ ğ›¼âˆ—we have:
ğ‘¥ğ›¼,ğ‘›â‰¤(ğœƒâ€²)âˆ’1 ğœƒâ€²(1)+ğ‘¦ğ›¼,ğ‘›âˆ’ğ‘¦ğ›¼âˆ—,ğ‘›(A.4)
Therefore,
âˆ¥ğ‘¥ğ‘›âˆ’ğ‘¥âˆ—âˆ¥âˆ=1âˆ’ğ‘¥ğ›¼âˆ—,ğ‘›=âˆ‘ï¸
ğ›¼â‰ ğ›¼âˆ—ğ‘¥ğ›¼,ğ‘›â‰¤âˆ‘ï¸
ğ›¼â‰ ğ›¼âˆ—(ğœƒâ€²)âˆ’1 ğœƒâ€²(1)+ğ‘¦ğ›¼,ğ‘›âˆ’ğ‘¦ğ›¼âˆ—,ğ‘›(A.5)
and our proof is complete. â– 
B Proofs for Continuous Time Algorithms
In this appendix, we provide the proof of Theorem 1 and discuss the convergence of (FTXL-D) under
anon-vanishing friction coefficient â€“ that is, ğ‘ŸÂ¤ğ‘¦instead of(ğ‘Ÿ/ğ‘¡)Â¤ğ‘¦. First, we provide a lemma that is
necessary for our analysis.
13Lemma B.1. Letğ‘¥âˆ—=(ğ›¼âˆ—
1,...,ğ›¼âˆ—
ğ‘)âˆˆXbe a strict Nash equilibrium of Î“, and letğ‘‘denote the
minimum payoff difference at equilibrium, i.e.,
ğ‘‘:=min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]. (B.1)
Then, for any ğ‘âˆˆ(0,ğ‘‘), there exists ğ‘€ > 0such that ifğ‘¦ğ‘–ğ›¼âˆ—
ğ‘–âˆ’ğ‘¦ğ‘–ğ›¼ğ‘–> ğ‘€ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–and
ğ‘–âˆˆN, then
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘„(ğ‘¦))âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘„(ğ‘¦))>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN. (B.2)
Proof. Sinceğ‘¥âˆ—is a strict Nash equilibrium, the minimum payoff difference ğ‘‘atğ‘¥âˆ—is bounded away
from zero. Then, by continuity of the function ğ‘¥â†¦â†’ğ‘£(ğ‘¥), there exists a neighborhood Uâˆ—ofğ‘¥âˆ—such
that for anyğ‘¥âˆˆUâˆ—, it holds
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘¥)âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘¥)>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN (B.3)
Finally, by Giannou et al. [18, Lemma C.2. ], there exists ğ‘€ > 0, such thatğ‘„(ğ‘¦)âˆˆUâˆ—for allğ‘¦âˆˆVâˆ—
with
ğ‘¦ğ‘–ğ›¼âˆ—
ğ‘–âˆ’ğ‘¦ğ‘–ğ›¼ğ‘–>ğ‘€ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN (B.4)
Therefore, we readily get that if ğ‘¦âˆˆVâˆ—satisfies the above relation, then
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘„(ğ‘¦))âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘„(ğ‘¦))>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN. â– 
We are now in a position to prove Theorem 1, which we restate below for convenience.
Theorem 1. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, and letğ‘¥(ğ‘¡)=ğ‘„(ğ‘¦(ğ‘¡))be a solution orbit of
(FTXL-D) . Ifğ‘¥(0)is sufficiently close to ğ‘¥âˆ—, thenğ‘¥(ğ‘¡)converges to ğ‘¥âˆ—; in particular, if (FTXL-D) is
run with logit best responses (that is,ğ‘„â†Î›), we have
âˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ‘¡2
2(ğ‘Ÿ+1)
(6)
whereğ¶ >0is a constant that depends only on the initialization of (FTXL-D) and
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]>0 (7)
is the minimum payoff difference at equilibrium.
Proof. First of all, since ğ‘¥âˆ—is a strict Nash equilibrium, by Lemma B.1 for
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]
there existsğ‘€ > 0such that ifğ‘¦ğ‘–ğ›¼âˆ—
ğ‘–âˆ’ğ‘¦ğ‘–ğ›¼ğ‘–>ğ‘€ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–andğ‘–âˆˆN, then
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘„(ğ‘¦))âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘„(ğ‘¦))>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN. (B.5)
From now on, for notational convenience, we focus on player ğ‘–âˆˆNand drop the player-specific
indices altogether. Then, for ğ›¼â‰ ğ›¼âˆ—âˆˆA, we letğ‘§ğ›¼(ğ‘¡):=ğ‘¦ğ›¼(ğ‘¡)âˆ’ğ‘¦âˆ—
ğ›¼(ğ‘¡), which evolves as:
Â¥ğ‘§(ğ‘¡)=ğ‘£ğ›¼(ğ‘¥(ğ‘¡))âˆ’ğ‘£ğ›¼âˆ—(ğ‘¥(ğ‘¡))âˆ’ğ‘Ÿ
ğ‘¡Â¤ğ‘§ğ›¼(ğ‘¡) (B.6)
Letğ‘¦(0)such thatğ‘§ğ›¼(0)=âˆ’ğ‘€âˆ’ğœ€, for allğ›¼â‰ ğ›¼âˆ—âˆˆA, whereğœ€>0small. We will, first, show that
ğ‘§(ğ‘¡)<âˆ’ğ‘€for allğ‘¡â‰¥0. For the sake of contradiction, and denoting ğ‘‡0:=inf{ğ‘¡â‰¥0 :ğ‘§(ğ‘¡)â‰¥âˆ’ğ‘€},
suppose that ğ‘‡0<âˆ. Then, we readily get that for all ğ‘¡ <ğ‘‡ 0, it holds
ğ‘£ğ›¼(ğ‘¥(ğ‘¡))âˆ’ğ‘£ğ›¼âˆ—(ğ‘¥(ğ‘¡))<âˆ’ğ‘ (B.7)
and therefore, for all ğ‘¡â‰¤ğ‘‡0:
Â¥ğ‘§ğ›¼(ğ‘¡)ğ‘¡ğ‘Ÿ+ğ‘Ÿğ‘¡ğ‘Ÿâˆ’1Â¤ğ‘§ğ›¼(ğ‘¡)=ğ‘¡ğ‘Ÿ[ğ‘£ğ›¼(ğ‘¥)âˆ’ğ‘£ğ›¼âˆ—(ğ‘¥)]â‰¤âˆ’ğ‘ğ‘¡ğ‘Ÿ(B.8)
14which can be rewritten as:
ğ‘‘
ğ‘‘ğ‘¡(Â¤ğ‘§ğ›¼(ğ‘¡)ğ‘¡ğ‘Ÿ)â‰¤âˆ’ğ‘ğ‘¡ğ‘Ÿ(B.9)
Integrating over ğ‘¡ <ğ‘‡ 0, we obtainÂ¤ğ‘§ğ›¼(ğ‘¡)ğ‘¡ğ‘Ÿâ‰¤âˆ’ğ‘ğ‘¡ğ‘Ÿ+1/(ğ‘Ÿ+1), which readily implies:
ğ‘§ğ›¼(ğ‘¡)â‰¤ğ‘§ğ›¼(0)âˆ’ğ‘
2(ğ‘Ÿ+1)ğ‘¡2
<âˆ’ğ‘€âˆ’ğ‘
2(ğ‘Ÿ+1)ğ‘¡2(B.10)
By sendingğ‘¡â†’ğ‘‡0, we arrive at a contradiction. Therefore ğ‘§ğ›¼(ğ‘¡)<âˆ’ğ‘€for allğ‘¡â‰¥0, and the previous
equation implies that for all ğ‘¡â‰¥0 :
ğ‘§ğ›¼(ğ‘¡)â‰¤ğ‘§ğ›¼(0)âˆ’ğ‘
2(ğ‘Ÿ+1)ğ‘¡2(B.11)
and invoking Lemma A.1, we get the convergence result. Finally, translating the score-differences to
the primal space X, we get:
âˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥âˆ=max
ğ‘–âˆˆN
1âˆ’ğ‘¥ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘¡)	
(B.12)
For the case of logit best responses, i.e., when ğ‘„â†Î›, and assuming that the maximum above is
attained for player ğ‘–âˆˆN, we obtain
âˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥âˆ=P
ğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–exp(ğ‘§ğ›¼ğ‘–(ğ‘¡))
1+P
ğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–exp(ğ‘§ğ›¼ğ‘–(ğ‘¡))
â‰¤âˆ‘ï¸
ğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–exp(ğ‘§ğ›¼ğ‘–(ğ‘¡))
â‰¤|Ağ‘–|exp
ğ‘§ğ›¼ğ‘–(0)âˆ’ğ‘
2(ğ‘Ÿ+1)ğ‘¡2
â‰¤exp
ğ¶âˆ’ğ‘
2(ğ‘Ÿ+1)ğ‘¡2
(B.13)
forğ¶=log|Ağ‘–|+ğ‘§ğ›¼ğ‘–(0). â– 
Now, moving to the case where we use a constant friction coefficient â€“ ğ‘ŸÂ¤ğ‘¦instead of(ğ‘Ÿ/ğ‘¡)Â¤ğ‘¦,(FTXL-D)
becomes:
ğ‘‘2ğ‘¦
ğ‘‘ğ‘¡2=ğ‘£(ğ‘„(ğ‘¦))âˆ’ğ‘Ÿğ‘‘ğ‘¦
ğ‘‘ğ‘¡(B.14)
Under, (B.14), we obtain the following convergence result.
Theorem B.1. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, and letğ‘¥(ğ‘¡)=ğ‘„(ğ‘¦(ğ‘¡))be a solution orbit of
(B.14) . Ifğ‘¥(0)is sufficiently close to ğ‘¥âˆ—, thenğ‘¥(ğ‘¡)converges to ğ‘¥âˆ—; in particular, if (B.14) is run with
logit best responses (that is,ğ‘„â†Î›), we have
âˆ¥ğ‘¥(ğ‘¡)âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘
ğ‘Ÿğ‘¡âˆ’ğ‘
ğ‘Ÿ2ğ‘’âˆ’ğ‘Ÿğ‘¡+ğ‘
ğ‘Ÿ2
(B.15)
whereğ¶ >0is a constant that depends on the initialization of (B.14) and
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]>0 (B.16)
is the minimum payoff difference at equilibrium.
Proof. The initial steps of proof of Theorem B.1 are similar to the proof of Theorem 1, which we
include for the sake of completeness.
Specifically, by Lemma B.1 there exists ğ‘€ > 0such that ifğ‘¦ğ‘–ğ›¼âˆ—
ğ‘–âˆ’ğ‘¦ğ‘–ğ›¼ğ‘–>ğ‘€ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–and
ğ‘–âˆˆN, then
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘„(ğ‘¦))âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘„(ğ‘¦))>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN. (B.17)
15Now, for notational convenience, we focus on player ğ‘–âˆˆNand drop the player-specific indices
altogether. Then, for ğ›¼â‰ ğ›¼âˆ—âˆˆA, we letğ‘§ğ›¼(ğ‘¡):=ğ‘¦ğ›¼(ğ‘¡)âˆ’ğ‘¦âˆ—
ğ›¼(ğ‘¡), which evolves as:
Â¥ğ‘§(ğ‘¡)=ğ‘£ğ›¼(ğ‘¥(ğ‘¡))âˆ’ğ‘£ğ›¼âˆ—(ğ‘¥(ğ‘¡))âˆ’ğ‘ŸÂ¤ğ‘§ğ›¼(ğ‘¡) (B.18)
Letğ‘¦(0)such thatğ‘§ğ›¼(0)=âˆ’ğ‘€âˆ’ğœ€, for allğ›¼â‰ ğ›¼âˆ—âˆˆA, whereğœ€ >0small. As in the proof of
Theorem 1, we will, first, show that ğ‘§(ğ‘¡)<âˆ’ğ‘€for allğ‘¡â‰¥0. For the sake of contradiction, and
denotingğ‘‡0:=inf{ğ‘¡â‰¥0 :ğ‘§(ğ‘¡)â‰¥âˆ’ğ‘€}, suppose that ğ‘‡0<âˆ. Then, we readily get that for all ğ‘¡ <ğ‘‡ 0,
it holds
ğ‘£ğ›¼(ğ‘¥(ğ‘¡))âˆ’ğ‘£ğ›¼âˆ—(ğ‘¥(ğ‘¡))<âˆ’ğ‘ (B.19)
and therefore, for all ğ‘¡â‰¤ğ‘‡0:
Â¥ğ‘§ğ›¼(ğ‘¡)ğ‘’ğ‘Ÿğ‘¡+ğ‘Ÿğ‘’ğ‘Ÿğ‘¡Â¤ğ‘§ğ›¼(ğ‘¡)=ğ‘’ğ‘Ÿğ‘¡[ğ‘£ğ›¼(ğ‘¥)âˆ’ğ‘£ğ›¼âˆ—(ğ‘¥)]â‰¤âˆ’ğ‘ğ‘’ğ‘Ÿğ‘¡(B.20)
which can be rewritten as:
ğ‘‘
ğ‘‘ğ‘¡ Â¤ğ‘§ğ›¼(ğ‘¡)ğ‘’ğ‘Ÿğ‘¡â‰¤âˆ’ğ‘ğ‘’ğ‘Ÿğ‘¡(B.21)
Integrating over ğ‘¡ <ğ‘‡ 0, and using thatÂ¤ğ‘§ğ›¼(0)=0, we obtainÂ¤ğ‘§ğ›¼(ğ‘¡)â‰¤âˆ’ğ‘/ğ‘Ÿ+ğ‘ğ‘’âˆ’ğ‘Ÿğ‘¡/ğ‘Ÿ, which implies:
ğ‘§ğ›¼(ğ‘¡)â‰¤ğ‘§ğ›¼(0)âˆ’ğ‘
ğ‘Ÿğ‘¡âˆ’ğ‘
ğ‘Ÿ2ğ‘’âˆ’ğ‘Ÿğ‘¡+ğ‘
ğ‘Ÿ2
=ğ‘§ğ›¼(0)âˆ’ğ‘
ğ‘Ÿ2 ğ‘Ÿğ‘¡+ğ‘’âˆ’ğ‘Ÿğ‘¡âˆ’1
<ğ‘§ğ›¼(0)
<âˆ’ğ‘€ (B.22)
where we used the fact that ğ‘¥+ğ‘’âˆ’ğ‘¥âˆ’1â‰¥0for allğ‘¥âˆˆâ„with equality if and only if ğ‘¥=0. By
sendingğ‘¡â†’ğ‘‡0, we arrive at a contradiction. Therefore ğ‘§ğ›¼(ğ‘¡)<âˆ’ğ‘€for allğ‘¡â‰¥0, and the previous
equation implies that for all ğ‘¡â‰¥0 :
ğ‘§ğ›¼(ğ‘¡)â‰¤ğ‘§ğ›¼(0)âˆ’ğ‘
ğ‘Ÿğ‘¡âˆ’ğ‘
ğ‘Ÿ2ğ‘’âˆ’ğ‘Ÿğ‘¡+ğ‘
ğ‘Ÿ2(B.23)
and invoking Lemma A.1 for ğœƒ(ğ‘¥)=ğ‘¥logğ‘¥, we get the convergence result. â– 
C Proofs for discrete-time algorithms with full information
In this section, we provide the results for the (FTXL) algorithm with full-information feedback. First,
we discuss the rates obtained by the direct discretization of (FTXL-D) with both vanishing and non-
vanishing friction, and then provide the proof of Theorem 2, our main result, for the full-information
case.
C.1. FTXL with vanishing friction. First, we provide the rate of convergence for the discrete
version of (FTXL-D) with vanishing friction:
ğ‘ğ‘–,ğ‘›+1=ğ‘ğ‘–,ğ‘›
1âˆ’ğ›¾ğ‘Ÿ
ğ‘›
+ğ›¾Ë†ğ‘£ğ‘–,ğ‘›
ğ‘¦ğ‘–,ğ‘›+1=ğ‘¦ğ‘–,ğ‘›+ğ›¾ğ‘ğ‘–,ğ‘›+1(C.1)
To streamline our presentation, we consider the setup of Example 3.1 that provides a lower bound for
the algorithm.
Proposition C.1. Consider the single-player game Î“with actions AandBsuch thatğ‘¢(A)âˆ’ğ‘¢(B)=1
of Example 3.1, and let ğ‘¥ğ‘›=Î›(ğ‘¦ğ‘›)be the sequence of play generated by (C.1) . Then, denoting by
ğ‘¥âˆ—=(1,0)the strict Nash equilibrium, we have:
âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆâˆ¼exp
ğ¶âˆ’ğ›¾2ğ‘‡2
2(ğ›¾ğ‘Ÿ+1)
. (C.2)
whereğ¶ >0is a constant that depends only on the initialization of the algorithm.
16Proof. We first define the score-difference
ğ‘¤ğ‘›:=ğ‘B,ğ‘›âˆ’ğ‘A,ğ‘› (C.3)
with initial condition ğ‘¤1=0. Then, unfolding according to the sequence of play, we obtain:
ğ‘¤ğ‘›+1=ğ‘¤ğ‘›
1âˆ’ğ›¾ğ‘Ÿ
ğ‘›
+ğ›¾(ğ‘¢(B)âˆ’ğ‘¢(A))
=ğ‘¤ğ‘›
1âˆ’ğ›¾ğ‘Ÿ
ğ‘›
âˆ’ğ›¾
=âˆ’ğ›¾ğ‘›âˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’1Y
â„“=0
1âˆ’ğ›¾ğ‘Ÿ
ğ‘›âˆ’â„“
âˆ’ğ›¾ (C.4)
We next define for ğ‘›âˆˆâ„•the difference ğ‘§ğ‘›:=ğ‘¦B,ğ‘›âˆ’ğ‘¦A,ğ‘›. Thus, unfolding it, we obtain:
ğ‘§ğ‘›+1=ğ‘§ğ‘›+ğ›¾ğ‘¤ğ‘›+1
=ğ‘§ğ‘›âˆ’ğ›¾2 
1+ğ‘›âˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’1Y
â„“=0
1âˆ’ğ›¾ğ‘Ÿ
ğ‘›âˆ’â„“!
=ğ‘§1âˆ’ğ›¾2ğ‘›âˆ‘ï¸
ğ‘š=1 
1+ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’1Y
â„“=0
1âˆ’ğ›¾ğ‘Ÿ
ğ‘šâˆ’â„“!
(C.5)
Now, using Lemma C.1, which we provide after this proof, we obtain that
ğ‘§ğ‘›+1=ğ‘§1âˆ’ğ›¾2ğ‘›âˆ‘ï¸
ğ‘š=1 
1+ğ‘šâˆ’ğ›¾ğ‘Ÿ
1+ğ›¾ğ‘Ÿâˆ’1
1+ğ›¾ğ‘Ÿğ‘šY
â„“=1
1âˆ’ğ›¾ğ‘Ÿ
â„“!
=ğ‘§1âˆ’ğ›¾2ğ‘›(ğ‘›+1)
2(1+ğ›¾ğ‘Ÿ)âˆ’ğ›¾2ğ‘›
1âˆ’ğ›¾ğ‘Ÿ
1+ğ›¾ğ‘Ÿ
+ğ›¾2
1+ğ›¾ğ‘Ÿğ‘›âˆ‘ï¸
ğ‘š=1ğ‘šY
â„“=1
1âˆ’ğ›¾ğ‘Ÿ
â„“
=ğ‘§1âˆ’ğ›¾2ğ‘›2
2(1+ğ›¾ğ‘Ÿ)+Î˜(ğ‘›) (C.6)
and invoking Lemma A.1 for ğœƒ(ğ‘¥)=ğ‘¥logğ‘¥, we get the result. â– 
The following lemma is a necessary tool for obtaining the exact convergence rate in Proposition C.2.
Lemma C.1. For anyğ‘šâˆˆâ„•andğ‘>0, we have that
ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’1Y
â„“=0(1âˆ’ğ‘
ğ‘šâˆ’â„“)=ğ‘šâˆ’ğ‘
1+ğ‘âˆ’1
1+ğ‘ğ‘šY
â„“=1
1âˆ’ğ‘
â„“
(C.7)
Proof. First, by expanding the inner product, we can rewrite the expression as
ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’1Y
â„“=0(1âˆ’ğ‘
ğ‘šâˆ’â„“)=ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’1Y
â„“=0(ğ‘šâˆ’â„“âˆ’ğ‘
ğ‘šâˆ’â„“)
ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1(ğ‘šâˆ’ğ‘)...(ğ‘šâˆ’ğ‘˜+1âˆ’ğ‘)
ğ‘š...(ğ‘šâˆ’ğ‘˜+1)
=ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1(ğ‘šâˆ’ğ‘)!(ğ‘šâˆ’ğ‘˜)!
(ğ‘šâˆ’ğ‘˜âˆ’ğ‘)!ğ‘š!
=(ğ‘šâˆ’ğ‘)!
ğ‘š!ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1(ğ‘šâˆ’ğ‘˜)!
(ğ‘šâˆ’ğ‘˜âˆ’ğ‘)!(C.8)
where with a slight abuse of notation we use the factorial notation (ğ‘šâˆ’ğ‘)!to denote the Gamma
function evaluated at ğ‘šâˆ’ğ‘+1, i.e.,Î“(ğ‘šâˆ’ğ‘+1).
Now, defining the quantity
ğ¹ğ‘š:=(ğ‘šâˆ’ğ‘)!
ğ‘š!ğ‘šâˆ‘ï¸
ğ‘˜=1(ğ‘šâˆ’ğ‘˜)!
(ğ‘šâˆ’ğ‘˜âˆ’ğ‘)!
17the difference of two consecutive terms evolves as:
ğ¹ğ‘š+1âˆ’ğ¹ğ‘š=(ğ‘š+1âˆ’ğ‘)!
(ğ‘š+1)!ğ‘š+1âˆ‘ï¸
ğ‘˜=1(ğ‘š+1âˆ’ğ‘˜)!
(ğ‘š+1âˆ’ğ‘˜âˆ’ğ‘)!âˆ’(ğ‘šâˆ’ğ‘)!
ğ‘š!ğ‘šâˆ‘ï¸
ğ‘˜=1(ğ‘šâˆ’ğ‘˜)!
(ğ‘šâˆ’ğ‘˜âˆ’ğ‘)!
=ğ‘š+1âˆ’ğ‘
ğ‘š+1+(ğ‘š+1âˆ’ğ‘)!
(ğ‘š+1)!ğ‘š+1âˆ‘ï¸
ğ‘˜=2(ğ‘š+1âˆ’ğ‘˜)!
(ğ‘š+1âˆ’ğ‘˜âˆ’ğ‘)!âˆ’(ğ‘šâˆ’ğ‘)!
ğ‘š!ğ‘šâˆ‘ï¸
ğ‘˜=1(ğ‘šâˆ’ğ‘˜)!
(ğ‘šâˆ’ğ‘˜âˆ’ğ‘)!
=ğ‘š+1âˆ’ğ‘
ğ‘š+1+(ğ‘š+1âˆ’ğ‘)!
(ğ‘š+1)!ğ‘š+1âˆ‘ï¸
ğ‘˜=2(ğ‘š+1âˆ’ğ‘˜)!
(ğ‘š+1âˆ’ğ‘˜âˆ’ğ‘)!âˆ’(ğ‘šâˆ’ğ‘)!
ğ‘š!ğ‘š+1âˆ‘ï¸
ğ‘˜=2(ğ‘šâˆ’ğ‘˜+1)!
(ğ‘šâˆ’ğ‘˜+1âˆ’ğ‘)!
=ğ‘š+1âˆ’ğ‘
ğ‘š+1+ğ‘š+1âˆ‘ï¸
ğ‘˜=2(ğ‘š+1âˆ’ğ‘)!(ğ‘š+1âˆ’ğ‘˜)!âˆ’(ğ‘š+1)(ğ‘šâˆ’ğ‘)!(ğ‘šâˆ’ğ‘˜+1)!
(ğ‘š+1)!(ğ‘š+1âˆ’ğ‘âˆ’ğ‘˜)!
=ğ‘š+1âˆ’ğ‘
ğ‘š+1+ğ‘š+1âˆ‘ï¸
ğ‘˜=2(ğ‘šâˆ’ğ‘)!(ğ‘š+1âˆ’ğ‘˜)!(ğ‘š+1âˆ’ğ‘âˆ’ğ‘šâˆ’1)
(ğ‘š+1)!(ğ‘š+1âˆ’ğ‘âˆ’ğ‘˜)!
=ğ‘š+1âˆ’ğ‘
ğ‘š+1âˆ’ğ‘ğ‘š+1âˆ‘ï¸
ğ‘˜=2(ğ‘šâˆ’ğ‘)!(ğ‘š+1âˆ’ğ‘˜)!
(ğ‘š+1)!(ğ‘š+1âˆ’ğ‘âˆ’ğ‘˜)!
=ğ‘š+1âˆ’ğ‘
ğ‘š+1âˆ’ğ‘
ğ‘š+1âˆ’ğ‘"
ğ‘š+1âˆ‘ï¸
ğ‘˜=1(ğ‘š+1âˆ’ğ‘˜)!(ğ‘š+1âˆ’ğ‘)!
(ğ‘š+1)!(ğ‘š+1âˆ’ğ‘âˆ’ğ‘˜)!âˆ’ğ‘š+1âˆ’ğ‘
ğ‘š+1#
=1âˆ’ğ‘
ğ‘š+1âˆ’ğ‘ğ¹ğ‘š+1 (C.9)
Thus, we readily obtain the recurrence relation
ğ‘š+1
ğ‘š+1âˆ’ğ‘ğ¹ğ‘š+1=ğ¹ğ‘š+1. (C.10)
We continue the proof by induction. To this end, we will show that
ğ¹ğ‘š=ğ‘šâˆ’ğ‘
1+ğ‘+ğ‘
1+ğ‘ğ‘šY
â„“=1â„“âˆ’ğ‘
â„“. (C.11)
For the base case, note that
ğ¹1=(1âˆ’ğ‘)=1âˆ’ğ‘
1+ğ‘+ğ‘
1+ğ‘(1âˆ’ğ‘) (C.12)
For the inductive step, suppose that (C.11) holds for ğ‘šâˆˆâ„•. Then, we have:
ğ‘š+1
ğ‘š+1âˆ’ğ‘ğ¹ğ‘š+1=ğ‘šâˆ’ğ‘
1+ğ‘+ğ‘
1+ğ‘ğ‘šY
â„“=1â„“âˆ’ğ‘
â„“
+1
=ğ‘š+1
1+ğ‘+ğ‘
1+ğ‘ğ‘šY
â„“=1â„“âˆ’ğ‘
â„“(C.13)
which implies the inductive step
ğ¹ğ‘š+1=ğ‘š+1âˆ’ğ‘
1+ğ‘+ğ‘
1+ğ‘ğ‘š+1Y
â„“=1â„“âˆ’ğ‘
â„“(C.14)
and thus (C.11) holds for all ğ‘šâˆˆâ„•. Finally, to complete the proof notice that
ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’1Y
â„“=0(1âˆ’ğ‘
ğ‘šâˆ’â„“)=(ğ‘šâˆ’ğ‘)!
ğ‘š!ğ‘šâˆ’1âˆ‘ï¸
ğ‘˜=1(ğ‘šâˆ’ğ‘˜)!
(ğ‘šâˆ’ğ‘˜âˆ’ğ‘)!
=ğ¹ğ‘šâˆ’ğ‘šâˆ’1Y
â„“=0
1âˆ’ğ‘
ğ‘šâˆ’â„“
=ğ‘šâˆ’ğ‘
1+ğ‘+ğ‘
1+ğ‘ğ‘šY
â„“=1â„“âˆ’ğ‘
â„“âˆ’ğ‘šY
â„“=1
1âˆ’ğ‘
â„“
=ğ‘šâˆ’ğ‘
1+ğ‘âˆ’1
1+ğ‘ğ‘šY
â„“=1
1âˆ’ğ‘
â„“
(C.15)
as was to be shown. â– 
18Next, we discuss the cases of non-vanishing and zero friction.
C.2. FTXL with non-vanishing friction. We continue this section by considering the case of
non-vanishing friction in analogy to the continuous-time case, as per Appendix B. Specifically, we
consider the discrete version of (FTXL-D) with non-vanishing friction, as follows:
ğ‘ğ‘–,ğ‘›+1=ğ‘ğ‘–,ğ‘›(1âˆ’ğ›¾ğ‘Ÿ)+ğ›¾Ë†ğ‘£ğ‘–,ğ‘›
ğ‘¦ğ‘–,ğ‘›+1=ğ‘¦ğ‘–,ğ‘›+ğ›¾ğ‘ğ‘–,ğ‘›+1(C.16)
withğ›¾ğ‘Ÿ < 1. Below, we provide the rate of convergence for the setup of ğ¸ğ‘¥ğ‘ğ‘šğ‘ğ‘™ğ‘’ 3.1, as we did
before. Namely, we obtain a linear convergence rate, as the following proposition suggests.
Proposition C.2. Consider the single-player game Î“with actions AandBsuch thatğ‘¢(A)âˆ’ğ‘¢(B)=1
of Example 3.1, and let ğ‘¥ğ‘›=Î›(ğ‘¦ğ‘›)be the sequence of play generated by (C.16) . Then, denoting by
ğ‘¥âˆ—=(1,0)the strict Nash equilibrium, we have:
âˆ¥ğ‘¥ğ‘›âˆ’ğ‘¥âˆ—âˆ¥âˆâˆ¼exp
ğ¶âˆ’ğ›¾
ğ‘Ÿğ‘›
. (C.17)
whereğ¶ >0is a constant that depends on the initialization of the algorithm.
Proof. We first define the score-difference
ğ‘¤ğ‘›:=ğ‘B,ğ‘›âˆ’ğ‘A,ğ‘› (C.18)
with initial condition ğ‘¤1=0. Then, unfolding according to the sequence of play, we obtain:
ğ‘¤ğ‘›+1=ğ‘¤ğ‘›(1âˆ’ğ›¾ğ‘Ÿ)+ğ›¾(ğ‘¢(B)âˆ’ğ‘¢(A))
=ğ‘¤ğ‘›(1âˆ’ğ›¾ğ‘Ÿ)âˆ’ğ›¾
=...
=âˆ’ğ›¾ğ‘›âˆ’1âˆ‘ï¸
ğ‘˜=0(1âˆ’ğ›¾ğ‘Ÿ)ğ‘˜
=âˆ’1âˆ’(1âˆ’ğ›¾ğ‘Ÿ)ğ‘›
ğ‘Ÿ(C.19)
We next define for ğ‘›âˆˆâ„•the difference ğ‘§ğ‘›:=ğ‘¦B,ğ‘›âˆ’ğ‘¦A,ğ‘›. Thus, unfolding it, we obtain:
ğ‘§ğ‘›+1=ğ‘§ğ‘›+ğ›¾ğ‘¤ğ‘›+1
=ğ‘§ğ‘›âˆ’ğ›¾1âˆ’(1âˆ’ğ›¾ğ‘Ÿ)ğ‘›
ğ‘Ÿ
=ğ‘§1âˆ’ğ›¾ğ‘›âˆ‘ï¸
ğ‘š=11âˆ’(1âˆ’ğ›¾ğ‘Ÿ)ğ‘š
ğ‘Ÿ
=ğ‘§1âˆ’ğ›¾
ğ‘Ÿ
ğ‘›âˆ’(1âˆ’ğ›¾ğ‘Ÿ)1âˆ’(1âˆ’ğ›¾ğ‘Ÿ)ğ‘›
ğ›¾ğ‘Ÿ
=ğ‘§1âˆ’ğ›¾
ğ‘Ÿğ‘›+O(1) (C.20)
and invoking Lemma A.1 for ğœƒ(ğ‘¥)=ğ‘¥logğ‘¥, we get the result. â– 
C.3. FTXL with zero friction. Moving forward to the case of ğ‘Ÿ=0as presented in Section 4, we
provide the proof of Theorem 2, which we restate below for convenience.
Theorem 2. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, and letğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)be the sequence of play
generated by (FTXL) with full information feedback of the form (11a) . Ifğ‘¥1is initialized sufficiently
close toğ‘¥âˆ—, thenğ‘¥ğ‘›converges to ğ‘¥âˆ—; in particular, if (FTXL) is run with logit best responses (that is,
ğ‘„â†Î›), we have
âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2
=exp âˆ’Î˜(ğ‘‡2)(12)
whereğ¶ >0is a constant that depends only on the initialization of (FTXL) and
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]>0 (13)
is the minimum payoff difference at equilibrium.
19Proof. First of all, since ğ‘¥âˆ—is a strict Nash equilibrium, by Lemma B.1 for
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]
there existsğ‘€ > 0such that ifğ‘¦ğ‘–ğ›¼âˆ—
ğ‘–âˆ’ğ‘¦ğ‘–ğ›¼ğ‘–>ğ‘€ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–andğ‘–âˆˆN, then
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘„(ğ‘¦))âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘„(ğ‘¦))>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN. (C.21)
For notational convenience, we focus on player ğ‘–and drop the player-specific indices altogether. Let
ğ›¼â‰ ğ›¼âˆ—âˆˆA, and define for ğ‘›âˆˆâ„•the quantities ğ‘¤ğ›¼,ğ‘›andğ‘§ğ›¼,ğ‘›as
ğ‘¤ğ›¼,ğ‘›:=âŸ¨ğ‘ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©, ğ‘§ğ›¼,ğ‘›:=âŸ¨ğ‘¦ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ© (C.22)
whereğ‘’ğ›¼,ğ‘’âˆ—
ğ›¼are the standard basis vectors corresponding to ğ›¼,ğ›¼âˆ—âˆˆA.
Let initial conditions ğ‘¦1such thatğ‘¦ğ›¼,1âˆ’ğ‘¦ğ›¼âˆ—,1=âˆ’ğ‘€âˆ’ğœ€, for allğ›¼â‰ ğ›¼âˆ—âˆˆA, whereğœ€>0small, and
ğ‘1=0. We will first show by induction that ğ‘§ğ›¼,ğ‘›<âˆ’ğ‘€for allğ‘›âˆˆâ„•. To this end, unfolding the
recursion, we obtain:
ğ‘¤ğ›¼,ğ‘›+1=ğ‘¤ğ›¼,ğ‘›+ğ›¾âŸ¨Ë†ğ‘£ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©
=ğ‘¤ğ›¼,ğ‘›+ğ›¾âŸ¨ğ‘£(ğ‘¥ğ‘›),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©
=ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ© (C.23)
where we used that ğ‘¤1=0. Now, for the sake of induction, suppose that
ğ‘§ğ›¼,ğ‘˜<âˆ’ğ‘€ for allğ‘˜=1,...,ğ‘› (C.24)
which implies that âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©<âˆ’ğ‘. With this in hand, we will prove that ğ‘§ğ›¼,ğ‘›+1<âˆ’ğ‘€, as
well. Specifically, we have:
ğ‘§ğ›¼,ğ‘›+1=ğ‘§ğ›¼,ğ‘›+ğ›¾ğ‘¤ğ›¼,ğ‘›+1=ğ‘§ğ›¼,ğ‘›+ğ›¾2ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©
â‰¤ğ‘§ğ›¼,ğ‘›âˆ’ğ‘ğ›¾2ğ‘›
â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾2ğ‘›âˆ‘ï¸
â„“=1â„“
<âˆ’ğ‘€ (C.25)
where we used the inductive hypothesis and the initial condition. Therefore, we conclude by induction
thatğ‘§ğ›¼,ğ‘›<âˆ’ğ‘€for allğ‘›âˆˆâ„•. Thus, we readily obtain that after ğ‘‡time-step:
ğ‘§ğ‘‡â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾2ğ‘‡âˆ’1âˆ‘ï¸
â„“=1â„“â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2(C.26)
and invoking Lemma A.1 for ğœƒ(ğ‘¥)=ğ‘¥logğ‘¥, we get the result. â– 
D Proofs for discrete-time algorithms with partial information
In this appendix, we provide the proofs of Theorem 3 and Theorem 4 that correspond to the con-
vergence of (FTXL) with realization-based and bandit feedback, respectively. For this, we need the
following lemma, which provides a maximal bound on a martingale process. Namely, we have:
Lemma D.1. Letğ‘€ğ‘›:=Pğ‘›
ğ‘˜=1ğ›¾ğ‘˜ğœ‰ğ‘˜be a martingale with respect to (Fğ‘›)ğ‘›âˆˆâ„•withğ”¼[âˆ¥ğœ‰ğ‘›âˆ¥ğ‘
âˆ—]â‰¤ğœğ‘
ğ‘›
for someğ‘>2. Then, forğœ‡âˆˆ(0,1)andğ‘›âˆˆâ„•:
â„™ 
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|>ğ‘ 
ğ‘›âˆ‘ï¸
ğ‘˜=1ğ›¾ğ‘˜!ğœ‡!
â‰¤ğ´ğ‘Pğ‘›
ğ‘˜=1ğ›¾ğ‘/2+1
ğ‘˜ğœğ‘
ğ‘˜ Pğ‘›
ğ‘˜=1ğ›¾ğ‘˜1+ğ‘(ğœ‡âˆ’1/2)(D.1)
whereğ´ğ‘is a constant depending only on ğ‘andğ‘.
20Proof. Fix someğœ‡âˆˆ(0,1). By Doobâ€™s maximal inequality [20, Corollary 2.1], we have:
â„™ 
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|>ğ‘ 
ğ‘›âˆ‘ï¸
ğ‘˜=1ğ›¾ğ‘˜!ğœ‡!
â‰¤ğ”¼[|ğ‘€ğ‘›|ğ‘]
ğ‘ğ‘ Pğ‘›
ğ‘˜=1ğ›¾ğ‘˜ğ‘ğœ‡ (D.2)
Now, applying the Burkholderâ€“Davisâ€“Gundy inequality [20, Theorem 2.10], we get that
ğ”¼[|ğ‘€ğ‘›|ğ‘]â‰¤ğ´ğ‘ğ”¼ï£®ï£¯ï£¯ï£¯ï£¯ï£° 
ğ‘›âˆ‘ï¸
ğ‘˜=1ğ›¾2
ğ‘˜âˆ¥ğœ‰ğ‘˜âˆ¥2
âˆ—!ğ‘/2ï£¹ï£ºï£ºï£ºï£ºï£»(D.3)
whereğ´ğ‘is a constant depending only on ğ‘andğ‘. Now, we will invoke the generalized HÃ¶lderâ€™s
inequality [4], we have:
 
ğ‘›âˆ‘ï¸
ğ‘˜=1ğ‘ğ‘˜ğ‘ğ‘˜!ğœŒ
â‰¤ 
ğ‘›âˆ‘ï¸
ğ‘˜=1ğ‘ğœ†ğœŒ
ğœŒâˆ’1
ğ‘˜!ğœŒâˆ’1ğ‘›âˆ‘ï¸
ğ‘˜=1ğ‘(1âˆ’ğœ†)ğœŒ
ğ‘˜ğ‘ğœŒ
ğ‘˜(D.4)
forğ‘ğ‘˜,ğ‘ğ‘˜â‰¥0,ğœŒ>1andğœ†âˆˆ[0,1). Thus, setting ğ‘ğ‘˜=ğ›¾2
ğ‘˜,ğ‘ğ‘˜=âˆ¥ğœ‰ğ‘˜âˆ¥2
âˆ—,ğœŒ=ğ‘/2andğœ†=1/2âˆ’1/ğ‘,
(D.2), combined with (D.3), becomes:
â„™ 
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|>ğ‘ 
ğ‘›âˆ‘ï¸
ğ‘˜=1ğ›¾ğ‘˜!ğœ‡!
â‰¤ğ´ğ‘ Pğ‘›
ğ‘˜=1ğ›¾ğ‘˜ğ‘/2âˆ’1Pğ‘›
ğ‘˜=1ğ›¾ğ‘/2+1
ğ‘˜ğ”¼[âˆ¥ğœ‰ğ‘˜âˆ¥ğ‘
âˆ—]
 Pğ‘›
ğ‘˜=1ğ›¾ğ‘˜ğ‘ğœ‡
â‰¤ğ´ğ‘Pğ‘›
ğ‘˜=1ğ›¾ğ‘/2+1
ğ‘˜ğœğ‘
ğ‘˜ Pğ‘›
ğ‘˜=1ğ›¾ğ‘˜1+ğ‘(ğœ‡âˆ’1/2)(D.5)
and our proof is complete. â– 
With this tool in hand, we proceed to prove the convergence of (FTXL) under realization-based
feedback. For convenience, we restate the relevant result below.
Theorem 3. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, fix some confidence level ğ›¿ > 0, and let
ğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)be the sequence of play generated by (FTXL) with realization-based feedback as per
(11b) and a sufficiently small step-size ğ›¾>0. Then there exists a neighborhood Uofğ‘¥âˆ—such that
â„™(ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ)â‰¥1âˆ’ğ›¿ ifğ‘¥1âˆˆU. (14)
In particular, if (FTXL) is run with logit best responses (that is,ğ‘„â†Î›), there exist positive
constantsğ¶,ğ‘> 0as in Theorem 2 such that on the event {ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ}:
âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+3
5ğ‘ğ›¾5/3ğ‘‡5/3
=exp âˆ’Î˜(ğ‘‡2). (15)
Proof. First of all, since ğ‘¥âˆ—is a strict Nash equilibrium, by Lemma B.1 for
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]
there existsğ‘€ > 0such that ifğ‘¦ğ‘–ğ›¼âˆ—
ğ‘–âˆ’ğ‘¦ğ‘–ğ›¼ğ‘–>ğ‘€ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–andğ‘–âˆˆN, then
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘„(ğ‘¦))âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘„(ğ‘¦))>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN. (D.6)
For notational convenience, we focus on player ğ‘–and drop the player-specific indices altogether. Let
ğ›¼â‰ ğ›¼âˆ—âˆˆA, and define for ğ‘›âˆˆâ„•the quantities ğ‘¤ğ›¼,ğ‘›andğ‘§ğ›¼,ğ‘›as
ğ‘¤ğ›¼,ğ‘›:=âŸ¨ğ‘ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©, ğ‘§ğ›¼,ğ‘›:=âŸ¨ğ‘¦ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ© (D.7)
whereğ‘’ğ›¼,ğ‘’âˆ—
ğ›¼are the standard basis vectors corresponding to ğ›¼,ğ›¼âˆ—âˆˆA.
Then, unfolding the recursion, we obtain:
ğ‘¤ğ›¼,ğ‘›+1=ğ‘¤ğ›¼,ğ‘›+ğ›¾âŸ¨Ë†ğ‘£ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©=ğ‘¤ğ›¼,ğ‘›+ğ›¾âŸ¨ğ‘£(ğ‘¥ğ‘›),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ›¾âŸ¨ğ‘ˆğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©
=ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘ˆğ‘˜,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ© (D.8)
21where we used that ğ‘¤1=0. Now, define the stochastic process {ğ‘€ğ‘›}ğ‘›âˆˆâ„•as
ğ‘€ğ‘›:=ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘ˆğ‘˜,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ© (D.9)
which is a martingale, since ğ”¼[ğ‘ˆğ‘›|Fğ‘›]=0. Moreover, note that
âˆ¥ğ‘ˆğ‘›âˆ¥âˆ—=âˆ¥ğ‘£(ğ›¼ğ‘›)âˆ’ğ‘£(ğ‘¥ğ‘›)âˆ¥âˆ—â‰¤2 max
ğ›¼âˆˆAâˆ¥ğ‘£(ğ›¼)âˆ¥âˆ— (D.10)
and, thus, we readily obtain that ğ”¼[âˆ¥ğ‘ˆğ‘›âˆ¥ğ‘
âˆ—|Fğ‘›]â‰¤ğœğ‘forğœ=2 maxğ›¼âˆˆAâˆ¥ğ‘£(ğ›¼)âˆ¥âˆ—and allğ‘âˆˆ[1,âˆ].
By Lemma D.1 for ğ›¾ğ‘›=ğ›¾,ğœğ‘›=ğœ,ğœ‰ğ‘›=âŸ¨ğ‘ˆğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©,ğ‘as in Theorem 2, and ğœ‡âˆˆ(0,1),ğ‘ > 2
whose values will be determined next, there exists ğ´ğ‘>0such that:
ğ›¿ğ‘›:=â„™
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|>ğ‘(ğ›¾ğ‘›)ğœ‡
â‰¤ğ´ğ‘ğœğ‘ğ‘›ğ›¾ğ‘/2+1
(ğ›¾ğ‘›)1+ğ‘(ğœ‡âˆ’1/2)
â‰¤ğ´ğ‘ğœğ‘ğ›¾ğ‘(1âˆ’ğœ‡)
ğ‘›ğ‘(ğœ‡âˆ’1/2)(D.11)
Now, we need to guarantee that there exist ğœ‡âˆˆ(0,1),ğ‘> 2, such that
âˆâˆ‘ï¸
ğ‘›=1ğ›¿ğ‘›<âˆ (D.12)
For this, we simply need ğ‘(ğœ‡âˆ’1/2)>1, or equivalently, ğœ‡ > 1/2+1/ğ‘, which implies that
ğœ‡âˆˆ(1/2,1).
Therefore, for ğ›¾small enough, we getPâˆ
ğ‘›=1ğ›¿ğ‘›<ğ›¿, and therefore:
â„™ 
âˆ\
ğ‘›=1
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|â‰¤ğ‘(ğ›¾ğ‘›)ğœ‡!
=1âˆ’â„™ 
âˆ[
ğ‘›=1
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|>ğ‘(ğ›¾ğ‘›)ğœ‡!
â‰¥1âˆ’âˆâˆ‘ï¸
ğ‘›=1ğ›¿ğ‘›
â‰¥1âˆ’ğ›¿ (D.13)
From now on, we denote the good eventTâˆ
ğ‘›=1
supğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|â‰¤ğ‘(ğ›¾ğ‘›)ğœ‡	
byğ¸. Then, with probability
at least 1âˆ’ğ›¿:
ğ‘¤ğ›¼,ğ‘›+1â‰¤ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ‘(ğ›¾ğ‘›)ğœ‡for allğ‘›âˆˆâ„•. (D.14)
Furthermore, we have that for ğ‘›>ğ‘ 0:=âŒˆ1/ğ›¾âŒ‰, we readily get that ğ›¾ğ‘›>(ğ›¾ğ‘›)ğœ‡. Therefore, setting
ğ‘…:=ğ‘ğ›¾ğ‘0âˆ’1âˆ‘ï¸
ğ‘˜=1((ğ›¾ğ‘˜)ğœ‡âˆ’ğ›¾ğ‘˜) (D.15)
we obtain:
âˆ’ğ‘ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1(ğ›¾ğ‘˜âˆ’(ğ›¾ğ‘˜)ğœ‡)â‰¤ğ‘… (D.16)
for allğ‘›âˆˆâ„•. Then, initializing ğ‘¦1such thatğ‘§ğ›¼,1<âˆ’ğ‘€âˆ’ğ‘…, we will show that ğ‘§ğ›¼,ğ‘›<âˆ’ğ‘€for all
ğ‘›âˆˆâ„•with probability at least 1âˆ’ğ›¿. For this, suppose that ğ¸is realized, and assume that
ğ‘§ğ›¼,ğ‘˜<âˆ’ğ‘€ for allğ‘˜=1,...,ğ‘› (D.17)
We will show that ğ‘§ğ›¼,ğ‘›+1<âˆ’ğ‘€, as well. For this, we have:
ğ‘§ğ›¼,ğ‘›+1=ğ‘§ğ›¼,ğ‘›+ğ›¾ğ‘¤ğ›¼,ğ‘›+1
â‰¤ğ‘§ğ›¼,ğ‘›+ğ›¾ 
ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ‘(ğ›¾ğ‘›)ğœ‡!
22â‰¤ğ‘§ğ›¼,ğ‘›âˆ’ğ‘ğ›¾(ğ›¾ğ‘›âˆ’(ğ›¾ğ‘›)ğœ‡)
â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1(ğ›¾ğ‘˜âˆ’(ğ›¾ğ‘˜)ğœ‡)
â‰¤âˆ’ğ‘€âˆ’ğ‘…âˆ’ğ‘ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1(ğ›¾ğ‘˜âˆ’(ğ›¾ğ‘˜)ğœ‡)
<âˆ’ğ‘€ (D.18)
Therefore, we conclude by induction that ğ‘§ğ›¼,ğ‘›<âˆ’ğ‘€for allğ‘›âˆˆâ„•. Thus, we readily obtain that with
probability at least 1âˆ’ğ›¿it holds:
ğ‘§ğ›¼,ğ‘‡â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾ğ‘‡âˆ’1âˆ‘ï¸
ğ‘˜=1(ğ›¾ğ‘˜âˆ’(ğ›¾ğ‘˜)ğœ‡)
â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+ğ‘ğ›¾1+ğœ‡âˆ«ğ‘‡
0ğ‘¡ğœ‡ğ‘‘ğ‘¡
â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+ğ‘ğ›¾1+ğœ‡ğ‘‡ğœ‡+1
ğœ‡+1(D.19)
for allğ‘‡âˆˆâ„•. Settingğœ‡=2/3and invoking Lemma A.1 for ğœƒ(ğ‘¥)=ğ‘¥logğ‘¥, we get the result. â– 
Finally, we prove the convergence of (FTXL) with bandit feedback. Again, for convenience, we
restate the relevant result below.
Theorem 4. Letğ‘¥âˆ—be a strict Nash equilibrium of Î“, fix some confidence level ğ›¿ > 0, and let
ğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)be the sequence of play generated by (FTXL) with bandit feedback of the form (11c) ,
anIWE exploration parameter ğœ€ğ‘›âˆ1/ğ‘›â„“ğœ€for someâ„“ğœ€âˆˆ(0,1/2), and a sufficiently small step-size
ğ›¾>0. Then there exists a neighborhood Uofğ‘¥âˆ—inXsuch that
â„™(ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ)â‰¥1âˆ’ğ›¿ ifğ‘¥1âˆˆU. (17)
In particular, if (FTXL) is run with logit best responses (that is,ğ‘„â†Î›), there exist positive
constantsğ¶,ğ‘> 0as in Theorem 2 such that on the event {ğ‘¥ğ‘›â†’ğ‘¥âˆ—asğ‘›â†’âˆ}
âˆ¥ğ‘¥ğ‘‡âˆ’ğ‘¥âˆ—âˆ¥âˆâ‰¤exp
ğ¶âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+5
9ğ‘ğ›¾9/5ğ‘‡9/5
=exp âˆ’Î˜(ğ‘‡2). (18)
Proof. First of all, since ğ‘¥âˆ—is a strict Nash equilibrium, by Lemma B.1 for
ğ‘=1
2min
ğ‘–âˆˆNmin
ğ›½ğ‘–âˆ‰supp(ğ‘¥âˆ—
ğ‘–)[ğ‘¢ğ‘–(ğ‘¥âˆ—
ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)âˆ’ğ‘¢ğ‘–(ğ›½ğ‘–;ğ‘¥âˆ—
âˆ’ğ‘–)]
there existsğ‘€ > 0such that ifğ‘¦ğ‘–ğ›¼âˆ—
ğ‘–âˆ’ğ‘¦ğ‘–ğ›¼ğ‘–>ğ‘€ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–andğ‘–âˆˆN, then
ğ‘£ğ‘–ğ›¼âˆ—
ğ‘–(ğ‘„(ğ‘¦))âˆ’ğ‘£ğ‘–ğ›¼ğ‘–(ğ‘„(ğ‘¦))>ğ‘ for allğ›¼ğ‘–â‰ ğ›¼âˆ—
ğ‘–âˆˆAğ‘–,andğ‘–âˆˆN. (D.20)
For notational convenience, we focus on player ğ‘–and drop the player-specific indices altogether. Let
ğ›¼â‰ ğ›¼âˆ—âˆˆA, and define for ğ‘›âˆˆâ„•the quantities ğ‘¤ğ›¼,ğ‘›andğ‘§ğ›¼,ğ‘›as
ğ‘¤ğ›¼,ğ‘›:=âŸ¨ğ‘ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©, ğ‘§ğ›¼,ğ‘›:=âŸ¨ğ‘¦ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ© (D.21)
whereğ‘’ğ›¼,ğ‘’âˆ—
ğ›¼are the standard basis vectors corresponding to ğ›¼,ğ›¼âˆ—âˆˆA. For notational convenience,
we focus on player ğ‘–and drop the player-specific indices altogether. Now, decomposing the IWE Ë†ğ‘£ğ‘›,
we obtain
Ë†ğ‘£ğ‘›=ğ‘£(ğ‘¥ğ‘›)+ğ‘ˆğ‘›+ğ‘ğ‘› (D.22)
whereğ‘ˆğ‘›:=Ë†ğ‘£ğ‘›âˆ’ğ‘£ğ‘–(Ë†ğ‘¥ğ‘›)is a zero-mean noise, and ğ‘ğ‘–,ğ‘›:=ğ‘£ğ‘–(Ë†ğ‘¥ğ‘›)âˆ’ğ‘£ğ‘–(ğ‘¥ğ‘›).
Then, unfolding the recursion, we obtain:
ğ‘¤ğ›¼,ğ‘›+1=ğ‘¤ğ›¼,ğ‘›+ğ›¾âŸ¨Ë†ğ‘£ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©
=ğ‘¤ğ›¼,ğ‘›+ğ›¾âŸ¨ğ‘£(ğ‘¥ğ‘›),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ›¾âŸ¨ğ‘ˆğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ›¾âŸ¨ğ‘ğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©
â‰¤ğ‘¤ğ›¼,ğ‘›+ğ›¾âŸ¨ğ‘£(ğ‘¥ğ‘›),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ›¾âŸ¨ğ‘ˆğ‘›,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+2ğ›¾âˆ¥ğ‘ğ‘›âˆ¥âˆ—
23â‰¤ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘ˆğ‘˜,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+2ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âˆ¥ğ‘ğ‘˜âˆ¥âˆ—
â‰¤ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘ˆğ‘˜,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+2ğ›¾ğµğ‘›âˆ‘ï¸
ğ‘˜=1ğœ€ğ‘˜ (D.23)
where we used that âˆ¥ğ‘ğ‘›âˆ¥âˆ—=Î˜(ğœ€ğ‘›)for allğ‘›âˆˆâ„•. Now, define the process {ğ‘€ğ‘›}ğ‘›âˆˆâ„•as
ğ‘€ğ‘›:=ğ›¾ğ‘›âˆ‘ï¸
ğ‘˜=1âŸ¨ğ‘ˆğ‘˜,ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ© (D.24)
which is a martingale, since ğ”¼[ğ‘ˆğ‘›|Fğ‘›]=0. Moreover, note that
âˆ¥ğ‘ˆğ‘›âˆ¥âˆ—=âˆ¥Ë†ğ‘£ğ‘›âˆ’ğ‘£(Ë†ğ‘¥ğ‘›)âˆ¥âˆ—â‰¤âˆ¥Ë†ğ‘£ğ‘›âˆ¥âˆ—+âˆ¥ğ‘£(Ë†ğ‘¥ğ‘›)âˆ¥âˆ— (D.25)
i.e.,âˆ¥ğ‘ˆğ‘›âˆ¥âˆ—=Î˜(1/ğœ€ğ‘›). Thus, we readily obtain that ğ”¼[âˆ¥ğ‘ˆğ‘›âˆ¥ğ‘
âˆ—|Fğ‘›]â‰¤ğœğ‘
ğ‘›forğœğ‘›=Î˜(1/ğœ€ğ‘›)and all
ğ‘âˆˆ[1,âˆ]. So, by Lemma D.1 for ğ›¾ğ‘›=ğ›¾,ğœğ‘›=ğœ,ğ‘as in Theorem 2, and ğœ‡âˆˆ(0,1),ğ‘> 2whose
values will be determined next, there exists ğ´ğ‘>0such that:
ğ›¿ğ‘›:=â„™
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|>ğ‘
2(ğ›¾ğ‘›)ğœ‡
â‰¤ğ´ğ‘ğ›¾ğ‘/2+1Pğ‘›
ğ‘˜=1ğœğ‘
ğ‘˜
(ğ›¾ğ‘›)1+ğ‘(ğœ‡âˆ’1/2)
â‰¤ğ´ğ‘ğ›¾ğ‘(1âˆ’ğœ‡)Pğ‘›
ğ‘˜=1ğœğ‘
ğ‘˜
ğ‘›1+ğ‘(ğœ‡âˆ’1/2)(D.26)
Now, note that for ğœ€ğ‘›=ğœ€/ğ‘›â„“ğœ€, and sinceğœğ‘›=Î˜(1/ğœ€ğ‘›), we get that there exists ğ‘€ > 0such that
ğ‘›âˆ‘ï¸
ğ‘˜=1ğœğ‘
ğ‘˜â‰¤ğ‘€ğœ€âˆ’ğ‘ğ‘›âˆ‘ï¸
ğ‘˜=1ğ‘˜ğ‘â„“ğœ€(D.27)
withPğ‘›
ğ‘˜=1ğ‘˜ğ‘â„“ğœ€=Î˜(ğ‘›1+ğ‘â„“ğœ€). Therefore,
ğ›¿ğ‘›â‰¤ğ´â€²
ğ‘ğ›¾ğ‘(1âˆ’ğœ‡)ğœ€âˆ’ğ‘ğ‘›1+ğ‘â„“ğœ€
ğ‘›1+ğ‘(ğœ‡âˆ’1/2)
â‰¤ğ´â€²
ğ‘ğ›¾ğ‘(1âˆ’ğœ‡)ğœ€âˆ’ğ‘
ğ‘›ğ‘(ğœ‡âˆ’1/2âˆ’â„“ğœ€)(D.28)
Now, we need to guarantee that there exist ğœ‡âˆˆ(0,1),ğ‘> 2, such that
âˆâˆ‘ï¸
ğ‘›=1ğ›¿ğ‘›<âˆ (D.29)
For this, we need to ensure that ğ‘(ğœ‡âˆ’1/2âˆ’â„“ğœ€)>1, or, equivalently,
â„“ğœ€<ğœ‡âˆ’1/2âˆ’1/ğ‘ (D.30)
which we will do later. Then, we will get for ğ›¾small enough:
â„™ 
âˆ\
ğ‘›=1
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|â‰¤ğ‘
2(ğ›¾ğ‘›)ğœ‡!
=1âˆ’â„™ 
âˆ[
ğ‘›=1
sup
ğ‘˜â‰¤ğ‘›|ğ‘€ğ‘˜|>ğ‘
2(ğ›¾ğ‘›)ğœ‡!
â‰¥1âˆ’âˆâˆ‘ï¸
ğ‘›=1ğ›¿ğ‘›
â‰¥1âˆ’ğ›¿ (D.31)
Regarding the term 2ğ›¾ğµPğ‘›
ğ‘˜=1ğœ€ğ‘˜in (D.23), we have that:
2ğ›¾ğµğ‘›âˆ‘ï¸
ğ‘˜=1ğœ€ğ‘˜=2ğµğ›¾ğœ€ğ‘›âˆ‘ï¸
ğ‘˜=1ğ‘˜âˆ’â„“ğœ€â‰¤ğµâ€²ğ›¾ğœ€ğ‘›1âˆ’â„“ğœ€(D.32)
where we used thatPğ‘›
ğ‘˜=1ğ‘˜âˆ’â„“ğœ€=Î˜(ğ‘›1âˆ’â„“ğœ€). Thus, for
1âˆ’â„“ğœ€<ğœ‡ (D.33)
24we have forğœ€,ğ›¾> 0small enough:
2ğ›¾ğµğ‘›âˆ‘ï¸
ğ‘˜=1ğœ€ğ‘˜â‰¤ğµâ€²ğ›¾ğœ€ğ‘›1âˆ’â„“ğœ€â‰¤ğµâ€²ğ›¾ğœ€ğ‘›ğœ‡â‰¤ğ‘
2(ğ›¾ğ‘›)ğœ‡(D.34)
for allğ‘›âˆˆâ„•. Hence, by (D.30), (D.33) we need the following two conditions to be satisfied:
1âˆ’â„“ğœ€<ğœ‡ andâ„“ğœ€<ğœ‡âˆ’1
2âˆ’1
ğ‘(D.35)
for which we get that for â„“ğœ€âˆˆ(0,1/2), there exists always ğœ‡âˆˆ(3/4,1)andğ‘large that satisfy
(D.35). Thus, combining (D.34) and (D.31), we get by (D.23) that with probability at least 1âˆ’ğ›¿:
ğ‘¤ğ›¼,ğ‘›+1â‰¤ğ‘›âˆ‘ï¸
ğ‘˜=1ğ›¾ğ‘˜âŸ¨ğ‘£(ğ‘¥ğ‘˜),ğ‘’ğ›¼âˆ’ğ‘’âˆ—
ğ›¼âŸ©+ğ‘(ğ›¾ğ‘›)ğœ‡for allğ‘›âˆˆâ„•. (D.36)
Thus, following similar steps as in the proof Theorem 3 after (D.14) , we readily obtain that with
probability at least 1âˆ’ğ›¿, we have:
ğ‘§ğ›¼,ğ‘‡â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾ğ‘‡âˆ’1âˆ‘ï¸
ğ‘˜=1(ğ›¾ğ‘˜âˆ’(ğ›¾ğ‘˜)ğœ‡)
â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+ğ‘ğ›¾1+ğœ‡âˆ«ğ‘‡
0ğ‘¡ğœ‡ğ‘‘ğ‘¡ (D.37)
â‰¤ğ‘§ğ›¼,1âˆ’ğ‘ğ›¾2ğ‘‡(ğ‘‡âˆ’1)
2+ğ‘ğ›¾1+ğœ‡ğ‘‡ğœ‡+1
ğœ‡+1
(D.38)
for allğ‘‡âˆˆâ„•. Settingğœ‡=4/5and invoking Lemma A.1 for ğœƒ(ğ‘¥)=ğ‘¥logğ‘¥, our claim follows. â– 
E Numerical experiments
In this section, we provide numerical simulations to validate and explore the performance of (FTXL) .
To this end, we consider two game paradigms, (i) a zero-sum game, and (ii) a congestion game.
Zero-sum Game. First, we consider a 2-player zero-sum game with actions {ğ›¼1,ğ›¼2,ğ›¼3}and
{ğ›½1,ğ›½2,ğ›½3}, and payoff matrix
ğ‘ƒ= (2,âˆ’2) (1,âˆ’1) (2,âˆ’2)
(âˆ’2,2) (âˆ’ 1,1) (âˆ’ 2,2)
(âˆ’2,2) (âˆ’ 1,1) (âˆ’ 2,2)!
Here, the rows of ğ‘ƒcorrespond to the actions of player ğ´and the columns to the actions of player
ğµ, while the first item of each entry of ğ‘ƒcorresponds to the payoff of ğ´, and the second one to the
payoff ofğµ. Clearly, the action profile (ğ›¼1,ğ›½2)is a strict Nash equilibrium.
Congestion Game. As a second example, we consider a congestion game with ğ‘=100and2
roads,ğ‘Ÿ1andğ‘Ÿ2, with costsğ‘1=1.1andğ‘2=ğ‘‘/ğ‘whereğ‘‘is the number of drivers on ğ‘Ÿ2. In words,
ğ‘Ÿ1has a fixed delay equal to 1.1, whileğ‘Ÿ2has a delay proportional to the drivers using it. Note, that
the strategy profile where all players are using ğ‘Ÿ2is a strict Nash equilibrium.
In Fig. 1, we assess the convergence of (FTXL) with logit best responses, under realization-based
and bandit feedback, and compare it to the standard (EW) with the same level of information. For
each feedback mode, we conducted 100separate trials, each with ğ‘‡=103steps, and calculated
the average normâˆ¥ğ‘¥ğ‘›âˆ’ğ‘¥âˆ—âˆ¥1as a function of the iteration counter ğ‘›=1,2,...,ğ‘‡ . The solid lines
represent the average distance from equilibrium for each method, while the shaded areas enclose the
range ofÂ±1standard deviation from the mean across the different trials. All the plots are displayed in
logarithmic scale. For the zero-sum game, all runs were initialized with ğ‘¦1=0, and we used constant
step-sizeğ›¾=10âˆ’2, and exploration parameter ğœ€=10âˆ’1, where applicable. For the congestion game,
the initial state ğ‘¦1for each run was drawn uniformly at random in [âˆ’1,1]2, and we used constant
step-sizeğ›¾=10âˆ’2, and exploration parameter ğœ€ğ‘›=1/ğ‘›1/4, where applicable.
The experiments have been implemented using Python 3.11.5 on a M1 MacBook Air with 16GB of
RAM.
25F Connection with other acceleration mechanisms
In this appendix, we discuss the connection between (FTXL) and the â€œlinear couplingâ€ method of
Allen-Zhu & Orecchia [1]. Because [ 1] is not taking a momentum-based approach, it is difficult
to accurately translate the coupling approach of [ 1] to our setting and provide a direct comparison
between the two methods. One of the main reasons for this is that [ 1] is essentially using two
step-sizes: the first is taken equal to the inverse Lipschitz modulus of the function being minimized
and is used to take a gradient step; the second step-size sequence is much more aggressive, and it is
used to generate an ancillary, exploration sequence which â€œscouts aheadâ€. These two sequences are
then â€œcoupledâ€ with a mixing coefficient which plays a role â€œsimilarâ€ â€“ but not equivalent â€“ to the
friction coefficient in the (HBVF) formulation of (NAG) by Su et al. [40].
The above is the best high-level description and analogy we can make between the coupling approach
of [1] and the momentum-driven analysis of Su et al. [40] and/or momentum analysis in Nesterovâ€™s
2004 textbook. At a low level (and omitting certain technical details and distinctions that are not
central to this discussion), the linear coupling approach of [ 1] applied to our setting would correspond
to the update scheme:
ğ‘¥ğ‘›=ğ‘„(ğ‘¦ğ‘›)
ğ‘¤ğ‘›=ğœ†ğ‘›ğ‘§ğ‘›+(1âˆ’ğœ†ğ‘›)ğ‘¥ğ‘›
ğ‘¦ğ‘›+1=ğ‘¦ğ‘›+(1âˆ’ğœ†ğ‘›)ğœ‚ğ‘›Ë†ğ‘£ğ‘›
ğ‘§ğ‘›+1=ğœ†ğ‘›ğ‘§ğ‘›+(1âˆ’ğœ†ğ‘›)ğ‘¥ğ‘›+1
with Ë†ğ‘£ğ‘›obtained by querying a first-order oracle at ğ‘¤ğ‘›- that is, Ë†ğ‘£ğ‘›is an estimate, possibly imperfect,
ofğ‘£(ğ‘¤ğ‘›). The first and third lines of this update scheme are similar to the corresponding update
structure of (FTXL) . However, whereas (FTXL) builds momentum by the aggregation of gradient
information via the momentum variables ğ‘ğ‘›, the linear coupling method above achieves acceleration
through the coupling of the sequences ğ‘¤ğ‘›,ğ‘§ğ‘›andğ‘¥ğ‘›, and by taking an increasing step-size sequence
ğœ‚ğ‘›that grows roughly as Î˜(ğ‘›), and a mixing coefficient ğœ†ğ‘›that evolves as ğœ†ğ‘›=1âˆ’1/(ğ¿ğœ‚ğ‘›), where
ğ¿is the Lipschitz modulus of ğ‘£(Â·). Beyond this comparison, we cannot provide a term-by-term
correspondence between the momentum-based and coupling-based approaches, because the two
methods are not equivalent (even though they give the same value convergence rates in convex
minimization problems). In particular, we do not see a way of linking the parameters ğœ‚ğ‘›andğœ†ğ‘›of
the coupling approach to the friction and step-size parameters of the momentum approach.
In the context of convex minimization problems, the coupling-based approach of [ 1] is more amenable
to a regret-based analysis â€“ this is the â€œunificationâ€ aspect of [ 1] â€“ while the momentum-based
approach of Su et al. [40] facilitates a Lyapunov-based analysis. From a game-theoretic standpoint,
the momentum-based approach seems to be more fruitful and easier to implement, but studying the
linear coupling approach of [1] could also be very relevant.
26NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paperâ€™s
contributions and scope?
Answer: [Yes]
Justification: It can be found in Section 3, Section 4 and the appendix.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims made in the
paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: It can be found in Section 1.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that the
paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these
assumptions might be violated in practice and what the implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to
provide closed captions for online lectures because it fails to handle technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
arenâ€™t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms that
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?
Answer: [Yes]
Justification: It can be found in Section 2, Section 3, Section 4 and the appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
27â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if they appear
in the supplemental material, the authors are encouraged to provide a short proof sketch to
provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experi-
mental results of the paper to the extent that it affects the main claims and/or conclusions of the
paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: It can be found in Appendix E, and the code is included in the supplemental material.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data
are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might
suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary
to either make it possible for others to replicate the model with the same dataset, or provide
access to the model. In general. releasing code and data is often one good way to accomplish
this, but reproducibility can also be provided via detailed instructions for how to replicate the
results, access to a hosted model (e.g., in the case of a large language model), releasing of a
model checkpoint, or other means that are appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should either
be a way to access this model for reproducing the results or a way to reproduce the model
(e.g., with an open-source dataset or instructions for how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [Yes]
Justification: The code is included in the supplemental material.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be possible,
so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
28â€¢The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
â€¢Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justification: It can be found in Appendix E.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
â€¢ The full details can be provided either with the code, in appendix, or as supplemental material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: No statistical significance applicable.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main claims
of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should preferably
report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of
errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experi-
ments?
Answer: [Yes]
Justification: It can be found in Appendix E.
Guidelines:
29â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didnâ€™t make it
into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The paper conforms with the NeurIPS Code of Ethics.
Guidelines:
â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied to par-
ticular applications, let alone deployments. However, if there is a direct path to any negative
applications, the authors should point it out. For example, it is legitimate to point out that
an improvement in the quality of generative models could be used to generate deepfakes for
disinformation. On the other hand, it is not needed to point out that a generic algorithm for
optimizing neural networks could enable people to train models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for
monitoring misuse, mechanisms to monitor how a system learns from feedback over time,
improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators,
or scraped datasets)?
Answer: [NA]
Justification: There are no such risks.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
30â€¢Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere
to usage guidelines or restrictions to access the model or implementing safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the
paper, properly credited and are the license and terms of use explicitly mentioned and properly
respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢ The authors should state which version of the asset is used and, if possible, include a URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to the assetâ€™s
creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
â€¢The paper should discuss whether and how consent was obtained from people whose asset is
used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either create
an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well as
details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribution of
the paper involves human subjects, then as much detail as possible should be included in the
main paper.
31â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub-
jects
Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution) were
obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly
state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review.
32