Optimizing over Multiple Distributions under
Generalized Quasar-Convexity Condition
Shihong Ding1
dingshihong@stu.pku.edu.cnLong Yang1
YANGLONG001@pku.edu.cn
Luo Luo2,4
luoluo@fudan.edu.cnCong Fang1,3‚Ä†
fangcong@pku.edu.cn
1State Key Lab of General AI, School of Intelligence Science and Technology, Peking University
2School of Data Science, Fudan University
3Institute for Artificial Intelligence, Peking University
4Shanghai Key Laboratory for Contemporary Applied Mathematics
Abstract
We study a typical optimization model where the optimization variable is com-
posed of multiple probability distributions. Though the model appears frequently in
practice, such as for policy problems, it lacks specific analysis in the general setting.
For this optimization problem, we propose a new structural condition/landscape
description named generalized quasar-convexity (GQC) beyond the realms of con-
vexity. In contrast to original quasar-convexity [ 24], GQC allows an individual
quasar-convex parameter Œ≥ifor each variable block iand the smaller of Œ≥iimplies
less block-convexity. To minimize the objective function, we consider a generalized
oracle termed as the internal function that includes the standard gradient oracle as
a special case. We provide optimistic mirror descent (OMD) for multiple distri-
butions and prove that the algorithm can achieve an adaptive ÀúO((Pd
i=11/Œ≥i)Œµ‚àí1)
iteration complexity to find an Œµ-suboptimal global solution without pre-known
the exact values of Œ≥iwhen the objective admits ‚Äúpolynomial-like‚Äù structural.
Notably, it achieves iteration complexity that does not explicitly depend on the
number of distributions and strictly faster (Pd
i=11/Œ≥iv.s.dmax i‚àà[1:d]1/Œ≥i)than
mirror decent methods. We also extend GQC to the minimax optimization problem
proposing the generalized quasar-convexity-concavity (GQCC) condition and a
decentralized variant of OMD with regularization. Finally, we show the appli-
cations of our algorithmic framework on discounted Markov Decision Processes
problem and Markov games, which bring new insights on the landscape analysis of
reinforcement learning.
1 Introduction
We study a common class of generic minimization problem
min
x‚ààXf(x), (1)
where the optimization variable xis composed of dprobability distributions {xi}d
i=1andXdenotes
the product space of the dprobability simplexes. Problem (1)meets widespread applications in
‚Ä†Corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).reinforcement learning optimization [ 62,2,35], multi-class classification [ 53] and model selection
type aggregation [ 29]. In this paper, we are particularly interested in the case where dis reasonably
large and we manage to obtain complexities dependent of dnon-explicitly.
When fis convex with respect to x, many efficient algorithms can be powerful tools for solving
Problem (1). One well-known algorithm is mirror descent (MD) [ 5] which is based on Bregman
divergence. The wide choices of Bregman divergence enable the algorithm to iterate and converge
under specifically constrained region [ 34]. In particular, if one applies the usual Euclidean distance,
the algorithm reduces to project gradient descent [ 37]. One common and more sophisticated selection
is the Kullback-Leibler (KL) divergence, the algorithm thereby becoming the variant of multiplicative
weights update (MWU) [41] over probability distribution.
Turning to the non-convex world, specific analysis for Problem (1)is rare. In general, finding an
approximate global solution suffers from the curse of dimensionality [ 51,46]. And one interesting
direction is to consider suitable relaxations for the desired solutions, such as an approximate local
stationary point of smooth functions [ 31,19]. However, for many cases, local solutions may not
be sufficient. Moreover, the algorithms often converge much faster in practice than the theoretic
lower bounds in non-convex optimization suggest. This observed discrepancy can be attributed
to the fairly weak assumptions underpinning these generic bounds. For example, many generic
non-convex optimization theories, e.g. Carmon et al. [7,8]only focus on the consideration of
Lipschitz continuity of the gradient and some higher-order derivatives. In practice, the objective is
often more ‚Äústructured‚Äù. For example, the recent progress in neural networks shows that systems of
neural networks approximate convex kernel systems when the model is overparameterized [ 28]. As
pointed out by Hinder et al. [24], much more research is needed to characterize structured sets of
functions for which minimizers can be efficiently found; It was also noted by Yurii Nesterov [ 47] that
lots of functions are essentially convex; Our work follows this research line.
We propose generalized quasar-convexity (GQC) for the class of ‚Äústructure‚Äù. The original quasar-
convex functions [ 22] is parameterized by a constant Œ≥‚àà(0,1]and requires f(x)‚àíf(x‚àó)‚â§
1
Œ≥‚ü®‚àáf(x),x‚àíx‚àó‚ü©. These functions are unimodal on all lines that pass through a global minimizer
and so all critical points are minimizers. We extend quasar-convexity by introducing individual
quasar-convex parameter Œ≥ifor each distribution xi. Therefore GQC is parameterized by dconstants
{Œ≥i}d
i=1and implies quasar-convexity in the case d= 1. The main intuition of the generalization is
the observation that d/mini‚àà[1:d]Œ≥ioften depends on the number of distributions din real problems,
whereas,Pd
i=11/Œ≥imay not. That is to say, the hardness for distribution idiverges according to the
magnitude of Œ≥i. The larger of Œ≥iimplies more convexity and the simpler to solve xi. In general,
one always havePd
i=11/Œ≥i‚â§dmax i‚àà[1:d]1/Œ≥i. In the worst case,Pd
i=11/Œ≥ican be dtimes
smaller than dmax i‚àà[1:d]1/Œ≥i(see discussions in Section 3.3), which motivates us to study the GQC
condition.
We then study designing efficient algorithms to solve (1). One simple case is when {Œ≥i}m
i=1is
pre-known by the algorithms. The possible direction is to impose a Œ≥i-dependent update rule, such as
by non-uniform sampling. However, in general cases, {Œ≥i}m
i=1is not known and determining {Œ≥i}m
i=1
require non-negligible costs.
In this paper, we consider a generalized oracle, which we refer to as the internal function. Here
the standard gradient oracle can be viewed as a special case of the internal function. We pro-
vide the optimistic mirror descent algorithm for multiple distributions, which makes sure that
each probability distribution is updated according to its own internal function. We first establish
anO((dŒ≥max)1/2(Pd
i=1Œ≥‚àí1
i)3/2LŒµ‚àí1log(N))complexity with N= max i‚àà[1:d]niandŒ≥max=
max i‚àà[1:d]Œ≥iwhen Œ≥max<‚àû. However, such an complexity depends on dŒ≥maxand requires the step
size rely on pre-known Œ≥maxPd
i=1Œ≥‚àí1
i. We then consider fsatisfies ‚Äúpolynomial-like‚Äù structural (see
Assumption 3.3). We show the assumption can be achieved in a variety of function classes and impor-
tant machine learning problems. Under the assumption, we show the algorithm can adapt to the values
of{Œ≥i}m
i=1and guarantees an reduced iteration complexity O((Pd
i=11/Œ≥i)Œµ‚àí1log(N) log4.5(Œµ‚àí1)).
In the following, the eO(¬∑)notation hides factors that are polynomial in log(Œµ‚àí1)andlog(N).
We also extend our framework to the minimax optimization
min
x‚ààXmax
y‚ààYf(x,y), (2)
2Solution
typeRelated
workIteration
complexitySingle
loop
Œµ-approximate NECen et al. [9]
Chen et al. [12]eO
1
(1‚àíŒ∏)2Œµ
‚úó
Wei et al. [67] eO
|S|3
(1‚àíŒ∏)8Œµ2
‚úì
Cen et al. [10] eO
|S|
(1‚àíŒ∏)4Œµ
‚úì
This Work eO
1
(1‚àíŒ∏)2.5Œµ
‚úì
Table 1: Comparison of policy optimization methods for finding an Œµ-approximate NE of infinite
horizon two-player zero-sum Markov games in terms of the max-min gap (see Eq. (4)). Since the
iteration complexity of several research works (such as Zhao et al. [75], Alacaoglu et al. [3]and Zeng
et al. [72]) involve concentrability coefficient and initial distribution mismatch coefficient, we will
not delve into them here.
where both xandyare composed of dprobability distributions, and Z=X √ó Y is a joint region. In
the general non-convex and non-concave setting, it is known that finding even an approximated local
solution for (2)is computationally intractable [ 16]. We introduce the generalized quasar-convexity-
concavity (GQCC) condition analogous to GQC and demonstrate the feasibility of obtaining an
Œµ-approximate Nash equilibrium with O((1‚àíŒ∏)‚àí2.5maxz‚ààZ(Pd
i=1œài(z))Œµ‚àí1log(M) log( Œµ‚àí1))
iteration complexities, where maxz‚ààZ(Pd
i=1œài(z))is analogous to (Pd
i=11/Œ≥i)withœài(z)defined
in the GQCC condition; Œ∏is the discount parameter; M= max i‚àà[1:d]{mi+ni}. Intuitively, the
GQCC condition can be viewed as the generalization of convexity-concavity condition. Similarly, the
eO(¬∑)notation hides factors that are polynomial in log(Œµ‚àí1)andlog(M).
Finally, we demonstrate the applications of our framework. For problem (1), we consider both infinite
horizon discounted and finite horizon MDPs problem. For problem (2), we study the infinite horizon
two-player zero-sum Markov games. We prove the learning objectives admit the GQC and GQCC
conditions, respectively. This provides new landscape description for RL problems, thereby bringing
new insights. Accordingly, our algorithms achieve state-of-the-art iteration complexities up to loga-
rithmic factors. We provide eO(Œµ‚àí1)iteration bound for finding an Œµ-approximate Nash equilibrium
of infinite horizon two-player zero-sum Markov games, which outperforms the eO(|S|3Œµ‚àí2)bound of
Wei et al. [67] and the eO(|S|Œµ‚àí1)bound of Cen et al. [10] by factors of |S|3Œµ‚àí1and|S|, respectively,
up to a logarithmic factor.
1.1 Contribution
(A)We introduce new structural conditions GQC for minimization problems and GQCC for
minimax problems over multiple distributions.
(B)We provide adaptive algorithm that achieves eO((Pd
i=11/Œ≥i)Œµ‚àí1)iteration complexities to
find an Œµ-suboptimal global minimum of ‚Äúpolynomial-like‚Äù function under GQC. We also
provide an implementable minimax algorithm, given a generalized quasar-convex-concave
function with proper conditions, uses eO((1‚àíŒ∏)‚àí2.5maxz‚ààZ(Pd
i=1œài(z))Œµ‚àí1)iterations
to find an Œµ-approximate Nash equilibrium.
(C)We show that discounted MDP and infinite horizon two-player zero-sum Markov games
admit the GQC and GQCC conditions, respectively, and also satisfy our mild assumptions. In
addition, we provide eO((1‚àíŒ∏)‚àí2.5Œµ‚àí1)iteration bound for finding an Œµ-approximate Nash
equilibrium of infinite horizon two-player zero-sum Markov games. Detailed comparisons
between our method and prior arts are provided in Table 1.
1.2 Related Works
Minimization: Convexity condition has been studied at length and plays a critical role in optimizing
minimization problems [ 59,44,25,60,6,49]. Several other ‚Äúconvexity-like‚Äù conditions have
3attracted considerable attention, which provide opportunity for designing algorithmic framework to
achieve global convergence. Star-convexity [ 47] is a typical example that relaxes convexity, showing
potential in machine learning recently [ 32,76]. Quasi-convexity, which admits that the highest
point along any line segment is one of the endpoints, is also an important condition [ 6]. Following
this, the concept of weak quasi-convexity is proposed by Hardt et al. [22] which is an extension of
star-convexity in the differentiable case, and Hinder et al. [24] provides lower bound for the number
of gradient evaluations to find an Œµ-minimizer of a quasar-convex function (a linguistically clearer
redefinition of weak quasi-convex function claimed by Hinder et al. [24] ).
Minimax Optimization: Minimax problem attracted considerable attention in machine learning.
There exist a variety of algorithms to find the approximate Nash equilibrium points [ 63,43,48,
45,40,33,55,66,27] or stationary points [ 71] for convex-concave functions. Without convex-
concave assumption, there exist related work considered specific structures in objective, including
nonconvex-(strongly-)concave assumption [ 39,73,50], Kurdyka‚ÄìLojasiewicz condition (or specific
PL condition) [ 68,11,69,38], interaction dominant condition [ 21] and negative comonotonicity
[17, 36].
RL Landscape Descriptions: For the policy gradient based model of infinite horizon reinforcement
learning problems, Agarwal et al. [2]provides a convergence proof for the natural policy gradient
descent, which is the same as the mirror descent-modified policy iteration algorithm [ 20] with
negative entropy as the Bregman divergence. Subsequently, Lan [35] focuses on exploring the
structural properties of infinite horizon reinforcement learning problems with convex regularizers.
For two-player zero-sum Markov games [ 61,42] under full information setting, there are various
algorithms [ 26,54,64,18,42,67,9,74,70] have been proposed. Specifically, Cen et al. [9]focus on
finding approximate minimax soft Q-function in regularized infinite horizon setting; Zhao et al. [74]
focus on finding one-sided approximate Nash equilibrium in standard infinite horizon setting with
ÀúO(Œµ‚àí1)iteration bound which depends on the concentrability coefficient; Yang and Ma [70] focus on
finding approximate Nash equilibrium in standard finite horizon setting with ÀúO(Œµ‚àí1)iteration bound.
Related Works on Optimistic Mirror Descent (OMD) and Optimistic Multiplicative Weights
Update (OMWU): The connection between online learning and game theory [ 58,4,23,1] has
since led to the discovery of broad learning algorithms such as multiplicative weights update (MWU)
[41]. Rakhlin and Sridharan [57] introduces an optimistic variant of online mirror descent [ 56,14]‚Äì
optimistic mirror descent. Daskalakis et al. [15] shows that the external regret of each player achieves
near-optimal growth in multi-player general-sum games, with all players employ the optimistic
multiplicative weights update.
2 Preliminary
Notation: Letx= (x1,¬∑¬∑¬∑,xd)‚ààRPd
i=1nibe the joint vector variable, for every vector
variable xi‚ààRni. LetŒ±= (Œ±(1),¬∑¬∑¬∑,Œ±(n))be the multi-indices, where Œ±(i)‚ààZ+, we define
|Œ±|=Pn
i=1Œ±(i)andŒ±! =Œ±(1)!¬∑¬∑¬∑Œ±(n)!. For any vector u= (u(1),¬∑¬∑¬∑,u(n))‚ààRn, we
define uŒ±=u(1)Œ±(1)¬∑¬∑¬∑u(n)Œ±(n). Let f:Rn‚ÜíRbe a smooth function, we expand its Taylor
expansion with Lagrange remainder Rf
K,w(u)as follows,
Rf
K,w(u) =f(u)‚àíKX
i=0X
|Œ±|=iDŒ±f(w)
Œ±!¬∑(u‚àíw)Œ±. (3)
Given matrices QandPinR‚Ñì1√ó‚Ñì2we claim that Q‚â§Pif[Q]i,j‚àí[P]i,j‚â§0for every i, j.
For a sequence of vector-valued functions {Fi}d
i=1, we say that {Fi}d
i=1is uniformly L-Lipschitz
continuous with respect to ‚à•¬∑‚à•‚Ä≤under‚à•¬∑‚à•if‚à•Fi(xi)‚àíFi(ui)‚à•‚Ä≤‚â§L‚à•xi‚àíui‚à•for every i‚àà[1 :d]
and any x,u‚àà X. We denote by ‚à•¬∑‚à•‚àóthe dual norm of ‚à•¬∑‚à•. LetP:R‚Ñì1√ó‚Ñì2‚ÜíRn1√ón2be a matrix
function, we say that Pis aŒ∏-contraction mapping under ‚à•¬∑‚à•if‚à•P(Q1)‚àíP(Q2)‚à•‚àû‚â§Œ∏‚à•Q1‚àíQ2‚à•
for any Q1,Q2‚ààR‚Ñì1√ó‚Ñì2. For matrix-valued function P:Rn‚ÜíR‚Ñì1√ó‚Ñì2,we define DP(x,x‚Ä≤) =
P(x)‚àíP(x‚Ä≤)for any x,x‚Ä≤‚ààRn. The KL divergence KL(p‚à•q) =Pn
j=1p(j)¬∑log
p(j)
q(j)
between distributions pandqis defined on probability simplex ‚àÜn. And the variance of xoverp
is defined by Varp(x) =Pn
j=1p(j)¬∑(x(j)‚àíEj‚Ä≤‚àºp[x(j‚Ä≤)])2. We define max-min gap of function
4f:X √ó Y ‚Üí Ras follows,
Gf(x,y) := max
y‚Ä≤‚ààYf(x,y‚Ä≤)‚àímin
x‚Ä≤‚ààXf(x‚Ä≤,y). (4)
We claim that (x,y)is an Œµ-approximate Nash equilibrium ( Œµ-approximate NE) if Gf(x,y)‚â§Œµ.
When Œµ= 0,(x,y)is a Nash equilibrium.
Infinite Horizon Discounted Markov Decision Process: We consider the setting of an infinite
horizon discounted Markov decision process (MDP), denoted by M:= (S,A,P, œÉ, Œ∏,œÅ0).Sis
a finite state space; Ais a finite action space; P(s|s‚Ä≤, a‚Ä≤)denotes the probability of transitioning
from stos‚Ä≤under playing action a‚Ä≤;œÉ:S √ó A ‚Üí [0,1]is a cost function, which quantifies the
cost associated with taking action ain state s;Œ∏‚àà[0,1)is a discount factor; œÅ0is an initial state
distribution over S.
œÄ:S ‚Üí ‚àÜA(where ‚àÜAis the probability simplex over A) denotes a stochastic policy, i.e., the
agent play actions according to a‚àºœÄ(¬∑|s). We use PrœÄ
t(s‚Ä≤|s) =PrœÄ(st=s‚Ä≤|s0=s)to denote
the probability of visiting the state s‚Ä≤from the state safterttime steps according to policy œÄ. Let
trajectory œÑ={(st, at)}‚àû
t=0, where s0‚àºœÅ0, and, for all subsequent time steps t,at‚àºœÄ(¬∑|st)and
st+1‚àºP(¬∑|st, at). The value function VœÄ:S ‚ÜíRis defined as the discounted sum of future cost
starting at state sand executing œÄ, i.e.
VœÄ(s) = (1 ‚àíŒ∏)E"‚àûX
t=0Œ∏tœÉ(st, at)œÄ, s0=s#
.
Moreover, we define the action-value function QœÄ:S √ó A ‚Üí Rand the advantage function
AœÄ:S √ó A ‚Üí Ras follows:
QœÄ(s, a) = (1 ‚àíŒ∏)E"‚àûX
t=0Œ∏tœÉ(st, at)œÄ, s0=s, a0=a#
, AœÄ(s, a) =QœÄ(s, a)‚àíVœÄ(s).
It‚Äôs also useful to define the discounted state visitation distribution dœÄ
s0of a policy œÄasdœÄ
s0(s) =
(1‚àíŒ∏)P‚àû
t=0Œ∏tPrœÄ
t(s|s0). In order to simplify notation, we write dœÄ
œÅ0(s) =Es0‚àºœÅ0[dœÄ
s0(s)], where
dœÄ
œÅ0is the discounted state visitation distribution under initial distribution œÅ0.
3 Minimization Optimization
In this section, we propose the generalized quasar-convexity (GQC) condition, and analyze a related
algorithmic framework for minimization over X=Qd
i=1‚àÜni, under mild assumptions.
3.1 Generalized Quasar-Convexity (GQC)
We provide a novel depiction of function structure‚Äìgeneralized quasar-convexity, which is defined as
follows:
Definition 3.1 (Generalized Quasar-Convexity (GQC)) .Letx‚àó‚àà X ‚äÇ RPd
i=1nibe a minimizer of
the function f:X ‚ÜíR. We say that fis generalized quasar-convex on Xwith respect to x‚àóif for
allx‚àà X, there exist a sequence of vector-valued functions {Fi:X ‚ÜíRni}d
i=1and a sequence of
positive scalars {Œ≥i}d
i=1such that
f(x‚àó)‚â•f(x) +dX
i=11
Œ≥i‚ü®Fi(x),x‚àó
i‚àíxi‚ü©. (5)
If Eq. (5)holds, we say that F= (F‚ä§
1,¬∑¬∑¬∑,F‚ä§
d)‚ä§is the internal function of f. Given i‚àà[1 :d]we
say that Fiis the internal function of ffor variable block xi.
Our proposed GQC condition concerns the multi-variable generalized extension of the quasar-
convexity condition. In the case d= 1, the GQC condition degenerates into the Œ≥-quasar-convexity
condition as studied in Hinder et al. [24] with the gradient ‚àáf(x)belongs to the internal functions
off. In the case d >1, the GQC condition is instrumental in capturing the crucial characteristic of
those optimization applications with each variable block has difficulty to be optimized.
5Algorithm 1 Optimistic Mirror Descent for Multi-Distributions
Input:
g0
i=x0
i= (1/ni,¬∑¬∑¬∑,1/ni)	d
i=1,Œ∑andT.
Output: Randomly pick up t‚àà {1,¬∑¬∑¬∑, T}following the probability P[t] = 1/Tand return xt.
1:while t‚â§Tdo
2: for all i‚àà[1 :d]do
3: xt
i= argmin
xi‚àà‚àÜniŒ∑
Fi(xt‚àí1),xi
+ KL 
xi‚à•gt‚àí1
i
,
4: gt
i= argmin
gi‚àà‚àÜniŒ∑
Fi(xt),gi
+ KL 
gigt‚àí1
i
.
5: end for
6: t‚Üêt+ 1.
7:end while
3.2 Main Results
Recall that GQC condition provides a perspective to bound function error f(x)‚àíf(x‚àó)based on
internal function, which is different from that based on gradient oracle. We therefore aim to provide
an algorithmic framework for finding an approximate suboptimal global solution using internal
function. Given an objective function f:X ‚ÜíRwith internal function F, our algorithm (Algorithm
1) independently computes points gt
iandxt
ifollowing OMD over each block. If max i‚àà[1:d]Œ≥i<‚àû
and internal function Fhas Lipschitz continuity, we have following basic and primary convergence
result of Algorithm 1,
Theorem 3.2. Assuming that FisL-Lipschitz continuous with respect to ‚à• ¬∑ ‚à•‚àóunder ‚à• ¬∑ ‚à• and
Œ≥max= max i‚àà[1:d]Œ≥i<‚àû, and setting Œ∑= (L2dŒ≥maxPd
i=1Œ≥‚àí1
i)‚àí1/2/2, we have
1
TTX
t=1(f(xt)‚àíf(x‚àó))‚â§2Lmax i‚àà[1:d]log(ni) (dŒ≥max)1/2Pd
i=1Œ≥‚àí1
i3/2
T. (6)
However, the estimation provided by Theorem 3.2 depends on dŒ≥max. And the step size relying on
Œ≥maxPd
i=1Œ≥‚àí1
i
might be difficult to set when {Œ≥i}d
i=1is unknown.
We then hope to propose an alternative analytical method that can adapt to unknown {Œ≥i}d
i=1and
obtain complexity which does not depends on block dimension dexplicitly. The challenges includes:
1) The algorithm does not know the weight 1/Œ≥i; 2) every Fihas dependence on the joint variable
xinstead of depending on xi. Before we present the details of convergence analysis, we need the
following notations and assumptions:
Denote Pf
K,y(x)) =PK
i=0P
|Œ±|=i|DŒ±f(y)|
Œ±!¬∑(|x|+|y|)Œ±and let Pœï
K,y(x) = ( Pœï(1)
K,y(x),¬∑¬∑¬∑,
Pœï(‚Ñì)
K,y(x))for any vector-valued function œï:Rn‚ÜíR‚Ñì. Recalling the definition of Rf
K,win Eq. (3),
we shall also define Rœï
K,y(x) = (Rœï(1)
K,y(x),¬∑¬∑¬∑, Rœï(‚Ñì)
K,y(x)).
Assumption 3.3. LetFbe the internal function of f. There exists Œò1,Œò2>0,K0‚ààZ+, and
Œ∏‚àà[0,1), and a fixed y‚ààRPd
i=1nisuch that
[A1]RF
K,y(x)
‚àû‚â§Œò1Œ∏Kfor any integer K > K 0andx‚àà X.
[A2]PF
K,y(x)
‚àû‚â§Œò2for any integer K‚ààZ+andx‚àà X.
Assumption 3.3 is a characterization of ‚Äúpolynomial-like‚Äù functions. We clarify this view as follows.
For a standard polynomial function p, it‚Äôs clear that psatisfies Assumption 3.3, since the Taylor
expansion of pafter order K0is always equal to 0 ( [A1]in Assumption 3.3 holds) and Xis a
bounded and closed set ( [A2]in Assumption 3.3 holds). Assumption 3.3 is easy to achieve. Shown in
Proposition B.2 and Remark B.3 in Appendix B, Assumption 3.3 can be satisfied by many smooth
functions defined on bounded region X. In addition, we introduce a simple machine learning example:
learning one single neuron network over a simplex in the realizable setting.
6Example 3.4.The objective function is written as f(p,P) =1
2Ex,y(Pm
i=1piœÉ(x‚ä§Pi)‚àíy)2, where
p‚àà‚àÜmandP= (P1,¬∑¬∑¬∑,Pm)‚ààQm
i=1‚àÜdand the target ygiven x‚àà[‚àíC, C]dadmits y=
œÉ(x‚ä§P‚àó
1)for some P‚àó
1‚àà‚àÜd. For activation function œÉ(x) = exp {x},fsatisfies GQC condition
and Assumption 3.3 with the internal functions Fp={E[(Pm
j=1pjœÉ(x‚ä§Pj)‚àíy)œÉ(x‚ä§Pi)]}m
i=1
for block pandFPi=E[(œÉ(x‚ä§Pi)‚àíy)x]for block Pi.
Note previous work [ 65] studies single neuron learning by considering P‚àó
1in the sphere and assuming
xfollows from a Gaussian distribution. To our knowledge, there is no evidence shows that objective
function of Example 3.4 has quasar-convexity. This example demonstrates the advantage of studying
the GQC framework over the previous approach. The proof of Example 3.4 is in Section B.2.
Parameter Setting Before stating the convergence result, we set the parameters as follows:
Œò = Œò 1+ Œò 2+ 1, H =‚åàlog(T)‚åâ, Œ≤ 0= (4H)‚àí1, Œ≤ = min(p
Œ≤0/8
H3,1
2Œò(H+ 3))
,
Œì =e2+O(Œò2),ÀÜK= maxHlog(4Œ≤‚àí1) + log(Œò 1)
log(Œ∏‚àí1), K0
, Œ∑ = min(
Œ≤
6e3ÀÜKŒì max{Œò,1},Œ≤4
0
O(Œò))
.
(7)
Theorem 3.5. Letfsatisfies the GQC condition and denote N= max i‚àà[1:d]{ni}. Under Assumption
3.3, the following estimation holds for Algorithm 1‚Äôs output {xt}T
t=1
1
TTX
t=1(f(xt)‚àíf(x‚àó))‚â§ dX
i=11/Œ≥i!1
Œ∑log(N) +Œ∑Œò3(6 + 330240Œò H5)
T‚àí1, (8)
Theorem 3.5 implies that for any generalized quasar-convex function fsatisfies Assumption 3.3,
theT-step random solution outputted by Algorithm 1 is a O((Pd
i=11/Œ≥i)T‚àí1log(N) log4.5(T))-
suboptimal solution. Ignoring the logarithmic factor, the iteration complexity of our algorithm
is competitive to the state-of-the-art algorithm when applied to specific application (i.e. policy
optimization of reinforcement learning [ 2]). Moreover, our algorithm makes iteration complexity
depend onPd
i=11/Œ≥ilinearly. In some common applications,Pd
i=11/Œ≥ihas no dependence on d,
which is the number of variable blocks (see discussions in Section 3.3).
3.3 Application to Reinforcement Learning
This section reveals that GQC condition provides a novel analytical approach to reinforcement
learning. We show how to leverage Algorithm 1 to find Œµ-suboptimal global solution for infinite
horizon reinforcement learning problem. And in Appendix B.3.2, we show how to leverage Algorithm
1 to minimize finite horizon reinforcement learning problem.
The infinite horizon reinforcement learning is formulated as the following policy optimization
problem:
min
œÄ‚ààXJœÄ(œÅ0), (9)
where JœÄ(œÅ0) =Es0‚àºœÅ0[VœÄ(s0)]andX=Q|S|
i=1‚àÜAdenotes |S|probability simplexes. We write
S={si}|S|
i=1and denote the action-value vector on state sibyQœÄ(si,¬∑). The next Proposition 3.6
states that JœÄ(œÅ0)satisfies the GQC condition for any initial state distribution œÅ0.
Proposition 3.6. Let{œÄ‚àó(¬∑|s)‚àà‚àÜA}s‚ààSdenote the optimal global solution of problem (9). We
have that JœÄ(œÅ0)satisfies the GQC condition in Eq. (5)with internal function Fi(œÄ) =QœÄ(si,¬∑)
for variable block œÄiandFsatisfies Assumption 3.3 with Œò1=Œ∏,Œò2= 1andK0= 1.
According to Theorem 3.5, if we apply Algorithm 1 to the infinite horizon reinforcement learning
basing action-value vector QœÄwith parameter selection Eq. (7), which is actually a simple variant of
natural policy gradient descent [ 2], then the iteations Twe need to find an Œµ-suboptimal global solution
is upper-bounded by O(max{1,log‚àí1(Œ∏‚àí1)}(1‚àíŒ∏)‚àí1Œµ‚àí1log4.5(Œµ‚àí1) log(|A|))under Agarwal et al.
[2]‚Äôs setting. Therefore, the iteration complexity of Algorithm 1 does not depend on the size of states,
since the summation of dœÄ‚àó
œÅ0overS(P|S|
i=11/Œ≥i=P|S|
i=1dœÄ‚àó
œÅ0(si) = 1) mollifies the accumulation
7of the maximum of dœÄ‚àó
œÅ0overSwith|S|times. Specifically, if we take into account the loosest upper
bound |S|max i‚àà[1:|S|]dœÄ‚àó
œÅ0(si), then the iteration complexity of algorithm may suffer from the linear
dependence on |S|, since max i‚àà[1:|S|]dœÄ‚àó
œÅ0(si)‚â•(1‚àíŒ∏) max i‚àà[1:|S|]œÅ0(si). Previous research
[2, Theorem 5.3] has demonstrated that utilizing the information of joint variables to separately
update each variable block ensures global convergence for problem (9)withO((1‚àíŒ∏)‚àí2Œµ‚àí1)
iteration complexity. However, their analytical approach is carefully designed for infinite horizon
reinforcement learning problems.
4 Minimax Optimization
In this section, we introduce the generalized quasar-convexity-concavity (GQCC) condition, which
can be verified in real applications such as two-player zero-sum Markov games. We provide a
related algorithm for minimax optimization (minimizing Gf(x,y)has been defined in Eq. (4))
overZ=Qd
i=1Zi=Qd
i=1(‚àÜni√ó‚àÜmi), under proper assumptions. We specify the divergence-
generating function vasv(x) =Ei‚àºx(¬∑)[log(x(i))]in probability simplexes setting. We also provide
a framework for minimax problem over the general compact convex regions in Appendix C.
4.1 Generalized Quasar-Convexity-Concavity (GQCC)
We provide a new notion called generalized quasar-convexity-concavity for nonconvex-nonconcave
minimax optimization, which is defined as follows:
Definition 4.1 (Generalized Quasar-Convexity-Concavity (GQCC)) .Denote Zi=Xi√ó Yifor any
i‚àà[1 :d], and let f:Z ‚ÜíRbe the objective function. We say that fis generalized quasar-convex-
concave on Zif for all z= (x,y)‚àà Z, there exist a sequence of functions {fi:R‚Ñì√ód√ó Zi‚Üí
R}d
i=1, a sequence of non-negative functions {œài:Z ‚ÜíR+‚à™0}d
i=1and a matrix-valued function
P= (P1,¬∑¬∑¬∑,Pd) :Z ‚ÜíR‚Ñì√ódwhere every Piis a‚Ñì-dimensional vector-valued function, such
that
Gf(x,y)‚â§dX
i=1œài(z)Gfi(P(z),¬∑,¬∑)(xi,yi), (10)
where each fi(Q,¬∑,¬∑)is convex-concave for a fixed Q= (Q1,¬∑¬∑¬∑,Qd)‚ààR‚Ñì√ód. We denote the in-
ternal operator of ffor variable block zibyFiwhere Fi(Q,zi) = ((‚àáxifi(Q,zi))‚ä§,(‚àí‚àáyifi(Q,
zi))‚ä§)‚ä§. Moreover, we say that F= (F‚ä§
1,¬∑¬∑¬∑,F‚ä§
d)‚ä§is the internal operator of f.
The GQCC condition is an extension of the GQC condition in minimax optimization setting. The
specific connection between them can be found in Appendix C. The GQCC condition can be viewed
as an extension of the convexity-concavity condition in multi-variable optimization; it seamlessly
reduces to the convexity-concavity condition with f1(P(z),z) =f(z)andœà1(z)‚â°1, in the case
d= 1. Assuming every œàiis bounded, fi(P(z),zi)‚â°fi(0,zi)with Lipschitz continuous gradient
and is convex-concave with respect to zi, then finding the Nash equilibrium point of fis reduced to
finding the Nash equilibrium points of dindependent convex-concave minimax problems. However,
how to find the approximate Nash equilibrium points in more general case has not been well-studied.
Most of existing work for minimax optimization without convex-concave assumption are focused on
finding the approximate stationary points.
4.2 Main Results
For simplicity, we denote by Fx
iandFy
ithe projection of Fiin thexiandyidirections, respectively,
i.e.,F‚ä§
i= 
(Fx
i)‚ä§,(Fy
i)‚ä§
. Given an objective function f:Z ‚ÜíRwith internal operator F, our
algorithm (Algorithm 2) employs regularized OMD over each distribution independently basing on
Fiand updates matrix Qtto track the behavior of function Piteratively. It‚Äôs worth noting that each
iteration of Algorithm 2 provides explicit expressions for xt
iandgt
i(see the proof of Theorem 3.5 in
Appendix B). Consequently, Algorithm 2 essentially operates as a single-loop algorithm.
Assumption 4.2. In Definition 4.1, we assume that matrix-valued function Phas the form of
P(Qz,z)where Qz‚ààR‚Ñì√óddepends on z, and Psatisfies the following properties on region
Q‚ààR‚Ñì√ód‚à•Q‚à•‚àû‚â§C} √ó Z for some constant C >0:
8Algorithm 2 Optimistic Mirror Descent with Regularization for Multiple Distributions
Input:
z0
i	d
i=1=
g0
i	d
i=1={(1/ni,¬∑¬∑¬∑,1/ni),(1/mi,¬∑¬∑¬∑,1/mi)}d
i=1,{Œ±t‚â•0}T
t=1withPT
t=1Œ±t= 1,
{Œ≥t‚â•0}T
t=1,{Œªt‚â•0}T
t=1,Œ∑andQ0=0.
Output: ¬ØzT=PT
t=1Œ±tzt.
1:while t‚â§Tdo
2:Qt= (1‚àíŒ≤t‚àí1)Qt‚àí1+Œ≤t‚àí1P(Qt‚àí1,zt‚àí1).
3: for all i‚àà[1 :d]do
4: xt
i= argmin
xi‚ààXiŒ∑
Fx
i(Qt‚àí1,zt‚àí1
i),xi
+Œ≥tKL 
xi(gx
i)t‚àí1
+Œªtv(xi),
5: yt
i= argmin
yi‚ààYiŒ∑
Fy
i(Qt‚àí1,zt‚àí1
i),yi
+Œ≥tKL 
yi(gy
i)t‚àí1
+Œªtv(yi),
6: (gx
i)t= argmin
gx
i‚ààXiŒ∑
Fx
i(Qt,zt
i),gx
i
+Œ≥tKL 
gx
i(gx
i)t‚àí1
+Œªtv(gx
i),
7: (gy
i)t= argmin
gy
i‚ààYiŒ∑
Fy
i(Qt,zt
i),gy
i
+Œ≥tKL 
gy
i(gy
i)t‚àí1
+Œªtv(gy
i).
8: end for
9: t‚Üêt+ 1.
10:end while
[A1]There exist constants L1, L2‚â•0such that Fi(¬∑,zi)is uniformly L1-Lipschitz continuous
with respect to ‚à•¬∑‚à•‚àûunder‚à•¬∑‚à•‚àû, andFi(Q,¬∑)is uniformly L2-Lipschitz continuous with
respect to ‚à• ¬∑ ‚à•‚àûunder‚à• ¬∑ ‚à• 1.
[A2]There are a positive constant Œ≥ > 0and a set of non-negative constant matrices
{Bi,Ci}d
i=1satisfyingPd
i=1(Bi+Ci)
‚àû‚â§Œ≥, such that DP(Q,¬∑,y)(x,x‚Ä≤)‚â§Pd
i=1Ci‚ü®Fx
i(Q,zi),xi‚àíx‚Ä≤
i‚ü©andDP(Q,x,¬∑)(y,y‚Ä≤)‚â•Pd
i=1Bi‚ü®Fy
i(Q,zi),y‚Ä≤
i‚àíyi‚ü©.
[A3]There exists Œ∏‚àà[0,1)such that P(¬∑,z)is aŒ∏-contraction mapping under ‚à• ¬∑ ‚à•‚àû, and
‚à•P(Q,z)‚à•‚àû‚â§Cfor any z‚àà Z.
We present Lemma 4.3 to demonstrate that there exist Q‚àó‚ààR‚Ñì√ód,x‚àó‚àà X andy‚àó‚àà Y satisfy the
saddle point and fixed point conditions of function P, i.e., Eq. (11), under proper assumptions.
Lemma 4.3. Assuming that Assumption 4.2 holds, [P(Q,¬∑,¬∑)]k,jis continuous, convex with respect
tox, concave with respect to yfor any (k, j), and mink,j,imin{[Ci]k,j,[Bi]k,j}
[Ci]k,j+[Bi]k,j‚â•C‚Ä≤for some C‚Ä≤>0,
then there exist Q‚àó‚ààR‚Ñì√ódandz‚àó‚àà Z such that
Q‚àó=P(Q‚àó,x‚àó,y‚àó),Q‚àó‚â§P(Q‚àó,x,y‚àó),and Q‚àó‚â•P(Q‚àó,x‚àó,y). (11)
For Algorithm 2, we let Œ≤T,t=Œ≤tQT
j=t+1(1‚àíŒ≤j)for any T‚â•tandŒ≤T,T=Œ≤T, and set parameters
c= 2(1‚àíŒ∏)‚àí1, Œ∑‚â§(1‚àíŒ∏)1/2
16L2((Œ≥L1)1/2+ 1), Œ≤t=c
c+t, Œ±t=Œ≤T,t, Œ≥t=Œ±t‚àí1
Œ±t, Œªt= 1‚àíŒ≥t. (12)
Then we have the following convergence result by denoting M= max i‚àà[1:d]{mi+ni}.
Theorem 4.4. For any generalized quasar-convex-concave function fwhich satisfies Assumption 4.2
withP‚â°Q‚àó, where Q‚àósatisfies Eq. (11). Algorithm 2‚Äôs output ¬ØzT= (¬ØxT,¬ØyT)satisfies
Gf(¬ØxT,¬ØyT)‚â§60 max
z‚ààZ dX
i=1œài(z)!
(1‚àíŒ∏)‚àí12
Œ∑log(M) +Œ∑L2
1+L1YŒ∑
T
T‚àí1,
where YŒ∑
T= 8(c+ 1)[4Œ≥
Œ∑log(M) + 160 Œ≥L2+ 2Œ∑Œ≥L2
1(1 + 64 C2)](log( c+T) + 1) .
Similar to minimization Algorithm 1, the iteration complexity of minimax Algorithm 2 linearly
depends on the upper bound ofPd
i=1œàioverZ. Generally, the upper bound ofPd
i=1œàionZ
is related to d. In specific problems of multi-variable optimization (such as two-player zero-sum
Markov games), one can uniformly boundPd
i=1œàionZby a constant.
94.3 Application to Infinite Horizon Two-Player Zero-Sum Markov Games
In this section, we show how to leverage Algorithm 2 to achieve accelerated rates for optimizing
infinite horizon two-player zero-sum Markov games. Our algorithm use ÀúO(Œµ‚àí1)iteration bound to
find an Œµ-approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games.
As similar as the definition of discounted MDP in Preliminary, we utilize M= (S,A,B,P, œÉ, Œ∏,œÅ0)
to define a infinite horizon two-player zero-sum Markov game. The difference here compared to
Section 3.3 is that the cost function œÉis defined on S √ó A √ó B with values in [0,1], and the transition
model P(s|s‚Ä≤, a‚Ä≤, b‚Ä≤)denotes the probability of transitioning into state supon player 1 taking action
a‚Ä≤and player 2 taking action b‚Ä≤in state s‚Ä≤. We can define the value function Vzand action-value
function Qzon the joint distribution z= (x,y)‚àà Z=Q|S|
i=1‚àÜA√óQ|S|
i=1‚àÜB. The infinite horizon
two-player zero-sum Markov games consider the following policy optimization problem:
min
x‚ààXmax
y‚ààYJx,y(œÅ0), (13)
where Jz(œÅ0) =Es0‚àºœÅ0[Vz(s0)]. The following proposition indicates that Jzis general quasar
convex-concave, and satisfies Assumption 4.2 and the condition of Theorem 4.4,
Proposition 4.5. For any Q= (Q1,¬∑¬∑¬∑,Q|S|)with every Qi‚ààR|A|√ó|B|, define function
fi(Q,zi) :=x‚ä§
iQiyifor any i‚àà[1 :|S|]. There exists a tensor-valued function Psuch that
Jz(œÅ0)satisfies GQCC condition with fi(P(z),zi) =fi(Q‚àó,zi)for any œÅ0‚àà‚àÜS, where Q‚àó
satisfies the conditions mentioned in Eq. (11). Moreover, Psatisfies Assumption 4.2.
According to Proposition 4.5 and Theorem 4.4, if we apply Algorithm 2 to the infinite horizon
two-player Markov games basing internal operator Fi(Q,z) = (y‚ä§
iQ‚ä§
i,‚àíx‚ä§
iQi)‚ä§for block zi
with parameter selection Eq. (12), which is actually a variant of optimistic gradient descent/ascent for
Markov games [ 67], then the iterations Twe need to find an Œµ-approximate Nash equilibrium is upper-
bounded by ÀúO((1‚àíŒ∏)‚àí2.5Œµ‚àí1). To the best of our knowledge, our iteration bound matches state-of-
the-art iteration bound and is a factor of (1‚àíŒ∏)‚àí1.5|S|better than ÀúO((1‚àíŒ∏)‚àí4|S|Œµ‚àí1)bound of Cen
et al. [10]. Since the upper bound ofP|S|
i=1œàiover feasible region Zin infinite horizon two-player
zero-sum Markov games‚Äô setting satisfiesP|S|
i=1œài(z)‚â§P|S|
i=1[dx,y‚àó(x)
œÅ0(si) +dx‚àó(y),y
œÅ0(si)]‚â§2
for any z‚àà Z, our algorithm‚Äôs iteration bound does not depend on the size of states.
5 Conclusion
In this work, we introduce two function structures: GQC and GQCC and provide related algorithmic
frameworks with convergence result. To complement our result, we also show that discounted MDP
and infinite horizon two-player zero-sum Markov games admit the GQC and GQCC condition,
respectively, and satisfy our mild assumptions.
6 Acknowledgements
C. Fang was supported by National Key R&D Program of China (2022ZD0114902) and the NSF
China (No.62376008). L. Luo was supported by National Natural Science Foundation of China
(No. 62206058), Shanghai Sailing Program (22YF1402900), Shanghai Basic Research Program
(23JC1401000), and the Major Key Project of PCL under Grant PCL2024A06.
References
[1]Jacob D. Abernethy, Peter L. Bartlett, and Elad Hazan. Blackwell approachability and no-regret
learning are equivalent. arXiv preprint arXiv:1011.1936 , 2010.
[2]Alekh Agarwal, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan. On the theory of
policy gradient methods: Optimality, approximation, and distribution shift. Journal of Machine
Learning Research , 2021.
[3]Ahmet Alacaoglu, Luca Viano, Niao He, and V olkan Cevher. A natural actor-critic framework
for zero-sum Markov games. In International Conference on Machine Learning . PMLR, 2022.
10[4]David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of
Mathematics , 1956.
[5] Charles E. Blair. Problem complexity and method efficiency in optimization (a. s. nemirovsky
and d. b. yudin). Siam Review , 1985.
[6]Stephen P. Boyd and Lieven Vandenberghe. Convex optimization. Journal of the American
Statistical Association , 2005.
[7]Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding
stationary points I. Mathematical Programming , 2017.
[8]Yair Carmon, John C. Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding
stationary points II: first-order methods. Mathematical Programming , 2017.
[9]Shicong Cen, Yuting Wei, and Yuejie Chi. Fast policy extragradient methods for competitive
games with entropy regularization. Advances in Neural Information Processing Systems , 2021.
[10] Shicong Cen, Yuejie Chi, Simon Shaolei Du, and Lin Xiao. Faster last-iterate convergence
of policy optimization in zero-sum markov games. International Conference on Learning
Representations , 2023.
[11] Lesi Chen, Boyuan Yao, and Luo Luo. Faster stochastic algorithms for minimax optimization
under Polyak Lojasiewicz condition. Advances in Neural Information Processing Systems ,
2022.
[12] Ziyi Chen, Shaocong Ma, and Yi Zhou. Sample efficient stochastic policy extragradient
algorithm for zero-sum markov game. 2021.
[13] Ching-An Cheng, Remi Tachet des Combes, Byron Boots, and Geoff Gordon. A reduction
from reinforcement learning to no-regret online learning. Proceedings of the Twenty Third
International Conference on Artificial Intelligence and Statistics , 2020.
[14] Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong Jin, and
Shenghuo Zhu. Online optimization with gradual variations. Conference on Learning Theory ,
2012.
[15] Constantinos Daskalakis, Maxwell Fishelson, and Noah Golowich. Near-optimal no-regret
learning in general games. Advances in Neural Information Processing Systems , 2021.
[16] Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of
constrained min-max optimization. Proceedings of the 53rd Annual ACM SIGACT Symposium
on Theory of Computing , 2021.
[17] Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan. Efficient methods for
structured nonconvex-nonconcave min-max optimization. Proceedings of The 24th International
Conference on Artificial Intelligence and Statistics , 2021.
[18] Jerzy A. Filar and Boleslaw Tolwinski. On the algorithm of Pollatschek and Avi-ltzhak. 1991.
[19] Anders Forsgren, Philip E. Gill, and Margaret H. Wright. Interior methods for nonlinear
optimization. SIAM Rev. , 2002.
[20] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. International Conference on Machine Learning , 2019.
[21] Benjamin Grimmer, Haihao Lu, Pratik Worah, and Vahab Mirrokni. The landscape of the
proximal point method for nonconvex‚Äìnonconcave minimax optimization. Mathematical
Programming , 2023.
[22] Moritz Hardt, Tengyu Ma, and Benjamin Recht. Gradient descent learns linear dynamical
systems. The Journal of Machine Learning Research , 2018.
[23] Sergiu Hart and Andreu Mas-Colell. A simple adaptive procedure leading to correlated equilib-
rium. Econometrica , 2000.
11[24] Oliver Hinder, Aaron Sidford, and Nimit Sohoni. Near-optimal methods for minimizing
star-convex functions and beyond. Conference on learning theory , 2020.
[25] Jean-Baptiste Hiriart-Urruty and Claude Lemar√©chal. Convex analysis and minimization
algorithms. 1993.
[26] Alan J. Hoffman and Richard M. Karp. On nonterminating stochastic games. Management
Science , 1966.
[27] Feihu Huang, Xidong Wu, and Heng Huang. Efficient mirror descent ascent methods for
nonsmooth minimax problems. Advances in Neural Information Processing Systems , 2021.
[28] Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in neural information processing systems , 2018.
[29] Anatoli B. Juditsky, Philippe Rigollet, and A. Tsybakov. Learning by mirror averaging. Annals
of Statistics , 2005.
[30] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement
learning. International Conference on Machine Learning , 2002.
[31] Alexander Kaplan and Rainer Tichatschke. Proximal point methods and nonconvex optimization.
Journal of Global Optimization , 1998.
[32] Robert D. Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does SGD escape
local minima? International Conference on Machine Learning , 2018.
[33] GM Korpelevich. Extragradient method for finding saddle points and other problems. Matekon ,
1977.
[34] Guanghui Lan. First-order and Stochastic Optimization Methods for Machine Learning .
Springer Cham, 2020.
[35] Guanghui Lan. Policy mirror descent for reinforcement learning: Linear convergence, new
sampling complexity, and generalized problem classes. Mathematical Programming , 2022.
[36] Sucheol Lee and Donghwan Kim. Fast extra gradient methods for smooth structured nonconvex-
nonconcave minimax problems. Advances in Neural Information Processing Systems , 2021.
[37] E.S. Levitin and Boris Polyak. Constrained minimization methods. USSR Computational
Mathematics and Mathematical Physics , 1966.
[38] Jiajin Li, Linglingzhi Zhu, and Anthony Man-Cho So. Nonsmooth composite nonconvex-
concave minimax optimization. arXiv preprint arXiv:2209.10825 , 2022.
[39] Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. International Conference on Machine Learning , 2020.
[40] Tianyi Lin, Chi Jin, and Michael I. Jordan. Near-optimal algorithms for minimax optimization.
Proceedings of Thirty Third Conference on Learning Theory , 2020.
[41] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. 30th Annual
Symposium on Foundations of Computer Science , 1989.
[42] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning.
International Conference on Machine Learning , 1994.
[43] Arkadi Nemirovski. Prox-method with rate of convergence O(1/t)for variational inequali-
ties with lipschitz continuous monotone operators and smooth convex-concave saddle point
problems. SIAM Journal on Optimization , 2004.
[44] Yurii Nesterov. A method of solving a convex programming problem with convergence rate
O 
k‚àí2
. InDoklady Akademii Nauk . Russian Academy of Sciences, 1983.
[45] Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and
related problems. Mathematical Programming , 2007.
12[46] Yurii Nesterov. Introductory lectures on convex optimization - a basic course. In Applied
Optimization , 2014.
[47] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global
performance. Mathematical Programming , 2006.
[48] Yurii Nesterov and Laura Rosa Maria Scrimali. Solving strongly monotone variational and
quasi-variational inequalities. Econometrics eJournal , 2006.
[49] Jorge Nocedal and Stephen J. Wright. Numerical optimization. In Fundamental Statistical
Inference , 2018.
[50] Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn.
Solving a class of non-convex min-max games using iterative first order methods. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d 'Alch√©-Buc, E. Fox, and R. Garnett, editors, Advances in
Neural Information Processing Systems . Curran Associates, Inc., 2019.
[51] Erich Novak. Deterministic and stochastic error bounds in numerical analysis. 1988.
[52] Yuyuan Ouyang and Yangyang Xu. Lower complexity bounds of first-order methods for
convex-concave bilinear saddle-point problems. Mathematical Programming , 2018.
[53] Sam Patterson and Yee Whye Teh. Stochastic gradient riemannian langevin dynamics on the
probability simplex. In Neural Information Processing Systems , 2013.
[54] Moshe Asher Pollatschek and Benjamin Avi-Itzhak. Algorithms for stochastic games with
geometrical interpretation. Management Science , 1969.
[55] Leonid Denisovich Popov. A modification of the arrow-hurwicz method for search of saddle
points. Mathematical notes of the Academy of Sciences of the USSR , 1980.
[56] Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. arXiv
preprint arXiv:1208.3728 , 2012.
[57] Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable
sequences. Advances in Neural Information Processing Systems , 2013.
[58] Julia Jean Robinson. An iterative method of solving a game. Classics in Game Theory , 1951.
[59] R. Tyrrell Rockafellar. Convex analysis: (pms-28). 1970.
[60] R. Tyrrell Rockafellar, Roger J.-B. Wets, and Maria Wets. Variational analysis. In Grundlehren
der mathematischen Wissenschaften , 1998.
[61] Lloyd S. Shapley. Stochastic games*. Proceedings of the National Academy of Sciences , 1953.
[62] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. Advances in neural information
processing systems , 1999.
[63] Paul Tseng. On linear convergence of iterative methods for the variational inequality problem.
Journal of Computational and Applied Mathematics , 1995.
[64] Jan van der Wal. Discounted markov games: Generalized policy iteration method. Journal of
Optimization Theory and Applications , 1978.
[65] Gal Vardi, Gilad Yehudai, and Ohad Shamir. Learning a single neuron with bias using gradient
descent. ArXiv , 2021.
[66] Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization.
Advances in Neural Information Processing Systems , 2020.
[67] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Last-iterate convergence of
decentralized optimistic gradient descent/ascent in infinite-horizon competitive markov games.
InAnnual Conference Computational Learning Theory , 2021.
13[68] Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance-reduced optimiza-
tion for a class of nonconvex-nonconcave minimax problems. arXiv preprint arXiv:2002.09621 ,
2020.
[69] Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He. Faster single-loop algorithms
for minimax optimization without strong concavity. In International Conference on Artificial
Intelligence and Statistics , 2022.
[70] Yuepeng Yang and Cong Ma. O(T‚àí1) convergence of optimistic-follow-the-regularized-leader
in two-player zero-sum markov games. arXiv preprint arXiv:2209.12430 , 2022.
[71] TaeHo Yoon and Ernest K Ryu. Accelerated algorithms for smooth convex-concave minimax
problems with O(1/k2)rate on squared gradient norm. International Conference on Machine
Learning , 2021.
[72] Sihan Zeng, Thinh T. Doan, and Justin Romberg. Regularized gradient descent ascent for
two-player zero-sum Markov games. Advances in Neural Information Processing Systems ,
2022.
[73] Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhi-Quan Luo. A single-loop smoothed gradient
descent-ascent algorithm for nonconvex-concave min-max problems. Advances in Neural
Information Processing Systems , 2020.
[74] Yulai Zhao, Yuandong Tian, Jason D. Lee, and Simon Shaolei Du. Provably efficient policy
optimization for two-player zero-sum markov games. In International Conference on Artificial
Intelligence and Statistics , 2021.
[75] Yulai Zhao, Yuandong Tian, Jason Lee, and Simon Du. Provably efficient policy optimization
for two-player zero-sum markov games. In International Conference on Artificial Intelligence
and Statistics . PMLR, 2022.
[76] Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to
global minimum in deep learning via star-convex path. arXiv preprint arXiv:1901.00451 , 2019.
14Contents
1 Introduction 1
1.1 Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminary 4
3 Minimization Optimization 5
3.1 Generalized Quasar-Convexity (GQC) . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Application to Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 7
4 Minimax Optimization 8
4.1 Generalized Quasar-Convexity-Concavity (GQCC) . . . . . . . . . . . . . . . . . 8
4.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.3 Application to Infinite Horizon Two-Player Zero-Sum Markov Games . . . . . . . 10
5 Conclusion 10
6 Acknowledgements 10
A Preliminary 16
A.1 Supplemental Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Finite Differences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Finite Horizon Markov Decision Process . . . . . . . . . . . . . . . . . . . . . . . 16
B Minimization Optimization 17
B.1 Proof of Theorem 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.1.1 Part I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.1.2 Part II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.1.3 The Last Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.2 Simple Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.3 Application to Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . 25
B.3.1 Analysis of Infinite Horizon Reinforcement Learning . . . . . . . . . . . . 25
B.3.2 Analysis of Finite Horizon Reinforcement Learning . . . . . . . . . . . . . 26
C Minimax Optimization 26
C.1 Preparatory Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C.2 Theorem C.7 and Relate Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.2.1 Part I . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
C.2.2 Part II: Estimation of Approximation Error ‚à•Qt‚àíQ‚àó‚à•. . . . . . . . . . . 32
C.2.3 The Last Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
15C.3 Application to Minimax Problems . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.3.1 Infinite Horizon Two-Player Zero-Sum Markov Games . . . . . . . . . . . 36
C.3.2 Convex-Concave Minimax Problems . . . . . . . . . . . . . . . . . . . . 37
D Auxiliary Lemma 37
E Limitation 41
A Preliminary
A.1 Supplemental Notation
For simplicity, we denote g(Œì) :=P‚àû
k=1Œì‚àík[k7+ (k+ 1) exp {2k}], the chi-squared divergence
between p,qasœá2(p‚à•q) :=Pn
j=1(p(j)‚àíq(j))2
q(j),Ep(x) :=Pn
j=1p(j)x(j)andVarp(x) :=
Pn
j=1p(j)¬∑(x(j)‚àíEp(x))2for any p,q‚àà‚àÜnandx‚ààRn. For Œ∂ > 0, n‚ààZ+, we say
that a sequence of distributions p1,¬∑¬∑¬∑,pT‚àà‚àÜnisŒ∂-consecutively close if for each 1‚â§t < T , it
holds that maxnpt
pt+1,pt+1
pto
‚â§1 +Œ∂. For positive scalar Œ∏‚àà[0,1), non-negative integers t
andT, we define Œ≤Œ∏
T,t:=Œ≤tQT‚àí1
j=t(1‚àíŒ≤j+Œ∏Œ≤j), and Œ≤Œ∏
T,T= 1.
A.2 Finite Differences
Definition A.1 (Finite Differences) .For a sequence of vectors L= (L0,¬∑¬∑¬∑,LT)where each
Lt‚ààRn, and integers h‚ààZ+, the order- hfinite difference sequence for the sequence Lis denoted
byDhL:= 
(DhL)0,¬∑¬∑¬∑,(DhL)T‚àíh
recursively with (D0L)t:=Ltfor all t‚àà[0 :T], and
(DhL)t:= (Dh‚àí1L)t+1‚àí(Dh‚àí1L)t, (14)
for all h‚â•1andt‚àà[1 :T‚àíh].
As stated in [15, Remark 4.3], we have
(DhL)t=hX
s=0
h
s
(‚àí1)h‚àísLt+s. (15)
To guarantee the coherence of the analysis‚Äôs structure, we introduce the definition of the shift operator
Esas follows:
Definition A.2 (Shift Operator) .For a sequence of vectors L= (L0,¬∑¬∑¬∑,LT)where each Lt‚ààRn,
and integers s‚ààZ+, thes-shift sequence for the sequence Lis denoted by EsL:= 
(EsL)0,¬∑¬∑¬∑,
(EsL)T‚àíh
with(EsL)t=Lt+sfort‚àà[1 :T‚àís].
A.3 Finite Horizon Markov Decision Process
We also consider the following finite horizon Markov decision process (MDP), denoted by M:=
(H,S1:H,A1:H,P2:H, œÉ,œÅ1).H‚ààZ+denotes the number of horizon; S1:H= (S1,¬∑¬∑¬∑,SH)is a
sequence of Hfinite state spaces; A1:H= (A1,¬∑¬∑¬∑,AH)is a sequence of Hfinite action spaces;
Ph(sh|sh‚àí1, ah‚àí1)denotes the probability of transitioning from sh‚àí1toshunder playing action
ah‚àí1at horizon h‚àí1;œÉ:S1:H√ó A 1:H‚Üí[0,1]is a cost function; œÅ1is a initial state distribution
overS1.
œÄ= (œÄ1,¬∑¬∑¬∑,œÄH) :S1:H‚Üí‚àÜA1√ó ¬∑¬∑¬∑ √ó ‚àÜAHdenotes a stochastic policy. Similarly, we
usePrœÄ1:h‚àí1
h(s‚Ä≤|s) =PrœÄ1:h‚àí1
h(sh=s‚Ä≤|s1=s)to denote the probability of visiting the state s‚Ä≤
from the state sat horizon haccording to policy œÄ1:h‚àí1. Let trajectory œÑ= (sh, ah)H
h=1, where
s1‚àºœÅ1, and, for all subsequent horizon h,ah‚àºœÄh(¬∑|sh)andsh+1‚àºPh+1(¬∑|sh, ah). The value
function VœÄh:H
h:Sh‚ÜíRis defined as the sum of future cost starting at state shand executing
16Algorithm 3 Optimistic Mirior Descent for Multi-Variables
Input:
g0
i=x0
i	d
i=1,Œ∑andT.
Output: Randomly pick up t‚àà {1,¬∑¬∑¬∑, T}following the probability P[t] = 1/Tand return xt.
1:while t‚â§Tdo
2: for all i‚àà[1 :d]do
3: xt
i= argmin
xi‚ààXiŒ∑
Fi(xt‚àí1),xi
+V 
xi,gt‚àí1
i
,
4: gt
i= argmin
gi‚ààXiŒ∑‚ü®Fi(xt),gi‚ü©+V 
gi,gt‚àí1
i
.
5: end for
6: t‚Üêt+ 1.
7:end while
œÄh:H= (œÄh,¬∑¬∑¬∑,œÄH), i.e.,
VœÄh:H
h(sh) =E"HX
h‚Ä≤=hœÉ(sh‚Ä≤, ah‚Ä≤)œÄh:H, sh#
.
For convenience, we define VœÄ
1(s1) =VœÄ1:H
1(s1). Moreover, we define the action-value function
QœÄh+1:H
h:Sh√ó A h‚Üí[0,1 +H‚àíh]as follows:
QœÄh+1:H
h(sh, ah) =œÉ(sh, ah) +E"HX
h‚Ä≤=h+1œÉ(sh‚Ä≤, ah‚Ä≤)œÄh+1:H, sh, ah#
.
B Minimization Optimization
We begin with a general version of Theorem 3.2 basing Algorithm 3 in this part.
Theorem B.1. [General Version of Theorem 3.2] We consider the divergence-generating function
vwith Bregman‚Äôs divergence V(xi,ui) =v(xi)‚àív(ui)‚àí ‚ü®‚àá v(ui),xi‚àíui‚ü©for any block Xi
and any xi,ui‚àà Xi. Assuming that FisL-Lipschitz continuous with respect to ‚à• ¬∑ ‚à•‚àóunder‚à• ¬∑ ‚à•,
V(xi,ui)‚â• ‚à•xi‚àíui‚à•2for any xi,ui‚àà XiandŒ≥max= max i‚àà[1:d]Œ≥i<‚àû, we have
1
TTX
t=1(f(xt)‚àíf(x‚àó))‚â§2L(dŒ≥max)1/2Pd
i=1Œ≥‚àí1
i3/2
Tmax
i‚àà[1:d]
max
xi‚ààXiV(xi,g0
i)
, (16)
with setting Œ∑= (L2dŒ≥maxPd
i=1Œ≥‚àí1
i)‚àí1/2/2.
Proof. According to GQC condition (Definition 3.1), we have the following estimation
TX
t=1(f(xt)‚àíf(x‚àó))‚â§dX
i=11
Œ≥iTX
t=1
Fi(xt),xt
i‚àíx‚àó
i
. (17)
For any fixed i‚àà[1 :d], we obtain that
‚ü®Fi(xt),xt
i‚àíx‚àó
i‚ü©=‚ü®Fi(xt)‚àíFi(xt‚àí1),xt
i‚àígt
i‚ü©| {z }
I+‚ü®Fi(xt‚àí1),xt
i‚àígt
i‚ü©| {z }
II
+‚ü®Fi(xt),gt
i‚àíx‚àó
i‚ü©| {z }
III(18)
SinceFisL-Lipschitz continuous with respect to ‚à• ¬∑ ‚à•‚àóunder‚à• ¬∑ ‚à•, we have following estimation of
Iby using Cauchy-Schwarz inequality
I ‚â§L2Œ∑
2xt‚àíxt‚àí12+1
2Œ∑xt
i‚àígt
i2. (19)
17In addition, utilizing the result of [Lemma 3.4, [34]] on step-3 and step-4 of Algorithm 3, we have
II ‚â§1
Œ∑
V 
gt
i,gt‚àí1
i
‚àíV 
gt
i,xt
i
‚àíV 
xt
i,gt‚àí1
i
, (20)
III ‚â§1
Œ∑
V 
x‚àó
i,gt‚àí1
i
‚àíV 
x‚àó
i,gt
i
‚àíV 
gt
i,gt‚àí1
i
. (21)
Therefore, by applying Eq. (19), (20) and (21) into Eq. (18), we obtain
TX
t=1‚ü®Fi(xt),xt
i‚àíx‚àó
i‚ü© ‚â§1
Œ∑V(x‚àó
i,g0
i) +TX
t=1L2Œ∑
2xt‚àíxt‚àí12+1
2Œ∑xt
i‚àígt
i2
‚àí1
Œ∑TX
t=1V(gt
i,xt
i)‚àí1
Œ∑TX
t=1V(xt
i,gt‚àí1
i)
‚â§
(a)1
Œ∑V(x‚àó
i,g0
i) +L2Œ∑
2TX
t=1xt‚àíxt‚àí12
‚àí1
2Œ∑TX
t=1gt
i‚àíxt
i2‚àí1
2Œ∑TX
t=1xt
i‚àígt‚àí1
i2
‚â§
(b)1
Œ∑V(x‚àó
i,g0
i) +1
2Œ∑g0
i‚àíx0
i2+L2Œ∑
2TX
t=1xt‚àíxt‚àí12
‚àí1
4Œ∑TX
t=1xt
i‚àíxt‚àí1
i2, (22)
where (a) is derived from the assumption that V(xi,ui)‚â• ‚à•xi‚àíui‚à•2for any xi,ui‚àà Xiand (b)
follows from the convexity of ‚à• ¬∑ ‚à•. Applying Eq. (22) to Eq. (17), we have
TX
t=1(f(xt)‚àíf(x‚àó))‚â§
(c)1
Œ∑dX
i=1V(x‚àó
i,g0
i)
Œ≥i+L2Œ∑
2 dX
i=1Œ≥‚àí1
i!TX
t=1xt‚àíxt‚àí12
‚àí1
4Œ∑TX
t=1"dX
i=1xt
i‚àíxt‚àí1
i2
Œ≥i#
‚â§
(d)Pd
i=1Œ≥‚àí1
i
Œ∑max
i‚àà[1:d]
max
xi‚ààXiV(xi,g0
i)
‚àí 
1
4dŒ∑Œ≥ max‚àíL2Œ∑
2dX
i=1Œ≥‚àí1
i!TX
t=1xt‚àíxt‚àí12, (23)
where (c) is derived from the fact that g0
i=x0
ifor any i‚àà[1 :d]and (d) follows from the convexity
of‚à• ¬∑ ‚à• (1
dPd
i=1‚à•xi‚à•2‚â§ ‚à•1
dPd
i=1xi‚à•2).
Since KL divergence satisfies KL(xi‚à•ui)‚â• ‚à•xi‚àíui‚à•2
1(Pinsker‚Äôs inequality), Theorem 3.2 can be
directly derived from Theorem B.1. Next, we propose Proposition B.2 and provide related proof.
Proposition B.2. We denote N=Pd
i=1niand let a smooth vector-valued function F:RN‚ÜíR‚Ñì
satisfies:
1. There is a point y‚ààRNsuch that ‚à•DŒ±F(y)‚à•‚àû‚â§Œ≥kwith|Œ±|=kfor all k‚àà[0 :K],
2.For any positive integer kgreater than K,‚à•DŒ±F‚à•‚àû‚â§Œ≥kwith|Œ±|=kuniformly over X,
with a positive constant Œ≥and a positive integer K, thenFsatisfies Assumption 3.3.
Proof of Proposition B.2. For any k‚ààZ+andj‚àà[1 :l], we have
PF(j)
k,y(x)‚â§kX
i=0X
|Œ±|=iŒ≥i
Œ±!¬∑(|x|+|y|)Œ±=kX
i=0[Œ≥(d+‚à•y‚à•1)]i
i!‚â§exp{Œ≥(d+‚à•y‚à•1)}, (24)
18using the fact that ‚à•DŒ±F(y)‚à•‚àû‚â§Œ≥kfor any k‚ààZ+and|Œ±|=k. In addition, by the Taylor
expansion of F(j)with Lagrange remainder formula for any j‚àà[1 :l]andk >1, we can obtain
RF(j)
k,y(x)=X
|Œ±|=kDŒ±F(j)(y+t(x‚àíy))
Œ±!(x‚àíy)Œ±‚â§[Œ≥(d+‚à•y‚à•1)]k
k!, (25)
where t‚àà[0,1]depends on F(j),xandy. Letting k0=‚åà3Œ≥(d+‚à•y‚à•1)‚åâand supposing k‚â•
k0
1 +log(1+ Œ≥(d+‚à•y‚à•1))
log(3/2)
, we derive that
[Œ≥(d+‚à•y‚à•1)]k
k!‚â§3k0‚àík. (26)
Therefore, in the light of Eq. (24), Eq. (25) and Eq. (26), it‚Äôs direct to derive that Fstatisfies
Assumption 3.3 with K0=k0
1 +log(1+ Œ≥(d+‚à•y‚à•1))
log(3/2)
,Œ∏=1
3,Œò1= 3k0andŒò2= exp {Œ≥(d+
‚à•y‚à•1)}.
The following remark discusses the reasonability of Proposition B.2 conditions, which supports the
reasonability of Assumption 3.3.
Remark B.3.Since region X=Qd
i=1‚àÜniis bounded, it‚Äôs reasonable to assume that the growth rate
of the upper bound of internal function‚Äôs high-order derivatives is not faster than linear growth rate.
For example, the upper bounds of high-order derivatives of sin(Cx),cos(Cx)andexp{Cx}have
linear growth rate over Xfor fixed constant C. Therefore, if the internal function Fcan be generated
by the linear combination of {sin(Ckx)}K
k=1and{cos(Ckx)}K
k=1(or{exp{Ckx}}K
k=1) with finite
K,Fsatisfies Assumption 3.3 by using Proposition B.2.
B.1 Proof of Theorem 3.5
We briefly introduce our techniques to make the proof of Theorem 3.5 more comprehensible
in this part. Our proof consists of two ingredients. The first is applying Lemma B.4 to con-
struct a variant upper bound of average function error1
TPT
t=1(f(xt)‚àíf(x‚àó))that is differ-
ent from the upper bound derived from the classical OMD algorithm. This bound is com-
posed of a) O
1
Œ∑T
invariant error and b) weighted sum of the variance for finite difference
sequence {(D1Fi(xt‚àí1)}T
t=1and{(D0Fi(xt‚àí1)}T
t=1overi‚àà[1 :d], which has the form ofPd
i=11
Œ≥i[O(1)
TPT
t=1Varxt
i(D1Fi(xt‚àí1))‚àíO(1)
TPT
t=1Varxt
i(Fi(xt‚àí1))]. The second is applying
Lemma D.7 (refer to it as control lemma) on each {Fi(xt)}T
t=1to bound (b) by a quantity that grows
poly-logarithmically in T. Therefore, it‚Äôs necessary to leverage Theorem B.5 and Lemma B.7 to show
that every sequence {Fi(xt)}T
t=0outputted by Algorithm 1 satisfies the preconditions of Lemma
D.7.
B.1.1 Part I
The next Lemma B.4 provides a variant convergence proof of the OMD algorithm. In this Lemma,
basing on KL divergence, an explicit expression for the optimal solution of the OMD sub-problem is
utilized to provide an upper bound ofPT
t=1(f(xt)‚àíf(x‚àó)).
Lemma B.4. Suppose ‚à•F(x)‚à•‚àû‚â§Œò(Œò‚â•1) for any x‚àà X and policy set {xt}T
t=1follows the
iteration of Algorithm 1 with step size Œ∑‚àà(0,1
32Œò). Then, it holds that
TX
t=1 
f(xt)‚àíf(x‚àó)
‚â§dX
i=11
Œ≥i"
log(ni)
Œ∑+ ÀÜg1(Œ∑Œò)Œ∑Œò2TX
t=1Varxt
i 
Fi(xt)‚àíFi(xt‚àí1)
‚àíÀÜg2(Œ∑Œò)Œ∑Œò2TX
t=1Varxt
i(Fi(xt‚àí1))#
, (27)
where ÀÜg1(Œ∑) :=1
2+ 64
1
3(1‚àí16Œ∑)+ 2
Œ∑andÀÜg2(Œ∑) :=1
2‚àí16
1
3(1‚àí16Œ∑)+ 2
Œ∑.
19Proof. As claimed by Definition 3.1, we have the following estimation
TX
t=1(f(xt)‚àíf(x‚àó))‚â§dX
i=11
Œ≥iTX
t=1
Fi(xt),xt
i‚àíx‚àó
i
. (28)
In the following, considering a fixed i‚àà[1 :d], it‚Äôs easy to obtain that
‚ü®Fi(xt),xt
i‚àíx‚àó
i‚ü©=‚ü®Fi(xt)‚àíFi(xt‚àí1),xt
i‚àígt
i‚ü©| {z }
I+‚ü®Fi(xt‚àí1),xt
i‚àígt
i‚ü©| {z }
II
+‚ü®Fi(xt),gt
i‚àíx‚àó
i‚ü©| {z }
III(29)
Recall the update of Algorithm 1 can be devided into two parts:
gt
i= arg min
gi‚àà‚àÜniŒ∑
Fi(xt),gi
+ KL( gi‚à•gt‚àí1
i), (30)
xt+1
i= arg min
xi‚àà‚àÜniŒ∑
Fi(xt),xi
+ KL( xi‚à•gt
i), (31)
for any i‚àà[1 :d]where g0
i‚àùx0
i¬∑exp{Œ∑(Fi(x0)‚àíFi(x‚àí1))}andx‚àí1
i=x0
i=
1
ni,¬∑¬∑¬∑,1
ni‚ä§
.
According to Cauchy-Schwarz inequality, we can evaluate Ias follows
I ‚â§gt
i‚àíxt
i‚àó
xt
i¬∑q
Varxt
i(Fi(xt)‚àíFi(xt‚àí1)). (32)
In addition, utilizing the result of Lemma D.2, we have
II=1
Œ∑
KL 
gt
i||gt‚àí1
i
‚àíKL 
gt
i‚à•xt
i
‚àíKL 
xt
i‚à•gt‚àí1
i
, (33)
III=1
Œ∑
KL 
x‚àó
i‚à•gt‚àí1
i
‚àíKL 
x‚àó
i‚à•gt
i
‚àíKL 
gt
i‚à•gt‚àí1
i
. (34)
Therefore, by applying Eq. (32), (33) and (34) into Eq. (29), we obtain
TX
t=1‚ü®Fi(xt),xt
i‚àíx‚àó
i‚ü© ‚â§1
Œ∑KL(x‚àó
i‚à•g0
i) +TX
t=1‚à•gt
i‚àíxt
i‚à•‚àó
xt
i¬∑q
Varxt
i(Fi(xt)‚àíFi(xt‚àí1))
‚àí1
Œ∑TX
t=1KL(gt
i‚à•xt
i)‚àí1
Œ∑TX
t=1KL(xt
i‚à•gt‚àí1
i). (35)
Since there is a vector Fi(xt)‚àíFi(xt‚àí1)such that for any j‚àà[1 :ni]
gt
i(j) =xt
i(j) exp
Œ∑ 
Fi(j)(xt)‚àíFi(j)(xt‚àí1)	
Pni
j‚Ä≤=1xt
i(j‚Ä≤) exp{Œ∑(Fi(j‚Ä≤)(xt)‚àíFi(j‚Ä≤)(xt‚àí1))}, (36)
we have that
max
i‚àà[1:d]gt
i
xt
i
‚àû‚â§exp{2Œ∑Fi(xt)‚àíFi(xt‚àí1)
‚àû} ‚â§exp{4Œ∑Œò} ‚â§1 + 8 Œ∑Œò, (37)
and
max
i‚àà[1:d]xt
i
gt‚àí1
i
‚àû‚â§exp{2Œ∑‚à•Fi(xt‚àí1)‚à•‚àû} ‚â§exp{2Œ∑Œò} ‚â§1 + 4 Œ∑Œò,
with combining Eq. (31) and choosing proper Œ∑such that Œ∑Œò‚â§1
4. According to Lemma D.3, we
have
KL(gt
i‚à•xt
i)‚â•1‚àí8Œ∑Œò
2‚àí16Œ∑Œò
3(1‚àí8Œ∑Œò)
X2(gt
i,xt
i),
KL(xt
i‚à•gt‚àí1
i)‚â•1‚àí4Œ∑Œò
2‚àí8Œ∑Œò
3(1‚àí4Œ∑Œò)
X2(xt
i,gt‚àí1
i),(38)
20for any i‚àà[1 :d]. Noting that X2(œÅ, ¬µ) =
‚à•œÅ‚àí¬µ‚à•‚àó
¬µ2
, in the light of Lemma D.4, we derive that
X2(gt
i,xt
i)‚â§
1 + 321
3(1‚àí16Œ∑Œò)+ 2
Œ∑Œò
(Œ∑Œò)2Varxt
i 
Fi(xt)‚àíFi(xt‚àí1)
,
X2(gt
i,xt
i)‚â•
1‚àí321
3(1‚àí16Œ∑Œò)+ 2
Œ∑Œò
(Œ∑Œò)2Varxt
i 
Fi(xt)‚àíFi(xt‚àí1)
,(39)
as long as Œ∑Œò‚â§1
32. There exists a similar lower bound with respect to X2(xt
i,gt‚àí1
i)
X2(xt
i,gt‚àí1
i)‚â•
1‚àí161
3(1‚àí8Œ∑Œò)+ 2
Œ∑Œò
(Œ∑Œò)2Vargt‚àí1
i(Fi(xt‚àí1))
‚â•
1‚àí161
3(1‚àí8Œ∑Œò)+ 2
Œ∑Œò
(Œ∑Œò)2exp{‚àí2Œ∑Œò}Varxt
i(Fi(xt‚àí1))
‚â•
(a)
1‚àí161
3(1‚àí8Œ∑Œò)+ 3
Œ∑Œò
(Œ∑Œò)2Varxt
i(Fi(xt‚àí1)), (40)
where (a) is derived from exp{‚àí2Œ∑Œò} ‚â•1‚àí4Œ∑Œòfor any Œ∑Œò‚â§1
32. Relying on Eq. (35), Eq. (38)-
(40), we conclude that
TX
t=1
Fi(xt),xt
i‚àíx‚àó
i
‚â§log(ni)
Œ∑+
1 + 321
3(1‚àí16Œ∑Œò)+ 2
Œ∑Œò
Œ∑Œò2TX
t=1Varxt
i 
Fi(xt)‚àíFi(xt‚àí1)
‚àí1
2‚àí32
3(1‚àí16Œ∑Œò)+ 36
Œ∑Œò
Œ∑Œò2TX
t=1Varxt
i 
Fi(xt)‚àíFi(xt‚àí1)
‚àí1
2‚àí16
3(1‚àí8Œ∑Œò)+ 27
Œ∑Œò
Œ∑Œò2TX
t=1Varxt
i(Fi(xt‚àí1))
‚â§log(ni)
Œ∑+1
2+ 641
3(1‚àí16Œ∑Œò)+ 2
Œ∑Œò
Œ∑Œò2TX
t=1Varxt
i 
Fi(xt)‚àíFi(xt‚àí1)
‚àí1
2‚àí161
3(1‚àí16Œ∑Œò)+ 2
Œ∑Œò
Œ∑Œò2TX
t=1Varxt
i(Fi(xt‚àí1)). (41)
Finally, applying the estimation Eq. (41) to Eq. (28), we complete the proof.
B.1.2 Part II
Basing on the conclusion of Lemma B.4, if the finite sum of Varxt
i(Fi(xt)‚àíFi(xt‚àí1))can be
controlled by the finite sum of Varxt
i(Fi(xt‚àí1))with aO(poly(log( T)))constant for each i‚àà[1 :d],
the final convergence result can be obtained directly. Hence, to demonstrate this relationship, we
require the assistance of auxiliary Lemma D.7. Our initial step is to prove that Fi(xt)satisfies the
first condition in Lemma D.7 for any i‚àà[1 :d].
Theorem B.5. Assuming fsatisfies GQC condition and Assumption 3.3 holds, xtfollows the
iteration of Algorithm 1, we set Œ≤‚àà
0,1
(Œò1+Œò2+1)(H+3)
,Œì‚â•e2+ 322560Œò 2,ÀÜK‚â•max{K0,
Hlog(4Œ≤‚àí1)+log(Œò 1)
log(Œ∏‚àí1)}andŒ∑=Œ≤
6e3ÀÜKŒì max{Œò,1}. Then, the following finite difference bound with
respect to {Fi(xt)}d
i=1holds
max
i‚àà[1:d](DhFi(x))t0
‚àû‚â§Œ≤hh3h+1, (42)
for all h‚àà[1 :H]andt0‚àà[0 :T‚àíh]. Without loss of generality, we require that Hdoes not
exceed T.
21Proof of Theorem B.5. According to the Taylor expansion of each component kofFiaty, one can
notice that
(DhFi(k)(x))t0‚â§ÀÜKX
j=0X
|Œ±|=j|DŒ±Fi(k)(y)|
Œ±!(Dh(x‚àíy)Œ±)t0+
DhRFi(k)
ÀÜK,y(x)t0,(43)
for any ÀÜK‚ààZ+. Therefore, setting ÀÜK‚â•maxn
Hlog(4Œ≤‚àí1)+log(Œò 1)
log(Œ∏‚àí1), K0o
and combining the remark
Eq. (15) of operator Dhin Appendix A.2, we can guarantee the validity of the following estimation
DhRFi(k)
ÀÜK,y(x)t0‚â§2hmax
x‚ààXRFi(k)
ÀÜK,y(x)‚â§Œò12hŒ∏ÀÜK‚â§1
2Œ≤H‚â§1
2Œ≤HhBh+1, (44)
for any h‚àà[1 :H]. Moreover, as stated by Assumption 3.3, we obtain max
i‚àà[1:d]‚à•Fi(x)‚à•‚àû‚â§Œò1+ Œò 2
for any x‚àà X. Suppose that max
i‚àà[1:d]‚à•(Dh‚Ä≤Fi(x))t0‚à•‚àû‚â§Œ≤h‚Ä≤h‚Ä≤Bh‚Ä≤+1holds for any h‚Ä≤‚àà[1 :h]and
t0‚àà[0 :T‚àíh‚Ä≤], we deduce
(Dh+1Fi(k)(x))t0‚â§g(Œì)Œ≤h+1(h+ 1)B(h+1)+1PFi(k)
ÀÜK,y(xt0) +1
2Œ≤h+1(h+ 1)B(h+1)+1
‚â§1
2+g(Œì)Œò 2
Œ≤h+1(h+ 1)B(h+1)+1‚â§Œ≤h+1(h+ 1)B(h+1)+1,(45)
by using Lemma B.6 with p(x) :=|DŒ±Fi(k)(y)|
Œ±!(x‚àíy)Œ±and the fact that g(Œì)Œò 2‚â§1
2(which
can be derived from Lemma D.1). Therefore, to apply mathematical induction, it suffices to
prove that max
i‚àà[1:d]‚à•(Dh‚Ä≤Fi(x))t0‚à•‚àû‚â§Œ≤h‚Ä≤h‚Ä≤Bh‚Ä≤+1holds when h‚Ä≤= 1. Observe that Lemma
B.6 holds in the case h= 0. Thus, we can obtain Eq. (45) forh= 0 as well. Hence, we have
max
i‚àà[1:d]‚à•(D1Fi(x))t0‚à•‚àû‚â§Œ≤.
The proof of Theorem B.5 relies on the next Lemma B.6.
Lemma B.6. Assume max
i‚àà[1:d]‚à•Fi(x)‚à•‚àû‚â§Œòfor any x‚àà X and each element in utbelongs to
one of the dprobability distributions generated by Algorithm 1 with Œ∑‚â§Œ≤
6e3ŒìÀÜKmax{Œò,1}for some
Œì>1,ÀÜK‚â•Kin iteration t, and consider positive constants B‚â•3,Œ≤‚àà
0,1
(Œò+1)( H+3)
and polynomial function p(u) := CQK
k=1(u(k)‚àíy(k))where u:= (u(1),¬∑¬∑¬∑,u(K))‚ä§and
y:= (y(1),¬∑¬∑¬∑,y(K))‚ä§‚ààRKis a fixed point. Given h‚àà[1 :H‚àí1], we derive that
(Dh+1p(u))t0‚â§g(Œì)CKY
k=1(ut0(k) +|y(k)|)Œ≤h+1(h+ 1)B(h+1)+1, (46)
if the condition max
i‚àà[1:d]‚à•(Dh‚Ä≤Fi(x))t0‚à•‚àû‚â§Œ≤h‚Ä≤h‚Ä≤Bh‚Ä≤+1holds for any h‚Ä≤‚àà[1 :h]andt0‚àà[0 :
T‚àíh‚Ä≤].
Proof. Drawing on the premises outlined in the lemma, we assume that each u(k)corresponds to a
unique xi(k)(j(k)). According to the iteration of Algorithm 1, we can obtain
ut+1(k) =xt
i(k)(j(k))¬∑exp
Œ∑¬∑ 
2Fi(k)(j(k)) (xt)‚àíFi(k)(j(k)) 
xt‚àí1	
Pni(k)
j=1xt
i(k)(j)¬∑exp
Œ∑¬∑ 
2Fi(k)(j) (xt)‚àíFi(k)(j) (xt‚àí1)	,
=ut(k)¬∑exp
Œ∑¬∑ 
2Fi(k)(j(k)) (xt)‚àíFi(k)(j(k)) 
xt‚àí1	
Pni(k)
j=1xt
i(k)(j)¬∑exp
Œ∑¬∑ 
2Fi(k)(j) (xt)‚àíFi(k)(j) (xt‚àí1)	, (47)
for any k‚àà[1 :K]andt‚àà[1 :T‚àí1]. Given the sequence x1,¬∑¬∑¬∑,xt0+hgenerated by Algorithm
1, it is straightforward to derive that
ut0+t+1(k) = (Nk
u)‚àí1ut0(k)¬∑exp{Œ∑¬∑(Fi(k)(j(k))(xt0+t) +tX
t‚Ä≤=0Fi(k)(j(k))(xt0+t‚Ä≤)
‚àíFi(k)(j(k))(xt0‚àí1))}, (48)
22for any k‚àà[1 :K],t0‚àà[1 :T‚àíh‚àí1]andt‚àà[1 :h], where Nk
u=Pni(k)
j=1xt0
i(k)(j)¬∑exp{Œ∑¬∑
(Fi(k)(j)(xt0+t) +Pt
t‚Ä≤=0Fi(k)(j)(xt0+t‚Ä≤)‚àíFi(k)(j)(xt0‚àí1))}. We write
rt
t0,k:=Fi(k)(xt0+t‚àí1) +t‚àí1X
t‚Ä≤=0Fi(k)(xt0+t‚Ä≤)‚àíFi(k)(xt0‚àí1). (49)
Also, for a vector z‚ààRni(k)and an index j‚àà[1 :ni(k)], define
œàj
t0,k(z) =exp{z(j)}
Pni(k)
j‚Ä≤=1xi(k)
t0(j‚Ä≤)¬∑exp{z(j‚Ä≤)}, (50)
so that ut0+t(k) =xt0
i(k)(j(k))¬∑œàj(k)
t0,k
Œ∑rt
t0,k
=ut0(k)¬∑œàj(k)
t0,k
Œ∑rt
t0,k
fort‚â•1. For conve-
nience, we denote that D:=
Œ±‚ààNK|Œ±(i)‚àà {0,1},‚àÄi‚àà[1 :K]	
ande:= (1 ,¬∑¬∑¬∑,1)‚ààNK. In
particular, for any Œ±‚àà D, we have 
Dh‚Ä≤(ue‚àíŒ±)t0‚â§(ut0)e‚àíŒ± 
Dh‚Ä≤(œàt0(Œ∑rt0))e‚àíŒ±0
| {z }
I(Œ±,h‚Ä≤,t0), (51)
where œàt0(Œ∑rt
t0) :=
œàj(1)
t0,1(Œ∑rt
t0,1),¬∑¬∑¬∑,œàj(K)
t0,K(Œ∑rt
t0,K)
,h‚Ä≤‚àà[1 :h+1] andt0‚àà[1 :T‚àíh‚àí1].
It is important to observe that the finite difference in Eq. (51) pertains specifically to œàj(k)
t0,k(Œ∑rt
t0,k).
Notice that
(D1rt0,k)t= 2(E1Fi(k)(x))t0+t‚àí1‚àíFi(k)(xt0+t‚àí1), (52)
for any t‚àà[0 :h]. Therefore, for any h‚Ä≤‚àà[1 :h+ 1], we obtain
(Dh‚Ä≤rt0,k)t= 2 
E1Dh‚Ä≤‚àí1 
Fi(k)(x)t0+t‚àí1‚àí 
Dh‚Ä≤‚àí1 
Fi(k)(x)t0+t‚àí1, (53)
for any t‚àà[0 :h+ 1‚àíh‚Ä≤]. Because the step size Œ∑satisfies Œ∑‚â§Œ≤
6e3ŒìÀÜKmax{Œò,1},(D0Œ∑rt0)t
‚àû‚â§
Œ∑HŒò‚â§1
6e3ŒìÀÜKandmax
i‚àà[1:d](Dh‚Ä≤Fi(x))t0
‚àû‚â§Œ≤h‚Ä≤h‚Ä≤Bh‚Ä≤+1for all h‚Ä≤‚àà[1 :h], the following
estimation holds(Dh‚Ä≤+1Œ∑rt0)0
‚àû‚â§1
2e2ŒìÀÜKŒ≤h‚Ä≤+1(h‚Ä≤+ 1)B(h‚Ä≤+1), (54)
for any h‚Ä≤‚àà[0 :h]by using Eq. (53) where rt
t0:= 
rt
t0,1(j(1)),¬∑¬∑¬∑,rt
t0,K(j(K))
. By Lemma
D.5 and Lemma D.6, we have
I(Œ±, h+ 1, t0)‚â§g(Œì)Œ≤h+1(h+ 1)B(h+1)+1. (55)
Noting that
p(u) =CKX
i=0(‚àí1)iÔ£´
Ô£≠X
Œ±‚ààD:|Œ±|=iyŒ±ue‚àíŒ±Ô£∂
Ô£∏, (56)
and applying bound Eq. (55) to Eq. (50), we can derive
(Dh+1p(u))t0=
(a)|C|KX
i=0(‚àí1)iÔ£´
Ô£≠X
Œ±‚ààD:|Œ±|=iyŒ±h+1X
h‚Ä≤=1h+ 1
h‚Ä≤
(‚àí1)h‚Ä≤(ut0+h‚Ä≤)e‚àíŒ±Ô£∂
Ô£∏
‚â§|C|KX
i=0X
Œ±‚ààD:|Œ±|=i|y|Œ± 
Dh+1 
ue‚àíŒ±t0
‚â§|C|KX
i=0X
Œ±‚ààD:|Œ±|=i|y|Œ±(ut0)e‚àíŒ±g(Œì)Œ≤h+1(h+ 1)B(h+1)+1
‚â§g(Œì)Œ≤h+1(h+ 1)B(h+1)+1CKY
k=1(ut0(k) +|y(k)|), (57)
for any t0‚àà[1 :T‚àíh‚àí1]where (a) is derived from the equivalent expression Eq. (56) of the
polynomial p(u)and Eq. (15) of the finite difference (Dhf(x))t0w.r.t function frespectively.
23Recalling that we set parameters as follows
T‚â•4, H:=‚åàlog(T)‚åâ, Œ≤=1
8(Œò1+ Œò 2+ 1)H7/2,Œì =e2+ 322560Œò 2,
ÀÜK= maxHlog(4Œ≤‚àí1) + log(Œò 1)
log(Œ∏‚àí1), K0
, Œ∑=Œ≤
6e2ÀÜKŒì, B‚â•3,(58)
According to Theorem B.5, we have max
i‚àà[1:d](DhFi(x))t
‚àû‚â§Œ≤hH3h+1for each h‚àà[0 :H]and
t‚àà[1 :T‚àíh]. We are now prepared to prove that xt
isatisfies the second condition of Lemma D.7.
Lemma B.7. The sequence {xt
i}T
t=1which has been generated from Algorithm 1 is 7Œ∑(Œò1+ Œò 2)‚àí
consecutively close when H‚â•1,Œ≤0= (4H)‚àí1andŒ∑‚àà(0, Œ≤4
0(Œò1+ Œò 2+ 1)‚àí1/57792] .
Proof. According to the iteration of Algorithm 1, we have
xt+1
i(k) =xt
i(k)¬∑exp{Œ∑¬∑(2Fi(k)(xt)‚àíFi(k)(xt‚àí1))}Pni
k‚Ä≤=1xt
i(k‚Ä≤)¬∑exp{Œ∑¬∑(2Fi(k‚Ä≤)(xt)‚àí2Fi(k‚Ä≤)(xt‚àí1))}, (59)
for any i‚àà[1 :d]andk‚àà[1 :ni]. Therefore, for any i‚àà[1 :d]andt‚àà[1 :T‚àí1], we obtain
max(xt
i
xt+1
i
‚àû,xt+1
i
xt
i
‚àû)
‚â§exp{6Œ∑(Œò1+ Œò 2)}=
(a)(1 + 7 Œ∑(Œò1+ Œò 2)), (60)
where (a) is derived from the fact that exp(x)‚â§1 +7
6xforx‚àà[0,1/24].
B.1.3 The Last Step
With the preparatory work for proving Theorem 3.5 is completed, we now turn to providing the final
proof:
Proof of Theorem 3.5. Applying Theorem B.5 and Lemma B.7 to Lemma D.7, we have
TX
t=1Varxt
i(Fi(xt)‚àíFi(xt‚àí1))‚â§2Œ≤0TX
t=1Varxt
i(Fi(xt‚àí1)) + 165120Œò2(1 + 7 Œ∑Œò)H5+ 2.
(61)
According to the result of Lemma B.4, we obtain
TX
t=1(f(xt)‚àíf(x‚àó))‚â§dX
i=11
Œ≥i"
log(ni)
Œ∑‚àí 
ÀÜg2(Œ∑Œò)Œ∑Œò2‚àí2Œ≤0ÀÜg1(Œ∑Œò)Œ∑Œò2TX
t=1Varxt
i(Fi(xt))#
+ ÀÜg1(Œ∑Œò)Œ∑Œò2dX
i=11
Œ≥i[8Œ≤0Œò2+ 165120Œò2(1 + 7 Œ∑Œò)H5+ 2]. (62)
Combining Assumptions 3.3 and parameters selection Eq. (7), we complete the proof.
B.2 Simple Example
In this section, we provide the proof of Example 3.4 which satisfies GQC condition and Assumption
3.3.
Proof of Example 3.4. Recalling the objective function f(p,P) =1
2Ex,y(Pm
i=1piœÉ(x‚ä§Pi)‚àíy)2,
we have
f(p‚àó,P)‚àíf(p,P)‚â• ‚ü®Fp(p,P),p‚àó‚àíp‚ü©, (63)
24since f(¬∑,P)is convex for any fixed P. In addition, we obtain
f(p‚àó,P‚àó)‚àíf(p‚àó,P) =‚àí1
2E
(œÉ(x‚ä§P1)‚àíœÉ(x‚ä§P‚àó
1))2
(a)
‚â•BC
2E
‚ü®œÉ(x‚ä§P1)‚àíœÉ(x‚ä§P‚àó
1),x‚ä§(P‚àó
1‚àíP1)‚ü©
=BC
2E
(œÉ(x‚ä§P‚àó
1)‚àíy)x,P‚àó
1‚àíP1
, (64)
where BCis a constant depends on Cand (a) is derived from the fact that BC‚ü®exp{x1} ‚àí
exp{x2}, x1‚àíx2‚ü© ‚â• | exp{x1} ‚àíexp{x2}|2for any x1, x2‚àà[‚àíC, C]. Therefore, summing
up Eq. (63) and Eq. (64), we have that fsatisfies GQC condition with the internal functions
Fp={E[(Pm
j=1pjœÉ(x‚ä§Pj)‚àíy)]œÉ(x‚ä§Pi)}m
i=1for block pandFPi=E
(œÉ(x‚ä§Pi)‚àíy)x
for block Pi. Notice that Œ≥p= 1,Œ≥P1=BC
2andŒ≥Pi= 0 for any iÃ∏= 1. Furthermore, we
have‚à•DŒ±Fp(¬∑)‚à•‚àû‚â§2 exp{C}(2C)|Œ±|and‚à•DŒ±FPi(¬∑)‚à•‚àû‚â§exp{C}C|Œ±|+1by using and
x‚àà[‚àíC, C]d. According to Proposition B.2, we complete the proof.
There is also a toy example satisfying GQC condition and Assumption 3.3.
Example B.8.Assuming (p1,p2)‚àà‚àÜm√ó‚àÜn, the function f(p1,p2) =1
2‚à•p1p‚ä§
2‚à•2
Fsatisfies
GQC condition and Assumption 3.3 with the internal functions Fp1=‚à•p2‚à•2p1for block p1and
Fp2=p2for block p2.
Proof. We have1
2‚à•(p‚àó
1)‚ä§p2‚à•F‚àí1
2‚à•p‚ä§
1p2‚à•F‚â• ‚à•p2‚à•2p‚ä§
1(p‚àó
1‚àíp1)and1
2‚à•(p‚àó
1)‚ä§p‚àó
2‚à•F‚àí
1
2‚à•(p‚àó
1)‚ä§p2‚à•F‚â• ‚à•p‚àó
1‚à•2p‚ä§
2(p‚àó
2‚àíp2). Therefore, we have that fsatisfies GQC condition with
the internal functions Fp1=‚à•p2‚à•2p1for block p1andFp2=p2for block p2. Notice that Œ≥p1= 1
andŒ≥p2=‚à•p‚àó
1‚à•2. Since both ‚à•p2‚à•2p1andp2are polynomials with respect to (p1,p2), we derive
that the internal function of fsatisfies Assumption 3.3.
B.3 Application to Reinforcement Learning
B.3.1 Analysis of Infinite Horizon Reinforcement Learning
Proof of Proposition 3.6. The following performance difference lemma [ 30,13,2,35] plays an
important role in the policy gradient based model of infinite horizon reinforcement learning problems,
VœÄ‚àó(œÅ0)‚àíVœÄ(œÅ0) =Es‚àºdœÄ‚àó
œÅ0‚ü®AœÄ(s,¬∑),œÄ‚àó(¬∑|s)‚àíœÄ(¬∑|s)‚ü©. (65)
Letd=|S|,S={si}d
i=1and write 1/Œ≥i=dœÄ‚àó
œÅ0(si),Fi(œÄ) = QœÄ(si,¬∑). According
to Eq. (65) whose proof is given in Cheng et al. [13] and‚ü®AœÄ(si,¬∑),œÄ‚Ä≤(¬∑|si)‚àíœÄ(¬∑|s)‚ü©=
‚ü®QœÄ(si,¬∑),œÄ‚Ä≤(¬∑|si)‚àíœÄ(¬∑|s)‚ü©for any policy œÄandœÄ‚Ä≤, we obtain that
VœÄ‚àó(œÅ0)‚àíVœÄ(œÅ0) =dX
i=11
Œ≥i‚ü®Fi(œÄ),œÄ‚àó(¬∑|si)‚àíœÄ(¬∑|si)‚ü©. (66)
Eq.(66) implies that VœÄ(œÅ0)satisfies GQC condition. For every a‚àà A, the Taylor expansion of
QœÄ(si, a)up to K-th order at origin is the same as its truncation at horizon K, which indicates
RQœÄ(si,a)
K,0 (œÄ) =Œ∏K+1EsK+1[VœÄ(sK+1)|s0=si, a0=a]‚â§Œ∏K+1.
Therefore, according to the fact that
PQœÄ(si,a)
K,0 (œÄ)‚â§QœÄ(si, a)‚â§1,
we have that QœÄ(si,¬∑)satisfies Assumption 3.3 with Œò1=Œ∏,Œò2= 1andK0= 1.
25B.3.2 Analysis of Finite Horizon Reinforcement Learning
The function structure of finite horizon reinforcement learning on policy is strictly polynomial.
Moreover, since the action-value functions on horizon his only dependent of policy œÄh+1:H, we may
therefore verify that the objective function of finite horizon reinforcement learning satisfies GQC
condition by utilizing finite difference expansion on function error JœÄ
1(œÅ1)‚àíJœÄ‚àó
1(œÅ1).
The finite horizon reinforcement learning considers the following policy optimization problem:
min
œÄ‚ààXJœÄ
1(œÅ1), (67)
where JœÄ
1(œÅ1) =Es1‚àºœÅ1[VœÄ
1(s1)], andX=X1√ó ¬∑¬∑¬∑ √ó X H, and each Xhdenotes |Sh|probability
simplexes. We write Sh={sh,ih}|Sh|
ih=1for any h‚àà[1 :H]and denote the action-value vector on
state sh,ihat horizon hbyQœÄh+1:H
h(sh,ih,¬∑). According to the definition of finite horizon value
function VœÄh:H
h, we obtain the observation as Eq. (68).
JœÄ‚àó
1(œÅ1)‚àíJœÄ
1(œÅ1) =HX
h=1h
JœÄ‚àó
1:h,œÄh+1:H
1 (œÅ1)‚àíJœÄ‚àó
1:h‚àí1,œÄh:H
1 (œÅ1)i
=HX
h=1E
sh‚àºEs1‚àºœÅ1PrœÄ‚àó
1:h‚àí1
h(¬∑|s1)
QœÄh+1:H
h(sh,¬∑),œÄ‚àó
h(¬∑|sh)‚àíœÄh(¬∑|sh)
,
(68)
Since QœÄh+1:H
h(sh, ah)is a polynomial with respect to policy œÄ1:H, whose value is bounded by
1 +H‚àíhfor any sh‚àà Shandah‚àà A h, we derive that JœÄ
1(œÅ1)satisfies GQC condition with
internal function Fh,ih(œÄ) =QœÄh+1:H
h(sh,ih,¬∑)for variable block xh,ihwhere ih‚àà[1 :|Sh|],
andFsatisfies Assumption 3.3 with Œ∏= 0,Œò1= 0,Œò2=HandK0=H. Therefore, for
finite horizon reinforcement learning, it follows from Theorem 3.5 that Algorithm 1 with parameter
selection Eq. (7)finds an Œµ-suboptimal global solution in a number of iterations that is at most
O(Hmax h‚àà[0:H]log(|Ah|)Œµ‚àí1log4(Œµ‚àí1)).
C Minimax Optimization
We begin with showing the connection between GQCC condition and GQC condition. Without loss
of generality, we assume ni=nandmi=mfor any i‚àà[1 :d], and let ‚Ñì=n+m. Iff(¬∑,y)and
‚àíf(x,¬∑)satisfy GQC condition with respect to a pair of minimizers x‚àó(y)andy‚àó(x), respectively,
then we have the following estimations of function error
f(x,y)‚àíf(x‚àó(y),y)‚â§dX
i=11
Œ≥i(y)(fi(P(z),xi,yi)‚àífi(P(z),x‚àó(y)i,yi)), (69)
f(x,y‚àó(x))‚àíf(x,y)‚â§dX
i=11
œÑi(x)(fi(P(z),xi,y‚àó(x)i)‚àífi(P(z),xi,yi)), (70)
where fi(Q,zi) =‚ü®Qi,zi‚ü©for any Q‚ààR‚Ñì√ódandzi‚ààRn+m, and each Piincludes the internal
function of f(¬∑,y)for varriable block xiand the internal function of ‚àíf(x,¬∑)for variable block yi. It
follows from Eq. (69) and Eq. (70) that Eq. (10) holds for fwithœài(z) = max {1/Œ≥i(y),1/œÑi(x)}.
C.1 Preparatory Discussion
In this section, we provide the convergence analysis of general version of Algorithm 2, i.e., Algorithm
4. We consider the divergence-generating function vwith Bregman‚Äôs divergence V(i.e.,V(x,u) =
v(x)‚àív(u)‚àí ‚ü®‚àá v(u),x‚àíu‚ü©for any x,u) over general compact convex regions Z=X √ó Y ‚äÇ
RPd
i=1ni√óRPd
i=1mi. Before we introduce the main theorem, we need the following assumptions:
Assumption C.1. There exists positive constants A, D such that
[A1]max
i‚àà[1:d]{‚à•zi‚à•} ‚â§ Auniformly on Z.
26[A2]max(
max
zi‚ààZi
i‚àà[1:d] 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
,max
zi‚ààZi
i‚àà[1:d](v(xi) +v(yi)))
‚â§D.
[A3]vmodulus 2 with respect to ‚à• ¬∑ ‚à• (i.e.,‚àÄi‚àà[1 :d],V(xi,ui)‚â• ‚à•xi‚àíui‚à•2for any
xi,ui‚àà XiandV(yi,wi)‚â• ‚à•yi‚àíwi‚à•2for any yi,wi‚àà Yi).
If we choose v(x) =Pn
j=1x(j) log(x(j))and‚à• ¬∑ ‚à•=‚à• ¬∑ ‚à•1, then (1) in Assumption C.1 holds with
A= 2; (2) in Assumption C.1 holds with D= 2 max i‚àà[1:d]{log(ni) + log( mi)}; (3) in Assumption
C.1 holds following Pinsker‚Äôs inequality. According to Remark C.2, we state that there exist some
compact convex regions in RPd
i=1(ni+mi)with proper divergence-generating function vand proper
choice of g0satisfy Assumption C.1.
Remark C.2.If the feasible region Zis a compact set of Euclidean space, then it is reasonable
that assuming the divergence-generating function v(i.e., v(x) =Pn
j=1x(j) log(x(j))over the
probability simplex or v(x) =‚à•x‚à•2
2over the standard compact set) and the norm ‚à• ¬∑ ‚à• are uniformly
bounded on every Zi. For some Bregman divergences, if x0is a fixed point, V(¬∑,x0)can be
bounded by a constant (may depend on the dimension of space) on a compact feasible region,
such as V(¬∑,x0) =‚à• ¬∑ ‚àíx0‚à•2
2withx0=0on the closed ball BR(0)for radius R‚àà(0,‚àû)and
V(¬∑,x0) = KL( ¬∑‚à•x0)withx0= (1/n,¬∑¬∑¬∑,1/n)on the probability simplex ‚àÜn.
Assumption C.3. In Definition 4.1, let matrix-valued function Phas the form of P(Qz,z)
where Qz‚ààR‚Ñì√óddepends on z, and assume that Psatisfies the following properties on region 
Q‚ààR‚Ñì√ód‚à•Q‚à•‚àû‚â§C} √ó Z for some constant C >0:
[A4]There exist constants L1, L2‚â•0such that Fi(¬∑,zi)is uniformly L1-Lipschitz continuous
with respect to ‚à• ¬∑ ‚à•‚àóunder‚à• ¬∑ ‚à•‚àû, andFi(P,¬∑)is uniformly L2-Lipschitz continuous with
respect to ‚à• ¬∑ ‚à•‚àóunder‚à• ¬∑ ‚à•.
[A5]There are a positive constant Œ≥ >0and a pair of sets of matrices
{Bi}d
i=1,{Ci}d
i=1	
‚äÇ
R‚Ñì√ód
+‚à™0satisfyingPd
i=1(Bi+Ci)
‚àû‚â§Œ≥, such that the following bounds hold
DP(Q,¬∑,y)(x,x‚Ä≤)‚â§dX
i=1Ci‚ü®Fx
i(Q,zi),xi‚àíx‚Ä≤
i‚ü©,
DP(Q,x,¬∑)(y‚Ä≤,y)‚â§dX
i=1Bi‚ü®‚àíFy
i(Q,zi),y‚Ä≤
i‚àíyi‚ü©,
for any y,y‚Ä≤‚àà Y andx,x‚Ä≤‚àà X.
[A6]There exists Œ∏‚àà[0,1)such that P(¬∑,z)is aŒ∏-contraction mapping under ‚à• ¬∑ ‚à•‚àû, and
‚à•P(Q,z)‚à•‚àû‚â§Cfor any z‚àà Z.
Lemma C.4 (General Version of Lemma 4.3) .Assuming that Assumption C.1 and C.3 hold, [P(Q,¬∑,
¬∑)]k,jis continuous, and convex with respect to x, and concave with respect to yfor any (k, j), and
min
k,j
i‚àà[1:d]min{[Ci]k,j,[Bi]k,j}
[Ci]k,j+[Bi]k,j‚â•C‚Ä≤for some C‚Ä≤>0, then we claim that there exist Q‚àó‚ààR‚Ñì√ódand
z‚àó‚àà Z such that
Q‚àó=P(Q‚àó,x‚àó,y‚àó), (71)
Q‚àó‚â§P(Q‚àó,x,y‚àó), (72)
Q‚àó‚â•P(Q‚àó,x‚àó,y). (73)
Proof. We shall begin the proof by proving the following lemma.
Lemma C.5. Under the conditions of Lemma C.4, it can be proven that for any Q‚ààR‚Ñì√ód, there
exists a pair of x‚àó,y‚àóthat satisfy the following
P(Q,x‚àó,y)‚â§P(Q,x‚àó,y‚àó)‚â§P(Q,x,y‚àó). (74)
27Proof. Considering the following iteration
zt
i= argmin
zi‚ààZiŒ∑
Fi(Q,zt‚àí1
i),zi
+V 
xi,(gx
i)t‚àí1
+V 
yi,(gy
i)t‚àí1
,
gt
i= argmin
gi‚ààZiŒ∑
Fi(Q,zt
i),gi
+V 
gx
i,(gx
i)t‚àí1
+V 
gy
i,(gy
i)t‚àí1
,(75)
for any i‚àà[1 :d]and combining [57, Lemma 1], we have
[Ci]k,jTX
t=1
Fx
i(Q,zt
i),xt
i‚àíx‚Ä≤
i
+ [Bi]k,jTX
t=1
Fy
i(Q,zt
i),yt
i‚àíy‚Ä≤
i
‚â§([Ci]k,j+ [Bi]k,j)Œ∑‚àí1D+ [Ci]k,jTX
t=1Fx
i(Q,zt
i)‚àíFx
i(Q,zt‚àí1
i)
‚àóxt
i‚àí(gx
i)t
+ [Bi]k,jTX
t=1Fy
i(Q,zt
i)‚àíFy
i(Q,zt‚àí1
i)
‚àóyt
i‚àí(gy
i)t
‚àí[Ci]k,j
Œ∑TX
t=1xt
i‚àí(gx
i)t2+(gx
i)t‚àí1‚àíxt
i2
‚àí[Bi]k,j
Œ∑TX
t=1yt
i‚àí(gy
i)t2+(gy
i)t‚àí1‚àíyt
i2
‚â§
(a)([Ci]k,j+ [Bi]k,j)Œ∑‚àí1D+Œ∑L2
2([Ci]k,j+ [Bi]k,j)
2TX
t=1zt
i‚àízt‚àí1
i2
‚àímin{[Ci]k,j,[Bi]k,j}
Œ∑TX
t=11
4zt
i‚àígt
i2+1
2gt‚àí1
i‚àízt
i2
, (76)
for any i‚àà[1 :d],(k, j)‚àà[1 :‚Ñì]√ó[1 :d], andz‚Ä≤
i‚àà Z i, where (a) is derived from A4in
Assumption C.3 and Cauchy-Schwarz inequality. Therefore, by setting Œ∑=‚àö
C‚Ä≤
2L2and1
TPT
t=1zt=
¬ØzT= (¬ØxT,¬ØyT), the following estimation holds for any (k, j)
max
z‚Ä≤=(x‚Ä≤,y‚Ä≤)‚ààZ[P(Q,¬ØxT,y‚Ä≤)‚àíP(Q,x‚Ä≤,¬ØyT)]k,j
‚â§
(b)1
TTX
t=1
P(Q,xt,¬Øy‚àó
T)‚àíP(Q,¬Øx‚àó
T,yt)
k,j
‚â§
(c)1
TdX
i=1TX
t=1 
[Ci]k,j
Fi(Q,zt
i),xt
i‚àí(¬Øx‚àó
T)i
+ [Bi]k,j
Fi(Q,zt
i),yt
i‚àí(¬Øy‚àó
T)i
‚â§2Œ≥Œ∑‚àí1D+ 4Œ∑Œ≥A2L2
2
T, (77)
where the convexity of function [P(Q,¬∑,w)‚àíP(Q,u,¬∑)]k,jfor fixed Qandv= (u,w)implies
(b), and (c) is derived from Eq. (76) and the definition that (¬Øx‚àó
T,¬Øy‚àó
T) := argmax
z‚Ä≤‚ààZ[P(Q,¬ØxT,y‚Ä≤)‚àí
P(Q,x‚Ä≤,¬ØyT)]k,j. Since Zis a compact set, the sequence {(¬ØxT,¬ØyT)}‚àû
T=1must have a convergent
subsequence. Therefore, all accumulation points of the sequence {(¬ØxT,¬ØyT)}‚àû
T=1satisfy Eq. (74) by
using the continuity of P(Q,¬∑,¬∑).
Now, we define the iterately update as follows
Qt+1=P(Qt,x‚àó
t,y‚àó
t), (78)
28where (x‚àó
t,y‚àó
t)satisfies Eq. (74) in Lemma C.5 w.r.t P(Qt,¬∑,¬∑). It‚Äôs direct to derive that
Qt+1‚àíQt‚â§P(Qt,x‚àó
t‚àí1,y‚àó
t)‚àíP(Qt‚àí1,x‚àó
t‚àí1,y‚àó
t)
‚â§Œ∏Qt‚àíQt‚àí1
‚àû, (79)
Qt+1‚àíQt‚â•P(Qt,x‚àó
t,y‚àó
t‚àí1)‚àíP(Qt‚àí1,x‚àó
t,y‚àó
t‚àí1)
‚â• ‚àíŒ∏Qt‚àíQt‚àí1
‚àû. (80)
Finally, according to the contraction mapping principle, we complete the proof.
Corollary C.6. Assuming preconditions of Lemma C.4 hold, and letting {fi(Q,¬∑) :Rni+mi‚Üí
R}d
i=1be a sequence of continuous convex-concave functions which satisfies ‚àáfi(Q,¬∑) = (Fx
i(Q,¬∑),
‚àíFy
i(Q,¬∑))for any fixed Q‚ààR‚Ñì√ódandi‚àà[1 :d], then there exist a matrix Q‚àóand a pair of
(x‚àó,y‚àó)which satisfy Eq. (71)-Eq. (73) and
fi(Q‚àó,x‚àó
i,y‚àó
i)‚â•fi(Q‚àó,x‚àó
i,yi),
fi(Q‚àó,x‚àó
i,y‚àó
i)‚â§fi(Q‚àó,xi,y‚àó
i),
for any zi‚àà Ziandi‚àà[1 :d].
Proof. With proper selection of Œ∑, we have the following bound which is similar to that derived from
Eq.(77)
max
y‚Ä≤
i‚ààYifi(Q,(¬ØxT)i,y‚Ä≤
i)‚àímin
x‚Ä≤
i‚ààXifi(Q,x‚Ä≤
i,(¬ØyT)i)
‚â§TX
t=1[fi(Q,xt
i,(¬Øy‚àó
T)i)‚àífi(Q,(¬Øx‚àó
T)i,yt
i)]
‚â§TX
t=1
Fi(Q,zt
i),xt
i‚àí(¬Øx‚àó
T)i
+
Fi(Q,zt
i),yt
i‚àí(¬Øy‚àó
T)i
‚â§4Œ∑‚àí1D+ 8Œ∑A2L2
2
T, (81)
for every i‚àà[1 :d], where {zt= (xt,yt)}T
t=1follows from the iteration (75) and(¬Øz‚àó
T)i=
((¬Øx‚àó
T)i,(¬Øy‚àó
T)i)denotes argmax
z‚Ä≤
i‚ààZi[fi(Q,(¬ØxT)i,y‚Ä≤
i)‚àífi(Q,x‚Ä≤
i,(¬ØyT)i)]. Hence, by directly leveraging
the result of Lemma 4.3, we obtain the result.
Before stating the general version of Theorem 4.4 as follows, we define
YŒ∑
T= 8(c+ 1)
Œ≥D1
Œ∑+ 16Œ∑L2
+ 40Œ∑3Œ≥A2L4
2+ 2Œ∑Œ≥L2
1(1 + 64 Œ∑2L2
2C2)
(log(c+T) + 1) .
(82)
C.2 Theorem C.7 and Relate Proof
Theorem C.7. [General Version of Theorem 4.4] For any generaized quasar-convex-concave function
fwhich satisfies Assumption C.1 and C.3 with constant matrix function P‚â°Q‚àó, where Q‚àóis
unknown and satisfies Eq. (71)-Eq. (73), with parameter configuration in Eq. (12), the weighted
average of Algorithm 2‚Äôs outputs {zt}T
t=1satisfies the following inequality
Gf(¬ØxT,¬ØyT)‚â§6
max
z‚ààZPd
i=1œài(z)
(1‚àíŒ∏)‚àí1
3D
Œ∑+ 10Œ∑L2
1+ 5AL1YŒ∑
T+ 4Œ∑A2L2
2
T+ 3.(83)
For a generalized quasar-convex-concave function satisfying smoothness and recurrence conditions,
the iteration complexity of our algorithm matches the lower bound [ 52] for solving Œµ-approximate
Nash equilibrium points in the smooth convex-concave setting, up to a logarithmic factor. Furthermore,
we prove that standard smooth convex-concave functions satisfy the preconditions of Theorem C.7
(as discussed in Appendix C.3.2).
29Algorithm 4 Optimistic Mirror Descent with Regularization for Multi-Variables
Input:
z0
i	d
i=1=
g0
i	d
i=1,{Œ±t‚â•0}T
t=1(PT
t=1Œ±t= 1) ,{Œ≥t‚â•0}T
t=1,{Œªt‚â•0}T
t=1,Œ∑andQ0=0.
Output: ¬ØzT=PT
t=1Œ±tzt.
1:while t‚â§Tdo
2:Qt= (1‚àíŒ≤t‚àí1)Qt‚àí1+Œ≤t‚àí1P(Qt‚àí1,zt‚àí1).
3: for all i‚àà[1 :d]do
4: xt
i= argmin
xi‚ààXiŒ∑
Fx
i(Qt‚àí1,zt‚àí1
i),xi
+Œ≥tV 
xi,(gx
i)t‚àí1
+Œªtv(xi),
5: yt
i= argmin
yi‚ààYiŒ∑
Fy
i(Qt‚àí1,zt‚àí1
i),yi
+Œ≥tV 
yi,(gy
i)t‚àí1
+Œªtv(yi),
6: (gx
i)t= argmin
gx
i‚ààXiŒ∑
Fx
i(Qt,zt
i),gx
i
+Œ≥tV 
gx
i,(gx
i)t‚àí1
+Œªtv(gx
i),
7: (gy
i)t= argmin
gy
i‚ààYiŒ∑
Fy
i(Qt,zt
i),gy
i
+Œ≥tV 
gy
i,(gy
i)t‚àí1
+Œªtv(gy
i).
8: end for
9: t‚Üêt+ 1.
10:end while
Our analysis relies on the connection between Fi(Qt,zt
i)andFi(Q‚àó,zt
i). Theorem C.8 combines
a) classical O(log(T)/T)bound derived from regularized OMD, and b) the weighted average of
iteration error ‚à•Qt‚àíQt‚àí1‚à•2
‚àûovert‚àà[1 :T]which has the form ofPT
t=1Œ±t‚à•Qt‚àíQt‚àí1‚à•2
‚àû,
with magnitude O(T‚àí1log(T)), and c) weighted average of approximation error ‚à•Qt‚àíQ‚àó‚à•‚àûover
t‚àà[1 :T]which has the form ofPT
t=1Œ±t‚à•Qt‚àíQ‚àó‚à•‚àûto bound the max-min gap of fat¬ØzT. Next,
we leverage Lemma C.9 to show the decreasing trend of approximation error ‚à•Qt‚àíQ‚àó‚à•‚àûand
boundPT
t=1Œ±t‚à•Qt‚àíQ‚àó‚à•‚àûby a quantity that grows only logarithmically in T. We may therefore
obtain the result of Theorem C.7 by applying the estimation of weighted average of approximation
errorPT
t=1Œ±t‚à•Qt‚àíQ‚àó‚à•‚àûto Theorem C.8.
C.2.1 Part I
Theorem C.8. Assuming that Assumption C.1 holds, we set the hyper-parameters for Algorithm 2
carefully such that
Œ±t(Œ≥t+Œªt)‚â•Œ±t+1Œ≥t+1, (84)
Œ∑‚â§min
t‚àà[1:T]p
Œ≥t(Œ≥t+Œªt)
4L2. (85)
Suppose that vmodulus 2 w.r.t ‚à• ¬∑ ‚à•,‚à•z‚à• ‚â§Afor any z‚àà Z, and{zt}T
t=1follows the iterations of
Algorithm 4, then we can show that
max
y‚Ä≤‚ààYf(¬ØxT,y‚Ä≤)‚àímin
x‚Ä≤‚ààXf(x‚Ä≤,¬ØyT)‚â§B(œà) max
i‚àà[1:d]Œ±1Œ≥1
Œ∑max
zi‚ààZi 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
‚àí1
2Œ∑TX
t=1Œ±t
Œ≥t‚à•zt
i‚àígt‚àí1
i‚à•2+Œ≥t+Œªt
2‚à•gt
i‚àízt
i‚à•2
+2PT
t=1Œ±tŒªt
Œ∑max
zi‚ààZi(v(xi) +v(yi)))
+ 2B(œà)Œ∑L2
1TX
t=1Œ±t
Œ≥t+Œªt‚à•Qt‚àíQt‚àí1‚à•2
‚àû
+ 2AB(œà)L1TX
t=1Œ±t‚à•Qt‚àíQ‚àó‚à•‚àû+8A2B(œà)L2
2Œ±1Œ∑
Œ≥1+Œª1,
(86)
where B(œà) := max
z‚ààZPd
i=1œài(z)and¬ØzT:=PT
t=1Œ±tzt.
30Proof. Recalling the definition of GQCC, we derive that
max
y‚Ä≤‚ààYf(x,y‚Ä≤)‚àímin
x‚Ä≤‚ààXf(x‚Ä≤,y)‚â§B(œà) max
i‚àà[1:d]
max
wi‚ààYifi(Q‚àó,xi,wi)‚àímin
ui‚ààXifi(Q‚àó,ui,yi)
,(87)
for any z= (x,y),v= (u,w)‚àà Z and
fi(Q‚àó,(¬ØxT)i,wi)‚àífi(Q‚àó,ui,(¬ØyT)i)‚â§TX
t=1Œ±t
fi(Q‚àó,xt
i,wi)‚àífi(Q‚àó,ui,yt
i)
(88)
‚â§TX
t=1Œ±t
Fi(Q‚àó,zt
i),zt
i‚àívi
‚â§TX
t=1Œ±t
Fi(Qt,zt
i),zt
i‚àívi
+ 2AL1TX
t=1Œ±t‚à•Qt‚àíQ‚àó‚à•‚àû,
for any vi‚àà Zi. Using the optimality condition, we obtain

Fi(Qt‚àí1,zt‚àí1
i),zt
i‚àígt
i
‚â§Œ≥t
Œ∑ 
V 
(gx
i)t,(gx
i)t‚àí1
+V 
(gy
i)t,(gy
i)t‚àí1
‚àíŒ≥t
Œ∑ 
V 
xt
i,(gx
i)t‚àí1
+V 
yt
i,(gy
i)t‚àí1
‚àíŒ≥t+Œªt
Œ∑ 
V 
(gx
i)t,xt
i
+V 
(gy
i)t,yt
i
+Œªt
Œ∑ 
v 
(gx
i)t
+v 
(gy
i)t
‚àív 
xt
i
‚àív 
yt
i
, (89)

Fi(Qt,zt
i),gt
i‚àívi
‚â§Œ≥t
Œ∑ 
V 
ui,(gx
i)t‚àí1
+V 
wi,(gy
i)t‚àí1
‚àíŒ≥t
Œ∑ 
V 
(gx
i)t,(gx
i)t‚àí1
+V 
(gy
i)t,(gy
i)t‚àí1
‚àíŒ≥t+Œªt
Œ∑ 
V 
ui,(gx
i)t
+V 
wi,(gy
i)t
+Œªt
Œ∑ 
v(ui) +v(wi)‚àív 
(gx
i)t
‚àív 
(gy
i)t
. (90)
For each t‚àà[1 :T], we can apply Eq. (89) and Eq. (90) to the following equation
Œ±t
Fi(Qt,zt
i),zt
i‚àívi
=Œ±t
Fi(Qt,zt
i),gt
i‚àívi
+
Fi(Qt‚àí1,zt‚àí1
i),zt
i‚àígt
i
+
Fi(Qt,zt
i)‚àíFi(Qt‚àí1,zt‚àí1
i),zt
i‚àígt
i
,
‚â§Œ±tŒ≥t
Œ∑ 
V 
ui,(gx
i)t‚àí1
+V 
wi,(gy
i)t‚àí1
‚àíŒ≥t+Œªt
Œ∑ 
V 
ui,(gx
i)t
+V 
wi,(gy
i)t
‚àíŒ≥t
Œ∑ 
V 
xt
i,(gx
i)t‚àí1
+V 
yt
i,(gy
i)t‚àí1
‚àíŒ≥t+Œªt
Œ∑ 
V 
(gx
i)t,xt
i
+V 
(gy
i)t,yt
i
+Œ±tŒªt
Œ∑(v(ui) +v(wi)
‚àív 
(gx
i)t
‚àív 
(gy
i)t
+Œ±t
Fi(Qt,zt
i)‚àíFi(Qt‚àí1,zt‚àí1
i),zt
i‚àígt
i
.
(91)
31Therefore, by summing Eq.(91) from t= 1tot=Tand utilizing Eq. (84), we have
TX
t=1Œ±t
Fi(Qt,zt
i),zt
i‚àívi
‚â§Œ±1Œ≥1
Œ∑ 
V 
ui,(gx
i)0
+V 
wi,(gy
i)0
+2PT
t=1Œ±tŒªt
Œ∑max
zi‚ààZi(v(xi) +v(yi))
‚àí1
Œ∑TX
t=1Œ±t
Œ≥t‚à•zt
i‚àígt‚àí1
i‚à•2+Œ≥t+Œªt
2‚à•gt
i‚àízt
i‚à•2
+Œ∑TX
t=1Œ±t
Œ≥t+ŒªtFi(Qt,zt
i)‚àíFi(Qt‚àí1,zt‚àí1
i)2
‚àó
.(92)
According to the Lipschitz continuity of Fi, we derive that
Œ∑TX
t=1Œ±t
Œ≥t+ŒªtFi(Qt,zt
i)‚àíFi(Qt‚àí1,zt‚àí1
i)2
‚àó
‚â§2Œ∑L2
2TX
t=1Œ±t
Œ≥t+Œªt‚à•zt
i‚àízt‚àí1
i‚à•2
+ 2Œ∑L2
1TX
t=1Œ±t
Œ≥t+Œªt‚à•Qt‚àíQt‚àí1‚à•2
‚àû
. (93)
It follows from parameter setting Eq. (84) and Cauchy-Schwarz inequality that
Œ±tŒ≥t
2‚à•zt
i‚àígt‚àí1
i‚à•2+Œ±t‚àí1(Œ≥t‚àí1+Œªt‚àí1)
2‚à•gt‚àí1
i‚àízt‚àí1
i‚à•2‚â•Œ±tŒ≥t
4‚à•zt
i‚àízt‚àí1
i‚à•2. (94)
Combining Eq. (85) and Eq. (94), we may therefore obtain
‚àí1
Œ∑TX
t=1Œ±tŒ≥t
2‚à•zt
i‚àígt‚àí1
i‚à•2+Œ≥t+Œªt
4‚à•gt
i‚àízt
i‚à•2
+ 2Œ∑L2
2TX
t=1Œ±t
Œ≥t+Œªt‚à•zt
i‚àízt‚àí1
i‚à•2
‚â§2Œ∑L2
2Œ±1
Œ≥1+Œª1‚à•z1
i‚àíz0
i‚à•2. (95)
Applying Eq. (93) and Eq. (95) to Eq. (92) and utilizing Eq. (87), Eq. (88), we complete the proof.
C.2.2 Part II: Estimation of Approximation Error ‚à•Qt‚àíQ‚àó‚à•
According to the iterately update of Qt, we can derive the upper bound of weighted average of
‚à•Qt‚àíQt‚àí1‚à•2
‚àû. Next, we aim to bound ‚à•Qt‚àíQ‚àó‚à•for each iteration t. In this section, we select
the following parameter settings:
c= 2(1 ‚àíŒ∏)‚àí1, Œ∑‚â§(1‚àíŒ∏)1/2
8(Œ≥AL 1)1/2L2, Œ≤t=c
c+t, Œ±t=Œ≤T,t, Œ≥t=Œ±t‚àí1
Œ±t, Œªt= 1‚àíŒ≥t.(96)
Lemma C.9. Consider the settings: Œ≥t=Œ±t‚àí1
Œ±t‚â§1, Œªt= 1‚àíŒ≥t, and Œ∑‚â§(1‚àíŒ∏)1/2
8(Œ≥AL 1L2)1/2. Then, we
obtain the estimation of ‚à•Qt‚àíQ‚àó‚à•as follows,
‚à•Qt‚àíQ‚àó‚à•‚àû‚â§tX
j=2Œ≤(1+Œ∏)/2
t,j Hj, (97)
for any t‚â•2where
Hj:=Œ≥D1
Œ∑+ 16Œ∑L2 
Œ≤j‚àí1,1Œ≥1+ 2j‚àí1X
Œ∫=1Œ≤j‚àí1,Œ∫ŒªŒ∫!
+ 2Œ∑Œ≥L2
1(1 + 64 Œ∑2L2
2C2)j‚àí1X
Œ∫=1Œ≤j‚àí1,Œ∫Œ≤2
Œ∫‚àí1+ 128 Œ∑3Œ≥A2L4
2Œ≤j‚àí1,1. (98)
32Proof. According to the fact that Q‚àóis a fixed point of function P, we have
Qt‚àíQ‚àó=t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫[P(QŒ∫,zŒ∫)‚àíP(Q‚àó,z‚àó)]
‚â§
(a)t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫{[P(QŒ∫,xŒ∫,yŒ∫)‚àíP(QŒ∫,x‚àó,yŒ∫)] + [P(QŒ∫,x‚àó,yŒ∫)‚àíP(Q‚àó,x‚àó,yŒ∫)]}
‚â§dX
i=1 t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚ü®Fx
i(QŒ∫,zŒ∫
i),xŒ∫
i‚àíx‚àó
i‚ü©!
Ci+Œ∏ t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚à•QŒ∫‚àíQ‚àó‚à•‚àû!
ede‚ä§
d.
(99)
Where (a) is derived from the maximizer‚Äôs property of y‚àófor matrix-valued function P(Q‚àó,x‚àó,¬∑).
Similarly, we can obtain
Qt‚àíQ‚àó‚â• ‚àídX
i=1 t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚ü®Fy
i(QŒ∫,zŒ∫
i),yŒ∫
i‚àíy‚àó
i‚ü©!
Bi‚àíŒ∏ t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚à•QŒ∫‚àíQ‚àó‚à•‚àû!
ede‚ä§
d.
(100)
Hence, we derive
‚à•Qt‚àíQ‚àó‚à•‚àû‚â§Œ≥max
i‚àà[1:d]Œ≤t‚àí1,1Œ≥1
Œ∑max
zi‚ààZi 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
+2Pt‚àí1
Œ∫=1Œ≤t‚àí1,Œ∫ŒªŒ∫
Œ∑max
zi‚ààZi(v(xi) +v(yi))
+2Œ∑L2
2t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫
Œ≥Œ∫+ŒªŒ∫zŒ∫
i‚àízŒ∫‚àí1
i2)
+ 2Œ∑Œ≥L2
1t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫
Œ≥Œ∫+ŒªŒ∫‚à•QŒ∫‚àíQŒ∫‚àí1‚à•2
‚àû+Œ∏ t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚à•QŒ∫‚àíQ‚àó‚à•‚àû!
,
(101)
by combining Œ≤t‚àí1,Œ∫QT
j=t(1‚àíŒ≤j) = Œ±Œ∫and Eq. (99), and using the proof technique of
Theorem C.8. Next, for any i‚àà[1 : d], we can obtain an upper bound estimation of
max
vi‚ààZiPt‚àí1
Œ∫=1Œ≤t‚àí1,Œ∫‚ü®Fi(QŒ∫,zŒ∫
i),zŒ∫
i‚àívi‚ü©as follows
max
vi‚ààZit‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚ü®Fi(QŒ∫,zŒ∫
i),zŒ∫
i‚àívi‚ü© ‚â§D
Œ∑ 
Œ≤t‚àí1,1Œ≥1+ 2t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫ŒªŒ∫!
+ 8Œ∑A2L2
2Œ≤t‚àí1,1
+ 8Œ∑L2
1C2t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫Œ≤2
Œ∫‚àí1‚àí1
8Œ∑t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫zk
Œ∫‚àízk
Œ∫‚àí12.
(102)
Furthermore, we also have a lower bound estimation of it
max
vi‚ààZit‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚ü®Fi(QŒ∫,zŒ∫
i),zŒ∫
i‚àívi‚ü© ‚â•max
vi‚ààZit‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚ü®Fi(QŒ∫,vi),zŒ∫
i‚àívi‚ü©
‚â•max
vi‚ààZit‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚ü®Fi(Q‚àó,vi),zŒ∫
i‚àívi‚ü©
‚àí2AL1t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚à•QŒ∫‚àíQ‚àó‚à•‚àû
‚â• ‚àí2AL1t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚à•QŒ∫‚àíQ‚àó‚à•‚àû. (103)
33Therefore, combining Eq. (102) and Eq. (103), we derive the following result
t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫zŒ∫
i‚àízŒ∫‚àí1
i2‚â§16Œ∑AL 1t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚à•QŒ∫‚àíQ‚àó‚à•‚àû+ 64Œ∑2A2L2
2Œ≤t‚àí1,1
+ 8D 
Œ≤t‚àí1,1Œ≥1+ 2t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫ŒªŒ∫!
+ 64Œ∑2C2L2
1t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫Œ≤2
Œ∫‚àí1,
(104)
for any i‚àà[1 :d].
‚à•Qt‚àíQ‚àó‚à•‚àû‚â§Œ≥D1
Œ∑+ 16Œ∑L2
2 
Œ≤t‚àí1,1Œ≥1+ 2t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫ŒªŒ∫!
+ 128 Œ∑3Œ≥A2L4
2Œ≤t‚àí1,1
+ 2Œ∑Œ≥L2
1 
1 + 64 Œ∑2C2L2
2t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫Œ≤2
Œ∫‚àí1
+ (32 Œ∑2Œ≥AL 1L2
2+Œ∏)t‚àí1X
Œ∫=1Œ≤t‚àí1,Œ∫‚à•QŒ∫‚àíQ‚àó‚à•‚àû. (105)
Finally, by applying [67, Lemma 33] to Eq. (105), we complete the proof.
Under parameter settings Eq. (96), the following auxiliary Lemma C.10 provides both lower bound
and upper bound of Œ≤T,t.
Lemma C.10. Assuming that Œ≤t=c‚Ä≤
c+tandc‚â•c‚Ä≤, we can obtain the following result:
exp
‚àí(c‚Ä≤+c‚Ä≤c)2
2c(c+t)c‚Ä≤‚àí1
(c+T)c‚Ä≤‚â§Œ≤T,t‚â§(1 +c)(c+t+ 1)c‚Ä≤‚àí1
(c+T+ 1)c‚Ä≤ , (106)
for any T‚â•t‚â•1.
Proof. Recalling that
Œ≤T,t=c‚Ä≤
c+tTY
k=t+1
1‚àíc‚Ä≤
c+k
=c‚Ä≤
c+texp(TX
k=t+1log
1‚àíc‚Ä≤
c+k)
, (107)
we have
Œ≤T,t‚â§c‚Ä≤
c+tc+t+ 1
c+T+ 1c‚Ä≤
‚â§(1 +c)(c+t+ 1)c‚Ä≤‚àí1
(c+T+ 1)c‚Ä≤ , (108)
and
Œ≤T,t‚â•exp
‚àí(c‚Ä≤+c‚Ä≤c)2
2c(c+t)c‚Ä≤‚àí1
(c+T)c‚Ä≤, (109)
by combining the result of Lemma D.9.
Corollary C.11. Assuming that Œ≤t=c‚Ä≤
c+t,c‚â•1andc‚Ä≤(1‚àíŒ∏)‚â•1, we can obtain
Œ≤Œ∏
T,t‚â§c‚Ä≤
c+T, (110)
for any T‚â•t‚â•1.
By utilizing the result of Lemma C.10, we notice that
j‚àí1X
Œ∫=1Œ≤j‚àí1,Œ∫ŒªŒ∫‚â§j‚àí1X
Œ∫=1(1 +c)2(c+Œ∫+ 1)c‚àí2
(c+j)c
‚â§
(a)(1 +c)2
(c+j)cZj‚àí1
1(c+x+ 1)c‚àí2dx+(1 +c)2
(c+j)2
‚â§(1 +c)2
(c‚àí1)(c+j)+(1 +c)2
(c+j)2, (111)
34and
j‚àí1X
Œ∫=1Œ≤j‚àí1,Œ∫Œ≤2
Œ∫‚àí1‚â§j‚àí1X
Œ∫=1(1 +c)4(c+Œ∫+ 1)c‚àí3
c(c+j)c
‚â§
(b)(1 +c)3
c(c+j)cZj‚àí1
1(c+x+ 1)c‚àí2dx+(1 +c)4
c(c+j)3
‚â§(1 +c)3
c(c‚àí1)(c+j)+(1 +c)4
c(c+j)3, (112)
where (a) and (b) are derived from the fact thatPj‚àí2
Œ∫=1(c+Œ∫+ 1)c‚àí2‚â§Rj‚àí1
1(c+x+ 1)c‚àí2dx.
Next, we have
Hj‚â§Œ≥D1
Œ∑+ 16Œ∑L22(c+ 2)c‚àí1
(c+j)c+2(1 + c)2
(c‚àí1)(c+j)+2(1 + c)2
(c+j)2
+ 2Œ∑Œ≥L2
1(1 + 64 Œ∑2L2
2C2)(1 +c)3
c(c‚àí1)(c+j)+(1 +c)4
c(c+j)3
+ 128 Œ∑3Œ≥A2L4
2(c+ 2)c
(c+j)c, (113)
and
tX
j=2Œ≤(1+Œ∏)/2
t,j Hj‚â§
cc
c+t
Œ≥D1
Œ∑+ 16Œ∑L2Zt
22(c+ 2)c‚àí1
(c+x)cdx
+Zt
12(1 + c)2
(c‚àí1)(c+x)+2(1 + c)2
(c+x)2
dx+ 1
+2Œ∑Œ≥L2
1(1 + 64 Œ∑2L2
2C2)Zt
1(1 +c)3
c(c‚àí1)(c+x)+(1 +c)4
c(c+x)3
dx
+ 128 Œ∑3Œ≥A2L4
2
1 +Zt
2(c+ 2)c
(c+x)cdx
‚â§c
c+t
Œ≥D1
Œ∑+ 16Œ∑L22(c+ 1)2
c‚àí1log(c+t) + 5 + 2 c
+ 640 Œ∑3Œ≥A2L4
2
+ 2Œ∑Œ≥L2
1(1 + 64 Œ∑2L2
2C2)2(c+ 1)2
c‚àí1log(c+t) +(c+ 1)2
2c
,
(114)
where (c) follows from Corollary C.11. For simplicity, we denote
YŒ∑
T:= 8( c+ 1)
Œ≥D1
Œ∑+ 16Œ∑L2
+ 40Œ∑3Œ≥A2L4
2+ 2Œ∑Œ≥L2
1(1 + 64 Œ∑2L2
2C2)
(log(c+T) + 1) .
(115)
Therefore, it follows from Eq. (114) and Lemma C.9 that
‚à•Qt‚àíQ‚àó‚à•‚àû‚â§tX
j=2Œ≤(1+Œ∏)/2
t,j Hj‚â§c
c+tYŒ∑
t. (116)
C.2.3 The Last Step
According to Eq. (116) and the initial Q0satisfies ‚à•Q0‚à•‚àû‚â§C, we have ‚à•Q‚àó‚à• ‚â§C. We are ready
to complete the proof of Theorem C.7.
Proof of Theorem C.7. It is noteworthy that the hyper-parameters selected in Eq. (96) satisfies the
preconditions of Theorem C.8. Combining the conclusion of Theorem C.8, Eq. (116) and the
35estimation of Œ±t(i.e.Œ≤T,t) in Lemma C.10, we obtain:
Gf(x,y)‚â§Bmax
i‚àà[1:d]Œ±1Œ≥1
Œ∑max
zi‚ààZi 
V 
xi,(gx
i)0
+V 
yi,(gy
i)0
+2PT
t=1Œ±tŒªt
Œ∑max
zi‚ààZi(v(xi) +v(yi)))
+ 2Œ∑BL2
1TX
t=1Œ±tŒ≤2
t‚àí1
+ 2ABcL 1YŒ∑
TTX
t=1Œ±t
c+t+ 8Œ∑A2BL2
2Œ±1
‚â§
a2BD
Œ∑(c+ 2)c‚àí1
(c+T+ 1)c+c(c+ 2)
(c+T+ 1)2+2(c+ 1)
c+T+ 1
+ 2Œ∑BL2
1(c+ 1)3
c(c‚àí1)(c+T+ 1)+(c+ 1)4
c(c+T+ 1)3
+ 2ABL 1YŒ∑
T4c
c+T+ 1+c(c+ 2)
(c+T+ 1)2
+ 8Œ∑A2BL2
2(1 +c)(c+ 2)c‚àí1
(c+T+ 1)c
‚â§2B3D
Œ∑+ 10Œ∑L2
1+ 5AL1YŒ∑
T+ 4Œ∑A2L2
2c+ 1
c+T+ 1, (117)
when T‚â•1, where Bdenotes maxz‚ààZPd
i=1œài(z)and (a) is derived from parameter settings
Eq. (96) and the result of Lemma C.10.
C.3 Application to Minimax Problems
C.3.1 Infinite Horizon Two-Player Zero-Sum Markov Games
To simplify notations, in the following discussion, we write S={si}|S|
i=1, and denote by Qz= 
Qz(s1,¬∑,¬∑),¬∑¬∑¬∑,Qz(s|S|,¬∑,¬∑)
the joint action-value matrix where Qz(si,¬∑,¬∑)‚ààR|A|√ó|B|is an
action-value matrix on state si. According to the connection between value function and action-value
function
Vz(si) =Ea‚àºx(¬∑|si)
b‚àºy(¬∑|si)[Qz(si, a, b)], Qz(si, a, b) = (1 ‚àíŒ∏)œÉ(si, a, b) +Œ∏Esi‚Ä≤‚àºP(¬∑|si,a,b)[Vz(si‚Ä≤)],
we provide the following proof for Proposition 4.5.
Proof of Proposition 4.5. By defining
[Pi(Q,z)]a,b:= (1‚àíŒ∏)œÉ(si, a, b) +Œ∏Esi‚Ä≤‚àºP(¬∑|si,a,b)[‚ü®Qi‚Ä≤yi‚Ä≤,xi‚Ä≤‚ü©], (118)
we derive that Qz=P(Qz,z). We can notice that
[Pi(Q,x,y)‚àíPi(Q,x‚Ä≤,y)]a,b=Œ∏Esi‚Ä≤‚àºP(¬∑|si,a,b)[‚ü®Qi‚Ä≤yi‚Ä≤,xi‚Ä≤‚àí(x‚Ä≤)i‚Ä≤‚ü©],
[Pi(Q,x,y‚Ä≤)‚àíPi(Q,x,y)]a,b=Œ∏Esi‚Ä≤‚àºP(¬∑|si,a,b)
‚ü®‚àíQ‚ä§
i‚Ä≤xi‚Ä≤,yi‚Ä≤‚àí(y‚Ä≤)i‚Ä≤‚ü©
.(119)
Therefore, for any Qsatisfies ‚à•Q‚à•‚àû‚â§1, it‚Äôs easy to verify that
1.Fi(¬∑,zi)is uniformly 2-Lipschitz continuous with respect to ‚à• ¬∑ ‚à•‚àûunder‚à• ¬∑ ‚à•‚àûfor any
zi‚àà Zi, andFi(Q,¬∑)is uniformly 1-Lipschitz continuous with respect to ‚à• ¬∑ ‚à•‚àûunder
‚à• ¬∑ ‚à• 1, since F(Q,zi) = 
y‚ä§
iQ‚ä§
i,‚àíx‚ä§
iQi‚ä§,
2.Psatisfies [A2]in Assumptions 4.2 with [Bi]s,a,b= [Ci]s,a,b=Œ∏P(si|s, a, b )andŒ≥= 2Œ∏,
since Eq. (119),
3.P(¬∑,z)is aŒ∏-contraction mapping under ‚à• ¬∑ ‚à•‚àû, and‚à•P(¬∑,¬∑)‚à•‚àû‚â§1, since the definition
ofP,
4.[Pi(Q,¬∑,¬∑)]a,bis bi-linear with respect to xandy, andmin{[Ci]s,a,b,[Bi]s,a,b}
[Ci]s,a,b+[Bi]s,a,b‚â°1/2for any
iands, a, b .
36Therefore, according to Lemma 4.3, there exist a tensor Q‚àóand a pair of (x‚àó,y‚àó)satisfy Eq. (11).
Furthermore, the (x‚àó,y‚àó)mentioned above is a Nash equilibrium of Jx,y(œÅ0)by utilizing Corollary
C.6. We may therefore derive that Q‚àó‚â°Qz‚àó. Leveraging Eq. (65) for any Nash equilibrium
(x‚àó,y‚àó)‚àà Z and denoting Q‚àó
i=Qx‚àó,y‚àó(si,¬∑,¬∑), we have
Jx‚àó,y‚àó(œÅ0)‚àíJx‚àó(y),y(œÅ0) =X
s‚ààSdx‚àó(y),y
œÅ0(s)h
‚ü®Qx‚àó,y‚àó(s,¬∑,¬∑)y‚àó(¬∑|s),x‚àó(¬∑|s)‚ü© (120)
‚àí‚ü®Qx‚àó,y‚àó(s,¬∑,¬∑)y(¬∑|s),x‚àó(y)(¬∑|s)‚ü©i
‚â§|S|X
i=1dx‚àó(y),y
œÅ0(si)
‚ü®Q‚àó
iy‚àó
i,x‚àó
i‚ü© ‚àímin
ui‚ààXi‚ü®Q‚àó
iyi,ui‚ü©
, (121)
where x‚àó(y) = argmin
u‚ààXJu,y(œÅ0)andy‚àó(x) = argmax
w‚ààYJx,w(œÅ0). Similarly, we have
Jx,y‚àó(x)(œÅ0)‚àíJx‚àó,y‚àó(œÅ0)‚â§|S|X
i=1dx,y‚àó(x)
œÅ0(si)
max
wi‚ààYi‚ü®(Q‚àó
i)‚ä§xi,wi‚ü© ‚àí ‚ü®(Q‚àó
i)‚ä§x‚àó
i,y‚àó
i‚ü©
.
(122)
By setting œài(z) := max {dx,y‚àó(x)
œÅ0(si),dx‚àó(y),y
œÅ0(si)}and combining the facts that fi(Q‚àó,x‚àó
i,y‚àó
i)‚àí
min
ui‚ààXifi(Q‚àó,ui,yi)‚â•0andmax
wi‚ààYifi(Q‚àó,xi,wi)‚àífi(Q‚àó,x‚àó
i,y‚àó
i)‚â•0derived from Corollary
C.6, we have
Jx,y‚àó(x)(œÅ0)‚àíJx‚àó(y),y(œÅ0)‚â§dX
i=1œài(z)
max
wi‚ààYifi(Q‚àó,xi,wi)‚àímin
ui‚ààXifi(Q‚àó,ui,yi)
.
(123)
C.3.2 Convex-Concave Minimax Problems
In this section, we consider convex-concave minimax problem over compact concave region Z=
X √óY ‚äÇ RPd
i=1ni√óRPd
i=1miwhich satisfies Assumption C.1 with divergence-generating function
v. The standard convex-concave minimax problem is formulated as follows:
min
x‚ààXmax
y‚ààYf(x,y), (124)
where fis convex with respect to xand concave with respect to y. Therefore, we obtain that
f(x,y‚àó(x))‚àíf(x‚àó(y),y)‚â§ ‚ü®‚àá xf(z),x‚àíx‚àó(y)‚ü©+‚ü®‚àí‚àá yf(z),y‚àíy‚àó(x)‚ü©, (125)
for any z= (x,y)‚àà Z. We may therefore derive that fsatisfies GQCC condition with g(z)‚â°1
andf(P(z),z) =f(z). Furthermore, assuming ‚àáfisL-lipschitz continuous (i.e., ‚à•‚àáf(z)‚àí
‚àáf(v)‚à•‚àó‚â§L‚à•z‚àív‚à•for any z,v‚àà Z ) and choosing P‚â°0, then verifying that fsatisfies
the preconditions of general version of Theorem C.7 is reduced to verifying that fsatisfies (1) in
Assumption C.3. Since F=‚àáfonly depends on variable z, it is evident that fsatisfies (1) in
Assumption C.3 when ‚àáfis L-Lipschitz. Therefore, under the smoothness condition of f, Theorem
C.7 implies that O(Œµ‚àí1)iterations Algorithm 4 needs to find an Œµ-approximate Nash equilibrium
offmatches the lower bounds of ‚Ñ¶(Œµ‚àí1)[52] for the number of iterations that any deterministic
first-order method requires to find an Œµ-approximate Nash equilibrium of a smooth convex-concave
function.
D Auxiliary Lemma
Lemma D.1. ForŒì‚â•17, the function g(Œì)can be bounded by80640
Œì‚àí1+2
Œìe‚àí2‚àí1. Letg(Œì)be defined
asP‚àû
k=1Œì‚àík[k7+ (k+ 1) exp {2k}].
37Proof.
g(Œì)‚â§‚àûX
k=1"
Œì‚àík(k+ 7)!
k!+e2
Œìk
(k+ 1)#
=d7
dŒ±7Œ±8
1‚àíŒ±
Œ±=Œì‚àí1+d
dŒ±Œ±2
1‚àíŒ±
Œ±=e2Œì‚àí1
‚â§
(a)80640
Œì‚àí1+2
Œìe‚àí2‚àí1, (126)
(a) can be deduced based on the following inequality
d7
dŒ±7Œ±8
1‚àíŒ±
=7X
k=0(‚àí1)k
7
k8!k!
(k+ 1)!Œ±
1‚àíŒ±k+1
‚â§
(b)7!8X
k=1
8
kŒ±
1‚àíŒ±k
=7!"
1 +Œ±
1‚àíŒ±8
‚àí1#
‚â§7!
exp8Œ±
1‚àíŒ±
‚àí1
‚â§
(c)80640 Œ±
1‚àíŒ±, (127)
where (b) and (c) are derived from Leibniz equation, and the inequality ex‚àí1‚â§2xholds for
0‚â§x‚â§1/2respectively.
Lemma D.2. For any n‚ààN,r‚ààRn,p‚àà‚àÜn, if it holds that p‚àó= arg min
p‚àà‚àÜnŒ∑‚ü®p,r‚ü©+ KL( p‚à•q),
then we have
‚ü®p‚àó‚àíp,r‚ü©=1
Œ∑(KL(p‚à•q)‚àíKL(p‚à•p‚àó)‚àíKL(p‚àó‚à•q)). (128)
Proof. We just need to prove p‚àó(i)‚â°p‚Ä≤(i) :=q(i) exp{‚àíŒ∑r(i)}Pn
j=1q(j) exp{‚àíŒ∑r(j)}for any i‚àà[n]which satisfies
‚ü®p‚àíp‚Ä≤, Œ∑r+ log( p‚Ä≤)‚àílog(q)‚ü©= 0, (129)
for any p‚àà‚àÜn. Assume that F(p) :=Œ∑‚ü®p,r‚ü©+ KL( p‚à•q)and define E(p) =Pn
i=1p(i) log(p(i))
for any p‚àà‚àÜn. Clearly, p‚Ä≤‚àà‚àÜn. Hence, for all p‚àà‚àÜn,
F(p) =Œ∑‚ü®p,r‚ü©+ KL( p‚à•q)
=Œ∑‚ü®p‚Ä≤,r‚ü©+ KL( p‚Ä≤‚à•q) +‚ü®p‚àíp‚Ä≤, Œ∑r‚àílog(q)‚ü©+E(p)‚àí E(p‚Ä≤)
=
(a)Œ∑‚ü®p‚Ä≤,r‚ü©+ KL( p‚Ä≤‚à•q) +E(p)‚àí E(p‚Ä≤) +‚ü®p‚àíp‚Ä≤,‚àílog(p‚Ä≤)‚ü©
=
(b)F(p‚Ä≤)‚àíKL(p‚à•p‚Ä≤), (130)
where (a) is derived from Eq. (129) . Therefore, we obtain that p‚àó‚â°p‚Ä≤. By using equality (b), we
finish the proof.
Lemma D.3. Suppose that for œÑ‚àà(0,1), we havep
q
‚àû‚â§1 +œÑ. Then
1‚àíœÑ
2‚àí2œÑ
3(1‚àíœÑ)
X2(p,q)‚â§KL(p‚à•q).
38Proof. We consider the Taylor expansion of the function log(1 + x) =P‚àû
k=1(‚àí1)k‚àí1
kxkand define
QœÑ,D(x) :=x‚àí 1
2+DœÑ
x2. According to
log(1 + x)‚àíQœÑ,D(x)‚â•DœÑx2‚àí|x|3
3(1‚àíœÑ), (131)
for any x‚àà[‚àíœÑ, œÑ], we have log(1 + x)‚â•QœÑ,D(x)when D‚â•1
3(1‚àíœÑ)andx‚àà[‚àíœÑ, œÑ]. Therefore,
we obtain
KL(p‚à•q) =nX
j=1p(j) logp(j)
q(j)
‚â•nX
j=1p(j)"p(j)
q(j)‚àí1
‚àí1
2+DœÑp(j)
q(j)‚àí12#
=X2(p,q)‚àí1
2+DœÑnX
j=1p(j)
q(j)q(j)p(j)
q(j)‚àí12
‚â•X2(p,q)‚àí1 +œÑ
2+DœÑ(1 +œÑ)
X2(p,q)
=1‚àíœÑ
2‚àíDœÑ(1 +œÑ)
X2(p,q). (132)
We complete the proof if D=1
3(1‚àíœÑ).
Lemma D.4. Suppose that r‚ààRn, œÑ‚àà(0,1/2),‚à•r‚à•‚àû‚â§œÑ
2,andp,Àúp‚àà‚àÜnsatisfy, for each
j‚àà[n],
Àúp(j) =p(j)¬∑exp{r(j)}P
j‚Ä≤‚àà[n]p(j‚Ä≤)¬∑exp{r(j‚Ä≤)}. (133)
Then
1‚àí2
3(1‚àíœÑ)+ 4
œÑ
Varp(r)‚â§ X2(Àúp,p)‚â§
1 +2
3(1‚àíœÑ)+ 4
œÑ
Varp(r).(134)
Proof. Without loss of generality, we consider the case ‚ü®p,r‚ü©= 0. If not, redefine Àúr:=r‚àí ‚ü®p,r‚ü© ¬∑
e(‚à•Àúr‚à•‚àû‚â§œÑ)and analyze Àúrwhere e‚ààRnis an all 1 vector. It‚Äôs clear that
X2(Àúp,p) =‚àí1 +nX
j=1p(j)Àúp(j)
p(j)2
=‚àí1 +Epexp{r}
Ep[exp{r}]2
. (135)
We define F1
D(x) := 1 + x+1‚àíDœÑ
2x2, F2
D(x) := 1 + x+1+DœÑ
2x2and note that for any x‚àà[‚àíœÑ, œÑ]
exp{x} ‚àíF1
D(x)‚â•DœÑ
2x2‚àíx3
6, (136)
F2
D(x)‚àíexp{x} ‚â•DœÑ
2x2‚àí|x|3
6(1‚àíœÑ), (137)
where Eq. (136) is derived from the summation of the 2k-th and 2k+ 1-th (k‚â•2) terms in the Taylor
expansion of exp{x}is always non-negative, Eq. (137) is derived fromP‚àû
k=3xk
k!‚â§|x|3
6(1‚àíx)‚â§|x|3
6(1‚àíœÑ)
for any x‚àà[‚àíœÑ, œÑ]. Therefore, we have exp{x} ‚àíF1
D(x)‚â•0andF2
D(x)‚àíexp{x} ‚â•0for all
x‚àà[‚àíœÑ, œÑ]ifD‚â•1
3(1‚àíœÑ). Then, we have
1 + 2 x+ (2‚àí(D+ 2)œÑ)x2‚â§(exp{x})2‚â§1 + 2 x+ (2 + ( D+ 2)œÑ)x2, (138)
when DœÑ‚â§1
2. In addition, by ‚ü®p,r‚ü©= 0, it‚Äôs obvious that
1 +1‚àíDœÑ
2Ep[r2]‚â§Ep[exp{r}]‚â§1 +1 +DœÑ
2Ep[r2]. (139)
39Combining Eq. (138) and (139), we derived that
1 + (1 ‚àí(D+ 1)œÑ)Ep[r2]‚â§(Ep[exp{r}])2‚â§1 + (1 + ( D+ 1)œÑ)Ep[r2], (140)
1 + (2 ‚àí(D+ 2)œÑ)Ep[r2]‚â§Ep
(exp{r})2
‚â§1 + (2 + ( D+ 2)œÑ)Ep[r2], (141)
forDœÑ‚â§1
2. According to Eq. (135) ,(140) and (141), we have
‚àí1 +Epexp{r}
Ep[exp{r}]2
‚â•(1‚àí(2D+ 3)œÑ)Ep[r2]
1 + (1 + ( D+ 1)œÑ)Ep[r2]‚â•(1‚àí(2D+ 4)œÑ)Ep[r2],
‚àí1 +Epexp{r}
Ep[exp{r}]2
‚â§(1 + (2 D+ 3)œÑ)Ep[r2]
1 + (1 ‚àí(D+ 1)œÑ)Ep[r2]‚â§(1‚àí(2D+ 4)œÑ)Ep[r2].
We derive Eq. (134) by setting D=1
3(1‚àíœÑ).
Lemma D.5 (Lemma B.6, [15]) .Letœï1,¬∑¬∑¬∑, œïlbe softmax-type functions.
œïi(x) =exp{x(ji)}Pn
k=1œÑikexp{x(k)}, (142)
where ji‚àà[1,¬∑¬∑¬∑, n],Pn
k=1œÑik= 1for any i‚àà[1,¬∑¬∑¬∑, l]. LetP(x) =P
k=0P
|Œ±|=kDŒ±P(0)
Œ±!xŒ±
denote the Taylor series ofQl
i=1œïi. Then for any integer k,
X
|Œ±|=k|DŒ±P(0)|
Œ±!‚â§(e3l)k. (143)
We introduce the conception of (Q, R)-bounded function briefly. Suppose œï:Rn‚ÜíRis real-
analytic in a neighborhood of the origin. For real numbers Q, R > 0, we say that œïis(Q, R)-bounded
if the Taylor expansion of œïat0, denoted Pœï(x) =P‚àû
k=0P
|Œ±|=kDŒ±f(0)
Œ±!xŒ±, satisfies, for each
integer i‚â•0,P
|Œ±|=k|DŒ±œï(0)|
Œ±!‚â§Q¬∑Rk.
Lemma D.6 (Detailed version of Lemma 4.5, [ 15]).Suppose that h, n‚ààN, œï:Rn‚ÜíRis a
(Q, R)-bounded function such that the radius of convergence of its power series at 0is at least
ŒΩ > 0, and Z={Z0,¬∑¬∑¬∑,ZT} ‚äÇRnis a sequence of vectors satisfyingZt
‚àû‚â§ŒΩfor
t‚àà[0,¬∑¬∑¬∑, T]. Suppose for some Œ≤‚àà(0,1), for each 0‚â§h‚Ä≤‚â§handt‚àà[0,¬∑¬∑¬∑, T‚àíh‚Ä≤], it holds
thatDh‚Ä≤Zt
‚àû‚â§1
ŒìRŒ≤h‚Ä≤(h‚Ä≤)Bh‚Ä≤for some B‚â•3,Œì‚â•e3. Then for all t‚àà[0,¬∑¬∑¬∑, T‚àíh],(Dh(œï‚ó¶Z))t‚â§Q¬∑g(Œì)¬∑Œ≤hhBh+1, (144)
where g(Œì)is a bounded function with respect to Œì.
Proof. Without loss of generality, we assume œï(0) = 0 . We define (œï‚ó¶Z)t=P
Œ≥‚ààZn
‚â•0:|Œ≥|=kaŒ≥ 
ZtŒ≥and obtain
(Dh(œï‚ó¶Z))t=‚àûX
k=1X
Œ≥‚ààZn
‚â•0:|Œ≥|=kaŒ≥(DhZŒ≥)t
‚â§‚àûX
k=1X
Œ≥‚ààZn
‚â•0:|Œ≥|=k|aŒ≥|Ô£´
Ô£≠X
x:[h]‚Üí[k]kY
j=1
Et‚Ä≤
x,jDh‚Ä≤
x,jZ(l‚Ä≤
x,j)tÔ£∂
Ô£∏
‚â§‚àûX
k=1X
Œ≥‚ààZn
‚â•0:|Œ≥|=k|aŒ≥| ¬∑Œ≤h
(ŒìR)k¬∑Ô£´
Ô£≠X
x:[h]‚Üí[k]kY
j=1(h‚Ä≤
x,j)Bh‚Ä≤
x,jÔ£∂
Ô£∏
‚â§‚àûX
k=1X
Œ≥‚ààZn
‚â•0:|Œ≥|=k|aŒ≥| ¬∑Œ≤h
(ŒìR)khBhmax
k7,(hk+ 1) exp2k
hB‚àí1
‚â§
(c)‚àûX
k=1QR
ŒìRk
¬∑max
k7,(k+ 1) exp {2k}	
¬∑Œ≤hhBh+1
‚â§Q¬∑g(Œì)¬∑Œ≤hhBh+1, (145)
40where (c) is derived from (Q, R)-bounded condition.
Lemma D.7 (Lemma C.4, [ 15]).Let{n, T} ‚äÇZ+withn‚â•2andT‚â•4, we select H:=
‚åàlog(T)‚åâ, Œ≤0=1
4H, and Œ≤=‚àö
Œ≤0/8
H3. Assume that {zt}T
t=1‚äÇ[0,1]nand{pt}T
t=1‚äÇ‚àÜnsatisfy the
following condition
1. For each 0‚â§h‚â§Hand1‚â§t‚â§T‚àíh, it holds that ‚à•(Dhz)t‚à•‚àû‚â§Œ≤hH3h+1.
2. The sequence {pt}T
t=1isŒ∂‚àíconsecutively close for some Œ∂‚àà
(2T)‚àí1, Œ≤4
0/8256
.
Then, we have
TX
t=1Varpt(zt‚àízt‚àí1)‚â§2Œ≤0TX
t=1Varpt(zt‚àí1) + 165120(1 + Œ∂)H5+ 2. (146)
Proposition D.8. Given a constant c >0, we have
tX
k=1c
c+k2
‚â§c. (147)
Lemma D.9. For a constant c‚â•c‚Ä≤>0, the following inequality holds
c‚Ä≤logc+t‚àí1
c+T
‚àí(c‚Ä≤+c‚Ä≤c)2
2c‚â§TX
k=tlog
1‚àíc‚Ä≤
c+k
‚â§c‚Ä≤logc+t
c+ 1 + T
, (148)
when T > t ‚â•1.
Proof. Accoding to the Taylor expansion of log(1‚àíx)when x <1, we obtain the estimation of
log
1‚àíc‚Ä≤
c+k
for any k‚â•1as follows
log
1‚àíc‚Ä≤
c+k
‚â§ ‚àíc‚Ä≤
c+k, (149)
log
1‚àíc‚Ä≤
c+k
‚â• ‚àíc‚Ä≤
c+k‚àí(c‚Ä≤+cc‚Ä≤)2
21
c+k2
. (150)
Next, we have
TX
k=t‚àíc‚Ä≤
c+k‚â§ ‚àíZT+1
tc‚Ä≤
c+xdx=c‚Ä≤logc+t
c+ 1 + T
, (151)
TX
k=t"
‚àíc‚Ä≤
c+k‚àí(c‚Ä≤+cc‚Ä≤)2
21
c+k2#
‚â• ‚àíZT
t‚àí1"
c‚Ä≤
c+x+(c‚Ä≤+cc‚Ä≤)2
21
c+x2#
dx
‚â•c‚Ä≤logc+t‚àí1
c+T
‚àí(c‚Ä≤+cc‚Ä≤)2
2c. (152)
E Limitation
For objectives with GQC condition (GQCC condition) and general smooth internal function (i.e.
Lipschitz continuous internal function), our analytical method might not provide similar iteration
complexity. We leave the related algorithmic analysis on more generalized smoothness conditions as
a future work.
41NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: YES
Justification: We have a detailed explanation in the contribution section of the introduction.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: YES
Justification: See Section E in appendix.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: YES
42Justification: We provide the assumptions and the associated theoretical results in Section 3
and Section 4, respectively.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
43Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
44‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: YES
Justification:
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: NA
Justification: Since this paper is a theoretical paper, it may not have other social impacts.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
45generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the license
of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
46Answer: NA
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: NA
Justification:
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: NA
Justification:
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
47