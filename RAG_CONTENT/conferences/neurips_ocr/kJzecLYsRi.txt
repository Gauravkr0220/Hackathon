On the Saturation Effects of Spectral Algorithms
in Large Dimensions
Weihao Lu
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
luwh19@mails.tsinghua.edu.cnHaobo Zhang
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
zhang-hb21@mails.tsinghua.edu.cn
Yicheng Li
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
liyc22@mails.tsinghua.edu.cnQian Linâˆ—
Department of Statistics and Data Science
Tsinghua University
Beijing, China 100084
qianlin@tsinghua.edu.cn
Abstract
The saturation effects, which originally refer to the fact that kernel ridge regression
(KRR) fails to achieve the information-theoretical lower bound when the regres-
sion function is over-smooth, have been observed for almost 20 years and were
rigorously proved recently for kernel ridge regression and some other spectral
algorithms over a fixed dimensional domain. The main focus of this paper is to
explore the saturation effects for a large class of spectral algorithms (including the
KRR, gradient descent, etc.) in large dimensional settings where nâ‰dÎ³. More
precisely, we first propose an improved minimax lower bound for the kernel regres-
sion problem in large dimensional settings and show that the gradient flow with
early stopping strategy will result in an estimator achieving this lower bound (up to
a logarithmic factor). Similar to the results in KRR, we can further determine the
exact convergence rates (both upper and lower bounds) of a large class of (optimal
tuned) spectral algorithms with different qualification Ï„â€™s. In particular, we find
that these exact rate curves (varying along Î³) exhibit the periodic plateau behavior
and the polynomial approximation barrier. Consequently, we can fully depict the
saturation effects of the spectral algorithms and reveal a new phenomenon in large
dimensional settings (i.e., the saturation effect occurs in large dimensional setting
as long as the source condition s > Ï„ while it occurs in fixed dimensional setting
as long as s >2Ï„).
1 Introduction
Letâ€™s assume we have ni.i.d. samples (xi, yi)from a joint distribution supported on RdÃ—R. The
regression problem, one of the most fundamental problems in statistics, aims to find a function Ë†f
based on these samples such that the excess risk ,âˆ¥Ë†fâˆ’fâ‹†âˆ¥2
L2=Ex[(fâ‹†(x)âˆ’Ë†f(x))2], is small, where
fâ‹†(x) =E[Y|x]is the regression function . Many non-parametric regression methods are proposed
to solve the regression problem by assuming that fâ‹†falls into certain function classes, including
polynomial splines Stone (1994), local polynomials Cleveland (1979); Stone (1977), the spectral
algorithms Caponnetto (2006); Caponnetto and De Vito (2007); Caponnetto and Yao (2010), etc.
âˆ—Corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Spectral algorithms, as a classical topic, have been studied since the 1990s. Early works treated
certain types of spectral algorithms in their theoretical analysis (Caponnetto (2006); Caponnetto and
De Vito (2007); Raskutti et al. (2014); Lin et al. (2020)). These works often consider das a fixed
constant and impose the polynomial eigenvalue decay assumption under a kernel (i.e., there exist
constants 0<câ‰¤C<âˆž, such that the eigenvalues of the kernel satisfy cjâˆ’Î²â‰¤Î»jâ‰¤Cjâˆ’Î²,jâ‰¥1
for certain Î² >1depending on the fixed d). They further assume that fâ‹†belongs to the reproducing
kernel Hilbert space (RKHS) Hassociated with the kernel. Under the above assumptions, they
then showed that the minimax rate of the excess risk of regression over the corresponding RKHS
is lower bounded by nâˆ’Î²/(Î²+1)and that some (regularized) spectral algorithms, e.g., the kernel
ridge regression (KRR) and the kernel gradient flow, can produce estimators achieving this minimax
optimal rate.
However, subsequent studies have revealed that when higher regularity (or smoothness) of fâ‹†is
assumed, KRR fails to achieve the information-theoretical lower bound on the excess risk, while
kernel gradient flow can do so. Specifically, letâ€™s assume that fâ‹†belongs to the interpolation space
[H]sof the RKHS Hwiths >0(see, e.g., Steinwart et al. (2009); Dieuleveut et al. (2017); Dicker
et al. (2017); Pillaud-Vivien et al. (2018); Lin et al. (2020); Fischer and Steinwart (2020); Celisse
and Wahl (2021)). It is then shown that the information-theoretical lower bound on the excess risk
isnâˆ’sÎ²/(sÎ²+1). When 0< sâ‰¤2, Caponnetto and De Vito (2007); Yao et al. (2007); Lin et al.
(2020); Zhang et al. (2023) have already shown that the upper bound of the excess risks of both KRR
and the kernel gradient flow is nâˆ’sÎ²/(sÎ²+1), and hence they are minimax optimal. On the contrary,
when s >2, Yao et al. (2007); Lin et al. (2020) showed that the upper bound of the excess risks
of kernel gradient flow is nâˆ’sÎ²/(sÎ²+1)while the best upper bound of the excess risks of KRR is
nâˆ’2Î²/(2Î²+1)(Caponnetto and De Vito (2007)). Bauer et al. (2007); Gerfo et al. (2008); Dicker et al.
(2017) conjectured that the convergence rate of KRR is bounded below by nâˆ’2Î²/(2Î²+1)and Li et al.
(2022) rigorously proved it. The above phenomenon is often referred to as the saturation effect of
KRR:
KRR is inferior to certain spectral algorithms, such as kernel gradient flow, when s >2.
In recent years, neural network methods have gained tremendous success in many large-dimensional
problems, such as computer vision He et al. (2016); Krizhevsky et al. (2017) and natural language
processing Devlin (2018). Several groups of researchers tried to explain the superior performance of
neural networks on large-dimensional data from the aspects of "lazy regime" (Arora et al. (2019); Du
et al. (2019, 2018); Li and Liang (2018)). They noticed that, when the width of a neural network is
sufficiently large, its parameters/weights stay in a small neighborhood of their initial position during
the training process. Later, Jacot et al. (2018); Arora et al. (2019); Hu et al. (2021); Suh et al. (2021);
Lai et al. (2023); Li et al. (2024) proved that the time-varying neural network kernel (NNK) converges
(uniformly) to a time-invariant neural tangent kernel (NTK) as the width of the neural network goes
to infinity, and thus the excess risk of kernel gradient flow with NTK converges (uniformly) to the
excess risk of neural networks in the â€˜lazy regimeâ€™.
Inspired by the concepts of the "lazy regime" and the uniform convergence of excess risk, the
machine learning community has experienced a renewed surge of interest in large-dimensional
spectral algorithms. The earliest works focused on the consistency of two specific types of spectral
algorithms: KRR and kernel interpolation (Liang and Rakhlin (2020); Liang et al. (2020); Ghorbani
et al. (2020, 2021); Mei et al. (2021, 2022); Misiakiewicz and Mei (2022); Aerni et al. (2023); Barzilai
and Shamir (2023)). In comparison, results on large-dimensional kernel gradient flow were somewhat
scarce, and these results largely mirrored those associated with KRR (e.g., Ghosh et al. (2021)).
Recently, Lu et al. (2023) proved that large-dimensional kernel gradient flow is minimax optimal
when s= 1. Then, Zhang et al. (2024) provided upper and lower bounds on the convergence rate on
the excess risk of KRR for any s >0. Surprisingly, they discovered that for s >1, the convergence
rate of KRR did not match the lower bound on the minimax rate. Unfortunately, they didnâ€™t prove that
certain spectral algorithms can reach the lower bound on the minimax rate they provided, and hence
they didnâ€™t rigorously prove that the saturation effect of KRR occurs in large dimensions. Instead,
Zhang et al. (2024) only conjectured that certain spectral algorithms (e.g., kernel gradient flow) can
provide minimax optimal estimators after their main results.
If Zhang et al. (2024)â€™s conjecture is true, then we can safely conclude that: when the regression
function fâ‹†is smooth enough, KRR is inferior to kernel gradient flow in large dimensions as well.
Consequently, previous results on large-dimensional KRR may not be directly extendable to large-
2dimensional neural networks, even if the neural networks are in the â€˜lazy regimeâ€™. The main focus of
this paper is to prove this conjecture by showing that kernel gradient flow is minimax optimal in large
dimensions.
1.1 Related work
Saturation effects of fixed-dimensional spectral algorithms. When the dimension dof the data
is fixed, the saturation effect of KRR has been conjectured for decades and is rigorously proved in
the recent work Li et al. (2022). Suppose fâ‹†âˆˆ[H]swiths >2. It is shown that: (i) the minimax
optimal rate is nâˆ’sÎ²/(sÎ²+1)(Rastogi and Sampath (2017); Yao et al. (2007); Lin et al. (2020));
and (ii) the convergence rate on the excess risk of KRR is nâˆ’2Î²/(2Î²+1)(Li et al. (2022)). More
recently, Li et al. (2024) determined the exact generalization error curves of a class of analytic spectral
algorithms, which allowed them to further show the saturation effect of spectral algorithms with finite
qualification Ï„(see, e.g., Appendix C): suppose fâ‹†âˆˆ[H]swiths >2Ï„, then the convergence rate on
the excess risk of the above spectral algorithms is nâˆ’2Ï„Î²/(2Ï„Î²+1).
New phenomena in large-dimensional spectral algorithms. In the large-dimensional setting
where nâ‰dÎ³withÎ³ > 0, new phenomena exhibited in spectral algorithms are popular topics
in recent machine-learning research. A line of work focused on the polynomial approximation
barrier phenomenon (e.g., Ghorbani et al. (2021); Donhauser et al. (2021); Mei et al. (2022); Xiao
et al. (2023); Misiakiewicz (2022); Hu and Lu (2022)). They found that, for the square-integrable
regression function, KRR and kernel gradient flow are consistent if and only if the regression function
is a polynomial with a low degree. Another line of work considered the benign overfitting of
kernel interpolation (i.e., kernel interpolation can generalize) (e.g., Liang and Rakhlin (2020); Liang
et al. (2020); Aerni et al. (2023); Barzilai and Shamir (2023); Zhang et al. (2024)). Moreover, two
recent work (Lu et al. (2023); Zhang et al. (2024)) discussed two new phenomena exhibited in
large-dimensional KRR and kernel gradient flow: the multiple descent behavior and the periodic
plateau behavior. The multiple descent behavior refers to the phenomenon that the curve of the
convergence rate ( with respect to n) of the optimal excess risk is non-monotone and has several
isolated peaks and valleys; while the periodic plateau behavior refers to the phenomenon that the
curve of the convergence rate ( with respect to d) of the optimal excess risk has constant values when
Î³is within certain intervals. Finally, Zhang et al. (2024) conjectured that the saturation effect of KRR
occurs in large dimensions. The above works imply that these phenomena occur in many spectral
algorithms in large dimensions, hence encouraging us to provide a unified explanation of these new
phenomena.
1.2 Our contributions
In this paper, we focus on the large-dimensional spectral algorithms with inner product kernels, and
we assume that the regression function falls into an interpolation space [H]swiths >0. We state our
main results as follows:
Theorem 1.1 (Restate Theorem 4.1 and 4.2, non-rigorous) .Lets >0,Ï„â‰¥1, and Î³ >0be fixed
real numbers. Denote pas the integer satisfying Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Then under certain
conditions, the excess risk of large-dimensional spectral algorithm with qualification Ï„satisfies
EË†fÎ»â‹†âˆ’fâ‹†2
L2X
=(
Î˜P 
dâˆ’min{Î³âˆ’p,s(p+1)}
Â·poly(ln(d)), sâ‰¤Ï„
Î˜P
dâˆ’min{Î³âˆ’p,Ï„(Î³âˆ’p+1)+ pËœs
Ï„+1,Ëœs(p+1)}
Â·poly(ln(d)), s > Ï„,
where Ëœs= min {s,2Ï„}.
More specifically, we list the main contributions of this paper as follows:
(1)In Theorem 3.1, we show that the convergence rate on the excess risk of (optimally-tuned)
kernel gradient flow in large dimensions is Î˜P(dâˆ’min{Î³âˆ’p,s(p+1)})Â·poly(ln(d)), which
matches the lower bound on the minimax rate given in Theorem 3.3 (up to a logarithmic
factor). We find that kernel gradient flow is minimax optimal for any s >0and any Î³ >0,
and KRR is not minimax optimal for s >1and for certain ranges of Î³(We provide a visual
illustration in Figure 2). Consequently, we rigorously prove that the saturation effect of
KRR occurs in large dimensions.
3(2)In Theorem 3.3, we enhanced the previous minimax lower bound results given in Lu et al.
(2023) and Zhang et al. (2024). Specifically, we show that the minimax lower bound is
â„¦(dâˆ’min{Î³âˆ’p,s(p+1)})/poly(ln(d)). In comparison, the previous minimax lower bound is
â„¦(dâˆ’min{Î³âˆ’p,s(p+1)})/dÎµfor any Îµ >0, and the additional term dÎµchanges the desired
convergence rate.
(3)In Section 4, we determine the convergence rate on the excess risk of large-dimensional
spectral algorithms. From our results, we find several new phenomena exhibited in spectral
algorithms in large-dimensional settings. We provide a visual illustration of the above
phenomena in Figure 1: i) The first phenomenon is the polynomial approximation barrier,
and as shown in Figure 1(a), when sis close to zero, the curve of the convergence rate of
spectral algorithm drops when Î³â‰ˆpfor any integer pand will stay invariant for most of the
other Î³; ii) The second one is the periodic plateau behavior, and as shown in Figure 1(b) and
Figure 1(c), when 0< s < 2Ï„andÎ³âˆˆ[p(s+ 1) + s+ (max {s, Ï„}âˆ’Ï„)/Ï„,(p+ 1)( s+ 1))
for an integer pâ‰¥0, the convergence rate does not change when Î³varies; iii) The final
one is the saturation effect, and as shown in Figure 1(c) and Figure 1(d), when s > Ï„ , the
convergence rate of spectral algorithm can not achieve the minimax lower bound for certain
ranges of Î³. A detailed discussion about the above three phenomena can be found in Section
4.
(a)
 (b)
 (c)
 (d)
Figure 1: Convergence rates of spectral algorithm with qualification Ï„= 2in Theorem 4.1, Theorem
4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension d. We
present four graphs corresponding to four kinds of source conditions: s= 0.01,1,3,5. The x-axis
represents asymptotic scaling, Î³:nâ‰dÎ³; the y-axis represents the convergence rate of excess risk,
r:Excess risk â‰dr.
2 Preliminaries
Suppose that we have observed ni.i.d. samples (xi, yi), iâˆˆ[n]from the model:
y=fâ‹†(x) +Ïµ, (1)
where xiâ€™s are sampled from ÏX,ÏXis the marginal distribution on X âŠ‚Rd+1,yâˆˆ Y âŠ‚ R,fâ‹†is
some function defined on a compact set X, and
E(x,y)âˆ¼Ïh
Ïµ2xi
â‰¤Ïƒ2, ÏX-a.e.xâˆˆ X,
for some fixed constant Ïƒ >0, where Ïis the joint distribution of (x, y)onX Ã— Y . Denote the nÃ—1
data vector of yiâ€™s and the nÃ—ddata matrix of xiâ€™s byYandXrespectively.
2.1 Kernel ridge regression and kernel gradient flow
In this subsection, we introduce two specific spectral algorithms, kernel ridge regression and kernel
gradient flow, which produce estimators of the regression function fâ‹†. A further discussion on general
spectral algorithms will be provided in Section 4.
Throughout the paper, we denote Has a separable RKHS on Xwith respect to a continuous and
positive definite kernel function K(Â·,Â·) :X Ã— X â†’ Rand there exists a constant Îºsatisfying
max
xâˆˆXK(x, x)â‰¤Îº2.
4Kernel ridge regression Kernel ridge regression (KRR) constructs an estimator Ë†fKRR
Î»by solving
the penalized least square problem
Ë†fKRR
Î»= arg min
fâˆˆH 
1
nnX
i=1(yiâˆ’f(xi))2+Î»âˆ¥fâˆ¥2
H!
,
where Î» > 0is referred to as the regularization parameter. The representer theorem (see, e.g.,
Steinwart and Christmann (2008)) gives an explicit expression of the KRR estimator, i.e.,
Ë†fKRR
Î»(x) =K(x, X)(K(X, X ) +nÎ»I)âˆ’1Y. (2)
Kernel gradient flow The gradient flow of the loss function L=1
2nP
i(yiâˆ’f(xi))2induced a
gradient flow in Hwhich is given by
d
dtË†fGF
t(x) =âˆ’1
nK(x, X)(Ë†fGF
t(X)âˆ’Y). (3)
If we further assume that Ë†fGF
0(x) = 0 , then we can also give an explicit expression of the kernel
gradient flow estimator
Ë†fGF
t(x) =K(x, X)K(X, X )âˆ’1(Iâˆ’eâˆ’1
nK(X,X)t)Y. (4)
2.2 The interpolation space
Define the integral operator TKasTK(f)(x) =R
K(x, xâ€²)f(xâ€²)dÏX(xâ€²). It is well known that TK
is a positive, self-adjoint, trace-class, and hence a compact operator (Steinwart and Scovel (2012)).
The celebrated Mercerâ€™s theorem further assures that
K(x, xâ€²) =X
jÎ»jÏ•j(x)Ï•j(xâ€²), (5)
where the eigenvalues {Î»j, j= 1,2, ...}is a non-increasing sequence, and the corresponding
eigenfunctions {Ï•j(Â·), j= 1,2, ...}are orthonormal in L2(X, ÏX)function space.
The interpolation space [H]swith source condition sis defined as
[H]s:=nX
jajÎ»s/2
jÏ•j: (aj)jâˆˆâ„“2o
âŠ†L2(X, ÏX), (6)
with the inner product deduced from
âˆžX
j=1ajÎ»s/2
jÏ•j
[H]s=âˆžX
j=1a2
j1/2
. (7)
It is easy to show that [H]sis also a separable Hilbert space with orthonormal basis {Î»s/2
jÏ•j}j.
Generally speaking, functions in [H]sbecome smoother as sincreases (see, e.g., the example of
Sobolev RKHS in Edmunds and Triebel (1996); Zhang et al. (2023).
2.3 Assumptions
In this subsection, we list the assumptions that we need for our main results.
To avoid potential confusion, we specify the following large-dimensional scenario for kernel regres-
sion where we perform our analysis: suppose that there exist three positive constants c1,c2andÎ³,
such that
c1dÎ³â‰¤nâ‰¤c2dÎ³, (8)
and we often assume that dis sufficiently large.
In this paper, we only consider the inner product kernels defined on the sphere. An inner product
kernel is a kernel function Kdefined on Sdsuch that there exists a function Î¦ : [âˆ’1,1]â†’R
independent of dsatisfying that for any x, xâ€²âˆˆSd, we have K(x, xâ€²) = Î¦( âŸ¨x, xâ€²âŸ©). If we further
5assume that the marginal distribution ÏXis the uniform distribution on X=Sd, then the Mercerâ€™s
decomposition for Kcan be rewritten as
K(x, xâ€²) =âˆžX
k=0ÂµkN(d,k)X
j=1Yk,j(x)Yk,j(xâ€²), (9)
where Yk,jforj= 1,Â·Â·Â·, N(d, k)are spherical harmonic polynomials of degree kandÂµkâ€™s are the
eigenvalues of Kwith multiplicity N(d,0) = 1 ;N(d, k) =2k+dâˆ’1
kÂ·(k+dâˆ’2)!
(dâˆ’1)!(kâˆ’1)!, k= 1,2,Â·Â·Â·.
For more details of the inner product kernels, readers can refer to Gallier (2009).
Remark 2.1.We consider the inner product kernels on the sphere mainly because the harmonic
analysis is clear on the sphere ( e.g., properties of spherical harmonic polynomials are more concise
than the orthogonal series on general domains). This makes Mercerâ€™s decomposition of the inner
product more explicit rather than several abstract assumptions ( e.g., Mei and Montanari (2022)).
We also notice that very few results are available for Mercerâ€™s decomposition of a kernel defined on
the general domain, especially when the dimension of the domain is taking into consideration. e.g.,
even the eigen-decay rate of the neural tangent kernels is only determined for the spheres. Restricted
by this technical reason, most works analyzing the spectral algorithm in large-dimensional settings
focus on the inner product kernels on spheres (Liang et al., 2020; Ghorbani et al., 2021; Misiakiewicz,
2022; Xiao et al., 2023; Lu et al., 2023, etc.). Though there might be several works that tried to
relax the spherical assumption (e.g., Liang et al. (2020); Aerni et al. (2023); Barzilai and Shamir
(2023), we can find that most of them (i) adopted a near-spherical assumption; (ii) adopted strong
assumptions on the regression function, e.g., fâ‹†(x) =x[1]x[2]Â·Â·Â·x[L]for an integer L >0, where
x[i]denotes the i-th component of x; or (iii) can not determine the convergence rate on the excess
risk of the spectral algorithm.
To avoid unnecessary notation, let us make the following assumption on the inner product kernel K.
Assumption 1.Î¦(t)âˆˆ Câˆž([âˆ’1,1])is a fixed function independent of dand there exists a non-
negative sequence of absolute constants {ajâ‰¥0}jâ‰¥0, such that we have
Î¦(t) =Xâˆž
j=0ajtj,
where aj>0for any jâ‰¤ âŒŠÎ³âŒ‹+ 3.
The purpose of Assumption 1 is to keep the main results and proofs clean. Notice that, by Theorem
1.b in Gneiting (2013), the inner product kernel Kon the sphere is semi-positive definite for all
dimensions if and only if all coefficients {aj, j= 0,1,2, ...}are non-negative. One can easily extend
our results in this paper when certain coefficients akâ€™s are zero (e.g., one can consider the two-layer
NTK defined as in Section 5 of Lu et al. (2023), with ai= 0for any i= 3,5,7,Â·Â·Â·).
In the next assumption, we formally introduce the source condition, which characterizes the relative
smoothness of fâ‹†with respect to H.
Assumption 2 (Source condition) .Suppose that fâ‹†(x) =Pâˆž
i=1fiÏ•i(x).
(a)fâ‹†âˆˆ[H]sfor some s >0, and there exists a constant RÎ³only depending on Î³, such that
âˆ¥fâ‹†âˆ¥[H]sâ‰¤RÎ³. (10)
(b)Denote qas the smallest integer such that q > Î³ andÂµqÌ¸= 0. Define Id,kas the index set
satisfying Î»iâ‰¡Âµk, iâˆˆ Id,k. Further suppose that there exists an absolute constant c0>0
such that for any dandkâˆˆ {0,1,Â·Â·Â·, q}withÂµkÌ¸= 0, we have
X
iâˆˆId,kÂµâˆ’s
kf2
iâ‰¥c0. (11)
Assumption 2 is a common assumption when one is interested in the tight bounds on the excess risk
of spectral algorithms (e.g., Caponnetto and De Vito (2007); Fischer and Steinwart (2020), Eq.(8)
in Cui et al. (2021), Assumption 3 in Li et al. (2024), and Assumption 5 in Zhang et al. (2024)).
Assumption 2 implies that the regression function exactly falls into the interpolation space [H]s, that
is,fâ‹†âˆˆ[H]sandfâ‹†/âˆˆ[H]tfor any t > s . For example, from the proof part I of Lemma D.14, one
can check that fâ‹†withP
iâˆˆId,pÂµâˆ’s
pf2
i=P
iâˆˆId,p+1Âµâˆ’s
p+1f2
i= 0can have a faster convergence rate
on the excess risk.
6Notations. Letâ€™s denote the norm in L2(X, ÏX)asâˆ¥ Â· âˆ¥ L2. For a vector x, we use x[i]to de-
note its i-th component. We use asymptotic notations O(Â·), o(Â·),â„¦(Â·)andÎ˜(Â·). For instance,
we say two (deterministic) quantities U(d), V(d)satisfy U(d) =o(V(d))if and only if for any
Îµ > 0, there exists a constant DÎµthat only depends on Îµand the absolute positive constants
Ïƒ, Îº, s, Î³, c 0, c1, c2,C1,Â·Â·Â·,C8>0, such that for any d > D Îµ, we have U(d)< ÎµV (d). We also
write an=poly(bn)if there exist a constant Î¸â‰¥0, such that an= Î˜( bÎ¸
n). We use the probability
versions of the asymptotic notations such as OP(Â·), oP(Â·),â„¦P(Â·),Î˜P(Â·). For instance, we say the
random variables Xn, Ynsatisfying Xn=OP(Yn)if and only if for any Îµ >0, there exist constants
CÎµandNÎµsuch that P(|Xn| â‰¥CÎµ|Yn|)â‰¤Îµ,âˆ€n > N Îµ.
2.4 Review of the previous results
The following two results are restatements of Theorem 2 and Theorem 5 in Zhang et al. (2024).
Proposition 2.2. Letsâ‰¥1andÎ³ >0be fixed real numbers. Denote pas the integer satisfying
Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold for sandÎ³. Let
Ë†fKRR
Î»be the function defined in (2). Define Ëœs= min {s,2}, then there exists Î»â‹†>0, such that we have
EË†fKRR
Î»â‹†âˆ’fâ‹†2
L2X
= Î˜ P
dâˆ’min{Î³âˆ’p,Î³âˆ’p+pËœs+1
2,Ëœs(p+1)}
Â·poly(ln(d)),
where Î˜Ponly involves constants depending on s, Ïƒ, Î³, c 0, Îº, c 1andc2. In addition, the convergence
rates of the generalization error can not be faster than above for any choice of regularization
parameter Î»=Î»(d, n)â†’0.
Proposition 2.3 (Lower bound on the minimax rate) .Lets >0andÎ³ >0be fixed real numbers.
Denote pas the integer satisfying Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . LetPconsist of all the distributions
ÏonX Ã—Y such that Assumption 1 and Assumption 2 hold for sandÎ³. Then for any Îµ >0, we have:
min
Ë†fmax
ÏâˆˆPE(X,Y)âˆ¼ÏâŠ—nË†fâˆ’fâ‹†2
L2= â„¦
dâˆ’min{Î³âˆ’p,s(p+1)}Â·dâˆ’Îµ
,
where â„¦only involves constants depending on s, Ïƒ, Î³, c 0, Îº, c 1, c2andÎµ.
From the above two propositions, we can find that when s >1, the convergence rate on the excess
risk of KRR does not always match the lower bound on the minimax optimal rate. Zhang et al. (2024)
further conjectured that the lower bound on the minimax optimal rate provided in Proposition 2.3
is tight (ignoring the additional term dâˆ’Îµ). Hence, they believed that the saturation effect exists for
large-dimensional KRR.
3 Main results
In this section, we determine the convergence rate on the excess risk of kernel gradient flow as
dâˆ’min{Î³âˆ’p,s(p+1)}poly(ln(d)), which differs from the lower bound on the minimax rate provided
in Proposition 2.3 by dÎµfor any Îµ >0. We then tighten the lower bound on the minimax rate
todâˆ’min{Î³âˆ’p,s(p+1)}/poly(ln(d)). Based on the above results, we find that KRR is not minimax
optimal for s >1and for certain ranges of Î³. Therefore, we show that the saturation effect of KRR
occurs in large dimensions.
3.1 Exact convergence rate on the excess risk of kernel gradient flow
We first state our main results in this paper.
Theorem 3.1 (Kernel gradient flow) .Lets >0andÎ³ >0be fixed real numbers. Denote pas the
integer satisfying Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold
forsandÎ³. Let Ë†fGF
tbe the function defined in (4). Then there exists tâ‹†>0, such that we have
EË†fGF
tâ‹†âˆ’fâ‹†2
L2X
= Î˜ P
dâˆ’min{Î³âˆ’p,s(p+1)}
Â·poly(ln(d)), (12)
where Î˜Ponly involves constants depending on s, Ïƒ, Î³, c 0, Îº, c 1andc2.
7Theorem 3.1 is a direct corollary of Theorem 4.1 and Example 2. Combining with the previous
results in Proposition 2.3, or our modified minimax rate given in Theorem 3.3, we can conclude
that large-dimensional kernel gradient flow is minimax optimal for any s > 0and any Î³ > 0.
More importantly, the convergence rate of kernel gradient flow is faster than that of KRR given in
Proposition 2.2 when (i) 1< sâ‰¤2andÎ³âˆˆ(p(s+ 1) + 1 , p(s+ 1) + 2 sâˆ’1)for some pâˆˆN, or
(ii)s >2andÎ³âˆˆ(p(s+ 1) + 1 ,(p+ 1)( s+ 1)) for some pâˆˆN. Therefore, we have proved the
saturation effect of KRR in large dimensions.
Remark 3.2.When pâ‰¥1, the logarithm term poly(ln(d))in (12) can be removed. When p= 0, we
have poly (ln(d)) = (ln( d))2in (12). See Appendix D.4 for details.
3.2 Improved minimax lower bound
Recall that Proposition 2.3 gave a lower bound on the minimax rate as dâˆ’min{Î³âˆ’p,s(p+1)}Â·dâˆ’Îµ. The
following theorem replaces the additional term dâˆ’Îµ(which has changed the convergence rate) into a
logarithm term polyâˆ’1(ln(d))(which does not change the desired convergence rate).
Theorem 3.3 (Improved minimax lower bound) .Lets >0andÎ³ >0be fixed real numbers. Denote
pas the integer satisfying Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . LetPconsist of all the distributions Ïon
X Ã— Y such that Assumption 1 and Assumption 2 hold for sandÎ³. Then we have:
min
Ë†fmax
ÏâˆˆPE(X,Y)âˆ¼ÏâŠ—nË†fâˆ’fâ‹†2
L2= â„¦
dâˆ’min{Î³âˆ’p,s(p+1)}.
poly(ln(d)), (13)
where â„¦only involves constants depending on s, Ïƒ, Î³, c 0, Îº, c 1, and c2.
4 Exact convergence rate on the excess risk of spectral algorithms
In this section, we will give tight bounds on the excess risks of certain types of spectral algorithms,
such as kernel ridge regression, iterated ridge regression, kernel gradient flow, and kernel gradient
descent.
Given an analytic filter function Ï†Î»(Â·)with qualification Ï„â‰¥1(refer to Appendix C for the definitions
of analytic filter function and its qualification), we can define a spectral algorithm in the following
way (see, e.g., Bauer et al. (2007)). For any yâˆˆR, letKx:Râ†’ H be given by Kx(y) =yÂ·K(x,Â·),
whose adjoint Kâˆ—
x:H â†’Ris given by Kâˆ—
x(f) =âŸ¨K(x,Â·), fâŸ©H=f(x). Moreover, we denote by
Tx=KxKâˆ—
xandTX=1
nPn
i=1Txi. We also define the sample basis function
Ë†gZ=1
nXn
i=1Kxi(yi) =1
nXn
i=1yiÂ·K(xi,Â·). (14)
Now, the estimator of the spectral algorithm is defined by
Ë†fÎ»=Ï†Î»(TX)Ë†gZ. (15)
Many commonly used spectral algorithms can be constructed by certain analytic filter functions. We
provide two examples (kernel ridge regression and kernel gradient flow) as follows, and put two more
examples (iterated ridge regression and kernel gradient descent) in Appendix C. We provide rigorous
proof for these examples in Lemma C.3.
Example 1 (Kernel ridge regression) .The filter function of kernel ridge regression (KRR) is well-
known to be
Ï†KRR
Î»(z) =1
z+Î», ÏˆKRR
Î»(z) =Î»
z+Î», Ï„ = 1. (16)
Example 2 (Kernel gradient flow) .The filter function is
Ï†GF
Î»(z) =1âˆ’eâˆ’tz
z, ÏˆGF
Î»(z) =eâˆ’tz, t=Î»âˆ’1, Ï„ =âˆž. (17)
For any analytic filter function Ï†Î»with qualification Ï„â‰¥1and the corresponding estimator of the
spectral algorithm defined in (15), the following two theorems provide exact convergence rates on the
excess risk when (i) the regression function is less-smooth, i.e., we have sâ‰¤Ï„, and (ii) s > Ï„ , where
sis the source condition coefficient of the regression function given in Assumption 2.
8Theorem 4.1. Let0< sâ‰¤Ï„andÎ³ >0be fixed real numbers. Denote pas the integer satisfying
Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold for sandÎ³.
LetÏ†Î»(z)be an analytic filter function and Ë†fÎ»be the function defined in (15). Suppose one of the
following conditions holds:
(i)Ï„=âˆž,(ii)s >1/(2Ï„),(iii)Î³ >((2Ï„+ 1)s)/(2Ï„(1 +s));
then there exists Î»â‹†>0, such that we have
EË†fÎ»â‹†âˆ’fâ‹†2
L2X
= Î˜ P
dâˆ’min{Î³âˆ’p,s(p+1)}
Â·poly(ln(d)),
where Î˜Ponly involves constants depending on s, Ïƒ, Î³, c 0, Îº, c 1andc2.
Theorem 4.2. Lets > Ï„ andÎ³ > 0be fixed real numbers. Denote pas the integer satisfying
Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Suppose that Assumption 1 and Assumption 2 hold for sandÎ³. Let
Ï†Î»(z)be an analytic filter function and Ë†fÎ»be the function defined in (15). Define Ëœs= min {s,2Ï„},
then there exists Î»â‹†>0, such that we have
EË†fÎ»â‹†âˆ’fâ‹†2
L2X
= Î˜ P
dâˆ’min{Î³âˆ’p,Ï„(Î³âˆ’p+1)+ pËœs
Ï„+1,Ëœs(p+1)}
Â·poly(ln(d)),
where Î˜Ponly involves constants depending on s, Ïƒ, Î³, c 0, Îº, c 1andc2. In addition, the convergence
rates of the generalization error can not be faster than above for any choice of regularization
parameter Î»=Î»(d, n)â†’0.
Remark 4.3.These theorems substantially generalize the results on exact generalization error bounds
of analytic spectral algorithms under the fixed-dimensional setting given in Li et al. (2024). Although
the â€œanalytic functional argumentâ€ introduced in their proof is still vital for us to deal with the
general spectral algorithms, their proof has to rely on the polynomial eigendecay assumption that
Î»jâ‰jâˆ’Î²(Assumption 1), which does not hold in large dimensions since the hidden constant factors
in the assumption vary with d11 (Lu et al. (2023)). Hence, their proof is not easy to generalize to
large-dimensional spectral algorithms.
We provide some graphical illustrations of Theorem 4.1 and Theorem 4.2 in Figure 1 (with Ï„= 2)
and in Appendix A (with Ï„= 1,Ï„= 2,Ï„= 4, and Ï„=âˆž, corresponding to KRR, iterated ridge
regression in Example 3 and kernel gradient flow).
As a direct consequence of Theorem 3.3, Theorem 4.1, and Theorem 4.2, we find that for the spectral
algorithm with estimator defined in (15), it is minimax optimal if sâ‰¤Ï„and the conditions in Theorem
4.1 hold. Moreover, these results show several phenomena for large-dimensional spectral algorithms.
Saturation effect of large-dimensional spectral algorithms with finite qualification. In the
large-dimensional setting and for the inner product kernel on the sphere, our results show that the
saturation effect of spectral algorithms occurs when s > Ï„ . As shown in Figure 1(c) and Figure 1(d),
when s > Ï„ , no matter how carefully one tunes the regularization parameter Î», the convergence rate
can not be faster than dâˆ’min{Î³âˆ’p,Ï„(Î³âˆ’p+1)+ pËœs
Ï„+1,Ëœs(p+1)}, thus can not achieve the minimax lower bound
dâˆ’min{Î³âˆ’p,s(p+1)}.
Periodic plateau behavior of spectral algorithms when sâ‰¤2Ï„.When 0< sâ‰¤2Ï„and
Î³âˆˆ[p(s+ 1) + s+ max {s, Ï„}/Ï„âˆ’1,(p+ 1)( s+ 1)) for an integer pâ‰¥0, from Theorem 4.1 and
Theorem 4.2, the convergence rate on the excess risk of spectral algorithm dâˆ’s(p+1). The above rate
does not change when Î³varies, which can also be found in Figure 1(b) and Figure 1(c). BIn other
words, if we fix a large dimension dand increase Î³(or equivalently, increase the sample size n), the
optimal rate of excess risk of a spectral algorithm stays invariant in certain ranges. Therefore, in order
to improve the rate of excess risk, one has to increase the sample size above a certain threshold.
Polynomial approximation barrier of spectral algorithms when sâ†’0.From Theorem 4.1,
when sis close to zero, the convergence rate dâˆ’min{Î³âˆ’p,s(p+1)}is unchanged in the range Î³âˆˆ
[p(s+ 1) + s,(p+ 1)( s+ 1)) , and increases in the short range Î³âˆˆ[p(s+ 1), p(s+ 1) + s). In other
words, the excess risk of spectral algorithms will drop when Î³exceeds p(s+ 1)â‰ˆpfor any integer
pand will stay invariant for most of the other Î³. We term the above phenomenon as the polynomial
approximation barrier of spectral algorithms (borrowed from Ghorbani et al. (2021)), and it can be
illustrated by Figure 1(a) with s= 0.01.
9Remark 4.4.Ghorbani et al. (2021) discovered the polynomial approximation barrier of KRR. As
shown by Figure 5 and Theorem 4 in Ghorbani et al. (2021), if s= 0and the true function falls into
L2= [H]0, then with high probability we have
EË†fKRR
Î»â‹†âˆ’fâ‹†2
L2
âˆ’P>pfâ‹†2
L2â‰¤Îµfâ‹†2
L2+Ïƒ2
, (18)
where pis the integer satisfying Î³âˆˆ[p, p+ 1) ,Î»â‹†is defined as in Theorem 4 in Ghorbani et al.
(2021), P>â„“means the projection onto polynomials with degree > â„“, and Îµis any positive real
number. Notice that (18) implies that the excess risk of KRR will drop when Î³exceeds any integer
and will stay invariant for other Î³, and is consistent with our results for spectral algorithms.
5 Conclusion
In this paper, we rigorously prove the saturation effect of KRR in large dimensions. Let s >0and
Î³ >0be fixed real numbers, denote pas the integer satisfying Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Given
that the kernel is an inner product kernel defined on the sphere and that fâ‹†falls into the interpolation
space [H]s, we first show that the convergence rate on the excess risk of large-dimensional kernel
gradient flow is Î˜P 
dâˆ’min{Î³âˆ’p,s(p+1)}
Â·poly(ln(d))(Theorem 3.1), which is faster than that
of KRR given in Zhang et al. (2024). We then determine the improved minimax lower bound as
â„¦ 
dâˆ’min{Î³âˆ’p,s(p+1)}
/poly(ln(d))(Theorem 3.3). Combining these results, we know that kernel
gradient flow is minimax optimal in large dimensions, and KRR is inferior to kernel gradient flow in
large dimensions. Our results suggest that previous results on large-dimensional KRR may not be
directly extendable to large-dimensional neural networks if the regression function is over-smooth.
In Section 4, we generalize our results to certain spectral algorithms. We determine the convergence
rate on the excess risk of large-dimensional spectral algorithms (Theorem 4.1 and Theorem 4.2). From
these results, we find several new phenomena exhibited in large-dimensional spectral algorithms,
including the saturation effect, the periodic plateau behavior, and the polynomial approximation
barrier.
In this paper, we only consider the convergence rate on the excess risk of optimal-tuned large-
dimensional spectral algorithms with uniform input distribution on a hypersphere. We believe that
several results in fixed-dimensional settings with input distribution on more general domains (e.g.,
Haas et al. (2024); Li et al. (2024)) can indeed be extended to large-dimensional settings, although we
must carefully consider the constants that depend on d. Furthermore, we believe that by considering
the learning curve of large-dimensional spectral algorithms (i.e., the convergence rate on the excess
risk of spectral algorithms with any regularization parameter Î» >0) or the convergence rate on the
excess risk of large-dimensional kernel interpolation (i.e., KRR with Î»= 0), further research can
find a wealth of new phenomena compared with the fixed-dimensional setting.
Acknowledgments and Disclosure of Funding
Linâ€™s research was supported in part by the National Natural Science Foundation of China (Grant
92370122, Grant 11971257). The authors are grateful to the reviewers for their constructive comments
that greatly improved the quality and presentation of this paper.
References
Aerni, M., M. Milanta, K. Donhauser, and F. Yang (2023). Strong inductive biases provably prevent
harmless interpolation. arXiv preprint arXiv:2301.07605 .
Arora, S., S. Du, W. Hu, Z. Li, and R. Wang (2019). Fine-grained analysis of optimization and
generalization for overparameterized two-layer neural networks. In International Conference on
Machine Learning , pp. 322â€“332. PMLR.
Arora, S., S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang (2019). On exact computation
with an infinitely wide neural net. Advances in Neural Information Processing Systems 32 .
Barzilai, D. and O. Shamir (2023). Generalization in kernel regression under realistic assumptions.
arXiv preprint arXiv:2312.15995 .
10Bauer, F., S. Pereverzev, and L. Rosasco (2007). On regularization algorithms in learning theory.
Journal of Complexity 23 (1), 52â€“72.
Caponnetto, A. (2006, September). Optimal rates for regularization operators in learning theory.
Technical Report CBCL Paper #264/AI Technical Report #062, Massachusetts Institute of Tech-
nology.
Caponnetto, A. and E. De Vito (2007). Optimal rates for the regularized least-squares algorithm.
Foundations of Computational Mathematics 7 (3), 331â€“368.
Caponnetto, A. and Y . Yao (2010). Cross-validation based adaptation for regularization operators in
learning theory. Analysis and Applications 8 (02), 161â€“183.
Celisse, A. and M. Wahl (2021). Analyzing the discrepancy principle for kernelized spectral filter
learning algorithms. Journal of Machine Learning Research 22 (76), 1â€“59.
Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. Journal of
the American Statistical Association 74 (368), 829â€“836.
Cui, H., B. Loureiro, F. Krzakala, and L. ZdeborovÃ¡ (2021). Generalization error rates in kernel
regression: The crossover from the noiseless to noisy regime. Advances in Neural Information
Processing Systems 34 , 10131â€“10143.
Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805 .
Dicker, L. H., D. P. Foster, and D. Hsu (2017). Kernel ridge vs. principal component regression: Mini-
max bounds and the qualification of regularization operators. Electronic Journal of Statistics 11 (1),
1022 â€“ 1047.
Dieuleveut, A., N. Flammarion, and F. Bach (2017). Harder, better, faster, stronger convergence rates
for least-squares regression. Journal of Machine Learning Research 18 (101), 1â€“51.
Donhauser, K., M. Wu, and F. Yang (2021). How rotational invariance of common kernels prevents
generalization in high dimensions. In International Conference on Machine Learning , pp. 2804â€“
2814. PMLR.
Du, S., J. Lee, H. Li, L. Wang, and X. Zhai (2019). Gradient descent finds global minima of deep
neural networks. In International Conference on Machine Learning , pp. 1675â€“1685. PMLR.
Du, S. S., X. Zhai, B. Poczos, and A. Singh (2018). Gradient descent provably optimizes over-
parameterized neural networks. arXiv preprint arXiv:1810.02054 .
Edmunds, D. E. and H. Triebel (1996). Function Spaces, Entropy Numbers, Differential Operators .
Cambridge: Cambridge University Press.
Fischer, S. and I. Steinwart (2020). Sobolev norm learning rates for regularized least-squares
algorithms. Journal of Machine Learning Research 21 (205), 1â€“38.
Gallier, J. (2009). Notes on spherical harmonics and linear representations of lie groups. preprint .
Gerfo, L. L., L. Rosasco, F. Odone, E. D. Vito, and A. Verri (2008, 07). Spectral Algorithms for
Supervised Learning. Neural Computation 20 (7), 1873â€“1897.
Ghorbani, B., S. Mei, T. Misiakiewicz, and A. Montanari (2020). When do neural networks
outperform kernel methods? Advances in Neural Information Processing Systems 33 , 14820â€“
14830.
Ghorbani, B., S. Mei, T. Misiakiewicz, and A. Montanari (2021). Linearized two-layers neural
networks in high dimension. The Annals of Statistics 49 (2), 1029 â€“ 1054.
Ghosh, N., S. Mei, and B. Yu (2021). The three stages of learning dynamics in high-dimensional
kernel methods. arXiv preprint arXiv:2111.07167 .
11Gneiting, T. (2013). Strictly and non-strictly positive definite functions on spheres. Bernoulli 19 (4),
1327 â€“ 1349.
Haas, M., D. HolzmÃ¼ller, U. Luxburg, and I. Steinwart (2024). Mind the spikes: Benign overfitting
of kernels and neural networks in fixed dimension. Advances in Neural Information Processing
Systems 36 .
He, K., X. Zhang, S. Ren, and J. Sun (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770â€“778.
Hu, H. and Y . M. Lu (2022). Sharp asymptotics of kernel ridge regression beyond the linear regime.
arXiv preprint arXiv:2205.06798 .
Hu, T., W. Wang, C. Lin, and G. Cheng (2021). Regularization matters: A nonparametric perspective
on overparametrized neural network. In International Conference on Artificial Intelligence and
Statistics , pp. 829â€“837. PMLR.
Jacot, A., F. Gabriel, and C. Hongler (2018). Neural tangent kernel: Convergence and generalization
in neural networks. Advances in Neural Information Processing Systems 31 .
Krizhevsky, A., I. Sutskever, and G. E. Hinton (2017). Imagenet classification with deep convolutional
neural networks. Communications of the ACM 60 (6), 84â€“90.
Lai, J., M. Xu, R. Chen, and Q. Lin (2023). Generalization ability of wide neural networks on R.
arXiv preprint arXiv:2302.05933 .
Li, Y ., W. Gan, Z. Shi, and Q. Lin (2024). Generalization error curves for analytic spectral algorithms
under power-law decay. arXiv preprint arXiv:2401.01599 .
Li, Y . and Y . Liang (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. Advances in Neural Information Processing Systems 31 .
Li, Y ., Z. Yu, G. Chen, and Q. Lin (2024). On the eigenvalue decay rates of a class of neural-network
related kernel functions defined on general domains. Journal of Machine Learning Research 25 (82),
1â€“47.
Li, Y ., H. Zhang, and Q. Lin (2022). On the saturation effect of kernel ridge regression. In The
Eleventh International Conference on Learning Representations .
Li, Y ., H. Zhang, and Q. Lin (2024). On the asymptotic learning curves of kernel ridge regression
under power-law decay. Advances in Neural Information Processing Systems 36 .
Liang, T. and A. Rakhlin (2020). Just interpolate: Kernel â€œRidgelessâ€ regression can generalize. The
Annals of Statistics 48 (3), 1329 â€“ 1347.
Liang, T., A. Rakhlin, and X. Zhai (2020). On the multiple descent of minimum-norm interpolants
and restricted lower isometry of kernels. In Conference on Learning Theory , pp. 2683â€“2711.
PMLR.
Lin, J., A. Rudi, L. Rosasco, and V . Cevher (2020). Optimal rates for spectral algorithms with
least-squares regression over hilbert spaces. Applied and Computational Harmonic Analysis 48 (3),
868â€“890.
Lu, W., H. Zhang, Y . Li, M. Xu, and Q. Lin (2023). Optimal rate of kernel regression in large
dimensions. arXiv preprint arXiv:2309.04268 .
Mei, S., T. Misiakiewicz, and A. Montanari (2021). Learning with invariances in random features
and kernel models. In Conference on Learning Theory , pp. 3351â€“3418. PMLR.
Mei, S., T. Misiakiewicz, and A. Montanari (2022). Generalization error of random feature and
kernel methods: Hypercontractivity and kernel matrix concentration. Applied and Computational
Harmonic Analysis 59 , 3â€“84.
12Mei, S. and A. Montanari (2022). The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathemat-
ics 75 (4), 667â€“766.
Misiakiewicz, T. (2022). Spectrum of inner-product kernel matrices in the polynomial regime and
multiple descent phenomenon in kernel ridge regression. arXiv preprint arXiv:2204.10425 .
Misiakiewicz, T. and S. Mei (2022). Learning with convolution and pooling operations in kernel
methods. Advances in Neural Information Processing Systems 35 , 29014â€“29025.
Pillaud-Vivien, L., A. Rudi, and F. Bach (2018). Statistical optimality of stochastic gradient descent
on hard learning problems through multiple passes. Advances in Neural Information Processing
Systems 31 .
Raskutti, G., M. J. Wainwright, and B. Yu (2014). Early stopping and non-parametric regression: An
optimal data-dependent stopping rule. Journal of Machine Learning Research 15 (11), 335â€“366.
Rastogi, A. and S. Sampath (2017). Optimal rates for the regularized learning algorithms under
general source condition. Frontiers in Applied Mathematics and Statistics 3 , 3.
Steinwart, I. and A. Christmann (2008). Support vector machines . Springer Science & Business
Media.
Steinwart, I., D. Hush, and C. Scovel (2009). Optimal rates for regularized least squares regression.
InConference on Learning Theory , pp. 79â€“93. PMLR.
Steinwart, I. and C. Scovel (2012). Mercerâ€™s theorem on general domains: On the interaction between
measures, kernels, and rkhss. Constructive Approximation 35 , 363â€“417.
Stone, C. J. (1977). Consistent Nonparametric Regression. The Annals of Statistics 5 (4), 595 â€“ 620.
Stone, C. J. (1994). The Use of Polynomial Splines and Their Tensor Products in Multivariate
Function Estimation. The Annals of Statistics 22 (1), 118 â€“ 171.
Suh, N., H. Ko, and X. Huo (2021). A non-parametric regression viewpoint: Generalization of
overparametrized deep relu network under noisy observations. In International Conference on
Learning Representations .
Xiao, L., H. Hu, T. Misiakiewicz, Y . M. Lu, and J. Pennington (2023). Precise learning curves and
higher-order scaling limits for dot product kernel regression. Journal of Statistical Mechanics:
Theory and Experiment 2023 (11), 114005.
Yang, Y . and A. Barron (1999). Information-theoretic determination of minimax rates of convergence.
The Annals of Statistics 27 (5), 1564 â€“ 1599.
Yao, Y ., L. Rosasco, and A. Caponnetto (2007). On early stopping in gradient descent learning.
Constructive Approximation 26 , 289â€“315.
Zhang, H., Y . Li, W. Lu, and Q. Lin (2023). On the optimality of misspecified kernel ridge regression.
InInternational Conference on Machine Learning , pp. 41331â€“41353. PMLR.
Zhang, H., Y . Li, W. Lu, and Q. Lin (2024). Optimal rates of kernel ridge regression under source
condition in large dimensions. arXiv preprint arXiv:2401.01270 .
Zhang, H., W. Lu, and Q. Lin (2024). The phase diagram of kernel interpolation in large dimensions.
arXiv preprint arXiv:2404.12597 .
13A Graphical illustration and numerical experiments of main results
A.1 Graphical illustration of Theorem 3.1, Theorem 4.1, and Theorem 4.2
Recall that Theorem 3.1, Theorem 4.1, and Theorem 4.2 determined the convergence rate on the
excess risk of: (i) large-dimensional kernel gradient flow with s >0; (ii) large-dimensional spectral
algorithm with Ï„â‰¥1andsâ‰¤Ï„; and (iii) large-dimensional spectral algorithm with Ï„â‰¥1and
s > Ï„ .
In Figure 1, we have provided a visual illustration of Theorem 4.1 and Theorem 4.2 when Ï„= 2.
Now, in Figure 2, we provide more visual illustrations of the results of spectral algorithms with
Ï„= 1,Ï„= 2,Ï„= 4, andÏ„=âˆž, which correspond to kernel ridge regression (KRR), iterated ridge
regression in Example 3, and kernel gradient flow.
(a)
 (b)
 (c)
 (d)
(e)
 (f)
 (g)
 (h)
(i)
 (j)
 (k)
 (l)
(m)
 (n)
 (o)
 (p)
Figure 2: Convergence rates of spectral algorithms with qualification Ï„= 1(KRR), Ï„= 2(iterated
ridge regression), Ï„= 4(iterated ridge regression), and Ï„=âˆž(kernel gradient flow) in Theorem
4.1, Theorem 4.2, and corresponding minimax lower rates in Theorem 3.3 with respect to dimension
d. We present four graphs corresponding to four kinds of source conditions: s= 0.01,1,3,5. The
x-axis represents asymptotic scaling, Î³:nâ‰dÎ³; the y-axis represents the convergence rate of excess
risk,r:Excess risk â‰dr.
14A.2 Numerical experiments
We conducted two experiments using two specific kernels: the RBF kernel and the NTK kernel.
Experiment 1 was designed to confirm the optimal rate of kernel gradient flow and KRR when s= 1.
Experiment 2 was designed to illustrate the saturation effect of KRR when s >1.
Experiment 1: We consider the following two inner product kernels:
(i) RBF kernel with a fixed bandwidth:
Krbf(x, xâ€²) = exp
âˆ’âˆ¥xâˆ’xâ€²âˆ¥2
2
2
, x, xâ€²âˆˆSd.
(ii) Neural Tangent Kernel (NTK) of a two-layer ReLU neural network:
Kntk(x, xâ€²) := Î¦( âŸ¨x, xâ€²âŸ©), x, xâ€²âˆˆSd,
where Î¦(t) = [sin (arccos t) + 2( Ï€âˆ’arccos t)t]/(2Ï€).
The RBF kernel satisfies Assumption 1. For the NTK, the coefficients of Î¦(Â·),{aj}âˆž
j=0, satisfy
aj>0, jâˆˆ {0,1}âˆª{2,4,6, . . .}andaj= 0, jâˆˆ {3,5,7, . . .}(see, e.g., Lu et al. (2023)). As noted
after Assumption 1, our results can be extended to inner product kernels with certain zero coefficients
aj. Specifically, for any Î³ >0, as long as aj>0forj=âŒŠÎ³âŒ‹,âŒŠÎ³âŒ‹+ 1, the proof and convergence
rate remain the same. Therefore, for Î³ <2in our experiments, the convergence rates for NTK will
be the same as for the RBF kernel.
We used the following data generation procedure:
yi=fâˆ—(xi) +Ïµi, i= 1, . . . , n,
where each xiis i.i.d. sampled from the uniform distribution on Sd, and Ïµii.i.d.âˆ¼ N (0,1).
We selected the training sample sizes nwith corresponding dimensions dsuch that n=dÎ³, Î³=
0.5,1.0,1.5,1.8. For each kernel and dimension d, we consider the following regression function fâˆ—:
fâˆ—(x) =K(u1, x) +K(u2, x) +K(u3, x),for some u1, u2, u3âˆˆSd. (19)
This function is in the RKHS H, and it is easy to prove that, for any u0âˆˆSd, Assumption 2 (b) holds
forK(u0,Â·)withs= 1. Therefore, Assumption 2 holds for s= 1. We used logarithmic least squares
to fit the excess risk with respect to the sample size, resulting in the convergence rate r. As shown in
Figure 3 and Figure 4, the experimental results align well with our theoretical findings.
Experiment 2: We use most of the settings from Experiment 1, except that the regression function
is changed to fâˆ—(x) =p
Âµs
2N(d,2)P2(< Î¾, x > )with s= 1.9,P2(t) := ( dt2âˆ’1)/(dâˆ’1)
the Gegenbauer polynomial, and Î¾âˆˆSd. Notice that the addition formula P2(< Î¾, x > ) =
1
N(d,2)PN(d,2)
j=1Y2,j(Î¾)Y2,j(x)implies that
âˆ¥fâˆ—âˆ¥2
[H]s=1
N(d,2)N(d,2)X
j=1Y2
2,j(Î¾) =P2(1) = 1 ,
hence fâˆ—âˆˆ[H]sand satisfies Assumption 2.
Our experiment settings are similar to those on page 30 of Li et al. (2022). We choose the regulariza-
tion parameter for KRR and kernel gradient flow as Î»= 0.05Â·dâˆ’Î¸. For KRR, since Corollary D.16
suggests that the optimal regularization parameter is Î»â‰dâˆ’0.7, we set Î¸= 0.7. Similarly, based on
Corollary D.16, we set Î¸= 0.5for kernel gradient flow. Additionally, we set Î³= 1.8. The results
indicate that the best convergence rate of KRR is slower than that of kernel gradient flow, implying
that KRR is inferior to kernel gradient flow when the regression function is sufficiently smooth.
B Proof of Theorem 3.3
We first restate Theorem 3.3.
152.00 2.05 2.10 2.15 2.20 2.25 2.30
log10 n2.10
2.05
2.00
1.95
1.90
1.85
1.80
1.75
1.70
log10 ErrNTK,  =0.5,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  1.03log10n + 0.37
Kernel regression
log10Err  1.09log10n + 0.42
(a)
2.70 2.75 2.80 2.85 2.90 2.95 3.00
log10 n2.1
2.0
1.9
1.8
1.7
log10 ErrNTK,  =0.8,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  0.84log10n + 0.38
Kernel regression
log10Err  0.84log10n + 0.59
 (b)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.2
2.1
2.0
1.9
1.8
1.7
log10 ErrNTK,  =1.5,  theretical rate=-2/3
KRR, parameters chosen by CV
log10Err  0.63log10n + 0.23
Kernel regression
log10Err  0.66log10n + 0.21
(c)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.0
1.9
1.8
1.7
1.6
log10 ErrNTK,  =1.8,  theretical rate=-5/9
KRR, parameters chosen by CV
log10Err  0.54log10n + 0.03
Kernel regression
log10Err  0.56log10n + 0.04
 (d)
Figure 3: Results of Experiment 1. We repeated each experiment 50 times and reported the average
excess risk for (a) kernel gradient flow (labeled as "kernel regression" in our reports) and (b) kernel
ridge regression (KRR) on 1000 test samples. We randomly selected u1, u2, u3and kept them
fixed for each repeat. We choose the stopping time tin kernel gradient flow as C1n0.5, where
C1âˆˆ {0.001,0.01,0.1,1,10,100,1000}. We use 5-fold cross-validation to select the regularization
parameter Î»in kernel ridge regression. The alternative values of Î»in cross-validation are C2nâˆ’C3,
where C2âˆˆ {0.001,0.005,0.01,0.1,0.5,1,2,5,10,40,100,300,1000}, C3âˆˆ {0.1,0.2, . . . , 1.5}.
Theorem B.1 (Restate Theorem 3.3) .Lets >0andÎ³ >0be fixed real numbers. Denote pas the
integer satisfying Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . LetPconsist of all the distributions ÏonX Ã— Y
such that Assumption 1 and Assumption 2 hold for sandÎ³. Then for any dâ‰¥C, a sufficiently large
constant only depending on s,Î³,c1, and c2, we have the following claims:
(i) When Î³âˆˆ(p(s+ 1), p+ps+s], we have
min
Ë†fmax
ÏâˆˆPE(X,Y)âˆ¼ÏâŠ—nË†fâˆ’fâ‹†2
L2â‰¥ln ln( d)
50(Î³âˆ’p(s+ 1))(ln( d))2dpâˆ’Î³.
(ii) When Î³âˆˆ(p+ps+s,(p+ 1)( s+ 1)] , we have
min
Ë†fmax
ÏâˆˆPE(X,Y)âˆ¼ÏâŠ—nË†fâˆ’fâ‹†2
L2= â„¦
dâˆ’s(p+1)
,
where â„¦only involves constants depending on s, Ïƒ, Î³, c 0, Îº, c 1, and c2.
Proof of Theorem B.1. The item (ii) is a direct corollary of Theorem 5 in Zhang et al. (2024). Now
we begin to proof the item (i). We need the following lemma.
162.00 2.05 2.10 2.15 2.20 2.25 2.30
log10 n2.3
2.2
2.1
2.0
1.9
1.8
log10 ErrRBF,  =0.5,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  1.23log10n + 0.65
Kernel regression
log10Err  1.31log10n + 0.73
(a)
2.70 2.75 2.80 2.85 2.90 2.95 3.00
log10 n2.40
2.35
2.30
2.25
2.20
2.15
2.10
2.05
log10 ErrRBF,  =0.8,  theretical rate=-1
KRR, parameters chosen by CV
log10Err  1.06log10n + 0.76
Kernel regression
log10Err  0.90log10n + 0.32
 (b)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.8
2.7
2.6
2.5
2.4
2.3
2.2
2.1
2.0
log10 ErrRBF,  =1.5,  theretical rate=-2/3
KRR, parameters chosen by CV
log10Err  0.65log10n + -0.09
Kernel regression
log10Err  0.72log10n + -0.09
(c)
3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7
log10 n2.5
2.4
2.3
2.2
2.1
2.0
1.9
log10 ErrRBF,  =1.8,  theretical rate=-5/9
KRR, parameters chosen by CV
log10Err  0.59log10n + -0.13
Kernel regression
log10Err  0.61log10n + -0.22
 (d)
Figure 4: A similar plot as Figure 3, but with the RBF kernel.
Lemma B.2 (Restate Lemma 4.1 in Lu et al. (2023)) .For any Î´âˆˆ(0,1)and any 0<ËœÎµ1,ËœÎµ2<âˆž
only depending on n,d,{Î»j},c1,c2, and Î³and satisfying
VK(ËœÎµ2,D) +nËœÎµ2
2+ ln(2)
V2(ËœÎµ1,B)â‰¤Î´, (20)
we have
min
Ë†fmax
ÏâˆˆPE(X,Y)âˆ¼ÏâŠ—nË†fâˆ’fâ‹†2
L2â‰¥1âˆ’Î´
4ËœÎµ2
1, (21)
where Ïfâ‹†is the joint-p.d.f. of x, ygiven by (1) with f=fâ‹†,B:=
fâˆˆ H,âˆ¥fâˆ¥[H]sâ‰¤RÎ³	
D:=
Ïfjoint distribution of (y, x) where xâˆ¼ÏX, y=f(x) +Ïµ, Ïµâˆ¼N(0, Ïƒ2), fâˆˆ B
,
andV2,VKare the Îµ-covering entropies ( as defined in Yang and Barron (1999); Lu et al. (2023)) of
(B, d2=âˆ¥ Â· âˆ¥2
L2)and(D, d2=KL divergence ).
Suppose Î³âˆˆ(p(s+ 1), p+ps+s]. LetC(p) =C12/10be a constant only depending on Î³, where
C12are given in Lemma D.13. Then we introduce
ËœÎµ2
1â‰œdpâˆ’Î³/ln(d)andËœÎµ2
2â‰œC(p)dp
nln ln( d). (22)
173.0 3.1 3.2 3.3 3.4
log10 n4.5
4.0
3.5
3.0
2.5
2.0
1.5
log10 ErrNTK,  =1.8
Kernel gradient flow, =0.5
log10Err  0.88log10n + -1.37
KRR, =0.7
log10Err  0.52log10n + 0.09
Figure 5: Results of Experiment 2. It can be seen that the best rate of excess risk for KRR is slower
than that of kernel gradient flow.
Let us further assume that dâ‰¥C, where Cis a sufficiently large constant only depending on Î³,s, and
c1. By Lemma D.11 and Lemma D.13 we have
ËœÎµ2
1=dpâˆ’Î³/ln(d)<C9
dpsâ‰¤Âµs
p
Âµs
p+1<ËœÎµ2
2=C(p)dp
nln ln( d)â‰¤C(p)
c1dpâˆ’Î³ln ln( d)< Âµs
p
nËœÎµ2
2Definition of C12â‰¤1
10N(d, p) ln ln( d).(23)
Therefore, for any dâ‰¥C, where Cis a sufficiently large constant only depending on s,Î³, andc1, we
have
V2(ËœÎµ1,B)Lemma A.5 in Lu et al. (2023)
â‰¥ K(ËœÎµ1)â‰¥1
2N(d, p) lnÂµs
p
ËœÎµ2
1
Definition of ËœÎµ2
1â‰¥1
2N(d, p) ln
C9dÎ³âˆ’p(s+1)ln(d)
â‰¥1
2N(d, p)
(Î³âˆ’p(s+ 1)) ln( d) +1
2ln ln( d)
.(24)
On the other hand, from Lemma D.11, Lemma D.13, and Lemma D.12, one can check the following
claim:
Claim 1. Suppose Î³âˆˆ(p(s+ 1), p+ps+s]. For any dâ‰¥C, where Cis a sufficiently large
constant only depending on s,Î³,c1, and c2, we have
Kâˆš
2ÏƒËœÎµ2/6
â‰¤1
2N(d, p) ln18Âµs
p
Ïƒ2ËœÎµ2
2ln ln( d)
.
18Therefore, for any dâ‰¥C, where Cis a sufficiently large constant only depending on s,Î³,c1, and c2,
we have
VK(ËœÎµ2,D) =V2(âˆš
2ÏƒËœÎµ2,B)Lemma A.5 in Lu et al. (2023)
â‰¤ Kâˆš
2ÏƒËœÎµ2/6
Claim 1
â‰¤1
2N(d, p) ln18Âµs
p
Ïƒ2ËœÎµ2
2ln ln( d)
Definition of ËœÎµ2
2â‰¤1
2N(d, p) ln
18C10Ïƒâˆ’2[C(p)]âˆ’1c2dÎ³âˆ’p(s+1)
â‰¤1
2N(d, p)
(Î³âˆ’p(s+ 1)) ln( d) +1
5ln ln( d)
.(25)
Combining (23), (24), and (25), we finally have:
VK(ËœÎµ2,D) +nËœÎµ2
2+ ln(2)
V2(ËœÎµ1,B)â‰¤[10(Î³âˆ’p(s+ 1)) ln( d) + 4 ln ln( d)]
[10(Î³âˆ’p(s+ 1)) ln( d) + 5 ln ln( d)]<1,
and from Lemma B.2, we get
min
Ë†fmax
fâ‹†âˆˆBE(X,y)âˆ¼ÏâŠ—n
fâ‹†Ë†fâˆ’fâ‹†2
L2â‰¥ln ln( d)
4 ln(d) [10( Î³âˆ’p(s+ 1)) ln( d) + 5 ln ln( d)]dpâˆ’Î³
â‰¥ln ln( d)
50(Î³âˆ’p(s+ 1))(ln( d))2dpâˆ’Î³,
finishing the proof. â– 
C Definition of analytic filter functions
We first introduce the following definition of analytic filter functions (Bauer et al. (2007); Li et al.
(2024)).
Definition C.1 (Analytic filter functions) .Let
Ï†Î»: [0, Îº2]â†’Râ‰¥0|Î»âˆˆ(0,1)	
be a family of
functions indexed with regularization parameter Î»and define the remainder function
ÏˆÎ»(z) := 1 âˆ’zÏ†Î»(z). (26)
We say that {Ï†Î»|Î»âˆˆ(0,1)}(or simply Ï†Î»(z)) is an analytic filter function if:
(1)zÏ†Î»(z)âˆˆ[0,1]is non-decreasing with respect to zand non-increasing with respect to Î».
(2)The qualification of this filter function is Ï„âˆˆ[1,âˆž]such that âˆ€0â‰¤Ï„â€²â‰¤Ï„(and also
Ï„â€²<âˆž), there exist positive constants Cionly depending on Ï„â€²,i= 1,2,3,4,5, such that
we have
Ï†Î»(z)â‰¥C1zâˆ’1, Ïˆ Î»(z)â‰¤C2(z/Î»)âˆ’Ï„â€²,âˆ€Î»âˆˆ(0,1), z > Î» (27)
C3â‰¤Î»Ï†Î»(z)â‰¤C4, Ïˆ Î»(z)â‰¥C5,âˆ€Î»âˆˆ(0,1), zâ‰¤Î». (28)
(3)IfÏ„ <âˆž, then there exists a positive constant C6only depending on Ï„andÎ»1, such that we
have
ÏˆÎ»(Î»1)â‰¥C6Î»Ï„, (29)
where Î»1is the largest eigenvalue of Kdefined in (5); and there exist positive constants C7
andC8only depending on Ï„, such that we have
(z/Î»)2Ï„Ïˆ2
Î»(z)â‰¥C7,âˆ€Î»âˆˆ(0,1), z > Î» (30)
(z/Î»)2Ï„Ïˆ2
Î»(z)â‰¤C8zÏ†Î»(z),âˆ€Î»âˆˆ(0,1), zâ‰¤Î». (31)
(4) Let
DÎ»=
zâˆˆC: Rezâˆˆ[âˆ’Î»/2, Îº2],|Imz| â‰¤Rez+Î»/2	
âˆª
zâˆˆC:zâˆ’Îº2â‰¤Îº2+Î»/2,Rezâ‰¥Îº2	
;
Then Ï†Î»(z)can be extended to be an analytic function on some domain containing DÎ»and
the following conditions holds for all Î»âˆˆ(0,1):
19(C1)|(z+Î»)Ï†Î»(z)| â‰¤ËœEfor all zâˆˆDÎ»;
(C2)|(z+Î»)ÏˆÎ»(z)| â‰¤ËœFÎ»for all zâˆˆDÎ»;
where ËœE,ËœFare positive constants.
Remark C.2.We remark that some of the above properties are not essential for the definition of filter
functions in the literature (Bauer et al., 2007; Gerfo et al., 2008), but we introduce them to avoid
some unnecessary technicalities in the proof. The requirements of analytic filter functions are first
considered in Li et al. (2024) and used for their â€œanalytic functional argumentâ€, which will also be
vital in our proof.
The following examples show many commonly used analytic filter functions and their proofs can be
found in Lemma C.3, see also Li et al. (2024).
Example 3 (Iterated ridge regression) .Letqâ‰¥1be fixed. We define
Ï†IT,q
Î»(z) =1
z
1âˆ’Î»q
(z+Î»)q
, ÏˆIT,q
Î»(z) =Î»q
(z+Î»)q, Ï„ =q. (32)
Example 4 (Kernel gradient descent) .The gradient descent method is the discrete version of gradient
flow. Let Î· >0be a fixed step size. Then, iterating gradient descent with respect to the empirical loss
tsteps yields the filter function
Ï†GD
Î»(z) =Î·tâˆ’1X
k=0(1âˆ’Î·z)k=1âˆ’(1âˆ’Î·z)t
z, Î» = (Î·t)âˆ’1, (33)
ÏˆGD
Î»(z) = (1 âˆ’Î·z)t, Ï„ =âˆž. (34)
Moreover, when Î·is small enough, say Î· <1/(2Îº2), we have Re(1âˆ’Î·z)>0forzâˆˆDÎ», so we can
take the single-valued branch of (1âˆ’Î·z)teven when tis not an integer. Therefore, we can extend
the definition of the filter function so that Î»can be arbitrary and t= (Î·Î»)âˆ’1.
Lemma C.3. Ï†KRR
Î»,Ï†IT,q
Î»,Ï†GF
Î», and Ï†GD
Î»are analytic filter functions.
Proof. Notice that (i) zâ‰¤z+Î»â‰¤2zwhen z > Î» ; and that (ii) Î»â‰¤z+Î»â‰¤2Î»when zâ‰¤Î».
Hence, the constants C1,C2,C3,C4, andC6are given in Li et al. (2024).
ForC5, when zâ‰¤Î», we can take C5= min {1/2,2âˆ’q, eâˆ’1, eâˆ’1}>0.
ForC7, when z > Î» , we have
(z/Î»)2Ï„(ÏˆKRR
Î»(z))2=z
z+Î»2
â‰¥1/4
(z/Î»)2Ï„(ÏˆIT,q
Î»(z))2=z
z+Î»2q
â‰¥2âˆ’2q.
ForC8, when zâ‰¤Î», we have
z2Ï„âˆ’1(ÏˆKRR
Î»(z))2
Î»2Ï„Ï†KRR
Î»(z)=z
z+Î»â‰¤1
2
z2Ï„âˆ’1(ÏˆIT,q
Î»(z))2
Î»2Ï„Ï†IT,q
Î»(z)=z2q
(z+Î»)2qâˆ’[Î»(z+Î»)]qâ‰¤1
22qâˆ’2q.
â– 
D Proof of Theorem 4.1 and Theorem 4.2
D.1 Bias-variance decomposition
We first apply a standard bias-variance decomposition on the excess risk of spectral algorithms, and
readers can also refer to Zhang et al. (2023, 2024) for more details.
20Recall the definition of Ë†gZandË†fÎ»in (14) and (15). Letâ€™s define their conditional expectations as
ËœgZ:=E(Ë†gZ|X) =1
nnX
i=1Kxifâ‹†(xi)âˆˆ H; (35)
and
ËœfÎ»:=E
Ë†fÎ»|X
=Ï†Î»(TX) ËœgZâˆˆ H. (36)
Letâ€™s also define their expectations as
g=EË†gZ=Z
XK(x,Â·)fâ‹†(x)dÏX(x)âˆˆ H, (37)
and
fÎ»=Ï†Î»(T)g. (38)
Then we have the decomposition
Ë†fÎ»âˆ’fâ‹†=1
nÏ†Î»(TX)nX
i=1Kxiyiâˆ’fâ‹†
=1
nÏ†Î»(TX)nX
i=1Kxi(fâˆ—
Ï(xi) +Ïµi)âˆ’fâ‹†
=Ï†Î»(TX) ËœgZ+1
nnX
i=1Ï†Î»(TX)KxiÏµiâˆ’fâ‹†
=
ËœfÎ»âˆ’fâ‹†
+1
nnX
i=1Ï†Î»(TX)KxiÏµi. (39)
Taking expectation over the noise Ïµconditioned on Xand noticing that Ïµ|Xare independent noise
with mean 0 and variance Ïƒ2, we obtain the bias-variance decomposition:
EË†fÎ»âˆ’fâ‹†2
L2X
=Bias2(Î») +Var(Î»), (40)
where
Bias2(Î») :=ËœfÎ»âˆ’fâ‹†2
L2,Var(Î») :=Ïƒ2
n2nX
i=1âˆ¥Ï†Î»(TX)K(xi,Â·)âˆ¥2
L2. (41)
Given the decomposition (40), we next derive the upper and lower bounds of Bias2(Î»)andVar(Î»)
in the following two subsections.
Before we close this subsection, letâ€™s introduce some quantities and an assumption that will be used
frequently in our proof later. Denote the true function as fâ‹†=âˆžP
i=1fiÏ•i(x), letâ€™s define the following
quantities:
N1,Ï†(Î») =âˆžX
j=1[Î»jÏ†Î»(Î»j)] ;N2,Ï†(Î») =âˆžX
j=1[Î»jÏ†Î»(Î»j)]2;
M1,Ï†(Î») = ess sup
xâˆˆXâˆžX
j=1(ÏˆÎ»(Î»j)fjÏ•j(x));M2,Ï†(Î») =âˆžX
j=1(ÏˆÎ»(Î»j)fj)2;(42)
moreover, when Ï†Î»=Ï†KRR
Î», we denote Nk(Î») =Nk,Ï†KRR(Î»)andMk(Î») =Mk,Ï†KRR(Î»)for simplic-
ity, where k= 1,2.
Assumption 3.Suppose that
ess sup
xâˆˆXâˆžX
j=1[Î»jÏ†Î»(Î»j)]2Ï•2
j(x)â‰¤ N 2,Ï†(Î»); (43)
21and
ess sup
xâˆˆXâˆžX
j=1[Î»jÏ†Î»(Î»j)]Ï•2
j(x)â‰¤ N 1,Ï†(Î»); (44)
and
ess sup
xâˆˆXâˆžX
j=1
Î»jÏ†KRR
Î»(Î»j)
Ï•2
j(x)â‰¤ N 1(Î»). (45)
For simplicity of notations, we denote hx(Â·) =K(x,Â·),xâˆˆ X in the rest of the proof. Moreover, we
denote TÎ»:= (T+Î»)âˆ’1andTXÎ»:= (TX+Î»)âˆ’1.
D.2 Variance term
The following proposition rewrites the variance term using the empirical semi-norm.
Proposition D.1 (Restate Lemma 9 in Zhang et al. (2024)) .The variance term in (41) satisfies that
Var(Î») =Ïƒ2
nZ
Xâˆ¥Ï†Î»(TX)hx(Â·)âˆ¥2
L2,ndÏX(x). (46)
The operator form (46) allows us to apply concentration inequalities and establish the following
two-step approximation.
Z
Xâˆ¥Ï†Î»(TX)hxâˆ¥2
L2,ndÏX(x)Aâ‰ˆZ
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x)Bâ‰ˆZ
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2dÏX(x).
(47)
Approximation B The following lemma characterizes the magnitude of Approximation B in high
probability. Recall the definitions of N1,Ï†(Î»)andN2,Ï†(Î»)in (42).
Lemma D.2 (Approximation B) .Suppose that (43) in Assumption 3 holds. Then, for any fixed
Î´âˆˆ(0,1), with probability at least 1âˆ’Î´, we have
1
2Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2dÏX(x)âˆ’R2 (48)
â‰¤Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x) (49)
â‰¤3
2Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2dÏX(x) +R2, (50)
where
R2=5N2,Ï†(Î»)
3nln2
Î´. (51)
Proof. Define a function
f(z) =Z
X(Ï†Î»(T)hx(z))2dÏX(x)
=Z
XâˆžX
j=1(Î»jÏ†Î»(Î»j))2Ï•2
j(x)Ï•2
j(z)dÏX(x)
=âˆžX
j=1(Î»jÏ†Î»(Î»j))2Ï•2
j(z). (52)
Since (43) in Assumption 3 holds, we have
âˆ¥fâˆ¥Lâˆžâ‰¤ N 2,Ï†(Î»);âˆ¥fâˆ¥L1=N2,Ï†(Î»).
22Applying Proposition 34 in Zhang et al. (2024) forâˆšfand noticing that âˆ¥âˆšfâˆ¥Lâˆž=p
âˆ¥fâˆ¥Lâˆž=
N2,Ï†(Î»)1
2, we have
1
2p
f2
L2âˆ’5N2,Ï†(Î»)
3nln2
Î´â‰¤p
f2
L2,nâ‰¤3
2p
f2
L2+5N2,Ï†(Î»)
3nln2
Î´, (53)
with probability at least 1âˆ’Î´.
On the one hand, we have
p
f2
L2,n=Z
Xf(z)dPn(z) =Z
XZ
X(Ï†Î»(T)hx(z))2dÏX(x)
dPn(z)
=Z
XZ
X(Ï†Î»(T)hx(z))2dPn(z)
dÏX(x)
=Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x).
On the other hand, we have
p
f2
L2=Z
Xf(z)dÏX(z)
=Z
XZ
X(Ï†Î»(T)hx(z))2dÏX(x)
dÏX(z)
=Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2dÏX(x).
Therefore, (53) implies the desired results. â– 
Approximation A
Lemma D.3. Suppose that (43) and (45) in Assumption 3 hold. Suppose that there exists a constant Ïµ
only depending on sandÎ³, such that Î»=Î»(n, d)satisfies nÏµâˆ’1N1(Î»)â†’0. Then there exists an
absolute constant C1, such that for any fixed Î´âˆˆ(0,1), when nis sufficiently large, with probability
at least 1âˆ’Î´, we have
Z
Xâˆ¥Ï†Î»(TX)hxâˆ¥2
L2,ndÏX(x)âˆ’Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x)(54)
â‰¤C1q
N2,Ï†(Î») +C1p
vN1(Î») lnÎ»âˆ’1
Â·p
vN1(Î») lnÎ»âˆ’1, (55)
where v=N1(Î»)
nlnn.
Remark D.4.The proof of Lemma D.3 is mainly based on Lemma 4.18 in Li et al. (2024). Notice
that we replace the Assumption 2 in Li et al. (2024) by (45) in Assumption 3 (borrowed from Zhang
et al. (2024)), since both of them can deduce same results given by Lemma 4.2 in Li et al. (2024) or
Lemma 37 in Zhang et al. (2024).
Proof. We start with
D=|âˆ¥Ï†Î»(TX)hxâˆ¥L2âˆ’ âˆ¥Ï†Î»(T)hxâˆ¥L2| â‰¤T1
2[Ï†Î»(T)âˆ’Ï†Î»(TX)]hx
H.
Using operator calculus, we get
T1
2[Ï†Î»(T)âˆ’Ï†Î»(TX)]hx
=T1
21
2Ï€iI
Î“Î»RTX(z)(Tâˆ’TX)RT(z)Ï†Î»(z)dz
hx
=1
2Ï€iI
Î“Î»T1
2(TXâˆ’z)âˆ’1(Tâˆ’TX)(Tâˆ’z)âˆ’1hxÏ†Î»(z)dz
=1
2Ï€iI
Î“Î»T1
2Tâˆ’1
2
Î»Â·T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»Â·T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»hxÏ†Î»(z)dz.
23Therefore, taking the norms yields
Dâ‰¤1
2Ï€T1
2Tâˆ’1
2
Î»Â·T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»Â·T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»
Â·Tâˆ’1
2
Î»hx
HI
Î“Î»|Ï†Î»(z)dz|
=1
2Ï€Â·IÂ·IIÂ·IIIÂ·IVÂ·VÂ·I
Î“Î»|Ï†Î»(z)dz|
â‰¤1
2Ï€Â·1Â·âˆš
6CÂ·r
N1(Î»)
nlnnÂ·CÂ·p
N1(Î»)I
Î“Î»|Ï†Î»(z)dz|,
where in the second estimation, we use ( I) operator calculus, ( IIandIV) Proposition E.8, ( III)
Lemma E.7, and ( V) Lemma 37 in Zhang et al. (2024) for each term respectively. Finally, from (63)
in Li et al. (2024), we getI
Î“Î»|Ï†Î»(z)dz| â‰¤ClnÎ»âˆ’1, (56)
and thus there exists an absolute constant C1, such that we have
D=|âˆ¥Ï†Î»(TX)hxâˆ¥L2âˆ’ âˆ¥Ï†Î»(T)hxâˆ¥L2| â‰¤C1p
vN1(Î») lnÎ»âˆ’1.
On the other hand, combining (52) and (43) in Assumption 3, we have âˆ¥Ï†Î»(T)hxâˆ¥2
L2â‰¤ N 2,Ï†(Î»),
and hence
âˆ¥Ï†Î»(TX)hxâˆ¥L2+âˆ¥Ï†Î»(T)hxâˆ¥L2â‰¤2âˆ¥Ï†Î»(T)hxâˆ¥L2+D
â‰¤q
N2,Ï†(Î») +C1p
vN1(Î») lnÎ»âˆ’1.
Finally,âˆ¥Ï†Î»(TX)hxâˆ¥2
L2âˆ’ âˆ¥Ï†Î»(T)hxâˆ¥2
L2
=|âˆ¥Ï†Î»(TX)hxâˆ¥L2âˆ’ âˆ¥Ï†Î»(T)hxâˆ¥L2|(âˆ¥Ï†Î»(TX)hxâˆ¥L2+âˆ¥Ï†Î»(T)hxâˆ¥L2)
â‰¤C1q
N2,Ï†(Î») +C1p
vN1(Î») lnÎ»âˆ’1
Â·p
vN1(Î») lnÎ»âˆ’1,
and henceZ
Xâˆ¥Ï†Î»(TX)hxâˆ¥2
L2,ndÏX(x)âˆ’Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x)
â‰¤1
nnX
i=1âˆ¥Ï†Î»(TX)hxiâˆ¥2
L2âˆ’ âˆ¥Ï†Î»(T)hxiâˆ¥2
L2
â‰¤sup
xâˆˆXâˆ¥Ï†Î»(TX)hxâˆ¥2
L2âˆ’ âˆ¥Ï†Î»(T)hxâˆ¥2
L2
â‰¤C1q
N2,Ï†(Î») +C1p
vN1(Î») lnÎ»âˆ’1
Â·p
vN1(Î») lnÎ»âˆ’1,
â– 
Final proof of the variance term Now we are ready to state the theorem about the variance term.
Theorem D.5. Suppose that (43) and (45) in Assumption 3 hold. Suppose there exists a constant
Ïµ >0only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
N1(Î»)Â·nÏµâˆ’1â†’0, (57)
N2
1(Î»)
nN2,Ï†(Î»)Â·ln(n)(lnÎ»âˆ’1)2â†’0; (58)
then we have
Var(Î») = [1 + oP(1)]Ïƒ2
nN2,Ï†(Î»). (59)
24Proof. Recall that Var(Î») =Ïƒ2
nR
Xâˆ¥Ï†Î»(TX)hxâˆ¥2
L2,ndÏX(x). Hence, when nis large enough,
with probability at least 1âˆ’Î´we have
Z
Xâˆ¥Ï†Î»(TX)hxâˆ¥2
L2,ndÏX(x)âˆ’Z
âˆ¥Ï†Î»(T)hxâˆ¥2
L2dÏX(x)
â‰¤Z
Xâˆ¥Ï†Î»(TX)hxâˆ¥2
L2,ndÏX(x)âˆ’Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x)
+Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x)âˆ’Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2dÏX(x)
Lemma D.2
â‰¤Z
Xâˆ¥Ï†Î»(TX)hxâˆ¥2
L2,ndÏX(x)âˆ’Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2,ndÏX(x)+5N2,Ï†(Î»)
3nln2
Î´
Lemma D.3
â‰¤q
N2,Ï†(Î»)Â·C1p
vN1(Î») lnÎ»âˆ’1+C2
1vN1(Î»)(lnÎ»âˆ’1)2
+5N2,Ï†(Î»)
3nln2
Î´
Definition of v=r
N2,Ï†(Î»)
nN1(Î»)Â·C1p
ln(n) lnÎ»âˆ’1+N2
1(Î»)
nÂ·C2
1ln(n)(lnÎ»âˆ’1)2+N2,Ï†(Î»)
nÂ·5
3ln2
Î´
=IÂ·C1p
ln(n) lnÎ»âˆ’1+IIÂ·C2
1ln(n)(lnÎ»âˆ’1)2+IIIÂ·5
3ln2
Î´.
When nâ‰¥C, a sufficiently large constant only depending on Î³andC1, we have
IÂ·C1p
ln(n) lnÎ»âˆ’1â‰¤1
6N2,Ï†(Î»).
Furthermore, whenN2
1(Î»)
nN2,Ï†(Î»)Â·nÏµâ†’0, we have IÂ·C1p
ln(n) lnÎ»âˆ’1/N2,Ï†(Î»)â†’0andIIÂ·
C2
1ln(n)(lnÎ»âˆ’1)2/N2,Ï†(Î»)â†’0.
Finally, from (52) we have
âˆ¥Ï†Î»(T)hxâˆ¥2
L2=âˆžX
i=1(Î»jÏ†Î»(Î»j))2Ï•2
i(z),
and thus the deterministic term writes
Z
Xâˆ¥Ï†Î»(T)hxâˆ¥2
L2dÏX(x) =N2,Ï†(Î»).
â– 
D.3 Bias term
In this subsection, our goal is to determine the upper and lower bounds of bias under some approxi-
mation conditions.
The triangle inequality implies that
Bias(Î») =ËœfÎ»âˆ’fâ‹†
L2â‰¥ âˆ¥fÎ»âˆ’fâ‹†âˆ¥L2âˆ’ËœfÎ»âˆ’fÎ»
L2
Bias(Î»)â‰¤ âˆ¥fÎ»âˆ’fâ‹†âˆ¥L2+ËœfÎ»âˆ’fÎ»
L2.(60)
The following lemma characterizes the dominant term of Bias(Î»).
Lemma D.6. For any Î» >0, we have
âˆ¥fÎ»âˆ’fâ‹†âˆ¥L2=M2,Ï†(Î»)1
2. (61)
25Proof. We have
âˆ¥fÎ»âˆ’fâ‹†âˆ¥2
L2=âˆžX
i=1Î»iÏ†Î»(Î»i)fiÏ•i(x)âˆ’âˆžX
i=1fiÏ•i(x)2
L2
=âˆžX
i=1ÏˆÎ»(Î»i)fiÏ•i(x)2
L2
=âˆžX
i=1(ÏˆÎ»(Î»i)fi)2
=M2,Ï†(Î»).
â– 
The following lemma bounds the remainder term of Bias(Î»)when sâ‰¥1.
Lemma D.7. Suppose that (45) in Assumption 3 holds. Suppose that there exist constants ÏµandC
only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
nÏµâˆ’1N1(Î»)â†’0, (62)
N1(Î»)M2
1,Ï†(Î»)
n2=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
, (63)
N1(Î»)
nln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1Î»2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
i=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
; (64)
then we have
ËœfÎ»âˆ’fÎ»2
L2=oP
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
. (65)
Proof. Do the decomposition,
ËœfÎ»âˆ’fÎ»=Ï†Î»(TX)ËœgXâˆ’(ÏˆÎ»(TX) +Ï†Î»(TX)TX)fÎ»
=Ï†Î»(TX)(ËœgXâˆ’TXfÎ»)âˆ’ÏˆÎ»(TX)TÏ†Î»(T)fâ‹†
=Ï†Î»(TX)(ËœgXâˆ’TXfÎ»)âˆ’Ï†Î»(TX)ÏˆÎ»(T)g+Ï†Î»(TX)ÏˆÎ»(T)gâˆ’ÏˆÎ»(TX)TÏ†Î»(T)fâ‹†
=Ï†Î»(TX) [ËœgXâˆ’TXfÎ»âˆ’ÏˆÎ»(T)g] + [Ï†Î»(TX)ÏˆÎ»(T)Tfâ‹†âˆ’ÏˆÎ»(TX)TÏ†Î»(T)fâ‹†]
=Ï†Î»(TX)(ËœgXâˆ’TXfÎ»âˆ’g+TfÎ») + (Ï†Î»(TX)TÏˆÎ»(T)âˆ’ÏˆÎ»(TX)TÏ†Î»(T))fâ‹†
=I+II.
(66)
Bound on I:For the first term in (66), we have
âˆ¥Iâˆ¥L2=âˆ¥Ï†Î»(TX)(ËœgXâˆ’TXfÎ»âˆ’g+TfÎ»)âˆ¥L2
=T1
2Ï†Î»(TX)(ËœgXâˆ’TXfÎ»âˆ’g+TfÎ»)
H
â‰¤T1
2Tâˆ’1
2
Î»Â·T1
2
Î»Ï†Î»(TX)T1
2
Î»Â·Tâˆ’1
2
Î»[(ËœgXâˆ’TXfÎ»)âˆ’(gâˆ’TfÎ»)]
H
(72) in Zhang et al. (2024)
â‰¤T1
2
Î»Ï†Î»(TX)T1
2
Î»Â·Tâˆ’1
2
Î»[(ËœgXâˆ’TXfÎ»)âˆ’(gâˆ’TfÎ»)]
H
Proposition E.1
â‰¤ 4T1
2
Î»Tâˆ’1
XÎ»T1
2
Î»Â·Tâˆ’1
2
Î»[(ËœgXâˆ’TXfÎ»)âˆ’(gâˆ’TfÎ»)]
H
(62) and(73) in Zhang et al. (2024)
â‰¤ 12Tâˆ’1
2
Î»[(ËœgXâˆ’TXfÎ»)âˆ’(gâˆ’TfÎ»)]
H,
26Denote Î¾i=Î¾(xi) =Tâˆ’1
2
Î»(Kxifâ‹†(xi)âˆ’TxifÎ»). To use Bernstein inequality, we need to bound the
m-th moment of Î¾(x):
Eâˆ¥Î¾(x)âˆ¥m
H=ETâˆ’1
2
Î»Kx(fâ‹†âˆ’fÎ»(x))m
H
â‰¤ETâˆ’1
2
Î»K(x,Â·)m
HE 
|(fâ‹†âˆ’fÎ»(x))|mx
. (67)
Note that Lemma 37 in Zhang et al. (2024) shows that
Tâˆ’1
2
Î»K(x,Â·)
Hâ‰¤ N 1(Î»)1
2, Âµ-a.e.xâˆˆ X;
By definition of M1,Ï†(Î»), we also have
âˆ¥fÎ»âˆ’fâ‹†âˆ¥Lâˆž=âˆžX
i=1ÏˆÎ»(Î»i)fiÏ•i(x)
Lâˆž=M1,Ï†(Î»). (68)
In addition, we have proved in Lemma D.6 that
E|(fÎ»(x)âˆ’fâ‹†(x))|2=M2,Ï†(Î»).
So we get the upper bound of (67), i.e.,
(67)â‰¤ N 1(Î»)m
2Â· âˆ¥fÎ»âˆ’fâ‹†âˆ¥mâˆ’2
LâˆžÂ·E|(fÎ»(x)âˆ’fâ‹†(x))|2
=N1(Î»)m
2M1,Ï†(Î»)mâˆ’2M2,Ï†(Î»)
=
N1(Î»)1
2M1,Ï†(Î»)mâˆ’2
N1(Î»)1
2M2,Ï†(Î»)1
22
.
Using Lemma 36 in Zhang et al. (2024) with therein notations: L=N1(Î»)1
2M1,Ï†(Î»)andÏƒ=
N1(Î»)1
2M2,Ï†(Î»)1
2, for any fixed Î´âˆˆ(0,1), with probability at least 1âˆ’Î´, we have
âˆ¥Iâˆ¥L2â‰¤12Â·4âˆš
2 log2
Î´ 
N1(Î»)1
2M1,Ï†(Î»)
n+N1(Î»)1
2M2,Ï†(Î»)1
2âˆšn!
. (69)
Bound on II:For the second term in (66), we have
âˆ¥IIâˆ¥L2=âˆ¥(Ï†Î»(TX)TÏˆÎ»(T)âˆ’ÏˆÎ»(TX)TÏ†Î»(T))fâ‹†âˆ¥L2
â‰¤T1
2(Ï†Î»(TX)TÏˆÎ»(T)âˆ’ÏˆÎ»(T)TÏ†Î»(T))fâ‹†
H
+T1
2(ÏˆÎ»(TX)TÏ†Î»(T)âˆ’ÏˆÎ»(T)TÏ†Î»(T))fâ‹†
H.(70)
For the first term in (70), we still employ the analytic functional argument:
T1
2(Ï†Î»(TX)TÏˆÎ»(T)âˆ’ÏˆÎ»(T)TÏ†Î»(T))fâ‹†
=T1
2(Ï†Î»(TX)âˆ’Ï†Î»(T))TÏˆÎ»(T)fâ‹†
=1
2Ï€iI
Î“Î»T1
2(TXâˆ’z)âˆ’1(TXâˆ’T)(Tâˆ’z)âˆ’1Ï†Î»(z)TÏˆÎ»(T)fâ‹†dz
=1
2Ï€iI
Î“Î»T1
2Tâˆ’1
2
Î»Â·T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»
Â·T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»T1
2Â·T1
2ÏˆÎ»(T)fâ‹†Ï†Î»(z)dz.
27Therefore,
2Ï€âˆ¥T1
2(Ï†Î»(TX)TÏˆÎ»(T)âˆ’ÏˆÎ»(T)TÏ†Î»(T))fâ‹†âˆ¥H
â‰¤I
Î“Î»T1
2Tâˆ’1
2
Î»Â·T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»
Â·T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»T1
2Â·T1
2ÏˆÎ»(T)fâ‹†
H|Ï†Î»(z)dz|
(72) in Zhang et al. (2024)
â‰¤I
Î“Î»T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»
Â·T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»Â·T1
2ÏˆÎ»(T)fâ‹†
H|Ï†Î»(z)dz|
(45) and Proposition E.8
â‰¤âˆš
6C2I
Î“Î»Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»
Â·T1
2ÏˆÎ»(T)fâ‹†
H|Ï†Î»(z)dz|
Lemma E.7
â‰¤âˆš
6C2âˆšvI
Î“Î»T1
2ÏˆÎ»(T)fâ‹†
H|Ï†Î»(z)dz|
Definition of M2,Ï†(Î»)=âˆš
6C2âˆšvM1/2
2,Ï†(Î»)I
Î“Î»|Ï†Î»(z)dz|
(56)
â‰¤âˆš
6C3âˆšvM1/2
2,Ï†(Î») lnÎ»âˆ’1,(71)
where v=N1(Î»)
nlnn.
For the second term in (70), we have
T1
2(ÏˆÎ»(TX)TÏ†Î»(T)âˆ’ÏˆÎ»(T)TÏ†Î»(T))fâ‹†
=T1
21
2Ï€iI
Î“Î»RTX(z)(Tâˆ’TX)RT(z)ÏˆÎ»(z)dz
TÏ†Î»(T)fâ‹†
=1
2Ï€iI
Î“Î»T1
2(TXâˆ’z)âˆ’1(Tâˆ’TX)(Tâˆ’z)âˆ’1ÏˆÎ»(z)TÏ†Î»(T)fâ‹†dz
=1
2Ï€iZ
Î“Î»T1
2Tâˆ’1
2
Î»Â·T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»
Â·T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»TÏ†Î»(T)fâ‹†ÏˆÎ»(z)dz.
Hence, similar to (71), we have
2Ï€T1
2(ÏˆÎ»(TX)TÏ†Î»(T)âˆ’ÏˆÎ»(T)TÏ†Î»(T))fâ‹†
H
â‰¤Z
Î“Î»T1
2Tâˆ’1
2
Î»Â·T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»
Â·T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»Â·Tâˆ’1
2
Î»TÏ†Î»(T)fâ‹†
H|ÏˆÎ»(z)dz|
â‰¤âˆš
6C2âˆšvTâˆ’1
2
Î»TÏ†Î»(T)fâ‹†
HZ
Î“Î»|ÏˆÎ»(z)dz|
Definition of analytic filter functions
â‰¤âˆš
6C2âˆšvTâˆ’1
2
Î»TÏ†Î»(T)fâ‹†
HCËœFÎ»lnÎ»âˆ’1.(72)
28Combining (66), (69), (70), (71), and (72), there exists a constant C1only depending on Î´andËœF,
such that we haveËœfÎ»âˆ’fÎ»
L2
â‰¤C1 
N1(Î»)1
2M1,Ï†(Î»)
n+N1(Î»)1
2M2,Ï†(Î»)1
2âˆšn!
+C1âˆšvM1/2
2,Ï†(Î») lnÎ»âˆ’1+C1âˆšvTâˆ’1
2
Î»TÏ†Î»(T)fâ‹†
HÎ»lnÎ»âˆ’1
(62)
â‰¤ 
nâˆ’1N1(Î»)1/2Â·C1C1/2Â·(M2,Ï†(Î»))1/2
+ 
nâˆ’1N1(Î»)1/2Â·C1Â·(M2,Ï†(Î»))1/2
+ 
nÏµâˆ’1N1(Î»)1/2Â·C1Â·(M2,Ï†(Î»))1/2
+o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)1/2
.(73)
â– 
When s <1, we can use the following lemma to bound the remainder term of Bias(Î»). This lemma
is a modification of Lemma D.7, and its proof is partly based on Lemma 26 in Zhang.
Lemma D.8. Suppose that (45) in Assumption 3 holds. Suppose that there exist constants ÏµandC
only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
nÏµâˆ’1N1(Î»)â†’0, (74)
N1(Î»)
nln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1Î»2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
iâ‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
; (75)
nâˆ’1N1(Î»)1
2
âˆ¥fÎ»âˆ¥Lâˆž+n1âˆ’s
2+Ïµ
=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)1/2
; (76)
then we haveËœfÎ»âˆ’fÎ»2
L2=oP
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
. (77)
Proof. Similar to the proof in Lemma D.7, we have the decomposition ËœfÎ»âˆ’fÎ»=I+II, with
âˆ¥Iâˆ¥2
L2â‰¤122Tâˆ’1
2
Î»[(ËœgXâˆ’TXfÎ»)âˆ’(gâˆ’TfÎ»)]2
H,
âˆ¥IIâˆ¥2
L2=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
.
Denote Î¾i=Î¾(xi) =Tâˆ’1
2
Î»(Kxifâ‹†(xi)âˆ’TxifÎ»). Further consider the subset â„¦1={xâˆˆ X :
|fâ‹†(x)| â‰¤t}andâ„¦2=X\â„¦1, where twill be chosen appropriately later. Decompose Î¾ias
Î¾iIxiâˆˆâ„¦1+Î¾iIxiâˆˆâ„¦2and we have the following decomposition:
Tâˆ’1
2
Î»[(ËœgXâˆ’TXfÎ»)âˆ’(gâˆ’TfÎ»)]
H=1
nnX
i=1Î¾iâˆ’EÎ¾x
H(78)
â‰¤1
nnX
i=1Î¾iIxiâˆˆâ„¦1âˆ’EÎ¾xIxâˆˆâ„¦1
H+âˆ¥1
nnX
i=1Î¾iIxiâˆˆâ„¦2âˆ¥H+âˆ¥EÎ¾xIxâˆˆâ„¦2âˆ¥H
:=I+II+III. (79)
Next we choose t=n1âˆ’s
2+Ïµt, q=2
1âˆ’sâˆ’Ïµqsuch that
Ïµt< Ïµ;and1âˆ’s
2+Ïµt>1/2
1âˆ’sâˆ’Ïµq
. (80)
29Then we can bound the three terms in (78) as follows:
(i)For the first term in (78), denoted as I, notice that
âˆ¥(fÎ»âˆ’fâ‹†)Ixiâˆˆâ„¦1âˆ¥Lâˆžâ‰¤ âˆ¥fÎ»âˆ¥Lâˆž+n1âˆ’s
2+Ïµt. (81)
Imitating (67) in the proof of Lemma D.7, we have
I=oP
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)1/2
. (82)
(ii)For the second term in (78), denoted as II. Since q=2
1âˆ’sâˆ’Ïµq<2
1âˆ’s, Lemma 42 in Zhang et al.
(2024) shows that,
[H]s,â†’Lq(X, Âµ), (83)
with embedding norm less than a constant Cs,Îº. Then Assumption 2 (a) implies that there exists
0< C q<âˆžonly depending on Î³, sandÎºsuch that âˆ¥fâ‹†âˆ¥Lq(X,Âµ)â‰¤Cq. Using the Markov
inequality, we have
P(xâˆˆâ„¦2) =P
|fâ‹†(x)|> t
â‰¤E|fâ‹†(x)|q
tqâ‰¤(Cq)q
tq.
Further, since (80) guarantees tqâ‰«n, we have
Ï„n:=P(II>0) (84)
â‰¤P
âˆƒxis.t.xiâˆˆâ„¦2,
= 1âˆ’P
xi/âˆˆâ„¦2,âˆ€xi, i= 1,2,Â·Â·Â·, n
= 1âˆ’P
x /âˆˆâ„¦2n
= 1âˆ’P
|fâ‹†(x)| â‰¤tn
â‰¤1âˆ’
1âˆ’(Cq)q
tqn
â†’0. (85)
(iii)For the third term in (78), denoted as III. Since Lemma 37 in Zhang et al. (2024) implies that
âˆ¥Tâˆ’1
2
Î»k(x,Â·)âˆ¥Hâ‰¤ N 1(Î»)1
2, Âµ-a.e.xâˆˆ X,so
IIIâ‰¤Eâˆ¥Î¾xIxâˆˆâ„¦2âˆ¥Hâ‰¤Eh
âˆ¥Tâˆ’1
2
Î»k(x,Â·)âˆ¥HÂ· 
fâ‹†âˆ’fÎ»(x)
Ixâˆˆâ„¦2i
â‰¤ N 1(Î»)1
2E 
fâ‹†âˆ’fÎ»(x)
Ixâˆˆâ„¦2
â‰¤ N 1(Î»)1
2âˆ¥fâ‹†âˆ’fÎ»âˆ¥1
2
L2Â·P(xâˆˆâ„¦2)1
2
â‰¤ N 1(Î»)1
2M2,Ï†(Î»)1
2tâˆ’q
2, (86)
where we use Cauchy-Schwarz inequality for the third inequality and Lemma D.6 for the fourth
inequality. Recalling that the choices of t, qsatisfy tâˆ’qâ‰ªnâˆ’1and we have assumed nÏµâˆ’1N1(Î»)â†’
0, we have
III=o
M2,Ï†(Î»)1
2
. (87)
Plugging (82), (84) and (87) into (78), we finish the proof. â– 
Final proof of the bias term Now we are ready to state the theorem about the bias term.
Theorem D.9 (sâ‰¥1).Suppose that (45) in Assumption 3 holds. Suppose that there exist constants Ïµ
andConly depending on sandÎ³, such that Î»=Î»(n, d)satisfies
nÏµâˆ’1N1(Î»)â†’0,
N1(Î»)M2
1,Ï†(Î»)
n2â‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
,
N1(Î»)
nln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1Î»2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
iâ‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
;
then we haveBias2(Î»)âˆ’ M 2,Ï†(Î»)=oP
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
. (88)
30Theorem D.10 (s <1).Suppose that (45) in Assumption 3 holds. Suppose that there exist constants
ÏµandConly depending on sandÎ³, such that Î»=Î»(n, d)satisfies
nÏµâˆ’1N1(Î»)â†’0,
N1(Î»)
nln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1Î»2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
iâ‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
;
nâˆ’1N1(Î»)1
2
âˆ¥fÎ»âˆ¥Lâˆž+n1âˆ’s
2+Ïµ
=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)1/2
;
then we haveBias2(Î»)âˆ’ M 2,Ï†(Î»)=oP
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
. (89)
D.4 Quantity calculations and conditions verification for the inner product kernels
In the previous two sections, we have successfully bounded the bias and the variance terms by the
quantities M2,Ï†(Î»)andN2,Ï†(Î»). In this subsection, we will focus on the inner product kernels on
the sphere. We will (i) determine the rates for the above quantities, and (ii) verify all the conditions
in Theorem D.5, Theorem D.9 and Theorem D.10.
Recall that ÂµkandN(d, k), defined in (9), are the eigenvalues of the inner product kernel Kdefined
on the sphere and the corresponding multiplicity. The following three lemmas (mainly cited from Lu
et al. (2023)) give concise characterizations of ÂµkandN(d, k), which is sufficient for the analysis in
this paper.
Lemma D.11. For any fixed integer pâ‰¥0, there exist constants C,C9andC10only depending on p
and{aj}jâ‰¤p+1, such that for any dâ‰¥C, we have
C9dâˆ’kâ‰¤Âµkâ‰¤C10dâˆ’k, k= 0,1,Â·Â·Â·, p+ 1. (90)
Lemma D.12. For any fixed integer pâ‰¥0, there exist constants Conly depending on pand
{aj}jâ‰¤p+1, such that for any dâ‰¥C, we have
Âµkâ‰¤C10
C9dâˆ’1Âµp, k =p+ 1, p+ 2,Â·Â·Â·
where C9andC10are constants given in Lemma D.11.
Lemma D.13. For any fixed integer pâ‰¥0, there exist constants C11,C12andConly depending on p,
such that for any dâ‰¥C, we have
C11dkâ‰¤N(d, k)â‰¤C12dk, k = 0,1,Â·Â·Â·, p+ 1. (91)
With these lemmas, we can begin to bound the quantities M2,Ï†(Î»)andN2,Ï†(Î»).
Lemma D.14. Suppose that Assumption 1 and Assumption 2 hold for sand an integer p. Suppose
â„“â‰¤p,t=Î»âˆ’1âˆˆ(dâ„“, dâ„“+1]. Then we have the following bound.
M2,Ï†(Î») =ï£±
ï£²
ï£³Î˜ 
dâˆ’s(â„“+1)
Ï„=âˆž
Î˜ 
tâˆ’2Ï„dâ„“(2Ï„âˆ’s)+dâˆ’s(â„“+1)
sâ‰¤2Ï„ <âˆž
Î˜ 
Î»2Ï„
s >2Ï„
N2,Ï†(Î»)
n= Î˜dâ„“
n+t2
ndâ„“+1
âˆžX
k=0Î»2ÂµkÏ†2
Î»(Âµk)
Î»+ÂµkN(d,k)X
j=1f2
k,j=O
Î»2dmax{p(2âˆ’s),0}+dâˆ’s(â„“+1)
;(92)
and thus Assumption 3 holds. Moreover, when sâ‰¥1, We have
M2
1,Ï†(Î») =ï£±
ï£²
ï£³O 
dâˆ’(â„“+1)(sâˆ’1)
Ï„=âˆž
O 
Î»2Ï„âˆ’1dâ„“(2Ï„âˆ’s)+dâˆ’(â„“+1)(sâˆ’1)
sâ‰¤2Ï„ <âˆž
O 
Î»2Ï„âˆ’1
s >2Ï„(93)
31Proof. I.We begin with M2,Ï†(Î»). Ifsâ‰¤2Ï„andÏ„ <âˆž, then we have
M2,Ï†(Î») =âˆžX
k=0Ïˆ2
Î»(Âµk)N(d,k)X
j=1f2
k,j
â‰¤â„“X
k=0C2
2(tÂµk)âˆ’2Ï„(Âµk)sN(d,k)X
j=1(Âµk)âˆ’sf2
k,j+âˆžX
k=â„“+1Ïˆ2
Î»(Âµk)N(d,k)X
j=1f2
k,j
â‰¤â„“X
k=0C2
2(tÂµk)âˆ’2Ï„(Âµk)sN(d,k)X
j=1(Âµk)âˆ’sf2
k,j+âˆžX
k=â„“+1(Âµk)sN(d,k)X
j=1(Âµk)âˆ’sf2
k,j
â‰¤C2
2tâˆ’2Ï„(C9dâˆ’â„“)sâˆ’2Ï„â„“X
k=0N(d,k)X
j=1(Âµk)âˆ’sf2
k,j+ (C10dâˆ’â„“âˆ’1)sâˆžX
k=â„“+1N(d,k)X
j=1(Âµk)âˆ’sf2
k,j
=O
tâˆ’2Ï„dâ„“(2Ï„âˆ’s)+dâˆ’s(â„“+1)
;
and when Ï„=âˆž, a similar argument ( taking Ï„â€²< Ï„and let Ï„â€²â†’ âˆž , then we have (tdâˆ’â„“)âˆ’2Ï„â€²â†’0)
shows that M2,Ï†(Î») =O(dâˆ’s(â„“+1)).
Similarly, if sâ‰¤2Ï„, then we have
M2,Ï†(Î»)â‰¥1{Ï„ <âˆž}â„“X
k=0C2
7(tÂµk)âˆ’2Ï„(Âµk)sN(d,k)X
j=1(Âµk)âˆ’sf2
k,j
+âˆžX
k=â„“+1Ïˆ2
Î»(Âµk)N(d,k)X
j=1f2
k,j
â‰¥1{Ï„ <âˆž}â„¦
tâˆ’2Ï„dâ„“(2Ï„âˆ’s)
+âˆžX
k=â„“+1C2
5(Âµk)sN(d,k)X
j=1(Âµk)âˆ’sf2
k,j
â‰¥1{Ï„ <âˆž}â„¦
tâˆ’2Ï„dâ„“(2Ï„âˆ’s)
+C2
5(C10dâˆ’â„“âˆ’1)sN(d,â„“)X
j=1(Âµâ„“)âˆ’sf2
â„“,j
=1{Ï„ <âˆž}â„¦
tâˆ’2Ï„dâ„“(2Ï„âˆ’s)
+ â„¦
dâˆ’s(â„“+1)
.
If2Ï„ < s , then
M2,Ï†(Î») =âˆžX
k=0Ïˆ2
Î»(Âµk)N(d,k)X
j=1f2
k,j
Lemma E.3
â‰¤ Îº2(sâˆ’2Ï„)Î»2Ï„âˆžX
k=0N(d,k)X
j=1Âµâˆ’s
kf2
k,j
=O 
Î»2Ï„
.
Similarly, if 2Ï„ < s , then we have
M2,Ï†(Î»)â‰¥Ïˆ2
Î»(Âµ0)f2
0,1
â‰¥C2
6f2
0,1Â·Î»2Ï„
= â„¦ 
Î»2Ï„
.
32II.Now letâ€™s bound the second term N2,Ï†(Î»)/n. We have
N2,Ï†(Î»)
n=1
nâˆžX
k=0N(d, k) [ÂµkÏ†Î»(Âµk)]2
â‰¤1
nâ„“X
k=0N(d, k) +1
nâˆžX
k=â„“+1N(d, k) [ÂµkÏ†Î»(Âµk)]2
â‰¤1
nâ„“X
k=0N(d, k) +C2
4t2
nâˆžX
k=â„“+1N(d, k)(Âµk)2
â‰¤â„“N(d, â„“)
n+C2
4t2
nÂµâ„“+1
=Odâ„“
n+t2
ndâ„“+1
.(94)
Similarly, we have
N2,Ï†(Î»)
nâ‰¥C2
1
nâ„“X
k=0N(d, k) +C2
3t2
nâˆžX
k=â„“+1N(d, k)(Âµk)2
â‰¥C2
1N(d, â„“)
n+C2
3t2
nÂµâ„“+1
= â„¦dâ„“
n+t2
ndâ„“+1
.(95)
III. For the third term, we have
âˆžX
k=0Î»2ÂµkÏ†2
Î»(Âµk)
Î»+ÂµkN(d,k)X
j=1f2
k,jâ‰¤Î»2R2
Î³ï£«
ï£­pX
k=0Âµs
kÏ†2
Î»(Âµk) +Î»âˆ’1âˆžX
k=p+1Âµs+1
kC2
4Î»âˆ’2ï£¶
ï£¸
=O
Î»2dmax{p(2âˆ’s),0}+Î»âˆ’1dâˆ’(s+1)(â„“+1)
=O
Î»2dmax{p(2âˆ’s),0}+dâˆ’s(â„“+1)
IV .Now we show that Assumption 3 holds. Notice that (45) has been verified in Lemma 20 of Zhang
et al. (2024). Similarly, one can prove (43) and (44) hold using a similar proof as that for Lemma 20
of Zhang et al. (2024).
V .For the final term, when sâ‰¥1, we have
M2
1,Ï†(Î») = ess sup
xâˆˆXâˆžX
i=1(ÏˆÎ»(Î»i)fiei(x))2
â‰¤ âˆžX
i=1ÏˆÎ»(Î»i)
Î»iÏ†Î»(Î»i)f2
i!
Â·ess sup
xâˆˆXâˆžX
i=1 
Î»iÏ†Î»(Î»i)ei(x)2
Assumption 3
â‰¤ âˆžX
i=1ÏˆÎ»(Î»i)
Î»iÏ†Î»(Î»i)f2
i!
Â·âˆžX
i=1Î»iÏ†Î»(Î»i)
:=Q1,Ï†(Î»)Â· N1,Ï†(Î»). (96)
33ForQ1,Ï†(Î»), when Ï„â‰¥s/2andÏ„ <âˆž, we have
Q1,Ï†(Î») =âˆžX
k=0Ïˆ2
Î»(Âµk)Âµsâˆ’1
k
Ï†Î»(Âµk)N(d,k)X
j=1Âµâˆ’s
kf2
k,j
â‰¤C2
2
C1â„“X
k=0Î»2Ï„Âµâˆ’2Ï„+s
kN(d,k)X
j=1Âµâˆ’s
kf2
k,j
+ (C3)âˆ’1Î»âˆžX
k=â„“+1Âµsâˆ’1
kN(d,k)X
j=1Âµâˆ’s
kf2
k,j
=O
Î»2Ï„dâ„“(2Ï„âˆ’s)+Î»dâˆ’(â„“+1)(sâˆ’1)
.(97)
Similarly, when Ï„=âˆž, we can show that Q1,Ï†(Î») =O(Î»dâˆ’(â„“+1)(sâˆ’1)).
And when Ï„ < s/ 2, we have
Q1,Ï†(Î») =âˆžX
k=0Ïˆ2
Î»(Âµk)Âµsâˆ’1
k
Ï†Î»(Âµk)N(d,k)X
j=1Âµâˆ’s
kf2
k,j
Lemma E.3
â‰¤C2
2Îº2(sâˆ’2Ï„)
C1Î»2Ï„pX
k=0N(d,k)X
j=1Âµâˆ’s
kf2
k,j
+âˆžX
k=p+1Ïˆ2
Î»(Âµk)Âµsâˆ’1
k
Ï†Î»(Âµk)N(d,k)X
j=1Âµâˆ’s
kf2
k,j
(30)
â‰¤C2
2Îº2(sâˆ’2Ï„)
C1Î»2Ï„pX
k=0N(d,k)X
j=1Âµâˆ’s
kf2
k,j
+âˆžX
k=p+1C8Î»2Ï„N(d,k)X
j=1Âµâˆ’s
kf2
k,j
=O 
Î»2Ï„
.
ForN1,Ï†(Î»), we have
N1,Ï†(Î») =âˆžX
k=0N(d, k) [ÂµkÏ†Î»(Âµk)]
â‰¤â„“X
k=0N(d, k) +âˆžX
k=â„“+1N(d, k) [ÂµkÏ†Î»(Âµk)]
â‰¤â„“X
k=0N(d, k) +C4tâˆžX
k=â„“+1N(d, k)Âµk
â‰¤â„“N(d, â„“) +C4t
=O 
dâ„“+Î»âˆ’1
=O 
Î»âˆ’1
.(98)
Therefore, when sâ‰¥1, we have
M2
1,Ï†(Î») =ï£±
ï£²
ï£³O 
dâˆ’(â„“+1)(sâˆ’1)
Ï„=âˆž
O 
Î»2Ï„âˆ’1dâ„“(2Ï„âˆ’s)+dâˆ’(â„“+1)(sâˆ’1)
sâ‰¤2Ï„ <âˆž
O 
Î»2Ï„âˆ’1
s >2Ï„(99)
â– 
34From Lemma D.14, we have the following three corollaries.
Corollary D.15. Let1â‰¤sâ‰¤Ï„andÎ³ > 0be fixed real numbers. Denote pas the integer
satisfying Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Suppose one of the following cases holds for Î»â‹†=dâˆ’â„“or
Î»â‹†=dâˆ’â„“Â·poly(ln(d)):
(1)pâ‰¥1,p(s+ 1)â‰¤Î³ < ps +p+s,â„“=p+ 1/2
(2)pâ‰¥1,ps+p+sâ‰¤Î³ < ps +p+s+ 1,â„“= (Î³âˆ’(p+ 1)( sâˆ’1))/2
(3)Î³ < s ,â„“= min {Î³,1}/2
(4)sâ‰¤Î³ < s + 1,â„“= (Î³âˆ’(sâˆ’1))/2
Then we have
M2,Ï†(Î»â‹†)â‰²N2,Ï†(Î»â‹†)
n= Î˜
dâˆ’s(p+1)+dp
n
, (100)
or
M2,Ï†(Î»â‹†)â‰²N2,Ï†(Î»â‹†)
n= Î˜
dâˆ’s(p+1)+dp
n
Â·poly(ln(d)). (101)
Corollary D.16. LetÏ„ < sâ‰¤2Ï„andÎ³ >0be fixed real numbers. Denote pas the integer satisfying
Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Denote âˆ† =Î³âˆ’p(s+ 1) . Suppose one of the following cases holds
forÎ»â‹†=dâˆ’â„“orÎ»â‹†=dâˆ’â„“Â·poly(ln(d)):
(1)Î³â‰¥1,0â‰¤âˆ†â‰¤Ï„,â„“=â„“1:=p+ âˆ†/(2Ï„)
(2)Î³â‰¥1,Ï„â‰¤âˆ†â‰¤s+s/Ï„âˆ’1,â„“=â„“2:=p+ (âˆ† + 1) /(2Ï„+ 2)
(3)Î³â‰¥1,âˆ†â‰¥s+s/Ï„âˆ’1,â„“=â„“3:=p+ (âˆ† + 1 âˆ’s)/2
(4)Î³ <1,â„“=Î³/2
Then we have
M2,Ï†(Î»â‹†)â‰N2,Ï†(Î»â‹†)
n= Î˜
dâˆ’min{Î³âˆ’p,Ï„(Î³âˆ’p+1)+ ps
Ï„+1,s(p+1)}
, (102)
or
M2,Ï†(Î»â‹†)â‰N2,Ï†(Î»â‹†)
n= Î˜
dâˆ’min{Î³âˆ’p,Ï„(Î³âˆ’p+1)+ ps
Ï„+1,s(p+1)}
Â·poly(ln(d)). (103)
Proof. Denote I=âˆ’2â„“Ï„+ 2pÏ„âˆ’ps,II=âˆ’spâˆ’s,III=pâˆ’Î³, andIV= 2â„“âˆ’Î³âˆ’pâˆ’1. From
Lemma D.14 we have
M2,Ï†(Î»â‹†)â‰dI+dII,N2,Ï†(Î»â‹†)
nâ‰dIII+dIV.
We can verify that:
(1) When 0â‰¤âˆ†â‰¤Ï„andâ„“=p+ âˆ†/(2Ï„), we have
IIâ‰¤I=IIIâ‰¥IVandmin
Î³âˆ’p,Ï„(Î³âˆ’p+ 1) + ps
Ï„+ 1, s(p+ 1)
=Î³âˆ’p;
(2) When Ï„â‰¤âˆ†â‰¤s+s/Ï„âˆ’1andâ„“=p+ (âˆ† + 1) /(2Ï„+ 2) , we have
IIâ‰¤I=IVâ‰¥IIIandmin
Î³âˆ’p,Ï„(Î³âˆ’p+ 1) + ps
Ï„+ 1, s(p+ 1)
=Ï„(Î³âˆ’p+ 1) + ps
Ï„+ 1;
(3) When âˆ†â‰¥s+s/Ï„âˆ’1andâ„“=p+ (âˆ† + 1 âˆ’s)/2, we have
Iâ‰¤II=IVâ‰¥IIIandmin
Î³âˆ’p,Ï„(Î³âˆ’p+ 1) + ps
Ï„+ 1, s(p+ 1)
=s(p+ 1);
35(4) When Î³ <1andâ„“=Î³/2, we have
IIIâ‰¥max{I,II,IV}.
â– 
Corollary D.17. Lets <1andÎ³ >0be fixed real numbers. Denote pas the integer satisfying
Î³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Suppose one of the following cases holds for Î»â‹†=dâˆ’â„“or
Î»â‹†=dâˆ’â„“Â·poly(ln(d)):
(1)Ï„=âˆž,pâ‰¥1,p(s+ 1)â‰¤Î³ < ps +p+s,â„“=p+s/2
(2)Ï„=âˆž,pâ‰¥1,ps+p+sâ‰¤Î³ < ps +p+s+ 1,â„“= (Î³+p(1âˆ’s))/2
(3)Ï„=âˆž,Î³ < s ,â„“= min {Î³,1,2Î³s}/2
(4)Ï„=âˆž,sâ‰¤Î³ < s + 1,â„“= min {(Î³+ (1âˆ’s))/2, Î³(1 +s)âˆ’s, Î³/2}
(5)Ï„ <âˆž,p(s+ 1)â‰¤Î³ < ps +p+s,â„“= (Î³+ 2Ï„pâˆ’spâˆ’p)/(2Ï„)
(6)Ï„ <âˆž,ps+p+sâ‰¤Î³ < ps +p+s+ 1,â„“=p+s/(2Ï„)
Then we have
M2,Ï†(Î»â‹†) +N2,Ï†(Î»â‹†)
n= Î˜
dâˆ’s(p+1)+dp
n
, (104)
or
M2,Ï†(Î»â‹†) +N2,Ï†(Î»â‹†)
n= Î˜
dâˆ’s(p+1)+dp
n
Â·poly(ln(d)). (105)
D.4.1 Verification of variance conditions
Lemma D.18 (Verification of variance conditions for inner-product kernels) .Suppose nâ‰dÎ³and
sâ‰¥1, forÎ³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . For any given â„“â‰¥0, if
Î»â‰¥ï£±
ï£²
ï£³dâˆ’â„“ 
1 + ln2(d)1{Î³= 2, s= 1}
pâ‰¥1,2â„“â‰¤max{2p+ 1, Î³âˆ’(p+ 1)( sâˆ’1)}
dâˆ’â„“ln2(d) p= 0, Î³â‰¥1,2â„“â‰¤max{1, Î³âˆ’(sâˆ’1)}
dâˆ’â„“p= 0, Î³ < 1,2â„“â‰¤Î³;
then there exists a constant Ïµ >0only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
N1(Î»)Â·nÏµâˆ’1â†’0,
N2
1(Î»)
nN2,Ï†(Î»)Â·ln(n)(lnÎ»âˆ’1)2â†’0.
Proof. From Lemma 21 in Zhang et al. (2024), we have N1(Î»)â‰Î»âˆ’1. When p= 0, we have
Î³âˆ’â„“ >0. When pâ‰¥1, we have Î³âˆ’pâˆ’1/2â‰¥psâˆ’1/2>0. Therefore, there exists a constant
Ïµ >0only depending on sandÎ³, such that we have
N1(Î»)Â·nÏµâˆ’1â†’0.
Denote q:=âŒŠâ„“âŒ‹. From Lemma D.14, we further have N2,Ï†(Î») = â„¦ 
dq+Î»âˆ’2dâˆ’qâˆ’1
. Hence, we
have
N2
1(Î»)
nN2,Ï†(Î»)Â·ln(n)(lnÎ»âˆ’1)2=O(ln(d))3
n(Î»2dq+dâˆ’qâˆ’1)
.
Denote âˆ† :=(ln(d))3
nÎ»2dq,âˆ†â€²:=(ln(d))3
dÎ³âˆ’qâˆ’1, then when âˆ† =o(1)orâˆ†â€²=o(1), we have:
N2
1(Î»)
nN2,Ï†(Î»)Â·ln(n)(lnÎ»âˆ’1)2â†’0.
Now we show that âˆ† =o(1):
36â€¢When pâ‰¥3andp= 2, s > 1, since Î³âˆ’2â„“+qâ‰¥(Î³âˆ’â„“âˆ’1) + ( q+ 1âˆ’â„“)>0, we have
âˆ† =o(1).
â€¢ When p= 2, s= 1, since 2â„“âˆ’q < â„“ + 1<4â‰¤Î³, we have âˆ† =o(1).
â€¢ When p= 2, s= 1, since 2â„“âˆ’q < â„“ + 1<4â‰¤Î³, we have âˆ† =o(1).
â€¢ When p= 1, Î³ > 2s+ 1, since â„“ <2and hence 2â„“âˆ’q <3â‰¤Î³, we have âˆ† =o(1).
â€¢When p= 1, s > 1, Î³â‰¤2s+ 1, orp= 1, s= 1, Î³ > 2, since 2â„“âˆ’qâ‰¤2< Î³, we have
âˆ† =o(1).
â€¢ When p= 1,s= 1,Î³= 2, since 2â„“âˆ’qâ‰¤2â‰¤Î³, we have âˆ† =O((ln(d))âˆ’1).
â€¢ When p= 0, since Î³âˆ’2â„“â‰¥0, we have âˆ† =O((ln(d))âˆ’1).
â– 
Lemma D.19 (Verification of variance conditions for inner-product kernels: saturation case) .Suppose
Ï„ < sâ‰¤2Ï„. Suppose nâ‰dÎ³, forÎ³âˆˆ[p(s+ 1) + Ï„, p(s+ 1) + s+s/Ï„âˆ’1]. For any given â„“â‰¥0, if
Î»â‰¥dâˆ’â„“, â„“â‰¤p+ (Î³âˆ’p(s+ 1) + 1) /(2Ï„+ 2);
then there exists a constant Ïµ >0only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
N1(Î»)Â·nÏµâˆ’1â†’0,
N2
1(Î»)
nN2,Ï†(Î»)Â·ln(n)(lnÎ»âˆ’1)2â†’0.
Proof. From Lemma 21 in Zhang et al. (2024), we have N1(Î»)â‰Î»âˆ’1. Notice that we have
2(Ï„+ 1)( Î³âˆ’p)â‰¥
psâˆ’1 pâ‰¥1
2Ï„2+ (Ï„âˆ’1)p= 0>0;
Therefore, there exists a constant Ïµ >0only depending on Ï„,s, and Î³, such that we have
N1(Î»)Â·nÏµâˆ’1â†’0.
Denote q:=âŒŠâ„“âŒ‹. From Lemma D.14, we further have N2,Ï†(Î») = â„¦ 
dq+Î»âˆ’2dâˆ’qâˆ’1
. Hence, we
have
N2
1(Î»)
nN2,Ï†(Î»)Â·ln(n)(lnÎ»âˆ’1)2=O(ln(d))3
n(Î»2dq+dâˆ’qâˆ’1)
=O(ln(d))3
nÎ»2dq
+O(ln(d))3
dÎ³âˆ’qâˆ’1
.
Denote âˆ† :=(ln(d))3
nÎ»2dq,âˆ†â€²:=(ln(d))3
dÎ³âˆ’qâˆ’1. We have:
â€¢ When pâ‰¥1, since
2(Ï„+ 1)[Î³âˆ’2â„“+q]â‰¥2(Ï„+ 1)[( Î³âˆ’â„“âˆ’1) + ( q+ 1âˆ’â„“)]
â‰¥
psâˆ’2 pâ‰¥2
2(Ï„+ 1)( Ï„âˆ’1) + 2[ Ï„s+sâˆ’1]p= 1
>0,
we have âˆ† =o(1).
â€¢ When p= 0, since Î³ >1, we have âˆ†â€²=o(1).
â– 
37Lemma D.20 (Verification of variance conditions for inner-product kernels: misspecified case) .
Suppose nâ‰dÎ³and0< s < 1, forÎ³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . For any given â„“â‰¥0, if
Î»â‰¥ï£±
ï£²
ï£³dâˆ’â„“pâ‰¥1,2â„“â‰¤max{2p+s, Î³+p(1âˆ’s)}
dâˆ’â„“p= 0, Î³ > s, 2â„“â‰¤Î³
dâˆ’â„“ln(d) p= 0, Î³â‰¤s,2â„“â‰¤Î³;
then there exists a constant Ïµ >0only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
N1(Î»)Â·nÏµâˆ’1â†’0,
N2
1(Î»)
nN2,Ï†(Î»)Â·ln(n)(lnÎ»âˆ’1)2â†’0.
Proof. When pâ‰¥1, it is a direct result of step 2 (the verification of the second condition in
(146) of Zhang et al. (2024)) in the proof of Theorem 3 in Zhang et al. (2024) and the fact that
N2,Ï†(Î»)â‰ N 2(Î»).
When p= 0, a similar argument as the proof for Lemma D.18 give the desired results. â– 
D.4.2 Verification of bias conditions
Lemma D.21 (Verification of bias conditions) .Suppose 1â‰¤sâ‰¤Ï„. Suppose nâ‰dÎ³, forÎ³âˆˆ
[p(s+ 1),(p+ 1)( s+ 1)) . For any given â„“â‰¥0, if
Î»â‰¥ï£±
ï£²
ï£³dâˆ’â„“ 
1 + ln2(d)1{Î³= 2, s= 1}
pâ‰¥1,2â„“â‰¤max{2p+ 1, Î³âˆ’(p+ 1)( sâˆ’1)}
dâˆ’â„“ln2(d) Î³âˆˆ[1, s+ 1),2â„“â‰¤max{1, Î³âˆ’(sâˆ’1)}
dâˆ’â„“Î³âˆˆ(0,1),2â„“â‰¤Î³;
then there exists a constant Ïµ >0only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
N1(Î»)M2
1,Ï†(Î»)
n2â‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
,
N1(Î»)
nln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1Î»2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
iâ‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
.(106)
Proof. When 1â‰¤sâ‰¤Ï„, from Lemma D.14, we have
n
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
= â„¦
dÎ³âˆ’s(q+1)+dq
N1(Î»)M2
1,Ï†(Î»)
n=O
Î»2(sâˆ’1)dâˆ’Î³+qs+Î»âˆ’1dâˆ’Î³âˆ’(q+1)(sâˆ’1)
N1(Î») ln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1(Î»)2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
i=O 
(ln(d))3
Â·O
Î»dmax{q(2âˆ’s),0}+Î»âˆ’1dâˆ’s(q+1)
,
Denote I=Î»2(sâˆ’1)dâˆ’Î³+qs,II=Î»âˆ’1dâˆ’Î³âˆ’(q+1)(sâˆ’1),III=Î»dmax{q(2âˆ’s),0}(ln(d))3, andIV=
Î»âˆ’1dâˆ’s(q+1)(ln(d))3.
For any pâ‰¥0and any sâ‰¥1:
â€¢ From Lemma D.18, we have IVâ‰ªdÎ³âˆ’s(q+1).
â€¢When Î³â‰¥1, we have Î³â‰¥p+ 1, and hence IIâ‰ªIVâ‰ªdÎ³âˆ’s(q+1); when Î³ <1, we have
IIâ‰ªdqwithq= 0.
â€¢When pâ‰¥1orÎ³âˆˆ(s, s+ 1) , since âˆ’â„“s+qsâ‰¤0, we have I/dÎ³âˆ’s(q+1)=
O(dâˆ’2(Î³âˆ’â„“âˆ’s/2))â‰ª1; when Î³âˆˆ(0, s], we have I=O(dâˆ’2sâ„“+2â„“âˆ’Î³) =O(dâˆ’2sâ„“)â‰ªdq
withq= 0.
38â€¢When sâ‰¥2, we have IIIâ‰ªdq; when s <2andp= 0, we have IIIâ‰ªdq; when s <2
andpâ‰¥1andqâ‰¥1, since Î³âˆ’â„“âˆ’s >min{(s+ 1)qâˆ’â„“, psâˆ’1/2}>0, we have
III/dÎ³âˆ’s(q+1)=dâˆ’(Î³âˆ’â„“âˆ’s)âˆ’2(â„“âˆ’q)â‰ª1orIII/dqâ‰ª1; when s <2andpâ‰¥1and
q= 0, we have IIIâ‰ªdq.
Combining all these, we get the desired results. â– 
Lemma D.22. [Verification of bias conditions: saturation case] Suppose Ï„ < s â‰¤2Ï„. Suppose
nâ‰dÎ³, forÎ³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . For any given â„“â‰¥0, if
Î»â‰¥ï£±
ï£²
ï£³dâˆ’â„“pâ‰¥1, â„“â‰¤max{â„“1, â„“2, â„“3}
dâˆ’â„“ln2(d)Î³âˆˆ[1, s+ 1), â„“â‰¤max{â„“1, â„“2, â„“3}
dâˆ’â„“Î³âˆˆ(0,1),2â„“â‰¤Î³,
where Ï„,âˆ†,â„“1,â„“2, andâ„“3are given in Lemma D.16; then there exists a constant Ïµ >0only depending
onsandÎ³, such that Î»=Î»(n, d)satisfies
N1(Î»)M2
1,Ï†(Î»)
n2â‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
,
N1(Î»)
nln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1Î»2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
iâ‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
.(107)
Proof. When Ï„ < sâ‰¤2Ï„, from Lemma D.14, we have
n
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
= â„¦
Î»2Ï„dq(2Ï„âˆ’s)+dÎ³âˆ’s(q+1)+dq
N1(Î»)M2
1,Ï†(Î»)
n=O
Î»2(Ï„âˆ’1)dâˆ’Î³+q(2Ï„âˆ’s)+Î»âˆ’1dâˆ’Î³âˆ’(q+1)(sâˆ’1)
N1(Î») ln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1(Î»)2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
i=O 
(ln(d))3
Â·O
Î»dmax{q(2âˆ’s),0}+Î»âˆ’1dâˆ’s(q+1)
.
Denote Iâ€²=Î»2(Ï„âˆ’1)dâˆ’Î³+q(2Ï„âˆ’s),II=Î»âˆ’1dâˆ’Î³âˆ’(q+1)(sâˆ’1),III=Î»dmax{q(2âˆ’s),0}(ln(d))3, and
IV=Î»âˆ’1dâˆ’s(q+1)(ln(d))3.
For any pâ‰¥0and any 1â‰¤Ï„ < sâ‰¤2Ï„:
â€¢From Lemma D.18 and Lemma D.19, since N1(Î»)Â·nÏµâˆ’1â†’0, we have IVâ‰ªdÎ³âˆ’s(q+1).
â€¢When Î³â‰¥1, we have Î³â‰¥p+ 1, and hence IIâ‰ªIVâ‰ªdÎ³âˆ’s(q+1); when Î³ <1, we have
IIâ‰ªdqwithq= 0.
â€¢ When pâ‰¥1, since âˆ’â„“Ï„+qÏ„â‰¤0and
Î³âˆ’â„“âˆ’s/2
â‰¥maxs(2pâˆ’1)
2,(2Ï„+ 1)( Ï„+ps)âˆ’(Ï„+ 1)s+psâˆ’1
2(Ï„+ 1, ps+s(Ï„+ 1)
2Ï„âˆ’1
>0,
we have Iâ€²/dÎ³âˆ’s(q+1)â‰ª1; when p= 0, we have Iâ€²=O(dâˆ’2Ï„â„“+2â„“âˆ’Î³)â‰ªdqwithq= 0.
â€¢When Î³âˆ’pâˆ’psâˆˆ[0, Ï„]âˆª[s+s/Ï„âˆ’1, s+ 1], we have â„“â‰¤max{â„“1, â„“3}. Similar to the
proof in Lemma D.21, we can show that IIIâ‰ªdÎ³âˆ’s(q+1)+dq.
â€¢Finally, consider the case Î³âˆ’pâˆ’psâˆˆ[Ï„, s+s/Ï„âˆ’1]. When sâ‰¥2, we have IIIâ‰ªdq;
when s <2, since s >1, we have III/dq=Î»dâˆ’q(sâˆ’1)â‰ª0.
Combining all these, we get the desired results. â– 
39Lemma D.23 (Verification of bias conditions: misspecified case) .Suppose 0< s < 1. Suppose
nâ‰dÎ³, forÎ³âˆˆ[p(s+ 1),(p+ 1)( s+ 1)) . Suppose one of the following holds:
(1)Ï„=âˆž.
(2)s >1/(2Ï„),
(3)Î³ >((2Ï„+ 1)s)/(2Ï„(1 +s)).
Suppose one of the following cases holds for Î»=dâˆ’â„“orÎ»=dâˆ’â„“(ln(d))2:
(1)Ï„=âˆž,p(s+ 1)â‰¤Î³â‰¤ps+p+s,
â„“âˆˆ[p, p+ min {1/2, Î³s}]
(2)Ï„=âˆž,ps+p+s < Î³ < ps +p+s+ 1,
â„“âˆˆ[p,min{(Î³âˆ’(p+ 1)( sâˆ’1))/2, Î³(1 +s)âˆ’s(p+ 1)}]
(3)Ï„ <âˆž,p(s+ 1)â‰¤Î³â‰¤ps+p+s,
â„“= (Î³+ 2Ï„pâˆ’spâˆ’p)/(2Ï„)
(4)Ï„ <âˆž,ps+p+s < Î³ < ps +p+s+ 1,
â„“=p+s/(2Ï„).
then there exists a constant Ïµ >0only depending on sandÎ³, such that Î»=Î»(n, d)satisfies
N1(Î»)
nln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1Î»2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
iâ‰ª
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
;
nâˆ’2N1(Î»)
âˆ¥fÎ»âˆ¥Lâˆž+n1âˆ’s
2+Ïµ2
=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
.
Proof. When 0< s < 1, from Lemma D.14, we have
n
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
= â„¦
dÎ³âˆ’s(p+1)+dp
nâˆ’1N1(Î»)n1âˆ’s=O 
Î»âˆ’1dâˆ’Î³s
N1(Î») ln(n)(lnÎ»âˆ’1)2Â·âˆžX
j=1(Î»)2Î»iÏ†2
Î»(Î»i)
Î»+Î»if2
i=O 
(ln(d))3
Â·O
Î»dmax{p(2âˆ’s),0}+Î»âˆ’1dâˆ’s(p+1)
,
and the convergence rate of âˆ¥fÎ»âˆ¥Lâˆžcan be attained similar to Lemma 25 in Zhang et al. (2024).
Since Ï„â‰¥1, similar to the proof of Theorem 3 of Zhang et al. (2024), when 1/2< s < 1, we have
nâˆ’2N1(Î»)
âˆ¥fÎ»âˆ¥Lâˆž+n1âˆ’s
2+Ïµ2
=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
,
and when sâ‰¤1/2, we have
nâˆ’2N1(Î»)âˆ¥fÎ»âˆ¥2
Lâˆž=o
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î»)
.
Denote I=Î»âˆ’1dâˆ’Î³s,II=Î»dp(2âˆ’s)(ln(d))3, andIII=Î»âˆ’1dâˆ’s(p+1)(ln(d))3.
For any pâ‰¥0and any 0< s < 1:
â€¢ From Lemma D.20, we have IIIâ‰ªdÎ³âˆ’s(p+1),
â€¢When Î³â‰¤ps+p+s, we can show Iâ‰ªdpwhen: (1) pâ‰¥1, or (2) p= 0 and
s >1/(2Ï„)>0, or (3) Ï„=âˆž,
â€¢ When Î³ > ps +p+s, we can show Iâ‰ªdÎ³âˆ’s(p+1)holds if and only if Ï„=âˆžor
Î³ >(2Ï„+ 1)s+ 2Ï„(1 +s)p
2Ï„(1 +s), Ï„ =Ï„ <âˆž;
40and the above inequality holds when (1) p > 0or (2) p= 0, s > 1/(2Ï„)>0, or (3)
p= 0, Î³ > ((2Ï„+ 1)s)/(2Ï„(1 +s));
â€¢ When Î³â‰¤ps+p+s, since â„“â‰¥p > pâˆ’ps, we have IIâ‰ªdp;
â€¢ When Î³ > ps +p+s, since â„“â‰¥p > pâˆ’ps, we have IIâ‰ªdÎ³âˆ’s(p+1).
Combining all these, we get the desired results. â– 
D.5 Final proof of Theorem 4.1 and Theorem 4.2
For each case, the proof can be done in the following steps:
(i) When Î»â‰¥Î»â‹†andsâ‰¤2Ï„, where the definition of the balanced parameter Î»â‹†can be found
in Corollary D.15 and Corollary D.16, we have
M2,Ï†(Î»â‹†) +Ïƒ2
nN2,Ï†(Î»â‹†) = Î˜ P
dâˆ’Î²â‹†
Â·poly(ln(d))
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î») = Î˜ P 
dâˆ’Î²
Â·poly(ln(d)),
where dâˆ’Î²â‹†is the desired convergence rate given in Theorem 4.1 or Theorem 4.2 and
Î²â‰¤Î²âˆ—. Similarly, when s >2Ï„, by taking s= 2Ï„in Corollary D.16, we also have
M2,Ï†(Î»â‹†) +Ïƒ2
nN2,Ï†(Î»â‹†) = Î˜ P
dâˆ’Î²â‹†
Â·poly(ln(d))
M2,Ï†(Î») +Ïƒ2
nN2,Ï†(Î») = Î˜ P 
dâˆ’Î²
Â·poly(ln(d)).
(ii)When Î»â‰¥Î»â‹†, from Lemma D.14, Lemma D.18, Lemma D.19, Lemma D.20, Lemma D.21,
Lemma D.22, and Lemma D.23, we know that conditions in Theorem D.5, Theorem D.9,
and Theorem D.10 are satisfied. Therefore, we have
EË†fÎ»â‹†âˆ’fâ‹†2
L2X
= Î˜ P
dâˆ’Î²â‹†
Â·poly(ln(d))
EË†fÎ»âˆ’fâ‹†2
L2X
= Î˜ P 
dâˆ’Î²
Â·poly(ln(d)).
(iii) Finally, when s > Ï„ , we can further show that: the convergence rates of the generalization
error can not be faster than above for any choice of regularization parameter Î»=Î»(d, n)â†’
0. Notice that, when sâ‰¥1, for any Î» < Î»â‹†, from the monotonicity of Var(Î»)(see, e.g., Li
et al. (2024); Zhang et al. (2024)), we have
EË†fÎ»âˆ’fâ‹†2
L2X
â‰¥Var(Î»)â‰¥Var(Î»â‹†)â‰EË†fÎ»â‹†âˆ’fâ‹†2
L2X
,
and hence
EË†fÎ»âˆ’fâ‹†2
L2X
= â„¦ P
dâˆ’Î²â‹†
Â·poly(ln(d)).
E Auxiliary lemmas
Proposition E.1. For any analytic filter function Ï†Î», we have (z+Î»)Ï†Î»(z)â‰¤4and(z+Î»)ÏˆÎ»(z)â‰¤
4Î».
Proof. From (28), we have (z+Î»)Ï†Î»(z)â‰¤2 max{z, Î»}Ï†Î»(z)â‰¤2 max{1,C4} â‰¤4. From (27),
we have (z+Î»)ÏˆÎ»(z)â‰¤2 max{z, Î»}ÏˆÎ»(z)â‰¤2 max{C2,1}Î»â‰¤4Î». â– 
Lemma E.2. LetÏ†Î»be an analytic filter function defined in Definition C.1. Then, for any sâˆˆ[0,1],
we have
sup
zâˆˆ[0,Îº2]Ï†Î»(z)zsâ‰¤4Î»sâˆ’1.
41Proof. For any zâˆˆ[0, Îº2], from Proposition E.1, we have (z+Î»)Ï†Î»(z)â‰¤4. Therefore, from
Proposition B.3 in Li et al. (2024), we have
Ï†Î»(z)zsâ‰¤4zs
z+Î»â‰¤4Î»sâˆ’1.
â– 
Lemma E.3. LetÏˆÎ»be defined in Definition C.1. Then, for any s >2Ï„, we have
sup
zâˆˆ[0,Îº2]zsÏˆ2
Î»(z)â‰¤C2
2Îº2(sâˆ’2Ï„)Î»2Ï„.
Proof. For any z, we have
ÏˆÎ»(z)â‰¤C2(z/Î»)âˆ’Ï„1{z > Î»}+1{zâ‰¤Î»} â‰¤C2(z/Î»)âˆ’Ï„,
hence
zsÏˆ2
Î»(z)â‰¤C2
2zszâˆ’2Ï„Î»2Ï„â‰¤C2
2Îº2(sâˆ’2Ï„)Î»2Ï„.
â– 
E.1 Analytic functional calculus
The â€œanalytic functional argumentâ€ introduced in Li et al. (2024) is vital in our proof for Theorem
4.1. For readersâ€™ convenience, we collect some of the main ingredients here, see Li et al. (2024) for
details.
Definition E.4.LetAbe a linear operator on a Banach space X. The resolvent set Ï(A)is given by
Ï(A):={Î»âˆˆC|Aâˆ’Î»is invertible },
and we denote RA(Î»):= (Aâˆ’Î»)âˆ’1. The spectrum of Ais defined by
Ïƒ(A):=C\Ï(A).
A simple but key ingredient in the analytic functional calculus is the following resolvent identity :
RA(Î»)âˆ’RB(Î») =RA(Î»)(Bâˆ’A)RB(Î») =RB(Î»)(Bâˆ’A)RA(Î»). (108)
The resolvent allows us to define the value of f(A)in analog to the form of Cauchy integral formula,
where Ais an operator and fis an analytic function. The following two propositions are well-known
results on operator calculus.
Proposition E.5 (analytic functional calculus) .LetAbe an operator on a Hilbert space Handf
be an analytic function defined on DfâŠ‚C. LetÎ“be a contour contained in Dfsurrounding Ïƒ(A).
Then,
f(A) =1
2Ï€iI
Î“f(z)(zâˆ’A)âˆ’1dz=âˆ’1
2Ï€iI
Î“f(z)RA(z)dz, (109)
and it is independent of the choice of Î“.
Now, let Î“be a contour contained in Dfsurrounding both Ïƒ(A)andÏƒ(B). Using (108), we get
f(A)âˆ’f(B) =âˆ’1
2Ï€iI
Î“f(z) [RA(z)âˆ’RB(z)] dz=1
2Ï€iI
Î“RB(z)(Aâˆ’B)RA(z)f(z)dz.
(110)
Proposition E.6 (Spectral mapping theorem) .LetAbe a bounded self-adjoint operator and fbe a
continuous function on Ïƒ(A). Then
Ïƒ(f(A)) ={f(Î»)|Î»âˆˆÏƒ(A)}. (111)
Consequently, âˆ¥f(A)âˆ¥= supÎ»âˆˆÏƒ(A)|f(Î»)| â‰¤ âˆ¥fâˆ¥âˆž.
42Let us define the contour Î“Î»considered in Li et al. (2024) by
Î“Î»= Î“Î»,1âˆªÎ“Î»,2âˆªÎ“Î»,3
Î“Î»,1={xÂ±(x+Î·)iâˆˆC|xâˆˆ[âˆ’Î·,0]}
Î“Î»,2=
xÂ±(x+Î·)iâˆˆC|xâˆˆ(0, Îº2)	
Î“Î»,3=
zâˆˆC|zâˆ’Îº2=Îº2+Î·,Re(z)â‰¥Îº2	
,(112)
where Î·=Î»/2. Then, since TandTXare positive self-adjoint operators with âˆ¥Tâˆ¥,âˆ¥TXâˆ¥ â‰¤Îº2,
we have Ïƒ(T), Ïƒ(TX)âŠ‚[0, Îº2]. Therefore, Î“Î»is indeed a contour satisfying the requirement in
Proposition E.5.
Proposition E.7. Suppose that (45) in Assumption 3 holds. Suppose that Î»=Î»(n, d)satisfies
v:=N1(Î»)
nlnn=o(1). Then for any fixed Î´âˆˆ(0,1), when nis sufficiently large, with probability
at least 1âˆ’Î´, we have
âˆ¥Tâˆ’1
2
Î»(Tâˆ’TX)Tâˆ’1
2
Î»âˆ¥ â‰¤âˆšv.
Tâˆ’1
2
Î»T1
2
XÎ»2
â‰¤2 (113)
T1
2
Î»Tâˆ’1
2
XÎ»2
â‰¤3. (114)
Proof. These inequalities are direct results of (56), (58), and (59) in Zhang et al. (2024). â– 
Proposition E.8 (Restate Proposition 4.13 in Li et al. (2024) with only the constant modified) .When
(113) holds, there is an absolute constant that for any zâˆˆÎ“Î»,
âˆ¥T1
2
Î»(Tâˆ’z)âˆ’1T1
2
Î»âˆ¥ â‰¤C
âˆ¥T1
2
Î»(TXâˆ’z)âˆ’1T1
2
Î»âˆ¥ â‰¤âˆš
6C.(115)
43NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: We first propose an improved minimax lower bound for the kernel regression
problem in large dimensional settings in Theorem 3.3 and show that the gradient flow
with early stopping strategy will result in an estimator achieving this lower bound (up to
a logarithmic factor) in Theorem 3.1. We further determine the exact convergence rates
of a large class of (optimal tuned) spectral algorithms with different qualification Ï„â€™s, and
provide a discussion on new phenomena we find in Section 4.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We explain the reason for considering spherical data in Remark 2.1. We
point out in the Conclusion section that our work only considers the optimal-tuned spectral
algorithms.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
443.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We list all assumptions we need in the statement of our main theorems. We
provide a complete (and correct) proof in the Appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
455.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper does not include experiments requiring code.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
46â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The paper does not include experiments.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
47â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
48â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
49