Logarithmic Smoothing for Pessimistic Off-Policy
Evaluation, Selection and Learning
Otmane Sakhi
Criteo AI Lab, Paris, France
o.sakhi@criteo.comImad Aouali
CREST, ENSAE
Criteo AI Lab, Paris, France
i.aouali@criteo.com
Pierre Alquier
ESSEC Business School, Singapore
alquier@essec.eduNicolas Chopin
CREST, ENSAE
nicolas.chopin@ensae.fr
Abstract
This work investigates the offline formulation of the contextual bandit problem,
where the goal is to leverage past interactions collected under a behavior policy to
evaluate, select, and learn new, potentially better-performing, policies. Motivated
by critical applications, we move beyond point estimators. Instead, we adopt the
principle of pessimism where we construct upper bounds that assess a policy‚Äôs worst-
case performance, enabling us to confidently select and learn improved policies.
Precisely, we introduce novel, fully empirical concentration bounds for a broad
class of importance weighting risk estimators. These bounds are general enough to
cover most existing estimators and pave the way for the development of new ones.
In particular, our pursuit of the tightest bound within this class motivates a novel
estimator ( LS), that logarithmically smooths large importance weights. The bound
forLSis provably tighter than its competitors, and naturally results in improved
policy selection and learning strategies. Extensive policy evaluation, selection, and
learning experiments highlight the versatility and favorable performance of LS.
1 Introduction
In decision-making under uncertainty, offline contextual bandit [ 16] presents a practical framework
for leveraging past interactions with an environment to optimize future decisions. This comes into
play when we possess logged data summarizing an agent‚Äôs past interactions [ 10]. These interactions,
typically captured as context-action-reward tuples, hold valuable insights into the underlying dynamics
of the environment. Each tuple represents a single round of interaction, where the agent observes
a context (including relevant features), takes an action according to its current policy, often called
behavior policy , and receives a reward that depends on both the observed context and the taken
action. This framework is prevalent in interactive systems like online advertising, music streaming,
and video recommendation. In online advertising, for instance, the user‚Äôs profile is the context, the
recommended product is the action, and the click-through rate (CTR) is the expected reward. By
learning from past interactions, the recommender system tailors product suggestions to individual
preferences, maximizing engagement and ultimately, business success.
To optimize future decisions without requiring real-time deployments, this framework presents us
with three tasks: off-policy evaluation (OPE) [ 16], off-policy selection (OPS) [ 32], and off-policy
learning (OPL) [ 55]. OPE estimates the risk: the negative of expected reward that a target policy
would achieve, essentially predicting its performance if deployed. OPS selects the best-performing
38th Conference on Neural Information Processing Systems (NeurIPS 2024).policy from a finite set of options, and OPL finds the optimal policy within an infinite class of policies.
In general, OPE is an intermediary step for OPS and OPL since its primary goal is policy comparison.
A significant amount of research in OPE has centered around Inverse Propensity Scoring (IPS)
estimators [ 24,16‚Äì18,60,19,54,38,32,45]. These estimators rely on importance weighting to
address the discrepancy between the target and behavior policies. While unbiased under some
conditions, IPS induces high variance. To mitigate this, regularization techniques have been proposed
for IPS [ 10,38,54,5,21] trading some bias for reduced variance. However, these estimators
can still deviate from the true risk, undermining their reliability for decision-making, especially in
critical applications. In such scenarios, practitioners need estimates that cover the true risk with high
confidence. To address this, several approaches focused on constructing either asymptotic [ 10,48,15]
or finite sample [ 32,21], high probability, empirical upper bounds on the risk. These bounds evaluate
the performance of a policy in the worst-case scenario, adopting the principle of pessimism [27].
If this principle is used in OPE, it is central in OPS and OPL, where strategies are inspired by, or
directly derived from, upper bounds on the risk [ 55,35,32,49,5,59,21]. Examples for OPS include
Kuzborskij et al. [32] who employed an Efron-Stein bound for self-normalized IPS, or Gabbianelli
et al. [21] that based their analysis on an upper bound constructed with the Implicit Exploration
estimator. Focusing on OPL, Swaminathan and Joachims [55] exploited the empirical Bernstein
bound [ 36] alongside the Clipping estimator to motivate sample variance penalization. This work
was recently improved by either modifying the penalization [ 59] or analyzing the problem from the
PAC-Bayesian lens [ 35]. The latter direction was further explored by Sakhi et al. [49], Aouali et al.
[5, 7], Gabbianelli et al. [21] resulting in tight PAC-Bayesian bounds that can be directly optimized.
Existing pessimistic OPE, OPS, and OPL approaches involve analyzing the concentration properties
of apre-defined risk estimator , often chosen to simplify the analysis. We propose a different approach:
we derive general concentration bounds applicable to a broad class of regularized IPS estimators and
then identify the estimator within this class that achieves the tightest concentration bound. This leads
to a tailored estimator, named Logarithmic Smoothing ( LS).LSenjoys several desirable properties.
It concentrates at a sub-Gaussian rate, and has a finite variance without being necessarily bounded.
Its concentration upper bound allows us to evaluate the worst-case risk of any policy, enables us to
derive a simple OPS strategy that directly minimizes our estimator akin to Gabbianelli et al. [21],
and achieves state-of-the-art learning guarantees for OPL when analyzed within the PAC-Bayesian
framework akin to [35, 49, 5, 7, 21].
This paper is structured as follows. Section 2 introduces the necessary background. In Section 3,
we provide unified risk bounds for a broad class of regularized IPS estimators, for which LSenjoys
the tightest upper bound. In Section 4, we analyze LSfor OPS and OPL, and we further extend
the analysis within the PAC-Bayesian framework. Extensive experiments in Section 5 highlight the
favorable performance of LS, and Section 6 provides concluding remarks.
2 Setting and background
Offline contextual bandit. LetX ‚äÇRdbe the context space , which is a compact subset of Rd,
and let A= [K]be a finite action set . An agent‚Äôs actions are guided by a stochastic andstationary
policy œÄ‚ààŒ†within a policy space Œ†. Given a context x‚àà X,œÄ(¬∑|x)is a probability distribution
over the action set A;œÄ(a|x)is the probability that the agent selects action ain context x. Then,
an agent interacts with a contextual bandit over nrounds. In round i‚àà[n], the agent observes a
context xi‚àºŒΩwhere ŒΩis a distribution with support X. After this, the agent selects an action
ai‚àºœÄ0(¬∑|xi), where œÄ0is the behavior policy of the agent. Finally, the agent receives a stochastic
costci‚àà[‚àí1,0]that depends on the observed context xiand the taken action ai. This cost ciis
sampled from a cost distribution p(¬∑|xi, ai). This leads to n-sized logged data, Dn= (xi, ai, ci)i‚àà[n],
where tuples (xi, ai, ci)fori‚àà[n]are i.i.d. The expected cost of taking action ain context xis
c(x, a) =Ec‚àºp(¬∑|x,a)[c], and the costs are negative because they are interpreted as the negative of
rewards. The performance of a policy œÄ‚ààŒ†is evaluated through its risk, which aggregates the
expected costs c(x, a)over all possible contexts x‚àà X and taken actions a‚àà A by policy œÄ, such as
R(œÄ) =Ex‚àºŒΩ,a‚àºœÄ(¬∑|x),c‚àºp(¬∑|x,a)[c] =Ex‚àºŒΩ,a‚àºœÄ(¬∑|x)[c(x, a)]. (1)
The main goal is to use logged dataset Dnto enhance future decision-making without necessitating
live deployments. This often entails three tasks: OPE, OPS, and OPL. First, OPE is concerned
2with constructing an estimator ÀÜRn(œÄ)of the risk R(œÄ)of a fixed target policy œÄand study its
deviation, aspiring for ÀÜRn(œÄ)to concentrate well around R(œÄ). Second, OPS focuses on selecting
the best performing policy ÀÜœÄS
nfrom a predefined andfinite collection of target policies {œÄ1, . . . , œÄ m},
effectively seeking to determine argmink‚àà[m]R(œÄk). Third, OPL aims to find a policy ÀÜœÄL
nwithin
thepotentially infinite policy space Œ†that achieves the lowest risk, essentially aiming to find
argminœÄ‚ààŒ†R(œÄ). In general, both OPS and OPL rely on OPE‚Äôs initial estimation of the risk.
Regularized IPS. Our work focuses on the inverse propensity scoring (IPS) estimator [ 24]. IPS
approximates the risk of a policy œÄ,R(œÄ), by adjusting the contribution of each sample in logged
data according to its importance weight (IW) , which is the ratio of the probability of an action under
the target policy œÄto its probability under the behavior policy œÄ0,
ÀÜRn(œÄ) =1
nnX
i=1wœÄ(xi, ai)ci, (2)
where for any (x, a)‚àà X √ó A , wœÄ(x, a) =œÄ(a|x)/œÄ0(a|x)are the IWs. IPS is unbiased under
the coverage assumption (see for example Owen [39, Chapter 9 ]). However, it can suffer high
variance, which tends to scale linearly with IWs [ 57]. This issue becomes pronounced when there is
a significant discrepancy between the target policy œÄand the behavior policy œÄ0. To mitigate this,
a common strategy consists in applying a regularization function h: [0,1]2√ó[‚àí1,0]‚Üí(‚àí‚àû,0]
toœÄ(a|x),œÄ0(a|x)andc. This function is designed to reduce the estimator‚Äôs variance at the cost of
introducing some bias. Formally, the function hneeds to satisfy the condition (C1), that is defined by
hsatisfies (C1)‚áê‚áí ‚àÄ (p, q, c )‚àà[0,1]2√ó[‚àí1,0], pc/q ‚â§h(p, q, c )‚â§0. (C1)
With such function h, the regularized IPS estimator reads
ÀÜRh
n(œÄ) =1
nnX
i=1h(œÄ(ai|xi), œÄ0(ai|xi), ci) =1
nnX
i=1hi, (3)
where hi=h(œÄ(ai|xi), œÄ0(ai|xi), ci). We recover standard IPS in (2)when h(p, q, c ) =pc/q .
Numerous regularization functions hwere studied in the literature. For example,
h(p, q, c ) = min( p/q, M )c , M‚ààR+=‚áíClipping [10] , (4)
h(p, q, c ) =pc/qŒ±, Œ±‚àà[0,1] =‚áíExponential Smoothing [5] ,
h(p, q, c ) =pc/(q+Œ≥), Œ≥‚â•0 =‚áíImplicit Exploration [21] .
Other IW regularizations include Harmonic [ 38] and Shrinkage [ 54]. With hsatisfying (C1), we can
derive our core result: a family of high-probability bounds that hold for regularized IPS.
3 Pessimistic off-policy evaluation
Standard OPE directly uses estimates of the risk, without capturing their associated uncertainty. This
limits its effectiveness in critical applications. Pessimistic OPE addresses this issue by relying on
finite sample, high-probability upper bounds to assess any policy‚Äôs worst-case risk [ 15,32]. This
section contributes to this effort and focuses on providing novel, finite sample, tight upper bounds on
the risk. This is achieved by deriving general bounds applicable to regularized IPS in (3).
3.1 Preliminaries and unified risk bounds
LetŒª >0,œÄ‚ààŒ†, and hsatisfying ( C1), we define
ÀÜMh,‚Ñì
n(œÄ) =1
nnX
i=1h‚Ñì
i, and œàŒª(x) =1
Œª(1‚àíexp(‚àíŒªx)),‚àÄx‚ààR, (5)
where ÀÜMh,‚Ñì
n(œÄ)is the empirical ‚Ñì-th moment of regularized IPS ÀÜRh
n(œÄ), and œàŒª:R‚ÜíRis a
contraction function satisfying œàŒª(x)‚â§xfor any x‚ààR. Then, we state our first result.
Proposition 1 (Empirical moments risk bound) .LetœÄ‚ààŒ†,L‚â•1,Œ¥‚àà(0,1],Œª > 0, and h
satisfying (C1). Then it holds with probability at least 1‚àíŒ¥that
R(œÄ)‚â§UŒª,h
L(œÄ),with UŒª,h
L(œÄ) =œàŒª
ÀÜRh
n(œÄ) +2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ) +ln(1/Œ¥)
Œªn
,(6)
where œàŒªand ÀÜMh,‚Ñì
n(œÄ)are both defined in (5), and recall that œàŒª(x)‚â§x.
3In Appendix F.1, we provide detailed proof, leveraging Chernoff bounds with a careful analysis of the
moment-generating function. This results in the first empirical, high-order moment bound for offline
contextual bandits, with several advantages. First, the bound applies to any regularization function h
that satisfies the mild condition (C1), enabling the design of a tailored hthat minimizes the bound.
Second, it relies solely on empirical moments, without assuming the existence of theoretical moments.
Third, the bound is fully empirical and tractable, facilitating efficient implementation of pessimism.
Lastly, the parameter Lcontrols the number of moments used, allowing a balance between bound
tightness and computational cost. Specifically, for sufficiently small values of Œª, higher values of
Lyield tighter bounds, though potentially at the cost of increased computational complexity as we
would need to compute higher order moments. This is formally stated as follows.
Proposition 2 (Impact of L).LetœÄ‚ààŒ†,Œ¥‚àà(0,1],Œª >0,L‚â•1, and hsatisfying (C1). Then,
Œª‚â§min
i‚àà[n]2L+ 2
(2L+ 1)|hi|
=‚áíUŒª,h
L+1(œÄ)‚â§UŒª,h
L(œÄ). (7)
From (7), the bound UŒª,h
L(œÄ)in(6)becomes a decreasing function of Lwhen Œª‚â§mini‚àà[n](1/|hi|),
suggesting that for sufficiently small Œª, the tightest bound is achieved as L‚Üí ‚àû . This condition on
Œªalso depends on the values of h, highlighting the importance of the regularizer choice h. In fact,
once we evaluate our bounds at their optimal regularizer function h, this condition on Œªbecomes
unnecessary when comparing some of the optimal bounds. Specifically, we demonstrate in the
following proposition that the bound with L= 1can be always improved by increasing L.
Proposition 3 (Comparison of our bounds) .LetœÄ‚ààŒ†, and Œª >0, we define
UŒª
L(œÄ) = min
hUŒª,h
L(œÄ),and h‚àó,L= argmin
hUŒª,h
L(œÄ), (8)
with the minimum taken over hsatisfying (C1). Then, for any Œª >0, it holds that for any L >1,
UŒª
L(œÄ)‚â§UŒª
1(œÄ).In particular, for any Œª >0,
UŒª
‚àû(œÄ)‚â§UŒª
1(œÄ). (9)
Proposition 3 shows that, irrespective of the value of Œª, the bound with L= 1can be always improved
by bounds of increased moment order L, evaluated at their optimal regularizer h‚àó,L. This result
encourages us to study bounds with high moment order L, especially if we can derive their optimal
regularizers h‚àó,L. To this end, we examine two cases: L= 1, which results in an empirical second-
moment bound, and L‚Üí ‚àû , yielding a tight bound that does not require computing high-order
moments. For each case, we identify the function hthat minimizes the bound. If the minimizer for
L= 1is a variant of the clipping estimator [ 10], minimizing L‚Üí ‚àû motivates a novel logarithmic
smoothing estimator. We begin by analyzing our empirical moment risk bound at L= 1.
3.2 Global clipping
Corollary 4 (Empirical second-moment risk bound with L= 1).LetœÄ‚ààŒ†,Œ¥‚àà(0,1],Œª >0, and
hsatisfying (C1). Then it holds with probability at least 1‚àíŒ¥that
R(œÄ)‚â§œàŒª
ÀÜRh
n(œÄ) +Œª
2ÀÜMh,2
n(œÄ) +ln(1/Œ¥)
Œªn
. (10)
This is a direct consequence of (6)when L= 1. The bound holds for any hsatisfying (C1). Thus we
search for a function h‚àó,1that minimizes bound in (10). This function h‚àó,1writes
h‚àó,1(p, q, c ) =‚àímin(p|c|/q,1/Œª). (11)
In particular, if we assume that costs are binary, c‚àà {‚àí 1,0}, then h‚àó,1corresponds to clipping
in(4)with parameter M= 1/Œª. This is because ‚àímin(|c|p/q,1/Œª) = min 
p/q,1
Œª
cwhen cis
binary. This motivates the widely used clipping estimator [ 10]. However, this also suggests that the
standard way of clipping (as in (4)) is only optimal1for binary costs. In general, the cost should also
be clipped (as in (11)). Finally, with a suitable choice of Œª=O(1/‚àön), our bound in Corollary 4,
using clipping (i.e., h=h‚àó,1), outperforms the existing empirical Bernstein bound [ 55], which was
specifically derived for clipping. This confirms the strength of our general bound, as minimizing it
results in a bound with tighter concentration than specialized bounds. Appendix F.4 gives the the
proof to find h‚àó,1and formal comparisons with empirical Bernstein are provided in Appendix F.5. In
the next section, we study our general bound when we set L‚Üí ‚àû .
1Here, optimality of a function his defined with respect to our bound with L= 1(Corollary 4).
43.3 Logarithmic smoothing
Corollary 5 (Empirical infinite-moment bound with L‚Üí ‚àû ).LetœÄ‚ààŒ†,Œ¥‚àà(0,1],Œª >0, and h
satisfying (C1). Then it holds with probability at least 1‚àíŒ¥that
R(œÄ)‚â§œàŒª
‚àí1
nnX
i=11
Œªlog (1‚àíŒªhi) +ln(1/Œ¥)
Œªn
. (12)
Appendix F.6 provides detailed proof. Setting L‚Üí ‚àû in(6)results in the bound in Corollary 5,
which has different properties than Corollary 4. The resulting bound has a simple expression that does
not require computing high order moments. This means that we can obtain the best of both worlds, a
tight concentration bound with no additional computational complexity. As the bound is increasing
inh, the function h‚àó,‚àûthat minimizes this bound is h‚àó,‚àû(p, q, c ) =pc/q . This corresponds to the
standard IPS in (2). This differs from the L= 1bound in Corollary 4 that favored clipping. This
shows the impact of the moment order Lon the optimal function h. For any œÄ‚ààŒ†, applying the
bound in Corollary 5 with the optimal h‚àó,‚àûleads to UŒª
‚àû(œÄ), of the following expression:
UŒª
‚àû(œÄ) =œàŒª
ÀÜRŒª
n(œÄ) +ln(1/Œ¥)
Œªn
. (13)
Even if we set h‚àó,‚àû(p, q, c ) =pc/q (without IW regularization), UŒª
‚àû(œÄ)can be seen as a risk upper
bound of a novel regularized IPS estimator (satisfying ( C1)), called Logarithmic Smoothing ( LS):
ÀÜRŒª
n(œÄ) =‚àí1
nnX
i=11
Œªlog (1‚àíŒªwœÄ(xi, ai)ci). (14)
0 10 20 30 40 50
Importance Weight w¬ºIPS
Clipping, M = 20
LS, ¬∏ = 0.1
LS, ¬∏ = 0.05
LS, ¬∏ = 0.01
Figure 1: LSwith different Œªs.TheLSestimator in (14) is defined for any non-negative Œª‚â•0,
with its bound in (13) holding for any positive Œª > 0. No-
tably, Œª= 0retrieves the standard IPS estimator in (2), while
Œª >0introduces a bias-variance trade-off by logarithmically
smoothing the IWs (Figure 1). This estimator acts as a soft,
differentiable variant of clipping with parameter 1/Œª. A Taylor
expansion of our estimator around Œª= 0yields
ÀÜRŒª
n(œÄ) =ÀÜRn(œÄ) +‚àûX
‚Ñì=2Œª‚Ñì‚àí1
‚Ñì1
nnX
i=1(wœÄ(xi, ai)ci)‚Ñì
.
Thus, LSis a pessimistic estimator by design , implicitly im-
plementing a form of Sample All Moments Penalization , which generalizes the Sample Variance
Penalization [55]. To examine the statistical properties of our estimator, we introduce
SŒª(œÄ) =E(wœÄ(x, a)c)2
(1‚àíŒªwœÄ(x, a)c)
, (15)
which quantifies the discrepancy between œÄandœÄ0. Notably, SŒª(œÄ)is always smaller than the
second moment of the IW, effectively interpolating between a weighted first moment ( Œª‚â´1) and the
second moment ( Œª= 0) ofIPS. This quantity SŒªcharacterizes the concentration properties of the LS
estimator akin to the coverage ratio for IXestimator [ 21]. With SŒªdefined, we proceed by bounding
the mean squared error (MSE) of our estimator, specifically bounding its bias and variance.
Proposition 6 (Bias-variance trade-off) .LetœÄ‚ààŒ†andŒª‚â•0. LetBŒª(œÄ)andVŒª(œÄ)be respectively
the bias and the variance of the LSestimator. Then we have that
0‚â§ BŒª(œÄ)‚â§ŒªSŒª(œÄ),andVŒª(œÄ)‚â§SŒª(œÄ)
n.
Moreover, it holds that for any Œª >0, the variance is finite as VŒª(œÄ)‚â§ |R(œÄ)|/Œªn‚â§1/Œªn.
We observe that both the bias and variance are controlled by SŒª(œÄ). Particularly, Œª= 0recovers the
IPS estimator in (2), with zero bias and a variance bounded by E
w2(x, a)c2
/n. When Œª >0, a
bias-variance trade-off emerges. The bias is always non-negative and is capped at ŒªSŒª(œÄ), which
diminishes to zero when Œªis small and goes to |R(œÄ)|asŒªincreases. Conversely, the variance
decreases with a higher Œª. Notably, Œª > 0ensures finite variance bounded by 1/Œªn, despite
the estimator being unbounded. This is different from previous estimators that relied on bounded
functions to ensure finite variance. We also prove in the following that a good choice of Œª=O(1/‚àön)
ensures that our LSestimator enjoys a sub-Gaussian concentration [38].
5Proposition 7 (Sub-Gaussianity and comparison with Metelli et al. [38]).LetœÄ‚ààŒ†,Œ¥‚àà(0,1]and
Œª >0. Then the following inequalities holds with probability at least 1‚àíŒ¥:
R(œÄ)‚àíÀÜRŒª
n(œÄ)‚â§ln(2/Œ¥)
Œªn, and ÀÜRŒª
n(œÄ)‚àíR(œÄ)‚â§ŒªSŒª(œÄ) +ln(2/Œ¥)
Œªn.
In particular, setting Œª=Œª‚àó=p
ln(2/Œ¥)/nE[wœÄ(x, a)2c2]yields that
|R(œÄ)‚àíÀÜRŒª‚àó
n(œÄ)| ‚â§p
2œÉ2ln(2/Œ¥), where œÉ2= 2E
wœÄ(x, a)2c2
/n . (16)
Thus, a particular choice of Œª‚àóensures that ÀÜRŒª‚àón(œÄ)is sub-Gaussian, with a variance proxy œÉ2that
improves on that obtained for the Harmonic estimator of Metelli et al. [38]. We refer the interested
reader to Appendix E.2 for further discussions and proofs.
Next, we focus on the tightness of the LSupper bound in (13) as it will motivate our selection and
learning strategies. Proposition 3 already showed that UŒª
‚àû(œÄ), the bound of LSis tighter than UŒª
1(œÄ),
the bound in Corollary 4 evaluated at the Global clipping function h‚àó,1. In this section, we compare
theLSbound to the already tight IXbound presented by Gabbianelli et al. [21] and demonstrate in
the following that the LSbound dominates it in all scenarios.
Proposition 8 (Comparison with IXof Gabbianelli et al. [21]).LetœÄ‚ààŒ†,Œ¥‚àà]0,1]andŒª >0, the
IXbound from [21] states that we have with probability at least 1‚àíŒ¥
R(œÄ)‚â§ÀÜRŒª-IX
n(œÄ) +ln(1/Œ¥)
Œªn,with ÀÜRŒª-IX
n(œÄ) =1
nnX
i=1œÄ(ai|xi)
œÄ0(ai|xi) +Œª/2ci. (17)
LetUŒª
IX(œÄ)be the upper bound of (17), we have for any Œª >0:
UŒª
‚àû(œÄ)‚â§UŒª
IX(œÄ). (18)
This result states that no matter the scenario, for any evaluated policy œÄ, and any chosen Œª >0, the
LSbound will be always tighter than IX. The gap between the LSandIXbounds increases when nis
small, or when the evaluated policy œÄis stochastic, as demonstrated and developed in Appendix F.8.
These findings further validate the effectiveness of our approach, enabling us to identify the LS
estimator, with an empirical bound that improves upon the tightest existing bounds. Consequently,
we leverage the LSbound in the next section to derive our pessimistic OPS and OPL strategies.
4 Off-policy selection and learning
4.1 Off-policy selection
LetŒ†S={œÄ1, ..., œÄ m}be a finite set of policies. In OPS, the goal is to find œÄS
‚àó‚ààŒ†Sthat satisfies
œÄS
‚àó= argmin
œÄ‚ààŒ†SR(œÄ) = argmin
k‚àà[m]R(œÄk). (19)
As we do not have access to the true risk, we use a data-driven selection strategy that guarantees the
identification of policies of performance close to that of œÄS
‚àó. Precisely, for Œª >0, we search for
ÀÜœÄS
n= argmin
œÄ‚ààŒ†SÀÜRŒª
n(œÄ) = argmin
k‚àà[m]ÀÜRŒª
n(œÄk). (20)
To derive our strategy in (20), we minimize the bound of LSin(13), employing pessimism [ 27].
Fortunately, in our case, this boils down to minimizing ÀÜRŒª
n(œÄ), since the other terms in the bound are
independent of the target policy œÄ. This allows us to avoid computing complex statistics [ 55,32] and
does not require access to the behavior policy œÄ0. As we show next, it also ensures low suboptimality.
Proposition 9 (Suboptimality of our selection strategy in (20)).LetŒª >0andŒ¥‚àà(0,1]. Then, it
holds with probability at least 1‚àíŒ¥that
0‚â§R(ÀÜœÄS
n)‚àíR(œÄS
‚àó)‚â§ŒªSŒª(œÄS
‚àó) +2 ln(2|Œ†S|/Œ¥)
Œªn, (21)
where SŒª(œÄ),œÄS
‚àóandÀÜœÄS
nare defined in (15),(19) and(20).
6The derived suboptimality bound only requires coverage of the optimal actions (support of the optimal
policy œÄs
‚àó), and improves on IXsuboptimality [ 21], matching the minimax suboptimality lower bound
of pessimistic methods [ 34,27,28]. Appendix G.1 provides proof of this suboptimality bound, and
we discuss how this suboptimality improves upon existing strategies in Appendix E.3. By selecting
Œªs
n=p
2 ln(2|Œ†S|/Œ¥)/nforLS, we achieve a suboptimality scaling of O(1/‚àön),
0‚â§R(ÀÜœÄS
n)‚àíR(œÄS
‚àó)‚â§ 
1 +SŒªsn(œÄS
‚àó)p
2 ln(2|Œ†S|/Œ¥)/n, (22)
which ensures finding the optimal policy with sufficient samples. Additionally, the multiplicative
constant is smaller when œÄ0is close to œÄS
‚àó, confirming the known observation that it is easier to
identify the best policy if it is similar to the behavior policy œÄ0.
4.2 Off-policy learning
Similar to how we extended the evaluation bound in Corollary 5 (which applies to a single fixed
target policy) to OPS (where it applies to a finite set of target policies), we can further derive bounds
for an infinite policy class Œ†, enabling OPL. Several approaches have been proposed in previous
work, primarily based on replacing the finite union bound over policies with more sophisticated
uniform-convergence arguments. This was used by [ 55], which derived a variance-sensitive bound
scaling with the covering number [ 61]. Since these approaches incorporate a complexity term that
depends only on the policy class, the resulting pessimistic learning strategy (which minimizes the
upper bound) would be similar to the selection strategy adopted earlier, leading, for a fixed Œª, to
ÀÜœÄL
n= argmin
œÄ‚ààŒ†ÀÜRŒª
n(œÄ) +C(Œ†)
Œªn= argmin
œÄ‚ààŒ†ÀÜRŒª
n(œÄ). (23)
whereC(Œ†)is a complexity measure [ 61]. This learning strategy is straightforward because it involves
a smooth estimator that can be optimized using first-order methods and does not require second-order
statistics. However, analyzing this approach is more challenging because the complexity measure
C(Œ†)varies depending on the policy class considered, is often intractable [ 49] and can only be upper
bounded with problem dependent constants [28].
Instead of the method described above, we derive PAC-Bayesian generalization bounds [ 37,11] that
apply to arbitrary policy classes. This framework has been shown to provide strong performance
guarantees for OPL in practical scenarios [ 49,5]. The PAC-Bayesian framework analyzes the
performance of policies by viewing them as randomized predictors [ 35]. Specifically, let F(Œò) =
{fŒ∏:X ‚Üí [K], Œ∏‚ààŒò}be a set of parameterized predictors that associate the context xwith the
action fŒ∏(x)‚àà[K]. LetP(Œò)be the set of all probability distributions on Œò. Each distribution
Q‚àà P(Œò)defines a policy œÄQby setting the probability of action agiven context xas the probability
that a random predictor fŒ∏‚àºQmaps xto action a, that is,
œÄQ(a|x) =EŒ∏‚àºQ[ 1[fŒ∏(x) =a]], ‚àÄ(x, a)‚àà X √ó A . (24)
This characterization is not restrictive as any policy can be represented in this form [ 49]. Deriving
PAC-Bayesian generalization bounds with this policy definition requires the regularized IPS to be
linear in the target policy œÄ[35,5,21]. Our estimator LSin(14) is non-linear in œÄ. Therefore, for
this PAC-Bayesian analysis, we introduce a linearized variant of LS, called LS-LIN, and defined as
ÀÜRŒª-LIN
n(œÄ) =‚àí1
nnX
i=1œÄ(ai|xi)
Œªlog
1‚àíŒªci
œÄ0(ai|xi)
, (25)
which smooths the impact of the behavior propensity œÄ0instead of the IWs œÄ/œÄ0. We provide in the
following a core result of this section, the PAC-Bayesian bound that defines our learning strategy.
Proposition 10 (PAC-Bayes learning bound for ÀÜRŒª-LIN
n).Given a prior P‚àà P(Œò),Œ¥‚àà(0,1]and
Œª >0, the following holds with probability at least 1‚àíŒ¥:
‚àÄQ‚àà P(Œò), R (œÄQ)‚â§œàŒª
ÀÜRŒª-LIN
n(œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn
, (26)
where KL(Q||P)is the Kullback-Leibler divergence from PtoQ.
7PAC-Bayes bounds hold uniformly for all distributions Q‚àà P(Œò) and replace the complexity
measure C(Œ†)with the divergence KL(Q||P)from a reference prior distribution P. Extensive
research focuses on identifying the best strategies for choosing this prior P[40]. While these bounds
hold for any fixed prior P, in practice, it is typically set to the distribution inducing the behavior
policy œÄ0, meaning Psatisfies œÄ0=œÄP. This leads to an intuitive learning principle: by minimizing
the upper bound, we seek policies with good empirical risk that do not deviate significantly from œÄ0.
Our bound can also be obtained using the truncation method from Alquier [1, Corollary 2.5 ]. This
bound surpasses the already tight PAC-Bayesian bounds derived for Clipping [ 49], Exponential
Smoothing [ 5], and Implicit Exploration [ 21], resulting in the tightest known generalization bound in
OPL. Appendix G.2 gives formal proof of this bound and comparisons with existing PAC-Bayesian
bounds can be found in Appendix E.4. For a fixed Œªand a fixed prior P, we derive a learning strategy
that minimizes the upper bound for a subset L(Œò)‚äÜ P(Œò)of distributions, seeking
Qn= argmin
Q‚ààL(Œò)
ÀÜRŒª-LIN
n(œÄQ) +KL(Q||P)
Œªn
,and setting ÀÜœÄL
n=œÄQn. (27)
(27) is tractable and can be efficiently optimized for various policy classes [ 49,5]. Below, we analyze
its suboptimality compared to the best policy in the chosen class, œÄQ‚àó= argminQ‚ààL(Œò)R(œÄQ).
Proposition 11 (Suboptimality of the learning strategy in (27)).LetŒª >0,P‚àà L(Œò)andŒ¥‚àà(0,1].
Then, it holds with probability at least 1‚àíŒ¥that
0‚â§R(ÀÜœÄL
n)‚àíR(œÄQ‚àó)‚â§ŒªSLIN
Œª(œÄQ‚àó) +2 (KL(Q‚àó||P) + ln(2 /Œ¥))
Œªn, (28)
where SLIN
Œª(œÄ) =E
œÄ(a|x)c2/(œÄ2
0(a|x)‚àíŒªœÄ0(a|x)c)
andÀÜœÄL
nis defined in (27).
Our suboptimality bound only requires coverage of the support of the optimal policy œÄQ‚àó. This
bound matches the minimax suboptimality lower bound of pessimistic learning with deterministic
policies [ 28]. Appendix G.3 provides a proof of Proposition 11, while Appendix E.5 discusses the
suboptimality bound further and proves that it improves on the IX learning strategy of [ 21, Section 5].
Setting Œªl
n= 2/‚àönguarantees us a suboptimality that scales with O(1/‚àön)as
0‚â§R(ÀÜœÄL
n)‚àíR(œÄQ‚àó)‚â§(2SLIN
Œªln(œÄQ‚àó) +KL(Q‚àó||P) + ln(2 /Œ¥))/‚àön.
By setting the reference Pto the distribution inducing œÄ0, we find that the learning suboptimality
is reduced when the behavior policy œÄ0is close to the optimal policy œÄQ‚àó. This is similar to the
suboptimality for our selection strategy. The suboptimality upper bound reflects a common intuition
in the OPL literature: pessimistic learning algorithms converge faster when œÄ0is close to œÄQ‚àó.
5 Experiments
Our experimental setup follows the standard multiclass-to-bandit conversion used in prior studies
[18,55]. Each multi-class dataset has features and labels and we convert it to contextual bandit
problems where contexts correspond to features and actions to labels. Precisely, the reward rfor
taking action (label) awith context (features) xis modeled as Bernoulli with probability px=
œµ+ 1[a=œÅ(x)] (1‚àí2œµ), where œÅ(x)be the true label of features x, and œµis a noise parameter. In
particular, the true label œÅ(x)represents the action with the highest average reward for context x. This
setup ensures an average reward of 1‚àíœµfor the optimal action œÅ(x)andœµfor all others, constructing
a logged bandit feedback dataset in the form {xi, ai, ci}i‚àà[n], where ci=‚àíriis the associated cost.
5.1 Off-policy evaluation and selection experiments
For both evaluation and selection, we adopt the same experimental design as [ 32] to facilitate the
comparison. We consider exponential target policies œÄ(a|x)‚àùexp(1
œÑf(a, x)), with œÑa temperature
controlling the policy‚Äôs entropy and f(a, x)the score of the item afor the context x. We use this
to define ideal policies as œÄideal(a|x)‚àùexp(1
œÑI{œÅ(x) =a}), and also create faulty, mismatching
policies for which the peak is shifted to another, wrong action for a set of faulty actions F‚äÇ[K]. To
recreate real world scenarios, we also consider policies directly learned from logged bandit feedback,
of the form œÄŒ∏IPS(a|x)‚àùexp(1
œÑxtŒ∏IPS
a)andœÄŒ∏SN(a|x)‚àùexp(1
œÑxtŒ∏SN
a), with their parameters learned
8Table 1: Bound‚Äôs tightness (|U(œÄ)/R(œÄ)‚àí1|)with varying number of samples of the kropt dataset.
Number of samples SN-ES cIPS-EB IX cIPS-L=1 (Ours) LS(Ours)
281.000 0.917 0.373 0.364 0.362
291.000 0.732 0.257 0.289 0.236
2100.794 0.554 0.226 0.240 0.213
2110.649 0.441 0.171 0.197 0.159
2120.472 0.327 0.126 0.147 0.117
2130.374 0.204 0.062 0.077 0.054
2140.257 0.138 0.041 0.049 0.035
by respectively minimizing the IPS[24] and SN[56] empirical risks. More details on the definition
of the different policies are given in Appendix H. Finally, 11 real multiclass classification datasets are
chosen from the UCI ML Repository [ 8] (See Table 3 in Appendix H.1.1) with various number of
samples, dimensions and action space sizes to conduct our experiments2.
(OPE) Tightness of the bounds. Evaluating the worst case performance of a policy is done through
evaluating risk upper bounds [10, 32]. This means that a better evaluation will solely depend on the
tightness of the bounds used. To this end, given a policy œÄ, we are interested in bounds U(œÄ)with a
small relative radius |U(œÄ)/R(œÄ)‚àí1|. We compare our newly derived bounds ( cIPS-L=1 forUŒª
1
andLSforUŒª
‚àûboth with Œª= 1/‚àön) to empirical evaluation bounds of the literature: SN-ES : the
Efron Stein bound for Self Normalized IPS [ 32],cIPS-EB : Empirical Bernstein for Clipping [ 55]
and the recent IX: Implicit Exploration bound [ 21]. The first experiment uses the kropt dataset
withœµ= 0.2, collects bandit feedback with faulty behavior policy (with œÑ= 0.25) to evaluate
an ideal policy ( œÑ= 0.1), and explores how the relative radiuses of the considered bounds shrink
while varying the number of datapoints. Table 1 compiles the results of the experiments and suggest
that the LSbound is tighter than its competitors no matter the size of the feedback collected. The
second experiments uses all 11 datasets, with different behavior policies ( œÑ0‚àà {0.2,0.25,0.3})
and different noise levels ( œµ‚àà {0.,0.1,0.2}) to evaluate ideal policies with different temperatures
(œÑ‚àà {0.1,0.2,0.3,0.4,0.5}), defining ‚àº500different scenarios to validate our findings. We plot in
Figure 2 the cumulative distribution of the relative radius of the considered bounds. We observe that
while cIPS-L=1 andIXcan be comparable, the LSbound is tighter than all its competitors. We also
provide detailed results in Appendix H.1.2 that further confirm the superiority of the LSbound.
(OPS) Find the best, avoid the worst policy. Policy selection aims at identifying the best policy
among a set of finite candidates. In practice, we are interested in finding policies that improve on
œÄ0and avoid policies that perform worse than œÄ0. To replicate real world scenarios, we design
an experiment where œÄ0is a faulty policy ( œÑ= 0.2), that collects noisy ( œµ= 0.2) interaction
data, some of which is used to learn œÄŒ∏IPS, œÄŒ∏SN, and that we add to our discrete set of policies
Œ†k=4={œÄ0, œÄideal, œÄŒ∏IPS, œÄŒ∏SN}. The goal is to measure the ability of our selection strategies to
choose from Œ†k=4, better performing policies than œÄ0. We thus define three possible outcomes:
a strategy can select worse performing policies, better performing or the best policy. Our goal in
these experiments is to empirically validate the pitfalls of point estimators while confirming the
benefits of using the pessimism principle. To this end, we compare pessimistic selection strategies
to policy selection using the classical point estimators IPS[24] and SN[56]. The comparison is
conducted on the 11 UCI datasets with 10 different seeds resulting in 110 scenarios. We plot in
Figure 2 the percentage of time each method selected the best policy, a better or a worse policy than
œÄ0. While risk estimators can identify the best policy, they are unreliable as they can choose worse
performing policies than œÄ0, a catastrophic outcome in critical applications. Pessimistic selection is
more conservative, as it avoids poor performing policies completely and empirically confirms that
tighter upper bounds result in better selection strategies: LSupper bound is less conservative and
finds best policies the most (comparable to SN) while never selecting poor performing policies. Fine
grained results (for each dataset) can be found in Appendix H.1.3.
5.2 Off-policy learning experiments
We follow the successful off policy learning paradigm based on directly minimizing PAC-Bayesian
risk generalization bounds [ 49,5] as it comes with guarantees of improvement and avoids hyper-
2The code can be found at https://github.com/otmhi/offpolicy_ls .
90.0 0.2 0.4 0.6 0.8 1.0
Tightness of the Upper Bound (Relative Radius)0.00.20.40.60.81.0Cumulative ProbabilityOPE: Comparing Tightness of the Bounds of Pessimistic Methods
SN-ES
cIPS-EB
IX
cIPS-L=1 (Ours)
LS (Ours)
IPS SN SN-ES cIPS-EB IXcIPS-L=1 LS
Selection Strategy (cIPS-L=1 and LS are Ours)020406080100Percentage of Worse, Better and BestOPS: Performance of Selected Policies Compared to Logging Policies
Worse
Better
BestFigure 2: Results for OPE and OPS experiments.
cIPS cvcIPS ES IX LS-LIN (Ours)
rI(U(ÀÜœÄL
n)) 14.48% 21.28% 7.78% 24.74% 26.31%
rI(R(ÀÜœÄL
n)) 28.13% 33.64% 29.44% 36.70% 36.76%
Table 2: OPL: Relative Improvement of guaranteed risk and true risk averaged over 200 scenarios.
parameter tuning. For comparable results, we use the same 4 datasets (described in Appendix H.2,
Table 7) as in [ 49,5] and adopt the LGP : Linear Gaussian Policies [ 49] as our class of parametrized
policies. For each dataset, we use behavior policies trained on a small fraction of the data in a super-
vised fashion, combined with different inverse temperature parameters Œ±‚àà {0.1,0.3,0.5,0.7,1.}
to cover cases of diffused and peaked behavior policies. These policies generate for 10 different
seeds, 10 logged bandit feedback datasets resulting in 200 different scenarios to test our learning
approaches. In the PAC-Bayesian OPL paradigm, we minimize the empirical upper bounds U(œÄ)
directly and obtain the learned policy as the bound‚Äôs minimizer ÀÜœÄL
n(as in (27)). With ÀÜœÄL
nobtained, we
are interested in two quantities: The guaranteed risk by the bound, which is the value of the bound
U(ÀÜœÄL
n)at its minimizer. This quantity reflects the worst case performance of the learned policy, a
lower value implies stronger performance guarantees. We are also interested in the true risk of the
minimizer of the bound R(ÀÜœÄL
n)as it translates the performance of the obtained policy acting on unseen
data. As this learning paradigm is based on optimizing tractable, generalization bounds, we only
compare our approach to methods that provide them. Precisely, we compare our LS-LIN learning
strategy in (27) to strategies based on minimizing off-policy PAC Bayesian bounds from the literature:
clipped IPS ( cIPS ) and Control Variate clipped IPS ( cvcIPS ) [49], Exponential Smoothing ( ES) [5]
and Implicit Exploration ( IX) [21]. The results are summarized in Table 2 where we compute:
rI(x) = (R(œÄ0)‚àíx)/(R(œÄ0)‚àíR(œÄ‚àó)) = ( R(œÄ0)‚àíx)/(R(œÄ0) + 1) ,
the improvement over R(œÄ0)achieved by minimizing the different bounds in terms of x‚àà {U, R}
(guaranteed risk and true risk respectively), relative to an ideal improvement. This metric helps us
normalize the results, and we report its average over 200 different scenarios, with results in bold
being significantly better. Fine grained results can be found in Appendix H.2.4. We observe that the
LS-LIN PAC-Bayesian bound improves substantially on its competitors in terms of the guaranteed
risk, and also obtains the best performing policies (on par with the IXPAC-Bayesian bound).
6 Conclusion
Motivated by the pessimism principle, we have derived novel, empirical risk upper bounds tailored
for the regularized IPS family of estimators. Minimizing these bounds within this family unveiled
Logarithmic Smoothing, a simple estimator with good concentration properties. With its tight upper
bound, LSconfidently evaluates a policy, and shows provably better guarantees for both selecting and
learning policies than all competitors. Our upper bounds remain broadly applicable, only requiring
negative costs . While this condition does not impact importance weighting estimators, it does not
hold for doubly robust estimators. Extending our approach to derive empirical bounds for this type
of estimators presents a nontrivial, yet interesting task to explore in future work. Another potential
extension would be to relax the i.i.d. assumption of the contextual bandit problem to address, the
general offline Reinforcement Learning setting. This direction will introduce a more challenging
estimation task and requires developing new concentration bounds.
10References
[1]Pierre Alquier. Transductive and Inductive Adaptative Inference for Regression and Density Esti-
mation . Theses, ENSAE ParisTech, December 2006. URL https://pastel.hal.science/
tel-00119593 .
[2]Pierre Alquier. User-friendly introduction to PAC-Bayes bounds. Foundations and Trends ¬Æin
Machine Learning , 17(2), 2024.
[3]Imad Aouali, Amine Benhalloum, Martin Bompaire, Achraf Ait Sidi Hammou, Sergey Ivanov,
Benjamin Heymann, David Rohde, Otmane Sakhi, Flavian Vasile, and Maxime V ono. Reward
Optimizing Recommendation using Deep Learning and Fast Maximum Inner Product Search. In
Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining ,
KDD ‚Äô22, page 4772‚Äì4773, New York, NY , USA, 2022. Association for Computing Machinery.
ISBN 9781450393850. doi: 10.1145/3534678.3542622. URL https://doi.org/10.1145/
3534678.3542622 .
[4]Imad Aouali, Achraf Ait Sidi Hammou, Sergey Ivanov, Otmane Sakhi, David Rohde, and
Flavian Vasile. Probabilistic Rank and Reward: A Scalable Model for Slate Recommendation,
2022.
[5]Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Exponential Smoothing
for Off-Policy Learning. In Proceedings of the 40th International Conference on Machine
Learning , pages 984‚Äì1017. PMLR, 2023.
[6]Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Bayesian off-policy
evaluation and learning for large action spaces. arXiv preprint arXiv:2402.14664 , 2024.
[7]Imad Aouali, Victor-Emmanuel Brunel, David Rohde, and Anna Korba. Unified PAC-Bayesian
Study of Pessimism for Offline Policy Learning with Regularized Importance Sampling. In The
40th Conference on Uncertainty in Artificial Intelligence , 2024. URL https://openreview.
net/forum?id=d7W4H0sTXU .
[8]A. Asuncion and D. J. Newman. UCI machine learning repository, 2007. URL http://www.
ics.uci.edu/$\sim$mlearn/{MLR}epository.html .
[9]Heejung Bang and James M Robins. Doubly robust estimation in missing data and causal
inference models. Biometrics , 61(4):962‚Äì973, 2005.
[10] L√©on Bottou, Jonas Peters, Joaquin Qui√±onero-Candela, Denis X Charles, D Max Chickering,
Elon Portugaly, Dipankar Ray, Patrice Simard, and Ed Snelson. Counterfactual reasoning and
learning systems: The example of computational advertising. Journal of Machine Learning
Research , 14(11), 2013.
[11] Olivier Catoni. PAC-Bayesian supervised classification: The thermodynamics of statistical
learning. IMS Lecture Notes Monograph Series , page 1‚Äì163, 2007. ISSN 0749-2170. doi: 10.
1214/074921707000000391. URL http://dx.doi.org/10.1214/074921707000000391 .
[12] Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H. Chi. Top-K
Off-Policy Correction for a REINFORCE Recommender System. In Proceedings of the Twelfth
ACM International Conference on Web Search and Data Mining , WSDM ‚Äô19, page 456‚Äì464,
New York, NY , USA, 2019. Association for Computing Machinery. ISBN 9781450359405. doi:
10.1145/3289600.3290999. URL https://doi.org/10.1145/3289600.3290999 .
[13] Victor Chernozhukov, Mert Demirer, Greg Lewis, and Vasilis Syrgkanis. Semi-parametric
efficient policy learning with continuous actions. Advances in Neural Information Processing
Systems , 32, 2019.
[14] Matej Cief, Jacek Golebiowski, Philipp Schmidt, Ziawasch Abedjan, and Artur Bekasov.
Learning action embeddings for off-policy evaluation. In European Conference on Information
Retrieval , pages 108‚Äì122. Springer, 2024.
11[15] Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesvari, and Dale Schu-
urmans. Coindice: Off-policy confidence interval estimation. In H. Larochelle,
M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural In-
formation Processing Systems , volume 33, pages 9398‚Äì9411. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/
6aaba9a124857622930ca4e50f5afed2-Paper.pdf .
[16] Miroslav Dud√≠k, John Langford, and Lihong Li. Doubly robust policy evaluation and learning.
InProceedings of the 28th International Conference on International Conference on Machine
Learning , ICML‚Äô11, page 1097‚Äì1104, 2011.
[17] Miroslav Dud√≠k, Dumitru Erhan, John Langford, and Lihong Li. Sample-efficient nonstationary
policy evaluation for contextual bandits. In Proceedings of the Twenty-Eighth Conference on
Uncertainty in Artificial Intelligence , UAI‚Äô12, page 247‚Äì254, Arlington, Virginia, USA, 2012.
AUAI Press.
[18] Miroslav Dudik, Dumitru Erhan, John Langford, and Lihong Li. Doubly robust policy evaluation
and optimization. Statistical Science , 29(4):485‚Äì511, 2014.
[19] Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust
off-policy evaluation. In International Conference on Machine Learning , pages 1447‚Äì1456.
PMLR, 2018.
[20] Hamish Flynn, David Reeb, Melih Kandemir, and Jan Peters. PAC-Bayes Bounds for Bandit
Problems: A Survey and Experimental Comparison. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 45(12):15308‚Äì15327, 2023. doi: 10.1109/TPAMI.2023.3305381.
[21] Germano Gabbianelli, Gergely Neu, and Matteo Papini. Importance-weighted offline learning
done right. In Proceedings of The 35th International Conference on Algorithmic Learning
Theory , volume 237 of Proceedings of Machine Learning Research , pages 614‚Äì634. PMLR,
25‚Äì28 Feb 2024. URL https://proceedings.mlr.press/v237/gabbianelli24a.html .
[22] Alexandre Gilotte, Cl√©ment Calauz√®nes, Thomas Nedelec, Alexandre Abraham, and Simon
Doll√©. Offline A/B testing for recommender systems. In Proceedings of the Eleventh ACM
International Conference on Web Search and Data Mining , pages 198‚Äì206, 2018.
[23] Torben Hagerup and Christine R√ºb. A Guided Tour of Chernoff Bounds. Inf. Process. Lett. , 33
(6):305‚Äì308, 1990. URL http://dblp.uni-trier.de/db/journals/ipl/ipl33.html#
HagerupR90 .
[24] Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement
from a finite universe. Journal of the American statistical Association , 47(260):663‚Äì685, 1952.
[25] Edward L Ionides. Truncated importance sampling. Journal of Computational and Graphical
Statistics , 17(2):295‚Äì311, 2008.
[26] Olivier Jeunen and Bart Goethals. Pessimistic reward models for off-policy learning in recom-
mendation. In Fifteenth ACM Conference on Recommender Systems , pages 63‚Äì74, 2021.
[27] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In
International Conference on Machine Learning , pages 5084‚Äì5096. PMLR, 2021.
[28] Ying Jin, Zhimei Ren, Zhuoran Yang, and Zhaoran Wang. Policy learning "without‚Äù overlap:
Pessimism and generalized empirical Bernstein‚Äôs inequality, 2023.
[29] Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments.
InInternational conference on artificial intelligence and statistics , pages 1243‚Äì1251. PMLR,
2018.
[30] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[31] Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual
bandits with continuous actions: Smoothing, zooming, and adapting. Journal of Machine
Learning Research , 21(137):1‚Äì45, 2020.
12[32] Ilja Kuzborskij, Claire Vernade, Andras Gyorgy, and Csaba Szepesv√°ri. Confident off-policy
evaluation and selection through self-normalized importance weighting. In International
Conference on Artificial Intelligence and Statistics , pages 640‚Äì648. PMLR, 2021.
[33] Tor Lattimore and Csaba Szepesvari. Bandit Algorithms . Cambridge University Press, 2019.
[34] Lihong Li, Remi Munos, and Csaba Szepesvari. Toward Minimax Off-policy Value Estimation.
In Guy Lebanon and S. V . N. Vishwanathan, editors, Proceedings of the Eighteenth International
Conference on Artificial Intelligence and Statistics , volume 38 of Proceedings of Machine
Learning Research , pages 608‚Äì616, San Diego, California, USA, 09‚Äì12 May 2015. PMLR.
URL https://proceedings.mlr.press/v38/li15b.html .
[35] Ben London and Ted Sandler. Bayesian counterfactual risk minimization. In International
Conference on Machine Learning , pages 4125‚Äì4133. PMLR, 2019.
[36] Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample variance
penalization. arXiv preprint arXiv:0907.3740 , 2009.
[37] David A. McAllester. Some PAC-Bayesian theorems. In Proceedings of the Eleventh Annual
Conference on Computational Learning Theory , COLT‚Äô 98, page 230‚Äì234, New York, NY , USA,
1998. Association for Computing Machinery. ISBN 1581130570. doi: 10.1145/279943.279989.
URL https://doi.org/10.1145/279943.279989 .
[38] Alberto Maria Metelli, Alessio Russo, and Marcello Restelli. Subgaussian and differentiable
importance sampling for off-policy evaluation and learning. Advances in Neural Information
Processing Systems , 34:8119‚Äì8132, 2021.
[39] Art B. Owen. Monte Carlo theory, methods and examples .https://artowen.su.domains/
mc/, 2013.
[40] Emilio Parrado-Hern√°ndez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-
Bayes Bounds with Data Dependent Priors. Journal of Machine Learning Research , 13(112):
3507‚Äì3531, 2012. URL http://jmlr.org/papers/v13/parrado12a.html .
[41] Jie Peng, Hao Zou, Jiashuo Liu, Shaoming Li, Yibao Jiang, Jian Pei, and Peng Cui. Offline
policy evaluation in large action spaces via outcome-oriented action grouping. In Proceedings
of the ACM Web Conference 2023 , pages 1220‚Äì1230, 2023.
[42] James M Robins and Andrea Rotnitzky. Semiparametric efficiency in multivariate regression
models with missing data. Journal of the American Statistical Association , 90(429):122‚Äì129,
1995.
[43] Noveen Sachdeva, Yi Su, and Thorsten Joachims. Off-policy bandits with deficient support. In
Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &
Data Mining , pages 965‚Äì975, 2020.
[44] Noveen Sachdeva, Lequn Wang, Dawen Liang, Nathan Kallus, and Julian McAuley. Off-
policy evaluation for large action spaces via policy convolution. In Proceedings of the ACM
Web Conference 2024 , WWW ‚Äô24, page 3576‚Äì3585, New York, NY , USA, 2024. Association
for Computing Machinery. ISBN 9798400701719. doi: 10.1145/3589334.3645501. URL
https://doi.org/10.1145/3589334.3645501 .
[45] Yuta Saito and Thorsten Joachims. Off-policy evaluation for large action spaces via embeddings.
InProceedings of the 39th International Conference on Machine Learning , volume 162 of
Proceedings of Machine Learning Research , pages 19089‚Äì19122. PMLR, 17‚Äì23 Jul 2022. URL
https://proceedings.mlr.press/v162/saito22a.html .
[46] Yuta Saito, Qingyang Ren, and Thorsten Joachims. Off-policy evaluation for large action
spaces via conjunct effect modeling. In international conference on Machine learning , pages
29734‚Äì29759. PMLR, 2023.
[47] Otmane Sakhi, Stephen Bonner, David Rohde, and Flavian Vasile. BLOB: A Probabilistic
model for recommendation that combines organic and bandit signals. In Proceedings of the
26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages
783‚Äì793, 2020.
13[48] Otmane Sakhi, Louis Faury, and Flavian Vasile. Improving Offline Contextual Bandits with
Distributional Robustness, 2020.
[49] Otmane Sakhi, Pierre Alquier, and Nicolas Chopin. PAC-Bayesian Offline Contextual Bandits
with Guarantees. In International Conference on Machine Learning , pages 29777‚Äì29799.
PMLR, 2023.
[50] Otmane Sakhi, David Rohde, and Nicolas Chopin. Fast Slate Policy Optimization: Going
Beyond Plackett-Luce. Transactions on Machine Learning Research , 2023. ISSN 2835-8856.
URL https://openreview.net/forum?id=f7a8XCRtUu .
[51] Otmane Sakhi, David Rohde, and Alexandre Gilotte. Fast Offline Policy Optimization for Large
Scale Recommendation. Proceedings of the AAAI Conference on Artificial Intelligence , 37
(8):9686‚Äì9694, Jun. 2023. doi: 10.1609/aaai.v37i8.26158. URL https://ojs.aaai.org/
index.php/AAAI/article/view/26158 .
[52] Yevgeny Seldin, Nicol√≤ Cesa-Bianchi, Peter Auer, Fran√ßois Laviolette, and John Shawe-Taylor.
PAC-Bayes-Bernstein Inequality for Martingales and its Application to Multiarmed Bandits. In
Dorota Glowacka, Louis Dorard, and John Shawe-Taylor, editors, Proceedings of the Workshop
on On-line Trading of Exploration and Exploitation 2 , volume 26 of Proceedings of Machine
Learning Research , pages 98‚Äì111, Bellevue, Washington, USA, 02 Jul 2012. PMLR. URL
https://proceedings.mlr.press/v26/seldin12a.html .
[53] Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for Sublinear Time Maximum
Inner Product Search (MIPS). In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q.
Weinberger, editors, Advances in Neural Information Processing Systems , volume 27. Curran
Associates, Inc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/
2014/file/310ce61c90f3a46e340ee8257bc70e93-Paper.pdf .
[54] Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, and Miroslav Dud√≠k. Doubly robust
off-policy evaluation with shrinkage. In International Conference on Machine Learning , pages
9167‚Äì9176. PMLR, 2020.
[55] Adith Swaminathan and Thorsten Joachims. Batch learning from logged bandit feedback
through counterfactual risk minimization. The Journal of Machine Learning Research , 16(1):
1731‚Äì1755, 2015.
[56] Adith Swaminathan and Thorsten Joachims. The self-normalized estimator for counterfactual
learning. advances in neural information processing systems , 28, 2015.
[57] Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford,
Damien Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation. Advances in
Neural Information Processing Systems , 30, 2017.
[58] Muhammad Faaiz Taufiq, Arnaud Doucet, Rob Cornish, and Jean-Francois Ton. Marginal
density ratio for off-policy evaluation in contextual bandits. Advances in Neural Information
Processing Systems , 36, 2024.
[59] Lequn Wang, Akshay Krishnamurthy, and Aleksandrs Slivkins. Oracle-efficient pessimism:
Offline policy optimization in contextual bandits. arXiv preprint arXiv:2306.07923 , 2023.
[60] Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudƒ±k. Optimal and adaptive off-policy evalua-
tion in contextual bandits. In International Conference on Machine Learning , pages 3589‚Äì3597.
PMLR, 2017.
[61] Ding-Xuan Zhou. The covering number in learning theory. J. Complex. , 18(3):739‚Äì767, sep
2002. ISSN 0885-064X. doi: 10.1006/jcom.2002.0635. URL https://doi.org/10.1006/
jcom.2002.0635 .
14Table of Contents for Supplementary Material
A Limitations 16
B Broader impact 16
C Extended related work 16
D Useful lemmas 18
E Additional results and discussions 19
E.1 Plots of the empirical moments bounds (Proposition 1) . . . . . . . . . . . . . . . 19
E.2 The study of Logarithmic Smoothing estimator and proofs . . . . . . . . . . . . . 19
E.3 OPS: Formal comparison with IX suboptimality . . . . . . . . . . . . . . . . . . . 23
E.4 OPL: Formal comparison of PAC-Bayesian bounds . . . . . . . . . . . . . . . . . 24
E.5 OPL: Formal comparison with IX PAC-Bayesian learning suboptimality . . . . . . 27
F Proofs of OPE 29
F.1 Proof of high order empirical moments bound (Proposition 1) . . . . . . . . . . . . 29
F.2 Proof of the impact of Lon the bound‚Äôs tightness (Proposition 2) . . . . . . . . . . 31
F.3 Comparisons of the bounds UŒª
L(Proposition 3) . . . . . . . . . . . . . . . . . . . 31
F.4 Proof of the optimality of global clipping for Corollary 4 . . . . . . . . . . . . . . 32
F.5 Comparison with empirical Bernstein . . . . . . . . . . . . . . . . . . . . . . . . 34
F.6 Proof of the L‚Üí ‚àû bound (Corollary 5) . . . . . . . . . . . . . . . . . . . . . . 35
F.7 Proof of the optimality of IPS for Corollary 5 . . . . . . . . . . . . . . . . . . . . 36
F.8 Comparison with the IX bound (Proposition 8) . . . . . . . . . . . . . . . . . . . 36
G Proofs of OPS and OPL 37
G.1 OPS: Proof of suboptimality bound (Proposition 9) . . . . . . . . . . . . . . . . . 37
G.2 OPL: Proof of PAC-Bayesian LS-LINbound (Proposition 10) . . . . . . . . . . . . 38
G.3 OPL: Proof of PAC-Bayesian suboptimality bound (Proposition 11) . . . . . . . . 39
H Experimental design and detailed experiments 40
H.1 Off-policy evaluation and selection . . . . . . . . . . . . . . . . . . . . . . . . . . 40
H.1.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
H.1.2 (OPE) Tightness of the bounds . . . . . . . . . . . . . . . . . . . . . . . . 40
H.1.3 (OPS) Find the best, avoid the worst policy . . . . . . . . . . . . . . . . . 41
H.2 Off-policy learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
H.2.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
H.2.2 Policy class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
H.2.3 Detailed hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . 42
H.2.4 Detailed results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
15A Limitations
This work develops theoretically grounded and practical pessimistic approaches for the offline
contextual bandit setting. Even if the proposed algorithms are general, and provably better than
competitors, they still suffer from the intrinsic limitations of importance weighting estimators.
Specifically, our method, as presented, will perform poorly in extremely large action spaces. However,
these limitations can be mitigated by incorporating additional structure as in Saito and Joachims
[45], Saito et al. [46]. Another limitation arises from the offline contextual bandit setting itself, which
assumes i.i.d. observations. While this assumption is valid in simple scenarios, it becomes unsuitable
once we want to capture the long term effect of interventions. Extending our results to the more
general, reinforcement learning setting would be an interesting research direction as it comes with a
challenging estimation task and will require developing new concentration bounds.
B Broader impact
Our work contributes to the development of theoretically grounded and practical pessimistic ap-
proaches for the offline contextual bandit setting. The derived algorithms can improve the robustness
of decision-making processes by prioritizing safety and minimizing uncertainty associated risks. By
leveraging pessimistic strategies, we ensure that decisions are made with a conservative bias, thereby
potentially improving outcomes in high-stakes environments where the cost of errors is substantial.
Although our framework and algorithms have broad, potentially good applications, their specific
social impacts will solely depend on the chosen application domain.
C Extended related work
Offline contextual bandits. Contextual bandit is a widely adopted framework for online learning
in uncertain environments [ 33]. However, some real-world applications present challenges for
existing online algorithms, and thus offline methods that leverage historical data to optimize decision-
making have gained traction [ 10]. Fortunately, large datasets summarizing past interactions are
often available, allowing agents to improve their policies offline [ 55]. Our work explores this
offline approach, known as offline (or off-policy) contextual bandits [ 16]. In this setting, off-policy
evaluation (OPE) estimates policy performance using historical data, mimicking real-time evaluations.
Depending on the application, the goal might be to find the best policy within a predefined finite set
(off-policy selection (OPS)) or the optimal policy overall (off-policy learning (OPL)).
Off-policy evaluation. In recent years, OPE has experienced a noticeable surge of interest, with
numerous significant contributions [ 16‚Äì18,60,19,54,38,32,45,47,26]. The literature on OPE
can be broadly classified into three primary approaches. The first, referred to as the direct method
(DM) [ 26,6], involves the development of a model designed to approximate expected costs for
any context-action pair. This model is subsequently employed to estimate the performance of the
policies. This approach is often designed for specific applications such as large-scale recommender
systems [ 47,26,4]. The second approach, known as inverse propensity scoring (IPS) [ 24,17], aims to
estimate the costs associated with the evaluated policies by correcting for the inherent preference bias
of the behavior policy within the dataset. While IPS maintains its unbiased nature when operating
under the assumption that the evaluation policy is absolutely continuous with respect to the behavior
policy, it can be susceptible to high variance and substantial bias when this assumption is violated
[43]. In response to the variance issue, various techniques have been introduced, including clipping
[25,10], shrinkage [ 54], power-mean correction [ 38], implicit exploration [ 21], self-normalization
[56], among others [ 22]. The third approach, known as doubly robust (DR) [ 42,9,16,18,19],
combines elements from both the direct method (DM) and inverse propensity scoring (IPS). This
work focuses on regularized IPS.
Off-policy selection and learning. as in OPE, three key approaches dominate: DM, IPS and DR
in OPS and OPL. In OPS, all these methods share the same core objective: identifying the policy
with the highest estimated reward from a finite set of candidates. However, they differ in their
reward estimation techniques, as discussed in the OPE section above. In contrast, in OPL, DM either
deterministically selects the action with the highest estimated reward or constructs a distribution
based on these estimates. IPS and DR, on the other hand, employ gradient descent for policy learning
[55], updating a parameterized policy denoted by œÄŒ∏asŒ∏t+1‚ÜêŒ∏t‚àí ‚àá Œ∏R(œÄŒ∏)for each iteration t.
16Since the true risk Ris unknown, ‚àáŒ∏R(œÄŒ∏)is unknown and needs to be estimated using techniques
like IPS or DR.
Pessimism in offline contextual bandits. Most OPE studies directly use their point estimators of
the risk in OPE, OPS and OPL. However, point estimators can deviate from the true value of the
risk, rendering them unreliable for decision-making. Therefore, and to increase safety, alternative
approaches focus on constructing bounds on the risk. These bounds, either asymptotic [ 10,48,15] or
finite sample [ 32,21], aim to evaluate a policy‚Äôs worst-case performance, adhering to the principle
ofpessimism in face of uncertainty [27]. The principle of pessimism transcends OPE, influencing
both OPS and OPL. In these domains, strategies are predominantly inspired by, or directly derived
from, upper bounds on the true risk [ 55,35,32,49,5,59]. Consider OPS: [ 32] leveraged an Efron-
Stein bound for the self-normalized IPS estimator, while [ 21] anchored their analysis on a bound
constructed with the Implicit Exploration estimator. Shifting focus to OPL, [ 55] combined the
empirical Bernstein bound [ 36] with the clipping estimator, motivating sample variance penalization
for policy learning. Recent advancements include modifications to the penalization term [ 59] to be
scalable and efficient.
PAC-Bayes extension. The PAC-Bayesian paradigm [ 37,11] (see Alquier [2]for a recent intro-
duction) provides a rich set of tools to prove generalization bounds for different statistical learning
problems. The classical (online) contextual bandit problem received a lot of attention from the
PAC-Bayesian community with the seminal work of Seldin et al. [52]. It is just recently that these
tools were adapted to the offline contextual bandit setting, with [ 35] that introduced a clean and
scalable PAC-Bayesian perspective to OPL. This perspective was further explored by [ 20,49,5,7,21],
leading to the development of tight, tractable PAC-Bayesian bounds suitable for direct optimization.
Large action space extension. While regularization techniques can improve IPS properties, they
often fall short when dealing with extremely large action spaces. Additional assumptions regarding
the structure of the contextual bandit problem become necessary. For example, Saito and Joachims
[45] introduced the Marginalized IPS (MIPS) framework and estimator. MIPS leverages auxiliary
information about the actions in the form of action embeddings. Roughly speaking, MIPS assumes
access to embeddings eiwithin logged data and defines the risk estimator as
ÀÜRMIPS
n(œÄ) =1
nnX
i=1œÄ(ei|xi)
œÄ0(ei|xi)ci=1
nnX
i=1w(xi, ei)ci,
where the logged data Dn={(xi, ai, ei, ri)}n
i=1now includes action embeddings for each data point.
The marginal importance weight
w(x, e) =œÄ(e|x)
œÄ0(e|x)=P
ap(e|x, a)œÄ(a|x)P
ap(e|x, a)œÄ0(a|x)
is a key component of this approach. Compared to IPS and DR, MIPS achieves significantly lower
variance in large action spaces [ 45] while maintaining unbiasedness if the action embeddings directly
influence costs c. This necessitates informative embeddings that capture the causal effects of actions
on costs. However, high-dimensional embeddings can still lead to high variance for MIPS, similar
to IPS. Additionally, high bias can arise if the direct effect assumption is violated and embeddings
fail to capture these causal effects. This bias is particularly present when performing action feature
selection for dimensionality reduction. Recent work proposes learning such embeddings directly
from logged data [ 41,44,14], or loosen this assumption [ 58,46]. Our proposed importance weight
regularization can be potentially combined with these estimators under their respective assumptions
on the underlying structure of the contextual bandit problem, extending our approach to large action
spaces, and we posit that this will be beneficial when, for example, the action embedding dimension
is high. Another line of research in large action spaces is more interested with the learning problem,
precisely solving the optimization issues arising from policies defined on large action spaces. Indeed,
naive optimization tends to be slow and scales linearly with the number of actions K[12]. Recent
work [ 51,50] solve this by leveraging fast maximum inner product search [ 53,3] in the training loop,
reducing the optimization complexity to logarithmic in the action space size. These methods however
require a linear objective on the target policy. Luckily, our PAC-Bayesian learning objective is linear
in the policy and its optimization is amenable to such acceleration.
Continuous action space extension. While research has predominantly focused on discrete action
spaces, a limited number of studies have tackled the continuous case [ 29,13,59]. For example, [ 29]
17explored non-parametric evaluation and learning of continuous action policies using kernel smoothing,
while [ 13] investigated the semi-parametric setting. Recently, [ 59] leveraged the smoothing approach
from [ 31] to extend their discrete OPL method to continuous actions. Our work can either use the
densities directly, or be similarly extended to continuous actions through a well-defined discretization
of the space. Imagine a scenario with infinitely many actions, where policies are defined by density
functions. For any context x,œÄ(a|x)represents the density function that maps actions ato probabil-
ities. The discretization process transforms the original contextual bandit problem characterized by
the density-based policy class Œ†into an OPL problem defined by a discrete, mass-based policy class
Œ†K(for a finite number of actions K). Each policy within Œ†Kapproximates a policy in Œ†through a
smoothing process.
D Useful lemmas
In the following, and for any quantity Z, all expectations are computed w.r.t to the distribution of the
data when playing actions under the behaviour policy œÄ0, as in:
E[Z] =Ex‚àºŒΩ,a‚àºœÄ0(¬∑|x),c‚àºp(¬∑|x,a)[Z].
A lot of the results derived in the paper are based on the use of the well known Chernoff Inequality,
that we state below for a sum of i.i.d. random variables:
Lemma 12 (Chernoff Inequality for a sum of i.i.d. random variables.) .Leta‚àà R,n‚àà N‚àó
and{Xi, i‚àà[n]}a collection of ni.i.d. random variables. The following concentration
bounds on the right tail ofP
i‚àà[n]Xihold for any Œª‚â•0:
PÔ£´
Ô£≠X
i‚àà[n]Xi> aÔ£∂
Ô£∏‚â§( E[exp ( ŒªX1)])nexp(‚àíŒªa)
This result is classical in the literature [ 23] and we omit its proof. We will also need the following
lemma, that states the monotonous nature of a key function in our analysis, and that we take the time
to prove.
Lemma 13. LetL‚â•1andfLbe the following function:
fL(x) =log(1 + x)‚àíPL
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñìx‚Ñì
(‚àí1)LxL+1.
We have that fLis a decreasing function in R+for all L‚àà N‚àó.
Proof. LetL‚â•1andfLbe the following function:
fL(x) =log(1 + x)‚àíPL
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñìx‚Ñì
(‚àí1)LxL+1.
Letx‚àà R+, we have the following identity holding ‚àÄt >0and‚àÄn‚â•0:
1 + (‚àí1)ntn+1
1 +t=nX
k=0(‚àí1)ktk‚áê‚áí1
1 +t=nX
k=0(‚àí1)ktk+(‚àí1)n+1tn+1
1 +t. (29)
Recall the integral form of the log function:
log(1 + x) =Zx
01
1 +tdt.
We integrate both sides of the Equality (29) and show that the numerator of fL(x)is equal to:
log(1 + x)‚àíKX
k=1(‚àí1)k‚àí1
kxk= (‚àí1)KZx
0tK
1 +tdt.
18This result enables us to rewrite the function fLas:
fL(x) =1
xL+1Zx
0tL
1 +tdt.
Using the change of variable t=ux, we obtain:
fL(x) =Z1
0uL
1 +xudt
which is clearly decreasing for in R+. This ends the proof.
Finally, we also state the important change of measure lemma:
Lemma 14 (Change of measure) .Letgbe a function of the parameter Œ∏and data Dn, for
any distribution Q that is P continuous, for any Œ¥‚àà(0,1], we have with probability 1‚àíŒ¥:
EŒ∏‚àºQ[g(Œ∏,Dn)]‚â§ KL (Q||P) + lnŒ®g
Œ¥(30)
withŒ®g= EDn EŒ∏‚àºP[eg(Œ∏,Dn)].
Lemma 14 is the backbone of a multitude of PAC-Bayesian bounds. It is proven in many references,
see for example [ 2] or Lemma 1.1.3 in [ 11]. With this result, the recipe of constructing a generalization
bound reduces to choosing an adequate function gfor which we can control Œ®g.
E Additional results and discussions
E.1 Plots of the empirical moments bounds (Proposition 1)
For any œÄ‚ààŒ†, letUŒª,h
L(œÄ)be the upper bound of Proposition 1:
UŒª,h
L(œÄ) =œàŒª 
ÀÜRh
n(œÄ) +ln(1/Œ¥)
Œªn+2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ)!
.
One can observe that the bound UŒª,h
Ldepends on three parameters, the regularized IPS function h,
the free parameter Œªand the moment order L. We choose a dataset (balance-scale) with n= 612 , and
evaluate a policy œÄwithR(œÄ) =‚àí0.93to evaluate our bound for different parameters. We fix Œª=p
1/nand plot the value of UŒª,h
Lfor different values of the moment order L‚àà {1,2,3,4,6,8,‚àû}
and for 4 different regularization functions, namely IPS, clipped IPS ( M=‚àön), Implicit Exploration
(IX) ( Œª=p
1/n) and Exponential Smoothing (ES) ( Œ±= 1‚àíp
1/n). The results are shown in
Figure 3. One can observe from the plot that The decreasing nature of UŒª,h
Ldepends on Œªand the
regularization function h. Indeed, Proposition 2 states that Œª < mini‚àà[n]1/|hi|implies that the
bound is decreasing w.r.t L. Which means that once this condition is not verified, we do not know
if the bound will keep decreasing with L. If the bound seems decreasing for CIPS and IX, One
can observe that for both IPS and ES, the bound increased from L= 4toL= 8, but achieved its
minimum at L=‚àû, with IPS being optimal for this value. This highlights the connection between
L, the value of Œªand the regularizer h.
E.2 The study of Logarithmic Smoothing estimator and proofs
Recall the form of the Logarithmic Smoothing estimator, defined for any Œª‚â•0:
ÀÜRŒª
n(œÄ) =‚àí1
nnX
i=11
Œªlog (1‚àíŒªwœÄ(xi, ai)ci). (31)
Our estimator ÀÜRŒª
n(œÄ), is defined for a non-negative Œª‚â•0. In particular, Œª= 0recovers the unbiased
IPS estimator in (2)andŒª >0introduces a bias variance trade-off. This estimator can be interpreted
191 2 3 4 6 8+1
Values of L‚àí0.72‚àí0.68‚àí0.64‚àí0.60‚àí0.56‚àí0.52U¬∏;h
L(¬º);¬∏=1=p
nR(¬º) = -0.93, n = 612
IPS
CIPS
IX
ESFigure 3: Proposition 1 for different values of Land with different regularized IPS h.
as Logarithmic Soft Clipping, and have a similar behavior than Clipping of Bottou et al. [10]. Indeed,
1/Œªplays a similar role to the clipping parameter M, as for any i‚àà[n], we have:
wœÄ(xi, ai)ci‚â™1
Œª=‚áí ‚àí1
Œªlog (1‚àíŒªwœÄ(xi, ai)ci)‚âàwœÄ(xi, ai)ci.
wœÄ(xi, ai)ci< M =‚áímin ( wœÄ(xi, ai), M)ci=wœÄ(xi, ai)ci.
LScan be seen as a smooth, differentiable version of clipping. We plot the graph of the two functions
in Figure 4. One can observe that once Œª >0,LSexhibits a bias-variance trade-off, with a declining
bias with Œª‚Üí0. This is different than Clipping as no bias is suffered once Mis bigger than
the support of wœÄ, this comes however with the price of suffering the full variance of IPS. In the
following, we study the bias-variance trade-off that emerges with the new Logarithmic Smoothing
estimator.
0 10 20 30 40 50
Importance Weight w¬ºIPS
Clipping, M = 20
LS, ¬∏ = 0.1
LS, ¬∏ = 0.05
LS, ¬∏ = 0.01
Figure 4: Comparison of Logarithmic Smoothing and Clipping.
We begin by defining the bias and variance of ÀÜRŒª
n(œÄ):
BŒª(œÄ) =Eh
ÀÜRŒª
n(œÄ)i
‚àíR(œÄ), VŒª(œÄ) =E
ÀÜRŒª
n(œÄ)‚àíEh
ÀÜRŒª
n(œÄ)i2
. (32)
Moreover, for any Œª‚â•0, we define the following quantity
SŒª(œÄ) =EwœÄ(x, a)2c2
1‚àíŒªwœÄ(x, a)c
, (33)
that will be essential in studying the properties of this estimator akin to the coverage ratio used for
the IX-estimator [ 21]. In the following, we study the properties of our estimator ÀÜRŒª
n(œÄ)in(14). We
start with bounding its mean squared error (MSE), which involves bounding its bias and variance.
20Proposition (Bias-variance trade-off) .LetœÄ‚ààŒ†andŒª‚â•0. Then we have that
0‚â§ BŒª(œÄ)‚â§ŒªSŒª(œÄ),andVŒª(œÄ)‚â§ SŒª(œÄ)/n .
Moreover, it holds that for any Œª >0:
VŒª(œÄ)‚â§|R(œÄ)|
nŒª‚â§1
nŒª.
Proof. Let us start with bounding the bias. We have for any Œª‚â•0:
BŒª(œÄ) =Eh
ÀÜRŒª
n(œÄ)i
‚àíR(œÄ)
=E
‚àí1
Œªlog(1‚àíŒªwœÄ(x, a)c)‚àíwœÄ(x, a)c
(IPS is unbiased ).
Using log(1 + x)‚â§xfor any x‚â•0proves that the bias is positive. For its upper bound, we use the
following inequality log(1 + x)‚â•x
1+xholding for x‚â•0:
BŒª(œÄ) =E
‚àí1
Œªlog(1‚àíŒªwœÄ(x, a)c)‚àíwœÄ(x, a)c
‚â§EwœÄ(x, a)c
1‚àíŒªwœÄ(x, a)c‚àíwœÄ(x, a)c
=ŒªE(wœÄ(x, a)c)2
1‚àíŒªwœÄ(x, a)c
=ŒªSŒª(œÄ).
Now focusing on the variance, we have:
VŒª(œÄ) =E
ÀÜRŒª
n(œÄ)‚àíEh
ÀÜRŒª
n(œÄ)i2
‚â§1
nŒª2E
log(1‚àíŒªwœÄ(x, a)c)2
.
We use the following inequality log(1 + x)‚â§x/‚àöx+ 1holding for x‚â•0to obtain our result:
VŒª(œÄ)‚â§1
nSŒª(œÄ).
Notice that once Œª >0, we have:
SŒª(œÄ) =EwœÄ(x, a)2c2
1‚àíŒªwœÄ(x, a)c
‚â§1
ŒªE[wœÄ(x, a)|c|] =|R(œÄ)|
Œª,
resulting in a finite variance whenever Œª >0:
VŒª(œÄ)‚â§|R(œÄ)|
nŒª‚â§1
nŒª.
Œª= 0recovers the IPS estimator in (2), with zero bias and variance bounded by E
w2(x, a)c2
/n.
When Œª >0, a bias-variance trade-off emerges. The bias is always non-negative as we still recover
an estimator that verifies (C1) . The bias is capped at ŒªSŒª(œÄ), which diminishes to zero when Œªis
small and goes to |R(œÄ)|asŒªincreases. Conversely, the variance decreases with a higher Œª. Notably,
Œª >0ensures finite variance bounded by 1/Œªn, despite the estimator being unbounded. This is
different from previous regularizations that relied on bounded functions to ensure finite variance.
While prior evaluations of estimators often relied on bias and variance analysis, Metelli et al. [38]
argued for studying the non-asymptotic concentration rate of the estimators, advocating for sub-
Gaussianity as a desired property. Even if our estimator is not bounded, we prove in the following
that it is sub-Gaussian.
21Proposition (Sub-Gaussianity) .LetœÄ‚ààŒ†,Œ¥‚àà(0,1]andŒª > 0. Then the following
inequalities holds with probability at least 1‚àíŒ¥:
R(œÄ)‚àíÀÜRŒª
n(œÄ)‚â§ln(2/Œ¥)
Œªn, and ÀÜRŒª
n(œÄ)‚àíR(œÄ)‚â§ŒªSŒª(œÄ) +ln(2/Œ¥)
Œªn.
In particular, setting Œª=Œª‚àó=p
ln(2/Œ¥)/nE[wœÄ(x, a)2c2]yields that
|R(œÄ)‚àíÀÜRŒª‚àó
n(œÄ)| ‚â§p
2œÉ2ln(2/Œ¥), where œÉ2= 2E
wœÄ(x, a)2c2
/n . (34)
Proof. LetœÄ‚ààŒ†,Œª >0andŒ¥ >0. To prove sub-Gaussianity, we need both upper bounds and
lower bounds on R(œÄ)using ÀÜRŒª
n(œÄ). For the upper bound, we can use the bound of Corollary 5, and
recall that œàŒª(x)‚â§xfor all x. We then obtain with a probability 1‚àíŒ¥:
R(œÄ)‚â§œàŒª
ÀÜRŒª
n(œÄ) +ln(1/Œ¥)
Œªn
=‚áíR(œÄ)‚àíÀÜRŒª
n(œÄ)‚â§ln(1/Œ¥)
Œªn.
For the lower bound on the risk, we go back to our Chernoff Lemma 12, and use the collection of
i.i.d. random variable, that for any i‚àà[n], are defined as:
¬ØXi=‚àí1
Œªlog (1‚àíŒªwœÄ(xi, ai)ci).
This gives for a‚àà R:
PÔ£´
Ô£≠X
i‚àà[n]¬ØXi> aÔ£∂
Ô£∏‚â§ 
E
exp 
Œª¬ØX1nexp(‚àíŒªa)
PÔ£´
Ô£≠X
i‚àà[n]¬ØXi> aÔ£∂
Ô£∏‚â§
E1
1‚àíŒªwœÄ(x, a)cn
exp(‚àíŒªa)
Solving for Œ¥=
Eh
1
1‚àíŒªwœÄ(x,a)cin
exp(‚àíŒªa), we get:
PÔ£´
Ô£≠1
nX
i‚àà[n]¬ØXi>1
Œªlog
E1
1‚àíŒªwœÄ(x, a)c
+ln(1/Œ¥)
ŒªnÔ£∂
Ô£∏‚â§Œ¥
The complementary event holds with at least probability 1‚àíŒ¥:
ÀÜRŒª
n(œÄ)‚â§1
Œªlog
E1
1‚àíŒªwœÄ(x, a)c
+ln(1/Œ¥)
Œªn,
which implies using the inequality log(x)‚â§x‚àí1for all x >0:
ÀÜRŒª
n(œÄ)‚àíR(œÄ)‚â§1
Œªlog
E1
1‚àíŒªwœÄ(x, a)c
‚àíR(œÄ) +ln(1/Œ¥)
Œªn
‚â§1
Œª
E1
1‚àíŒªwœÄ(x, a)c
‚àí1
‚àíR(œÄ) +ln(1/Œ¥)
Œªn
‚â§ EwœÄ(x, a)c
1‚àíŒªwœÄ(x, a)c‚àíwœÄ(x, a)c
+ln(1/Œ¥)
Œªn
‚â§Œª EwœÄ(x, a)2c2
1‚àíŒªwœÄ(x, a)c
+ln(1/Œ¥)
Œªn=ŒªSŒª(œÄ) +ln(1/Œ¥)
Œªn,
which proves the lower bound on the risk. As both results hold with high probability, we use a union
argument to have them both holding for probability at least 1‚àíŒ¥:
R(œÄ)‚àíÀÜRŒª
n(œÄ)‚â§ln(2/Œ¥)
Œªn, and ÀÜRŒª
n(œÄ)‚àíR(œÄ)‚â§ŒªSŒª(œÄ) +ln(2/Œ¥)
Œªn,
22which implies that:
|R(œÄ)‚àíÀÜRŒª
n(œÄ)| ‚â§ŒªSŒª(œÄ) +ln(2/Œ¥)
Œªn‚â§ŒªE
wœÄ(x, a)2c2
+ln(2/Œ¥)
Œªn.
This means that setting Œª=Œª‚àó=p
ln(2/Œ¥)/nE[wœÄ(x, a)2c2]yields a sub-Gaussian concentration:
|R(œÄ)‚àíÀÜRŒª‚àó
n(œÄ)| ‚â§2r
E[wœÄ(x, a)2c2] ln(2/Œ¥)
n.
This ends the proof.
From (34),ÀÜRŒª‚àón(œÄ)is sub-Gaussian with variance proxy œÉ2= 2E
œâ(x, a)2c2
/n, which is lower
that the variance proxy of the Harmonic estimator of Metelli et al. [38]. Indeed, the Harmonic
estimator has a slightly worse variance proxy of œÉ2
H=(2+‚àö
3)2
3E
œâ(x, a)2c2
/n, giving œÉ2< œÉ2
H.
E.3 OPS: Formal comparison with IX suboptimality
Let us begin by stating results from the IX work [ 21]. Recall that the IX estimator is defined for any
Œª >0, by:
ÀÜRŒª-IX
n(œÄ) =1
nnX
i=1œÄ(ai|xi)
œÄ0(ai|xi) +Œª/2ci.
LetŒ†S={œÄ1, ..., œÄ m}be a finite set of predefined policies. In OPS, the goal is to find œÄS
‚àó‚ààŒ†Sthat
satisfies
œÄS
‚àó= argminœÄ‚ààŒ†SR(œÄ) = argmink‚àà[m]R(œÄk).
forŒª >0, the selection strategy suggested in Gabbianelli et al. [21] was to search for:
ÀÜœÄS,IX
n= argmin
œÄ‚ààŒ†SÀÜRŒª-IX
n(œÄ) = argmin
k‚àà[m]ÀÜRŒª-IX
n(œÄ). (35)
Proposition 15 (Suboptimality of the IX selection strategy) .LetŒª >0andŒ¥‚àà(0,1]. Then,
it holds with probability at least 1‚àíŒ¥that
0‚â§R(ÀÜœÄS,IX
n)‚àíR(œÄS
‚àó)‚â§ŒªCŒª/2(œÄS
‚àó) +2 ln(2|Œ†S|/Œ¥)
Œªn, (36)
where
CŒª(œÄ) =EœÄ(a|x)
œÄ2
0(a|x) +ŒªœÄ0(a|x)|c|
.
Both suboptimalities ( LSand IX) have the same form, they only depend on two different quantities
(SŒªandCŒªrespectively). For a œÄ‚ààŒ†andŒª >0, If we can identify when SŒª(œÄ)‚â§ CŒª/2(œÄ), then
we can prove that the sub-optimality of LSselection strategy is better than the one of IX. Luckily, this
is always the case, and it is stated formally below.
Proposition 16. LetœÄ‚ààŒ†andŒª >0. We have:
SŒª(œÄ)‚â§ CŒª/2(œÄ). (37)
23Proof. LetœÄ‚ààŒ†andŒª >0, we have:
CŒª/2(œÄ)‚àí SŒª(œÄ) =E"
œÄ(a|x)
œÄ2
0(a|x) +Œª
2œÄ0(a|x)|c| ‚àíwœÄ(x, a)2c2
1‚àíŒªwœÄ(x, a)c#
=E"
œÄ(a|x)
œÄ2
0(a|x) +Œª
2œÄ0(a|x)|c| ‚àíœÄ(a|x)2c2
œÄ2
0(a|x)‚àíŒªœÄ0(a|x)œÄ(a|x)c#
=E"
œÄ(a|x)|c| 
1
œÄ2
0(a|x) +Œª
2œÄ0(a|x)‚àíœÄ(a|x)|c|
œÄ2
0(a|x) +ŒªœÄ0(a|x)œÄ(a|x)|c|!#
=E"
œÄ(a|x)|c| 
œÄ2
0(a|x) (1‚àíœÄ(a|x)|c|) +Œª
2œÄ0(a|x)œÄ(a|x)|c|
(œÄ2
0(a|x) +Œª
2œÄ0(a|x))(œÄ2
0(a|x) +ŒªœÄ0(a|x)œÄ(a|x)|c|)!#
‚â•0.
This means that the suboptimality of LSselection strategy is better bounded than the one of IX. Our
experiments confirm that the LSselection strategy is better than IX in practical scenarios.
Minimax optimality of our selection strategy. As discussed in Gabbianelli et al. [21], pessimistic
algorithms tend to have the property that their regret scales with the minimax sample complexity
of estimating the value of the optimal policy [ 27]. For the case of multi-armed bandit (one con-
textx), this estimation minimax sample complexity is proved by Li et al. [34] and is of the rate
O(E[wœÄ‚àó(x, a)2c2]), with œÄ‚àóbeing the optimal policy. Our bound matches the lower bound proved
by Li et al. [34], as:
SŒª(œÄ‚àó) =EwœÄ‚àó(x, a)2c2
1‚àíŒªwœÄ‚àó(x, a)c
‚â§E
wœÄ‚àó(x, a)2c2
,
which is not the case for the suboptimality of IX, that only matches it in the deterministic setting with
binary costs, as:
CŒª(œÄ‚àó) =EœÄ‚àó(a|x)
œÄ2
0(a|x) +ŒªœÄ0(a|x)|c|
‚â§EœÄ‚àó(a|x)
œÄ2
0(a|x)|c|
=E"œÄ‚àó(a|x)
œÄ0(a|x)2
c2#
,
with the last inequality only holding when œÄ‚àóis deterministic and the costs are binary. For deter-
ministic policies and the general contextual bandit, we invite the reader to see a formal proof of the
minimax lower bound of pessimism in Jin et al. [28, Theorem 4.4], matched for both IX and LS.
E.4 OPL: Formal comparison of PAC-Bayesian bounds
As it is easier to work with linear estimators within the PAC-Bayesian framework, we define the
following estimator of the risk ÀÜRp‚àíLIN
n (œÄ), with the help of a function p: R‚Üí Ras:
ÀÜRp‚àíLIN
n (œÄ) =1
nnX
i=1œÄ(ai|xi)
p(œÄ0(ai|xi))ci
with the only condition on pto be{CLIN
1:‚àÄx, p(x)‚â•x}. This condition helps us control the
impact of actions with low probabilities under œÄ0. This risk estimator encompasses well known risk
estimators depending on the choice of p.
Now that we defined the family of estimators covered by our analysis, we attack the problem of
deriving generalization bounds. We derive our empirical high order bound expressed in the following:
24Proposition 17 (Empirical High Order PAC-Bayes bound) .LetL‚â•1. Given a prior Pon
FŒò,Œ¥‚àà(0,1]andŒª >0, the following bound holds with probability at least 1‚àíŒ¥uniformly
for all distribution QoverFŒò:
R(œÄQ)‚â§œàŒª 
ÀÜRp‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn+2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMp‚àíLIN,‚Ñì
n (œÄQ)!
(38)
with:
ÀÜMp‚àíLIN,‚Ñì
n (œÄQ) =1
nnX
i=1œÄQ(ai|xi)
p(œÄ0(ai|xi))‚Ñìc‚Ñì
i
œàŒª=x:‚Üí1‚àíexp(‚àíŒªx)
Œª.
Proof. LetL‚â•1, we have from Lemma 13, and for any positive random variable X‚â•0andŒª >0:
f2L‚àí1(0) =1
2L‚â•f2L‚àí1(ŒªX) =‚àílog(1 + ŒªX)‚àíP2L‚àí1
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì
(ŒªX)2L
which is equivalent to:
2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì‚â§log(1 + ŒªX)‚áê‚áí exp 2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!
‚â§1 +ŒªX
=‚áí E"
exp 2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!#
‚â§1 + E[ŒªX]
=‚áí E"
exp 2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!#
‚â§exp (log(1 + E[ŒªX])),
which implies that:
E"
exp 
Œª(X‚àí1
Œªlog(1 + E[ŒªX])) +2LX
‚Ñì=2(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!#
‚â§1.
For any X‚â§0, we can inject ‚àíX‚â•0to obtain:
‚àÄX‚â§0, E"
exp 
Œª
‚àí1
Œªlog(1 + E[ŒªX])‚àíX
‚àí2KX
k=21
k(ŒªX)k!#
‚â§1. (39)
Let:
dŒ∏(a|x) = 1[fŒ∏(x) =a],‚àÄ(x, a)‚àà X √ó A ,
it means that:
œÄQ(a|x) =EŒ∏‚àºQ[dŒ∏(a|x)],‚àÄ(x, a)‚àà X √ó A .
LetŒª >0. The adequate function gwe are going to use in combination with Lemma 14 is:
g(Œ∏,Dn) =nX
i=1Œª
‚àí1
Œªlog(1 + ŒªRp‚àíLIN(dŒ∏))‚àídŒ∏(ai|xi)
p(œÄ0(ai|xi))ci
‚àí2LX
‚Ñì=21
‚Ñì
ŒªdŒ∏(ai|xi)
p(œÄ0(ai|xi))ci‚Ñì
=nX
i=1Œª
‚àí1
Œªlog(1 + ŒªRp‚àíLIN(dŒ∏))‚àídŒ∏(ai|xi)
p(œÄ0(ai|xi))ci
‚àí2LX
‚Ñì=2dŒ∏(ai|xi)
‚ÑìŒª
p(œÄ0(ai|xi))ci‚Ñì
.
25By exploiting the i.i.d. nature of the data and exchanging the order of expectations ( Pis independent
ofDn), we can naturally prove using (39) that:
Œ®g= EP"nY
i=1E"
exp 
Œª
‚àí1
Œªlog(1 + ŒªRp‚àíLIN(dŒ∏))‚àíXi(Œ∏)
‚àí2KX
k=21
k(ŒªXi(Œ∏))k!##
‚â§1,
as we have :
Xi(Œ∏) =dŒ∏(ai|xi)
p(œÄ0(ai|xi))ci‚â§0‚àÄi.
Injecting Œ®gin Lemma 14, rearranging terms and using that ÀÜRp‚àíLIN
n (œÄ)has positive bias concludes
the proof.
Similarly to the OPE section, we use this general bound to obtain a PAC-Bayesian Empirical Second
Moment bound and the PAC-Bayesian LS-LINbound. That we state directly below:
Empirical second moment bound. With L= 1, we obtain the following:
Corollary 18 (Second Moment Upper bound) .Given a prior PonFŒò,Œ¥‚àà(0,1]andŒª >0.
The following bound holds with probability at least 1‚àíŒ¥uniformly for all distribution Q
overFŒò:
R(œÄQ)‚â§œàŒª
ÀÜRp
n(œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn+Œª
2ÀÜMp‚àíLIN,2
n (œÄQ)
. (40)
Log Smoothing PAC-Bayesian Bound. With L‚Üí ‚àû , we obtain the following:
Proposition 19 (ÀÜRŒª‚àíLIN
n PAC-Bayes bound) .Given a prior PonFŒò,Œ¥‚àà(0,1]andŒª >0,
the following bound holds with probability at least 1‚àíŒ¥uniformly for all distribution Qover
FŒò:
R(œÄQ)‚â§œàŒª
ÀÜRŒª‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn
. (41)
with:
ÀÜRŒª-LIN
n(œÄ) =‚àí1
nnX
i=1œÄ(ai|xi)
Œªlog
1‚àíŒªci
œÄ0(ai|xi)
.
Following the same proof schema as of the OPE section, we can demonstrate that the Log Smoothing
PAC-Bayesian bound dominates the Empirical Second moment PAC-Bayesian bound L= 1. How-
ever, we use the bound of L= 1as an intermediary to state the dominance of the Log Smoothing
PAC-Bayesian bound.
Indeed, we can easily compare the result obtained with L= 1to previously derived PAC-Bayesian
bounds for off-policy learning. We start by writing down the conditional Bernstein bound of Sakhi
et al. [49] holding for the (linear) cIPS (p:x‚Üímax( x, œÑ)). For a policy œÄQand a Œª >0, we have:
R(œÄQ)‚â§ÀÜRœÑ
n(œÄQ) +s
KL(Q||P) + ln4‚àön
Œ¥
2n+KL(Q||P) + ln2
Œ¥
Œªn+Œªg(Œª/œÑ)VœÑ
n(œÄQ).
R(œÄQ)‚â§ÀÜRœÑ
n(œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn+Œª
2ÀÜSœÑ
n(œÄQ). (L = 1 )
We can observe that the previously derived conditional Bernstein bound has several terms that make
it less tight:
‚Ä¢ It has an additional, strictly positive square root KL divergence term.
26‚Ä¢ The multiplicative factor g(Œª/œÑ)is always bigger than 1/2, and diverges when œÑ‚Üí0.
‚Ä¢ With enough data ( n‚â´1), we also have:
ÀÜSœÑ
n(œÄQ)‚âà EœÄQ(a|x)
max{œÄ0(a|x), œÑ}2c(a, x)2
‚â§ EœÄQ(a|x)
max{œÄ0(a|x), œÑ}2
‚âà VœÑ
n(œÄQ).
These observations confirm that the new bound derived with L= 1is tighter than what was previously
proposed for cIPS , especially when n‚â´1. As our bound can work for other estimators, we also
compare it to a recently proposed PAC-Bayes bound in Aouali et al. [5]for the exponentially-smoothed
estimator ( p:x‚ÜíxŒ±) with Œ±‚àà[0,1]:
R(œÄQ)‚â§ÀÜRŒ±
n(œÄQ) +s
KL(Q||P) + ln4‚àön
Œ¥
2n+KL(Q||P) + ln2
Œ¥
Œªn+Œª
2
VŒ±
n(œÄQ) +ÀÜSŒ±
n(œÄQ)
.
R(œÄQ)‚â§ÀÜRŒ±
n(œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn+Œª
2ÀÜSŒ±
n(œÄQ). (L = 1 )
We can clearly see that the previously proposed bound for the exponentially smoothed estimator has
two additional positive quantities that makes it less tight than our bound. In addition, computing our
bound does not rely on expectations under œÄ0(contrary to the previous bounds that have Vn) which
alleviates the need to access the logging policy and reduce the computations.
This demonstrates the superiority of L= 1compared to existing variance sensitive PAC-Bayesian
bounds. It means that L‚Üí ‚àû is even better. We can also prove that the Log smoothing PAC-Bayesian
Bound is better than the one of IX in Gabbianelli et al. [21]. Indeed, using log(1 + x)‚â•x
1+x/2for
allx‚â•0, we have for any P, Q‚àà P(Œò)andŒª >0:
œàŒª
ÀÜRŒª‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn
‚â§ÀÜRŒª‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn
‚â§ ‚àí1
nnX
i=1œÄQ(ai|xi)
Œªlog
1‚àíŒªci
œÄ0(ai|xi)
+KL(Q||P) + ln1
Œ¥
Œªn
‚â§1
nnX
i=1œÄQ(ai|xi)
œÄ0(ai|xi)‚àíŒªci/2+KL(Q||P) + ln1
Œ¥
Œªn
‚â§ÀÜRŒª‚àíIX
n(œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn, (IX-bound )
with the last implication leveraging that the cost is always bigger than ‚àí1/ This proves that our bound
is better than the IX bound. This means that our PAC-Bayesian bound is better than all existing
PAC-Bayesian off-policy learning bounds.
E.5 OPL: Formal comparison with IX PAC-Bayesian learning suboptimality
Let us begin by stating results from the IX work [ 21]. Recall that the IX estimator is defined for any
Œª >0, by:
ÀÜRIX‚àíŒª
n(œÄ) =1
nnX
i=1œÄ(ai|xi)
œÄ0(ai|xi) +Œª/2ci,
and that we used the linearized version of the LSestimator, LS-LINdefined as:
ÀÜRŒª-LIN
n(œÄ) =‚àí1
nnX
i=1œÄ(ai|xi)
Œªlog
1‚àíŒªci
œÄ0(ai|xi)
.
LetŒòbe a parameter space and P(Œò)be the set of all probability distribution on Œò. Our goal is to
find the best policy in a chosen class L(Œò)‚äÇ P(Œò):
œÄQ‚àó= argmin
Q‚ààL(Œò)R(œÄQ).
27ForŒª >0and a prior P‚àà P(Œò), the PAC-Bayesian learning strategy suggested in Gabbianelli et al.
[21] is to find in L(Œò)‚äÇ P(Œò):
ÀÜœÄIX
Qn= argmin
Q‚ààL(Œò)
ÀÜRIX‚àíŒª
n(œÄQ) +KL(Q||P)
Œªn
.
This learning strategy suffers from a suboptimality bounded in the result below:
Proposition 20 (Suboptimality of the IX PAC-Bayesian learning strategy from [ 21]).Let
Œª >0andŒ¥‚àà(0,1]. Then, it holds with probability at least 1‚àíŒ¥that
0‚â§R(ÀÜœÄIX
Qn)‚àíR(œÄQ‚àó)‚â§ŒªCŒª/2(œÄQ‚àó) +2 (KL(Q‚àó||P) + ln(2 /Œ¥))
Œªn,
where
CŒª(œÄ) =EœÄ(a|x)
œÄ2
0(a|x) +ŒªœÄ0(a|x)|c|
.
Similarly for PAC-Bayesian learning, both suboptimalities ( LSand IX) have the same form, they only
depend on two different quantities ( SLIN
ŒªandCŒªrespectively). For a œÄ‚ààŒ†andŒª >0, If we can
identify when SLIN
Œª(œÄ)‚â§ CŒª/2(œÄ), then we can prove that the sub-optimality of LSPAC-Bayesian
learning strategy is better than the one of IX in certain cases. Luckily, this is always the case, and it is
stated formally below.
Proposition 21. LetœÄ‚ààŒ†andŒª >0. We have:
SLIN
Œª(œÄ)‚â§ CŒª/2(œÄ). (42)
Proof. LetœÄ‚ààŒ†andŒª >0, and recall that:
SLIN
Œª(œÄ) =EœÄ(a|x)c2
œÄ2
0(a|x)‚àíŒªœÄ0(a|x)c
.
We have:
CŒª/2(œÄ)‚àí SLIN
Œª(œÄ) =E"
œÄ(a|x)
œÄ2
0(a|x) +Œª
2œÄ0(a|x)|c| ‚àíœÄ(a|x)c2
œÄ2
0(a|x)‚àíŒªœÄ0(a|x)c#
=E"
œÄ(a|x)|c| 
1
œÄ2
0(a|x) +Œª
2œÄ0(a|x)‚àí|c|
œÄ2
0(a|x) +ŒªœÄ0(a|x)|c|!#
=E"
œÄ(a|x)|c| 
œÄ2
0(a|x) (1‚àí |c|) +Œª
2œÄ0(a|x)|c|
(œÄ2
0(a|x) +Œª
2œÄ0(a|x))(œÄ2
0(a|x) +ŒªœÄ0(a|x)|c|)!#
‚â•0.
Similarly, this means that the suboptimality of LS-LINPAC-Bayesian learning strategy is also, better
bounded than the one of IX.
Minimax optimality of our learning strategy. From Jin et al. [28, Theorem 4.4 ]we can state that
the minimax suboptimality lower bound, in the case of deterministic optimal policies is of the rate
O(1/‚àö
nC‚àó)withinfx‚ààXœÄ0(œÄ‚àó(x)|x)> C‚àó. Our bound as well as IX bound match this minimax
lower bound, as:
SLIN
Œª(œÄ‚àó) =Ex,cc2
œÄ0(œÄ‚àó(x)|x)‚àíŒªc
‚â§1
C‚àó
CŒª(œÄ‚àó) =Ex,c|c|
œÄ0(œÄ‚àó(x)|x) +Œª
‚â§1
C‚àó.
28One can see that for both, selecting a
Œª‚àó=r
2 (KL(Q‚àó||P) + ln(2 /Œ¥))C‚àó
n,
gets you the desired bound, matching this minimax rate.
F Proofs of OPE
F.1 Proof of high order empirical moments bound (Proposition 1)
Proposition (Empirical moments risk bound) .LetœÄ‚ààŒ†,L‚â•1,Œ¥‚àà(0,1],Œª >0andh
satisfying (C1). Then it holds with probability at least 1‚àíŒ¥that
R(œÄ)‚â§œàŒª
ÀÜRh
n(œÄ) +2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ) +ln(1/Œ¥)
Œªn
,
where œàŒªand ÀÜMh,‚Ñì
n(œÄ)are defined in (5), respectively, and recall that œàŒª(x)‚â§x.
Proof. LetL‚àà N‚àó,Œª >0andX‚â•0apositive random variable . We have 2L‚àí1‚â•1, and with
the decreasing nature of f(2L‚àí1)(Lemma 13), we also have:
f(2L‚àí1)(0)‚â•f2L‚àí1(ŒªX)‚áê‚áí1
2L‚â• ‚àílog(1 + ŒªX)‚àíP2L‚àí1
l=1(‚àí1)‚Ñì‚àí1
k(ŒªX)‚Ñì
(ŒªX)2L
‚áê‚áí2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
k(ŒªX)‚Ñì‚â§log(1 + ŒªX)
‚áê‚áí exp 2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!
‚â§1 +ŒªX
=‚áí E"
exp 2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!#
‚â§1 +Œª E[X]
=‚áí E"
exp 2LX
‚Ñì=1(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!#
‚â§exp ((log(1 + Œª E[X]))
=‚áí E"
exp 
Œª(X‚àí1
Œªlog (1 + Œª E[X])) +2LX
‚Ñì=2(‚àí1)‚Ñì‚àí1
‚Ñì(ŒªX)‚Ñì!#
‚â§1.
For any X‚â§0, we can inject ‚àíX‚â•0to obtain:
‚àÄX‚â§0, E"
exp 
Œª
‚àí1
Œªlog (1‚àíŒª E[X])‚àíX
‚àí2LX
‚Ñì=21
‚Ñì(ŒªX)‚Ñì!#
‚â§1. (43)
The result in Equation (43) will be combined with Chernoff Inequality (Lemma 12) to finally prove
our bound. Let Œª >0, for our problem, we define the random variable Xito use in the Chernoff
Inequality as:
Xi=‚àí1
Œªlog (1‚àíŒª E[h])‚àíhi‚àí2LX
‚Ñì=21
‚Ñì(Œªhi)‚Ñì.
29For any a‚àà R, this gives us the following:
PÔ£´
Ô£≠X
i‚àà[n]Xi> aÔ£∂
Ô£∏‚â§( E[exp ( ŒªX1)])nexp(‚àíŒªa)
PÔ£´
Ô£≠‚àín
Œªlog (1‚àíŒª E[h])‚àíX
i‚àà[n] 
hi+2LX
‚Ñì=21
‚Ñì(Œªhi)‚Ñì!
> aÔ£∂
Ô£∏‚â§( E[exp ( ŒªX1)])nexp(‚àíŒªa)
PÔ£´
Ô£≠‚àín
Œªlog (1‚àíŒª E[h])‚àíX
i‚àà[n] 
hi+2LX
‚Ñì=21
‚Ñì(Œªhi)‚Ñì!
> aÔ£∂
Ô£∏‚â§exp(‚àíŒªa) (Use of Equation (43) )
Solving for Œ¥= exp( ‚àíŒªa), we get:
PÔ£´
Ô£≠‚àín
Œªlog (1‚àíŒª E[h])‚àíX
i‚àà[n] 
hi+2LX
‚Ñì=21
‚Ñì(Œªhi)‚Ñì!
>ln(1/Œ¥)
ŒªÔ£∂
Ô£∏‚â§Œ¥
PÔ£´
Ô£≠‚àí1
Œªlog (1‚àíŒª E[h])‚àí1
nX
i‚àà[n] 
hi+2LX
‚Ñì=2Œª‚Ñì
‚Ñìh‚Ñì
i!
>ln(1/Œ¥)
ŒªnÔ£∂
Ô£∏‚â§Œ¥
P 
‚àí1
Œªlog (1‚àíŒª E[h])‚àíÀÜRh
n(œÄ)‚àí2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ)>ln(1/Œ¥)
Œªn!
‚â§Œ¥
P 
‚àí1
Œªlog (1‚àíŒª E[h])>ÀÜRh
n(œÄ) +2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ)ln(1/Œ¥)
Œªn!
‚â§Œ¥.
This means that the following, complementary event will hold with probability at least 1‚àíŒ¥:
‚àí1
Œªlog (1‚àíŒª E[h])‚â§ÀÜRh
n(œÄ) +2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ)ln(1/Œ¥)
Œªn.
œàŒªbeing a non-decreasing function, applying it to the two sides of this inequality gives us:
E[h]‚â§œàŒª
ÀÜRh
n(œÄ) +2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ) +ln(1/Œ¥)
Œªn
.
Finally, hsatisfies (C1) , this means that the bound is also an upper bound on the true risk, giving:
R(œÄ)‚â§œàŒª
ÀÜRh
n(œÄ) +2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ) +ln(1/Œ¥)
Œªn
,
which concludes the proof.
30F.2 Proof of the impact of Lon the bound‚Äôs tightness (Proposition 2)
Proposition (Impact of Lon the bound‚Äôs tightness) .LetœÄ‚ààŒ†,Œ¥‚àà(0,1],Œª >0,L‚â•1
andhsatisfying (C1). Let
UŒª,h
L(œÄ) =œàŒª 
ÀÜRh
n(œÄ) +ln(1/Œ¥)
Œªn+2LX
‚Ñì=2Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ)!
be the upper bound in Equation (6). Then,
Œª‚â§min
i‚àà[n]2L+ 2
(2L+ 1)|hi|
=‚áíUŒª,h
L+1(œÄ)‚â§UŒª,h
L(œÄ). (44)
which implies that:
Œª‚â§min
i‚àà[n]1
|hi|
=‚áíUŒª,h
L(œÄ)is a decreasing function w.r.t L.
Proof. We want to prove the implication (44) from which the condition on the decreasing nature of
our bound will follow. Indeed, Let us suppose that (44) is true, we have:
Œª‚â§min
i‚àà[n]1
|hi|
=‚áí ‚àÄL‚â•1, Œª‚â§min
i‚àà[n]2L+ 2
(2L+ 1)|hi|
=‚áí ‚àÄL‚â•1, UŒª,h
L+1(œÄ)‚â§UŒª,h
L(œÄ) (Using (44) )
=‚áíUŒª,h
L(œÄ)is a decreasing function w.r.t L.
Now let us prove the implication in (44). We have for any L‚â•1:
UŒª,h
L+1(œÄ)‚â§UŒª,h
L(œÄ)‚áê‚áí2L+2X
‚Ñì=2L+1Œª‚Ñì‚àí1
‚ÑìÀÜMh,‚Ñì
n(œÄ)‚â§0
‚áê‚áíŒª2L
nnX
i=1h2L+1
i1
2L+ 1+Œªhi
2L+ 2
‚â§0
Ashi‚â§0, we can ensure this inequality by choosing a Œªthat verifies:
‚àÄi‚àà[n], Œª‚â§2L+ 2
(2L+ 1)|hi|
‚áê‚áí Œª‚â§min
i‚àà[n]2L+ 2
(2L+ 1)|hi|
which concludes the proof.
F.3 Comparisons of the bounds UŒª
L(Proposition 3)
We compare the bounds evaluated in their optimal regularisation function h. We start by stating the
proposition and proving it.
Proposition. LetœÄ‚ààŒ†, and Œª >0, we define:
UŒª
L(œÄ) = min
hUŒª,h
L(œÄ).
Then, for any Œª >0, it holds that for any L >1:
UŒª
L(œÄ)‚â§UŒª
1(œÄ).
In particular, ‚àÄŒª >0:
UŒª
‚àû(œÄ)‚â§UŒª
1(œÄ), (45)
31Proof. LetœÄ‚ààŒ†,Œª >0and
UŒª
L(œÄ) = min
hUŒª,h
L(œÄ).
We can prove (see Appendix F.4) that:
UŒª
1(œÄ) =UŒª,h‚àó,1
1 (œÄ) =œàŒª
ÀÜRh‚àó,1
n(œÄ) +Œª
2ÀÜMh‚àó,1,2
n(œÄ) +ln(1/Œ¥)
Œªn
with:
h‚àó,1(p, q, c ) =‚àímin(|c|p/q,1/Œª),
and that (see Appendix F.7):
UŒª
‚àû(œÄ) =œàŒª
ÀÜRŒª
n(œÄ) +ln(1/Œ¥)
Œªn
.
From Proposition 2, we have that for any h:
Œª‚â§min
i‚àà[n]1
|hi|
=‚áíUŒª,h
L(œÄ)is a decreasing function w.r.t L.
It appears that the optimal function h‚àó,1respects this condition, as by definition:
min
i‚àà[n]1
|(h‚àó,1)i|
‚â•Œª,
meaning that:
UŒª,h‚àó,1
L (œÄ)is a decreasing function w.r.t L.
This result suggests that the Empirical Second Moment bound, evaluated in its optimal function h‚àó,1,
is always bigger than bounds with additional moments (evaluated in the same h‚àó,1). This leads us to
the result wanted, as for any L >1:
UŒª
L(œÄ) = min
hUŒª,h
L(œÄ)‚â§UŒª,h‚àó,1
L (œÄ)‚â§UŒª,h‚àó,1
1 (œÄ) =UŒª
1(œÄ).
In particular, we get:
UŒª
‚àû(œÄ)‚â§UŒª
1(œÄ),
which ends the proof.
This means that UŒª
‚àûis tighter than UŒª
1, and thus can also be tighter than empirical Bernstein.
F.4 Proof of the optimality of global clipping for Corollary 4
Proposition (Optimal hforL= 1).LetŒª >0. The function hthat minimizes the bound for
L= 1, giving the tightest result is:
‚àÄi, h i=h(œÄ(ai|xi), œÄ0(ai|xi), ci)) =‚àíminœÄ(ai|xi)
œÄ0(ai|xi)|ci|,1
Œª
This means that when the costs are binary, we obtain the classical Clipping estimator of
parameter 1/Œª:
hi= minœÄ(ai|xi)
œÄ0(ai|xi),1
Œª
ci.
Proof. We want to look for the value of hthat minimizes the bound. Formally, by fixing all variables
of the bound, this problem reduces to:
argmin
h‚àà(C1)ÀÜRh
n(œÄ) +Œª
2ÀÜMh,2
n(œÄ) = argmin
h‚àà(C1)1
nnX
i=1
hi+Œª
2h2
i
.
32The objective decomposes across data points, so we can solve it for every hiindependently. Let us
fix aj‚àà[n], the following problem:
argmin
hj‚àà RÀÜRh
n(œÄ) +Œª
2ÀÜMh,2
n(œÄ) = argmin
hj‚àà R
hj+Œª
2h2
j
subject to hj‚â•œÄ(aj|xj)
œÄ0(aj|xj)cj
is strongly convex in hj. We write the KKT conditions for hjto be optimal; there exists Œ±‚àóthat
verifies:
1 +Œªhj‚àíŒ±‚àó= 0 (46)
Œ±‚àó‚â•0 (47)
Œ±‚àóœÄ(aj|xj)
œÄ0(aj|xj)cj‚àíhj
= 0 (48)
hj‚â•œÄ(aj|xj)
œÄ0(aj|xj)cj (49)
We study the two following two cases:
Case 1: hj‚â§ ‚àí1
Œª:
we have Œ±‚àó= 1 + Œªhj‚â§0 =‚áíŒ±‚àó= 0, meaning that:
hj=‚àí1
Œª
Case 2: hj>‚àí1
Œª:
we have Œ±‚àó= 1 + Œªhj>0, which combined to condition (36) gives:
hj=œÄ(aj|xj)
œÄ0(aj|xj)cj.
The two results combined mean that we always have:
hj‚â• ‚àí1
Œª,and whenever hj>‚àí1
Œª=‚áíhj=œÄ(aj|xj)
œÄ0(aj|xj)cj.
We deduce that hjhas the following form:
hj=h(œÄ(aj|xj), œÄ0(aj|xj), cj) =‚àíminœÄ(aj|xj)
œÄ0(aj|xj)|cj|,1
Œª
(50)
Œ±‚àó= 1‚àíŒªminœÄ(aj|xj)
œÄ0(aj|xj)|cj|,1
Œª
(51)
These values verify the KKT conditions. As the problem is strongly convex, hjhas a unique possible
value and must be equal to equation (38). The form of hjis a global clipping that includes the cost in
the function as well. In the case where the cost function cis binary:
‚àÄi c i‚àà {‚àí 1,0},
we recover the classical Clipping with parameter 1/Œªas an optimal solution for h:
hj= minœÄ(aj|xj)
œÄ0(aj|xj),1
Œª
cj.
33F.5 Comparison with empirical Bernstein
We begin by comparing the Second Moment Bound with Swaminathan and Joachims [55]‚Äôs bound as
they both manipulate similar quantities. The bound of [ 55] uses the Empirical Bernstein bound of
[36] applied to the Clipping Estimator. We recall its expression below for a parameter M > 0:
ÀÜRM
n(œÄ) =1
nnX
i=1minœÄ(ai|xi)
œÄ0(ai|xi), M
ci.
We also give below the Empirical Bernstein Bound applied to this estimator:
Proposition (Empirical Bernstein for Clipping of [ 55]).LetœÄ‚ààŒ†,Œ¥‚àà(0,1]andM > 0.
Then it holds with probability at least 1‚àíŒ¥that
R(œÄ)‚â§ÀÜRM
n(œÄ) +s
2ÀÜVMn(œÄ) ln(2 /Œ¥)
n+7Mln(2/Œ¥)
3(n‚àí1), (52)
withÀÜVM
n(œÄ)the empirical variance of the clipping estimator.
We are usually interested in the case where œÄandœÄ0are different, leading to substantial importance
weights. In this practical scenario, the variance and the second moment are of the same magnitude of
M. Indeed, one can see it from the following equality:
ÀÜVM
n(œÄ)|{z}
O(M)=ÀÜMM,2
n(œÄ)|{z}
O(M)‚àí
ÀÜRM
n(œÄ)2
|{z}
O(¬Øc2)
‚âàÀÜMM,2
n(œÄ)|{z}
O(M)(M‚â´¬Øc2=o(1).)
This means that in practical scenarios, the empirical variance and the empirical second moment are
approximately the same. Recall that the Second Moment Bound works for any regularizer h, As
Clipping satisfies (C1) , we give the Second Moment Upper of Corollary 4 with Clipping below:
œàŒª
ÀÜRM
n(œÄ) +Œª
2ÀÜMM,2
n(œÄ) +ln(1/Œ¥)
Œªn
‚â§ÀÜRM
n(œÄ) +Œª
2ÀÜMM,2
n(œÄ) +ln(1/Œ¥)
Œªn(œàŒª(x)‚â§x,‚àÄx)
‚â§ÀÜRM
n(œÄ) +Œª
2ÀÜMM,2
n(œÄ) +ln(1/Œ¥)
Œªn.
Choosing a Œª‚âàq
2 ln(1 /Œ¥)/(nÀÜMM,2
n(œÄ))gives us an upper bound that is close to:
ÀÜRM
n(œÄ) +Œª
2ÀÜMM,2
n(œÄ) +ln(1/Œ¥)
Œªn‚âàÀÜRM
n(œÄ) +s
2ÀÜMM,2
n(œÄ) ln(1 /Œ¥)
n
‚âàÀÜRM
n(œÄ) +s
2ÀÜVMn(œÄ) ln(1 /Œ¥)
n
‚â§ÀÜRM
n(œÄ) +s
2ÀÜVMn(œÄ) ln(2 /Œ¥)
n+7Mln(2/Œ¥)
3(n‚àí1).
This means that in practical scenarios, and with a good choice of Œª‚àº O(1/‚àön), the Second Moment
bound would be better than the Empirical Bernstein bound, and this difference will be even greater
when M‚â´1. This is aligned with our experiments, where we see that the new Second Moment
bound is much tighter in practice. This also confirms that the Logarithmic smoothing bound is even
tighter, because it is smaller than the Second Moment bound as stated in Proposition 3.
34F.6 Proof of the L‚Üí ‚àû bound (Corollary 5)
Proposition (Empirical Logarithmic Smoothing bound with L‚Üí ‚àû ).LetœÄ‚ààŒ†,Œ¥‚àà(0,1]
andŒª >0. Then it holds with probability at least 1‚àíŒ¥that
R(œÄ)‚â§œàŒª
‚àí1
nnX
i=11
Œªlog (1‚àíŒªhi) +ln(1/Œ¥)
Œªn
.
Taking the limit of Lnaively recovers this form of the bound, but imposes a condition on Œªfor the
bound to converge. We instead, take another path of proof that does not impose any condition on Œª,
developed below. The main idea is to take the limit of Lto recover the variable to use along Chernoff.
Proof. Recall that for the proof of the Empirical moments bounds, we used the following random
variable defined with Œª >0:
Xi=‚àí1
Œªlog (1‚àíŒª E[h])‚àíhi‚àí2LX
‚Ñì=21
‚Ñì(Œªhi)‚Ñì,
combined with Chernoff Inequality (Lemma 12) to prove our bound. If we take the limit L‚Üí ‚àû for
our random variable, we obtain the following random variable:
ÀúXi=‚àí1
Œªlog (1‚àíŒª E[h]) +1
Œªlog (1‚àíŒªhi)
=1
Œªlog1‚àíŒªhi
1‚àíŒª E[h]
.
We use the random variable ÀúXiwith the Chernoff Inequality. For any a‚àà R, we have:
PÔ£´
Ô£≠X
i‚àà[n]ÀúXi> aÔ£∂
Ô£∏‚â§
Eh
exp
ŒªÀúX1in
exp(‚àíŒªa)
PÔ£´
Ô£≠‚àín
Œªlog (1‚àíŒª E[h]) +X
i‚àà[n]1
Œªlog (1‚àíŒªhi)
> aÔ£∂
Ô£∏‚â§
Eh
exp
ŒªÀúX1in
exp(‚àíŒªa)
On the other hand, we have:
Eh
exp
ŒªÀúX1i
=E[1‚àíŒªhi]
1‚àíŒª E[h]= 1.
Using this equality and solving for Œ¥= exp( ‚àíŒªa), we get:
PÔ£´
Ô£≠‚àín
Œªlog (1‚àíŒª E[h]) +X
i‚àà[n]1
Œªlog (1‚àíŒªhi)
>ln(1/Œ¥)
ŒªÔ£∂
Ô£∏‚â§Œ¥
PÔ£´
Ô£≠‚àí1
Œªlog (1‚àíŒª E[h]) +1
nX
i‚àà[n]1
Œªlog (1‚àíŒªhi)>ln(1/Œ¥)
ŒªnÔ£∂
Ô£∏‚â§Œ¥
This means that the following, complementary event will hold with probability at least 1‚àíŒ¥:
‚àí1
Œªlog (1‚àíŒª E[h])‚â§ ‚àí1
nnX
i=11
Œªlog (1‚àíŒªhi) +ln(1/Œ¥)
Œªn.
œàŒªbeing a non-decreasing function, applying it to the two sides of this inequality gives us:
E[h]‚â§œàŒª
‚àí1
nnX
i=11
Œªlog (1‚àíŒªhi) +ln(1/Œ¥)
Œªn
.
Ashsatisfies (C1) , we obtain the required inequality:
R(œÄ)‚â§œàŒª
‚àí1
nnX
i=11
Œªlog (1‚àíŒªhi) +ln(1/Œ¥)
Œªn
.
and conclude the proof.
35F.7 Proof of the optimality of IPS for Corollary 5
Proposition (Optimal hforL‚Üí ‚àû ).LetŒª >0. The function hthat minimizes the bound
forL‚Üí ‚àû , giving the tightest result is:
‚àÄi, h i=h(œÄ(ai|xi), œÄ0(ai|xi), ci)) =œÄ(ai|xi)
œÄ0(ai|xi)ci
Proof. The proof of this proposition is quite simple. The function:
f(x) =‚àílog (1‚àíŒªx)
is increasing. This means that the lowest possible value of hiensures the tightest result. As our
variables hiverifies (C1) , we recover IPS as an optimal choice for this bound.
F.8 Comparison with the IX bound (Proposition 8)
We now attack the recently derived IX bound in Gabbianelli et al. [21] and show that our newly
proposed bound dominates it in all scenarios.
Proposition (Comparison with IX [ 21]).LetœÄ‚ààŒ†,Œ¥‚àà]0,1]andŒª >0, the IX bound from
[21] states that we have with at least probability 1‚àíŒ¥:
R(œÄ)‚â§ÀÜRŒª-IX
n(œÄ) +ln(1/Œ¥)
Œªn(53)
with:
ÀÜRŒª-IX
n(œÄ) =1
nnX
i=1œÄ(ai|xi)
œÄ0(ai|xi) +Œª/2ci.
LetUŒª
IX(œÄ)be the IX upper bound defined above, we have for any Œª >0:
UŒª
‚àû(œÄ)‚â§UŒª
IX(œÄ). (54)
Proof. LetœÄ‚ààŒ†,Œ¥‚àà]0,1]andŒª >0. Recall that UŒª
‚àû(œÄ) =œàŒª
ÀÜRŒª
n(œÄ) +ln(1/Œ¥)
Œªn
. We have:
œàŒª
ÀÜRŒª
n(œÄ) +ln(1/Œ¥)
Œªn
‚â§ÀÜRŒª
n(œÄ) +ln(1/Œ¥)
Œªn(‚àÄx, œàŒª(x)‚â§x)
‚â§ ‚àí1
nnX
i=11
Œªlog (1‚àíŒªwœÄ(xi, ai)ci) +ln(1/Œ¥)
Œªn.
Using the inequality log(1 + x)‚â•x
1+x/2for all x >0, we get:
UŒª
‚àû(œÄ)‚â§ ‚àí1
nnX
i=11
Œªlog (1‚àíŒªwœÄ(xi, ai)ci) +ln(1/Œ¥)
Œªn
‚â§1
nnX
i=1wœÄ(xi, ai)
1‚àíŒªwœÄ(xi, ai)ci/2ci+ln(1/Œ¥)
Œªn
log(1 + x)‚â•x
1 +x/2
‚â§1
nnX
i=1œÄ(ai|xi)
œÄ0(ai|xi)‚àíŒªœÄ(ai|xi)ci/2ci+ln(1/Œ¥)
Œªn
‚â§1
nnX
i=1œÄ(ai|xi)
œÄ0(ai|xi) +Œª/2ci+ln(1/Œ¥)
Œªn(‚àíœÄ(ai|xi)ci‚â§1andci‚â§0)
‚â§ÀÜRIX‚àíŒª
n (œÄ) +ln(1/Œ¥)
Œªn=UŒª
IX(œÄ),
which ends the proof.
36The result states the dominance of the LSbound compared to IX. The proof of this result also gives
us insight on when the LSbound will be much tighter than IX. Indeed, to obtain the IX bound, LS
bound is loosened through 3 steps:
1. The use of œàŒª(x)‚â§x,‚àÄx.
2. The use of log(1 + Œªx)‚â•Œªx
1+Œªx/2,‚àÄx‚â•0.
3. The use of ‚àíœÄ(ai|xi)ci‚â§1,‚àÄi‚àà[n].
The two first inequalities are loose when Œª‚àº1/‚àönis not too small, which means that LSwill be
much better in problems with few samples. The third inequality is loose when œÄis not a peaked
policy or the cost is way less than 1. Even if LSbound is always smaller than IX, LSwill give way
better result if the number of samples is small, and/or the policy evaluated is diffused.
G Proofs of OPS and OPL
G.1 OPS: Proof of suboptimality bound (Proposition 9)
Proposition (Suboptimality of our selection strategy in (20)).LetŒª >0andŒ¥‚àà(0,1]. Then,
it holds with probability at least 1‚àíŒ¥that
0‚â§R(ÀÜœÄS
n)‚àíR(œÄS
‚àó)‚â§ŒªSŒª(œÄS
‚àó) +2 ln(2|Œ†S|/Œ¥)
Œªn,
where œÄS
‚àóandÀÜœÄS
nare defined in (19) and(20), and
SŒª(œÄ) =E
(wœÄ(x, a)c)2/(1‚àíŒªwœÄ(x, a)c)
.
In addition, our upper bound is always finite as:
ŒªSŒª(œÄ) =ŒªE(wœÄ(x, a)c)2
1‚àíŒªwœÄ(x, a)c
‚â§min
|R(œÄ)|, ŒªE
(wœÄ(x, a)c)2	
‚â§ |R(œÄ)|.
Proof. To prove this bound on the suboptimality of our selection method, we need both an upper
bound and a lower bound on the true risk using the LSestimator. Luckily, we already have derived
them in ??. For a fixed Œª, taking a union of the two bounds over the cardinal of the finite policy class
|Œ†s|, we get the following holding with probability at least 1‚àíŒ¥for all œÄ‚ààŒ†s:
R(œÄ)‚àíÀÜRŒª
n(œÄ)‚â§ln(2|Œ†s|/Œ¥)
Œªn, and ÀÜRŒª
n(œÄ)‚àíR(œÄ)‚â§ŒªSŒª(œÄ) +ln(2|Œ†s|/Œ¥)
Œªn.
AsÀÜœÄS
n‚ààŒ†sand by definition of ÀÜœÄS
n(minimizer of ÀÜRŒª
n(œÄ)), we have:
R(ÀÜœÄS
n)‚â§ÀÜRŒª
n(ÀÜœÄS
n) +ln(2|Œ†s|/Œ¥)
Œªn‚â§ÀÜRŒª
n(ÀÜœÄS
‚àó) +ln(2|Œ†s|/Œ¥)
Œªn.
Using the lower bound on the risk of R(ÀÜœÄS
‚àó), we have:
R(ÀÜœÄS
n)‚â§ÀÜRŒª
n(ÀÜœÄS
‚àó) +ln(2|Œ†s|/Œ¥)
Œªn
‚â§R(ÀÜœÄS
‚àó) +ŒªSŒª(ÀÜœÄS
‚àó) +2 ln(2|Œ†s|/Œ¥)
Œªn.
which gives us the suboptimality upper bound:
0‚â§R(ÀÜœÄS
n)‚àíR(œÄS
‚àó)‚â§ŒªSŒª(œÄS
‚àó) +2 ln(2|Œ†S|/Œ¥)
Œªn.
Note that:
ŒªSŒª(œÄ) =ŒªE(wœÄ(x, a)c)2
1‚àíŒªwœÄ(x, a)c
‚â§min
|R(œÄ)|, ŒªE
(wœÄ(x, a)c)2	
,
always ensuring a finite bound.
37G.2 OPL: Proof of PAC-Bayesian LS-LINbound (Proposition 10)
Proposition (PAC-Bayes learning bound for ÀÜRŒª‚àíLIN
n ).Given a prior P‚àà P(Œò),Œ¥‚àà(0,1]
andŒª >0, the following holds with probability at least 1‚àíŒ¥:
‚àÄQ‚àà P(Œò), R (œÄQ)‚â§œàŒª
ÀÜRŒª‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn
Proof. To prove this proposition, we can either take the path of High Order Empirical moments as
for Pessimistic OPE, or we can prove it directly. We provide here a simple proof of this proposition
using ideas from Alquier [1, Corollary 2.5]. Let:
dŒ∏(a|x) = 1[fŒ∏(x) =a],‚àÄ(x, a)‚àà X √ó A , (55)
it means that:
œÄQ(a|x) =EŒ∏‚àºQ[dŒ∏(a|x)],‚àÄ(x, a)‚àà X √ó A .
Recall that to prove a PAC-Bayesian generalization bound, one can rely on the Change of measure
Lemma (Lemma 14). For any Œª >0, the adequate function gto consider is:
g(Œ∏,Dn) =nX
i=1
‚àílog(1‚àíŒªR(dŒ∏)) + log
1‚àíŒªdŒ∏(ai|xi)ci
œÄ0(ai|xi)
=nX
i=1logÔ£´
Ô£≠1‚àíŒªdŒ∏(ai|xi)ci
œÄ0(ai|xi)
1‚àíŒªR(dŒ∏)Ô£∂
Ô£∏.
By exploiting the i.i.d. nature of the data and exchanging the order of expectations ( Pis independent
ofDn), we can naturally prove that:
Œ®g= EPÔ£Æ
Ô£∞nY
i=1EÔ£Æ
Ô£∞expÔ£´
Ô£≠logÔ£´
Ô£≠1‚àíŒªdŒ∏(ai|xi)ci
œÄ0(ai|xi)
1‚àíŒªR(dŒ∏)Ô£∂
Ô£∏Ô£∂
Ô£∏Ô£π
Ô£ªÔ£π
Ô£ª
= EPÔ£Æ
Ô£∞nY
i=1EÔ£Æ
Ô£∞1‚àíŒªdŒ∏(ai|xi)ci
œÄ0(ai|xi)
1‚àíŒªR(dŒ∏)Ô£π
Ô£ªÔ£π
Ô£ª
= EP"nY
i=11‚àíŒªR(dŒ∏)
1‚àíŒªR(dŒ∏)#
= 1.
Injecting Œ®gin Lemma 14, gives:
EŒ∏‚àºQ[‚àílog(1‚àíŒªR(dŒ∏)]‚â§1
nnX
i=1EŒ∏‚àºQ
‚àílog
1‚àíŒªdŒ∏(ai|xi)ci
œÄ0(ai|xi)
+KL(Q||P) + ln1
Œ¥
n
‚â§1
nnX
i=1EŒ∏‚àºQ
‚àídŒ∏(ai|xi) log
1‚àíŒªci
œÄ0(ai|xi)
+KL(Q||P) + ln1
Œ¥
n
‚â§ ‚àí1
nnX
i=1œÄQ(ai|xi) log
1‚àíŒªci
œÄ0(ai|xi)
+KL(Q||P) + ln1
Œ¥
n
‚â§ŒªÀÜRŒª‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
n.
From the convexity of x‚Üí ‚àí log(1 + x), we have:
‚àí1
Œªlog (1‚àíŒªR(œÄQ))‚â§1
ŒªEŒ∏‚àºQ[‚àílog(1‚àíŒªR(dŒ∏)]‚â§ÀÜRŒª‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn.
Applying the increasing function œàŒªof Equation (5) to both sides concludes the proof.
38G.3 OPL: Proof of PAC-Bayesian suboptimality bound (Proposition 11)
Proposition (Suboptimality of the learning strategy in (27)).LetŒª >0,P‚àà L(Œò)and
Œ¥‚àà(0,1]. Then, it holds with probability at least 1‚àíŒ¥that
0‚â§R(ÀÜœÄQn)‚àíR(œÄQ‚àó)‚â§ŒªSLIN
Œª(œÄQ‚àó) +2 (KL(Q‚àó||P) + ln(2 /Œ¥))
Œªn,
where
SLIN
Œª(œÄ) =EœÄ(a|x)c2
œÄ2
0(a|x)‚àíŒªœÄ0(a|x)c
.
In addition, our upper bound is always finite as:
ŒªSLIN
Œª(œÄ)‚â§min
|R(œÄ)|, ŒªEœÄ(a|x)c2
œÄ2
0(a|x)
‚â§ |R(œÄ)|.
Proof. To prove this bound on the suboptimality of our learning strategy, we need both a PAC-
Bayesian upper bound and a lower bound on the true risk using the LS-LINestimator. Luckily, we
already have derived an upper bound in Proposition 10, that we linearize here as œàŒª(x)‚â§x:
‚àÄQ‚àà P(Œò), R (œÄQ)‚â§ÀÜRŒª‚àíLIN
n (œÄQ) +KL(Q||P) + ln1
Œ¥
Œªn.
For the lower bound, we rely a second time on the Change of measure Lemma (Lemma 14). For any
Œª >0, we choose the following function g:
g(Œ∏,Dn) =nX
i=1
‚àí1
Œªlog
1‚àíŒªdŒ∏(ai|xi)ci
œÄ0(ai|xi)
‚àíR(dŒ∏)‚àíŒªSLIN
Œª(dŒ∏)
.
By exploiting the i.i.d. nature of the data and exchanging the order of expectations ( Pis independent
ofDn), we can prove that:
Œ®g= EPÔ£Æ
Ô£∞nY
i=1Ô£´
Ô£≠exp 
‚àíŒª(R(dŒ∏) +ŒªSLIN
Œª(dŒ∏))
EÔ£Æ
Ô£∞1
1‚àíŒªdŒ∏(a|x)c
œÄ0(a|x)Ô£π
Ô£ªÔ£∂
Ô£∏Ô£π
Ô£ª
‚â§ EPÔ£Æ
Ô£∞nY
i=1Ô£´
Ô£≠expÔ£´
Ô£≠‚àíŒª(R(dŒ∏) +ŒªSLIN
Œª(dŒ∏)) + EÔ£Æ
Ô£∞1
1‚àíŒªdŒ∏(a|x)c
œÄ0(a|x)Ô£π
Ô£ª‚àí1Ô£∂
Ô£∏Ô£∂
Ô£∏Ô£π
Ô£ª
‚â§ EP"nY
i=1
exp
‚àíŒª(R(dŒ∏) +ŒªSLIN
Œª(dŒ∏)) + EŒªdŒ∏(a|x)c
œÄ0(a|x)‚àíŒªdŒ∏(a|x)c#
‚â§ EP"nY
i=1
exp
‚àíŒª(R(dŒ∏) +ŒªSLIN
Œª(dŒ∏)) + EŒªdŒ∏(a|x)c
œÄ0(a|x)‚àíŒªc#
(dŒ∏is binary. )
‚â§ EP"nY
i=1
exp
‚àíŒª2SLIN
Œª(dŒ∏) + EŒªdŒ∏(a|x)c
œÄ0(a|x)‚àíŒªc‚àíŒªdŒ∏(a|x)c
œÄ0(a|x)#
‚â§ EP"nY
i=1 
exp 
‚àíŒª2SLIN
Œª(dŒ∏) +Œª2SLIN
Œª(dŒ∏)#
‚â§1,
giving by rearranging terms, the following PAC-Bayesian bound:
‚àÄQ‚àà P(Œò),ÀÜRŒª‚àíLIN
n (œÄQ)‚â§R(œÄQ) +ŒªSLIN
Œª(œÄQ) +KL(Q||P) + ln(2 /Œ¥)
Œªn.
Now we take a union of the the two bounds, for them to hold with probability at least 1‚àíŒ¥for all Q.
By definition of ÀÜœÄQn(minimizer of the upper bound), we have:
R(ÀÜœÄQn)‚â§ÀÜRŒª‚àíLIN
n (ÀÜœÄQn) +KL(Qn||P) + ln(2 /Œ¥)
Œªn‚â§ÀÜRŒª‚àíLIN
n (œÄQ‚àó) +KL(Q‚àó||P) + ln(2 /Œ¥)
Œªn.
39Using the lower bound on the risk of R(œÄQ‚àó), we have:
R(ÀÜœÄQn)‚â§ÀÜRŒª‚àíLIN
n (œÄQ‚àó) +KL(Q‚àó||P) + ln(2 /Œ¥)
Œªn
‚â§R(œÄQ‚àó) +ŒªSLIN
Œª(œÄQ‚àó) +KL(Q‚àó||P) + ln(2 /Œ¥)
Œªn.
which gives us the PAC-Bayesian suboptimality upper bound:
0‚â§R(ÀÜœÄQn)‚àíR(œÄQ‚àó)‚â§ŒªSLIN
Œª(œÄQ‚àó) +2 (KL(Q‚àó||P) + ln(2 /Œ¥))
Œªn.
Concluding the proof.
H Experimental design and detailed experiments
All our experiments were conducted on a machine with 16 CPUs. The PAC-Bayesian learning
experiments require a moderate amount of computation due to the handling of medium-sized datasets.
However, our experiments remain reproducible with minimal computational resources.
H.1 Off-policy evaluation and selection
H.1.1 Datasets
For both our OPE and OPS experiments, we use 11 UCI datasets with different sizes, action spaces
and number of features. The statistics of all these datasets are described in Table 3.
Table 3: OPE and OPS: 11 Datasets used from OpenML [8].
Datasets N K p
ecoli 336 8 7
arrhythmia 452 13 279
micro-mass 571 20 1300
balance-scale 625 3 4
eating 945 7 6373
vehicle 846 4 18
yeast 1484 10 8
page-blocks 5473 5 10
optdigits 5620 10 64
satimage 6430 6 36
kropt 28 056 18 6
H.1.2 (OPE) Tightness of the bounds
Additional details. For these experiments, as we only use oracle policies (faulty policies to log data
and we evaluate ideal policies), we use the full 11 datasets without splitting them. The faulty policies
are defined exactly as described in the experiments of Kuzborskij et al. [32]. For each datapoint, the
behavior (faulty) policy plays an action and we record a cost. The triplets datapoint, action and cost
constitute our logged bandit dataset, with which we can compute our estimates and bounds. As we
have access to the true label, the original dataset can be used to compute the true risk of any policy.
Detailed results. Evaluating the worst case performance of a policy is done through evaluating risk
upper bounds [ 10,32]. This means that a better evaluation will solely depend on the tightness of the
bounds used. To this end, given a policy œÄ, we are interested in bounds with a small relative radius
|U(œÄ)/R(œÄ)‚àí1|. We compare our newly derived bounds (cIPS-L=1 for UŒª
1and LS for UŒª
‚àûboth with
Œª= 1/‚àön) to SNIPS-ES: the Efron Stein bound for Self Normalized IPS [ 32], cIPS-EB: Empirical
Bernstein for Clipping [ 55] and the recent IX: Implicit Exploration bound [ 21]. We use all 11 datasets,
with different behavior policies ( œÑ0‚àà {0.2,0.25,0.3}) and different noise levels ( œµ‚àà {0.,0.1,0.2})
to evaluate ideal policies with different temperatures ( œÑ‚àà {0.1,0.2,0.3,0.4,0.5}), defining ‚àº500
different scenarios to validate our findings. In addition to the cumulative distribution of the relative
radius of the considered bounds of Figure 2. We give two tables in the following: the average relative
40Table 4: OPE: Average relative radius for each datasets
Datasets SN-ES cIPS-EB IX cIPS-L=1 LS
ecoli 1.00 1.00 0.676 0.752 0.573
arrhythmia 1.00 1.00 0.677 0.707 0.548
micro-mass 0.962 0.840 0.394 0.346 0.311
balance-scale 1.00 0.950 0.469 0.550 0.422
eating 0.930 0.734 0.318 0.337 0.265
vehicle 0.981 0.867 0.409 0.482 0.358
yeast 0.861 0.660 0.307 0.311 0.254
page-blocks 0.760 0.547 0.371 0.447 0.312
optdigits 0.468 0.323 0.148 0.139 0.113
satimage 0.506 0.336 0.171 0.184 0.140
kropt 0.224 0.161 0.087 0.066 0.060
radius of our bounds for each dataset, compiled in Table 4, and the average relative radius of our
bounds for each policy evaluated, compiled in Table 5. One can observe that LSalways gives the
best results no matter the projection. However, the cIPS-L=1 bound is sometimes better than IX,
especially when it comes to evaluating diffused policies, see Table 5.
Table 5: OPE: Average relative radiuses for each target policies (ideal policies with different œÑ)
œÑ SN-ES cIPS-EB IX cIPS-L=1 LS
œÑ= 0.1 0.783 0.630 0.332 0.400 0.308
œÑ= 0.2 0.781 0.630 0.326 0.390 0.295
œÑ= 0.3 0.782 0.668 0.353 0.389 0.297
œÑ= 0.4 0.793 0.706 0.385 0.385 0.301
œÑ= 0.5 0.810 0.735 0.432 0.397 0.323
H.1.3 (OPS) Find the best, avoid the worst policy
Policy selection aims at identifying the best policy among a set of finite candidates. In practice,
we are interested in finding policies that improve on œÄ0and avoid policies that perform worse
thanœÄ0. To replicate real world scenarios, we design an experiment where œÄ0is a faulty policy
(œÑ= 0.2), that collects noisy ( œµ= 0.2) interaction data, some of which is used to learn œÄŒ∏IPS, œÄŒ∏SN,
and that we add to our discrete set of policies Œ†k=4={œÄ0, œÄideal, œÄŒ∏IPS, œÄŒ∏SN}. The splits for these
experiments are the following: 70% of the data is used to create bandit feedback ( 20% is used to train
œÄŒ∏IPS, œÄŒ∏SNand50% is used to evaluate policies based on estimators/upper bounds.) the rest is used to
evaluate the true value of the policies. The goal is to measure the ability of our selection strategies
to choose from Œ†k=4, better performing policies than œÄ0. We thus define three possible outcomes:
a strategy can select worse performing policies, better performing or the best policy. We compare
selection strategies based on upper bounds to the commonly used estimators IPSandSNIPS . The
hyperparameters of all bounds (the clipping parameter MandŒª) are set to 1/‚àön. The comparison is
conducted on the 11 datasets with 10 different seeds resulting in 110 scenarios. In addition to the
plot in Figure 2, we collect the number of times each method selected the best policy ( œÄS
‚àó), a better
(B) or a worse ( W) policy than œÄ0for all datasets in Table 6. We can see that risk estimators can be
unreliable, especially in small sample datasets, as they can choose worse performing policies than œÄ0,
a catastrophic outcome in highly sensitive applications. Selecting policies based on upper bounds
is more conservative, as it avoids completely poor performing policies. In addition, the tighter the
bound, the better its percentage of time it selects the best policy: LSupper bound is less conservative
and can find best policies more than any other bound, while never selecting poor performing policies.
41Table 6: OPS: Number of times the worst, better or best policy was selected for each dataset.
DatasetIPS SNIPS SN-ES cIPS-EB IX cIPS-L=1 LS
W BœÄS
‚àó W BœÄS
‚àó W BœÄS
‚àó W BœÄS
‚àó W BœÄS
‚àó W BœÄS
‚àó W BœÄS
‚àó
ecoli 2 6 2 4 1 5 0 10 0 0 10 0 0 7 3 0 10 0 0 6 4
arrhythmia 0 10 0 0 10 0 0 10 0 0 10 0 0 7 3 0 10 0 0 5 5
micro-mass 3 0 7 1 0 9 0 10 0 0 10 0 0 010 0 0 10 0 010
balance-scale 0 3 7 0 2 8 0 10 0 0 10 0 0 4 6 0 10 0 0 3 7
eating 3 2 5 2 1 7 0 10 0 0 10 0 0 4 6 0 8 2 0 4 6
vehicle 3 0 7 1 1 8 0 10 0 0 10 0 0 5 5 0 10 0 0 3 7
yeast 0 2 8 2 0 8 0 10 0 0 10 0 0 2 8 0 7 3 0 2 8
page-blocks 0 0 10 0 0 10 0 10 0 0 10 0 0 010 0 10 0 0 010
optdigits 0 1 9 0 0 10 0 10 0 0 10 0 0 1 9 0 3 7 0 1 9
satimage 0 0 10 0 0 10 0 10 0 0 10 0 0 010 0 7 3 0 010
kropt 0 0 10 0 0 10 0 10 0 0 0 10 0 010 0 0 10 0 010
H.2 Off-policy learning
H.2.1 Datasets
As described in the experiments section, we follow exactly the experimental design of Sakhi et al.
[49], Aouali et al. [5]to conduct our PAC-Bayesian Off-Policy learning experiments. We however
take the time to explain it in details. In this procedure, we need three splits: Dl(of size nl) to train the
logging policy œÄ0, another split Dc(of size nc) to generate the logging feedback with œÄ0, and finally
a test split Dtest(of size ntest) to compute the true risk R(œÄ)of any policy œÄ. In our experiments,
we split the training split Dtrain (of size N) of the four datasets considered into Dl(nl= 0.05N)
andDc(nc= 0.95N) and use their test split Dtest. The detailed statistics of the different splits can
be found in Table 7. Recall that Kis the number of actions and pthe number of features.
Table 7: OPL: Detailed statistics of the splits used.
Datasets N nl nc ntest K p
MNIST 60 000 3000 57 000 10 000 10 784
FashionMNIST 60 000 3000 57 000 10 000 10 784
EMNIST-b 112 800 5640 107 160 18 800 47 784
NUS-WIDE-128 161 789 8089 153 700 107 859 81 128
H.2.2 Policy class
In the PAC-Bayesian Learning paradigm, we are interested in the definition of policies as mixtures of
decision rules:
œÄQ(a|x) =EfŒ∏‚àºQ[ 1[fŒ∏(x) =a]], ‚àÄ(x, a)‚àà X √ó A . (56)
We use the Linear Gaussian Policy of Sakhi et al. [49]. To obtain these policies, we restrict fŒ∏to:
‚àÄx‚àà X, f Œ∏(x) = argmax
a‚Ä≤‚ààA
xtŒ∏a‚Ä≤	
(57)
This results in a parameter Œ∏of dimension d=p√óKwithpthe dimension of the features œï(x)
andKthe number of actions. We also restrict the family of distributions Qd+1={Q¬µ,œÉ=
N(¬µ, œÉ2Id),¬µ‚àà Rd, œÉ > 0}to independent Gaussians with shared scale. Estimating the propensity
ofagiven xreduces the computation to a one dimensional integral:
œÄ¬µ,œÉ(a|x) = Eœµ‚àºN(0,1)Ô£Æ
Ô£∞Y
a‚Ä≤Ã∏=aŒ¶
œµ+œï(x)T(¬µa‚àí¬µa‚Ä≤)
œÉ||œï(x)||Ô£π
Ô£ª
withŒ¶the cumulative distribution function of the standard normal.
H.2.3 Detailed hyperparameters
Contrary to previous work, our method does not require tuning any loss function hyperparameter
over a hold out set. We do however need to choose parameters to optimize the policies.
The logging policy œÄ0.œÄ0is trained on Dl(supervised manner) with the following parameters:
We use L2regularization of 10‚àí4. This is used to prevent the logging policy œÄ0from being close
to deterministic, allowing efficient learning with importance sampling. We use Adam [ 30] with a
learning rate of 10‚àí1for10epochs.
42Parameters of the bounds. cIPS and cvcIPS: The clipping parameter œÑis fixed to 1/KwithK
the action size of the dataset and cvcIPS is used with Œæ=‚àí0.5(the values used in Sakhi et al. [49]).
ES: The exponential smoothing parameter Œ±is fixed to 1‚àí1/K.
Optimizing the bounds. We use Adam [ 30] with a learning rate of 10‚àí3for100epochs. The
gradient of LIG policies is a one dimensional integral, and is approximated using S= 32 samples.
œÄ¬µ,œÉ(a|x) = Eœµ‚àºN(0,1)Ô£Æ
Ô£∞Y
a‚Ä≤Ã∏=aŒ¶
œµ+œï(x)T(¬µa‚àí¬µa‚Ä≤)
œÉ||œï(x)||Ô£π
Ô£ª
‚âà1
SSX
s=1Y
a‚Ä≤Ã∏=aŒ¶
œµs+œï(x)T(¬µa‚àí¬µa‚Ä≤)
œÉ||œï(x)||
œµ1, ..., œµ S‚àº N(0,1).
For all bounds, instead of fixing Œª, we take a union bound over a discretized space of possible
parameters Œõof size nŒõ= 100 and for each iteration jof the optimization procedure, we take Œªj‚ààŒõ
that minimizes the estimated bound and proceed to compute the gradient w.r.t ¬µandœÉwithŒªj.
H.2.4 Detailed results
In addition to the results of Table 2, we also provide a more detailed view of the results here. For
each Œ±and dataset, we average both {GR, R}over the 10 seeds and plot them in Figure 6 and
Figure 5. Note that the error bars are too small œÉ/‚àö
10‚âà0.001and all our results in these graphs are
significant. We observe that the LS PAC-Bayesian bound improves substantially on its competitors
in terms of the guaranteed risk, especially on MNIST and FashionMNIST and also obtains the best
performing policies, on par with the IXbound in the majority of scenarios.
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.8
0.7
0.6
0.5
0.4
0.3
Guaranteed Risk of Different Bounds 
MNIST
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.7
0.6
0.5
0.4
0.3
FashionMNIST
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.4
0.3
0.2
0.1
0.0
EMNIST
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.30
0.25
0.20
0.15
0.10
0.05
0.00
nuswide
cIPS
ES
cvcIPS
IX
LS-LINOPL: Guaranteed Risk Given by different Bounds
Figure 5: OPL: Guaranteed Risk given by the different bounds. We observe that our LS-LIN
dominates all other bounds. IXcomes close, especially on EMNIST and nuswide
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.85
0.80
0.75
0.70
0.65
True Risk of Obtained Policies
MNIST
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.75
0.70
0.65
0.60
0.55
FashionMNIST
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.5
0.4
0.3
0.2
0.1
EMNIST
0.2 0.4 0.6 0.8 1.0
Inverse temperature parameter 
0.35
0.30
0.25
0.20
0.15
0.10
0.05
nuswide
cIPS
ES
cvcIPS
IX
LS-LINOPL: True Risk of obtained policies
Figure 6: OPL: True risk of obtained policies after minimizing the PAC-Bayesian bounds. We observe
thatLS-LIN andIXare hardly distinguishable, they both give the best policies in the majority of
scenarios.
43NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction scopes our work in the offline contextual bandit
setting, describes our method and claims its superiority compared to existing work. We
provide both strong theoretical and empirical evidence in the paper to defend the claim.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We provide the limitations of our method in Appendix A and discuss ways to
mitigate them in future work.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
44Answer: [Yes]
Justification: All our results are proven in the paper and all assumptions are discussed.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Our paper follows the classical off-policy experimental design, and all details
to reproduce them are given in Appendix H.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
45Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: All data used is accessible UCI Repository, the code is also given in the
supplementary material to reproduce all experimental results.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The experimental settings are detailed in Section 5 and Appendix H.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: All our experiments are run with multiple seeds and for different scenarios
and datasets. Some graphs do not need error bars (cumulative distributions or selection
strategies), for the other results, we have very small error bars (Appendix H.2), making our
results significant.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
46‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Our experiments can be conducted in small machines and do not require heavy
compute, this is detailed in Appendix H.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: Our paper is of theoretical nature, presents ideas to increase safety in decision
making, uses publicly available data for the experiments and conforms to the NeurIPS Code
of Ethics.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The paper discusses both the positive and negative impacts in Appendix B.
Guidelines:
47‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our paper tackles theoretical questions for decision-making, the data used for
the experiments is openly accessible in UCI repository. We do not believe that our work
poses such risks.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: Our experimental design is inspired from the code base of some papers that
we cite, and all data used is openly accessible in UCI repository.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
48‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We do not release new assests, but we give the code to reproduce the experi-
ments in the supplementary material.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
49‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
50