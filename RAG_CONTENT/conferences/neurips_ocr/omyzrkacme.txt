Learning to Mitigate Externalities: the Coase Theorem
with Hindsight Rationality
Antoine Scheid1Aymeric Capitaine1
Etienne Boursier2Eric Moulines1Michael I. Jordan3,4Alain Durmus1
1Centre de MathÃ©matiques AppliquÃ©es â€“ CNRS â€“ Ã‰cole polytechnique â€“ Palaiseau, 91120, France
2INRIA Saclay, UniversitÃ© Paris Saclay, LMO - Orsay, 91400, France
3University of California, Berkeley
4Inria, Ecole Normale SupÃ©rieure, PSL Research University - Paris, 75, France
Abstract
In economic theory, the concept of externality refers to any indirect effect resulting
from an interaction between players that affects the social welfare. Most of the
models within which externality has been studied assume that agents have perfect
knowledge of their environment and preferences. This is a major hindrance to
the practical implementation of many proposed solutions. To address this issue,
we consider a two-player bandit setting where the actions of one of the players
affect the other player and we extend the Coase theorem [Coase, 2013]. This result
shows that the optimal approach for maximizing the social welfare in the presence
of externality is to establish property rights, i.e., enable transfers and bargaining
between the players. Our work removes the classical assumption that bargainers
possess perfect knowledge of the underlying game. We first demonstrate that in the
absence of property rights, the social welfare breaks down. We then design a policy
for the players which allows them to learn a bargaining strategy which maximizes
the total welfare, recovering the Coase theorem under uncertainty.
1 Introduction
The concept of externality is used in economics to capture phenomena that impact the global welfare
stemming from economic interactions without any compensation [Buchanan and Stubblebine, 2006,
Shah et al., 2018]. Externality is generally considered as market failure since they result in a loss
of collective welfare. Given its practical importance [Dahlman, 1979, Greenfield et al., 2009],
mechanisms that characterize and mitigate externalities are central to modern economic thinking.
A first approach to tackle the adverse effects of externalities in modern economics was based on
quotas and taxation [see, e.g., Pigou, 2017, and the references therein]. However, Coaseâ€™s theorem
[Coase, 2013] shows that in the presence of well-defined property rights and low transaction costs,
parties affected by externalities can privately negotiate efficient solutions, and recover a welfare
efficient allocation through transfers and bargaining.
Throughout the paper, we use the following simple example to illustrate our results.
Example 1. Consider two firms 1 (upstream) and 2 (downstream) respectively producing quantities
q1â©¾0andq2â©¾0of a good sold at a fixed price p >0. They incur strictly increasing and convex
costs, captured by the differentiable cost functions c1:q17â†’c1(q1)andc2:q27â†’c2(q2), satisfying
38th Conference on Neural Information Processing Systems (NeurIPS 2024).c1(0) = 0 andc2(0) = 0 . We assume that firm 1exerts on firm 2 a constant externality Î± >0per
unit produced. In other words, their profit functions (or utilities) are given by
Ï€1(q1) =pq1âˆ’c1(q1)and Ï€2(q1, q2) =pq2âˆ’c2(q2)âˆ’Î±q1.
One simple concrete illustration consists in an upstream firm emitting pollutants that reduce the
downstream firmâ€™s production. If the upstream firm owns the property rights, it may receive a
payment from the downstream one to reduce its output and thereby pollution. On the other hand, if
the downstream firm owns the property right, it may require compensation from the upstream firm to
allow its operation.
The Coase theorem demonstrates that in both cases with appropriate property rights, the resulting
levels of production would be welfare efficient. The theorem is typically explained in textbooks under
the assumption that the players have perfect knowledge of their own utility or profit function, as well
as that of others. However, this assumption is unlikely to hold in real-world scenarios, where players
have to learn about their own preferences and those of their competitors.
This example is, of course, a simplification of real world scenarios. For example, Abildtrup et al.
[2012] consider a more complex setting to model the interaction between farmers and waterworks
in Denmark where the farmers have the property rights whereas the waterworks can pay to reduce
pollution. In particular, they demonstrate the failure of the theorem, attributing it to the breakdown of
the main assumptions: no transaction costs, maximizing behaviors and perfect information, with an
important focus on strategic behaviors and the asymmetry of information. We restore the latter in our
work and provide foundations for the theorem to hold in more realistic scenarios.
A key question is whether the Coase theorem holds when players learn their preferences over time.
We investigate this question within the framework of a multi-armed bandit learning. We build upon
recent works that extend the classical bandit setting to economics in which there are two players
interacting via principal-agent protocols [Dogan et al., 2023b,a, Scheid et al., 2024]. This allows us
to capture, for example, a version of the two-firm problem where firms are uncertain regarding both
their profit functions and the degree of externality on other firms. We represent production decisions
in this problem as arms which can be played by the firms at any round over time, with the goal of
finding decisions that maximize their rewards. More precisely, we assume that the reward of the
upstream firm only depends on its own action, while the reward of the downstream firm depends on
both its action and the upstream firmâ€™s action. This dependency on both actions allows us to capture
externalities.
The property rights in Example 1, or more generally over a bandit instance, amount to giving a
firm the possibility of engaging in monetary transfers that influence the arms that are played over
time. The owner of the bandit instance will face a problem of bandit learning with transfers with an
upstream player who is also learning his preferences.
To account for the efficiency of a policy in this setup, we extend the classical static notion of welfare
efficiency to the online setting. We say that a policy is Welfare efficient if the social welfare regret is
sub-linear. Proving the Coase theorem within our setup therefore boils down to show that if the bandit
owner (who is without loss of generality the upstream player in this study1) runs a no-regret bandit
algorithm to learn and exploit his preferences, the downstream player can then choose an optimal
transfer scheme leading to a sub-linear total regret.
Our contributions are as follows:
â€¢We show that when an upstream agent exerts externality on a downstream agent, in the
absence of property right, the social welfare breaks down. Put differently, no joint policy of
the agents can be welfare efficient.
â€¢We then introduce property rights and show how it affects the game. In this case, bargaining
and transfers are available to the players. We propose a policy for the downstream player
that leads to welfare efficiency when the upstream player follows any black-box no-regret
policy under mild assumption. This solution addresses the breakdown issue at equilibrium.
Put together, we show an online version of the Coase theorem .
1The fact that efficiency is restored whoever is given the property rights is known as the invariance property
of the Coase theorem.
22 Setup and Inefficiency of Externality
2.1 Bandit game
We consider a sequential bandit game in which two players (downstream and upstream) simulta-
neously play actions in a bandit instance for a horizon TâˆˆNâ‹†. The action set for both players is
A={1, . . . , K }, KâˆˆNâ‹†.
The reward distributions of the agents differ. Given a family of distributions {Î³a:aâˆˆ A} indexed
byA, the upstream playerâ€™s rewards are provided by an i.i.d. family of random variables
{(Za(t))tâˆˆ[T]:aâˆˆ A} ,where Za(t)âˆ¼Î³afor any tâˆˆ[T]andaâˆˆ A.
To model the externality exerted by the upstream on the downstream player, we assume that the latter
has a reward that depends both on her action and on that of the upstream player. Formally, this is
modeled through a family of distributions {Î½a,b:a, bâˆˆ A} double-indexed by Aand an i.i.d. family
of random variables
{(Xa,b(t))tâˆˆ[T]:a, bâˆˆ A} ,where Xa,b(t)âˆ¼Î½a,b
is the reward received by the downstream player at time tif she pulls the arm band the upstream
player pulls the arm a.
Players. We assume that players are risk-neutral expected-utility maximizers, and we define their
expected utilities for any (a, b)âˆˆ A Ã— A as
vup(a) =Z
z Î³a(dz)âˆˆRand vdown(a, b) =Z
x Î½a,b(dx)âˆˆR.
The distributions (Î³a)aâˆˆAand(Î½a,b)(a,b)âˆˆA2are unknown to both the downstream and the upstream
players and they aim to learn the distributions with best mean rewards by sequentially observing
samples from {(Za(t))tâˆˆ[T]:aâˆˆ A} and{(Xa,b(t))tâˆˆ[T]:a, bâˆˆ A} .
Moreover, we suppose that players are rational in hindsight; that is, they minimize their regret.
Formally, the upstream player aims to minimize his regret defined as
Rup
n(T,Î up
n) =TÂµâ‹†,upâˆ’E"TX
t=1vup(At)#
,where Âµâ‹†,up= max
aâˆˆAvup(a), (1)
while the downstream player seeks to minimize her external regret defined as
Rdown
n(T,Î up
n,Î down
n) =E"TX
t=1max
bâˆˆAvdown(At, b)âˆ’vdown(At, Bt)#
, (2)
where the playersâ€™ actions (At)tâˆˆ[T],(Bt)tâˆˆ[T]as well as their policies Î up
n,Î down
n are defined below.
Note that the utility of the downstream player also depends on the actions taken by the upstream
player, which represents the externality exerted by the upstream player on the downstream player,
hence the strategic dimension of our setting. We first consider a game where no property right is
defined, so each player is free to pick his preferred arm irrespectively of the other playerâ€™s choice.
This will result in a breakdown of the total utility.
Policies without property rights. Consider first the upstream player. Based on a policy Î up
n(for
example a no-regret bandit algorithm such as the Upper Confidence Bounds algorithm ( UCB)
[Auer, 2002] or the Îµ-greedy algorithm [Robbins, 1952, Langford and Zhang, 2007]), we define
his history (Hup,n
t)tâˆˆ[T]by induction. We set Hup,n
0=âˆ…and supposing that Hup,n
tis defined for
tâˆˆ[T], then
Hup,n
t+1=Hup,n
tâˆª
At+1, Vt+1, ZAt+1(t+ 1)	
,
where (Vs)sâˆˆNâ‹†is a family of independent uniform random variables in [0,1], allowing for random-
ization in the policy, and At+1is provided by Î up
n, following Î up
n: (Vt+1,Hup,n
t)7â†’At+1.
Second, consider the downstream player and an algorithm Î down
n (specifically a no-regret bandit
algorithm). We define her history (Hdown ,n
t )tâˆˆ[T]by induction. We set Hdown ,n
0 =âˆ…and supposing
thatHdown ,n
t is defined for tâˆˆ[T], then
Hdown ,n
t+1 =Hdown ,n
t âˆª
At+1, Bt+1, Ut+1XAt+1,Bt+1(t+ 1)	
,
3where (Us)sâˆˆNâ‹†is a family of independent uniform random variables in [0,1]allowing for random-
ization in the policy and Bt+1is provided by Î down
n , following Î down
n: (Ut+1,Hdown ,n
t )7â†’Bt+1.
Welfare efficiency. We now introduce the notion of Welfare efficiency for our setup. The global
utility, or social welfare , of the players at round tis defined as vup(At) +vdown(At, Bt). We define
thesocially optimal action (asw, bsw)âˆˆ A Ã— A of the game as
(asw, bsw)âˆˆargmaxa,bâˆˆAvup(a) +vdown(a, b), (3)
as well as the global regret (orsocial welfare regret ) associated with policies Î up
nandÎ down
n as
Rsw(T,Î up
n,Î down
n) =T 
vup(asw) +vdown(asw, bsw)
âˆ’TX
t=1E
vup(At) +vdown(At, Bt)
.
(4)
Then, the joint policies Î up
nandÎ down
n for the players are said to be Welfare efficient if
lim
Tâ†’+âˆžRsw(T,Î up
n,Î down
n)/T= 0.
Intuitively, this condition implies that the frequency of the socially optimal action (asw, bsw)tends
to 1 as Tgoes to infinity. In this sense, it mimics the usual, static Welfare efficiency criterion. As
we will see, (Î up
n,Î down
n)is typically not Welfare efficient when there is a disalignment in the game
between the playersâ€™ individual interests based on their rationality and the social welfare.
2.2 Inefficiency without property rights
We first present a result that captures the adverse consequence of externality on social welfare. The
upstream player does not take into account the indirect cost incurred by the downstream player when
he chooses his action. This drives the social welfare away from its optimal level. We illustrate this
fact within our simple bilateral externality example.
Example 1 (continuing from p. 1) .We show that the competitive outcome, where each firm maximizes
its profit independently, is not welfare efficient in the presence of externality. Define the social welfare
as the function
W: (q1, q2)7â†’Ï€1(q1) +Ï€2(q1, q2) =p(q1+q2)âˆ’(c1(q1) +c2(q2))âˆ’Î±q1. (5)
By definition, the welfare efficient outcome (qâ‹†
1, qâ‹†
2)âˆˆR2
+satisfies W(qâ‹†
1, qâ‹†
2)â©¾W(q1, q2)for any
(q1, q2)âˆˆR2
+. Since Wis differentiable and strictly concave, (qâ‹†
1, qâ‹†
2)is uniquely defined by the
condition âˆ‡W(qâ‹†
1, qâ‹†
2) = 0 , that is
câ€²
1(q1)âˆ’Î±=pand câ€²
2(q2) =p . (6)
Note that at the welfare efficient optimum, firm 1 does not equalize marginal cost with marginal profit,
but produces less to account for the negative effect of externality on firm 2. We now characterize the
competitive outcome (qâ€²
1, qâ€²
2)âˆˆR2
+. Since Ï€1andÏ€2are differentiable and strictly concave, (qâ€²
1, qâ€²
2)
satisfies
câ€²
1(qâ€²
1) =pand câ€²
2(qâ€²
2) =p . (7)
For the competitive outcome to be welfare efficient, we require, by Equation (6)and Equation (7),
câ€²
1(qâ€²
1)âˆ’Î±=câ€²
1(qâ€²
1),that is Î±= 0.
This proves that whenever there are externalities, no competitive outcome is efficient.
We now show that in our model, when there is no property right and under mild assumptions,
no achievable policy is welfare efficient whenever there is a misalignment between the playersâ€™
interests and the social welfare. The upstream playerâ€™s policy Î up
nis said to be no-regret if
limTâ†’+âˆžRup
n(T,Î up
n)/T= 0, where Rup
nis defined in (1).
Theorem 2. Suppose that argmaxaâˆˆAvup(a)is the singleton {au
â‹†}and that
vup(asw) +vdown(asw, bsw)âˆ’vup(au
â‹†) +vdown(au
â‹†, b)>0, (8)
for any bâˆˆ A. In the absence of property rights and when the upstream player runs any no-regret
policy Î up
n, we have Rsw(T,Î up
n,Î down
n) = â„¦( T). Therefore, Rsw(T,Î up
n,Î down
n) = â„¦( T)and
(Î up
n,Î down
n)is not welfare efficient.
4Condition (8)in Theorem 2 represents the unalignment between the upstream playerâ€™s preference
and the optimal choice from a social welfare point of view. Note that the upstream and downstream
players can both have an o(T)external regret, while the social welfare regret still grows linearly
withTbecause of the unfavorable interactions between their policies.
3 Online Property Game with Bargaining Players
3.1 Online Property Game
We now consider the same repeated game in the form of a property game where one of the players
possesses the bandit instance ( upstream player ). As in the original setup of Coase [2013], the other
player ( downstream player ) will provide the bandit owner with transfers to incentivize him to choose
some specific action and influence the outcome of the game in her favor.
We show in Appendix B that our method applies similarly when property rights are given to the
upstream player rather than the downstream player. Hence, there is no loss of generality in considering
the aforementioned framework. In this sense, we recover the invariance property of the Coasean
bargaining [Mas-Colell et al., 1995].
Example 1 (continuing from p. 1) .We now illustrate how Coasean bargaining re-instaures efficiency.
Suppose without loss of generality that property rights are such that firm 2 can pay Ï„âˆˆR+to firm 1
for it to operate at a level Ëœq1. Profits become
Â¯Ï€2: (q1, q2, Ï„,Ëœq1)7â†’Ï€2(q1, q2)âˆ’ 1{q1=Ëœq1}Ï„and Â¯Ï€1: (q1, Ï„,Ëœq1)7â†’Ï€(q1) + 1{q1=Ëœq1}Ï„.
Consider the competitive outcome (q1, q2, Ï„,Ëœq1)âˆˆR4
+which satisfies
q1=q1(Ï„,Ëœq1)âˆˆarg max
qâ€²
1â©¾0Â¯Ï€1(qâ€²
1, Ï„,Ëœq1)and
Â¯Ï€2(q1(Ï„,Ëœq1), q2, Ï„,Ëœq1) = max
qâ€²
2,Ï„â€²,Ëœqâ€²
1Â¯Ï€2(q1(Ï„â€²,Ëœqâ€²
1), qâ€²
2, Ï„â€²,Ëœqâ€²
1).
The condition on q1accounts for the rationality of the firm 1and the fact that its choice depends on
the payment (Ï„,Ëœq1). Obviously, the optimal solution is reached for Ëœq1=q1andÏ„= max qâ€²Ï€1(qâ€²)âˆ’
Ï€1(Ëœq1). Plugging this back in the expression of Â¯Ï€2then yields
(q1, q2) = argmaxqâ€²
1,qâ€²
2Ï€1(qâ€²
1) +Ï€2(qâ€²
2) = argmaxqâ€²
1,qâ€²
2W(qâ€²
1, qâ€²
2),
so the competitive outcome (q1, q2)is welfare efficient.
The transfers at each step can be interpreted as a contract between two players [see, e.g., Bolton and
Dewatripont, 2004, SalaniÃ©, 2005, for general contract theory] and providing the right amount of
incentives relates to adjusting a contract in an online setting [see DÃ¼tting et al., 2019, Guruganesh
et al., 2021, Zhu et al., 2022, Fallah and Jordan, 2023, Guruganesh et al., 2024, Ananthakrishnan
et al., 2024, for learning-based perspectives about contracts].
Similarly to Example 1, we modify the playersâ€™ policies to now account for the transfer Ï„(t)that the
downstream player offers at round tto the upstream player if he picks action Ëœat. The downstream
playerâ€™s policy at round tdoes not only output an arm Btbut now a triple (Ëœat, Ï„(t), Bt), where Bt
is the arm that she should play and Ëœatis the arm on which a transfer Ï„(t)is offered to the upstream
player. On the upstream playerâ€™s side, the policy still outputs an arm Atto play but also takes as an
input the incentive (Ëœat, Ï„(t)). In addition, the instantaneous utility of the upstream player becomes
ZAt(t) +1Ëœat(At)Ï„(t), whereas the downstream player receives XAt,Bt(t)âˆ’1Ëœat(At)Ï„(t).
Policies with property rights. Based on policies Î up
pfor the upstream player and Î down
p for the
downstream player, we define their histories (Hup,p
t)tâˆˆ[T]and(Hdown ,p
t )tâˆˆ[T]by induction. We set
Hup,p
0=âˆ…,Hdown ,p
0 =âˆ…and supposing that Hup,p
t,Hdown ,p
t are defined for tâˆˆ[T], then
Hup,p
t+1=Hup,p
tâˆª
Ëœat+1, Ï„(t+ 1), At+1, Vt+1, ZAt+1(t+ 1)	
and
Hdown ,p
t+1 =Hdown ,p
t âˆª
Ëœat+1, Ï„(t+ 1), At+1, Bt+1, Ut+1, XAt+1,Bt+1(t+ 1)	
,
5where (Vs)sâˆˆNâ‹†,(Us)sâˆˆNâ‹†are two families of independent uniform random variables in
[0,1]allowing for randomization in the policies, and the remaining quantities are given by
Î down
p: (Ut+1,Hdown ,p
t )7â†’(Ëœat+1, Ï„(t+1), Bt+1)andÎ up
p: (Ëœat+1, Ï„(t+1), Vt+1,Hup,p
t)7â†’At+1.
Playersâ€™ goal. Given a transfer Ï„from the downstream to the upstream player on arm Ëœa, actions aand
brespectively are chosen by the upstream and the downstream player, the upstream playerâ€™s expected
utility reads vup(a) +1Ëœa(a)Ï„while the downstream playerâ€™s expected utility is vdown(a, b)âˆ’1Ëœa(a)Ï„.
This defines the upstream playerâ€™s expected regret for a horizon Tas
Rup
p(T,Î up
p,Î down
p) =E"TX
t=1max
aâˆˆA{vup(a) +1Ëœat(a)Ï„(t)} âˆ’(vup(At) +1Ëœat(At)Ï„(t))#
.(9)
Based on the upstream playerâ€™s utility, the downstream player aims on a single round at proposing an
optimal transfer Ï„opton an arm aoptâˆˆ A as well as picking an arm boptâˆˆ A which solves
maximize (a, b, Ï„ )7â†’vdown(a, b)âˆ’Ï„
such that Ï„âˆˆR+, bâˆˆ A, aâˆˆargmaxaâ€²âˆˆA{vup(aâ€²) +1a(aâ€²)Ï„}.(10)
Her regret for any horizon Tis defined as
Rdown
p(T,Î up
p,Î down
p) =TÂµâ‹†,downâˆ’E"TX
t=1vdown(At, Bt)âˆ’1Ëœat(At)Ï„(t)#
, (11)
where we define Âµâ‹†,down=vdown(aopt, bopt)âˆ’Ï„optas the optimal utility she can aim for. We can
see that the downstream playerâ€™s influence is exerted through her action choice Btas well as through
transfers which enable her to influence the upstream playerâ€™s actions. Hence, the notion of external
regret is obsolete here. The game has now the form of a repeated Stackelberg game [V on Stackelberg,
2010].
Lemma 1. Recall that Âµâ‹†,downis the downstream playerâ€™s optimal reward as defined as a solution
of(10). We have Âµâ‹†,down= max a,bâˆˆA{vdown(a, b) +vup(a)} âˆ’max aâ€²âˆˆA{vup(aâ€²)}, as well as
(aopt, bopt) = (asw, bsw)andÂµâ‹†,up+Âµâ‹†,down=vup(asw)+vdown(asw, bsw) = max a,bâˆˆA{vup(a)+
vdown(a, b)}, where Âµâ‹†,upis defined in Equation (1). Moreover, for any integer TâˆˆNâ‹†, and policies
Î up
p,Î down
p , we have that
Rsw(T,Î up
p,Î down
p)â©½Rup
p(T,Î up
p,Î down
p) +Rdown
p(T,Î up
p,Î down
p).
This lemma has an interesting economic interpretation: if both players individually seek for their
own interest within this online property game, they will together converge towards the optimal global
utility. Individual rationality moves the outcome of the game towards the optimal social welfare. The
transfers allow the players to align their goals and share the global reward, in line with the Coase
theorem. Consequently, if both players run no-regret policies Î up
pandÎ down
p, the social welfare
regret will also be in o(T). The rest of the paper shows that such no-regret policies exist. To this end,
we introduce the following assumptions.
Without loss of generality, we assume that the upstream playerâ€™s utility is rescaled and shifted, which
corresponds to the following assumption on the reward distribution (Î³a)aâˆˆAinR.
H1. For any aâˆˆ A, we have vup(a)âˆˆ[0,1].
We now make a high probability bound assumption on the upstream playerâ€™s regret.2.
H2.There exist C, Î¶ > 0, Îºâˆˆ[0,1)such that for any s, tâˆˆ[T]withs+tâ©½T, any{Ï„a}aâˆˆ[K]âˆˆRK
+
and any policy Î down
p that offers almost surely a transfer (Ëœal, Ï„(l)) = (Ëœ al, Ï„Ëœal)for any lâˆˆ {s+
1, . . . , s +t}, the batched regret of the upstream player following Î up
psatisfies, with probability at
least1âˆ’tâˆ’Î¶,
s+tX
l=s+1max
aâˆˆA{vup(a) +1Ëœal(a)Ï„Ëœal} âˆ’(vup(Al) +1Ëœal(Al)Ï„Ëœal)â©½CtÎº.
2A similar assumption is made in the work of Donahue et al. [2024] but with a stronger instantaneous regret
bound which does not encompass the UCBâ€™s regret bound.
6The constraint on the downstream playerâ€™s algorithm Î down
p enforces constant incentives associated
with any arm aâˆˆ A withtin the batch, while the incentivized actions (Ëœal)lâˆˆ{s+1,...,s+t}may change.
Proposition 2 in Appendix C shows that an adaptation of UCBtaking account the incentives satisfies
H2 with C = 8p
Klog(KT3),Îº= 1/2andÎ¶= 2. Note that usual bandit algorithms such as AAE,
ETCorEXP-IX also satisfy the assumption [see, e.g., Donahue et al., 2024, Lattimore and SzepesvÃ¡ri,
2020].
3.2 Downstream playerâ€™s procedure
We fix the policy Î up
pwhich can be any algorithm satisfying H2 for the upstream player and introduce
the algorithm BELGIC (Bandits and Externalities for a Learning Game with Incentivized Coase)
which provides a policy achieving sub-linear regret for the downstream player. It can be seen as an
online bargaining strategy to mitigate externalities. Simply put, BELGIC unfolds in two steps. First
note that for any action aâˆˆ A, the optimal (lowest) transfer to offer to the upstream player to make
him choose ais
Ï„â‹†
a= max
aâ€²âˆˆAvup(aâ€²)âˆ’vup(a), (12)
as detailed in Appendix A. Therefore, a batched binary search procedure (Algorithm 2) first allows
the downstream player to estimate the optimal transfers Ï„â‹†
1, . . . , Ï„â‹†
Kwith a good precision level of
1/TÎ², where Î² >0. More precisely, the downstream player offers a constant incentive (Ëœa, Ï„Ëœa)for a
batch of time steps of length ËœT=âŒˆTÎ±âŒ‰. The observation of TÌ¸=
Ëœa, the number of steps from the batch
for which the upstream player does not pick Ëœaallows her to estimate whether Ï„Ëœais above or below Ï„â‹†
Ëœa
and adjust it, following Lemma 2 in Appendix A under the condition that Î±, Î² satisfy
Î²/Î± < (1âˆ’Îº). (13)
The procedure needs to be run for KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰rounds since we have to make âŒˆlog2TÎ²âŒ‰batches
of binary search of length âŒˆTÎ±âŒ‰on each of the Karms [see Scheid et al., 2024]. This corresponds to
the first phase of BELGIC as described in Algorithm 2. At the end of this stage, the estimated transfers
(Ë†Ï„a)aâˆˆAsatisfy the bound in Proposition 1. These are then used to feed the subroutine Bandit-Alg .
Proposition 1. UnderH1 andH2, after the first phase of BELGIC which consists in KâŒˆTÎ±âŒ‰âŒˆlogTÎ²âŒ‰
steps of binary search grouped in âŒˆlog2TÎ²âŒ‰batches per arm aâˆˆ A, we have that
P
for any aâˆˆ A,Ë†Ï„aâˆ’4/TÎ²âˆ’CT(Îºâˆ’1)/2â©½Ï„â‹†
aâ©½Ë†Ï„a
â©¾1âˆ’KâŒˆlog2TÎ²âŒ‰/TÎ±Î¶.
The additional term 1/TÎ²+ CTÎºâˆ’1inË†Ï„aensures that if H2 holds, the upstream player necessarily
plays the incentivized action Ëœatat round twith high probability. Lemmas 2 and 6 from Appendix C
show how the binary search batches in BELGIC allow us to estimate Ï„â‹†
adepending on ËœTâˆ’TÌ¸=
a, the
number of times that arm ahas been pulled by the upstream player during the batch.
Then, any bandit subroutine Bandit-Alg , such as UCBorÎµ-greedy , for instance, can be run in a
black-box fashion on the shifted bandit instance, where the rewards are shifted by the upper estimated
transfers (Ë†Ï„a)aâˆˆA. The downstream player computes a shifted history ËœHdown ,p
t such that for any
tâ©½KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰,ËœHdown ,p
t =âˆ…and for any t > KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰
ËœHdown ,p
t =(
{Ëœat, Bt, Ï„(t), Ut, XËœat,Bt(t)âˆ’Ë†Ï„Ëœat} âˆªËœHdown ,p
tâˆ’1 ifËœat=At
ËœHdown ,p
tâˆ’1 otherwise ,(14)
which serves to feed Bandit-Alg , following
Bandit-Alg : (Ut,ËœHdown ,p
tâˆ’1)7â†’(Ëœat, Bt)âˆˆ A Ã— A . (15)
For any family of constant incentives {Ï„a}aâˆˆAâˆˆRK
+, we define RBandit-Alg (T, Î½,{Ï„a}aâˆˆA)as the
regret for the downstream playerâ€™s subroutine Bandit-Alg on the bandit instance with shifted means
overTrounds, following
RBandit-Alg (T, Î½,{Ï„a}aâˆˆA) =Tmax
a,bâˆˆA2E
vdown
a,b(1)âˆ’Ï„a
âˆ’E"TX
t=1vdown(Ëœat, Bt)âˆ’Ï„Ëœat#
.
Note that here, Bandit-Alg aims to maximize the shifted reward (vdown(a, b)âˆ’Ï„a)(a,b)âˆˆA2.
7Algorithm 1 BELGIC
1:Input: Set of actions A= [K], time horizon T, subroutine Î up
p, upstream playerâ€™s regret
constants C, Îº, parameters Î±andÎ².
2:Compute ËœHdown ,p
s =âˆ…for any sâ©½KâŒˆlog2TÎ²âŒ‰âŒˆTÎ±âŒ‰.
3:foraâˆˆ A do
4: # See Algorithm 2
5: Ï„a,Ï„a=Binary Search (a,âŒˆlog2TÎ²âŒ‰,âŒˆTÎ±âŒ‰,0,1)
6:end for
7:For any action aâˆˆ A,Ë†Ï„a=Ï„a+ 1/TÎ²+ CT(Îºâˆ’1)/2.
8:fort=KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰+ 1, . . . , T do
9: Get recommended actions by Bandit-Alg on the A Ã— A bandit instance, (Ëœat, Bt) =
Bandit-Alg (Ut,ËœHdown ,p
tâˆ’1).
10: Offer a transfer Ë†Ï„Ëœaton action Ëœat, nothing for any other action aâ€²âˆˆ A and play action Bt.
11: Observe At= Î up
p(Ëœat+1, Ï„(t+ 1), Vt,Hup,p
tâˆ’1), XËœat,Bt(t)
12: ifAt= Ëœatthen update history ËœHdown ,p
t .
13: end if
14: Update upstream playerâ€™s history Hup,p
t.
15:end for
Algorithm 2 Binary Search Subroutine
1:Input: action a, N T,ËœT, Ï„a,Ï„a.
2:ford= 0, . . . , N Tâˆ’1do
3: Compute Ï„mid
a= (Ï„a(d) +Ï„a(d))/2,TÌ¸=
a= 0.
4: fort=dËœT+ 1, . . . , d ËœT+ËœTdo
5: Propose transfer Ï„mid
a(d)on arm aand nothing for any other action aâ€²âˆˆ A.
6: At= Î up
p(t, Ï„mid(a), a, V t,Hup,p
tâˆ’1)
7: ifAtÌ¸=athen :TÌ¸=+ = 1
8: end if
9: Update upstream playerâ€™s history Hup,p
t.
10: end for
11: ifCËœTÎº+Î²/Î±< TÌ¸=
a<ËœTâˆ’CËœTÎº+Î²/Î±then Return Ï„a(d),Ï„a(d).
12: else if TÌ¸=
aâ©½ËœTâˆ’CËœTÎº+Î²/Î±thenÏ„a(d) =Ï„mid
a(d) + 1/TÎ²and update history ËœHdown ,p
t .
13: elseÏ„a(d) =Ï„mid
a(d)âˆ’1/TÎ²and update history ËœHdown ,p
t .
14: end if
15:end for
Theorem 3. Assume that H1 andH2 hold. Then BELGIC , run with Î±, Î² satisfying (13) and any
bandit subroutine Bandit-Alg , has an overall regret Rdown
p such that
Rdown
p(T,Î up
p,BELGIC )â©½2(3 + 2C + Â¯ vâˆ’v) log2(T)(2T1âˆ’Î±Î¶+T(Îº+1)/2+âŒˆTÎ±âŒ‰) + 4T1âˆ’Î²
+RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA)
where, for ease of notation
Â¯v= max
a,bâˆˆAÃ—A{vdown(a, b)}and v= min
a,bâˆˆAÃ—A{vdown(a, b)}.
Knowledge of CandÎº.An upper bound on CandÎºis sufficient to compute the hyperparameters in
BELGIC . Theorem 3 shows that the bigger CandÎºare, the worse is the downstream playerâ€™s regret,
hence the interest of knowing them more precisely.
Corollary 1. Assume that the upstream playerâ€™s distribution (Î³a)aâˆˆAis such that H1 holds. In
addition, suppose that the distributions (Î³a)aâˆˆAand(Î½a,b)a,bâˆˆAÃ—A are1-sub-Gaussian and that the
upstream player plays Î up
p=Algorithm 3(a slight modification of UCBto take into account the
incentives). Then the downstream playerâ€™s regret when she runs BELGIC with parameters Î±= 3/4
andÎ²= 1/4(which satisfy (13)) and subroutine Bandit-Alg =UCBsatisfies the following upper
8bound3
Rdown
p(T,UCB,BELGIC )â©½(10 + 4 K+ 32p
Klog2(KT3) + Â¯vâˆ’v) log2(T)(3 + 2 T3/4)
+ 3K2(Â¯vâˆ’v).
The upper bound on the social welfare regret in Lemma 1 together with Corollary 1 shows that when
the upstream player runs Î up
p=UCBand the downstream player runs BELGIC , the social welfare
regret then satisfies Rsw(T,Î up
p,BELGIC ) =O(Klog(T)T(Îº+1)/2).
In other words, if the downstream player runs BELGIC which produces a policy Î n
down, for any
upstream policy Î up
p,(Î n
down,Î up
p)is welfare efficient.
Influence of the upstream performance. It is interesting to note that in the downstream playerâ€™s
regret bound, the upstream playerâ€™s regret bound in O(TÎº)plays a significant role: the downstream
player never learns faster than the upstream player. The latterâ€™s performance determines the social
welfare convergence rate towards the social optimum. We can observe that the playersâ€™ bounded
rationality [Selten, 1990, Jones, 1999] and personal interest make the game converge towards the
optimal social welfare equilibriumâ€”even though they are both learning here.
Figure 1: Empirical frequencies of the upstream playerâ€™s actions when property rights are not defined
(left) and when they are defined (right).
Experiments. We conclude this section with experiments showing the empirical convergence of our
algorithm to a social optimum. In the simulation, we consider two firms, with firm 1 being upstream
and firm 2 being downstream. Their profit functions are respectively given by
Ï€1:7â†’max
q1âˆ’2q1
102
+ 2q1
10,0
,
and
Ï€2(q1, q2)7â†’max(
âˆ’16q1
10âˆ’6
102
+ 8
q1âˆ’6
102
+1
50q2,0)
.
Thus, firm 1â€™s and firm 2â€™s profit functions depends quadratically on q1with an firm 1optimum at
qâ€²
1= 5and a social optimum at qâ‹†
1= 8. Note that in the expression of Ï€2,q2has very little influence
as compared to q1- which allows to plot profits for only one value of q2.
We discretize the setup, consider a bandit instance (horizon T= 5.106,10arms, average over 10
rounds) and we assume that UCBis used as a subroutine. In the first setting, there are no property
rights and each firm runs UCBon their side. Second, property rights are defined and firm 2 runs
BELGIC as its policy. The plots in Figure 1 display the empirical frequencies and show empirically
the effectiveness of BELGIC to mitigate externalities.
4 Related work
Our work addresses the impact of externalities and is therefore related to taxation theory [see, e.g.,
Mirrlees et al., 2011, Salanie, 2011, and the references therein], a prominent solution for this issue, as
3Note that it is Kand notâˆš
Khere, since the action space is of cardinality K2for the downstream player.
9exemplified by the Pigouvian tax [see Pigou, 2017]. Taxation is a fundamental aspect of all developed
economies, with 30% to50% of national income derived from taxes. The topic has been fruitful
for various scenarios, including the carbon tax [Carattini et al., 2018, Metcalf and Weisbach, 2009],
alcohol markets [Griffith et al., 2019], or business taxation [Boadway and Bruce, 1984]. Taxation can
also be studied through an operations research lens, where it is used to enhance system efficiency or
manage specific games [Roughgarden, 2010, Caragiannis et al., 2010, BilÃ² and Vinci, 2019]. Recent
work by Cui et al. [2024] explores online mechanisms to maximize efficiency in congestion games.
Mechanism designs [Myerson, 1989, Nisan and Ronen, 1999, Laffont and Martimort, 2009] allow to
design games that have specific desired outcomes. Deploying these mechanisms in their classical
ecnomical form assume that playersâ€™ utility functions are known a priori, which is often unrealistic.
There is a major need to blend mechanism design with machine learning.
However, our approach differs, since, drawing inspiration from Coaseâ€™s theory, we implement an
online version of his theorem [Coase, 2013, Cooter, 1982], incorporating uncertainty to tackle
the breakdown of social welfare in an online setting. We use the bandit setup [see Lattimore and
SzepesvÃ¡ri, 2020, Slivkins et al., 2019] as a general and convenient way to model the game introduced
by Coase. However, our work differs from considering a single agent playing a bandit game. Instead,
we focus on the more general problem of multi-players bandits, a field receiving a growing attention
from the community [see e.g., Boursier and Perchet, 2019, 2022, Sankararaman et al., 2019].
Our approach is inspired by the principal-agent model introduced by Dogan et al. [2023b], which
was further extended by Dogan et al. [2023a], Scheid et al. [2024]. However, unlike the models
proposed in the work of Dogan et al. [2023b,a], we do not specify a particular bandit algorithm for the
upstream player and instead, we allow him to use any no-regret algorithm satisfying H2 in a black-box
fashion. Conversely, the model of Scheid et al. [2024] assumes that the upstream player is always
fully informed and best-responding, whereas we assume that he is also learning. Chen et al. [2023b]
leverages a similar model to study information acquisition by a principal through an agentâ€™s actions.
However, in their model, the agent is also almighty and knows exactly the costs associated with each
action. Designing incentives in an unknown environment is related to auction theory incorporating
uncertainty, as it is explored in the work of Feng et al. [see 2018], Li et al. [see 2023]. Similar issues
have been explored in the Reinforcement Learning framework within a leader-follower game [see
Chen et al., 2023a, with quantal responses by the follower] or in a principal-agent game with incentive
design as done by Ben-Porat et al. [2023]. Donahue et al. [2024] also study a two-players repeated
Stackelberg game on a bandit instance but instead of allowing for transfers, their main focus concerns
the achievability of a Stackelberg equilibrium through iterations of bandit policies: the same kind of
goal also appears in Collina et al. [2023]. Such principal-agent setups are of some interest to model
various real-world situations such as the design of fundings for hospitals [Wang et al., 2024] or have
been studied with multiple agents through the lens of auction design in dynamic setups [Bergemann
and Said, 2010, Chen et al., 2023c], or to account for fairness [Fallah et al., 2024].
In our game, the downstream player needs to learn the optimal transfers/incentives to offer to the
upstream player. This is related to the Incentivized Exploration literature [Mansour et al., 2016,
Simchowitz and Slivkins, 2023, Esmaeili et al., 2023], which is often cast in terms of a benevolent
planner who aims to optimize the global welfare of agents via plausible recommendations. A related
model is Bayesian Persuasion [Kamenica and Gentzkow, 2011], where a sender influences a receiverâ€™s
action through sending a signal. This model has begun to be studied in learning settings [see, e.g.,
Castiglioni et al., 2020, Bernasconi et al., 2022, Wu et al., 2022b,a].
5 Conclusion
This paper studies a model of externalities in a two-players sequential game where both players learn
their optimal actions. We first show that when the players act independently, then a misalignment
between the playersâ€™ interests and the social welfare leads to a breakdown of the global utility. We
then introduce interactions through transfers, which restores a social welfare optimum, representing
the online version of the Coase theorem . To that purpose, we propose a policy for the downstream
player which allows her to estimate the optimal transfers as well as choosing the best actions. The
mathematical difficulty comes from the learning aspect on both sides. Since our work is coined in a
learning framework for mechanism design, several directions for research are open, as for instance
extensions to the multi-agent setting, which raises many questions.
10Acknowledgements
Funded by the European Union (ERC, Ocean, 101071601). Views and opinions expressed are
however those of the author(s) only and do not necessarily reflect those of the European Union or
the European Research Council Executive Agency. Neither the European Union nor the granting
authority can be held responsible for them.
References
Jens Abildtrup, Frank Jensen, and Alex Dubgaard. Does the coase theorem hold in real markets?
an application to the negotiations between waterworks and farmers in denmark. Journal of
environmental management , 93(1):169â€“176, 2012.
Nivasini Ananthakrishnan, Stephen Bates, Michael Jordan, and Nika Haghtalab. Delegating data
collection in decentralized machine learning. In International Conference on Artificial Intelligence
and Statistics , pages 478â€“486. PMLR, 2024.
Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
Learning Research , 3(Nov):397â€“422, 2002.
Omer Ben-Porat, Yishay Mansour, Michal Moshkovitz, and Boaz Taitler. Principal-agent reward
shaping in mdps. arXiv preprint arXiv:2401.00298 , 2023.
Dirk Bergemann and Maher Said. Dynamic auctions: A survey. Wiley Encyclopedia of Operations
Research and Management Science , 2010.
Martino Bernasconi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, and Francesco TrovÃ².
Sequential information design: Learning to persuade in the dark. Advances in Neural Information
Processing Systems , 35:15917â€“15928, 2022.
Vittorio BilÃ² and Cosimo Vinci. Dynamic taxes for polynomial congestion games. ACM Transactions
on Economics and Computation (TEAC) , 7(3):1â€“36, 2019.
Robin Boadway and Neil Bruce. A general proposition on the design of a neutral business tax.
Journal of Public Economics , 24(2):231â€“239, 1984.
Patrick Bolton and Mathias Dewatripont. Contract theory . MIT press, 2004.
Etienne Boursier and Vianney Perchet. Sic-mmab: Synchronisation involves communication in
multiplayer multi-armed bandits. Advances in Neural Information Processing Systems , 32, 2019.
Etienne Boursier and Vianney Perchet. A survey on multi-player bandits. arXiv preprint
arXiv:2211.16275 , 2022.
SÃ©bastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic
multi-armed bandit problems. Foundations and Trends Â®in Machine Learning , 5(1):1â€“122, 2012.
James M Buchanan and Wm Craig Stubblebine. Externality. In Inframarginal Contributions to
Development Economics , pages 55â€“73. World Scientific, 2006.
Ioannis Caragiannis, Christos Kaklamanis, and Panagiotis Kanellopoulos. Taxes for linear atomic
congestion games. ACM Transactions on Algorithms (TALG) , 7(1):1â€“31, 2010.
Stefano Carattini, Maria Carvalho, and Sam Fankhauser. Overcoming public resistance to carbon
taxes. Wiley Interdisciplinary Reviews: Climate Change , 9(5):e531, 2018.
Matteo Castiglioni, Andrea Celli, Alberto Marchesi, and Nicola Gatti. Online bayesian persuasion.
Advances in Neural Information Processing Systems , 33:16188â€“16198, 2020.
Siyu Chen, Mengdi Wang, and Zhuoran Yang. Actions speak what you want: Provably sample-
efficient reinforcement learning of the quantal stackelberg equilibrium from strategic feedbacks.
arXiv preprint arXiv:2307.14085 , 2023a.
11Siyu Chen, Jibang Wu, Yifan Wu, and Zhuoran Yang. Learning to incentivize information acquisition:
Proper scoring rules meet principal-agent model. In International Conference on Machine Learning ,
pages 5194â€“5218. PMLR, 2023b.
Yurong Chen, Qian Wang, Zhijian Duan, Haoran Sun, Zhaohua Chen, Xiang Yan, and Xiaotie Deng.
Coordinated dynamic bidding in repeated second-price auctions with budgets. In International
Conference on Machine Learning , pages 5052â€“5086. PMLR, 2023c.
Ronald Coase. The problem of social cost. Journal of Law and Economics , 56(4):837 â€“ 877, 2013.
URL https://EconPapers.repec.org/RePEc:ucp:jlawec:doi:10.1086/674872 .
Natalie Collina, Eshwar Ram Arunachaleswaran, and Michael Kearns. Efficient stackelberg strategies
for finitely repeated games. In Proceedings of the 2023 International Conference on Autonomous
Agents and Multiagent Systems , pages 643â€“651, 2023.
Robert Cooter. The cost of coase. The Journal of Legal Studies , 11(1):1â€“33, 1982.
Qiwen Cui, Maryam Fazel, and Simon S Du. Learning optimal tax design in nonatomic congestion
games. arXiv preprint arXiv:2402.07437 , 2024.
Carl J Dahlman. The problem of externality. The journal of law and economics , 22(1):141â€“162,
1979.
Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Estimating and incentivizing imperfect-knowledge
agents with hidden rewards. arXiv preprint arXiv:2308.06717 , 2023a.
Ilgin Dogan, Zuo-Jun Max Shen, and Anil Aswani. Repeated principal-agent games with unobserved
agent rewards and perfect-knowledge agents. arXiv preprint arXiv:2304.07407 , 2023b.
Kate Donahue, Nicole Immorlica, Meena Jagadeesan, Brendan Lucier, and Aleksandrs Slivkins.
Impact of decentralized learning on player utilities in stackelberg games. arXiv preprint
arXiv:2403.00188 , 2024.
Paul DÃ¼tting, Tim Roughgarden, and Inbal Talgam-Cohen. Simple versus optimal contracts. In
Proceedings of the 2019 ACM Conference on Economics and Computation , pages 369â€“387, 2019.
Seyed A Esmaeili, Suho Shin, and Aleksandrs Slivkins. Robust and performance incentivizing
algorithms for multi-armed bandits with strategic agents. arXiv preprint arXiv:2312.07929 , 2023.
Alireza Fallah and Michael I Jordan. Contract design with safety inspections. arXiv preprint
arXiv:2311.02537 , 2023.
Alireza Fallah, Michael I Jordan, and Annie Ulichney. Fair allocation in dynamic mechanism design.
arXiv preprint arXiv:2406.00147 , 2024.
Zhe Feng, Chara Podimata, and Vasilis Syrgkanis. Learning to bid without knowing your value. In
Proceedings of the 2018 ACM Conference on Economics and Computation , pages 505â€“522, 2018.
Thomas K Greenfield, Yu Ye, William Kerr, Jason Bond, JÃ¼rgen Rehm, and Norman Giesbrecht.
Externalities from alcohol consumption in the 2005 us national alcohol survey: implications for
policy. International journal of environmental research and public health , 6(12):3205â€“3224, 2009.
Rachel Griffith, Martin Oâ€™Connell, and Kate Smith. Tax design in the alcohol market. Journal of
public economics , 172:20â€“35, 2019.
Guru Guruganesh, Jon Schneider, and Joshua R Wang. Contracts under moral hazard and adverse
selection. In Proceedings of the 22nd ACM Conference on Economics and Computation , pages
563â€“582, 2021.
Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen, Emmanouil-Vasileios
Vlatakis-Gkaragkounis, Joshua R Wang, and S Matthew Weinberg. Contracting with a learning
agent. arXiv preprint arXiv:2401.16198 , 2024.
Bryan D Jones. Bounded rationality. Annual review of political science , 2(1):297â€“321, 1999.
12Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review , 101(6):
2590â€“2615, 2011.
Jean-Jacques Laffont and David Martimort. The theory of incentives: the principal-agent model. In
The theory of incentives . Princeton university press, 2009.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. Advances in neural information processing systems , 20, 2007.
Tor Lattimore and Csaba SzepesvÃ¡ri. Bandit algorithms . Cambridge University Press, 2020.
Ningyuan Li, Yunxuan Ma, Yang Zhao, Zhijian Duan, Yurong Chen, Zhilin Zhang, Jian Xu, Bo Zheng,
and Xiaotie Deng. Learning-based ad auction design with externalities: the framework and a
matching-based approach. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge
Discovery and Data Mining , pages 1291â€“1302, 2023.
Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian explo-
ration: Incentivizing exploration in bayesian games. arXiv preprint arXiv:1602.07570 , 2016.
Andreu Mas-Colell, Michael D. Whinston, and Jerry R. Green. Microeconomic Theory . Oxford
University Press, New York, 1995.
Gillbert E Metcalf and David Weisbach. The design of a carbon tax. Harv. Envtl. L. Rev. , 33:499,
2009.
James Mirrlees et al. Tax by design: The Mirrlees review . OUP Oxford, 2011.
Roger B Myerson. Mechanism design . Springer, 1989.
Noam Nisan and Amir Ronen. Algorithmic mechanism design. In Proceedings of the thirty-first
annual ACM symposium on Theory of computing , pages 129â€“140, 1999.
Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems.
The Annals of Statistics , 44:660â€“681, 2016.
Arthur Pigou. The economics of welfare . Routledge, 2017.
Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American
Mathematical Society , page 527â€“535, 1952.
Tim Roughgarden. Algorithmic game theory. Communications of the ACM , 53(7):78â€“86, 2010.
Bernard SalaniÃ©. The economics of contracts: a primer . MIT press, 2005.
Bernard Salanie. The economics of taxation . MIT press, 2011.
Abishek Sankararaman, Ayalvadi Ganesh, and Sanjay Shakkottai. Social learning in multi agent multi
armed bandits. Proceedings of the ACM on Measurement and Analysis of Computing Systems , 3
(3):1â€“35, 2019.
Antoine Scheid, Daniil Tiapkin, Etienne Boursier, Aymeric Capitaine, El Mahdi El Mhamdi, Ã‰ric
Moulines, Michael I Jordan, and Alain Durmus. Incentivized learning in principal-agent bandit
games. ICML , 2024.
Reinhard Selten. Bounded rationality. Journal of Institutional and Theoretical Economics
(JITE)/Zeitschrift fÃ¼r die gesamte Staatswissenschaft , 146(4):649â€“658, 1990.
Virag Shah, Jose Blanchet, and Ramesh Johari. Bandit learning with positive externalities. Advances
in Neural Information Processing Systems , 31, 2018.
Max Simchowitz and Aleksandrs Slivkins. Exploration and incentives in reinforcement learning.
Operations Research , 2023.
Aleksandrs Slivkins et al. Introduction to multi-armed bandits. Foundations and Trends Â®in Machine
Learning , 12(1-2):1â€“286, 2019.
13Heinrich V on Stackelberg. Market structure and equilibrium . Springer Science & Business Media,
2010.
Serena Wang, Stephen Bates, P Aronow, and Michael Jordan. On counterfactual metrics for social
welfare: Incentives, ranking, and information asymmetry. In International Conference on Artificial
Intelligence and Statistics , pages 1522â€“1530. PMLR, 2024.
Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng
Xu. Markov persuasion processes and reinforcement learning. In ACM Conference on Economics
and Computation , 2022a.
Jibang Wu, Zixuan Zhang, Zhe Feng, Zhaoran Wang, Zhuoran Yang, Michael I Jordan, and Haifeng
Xu. Sequential information design: Markov persuasion process and its efficient reinforcement
learning. arXiv preprint arXiv:2202.10678 , 2022b.
Banghua Zhu, Stephen Bates, Zhuoran Yang, Yixin Wang, Jiantao Jiao, and Michael I Jordan. The
sample complexity of online contract design. arXiv preprint arXiv:2211.05732 , 2022.
14A Algorithmic Subroutine for the Binary Search
Optimal Transfer. For any given round tâ©¾1, action aâˆˆ A andÎµ >0, the downstream player
can incentivize any best-responding upstream player to choose aby offering a transfer, Ï„â‹†,Îµ
aâˆˆR+,
defined as:
Ï„â‹†,Îµ
a= max
aâ€²âˆˆAvup(aâ€²)âˆ’vup(a) +Îµ .
With this transfer, it holds that for any aâ€²âˆˆ A, aâ€²Ì¸=a, we have vup(aâ€²)< vup(a) +Ï„â‹†,Îµ
a, ensuring
the upstream playerâ€™s action At=a, since action ayields a superior reward. Consequently,
Ï„â‹†
a= lim
Îµâ†’0Ï„â‹†,Îµ
a= max
aâ€²âˆˆAvup(aâ€²)âˆ’vup(a)
represents the infimal transfer necessary to make arm athe best upstream playerâ€™s choice.
First step of BELGIC : estimation of the optimal transfers. Suppose that we consider an arm aâˆˆ A
and that the downstream player offers an incentive Ï„ato the upstream player if he picks this arm. We
consider this procedure with a constant incentive Ï„afor a batch of time steps of length ËœT=âŒˆTÎ±âŒ‰
due to the fact that the upstream player is learning [see Perchet et al., 2016, for batched bandits in
the usual multi-armed setting]. Lemma 2 shows that for BELGIC to accurately estimate Ï„â‹†
awith high
probability, it must hold
CËœTÎº+Î²/Î±<ËœT/2,which is equivalent to CTÎºÎ±+Î²âˆ’Î±<1/2, (16)
This is why we impose the condition (13) onÎ±andÎ², namely Î²/Î± < 1âˆ’Îº, which ensures that
Îº(Î±âˆ’1) +Î² <0and therefore limTâ†’+âˆžTÎº(Î±âˆ’1)+Î²= 0. More precisely, we define for aâˆˆ[K]
Î›a= (aâˆ’1)âŒˆlog2TÎ²âŒ‰ËœT ,
which is the step after which starts the binary search procedure on arm a. For any dâˆˆ
{1, . . . ,âŒˆlog2TÎ²âŒ‰}on arm a, we define
ka,d= Î›a+ (dâˆ’1)ËœT (17)
as the step after which starts the d-th batch iteration on arm a, and we consider
TÌ¸=
a,d=Card{tâˆˆ {kd,a+ 1, . . . , k d,a+ËœT}such that AtÌ¸=a}, (18)
where (At)tâˆˆ{1,...,KâŒˆlog2TÎ²âŒ‰ËœT}is given by Algorithm 2. Lemma 2 shows that for any aâˆˆ A, dâˆˆ
{1, . . . ,âŒˆlog2TÎ²âŒ‰}, with high probability
ifTÌ¸=
d,a<ËœTâˆ’CËœTÎº+Î²/Î±andTÌ¸=
d,a>CËœTÎº+Î²/Î±,then|Ï„â‹†
aâˆ’Ë†Ï„d,a|â©½1/TÎ², (19)
where Ë†Ï„a,dis the current estimate of Ï„â‹†
aoffered for iteration ka,d. In case (18) does not hold, it means
that the upstream player has misplayed and has chosen most of the steps a suboptimal action, leading
to an instantaneous regret for him larger than the bound given in H2. This is why (19) holds with
high probability. To sum up, the first phase of BELGIC consists in âŒˆlog2TÎ²âŒ‰batches of binary search
on each arm aâˆˆ A to obtain a precision level 1/TÎ²on the optimal transfer Ï„â‹†
a.
During this phase in Algorithm 2, we define Ï„a(d)âˆˆR+as the upper estimate and Ï„a(d)âˆˆR+as
the lower estimate of Ï„â‹†
aafterdâˆˆ {1, . . . ,âŒˆlog2TÎ²âŒ‰}rounds of binary search on arm a. For any
tâˆˆ[T]andaâˆˆ A, we define Ï„mid
a(t) = ( Ï„a(t) +Ï„a(t))/2.Ï„a(d), Ï„a(d), Ï„mid
a(d)are updated at
the end of the d-th binary search batch of length ËœTon arm a. We define NT=âŒˆlog2TÎ²âŒ‰as the
number of binary search steps per arm.
After this first binary search phase, the downstream player computes estimates of the optimal transfers
Ï„â‹†
a
(Ë†Ï„a)aâˆˆA= (Ï„a(âŒˆlog2TÎ²âŒ‰) + 1/TÎ²+ CT(Îºâˆ’1)/2)aâˆˆA,
and offers these transfers (Ï„â‹†
a)aâˆˆAto make the upstream player play any action Ëœaâˆˆ A she wants.
Second Step. After the first phase during which the optimal incentives are estimated by the down-
stream player through (Ë†Ï„a)aâˆˆA, she runs in the second phase the subroutine Bandit-Alg on theAÃ—A
bandit instance driven by her action Btand the upstream playerâ€™s one At. More precisely, any bandit
subroutine Bandit-Alg , such as UCBorÎµ-greedy , for instance, can be run in a black-box fashion
15on the shifted bandit instance, where rewards are shifted by the upper estimated transfers (Ë†Ï„a)aâˆˆA.
The downstream player computes a shifted history ËœHdown ,p
t =âˆ…for any tâ©½KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰and
for any t > KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰
ËœHdown ,p
t =(
{Ëœat, Bt, Ï„(t), Ut, XËœat,Bt(t)âˆ’Ë†Ï„Ëœat} âˆªËœHdown ,p
tâˆ’1 ifËœat=At
ËœHdown ,p
tâˆ’1 otherwise .
which serves to feed Bandit-Alg , following Bandit-Alg : (Ut,ËœHdown ,p
tâˆ’1)7â†’(Ëœat, Bt)âˆˆ A Ã— A .
Note that here, Bandit-Alg aims to maximize the shifted reward (Î½down(a, b)âˆ’Ë†Ï„a)(a,b)âˆˆA2.
Based on this decision by Bandit-Alg ,Î down
p offers the incentive Ë†Ï„Ëœatassociated with action Ëœatand
plays action Bt. Lemma 5 ensures that Ëœatis the upstream playerâ€™s best choice. Therefore, H2 ensures
that the upstream player will not deviate from the downstream playerâ€™s recommendation with high
probability.
B Invariance when the property rights are given to the downstream player
Our focus in the paper was the case where the upstream player possesses the bandit instance and
receives monetary payments. We argue here that the symmetric situation, i.e. when the property
rights are given to the downstream player, can be analysed in the exact same way.
Assume that the the downstream player owns the bandit instance. This implies that (i) they can
prescribe what arm the upstream player has to play at each round, and (ii) the upstream player may
perform a monetary transfer to influence the arm they are allowed to pull.
Consider the same bandit setup as before. Formally, the downstream playerâ€™s action is (At, Bt)âˆˆ
A Ã— A where Atis the arm that the upstream player is prescribed to pull (he cannot deviate since the
downstream player has the property rights), while Btis the arm played by the downstream player. On
the other hand, the upstream playerâ€™s policy outputs at each round the action (Ëœat, Ï„(t))âˆˆ A Ã— R+,
where Ëœatis the arm they choose to incentivize and Ï„(t)is the amount of transfer. It means that the
downstream player receives a transfer Ï„(t)if she prescribes action At= Ëœat. As a consequence, the
instantaneous utility of the upstream player is ZAtâˆ’1Ëœat(At)Ï„(t), while the downstream player
receives XAt,Bt+1Ëœat(At)Ï„(t). In that case, the upstream player may perform a binary search on
each arm Â¯aâˆˆ A to identify the optimal incentive Ï„â‹†
Â¯a, by considering Card{tâˆˆ[T]such that Ëœat= Â¯a}
during batches designed for the binary search and then play on the shifted bandit instance as we
explained. This situation and the upstream playerâ€™s strategy are now equivalent to the one presented
before.
C Proofs and Technical Results
Recall that we defined the shifted history ËœHdown ,p
t that will serve to feed Î down
p at time tasËœHdown ,p
t =
(Ëœas, Ï„(s), As, Vs, ZAs(s))sâ©½t.
Theorem 4. Suppose that argmaxaâˆˆAvup(a)is the singleton {au
â‹†}and that
vup(asw) +vdown(asw, bsw)âˆ’vup(au
â‹†) +vdown(au
â‹†, b)>0,
for any bâˆˆ A . In the absence of property rights and when the upstream player runs any no-
regret policy Î up
n, we have Rsw(T,Î up
n,Î down
n)â©¾Tâˆ†swâˆ’Rup
n(T,Î up
n)âˆ†sw/âˆ†up, where âˆ†up=
minaâ€²âˆˆA\{ auâ‹†}vup(au
â‹†)âˆ’vup(aâ€²)andâˆ†sw=vup(asw) +vdown(asw, bsw)âˆ’max bâˆˆA(vup(au
â‹†) +
vdown(au
â‹†, b)). Therefore, Rsw(T,Î up
n,Î down
n) = â„¦( T)and(Î up
n,Î down
n)is not welfare efficient.
Proof of Theorem 4. Since argmaxaâˆˆAvup(a)is the singleton {aup
â‹†}, we define âˆ†up=
minaâ€²âˆˆA\{ auâ‹†}vup(au
â‹†)âˆ’vup(aâ€²)as the upstream player reward gap and âˆ†sw=vup(asw) +
vdown(asw, bsw)âˆ’max bâˆˆA(vup(au
â‹†) +vdown(au
â‹†, b))as the social welfare reward gap if the up-
stream player plays his most preferred action.
Denote Nup
â‹†(T)the number of pulls of the upstream player up to time Ton the arm au
â‹†. By definition
ofâˆ†up, we have that for any step tâˆˆ[T]such that AtÌ¸=aup
â‹†,max aâˆˆA{vup(a)} âˆ’vup(At) =
16vup(aup
â‹†)âˆ’vup(At)â©¾minaâ€²âˆˆA\{ auâ‹†}vup(au
â‹†)âˆ’vup(aâ€²) = âˆ†up. There are Tâˆ’Nup
â‹†(T)such steps,
which leads to
Rup
n(T,Î up
n,Î down
p)â©¾E"TX
t=1max
aâˆˆA{vup(a)} âˆ’vup(At)#
â©¾(Tâˆ’E[Nu
â‹†(T)])âˆ†up,
and we obtain
E[Nu
â‹†(T)]â©¾Tâˆ’Rup
n(T,Î up
n)/âˆ†up.
Moreover, for any tâˆˆ[T]such that At=aup
â‹†and any Btâˆˆ A,vup(asw) +vdown(asw, bsw)âˆ’
(vup(At) +vdown(At, Bt)) =vup(asw) +vdown(asw, bsw)âˆ’(vup(aup
â‹†) +vdown(aup, Bt))â©¾âˆ†sw
by definition, which leads to
Rsw(T,Î up
n,Î down
n)â©¾E[Nu
â‹†(T)]âˆ†sw,
and we obtain
Rsw(T,Î up
n,Î down
n)â©¾Tâˆ†swâˆ’Rup
n(T,Î up
n)âˆ†sw/âˆ†up.
Since limTâ†’+âˆžRup
n(T,Î up
n)/T= 0, we have limTâ†’+âˆžRsw(T,Î up
n,Î down
n)/T= âˆ†sw>0,
hence the result.
The proof of Theorem 2 is an immediate consequence of Theorem 4.
Lemma 1. Recall that Âµâ‹†,downis the downstream playerâ€™s optimal reward as defined as a solution
of(10). We have Âµâ‹†,down= max a,bâˆˆA{vdown(a, b) +vup(a)} âˆ’max aâ€²âˆˆA{vup(aâ€²)}, as well as
(aopt, bopt) = (asw, bsw)andÂµâ‹†,up+Âµâ‹†,down=vup(asw)+vdown(asw, bsw) = max a,bâˆˆA{vup(a)+
vdown(a, b)}, where Âµâ‹†,upis defined in Equation (1). Moreover, for any integer TâˆˆNâ‹†, and policies
Î up
p,Î down
p , we have that
Rsw(T,Î up
p,Î down
p)â©½Rup
p(T,Î up
p,Î down
p) +Rdown
p(T,Î up
p,Î down
p).
Proof of Lemma 1. Recall that Âµâ‹†,downis defined as Âµâ‹†,down= supa,bâˆˆA2,Ï„âˆˆR+{vdown(a, b)âˆ’
Ï„},such that aâˆˆargmaxaâ€²âˆˆA{vup(aâ€²) +1a(aâ€²)Ï„}andaup
â‹†= argmaxaâ€²âˆˆAvup(a). Note that we
can write
Âµâ‹†,down= max {supa,bâˆˆA2,Ï„âˆˆR+1ËœA(a, Ï„)(vdown(a, b)âˆ’Ï„),max bâˆˆAvdown(aup
â‹†, b)},
where ËœA={(a, Ï„):vup(a) +Ï„â©¾max aâ€²vup(aâ€²) +1a(aâ€²)Ï„}which is the set of pairs (a, Ï„)âˆˆ
A Ã—R+such that the constraint binds. However, we also have by definition that vup(aup
â‹†) + 0â©¾
max aâ€²âˆˆAvup(aâ€²) +1aup
â‹†(aâ€²)Â·0and hence, (aup
â‹†,0)âˆˆËœA. Therefore, since vdown(aup
â‹†, b)â©¾0for
anybâˆˆ A, we can write
Âµâ‹†,down= sup
(a,Ï„)âˆˆËœA,bâˆˆA{vdown(a, b)âˆ’Ï„}.
First note that if (a, Ï„)âˆˆËœA, then for any aâ€²âˆˆ A, Ï„âˆˆR+,vup(a) +Ï„â©¾vup(aâ€²) +1a(aâ€²)Ï„, which
gives
vup(a)âˆ’vup(aâ€²)â©¾(1a(aâ€²)âˆ’1)Ï„ .
However, either a=aâ€²and hence vup(a)âˆ’vup(aâ€²) = 0 , either aÌ¸=aâ€²and hence 1a(aâ€²) = 0 .
Therefore, we have that
vup(a)âˆ’vup(aâ€²)â©¾âˆ’Ï„ ,
which implies by definition of the optimal incentives that Ï„â©¾vup(aâ€²)âˆ’vup(a)for any aâ€²âˆˆ A, and
hence Ï„â‹†
aâ©½Ï„for any (a, Ï„)âˆˆËœA.
In addition, (a, Ï„â‹†
a)âˆˆËœAby definition. Consequently,
Âµâ‹†,down= max
a,bâˆˆA2{vdown(a, b)âˆ’Ï„â‹†
a}= max
a,bâˆˆA{vdown(a, b)âˆ’max
aâ€²âˆˆA{vup(aâ€²)}+vup(a)},
17hence the first part of the result. Since Âµâ‹†,upis defined as Âµâ‹†,up= max aâˆˆAvup(a), we have that
Âµâ‹†,down+Âµâ‹†,up= max
a,bâˆˆA{vdown(a, b)âˆ’max
aâ€²âˆˆA{vup(aâ€²)}+vup(a)}+ max
aâˆˆAvup(a)
= max
a,bâˆˆA{vdown(a, b) +vup(a)}
=vup(asw) +vdown(asw, bsw).
Now summing Rdown
p andRup
pas defined in (9) and (11), we obtain
Rup
p(T,Î up
p,Î down
p) +Rdown
p(T,Î up
p,Î down
p)
=E"TX
t=1max
aâˆˆA{vup(a) +1Ëœat(a)Ï„(t)} âˆ’(vup(At) +1Ëœat(At)Ï„(t))#
+Tmax
a,bâˆˆA2{vdown(a, b)âˆ’max
aâ€²âˆˆA{vup(aâ€²)}+vup(a)} âˆ’E"TX
t=1vdown(At, Bt)âˆ’1Ëœat(At)Ï„(t)#
â©¾Tmax
aâˆˆA{vup(a)} âˆ’E"TX
t=1vup(At) +1Ëœat(At)Ï„(t)#
âˆ’E"TX
t=1vdown(At, Bt)âˆ’1Ëœat(At)Ï„(t)#
+Tmax
a,bâˆˆA2{vdown(a, b) +vup(a)} âˆ’Tmax
aâ€²âˆˆA{vup(aâ€²)}
=Tmax
a,bâˆˆA2{vup(a) +vdown(a, b)} âˆ’E"TX
t=1vup(At) +vdown(At, Bt)#
=Rsw(T,Î up
p,Î down
p),
hence the result.
For a downstream playerâ€™s policy Î down
p, we define ËœRup
pas the upstream playerâ€™s regret without
expectation, following
ËœRup
p({s+ 1, . . . , s +t},Î up
p,Î down
p) =s+tX
l=s+1max
aâˆˆA{vup(a) +1Ëœal(a)Ï„(l)} âˆ’(vup(Al) +1Ëœal(Al)Ï„(l)),
where (Ëœal, Ï„(l))lâˆˆ[T]are the incentives output by Î down
p and(Al)lâˆˆ[T]are the output of Î up
p. Recall
the assumption that we use on the upstream playerâ€™s regret for a policy Î up
p. We show here that it is
satisfied by typical no-regret bandit algorithms.
H2.There exist C, Î¶ > 0, Îºâˆˆ[0,1)such that for any s, tâˆˆ[T]withs+tâ©½T, any{Ï„a}aâˆˆ[K]âˆˆRK
+
and any policy Î down
p that offers almost surely a transfer (Ëœal, Ï„(l)) = (Ëœ al, Ï„Ëœal)for any lâˆˆ {s+
1, . . . , s +t}, the batched regret of the upstream player following Î up
psatisfies, with probability at
least1âˆ’tâˆ’Î¶,
s+tX
l=s+1max
aâˆˆA{vup(a) +1Ëœal(a)Ï„Ëœal} âˆ’(vup(Al) +1Ëœal(Al)Ï„Ëœal)â©½CtÎº.
We present the upstream playerâ€™s UCBsubroutine.
Proposition 2. Lets, tâˆˆ[T]such that s+tâ©½T. Suppose that there exists a family (Ï„a)aâˆˆA
of constant incentives associated with each arm aâˆˆ A such that we have in Algorithm 3:
(Ëœal, Ï„(l))s+lâˆˆ{s+1,...,s+t}= (Ëœal, Ï„Ëœal)lâˆˆ{s+1,...,s+t}as an output of Î down
p. Suppose that the dis-
tributions Î³aare1-sub-Gaussian. Then with probability at least 1âˆ’Tâˆ’2, the regret of the version of
UCBgiven in Algorithm 3 run by the upstream player satisfies
ËœRup
p({s+ 1, . . . , s +t},UCB,Î down
p)â©½8p
log(KT3)âˆš
tK .
Note that the major difference between this assumption and the regret bounds that we generally
consider in multi-armed bandit problems is that we consider the regret without expectation here.
18Algorithm 3 Upstream playerâ€™s UCB
1:Input: Set of arms K, horizon T.
2:Initialize: For any arm aâˆˆ[K], setË†Âµa= 0, Ta= 0.
3:for1â©½tâ©½K:do
4: Pull arm At=t
5: Update Ë†ÂµAt=XAt(t), TAt(t) = 1
6:end for
7:fortâ©¾K+ 1do
8: Observe the incentive (Ëœat, Ï„(t)).
9: Pull arm Atâˆˆargmaxaâˆˆ[K]n
Ë†Âµa(tâˆ’1) + 2q
log(KT3)
Ta(tâˆ’1)+1Ëœat(a)Ï„(t)o
10: Update TAt(t) =TAt(tâˆ’1) + 1 ,Ë†ÂµAt(t) =1
TAt(t)(TAt(tâˆ’1)Ë†ÂµAt(tâˆ’1) +XAt(t))
11:end for
Proof of Proposition 2. The proof is adapted from the proof of Bubeck et al. [2012, Theorem 2.1].
Lets, tâˆˆ[T]such that s+tâ©½T. For any integer lâˆˆ {s+ 1, . . . , s +t}, we write nl(a) =
Card{lâ€²âˆˆ[l]such that Alâ€²=a}for the number of pulls of arm aandË†Âµa(l)for the empirical mean
utility of the arm aâˆˆ A estimated on the batch {1, . . . , l}:Ë†Âµa(l) =nl(a)âˆ’1Pl
k=11a(Ak)Xa(k).
Since the rewards on the incentivized bandit instance are 1-sub-Gaussian, a Hoeffding bound gives
that for any Î´âˆˆ(0,1),aâˆˆ A,lâˆˆ {s+ 1, . . . , s +t}, and any family of arms (alâ€²)lâ€²âˆˆ{1,...,l}such
that Card {j:aj=a}=k, we have that
P
|Ë†Âµa(l)âˆ’vup(a)|â©¾2p
log(2/Î´)/k(Alâ€²)lâ€²âˆˆ{1,...,s+l}= (alâ€²)lâ€²âˆˆ{1,...,s+l}
â©½Î´ .
Therefore, for any Î´âˆˆ(0,1),aâˆˆ A,lâˆˆ {s+ 1, . . . , s +t}, we have the following bound
P
|Ë†Âµa(l)âˆ’vup(a)|â©¾2p
log(2/Î´)/nl(a)
=Pï£«
ï£¬ï£¬ï£¬ï£­l[
k=1[
(alâ€²)lâ€²s.t.
Card{lâ€²âˆˆ{1,...,l}:alâ€²=a}=k{|Ë†Âµa(l)âˆ’vup(a)|â©¾2p
log(2/Î´)/k}ï£¶
ï£·ï£·ï£·ï£¸
â©½lX
k=1Pï£«
ï£¬ï£¬ï£¬ï£­[
(alâ€²)lâ€²s.t.
Card{lâ€²âˆˆ{1,...,l}:alâ€²=a}=k{|Ë†Âµa(s+l)âˆ’vup(a)|â©¾2p
log(2/Î´)/k}ï£¶
ï£·ï£·ï£·ï£¸
â©½lX
k=1X
(alâ€²)lâ€²s.t.
Card{lâ€²âˆˆ{1,...,l}:alâ€²=a}=kP
|Ë†Âµa(s+l)âˆ’vup(a)|â©¾2p
log(2/Î´)/kAl=al
P(Al=al)
â©½TÎ´ ,
and with an union bound, we obtain that
P
âˆƒlâˆˆ[t], aâˆˆ A such that |Ë†Âµa(l)âˆ’vup(a)|â©¾2p
log(2/Î´)/nl(a)
â©½T2KÎ´ ,
Considering the probability of the opposite event, we have that
P
for any lâˆˆ[t], aâˆˆ A,|Ë†Âµa(l)âˆ’vup(a)|â©½2p
log(2T2K/Î´)/nl(a)
â©¾1âˆ’Î´ , (20)
where we rescaled Î´asÎ´/T2K.
For the remaining of the proof, we take Î´=Tâˆ’2and define aâ‹†
l= argmaxaâˆˆA{vup(a) +1Ëœal(a)Ï„}.
We now assume that the event {for any lâˆˆ {s+ 1, . . . , s +t}, aâˆˆ A,|Ë†Âµa(l)âˆ’vup(a)|â©½
2p
log(2T2K/Î´)/nl(a)}holds. If at some step lâˆˆ {s+ 1, . . . , s +t}, action Alis chosen in
Algorithm 3, it means that
Ë†ÂµAl(l) + 2p
log(tK/Î´ )/nl(Al) +1Ëœal(Al)Ï„Ëœalâ©¾Ë†Âµaâ‹†
l(l) + 2q
log(tK/Î´ )/nl(aâ‹†
l) +1Ëœal(aâ‹†
l)Ï„Ëœal,
19with regards to the choice of actions in UCBbased on the upper confidence bound. We now decompose
the whole regret on the batch {s+ 1, . . . , s +t}defined as ËœRup
p({s+ 1, . . . , s +t},UCB,Î down
p).
(20) ensures that with probability at least 1âˆ’1/T2
ËœRup
p({s+ 1, . . . , s +t},UCB,Î down
p) =s+tX
l=s+1vup(aâ‹†
l) +1Ëœal(aâ‹†
l)Ï„Ëœalâˆ’(vup(Al) +1Ëœal(Al)Ï„Ëœal)
â©½s+tX
l=s+1Ë†Âµaâ‹†
l(l) + 2q
log(Kt/Î´ )/nl(aâ‹†
l)âˆ’vup(Al) +1Ëœal(aâ‹†
l)Ï„Ëœalâˆ’1Ëœal(Al)Ï„Ëœal
â©½s+tX
l=s+1Ë†ÂµAl(l) + 2p
log(Kt/Î´ )/nl(Al)âˆ’vup(Al)
â©½s+tX
l=s+1Ë†ÂµAl(l) + 2p
log(Kt/Î´ )/nl(Al)âˆ’(Ë†ÂµAl(l)âˆ’2p
log(Kt/Î´ )/nl(Al))
â©½4p
log(Kt/Î´ )s+tX
l=s+1p
1/nl(Al),
and we have
s+tX
l=s+1p
1/nl(Al)â©½KX
i=1s+tX
l=s+1p
1Al(i)/nl(i)
â©½KX
i=1ns+t(i)X
j=ns+1(i)1/p
jâ©½2KX
i=1p
ns+t(i)âˆ’ns(i), (21)
where the last step holds because for any integers s, tâˆˆNâ‹†, we have that
s+tX
l=s+11âˆš
l=s+tX
l=s+11âˆš
lZl
x=lâˆ’1dxâ©½s+tX
l=s+1Zl
x=lâˆ’1dxâˆšx=Zs+t
x=sdxâˆšx= 2(âˆš
s+tâˆ’âˆšs).
Using Cauchy-Schwarz inequality we obtain from (21)
1/Ks+tX
l=s+1p
1/nl(Al)â©½2vuut1/KKX
i=1ns+t(i)âˆ’ns(i) = 2p
t/K ,
which givesPs+t
l=s+1p
1/nl(Al)â©½2âˆš
tK.
Finally plugging all the terms together, since Î´= 1/T2, we obtain that with probability at least
1âˆ’1/T2
ËœRup
p({s+ 1, . . . , s +t},UCB)â©½8p
log(KT3)âˆš
tK .
Lemma 2. Assume H2 holds and consider some arm aâˆˆ A such that we run the d-th batch
of binary search on awithdâˆˆ {1, . . . ,âŒˆlog2TÎ²âŒ‰}: for any tâˆˆ {ka,d+ 1, . . . , k a,d+ËœT}, we
have (Ëœat, Ï„(t)) = ( a, Ï„a)withÏ„a=Ï„mid
a(d)andËœT=âŒˆTÎ±âŒ‰. Recall that we defined in (18):
TÌ¸=
a,d=Card{tâˆˆ {ka,d+ 1, . . . , k a,d+ËœT}such that AtÌ¸=a}. Let Î²âˆˆ(0,1)be such that
Î² < Î± (1âˆ’Îº). Given that the event {ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©½CËœTÎº}holds,
we have that
â€¢IfTÌ¸=
a,d<ËœTâˆ’CËœTÎº+Î²/Î±, then Ï„â‹†
a< Ï„a+ 1/TÎ².
â€¢IfTÌ¸=
a,d>CËœTÎº+Î²/Î±, then Ï„â‹†
a> Ï„aâˆ’1/TÎ².
Consequently, with probability at least 1âˆ’2Tâˆ’Î±Î¶, ifCËœTÎº+Î²/Î±< TÌ¸=
a,d<ËœTâˆ’CËœTÎº+Î²/Î±, then
|Ï„â‹†
aâˆ’Ï„a|â©½1/TÎ².
20Proof of Lemma 2. The whole proof is done conditionally on the event
{ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©½CËœTÎº}.
Note that it holds with probability at least 1âˆ’ËœTâˆ’Î¶â©¾1âˆ’Tâˆ’Î±Î¶since we suppose that Î up
psatisfies
H2.
Suppose that we have Ï„aâ©¾Ï„â‹†
a+ 1/TÎ². By definition of the optimal incentives, we obtain, using by
assumption Ï„aâ©¾Ï„â‹†
a+ 1/TÎ²
1a(a)Ï„a+vup(a)â©¾Ï„â‹†
a+vup(a) + 1/TÎ²
= max
aâ€²âˆˆAvup(aâ€²)âˆ’vup(a) +vup(a) + 1/TÎ²
= max
aâ€²vup(aâ€²) + 1/TÎ²,
which ensures that ais the optimal arm for the upstream player during the batch {ka,d+ 1, . . . , k a,d+
ËœT}that we consider. In that case, since ais the best arm, by definition of the upstream playerâ€™s
utility, the reward gap E[vup(a) +Ï„aâˆ’(vup(At) +1a(At)Ï„a)]for the upstream player at any step
tâˆˆ {ka,d+ 1, . . . , k a,d+ËœT}is at least vup(a) +Ï„aâˆ’max aâ€²âˆˆAvup(aâ€²), and we obtain
CËœTÎºâ©¾ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©¾TÌ¸=
a,d(Ï„a+vup(a)âˆ’max
aâ€²âˆˆA{vup(aâ€²)}),
byH2,TÌ¸=
a,dbeing the number of steps for which a suboptimal arm has been chosen. Therefore, by
definition of the optimal incentives, we obtain that
CËœTÎºâ©¾TÌ¸=
a,d(Ï„aâˆ’Ï„â‹†
a)â©¾TÌ¸=
a,d/TÎ²â©¾TÌ¸=
a,dËœTâˆ’Î²/Î±,
which gives: TÌ¸=
a,dâ©½CËœTÎº+Î²/Î±. Therefore, if we take the contrapositive, we obtain that during the
sequence {ka,d+ 1, . . . , k a,d+ËœT}, ifTÌ¸=
a,d>CËœTÎº+Î²/Î±, then with probability at least 1âˆ’Tâˆ’Î±Î¶,
Ï„a< Ï„â‹†
a+ 1/TÎ², or equivalently Ï„â‹†
a> Ï„aâˆ’1/TÎ².
Now suppose that Ï„aâ©½Ï„â‹†
aâˆ’1/TÎ². By definition of the optimal incentives, we obtain
1a(a)Ï„a+vup(a)â©½Ï„â‹†
a+vup(a)
â©½max
aâ€²âˆˆAvup(aâ€²)âˆ’vup(a) +vup(a)âˆ’1/TÎ²
= max
aâ€²vup(aâ€²)âˆ’1/TÎ²,
which ensures that ais a suboptimal arm for the upstream player during this batch of time steps.
Therefore, arm awhich has a reward gap bigger than 1/TÎ²since max aâ€²âˆˆA{vup(a) +1a(aâ€²)} âˆ’
(vup(a) +Ï„a)â©¾1/TÎ²and arm ahas been picked ËœTâˆ’TÌ¸=
a,dtimes. Consequently, we have that
CËœTÎºâ©¾ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)
â©¾(ËœTâˆ’TÌ¸=
a,d)(max
aâ€²âˆˆAvup(aâ€²)âˆ’(Ï„a+vup(a))
| {z }
â©¾1/TÎ²)
â©¾(ËœTâˆ’TÌ¸=
a,d)ËœTâˆ’Î²/Î±,
which gives TÌ¸=
a,dâ©¾ËœTâˆ’CËœTÎº+/Î²/Î±. Therefore, if we take the contrapositive, we obtain that if
TÌ¸=
a,d<ËœTâˆ’CËœTÎº+/Î²/Î±, then with probability at least 1âˆ’Tâˆ’Î±Î¶,Ï„a> Ï„â‹†
aâˆ’1/TÎ², or equivalently
Ï„â‹†
a< Ï„a+ 1/TÎ².
For the second part of the proof, suppose that: CËœTÎº+Î²/Î±< TÌ¸=
a,d<ËœTÎº+Î²/Î±âˆ’CËœTÎº+Î²/Î±.
From the above result, we have that Ï„â‹†
a< Ï„a+ 1/TÎ²andÏ„â‹†
a> Ï„aâˆ’1/TÎ²with probability at least
1âˆ’2Tâˆ’Î±Î¶. Plugging these inequalities in the absolute value |Ï„â‹†
aâˆ’Ï„a|concludes the proof.
Lemma 3. Assume that we run Algorithm 2 and consider some binary search batch iteration
dâˆˆ âŒˆlogTÎ²âŒ‰run on arm aâˆˆ A. Then 0â©½Ï„a(d)â©½Ï„mid
a(d)â©½Ï„a(d)â©½1.
21Proof of Lemma 3. The proof proceeds by induction. Considering some action aâˆˆ A, we have for
the initialisation before any binary search is run: Ï„a(0) = 0 ,Ï„a(0) = 1 and therefore Ï„mid
a(0)âˆˆ
[Ï„a(0),Ï„a(0)]. We now consider that a number dof binary search batches has been run on a. Suppose
that we run an additional binary search batch on action a. We have
Ï„mid
a(d+ 1) =Ï„a(d) +Ï„a(d)
2which gives Ï„mid
a(d+ 1)âˆˆ[Ï„a(d),Ï„a(d)]. (22)
After this iteration of binary search, we either have Ï„a(d+1) = Ï„mid
a(d+1)+1 /TÎ²andÏ„a(d+1) =
Ï„a(d)orÏ„a(d+ 1) = Ï„mid
a(d)andÏ„a(d+ 1) = Ï„mid
a(d+ 1)âˆ’1/TÎ². Therefore, we still have
0â©½Ï„a(d+ 1)â©½Ï„mid
a(d+ 1)â©½Ï„a(d+ 1)â©½1, hence the result for any dâˆˆ âŒˆlog2TÎ²âŒ‰by
induction.
Lemma 4. Consider some arm aâˆˆ A and suppose that we have run DâˆˆNâ‹†batches of binary
search of length ËœTona. Then, we have that
Pï£«
ï£­\
dâˆˆ[D]{ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©½CËœTÎº}ï£¶
ï£¸â©¾1âˆ’D/TÎ±Î¶.
Proof of Lemma 4. First observe that for any batch dâˆˆ {1, . . . , D }of binary search run on arm a
during steps {ka,d+ 1, . . . , k a,d+ËœT}, we have by H2 that
P
ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©¾CËœTÎº
â©½1/ËœTÎ¶,
and applying a union bound over the Dbatches, we have that
Pï£«
ï£­[
dâˆˆ[D]{ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©¾CËœTÎº}ï£¶
ï£¸
â©½DX
j=1P
{ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©¾CËœTÎº}
â©½D/ËœTÎ¶,
which gives that
Pï£«
ï£­\
dâˆˆ[D]{ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©½CËœTÎº}ï£¶
ï£¸â©¾1âˆ’D/TÎ±Î¶,
hence the result.
Lemma 5. Suppose that the upstream player runs a subroutine Î up
psatisfying H2. Consider some ac-
tionaâˆˆ A and the D-th binary search batch of length ËœTrun on arm awithDâˆˆ {1, . . . ,âŒˆlog2TÎ²âŒ‰}.
Then\
dâˆˆ[D]{ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©½CËœTÎº} âŠ† { Ï„â‹†
aâˆˆ[Ï„a(D),Ï„a(D)]},
and the probability of these events is at least 1âˆ’ âŒˆlog2TÎ²âŒ‰/TÎ±Î¶. We also have that
|Ï„a(D)âˆ’Ï„a(D)|â©½1/2D+ 2/TÎ²holds almost surely .
Proof of Lemma 5. Suppose that the conditions of the lemma hold and consider some arm a.
We show by induction on the number of binary search batches Dthat have been run on athatT
dâˆˆ[D]{ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©½CËœTÎº} âŠ† { Ï„â‹†
aâˆˆ[Ï„a(D),Ï„a(D)]}. If it is
true, Lemma 4 completes this first part of the proof.
The initialisation holds since Ï„a(0) = 0 ,Ï„a(0) = 1 andÏ„â‹†
a= max aâ€²âˆˆAvup(aâ€²)âˆ’vup(a)âˆˆ[0,1]
with probability 1- since max aâ€²âˆˆAvup(aâ€²)âˆˆ[0,1]andvup(a)âˆˆ[0,1].
22We suppose that the property is true for some integer D <âŒˆlog2TÎ²âŒ‰and that we have run one more
binary search on arm a. We have
Ï„mid
a(D+ 1) =Ï„a(D) +Ï„a(D)
2,
Ï„mid
a(D+ 1) being the incentive offered to the upstream player if he chooses action aduring
theD-th batch {ka,D+ 1, . . . , k a,D+ËœT}. After this batch, if TÌ¸=
a,D<ËœTâˆ’CËœTÎº+Î²/Î±,BELGIC
updates Ï„a(D+ 1) = Ï„mid
a(D+ 1) + 1 /TÎ²,Ï„a(D+ 1) = Ï„a(D)and Lemma 2 ensures that
Ï„a(D+ 1)< Ï„â‹†
a<Ï„a(D+ 1) given{ËœRup
p({ka,D+1+ 1, . . . , k a,D+1+ËœT},Î up
p,Î down
p)â©½CËœTÎº}.
Thus the induction holds.
Otherwise, if TÌ¸=
a,D>CËœTÎº+Î²/Î±,BELGIC updates Ï„a(D+ 1) = Ï„mid
a(D+ 1)âˆ’1/TÎ²,Ï„a(D+
1) = Ï„a(D)and Lemma 2 ensures that Ï„a(D+ 1) < Ï„â‹†
a<Ï„a(D+ 1) given{ËœRup
p({ka,D+1+
1, . . . , k a,D+1+ËœT},Î up
p,Î down
p)â©½CËœTÎº}. The induction still holds.
Consequently, we have that for any number Dof binary search batches run on arm a,Ï„â‹†
aâˆˆ
[Ï„a(D),Ï„a(D)]with probability 1âˆ’D/TÎ±Î¶
For the second part of the proof, we define u(D) =Ï„a(D)âˆ’Ï„a(D)â©¾0as the length of the interval
containing Ï„â‹†
awith probability at least 1âˆ’D/TÎ±Î¶. We have u(0) = 1 . Suppose that after Diterations
of binary search batches, the next batch of binary search {ka,D+1+ 1, . . . , k a,D+1+ËœT}outputs
TÌ¸=
a,D+1<ËœTâˆ’CËœTÎº+Î²/Î±. Then, the update of Algorithm 2 gives
uD+1=Ï„a(D+ 1)âˆ’Ï„a(D+ 1)
=Ï„mid
a(D+ 1) + 1 /TÎ²âˆ’Ï„a(D)
=Ï„a(D) +Ï„a(D)
2âˆ’Ï„a(D) + 1/TÎ²
=Ï„a(D)âˆ’Ï„a(D)
2+ 1/TÎ²
=uD/2 + 1 /TÎ².
On the other hand, if TÌ¸=
a,D+1>CËœTÎº+Î²/Î±, the update gives
uD+1=Ï„a(D+ 1)âˆ’Ï„a(D+ 1)
=Ï„a(Da
t)âˆ’(Ï„mid
a(D+ 1)âˆ’1/TÎ²)
=Ï„a(D)âˆ’Ï„a(D) +Ï„a(D)
2+ 1/TÎ²
=Ï„a(D)âˆ’Ï„a(D)
2+ 1/TÎ²
=uD/2 + 1 /TÎ².
We can see that (uD)Dâ©¾0is an arithmetico-geometric sequence defined by uD+1=uD/2 + 1 /TÎ²
with an initial term u0= 1. Writing r= 1/TÎ²/(1âˆ’1/2) = 2 /TÎ², we obtain that
|Ï„a(D)âˆ’Ï„a(D)|=uD= 1/2D(1âˆ’r) +r= 1/2D(1âˆ’2/TÎ²) + 2/TÎ²â©½1/2D+ 2/TÎ²,
for any Dâˆˆ {1, . . . ,âŒˆlog2TÎ²âŒ‰}, hence the result.
Lemma 6. Suppose that the upstream player runs a policy Î up
psatisfying H2. Considering some
action aâˆˆ A , we have that after the binary search batch D=âŒˆlog2TÎ²âŒ‰:P(Ï„a(D)â©½Ï„â‹†
aâ©½
Ï„a(D)â©½Ï„a+ 3/TÎ²)â©¾1âˆ’D/TÎ±Î¶.
Proof of Lemma 6. We suppose that the eventT
dâˆˆ[âŒˆlog2TÎ²âŒ‰]{ËœRup
p({ka,d+ 1, . . . , k a,d+
ËœT},Î up
p,Î down
p)â©½CËœTÎº}holds. Lemma 4 ensures that this event holds with probability at least
1âˆ’ âŒˆlog2TÎ²âŒ‰/TÎ±Î¶.
23After D=âŒˆÎ²log2TâŒ‰batches of binary search on arm a, we have by Lemma 5 that
|Ï„a(D)âˆ’Ï„a(D)|â©½1/2D+ 2/TÎ²â©½1/2Î²log2T+ 2/TÎ²= 3/TÎ².
Lemma 5 guarantees that Ï„â‹†
aâˆˆ[Ï„a(D);Ï„a(D)]with probability at least 1âˆ’D/TÎ±Î¶, and we obtain
Ï„a(D)â©½Ï„â‹†
aâ©½Ï„a(D)â©½Ï„a(D) + 3/TÎ²with the same probability.
Proposition 1. UnderH1 andH2, after the first phase of BELGIC which consists in KâŒˆTÎ±âŒ‰âŒˆlogTÎ²âŒ‰
steps of binary search grouped in âŒˆlog2TÎ²âŒ‰batches per arm aâˆˆ A, we have that
P
for any aâˆˆ A,Ë†Ï„aâˆ’4/TÎ²âˆ’CT(Îºâˆ’1)/2â©½Ï„â‹†
aâ©½Ë†Ï„a
â©¾1âˆ’KâŒˆlog2TÎ²âŒ‰/TÎ±Î¶.
Proof of Proposition 1. We consider a number of binary search batches D=âŒˆlogTÎ²âŒ‰and we define
the event GasG=
for any aâˆˆ A, Ï„a(D)â©½Ï„â‹†
aâ©½Ï„a(D)â©½Ï„a(D) + 3/TÎ²	
. We have that
P(G) =P \
aâˆˆA
Ï„a(D)â©½Ï„â‹†
aâ©½Ï„a(D)â©½Ï„a+ 3/TÎ²	!
= 1âˆ’P [
aâˆˆA
Ï„a(D)â©½Ï„â‹†
aâ©½Ï„a(t)â©½Ï„a(D) + 3/TÎ²	c!
â©¾1âˆ’X
aâˆˆAP
Ï„a(D)â©½Ï„â‹†
aâ©½Ï„a(D)â©½Ï„a(D) + 3/TÎ²	c
,
where the last inequality holds with an union bound. Lemma 6 with D=âŒˆlog2TÎ²âŒ‰ensures that we
have
P
Ï„a(D)â©½Ï„â‹†
aâ©½Ï„a(D)â©½Ï„a(D) + 3/TÎ²	c
â©½D/TÎ±Î¶,
and since Card {A}=K, we obtain
P(G)â©¾1âˆ’KâŒˆlog2TÎ²âŒ‰/TÎ±Î¶.
Since the estimated incentives are defined as Ë†Ï„a=Ï„a(âŒˆlog2TÎ²âŒ‰) + 1 /TÎ²+ CT(Îºâˆ’1)/2, we can
conclude
P(for any aâˆˆ A,Ë†Ï„aâˆ’4/TÎ²âˆ’CT(Îºâˆ’1)/2â©½Ï„â‹†
aâ©½Ë†Ï„a)â©¾1âˆ’KâŒˆlog2TÎ²âŒ‰/TÎ±Î¶,
since whenever Ï„a(âŒˆlog2TÎ²âŒ‰)â©½Ï„â‹†
aâ©½Ï„a(âŒˆlog2TÎ²âŒ‰)â©½Ï„a(âŒˆlog2TÎ²âŒ‰) + 1/TÎ², we also have by
definition: Ë†Ï„a(âŒˆlog2TÎ²âŒ‰)âˆ’4/TÎ²âˆ’CT(Îºâˆ’1)/2â©½Ï„a(âŒˆlog2TÎ²âŒ‰)â©½Ï„â‹†
aâ©½Ë†Ï„a(âŒˆlog2TÎ²âŒ‰).
Theorem 3. Assume that H1 andH2 hold. Then BELGIC , run with Î±, Î² satisfying (13) and any
bandit subroutine Bandit-Alg , has an overall regret Rdown
p such that
Rdown
p(T,Î up
p,BELGIC )â©½2(3 + 2C + Â¯ vâˆ’v) log2(T)(2T1âˆ’Î±Î¶+T(Îº+1)/2+âŒˆTÎ±âŒ‰) + 4T1âˆ’Î²
+RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA)
where, for ease of notation
Â¯v= max
a,bâˆˆAÃ—A{vdown(a, b)}and v= min
a,bâˆˆAÃ—A{vdown(a, b)}.
Proof of Theorem 3. Suppose that the conditions of Theorem 3 are satisfied. By definition,
Î›K+1+ 1âˆˆ[T]is the step at which starts the run of the subroutine Bandit-Alg , since
Î›K+1=KâŒˆTÎ±âŒ‰âŒˆÎ²logTâŒ‰. All the binary searche batches have length ËœT=âŒˆTÎ±âŒ‰. For any
aâˆˆ A, dâˆˆ {1, . . . ,âŒˆlog2TÎ²âŒ‰}, we define the event
Ba,d=n
ËœRup
p({ka,d+ 1, . . . , k a,d+ËœT},Î up
p,Î down
p)â©½CËœTÎºo
,
as well as
E=\
aâˆˆ[K]
dâˆˆ[âŒˆÎ²logTâŒ‰]Ba,d\n
ËœRup
p({Î›K+1+ 1, . . . , T },Î up
p)â©½C(Tâˆ’Î›K+1)Îºo
,
24and by H2 and Lemma 4, with an union bound, we have that
P(E) = 1âˆ’Pï£«
ï£¬ï£¬ï£­[
aâˆˆ[K]
d=âˆˆ[âŒˆlogTÎ²âŒ‰]Bc
a,d[n
ËœRup
p({Î›K+1+ 1, . . . , T },Î up
p,Î down
p)â©½C(Tâˆ’Î›K+1)Îºoï£¶
ï£·ï£·ï£¸
â©¾1âˆ’X
aâˆˆ[K]
dâˆˆ[âŒˆlogTÎ²âŒ‰]P(Bc
a,d) +P(ËœRup
p({Î›K+1+ 1, . . . , T },Î up
p)â©½C(Tâˆ’Î›K+1)Îº)
â©¾1âˆ’KâŒˆÎ²logTâŒ‰ËœTâˆ’Î¶âˆ’(Tâˆ’Î›K+1)âˆ’Î¶
â©¾1âˆ’KâŒˆÎ²logTâŒ‰Tâˆ’Î±Î¶âˆ’Tâˆ’Î¶,
and we now decompose
Rdown
p(T,Î up
p,Î down
p) =Eh
1(E)ËœRdown
p(T,Î up
p,Î down
p)i
+h
1(Ec)ËœRdown
p(T,Î up
p,Î down
p)i
,
(23)
where ËœRdown
p is defined as
ËœRdown
p(T,Î up
p,Î down
p) =TÂµâ‹†,downâˆ’TX
t=1(vdown(At, Bt)âˆ’1Ëœat(At)Ï„(t)).
By definition, Âµâ‹†,downâ©¾mina,bâˆˆAÃ—A vdown(a, b)â©¾vand since Lemma 1 allows to write Âµâ‹†,down=
max a,bâˆˆAÃ—A{vdown(a, b) +vup(a)} âˆ’max aâ€²âˆˆA{vup(aâ€²)}, we have that
vâ©½Âµâ‹†,downâ©½1 + Â¯v ,
and note that since Îº <1, for any aâˆˆ A,Ë†Ï„aâ©½2 + C T(Îºâˆ’1)/2â©½2 + C . Consequently, Tvâ©½
ËœRdown
p(T,Î up
p,Î down
p)â©½(3 + C + Â¯ vâˆ’v)Talmost surely. Therefore
Eh
1(Ec)ËœRdown
p(T,Î up
p,Î down
p)i
â©½(3 + C + Â¯ vâˆ’v)(KâŒˆlogTÎ²âŒ‰T1âˆ’Î±Î¶+T1âˆ’Î¶).(24)
We consider the second term in (23). We decompose it between the steps of binary search
{1, . . . , K âŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰}during which we run the Binary Search Subroutine and the fol-
lowing ones when we run Bandit-Alg , which gives
Eh
1(E)ËœRdown
p(T,Î up
p,Î down
p)i
(25)
â©½Eï£®
ï£¯ï£¯ï£¯ï£¯ï£°1(E)KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰X
t=1Âµâ‹†,downâˆ’(vdown(At, Bt)âˆ’1Ëœat(At)Ï„(t))
| {z }
(A)ï£¹
ï£ºï£ºï£ºï£ºï£»
+Eï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°1(E)TX
t=KâŒˆTÎ±âŒ‰âŒˆlog2TÎ²âŒ‰+1Âµâ‹†,downâˆ’(vdown(At, Bt)âˆ’1Ëœat(At)Ï„(t))
| {z }
(B)ï£¹
ï£ºï£ºï£ºï£ºï£ºï£».
Similarly to (24), we use the bound on Âµâ‹†,downto bound (A), which gives
E[1(E)(A)]â©½Eï£®
ï£°1(E)Î›K+1X
t=11 + Â¯vâˆ’(vâˆ’2âˆ’C)ï£¹
ï£»â©½KâŒˆTÎ±âŒ‰(1 + log2T)(3 + C + Â¯ vâˆ’v).
(26)
25After the binary search, at each step tâˆˆ {Î›K+1+ 1, . . . , T },Bandit-Alg recommends (Ëœat, Bt)âˆˆ
A Ã— A following (15) andBELGIC offers an incentive (Ëœat,Ë†Ï„Ëœat)withË†Ï„a=Ï„a(âŒˆlog2TÎ²âŒ‰) + 1/TÎ²+
CT(Îºâˆ’1)/2.
Lemma 5 ensures that E âŠ† { Ï„â‹†
aâˆˆ[Ï„a(âŒˆlog2TÎ²âŒ‰),Ï„a(âŒˆlog2TÎ²âŒ‰)]for any aâˆˆ A}T{ËœRup
p({Î›K+1+
1, . . . , T },Î up
p)â©½C(Tâˆ’Î›K+1)Îº}. Therefore, if Eholds, for any aâˆˆ A, we have that
vup(a) + Ë†Ï„a=vup(a) +Ï„a+ 1/TÎ²+ CT(Îºâˆ’1)/2
> vup(a) +Ï„â‹†
a+ CT(Îºâˆ’1)/2
=vup(a) + max
aâ€²â€²âˆˆAvup(aâ€²â€²)âˆ’vup(a) + C T(Îºâˆ’1)/2,
and therefore, for any aâ€²âˆˆ A such that aâ€²Ì¸=a, we have that
vup(a) + Ë†Ï„a> vup(aâ€²) + C T(Îºâˆ’1)/2. (27)
This shows that at any steps tâˆˆ {Î›K+1+ 1, . . . , T }after the binary search, we have on the event E
that
Ëœat= argmaxaâ€²âˆˆA{vup(aâ€²) +1Ëœat(aâ€²)Ë†Ï„Ëœat}, (28)
and the reward gap at step tfor any aÌ¸= Ëœatis defined as
max
aâ€²âˆˆA{vup(aâ€²) +1Ëœat(aâ€²)Ë†Ï„Ëœat} âˆ’(vup(a) +1Ëœat(a)Ë†Ï„Ëœat) =vup(Ëœat) + Ë†Ï„aâˆ’vup(a). (29)
Following (27), the reward gap from (29) satisfies
max
aâ€²âˆˆA{vup(aâ€²) +1Ëœat(aâ€²)Ë†Ï„Ëœat} âˆ’(vup(a) +1Ëœat(a)Ë†Ï„Ëœat)â©¾CT(Îºâˆ’1)/2.
We now define two sets
IT={tâˆˆ {KâŒˆTÎ±âŒ‰âŒˆlog2TâŒ‰+ 1, . . . , T }such that Ëœat=At},
JT={tâˆˆ {KâŒˆTÎ±âŒ‰âŒˆlog2TâŒ‰+ 1, . . . , T }such that ËœatÌ¸=At},
which satisfy ITâˆªJT={KâŒˆTÎ±âŒ‰âŒˆlog2TâŒ‰+ 1, . . . , T }almost surely. As shown in (28),IT
corresponds to all the steps during which the upstream player picked the best arm and for any tâˆˆIT
vup(At) +1Ëœat(At)Ï„(t)â©¾max
aâˆˆA{vup(a) +1Ëœat(a)Ï„(t)},
while by (29), for any tâˆˆJT, we have that
max
aâˆˆA{vup(a) +1Ëœal(a)Ë†Ï„Ëœal} âˆ’(vup(Al) +1Ëœal(Al)Ë†Ï„Ëœal)â©¾CT(Îºâˆ’1)/2. (30)
H2 ensures that if Eholds, then Rup
p({Î›K+1+ 1, . . . , T },Î up
p,BELGIC )â©½CTÎº, and this condition
together with (30) gives that
Card{JT}CT(Îºâˆ’1)/2â©½Rup
p({Î›K+1+ 1, . . . , T },Î up
p,BELGIC )â©½CTÎº,
26and consequently Card {JT}â©½T(Îº+1)/2. We now bound (B)as follows
E[1(E)(B)] =Eï£®
ï£°1(E)TX
t=Î›K+1+1Âµâ‹†,downâˆ’ 
vdown(At, Bt)âˆ’Ë†Ï„Ëœatï£¹
ï£»
=E"
1(E)X
tâˆˆITmax
a,bâˆˆAÃ—A{vdown(a, b)âˆ’Ï„â‹†
a} âˆ’ 
vdown(Ëœat, Bt)âˆ’Ë†Ï„Ëœat#
+Eï£®
ï£¯ï£°1(E)X
tâˆˆJTÂµâ‹†,downâˆ’ 
vdown(At, Bt)âˆ’Ë†Ï„Ëœat
| {z }
â©½3+C+Â¯ vâˆ’vï£¹
ï£ºï£»
â©½E"
1(E)X
tâˆˆITmax
a,bâˆˆAÃ—A
vdown(a, b)âˆ’Ë†Ï„aâˆ’(vdown(Ëœat, Bt)âˆ’Ë†Ï„Ëœat)	
+ max
aâ€²âˆˆA{Ë†Ï„aâ€²âˆ’Ï„â‹†
aâ€²}#
+ (3 + C + Â¯ vâˆ’v)E[1(E)Card{JT}]
=E"
1(E)X
tâˆˆITmax
a,bâˆˆAÃ—A
vdown(a, b)âˆ’Ë†Ï„a	
âˆ’(vdown(Ëœat, Bt)âˆ’Ë†Ï„Ëœat)#
+E"
1(E)X
tâˆˆITmax
aâ€²âˆˆA{Ë†Ï„aâ€²âˆ’Ï„â‹†
aâ€²}#
+ (3 + C + Â¯ vâˆ’v)T(Îº+1)/2
â©½RBandit-Alg (Card{IT}, Î½,{Ë†Ï„a}aâˆˆA) +E
1(E)Card{IT}max
aâ€²âˆˆA{Ë†Ï„aâ€²âˆ’Ï„â‹†
aâ€²}
+ (3 + C + Â¯ vâˆ’v)T(Îº+1)/2,
where the first step holds by Lemma 1. Using Lemma 5, as well as the definition of E, we obtain
E
1(E)Card{IT}max
aâ€²âˆˆA{Ë†Ï„aâ€²âˆ’Ï„â‹†
aâ€²}
â©½Eh
1(E)Card{E}(4/TÎ²+ CT(Îºâˆ’1)/2)i
â©½E[1(E)T](4/TÎ²+ CT(Îºâˆ’1)/2)
â©½4T1âˆ’Î²+ CT(Îº+1)/2,
which finally gives
E[1(E)(B)]â©½RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA) + 4T1âˆ’Î²+ (3 + 2C + Â¯ vâˆ’v)T(1+Îº)/2, (31)
and plugging together (26) and (31) in the decomposition (25) gives the following bound
Eh
1(E)ËœRdown
p(T,Î up
p,BELGIC ))i
â©½RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA) + 4T1âˆ’Î²+ (3 + 2C + Â¯ vâˆ’v)T(1+Îº)/2
+ (3 + C + Â¯ vâˆ’v)KâŒˆTÎ±âŒ‰(1 + log2T),
and summing the bounds on the events EandEcfinally gives
Rdown
p(T,Î up
p,BELGIC )â©½RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA) + 4T1âˆ’Î²+ (3 + 2C + Â¯ vâˆ’v)T(1+Îº)/2
+ (2 + C + Â¯ vâˆ’v)KâŒˆTÎ±âŒ‰(1 + log2T)
+ (3 + C + Â¯ vâˆ’v)(K(1 + log2T)T1âˆ’Î±Î¶+T1âˆ’Î¶)
â©½RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA) + 4T1âˆ’Î²+ (3 + 2C + Â¯ vâˆ’v)T(1+Îº)/2
+ (3 + Â¯C+ Â¯vâˆ’v)((1 + log2T)(âŒˆTÎ±âŒ‰+T1âˆ’Î±Î¶) +T1âˆ’Î¶)
â©½(3 + 2C + Â¯ vâˆ’v)(T1âˆ’Î¶+T(Îº+1)/2+ (1 + log2T)(âŒˆTÎ±âŒ‰+T1âˆ’Î±Î¶))
+RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA) + 4T1âˆ’Î²
â©½2(3 + 2C + Â¯ vâˆ’v) log2(T)(2T1âˆ’Î±Î¶+T(Îº+1)/2+âŒˆTÎ±âŒ‰) + 4T1âˆ’Î²
+RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA).
27Corollary 1. Assume that the upstream playerâ€™s distribution (Î³a)aâˆˆAis such that H1 holds. In
addition, suppose that the distributions (Î³a)aâˆˆAand(Î½a,b)a,bâˆˆAÃ—A are1-sub-Gaussian and that the
upstream player plays Î up
p=Algorithm 3(a slight modification of UCBto take into account the
incentives). Then the downstream playerâ€™s regret when she runs BELGIC with parameters Î±= 3/4
andÎ²= 1/4(which satisfy (13)) and subroutine Bandit-Alg =UCBsatisfies the following upper
bound4
Rdown
p(T,UCB,BELGIC )â©½(10 + 4 K+ 32p
Klog2(KT3) + Â¯vâˆ’v) log2(T)(3 + 2 T3/4)
+ 3K2(Â¯vâˆ’v).
Proof of Corollary 1. First note that Î up
p=Algorithm 3satisfies H2 with constants Îº= 1/2,
Î¶= 2,C = 8p
Klog(KT3), following Proposition 2. Note that Î²/Î± = 1/3<1/2 = 1 âˆ’Îº,
therefore Equation (13) is satisfied. Plugging these terms in the bound from Theorem 3 with
Î±= 3/4, Î²= 1/4gives
Rdown
p(T,UCB,BELGIC )â©½2(3 + 16p
Klog2(KT3) + Â¯vâˆ’v) log2(T)(2T1âˆ’3/2+T3/4+âŒˆT3/4âŒ‰)
+ 4T1/4+ 8p
K2log2(T)T1/2+ 3K2(Â¯vâˆ’v)
where we use the bound for RBandit-Alg (T, Î½,{Ë†Ï„a}aâˆˆA)with Bandit-Alg =UCBrun on any bandit
instance with K2arms, 1-subgaussian rewards and reward gaps of at most max a,bâˆˆAÃ—A vdown(a, b)âˆ’
mina,bâˆˆAÃ—A vdown(a, b) = Â¯vâˆ’v, following [Lattimore and SzepesvÃ¡ri, 2020, Theorem 7.2]. There-
fore, we have that
Rdown
p(T,UCB,BELGIC )â©½(6 + 32p
Klog2(KT3) + Â¯vâˆ’v) log2(T)(2 + 1 + 2 T3/4)
+ 4T1/4+ 3K2(Â¯vâˆ’v) + 8Kp
log2TT1/2
â©½(10 + 32p
Klog2(KT3) + Â¯vâˆ’v) log2(T)(3 + 2 T3/4)
+ 3K2(Â¯vâˆ’v) + 8Kp
log2(T)T1/2,
which finally gives
Rdown
p(T,UCB,BELGIC )â©½(10 + 4 K+ 32p
Klog2(KT3) + Â¯vâˆ’v) log2(T)(3 + 2 T3/4)
+ 3K2(Â¯vâˆ’v),
hence the result.
4Note that it is Kand notâˆš
Khere, since the action space is of cardinality K2for the downstream player.
28NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
â€¢ You should answer [Yes] , [No] , or [NA] .
â€¢[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
â€¢ Please provide a short (1â€“2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
â€¢Delete this instruction block, but keep the section heading â€œNeurIPS paper checklist" ,
â€¢Keep the checklist subsection headings, questions/answers and guidelines below.
â€¢Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: The abstract mostly presents the issue that we tackle in the paper, namely
presenting an online version of the Coase theorem . This contribution is the most important
part of our paper.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
29Justification: Our work presents a model to explore an online version of a theory from
economics. Thereby, it implicitly has limitations due to the fact that a choice was made in
the model.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: Our work presents a lot of theorems supported by assumptions (see H2,
Theorems 2 and 3...). All of Appendix C is here to provide a theoretical support to these
results.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
30Justification: Since our work is mostly a theoretic contribution, we do not present experi-
ments. Therefore, our paper is not concerned by this issue.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
31â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g., negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
32Answer: [NA]
Justification: Since our work is mostly a theoretic contribution, we do not present exper-
iments. Therefore, our paper is not concerned by this issue (same answer as for item
5).
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: As explained in the Related Works, an issue with externalities is the harm
caused to all the parties involved in the considered setting. Here, we try to provide a setup
so the players can negociate and achieve a social welfare optimal equilibrium. Therefore,
we are confident in the fact that we follow the NeurIPS Code of Ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We consider an issue arising in several real-world situations and provide an
algorithmic solution to it. We realize that this theoretical contribution still needs some work
to be implemented but we hope that it will contribute to positive social impacts in the future.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
33â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [Yes]
Justification: Our work is a theoretical contribution. Thereby, it does not propose a release
of data or any kind of trained model. We mix learning and theories from economics, which
does not involve yet risks for misuse.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: We do not use any assets requiring such conditions here.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: We do not present such kind of new assets.
Guidelines:
34â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: We do not incorporate any experiment involving human subjects. Therefore,
we are not concerned by this item.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: We do not incorporate any experiment involving human subjects, hence our
answer.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35