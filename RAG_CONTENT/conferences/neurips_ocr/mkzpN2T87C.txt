Non-asymptotic Global Convergence Analysis of BFGS
with the Armijo-Wolfe Line Search
Qiujiang Jin
ECE, UT Austin
qiujiangjin0@gmail.comRuichen Jiang
ECE, UT Austin
rjiang@utexas.eduAryan Mokhtari
ECE, UT Austin
mokhtari@austin.utexas.edu
Abstract
In this paper, we present the first explicit and non-asymptotic global convergence
rates of the BFGS method when implemented with an inexact line search scheme
satisfying the Armijo-Wolfe conditions. We show that BFGS achieves a global
linear convergence rate of (1âˆ’1
Îº)tforÂµ-strongly convex functions with L-Lipschitz
gradients, where Îº=L
Âµrepresents the condition number. Additionally, if the
objective functionâ€™s Hessian is Lipschitz, BFGS with the Armijo-Wolfe line search
achieves a linear convergence rate that depends solely on the line search parameters,
independent of the condition number. We also establish a global superlinear
convergence rate of O((1
t)t). These global bounds are all valid for any starting
point x0and any symmetric positive definite initial Hessian approximation matrix
B0, though the choice of B0impacts the number of iterations needed to achieve
these rates. By synthesizing these results, we outline the first global complexity
characterization of BFGS with the Armijo-Wolfe line search. Additionally, we
clearly define a mechanism for selecting the step size to satisfy the Armijo-Wolfe
conditions and characterize its overall complexity.
1 Introduction
In this paper, we focus on solving the following unconstrained convex minimization problem
min
xâˆˆRdf(x), (1)
where f:Rdâ†’Ris strongly convex and twice differentiable. Quasi-Newton methods are among the
most popular algorithms for solving this class of problems due to their simplicity and fast convergence.
Like gradient descent-type methods, they require only gradient information for implementation, while
they aim to mimic the behavior of Newtonâ€™s method by using gradient information to approximate
the curvature of the objective function. There are several variations of quasi-Newton methods,
primarily distinguished by their update rules for the Hessian approximation matrices. The most
well-known among these include the Davidon-Fletcher-Powell (DFP) method [1, 2], the Broyden-
Fletcher-Goldfarb-Shanno (BFGS) method [3â€“6], the Symmetric Rank-One (SR1) method [7, 8], and
the Broyden method [9]. Apart from these classical methods, other variants have also been proposed
in the literature, including randomized quasi-Newton methods [10â€“14], greedy quasi-Newton methods
[13â€“16], and those based on online learning techniques [17, 18]. In this paper, we mainly focus on the
global analysis of the BFGS method, arguably the most successful quasi-Newton method in practice.
The classic analyses of BFGS, including [19â€“28], primarily focused on demonstrating local asymp-
totic superlinear convergence without addressing an explicit global convergence rate when BFGS is
deployed with a line-search scheme. While attempts have been made to establish global convergence
for quasi-Newton methods using line search or trust-region techniques in previous studies [8, 29â€“33],
these efforts provided only asymptotic convergence guarantees without explicit global convergence
rates, thus not fully characterizing the global convergence rate of classical quasi-Newton methods.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).In recent years, there have been efforts to characterize the explicit convergence rate of BFGS within a
local neighborhood of the solution, establishing a superlinear convergence rate of the form (1âˆš
t)t;
see, for example, [34â€“37]. However, these results focus solely on local convergence analysis of
BFGS under conditions where the stepsize is consistently set to one, the iterate remains close to the
optimal solution, and the initial Hessian approximation matrix meets certain necessary conditions.
Consequently, these analyses do not extend to providing a global convergence guarantee. For more
details on this subject, we refer the reader to the discussion section in [38].
To the best of our knowledge, only few papers are closely related to our work and establish a global
non-asymptotic guarantee for BFGS. In [39], it was shown that BFGS with exact line search achieves
a global linear rate of (1âˆ’2Âµ3
L3(1 +ÂµTr(Bâˆ’1
0)
t)âˆ’1(1 +Tr(B0)
Lt)âˆ’1)t, where Âµis the strong convexity
parameter, Lis the Lipschitz constant of the gradient, B0is the initial Hessian approximation matrix,
andTr(Â·)denotes the trace of a matrix. After t=O(d)iterations, this rate approaches (1âˆ’2Âµ3
L3)t,
which is significantly slower than the convergence rate of gradient descent. Additionally, a recent
draft in [40] studied the global convergence of BFGS under an inexact line search. While this work
establishes a local superlinear rate, it only shows a global linear rate of the form (1âˆ’Âµ2
L2)t. Hence,
both these results fail to prove any global advantage for BFGS over gradient descent. In [38], the
authors improved upon [39] by showing a better global linear convergence rate and a faster superlinear
rate for BFGS with exact line search. Specifically, for an L-Lipschitz and Âµ-strongly convex function,
BFGS initialized with B0=LIachieves a global linear rate of (1âˆ’Âµ3/2
L3/2)tfortâ‰¥1, while BFGS
withB0=ÂµIachieves the same rate after dlogÎºiterations. With the additional assumption that the
objectiveâ€™s Hessian is Lipschitz, an improved linear rate of (1âˆ’Âµ
L)tis achieved after O(Îº)iterations
when B0=LIand after O(dlogÎº+Îº)when B0=ÂµI, matching the rate of gradient descent. A
superlinear rate of (1/âˆš
t)twas also shown when the number of iterations exceeds specific thresholds.
Contributions. In this paper, we analyze the BFGS method combined with the Armijo-Wolfe line
search, the most commonly used line search criteria in practical BFGS applications; see, e.g., [41].
For minimizing an L-smooth and Âµ-strongly convex function, we present a global convergence rate of
(1âˆ’Âµ
L)t. To the best of our knowledge, this is the first result demonstrating a global linear convergence
rate for BFGS that matches the rate of gradient descent under these assumptions. Furthermore, we
show that if the objective functionâ€™s Hessian is Lipschitz continuous, BFGS with the Armijo-Wolfe
line search converges at a linear rate determined solely by the line search parameters and not the
problemâ€™s condition number, Îº=L/Âµ, when the number of iterations is sufficiently large. Finally,
we prove a global non-asymptotic superlinear convergence rate of (h(d,Îº,C 0)/t)t, where h(d, Îº, C 0)
depends on the condition number Îº, the dimension d, and the weighted distance between the initial
point x0and the optimal solution xâˆ—, denoted by C0. We summarize our results in Table 1. By
combining these convergence results, we establish the total iteration complexity of BFGS with the
Armijo-Wolfe line search. We also specify the line search complexity by investigating a bisection
algorithm for choosing the step size that satisfies the Armijo-Wolfe conditions. Our result is one
of the first non-asymptotic analysis characterizing the global convergence complexity of the BFGS
quasi-Newton method with an inexact line search.
Notation. We denote the â„“2-norm by âˆ¥ Â· âˆ¥, the set of dÃ—dsymmetric positive definite matrices by
Sd
++, and use Aâª¯Bto mean Bâˆ’Ais symmetric positive semi-definite. The trace and determinant
of a matrix Aare represented as Tr(A)andDet(A), respectively.
2 Preliminaries
In this section, we present the assumptions, notations, and intermediate results useful for the global
convergence analysis. First, we state the following assumptions on the objective function f.
Assumption 2.1. The function fis twice differentiable and strongly convex with parameter Âµ >0.
Assumption 2.2. The gradient of fis Lipschitz continuous with parameter L >0.
These assumptions are common in the convergence analysis of quasi-Newton methods. Under these,
we show a global linear convergence rate of O((1âˆ’Âµ
L)t). To achieve a faster linear convergence rate
that is independent of the problem condition number, and a global superlinear rate, we require an
additional assumption that the objective function Hessian is Lipschitz continuous, as stated next.
Assumption 2.3. The Hessian of fis Lipschitz continuous with parameter M > 0, i.e., for x, yâˆˆRd,
we have âˆ¥âˆ‡2f(x)âˆ’ âˆ‡2f(y)âˆ¥ â‰¤Mâˆ¥xâˆ’yâˆ¥.
2Initial Matrix Convergence Phase Convergence Rate Starting moment
B0 Linear phase I 
1âˆ’1
ÎºtÎ¨(Â¯B0)
B0 Linear phase II 
1âˆ’1
3tÎ¨(ËœB0) +C0Î¨(Â¯B0) +C0Îº
B0 Superlinear phase
Î¨(ËœB0)+C0Î¨(Â¯B0)+C0Îº
tt
Î¨(ËœB0) +C0Î¨(Â¯B0) +C0Îº
LI Linear phase I 
1âˆ’1
Îºt1
LI Linear phase II 
1âˆ’1
3tdÎº+C0Îº
LI Superlinear phase dÎº+C0Îº
ttdÎº+C0Îº
ÂµI Linear phase I 
1âˆ’1
ÎºtdlogÎº
ÂµI Linear phase II 
1âˆ’1
3t(1 +C0)dlogÎº+C0Îº
ÂµI Superlinear phase
(1+C0)dlogÎº+C0Îº
tt
(1 +C0)dlogÎº+C0Îº
Table 1: Summary of our results for (i) an arbitrary positive definite B0, (ii)B0=LI, and (iii) B0=
ÂµI. Here, Î¨(A) :=Tr(A)âˆ’dâˆ’logDet(A),Â¯B0=1
LB0andËœB0=âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2.
The last column shows the number of iterations required to achieve the corresponding linear or
superlinear convergence phase. For brevity, the absolute constants are dropped.
Note that the above regularity condition on the Hessian assumption is also common for establishing
the superlinear convergence rate of quasi-Newton methods [19â€“28].
BFGS Update. Next, we state the general update rule of BFGS. If we denote xtas the iterate at
timet, the vector gt=âˆ‡f(xt)as the objective function gradient at xt, and Btas the Hessian
approximation matrix at step t, then the update is given by
xt+1=xt+Î·tdt, d t=âˆ’Bâˆ’1
tgt, (2)
where Î·t>0is the step size and dtis the descent direction. By defining the variable difference
st:=xt+1âˆ’xtand the gradient difference yt:=âˆ‡f(xt+1)âˆ’ âˆ‡f(xt), we can present the Hessian
approximation matrix update for BFGS as follows:
Bt+1=Btâˆ’BtstsâŠ¤
tBt
sâŠ¤
tBtst+ytyâŠ¤
t
sâŠ¤
tyt. (3)
To avoid the costly operation of inverting the matrix Bt, one can define the inverse Hessian approxi-
mation matrix as Ht:=Bâˆ’1
tand apply the Sherman-Morrison-Woodbury formula to obtain
Ht+1:=
Iâˆ’styâŠ¤
t
yâŠ¤
tst
Ht
Iâˆ’ytsâŠ¤
t
sâŠ¤
tyt
+stsâŠ¤
t
yâŠ¤
tst. (4)
It is well-known that for a strongly convex objective function, the Hessian approximation matrices
Btremain symmetric and positive definite if the initial matrix B0is symmetric positive definite [41].
Therefore, all matrices BtandHtare symmetric positive definite throughout this paper.
As mentioned earlier, establishing a global convergence guarantee for BFGS requires pairing it with
a line search scheme to select the stepsize Î·t. This paper focuses on implementing BFGS with the
Armijo-Wolfe line search, detailed in the following subsection.
Armijo-Wolfe Line Search. We consider a stepsize Î·t>0that satisfies the Armijo-Wolfe conditions
f(xt+Î·tdt)â‰¤f(xt) +Î±Î·tâˆ‡f(xt)âŠ¤dt, (5)
âˆ‡f(xt+Î·tdt)âŠ¤dtâ‰¥Î²âˆ‡f(xt)âŠ¤dt, (6)
where Î±andÎ²are the line search parameters, satisfying 0< Î± < Î² < 1and0< Î± <1
2. The
condition in (5)is the Armijo condition, ensuring that the step size Î·tprovides a sufficient decrease
in the objective function f. The condition in (6)is the curvature condition, which guarantees that
the slope âˆ‡f(xt+Î·tdt)âŠ¤dtatÎ·tis not strongly negative, indicating that further movement along dt
would significantly decrease the function value. These conditions provide upper and lower bounds on
the admissible step size Î·t. In some references, the Armijo-Wolfe line search conditions are known
as the weak Wolfe conditions [42, 43]. The procedure for finding Î·tthat satisfies these conditions is
described in Section 7. Next lemma presents key properties of the Armijo-Wolfe conditions.
3Lemma 2.1. Consider the BFGS method with Armijo-Wolfe inexact line search, where the step size
satisfies the conditions in (5)and(6). Then, for any initial point x0and any symmetric positive
definite initial Hessian approximation matrix B0, the following results hold for all tâ‰¥0:
f(xt)âˆ’f(xt+1)
âˆ’gâŠ¤
tstâ‰¥Î±,yâŠ¤
tst
âˆ’gâŠ¤
tstâ‰¥1âˆ’Î², and f(xt+1)â‰¤f(xt). (7)
Remark 2.1. While in this paper we only focus on the Armijo-Wolfe line search, our results are also
valid for some other line search schemes that require stricter conditions. For instance, in the strong
Wolfe line search, given 0< Î± < Î² < 1and0< Î± <1
2, the required conditions for the step size are
f(xt+Î·tdt)â‰¤f(xt) +Î±Î·tâˆ‡f(xt)âŠ¤dt,|âˆ‡f(xt+Î·tdt)âŠ¤dt| â‰¤Î²âˆ‡f(xt)âŠ¤dt,
Indeed, if Î·tsatisfies the strong Wolfe conditions, it also satisfies the Armijo-Wolfe conditions.
Another commonly employed line search scheme is Armijoâ€“Goldstein, which imposes the conditions
âˆ’c1Î·tâˆ‡f(xt)âŠ¤dtâ‰¤f(xt)âˆ’f(xt+Î·tdt)â‰¤ âˆ’c2Î·tâˆ‡f(xt)âŠ¤dt,
with0< c1â‰¤c2<1. The lower bound on f(xt)âˆ’f(xt+Î·tdt)in the Armijoâ€“Goldstein line
search indicates that Î·tsatisfies the sufficient decrease condition in (5)required for the Armijo-Wolfe
conditions, with Î±=c1. Moreover, given the convexity of f, the upper bound on f(xt)âˆ’f(xt+Î·tdt)
in the Armijoâ€“Goldstein line search suggests âˆ’Î·tâˆ‡f(xt+Î·tdt)âŠ¤dtâ‰¤f(xt)âˆ’f(xt+Î·tdt)â‰¤
âˆ’c2Î·tâˆ‡f(xt)âŠ¤dt. Thus, Î·talso meets the curvature condition in (6)required in the Armijo-Wolfe
conditions with Î²=c2. Hence, all our results derived under the Armijo-Wolfe line search are also
valid for both the strong Wolfe line search and the Armijoâ€“Goldstein line search.
3 Convergence Analysis
In this section, we present our theoretical framework for analyzing the global linear convergence
rates of BFGS with the Armijo-Wolfe line search scheme. To start, we introduce some necessary
definitions and notations. We define the average Hessian matrices JtandGtas
Jt:=Z1
0âˆ‡2f(xt+Ï„(xt+1âˆ’xt))dÏ„, G t:=Z1
0âˆ‡2f(xt+Ï„(xâˆ—âˆ’xt))dÏ„. (8)
Further, for measuring the suboptimality of the iterates we define the sequence Ctas
Ct:=M
Âµ3
2p
2(f(xt)âˆ’f(xâˆ—)),âˆ€tâ‰¥0, (9)
where Mis the Lipschitz constant of the Hessian defined in Assumption 2.3 and Âµis the strong con-
vexity parameter introduced in Assumption 2.1.To analyze the dynamics of the Hessian approximation
matrices {Bt}+âˆ
t=0, we use the function Î¨(A)
Î¨(A) :=Tr(A)âˆ’dâˆ’logDet(A), (10)
well-defined for any AâˆˆSd
++. It was introduced in [32] to capture the discrepancy between Aand
the identity matrix I. Note that Î¨(A)â‰¥0for any AâˆˆSd
++andÎ¨(A) = 0 if and only if A=I.
Before we start convergence analysis, given any weight matrix PâˆˆSd
++, we define the weighted
versions of the vectors gt,st,yt,dtand the matrix Bt,Jtas
Ë†gt=Pâˆ’1
2gt, Ë†st=P1
2st, Ë†yt=Pâˆ’1
2yt, Ë†dt=P1
2dt. (11)
Ë†Bt=Pâˆ’1
2BtPâˆ’1
2, Ë†Jt=Pâˆ’1
2JtPâˆ’1
2. (12)
Note that these weighted matrices and vectors preserve many properties of their unweighted coun-
terparts. For instance, two of these main properties are Ë†gâŠ¤
tË†st=gâŠ¤
tstandË†yâŠ¤
tË†st=yâŠ¤
tst. Similarly,
the update for the weighted version of Hessian approximation matrices closely mirrors the update of
their unweighted counterparts, as noted in the following expression:
Ë†Bt+1=Ë†Btâˆ’Ë†BtË†stË†sâŠ¤
tË†Bt
Ë†sâŠ¤
tË†BtË†st+Ë†ytË†yâŠ¤
t
Ë†sâŠ¤
tË†yt,âˆ€tâ‰¥0. (13)
Finally, we define a crucial quantity, Ë†Î¸t, which measures the angle between the weighted descent
direction and the negative of the weighted gradient direction, satisfying
cos(Ë†Î¸t) =âˆ’Ë†gâŠ¤
tË†st
âˆ¥Ë†gtâˆ¥âˆ¥Ë†stâˆ¥. (14)
43.1 Intermediate Results
In this section, we present our framework for analyzing the convergence of BFGS with an inexact line
search. We first characterize the relationship between the function value decrease at each iteration
and key quantities, including the angle Ë†Î¸tdefined in (14).
Proposition 3.1. Let{xt}tâ‰¥0be the iterates generated by BFGS. Recall the definitions of weighted
vectors in (11). Then, for any weight matrix Pand for all tâ‰¥1, we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’tâˆ’1Y
i=0Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†mi1
tt
. (15)
where Ë†pt,Ë†qt,Ë†mtandË†ntare defined as
Ë†pt:=f(xt)âˆ’f(xt+1)
âˆ’Ë†gâŠ¤
tË†st,Ë†qt:=âˆ¥Ë†gtâˆ¥2
f(xt)âˆ’f(xâˆ—),Ë†mt:=Ë†yâŠ¤
tË†st
âˆ¥Ë†stâˆ¥2,Ë†nt=Ë†yâŠ¤
tË†st
âˆ’Ë†gâŠ¤
tË†st.(16)
This result shows the convergence rate of BFGS with Armijo-Wolfe line search depends on four
products:Qtâˆ’1
i=0Ë†pi,Qtâˆ’1
i=0Ë†qi,Qtâˆ’1
i=0Ë†ni, andQtâˆ’1
i=0cos2(Ë†Î¸i)
Ë†mi. To establish an explicit rate, we need
lower bounds on these products. Lemma 2.1 shows that the lower bounds forQtâˆ’1
i=0Ë†piandQtâˆ’1
i=0Ë†ni
depend on the inexact line search parameters Î±andÎ². We will further prove that if the unit step size
Î·t= 1satisfies the Armijo-Wolfe conditions, better lower bounds can be obtained for these products.
The lower bounds forQtâˆ’1
i=0Ë†qiandQtâˆ’1
i=0cos2(Ë†Î¸i)
Ë†miwere established in previous work [38] as presented
in Appendix D. Specifically, the bounds forQtâˆ’1
i=0Ë†qidepend on the choice of the weight matrix,
which varies in different sections of the paper, requiring separate bounds for each case. However,
the bound forQtâˆ’1
i=0cos2(Ë†Î¸i)
Ë†midoes not require separate treatment. This is explicitly established in
Proposition D.1, a classical result, as discussed in [41, Section 6.4]. We build all our linear and
superlinear results by establishing different bounds on the terms in (15).
4 Global Linear Convergence Rates
Building on the tools introduced in Section 3, we establish explicit global linear convergence rates for
BFGS with the Armijo-Wolfe line search, requiring only the strong convexity and gradient Lipschitz
conditions from Assumptions 2.1 and 2.2. Our proof leverages the fundamental inequality in (15)
from Proposition 3.1 and lower bounds on the terms that appear in the contraction factor. Here, we
set the weight matrix PtoP=LIand hence define the initial weighted matrix Â¯B0asÂ¯B0=1
LB0.
The following theorem presents our first global linear convergence rate of BFGS for any B0âˆˆSd
++.
Theorem 4.1. Suppose Assumptions 2.1 and 2.2 hold. Let {xt}tâ‰¥0be the iterates generated by
BFGS, where the step size satisfies the Armijo-Wolfe conditions in (5)and(6). For any initial point
x0âˆˆRdand any initial Hessian approximation matrix B0âˆˆSd
++, we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’eâˆ’Î¨(Â¯B0)
t2Î±(1âˆ’Î²)
Îºt
,âˆ€tâ‰¥1. (17)
Remark 4.1. In [38], the authors analyzed BFGS with exact line search and established a global
linear rate of (1âˆ’eâˆ’Î¨(Â¯B0)
t1
Îº(1+âˆšÎº))t. In comparison, our result in (17) achieves a faster linear
rate by eliminating theâˆšÎºfactor in the denominator. This improvement arises from using the
Armijo-Wolfe conditions. Specifically, under these conditions, we showf(xt)âˆ’f(xt+1)
âˆ’gâŠ¤
tstâ‰¥Î±as shown
in Lemma 2.1, where Î±âˆˆ(0,1/2)is a line search parameter. In contrast, using exact line search, the
authors in [38] proved thatf(xt)âˆ’f(xt+1)
âˆ’gâŠ¤
tstâ‰¥2âˆšÎº+1, thus leading to the extraâˆšÎºfactor in their rate.
From Theorem 4.1, we observe that the linear convergence rate is determined by the quantity Î¨(Â¯B0)
Thus, to simplify our bounds, we consider two different initializations: B0=LIandB0=ÂµI.
Corollary 4.2. Suppose Assumptions 2.1 and 2.2 hold, {xt}tâ‰¥0are generated by BFGS with step
size satisfying the Armijo-Wolfe conditions in (5)and(6), and x0âˆˆRdis an arbitrary initial point.
5â€¢If the initial Hessian approximation matrix is set as B0=LI, then for any tâ‰¥1
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’2Î±(1âˆ’Î²)
Îºt
. (18)
â€¢If the initial Hessian approximation matrix is set as B0=ÂµI, then for any tâ‰¥1we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤(1âˆ’eâˆ’dlogÎº
t2Î±(1âˆ’Î²)
Îº)t. Moreover, for tâ‰¥dlogÎº, we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’2Î±(1âˆ’Î²)
3Îºt
. (19)
Corollary 4.2 shows that when initialized with B0=LI, BFGS achieves a linear rate of O((1âˆ’1
Îº)t)
from the first iteration, matching the rate of gradient descent. It also indicates that initializing with
B0=ÂµIachieves a similar rate but after dlogÎºiterations. While this suggests a preference for
initializing with B0=LI, subsequent analysis reveals that with enough iterations, BFGS with either
initialization can attain a faster linear rate independent of Îº. In some cases, starting with B0=ÂµI
may lead to fewer total iterations to achieve this faster rate. We will explore this trade-off later.
5 Condition Number Independent Linear Convergence Rates
In this section, we improve the previous results and establish a non-asymptotic, condition number-
free global linear convergence rate for BFGS with the Armijo-Wolfe line search. This requires the
additional assumption that the Hessian is Lipschitz continuous. Our analysis builds on the previous
methodology but uses P=âˆ‡2f(xâˆ—)instead of P=LIto prove the condition number-independent
global linear rate. Thus, the weighted initial matrix ËœB0isâˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2. Next, we
present a general global convergence bound for any initial Hessian approximation B0âˆˆSd
++.
Proposition 5.1. Suppose Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}tâ‰¥0be the iterates generated
by BFGS with the step size satisfying the Armijo-Wolfe conditions in (5)and(6). Recall the definition
ofCtin(9)andÎ¨(Â·)in(10). For any initial point x0âˆˆRdand any initial Hessian approximation
matrix B0âˆˆSd
++, the following result holds:
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’2Î±(1âˆ’Î²)eâˆ’Î¨(ËœB0)+3Ptâˆ’1
i=0Ci
tt
,âˆ€tâ‰¥1.
Proposition 5.1 demonstrates that the convergence rate of BFGS with the Armijo-Wolfe line search is
influenced by Î¨(ËœB0)and the sumPtâˆ’1
i=0Ci. The first term Î¨(ËœB0)is a constant that depends on our
choice of the initial Hessian approximation matrix B0. The second termPtâˆ’1
i=0Cican also be upper
bounded using the non-asymptotic global linear convergence rate provided in Theorem 4.1.
Theorem 5.2. Suppose Assumptions 2.1, 2.2 and 2.3 hold, and let {xt}tâ‰¥0be the iterates generated
by BFGS with the Armijo-Wolfe line search in (5)and(6). Then, for any initial point x0âˆˆRdand
any initial Hessian approximation B0âˆˆSd
++, iftâ‰¥Î¨(ËœB0) + 3C0Î¨(Â¯B0) +9
Î±(1âˆ’Î²)C0Îº, we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’2Î±(1âˆ’Î²)
3t
. (20)
This result shows that when the number of iterations meets tâ‰¥Î¨(ËœB0) + 3C0Î¨(Â¯B0) +9
Î±(1âˆ’Î²)C0Îº,
BFGS with Armijo-Wolfe conditions achieves a condition number-independent linear rate. The choice
ofB0is critical as it influences the required iterations through ËœB0=âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2and
Â¯B0=1
LB0. Different choices of B0affect Î¨(ËœB0) + 3C0Î¨(Â¯B0)and thus the number of iterations
needed for condition-free linear convergence. While optimizing B0to minimize Î¨(ËœB0) + 3C0Î¨(Â¯B0)
is possible, we focus on two practical initialization schemes: B0=LIandB0=ÂµI.
Corollary 5.3. Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}tâ‰¥0be the iterates generated
by the BFGS method, where the step size satisfies the Armijo-Wolfe conditions in (5)and(6), and
x0âˆˆRdas an arbitrary initial point. Then, given the result in Theorem 5.2, we have
â€¢If we set B0=LI, the rate in (20) holds for tâ‰¥dÎº+9
Î±(1âˆ’Î²)C0Îº,
â€¢If we set B0=ÂµI, the rate in (20) holds for tâ‰¥(1 + 3 C0)dlogÎº+9
Î±(1âˆ’Î²)C0Îº.
Based on Corollary 5.3, if C0â‰ªÎº, or equivalently f(x0)âˆ’f(xâˆ—)â‰ªL2Âµ
M2, then BFGS with B0=ÂµI
requires less iterations to achieve the condition number-independent linear convergence rate.
66 Global Superlinear Convergence Rates
In this section, we present our global superlinear result. Consider the definition ËœB0=
âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2as well as the definition of Ïtwhich is given by
Ït:=âˆ’gâŠ¤
tdt
âˆ¥Ëœdtâˆ¥2, Ëœdt:=âˆ‡2f(xâˆ—)1
2dt,âˆ€tâ‰¥0. (21)
To motivate, let us briefly discuss why we are only able to show a linear convergence rate instead
of a superlinear rate in Theorem 5.2. By inspecting the proof, we observe that the bottleneck is due
to the lower bounds on Ë†ptandË†nt: we used Ë†ptâ‰¥Î±andË†ntâ‰¥1âˆ’Î²from Lemma 2.1, which leads
to the constant factor Î±(1âˆ’Î²)in the final linear rate in Theorem 5.2. Thus, to show a superlinear
convergence rate, we need to establish tighter lower bounds for Ë†ptandË†nt. In the following lemma,
we show that if the step size Î·t= 1, we are able to establish such tighter lower bounds.
Lemma 6.1. Recall Ë†pt=f(xt)âˆ’f(xt+1)
âˆ’Ë†gâŠ¤
tË†standË†nt=Ë†yâŠ¤
tË†st
âˆ’Ë†gâŠ¤
tË†stdefined in (16). If the unit step size Î·t= 1
satisfies the Armijo-Wolfe conditions (5)and(6), then we have
Ë†ptâ‰¥1âˆ’1 +Ct
2Ït, Ë†ntâ‰¥1
(1 +Ct)Ït. (22)
In contrast to the constant lower bounds in Lemma 2.1, the lower bounds in (22) depend on CtandÏt.
Later, we show Ctâ†’0andÏtâ†’1. Hence, the lower bounds in (22) approach 1 as the number of
iterations increases, enabling us to prove a superlinear rate. That said, the lower bounds in Lemma 6.1
hold only when Î·t= 1. To complete the picture, we need to quantify when and how often the unit
step size is selected during BFGS execution. This is addressed in the next lemmas.
Lemma 6.2. Suppose Assumptions 2.1, 2.2, and 2.3 hold and define the constants
Î´1:=min1
6,p
2(1âˆ’Î±)âˆ’1,1âˆš1âˆ’Î²âˆ’1
, Î´2:= max {7
8,1p
2(1âˆ’Î±)}, Î´3:=1âˆš1âˆ’Î²,(23)
which satisfy 0< Î´1< Î´2<1< Î´3. IfCtâ‰¤Î´1andÎ´2â‰¤Ïtâ‰¤Î´3, then Î·t= 1 satisfies the
Armijo-Wolfe conditions (5)and(6).
Lemma 6.2 shows that when Ctâ‰¤Î´1andÏtfalls within the interval [Î´2, Î´3], the step size Î·t= 1
is admissible and meets the Armijo-Wolfe conditions. Note that by the linear convergence result
in Theorem 4.1, the first condition on Ctwill be satisfied when tis sufficiently large. Additionally,
using Proposition G.2 in the Appendix, we can show that the second condition on Ïtis violated only
for a finite number of iterations. These observations are formally presented in the following lemma.
Lemma 6.3. Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates {xt}tâ‰¥0are generated
by the BFGS method with step size satisfying the Armijo-Wolfe conditions in (5)and(6). Recall Ct
defined in (9),Î¨(Â·)defined in (10),{Î´i}3
i=1defined in (23) andÂ¯B0=1
LB0. We have Ctâ‰¤Î´1when
tâ‰¥t0:= max
Î¨(Â¯B0),3Îº
Î±(1âˆ’Î²)logC0
Î´1
. (24)
Moreover, if we define Ï‰(x) =xâˆ’log(1 + x), the size of the set I={t:Ït/âˆˆ[Î´2, Î´3]}is at most
|I| â‰¤Î´4
Î¨(ËœB0) + 2C0Î¨(Â¯B0) +6C0Îº
Î±(1âˆ’Î²)
,where Î´4:=1
min{Ï‰(Î´2âˆ’1), Ï‰(Î´3âˆ’1)}.(25)
Lemma 6.3 implies that conditions Ctâ‰¤Î´1andÏtâˆˆ[Î´2, Î´3]will be satisfied for all but a finite
number of iterations. Thus, if the line search always starts by testing the unit step size (as shown
in Section 7), we will choose Î·t= 1, and accordingly, the tighter lower bound in Lemma 6.1 will
apply for all but a finite number of iterations. By applying these lower bounds along with (15) from
Proposition 3.1, we can prove a global superlinear convergence rate, as presented next.
Remark 6.1. Lemmas 6.2 and 6.3 are inspired by the analysis in [40]. Specifically, Lemma 5.10 of
[40] characterized the conditions on CtandÏtunder which Î·= 1satisfies the Armijo condition (5),
and further bounded the number of iterations where these conditions are violated. However, our
Lemma 6.2 addresses both the Armijo condition in (5)and the curvature condition in (6), and the
arguments appear simpler. Additionally, our proof for the superlinear convergence rate differs from
[40]. Their approach analyzed the Dennis-MorÃ© ratio and measured â€œlocalâ€ superlinear convergence
using the distance âˆ¥âˆ‡f(xâˆ—)1
2(xtâˆ’xâˆ—)âˆ¥. In contrast, our â€œglobalâ€ result is based on the unified
framework in Proposition 3.1 and uses the function value gap as a measure of convergence.
7Theorem 6.4. Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates {xt}tâ‰¥0are generated
by BFGS with step size satisfying the Armijo-Wolfe conditions in (5)and(6). Recall the definition
ofCtin(9),Î¨(Â·)in(10),Â¯B0:=1
LB0,ËœB0:=âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2, and Î´1, Î´2, Î´3, Î´4in(23)
and(25). Then, for any x0âˆˆRdand any B0âˆˆSd
++, the following global superlinear result holds:
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ 
Î´7Î¨(ËœB0) + (Î´6+Î´8C0)Î¨(Â¯B0) + (3Î´6
Î±(1âˆ’Î²)logC0
Î´1+3Î´8
Î±(1âˆ’Î²)C0)Îº
t!t
,(26)
where {Î´i}8
i=5defined below are constants that only depend on line search parameters Î±andÎ²,
Î´5:=max{2 +2
Î´2,4Î´3}
2Î´2âˆ’1âˆ’Î´1, Î´6:=log1
2Î±(1âˆ’Î²), Î´7:=1+ Î´4Î´6+Î´5, Î´8:=1+2 Î´7+2Î´2âˆ’Î´1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1.
The above result shows a global superlinear convergence rate of the form O((Câ€²
t)t), where Câ€²depends
on the condition number Îº, the initial weighted distance C0, and the initial Hessian approximation
matrix B0. To simplify the expression, we report the above bound for B0=LIandB0=ÂµI.
Corollary 6.5. Suppose Assumptions 2.1, 2.2, and 2.3 hold and the iterates {xt}tâ‰¥0are generated by
the BFGS method with step size satisfying the Armijo-Wolfe conditions in (5)and(6), and x0âˆˆRd
as an arbitrary initial point. Then, given the result in Theorem 6.4, the following results hold:
â€¢If we set B0=LI, then we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ 
Î´7dÎº+ (3Î´6
Î±(1âˆ’Î²)logC0
Î´1+3Î´8
Î±(1âˆ’Î²)C0)Îº
t!t
. (27)
â€¢If we set B0=ÂµI, then we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ 
(Î´6+Î´7+Î´8C0)dlogÎº+ (3Î´6
Î±(1âˆ’Î²)logC0
Î´1+3Î´8
Î±(1âˆ’Î²)C0)Îº
t!t
.(28)
This result shows that BFGS with B0=LIachieves a global superlinear rate of O((dÎº+C0Îº
t)t), while
BFGS with the initialization B0=ÂµIconverges at a global superlinear rate of O((C0dlogÎº+C0Îº
t)t).
Hence, the superlinear result for B0=ÂµIoutperforms the rate for B0=LIwhen C0logÎºâ‰ªÎº.
Remark 6.2. We chose B0=LIandB0=ÂµIas two specific cases since they lead to explicit upper
bounds in terms of the dimension dand the condition number Îºin various theorems, simplifying
the interpretation of our results. In practice, however, we often set B0=cI, where c=sâŠ¤y
âˆ¥sâˆ¥2, with
s=x2âˆ’x1,y=âˆ‡f(x2)âˆ’ âˆ‡f(x1), and x1, x2as two randomly selected vectors. This choice
ensures câˆˆ[Âµ, L], and in the following numerical experiments, the performance of B0=cIis
similar to that of B0=ÂµI. The complexity of BFGS with this initialization is reported in Appendix H.
7 Complexity Analysis
Discussions on the iteration complexity. Using the three established convergence results in The-
orems 4.1, 5.2 and 6.4, we can characterize the total number of iterations required for the BFGS
method with the Armijo-Wolfe line search to find a solution with function suboptimality less than Ïµ.
However, as discussed above, the choice of the initial Hessian approximation B0heavily influences
the number of iterations required to observe these rates. To simplify our discussion, we focus on two
specific initializations: B0=LIandB0=ÂµI.
The case of B0=LI:The overall iteration complexity of BFGS with B0=LIis given by
Oï£«
ï£­minï£±
ï£²
ï£³Îºlog1
Ïµ,(d+C0)Îº+ log1
Ïµ,log1
Ïµ
log
1
2+q
1
4+1
dÎº+C0Îºlog1
Ïµï£¼
ï£½
ï£¾ï£¶
ï£¸.
The case of B0=ÂµI:The overall iteration complexity of BFGS with B0=ÂµIis given by
Oï£«
ï£­minï£±
ï£²
ï£³dlogÎº+Îºlog1
Ïµ, C0(dlogÎº+Îº) + log1
Ïµ,log1
Ïµ
log
1
2+q
1
4+1
C0(dlogÎº+Îº)log1
Ïµï£¼
ï£½
ï£¾ï£¶
ï£¸.
8We remark that the comparison between these two complexity bounds depends on the relative values
ofÎº,d,C0, and Ïµ, and neither is uniformly better than the other. It is worth noting that for BFGS
withB0=LI, we achieve a complexity that is consistently superior to the O 
Îºlog1
Ïµ
complexity
of gradient descent. Moreover, in scenarios where C0=O(1)anddâ‰ªÎº, BFGS with B0=ÂµI
could result in an iteration complexity of O 
Îº+ log1
Ïµ
, which is much more favorable than that of
gradient descent. The proof of these complexity bounds can be found in Appendix I.
Discussions on the line search complexity. We present the log bisection algorithm to choose
the step size Î·tat iteration tsatisfying the Armijo-Wolfe conditions (5)and(6)in Algorithm 1
in Appendix J. We define Î·minandÎ·max as the lower and upper bounds of the â€œslicing windowâ€
containing the trial step size Î·t, respectively. We start with the initial trial step size Î·t= 1 and
keep enlarging or decreasing it depending on whether the Armijo condition (5)or the curvature
condition (6)is satisfied. Then, we dynamically update Î·min,Î·max and shrink the size of this â€œslicing
windowâ€ (Î·min, Î·max). We pick the trial step size Î·as the geometric mean of Î·minandÎ·max, i.e.,
logÎ·= (log Î·max+ log Î·max)/2, which is the reason why we call this algorithm â€œlog bisectionâ€.
Note that in each loop of Algorithm 1, we query the function value and gradient at most once to check
the Armijo-Wolfe conditions at Lines 2 and 9. The next theorem characterizes the average number of
function value and gradient evaluations per iteration in Algorithm 1 after titerations, denoted by Î›t,
which is equivalent to the average number of loops per iterations.
Theorem 7.1. Suppose Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}tâ‰¥0be generated by BFGS
with step size satisfying the Armijo-Wolfe conditions in (5)and(6)and is chosen by Algorithm 1.
If we define Ïƒ:= (Î¨( Â¯B0) +3
Î±(1âˆ’Î²)Îº)C0, then for any initial point x0âˆˆRdand initial Hessian
approximation B0âˆˆSd
++, the average number of the function value and gradient evaluations per
iteration in Algorithm 1 after titerations satisfies
Î›tâ‰¤2+log2
1+1âˆ’Î²
Î²âˆ’Î±+2(1âˆ’Î²)
Î²âˆ’Î±Ïƒ
t
+2 log2
log216(1âˆ’Î±)+log2 
1+Ïƒ
t)+6Î¨(ËœB0) + 12 Ïƒ
t
.
The above result shows that when we run BFGS for Niterations, the total number of function and
gradient evaluations is O 
N+Nlog(1 +Ïƒ
N) +Nlog(1 +Î¨(ËœB0)+Ïƒ
N)
. Thus, the total line search
complexity can always be bounded by O(Nlog(Î¨( ËœB0) +Ïƒ)) =O(Nmax{logd,logÎº,logC0}).
Furthermore, notice that when Nis sufficiently large such that we reach the superlinear convergence
stage, i.e., N= â„¦(Î¨( ËœB0) +Ïƒ), the total line search complexity becomes O(N), which means the
average number of function and gradient evaluations per iteration is a constant O(1). We report the
line search complexity results of different B0=LIandB0=ÂµIin Appendix K.4.
8 Numerical Experiments
We conduct numerical experiments on a cubic objective function defined as
f(x) =Î±
12 dâˆ’1X
i=1g(vâŠ¤
ixâˆ’vâŠ¤
i+1x)âˆ’Î²vâŠ¤
1x!
+Î»
2âˆ¥xâˆ¥2, (29)
andg:Râ†’Ris defined as
g(w) =1
3|w|3|w| â‰¤âˆ†,
âˆ†w2âˆ’âˆ†2|w|+1
3âˆ†3|w|>âˆ†,(30)
where Î±, Î², Î», âˆ†âˆˆRare hyper-parameters and {vi}n
i=1are standard orthogonal unit vectors in Rd.
We focus on this objective function because it is used in [26] to establish a tight lower bound for
second-order methods. We compare the convergence paths of BFGS with an inexact line search step
sizeÎ·tthat satisfies the Armijo-Wolfe conditions (5)and(6)for various initialization matrices B0:
specifically, B0=LI,B0=ÂµI,B0=I, andB0=cIwhere cis defined in Remark 6.2. It is easily
verified that câˆˆ[Âµ, L]. We also compare the performance of BFGS methods to the gradient descent
(GD) method with backtracking line search, using Î±= 0.1in condition (5)andÎ²= 0.9in condition
(6). Step size Î·tis chosen at each iteration via log bisection in Algorithm 1. Empirical results are
compared across various dimensions dand condition numbers Îº, with the x-axis representing the
number of iterations tand the y-axis showing the ratiof(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—).
90 300 600 900 1200 1500 1800 2100 2400 2700 300010-2010-1510-1010-5100(a)d= 100 ,Îº= 100 .
0 300 600 900 1200 1500 1800 2100 2400 2700 300010-2010-1510-1010-5100 (b)d= 100 ,Îº= 1000 .
0 500 1000 1500 2000 2500 3000 3500 4000 4500 500010-2010-1510-1010-5100 (c)d= 300 ,Îº= 100 .
0 500 1000 1500 2000 2500 3000 3500 4000 4500 500010-2010-1510-1010-5100
(d)d= 300 ,Îº= 1000 .
0 600 1200 1800 2400 3000 3600 4200 4800 5400 600010-2010-1510-1010-5100 (e)d= 600 ,Îº= 100 .
0
1200 2400 3600 4800 6000 7200 8400 9600 10800 1200010-2010-1510-1010-5100 (f)d= 600 ,Îº= 1000 .
Figure 1: Convergence curves of BFGS with inexact line search of different B0and gradeint descent
with backtracking line search.
First, we observe that BFGS with B0=LIinitially converges faster than BFGS with B0=ÂµI
in most plots, aligning with our theoretical findings that the linear convergence rate of BFGS with
B0=LIsurpasses that of B0=ÂµIin Corollary 4.2. In Corollary 4.2, we show that BFGS
withB0=LIcould achieve the linear rate of (1âˆ’1/Îº)from the first iteration while BFGS with
B0=ÂµIneeds to run dlogÎºto reach the same linear rate. Second, the transition to superlinear
convergence for BFGS with B0=ÂµItypically occurs around tâ‰ˆd, as predicted by our theoretical
analysis. Although BFGS with B0=LIinitially converges faster, its transition to superlinear
convergence consistently occurs later than for B0=ÂµI. Notably, for a fixed dimension d= 600 , the
transition to superlinear convergence for B0=LIoccurs increasingly later as the problem condition
number rises, an effect not observed for B0=ÂµI. This phenomenon indicates that the superlinear
rate for B0=LIis more sensitive to the condition number Îº, which corroborates our results in
Corollary 6.5. In Corollary 6.5, we present that BFGS with B0=LIneeds dÎºsteps to reach the
superlinear convergence stage while this is improved to dlogÎºfor BFGS with B0=ÂµI. Moreover,
the performance of BFGS with B0=IandB0=cIis similar to BFGS with B0=ÂµI. Notice that
the initializations of B0=IandB0=cIare two commonly-used practical choices of the initial
Hessian approximation matrix B0.
9 Conclusions, Limitations, and Future Directions
In this paper, we analyzed the global non-asymptotic convergence rates of BFGS with Armijo-
Wolfe line search. We showed for an objective function that is Âµ-strongly convex with an L-
Lipschitz gradient, BFGS achieves a global convergence rate of (1âˆ’1/Îº)t, where Îº=L/Âµ.
Additionally, assuming the Hessian is M-Lipschitz, we showed BFGS achieves a linear convergence
rate determined solely by the line search parameters, independent of the condition number. Under
similar assumptions, we also established a global superlinear convergence rate. Given these bounds,
we determined the overall iteration complexity of BFGS with the Armijo-Wolfe line search and
specified this complexity for initial Hessian approximations B0=LIandB0=ÂµI.
One limitation of this paper is that the analysis only applies to strongly convex functions. Developing
an analysis for the general convex setting is still unsolved. Another drawback is that we focus solely
on the BFGS method. Extending our theoretical results to the entire convex Broydenâ€™s class of
quasi-Newton methods, including both BFGS and DFP, is a natural next step.
10Acknowledgments
The research of Q. Jin, R. Jiang, and A. Mokhtari is supported in part by NSF Award 2007668 and
the NSF AI Institute for Foundations of Machine Learning (IFML).
References
[1] W. Davidon. Variable metric method for minimization . Tech. rep. Argonne National Lab.,
Lemont, Ill., 1959 (page 1).
[2] R. Fletcher and M. J. Powell. â€œA rapidly convergent descent method for minimizationâ€. The
computer journal 6.2 (1963), pp. 163â€“168 (page 1).
[3] C. G. Broyden. â€œThe convergence of single-rank quasi-Newton methodsâ€. Mathematics of
Computation 24.110 (1970), pp. 365â€“382 (page 1).
[4] R. Fletcher. â€œA new approach to variable metric algorithmsâ€. The computer journal 13.3
(1970), pp. 317â€“322 (page 1).
[5] D. Goldfarb. â€œA family of variable-metric methods derived by variational meansâ€. Mathematics
of computation 24.109 (1970), pp. 23â€“26 (page 1).
[6] D. F. Shanno. â€œConditioning of quasi-Newton methods for function minimizationâ€. Mathemat-
ics of computation 24.111 (1970), pp. 647â€“656 (page 1).
[7] A. R. Conn, N. I. M. Gould, and P. L. Toint. â€œConvergence of quasi-Newton matrices generated
by the symmetric rank one updateâ€. Mathematical programming 50.1-3 (1991), pp. 177â€“195
(page 1).
[8] H. F. Khalfan, R. H. Byrd, and R. B. Schnabel. â€œA theoretical and experimental study of the
symmetric rank-one updateâ€. SIAM J. Optim. 3.1 (1993), pp. 1â€“24 (page 1).
[9] C. G. Broyden. â€œA class of methods for solving nonlinear simultaneous equationsâ€. Mathemat-
ics of computation 19.92 (1965), pp. 577â€“593 (page 1).
[10] R. Gower, D. Goldfarb, and P. RichtÃ¡rik. â€œStochastic block BFGS: Squeezing more curvature
out of dataâ€. In: Int. Conference on Machine Learning . PMLR. 2016, pp. 1869â€“1878 (page 1).
[11] R. M. Gower and P. RichtÃ¡rik. â€œRandomized quasi-Newton updates are linearly convergent
matrix inversion algorithmsâ€. SIAM Journal on Matrix Analysis and Applications 38.4 (2017),
pp. 1380â€“1409 (page 1).
[12] D. Kovalev, R. M. Gower, P. RichtÃ¡rik, and A. Rogozin. â€œFast linear convergence of random-
ized BFGSâ€. arXiv preprint arXiv:2002.11337 (2020) (page 1).
[13] D. Lin, H. Ye, and Z. Zhang. â€œGreedy and random quasi-Newton methods with faster explicit
superlinear convergenceâ€. Advances in Neural Information Processing Systems 34 (2021),
pp. 6646â€“6657 (page 1).
[14] D. Lin, H. Ye, and Z. Zhang. â€œExplicit convergence rates of greedy and random quasi-Newton
methodsâ€. Journal of Machine Learning Research 23.162 (2022), pp. 1â€“40 (page 1).
[15] A. Rodomanov and Y . Nesterov. â€œGreedy Quasi-Newton Methods with Explicit Superlinear
Convergenceâ€. SIAM Journal on Optimization 31.1 (2021), pp. 785â€“811 (page 1).
[16] Z.-Y . Ji and Y . -H. Dai. â€œGreedy PSB methods with explicit superlinear convergenceâ€. Compu-
tational Optimization and Applications 85.3 (2023), pp. 753â€“786 (page 1).
[17] R. Jiang, Q. Jin, and A. Mokhtari. â€œOnline Learning Guided Curvature Approximation: A
Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergenceâ€. In: Proceed-
ings of Thirty Sixth Conference on Learning Theory . V ol. 195. 2023, pp. 1962â€“1992 (page 1).
[18] R. Jiang and A. Mokhtari. â€œAccelerated quasi-newton proximal extragradient: Faster rate for
smooth convex optimizationâ€. Advances in Neural Information Processing Systems 36 (2023)
(page 1).
[19] C. G. Broyden, J. E. Dennis Jr, and J. J. MorÃ©. â€œOn the local and superlinear convergence
of quasi-Newton methodsâ€. IMA Journal of Applied Mathematics 12.3 (1973), pp. 223â€“245
(pages 1, 3).
[20] J. E. Dennis and J. J. MorÃ©. â€œA characterization of superlinear convergence and its application
to quasi-Newton methodsâ€. Mathematics of computation 28.126 (1974), pp. 549â€“560 (pages 1,
3).
[21] A. Griewank and P. L. Toint. â€œLocal convergence analysis for partitioned quasi-Newton
updatesâ€. Numerische Mathematik 39.3 (1982), pp. 429â€“448 (pages 1, 3).
11[22] J. Dennis, H. J. Martinez, and R. A. Tapia. â€œConvergence theory for the structured BFGS
secant method with an application to nonlinear least squaresâ€. Journal of Optimization Theory
and Applications 61.2 (1989), pp. 161â€“178 (pages 1, 3).
[23] Y . Yuan. â€œA modified BFGS algorithm for unconstrained optimizationâ€. IMA Journal of
Numerical Analysis 11.3 (1991), pp. 325â€“332 (pages 1, 3).
[24] M. Al-Baali. â€œGlobal and superlinear convergence of a restricted class of self-scaling methods
with inexact line searches, for convex functionsâ€. Computational Optimization and Applica-
tions 9.2 (1998), pp. 191â€“203 (pages 1, 3).
[25] D. Li and M. Fukushima. â€œA Globally and Superlinearly Convergent Gaussâ€“Newton-Based
BFGS Method for Symmetric Nonlinear Equationsâ€. SIAM Journal on Numerical Analysis
37.1 (1999), pp. 152â€“172 (pages 1, 3).
[26] H. Yabe, H. Ogasawara, and M. Yoshino. â€œLocal and superlinear convergence of quasi-
Newton methods based on modified secant conditionsâ€. Journal of Computational and Applied
Mathematics 205.1 (2007), pp. 617â€“632 (pages 1, 3, 9).
[27] A. Mokhtari, M. Eisen, and A. Ribeiro. â€œIQN: An incremental quasi-Newton method with local
superlinear convergence rateâ€. SIAM Journal on Optimization 28.2 (2018), pp. 1670â€“1698
(pages 1, 3).
[28] W. Gao and D. Goldfarb. â€œQuasi-Newton methods: superlinear convergence without line
searches for self-concordant functionsâ€. Optimization Methods and Software 34.1 (2019),
pp. 194â€“217 (pages 1, 3).
[29] M. Powell. â€œOn the convergence of the variable metric algorithmâ€. IMA Journal of Applied
Mathematics 7.1 (1971), pp. 21â€“36 (page 1).
[30] M. J. Powell. â€œSome global convergence properties of a variable metric algorithm for mini-
mization without exact line searchesâ€. Nonlinear programming 9.1 (1976), pp. 53â€“72 (page 1).
[31] R. H. Byrd, J. Nocedal, and Y . Yuan. â€œGlobal convergence of a class of quasi-Newton methods
on convex problemsâ€. SIAM Journal on Numerical Analysis 24.5 (1987), pp. 1171â€“1190
(page 1).
[32] R. H. Byrd and J. Nocedal. â€œA Tool for the Analysis of Quasi-Newton Methods with Applica-
tion to Unconstrained Minimizationâ€. SIAM Journal on Numerical Analysis, Vol. 26, No. 3
(1989) (pages 1, 4).
[33] R. H. Byrd, H. F. Khalfan, and R. B. Schnabel. â€œAnalysis of a symmetric rank-one trust region
methodâ€. SIAM Journal on Optimization 6.4 (1996), pp. 1025â€“1039 (page 1).
[34] A. Rodomanov and Y . Nesterov. â€œRates of Superlinear Convergence for Classical Quasi-
Newton Methodsâ€. Mathematical Programming (2021), pp. 1â€“32 (pages 2, 19).
[35] A. Rodomanov and Y . Nesterov. â€œNew Results on Superlinear Convergence of Classical
Quasi-Newton Methodsâ€. Journal of Optimization Theory and Applications 188.3 (2021),
pp. 744â€“769 (page 2).
[36] H. Ye, D. Lin, X. Chang, and Z. Zhang. â€œTowards explicit superlinear convergence rate for
SR1â€. Mathematical Programming 199.1 (2023), pp. 1273â€“1303 (page 2).
[37] Q. Jin and A. Mokhtari. â€œNon-asymptotic Superlinear Convergence of Standard Quasi-Newton
Methodsâ€. Mathematical Programming, Volume 200, pages 425â€“473 (2022) (page 2).
[38] Q. Jin, R. Jiang, and A. Mokhtari. â€œNon-asymptotic Global Convergence Rates of BFGS with
Exact Line Searchâ€. arXiv preprint arXiv:2404.01267 (2024) (pages 2, 5, 16, 37).
[39] V . Krutikov, E. Tovbis, P. Stanimirovi Â´c, and L. Kazakovtsev. â€œOn the Convergence Rate of
Quasi-Newton Methods on Strongly Convex Functions with Lipschitz Gradientâ€. Mathematics
11.23 (2023), p. 4715 (page 2).
[40] A. Rodomanov. â€œGlobal Complexity Analysis of BFGSâ€. arXiv preprint arXiv:2404.15051
(2024) (pages 2, 7).
[41] J. Nocedal and S. Wright. Numerical optimization . Springer Science Business Media, 2006
(pages 2, 3, 5).
[42] P. Wolfe. â€œConvergence Conditions for Ascent Methodsâ€. SIAM Review 11.2 (1969), pp. 226â€“
235 (page 3).
[43] P. Wolfe. â€œConvergence Conditions for Ascent Methods. II: Some Correctionsâ€. SIAM Review
13.2 (1971), pp. 185â€“188 (page 3).
[44] Y . Nesterov. Lectures on convex optimization . Springer Optimization and Its Applications
(SOIA, volume 137), 2018 (page 20).
12Appendix
A Some Results on the Connections between Different Hessian Matrices
Lemma A.1. Suppose Assumptions 2.1, 2.2, and 2.3 hold, and recall the definitions of the matrices
JtandGtin(8), and the quantity Ctin(9). Then, the following statements hold:
(a) Suppose that f(xt+1)â‰¤f(xt)for any tâ‰¥0, we have that
1
1 +Ctâˆ‡2f(xâˆ—)âª¯Jtâª¯(1 +Ct)âˆ‡2f(xâˆ—). (31)
(b) Suppose that f(xt+1)â‰¤f(xt)for any tâ‰¥0andË†Ï„âˆˆ[0,1], we have that
1
1 +Ctâˆ‡2f(xâˆ—)âª¯ âˆ‡2f(xt+ Ë†Ï„(xt+1âˆ’xt))âª¯(1 +Ct)âˆ‡2f(xâˆ—). (32)
(c) For any tâ‰¥0, we have that
1
1 +Ctâˆ‡2f(xâˆ—)âª¯ âˆ‡2f(xt)âª¯(1 +Ct)âˆ‡2f(xâˆ—). (33)
(d) For any tâ‰¥0, we have that
1
1 +Ctâˆ‡2f(xâˆ—)âª¯Gtâª¯(1 +Ct)âˆ‡2f(xâˆ—). (34)
(e) For any tâ‰¥0andËœÏ„âˆˆ[0,1], we have that
1
1 +CtGtâª¯ âˆ‡2f(xt+ ËœÏ„(xâˆ—âˆ’xt))âª¯(1 +Ct)Gt. (35)
(f) For any tâ‰¥0andËœÏ„,Ë†Ï„âˆˆ[0,1], suppose that f(xt+1)â‰¤f(xt). Then, we have that
1
1 + 2 Ctâˆ‡2f(xt+ Ë†Ï„st)âª¯ âˆ‡2f(xt+ ËœÏ„st)âª¯(1 + 2 Ct)âˆ‡2f(xt+ Ë†Ï„st). (36)
Proof. (a) Recall the definition of Jtin (8). Using the triangle inequality, we have that
âˆ¥âˆ‡2f(xâˆ—)âˆ’Jtâˆ¥=Z1
0 
âˆ‡2f(xâˆ—)âˆ’ âˆ‡2f(xt+Ï„(xt+1âˆ’xt))
dÏ„
â‰¤Z1
0âˆ¥âˆ‡2f(xâˆ—)âˆ’ âˆ‡2f(xt+Ï„(xt+1âˆ’xt))âˆ¥dÏ„.
Moreover, it follows from Assumption 2.3 that âˆ¥âˆ‡2f(xâˆ—)âˆ’ âˆ‡2f(xt+Ï„(xt+1âˆ’xt))âˆ¥ â‰¤
Mâˆ¥(1âˆ’Ï„)(xâˆ—âˆ’xt) +Ï„(xâˆ—âˆ’xt+1)âˆ¥for any Ï„âˆˆ[0,1]. Thus, we can further apply the
triangle inequality to obtain
âˆ¥âˆ‡2f(xâˆ—)âˆ’Jtâˆ¥ â‰¤Z1
0Mâˆ¥(1âˆ’Ï„)(xâˆ—âˆ’xt) +Ï„(xâˆ—âˆ’xt+1)âˆ¥dÏ„
â‰¤Mâˆ¥xtâˆ’xâˆ—âˆ¥Z1
0(1âˆ’Ï„)dÏ„+Mâˆ¥xt+1âˆ’xâˆ—âˆ¥Z1
0Ï„dÏ„
=M
2(âˆ¥xtâˆ’xâˆ—âˆ¥+âˆ¥xt+1âˆ’xâˆ—âˆ¥).
Since fis strongly convex, by Assumption 2.1 and f(xt+1)â‰¤f(xt), we haveÂµ
2âˆ¥xtâˆ’
xâˆ—âˆ¥2â‰¤f(xt)âˆ’f(xâˆ—), which implies that âˆ¥xtâˆ’xâˆ—âˆ¥ â‰¤p
2(f(xt)âˆ’f(xâˆ—))/Âµ. Similarly,
since f(xt+1)â‰¤f(xt), it also holds that âˆ¥xt+1âˆ’xâˆ—âˆ¥ â‰¤p
2(f(xt+1)âˆ’f(xâˆ—))/Âµâ‰¤p
2(f(xt)âˆ’f(xâˆ—))/Âµ. Hence, we obtain that
âˆ¥âˆ‡2f(xâˆ—)âˆ’Jtâˆ¥ â‰¤MâˆšÂµp
2(f(xt)âˆ’f(xâˆ—)). (37)
13Moreover, notice that by Assumption 2.1, we also have Jtâª°ÂµIandâˆ‡2f(xâˆ—)âª°ÂµI. Hence,
(37) implies that
âˆ‡2f(xâˆ—)âˆ’Jtâª¯ âˆ¥âˆ‡2f(xâˆ—)âˆ’Jtâˆ¥Iâª¯M
Âµ3
2p
2(f(xt)âˆ’f(xâˆ—))Jt=CtJt,
Jtâˆ’ âˆ‡2f(xâˆ—)âª¯ âˆ¥Jtâˆ’ âˆ‡2f(xâˆ—)âˆ¥Iâª¯M
Âµ3
2p
2(f(xt)âˆ’f(xâˆ—))âˆ‡2f(xâˆ—) =Ctâˆ‡2f(xâˆ—).
where we used the definition of Ctin (9). By rearranging the terms, we obtain (31).
(b) Similar to the arguments in (a), for any Ë†Ï„âˆˆ[0,1], we have that
âˆ‡2f(xt+ Ë†Ï„(xt+1âˆ’xt))âˆ’ âˆ‡2f(xâˆ—)
â‰¤Mâˆ¥(1âˆ’Ë†Ï„)(xtâˆ’xâˆ—) + Ë†Ï„(xt+1âˆ’xâˆ—)âˆ¥
â‰¤M
(1âˆ’Ë†Ï„)âˆ¥xtâˆ’xâˆ—âˆ¥+ Ë†Ï„âˆ¥xt+1âˆ’xâˆ—âˆ¥
â‰¤M
(1âˆ’Ë†Ï„)r2
Âµ(f(xt)âˆ’f(xâˆ—)) + Ë†Ï„r2
Âµ(f(xt+1)âˆ’f(xâˆ—))
â‰¤Mr2
Âµ(f(xt)âˆ’f(xâˆ—))
Moreover, notice that by Assumption 2.1, we also have âˆ‡2f(xt+ Ë†Ï„(xt+1âˆ’xt))âª°ÂµIand
âˆ‡2f(xâˆ—)âª°ÂµI. The rest follows similarly as in the proof of (a) and we prove (32).
(c) Similar to the arguments in (a), we have that
âˆ‡2f(xâˆ—)âˆ’ âˆ‡2f(xt)â‰¤Mâˆ¥xtâˆ’xâˆ—âˆ¥ â‰¤MâˆšÂµp
2(f(xt)âˆ’f(xâˆ—)).
Moreover, notice that by Assumption 2.1 we also have âˆ‡2f(xt)âª°ÂµIandâˆ‡2f(xâˆ—)âª°ÂµI.
The rest follows similarly as in the proof of (a) and we prove (33).
(d) Recall the definition of Gtin (8). Similar to the arguments in (a), we have that
âˆ¥âˆ‡2f(xâˆ—)âˆ’Gtâˆ¥=Z1
0 
âˆ‡2f(xâˆ—)âˆ’ âˆ‡2f(xt+Ï„(xâˆ—âˆ’xt))
dÏ„
â‰¤Z1
0âˆ¥âˆ‡2f(xâˆ—)âˆ’ âˆ‡2f(xt+Ï„(xâˆ—âˆ’xt))âˆ¥dÏ„
â‰¤MZ1
0âˆ¥(1âˆ’Ï„)(xâˆ—âˆ’xt)âˆ¥dÏ„=Mâˆ¥xtâˆ’xâˆ—âˆ¥Z1
0(1âˆ’Ï„)dÏ„
=M
2âˆ¥xtâˆ’xâˆ—âˆ¥ â‰¤MâˆšÂµp
2(f(xt)âˆ’f(xâˆ—)).
Moreover, notice that by Assumption 2.1 we also have Gtâª°ÂµIandâˆ‡2f(xâˆ—)âª°ÂµI. The
rest follows similarly as in the proof of (a) and we prove (34).
(e)Recall the definition of gtin(8). Similar to the arguments in (a), for any ËœÏ„âˆˆ[0,1], we have
thatâˆ‡2f(xt+ ËœÏ„(xâˆ—âˆ’xt))âˆ’Gt
=Z1
0 
âˆ‡2f(xt+ ËœÏ„(xâˆ—âˆ’xt))âˆ’ âˆ‡2f(xt+Ï„(xâˆ—âˆ’xt))
dÏ„
â‰¤Z1
0âˆ‡2f(xt+ ËœÏ„(xâˆ—âˆ’xt))âˆ’ âˆ‡2f(xt+Ï„(xâˆ—âˆ’xt))dÏ„
â‰¤Z1
0M|ËœÏ„âˆ’Ï„|âˆ¥xtâˆ’xâˆ—âˆ¥dÏ„â‰¤1
2Mâˆ¥xtâˆ’xâˆ—âˆ¥ â‰¤MâˆšÂµp
2(f(xt)âˆ’f(xâˆ—)).
Moreover, notice that by Assumption 2.1, we also have âˆ‡2f(xt+ ËœÏ„(xâˆ—âˆ’xt))âª°ÂµIand
Gtâª°ÂµI. The rest follows similarly as in the proof of (a) and we prove (35).
14(f) Similar to the arguments in (a), for any ËœÏ„,Ë†Ï„âˆˆ[0,1], we have that
âˆ‡2f(xt+ ËœÏ„st)âˆ’ âˆ‡2f(xt+ Ë†Ï„st)
â‰¤M|ËœÏ„âˆ’Ë†Ï„|âˆ¥stâˆ¥ â‰¤Mâˆ¥stâˆ¥ â‰¤M(âˆ¥xt+1âˆ’xâˆ—âˆ¥+âˆ¥xtâˆ’xâˆ—âˆ¥)
â‰¤Mr2
Âµ(f(xt)âˆ’f(xâˆ—)) +r2
Âµ(f(xt+1)âˆ’f(xâˆ—))
â‰¤2Mr2
Âµ(f(xt)âˆ’f(xâˆ—))
Moreover, notice that by Assumption 2.1, we also have âˆ‡2f(xt+ ËœÏ„st)âª°ÂµIand
âˆ‡2f(xt+ Ë†Ï„st)âª°ÂµI. The rest follows similarly as in the proof of (a) and we prove
(36).
B Proof of Lemma 2.1
Recall that gt=âˆ‡f(xt). Given the condition in (5) and the fact that st=Î·tdt, we have
f(xt+1)â‰¤f(xt) +Î±gâŠ¤
tst.
Moreover, since Btis symmetric positive definite, we have âˆ’gâŠ¤
tst=Î·tgâŠ¤
tBâˆ’1
tgt>0(unless gt= 0
and we are at the optimal solution). This further leads to the first claim, which is
f(xt)âˆ’f(xt+1)
âˆ’gâŠ¤
tstâ‰¥Î±.
Similarly, the above argument implies that Î±gâŠ¤
tst<0and as a result f(xt+1)â‰¤f(xt)and the last
claim also follows.
To prove the second claim, we leverage the condition in (6). Specifically, if we subtract gâŠ¤
tdtfrom
both sides of that condition, we obtain that
(gt+1âˆ’gt)âŠ¤dtâ‰¥(Î²âˆ’1)gâŠ¤
tdt
Next, using the fact that st=Î·tdt, by multiplying both sides by Î·tand use the simplification
yt=gt+1âˆ’gtwe obtain that
yâŠ¤
tstâ‰¥(Î²âˆ’1)gâŠ¤
tst=âˆ’gâŠ¤
tst(1âˆ’Î²).
Again using the argument that âˆ’gâŠ¤
tstis positive (if we are not at the optimal solution), we can divide
both sides of the above inequality by âˆ’gâŠ¤
tst, leading to the second claim.
C Proof of Proposition 3.1
First, we note that Ë†gâŠ¤
tË†st=gâŠ¤
tstandË†yâŠ¤
tË†st=yâŠ¤
tst. Using the definition of Ë†ptin (16), we have that
f(xt)âˆ’f(xt+1) = Ë†ptâˆ’Ë†gâŠ¤
tË†st
âˆ¥Ë†gtâˆ¥2âˆ¥Ë†gtâˆ¥2. (38)
Hence, using the definition of Ë†Î¸tin (14) and the definition of Ë†mt,Ë†ntin (16), it follows that
âˆ’Ë†gâŠ¤
tË†st
âˆ¥Ë†gtâˆ¥2=(Ë†gâŠ¤
tË†st)2
âˆ¥Ë†gtâˆ¥2âˆ¥Ë†stâˆ¥2âˆ¥Ë†stâˆ¥2
âˆ’Ë†gâŠ¤
tË†st=(Ë†gâŠ¤
tË†st)2
âˆ¥Ë†gtâˆ¥2âˆ¥Ë†stâˆ¥2âˆ¥Ë†stâˆ¥2
Ë†yâŠ¤
tË†stË†yâŠ¤
tË†st
âˆ’Ë†gâŠ¤
tË†st= Ë†ntcos2(Ë†Î¸t)
Ë†mt.
Furthermore, we have âˆ¥Ë†gtâˆ¥2= Ë†qt(f(xt)âˆ’f(xâˆ—))from the definition of Ë†qtin(16). Thus, the equality
in (38) can be rewritten as
f(xt)âˆ’f(xt+1) = Ë†ptË†qtË†ntcos2(Ë†Î¸t)
Ë†mt(f(xt)âˆ’f(xâˆ—)).
15By rearranging the term in the above equality, we obtain
f(xt+1)âˆ’f(xâˆ—) =
1âˆ’Ë†ptË†qtË†ntcos2(Ë†Î¸t)
Ë†mt
(f(xt)âˆ’f(xâˆ—)), (39)
To prove the inequality in (15), note that for any tâ‰¥1, we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)=tâˆ’1Y
i=0f(xi+1)âˆ’f(xâˆ—)
f(xi)âˆ’f(xâˆ—)=tâˆ’1Y
i=0 
1âˆ’Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†mi!
,
where the last equality is due to (39). Note that all the terms of the form 1âˆ’Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†miare
non-negative, for any iâ‰¥0. Thus, by applying the inequality of arithmetic and geometric means
twice, we obtain
tâˆ’1Y
i=0 
1âˆ’Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†mi!
â‰¤"
1
ttâˆ’1X
i=0 
1âˆ’Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†mi!#t
="
1âˆ’1
ttâˆ’1X
i=0Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†mi#t
â‰¤ï£®
ï£°1âˆ’ tâˆ’1Y
i=0Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†mi!1
tï£¹
ï£»t
.
This completes the proof.
D Results from [38]
In this section, we summarize some results that we use from [38] to establish a lower bound onQtâˆ’1
i=0cos2(Ë†Î¸i)
Ë†miandË†qt.
Proposition D.1 ([38, Proposition 2]) .Let{Bt}tâ‰¥0be the Hessian approximation matrices generated
by the BFGS update in (3). For a given weight matrix PâˆˆSd
++, recall the weighted vectors defined
in(11) and the weighted matrix in (12). Then, we have
Î¨(Ë†Bt+1)â‰¤Î¨(Ë†Bt) +âˆ¥Ë†ytâˆ¥2
Ë†yâŠ¤
tË†stâˆ’1 + logcos2Ë†Î¸t
Ë†mt,âˆ€tâ‰¥0,
where Ë†mtis defined in (16) andcos(Ë†Î¸t)is defined in (14). As a corollary, we have,
tâˆ’1X
i=0logcos2(Ë†Î¸i)
Ë†miâ‰¥ âˆ’Î¨(Ë†B0) +tâˆ’1X
i=0
1âˆ’âˆ¥Ë†yiâˆ¥2
Ë†yâŠ¤
iË†si
,âˆ€tâ‰¥1. (40)
If we take exponentiation on both sides of the above inequality (40) in Proposition D.1, we can obtain
a lower bound for the productQtâˆ’1
i=0cos2(Ë†Î¸i)
Ë†miwith the sumPtâˆ’1
i=0âˆ¥Ë†yiâˆ¥2
Ë†sâŠ¤
iË†yiandÎ¨(Ë†B0). This classical
inequality describing the relationship between the ratiocos2(Ë†Î¸t)
Ë†mtand the potential function Î¨(.)plays
a critical role in the following convergence analysis.
In the following two lemmas, we provide bounds on the quantities Ë†qtandâˆ¥Ë†ytâˆ¥2/Ë†sâŠ¤
tË†ytrespectively by
directly citing results from Lemma 4 and Lemma 5 in [38] again. Notice that both Ë†qtandâˆ¥Ë†ytâˆ¥2/Ë†sâŠ¤
tË†yt
depend on different choices of the weight matrix P.
Lemma D.2 ([38, Lemma 4]) .Recall the definition Ë†qt=âˆ¥Ë†gtâˆ¥2
f(xt)âˆ’f(xâˆ—)in(16). Suppose Assump-
tions 2.1, 2.2, and 2.3 hold. Then we have the following results:
(a) If we choose P=LI, then Ë†qtâ‰¥2/Îº.
(b) If we choose P=âˆ‡2f(xâˆ—), then Ë†qtâ‰¥2/(1 +Ct)2.
Lemma D.3 ([38, Lemma 5]) .Let{xt}tâ‰¥0be the iterates generated by the BFGS algorithm with
inexact line search satisfying (5)and(6). Suppose Assumptions 2.1, 2.2, and 2.3 hold. Then we have
the following results:
(a) If we choose P=LI, thenâˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâ‰¤1.
(b) If we choose P=âˆ‡2f(xâˆ—), thenâˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâ‰¤1 +Ct.
16E Proofs in Section 4
E.1 Proof of Theorem 4.1
Recall that we choose P=LIthroughout the proof. Note that given this weight matrix, it can
be easily verified thatâˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâ‰¤1for any tâ‰¥0by using Lemma D.3 (a). Hence, we use (40) in
Proposition D.1 to obtain
tâˆ’1X
i=0logcos2(Ë†Î¸i)
Ë†miâ‰¥ âˆ’Î¨(Â¯B0) +tâˆ’1X
i=0
1âˆ’âˆ¥Ë†yiâˆ¥2
Ë†sâŠ¤
iË†yi
â‰¥ âˆ’Î¨(Â¯B0),
which further implies that
tâˆ’1Y
i=0cos2(Ë†Î¸i)
Ë†miâ‰¥eâˆ’Î¨(Â¯B0).
Moreover, for the choice P=LI, it can be shown that Ë†qt=âˆ¥gtâˆ¥2
L(f(xt)âˆ’f(xâˆ—))â‰¥2
Îºby using Lemma D.2
(a). From Lemma 2.1, we know Ë†ptâ‰¥Î±andË†ntâ‰¥1âˆ’Î², which lead to
tâˆ’1Y
i=0Ë†piË†niË†qi
Ë†micos2(Ë†Î¸i)â‰¥tâˆ’1Y
i=0Ë†pitâˆ’1Y
i=0Ë†qitâˆ’1Y
i=0Ë†nitâˆ’1Y
i=0cos2(Ë†Î¸i)
Ë†miâ‰¥2Î±(1âˆ’Î²)
Îºt
eâˆ’Î¨(Â¯B0).
Thus, it follows from Proposition 3.1 that
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ï£®
ï£°1âˆ’ tâˆ’1Y
i=0Ë†piË†qiË†ni
Ë†micos2(Ë†Î¸i)!1
tï£¹
ï£»t
â‰¤
1âˆ’eâˆ’Î¨(Â¯B0)
t2Î±(1âˆ’Î²)
Îºt
.
This completes the proof.
E.2 Proof of Corollary 4.2
Notice that in the first case where B0=LI, we have Î¨(Â¯B0) = 0 and thus it achieves the best
linear convergence results according to Theorem 4.1. On the other hand, for B0=ÂµI, we have
Î¨(Â¯B0) = Î¨(Âµ
LI) =d(1
Îºâˆ’1 + log Îº)â‰¤dlogÎº. We complete the proof by combining these
conditions with the inequality (17) in Theorem 4.1. Notice that eâˆ’xâ‰¥eâˆ’1â‰¥1
3forxâ‰¤1.
F Proofs in Section 5
F.1 Proof of Proposition 5.1
Recall that we choose the weight matrix as P=âˆ‡2f(xâˆ—)throughout the proof. Similar to the proof
of Theorem 4.1, we start from the key inequality in (15), but we apply different bounds on the Ë†qtand
cos2(Ë†Î¸t)
Ë†mt. Specifically, by using Lemma D.3 (b), we haveâˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâ‰¤1 +Ctfor any tâ‰¥0. Hence, we
use (40) in Proposition D.1 to obtain
tâˆ’1X
i=0logcos2(Ë†Î¸i)
Ë†miâ‰¥ âˆ’Î¨(ËœB0) +tâˆ’1X
i=0
1âˆ’âˆ¥Ë†yiâˆ¥2
Ë†sâŠ¤
iË†yi
â‰¥ âˆ’Î¨(ËœB0)âˆ’tâˆ’1X
i=0Ci,
which further implies that
tâˆ’1Y
i=0cos2(Ë†Î¸i)
Ë†miâ‰¥eâˆ’Î¨(ËœB0)âˆ’Ptâˆ’1
i=0Ci. (41)
Moreover, since Ë†qtâ‰¥2
(1+Ct)2for any tâ‰¥0by using Lemma D.2 (b), we get
tâˆ’1Y
i=0Ë†qiâ‰¥tâˆ’1Y
i=02
(1 +Ci)2â‰¥2ttâˆ’1Y
i=0eâˆ’2Ci= 2teâˆ’2Ptâˆ’1
i=0Ci, (42)
17where we use the inequality 1 +xâ‰¤exfor any xâˆˆR. From Lemma 2.1, we know Ë†ptâ‰¥Î±and
Ë†ntâ‰¥1âˆ’Î², which lead to
tâˆ’1Y
i=0Ë†piË†niâ‰¥Î±t(1âˆ’Î²)t. (43)
Combining (41), (42), (43) and (15) from Proposition 3.1, we prove that
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ï£®
ï£°1âˆ’ tâˆ’1Y
i=0Ë†piË†qiË†ni
Ë†micos2(Ë†Î¸i)!1
tï£¹
ï£»t
â‰¤
1âˆ’2Î±(1âˆ’Î²)eâˆ’Î¨(ËœB0)+3Ptâˆ’1
i=0Ci
tt
.
This completes the proof.
F.2 Proof of Theorem 5.2
When we have tâ‰¥Î¨(ËœB0) + 3Ptâˆ’1
i=0Ci, Proposition 5.1 implies the condition thatf(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤

1âˆ’2Î±(1âˆ’Î²)
et
â‰¤
1âˆ’2Î±(1âˆ’Î²)
3t
, which leads to the linear rate in (20). Hence, it is sufficient
to establish an upper bound onPtâˆ’1
i=0Ci. Recall that Ci=M
Âµ3
2p
2(f(xi)âˆ’f(xâˆ—))defined in (9).
We decompose the sum into two parts:PâŒˆÎ¨(Â¯B0)âŒ‰âˆ’1
i=0CiandPt
i=âŒˆÎ¨(Â¯B0)âŒ‰Ci. For the first part, note
that since f(xi+1)â‰¤f(xi)by Lemma 2.1, we also have Ci+1â‰¤Ciforiâ‰¥0. Hence, we havePâŒˆÎ¨(Â¯B0)âŒ‰âˆ’1
i=0Ciâ‰¤C0âŒˆÎ¨(Â¯B0)âŒ‰ â‰¤C0(Î¨(Â¯B0) + 1) . Moreover, by Theorem 4.1, when tâ‰¥Î¨(Â¯B0)
we have
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’eâˆ’Î¨(Â¯B0)
t2Î±(1âˆ’Î²)
Îºt
â‰¤
1âˆ’2Î±(1âˆ’Î²)
eÎºt
â‰¤
1âˆ’2Î±(1âˆ’Î²)
3Îºt
.
Hence, this further implies that
tX
i=âŒˆÎ¨(Â¯B0)âŒ‰Ci=C0tX
i=âŒˆÎ¨(Â¯B0)âŒ‰s
f(xi)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤C0tX
i=âŒˆÎ¨(Â¯B0)âŒ‰
1âˆ’2Î±(1âˆ’Î²)
3Îºi
2
â‰¤C0âˆX
i=1
1âˆ’2Î±(1âˆ’Î²)
3Îºi
2
â‰¤C03Îº
Î±(1âˆ’Î²)âˆ’1
,
where we used the fact thatPâˆ
i=1(1âˆ’Ï)i
2=âˆš1âˆ’Ï
1âˆ’âˆš1âˆ’Ï=âˆš1âˆ’Ï+1âˆ’Ï
Ïâ‰¤2
Ïâˆ’1for any Ïâˆˆ(0,1).
Hence, by combining both inequalities, we have
tâˆ’1X
i=0Ci=âŒˆÎ¨(Â¯B0)âŒ‰âˆ’1X
i=0Ci+tX
i=âŒˆÎ¨(Â¯B0)âŒ‰Ciâ‰¤C0Î¨(Â¯B0) +3C0Îº
Î±(1âˆ’Î²). (44)
Hence, this proves that (20) is satisfied when tâ‰¥Î¨(ËœB0) + 3C0Î¨(Â¯B0) +9C0Îº
Î±(1âˆ’Î²).
F.3 Proof of Corollary 5.3
ForB0=LI, we have Â¯B0=1
LB0=IandËœB0=âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2=Lâˆ‡2f(xâˆ—)âˆ’1.
Thus, it holds that Î¨(Â¯B0) = Î¨( I) = 0 . Moreover, by Assumptions 2.1 and 2.2, we have1
LIâª¯
âˆ‡2f(xâˆ—)âˆ’1âª¯1
ÂµI, which implies that Iâª¯ËœB0âª¯ÎºI. Thus, we further have
Î¨(ËœB0)â‰¤Tr(ÎºI)âˆ’dâˆ’logDet(I) =dÎºâˆ’dâ‰¤dÎº.
Combining these two results, the threshold for transition time can be bounded by Î¨(ËœB0) +
3C0Î¨(Â¯B0) +9
Î±(1âˆ’Î²)C0Îºâ‰¤dÎº+9
Î±(1âˆ’Î²)C0Îº. Hence, by Theorem 5.2, the linear rate in (20)
is achieved when tâ‰¥dÎº+9
Î±(1âˆ’Î²)C0Îº.
18ForB0=ÂµI, we have Â¯B0=1
LB0=1
ÎºIandËœB0=âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2=Âµâˆ‡2f(xâˆ—)âˆ’1.
Thus, it holds that Î¨(Â¯B0) = Î¨(1
ÎºI) =d
Îºâˆ’d+dlogÎºâ‰¤dlogÎº. Moreover, by Assumptions 2.1
and 2.2, we have1
ÎºIâª¯ËœB0âª¯I. This implies that
Î¨(ËœB0) =Tr(ËœB0)âˆ’dâˆ’logDet(ËœB0)â‰¤Tr(I)âˆ’dâˆ’logDet(1
ÎºI) =dlogÎº.
Combining these two results, the threshold for the transition tume can be bounded by Î¨(ËœB0) +
3C0Î¨(Â¯B0) +9
Î±(1âˆ’Î²)C0Îºâ‰¤(1 + 3 C0)dlogÎº+9
Î±(1âˆ’Î²)C0Îº. Hence, by Theorem 5.2, the linear rate
in (20) is satisfied when tâ‰¥(1 + 3 C0)dlogÎº+9
Î±(1âˆ’Î²)C0Îº.
G Intermediate Results and Proofs in Section 6
G.1 Intermediate Results
To present our result we first introduce the following function
Ï‰(x) :=xâˆ’log (x+ 1), (45)
which is defined for x >âˆ’1. Further In the next result, we present some basic properties of the
function Ï‰(x)defined in (45).
Lemma G.1. Recall the definition of function Ï‰(x)in(45), we have that
(a)Ï‰(x)is increasing function for x >0and decreasing function for âˆ’1< x < 0. Moreover,
Ï‰(x)â‰¥0for all x >âˆ’1.
(b) When xâ‰¥0, we have that Ï‰(x)â‰¥x2
2(1+x).
(c) When âˆ’1< xâ‰¤0, we have that Ï‰(x)â‰¥x2
2+x.
Proof. Notice that Ï‰â€²(x) =x
1+x, we know that when x >0,Ï‰â€²(x)>0and when âˆ’1< x < 0,
Ï‰â€²(x)<0,Ï‰â€²(x)<0. Therefore, Ï‰(x)is increasing function for x >0andÏ‰(x)is decreasing
function for âˆ’1< x < 0. Hence, Ï‰(x)â‰¥Ï‰(0) = 0 for all x >âˆ’1.
Ï‰(x)â‰¥x2
2(1+x)is equivalent to Ï‰1(x) := 2(1 + x)Ï‰(x)âˆ’x2â‰¥0. Since Ï‰â€²
1(x) = 2 xâˆ’
2 log (1 + x) = 2 Ï‰(x)â‰¥0for all x >âˆ’1, we know that Ï‰1(x)is increasing function for x >âˆ’1
and hence, Ï‰1(x)â‰¥Ï‰1(0) = 0 forxâ‰¥0.
Ï‰(x)â‰¥x2
2+xis equivalent to Ï‰2(x) := (2+ x)Ï‰(x)âˆ’x2â‰¥0. Since Ï‰â€²
2(x) =x
1+xâˆ’log (1 + x)â‰¤0
for all x >âˆ’1, we know that Ï‰2(x)is decreasing function for x >âˆ’1and hence, Ï‰2(x)â‰¥Ï‰2(0) = 0
forxâ‰¤0.
Proposition G.2. Let{Bt}tâ‰¥0be the Hessian approximation matrices generated by the BFGS update
in(3). Suppose Assumptions 2.1, 2.2, and 2.3 hold and f(xt+1)â‰¤f(xt)for any tâ‰¥0. Recall the
definition of Î¨(.)in(10) andCtin(9), we have that
tâˆ’1X
i=0Ï‰(Ïiâˆ’1)â‰¤Î¨(ËœB0) + 2tâˆ’1X
i=0Ci,âˆ€tâ‰¥1, (46)
Proof. First, taking the trace and determinant on both sides of the equation (13) for any weight matrix
PâˆˆSd
++and using results from Lemma 6.2 of [34], we show that
Tr(Ë†Bt+1) =Tr(Ë†Bt)âˆ’âˆ¥Ë†BtË†stâˆ¥2
Ë†sâŠ¤
tË†BtË†st+âˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†yt,Det(Ë†Bt+1) =Det(Ë†Bt)Ë†sâŠ¤
tË†yt
Ë†sâŠ¤
tË†BtË†st.
Taking the logarithm on both sides of the second equation, we obtain that
logË†sâŠ¤
tË†yt
Ë†sâŠ¤
tË†BtË†st= log Det(Ë†Bt+1)âˆ’logDet(Ë†Bt).
19Thus, we obtain that
Î¨(Ë†Bt+1)âˆ’Î¨(Ë†Bt) =Tr(Ë†Bt+1)âˆ’Tr(Ë†Bt) + log Det(Ë†Bt)âˆ’logDet(Ë†Bt+1)
=âˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâˆ’âˆ¥Ë†BtË†stâˆ¥2
Ë†sâŠ¤
tË†BtË†stâˆ’logË†sâŠ¤
tË†yt
Ë†sâŠ¤
tË†BtË†st=âˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâˆ’âˆ¥Ë†BtË†stâˆ¥2
Ë†sâŠ¤
tË†BtË†stâˆ’logË†sâŠ¤
tË†yt
âˆ¥Ë†stâˆ¥2âˆ’logâˆ¥Ë†stâˆ¥2
Ë†sâŠ¤
tË†BtË†st,
which leads to
âˆ¥Ë†BtË†stâˆ¥2
Ë†sâŠ¤
tË†BtË†stâˆ’logË†sâŠ¤
tË†BtË†st
âˆ¥Ë†stâˆ¥2âˆ’1 = Î¨( Ë†Bt)âˆ’Î¨(Ë†Bt+1) +âˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâˆ’1 + logâˆ¥Ë†stâˆ¥2
Ë†sâŠ¤
tË†yt.
Notice that Ë†BtË†st=âˆ’Î·tË†gt,Ë†sâŠ¤
tË†BtË†st=âˆ’Î·2
tË†gâŠ¤
tË†dtandâˆ¥Ë†stâˆ¥2=Î·2
tâˆ¥Ë†dtâˆ¥2, we have that
âˆ¥Ë†gtâˆ¥2
âˆ’Ë†gâŠ¤
tË†dtâˆ’logâˆ’Ë†gâŠ¤
tË†dt
âˆ¥Ë†dtâˆ¥2âˆ’1 = Î¨( Ë†Bt)âˆ’Î¨(Ë†Bt+1) +âˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâˆ’1 + logâˆ¥Ë†stâˆ¥2
Ë†sâŠ¤
tË†yt.
Note that given the fact that âˆ’Ë†gâŠ¤
tË†dt= Ë†gâŠ¤
tË†Bâˆ’1
tË†gt>0, by using the Cauchyâ€“Schwarz inequality we
obtainâˆ¥Ë†gtâˆ¥2
âˆ’Ë†gâŠ¤
tË†dtâ‰¥âˆ’Ë†gâŠ¤
tË†dt
âˆ¥Ë†dtâˆ¥2. Hence, we can write
âˆ’Ë†gâŠ¤
tË†dt
âˆ¥Ë†dtâˆ¥2âˆ’logâˆ’Ë†gâŠ¤
tË†dt
âˆ¥Ë†dtâˆ¥2âˆ’1â‰¤Î¨(Ë†Bt)âˆ’Î¨(Ë†Bt+1) +âˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâˆ’1 + logâˆ¥Ë†stâˆ¥2
Ë†sâŠ¤
tË†yt.
Now, by selecting the weight matrix as P=âˆ‡2f(xâˆ—), many expressions get simplified and we have
âˆ’Ë†gâŠ¤
tË†dt
âˆ¥Ë†dtâˆ¥2=âˆ’gâŠ¤
tdt
âˆ¥Ëœdtâˆ¥2=Ït,Ïtâˆ’logÏtâˆ’1 =Ï‰(Ïtâˆ’1), and Ë†Bt=ËœBt=âˆ‡2f(xâˆ—)âˆ’1
2Btâˆ‡2f(xâˆ—)âˆ’1
2.
Hence, we have
Ï‰(Ïtâˆ’1)â‰¤Î¨(ËœBt)âˆ’Î¨(ËœBt+1) +âˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâˆ’1 + logâˆ¥Ë†stâˆ¥2
Ë†sâŠ¤
tË†yt. (47)
Notice thatâˆ¥Ë†ytâˆ¥2
Ë†sâŠ¤
tË†ytâ‰¤1 +Ctfor any tâ‰¥0by using Lemma D.3 (b) with P=âˆ‡2f(xâˆ—)and
logâˆ¥Ë†stâˆ¥2
Ë†sâŠ¤
tË†yt= logâˆ¥Ë†stâˆ¥2
Ë†sâŠ¤
tË†JtË†stâ‰¤log(1 + Ct)â‰¤Ctfor any tâ‰¥0by using (31) from Lemma A.1.
Leveraging these conditions with the inequality (47), we obtain that
Ï‰(Ïtâˆ’1)â‰¤Î¨(ËœBt)âˆ’Î¨(ËœBt+1) + 2Ct.
Summing both sides of the above inequality from i= 0totâˆ’1, we prove the conclusion
tâˆ’1X
i=0Ï‰(Ïiâˆ’1)â‰¤Î¨(ËœB0)âˆ’Î¨(ËœBt) + 2tâˆ’1X
i=0Ciâ‰¤Î¨(ËœB0) + 2tâˆ’1X
i=0Ci,
where the last inequality holds since Î¨(ËœBt)â‰¥0.
Lemma G.3. Suppose Assumptions 2.1, 2.2, and 2.3 hold and Ctâ‰¤1
6andÏtâ‰¥7
8at iteration t,
then we have
f(xt+dt)â‰¤f(xt). (48)
Proof. Since assumption 2.3 hold, using Lemma 1.2.4 in [44], we have that
|f(y)âˆ’f(x)âˆ’ âˆ‡f(x)âŠ¤(yâˆ’x)âˆ’1
2(yâˆ’x)âŠ¤âˆ‡2f(x)(yâˆ’x)| â‰¤M
6âˆ¥yâˆ’xâˆ¥3,âˆ€x, yâˆˆRd.
Setting x=xtandy=xt+dt, we have that
f(xt+dt)âˆ’f(xt)â‰¤gâŠ¤
tdt+1
2dâŠ¤
tâˆ‡2f(xt)dt+M
6âˆ¥dtâˆ¥3. (49)
Notice that using (33) from Lemma A.1 and the definition of Ïtin (21), we have that
dâŠ¤
tâˆ‡2f(xt)dtâ‰¤(1 +Ct)dâŠ¤
tâˆ‡2f(xâˆ—)dt=âˆ’gâŠ¤
tdt(1 +Ct)âˆ¥Ëœdtâˆ¥2
âˆ’gâŠ¤
tdt=âˆ’gâŠ¤
tdt1 +Ct
Ït. (50)
20Applying Assumption 2.1 with the definition Ëœdt=âˆ‡2f(xâˆ—)1
2dt, we obtain that
âˆ¥dtâˆ¥3â‰¤1
Âµ3
2âˆ¥Ëœdtâˆ¥3=âˆ’gâŠ¤
tdt
Âµ3
2âˆ¥Ëœdtâˆ¥2
âˆ’gâŠ¤
tdtâˆ¥Ëœdtâˆ¥=âˆ’gâŠ¤
tdt
Âµ3
21
Ïtâˆ¥Ëœdtâˆ¥.
Sinceâˆ’ËœgâŠ¤
tËœdtâ‰¤ âˆ¥Ëœgtâˆ¥âˆ¥Ëœdtâˆ¥by Cauchyâ€“Schwarz inequality where Ëœgt=âˆ‡2f(xâˆ—)âˆ’1
2gt, we obtain
âˆ¥Ëœdtâˆ¥=âˆ¥Ëœgtâˆ¥âˆ¥Ëœdtâˆ¥
âˆ¥Ëœgtâˆ¥â‰¤ âˆ¥Ëœgtâˆ¥âˆ¥Ëœdtâˆ¥2
âˆ’ËœgâŠ¤
tËœdt=1
Ïtâˆ¥Ëœgtâˆ¥,
which leads to
âˆ¥dtâˆ¥3â‰¤âˆ’gâŠ¤
tdt
Âµ3
21
Ïtâˆ¥Ëœdtâˆ¥ â‰¤âˆ’gâŠ¤
tdt
Âµ3
21
Ï2
tâˆ¥Ëœgkâˆ¥. (51)
By applying Taylorâ€™s theorem with Lagrange remainder, there exists ËœÏ„tâˆˆ[0,1]such that
f(xt) =f(xâˆ—) +âˆ‡f(xâˆ—)âŠ¤(xtâˆ’xâˆ—) +1
2(xtâˆ’xâˆ—)âŠ¤âˆ‡2f(xt+ ËœÏ„t(xâˆ—âˆ’xt))(xtâˆ’xâˆ—)
=f(xâˆ—) +1
2(xtâˆ’xâˆ—)âŠ¤âˆ‡2f(xt+ ËœÏ„t(xâˆ—âˆ’xt))(xtâˆ’xâˆ—),(52)
where we used the fact that âˆ‡f(xâˆ—) = 0 in the last equality. Moreover, by the fundamental theorem
of calculus, we have
âˆ‡f(xt)âˆ’ âˆ‡f(xâˆ—) =Z1
0âˆ‡2f(xt+Ï„(xâˆ—âˆ’xt))(xtâˆ’xâˆ—)dÏ„=Gt(xtâˆ’xâˆ—),
where we use the definition of Gtin(8). Since âˆ‡f(xâˆ—) = 0 and we denote gt=âˆ‡f(xt), this further
implies that
xtâˆ’xâˆ—=Gâˆ’1
t(âˆ‡f(xt)âˆ’ âˆ‡f(xâˆ—)) =Gâˆ’1
tgt. (53)
Combining (52) and (53) leads to
f(xt)âˆ’f(xâˆ—) =1
2gâŠ¤
tGâˆ’1
tâˆ‡2f(xt+ ËœÏ„t(xâˆ—âˆ’xt))Gâˆ’1
tgt. (54)
Based on (35) in Lemma A.1, we have âˆ‡2f(xt+ ËœÏ„t(xâˆ—âˆ’xt))âª°1
1+CtGt, which implies that
Gâˆ’1
tâˆ‡2f(xt+ ËœÏ„t(xâˆ—âˆ’xt))Gâˆ’1
tâª°1
1 +CtGâˆ’1
t.
Moreover, it follows from (34) in Lemma A.1 that Gtâª¯(1 +Ct)âˆ‡2f(xâˆ—), which implies that
Gâˆ’1
tâª°1
1 +Ct(âˆ‡2f(xâˆ—))âˆ’1.
Combining the above two conditions, we obtain that
Gâˆ’1
tâˆ‡2f(xt+ ËœÏ„t(xâˆ—âˆ’xt))Gâˆ’1
tâª°1
(1 +Ct)2(âˆ‡2f(xâˆ—))âˆ’1,
and hence
gâŠ¤
tGâˆ’1
tâˆ‡2f(xt+ ËœÏ„t(xâˆ—âˆ’xt))Gâˆ’1
tgtâ‰¥1
(1 +Ct)2gâŠ¤
t(âˆ‡2f(xâˆ—))âˆ’1gt=1
(1 +Ct)2âˆ¥Ëœgtâˆ¥2.(55)
Combining (54) and (55) leads to
âˆ¥Ëœgkâˆ¥ â‰¤(1 +Ct)p
2(f(xt)âˆ’f(xâˆ—)). (56)
Combining (51) and (56) leads to
âˆ¥dtâˆ¥3â‰¤âˆ’gâŠ¤
tdt
Âµ3
21
Ï2
tâˆ¥Ëœgkâˆ¥ â‰¤âˆ’gâŠ¤
tdt
Âµ3
21
Ï2
t(1 +Ct)p
2(f(xt)âˆ’f(xâˆ—)). (57)
21Leveraging (49), (50) and (57) with the definition of Ctin (9), we have that
f(xt+dt)âˆ’f(xt)â‰¤gâŠ¤
tdt+1
2dâŠ¤
tâˆ‡2f(xt)dt+M
6âˆ¥dtâˆ¥3
=âˆ’gâŠ¤
tdt(âˆ’1 +1 +Ct
2Ït+M
61
Âµ3
21
Ï2
t(1 +Ct)p
2(f(xt)âˆ’f(xâˆ—)))
=âˆ’gâŠ¤
tdt(âˆ’1 +1 +Ct
2Ït+Ct(1 +Ct)
6Ï2
t).(58)
Notice that âˆ’gâŠ¤
tdt=âˆ’gâŠ¤
tBâˆ’1
tgt>0and when Ctâ‰¤1
6andÏtâ‰¥7
8, we can verify that
1 +Ct
2Ït+Ct(1 +Ct)
6Ï2
t<1.
Therefore, (58) implies the conclusion that
f(xt+dt)âˆ’f(xt)â‰¤0.
G.2 Proof of Lemma 6.1
Since Î·t= 1satisfies Armijo-Wolfe conditions, we know that Î·tis chosen to be one at iteration tand
xt+1=xt+dt. We have f(xt+1)â‰¤f(xt)from Lemma 2.1. Using Taylorâ€™s expansion, we have
thatf(xt+1) =f(xt) +gâŠ¤
tdt+1
2dâŠ¤
tâˆ‡2f(xt+ Ë†Ï„(xt+1âˆ’xt))dt, where Ë†Ï„âˆˆ[0,1]. Hence, we have
that
Ë†pt=f(xt)âˆ’f(xt+1)
âˆ’gâŠ¤
tdt=âˆ’gâŠ¤
tdtâˆ’1
2dâŠ¤
tâˆ‡2f(xt+ Ë†Ï„(xt+1âˆ’xt))dt
âˆ’gâŠ¤
tdt
= 1âˆ’1
2dâŠ¤
tâˆ‡2f(xt+ Ë†Ï„(xt+1âˆ’xt))dt
âˆ’gâŠ¤
tdtâ‰¥1âˆ’1 +Ct
2dâŠ¤
tâˆ‡2f(xâˆ—)dt
âˆ’gâŠ¤
tdt= 1âˆ’1 +Ct
2Ït,
where we apply the (32) from Lemma A.1 since f(xt+1)â‰¤f(xt)and recall the definition of Ïtin
(21). Similarly, using (31) from Lemma A.1 since f(xt+1)â‰¤f(xt), we have that
Ë†nt=yâŠ¤
tst
âˆ’gâŠ¤
tst=sâŠ¤
tJtst
âˆ’gâŠ¤
tst=dâŠ¤
tJtdt
âˆ’gâŠ¤
tdtâ‰¥1
1 +CtdâŠ¤
tâˆ‡2f(xâˆ—)dt
âˆ’gâŠ¤
tdt=1
(1 +Ct)Ït,
where we use the fact that yt=JtstwithJtdefined in (8)andst=xt+1âˆ’xt=dt. Therefore, we
prove the conclusions.
G.3 Proof of Lemma 6.2
Denote Â¯xt+1=xt+dtandÂ¯st= Â¯xt+1âˆ’xt=dt. Since Î´1â‰¤1
6andÎ´2â‰¥7
8, we have f(Â¯xt+1)â‰¤
f(xt)from Lemma G.3. Using Taylorâ€™s expansion, we have that f(Â¯xt+1) =f(xt) +gâŠ¤
tdt+
1
2dâŠ¤
tâˆ‡2f(xt+ Ë†Ï„(Â¯xt+1âˆ’xt))dt, where Ë†Ï„âˆˆ[0,1]. Hence, we have
f(xt)âˆ’f(Â¯xk+1)
âˆ’gâŠ¤
tdt=âˆ’gâŠ¤
tdtâˆ’1
2dâŠ¤
tâˆ‡2f(xt+ Ë†Ï„(Â¯xt+1âˆ’xt))dt
âˆ’gâŠ¤
tdt
= 1âˆ’1
2dâŠ¤
tâˆ‡2f(xt+ Ë†Ï„(Â¯xt+1âˆ’xt))dt
âˆ’gâŠ¤
tdtâ‰¥1âˆ’1 +Ct
2dâŠ¤
tâˆ‡2f(xâˆ—)dt
âˆ’gâŠ¤
tdt= 1âˆ’1 +Ct
2Ït,
where we apply the (32) from Lemma A.1 since f(Â¯xt+1)â‰¤f(xt). Therefore, when Ctâ‰¤Î´1â‰¤p
2(1âˆ’Î±)âˆ’1andÏtâ‰¥Î´2â‰¥1âˆš
2(1âˆ’Î±), we obtain thatf(xt)âˆ’f(Â¯xk+1)
âˆ’gâŠ¤
tdtâ‰¥1âˆ’1+Ct
2Ïtâ‰¥Î±and unit
step size Î·t= 1satisfies the sufficient condition (5).
Similarly, using (31) from Lemma A.1 since f(Â¯xt+1)â‰¤f(xt)and denote Â¯gk+1=âˆ‡f(Â¯xt+1),
Â¯yt= Â¯gk+1âˆ’gt, we have that
Â¯yâŠ¤
tÂ¯st
âˆ’gâŠ¤
tÂ¯st=Â¯sâŠ¤
tJtÂ¯st
âˆ’gâŠ¤
tÂ¯st=dâŠ¤
tJtdt
âˆ’gâŠ¤
tdtâ‰¥1
1 +CtdâŠ¤
tâˆ‡2f(xâˆ—)dt
âˆ’gâŠ¤
tdt=1
(1 +Ct)Ït.
22Therefore, when Ctâ‰¤Î´1â‰¤1âˆš1âˆ’Î²âˆ’1andÏtâ‰¤Î´3=1âˆš1âˆ’Î², we obtain thatÂ¯yâŠ¤
tÂ¯st
âˆ’gâŠ¤
tÂ¯stâ‰¥1
(1+Ct)Ïtâ‰¥
1âˆ’Î², which indicates that Â¯gâŠ¤
t+1dt= Â¯gâŠ¤
t+1Â¯st= Â¯yâŠ¤
tÂ¯st+gâŠ¤
tÂ¯stâ‰¥ âˆ’gâŠ¤
tÂ¯st(1âˆ’Î²) +gâŠ¤
tÂ¯st=Î²gâŠ¤
tÂ¯st=
Î²gâŠ¤
tdt. Hence, unit step size Î·t= 1satisfies the curvature condition (6). Therefore, we prove that
when Ctâ‰¤Î´1andÎ´2â‰¤Ïtâ‰¤Î´3, step size Î·t= 1satisfies the Armijo-Wolfe conditions (5) and (6).
G.4 Proof of Lemma 6.3
Since in Theorem 4.1, we already prove that
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’eâˆ’Î¨(Â¯B0)
t2Î±(1âˆ’Î²)
Îºt
.
This implies that
Ctâ‰¤
1âˆ’eâˆ’Î¨(Â¯B0)
t2Î±(1âˆ’Î²)
Îºt
2
C0.
When tâ‰¥Î¨(Â¯B0), we obtain that
Ctâ‰¤
1âˆ’2Î±(1âˆ’Î²)
3Îºt
2
C0.
When tâ‰¥3Îº
Î±(1âˆ’Î²)logC0
Î´1, we obtain that
Ctâ‰¤
1âˆ’2Î±(1âˆ’Î²)
3Îºt
2
C0â‰¤Î´1.
Therefore, the first claim in (24) follows.
Now define I1={t:Ït< Î´2}andI2={t:Ït> Î´3}, we know that |I|=|I1|+|I2|.
Notice that for tâˆˆI1, we have that Ïtâˆ’1< Î´2âˆ’1<0since Î´2<1and the function Ï‰(x)
defined in (45) is decreasing for âˆ’1< x < 0from (a) in Lemma G.1. Hence, we have thatP
iâˆˆI1Ï‰(Ïiâˆ’1)â‰¥P
iâˆˆI1Ï‰(Î´2âˆ’1) = Ï‰(Î´2âˆ’1)|I1|. Similarly, we have that for tâˆˆI2, we have
thatÏiâˆ’1> Î´3âˆ’1>0since Î´3>1and the function Ï‰(x)is increasing for x >0from (a) in
Lemma G.1. Hence, we have thatP
iâˆˆI2Ï‰(Ïiâˆ’1)â‰¥P
iâˆˆI2Ï‰(Î´3âˆ’1) = Ï‰(Î´3âˆ’1)|I2|. Using (46)
from Proposition G.2, we have thatPtâˆ’1
i=0Ï‰(Ïiâˆ’1)â‰¤Î¨(ËœB0) + 2Ptâˆ’1
i=0Ciâ‰¤Î¨(ËœB0) + 2P+âˆ
i=0Ci
for any tâ‰¥1. Therefore, we obtain that
Î¨(ËœB0) + 2+âˆX
i=0Ciâ‰¥+âˆX
i=0Ï‰(Ïiâˆ’1)â‰¥X
iâˆˆI1Ï‰(Î²iâˆ’1) +X
iâˆˆI2Ï‰(Î²iâˆ’1)
â‰¥Ï‰(Î´2âˆ’1)|I1|+Ï‰(Î´3âˆ’1)|I2| â‰¥min{Ï‰(Î´2âˆ’1), Ï‰(Î´3âˆ’1)}(|I1|+|I2|),
which leads to the result
|I|=|I1|+|I2| â‰¤Î¨(ËœB0) + 2P+âˆ
i=0Ci
min{Ï‰(Î´2âˆ’1), Ï‰(Î´3âˆ’1)}=Î´4 
Î¨(ËœB0) + 2+âˆX
i=0Ci!
, (59)
where Î´4:=1
min{Ï‰(Î´2âˆ’1),Ï‰(Î´3âˆ’1)}. Using the upper bound ofP+âˆ
i=0Ciâ‰¤C0Î¨(Â¯B0) +3C0Îº
Î±(1âˆ’Î²)in
(44), we prove the second claim in (25).
G.5 Proof of Theorem 6.4
First, we prove that for any initial point x0âˆˆRdand any initial Hessian approximation matrix
B0âˆˆSd
++, the following result holds:
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ 
Î´6t0+Î´7Î¨(ËœB0) +Î´8P+âˆ
i=0Ci
t!t
,âˆ€t > t 0,
23where t0is defined in (24). We choose the weight matrix as P=âˆ‡2f(xâˆ—)throughout the proof.
Using results (41) and (42) from the proof of Proposition 5.1, we obtain that
tâˆ’1Y
i=0cos2(Ë†Î¸i)
Ë†miâ‰¥eâˆ’Î¨(ËœB0)âˆ’Ptâˆ’1
i=0Ciâ‰¥eâˆ’Î¨(ËœB0)âˆ’P+âˆ
i=0Ci. (60)
tâˆ’1Y
i=0Ë†qiâ‰¥2teâˆ’2Ptâˆ’1
i=0Ciâ‰¥2teâˆ’2P+âˆ
i=0Ci. (61)
Recall the definition of the set I={t:Ït/âˆˆ[Î´2, Î´3]}. Notice that for tâ‰¥t0, define I3={t:tâ‰¥
t0, Ït/âˆˆ[Î´2, Î´3]}andI4={t:tâ‰¥t0, Ïtâˆˆ[Î´2, Î´3]}. Then, we have that
tâˆ’1Y
i=0Ë†piË†ni=t0âˆ’1Y
i=0Ë†piË†nitâˆ’1Y
i=t0Ë†piË†ni=t0âˆ’1Y
i=0Ë†piË†niY
iâˆˆI3Ë†piË†niY
iâˆˆI4Ë†piË†ni. (62)
From Lemma 2.1, we know Ë†ptâ‰¥Î±andË†ntâ‰¥1âˆ’Î²for any tâ‰¥0, which lead to
t0âˆ’1Y
i=0Ë†piË†niâ‰¥Î±t0(1âˆ’Î²)t0=1
2t0eâˆ’t0log1
2Î±(1âˆ’Î²). (63)
Y
iâˆˆI3Ë†piË†niâ‰¥Y
iâˆˆI3Î±(1âˆ’Î²) =1
2|I3|eâˆ’|I3|log1
2Î±(1âˆ’Î²)â‰¥1
2|I3|eâˆ’|I|log1
2Î±(1âˆ’Î²)
â‰¥1
2|I3|eâˆ’Î´4
Î¨(ËœB0)+2P+âˆ
i=0Ci
log1
2Î±(1âˆ’Î²),(64)
where the second inequality holds since |I3| â‰¤ |I|,log1
2Î±(1âˆ’Î²)>0and the last inequality holds
since (59) from the proof of Lemma 6.3 in Appendix G.4. Notice that when index iâˆˆI4, we have
Ciâ‰¤Î´1from Lemma 6.3 and Ïiâˆˆ[Î´2, Î´3]. Applying Lemma 6.1 and Lemma 6.2, we know that for
iâˆˆI4,Î·i= 1satisfies the Armijo-Wolfe conditions (5),(6)and we have Ë†piâ‰¥1âˆ’1+Ci
2Ïi>0(since
Ciâ‰¤Î´1â‰¤1
6,Ïiâ‰¥Î´2â‰¥7
8) and Ë†niâ‰¥1
(1+Ci)Ïifrom (22). Hence, we obtain that
Y
iâˆˆI4Ë†piË†niâ‰¥1
2|I4|Y
iâˆˆI4(2âˆ’1 +Ci
Ïi)1
(1 +Ci)Ïiâ‰¥1
2|I4|eâˆ’P
iâˆˆI4CiY
iâˆˆI4(2âˆ’1 +Ci
Ïi)1
Ïi,(65)
where the last inequality holds since1
1+Ciâ‰¥eâˆ’Ci. Using the fact that logxâ‰¥1âˆ’1
x, we obtain
Y
iâˆˆI4(2âˆ’1 +Ci
Ïi)1
Ïi=Y
iâˆˆI4elog (2âˆ’1+Ci
Ïi)âˆ’logÏiâ‰¥Y
iâˆˆI4e1âˆ’1
2âˆ’1+Ci
Ïiâˆ’logÏi
=Y
iâˆˆI4eÏiâˆ’1âˆ’Ci
2Ïiâˆ’1âˆ’Ciâˆ’logÏi=Y
iâˆˆI4eÏiâˆ’1âˆ’logÏi+2(1âˆ’Ïi) logÏiâˆ’(1âˆ’logÏi)Ci
2Ïiâˆ’1âˆ’Ci
=Y
iâˆˆI4eÏ‰(Ïiâˆ’1)+2(1 âˆ’Ïi) logÏiâˆ’(1âˆ’logÏi)Ci
2Ïiâˆ’1âˆ’Ci â‰¥Y
iâˆˆI4eâˆ’2(Ïiâˆ’1) log Ïiâˆ’(1âˆ’logÏi)Ci
2Ïiâˆ’1âˆ’Ci
=Y
iâˆˆI4eâˆ’2(Ïiâˆ’1) log Ïi+(1âˆ’logÏi)Ci
2Ïiâˆ’1âˆ’Ci â‰¥Y
iâˆˆI4eâˆ’2(Ïiâˆ’1) log Ïi+(1âˆ’logÎ´2)Ci
2Î´2âˆ’1âˆ’Î´1 ,(66)
where the second inequality holds since Ï‰(Ïiâˆ’1)â‰¥0and the third inequality holds since Ïiâ‰¥Î´2
due to iâˆˆI4andCiâ‰¤Î´1due to iâ‰¥t0and Lemma 6.3. Notice that 2Ïiâˆ’1âˆ’Ciâ‰¥2Î´2âˆ’1âˆ’Î´1>0
for all iâˆˆI4since Ciâ‰¤Î´1â‰¤1
6andÏiâ‰¥Î´2â‰¥7
8.
When Ïiâ‰¥1, using logÏiâ‰¤Ïiâˆ’1, (b) in Lemma G.1 and Ïiâ‰¤Î´3due to iâˆˆI4, we have that
(Ïiâˆ’1) log Ïiâ‰¤(Ïiâˆ’1)2â‰¤2ÏiÏ‰(Ïiâˆ’1)â‰¤2Î´3Ï‰(Ïiâˆ’1). (67)
Similarly, when Ïi<1, using logÏiâ‰¥1âˆ’1
Ïi, (c) in Lemma G.1 and Ïiâ‰¥Î´2due to iâˆˆI4, we have
(Ïiâˆ’1) log Ïiâ‰¤(Ïiâˆ’1)2
Ïiâ‰¤Ïi+ 1
ÏiÏ‰(Ïiâˆ’1)â‰¤(1 +1
Î´2)Ï‰(Ïiâˆ’1). (68)
24Combining (66), (67) and (68), we obtain that
Y
iâˆˆI4(2âˆ’1 +Ci
Ïi)1
Ïi
â‰¥Y
iâˆˆI4eâˆ’2(Ïiâˆ’1) log Ïi+(1âˆ’logÎ´2)Ci
2Î´2âˆ’1âˆ’Î´1 =Y
iâˆˆI4eâˆ’2(Ïiâˆ’1) log Ïi
2Î´2âˆ’1âˆ’Î´1Y
iâˆˆI4eâˆ’(1âˆ’logÎ´2)Ci
2Î´2âˆ’1âˆ’Î´1
=Y
iâˆˆI4,Ïi<1eâˆ’2(Ïiâˆ’1) log Ïi
2Î´2âˆ’1âˆ’Î´1Y
iâˆˆI4,Ïiâ‰¥1eâˆ’2(Ïiâˆ’1) log Ïi
2Î´2âˆ’1âˆ’Î´1Y
iâˆˆI4eâˆ’(1âˆ’logÎ´2)Ci
2Î´2âˆ’1âˆ’Î´1
â‰¥Y
iâˆˆI4,Ïi<1eâˆ’2(1+1
Î´2)Ï‰(Ïiâˆ’1)
2Î´2âˆ’1âˆ’Î´1Y
iâˆˆI4,Ïiâ‰¥1eâˆ’4Î´3Ï‰(Ïiâˆ’1)
2Î´2âˆ’1âˆ’Î´1Y
iâˆˆI4eâˆ’(1âˆ’logÎ´2)Ci
2Î´2âˆ’1âˆ’Î´1
=eâˆ’2+2
Î´4
2Î´2âˆ’1âˆ’Î´1P
iâˆˆI2,Ïi<1Ï‰(Ïiâˆ’1)âˆ’4Î´3
2Î´2âˆ’1âˆ’Î´1P
iâˆˆI4,Ïiâ‰¥1Ï‰(Ïiâˆ’1)âˆ’1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1P
iâˆˆI4Ci
â‰¥eâˆ’Î´5P
iâˆˆI4,Ïi<1Ï‰(Ïiâˆ’1)+P
iâˆˆI4,Ïiâ‰¥1Ï‰(Ïiâˆ’1)
âˆ’1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1P
iâˆˆI4Ci
=eâˆ’Î´5P
iâˆˆI4Ï‰(Ïiâˆ’1)âˆ’1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1P
iâˆˆI4Ci(69)
where Î´5= max {2+2
Î´2
2Î´2âˆ’1âˆ’Î´1,4Î´3
2Î´2âˆ’1âˆ’Î´1}. Combining (65) and (69), we obtain that
Y
iâˆˆI4Ë†piË†niâ‰¥1
2|I4|eâˆ’P
iâˆˆI4CiY
iâˆˆI4(2âˆ’1 +Ci
Ïi)1
Ïi
â‰¥1
2|I4|eâˆ’Î´5P
iâˆˆI4Ï‰(Ïiâˆ’1)âˆ’(1+1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1)P
iâˆˆI4Ci
â‰¥1
2|I4|eâˆ’Î´5P+âˆ
i=0Ï‰(Ïiâˆ’1)âˆ’2Î´2âˆ’Î´1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1P+âˆ
i=0Ci
â‰¥1
2|I4|eâˆ’Î´5
Î¨(ËœB0)+2P+âˆ
i=0Ci
âˆ’2Î´2âˆ’Î´1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1P+âˆ
i=0Ci,(70)
where the last inequality is due to (46) from Lemma G.1. Combining (62),(63),(64) and(70), we
obtain that
tâˆ’1Y
i=0Ë†piË†ni=t0âˆ’1Y
i=0Ë†piË†niY
iâˆˆI3Ë†piË†niY
iâˆˆI4Ë†piË†ni
â‰¥1
2teâˆ’
t0log1
2Î±(1âˆ’Î²)+(Î´4log1
2Î±(1âˆ’Î²)+Î´5)Î¨(ËœB0)+(2Î´4log1
2Î±(1âˆ’Î²)+2Î´5+2Î´2âˆ’Î´1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1)P+âˆ
i=0Ci
.(71)
Leveraging (60), (61), (71) with (15) from Proposition 3.1, we prove that
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ï£®
ï£°1âˆ’ tâˆ’1Y
i=0Ë†piË†qiË†nicos2(Ë†Î¸i)
Ë†mi!1
tï£¹
ï£»t
=ï£®
ï£°1âˆ’ tâˆ’1Y
i=0Ë†piË†nitâˆ’1Y
i=0Ë†qitâˆ’1Y
i=0cos2(Ë†Î¸i)
Ë†mi!1
tï£¹
ï£»t
â‰¤ 
1âˆ’eâˆ’t0log1
2Î±(1âˆ’Î²)+(1+ Î´4log1
2Î±(1âˆ’Î²)+Î´5)Î¨(ËœB0)+(3+2 Î´4log1
2Î±(1âˆ’Î²)+2Î´5+2Î´2âˆ’Î´1âˆ’logÎ´2
2Î´2âˆ’1âˆ’Î´1)P+âˆ
i=0Ci
t!t
=
1âˆ’eâˆ’Î´6t0+Î´7Î¨(ËœB0)+Î´8P+âˆ
i=0Ci
tt
â‰¤ 
Î´6t0+Î´7Î¨(ËœB0) +Î´8P+âˆ
i=0Ci
t!t
,
where the inequality is due to the fact that 1âˆ’eâˆ’xâ‰¤xfor any xâˆˆRandÎ´6, Î´7, Î´8are defined in
Theorem 6.4. Hence, we prove that
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤ 
Î´6t0+Î´7Î¨(ËœB0) +Î´8P+âˆ
i=0Ci
t!t
,âˆ€t > t 0. (72)
25Using (44) from the proof of Theorem 5.2 in Appendix F.2, we have that
+âˆX
i=0Ciâ‰¤C0Î¨(Â¯B0) +3C0Îº
Î±(1âˆ’Î²). (73)
Notice that from (24) in Lemma 6.3, we have that
t0= max {Î¨(Â¯B0),3Îº
Î±(1âˆ’Î²)logC0
Î´1} â‰¤Î¨(Â¯B0) +3Îº
Î±(1âˆ’Î²)logC0
Î´1. (74)
Leveraging (72), (73) and (74), we prove the conclusion.
G.6 Proof of Corollary 6.5
Using the fact that for B0=LI, we have Î¨(Â¯B0) = 0 andÎ¨(ËœB0)â‰¤dÎº, and for the case that
B0=ÂµI, we have Î¨(Â¯B0)â‰¤dlogÎº,andÎ¨(ËœB0)â‰¤dlogÎº, we obtain the corresponding superlinear
results for these two conditions.
G.7 Specific Values of {Î´i}8
i=1
As we stated before, all the {Î´i}8
i=1are universal constants that only depend on line search parameters
Î±andÎ². We can choose specific values of Î±andÎ²to make definitions of {Î´i}8
i=1more clear. If we
pickÎ±=1
4andÎ²=3
4, we have that
Î´1=1
6, Î´ 2=7
8, Î´ 3= 2, Î´ 4= 118 , Î´ 5= 14, Î´ 6= log 8 , Î´ 7= 260 , Î´ 8= 524 .
H Complexity of BFGS with the Initialization B0=cI
Recall that câˆˆ[Âµ, L]by our choice of cin Remark 6.2. If we choose B0=cI, then Î¨(Â¯B0) =
Î¨(c
LI) =c
Ldâˆ’d+dlogL
c. Moreover, we have Î¨(ËœB0) = Î¨( câˆ‡2f(xâˆ—)âˆ’1) =cTr(âˆ‡2f(xâˆ—)âˆ’1)âˆ’
dâˆ’logDet(câˆ‡2f(xâˆ—)âˆ’1), which is determined by the Hessian matrix âˆ‡2f(xâˆ—)âˆ’1. In this case,
one can use the upper bounds Î¨(Â¯B0) =d(c
Lâˆ’1 + logL
c)andÎ¨(ËœB0) =Tr(câˆ‡2f(xâˆ—)âˆ’1)âˆ’dâˆ’
logDet(câˆ‡2f(xâˆ—)âˆ’1)â‰¤d(c
Âµâˆ’1 + logL
c)to simplify the expressions.
Applying these values of Î¨(Â¯B0)andÎ¨(ËœB0)to our linear convergence result in Theorem 4.1 and the
superlinear convergence result in Theorem 6.4, we can obtain the following convergence guarantees
forB0=cI:
â€¢ For tâ‰¥d(c
Lâˆ’1 + logL
c), we havef(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
1âˆ’2Î±(1âˆ’Î²)
3Îºt
;
â€¢Fort= â„¦( d(c
Âµâˆ’1 + logL
c) +C0d(c
Lâˆ’1 + logL
c) +C0Îº), we havef(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤
 
O d(c
Âµâˆ’1+logL
c)+C0d(c
Lâˆ’1+logL
c)+C0Îº
tt.
Moreover, we can derive similar iteration complexity bounds following the same arguments as in
Section I. We also include the performance of BFGS with B0=cIin our numerical experiments as
presented in Figure 1. We observe that the performance of BFGS with B0=cIis very similar to the
convergence curve of BFGS with B0=ÂµIin our numerical experiments.
I Proof of Iteration Complexity
When B0=LI, if we regard the line search parameters Î±andÎ²as absolute constants, the first result
established in Corollary 4.2 leads to a global complexity of O(Îºlog1
Ïµ), which is on par with gradient
descent. Moreover, the first result in Corollary 5.3 implies a complexity of O 
(d+C0)Îº+ log1
Ïµ
,
where the first term represents the number of iterations required to attain the linear rate in (20), and
the second term represents the additional number of iterations needed to achieve the desired accuracy
26Ïµfrom the condition number-independent linear rate. For the analysis of the superlinear convergence
rate, we denote that â„¦L=dÎº+C0Îº. From the first result in Corollary 6.5, we have that
f(xt)âˆ’f(xâˆ—)
f(x0)âˆ’f(xâˆ—)â‰¤(â„¦L
t)t
LetTâˆ—be the number such that the inequality (â„¦L
t)tâ‰¤Ïµabove becomes equality. we have
log1
Ïµ=Tâˆ—logTâˆ—
â„¦Lâ‰¤Tâˆ—(Tâˆ—
â„¦Lâˆ’1),
Tâˆ—â‰¥â„¦L+q
â„¦2
L+ 4â„¦ Llog1
Ïµ
2.
Hence, we have that
log1
Ïµ=Tâˆ—logTâˆ—
â„¦Lâ‰¥Tâˆ—logâ„¦L+q
â„¦2
L+ 4â„¦ Llog1
Ïµ
2â„¦Lâ‰¥Tâˆ—logï£«
ï£­1
2+s
1
4+log1
Ïµ
â„¦Lï£¶
ï£¸,
Tâˆ—â‰¤log1
Ïµ
log
1
2+q
1
4+log1
Ïµ
â„¦L.
Hence, to reach the accuracy of Ïµ, we need the number of iterations tto be at least
tâ‰¥log1
Ïµ
log
1
2+q
1
4+1
â„¦Llog1
Ïµ.
Therefore, the iteration complexity for the case of B0=LIis
Oï£«
ï£­minï£±
ï£²
ï£³Îºlog1
Ïµ,(d+C0)Îº+ log1
Ïµ,log1
Ïµ
log
1
2+q
1
4+1
dÎº+C0Îºlog1
Ïµï£¼
ï£½
ï£¾ï£¶
ï£¸.
Similarly, in this case of B0=ÂµI, the second result in Corollary 4.2 establishes a global complexity
ofO 
dlogÎº+Îºlog1
Ïµ
, where the first term represents the number of iterations before the linear
convergence rate in (19) begins, and the second term arises from the linear rate itself. Addition-
ally, following the same argument, the second result in Corollary 5.3 indicates a complexity of
O(C0dlogÎº+C0Îº+ log1
Ïµ). Here, the first term accounts for the wait time until the convergence
rate takes effect, and the second term is associated with the condition number-independent linear rate.
For the superlinear convergence rate, when B0=ÂµI, to reach the accuracy of Ïµ, we need the number
of iterations tto be at least
tâ‰¥log1
Ïµ
log
1
2+q
1
4+1
â„¦Âµlog1
Ïµ,
where â„¦Âµ=C0dlogÎº+C0Îº. The proof is the same as the proof for the case of B0=LI. Therefore,
the iteration complexity for the case of B0=ÂµIis
Oï£«
ï£­minï£±
ï£²
ï£³dlogÎº+Îºlog1
Ïµ, C0(dlogÎº+Îº) + log1
Ïµ,log1
Ïµ
log
1
2+q
1
4+1
C0(dlogÎº+Îº)log1
Ïµï£¼
ï£½
ï£¾ï£¶
ï£¸.
J Log Bisection Algorithm for Weak Wolfe Conditions
27Algorithm 1 Log Bisection Algorithm for Weak Wolfe Conditions
Require: Initial step size Î·(0)= 1,Î·(0)
min= 0,Î·(0)
max= +âˆ
1:fori= 0,1,2, . . .do
2: iff(xt+Î·(i)dt)> f(xt) +Î±Î·(i)âˆ‡f(xt)âŠ¤dtthen
3: SetÎ·(i+1)
max =Î·(i)andÎ·(i+1)
min =Î·(i)
min
4: ifÎ·(i)
min= 0then
5: Î·(i+1)= (1
2)2i+1âˆ’1
6: else
7: Î·(i+1)=q
Î·(i+1)
maxÎ·(i+1)
min
8: end if
9: else if âˆ‡f(xt+Î·(i)dt)âŠ¤dt< Î²âˆ‡f(xt)âŠ¤dtthen
10: SetÎ·(i+1)
max =Î·(i)
max andÎ·(i+1)
min =Î·(i)
11: ifÎ·(i)
max= +âˆthen
12: Î·(i+1)= 22i+1âˆ’1
13: else
14: Î·(i+1)=q
Î·(i+1)
maxÎ·(i+1)
min
15: end if
16: else
17: Return Î·(i)
18: end if
19:end for
K Results and Discussion on the Bisection Scheme for Line Search in
Section 7
K.1 Proof of Lemma K.1
First, we present major results concerning the complexity of the bisection method, which specifies a
range of values that meet the conditions in (5) and (6).
Lemma K.1. Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Recall the definition of Ïtin(21)
andCtin(9). At iteration t, there is unique Î·r>0such that the sufficient decrease condition (5)is
equity for Î·r, i.e.,
f(xt+Î·rdt) =f(xt) +Î±Î·râˆ‡f(xt)âŠ¤dt. (75)
Then, Î·tsatisfies the sufficient decrease condition (5)if and only if Î·tâ‰¤Î·r. We also have that
2(1âˆ’Î±)
1 +CtÏtâ‰¤Î·râ‰¤2(1âˆ’Î±)(1 + Ct)Ït. (76)
Similarly, there is also unique Î·l>0such that the curvature condition (6)is equity for Î·l, i.e.,
âˆ‡f(xt+Î·ldt)âŠ¤dt=Î²âˆ‡f(xt)âŠ¤dt. (77)
Then, Î·tsatisfies the curvature condition (6)if and only if Î·tâ‰¥Î·l. Moreover, we have that
Î·r
Î·lâ‰¥1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct)>1. (78)
Proof. Notice that Assumption 2.1 indicates that the objective function f(x)is strongly convex.
Consider function h1(Î·) =f(xt+Î·dt)âˆ’Î±Î·âˆ‡f(xt)âŠ¤dt. We observe that this function h1(Î·)
is strongly convex and h1(0) = f(xt),hâ€²
1(0)<0. Hence, there is unique Î·r>0such that
h1(Î·r) =f(xt)andÎ·tâ‰¤Î·rif and only if f(xt+Î·tdt)â‰¤f(xt) +Î±Î·tâˆ‡f(xt)âŠ¤dt.
Denote that Â¯xt+1=xt+Î·rdt. We know that f(Â¯xt+1)âˆ’f(xt) =Î±Î·rgâŠ¤
tdt. Since f(Â¯xt+1)âˆ’f(xt) =
Î·rgâŠ¤
tdt+1
2Î·2
rdâŠ¤
tâˆ‡2f(xt+Ï„(Â¯xt+1âˆ’xt))dtforÏ„âˆˆ(0,1), we have that
Î·rgâŠ¤
tdt+1
2Î·2
rdâŠ¤
tâˆ‡2f(xt+Ï„(Â¯xt+1âˆ’xt))dt=Î±Î·rgâŠ¤
tdt,
28Î·r= 2(1 âˆ’Î±)âˆ’gâŠ¤
tdt
dâŠ¤
tâˆ‡2f(xt+Ï„(Â¯xt+1âˆ’xt))dt.
which leads to
Î·r= 2(1 âˆ’Î±)âˆ’gâŠ¤
tdt
dâŠ¤
tâˆ‡2f(xt+Ï„(Â¯xt+1âˆ’xt))dtâ‰¤2(1âˆ’Î±)(1 + Ct)âˆ’gâŠ¤
tdt
dâŠ¤
tâˆ‡2f(xâˆ—)dt
= 2(1 âˆ’Î±)(1 + Ct)âˆ’gâŠ¤
tdt
âˆ¥Ëœdtâˆ¥2= 2(1 âˆ’Î±)(1 + Ct)Ït.
Î·r= 2(1 âˆ’Î±)âˆ’gâŠ¤
tdt
dâŠ¤
tâˆ‡2f(xt+Ï„(Â¯xt+1âˆ’xt))dtâ‰¥2(1âˆ’Î±)
1 +Ctâˆ’gâŠ¤
tdt
dâŠ¤
tâˆ‡2f(xâˆ—)dt=2(1âˆ’Î±)
1 +CtÏt.
where we use the (32) from Lemma A.1 and the fact that f(Â¯xt+1) =f(xt) +Î±Î·rgâŠ¤
tdtâ‰¤f(xt).
Hence, we prove the results in (76).
Similarly, consider function h2(Î·) =âˆ‡f(xt+Î·dt)âŠ¤dt. We observe that this function h2(Î·)is
strictly increasing function for Î·â‰¥0andh2(0) = âˆ‡f(xt)âŠ¤dt< Î²âˆ‡f(xt)âŠ¤dt,h2(Î·exact) =
âˆ‡f(xt+Î·exactdt)âŠ¤dt= 0> Î²âˆ‡f(xt)âŠ¤dtwhere Î·exact := arg minÎ·>0f(xt+Î·dt)is the exact
line search step size satisfying âˆ‡f(xt+Î·exactdt)âŠ¤dt= 0. Hence, there is unique Î·lâˆˆ(0, Î·exact)
such that h2(Î·l) =Î²âˆ‡f(xt)âŠ¤dtandÎ·tâ‰¥Î·lif and only if âˆ‡f(xt+Î·tdt)âŠ¤dtâ‰¥Î²âˆ‡f(xt)âŠ¤dt.
Notice that
f(xt+Î·rdt) =f(xt) +Î±Î·râˆ‡f(xt)âŠ¤dt.
Using mean value theorem, we know there exists Â¯Î·âˆˆ(0, Î·r)such that
f(xt+Î·rdt) =f(xt) +Î·râˆ‡f(xt+ Â¯Î·dt)âŠ¤dt.
The above two equities indicates that
âˆ‡f(xt+ Â¯Î·dt)âŠ¤dt=Î±âˆ‡f(xt)âŠ¤dt.
Recall that
âˆ‡f(xt+Î·ldt)âŠ¤dt=Î²âˆ‡f(xt)âŠ¤dt.
Combing the above two equities, we obtain that
(âˆ‡f(xt+ Â¯Î·dt)âˆ’ âˆ‡f(xt+Î·ldt))âŠ¤dt=âˆ’âˆ‡f(xt)âŠ¤dt(Î²âˆ’Î±).
Using mean value theorem again, we know there exists ËœÎ·âˆˆ(Î·l,Â¯Î·)such that
(âˆ‡f(xt+ Â¯Î·dt)âˆ’ âˆ‡f(xt+Î·ldt))âŠ¤dt= (Â¯Î·âˆ’Î·l)dâŠ¤
tâˆ‡2f(xt+ ËœÎ·dt)dt.
Leveraging the above two equities, we obtain that
Â¯Î·âˆ’Î·l= (Î²âˆ’Î±)âˆ’âˆ‡f(xt)âŠ¤dt
dâŠ¤
tâˆ‡2f(xt+ ËœÎ·dt)dt.
Notice that Â¯Î·â‰¤Î·r, we have that
Î·râˆ’Î·lâ‰¥Â¯Î·âˆ’Î·l= (Î²âˆ’Î±)âˆ’âˆ‡f(xt)âŠ¤dt
dâŠ¤
tâˆ‡2f(xt+ ËœÎ·dt)dt. (79)
Recall the definition of Î·lin (77), we have that
(âˆ‡f(xt+Î·ldt)âˆ’ âˆ‡f(xt))âŠ¤dt=âˆ’(1âˆ’Î²)âˆ‡f(xt)âŠ¤dt.
Notice that there exists Ë†Î·âˆˆ(0, Î·l), such that
(âˆ‡f(xt+Î·ldt)âˆ’ âˆ‡f(xt))âŠ¤dt=Î·ldâŠ¤
tâˆ‡2f(xt+ Ë†Î·dt)dt.
Combing the above two equities, we obtain that
Î·l=âˆ’(1âˆ’Î²)âˆ‡f(xt)âŠ¤dt
dâŠ¤
tâˆ‡2f(xt+ Ë†Î·dt)dt. (80)
29Leveraging (79) and (80), we have that
Î·r
Î·l= 1 +Î·râˆ’Î·l
Î·lâ‰¥1 +(Î²âˆ’Î±)dâŠ¤
tâˆ‡2f(xt+ Ë†Î·dt)dt
(1âˆ’Î²)dâŠ¤
tâˆ‡2f(xt+ ËœÎ·dt)dt.
Recall that Â¯xt+1=xt+Î·rdtand notice that Ë†Î·â‰¤Î·r,ËœÎ·â‰¤Î·r. We have that xt+ Ë†Î·dt=xt+
Ë†Ï„(Â¯xt+1âˆ’xt)andxt+ ËœÎ·dt=xt+ ËœÏ„(Â¯xt+1âˆ’xt)withË†Ï„=Ë†Î·
Î·râˆˆ(0,1)andËœÏ„=ËœÎ·
Î·râˆˆ(0,1). Since
f(Â¯xt+1) =f(xt+Î·rdt) =f(xt) +Î±Î·râˆ‡f(xt)âŠ¤dtâ‰¤f(xt), applying (36) in Lemma A.1, we
prove the conclusion that
Î·r
Î·lâ‰¥1 +(Î²âˆ’Î±)dâŠ¤
tâˆ‡2f(xt+ Ë†Î·dt)dt
(1âˆ’Î²)dâŠ¤
tâˆ‡2f(xt+ ËœÎ·dt)dt
= 1 +(Î²âˆ’Î±)dâŠ¤
tâˆ‡2f(xt+ Ë†Ï„(Â¯xt+1âˆ’xt))dt
(1âˆ’Î²)dâŠ¤
tâˆ‡2f(xt+ ËœÏ„(Â¯xt+1âˆ’xt))dtâ‰¥1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct).
K.2 Bound on the Number of Inner Loops
Proposition K.2. Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Consider the BFGS method with
inexact line search defined in (5)and(6)and we choose the step size Î·taccording to Algorithm 1. At
iteration t, denote Î»tas the number of loops in Algorithm 1 to terminate and return the Î·tsatisfying
the Wolfe conditions (5)and(6). Then Î»tis finite and upper bounded by
Î»tâ‰¤2 + log2
1 +(1âˆ’Î²)(1 + 2 Ct)
Î²âˆ’Î±
+ 2 log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)
+ max {log2Ït,log21
Ït}
.(81)
Proof. At the first iteration, if Î·(0)= 1satisfies the weak Wolfe conditions (5)and(6), the algorithm
terminates and returns the unit step size Î·t= 1. In this case, we have that Î»t= 1.
Suppose that at the first iteration, Î·(0)= 1doesnâ€™t satisfy the sufficient decrease condition (5)but
satisfies the curvature condition (6), we have that Î·(1)
max= +âˆ,Î·(1)
min= 1andÎ·(1)= 2. Assume
that in the Algorithm 1, Î·(i)
max is never set to a finite value and the algorithm never returns. This
means that the condition in line 2 is never satisfied, and as a result, we keep repeating steps in line
12. Thus, Î·(i)= 22iâˆ’1and since the condition in line 2 is never satisfied, we always have that
f(xt+Î·(i)dt)â‰¤f(xt) +Î±Î·(i)âˆ‡f(xt)âŠ¤dt. Notice that limiâ†’âˆÎ·(i)â†’+âˆandâˆ‡f(xt)âŠ¤dt<0.
We obtain that limiâ†’âˆf(xt+Î·(i)dt)â†’ âˆ’âˆ , which is a contradiction since fis strongly convex.
Hence, at some point, either the algorithm finds an admissible step size and returns, or Î·(i)
max must
become finite. Suppose that this happens at iteration K1â‰¥1of the loop in Algorithm 1. Then, we
know that Î·(K1)= 22K1âˆ’1. In the first case that the algorithm finds an admissible step size and
returns Î·K1,Î·K1satisfies the Armijo-Wolfe conditions and therefore Î·K1â‰¤Î·r. Using the upper
bound result in (76) from Lemma K.1, we obtain that Î·(K1)= 22K1âˆ’1â‰¤Î·râ‰¤2(1âˆ’Î±)(1 + Ct)Ït,
which leads to
Î»t=K1â‰¤log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
. (82)
In the second case that Î·(i)
max becomes finite but the algorithm does not terminate, we have that
Î·(K1âˆ’1)satisfies the sufficient condition (5) and Î·(K1âˆ’1)â‰¤Î·r. Similarly, this implies that
K1â‰¤1 + log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
. (83)
Then, we further go through the log bisection process. Notice that for any iteration i > K 1, the
sequence Î·(i)
max is finite and non-increasing and the sequence Î·(i)
minâ‰¥1and non-decreasing. The log
bisection process indicates that
log2Î·(i+1)
max
Î·(i+1)
min=1
2log2Î·(i)
max
Î·(i)
min,âˆ€i > K 1. (84)
30The Algorithm 1 implies that for any i > K 1, we have that
f(xt+Î·(i)
maxdt)> f(xt) +Î±Î·(i)
maxâˆ‡f(xt)âŠ¤dt,âˆ‡f(xt+Î·(i)
mindt)âŠ¤dt< Î²âˆ‡f(xt)âŠ¤dt.
Hence, we know that for any i > K 1,Î·(i)
maxâ‰¥Î·randÎ·(i)
minâ‰¤Î·lwhere Î·r, Î·lare defined in (75),
(77) from Lemma K.1. Therefore, using result (78) from Lemma K.1, we have that for any jâ‰¥1,
log2Î·(K1+j)
max
Î·(K1+j)
minâ‰¥log2Î·r
Î·l>0. (85)
Notice that (84) implies that
log2Î·(K1+j)
max
Î·(K1+j)
min=1
2jâˆ’1log2Î·(K1+1)
max
Î·(K1+1)
min, (86)
which leads to 0 = lim jâ†’+âˆ1
2jâˆ’1log2Î·(K1+1)
max
Î·(K1+1)
min= lim jâ†’+âˆlog2Î·(K1+j)
max
Î·(K1+j)
minâ‰¥log2Î·r
Î·l>0. This is a
contradiction. Hence, Algorithm 1 must terminate after finite number of loops. Now suppose that
Algorithm 1 terminates after K1+ Î“1iterations, (85) and (86) indicate that when Î“1â‰¥1, we have
1
2Î“1âˆ’1log2Î·(K1+1)
max
Î·(K1+1)
min= log2Î·(K1+Î“1)
max
Î·(K1+Î“1)
minâ‰¥log2Î·r
Î·l>log2
1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct)
(87)
where the last inequality holds since (78) in Lemma K.1. Notice that Î·(K1+1)
max = 22K1âˆ’1and
Î·K1+1
min = 22K1âˆ’1âˆ’1. Hence, we obtain that
log2Î·(K1+1)
max
Î·(K1+1)
min= 2K1âˆ’1â‰¤1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
. (88)
Combing (87), (88) and using logxâ‰¥1âˆ’1
x, we have that
Î“1â‰¤1 + log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
âˆ’log2log2
1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct)
â‰¤1 + log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
âˆ’log2log
1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct)
â‰¤1 + log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
âˆ’log2
1âˆ’1
1 +Î²âˆ’Î±
(1âˆ’Î²)(1+2 Ct)
= 1 + log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
+ log2
1 +(1âˆ’Î²)(1 + 2 Ct)
Î²âˆ’Î±
.(89)
Leveraging (83) and (89), we prove that
Î»t=K1+ Î“1
â‰¤2 + 2 log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
+ log2
1 +(1âˆ’Î²)(1 + 2 Ct)
Î²âˆ’Î±
.(90)
Similarly, suppose that at the first iteration, Î·(0)= 1satisfies the sufficient decrease condition (5)
but doesnâ€™t satisfy the curvature condition (6), we have that Î·(1)
max= 1,Î·(1)
min= 0 andÎ·(1)=1
2.
Assume that in the Algorithm 1, Î·(i)
minis never set to a positive value and the algorithm never returns.
This means that the condition in line 2 is always satisfied, and as a result, we keep repeating steps
in line 5. Thus, Î·(i)= (1
2)2iâˆ’1and since the condition in line 2 is always satisfied, we have that
f(xt+Î·(i)dt)> f(xt) +Î±Î·(i)âˆ‡f(xt)âŠ¤dt. Therefore, we know that Î·(i)â‰¥Î·rwhere Î·r>0is
defined in (75) from Lemma K.1. Notice that Î·(i)â‰¥Î·r>0for any iandlimiâ†’âˆÎ·(i)= 0, this
leads to a contradiction.
Hence, at some point either the algorithm returns a step size satisfying the weak Wolfe conditions or
Î·(i)
minmust become positive. Suppose that this happens at iteration K2â‰¥1of the loop in Algorithm 1.
Then, we know that Î·(K2)= (1
2)2K2âˆ’1.
31In the first case that the algorithm finds an admissible step size and returns Î·K2,Î·K2satisfies
the Armijo-Wolfe conditions and therefore Î·K2â‰¤Î·r. Using the upper bound result in (76) from
Lemma K.1, we obtain that Î·(K2)= 22K2âˆ’1â‰¤Î·râ‰¤2(1âˆ’Î±)(1 + Ct)Ït, which leads to
Î»t=K2â‰¤log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)Ït
. (91)
In the second case that Î·(i)
minbecomes positive but the algorithm does not terminate, we have that
Î·(K2âˆ’1)doesnâ€™t satisfy the sufficient condition (5)andÎ·(K2âˆ’1)â‰¥Î·r. Using the lower bound result
in (76) from Lemma K.1, we obtain that Î·(K2âˆ’1)= (1
2)2K2âˆ’1âˆ’1â‰¥Î·râ‰¥2(1âˆ’Î±)
1+CtÏt, which leads to
K2â‰¤1 + log2
1 + log21 +Ct
2(1âˆ’Î±)Ït
. (92)
Then, we further go through the log bisection process. Using the same techniques, we can assume
that Algorithm 1 terminates after K2+ Î“2iterations, where Î“2â‰¥1satisfies that
1
2Î“2âˆ’1log2Î·(K2+1)
max
Î·(K2+1)
min= log2Î·(K2+Î“2)
max
Î·(K2+Î“2)
minâ‰¥log2Î·r
Î·l>log2
1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct)
(93)
where the last inequality holds since (78) in Lemma K.1. Notice that Î·(K2+1)
max = (1
2)2K2âˆ’1âˆ’1and
Î·K2+1
min = (1
2)2K2âˆ’1. Hence, we obtain that
log2Î·(K2+1)
max
Î·(K2+1)
min= 2K2âˆ’1â‰¤1 + log21 +Ct
2(1âˆ’Î±)Ït. (94)
Combing (93), (94) and using logxâ‰¥1âˆ’1
x, we have that
Î“2â‰¤1 + log2
1 + log21 +Ct
2(1âˆ’Î±)Ït
âˆ’log2log2
1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct)
â‰¤1 + log2
1 + log21 +Ct
2(1âˆ’Î±)Ït
âˆ’log2log
1 +Î²âˆ’Î±
(1âˆ’Î²)(1 + 2 Ct)
â‰¤1 + log2
1 + log21 +Ct
2(1âˆ’Î±)Ït
âˆ’log2
1âˆ’1
1 +Î²âˆ’Î±
(1âˆ’Î²)(1+2 Ct)
= 1 + log2
1 + log21 +Ct
2(1âˆ’Î±)Ït
+ log2
1 +(1âˆ’Î²)(1 + 2 Ct)
Î²âˆ’Î±
.(95)
Leveraging (92) and (95), we prove that
Î»t=K2+ Î“2
â‰¤2 + 2 log2
1 + log21 +Ct
2(1âˆ’Î±)Ït
+ log2
1 +(1âˆ’Î²)(1 + 2 Ct)
Î²âˆ’Î±
.(96)
Notice that Î± <1
2and thus1
2(1âˆ’Î±)<2(1âˆ’Î±), combining (82),(90),(91) and(96), we prove the
final conclusion
Î»tâ‰¤2 + log2
1 +(1âˆ’Î²)(1 + 2 Ct)
Î²âˆ’Î±
+ 2 log2
1 + log2 
2(1âˆ’Î±)(1 + Ct)
+ max {log2Ït,log21
Ït}
.
K.3 Proof of Theorem 7.1
Using result from Proposition K.2, we have that
Î›t=1
ttâˆ’1X
i=0Î»iâ‰¤2 +1
ttâˆ’1X
i=0log2
1 +(1âˆ’Î²)(1 + 2 Ci)
Î²âˆ’Î±
+2
ttâˆ’1X
i=0log2
1 + log2 
2(1âˆ’Î±)(1 + Ci)
+ max {log2Ïi,log21
Ïi}
.(97)
32Using Jensenâ€™s inequality, we have that
1
ttâˆ’1X
i=0log2
1 +(1âˆ’Î²)(1 + 2 Ci)
Î²âˆ’Î±
â‰¤log2
1 +1âˆ’Î²
Î²âˆ’Î±+2(1âˆ’Î²)
Î²âˆ’Î±Ptâˆ’1
i=0Ci
t
. (98)
1
ttâˆ’1X
i=0log2
1 + log2 
2(1âˆ’Î±)(1 + Ci)
+ max {log2Ïi,log21
Ïi}
â‰¤log2
1 + log22(1âˆ’Î±) +1
ttâˆ’1X
i=0log2(1 +Ci) +1
ttâˆ’1X
i=0max{log2Ïi,log21
Ïi}
â‰¤log2
1 + log22(1âˆ’Î±) + log2 
1 +Ptâˆ’1
i=0Ci
t) +1
ttâˆ’1X
i=0max{log2Ïi,log21
Ïi}
.(99)
We also have that
1
ttâˆ’1X
i=0max{log2Ïi,log21
Ïi}=1
ttâˆ’1X
i=0,Ïiâ‰¥1log2Ïi+1
ttâˆ’1X
i=0,0â‰¤Ïi<1log21
Ïi
=1
ttâˆ’1X
i=0,Ïiâ‰¥2log2Ïi+1
ttâˆ’1X
i=0,1â‰¤Ïi<2log2Ïi+1
ttâˆ’1X
i=0,1
2<Ïi<1log21
Ïi+1
ttâˆ’1X
i=0,Ïiâ‰¤1
2log21
Ïi
â‰¤2 +1
ttâˆ’1X
i=0,Ïiâ‰¥2log2Ïi+1
ttâˆ’1X
i=0,Ïiâ‰¤1
2log21
Ïi,(100)
where the inequality is due to log2Ïiâ‰¤1forÏi<2andlog21
Ïiâ‰¤1forÏi>1
2. Using the definition
ofÏ‰and (b) in Lemma G.1, we obtain that
1
ttâˆ’1X
i=0,Ïiâ‰¥2log2Ïi=log2e
ttâˆ’1X
i=0,Ïiâ‰¥2logÏi=log2e
ttâˆ’1X
i=0,Ïiâ‰¥2(Ïiâˆ’1âˆ’Ï‰(Ïiâˆ’1))
â‰¤log2e
ttâˆ’1X
i=0,Ïiâ‰¥2(2Ïi
Ïiâˆ’1Ï‰(Ïiâˆ’1)âˆ’Ï‰(Ïiâˆ’1))
=log2e
ttâˆ’1X
i=0,Ïiâ‰¥2Ïi+ 1
Ïiâˆ’1Ï‰(Ïiâˆ’1)â‰¤3 log2e
ttâˆ’1X
i=0,Ïiâ‰¥2Ï‰(Ïiâˆ’1).(101)
Similarly, using (c) in Lemma G.1, we obtain that
1
ttâˆ’1X
i=0,Ïiâ‰¤1
2log21
Ïi=log2e
ttâˆ’1X
i=0,Ïiâ‰¤1
2log1
Ïi=log2e
ttâˆ’1X
i=0,Ïiâ‰¤1
2(Ï‰(Ïiâˆ’1) + 1 âˆ’Ïi)
â‰¤log2e
ttâˆ’1X
i=0,Ïiâ‰¤1
2(Ï‰(Ïiâˆ’1) +1 +Ïi
1âˆ’ÏiÏ‰(Ïiâˆ’1))
=log2e
ttâˆ’1X
i=0,Ïiâ‰¤1
22
1âˆ’ÏiÏ‰(Ïiâˆ’1)â‰¤4 log2e
ttâˆ’1X
i=0,Ïiâ‰¤1
2Ï‰(Ïiâˆ’1).(102)
Combining (100), (101) and (102), we prove that
1
ttâˆ’1X
i=0max{log2Ïi,log21
Ïi} â‰¤2 +1
ttâˆ’1X
i=0,Ïiâ‰¥2log2Ïi+1
ttâˆ’1X
i=0,Ïiâ‰¤1
2log21
Ïi
â‰¤2 +4 log2e
ttâˆ’1X
i=0Ï‰(Ïiâˆ’1)â‰¤2 +6
t
Î¨(ËœB0) + 2tâˆ’1X
i=0Ci
.(103)
33where we use the fact that Ï‰(Ïiâˆ’1)â‰¥0for any iâ‰¥0and the last inequality is due to (46) in
Proposition G.2. Leveraging (97), (98), (99) and (103), we have that
Î›tâ‰¤2 + log2
1 +1âˆ’Î²
Î²âˆ’Î±+2(1âˆ’Î²)
Î²âˆ’Î±Ptâˆ’1
i=0Ci
t
+ 2 log2
3 + log22(1âˆ’Î±) + log2 
1 +Ptâˆ’1
i=0Ci
t) +6
t 
Î¨(ËœB0) + 2tâˆ’1X
i=0Ci
â‰¤2 + log2
1 +1âˆ’Î²
Î²âˆ’Î±+2(1âˆ’Î²)
Î²âˆ’Î±Ptâˆ’1
i=0Ci
t
+ 2 log2
log216(1âˆ’Î±) + log2 
1 +Ptâˆ’1
i=0Ci
t) +6Î¨(ËœB0) + 12Ptâˆ’1
i=0Ci
t
.
We prove the final conclusion using (44) from the proof of Theorem 5.2 in Appendix F.2, i.e.,
tâˆ’1X
i=0Ciâ‰¤C0Î¨(Â¯B0) +3C0Îº
Î±(1âˆ’Î²).
K.4 Corollaries of Theorem 7.1 for B0=LIandB0=ÂµI
Corollary K.3 (B0=LI).Suppose that Assumptions 2.1, 2.2 and 2.3 hold. Let {xt}tâ‰¥0be the
iterates generated by the BFGS method, where the step size satisfies the Armijo-Wolfe conditions in
(5)and(6). For any initial point x0âˆˆRdand the initial Hessian approximation matrix B0=LI,
the average complexity of line search Algorithm 1 Tkis upper bounded by
Î›tâ‰¤2 + log2
1 +1âˆ’Î²
Î²âˆ’Î±+2(1âˆ’Î²)
Î²âˆ’Î±3C0Îº
Î±(1âˆ’Î²)t
+ 2 log2
log216(1âˆ’Î±) + log2 
1 +3C0Îº
Î±(1âˆ’Î²)t
+6dÎº+36C0Îº
Î±(1âˆ’Î²)
t
.
Moreover, when tâ‰¥6dÎº+36
Î±(1âˆ’Î²)C0Îº, we have that
Î›tâ‰¤2 + log2 
1 +3(1âˆ’Î²)
Î²âˆ’Î±
+ 2 log2(5 + log22(1âˆ’Î±)). (104)
Proof. Since B0=LI, we have Â¯B0=1
LB0=IandËœB0=âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2=
Lâˆ‡2f(xâˆ—)âˆ’1. Using results in the proof of Corollary 5.3, we have
Î¨(Â¯B0) = 0 , Î¨(ËœB0)â‰¤dÎº.
Combining these two results with the result in Theorem 7.1, we prove the conclusion.
Corollary K.4 (B0=ÂµI).Let{xt}tâ‰¥0be the iterates generated by the BFGS method with inexact
line search (5),(6)and suppose that Assumptions 2.1, 2.2 and 2.3 hold. For any initial point x0âˆˆRd
and the initial Hessian approximation matrix B0=ÂµI, the average complexity of line search
Algorithm 1 Tkis upper bounded by
Î›tâ‰¤2 + log2
1 +1âˆ’Î²
Î²âˆ’Î±+2(1âˆ’Î²)
Î²âˆ’Î±C0dlogÎº+3C0Îº
Î±(1âˆ’Î²)
t
+ 2 log2
log216(1âˆ’Î±) + log2 
1 +C0dlogÎº+3C0Îº
Î±(1âˆ’Î²)
t
+6(1 + 2 C0)dlogÎº+36C0Îº
Î±(1âˆ’Î²)
t
.
Moreover, when tâ‰¥6(1 + 2 C0)dlogÎº+36C0Îº
Î±(1âˆ’Î²), we have that
Î›tâ‰¤2 + log2 
1 +3(1âˆ’Î²)
Î²âˆ’Î±
+ 2 log2(5 + log22(1âˆ’Î±)). (105)
34Proof. Since B0=ÂµI, we have Â¯B0=1
ÎºB0=IandËœB0=âˆ‡2f(xâˆ—)âˆ’1
2B0âˆ‡2f(xâˆ—)âˆ’1
2=
Âµâˆ‡2f(xâˆ—)âˆ’1. Using results in the proof of Corollary 4.2, we have
Î¨(Â¯B0)â‰¤dlogÎº, Î¨(ËœB0)â‰¤dlogÎº.
Combining these two results with (26) in Theorem 6.4, we prove the conclusion.
35NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: Our claims in the abstract and introduction align with all the theoretical and
experimental results presented in our paper. We assert establishing global convergence of
BFGS with the Armijo-Wolfe conditions, and our theoretical results guarantee this.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have discussed the limitations and drawbacks of this paper in the second
paragraph of Section 9.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
36Answer: [Yes]
Justification: All the theorems, formulas, and proofs in the paper are numbered and cross-
referenced. All assumptions for each presented result are clearly stated or referenced in the
statements of the lemmas, propositions, or theorems. The proofs of all results are presented
in the supplemental material. High-level ideas of the proofs are included in the main text
whenever possible. Some lemmas are borrowed from [38], and this is explicitly mentioned
in both the paper and the supplementary material section D.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
37some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: The paper does not include experiments requiring code.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
38â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: The paper does not include experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
39â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
40â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
41â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
42