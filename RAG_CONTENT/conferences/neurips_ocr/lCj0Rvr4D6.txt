John Ellipsoids via Lazy Updates
David P. Woodruff
Carnegie Mellon University
dwoodruf@cs.cmu.eduTaisuke Yasuda
V oleon Group
yasuda.taisuke1@gmail.com
Abstract
We give a faster algorithm for computing an approximate John ellipsoid around ğ‘›
points in ğ‘‘dimensions. The best known prior algorithms are based on repeatedly
computing the leverage scores of the points and reweighting them by these scores
[CCLY19 ]. We show that this algorithm can be substantially sped up by delaying
the computation of high accuracy leverage scores by using sampling, and then later
computing multiple batches of high accuracy leverage scores via fast rectangular
matrix multiplication. We also give low-space streaming algorithms for John
ellipsoids using similar ideas.
1 Introduction
TheJohn ellipsoid problem is a classic algorithmic problem, which takes as input a set of ğ‘›points
{a1,a2, . . . ,ağ‘›}inğ‘‘dimensions, and asks for the minimum volume enclosing ellipsoid (MVEE)
of these ğ‘›points. If ğ‘ƒis the convex hull of these ğ‘›points, then a famous result of Fritz John
[Joh48 ] states that such an ellipsoid satisfies1
ğ‘‘ğ‘„âŠ†ğ‘ƒâŠ†ğ‘„, and if ğ‘ƒis furthermore symmetric, then
1âˆš
ğ‘‘ğ‘„âŠ†ğ‘ƒâŠ†ğ‘„. Equivalently, we may consider the ğ‘›input points to be constraints of a polytope
{x:âŸ¨ağ‘–,xâŸ©â‰¤1, ğ‘–âˆˆ[ğ‘›]}, in which case the problem is to compute a maximal volume inscribed
ellipsoid (MVIE). These two problems are related by taking polars, which corresponds to inverting
the quadratic form defining the ellipsoid. In this work, we focus on the symmetric case, so that the
polytope ğ‘ƒmay be written as ğ‘ƒ={x:â€–Axâ€–âˆâ‰¤1}, where Adenotes the ğ‘›Ã—ğ‘‘matrix with the
ğ‘›input points ağ‘–in the rows, and our goal is to output an ellipsoid ğ‘„which approximately satisfies
ğ‘„âŠ†ğ‘ƒâŠ†âˆš
ğ‘‘Â·ğ‘„.
The John ellipsoid problem has far-reaching applications in numerous fields of computer science.
In statistics, the John ellipsoid problem is equivalent to the dual of the D-optimal experiment
design problem [ Atw69 ,Sil13 ], in which one seeks weights for selecting a subset of a dataset to
observe in an experiment. Other statistical applications of John ellipsoids include outlier detection
[ST80 ] and pattern recognition [ Ros65 ,Gli98 ]. In the optimization literature, computing John
ellipsoids is a fundamental ingredient for the ellipsoid method for linear programming [ Kha79 ],
cutting plane methods [ TKE88 ], and convex programming [ Sho77 ,Vai96 ]. Other applications in
theoretical computer science include sampling [ Vem05 ,CDWY18 ,GN23 ], bandits [ BCK12 ,HK16 ],
differential privacy [ NTZ13 ], coresets [ TMF20 ,TWZ+22], and randomized numerical linear algebra
[Cla05, DDH+09, WY23, BMV23].
We refer to a survey of Todd [ Tod16 ] for an account on algorithms and applications of John ellipsoids.
1.1 John ellipsoids via iterated leverage scores
Following a long line of work on algorithms for computing John ellipsoids [ Wol70 ,Atw73 ,KT93 ,
NN94 ,Kha96 ,STT78 ,KY05 ,SF04 ,TY07 ,AST08 ,Yu11 ,HFR20 ] based on convex optimization
techniques, the work of [ CCLY19 ] developed a new approach towards computing approximate John
38th Conference on Neural Information Processing Systems (NeurIPS 2024).ellipsoids via a simple fixed point iteration approach. The approach of [ CCLY19 ] starts with an
observation on the optimality condition for the dual problem, given by
minimizeğ‘›âˆ‘ï¸
ğ‘–=1wğ‘–âˆ’log det( AâŠ¤WA)âˆ’ğ‘‘
subject to wğ‘–â‰¥0, ğ‘–âˆˆ[ğ‘›]
where W= diag( w).1The optimality condition requires that the dual weights wsatisfy
wğ‘–=ğœğ‘–(âˆš
WA), (1)
for each ğ‘–âˆˆ[ğ‘›], where ğœğ‘–(A)for a matrix Adenotes the ğ‘–-th leverage score ofA.
Definition 1.1 (Leverage score) .LetAâˆˆRğ‘›Ã—ğ‘‘. Then, for each ğ‘–âˆˆ[ğ‘›], we define the ğ‘–-th leverage
score as
ğœğ‘–(A):=aâŠ¤
ğ‘–(AâŠ¤A)âˆ’ağ‘–= sup
AxÌ¸=0[Ax](ğ‘–)2
â€–Axâ€–2
2.
The optimality condition of (1)can be viewed as a fixed point condition, which suggests the following
iterative algorithm for computing approximate John ellipsoids
w(ğ‘¡)
ğ‘–â†ğœğ‘–(âˆšï¸€
W(ğ‘¡âˆ’1)A) =w(ğ‘¡âˆ’1)
ğ‘–Â·aâŠ¤
ğ‘–(AâŠ¤W(ğ‘¡âˆ’1)A)âˆ’ağ‘– (2)
where W(ğ‘¡):= diag( w(ğ‘¡)). After repeating this update for ğ‘‡=ğ‘‚(ğœ€âˆ’1log(ğ‘›/ğ‘‘))iterations starting
withw(0)=ğ‘‘/ğ‘›Â·1ğ‘›, it can be shown that the ellipsoid ğ‘„defined by the quadratic form Q=
AâŠ¤W(ğ‘‡)A, i.e.ğ‘„={xâˆˆRğ‘‘:xâŠ¤Qxâ‰¤1}, satisfies
1âˆš1 +ğœ€ğ‘„âŠ†ğ‘ƒâŠ†âˆš
ğ‘‘Â·ğ‘„. (3)
Note that the computation of leverage scores can be done in ğ‘‚(ğ‘›ğ‘‘ğœ”âˆ’1)time, where ğœ”â‰¤2.371552 is
the current exponent of fast matrix multiplication [ DWZ23 ,WXXZ24 ]. Thus, this gives an algorithm
running in time ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘ğœ”âˆ’1log(ğ‘›/ğ‘‘))for outputting an ellipsoid with the guarantee of (3).
It is known that input matrices with additional structure admit even faster implementation of this
algorithm. For instance, [ CCLY19 ,SYYZ22 ] give faster algorithms for sparse matrices Afor which
the number of nonzero entries nnz(A)is much less than ğ‘›ğ‘‘. The work of [ SYYZ22 ] shows that this
approach can also be sped up for matrices Awith small treewidth.
1.2 Our results
Our first main result is a substantially faster algorithm for computing John ellipsoids. In the typical
regime where ğ‘›â‰«ğ‘‘, our algorithm runs in ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘) log( ğ‘›/ğ‘‘)time to output an ellipsoid ğ‘„
satisfying (3), and ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘2) log( ğ‘›/ğ‘‘)time to output an ellipsoid ğ‘„which approximates the
maximal volume up to a (1 +ğœ€)factor. We will discuss our techniques for this result in Sections
1.2.1 and 1.2.2.
Table 1: Running time of John ellipsoid approximation for dense ğ‘›Ã—ğ‘‘matrices, for ğ‘›â‰«ğ‘‘â‰«
poly( ğœ€âˆ’1logğ‘›). There is other prior work on sparse matrices and matrices with low treewidth
[CCLY19, SYYZ22].
Running time Guarantee
[KY05, TY07] ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘ğœ”) volume approximation
[CCLY19] ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘ğœ”âˆ’1) log( ğ‘›/ğ‘‘) (3)
[CCLY19, SYYZ22] ğ‘‚(ğœ€âˆ’2ğ‘›ğ‘‘)(log ğ‘›) log( ğ‘›/ğ‘‘)(3)
Theorem 1.6 ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘) log( ğ‘›/ğ‘‘) (3)
1For a weight vector wâˆˆRğ‘›, we will often write the corresponding ğ‘›Ã—ğ‘›diagonal matrix diag( w)as the
capitalized version W.
21.2.1 Linear time leverage scores via fast matrix multiplication
We start by showing how to approximate leverage scores up to (1 +ğœ€)factors in Ëœğ‘‚(ğ‘›ğ‘‘)time, which
had not been observed before to the best of our knowledge. Note that if we compute exact leverage
scores using fast matrix multiplication, then this takes time ğ‘‚(ğ‘›ğ‘‘ğœ”âˆ’1). Alternatively, sketching-based
algorithms for approximate leverage scores are known, which gives the following running time for
sparse matrices Awith nnz(A)nonzero entries.
Theorem 1.2 ([SS11 ,DMMW12 ,CW13 ]).There is an algorithm which, with probability at least
1âˆ’ğ›¿, outputs ğœâ€²
ğ‘–forğ‘–âˆˆ[ğ‘›]such that
ğœâ€²
ğ‘–= (1Â±ğœ€)ğœğ‘–(A)
and runs in time ğ‘‚(ğœ€âˆ’2nnz(A) log( ğ‘›/ğ›¿)) + poly( ğ‘‘ğœ€âˆ’1log(ğ‘›/ğ›¿)).
If the goal is to compute (1 +ğœ€)-approximate leverage scores for a dense ğ‘›Ã—ğ‘‘matrix, then we are
not aware of a previous result which does this in a nearly linear Ëœğ‘‚(ğ‘›ğ‘‘)time, which we now show:
Theorem 1.3. There is an algorithm which, with probability at least 1âˆ’ğ›¿, outputs ğœâ€²
ğ‘–forğ‘–âˆˆ[ğ‘›]
such that
ğœâ€²
ğ‘–= (1Â±ğœ€)ğœğ‘–(A)
in time ğ‘‚(ğ‘›ğ‘‘) poly log( ğœ€âˆ’1log(ğ‘›/ğ›¿)) +ğ‘‚(ğ‘›) poly( ğœ€âˆ’1log(ğ‘›/ğ›¿)).
Our improvement comes from improving the running time analysis of a sketching-based algorithm of
[DMMW12 ] by using fast rectangular matrix multiplication. We will need the following result on
fast matrix multiplication:
Theorem 1.4 ([Cop82 ,Wil11 ,Wil24 ]).There is a constant ğ›¼â‰¥0.1and an algorithm for multiplying
ağ‘šÃ—ğ‘šand a ğ‘šÃ—ğ‘šğ›¼matrix in ğ‘‚(ğ‘š2poly log ğ‘š)time, under the assumption that field operations
can be carried out in ğ‘‚(1)time.
By applying the above result in blocks, we get the following version of this result for rectangular
matrix multiplication.
Corollary 1.5. There is a constant ğ›¼â‰¥0.1and an algorithm for multiplying a ğ‘›Ã—ğ‘‘forğ‘›â‰¥ğ‘‘and
ağ‘‘Ã—ğ‘¡matrix in ğ‘‚(ğ‘›ğ‘‘+ğ‘›ğ‘¡1/ğ›¼+1) poly log ğ‘¡time, under the assumption that field operations can
be carried out in ğ‘‚(1)time.
Proof. Letğ‘š=ğ‘¡1/ğ›¼. Ifğ‘‘â‰¤ğ‘š, then matrix multiplication takes only ğ‘‚(ğ‘›ğ‘‘ğ‘¡) =ğ‘‚(ğ‘›ğ‘¡1/ğ›¼+1)time,
so assume that ğ‘‘â‰¥ğ‘š. We partition the first matrix into an ğ‘‚(ğ‘›/ğ‘š)Ã—ğ‘‚(ğ‘‘/ğ‘š)block matrix with
blocks of size ğ‘šÃ—ğ‘šand the second into an ğ‘‚(ğ‘‘/ğ‘š)Ã—1block matrix with blocks of size ğ‘šÃ—ğ‘šğ›¼.
By Theorem 1.4, each block matrix multiplication requires ğ‘‚(ğ‘š2poly log ğ‘š)time, and we have
ğ‘‚(ğ‘›ğ‘‘/ğ‘š2)of these to do, which gives the desired running time.
That is, the above result shows that when multiplying an ğ‘›Ã—ğ‘‘matrix Awith a ğ‘‘Ã—ğ‘¡matrix for a
much smaller ğ‘¡, then this multiplication can be done in roughly ğ‘‚(ğ‘›ğ‘‘) poly log ğ‘¡time. The work of
[DMMW12 ] shows that the leverage scores of Acan be written as the row norms of AR for ağ‘‘Ã—ğ‘¡
matrix with ğ‘¡=ğ‘‚(ğœ€âˆ’2log(ğ‘›/ğ›¿)), and thus this gives us the result of Theorem 1.3.
1.2.2 John ellipsoids via lazy updates
By using Theorem 1.3, we already obtain a John ellipsoid algorithm which runs in time
ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘) log( ğ‘›/ğ‘‘) poly log( ğœ€âˆ’1logğ‘›)time, which substantially improves upon prior algorithms
for dense input matrices A. We now show how to obtain further improvements by using the idea of
lazy updates . At the heart of our idea is to only compute the quadratic forms for the John ellipsoids for
most iterations, and defer the computation of the weights until we have computed roughly ğ‘‚(logğ‘›)
iterations. At the end of this group of iterations, we can then compute the John ellipsoid weights
via fast matrix multiplication as used in Theorem 1.3, which allows us to remove the suboptimal
poly log log ğ‘›terms in the dominating term of the running time.
Theorem 1.6. Given AâˆˆRğ‘›Ã—ğ‘‘, letğ‘ƒbe the polytope defined by ğ‘ƒ={xâˆˆRğ‘‘:â€–Axâ€–âˆâ‰¤1}.
Forğœ€âˆˆ(0,1), there is an algorithm, Algorithm 3, that runs in time
ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘)(log( ğ‘›/ğ‘‘) + poly log( ğœ€âˆ’1logğ‘›)) +ğ‘‚(ğ‘›) poly( ğœ€âˆ’1logğ‘›) +ğ‘‚(ğ‘›0.1)ğ‘‘ğœ”+1ğœ€âˆ’3(logğ‘›)2
and returns an ellipsoid ğ‘„such that1âˆš1+ğœ€Â·ğ‘„âŠ†ğ‘ƒâŠ†âˆš
ğ‘‘Â·ğ‘„with probability at least 1âˆ’1/poly( ğ‘›).
3The full proof of this result is given in Section 2.
LetQ(ğ‘¡)=AâŠ¤W(ğ‘¡)A, where the weights w(ğ‘¡)are defined as (2). Note that with this notation, the
update rule for the iterative algorithm of [CCLY19] can be written as
w(ğ‘¡)
ğ‘–=w(ğ‘¡âˆ’1)
ğ‘–Â·aâŠ¤
ğ‘–(Q(ğ‘¡âˆ’1))âˆ’ağ‘–. (4)
Thus, given high-accuracy spectral estimates to the quadratics Q(ğ‘¡), we can recover the weights w(ğ‘¡)
ğ‘–
to high accuracy in ğ‘‚(ğ‘‘ğœ”)time per iteration by evaluating the quadratic forms aâŠ¤
ğ‘–(Q(ğ‘¡))âˆ’ağ‘–and then
multiplying them together. This approach is useful for fast algorithms if we only need to to do this
for a small number of indices ğ‘–âˆˆ[ğ‘›]. This is indeed the case if we only need these weights for a row
sample ofâˆš
W(ğ‘¡)A, which is sufficient for computing a spectral approximation to the next quadratic
formQ(ğ‘¡). Furthermore, we only need low-accuracy leverage scores (up to a factor of, say, ğ‘›0.1) to
obtain a good row sample, which can be done quickly for all ğ‘›rows [ LMP13 ,CLM+15]. Thus, by
repeatedly sampling rows ofâˆš
W(ğ‘¡)A, computing high-accuracy weights on the sampled rows, and
then building an approximate quadratic, we can iteratively compute high-accuracy approximations
ËœQ(ğ‘¡)to the quadratics Q(ğ‘¡). More formally, our algorithm takes the following steps:
â€¢We first compute low-accuracy leverage scores ofâˆš
W(ğ‘¡âˆ’1)A, which can be done in ğ‘‚(ğ‘›ğ‘‘)
time. This gives us the weights w(ğ‘¡)to low accuracy, say u(ğ‘¡), for all ğ‘›rows.
â€¢We use the low-accuracy weights u(ğ‘¡)to obtain a weighted subset of rows ofâˆš
U(ğ‘¡)Awhich
spectrally approximates Q(ğ‘¡). Note, however, that we do not yet have the sampled rows ofâˆš
W(ğ‘¡)Ato high accuracy, since we do not know the weights w(ğ‘¡)to high accuracy.
â€¢If we only need the weights w(ğ‘¡)for a small number of sampled rows ğ‘†âŠ†[ğ‘›], then we
can explicitly compute these using (4), since we inductively have access to high-accuracy
quadratics ËœQ(ğ‘¡â€²)forğ‘¡â€²= 0,1,2, . . . , ğ‘¡âˆ’1. These can then be used to build ËœQ(ğ‘¡).
While this algorithm allows us to quickly compute high-accuracy approximate quadratics ËœQ(ğ‘¡), this
algorithm cannot be run for too many iterations, as the error in the low-accuracy leverage scores
u(ğ‘¡)grows to poly( ğ‘›)factors in ğ‘‚(logğ‘›)rounds. This is a problem, as this error factor directly
influences the number of leverage score samples needed to approximate ËœQ(ğ‘¡). We will now use the
fast matrix multiplication trick from the previous Section 1.2.1 to fix this problem. Indeed, after
ğ‘‚(logğ‘›)iterations, we will now have approximate quadratics ËœQ(1),ËœQ(2), . . . , ËœQ(ğ‘¡)forğ‘¡=ğ‘‚(logğ‘›).
Now we just need to compute the ğ‘›John ellipsoid weights which are given by
v(ğ‘¡)
ğ‘–=ğ‘¡âˆï¸
ğ‘¡â€²=1â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡â€²âˆ’1))âˆ’1/2â€–2
2.
To approximate this quickly, we can approximate each term â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡â€²âˆ’1))âˆ’1/2â€–2
2by
â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡â€²âˆ’1))âˆ’1/2G(ğ‘¡â€²)â€–2
2for a random Gaussian matrix G(ğ‘¡â€²), by the Johnsonâ€“Lindenstrauss
lemma [ JL84 ]. Here, the number of columns of the Gaussian matrix can be taken to be
poly( ğœ€âˆ’1logğ‘›), so now all we need to compute is the matrix product
AÂ·[(ËœQ(0))âˆ’1/2G(0),(ËœQ(1))âˆ’1/2G(1), . . . , (ËœQ(ğ‘¡))âˆ’1/2G(ğ‘¡)]
which is the product of a ğ‘›Ã—ğ‘‘matrix and a ğ‘‘Ã—ğ‘šmatrix for ğ‘š= poly( ğœ€âˆ’1logğ‘›). By Theorem
1.4, this can be computed in ğ‘‚(ğ‘›ğ‘‘poly log ğ‘š)time. However, this resetting procedure is only run
ğ‘‚(ğœ€âˆ’1)times across the ğ‘‡=ğ‘‚(ğœ€âˆ’1logğ‘›)iterations, so the running time contribution from the
resetting is just
ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘) poly log( ğœ€âˆ’1logğ‘›) +ğ‘‚(ğ‘›) poly( ğœ€âˆ’1logğ‘›).
Overall, the total running time of our algorithm is
ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘)(log( ğ‘›/ğ‘‘) + poly log( ğœ€âˆ’1logğ‘›)) +ğ‘‚(ğ‘›) poly( ğœ€âˆ’1logğ‘›).
Remark 1.7. In general, our techniques can be be viewed as a way of exploiting the increased
efficiency of matrix multiplication when performed on a larger instance by delaying large matrix
multiplication operations, so that the running time is ğ‘‚(ğœ€âˆ’1ğ‘›ğ‘‘log(ğ‘›/ğ‘‘)) +ğ‘‚(ğœ€âˆ’1)ğ‘‡ğ‘Ÿwhere ğ‘‡ğ‘Ÿ
is the time that it takes to multiply Aby ağ‘‘Ã—ğ‘Ÿmatrix for ğ‘Ÿ= poly( ğœ€âˆ’1logğ‘›). While we have
instantiated this general theme by obtaining faster running times via fast matrix multiplication,
one can expect similar improvements by other ways of exploiting the economies of scale of matrix
multiplication, such as parallelization. For instance, we recover the same running time if we can
multiply ğ‘Ÿvectors in parallel so that ğ‘‡ğ‘Ÿ=ğ‘‚(ğ‘›ğ‘‘).
41.2.3 Streaming algorithms
The problem of computing John ellipsoids is also well-studied in the streaming model , where the
input points ağ‘–arrive one at a time in a stream [ MSS10 ,AS15 ,WY22 ,MMO22 ,MMO23 ]. The
streaming model is often considered when the number of points ğ‘›is so large that we cannot fit all
of the points in memory at once, and the focus is primarily on designing algorithms with low space
complexity. Our second result of this work is that approaches similar to the one we take to prove
Theorem 1.6 in fact also give a low-space implementation of the iterative John ellipsoid algorithm of
[CCLY19].
Theorem 1.8 (Streaming algorithms) .Given AâˆˆRğ‘›Ã—ğ‘‘, letğ‘ƒbe the polytope defined by ğ‘ƒ=
{xâˆˆRğ‘‘:â€–Axâ€–âˆâ‰¤1}. Furthermore, suppose that Ais presented in a stream where the rows
ağ‘–âˆˆRğ‘‘arrive one by one in a stream. For ğœ€âˆˆ(0,1), there is an algorithm, Algorithm 1, that makes
ğ‘‡=ğ‘‚(ğœ€âˆ’1log(ğ‘›/ğ‘‘))passes over the stream, takes ğ‘‚(ğ‘‘2ğ‘‡)time to update after each new row, and
returns an ellipsoid ğ‘„such that1âˆš1+ğœ€Â·ğ‘„âŠ†ğ‘ƒâŠ†âˆš
ğ‘‘Â·ğ‘„. Furthermore, the algorithm uses at most
ğ‘‚(ğ‘‘2ğ‘‡)words of space.
In Section 1.2.2, we showed that by storing only the quadratics ËœQ(ğ‘¡)and only computing the weightsâˆï¸€ğ‘¡
ğ‘¡â€²=1ağ‘–(ËœQ(ğ‘¡â€²âˆ’1))âˆ’ağ‘–as needed, we could design fast algorithms for John ellipsoids. In fact, this
idea is also useful in the streaming setting, since storing all of the weights w(ğ‘¡)
ğ‘–requires ğ‘‚(ğ‘›)
space per iteration, whereas storing the quadratics ËœQ(ğ‘¡)requires only ğ‘‚(ğ‘‘2)space per iteration.
Furthermore, in the streaming setting, we may optimize the update running time by instead storing
the pseudo-inverse of the quadratics (ËœQ(ğ‘¡))âˆ’and then updating them by using the Sherman-Morrison
low rank update formula.2This gives the result of Theorem 1.8.
Algorithm 1 Streaming John ellipsoids via lazy updates
1:function STREAMING JOHN ELLIPSOID (input matrix A)
2: forğ‘¡= 0toğ‘‡do
3: LetQ(ğ‘¡)= 0
4: forğ‘–= 1toğ‘›do
5: ifğ‘¡= 0then
6: LetQ(ğ‘¡)â†Q(ğ‘¡)+ağ‘–aâŠ¤
ğ‘–
7: else
8: Letw(ğ‘¡)
ğ‘–=âˆï¸€ğ‘¡
ğ‘¡â€²=1aâŠ¤
ğ‘–(Q(ğ‘¡â€²âˆ’1))âˆ’ağ‘–
9: LetQ(ğ‘¡)â†Q(ğ‘¡)+w(ğ‘¡)
ğ‘–ağ‘–aâŠ¤
ğ‘–
10: return1
ğ‘‡+1âˆ‘ï¸€ğ‘‡
ğ‘¡=0Q(ğ‘¡)
1.3 Notation
Throughout this paper, Awill denote an ğ‘›Ã—ğ‘‘matrix whose ğ‘›rows are given by vectors ağ‘–âˆˆRğ‘‘. For
positive numbers ğ‘, ğ‘ > 0, we write ğ‘= (1Â±ğœ€)ğ‘to mean that (1âˆ’ğœ€)ğ‘â‰¤ğ‘â‰¤(1+ğœ€)ğ‘. For symmetric
positive semidefinite matrices Q,R, we write Q= (1Â±ğœ€)Rto mean that (1âˆ’ğœ€)Râª¯Qâª¯(1+ğœ€)R,
whereâª¯denotes the LÃ¶wner order on PSD matrices.
2 Fast algorithms
2.1 Approximating the quadratics
We will analyze the following algorithm, Algorithm 2, for approximating the quadratics Q(ğ‘¡)of the
iterative John ellipsoid algorithm.
Fix a row ğ‘–. Note then that for each iteration ğ‘¡,â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2Gâ€–2
2is distributed as an indepen-
dentğœ’2variable with ğ‘˜degrees of freedom, scaled by â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2â€–2
2, sayğ‘‹ğ‘¡. Note then that
2We note that storing the pseudo-inverse may increase the space complexity by polynomial factors in the bit
complexity model.
5Algorithm 2 Approximating the quadratics
1:function APPROX QUADRATIC (input matrix A, initial weights Ëœw(0), number of rounds ğ‘‡)
2: LetËœQ(0)=AâŠ¤ËœW(0)AforËœW(0)= diag( Ëœw(0)).
3: forğ‘¡= 1toğ‘‡do
4: Compute A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2Gfor ağ‘‘Ã—ğ‘˜Gaussian matrix G.
5: Letu(ğ‘¡)
ğ‘–=u(ğ‘¡âˆ’1)
ğ‘–Â·â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2Gâ€–2
2for each ğ‘–âˆˆ[ğ‘›].
6: LetS(ğ‘¡)be a(1 +ğœ€)-approximate leverage score sample ofâˆš
U(ğ‘¡)A(Thm. 2.3).
7: For rows ğ‘–sampled by S(ğ‘¡), setv(ğ‘¡)
ğ‘–=âˆï¸€ğ‘¡
ğ‘¡â€²=1aâŠ¤
ğ‘–(ËœQ(ğ‘¡â€²âˆ’1))âˆ’ağ‘–.
8: LetËœQ(ğ‘¡)= (S(ğ‘¡)âˆš
V(ğ‘¡)A)âŠ¤S(ğ‘¡)âˆš
V(ğ‘¡)A.
9: return{ËœQ(ğ‘¡)}ğ‘‡
ğ‘¡=0
afterğ‘‡iterations,
u(ğ‘‡)
ğ‘–=ğ‘‡âˆï¸
ğ‘¡=1â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2Gâ€–2
2,
which is distributed as
u(ğ‘‡)
ğ‘–âˆ¼ğ‘‡âˆï¸
ğ‘¡=1â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2â€–2
2Â·ğ‘‹ğ‘¡=ğ‘‡âˆï¸
ğ‘¡=1â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2â€–2
2Â·ğ‘‡âˆï¸
ğ‘¡=1ğ‘‹ğ‘¡ (5)
for i.i.d. ğœ’2variables ğ‘‹ğ‘¡withğ‘˜degrees of freedom. We will now bound each of the terms in this
product.
2.1.1 Bounds on products of ğœ’2variables
We need the following bound on products of ğœ’2variables, which generalizes [ SSK17 , Proposition 1].
Lemma 2.1. Letğ‘‹1, ğ‘‹2, . . . , ğ‘‹ ğ‘¡beğ‘¡i.i.d.ğœ’2variables with ğ‘˜degrees of freedom. Then,
Pr{ï¸ƒğ‘¡âˆï¸
ğ‘–=1ğ‘‹ğ‘–â‰¤1
ğ‘…}ï¸ƒ
â‰¤ inf
ğ‘ âˆˆ(0,ğ‘˜/2)ğ¶ğ‘¡
âˆ’ğ‘ ,ğ‘˜ğ‘…âˆ’ğ‘ 
and
Pr{ï¸ƒğ‘¡âˆï¸
ğ‘–=1ğ‘‹ğ‘–â‰¥ğ‘…}ï¸ƒ
â‰¤inf
ğ‘ >âˆ’ğ‘˜/2ğ¶ğ‘¡
ğ‘ ,ğ‘˜ğ‘…âˆ’ğ‘ 
where
ğ¶ğ‘ ,ğ‘˜=2ğ‘ Î“(ğ‘ +ğ‘˜/2)
Î“(ğ‘˜/2)>0
Proof. Forğ‘  >âˆ’ğ‘˜/2, the moment generating function of logğ‘‹ğ‘–is given by
Eğ‘’ğ‘ logğ‘‹ğ‘–=Eğ‘‹ğ‘ 
ğ‘–=1
2ğ‘˜/2Î“(ğ‘˜/2)âˆ«ï¸âˆ
0ğ‘¥ğ‘ ğ‘¥ğ‘˜/2âˆ’1ğ‘’âˆ’ğ‘¥/2ğ‘‘ğ‘¥=2ğ‘ Î“(ğ‘ +ğ‘˜/2)
Î“(ğ‘˜/2)=ğ¶ğ‘ ,ğ‘˜.
Then by Chernoff bounds,
Pr{ï¸ƒğ‘¡âˆï¸
ğ‘–=1ğ‘‹ğ‘–â‰¤1
ğ‘…}ï¸ƒ
=Pr{ï¸ƒ
exp(ï¸ƒğ‘¡âˆ‘ï¸
ğ‘–=1âˆ’ğ‘ logğ‘‹ğ‘–)ï¸ƒ
â‰¥ğ‘…ğ‘ }ï¸ƒ
â‰¤E[ğ‘’âˆ’ğ‘ logğ‘‹ğ‘–]ğ‘¡ğ‘…âˆ’ğ‘ =ğ¶ğ‘¡
âˆ’ğ‘ ,ğ‘˜ğ‘…âˆ’ğ‘ 
and
Pr{ï¸ƒğ‘¡âˆï¸
ğ‘–=1ğ‘‹ğ‘–â‰¥ğ‘…}ï¸ƒ
=Pr{ï¸ƒ
exp(ï¸ƒğ‘¡âˆ‘ï¸
ğ‘–=1ğ‘ logğ‘‹ğ‘–)ï¸ƒ
â‰¥ğ‘…ğ‘ }ï¸ƒ
â‰¤E[ğ‘’ğ‘ logğ‘‹ğ‘–]ğ‘¡ğ‘…âˆ’ğ‘ =ğ¶ğ‘¡
ğ‘ ,ğ‘˜ğ‘…âˆ’ğ‘ 
6Using the above result, we can show that if the ğœ’2variables have ğ‘˜=ğ‘‚(1/ğœƒ)degrees of freedom and
the number of rounds ğ‘‡isğ‘logğ‘›for a small enough constant ğ‘, then the product of the ğœ’2variablesâˆï¸€ğ‘‡
ğ‘¡=1ğ‘‹ğ‘¡will be within a ğ‘›ğœƒfactor for some small constant ğœƒ >0.
Lemma 2.2. Fix a small constant ğœƒ >0and let ğ‘˜=ğ‘‚(1/ğœƒ). Letğ‘‡=ğ‘logğ‘›for a sufficiently small
constant ğ‘ >0. Letğ‘‹1, ğ‘‹2, . . . , ğ‘‹ ğ‘‡beğ‘¡i.i.d.ğœ’2variables with ğ‘˜degrees of freedom. Then,
Pr{ï¸ƒ
1
ğ‘›ğœƒâ‰¤ğ‘¡âˆï¸
ğ‘–=1ğ‘‹ğ‘–â‰¤ğ‘›ğœƒ}ï¸ƒ
â‰¥1âˆ’1
poly( ğ‘›).
Proof. Setğ‘ =ğ‘˜/2. Then, ğ¶âˆ’ğ‘ ,ğ‘˜andğ¶ğ‘ ,ğ‘˜are absolute constants. Now let ğ‘to be small enough
(depending on ğ‘ andğ‘˜) such that for ğ‘‡=ğ‘logğ‘›,ğ¶ğ‘‡
âˆ’ğ‘ ,ğ‘˜, ğ¶ğ‘‡
ğ‘ ,ğ‘˜â‰¤ğ‘›. Furthermore, set ğ‘…=ğ‘›ğœƒ. Then,
by Lemma 2.1, we have that both Pr{âˆï¸€ğ‘¡
ğ‘–=1ğ‘‹ğ‘–â‰¤1
ğ‘…}andPr{âˆï¸€ğ‘¡
ğ‘–=1ğ‘‹ğ‘–â‰¥ğ‘…}are bounded by
ğ‘›Â·ğ‘…âˆ’ğ‘ =ğ‘›Â·ğ‘›ğœƒÂ·ğ‘ = 1/poly( ğ‘›), as desired.
It follows that with probability at least 1âˆ’1/poly( ğ‘›), the products of ğœ’2variables appearing in (5)
are bounded by ğ‘›ğœƒfor every row ğ‘–âˆˆ[ğ‘›]and for every iteration ğ‘¡âˆˆ[ğ‘‡]. We will condition on this
event in the following discussion.
2.1.2 Bounds on the quadratic ËœQ(ğ‘¡)
In the previous section, we have established that the products of ğœ’2variables in (5)are bounded by ğ‘›ğœƒ.
We will now use this fact to show that the quadratics ËœQ(ğ‘¡âˆ’1)in Algorithm 2 are good approximate
John ellipsoid quadratics. We will use the following leverage score sampling theorem for this.
Theorem 2.3 ([DMM06 ,SS11 ]).LetAâˆˆRğ‘›Ã—ğ‘‘. Let ğœâ€²
ğ‘–â‰¥ğœğ‘–(A)and let ğ‘ğ‘–= min{1,ğœâ€²
ğ‘–/ğ›¼}
forğ›¼= Î˜( ğœ€2)/log(ğ‘‘/ğ›¿). IfSis a diagonal matrix with Sğ‘–,ğ‘–= 1/âˆšğ‘ğ‘–with probability ğ‘ğ‘–and0
otherwise for ğ‘–âˆˆ[ğ‘›], then with probability at least 1âˆ’ğ›¿,
for all xâˆˆRğ‘‘,â€–SAxâ€–2
2= (1Â±ğœ€)â€–Axâ€–2
2.
This theorem, combined with the bounds on ğœ’2products, gives the following guarantee for the
approximate quadratics ËœQ(ğ‘¡).
Lemma 2.4. Fix a small constant ğœƒ >0and let ğ‘˜=ğ‘‚(1/ğœƒ). Suppose that the leverage score
sample in Line 6 oversamples by a factor of ğ‘‚(ğ‘›2ğœƒ), that is, uses leverage score estimates ğœâ€²
ğ‘–such
thatğœâ€²
ğ‘–â‰¥ğ‘‚(ğ‘›2ğœƒ)ğœğ‘–(âˆš
U(ğ‘¡)A). Then, with probability at least 1âˆ’1/poly( ğ‘›), we have for every
ğ‘¡âˆˆ[ğ‘‡]that
ËœQ(ğ‘¡)= (1Â±ğœ€)AâŠ¤V(ğ‘¡)A (6)
where v(ğ‘¡)
ğ‘–=âˆï¸€ğ‘¡âˆ’1
ğ‘¡â€²=1aâŠ¤
ğ‘–(ËœQ(ğ‘¡â€²))âˆ’ağ‘–forğ‘–âˆˆ[ğ‘›]. Furthermore, ËœQ(ğ‘¡)can be computed in
ğ‘‚(ğ‘›2ğœƒ)ğ‘‡ğœ€âˆ’2ğ‘‘ğœ”+1logğ‘›time in each iteration.
Proof. We will first condition on the success of the event of Lemma 2.2, so that the ğœ’2products in
(5)are bounded by ğ‘›ğœƒfactors for every row ğ‘–âˆˆ[ğ‘›]and every iteration ğ‘¡âˆˆ[ğ‘¡]. We will also condition
on the success of the leverage score sampling for all ğ‘‡iterations.
Note that by (5)and the bound the ğœ’2products, u(ğ‘¡)
ğ‘–is within a ğ‘‚(ğ‘›2ğœƒ)factor of v(ğ‘¡)
ğ‘–, and thus S(ğ‘¡)
is a correct leverage score sample forâˆš
V(ğ‘¡)A. We thus have that
ËœQ(ğ‘¡)= (S(ğ‘¡)âˆšï¸€
V(ğ‘¡)A)âŠ¤S(ğ‘¡)âˆšï¸€
V(ğ‘¡)A= (1Â±ğœ€)AâŠ¤V(ğ‘¡)A.
For the running time, note that S(ğ‘¡)samples at most ğ‘‚(ğœ€âˆ’2ğ‘›2ğœƒğ‘‘logğ‘›)rows in each iteration, and
each sampled row ğ‘–requires ğ‘‚(ğ‘‘ğœ”ğ‘‡)to compute v(ğ‘¡)
ğ‘–. This gives the running time claim.
7Algorithm 3 John ellipsoids via lazy updates
1:function JOHN ELLIPSOID (input matrix A)
2: Letğµ=ğ‘‚(ğ‘âˆ’1ğœ€âˆ’1)andğ‘‡=ğ‘‚(ğ‘log(ğ‘›/ğ‘‘))for a sufficiently small constant ğ‘.
3: LetËœw(0)
ğ‘–=ğ‘‘/ğ‘›forğ‘–âˆˆ[ğ‘›].
4: forğ‘= 1toğµdo
5: Let{ËœQ(ğ‘¡)}ğ‘‡
ğ‘¡=0be given by A PPROX QUADRATIC (A,Ëœw(0))(Algorithm 2).
6: LetG(ğ‘¡âˆ’1)forğ‘¡âˆˆ[ğ‘‡]be a random ğ‘‘Ã—ğ‘šGaussian for ğ‘š=ğ‘‚(ğœ€âˆ’2(ğµğ‘‡)2logğ‘›).
7: Compute AÂ·[(ËœQ(0))âˆ’1/2G(0),(ËœQ(1))âˆ’1/2G(1), . . . , (ËœQ(ğ‘‡))âˆ’1/2G(ğ‘‡)].
8: LetËœw(ğ‘,ğ‘¡)
ğ‘–=âˆï¸€ğ‘¡
ğ‘¡â€²=11
ğ‘šâ€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡â€²âˆ’1))âˆ’1/2G(ğ‘¡â€²âˆ’1)â€–2
2for each ğ‘–âˆˆ[ğ‘›].
9: LetËœw(0)=Ëœw(ğ‘,ğ‘‡).
10: LetËœw=1
ğµğ‘‡âˆ‘ï¸€
ğ‘,ğ‘¡Ëœw(ğ‘,ğ‘¡).
11: return AâŠ¤ËœWA
2.2 Full algorithm
We now combine the subroutine for approximating quadratics from Section 2.1 with a resetting
procedure using fast matrix multiplication to obtain our full algorithm for quickly computing John
ellipsoids. The full algorithm is presented in Algorithm 3.
We will need the following theorem, which summarizes results of [ CCLY19 ] on guarantees of the
fixed point iteration algorithm (2) under approximate leverage score computations.
Theorem 2.5 (Lemma 2.3 and Lemma C.4 of [ CCLY19 ]).LetAâˆˆRğ‘›Ã—ğ‘‘and let ğ‘ƒ={x:
â€–Axâ€–âˆâ‰¤1}. Letğ‘‡=ğ‘‚(ğœ€âˆ’1log(ğ‘›/ğ‘‘)). Suppose that
w(ğ‘¡)
ğ‘–= (1Â±ğœ€)ğœğ‘–(âˆšï¸€
W(ğ‘¡âˆ’1)A)
for all ğ‘¡âˆˆ[ğ‘‡]andğ‘–âˆˆ[ğ‘›]. Then, if ğ‘„is the ellipsoid given by the quadratic form AâŠ¤W(ğ‘‡)A, then
1âˆš1+ğœ€Â·ğ‘„âŠ†ğ‘ƒâŠ†âˆš
ğ‘‘Â·ğ‘„.
We also use the Johnsonâ€“Lindenstrauss lemma, which is a standard tool for randomized numerical
linear algebra, especially in the context of approximating leverage scores [ SS11 ,DMMW12 ,LMP13 ,
CLM+15].
Theorem 2.6 ([JL84 ,DG03 ]).Letğ‘š=ğ‘‚(ğœ€âˆ’2log(ğ‘›/ğ›¿))and let Gbe a random ğ‘šÃ—ğ‘‘Gaussian
matrix. Then, for any ğ‘›points a1,a2, . . . ,ağ‘›âˆˆRğ‘‘inğ‘‘dimensions, we have with probability at
least1âˆ’ğ›¿that
1
ğ‘šâ€–Gağ‘–â€–2
2= (1Â±ğœ€)â€–ağ‘–â€–2
2
simultaneously for every ğ‘–âˆˆ[ğ‘›].
We will now give a proof of Theorem 1.6.
Proof of Theorem 1.6. We first argue the correctness of this algorithm. By Theorem 2.5, it suffices
to verify that Ëœw(ğ‘,ğ‘¡)
ğ‘–computes (1 +ğœ€)-approximate leverage scores in order to prove the claimed
guarantees of Theorem 1.6. For the updates within APPROX QUADRATIC (Algorithm 2), we have by
Lemma 2.4 that
ËœQ(ğ‘¡)= (1Â±ğœ€)AâŠ¤V(ğ‘¡)A.
Thus,
v(ğ‘¡âˆ’1)
ğ‘–Â·â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡âˆ’1))âˆ’1/2â€–2
2= (1Â±ğœ€)v(ğ‘¡âˆ’1)
ğ‘–Â·â€–eâŠ¤
ğ‘–A(AâŠ¤V(ğ‘¡âˆ’1)A)âˆ’1/2â€–2
2
= (1Â±ğœ€)ğœğ‘–(âˆšï¸€
V(ğ‘¡âˆ’1)A)
and thus the weights v(ğ‘¡)
ğ‘–output by APPROX QUADRATIC (Algorithm 2) indeed satisfy the require-
ments of Theorem 2.5.
For the weights computed in Line 8 of Algorithm 3, note that we also compute these weights, but this
time with approximation error from the application of the Gaussian matrix1
ğ‘šG(ğ‘¡)to speed up the
8computation. Applying Theorem 2.6 with ğœ€set to ğœ€/ğµğ‘‡ and failure probability ğ›¿= 1/poly( ğ‘›), we
have that
1
ğ‘šâ€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡â€²âˆ’1))âˆ’1/2G(ğ‘¡â€²âˆ’1)â€–2
2= (1Â±ğœ€/ğµğ‘‡ )â€–eâŠ¤
ğ‘–A(ËœQ(ğ‘¡â€²âˆ’1))âˆ’1/2â€–2
2.
Note then that Line 8 takes a product of at most ğµğ‘‡of these approximations, so the total error in the
approximation is at most
(1Â±ğœ€/ğµğ‘‡ )ğµğ‘‡= (1Â±ğ‘‚(ğœ€)).
The running time is given by ğµğ‘‡ iterations of the inner loop of APPROX QUADRATIC andğµ
iterations of the fast matrix multiplication procedure in Line 7 of Algorithm 3. The inner loop of
APPROX QUADRATIC requires ğ‘‚(ğ‘›ğ‘‘)time to compute the product with the ğ‘‘Ã—ğ‘˜Gaussian as well
as the time to compute the approximate quadratic, which is bounded in Lemma 2.4. Altogether, this
gives the claimed running time bound.
3 Future directions
In this work, we developed fast algorithms and low-space streaming algorithms for the problem of
computing John ellipsoids. Our fast algorithms use a combination of using lazy updates together with
fast matrix multiplication to substantially improve the running time of John ellipsoids, and we apply
similar ideas to obtain a low-space streaming implementation of the John ellipsoid algorithm.
Our results have several limitations that we discuss here, which we leave for future work to resolve.
First, our algorithm makes crucial use of fast matrix multiplication in order to get running time
improvements. However, this makes it a less attractive option for practical implementations, and
also makes the polynomial dependence on ğœ€in the Ëœğ‘‚(ğ‘›) poly( ğœ€âˆ’1)term rather large. Thus, it is an
interesting question whether the running time that we obtain in Theorem 1.6 is possible without fast
matrix multiplication.
Question 3.1. Is there an algorithm with the guarantee of Theorem 1.6 that avoids fast matrix
multiplication?
More generally, it is an interesting question to design algorithms for approximating John ellipsoids
with optimal running time. This is an old question which has been studied in a long line of work
[Wol70 ,Atw73 ,KT93 ,NN94 ,Kha96 ,STT78 ,KY05 ,SF04 ,TY07 ,AST08 ,Yu11 ,HFR20 ], and
we believe that the investigation of this question will lead to further interesting developments in
algorithms research.
Question 3.2. What is the optimal running time of approximating John ellipsoids?
For instance, one interesting question is whether it is possible to obtain nearly linear time algorithms
that run in time Ëœğ‘‚(ğ‘›ğ‘‘) +Ëœğ‘‚(ğ‘›) poly( ğœ€âˆ’1), or even input sparsity time algorithms that run in time
Ëœğ‘‚(nnz(A))+Ëœğ‘‚(ğ‘›) poly( ğœ€âˆ’1). The resolution of such questions for the least squares linear regression
problem has led to great progress in algorithms, and studying these questions for the John ellipsoid
problem may have interesting consequences as well.
John ellipsoids are closely related to â„“ğ‘Lewis weights , which give a natural â„“ğ‘generalization of
John ellipsoids and leverage scores and have been a valuable tool in randomized numerical linear
algebra. There has been a recent focus on fast algorithms for computing (1 +ğœ€)-approximate â„“ğ‘
Lewis weights [ FLPS22 ,AGS24 ], and thus it is natural to ask whether developments in algorithms
for John ellipsoids would carry over to algorithms for â„“ğ‘Lewis weights as well.
Question 3.3. What is the optimal running time of approximating â„“ğ‘Lewis weights?
Finally, we raise questions concerning streaming algorithms for approximating John ellipsoids. In
Theorem 1.8, we gave a multi-pass streaming algorithm which obtains a (1+ğœ€)-optimal John ellipsoid.
A natural question is whether a similar algorithm can be achieved in fewer passes, or if there are pass
lower bounds for computing (1 +ğœ€)-optimal John ellipsoids.
Question 3.4. Are there small space streaming algorithms for approximating John ellipsoids up
to a(1 +ğœ€)factor which make fewer than ğ‘‚(ğœ€âˆ’1log(ğ‘›/ğ‘‘))passes, or do small space streaming
algorithms necessarily require many passes?
We note that there are one-pass small space streaming algorithms if we allow for approximation
factors that scale as ğ‘‚(âˆšlogğ‘›)rather than (1 +ğœ€)[WY22, MMO22, MMO23].
9Acknowledgments and Disclosure of Funding
The authors were supported in part by a Simons Investigator Award and NSF CCF-2335412.
Bibliography
[AGS24] Simon Apers, Sander Gribling, and Aaron Sidford. On computing approximate Lewis
weights. CoRR , abs/2404.02881, 2024. 3
[AS15] Pankaj K. Agarwal and R. Sharathkumar. Streaming algorithms for extent problems in
high dimensions. Algorithmica , 72(1):83â€“98, 2015. 1.2.3
[AST08] Selin Damla Ahipasaoglu, Peng Sun, and Michael J. Todd. Linear convergence of a
modified Frank-Wolfe algorithm for computing minimum-volume enclosing ellipsoids.
Optim. Methods Softw. , 23(1):5â€“19, 2008. 1.1, 3
[Atw69] Corwin L. Atwood. Optimal and efficient designs of experiments. Ann. Math. Statist. ,
40:1570â€“1602, 1969. 1
[Atw73] Corwin L. Atwood. Sequences converging to ğ·-optimal designs of experiments. Ann.
Statist. , 1:342â€“352, 1973. 1.1, 3
[BCK12] SÃ©bastien Bubeck, NicolÃ² Cesa-Bianchi, and Sham M. Kakade. Towards minimax
policies for online linear optimization with bandit feedback. In Shie Mannor, Nathan
Srebro, and Robert C. Williamson, editors, COLT 2012 - The 25th Annual Conference
on Learning Theory, June 25-27, 2012, Edinburgh, Scotland , volume 23 of JMLR
Proceedings , pages 41.1â€“41.14. JMLR.org, 2012. 1
[BMV23] Aditya Bhaskara, Sepideh Mahabadi, and Ali Vakilian. Tight bounds for volumetric
spanners and applications. In Alice Oh, Tristan Naumann, Amir Globerson, Kate
Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information
Processing Systems 36: Annual Conference on Neural Information Processing Systems
2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , 2023. 1
[CCLY19] Michael B. Cohen, Ben Cousins, Yin Tat Lee, and Xin Yang. A near-optimal algorithm
for approximating the John ellipsoid. In Alina Beygelzimer and Daniel Hsu, editors,
Conference on Learning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA ,
volume 99 of Proceedings of Machine Learning Research , pages 849â€“873. PMLR,
2019. (document), 1.1, 1, 1, 1.2.2, 1.2.3, 2.2, 2.5
[CDWY18] Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast MCMC sampling
algorithms on polytopes. J. Mach. Learn. Res. , 19:55:1â€“55:86, 2018. 1
[Cla05] Kenneth L. Clarkson. Subgradient and sampling algorithms for â„“1regression. In
Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms ,
SODA â€™05, pages 257â€“266, USA, 2005. Society for Industrial and Applied Mathemat-
ics. 1
[CLM+15]Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng,
and Aaron Sidford. Uniform sampling for matrix approximation. In Tim Roughgarden,
editor, Proceedings of the 2015 Conference on Innovations in Theoretical Computer
Science, ITCS 2015, Rehovot, Israel, January 11-13, 2015 , pages 181â€“190. ACM, 2015.
1.2.2, 2.2
[Cop82] Don Coppersmith. Rapid multiplication of rectangular matrices. SIAM J. Comput. ,
11(3):467â€“471, 1982. 1.4
[CW13] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression
in input sparsity time. In Dan Boneh, Tim Roughgarden, and Joan Feigenbaum, editors,
Symposium on Theory of Computing Conference, STOCâ€™13, Palo Alto, CA, USA, June
1-4, 2013 , pages 81â€“90. ACM, 2013. 1.2
10[DDH+09]Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael W. Ma-
honey. Sampling algorithms and coresets for â„“ğ‘regression. SIAM J. Comput. ,
38(5):2060â€“2078, 2009. 1
[DG03] Sanjoy Dasgupta and Anupam Gupta. An elementary proof of a theorem of Johnson
and Lindenstrauss. Random Struct. Algorithms , 22(1):60â€“65, 2003. 2.6
[DMM06] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for
â„“2regression and applications. In Proceedings of the Seventeenth Annual ACM-SIAM
Symposium on Discrete Algorithms, SODA 2006, Miami, Florida, USA, January 22-26,
2006 , pages 1127â€“1136. ACM Press, 2006. 2.3
[DMMW12] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff.
Fast approximation of matrix coherence and statistical leverage. J. Mach. Learn. Res. ,
13:3475â€“3506, 2012. 1.2, 1.2.1, 1.2.1, 2.2
[DWZ23] Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric
hashing. In 64th IEEE Annual Symposium on Foundations of Computer Science, FOCS
2023, Santa Cruz, CA, USA, November 6-9, 2023 , pages 2129â€“2138. IEEE, 2023. 1
[FLPS22] Maryam Fazel, Yin Tat Lee, Swati Padmanabhan, and Aaron Sidford. Computing
Lewis weights to high precision. In Joseph (Seffi) Naor and Niv Buchbinder, editors,
Proceedings of the 2022 ACM-SIAM Symposium on Discrete Algorithms, SODA 2022,
Virtual Conference / Alexandria, VA, USA, January 9 - 12, 2022 , pages 2723â€“2742.
SIAM, 2022. 3
[Gli98] FranÃ§ois Glineur. Pattern separation via ellipsoids and conic programming. MÃ©moire
de DEA, FacultÃ© Polytechnique de Mons, Mons, Belgium , 1998. 1
[GN23] Adam Gustafson and Hariharan Narayanan. Johnâ€™s walk. Adv. in Appl. Probab. ,
55(2):473â€“491, 2023. 1
[HFR20] Radoslav Harman, Lenka FilovÃ¡, and Peter RichtÃ¡rik. A randomized exchange algo-
rithm for computing optimal approximate designs of experiments. J. Amer. Statist.
Assoc. , 115(529):348â€“361, 2020. 1.1, 3
[HK16] Elad Hazan and Zohar S. Karnin. V olumetric spanners: An efficient exploration basis
for learning. J. Mach. Learn. Res. , 17:119:1â€“119:34, 2016. 1
[JL84] William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a
Hilbert space. In Conference in modern analysis and probability (New Haven, Conn.,
1982) , volume 26 of Contemp. Math. , pages 189â€“206. Amer. Math. Soc., Providence,
RI, 1984. 1.2.2, 2.6
[Joh48] Fritz John. Extremum problems with inequalities as subsidiary conditions. In Studies
and Essays Presented to R. Courant on his 60th Birthday, January 8, 1948 , pages
187â€“204. Interscience Publishers, Inc., New York, N. Y ., 1948. 1
[Kha79] Leonid G. Khachiyan. A polynomial algorithm in linear programming. Dokl. Akad.
Nauk SSSR , 244(5):1093â€“1096, 1979. 1
[Kha96] Leonid G. Khachiyan. Rounding of polytopes in the real number model of computation.
Math. Oper. Res. , 21(2):307â€“320, 1996. 1.1, 3
[KT93] Leonid G. Khachiyan and Michael J. Todd. On the complexity of approximating the
maximal inscribed ellipsoid for a polytope. Math. Programming , 61(2, Ser. A):137â€“159,
1993. 1.1, 3
[KY05] Piyush Kumar and E. Alper Yildirim. Minimum-volume enclosing ellipsoids and core
sets. J. Optim. Theory Appl. , 126(1):1â€“21, 2005. 1.1, 1, 3
[LMP13] Mu Li, Gary L. Miller, and Richard Peng. Iterative row sampling. In 54th Annual IEEE
Symposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013,
Berkeley, CA, USA , pages 127â€“136. IEEE Computer Society, 2013. 1.2.2, 2.2
11[MMO22] Yury Makarychev, Naren Sarayu Manoj, and Max Ovsiankin. Streaming algorithms for
ellipsoidal approximation of convex polytopes. In Po-Ling Loh and Maxim Raginsky,
editors, Conference on Learning Theory, 2-5 July 2022, London, UK , volume 178 of
Proceedings of Machine Learning Research , pages 3070â€“3093. PMLR, 2022. 1.2.3, 3
[MMO23] Yury Makarychev, Naren Sarayu Manoj, and Max Ovsiankin. Near-optimal streaming
ellipsoidal rounding for general convex polytopes. CoRR , abs/2311.09460, 2023. 1.2.3,
3
[MSS10] Asish Mukhopadhyay, Animesh Sarker, and Tom Switzer. Approximate ellipsoid
in the streaming model. In Weili Wu and Ovidiu Daescu, editors, Combinatorial
Optimization and Applications - 4th International Conference, COCOA 2010, Kailua-
Kona, HI, USA, December 18-20, 2010, Proceedings, Part II , volume 6509 of Lecture
Notes in Computer Science , pages 401â€“413. Springer, 2010. 1.2.3
[NN94] Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in
convex programming , volume 13 of SIAM Studies in Applied Mathematics . Society for
Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 1994. 1.1, 3
[NTZ13] Aleksandar Nikolov, Kunal Talwar, and Li Zhang. The geometry of differential
privacy: the sparse and approximate cases. In Dan Boneh, Tim Roughgarden, and Joan
Feigenbaum, editors, Symposium on Theory of Computing Conference, STOCâ€™13, Palo
Alto, CA, USA, June 1-4, 2013 , pages 351â€“360. ACM, 2013. 1
[Ros65] Judah Ben Rosen. Pattern separation by convex programming. Journal of Mathematical
Analysis and Applications , 10(1):123â€“134, 1965. 1
[SF04] Peng Sun and Robert M. Freund. Computation of minimum-volume covering ellipsoids.
Oper. Res. , 52(5):690â€“706, 2004. 1.1, 3
[Sho77] Naum Z Shor. Cut-off method with space extension in convex programming problems.
Cybernetics , 13(1):94â€“96, 1977. 1
[Sil13] Samuel Silvey. Optimal design: an introduction to the theory for parameter estimation ,
volume 1. Springer Science & Business Media, 2013. 1
[SS11] Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances.
SIAM J. Comput. , 40(6):1913â€“1926, 2011. 1.2, 2.3, 2.2
[SSK17] Å½eljka Stojanac, Daniel Suess, and Martin Kliesch. On products of gaussian random
variables. arXiv preprint arXiv:1711.10516 , 2017. 2.1.1
[ST80] BW Silverman and DM Titterington. Minimum covering ellipses. SIAM Journal on
Scientific and Statistical Computing , 1(4):401â€“409, 1980. 1
[STT78] Samuel D Silvey, DH Titterington, and Ben Torsney. An algorithm for optimal designs
on a design space. Communications in Statistics-Theory and Methods , 7(14):1379â€“
1389, 1978. 1.1, 3
[SYYZ22] Zhao Song, Xin Yang, Yuanyuan Yang, and Tianyi Zhou. Faster algorithm for structured
John ellipsoid computation. CoRR , abs/2211.14407, 2022. 1, 1
[TKE88] S. P. Tarasov, L. G. Khachiyan, and I. I. Ãˆrlikh. The method of inscribed ellipsoids.
Dokl. Akad. Nauk SSSR , 298(5):1081â€“1085, 1988. 1
[TMF20] Murad Tukan, Alaa Maalouf, and Dan Feldman. Coresets for near-convex functions.
In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual , 2020. 1
[Tod16] Michael J. Todd. Minimum volume ellipsoids - theory and algorithms , volume 23 of
MOS-SIAM Series on Optimization . SIAM, 2016. 1
12[TWZ+22]Murad Tukan, Xuan Wu, Samson Zhou, Vladimir Braverman, and Dan Feldman.
New coresets for projective clustering and applications. In Gustau Camps-Valls,
Francisco J. R. Ruiz, and Isabel Valera, editors, International Conference on Artificial
Intelligence and Statistics, AISTATS 2022, 28-30 March 2022, Virtual Event , volume
151 of Proceedings of Machine Learning Research , pages 5391â€“5415. PMLR, 2022. 1
[TY07] Michael J. Todd and E. Alper Yildirim. On Khachiyanâ€™s algorithm for the computation
of minimum-volume enclosing ellipsoids. Discrete Appl. Math. , 155(13):1731â€“1744,
2007. 1.1, 1, 3
[Vai96] Pravin M. Vaidya. A new algorithm for minimizing convex functions over convex sets.
Math. Programming , 73(3, Ser. A):291â€“341, 1996. 1
[Vem05] Santosh Vempala. Geometric random walks: a survey. Combinatorial and computa-
tional geometry , 52(573-612):2, 2005. 1
[Wil11] Ryan Williams. Non-uniform ACC circuit lower bounds. In Proceedings of the
26th Annual IEEE Conference on Computational Complexity, CCC 2011, San Jose,
California, USA, June 8-10, 2011 , pages 115â€“125. IEEE Computer Society, 2011. 1.4
[Wil24] Ryan Williams. Personal communication, 2024. 1.4
[Wol70] P. Wolfe. Convergence theory in nonlinear programming. In Integer and nonlinear
programming , pages 1â€“36. North-Holland, Amsterdam-London, 1970. 1.1, 3
[WXXZ24] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New
bounds for matrix multiplication: from alpha to omega. In David P. Woodruff, editor,
Proceedings of the 2024 ACM-SIAM Symposium on Discrete Algorithms, SODA 2024,
Alexandria, VA, USA, January 7-10, 2024 , pages 3792â€“3835. SIAM, 2024. 1
[WY22] David P. Woodruff and Taisuke Yasuda. High-dimensional geometric streaming in
polynomial space. In 63rd IEEE Annual Symposium on Foundations of Computer
Science, FOCS 2022, Denver, CO, USA, October 31 - November 3, 2022 , pages
732â€“743. IEEE, 2022. 1.2.3, 3
[WY23] David P. Woodruff and Taisuke Yasuda. New subset selection algorithms for low rank
approximation: Offline and online. In Barna Saha and Rocco A. Servedio, editors,
Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023,
Orlando, FL, USA, June 20-23, 2023 , pages 1802â€“1813. ACM, 2023. 1
[Yu11] Yaming Yu. D-optimal designs via a cocktail algorithm. Stat. Comput. , 21(4):475â€“481,
2011. 1.1, 3
13NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: [TODO]
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss limitations of our work in Section 3, and pose open questions
towards closing these gaps.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
14Justification: We provide complete proofs of all of our results, and references to prior work
whenever we need prior results.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
15Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
16â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: [TODO]
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
17generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
18Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [TODO]
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
19