Small coresets via negative dependence:
DPPs, linear statistics, and concentration
RÃ©mi Bardenetâˆ—
Univ. Lille, CNRS, Centrale Lille,
UMR 9189 â€“ CRIStAL,
F-59000 Lille, France
remi.bardenet@cnrs.frSubhroshekhar Ghoshâˆ—
Department of Mathematics
National University of Singapore
10 Lower Kent Ridge Road, 119076, Singapore
subhrowork@gmail.com
Hugo Simon-Onfroyâˆ—
UniversitÃ© Paris-Saclay, CEA, Irfu
DÃ©partement de Physique des Particules
91191, Gif-sur-Yvette, France
hugo.simon@cea.frHoang Son Tranâˆ—â€ 
Department of Mathematics
National University of Singapore
10 Lower Kent Ridge Road, 119076, Singapore
hoangson.tran@u.nus.edu
Abstract
Determinantal point processes (DPPs) are random configurations of points with
tunable negative dependence. Because sampling is tractable, DPPs are natural can-
didates for subsampling tasks, such as minibatch selection or coreset construction.
Acoreset is a subset of a (large) training set, such that minimizing an empirical
loss averaged over the coreset is a controlled replacement for the intractable min-
imization of the original empirical loss. Typically, the control takes the form of
a guarantee that the average loss over the coreset approximates the total loss uni-
formly across the parameter space. Recent work has provided significant empirical
support in favor of using DPPs to build randomized coresets, coupled with interest-
ing theoretical results that are suggestive but leave some key questions unanswered.
In particular, the central question of whether the cardinality of a DPP-based coreset
is fundamentally smaller than one based on independent sampling remained open.
In this paper, we answer this question in the affirmative, demonstrating that DPPs
can provably outperform independently drawn coresets . In this vein, we contribute
a conceptual understanding of coreset loss as a linear statistic of the (random)
coreset. We leverage this structural observation to connect the coresets problem to
a more general problem of concentration phenomena for linear statistics of DPPs,
wherein we obtain effective concentration inequalities that extend well-beyond
the state-of-the-art , encompassing general non-projection, even non-symmetric
kernels. The latter have been recently shown to be of interest in machine learning
beyond coresets, but come with a limited theoretical toolbox, to the extension
of which our result contributes. Finally, we are also able to address the coresets
problem for vector-valued objective functions, a novelty in the coresets literature.
1 Introduction
LetX={xi|iâˆˆJ1, nK}be a set of npoints in a Euclidean space, called the data set . LetFbe a
set of nonnegative functions on X, called queries . Many classical learning problems, supervised or
âˆ—The authors are listed in alphabetical order by their surnames
â€ Corresponding author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).unsupervised, are formulated as finding a query fâˆ—inFthat minimizes an additive loss function of
the form
L(f) :=X
xâˆˆXÂµ(x)f(x), (1)
where Âµ:X â†’R+is a weight function.
Example 1 (k-means) .ForX âŠ‚RdandkâˆˆN, the goal of k-means clustering is to find a set Câˆ—of
kâ€œcluster centers" by minimizing (1) over
F=
fC:x7â†’min
qâˆˆCâˆ¥xâˆ’qâˆ¥2
2| C âŠ‚ Rd,|C|=k
.
Here, each query fis indexed by a set of kcluster centers, and the loss (1) is the quantization error.
Example 2 (linear regression) .When X={xi:= (yi, zi)|iâˆˆJ1, nK} âŠ‚Rd+1, linear regression
corresponds to minimizing (1) over
F=
(y, z)7â†’(aâŠ¤y+bâˆ’z)2|aâˆˆRd, bâˆˆR	
.
Penalty terms can be added to each function, to cover e.g. ridge or lasso regression.
In many machine learning applications, the complexity of the corresponding optimization problem
grows with the cardinality nof the dataset. When nâ‰«1makes optimization intractable, one is
tempted to reduce the amount of data, using only a tractable number of representative samples. This
is the idea formalized by coresets ; we refer to (Bachem, Lucic, and Krause, 2017) for a survey, and
to (Huang, Li, and Wu, 2024; Cohen-Addad, Larsen, Saulpic, Schwiegelshohn, and Sheikh-Omar,
2022) for specific coreset constructions for k-means and Euclidean clustering. An Îµ-coreset is a
subset S âŠ‚ X , possibly with corresponding weights Ï‰(x),xâˆˆ S, such that
LS(f) :=X
xâˆˆSÏ‰(x)f(x) (2)
is within ÎµofL(f), uniformly in fâˆˆ F. If the cardinality mofSis significantly smaller than the
intractable size nof the original data set, one has reduced the complexity of the algorithm at a little
cost in accuracy.
Many randomized coreset constructions, where such guarantees are shown to hold with large proba-
bility, are built by drawing elements independently from the data set X(Bachem et al., 2017, Chapter
3). Because a representative coreset should intuitively be made of diverse data points, negative
dependence between the coreset elements has been proposed as an effective possibility to improve
their performance (Tremblay, BarthelmÃ©, and Amblard, 2019). In particular, the authors advocate the
use of Determinantal Point Processes (DPPs), a family of probability distributions over subsets of X
parametrized by an nÃ—nkernel matrix Kthat enforces diversity, all of this while coming with a
polynomial-time exact sampling algorithm.
Tremblay et al., 2019 give extensive theoretical and empirical justification for the use of DPPs in
randomized coreset construction. In one of their key results, using concentration results in (Pemantle
and Peres, 2011), Tremblay et al., 2019 bound the cardinality of a DPP-based Îµ-coreset, and their
bound is O(Îµâˆ’2). However, it is known that the best Îµ-coresets built with independent samples are
also of cardinality O(Îµâˆ’2). Thus, the crucial question of whether DPP-based coresets can provide
a strict improvement remained to be settled; given the computational simplicity of independent
schemes, this would be fundamental to justify the deployment of DPP-based methods.
In this paper, we settle this question in the affirmative, demonstrating that for carefully chosen kernels,
DPP-based coresets provably yield significantly better accuracy guarantees than independent schemes;
equivalently, to achieve similar accuracy it suffices to use significantly smaller coresets via DPPs. In
particular, we will show that DPP-based coresets actually can achieve cardinality m=O(Îµâˆ’2/(1+Î´)).
The quantity Î´depends on the variance of the subsampled loss under the considered DPP, and some
DPPs yields Î´ >0. A cornerstone of our approach is a structural understanding of the coreset loss (2)
as a so-called linear statistic of the random point set S, which enables us to go beyond earlier results
that were based on concentration properties of general Lipschitz functions of a DPP (Pemantle and
Peres, 2011).
In this endeavour, we obtain very widely-applicable concentration inequalities for linear statistics of
DPPs compared to the state of the art; cf. (Breuer and Duits, 2013) that mostly focuses on scalar-
valued statistics for finite rank ensembles on R. In particular, we are able to address all DPPs that
2have appeared so far in the ML literature. Specifically, our results are able to handle non-symmetric
kernels andvector-valued linear statistics.
DPPs with non-symmetric kernels have recently been shown to be of significant interest in machine
learning, such as recommendation systems (Gartrell, Brunel, et al., 2019; Gartrell, Han, et al., 2020;
Han et al., 2022), but they come with a limited theoretical toolbox, to which this paper makes a
contribution. On the other hand, vector-valued statistics arise naturally in many learning problems,
including coreset settings such as the gradient estimator in Stochastic Gradient Descent (Bardenet,
Ghosh, et al., 2021). However, the literature on coresets for vector-valued statistics is scarce, and in
this paper we inaugurate their study with effective approximation guarantees via DPPs.
The rest of the paper is organized as follows. Section 2 contains background on DPPs and coresets.
Section 3 contains our contributions. Section 4 provides numerical illustrations. Section 5 contains a
discussion on limitations and future work.
2 Background
We introduce here the two key notions of determinantal point process and coreset, and observe that a
coreset guarantee is a uniform control over specific linear statistics of a point process.
Determinantal point processes. A point process Son a Polish space Xis a random locally finite
subset of X. Given a reference measure ÂµonX(e.g., the Lebesgue measure if X=Rdor the
counting measure if Xis discrete), a point process Sis called a DPP (w.r.t. Âµ) if there exists a
measurable function K:X Ã— X â†’ Csuch that
EhX
Ì¸=f(xi1, . . . , x ik)i
=Z
Xkf(x1, . . . , x k) det[ K(xi, xj)]kÃ—kdÂµâŠ—k(x1, . . . , x k), (3)
where the sum in the LHS ranges over all pairwise distinct k-tuples of the random locally finite subset
S, for all bounded measurable f:Xkâ†’Rand for all kâˆˆN. Such a function Kis called a kernel
for the DPP S, and Âµis called the background measure.
When the ground set Xis of finite cardinality n, an equivalent but more intuitive way to define DPPs
is as follows: a random subset SofXis called a DPP if there exists an nÃ—n-matrix Ksuch that
P(TâŠ‚ S) = det[ KT],âˆ€TâŠ‚ X,
where KTdenotes the submatrix of Kwith rows and columns indexed by T.
In a similar vein to Gaussian processes, all the statistical properties of a DPP are encoded in this
kernel function Kand background measure Âµ. A feature of DPPs with far-reaching implications
for machine learning is that sampling and inference with DPPs are tractable. We refer the reader
to (Hough et al., 2006; Kulesza and Taskar, 2012) for general references. Originally introduced in
electronic optics (Macchi, 1975), they have been turned into generic statistical models for repulsion in
spatial statistics (Lavancier et al., 2014; Biscio and Lavancier, 2017) and machine learning (Kulesza
and Taskar, 2012; Belhadji et al., 2020a; Brunel, 2018; Derezinski and Mahoney, 2019; Derezinski,
Liang, et al., 2020; Gartrell, Brunel, et al., 2019; Ghosh and Rigollet, 2020).
Example 3 (L-ensemble and m-DPP) .LetXbe a finite set of cardinality n,Âµbe the counting
measure, and Lbe a positive semi-definite nÃ—n-matrix. The L-ensemble with parameter Lis the
point process SonXsuch that, for all TâŠ‚ X ,P(S=T)âˆdet[LT], where LTis the square
submatrix of Lcorresponding to the rows and columns indexed by the subset T. It can be shown
thatSis a DPP on Xwith kernel K:=L(I+L)âˆ’1. In general, the cardinality of Sis a random
variable. By conditioning on the event {|S|=m}, we obtain the so-called m-DPPs (Kulesza and
Taskar, 2012).
Example 4 (Multivariate OPE; Bardenet and Hardy, 2020) .LetX=RdandÂµbe a measure on
Rdhaving all moments finite, let (pk)kâˆˆNdbe the orthonormal sequence resulting from applying the
Gram-Schmidt procedure to the monomials xk1
1. . . xkd
d, taken in the graded lexical order. The kernel
K(m)
Âµ(x, y) :=Pmâˆ’1
k=0pk(x)pk(y)then defines a projection DPP on Rd, called the multivariate
Orthogonal Polynomial Ensemble (OPE) of rank mand reference measure Âµ.
Multivariate OPEs were used in (Bardenet and Hardy, 2020) as nodes for numerical integration,
leading to a Monte Carlo estimator with mean squared error decaying in mâˆ’1âˆ’1/d, faster than under
3independent sampling. In (Bardenet, Ghosh, and Lin, 2021), the authors investigated the problem
of DPP-based minibatch sampling for Stochastic Gradient Descent (SGD), and exploited a delicate
interplay between a finite dataset and its ambient data distribution to leverage this fast decay for
improved approximation guarantees. In particular, they proposed the following DPP defined on a
(large) finite ground set.
Example 5 (Discretized multivariate OPE; Bardenet, Ghosh, et al., 2021) .LetnâˆˆNandX=
{x1, . . . , x n} âŠ‚ [âˆ’1,1]d. Let q(x)dxbe a probability measure on [âˆ’1,1]d. Let K(m)
qbe the
multivariate OPE kernel of rank mwith reference measure q(x)dx, as defined in Example 4. Let
ËœÎ³: [âˆ’1,1]dâ†’R+be a function, assumed to be positive on X, and consider
K(m)
q,ËœÎ³(x, y) :=s
q(x)
ËœÎ³(x)K(m)
q(x, y)s
q(y)
ËœÎ³(y), x, y âˆˆ[âˆ’1,1]d.
Consider then the nÃ—nmatrix ËœK=K(m)
q,ËœÎ³|XÃ—X .ËœKis symmetric and positive semidefinite, and
we let Kbe the matrix with the same eigenvectors, the mlargest eigenvalues replaced by 1, and the
remaining eigenvalues replaced by 0. Then Kdefines a DPP on X.
Coresets. LetÎµ > 0andXbe a set of cardinality n. The classical definition of a coreset is
multiplicative.
Definition 1 (multiplicative coreset) .A subset3S âŠ‚ X is anÎµ-multiplicative coreset if
âˆ€fâˆˆ F,LS(f)
L(f)âˆ’1â‰¤Îµ, (4)
where LandLSare respectively defined in (1) and (2).
An immediate and important consequence of (2)is that the ratio of the minimum value of LSby that
ofLis within O(Îµ)of1(Bachem et al., 2017, Theorem 2.1).
One way to satisfy (2)with high probability for a single fis through importance sampling, taking S
to be formed of m > 0i.i.d. samples from some instrumental density qonX, and taking Ï‰=Âµ/qin
(2). Langberg and Schulman, 2010 showed that a suitable choice of qactually yields the uniform
guarantee (2). It suffices to take for instrumental pdf q(x)âˆÂµ(x)s(x), where supper-bounds the
so-called sensitivity
s(x)â‰¥sup
fâˆˆFf(x)P
yâˆˆXÂµ(y)f(y),âˆ€xâˆˆ X. (5)
ForÎ´ >0,kâ‰¥S2
2Îµ2log 2/Î´independent draws are then enough to build an Îµ-multiplicative coreset,
where S=P
xâˆˆXÂµ(x)s(x); see (Bachem et al., 2017)[Section 2.3]. The tighter the bound (5), the
smaller the size of the coreset. One important limitation is that finding a tight bound is nontrivial.
Although not standard, a natural alternative definition of a coreset is that of an additive coreset.
Definition 2 (additive coreset) .A subset S âŠ‚ X is anÎµ-additive coreset if
1
n|LS(f)âˆ’L(f)| â‰¤Îµ,âˆ€fâˆˆ F. (6)
Note the arbitrary scaling factor 1/nin(6)compared to (2), which we adopt to simplify comparisons
between the two coreset definitions. With an additive coreset, the minimal value of LSis guaranteed
to be within Â±nÎµof the minimal value of L: Similarly to a multiplicative coreset, with Îµsuitably
small one should be happy to train oneâ€™s algorithm only on S.
Coreset guarantee and linear statistics. LetSbe a point process on a finite X={x1, . . . , x n}.
For a test function Ï†:X â†’R, we denote by Î›(Ï†) :=P
xâˆˆSÏ†(x)the so-called linear statistic of
Ï†. In a coreset problem, for a query fâˆˆ F, the estimated loss LS(f)in(2)is the linear statistic
Î›(Ï‰f). When Sis a DPP with a kernel KonX(w.r.t. the counting measure), we will choose the
weight Ï‰(x) =K(x, x)âˆ’1, where for x=xiâˆˆ X, we define K(x, x)to beKii. By (3), this choice
makes LS(f)an unbiased estimator for L(f). Guaranteeing a coreset guarantee such as (6)with high
probability thus corresponds to a uniform-in- fconcentration inequality for the linear statistic Î›(Ï‰f).
This motivates studying the concentration of linear statistics under a DPP, to which we now turn.
3Note that we defined a coreset as a subset and not a sub-multiset of X, thus ignoring multiplicity. This is
because we allow weights in (2), so that repeated items are unnecessary in a coreset.
43 Theoretical results
We first give new results on the concentration of linear statistics under very general DPPs. These
results are of interest in their own right, and should find applications in ML beyond coresets. Next we
examine the implications of the concentration of linear statistics for coresets, showing that a suitable
DPP does yield a coreset size of size o(Îµâˆ’2), thus beating independent sampling.
Concentration inequalities for linear statistics of DPPs. We start with Hermitian kernels.
Theorem 1 (Hermitian kernels) .LetSbe a DPP on a Polish space Xwith reference measure Âµand
Hermitian kernel K. Then for any bounded test function Ï†:X â†’R, we have
P(|Î›(Ï†)âˆ’E[Î›(Ï†)]| â‰¥Îµ)â‰¤2 exp
âˆ’Îµ2
4AVar [Î›( Ï†)]
,âˆ€0â‰¤Îµâ‰¤2AVar [Î›( Ï†)]
3âˆ¥Ï†âˆ¥âˆ,
where A >0is a universal constant.
Our Theorem 1 is similar in spirit to a seminal concentration inequality by Breuer and Duits, 2013.
However, their result only applies to DPPs with Hermitian projection kernels of finite rank. We
emphasize that our Theorem 1 is applicable to all Hermitian kernels on general Polish spaces.
In view of recent interest in machine learning on DPPs with non-symmetric kernels, we present
here a concentration inequality for such DPPs. We propose a novel approach to control the Laplace
transform in the non-symmetric case (which can also be applied to the symmetric setting). As a
trade-off, the range for Îµbecomes a bit smaller. For simplicity, we present the result for a finite
ground set, but the proof applies more generally.
Theorem 2 (Non-symmetric kernels) .LetSbe a DPP on a finite set X={x1, . . . , x n}with a
non-symmetric kernel K. Then for any bounded test function Ï†:X â†’R, we have
P(|Î›(Ï†)âˆ’E[Î›(Ï†)]| â‰¥Îµ)â‰¤2 exp
âˆ’Îµ2
4Var [Î›( Ï†)]
,âˆ€0â‰¤Îµâ‰¤Var [Î›( Ï†)]2
40âˆ¥Ï†âˆ¥3âˆÂ·max(1 ,âˆ¥Kâˆ¥2op)Â· âˆ¥Kâˆ¥âˆ—,
where âˆ¥ Â· âˆ¥ opdenotes the spectral norm and âˆ¥ Â· âˆ¥âˆ—denotes the nuclear norm of a matrix.
Remark 2.1. For simplicity, we will use the concentration inequality in Theorem 1 from now on.
However, we keep in mind that we always can apply Theorem 2 to deduce analogous results for
non-symmetric kernels.
We conclude with a concentration inequality for linear statistics of vector-valued functions.
Theorem 3 (Vector-valued statistics) .LetSbe a DPP on a Polish space Xwith reference measure
Âµand Hermitian kernel K. Let Î¦ = ( Ï†1, . . . , Ï† p)âŠ¤:X â†’ Rpbe a vector-valued test function,
and we denote by Î›(Î¦) ,V(Î¦)the vectors (Î›(Ï†i))p
i=1and(Var [Î›( Ï†i)]1/2)p
i=1, respectively. Let
âˆ¥xâˆ¥2
Ï‰:=Pp
i=1Ï‰2
i|xi|2be a weighted norm on Rpfor some weights Ï‰1, . . . Ï‰ pâ‰¥0. Then, for some
universal constant A >0, we have
P(âˆ¥Î›(Î¦)âˆ’E[Î›(Î¦)] âˆ¥Ï‰â‰¥Îµ)â‰¤2pexp
âˆ’Îµ2
4Aâˆ¥V(Î¦)âˆ¥2Ï‰
,
for0â‰¤Îµâ‰¤2Aâˆ¥V(Î¦)âˆ¥Ï‰
3min1â‰¤iâ‰¤pâˆš
Var[Î›( Ï†i)]
âˆ¥Ï†iâˆ¥âˆÂ·
DPPs for coresets. We demonstrate the effectiveness of concentration inequalities for linear
statistics of DPPs in the coresets problem, achieving uniform approximation guarantees over function
classes. To accommodate as many ML settings as possible, we shall consider two natural types of
function classes: vector spaces of functions (A.1) and parametrized function spaces (A.2).
For vector spaces of functions, we assume that
dimspanR(F) =D <âˆfor some D. (A.1)
This assumption covers common situations like linear regression in Example 2, where we observe
that each fâˆˆ F is a quadratic function in (d+ 1) variables. Thus the dimension of the linear span of
Fis at most (d+1)2+(d+1)+1 . Another popular class of queries, originating in signal processing
5problems, is the class of band-limited functions . A function f:Td7â†’R(where Tddenotes the
d-dimensional torus) is said to be band-limited if there exists BâˆˆNsuch that its Fourier coefficients
Ë†f(k1, . . . , k d) = 0 whenever there is a kjsuch that |kj|> B . It is easy to see that the space Fof
B-bandlimited functions satisfies dimF â‰¤(2B+ 1)d.
Another common scenario is when Fis parametrized by a finite-dimensional parameter space:
F={fÎ¸:Î¸âˆˆÎ˜},where Î˜is a bounded subset of RDfor some D, (A.2)
âˆ¥fÎ¸âˆ’fÎ¸â€²âˆ¥âˆâ‰¤â„“âˆ¥Î¸âˆ’Î¸â€²âˆ¥for some â„“ >0, uniformly on Î˜. (A.3)
Conditions (A.2) and(A.3) cover e.g. the k-means problem of Example 1, as well as (non-)linear
regression settings. For k-means, for instance, each query is parametrized by its cluster centers
C={q1, . . . , q k}, which can be viewed as a parameter (q1, . . . , q k)âˆˆRkd.
Finally, with the idea in mind to derive multiplicative coresets from additive ones, we note that since
L(f)is typically of order n(for any fwhose effective support covers a positive fraction of the ground
set), it is natural to assume that
1
n|L(f)| â‰¥c,for some c >0, uniformly on F. (A.4)
Theorem 4. LetSbe a DPP with a Hermitian kernel Kon a finite set X={x1, . . . , x n}and
m=E[|S|]. Assume that for all iâˆˆ {1, . . . , n },Kiiâ‰¥ÏÂ·m/n for some Ï >0not depending on
m, n . LetVâ‰¥supfâˆˆFVar
nâˆ’1LS(f)
. Under (A.1) and(A.4) ,
P
âˆƒfâˆˆ F:LS(f)
L(f)âˆ’1â‰¥Îµ
â‰¤2 exp
6Dâˆ’c2Îµ2
16AV
,0â‰¤Îµâ‰¤4AÏmV
3csupfâˆˆFâˆ¥fâˆ¥âˆÂ·
Assuming (A.2) ,(A.3) ,(A.4) and|S| â‰¤ BÂ·ma.s. for some B >0, we have
P
âˆƒfâˆˆ F:LS(f)
L(f)âˆ’1â‰¥Îµ
â‰¤2 exp
CDâˆ’DlogÎµâˆ’c2Îµ2
16AV
,0â‰¤Îµâ‰¤4AÏmV
3csupfâˆˆFâˆ¥fâˆ¥âˆÂ·
Here A >0is a universal constant and C=C(Î˜, B, Ï, â„“, c )>0is some constant.
Remark 4.1. For a bounded query f,Var
nâˆ’1LS(f)
=O(mâˆ’1)for i.i.d. sampling. In compari-
son, sampling with DPPs often yields smaller variance for linear statistics, in O(mâˆ’(1+Î´))for some
Î´ >0; see Section 3 for an example. Thus, the upper bound for the range of Îµfor which we could
use our concentration result is O(mâˆ’Î´). Plugging in Îµ=mâˆ’Î±forÎ±â‰¥Î´gives the upper bounds
2 exp(6 Dâˆ’Câ€²m1+Î´âˆ’2Î±)and2 exp( CD+Î±Dlogmâˆ’Câ€²m1+Î´âˆ’2Î±)respectively ( CandCâ€²are
some positive constants independent of mandn), which both converge to 0asmâ†’ âˆ as long as
Î± <(1 +Î´)/2. In other words, the accuracy rate Îµcan be chosen to be as small as mâˆ’1/2âˆ’Î´â€²/2, for
any0< Î´â€²< Î´, which is strictly smaller than the best accuracy rate mâˆ’1/2of i.i.d. sampling.
Remark 4.2. For i.i.d. sampling Swith expected size m,P(xâˆˆ S) =m/n for all xâˆˆ X. For a
DPPSwith kernel K, one has P(xiâˆˆ S) =Kii. Thus, assuming that for all i,Kiiâ‰¥ÏÂ·m/n for
some Ï >0means that every point in the dataset Xshould have a reasonable chance to be sampled.
This also guarantees that the estimated loss LS(f) =P
xâˆˆSf(x)/K(x, x)will not blow up, where
forx=xiâˆˆ X, we write K(x, x)forKii.
Remark 4.3. For the parametrized function spaces, the assumption |S| â‰¤ BÂ·ma.s. is not
strictly necessary, and is introduced here only for the sake of simplicity in presenting the results. A
version of Theorem 4 without this assumption will be discussed in Appendix A.4. In fact, we only
neednâˆ’1P
xâˆˆSK(x, x)âˆ’1to be bounded with high probability, which follows from the condition
K(x, x)â‰¥ÏÂ·m/n and the fact that |S|is highly concentrated around its mean m.
Remark 4.4. However, we remark that the assumption |S| â‰¤ BÂ·ma.s. holds for most kernels of
interest; DPPs with projection kernels being typical and significant examples. In machine learning
terms, it entails that the coresets are not much bigger than their expected size m; whereas in practice,
sampling schemes typically produce coresets of a fixed size (such as with projection DPPs).
Remark 4.5. It is straightforward to derive a version for additive coresets from Theorem 1. In fact,
we will not need assumption (A.4) in the additive setting.
6For the coresets problem for vector-valued functions, let Fconsist of f:X â†’Rp. For each fâˆˆ F,
we denote by LS(f), L(f)andV(f)the vectors in Rpwhose i-coordinates are LS(fi), L(fi)and
Var
nâˆ’1LS(fi)1/2, respectively. Let âˆ¥xâˆ¥2
Ï‰:=Pp
i=1Ï‰2
i|xi|2be a weighted norm on Rp.
Theorem 5. LetSbe a DPP as in Theorem 4. Let Vâ‰¥supfâˆˆFmax 1â‰¤iâ‰¤pÏ‰2
iVar
nâˆ’1LS(fi)
.
Assuming (A.1) , then
P
âˆƒfâˆˆ F:1
nâˆ¥LS(f)âˆ’L(f)âˆ¥Ï‰â‰¥Îµ
â‰¤2pexp
6Dâˆ’c2Îµ2
16AV
,
where 0â‰¤Îµâ‰¤4AÏmV
3cÂ·(supfâˆˆFmax i=1,...,pâˆ¥fiâˆ¥âˆ)âˆ’1.
Application: Discretized multivariate OPE. We revisit Example 5. It has been shown in (Bardenet,
Ghosh, et al., 2021) that sampling with the DPP Sconstructed in this example yields significant
variance reduction for a wide class of linear statistics on X. To be more precise, in their setting,
X={x1, . . . , x n}is a random data set, where xiâ€™s are i.i.d. samples from a distribution Î³with
support inside a d-dimensional hypercube; ËœÎ³is a density estimator for Î³andq(x)dxis a reference
measure on that hypercube. The DPP Sis then defined by the kernel Kin Example 5 w.r.t. the
empirical measure nâˆ’1P
xâˆˆXÎ´x. We normalize the kernel by setting Ë†K:=nâˆ’1K, so that Sis a
DPP with kernel Ë†Kw.r.t. the counting measure on X. Then, under some mild assumptions on ËœÎ³and
q(x)dx, with high probability in the data set X, we have Var
nâˆ’1LS(f)
=O(mâˆ’(1+1/d))for any
test function fsatisfying some mild regularity conditions. For more details, we refer the reader to
(Bardenet, Ghosh, et al., 2021).
The significant reduction on the variance of linear statistics motivates us to apply Theorem 4 to this
setting. We also remark that all assumptions on the kernel in Theorem 4 are satisfied for Ë†Kwith high
probability in the data set X(see Appendix A.6). Let Fbe a family of test functions on Xsatisfying
regularity conditions as in Bardenet, Ghosh, et al., 2021. Then we can state:
Theorem 6. ForÎµ=O(mâˆ’1/d), w.h.p. in the data set X, we have
PS
âˆƒfâˆˆ F:LS(f)
L(f)âˆ’1â‰¥Îµ
â‰¤2 exp
6Dâˆ’Câ€²Îµ2m1+1/d
,assuming (A.1) ,
and
PS
âˆƒfâˆˆ F:LS(f)
L(f)âˆ’1â‰¥Îµ
â‰¤2 exp
CDâˆ’DlogÎµâˆ’Câ€²Îµ2m1+1/d
,assuming (A.2) ,(A.3),
where PSindicates the randomness only in SandC, Câ€²>0do not depend on m, n .
Remark 6.1. Theorem 6 confirms the discussion in Remark 4.1 for this particular example of DPP .
More precisely, Theorem 6 implies that, with probability tending to 1, sampling with this DPP gives
|LS(f)/L(f)âˆ’1| â‰¤mâˆ’(1
2+1
2d)âˆ’,âˆ€fâˆˆ F, where (1
2+1
2d)âˆ’denotes any positive number strictly
smaller than1
2+1
2d. Meanwhile, for i.i.d. sampling, the accuracy rate Îµis at best mâˆ’1/2.
Remark 6.2. It may be noted that, DPPs being Hilbert space-based models, they interact well with
linear projection based methods. As such, our method can be applied on dimensionally reduced data,
wherein the din Remark 6.1 can be taken to be the reduced dimension, which is usually quite small.
As such, the improvement in the approximation guarantees is substantial, especially for large scale
problems entailing large m.
4 Experiments
In this section, we compare randomized coresets for the k-means problem of Example 1, on different
datasets. One virtue of k-means as a benchmark is that asymptotically tight upper-bound on sensitivity
(5) can easily be computed (Bachem et al., 2017, Lemma 2.2).
Competing approaches. We compare 6 different coreset samplers.4Each sampler takes as input a
finite dataset X, an integer m, and sampler-specific parameters. It returns a random subset S âŠ‚ X of
4The link to a GitHub repository is temporarily hidden for anonymity.
7cardinality m. For the associated weight function Ï‰in(2), we always take the inverse of the marginal
probability of inclusion, i.e. Ï‰(x) = 1 /P(xâˆˆ S).
The first two baselines use independent sampling. The uniform method returns msamples from
X, uniformly and without replacement, and runs in O(m). The second method, sensitivity , is
specific to the k-means problem. It corresponds to the classical sensitivity-based importance sampling
coreset of Langberg and Schulman, 2010 described in Section 2. It runs in O(nk+nm).
The rest of the methods use negative dependence. The third method, termed G-mDPP , uses an m-DPP
sampler where the likelihood kernel is a Gaussian kernel, with adjustable bandwidth denoted by h.
It is basically Algorithm 1 of Tremblay et al., 2019, except we do not approximate the likelihood
kernel using random features. We prefer avoiding approximations in this paper to isolatedly probe
the benefit of negative dependence, but our choice comes at the cost O(n3)of performing SVD as a
preprocessing, in addition to the usual O(nm2)sampling time. Similarly, we compute the marginal
probabilities of inclusion of m-DPPs exactly, via Equation (205) and Algorithm 7 of Kulesza and
Taskar, 2012. These costly steps will likely be approximated in real data applications; see the
discussion of complexity to Section 5. The fourth method, OPE, is the discretized OPE of Example 5.
We take qto be a product of univariate beta pdfs, with parameters tuned to match the marginal
moments of the dataset, as in (Bardenet, Ghosh, et al., 2021). We take ËœÎ³to be a kernel density
estimator (KDE) built on X, using the Epanechnikov kernel, with Scottâ€™s bandwidth selection method,
as implemented in the scikit-learn package (Pedregosa et al., 2011). When KDE estimation
is precomputed as in our experiments, the method runs in O(nm2), andO(n2+nm2)otherwise.
Note that there is no cubic power of n, as one can perform the eigenvalue thresholding in Example 1
by a reduced SVD of the mÃ—nfeature matrix (pk(xi)). The fifth method, termed Vdm-DPP , is
Algorithm 2 of Tremblay et al., 2019, which runs in O(nm2). It is an OPE in the sense of Example 4,
but where the reference measure Âµis the discrete empirical measure of the dataset. Although we
have no result on how its linear statistics scale, its similarity with the discretized OPE, as well as
its numerical performance in the experiments of Tremblay et al., 2019, make us expect Vdm-DPP to
behave similarly to OPE. The sixth method, stratified , is a stratified sampling baseline limited
to the case where X âŠ‚ [âˆ’1,1]dandXis â€œwell-spread". It partitions [âˆ’1,1]dinto a grid of mbins,
and then independently draws one element uniformly in the intersection of Xwith each bin. It is a
special case of projection DPP, which runs in O(nm)and has obvious pitfalls, like requiring that X
has a non-empty intersection with each bin, which is unlikely to be the case for non-uniformly spread
datasets and high dimensions. Yet, this is a simple solution that one would likely implement to probe
the benefits of negative dependence.
The performance metric. To investigate the cardinality of a coreset for a given error, we let
QSdenote the quantile function of sup|LS(f)âˆ’L(f)|/L(f), the supremum over all queries of
the relative error. Intuitively, QS(0.9) = 10âˆ’2means that 90% of the sampled coresets have a
worst case relative error below 10âˆ’2. We shall look at how an estimated QS(0.9)varies with m,
especially its slope in log-log plots with respect to m. Now, the set Fof all queries for k-means
in combinatorially large, even for small values of k. Therefore, each time we need to evaluate the
supremum of the relative error, we rather uniformly sample without replacement kelements of X,
100times and independently, and we take the maximum value of the relative error among these
100values. Moreover, for each method and each coreset size m, the quantile function QS(0.9)is
estimated by an empirical quantile over 100 independent coresets sampled for each value of m.
Results. We first consider a synthetic dataset of n= 1024 data points, sampled uniformly and
independently in [âˆ’1,1]d; see Figure 1a. We consider d= 2 for demonstration purposes, but we
have observed similar results for other small dimensions. Figure 1b depicts our estimate of QS(0.9)
as a function of the coreset size m, in log-log format. The two i.i.d. baselines decrease as mâˆ’1/2, as
expected. The stratified baseline, intuitively well-suited to uniformly-spread datasets, outperforms all
other methods with a mâˆ’1rate, consistent with its known optimal variance reduction (Novak, 1988).
Finally, the m-DPP and the two DPPs also yield a faster decay, eventually outperforming the i.i.d.
baselines as mgrows. This is expected for the discretized OPE, as it follows from the theoretical
results from Section 3; but it is interesting to see that the Gaussian m-DPP and the Vdm-DPP seem
to reach a similar mâˆ’3/4fast rate. For the Gaussian m-DPP, however, the performance depends on
the value of the bandwidth of the Gaussian kernel: in Figure 1c, we see that the rate of decay can go
from i.i.d.-like to OPE-like as the bandwidth increases; this is expected from results like (BarthelmÃ©
et al., 2023). Note that the color code of Figure 1c differs from other figures.
8âˆ’1.0âˆ’0.5 0.0 0.5 1.0âˆ’1.00âˆ’0.75âˆ’0.50âˆ’0.250.000.250.500.751.00
data, n=1024
OPE, m=169(a) Uniform dataset and OPE sample
100101102
sample size10âˆ’11000.90-quantile sup. rel. error
uniform
sensitivity
OPE
Vdm-DPP
G-mDPP, h=0.1
stratiï¬ed (b)QS(0.9)vs. coreset size m
100101102
sample size10âˆ’11000.90-quantile sup. rel. error
uniform
G-mDPP, h=0.01
G-mDPP, h=0.05
G-mDPP, h=0.10
G-mDPP, h=0.20
G-mDPP, h=0.30 (c)QS(0.9)vs. coreset size m
Figure 1: Results for the uniform dataset.
In the uniform dataset of Fig. 1a, the sensitivity function is almost flat, which makes sensitivity
behave like uniform . To give an edge to sensitivity , we now consider the trimodal dataset shown
in Fig. 2a, with an OPE sample superimposed. The performance of sensitivity improves; see
Figure 2b, while the determinantal samplers still outperform the independent ones thanks to a faster
decay. For this dataset, it is not easy to stratify, and we thus do not show results for stratified .
We note that the size of a marker placed at xis proportional to the corresponding weight 1/K(x, x)
in the estimator of the average loss. Equivalently, the marker size is inversely proportional to the
marginal probability of xbeing included in the DPP sample.
Finally, we consider the classical MNIST dataset, after a PCA of dimension 4. Figure 2c shows again
the faster decay of the performance metric for the two DPPs ( OPEandVdm-DPP ), compared to the two
independent methods. However, the advantage progressively disappears as the dimension increases be-
yond 4(unshown), as expected from the gain in variance of the discretized multivariate OPE, which be-
comes negligible when dâ‰«1; see Section 5 for suggestions on how to prove a dimension-independent
decay. The source code used in this work is available at github.com/hsimonfroy/DPPcoresets ,
where DPP samplers are built upon the Python package DPPy (Gautier et al., 2019).
âˆ’1.0âˆ’0.5 0.0 0.5 1.0âˆ’1.00âˆ’0.75âˆ’0.50âˆ’0.250.000.250.500.751.00data, n=1023
OPE, m=169
(a) Trimodal dataset and OPE sample
100101102
sample size10âˆ’11000.90-quantile sup. rel. error
uniform
sensitivity
OPE
Vdm-DPP
G-mDPP, h=0.1 (b)QS(0.9)vs. coreset size m
100101102
sample size1000.90-quantile sup. rel. error
uniform
sensitivity
OPE
Vdm-DPP (c)QS(0.9)vs. coreset size m
Figure 2: Results on other datasets.
5 Discussion
Limitations. Our paper is a theoretical contribution, and our approach has several limitations
before it can be a practical addition to the coreset toolbox. The improvement over independent
sampling relies on a variance scaling for linear statistics of a particular DPP, which itself relies on
both 1) an Ansatz that the dataset was generated i.i.d. from some pdf Î³with a large support, and
2) the availability of a good approximation to Î³; see Section 3 and (Bardenet, Ghosh, et al., 2021).
While Item 1) is usually deemed to be reasonable in a wide range of situations, we solve Item 2) by
9relying on a kernel density estimator, which is costly to manipulate. Another limitation is that the
improvement over independent sampling is in 1/dand thus progressively vanishes as the dimension
increases. Finally, a classical caveat is that although tractable, sampling a DPP still costs O(nm2),
provided the kernel is available in diagonalized form.
Future work. The limitations above set up a research program. In particular, an intriguing observa-
tion in our empirical studies is the comparative performance of various DPP-based coreset samplers;
several of them exhibit effective performance. While we have sharp theoretical guarantees for the
discretized OPE-based scheme, obtaining similar guarantees and parameter-tuning protocols for other
samplers, like m-DPPs, will be of great practical interest as they would bypass the need, e.g., for an
approximation to the data-generating mechanism Î³. The DPP called Vdm-DPP in Section 4, which is
itself an OPE for a discrete measure, might be a bridge between OPEs and m-DPPs, as Vdm-DPP can
be seen as a limit of Gaussian m-DPPs (BarthelmÃ© et al., 2023). On a more general note, improving
the computational complexity of sampling DPPs remains an active topic, and we should examine
which techniques, e.g. by leveraging low-rank structures, preserve the small coreset property. Any
breakthrough in the complexity of DPP sampling would also have salutary consequences for the
broader program of negative dependence as a toolbox for machine learning. On the dependence of the
rate to the dimension, we propose to investigate the impact of smoothness of the test functions on the
rate: in numerical integration with mixtures of DPPs, smoothness does bring dimension-independent
rates (Belhadji et al., 2020b). Finally, in a more theoretical direction, extending concentration
inequalities for linear statistics beyond the restricted range of Îµappearing e.g. in Theorem 1 is a
mathematically challenging problem, with potential learning-theoretic consequences.
Acknowledgments and Disclosure of Funding
RB and HSO acknowledge support from ERC grant Blackjack (ERC-2019-STG-851866) and ANR
AI chair Baccarat (ANR-20-CHIA-0002). SG was supported in part by the MOE grants R-146-000-
250-133, R-146-000-312-114, A-8002014-00-00 and MOE-T2EP20121-0013. HST was supported
by the NUS Research Scholarship.
References
[1] Olivier Bachem, Mario Lucic, and Andreas Krause. â€œPractical Coreset Constructions
for Machine Learningâ€. In: arXiv: Machine Learning (2017). URL:https : / / api .
semanticscholar.org/CorpusID:88517375 .
[2] RÃ©mi Bardenet, Subhroshekhar Ghosh, and Meixia Lin. â€œDeterminantal point processes
based on orthogonal polynomials for sampling minibatches in SGDâ€. In: Advances in Neural
Information Processing Systems 34 (2021), pp. 16226â€“16237.
[3] RÃ©mi Bardenet and Adrien Hardy. â€œMonte Carlo with Determinantal Point Processesâ€. In:
Annals of Applied Probability (2020). URL:https://hal.archives-ouvertes.fr/hal-
01311263 .
[4] Simon BarthelmÃ©, Nicolas Tremblay, Konstantin Usevich, and Pierre-Olivier Amblard. â€œDeter-
minantal point processes in the flat limitâ€. In: Bernoulli 29.2 (2023), pp. 957â€“983.
[5] Ayoub Belhadji, RÃ©mi Bardenet, and Pierre Chainais. â€œA determinantal point process for
column subset selectionâ€. In: Journal of machine learning research 21.197 (2020), pp. 1â€“62.
[6] Ayoub Belhadji, RÃ©mi Bardenet, and Pierre Chainais. â€œKernel interpolation with continuous
volume samplingâ€. In: International Conference on Machine Learning . PMLR. 2020, pp. 725â€“
735.
[7] Christophe Ange NapolÃ©on Biscio and FrÃ©dÃ©ric Lavancier. â€œContrast estimation for parametric
stationary determinantal point processesâ€. In: Scandinavian Journal of Statistics 44.1 (2017),
pp. 204â€“229.
[8] Jonathan Breuer and Maurice Duits. â€œThe Nevai condition and a local law of large numbers
for orthogonal polynomial ensemblesâ€. In: Advances in Mathematics 265 (2013), pp. 441â€“484.
URL:https://api.semanticscholar.org/CorpusID:119731958 .
[9] Victor-Emmanuel Brunel. â€œLearning signed determinantal point processes through the principal
minor assignment problemâ€. In: Advances in Neural Information Processing Systems 31 (2018).
10[10] Vincent Cohen-Addad, Kasper Green Larsen, David Saulpic, Chris Schwiegelshohn, and
Omar Ali Sheikh-Omar. â€œImproved Coresets for Euclidean k-Meansâ€. English. In: Advances in
Neural Information Processing Systems 35 - 36th Conference on Neural Information Processing
Systems, NeurIPS 2022 . Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,
and A. Oh. Advances in Neural Information Processing Systems. Publisher Copyright: Â©
2022 Neural information processing systems foundation. All rights reserved.; 36th Conference
on Neural Information Processing Systems, NeurIPS 2022 ; Conference date: 28-11-2022
Through 09-12-2022. Neural Information Processing Systems Foundation, 2022.
[11] Michal Derezinski, Feynman Liang, and Michael Mahoney. â€œBayesian experimental design
using regularized determinantal point processesâ€. In: International Conference on Artificial
Intelligence and Statistics . PMLR. 2020, pp. 3197â€“3207.
[12] Michal Derezinski and Michael Mahoney. â€œDistributed estimation of the inverse Hessian by
determinantal averagingâ€. In: Advances in Neural Information Processing Systems 32 . Ed. by
H. Wallach, H. Larochelle, A. Beygelzimer, F. d AlchÃ©-Buc, E. Fox, and R. Garnett. Curran
Associates, Inc., 2019, pp. 11401â€“11411.
[13] Mike Gartrell, Victor-Emmanuel Brunel, Elvis Dohmatob, and Syrine Krichene. â€œLearning
nonsymmetric determinantal point processesâ€. In: Advances in Neural Information Processing
Systems 32 (2019).
[14] Mike Gartrell, Insu Han, Elvis Dohmatob, Jennifer Gillenwater, and Victor-Emmanuel Brunel.
â€œScalable learning and MAP inference for nonsymmetric determinantal point processesâ€. In:
arXiv preprint arXiv:2006.09862 (2020).
[15] Guillaume Gautier, Guillermo Polito, RÃ©mi Bardenet, and Michal Valko. â€œDPPy: DPP Sam-
pling with Pythonâ€. In: Journal of Machine Learning Research 20.180 (2019), pp. 1â€“7. URL:
http://jmlr.org/papers/v20/19-179.html .
[16] Subhroshekhar Ghosh and Philippe Rigollet. â€œGaussian determinantal processes: A new model
for directionality in dataâ€. In: Proceedings of the National Academy of Sciences 117.24 (2020),
pp. 13207â€“13213.
[17] Insu Han, Mike Gartrell, Elvis Dohmatob, and Amin Karbasi. â€œScalable MCMC sampling
for nonsymmetric determinantal point processesâ€. In: International Conference on Machine
Learning . PMLR. 2022, pp. 8213â€“8229.
[18] J. Ben Hough, Manjunath Krishnapur, Yuval Peres, and BÃ¡lint VirÃ¡g. â€œDeterminantal Processes
and Independenceâ€. In: Probability Surveys 3 (2006). URL:https://doi.org/10.1214%
2F154957806000000078 .
[19] Lingxiao Huang, Jian Li, and Xuan Wu. â€œOn Optimal Coreset Construction for Euclidean (k,z)-
Clusteringâ€. In: Proceedings of the 56th Annual ACM Symposium on Theory of Computing .
STOC 2024. Vancouver, BC, Canada: Association for Computing Machinery, 2024, pp. 1594â€“
1604. ISBN : 9798400703836. DOI:10.1145/3618260.3649707 .URL:https://doi.org/
10.1145/3618260.3649707 .
[20] Kurt Johansson and Gaultier Lambert. â€œGaussian and non-Gaussian fluctuations for meso-
scopic linear statistics in determinantal processesâ€. In: The Annals of Probability 46.3 (2018),
pp. 1201â€“1278. DOI:10.1214/17- AOP1178 .URL:https://doi.org/10.1214/17-
AOP1178 .
[21] Alex Kulesza and Ben Taskar. â€œDeterminantal Point Processes for Machine Learningâ€. In:
Foundations and Trends Â®in Machine Learning 5.2â€“3 (2012), pp. 123â€“286. ISSN : 1935-8237.
DOI:10.1561/2200000044 .URL:http://dx.doi.org/10.1561/2200000044 .
[22] Michael Langberg and Leonard J. Schulman. â€œUniversal epsilon-approximators for integralsâ€.
In:Proceedings of the 2010 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA) .
2010, pp. 598â€“607. DOI:10.1137/1.9781611973075.50 . eprint: https://epubs.siam.
org/doi/pdf/10.1137/1.9781611973075.50 .URL:https://epubs.siam.org/doi/
abs/10.1137/1.9781611973075.50 .
[23] FrÃ©dÃ©ric Lavancier, Jesper MÃ¸ller, and Ege Rubak. â€œDeterminantal Point Process Models
and Statistical Inferenceâ€. In: Journal of the Royal Statistical Society Series B: Statistical
Methodology 77.4 (Dec. 2014), pp. 853â€“877. ISSN : 1467-9868. DOI:10.1111/rssb.12096 .
URL:http://dx.doi.org/10.1111/rssb.12096 .
[24] Odile Macchi. â€œProcessus ponctuels et coincidences â€“ Contributions Ã  lâ€™Ã©tude thÃ©orique des
processus ponctuels, avec applications Ã  lâ€™optique statistique et aux communications optiquesâ€.
PhD thesis. UniversitÃ© Paris-Sud, 1972.
11[25] Odile Macchi. â€œThe Coincidence Approach to Stochastic Point Processesâ€. In: Advances in
Applied Probability 7.1 (1975), pp. 83â€“122. ISSN : 00018678. URL:http://www.jstor.
org/stable/1425855 (visited on 05/22/2024).
[26] Erich Novak. Deterministic and Stochastic Error Bounds in Numerical Analysis . V ol. 1349.
Lecture Notes in Mathematics. Berlin, Heidelberg: Springer, 1988. ISBN : 978-3-540-50368-2.
DOI:10.1007/BFb0079792 .URL:http://link.springer.com/10.1007/BFb0079792 .
[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P.
Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. â€œScikit-learn: Machine Learning in Pythonâ€. In: Journal of
Machine Learning Research 12 (2011), pp. 2825â€“2830.
[28] Robin Pemantle and Yuval Peres. â€œConcentration of Lipschitz Functionals of Determinantal
and Other Strong Rayleigh Measuresâ€. In: Combinatorics Probability and Computing 23 (Aug.
2011). DOI:10.1017/S0963548313000345 .
[29] A. Soshnikov. â€œDeterminantal random point fieldsâ€. In: Russian Mathematical Surveys 55
(2000), pp. 923â€“975.
[30] Nicolas Tremblay, Simon BarthelmÃ©, and Pierre-Olivier Amblard. â€œDeterminantal point pro-
cesses for coresetsâ€. In: Journal of Machine Learning Research 20.168 (2019), pp. 1â€“70.
12A Appendix / supplemental material
A.1 Proof of Theorem 1
We present here the proof for the general setting, i.e., when Xis a Polish space and Sis a DPP with a
Hermitian kernel Kw.r.t a background measure Âµ. By abuse of notation, we will also denote by K
the integral operator
K:L2(X, Âµ)â†’L2(X, Âµ), f(x)7â†’Z
XK(x, y)f(y)dÂµ(y).
We denote by Ck(Ï†), kâ‰¥1the cumulants of Î›(Ï†), i.e.
logE[etÎ›(Ï†)] =X
kâ‰¥1Ck(Ï†)
k!tk,fortnear0.
Note that C1(Ï†) =E[Î›(Ï†)], C2(Ï†) =Var [Î›( Ï†)]. In general, we have the formula (see Johansson
and Lambert, 2018)
Ck(Ï†) =kX
q=1(âˆ’1)q+1
qX
k1,...,kqâ‰¥1
k1+...+kq=kk!
k1!. . . k q!Tr[Î¦k1K . . . Î¦kqK], (7)
where Î¦ :L2(X, Âµ)â†’L2(X, Âµ)is the operator f(x)7â†’Ï†(x)f(x).
By the Macchi-Soshnikov theorem (Macchi, 1972; Soshnikov, 2000), 0âª¯Kâª¯I, and we can write
C2(Ï†) = Tr[Î¦( Iâˆ’K)Î¦K] = Tr[âˆš
KÎ¦(Iâˆ’K)Î¦âˆš
K] =âˆ¥âˆš
Iâˆ’KÎ¦âˆš
Kâˆ¥2
HS,
where âˆ¥ Â· âˆ¥ HSdenotes the Hilbert-Schmidt norm of an operator.
Lemma 1. Forkâ‰¥1, we have
âˆ¥âˆš
Iâˆ’KÎ¦kâˆš
Kâˆ¥HSâ‰¤kâˆ¥Ï†âˆ¥kâˆ’1
âˆâˆ¥âˆš
Iâˆ’KÎ¦âˆš
Kâˆ¥HS,
where âˆ¥Ï†âˆ¥âˆ:= supxâˆˆX|Ï†(x)|.
Proof. One has
âˆ¥âˆš
Iâˆ’KÎ¦kâˆš
Kâˆ¥2
HS= Tr[âˆš
KÎ¦k(Iâˆ’K)Î¦kâˆš
K]
= Tr[Î¦k(Iâˆ’K)Î¦kK]
= Tr[Î¦2kK]âˆ’Tr[Î¦kKÎ¦kK]
=Z
Ï†(x)2kK(x, x)dÂµ(x)âˆ’ZZ
Ï†(x)kK(x, y)Ï†(y)kK(y, x)dÂµ(x)dÂµ(y)
=Z
Ï†(x)2k
K(x, x)âˆ’Z
K(x, y)K(y, x)dÂµ(y)
dÂµ(x)
+1
2ZZ
(Ï†(x)kâˆ’Ï†(y)k)2K(x, y)K(y, x)dÂµ(x)dÂµ(y).
Since 0âª¯Kâª¯I, we have K2âª¯K, which implies K(x, x)â‰¥R
K(x, y)K(y, x)dÂµ(y)forÂµ-a.e.
x. Thus,Z
Ï†(x)2k
K(x, x)âˆ’Z
K(x, y)K(y, x)dÂµ(y)
dÂµ(x)
â‰¤ âˆ¥ Ï†âˆ¥2kâˆ’2
âˆZ
Ï†(x)2
K(x, x)âˆ’Z
K(x, y)K(y, x)dÂµ(y)
dÂµ(x).
On the other hand, by the symmetry of K, we have K(x, y)K(y, x) =|K(x, y)|2â‰¥0for all
x, yâˆˆ X. Note that
|Ï†(x)kâˆ’Ï†(y)k|=|Ï†(x)âˆ’Ï†(y)|kâˆ’1X
j=0Ï†(x)jÏ†(y)kâˆ’1âˆ’jâ‰¤kâˆ¥Ï†âˆ¥kâˆ’1
âˆ|Ï†(x)âˆ’Ï†(y)|.
Combining all ingredients, we deduce that âˆ¥âˆš
Iâˆ’KÎ¦kâˆš
Kâˆ¥2
HSâ‰¤k2âˆ¥Ï†âˆ¥2kâˆ’2
âˆâˆ¥âˆš
Iâˆ’KÎ¦âˆš
Kâˆ¥2
HS,
as desired.
13Lemma 2. Forkâ‰¥3, we have
|Ck(Ï†)|
k!â‰¤1âˆš
2Ï€ekk3/2âˆ¥Ï†âˆ¥kâˆ’2
âˆC2(Ï†).
Proof. We recall the formula (7), observe that
kX
q=1(âˆ’1)q+1
qX
k1,...,kqâ‰¥1
k1+...+kq=kk!
k1!. . . k q!= 0.
Then one can write
Ck(Ï†) =nX
q=1(âˆ’1)q+1
qX
k1,...,kqâ‰¥1
k1+...+kq=kk!
k1!. . . k q!
Tr[Î¦k1K . . . Î¦kqK]âˆ’Tr[Î¦kK]
=nX
q=2(âˆ’1)q+1
qX
k1,...,kqâ‰¥1
k1+...+kq=kk!
k1!. . . k q!
Tr[Î¦k1K . . . Î¦kqK]âˆ’Tr[Î¦kK]
.
For any k1, . . . , k qâ‰¥1such that k1+. . .+kq=k, we observe that
|Tr[Î¦k1K . . . Î¦kqâˆ’2KÎ¦kqâˆ’1+kqK]âˆ’Tr[Î¦k1K . . . Î¦kqâˆ’2KÎ¦kqâˆ’1KÎ¦kqK]|
=|Tr[Î¦k1K . . . Î¦kqâˆ’2KÎ¦kqâˆ’1(Iâˆ’K)Î¦kqK]|
=|Tr[âˆš
KÎ¦k1K . . . Î¦kqâˆ’2âˆš
Kâˆš
KÎ¦kqâˆ’1âˆš
Iâˆ’Kâˆš
Iâˆ’KÎ¦kqâˆš
K]|
â‰¤ âˆ¥âˆš
KÎ¦k1K . . . Î¦kqâˆ’2âˆš
Kâˆš
KÎ¦kqâˆ’1âˆš
Iâˆ’Kâˆ¥HSÂ· âˆ¥âˆš
Iâˆ’KÎ¦kqâˆš
Kâˆ¥HS
â‰¤ âˆ¥âˆš
KÎ¦k1K . . . Î¦kqâˆ’2âˆš
Kâˆ¥opÂ· âˆ¥âˆš
KÎ¦kqâˆ’1âˆš
Iâˆ’Kâˆ¥HSÂ· âˆ¥âˆš
Iâˆ’KÎ¦kqâˆš
Kâˆ¥HS
â‰¤ âˆ¥âˆš
KÎ¦k1K . . . Î¦kqâˆ’2âˆš
Kâˆ¥opÂ·kqâˆ’1kqâˆ¥Ï†âˆ¥kqâˆ’1+kqâˆ’2
âˆ âˆ¥âˆš
Iâˆ’KÎ¦âˆš
Kâˆ¥2
HS
â‰¤kqâˆ’1kqâˆ¥Ï†âˆ¥kâˆ’2
âˆC2(Ï†),
here we used Lemma 1, the fact that 0âª¯Kâª¯I,âˆ¥Î¦âˆ¥op=âˆ¥Ï†âˆ¥âˆand the âˆ¥ Â· âˆ¥ opnorm is
submultiplicative. Since kjâ‰¤kfor all 1â‰¤jâ‰¤q, using a telescoping argument gives
|Tr[Î¦k1K . . . Î¦kqK]âˆ’Tr[Î¦kK]| â‰¤qk2âˆ¥Ï†âˆ¥kâˆ’2
âˆC2(Ï†).
Hence
|Ck(Ï†)| â‰¤kX
q=2X
k1,...,kqâ‰¥1
k1+...+kq=kk!
k1!. . . k q!k2âˆ¥Ï†âˆ¥kâˆ’2
âˆC2(Ï†).
Now observe that for kâ‰¥3
kX
q=2X
k1,...,kqâ‰¥1
k1+...+kq=k1
k1!. . . k q!<kk
k!â‰¤ek
âˆš
2Ï€kÂ·
Thus,
|Ck(Ï†)|
k!â‰¤1âˆš
2Ï€ekk3/2âˆ¥Ï†âˆ¥kâˆ’2
âˆC2(Ï†).
Combining all ingredients above, one can show that.
Lemma 3. For|t| â‰¤1/(3âˆ¥Ï†âˆ¥âˆ), we have
|logE[etÎ›(Ï†)]âˆ’tE[Î›(Ï†)]| â‰¤At2Var [Î›( Ï†)],
where A >0is an universal constant.
14Proof. For|t| â‰¤1
3âˆ¥Ï†âˆ¥âˆ, we have
|logE[etÎ›(Ï†)]âˆ’tE[Î›(Ï†)]|=X
kâ‰¥2Ck(Ï†)
k!tk
â‰¤X
kâ‰¥2|Ck(Ï†)|
k!|t|k
â‰¤ |t|2C2(Ï†)1
2+X
kâ‰¥31âˆš
2Ï€ekk3/2âˆ¥Ï†âˆ¥kâˆ’2
âˆ|t|kâˆ’2
â‰¤ |t|2C2(Ï†)1
2+1âˆš
2Ï€e2X
kâ‰¥3k3/2(e/3)kâˆ’2
=At2Var [Î›( Ï†)],
where A >0is some universal constant.
We can finish the proof of Theorem 1 as follows.
Proof of Theorem 1. LetÎµ >0. We have
logP(Î›(Ï†)âˆ’E[Î›(Ï†)]â‰¥Îµ)â‰¤inf
t
logE[etÎ›(Ï†)]âˆ’tE[Î›(Ï†)]âˆ’tÎµ
â‰¤inf
t
âˆ’tÎµ+t2AVar [Î›( Ï†)]
where the infimum is taken on tâˆˆ(0,1/(3âˆ¥Ï†âˆ¥âˆ)].
For0â‰¤Îµâ‰¤2AVar[Î›( Ï†)]
3âˆ¥Ï†âˆ¥âˆ, choosing
t0=Îµ
2AVar [Î›( Ï†)]â‰¤1
3âˆ¥Ï†âˆ¥âˆ
gives
logP(Î›(Ï†)âˆ’E[Î›(Ï†)]â‰¥Îµ)â‰¤ âˆ’Îµ2
4AVar [Î›( Ï†)]
as desired.
A.2 Proof of Theorem 2
Denote by Î¦the diagonal matrix Diag( Ï†)âˆˆRnÃ—n. For each tâˆˆR, we define
Gt:=Iâˆ’exp(tÎ¦) =âˆ’âˆX
k=1tk
k!Î¦k.
By the Campbell formula, we have E[etÎ›(Ï†)] = det[ Iâˆ’GtK], tâˆˆR.By choosing tâ‰¥0small
enough such that âˆ¥GtKâˆ¥op<1, one can expand
log det[ Iâˆ’GtK] =âˆ’âˆX
k=11
kTr[(GtK)k].
Observe that âˆ¥Gtâˆ¥opâ‰¤e|t|âˆ¥Ï†âˆ¥âˆâˆ’1â‰¤2|t|âˆ¥Ï†âˆ¥âˆfor all |t| â‰¤1/(3âˆ¥Ï†âˆ¥âˆ). From now on, we will
consider tâ‰¥0such that
0â‰¤tâˆ¥Ï†âˆ¥âˆMâ‰¤1
3,
where M:= max( âˆ¥Kâˆ¥op,1). This choice for twill particularly imply that âˆ¥GtKâˆ¥opâ‰¤2/3.
15Fork= 1, we have
âˆ’Tr[GtK] =âˆX
p=1tp
p!Tr[Î¦pK]
â‰¤tE[Î›(Ï†)] +t2
2Tr[Î¦2K] +X
pâ‰¥3tp
p!|Tr[Î¦pK]|
â‰¤tE[Î›(Ï†)] +t2
2Tr[Î¦2K] +X
pâ‰¥3tp
p!âˆ¥Ï†âˆ¥p
âˆâˆ¥Kâˆ¥âˆ—
â‰¤tE[Î›(Ï†)] +t2
2Tr[Î¦2K] +t3âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—.
Fork= 2, we have
âˆ’1
2Tr[(GtK)2] = âˆ’1
2X
p,qâ‰¥1tp+q
p!q!Tr[Î¦pKÎ¦qK]
=âˆ’t2
2Tr[Î¦KÎ¦K]âˆ’1
2X
p+qâ‰¥3tp+q
p!q!Tr[Î¦pKÎ¦qK]
â‰¤ âˆ’t2
2Tr[Î¦KÎ¦K] +1
2X
lâ‰¥3tl
l!2lâˆ¥Ï†âˆ¥l
âˆâˆ¥Kâˆ¥opâˆ¥Kâˆ¥âˆ—
â‰¤ âˆ’t2
2Tr[Î¦KÎ¦K] +t3âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—âˆ¥Kâˆ¥op.
Forkâ‰¥3, we observe that
|Tr[(GtK)k]| â‰¤ âˆ¥(GtK)kâˆ¥âˆ—â‰¤ âˆ¥GtKâˆ¥kâˆ’3
opâˆ¥Gtâˆ¥3
opâˆ¥Kâˆ¥2
opâˆ¥Kâˆ¥âˆ—â‰¤2
3kâˆ’3
(2tâˆ¥Ï†âˆ¥âˆ)3âˆ¥Kâˆ¥2
opâˆ¥Kâˆ¥âˆ—.
ThusX
kâ‰¥31
k|Tr[(GtK)k]| â‰¤8t3âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥2
opâˆ¥Kâˆ¥âˆ—.
Combining all ingredients, we deduce that
logE[etÎ›(Ï†)]â‰¤tE[Î›(Ï†)] +t2
2Var [Î›( Ï†)] +t3âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—(1 +âˆ¥Kâˆ¥op+ 8âˆ¥Kâˆ¥2
op)
â‰¤tE[Î›(Ï†)] +t2
2Var [Î›( Ï†)] + 10 t3âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—M2.
LetÎµ >0. We have
logP(Î›(Ï†)âˆ’E[Î›(Ï†)]â‰¥Îµ)â‰¤inf
t
logE[etÎ›(Ï†)]âˆ’tE[Î›(Ï†)]âˆ’tÎµ
â‰¤inf
t
âˆ’tÎµ+t2
2Var [Î›( Ï†)] +t3Â·10âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—M2
where the infimum is taken on tâˆˆ(0,1/(3âˆ¥Ï†âˆ¥âˆM)].
For0â‰¤Îµâ‰¤Var[Î›( Ï†)]2
40âˆ¥Ï†âˆ¥3âˆâˆ¥Kâˆ¥âˆ—M2, we choose
t0=Îµ
Var [Î›( Ï†)]â‰¤Var [Î›( Ï†)]
40âˆ¥Ï†âˆ¥3âˆâˆ¥Kâˆ¥âˆ—M2â‰¤2Mâˆ¥Ï†âˆ¥2
âˆâˆ¥Kâˆ¥âˆ—
40âˆ¥Ï†âˆ¥3âˆâˆ¥Kâˆ¥âˆ—M2<1
3âˆ¥Ï†âˆ¥âˆMÂ·
This choice yields
logP(Î›(Ï†)âˆ’E[Î›(Ï†)]â‰¥Îµ)â‰¤ âˆ’Îµ2
2Var [Î›( Ï†)]+t3
0Â·10âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—M2.
16Note that
t3
0Â·10âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—M2=Îµ3
Var [Î›( Ï†)]3Â·10âˆ¥Ï†âˆ¥3
âˆâˆ¥Kâˆ¥âˆ—M2â‰¤Îµ2
4Var [Î›( Ï†)]Â·
This implies
logP(Î›(Ï†)âˆ’E[Î›(Ï†)]â‰¥Îµ)â‰¤ âˆ’Îµ2
4Var [Î›( Ï†)]
as desired.
A.3 Proof of Theorem 3
Proof of Theorem 3. By a scaling argument, it suffices to prove for the case Ï‰1=. . .=Ï‰p= 1. We
have
P(âˆ¥Î›(Î¦)âˆ’E[Î›(Î¦)] âˆ¥2â‰¥Îµ) = PpX
i=1|Î›(Ï†i)âˆ’E[Î›(Ï†i)]|2â‰¥Îµ2
=PpX
i=1|Î›(Ï†i)âˆ’E[Î›(Ï†i)]|2â‰¥Îµ2pX
i=1Var [Î›( Ï†i)]
âˆ¥V(Î¦)âˆ¥2
2
â‰¤pX
i=1P
|Î›(Ï†i)âˆ’E[Î›(Ï†i)]| â‰¥Îµp
Var [Î›( Ï†i)]
âˆ¥V(Î¦)âˆ¥2
Â·
For each 1â‰¤iâ‰¤p, applying Theorem 1 gives
P
|Î›(Ï†i)âˆ’E[Î›(Ï†i)]| â‰¥Îµp
Var [Î›( Ï†i)]
âˆ¥V(Î¦)âˆ¥2
â‰¤2 exp
âˆ’Îµ2
4Aâˆ¥V(Î¦)âˆ¥2
2
,âˆ€0â‰¤Îµâ‰¤2Aâˆ¥V(Î¦)âˆ¥2p
Var [Î›( Ï†i)]
3âˆ¥Ï†iâˆ¥âˆÂ·
The theorem follows.
A.4 Proof of Theorem 4
Using (A.4), we deduce that
LS(f)
L(f)âˆ’1â‰¤1
cn|LS(f)âˆ’L(f)|,âˆ€fâˆˆ F.
This implies
P
âˆƒfâˆˆ F:LS(f)
L(f)âˆ’1â‰¥Îµ
â‰¤P
âˆƒfâˆˆ F:1
n|LS(f)âˆ’L(f)| â‰¥cÎµ
.
Thus, it suffices to bound the RHS. For each fâˆˆ F, letVâ‰¥Var
nâˆ’1LS(f)
, we apply Theorem 1
for the linear statistic LS(f) = Î›( f/K)to obtain
P1
n|LS(f)âˆ’L(f)| â‰¥cÎµ
â‰¤2 exp
âˆ’c2Îµ2
4AV
,âˆ€0â‰¤Îµâ‰¤2AnV
3câˆ¥f/Kâˆ¥âˆÂ·
Using K(x, x)â‰¥Ïm/n , we deduce that the above inequality holds for any 0â‰¤Îµâ‰¤2AÏmV
3câˆ¥fâˆ¥âˆ.
Proof of Theorem 4: Assuming (A.1) .We let Fsym:={Î»f:|Î»| â‰¤1, fâˆˆ F} ,and let Bbe the
convex hull of Fsym. Since Bis a symmetric convex body in F, there exists a norm âˆ¥ Â· âˆ¥FinFsuch
thatBis the unit ball in (F,âˆ¥ Â· âˆ¥F).
Define
L(f) :=1
n
LS(f)âˆ’L(f)
, fâˆˆF,
then it is clear that L(f)is linear in f. Moreover, for any f, gâˆˆF, one has
|L(f)âˆ’ L(g)|=|L(fâˆ’g)|=âˆ¥fâˆ’gâˆ¥FÂ·Lfâˆ’g
âˆ¥fâˆ’gâˆ¥Fâ‰¤ âˆ¥fâˆ’gâˆ¥Fsup
hâˆˆB|L(h)|.
17For each Î´ >0, letBÎ´be aÎ´-net for (B,âˆ¥ Â· âˆ¥F). By definition of a Î´-net, for any fâˆˆ B, there exists
anf0âˆˆ BÎ´such that âˆ¥fâˆ’f0âˆ¥Fâ‰¤Î´. Thus, for every fâˆˆ B
|L(f)| â‰¤ |L (f0)|+|L(f)âˆ’ L(f0)| â‰¤ |L (f0)|+Î´sup
hâˆˆB|L(h)| â‰¤sup
gâˆˆBÎ´|L(g)|+Î´sup
hâˆˆB|L(h)|.
This implies supfâˆˆB|L(f)| â‰¤1
1âˆ’Î´supfâˆˆBÎ´|L(f)|,âˆ€0< Î´ < 1.In particular, choosing Î´= 1/2
gives
sup
fâˆˆB|L(f)| â‰¤2 sup
fâˆˆB1/2|L(f)|.
Therefore
P
sup
fâˆˆB|L(f)| â‰¥cÎµ
â‰¤P
2 sup
fâˆˆB1/2|L(f)| â‰¥cÎµ
=P
âˆƒfâˆˆ B1/2:1
n|LS(f)âˆ’L(f)| â‰¥cÎµ/2
.
LetN(B,âˆ¥ Â· âˆ¥F,1/2)be the 1/2-covering number of B, then
P
âˆƒfâˆˆ B:1
n|Ë†LS(f)âˆ’L(f)| â‰¥cÎµ
â‰¤N(B,âˆ¥ Â· âˆ¥F,1/2)Â·2eâˆ’c2Îµ2/16AV,
for any
Vâ‰¥sup
fâˆˆBVar1
nLS(f)
,0â‰¤Îµâ‰¤4AÏmV
3csupfâˆˆBâˆ¥fâˆ¥âˆÂ·
Note that for a finite dimensional normed vector space, for 0< Î´ < 1, one has
N(B,âˆ¥ Â· âˆ¥F, Î´)â‰¤3
Î´dimF
.
This implies
P
âˆƒfâˆˆ B:1
n|LS(f)âˆ’L(f)| â‰¥cÎµ
â‰¤2 exp
6Dâˆ’c2Îµ2
16AV
Â· (8)
SinceF âŠ‚ B , it is clear that
P
âˆƒfâˆˆ F:|LS(f)âˆ’L(f)| â‰¥ncÎµ
â‰¤P
âˆƒfâˆˆ B:|LS(f)âˆ’L(f)| â‰¥ncÎµ
. (9)
On the other hand, for each fâˆˆ B, there exist 0â‰¤tâ‰¤1,|Î»i| â‰¤1, fiâˆˆ F, i= 1,2such that
f=tÎ»1f1+ (1âˆ’t)Î»2f2.Therefore
Var [LS(f)]1/2=Var [LS(tÎ»1f1) +LS((1âˆ’t)Î»2f2)]1/2
â‰¤Var [LS(tÎ»1f1)]1/2+Var [LS((1âˆ’t)Î»2f2)]1/2
â‰¤tVar [LS(f1)]1/2+ (1âˆ’t)Var [LS(f2)]1/2
â‰¤sup
gâˆˆFVar [LS(g)]1/2.
Moreover,
âˆ¥fâˆ¥âˆ= sup
xâˆˆX|f(x)| â‰¤tsup
xâˆˆX|f1(x)|+ (1âˆ’t) sup
xâˆˆX|f2(x)| â‰¤sup
gâˆˆFâˆ¥gâˆ¥âˆ.
Thus,
sup
fâˆˆBVar [LS(f)] = sup
fâˆˆFVar [LS(f)],sup
fâˆˆBâˆ¥fâˆ¥âˆ= sup
fâˆˆFâˆ¥fâˆ¥âˆ. (10)
From (8), (9), (10), the theorem follows.
Proof of Theorem 4: Assuming (A.2) ,(A.3) .We define L(Î¸) :=1
n(LS(fÎ¸)âˆ’L(fÎ¸)), Î¸âˆˆÎ˜. Then
P
âˆƒfâˆˆ F:1
n|LS(f)âˆ’L(f)| â‰¥cÎµ
=P(âˆƒÎ¸âˆˆÎ˜ :|L(Î¸)| â‰¥cÎµ) =P(sup
Î¸âˆˆÎ˜|L(Î¸)| â‰¥cÎµ).
Using (A.3), we have |L(fÎ¸)âˆ’L(fÎ¸â€²)| â‰¤nâ„“âˆ¥Î¸âˆ’Î¸â€²âˆ¥and
|LS(fÎ¸)âˆ’LS(fÎ¸â€²)| â‰¤â„“âˆ¥Î¸âˆ’Î¸â€²âˆ¥X
xâˆˆS1
K(x, x)
â‰¤â„“âˆ¥Î¸âˆ’Î¸â€²âˆ¥nÏâˆ’1mâˆ’1|S|. (11)
18This implies |L(Î¸)âˆ’ L(Î¸â€²)| â‰¤Câˆ¥Î¸âˆ’Î¸â€²âˆ¥a.s., for some constant Cdepending on B, Ï, â„“ . LetÎ“be a
cÎµ
2C-net for Î˜, then
sup
Î¸âˆˆÎ˜|L(Î¸)| â‰¤sup
Î¸â€²âˆˆÎ“|L(Î¸â€²)|+cÎµ
2Â·
Thus
P(sup
Î¸âˆˆÎ˜|L(Î¸)| â‰¥cÎµ)â‰¤P(sup
Î¸â€²âˆˆÎ“|L(Î¸â€²)| â‰¥cÎµ/2)
We note that |Î“|=O(Îµâˆ’D). This completes the proof.
Remark 6.3. Without the assumption |S| â‰¤ BÂ·ma.s., one can continue from (11) as follows.
Denote by Î»1â‰¥. . .â‰¥Î»nâ‰¥0the eigenvalues of K, it is known that |S|=dX1+. . .+Xn, where
Xiâˆ¼Ber(Î»i)are independent. Let B >0, then using a multiplicative Chernoff bound for the sum
of independent Bernoulli variables gives
P(|S|>(B+ 1)m) =PnX
i=1Xi>(B+ 1)m
â‰¤exp
âˆ’B2
B+ 2m
Â·
By choosing Blarge, this event will have small probability. Meanwhile, on the event {|S| â‰¤
(B+ 1)m}, we can use exactly the same argument as in the proof above.
A.5 Proof of Theorem 5
Proof of Theorem 5. It suffices to show for the case Ï‰1=. . .=Ï‰p= 1. For each fâˆˆ F , by
applying Theorem 1 and an union bound argument, we have
P1
nâˆ¥LS(f)âˆ’L(f)âˆ¥ â‰¥Îµ
â‰¤2pexp
âˆ’c2Îµ2
4AV
,âˆ€0â‰¤Îµâ‰¤2AÏmV
3cmax iâˆ¥fiâˆ¥âˆÂ·
Using the same argument as in the proof of Theorem 4 under assumption (A.1) gives the result.
A.6 Proof of Theorem 6
Proof of Theorem 6. We remark that Var
nâˆ’1LS(f)
=O(mâˆ’(1+1/d))uniformly for all fâˆˆ F,
w.h.p. in the data set X. Hence, Theorem 6 is a direct application of Theorem 4 with V=
Cmâˆ’(1+1/d)for some constant C > 0. As we discussed in the Remark 4.1, the range for Îµis
O(mâˆ’1/d). Thus, it suffices to check the conditions on Ë†K. Since Ë†Kis a projection of rank m,
|S|=ma.s. Moreover, we have nË†K(x, x) =K(x, x), which is typically of order m, where we used
an uniform CLT result and an asymptotic for multivariate OPE kernels (see Bardenet, Ghosh, et al.,
2021 for more details).
19NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: We claim theoretical results, which are established as theorems in the main
text.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss limitations in Section 5.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We give our assumptions and results in Section 3.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We give experimental details in Section 4.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We will release code to reproduce the experimental section in a public GitHub
repository upon acceptance. We stress that our contributions are the theoretical results in the
main paper; the code is there only to support our theoretical claims.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We give details in Section 4.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: Our metric is a quantile of a worst-case error; there is no standard error bar.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Our experiments are small-scale and were performed on a personal computer;
we mention this in Section 4.
209.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: It is a theoretical paper, and we cannot see any potential harmful consequence.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: It is a theoretical paper, so societal impact is a long way downstream. A
positive impact we can see in the study of coresets is the possibility of saving energy when
training large models.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risk.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: we properly cite the Python libraries we use.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
21