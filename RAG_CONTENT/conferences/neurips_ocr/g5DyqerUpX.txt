SPARKLE: A Unified Single-Loop Primal-Dual
Framework for Decentralized Bilevel Optimization
Shuchen Zhu‚àó
Peking University
shuchenzhu@stu.pku.edu.cnBoao Kong‚àó
Peking University
kongboao@stu.pku.edu.cnSongtao Lu
IBM Research
songtao@ibm.com
Xinmeng Huang
University of Pennsylvania
xinmengh@sas.upenn.eduKun Yuan‚Ä†
Peking University
kunyuan@pku.edu.cn
Abstract
This paper studies decentralized bilevel optimization, in which multiple agents col-
laborate to solve problems involving nested optimization structures with neighbor-
hood communications. Most existing literature primarily utilizes gradient tracking
to mitigate the influence of data heterogeneity, without exploring other well-known
heterogeneity-correction techniques such as EXTRA or Exact Diffusion. Addi-
tionally, these studies often employ identical decentralized strategies for both
upper- and lower-level problems, neglecting to leverage distinct mechanisms across
different levels. To address these limitations, this paper proposes SPARKLE , a
unified Single-loop Primal-dual AlgoRithm framewor Kfor decentra Lized bil Evel
optimization. SPARKLE offers the flexibility to incorporate various heterogeneity-
correction strategies into the algorithm. Moreover, SPARKLE allows for different
strategies to solve upper- and lower-level problems. We present a unified conver-
gence analysis for SPARKLE , applicable to all its variants, with state-of-the-art
convergence rates compared to existing decentralized bilevel algorithms. Our
results further reveal that EXTRA and Exact Diffusion are more suitable for de-
centralized bilevel optimization, and using mixed strategies in bilevel algorithms
brings more benefits than relying solely on gradient tracking.
1 Introduction
Numerous modern machine learning tasks, such as reinforcement learning [ 25], meta-learning [ 4],
adversarial learning [ 36], hyper-parameter optimization [ 19], and imitation learning [ 3], entail nested
optimization formulations that extend beyond the traditional single-level paradigm. For instance,
hyper-parameter optimization aims to identify the optimal hyper-parameters for a specific learning
task in the upper level by minimizing the validation loss, achieved through training models in the
lower-level process. This nested optimization structure has spurred significant attention towards
Stochastic Bilevel Optimization (SBO). Since the size of data samples involved in bilevel problems
has become increasingly large, this paper investigates decentralized algorithms over a network of n
agents (nodes) that collaborate to solve the following distributed bilevel optimization problem:
min
x‚ààRpŒ¶(x) =f(x, y‚ãÜ(x)):=1
nnX
i=1fi(x, y‚ãÜ(x)), (upper-level) (1a)
‚àóEqual Contribution
‚Ä†Corresponding Author: Kun Yuan. Kun Yuan is also affiliated with National Engineering Labratory for Big
Data Analytics and Applications, and AI for Science Institute, Beijing, China.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).s.t. y‚ãÜ(x) = argmin
y‚ààRqn
g(x, y):=1
nnX
i=1gi(x, y)o
. (lower-level) (1b)
In this formulation, each agent iholds a private upper-level objective function fi:Rp√óRq‚ÜíR
and a strongly convex lower-level objective function gi:Rp√óRq‚ÜíRdefined as:
fi(x, y) =Eœï‚àºDfi[Fi(x, y;œï)], g i(x, y) =EŒæ‚àºDgi[Gi(x, y;Œæ)], (2)
where DfiandDgirepresent the local data distributions at agent i. This paper does not make any
assumptions about these data distributions, implying there might be data heterogeneity across agents.
Linear speedup and transient iteration complexity. A decentralized stochastic algorithm achieves
linear speedup if its iteration complexity decreases linearly with the network size n. Additionally, the
transient iteration complexity refers to the number of transient iterations a decentralized algorithm
must undergo to achieve the asymptotic linear speedup stage. The fewer the transient iterations, the
faster the algorithm can achieve linear speedup. This paper aims to develop decentralized stochastic
bilevel algorithms that can achieve linear speedup with as few transient iterations as possible.
Limitations in previous works. A significant challenge in decentralized bilevel optimization lies
in accurately estimating the hyper-gradient ‚àáŒ¶(x)through neighborhood communications. Several
studies have emerged to effectively address this challenge, such as those by [ 9,33,52,16,21,40,57,
29]. However, existing works suffer from several critical limitations:
‚Ä¢Stringent assumptions and inadequate convergence analysis. Many existing studies rely on
stringent assumptions to ensure convergence. For instance, references [ 9,10,33,21,52] assume
bounded gradients, while reference [ 29] assumes bounded data heterogeneity (also known as
bounded gradient dissimilarity). These restrictive assumptions do not arise in centralized bilevel
optimization, implying their potential unnecessity. Moreover, some of these works suffer from
inadequate convergence analysis, unable to clarify the transient iteration complexity [ 9,33,10] or
provide a sharp estimation of the influence of network topologies [52, 21].
‚Ä¢Limited exploration of various heterogeneity-correction techniques. Several concurrent stud-
ies [16,57,40] have utilized Gradient Tracking (GT) [50,13,38] to remove the assumption
of bounded data heterogeneity. However, it remains uncertain whether GTis the most suitable
mechanism for decentralized bilevel optimization. Many other techniques are also useful for
addressing data heterogeneity in single-level decentralized optimization, such as EXTRA [ 45]
and Exact-Diffusion (ED) [ 56,30,54] (which is also known as D2[46]). Even within GT, there
are variants including Adapt-Then-Combine GT (ATC-GT) [ 50], non-ATC-GT [ 38], and semi-
ATC-GT [ 13]. It remains unexplored whether these techniques for mitigating data heterogeneity
converge and even outperform GT when employed in decentralized bilevel algorithms.
‚Ä¢Unknown effects of employing different upper- and lower-level update strategies. In bilevel
optimization, the challenges in solving the upper- and lower-level problems differ substantially.
For instance, the upper-level problem (1a)is non-convex, whereas the lower-level problem (1b)
is strongly convex. Moreover, estimating the gradient at the lower level is considerably simpler
compared to estimating the hyper-gradient at the upper level. Understanding the roles of updates
at each level is crucial to develop more efficient algorithms. However, most existing algorithms
employ the same decentralized methods to solve both the upper- and lower-level problems. For
example, references [ 9,52,29] utilize decentralized gradient descent (DGD) for updates at both
levels while [57, 40, 16] leverage GT, overlooking the potential advantages of mixed strategies.
To address these limitations, several critical questions naturally arise: Should each heterogeneity-
correction mechanism listed in [ 50,13,38,56,30,54,46] be explored one-by-one? Should we
consider combining any two of these techniques to update the upper and lower-level problems,
respectively? It is evident that examining each individual heterogeneity-correction technique, and
even exploring their combinations, would involve an unbearable amount of effort.
Main results and contributions. This paper addresses all the aforementioned limitations without
exhaustively exploring all heterogeneity-correction techniques. Our main results are as follows.
‚Ä¢A unified decentralized bilevel framework. To avoid examining each single heterogeneity-
correction technique, we propose SPARKLE , a unified Single-loop Primal-dual AlgoRithm
2Table 1: Comparison between different decentralized stochastic bilevel algorithms. Kdenotes the number of
(upper-level) iterations; 1‚àíœÅdenotes the spectral gap of the mixing matrix (see Assumption 2); b2bounds
the gradient dissimilarity; Œµis the target stationarity such thatPK‚àí1
k=0E[‚à•‚àáŒ¶(¬Øxk)‚à•2]/K < Œµ ;pandqare the
dimensions of the upper- and lower-level variables, reflecting per-round communication costs. Assumptions
of bounded gradient, Lipschitz continuity, and bounded gradient dissimilarity are abbreviated as BG, LC, and
BGD, respectively. We also list the best-known results of single-level GT, EXTRA, and ED at the bottom.
Algorithms Assumption‚ñ∑A. Rate.‚ô¢A. Comp.‚Ä†A. Comm.‚Ä°Tran. Iter.‚óÅLoopless
DSBO [9] LC1‚àö
K1
Œµ3 
pqlog(1
Œµ) +q
Œµ1
Œµ2 N.A. No
MA-DSBO [10] LC1‚àö
K1
Œµ2log(1
Œµ)q
Œµ2log(1
Œµ) +p
Œµ2 N.A. No
SLAM [33] LC1‚àö
nK1
nŒµ2log(1
Œµ)p+q
nŒµ2 N.A. No
MDBO [21] BG1‚àö
nK1
nŒµ2log(1
Œµ)p+q
nŒµ2n3
(1‚àíœÅ)8 No
Gossip DSBO [52] BG1‚àö
nK1
nŒµ2log(1
Œµ)q2
nŒµ2log(1
Œµ) +pq
nŒµ2n3
(1‚àíœÅ)4 No
LoPA [40]‚àóBGD1‚àö
K1
Œµ2p+q
Œµ2 N.A. Yes
D-SOBA [29] BGD1‚àö
nK1
nŒµ2p+q
nŒµ2 maxn
n3
(1‚àíœÅ)2,n3b2
(1‚àíœÅ)4o
Yes
SPARKLE-GT (ours) None1‚àö
nK1
nŒµ2ap+q
nŒµ2‚ôØmaxn
n3
(1‚àíœÅ)2,n
(1‚àíœÅ)8/3o
Yes
SPARKLE-EXTRA (ours) None1‚àö
nK1
nŒµ2ap+q
nŒµ2‚ôØ n3
(1‚àíœÅ)2 Yes
SPARKLE-ED (ours) None1‚àö
nK1
nŒµ2ap+q
nŒµ2‚ôØ n3
(1‚àíœÅ)2 Yes
Single-level GT [2, 28] None1‚àö
nK1
nŒµ2p
nŒµ2 maxn
n3
(1‚àíœÅ)2,n
(1‚àíœÅ)8/3o
Yes
Single-level EXTRA [2] None1‚àö
nK1
nŒµ2p
nŒµ2n3
(1‚àíœÅ)2 Yes
Single-level ED [2] None1‚àö
nK1
nŒµ2p
nŒµ2n3
(1‚àíœÅ)2 Yes
‚ô¢The convergence rate when K‚Üí ‚àû (smaller is better).
‚Ä†The number of gradient/Jacobian/Hessian evaluations per agent to achieve Œµ-accuracy when œµ‚Üí0(smaller is better).
‚Ä°The communication costs per agent to achieve Œµ-stationarity when œµ‚Üí0(smaller is better).
‚óÅThe transient iteration complexity to achieve linear speedup (smaller is better). ‚ÄúN.A.‚Äù means that the algorithm cannot
achieve linear speedup or the transient time cannot be accessed from existing convergence analysis.
‚ñ∑Additional assumptions beyond Assumption 1.
‚àóLoPA solves the personalized problem, where the lower-level objectives are local to agents.
‚ôØa >0measures the relative sparsity of the mixing weights Wx,Wy,Wz, which can be very small in certain cases. Here
1‚àíœÅinTran. Iter. denotes the smallest spectral gap of Wx,Wy,Wz. See more discussions in Appendix C.2.3.
framewor Kfor decentra Lized bil Evel optimization. By specifying certain hyper-parameters,
SPARKLE can be tailored to SPARKLE -EXTRA, SPARKLE -ED, and SPARKLE -GT, which
employ EXTRA [ 45], ED [ 56,30], or multiple GT variants [ 50,13,38], respectively, to facilitate
the upper and lower-level problems. Additionally, SPARKLE is the first algorithm enabling
distinct updating strategies across different levels; for example, one can utilize GT in the upper-
level but ED in the lower-level, resulting in a brand new SPARKLE-GT-ED algorithm.
‚Ä¢A unified and sharp analysis of various heterogeneity-correction schemes. We provide a unified
convergence analysis for SPARKLE, which immediately applies to all SPARKLE variants with
distinct heterogeneity-correction techniques. The analysis does not require restrictive assumptions
such as gradient boundedness used in [ 9,10,33,21,52] or data-heterogeneity bounded used
in [29]. Moreover, our analysis demonstrates the provable superiority of SPARKLE compared to
existing algorithms, as evidenced by the convergence rates listed in Table 1. Most importantly, our
analysis shows that both SPARKLE -EXTRA and SPARKLE -ED outperform SPARKLE -GT
(see Table 1), implying that GT is not the best scheme for decentralized bilevel optimization.
‚Ä¢Mixing strategies outperform employing GT alone. We demonstrate how optimization at
different levels affects convergence rates. Our theoretical analysis suggests that the updating
strategy at the lower level is crucial in determining the overall performance in decentralized bilevel
algorithms. Building upon this insight, we establish that incorporating the ED or EXTRA strategy
in the lower-level update phase leads to better transient iteration complexity than relying solely on
the GT mechanism in both levels as proposed in [10, 16, 40], see Table 2 for more details.
‚Ä¢Comparable performance with single-level algorithms. We elucidate the comparison between
bilevel and single-level stochastic decentralized optimization. On one hand, we demonstrate that
the convergence performance of all our proposed algorithms is not inferior to their single-level
counterparts (see the bottom part in Table 1). On the other hand, by considering specific lower-level
loss functions, our bilevel results directly yield the non-asymptotic convergence of corresponding
3Table 2: The transient iteration complexity of SPARKLE with mixed updating strategies at various levels. The
smaller the transient iteration complexity is, the faster the algorithm will achieve its linear speedup stage. The
first row and column respectively indicate the updating strategy for the upper- and lower-level problems. Please
refer to Appendix B.3 for more implementation details and Appendix C.2.4 for proofs .
lowerupperED EXTRA GT
EDn3
(1‚àíœÅ)2n3
(1‚àíœÅ)2n3
(1‚àíœÅ)2
EXTRAn3
(1‚àíœÅ)2n3
(1‚àíœÅ)2n3
(1‚àíœÅ)2
GT maxn
n3
(1‚àíœÅ)2,n
(1‚àíœÅ)8/3o
maxn
n3
(1‚àíœÅ)2,n
(1‚àíœÅ)8/3o
maxn
n3
(1‚àíœÅ)2,n
(1‚àíœÅ)8/3o
single-level algorithms. This is the first result demonstrating bilevel optimization essentially
subsumes the convergence of the single-level optimization.
Our main results are listed in Table 1. All SPARKLE variants achieve the state-of-the-art asymp-
totic rate, asymptotic gradient complexity, asymptotic communication cost, and transient iteration
complexity under more relaxed assumptions compared to existing methods.
Related works. A significant challenge in decentralized bilevel optimization is accurately estimating
the hyper-gradient ‚àáŒ¶(x), necessitating solving global lower-level problems and estimating Hessian
inversion. To this end, various decentralized techniques have been applied in bilevel optimization,
including Neumann series in [ 52], JHIP oracle in [ 9], HIGP oracle in [ 10], and augmented Lagrangian-
based communication in [ 33]. Additionally, reference [ 29] proposes a single-loop algorithm utilizing
decentralized SOBA. To enhance algorithmic robustness against data heterogeneity, recent studies
have employed Gradient Tracking (GT) in both lower- and upper-level optimization. However,
existing works built upon GT suffer from several limitations. Results of [ 16,9] concentrate solely on
deterministic cases, while reference [ 40] addresses personalized problems in the lower-level, which
do not require achieving global consensus in the lower-level problem. Moreover, [ 9,10] introduce
computationally expensive inner loops for GT steps. None of these works can establish smaller
transient iteration complexity than D-SOBA for decentralized SBO, even though the latter algorithm
employs no heterogeneity-correction technique.
The unified framework for single-level decentralized optimization has been extensively studied in the
literature. References [ 1,49,26] propose frameworks for decentralized composite optimization in
deterministic settings, while [ 2] investigates a framework under stochastic settings. However, none
of these works can be directly applied to decentralized bilevel algorithms. Several studies [ 21,57]
utilize variance reduction techniques to accelerate the convergence of stochastic decentralized bilevel
algorithms. Our proposed SPARKLE framework is orthogonal to variance reduction; it can also
incorporate variance-reduced gradient estimation to achieve improved convergence rates. More
relevant works on decentralized optimization and bilevel optimization are discussed in Appendix A.
Notations. We use lowercase letters to represent vectors and uppercase letters to represent matrices.
We introduce col{x1, ..., x n}:= [x‚ä§
1, ..., x‚ä§
n]‚ä§‚ààRpnfor brevity. Variables with overbar denote the
average over all agents. For example, ¬Øxk=Pn
i=1xk
i/n. We denote A=A‚àí1
n1n1‚ä§
nfor matrix
A‚ààRn√ón, where 1n‚ààRndenotes the n-dimensional vector with all entries being one. For a
function f(x, y) :Rp√óRq‚ÜíR, we use ‚àá1f(x, y)‚ààRp,‚àá2f(x, y)‚ààRqto represent its partial
gradients with respect to xandy, respectively. Similarly, ‚àá12f(x, y)‚ààRp√óq,‚àá22f(x, y)‚ààRq√óq
represent the corresponding Jacobian and Hessian matrix. We use the notation ‚â≤to denote inequalities
that hold up to constants related to the initialization of algorithms and smoothness constants.
2SPARKLE : A unified framework for decentralized bilevel optimization
This section develops SPARKLE , a unified framework for decentralized bilevel optimization, and
discusses its numerous variants by specifying certain hyper-parameters.
2.1 Three pillar subproblems in decentralized bilevel optimization.
When solving the upper-level problem (1a), it is critical to obtain the hyper-gradient ‚àáŒ¶(x), which
can be expressed as [22]
‚àáŒ¶(x) =‚àá1f(x, y‚ãÜ(x))‚àí ‚àá2
12g(x, y‚ãÜ(x))
‚àá2
22g(x, y‚ãÜ(x))‚àí1‚àá2f(x, y‚ãÜ(x)). (3)
4Evaluating this hyper-gradient is computationally expensive due to the inversion of the Hessian matrix.
This evaluation becomes even more challenging over a decentralized network of collaborative agents.
First, the inverse of the Hessian matrix cannot be obtained by simply averaging the local Hessian
inverses due to [1
nPn
i=1‚àá2
22gi(x, y‚ãÜ(x))]‚àí1Ã∏=1
nPn
i=1[‚àá2
22gi(x, y‚ãÜ(x))]‚àí1. Second, the global
averaging operation cannot be realized through decentralized communication. To overcome these
challenges, one can introduce an auxiliary variable z‚ãÜ(x):= [‚àá2
22g(x, y‚ãÜ(x))]‚àí1‚àá2f(x, y‚ãÜ(x))[12],
which is the solution to a quadratic problem
z‚ãÜ(x) = argmin
z‚ààRq1
2z‚ä§‚àá2
22g(x, y‚ãÜ(x))z‚àíz‚ä§‚àá2f(x, y‚ãÜ(x))
. (4)
Once z‚ãÜ(x)is derived by solving (4), we can substitute it into (3) to achieve ‚àáŒ¶(x).
Following this idea, solving the distributed bilevel optimization problem (1)essentially involves
solving three subproblems, where hi(x, y‚ãÜ(x), z) :=1
2z‚ä§‚àá2
22gi(x, y‚ãÜ(x))z‚àíz‚ä§‚àá2fi(x, y‚ãÜ(x)),
x‚ãÜ= argmin
x‚ààRp1
nnX
i=1fi(x, y‚ãÜ(x)), (upper-level) (5a)
y‚ãÜ(x) = argmin
y‚ààRq1
nnX
i=1gi(x, y), (lower-level) (5b)
z‚ãÜ(x) = argmin
z‚ààRq1
nnX
i=1hi(x, y‚ãÜ(x), z). (auxiliary-level) (5c)
Given the variable x, one can achieve y‚ãÜ(x)by solving the lower-level problem in (5b). With y‚ãÜ(x)
determined, z‚ãÜ(x)can be obtained by solving the auxiliary-level problem in (5c). Subsequently, with
z‚ãÜ(x)available, one can directly compute the hyper-gradient and solve the upper-level problem in
(5a) using gradient descent. This constitutes the primary methodology to solve problem (1).
A bilevel algorithm essentially solves three subproblems listed in (5), each formulated as a single-level
decentralized optimization problem. Nevertheless, primary approaches may suffer from nested loops
in algorithmic development. A few recent studies [ 12,11,57,29] propose to solve each problem in
(5a)-(5c)approximately with one single iteration, leading to practical single-loop bilevel algorithms.
For example, applying a D-SGD step [ 43] to each of (5a)-(5c)yields the D-SOBA method [ 29], while
further leveraging the GT technique leads to decentralized bilevel methods in [9, 16, 57, 21].
However, it is less explored whether numerous other heterogeneity-correction techniques [ 50,13,
38,56,30,54,46] beyond GT can be incorporated into algorithmic design to achieve even better
performance in bilevel optimization. To avoid exploring each case individually, we next introduce a
general framework that unifies all these techniques for solving single-level problems.
2.2 A unified framework for decentralized single-level optimization.
In this subsection, we consider solving the single-level problem minx‚ààRp1
nPn
i=1fi(x)over a
network of nnodes. For each k-th (k‚â•0) iteration, we let xk
idenote the local x-variable maintained
by the i-th agent. Furthermore, we associate the topology with a weight matrix W= [wij]n
i,j=1‚àà
Rn√ónin which wij‚àà(0,1)if node jis connected to node iotherwise wij= 0. We use bold symbols
to denote stacked vectors or matrices across agents. For example, xk=col{xk
1, ..., xk
n} ‚ààRpnand
W=W‚äóIp, where ‚äódenotes the Kronecker product operator.
A unified framework with moving average. Building on the formulation in [ 1,2], we develop a
unified primal-dual framework with moving average for decentralized optimization:
rk+1= (1‚àíŒ∏)rk+Œ∏gk,xk+1=Cxk‚àíŒ±Ark+1‚àíBdk,dk+1=dk+Bxk+1. (6)
Herexkdenotes the primal variable, dkdenotes the dual variable introduced to mitigate the influence
of data-heterogeneity, gkstacks all (stochastic) gradients evaluated at xk
ifor1‚â§i‚â§n,rkdenotes
the momentum introduced to boost training with coefficient Œ∏‚àà[0,1], andŒ± >0is the learning rate.
Matrices A,B,C‚ààRpn√ópnare adapted from the mixing matrix W, which determine how agents
communicate with each other. See Appendix B.1 for more detailed motivations.
Framework (6)unifies various decentralized techniques in the literature. For instance, by letting
Œ∏= 1and specifying A,B,Cdelicately, framework (6)reduces to ED, EXTRA, and numerous GT
5Algorithm 1 SPARKLE: A unified framework for decentralized stochastic bilevel optimization
Require: Initialize x0=y0=z0=r0=0,d0
x=d0
y=d0
z=0, learning rate Œ±k, Œ≤k, Œ≥k, Œ∏k.
fork= 0,1,¬∑¬∑¬∑, K‚àí1do
yk+1=Cyyk‚àíŒ≤kAyvk‚àíBydk
y,dk+1
y=dk
y+Byyk+1; ‚ñ∑lower-level update
zk+1=Czzk‚àíŒ≥kAzpk‚àíBzdk
z,dk+1
z=dk
z+Bzzk+1; ‚ñ∑auxiliary-level update
rk+1= (1‚àíŒ∏k)rk+Œ∏kuk; ‚ñ∑momentum update
xk+1=Cxxk‚àíŒ±kAxrk+1‚àíBxdk
x,dk+1
x=dk
x+Bxxk+1; ‚ñ∑upper-level update
end for
variants, see Table 3 and Appendix B.1 for more details. Framework (6)is closely related to the
unified decentralized method developed in [ 1,2]. The primary difference lies in the incorporation
of the momentum variable rk, which can help improve the transient iteration complexity of the
framework (6)and relax the smoothness condition for bilevel algorithms [ 11]. A detailed comparison
between framework (6) and that proposed in [1, 2] is provided in Appendix B.2.
2.3 A unified framework for decentralized bilevel optimization.
By utilizing the unified framework (6)to approximately solve each subproblem in (5)with only
one iteration , we achieve SPARKLE, a unified single-loop framework for decentralized bilevel
optimization. In particular, we independently sample data Œæk
i‚àº D fi,Œ∂k
i‚àº D giwithin each node at
iteration k, and evaluate stochastic gradients/Jacobians/Hessians as follows
lk
i=‚àá1Fi(xk
i, yk
i;Œæk
i), bk
i=‚àá2Fi(xk
i, yk
i;Œæk
i), vk
i=‚àá2Gi(xk
i, yk
i;Œ∂k
i),
Jk
i=‚àá2
12Gi(xk
i, yk
i;Œ∂k
i), Hk
i=‚àá2
22Gi(xk
i, yk
i;Œ∂k
i).
Next we stack the descent directions for variables of each level as follows
lower-level stochstic gradient: vk=col{vk
1, ..., vk
n},
auxilliary-level stochstic gradient: pk=col{Hk
1zk
1‚àíbk
1, ..., Hk
nzk
n‚àíbk
n},
upper-level stochstic gradient: uk=col{lk
1‚àíJk
1zk+1
1, ..., lk
n‚àíJk
nzk+1
n}.
The SPARKLE algorithm is detailed in Algorithm 1. In this algorithm, we utilize different dual
variables dsand communication matrices As,Bs,Csfor each variable s‚àà {x, y, z}to optimize
their respective objective functions. We use momentum rkonly for updating the upper-level variable,
which is sufficient to enhance convergence of bilevel algorithms and relax the smoothness condition.
Versatility in decentralized strategies. SPARKLE is highly versatile, supporting various decentral-
ized strategies by allowing the specification of different communication matrices As,Bs, andCs.
For example, by setting As=I,Bs= (I‚àíW)1/2, andCs=Wfor any s‚àà {x, y, z}, SPARKLE
will utilize EXTRA to update variables x,y, andz, resulting in the SPARKLE-EXTRA variant. Other
variants can be achieved by setting As,Bs, andCsaccording to Table 3. These variants can be
implemented more efficiently than listed in Algorithm 1, see Appendix B.3.
Flexibility across optimization levels. SPARKLE supports different optimization and communi-
cation mechanisms for each level of (5), which can be directly achieved by choosing different As,
Bs, andCsmatrices for each level s‚àà {x, y, z}. For example, SPARKLE can utilize GT to update
the upper-level variable xwhile employing ED to update the auxiliary- and lower-level variables y
andz. Throughout this paper, we denote SPARKLE using the decentralized mechanism Lfor the
lower-level and auxiliary variables, and Ufor the upper-level in Algorithm 1, by SPARKLE- L-U,
or simply SPARKLE- LifL=U. In addition, SPARKLE even supports utilizing different mixing
matrices Wx,Wy,Wzacross levels.
3 Convergence analysis
In this section, we establish the convergence properties of the SPARKLE framework and examine
the influence of different decentralized techniques utilized across optimization levels.
6Table 3: SPARKLE facilitates different decentralized techniques by specifying As,Bs,Csfors‚àà{x, y, z}.
We denote the stacked local variables and the associate gradients estimates by s‚àà{x,y,z}andg(s), respectively.
The update rule refers to the specific algorithmic recursion for each level. See derivations in Appendix B.2.
Algorithms As Bs CsThe specific update rule at the k-th iteration.
ED Ws(I‚àíWs)1
2Wssk+2=Ws 
2sk+1‚àísk‚àíŒ± 
g(sk+1)‚àíg(sk)
EXTRA I (I‚àíWs)1
2Wssk+2=Ws 
2sk+1‚àísk
‚àíŒ± 
g(sk+1)‚àíg(sk)
ATC-GT W2
sI‚àíWsW2
ssk+1=Ws 
sk‚àíŒ±hk
s
,hk+1
s=Ws 
hk
s+g(sk+1)‚àíg(sk)
Semi-ATC-GT WsI‚àíWsW2
ssk+1=Wssk‚àíŒ±hk
s,hk+1
s=Ws 
hk
s+g(sk+1)‚àíg(sk)
Non-ATC-GT I I ‚àíWsW2
ssk+1=Wssk‚àíŒ±hk
s,hk+1
s=Wshk
s+g(sk+1)‚àíg(sk)
3.1 Assumptions
Before presenting the theoretical guarantees, we first introduce the following assumptions used
throughout this paper.
Assumption 1. There exist constants ¬µg, Lf,0, Lf,1, Lg,1, Lg,2such that for any 1‚â§i‚â§n,
1.‚àáfi,‚àági,‚àá2giareLf,1, Lg,1, Lg,2Lipschitz continuous, respectively;
2.‚à•‚àá2fi(x, y‚ãÜ(x))‚à• ‚â§Lf,0for any x‚ààRp;3
3.gi(x, y)is¬µg-strongly convex with respect to yfor any fixed x‚ààRp.
Moreover, we define L:= max {Lf,0, Lf,1, Lg,1, Lg,2}andŒ∫:=L/¬µ g.
Assumption 2. For each s‚àà {x, y, z}, the corresponding mixing matrix Ws‚ààRn√ónis non-negative,
symmetric and doubly stochastic, i.e.,
Ws=W‚ä§
s, W s1n=1n,(Ws)ij‚â•0,‚àÄ1‚â§i, j‚â§n,
and the corresponding communication graph is strongly-connected, i.e., its eigenvalues satisfy
1 =Œª1(Ws)> Œª2(Ws)‚â•. . .‚â•Œªn(Ws)andœÅ(Ws):= max {|Œª2(Ws)|,|Œªn(Ws)|}<1.
The value 1‚àíœÅ(Ws)is referred to as the spectral gap in the literature [ 34,53,31] ofWs, which
measures the connectivity of the communication graph. It would approach 0for sparse networks. For
example, it holds that 1‚àíœÅ(Ws) = Œò(1 /n2)for the matrix Wsinduced by a ring graph.
Assumption 3. For any s‚àà {x, y, z}, we assume the communication matrices As, Bs, Csused in
SPARKLE are polynomial functions of Ws. Furthermore, we assume As, Csare doubly stochastic,
andNull( Bs) = Span {1n}. In addition, we assume all eigenvalues of the augmented matrix
Ls:=
Cs‚àíB2
sBs
‚àíBs In
are strictly less than one in magnitude, where Cs‚âúCs‚àí1
n1n1‚ä§
nandIn‚âúIn‚àí1
n1n1‚ä§
n.
We remark that Assumption 3 is mild and is satisfied by all choices listed in Table 3. See more
discussions in Appendix C.2.2.
Assumption 4. We assume ‚àáFi(x, y;Œæ),‚àáGi(x, y;Œæ), and‚àá2Gi(x, y;Œæ)to be unbiased estimates
of‚àáfi(x, y),‚àági(x, y), and‚àá2gi(x, y)with bounded variances œÉ2
f,1, œÉ2
g,1, œÉ2
g,2, respectively.
3.2 Convergence theorem
Under the above assumptions, we establish the convergence properties as follows. Proof details can
be found in Appendix C.
3This is more relaxed than Lipschitz continuous fi, or bounded ‚àá2fiin [21, 57, 33, 11].
7Theorem 1. Under Assumptions 1 ‚Äì 4, there exist proper constant step-sizes Œ±, Œ≤, Œ≥ and momentum
coefficient Œ∏, such that the SPARKLE framework listed in Algorithm 1 will converge as follow:
1
K+ 1KX
k=0E[‚à•‚àáŒ¶(¬Øxk)‚à•2]‚â≤Œ∫5œÉ‚àö
nK+Œ∫16
3(Œ¥y,1+Œ¥z,1)œÉ2
3
K2
3+Œ∫7
2Œ¥x,1œÉ1
2
K3
4
+
Œ∫26
5Œ¥y,2+Œ∫6Œ¥z,2œÉ2
5
K4
5+
Œ∫16
3Œ¥y,3+Œ∫14
3Œ¥z,3+Œ∫8
3Œ¥x,31
K+ 
Œ∫CŒ±+Œ∫4CŒ∏1
K,
where œÉ‚âúmax{œÉf,1, œÉg,1, œÉg,2},{Œ¥s,i}3
i=1are constants depending only on Ws,As,Bs,Csfor
s‚àà {x, y, z}, and CŒ±, CŒ∏are constants independent of K. See Lemma 17 for their detailed values.
In the deterministic scenario with œÉ= 0,SPARKLE converges at the rate O(1/K), see the formal
theorem and derivation in Appendix C.3. This recovers the rate in [ 15] under even milder assumptions.
Unlike reference [ 15], which only considers GT in the deterministic setting, SPARKLE is a unified
bilevel framework for the more general stochastic setting.
Linear speedup. According to Theorem 1, SPARKLE achieves an asymptotic linear speedup
asKapproaches infinity, which applies to all SPARKLE variants regardless of the decentralized
strategies employed and whether they are utilized at different optimization levels. Furthermore, the
asymptotically dominant term Œ∫5œÉ/(‚àö
nK)matches exactly with the single-node bilevel algorithm
SOBA [12] when n= 1, implying the tightness of Theorem 1 in terms of the asymptotic rate.
Remark 1. We establish an upper bound for the consensus error1
KKP
k=0Eh
‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
ni
.
Please refer to Lemma 19 in Appendix C.2.1 for more details.
3.3 Transient iteration complexity
With the non-asymptotic rate established in Theorem 1, we can derive the transient iteration complex-
ity of SPARKLE as follows. The proof is in Lemma 18.
Corollary 1. Under the same assumptions as in Theorem 1, the transient iteration complexity of
SPARKLE ‚Äîwith the influence of Œ∫andœÉ2omitted for brevity‚Äîis on the order of
maxn
n2Œ¥x, n3Œ¥y, n3Œ¥z, nÀÜŒ¥x, nÀÜŒ¥y, nÀÜŒ¥zo
, (8)
where Œ¥s,ÀÜŒ¥sonly depend Ws,As,Bs,Csfors‚àà {x, y, z}. Their values are in Lemma 18.
We obtain the transient iteration complexity of each variant of SPARKLE by applying Corollary 1.
Corollary 2. ForSPARKLE-ED andSPARKLE-EXTRA , if we choose Wy=Wz, it holds that
Œ¥x=O 
(1‚àíœÅ(Wx))‚àí2
, Œ¥ y=Œ¥z=O 
(1‚àíœÅ(Wy))‚àí2
,
ÀÜŒ¥x=O
(1‚àíœÅ(Wx))‚àí3
2
, ÀÜŒ¥y=ÀÜŒ¥z=O 
(1‚àíœÅ(Wy))‚àí2
.(9)
Furthermore, if we choose Wx=Wy=Wzand denote œÅ‚âúœÅ(Wx), the transient iteration
complexity derived in (8)can be simplified as n3/(1‚àíœÅ)2.
Corollary 3. ForSPARKLE-GT and its variants with semi/non-ATC-GT, if we let Wy=Wz,
Œ¥x=O 
(1‚àíœÅ(Wx))‚àí2
, Œ¥ y=Œ¥z=O 
(1‚àíœÅ(Wy))‚àí2
,
ÀÜŒ¥x=O 
(1‚àíœÅ(Wx))‚àí2
, ÀÜŒ¥y=ÀÜŒ¥z=O
(1‚àíœÅ(Wy))‚àí8
3
.
Furthermore, if we let Wx=Wy=Wzand denote œÅ‚âúœÅ(Wx), the transient iteration complexity
derived in (8)can be simplified as max{n3/(1‚àíœÅ)2, n/(1‚àíœÅ)8/3}.
Remark 2 (SOTA transient iterations) .Comparing with algorithms listed in Table 1, all SPARKLE
variants achieve smaller transient iteration complexity, implying that they can achieve linear speedup
much faster than the other algorithms, especially over sparse network topologies with 1‚àíœÅ‚Üí0.
Remark 3 (GT is not the best technique for decentralized SBO) .While GT is widely adopted in the
literature [ 16,21,57] to facilitate decentralized SBO, a comparison of Corollary 2 and 3 reveals that
both SPARKLE-EXTRA andSPARKLE-ED outperform SPARKLE-GT in terms of transient
iteration complexity. This implies that EXTRA and ED are better than GT for decentralized SBO.
83.4 Different strategies across optimization levels
Corollary 1 clarifies how different update strategies for x,y, and zimpact the transient iterations
through constants {Œ¥s,ÀÜŒ¥s}fors‚àà {x, y, z}. Since Œ¥y=Œ¥zandÀÜŒ¥y=ÀÜŒ¥zwhen Wy=Wz
(Lemma 18), we naturally employ the same strategy to update yandz. The following corollary
studies the utilization of both ED and GT in SPARKLE . See the transient iterations complexity of
other mixed strategies in Appendix C.2.4 and Table 2.
Corollary 4. ForSPARKLE-ED-GT which uses ED to update yandzand GT to update x, if
Wx=Wy=Wzand we denote œÅ=œÅ(Wx), it then holds that
Œ¥x=Œ¥y=Œ¥z=O 
(1‚àíœÅ)‚àí2
, ÀÜŒ¥x=ÀÜŒ¥y=ÀÜŒ¥z=O 
(1‚àíœÅ)‚àí2
,
which implies that the transient iteration complexity in (8)can be simplified as n3/(1‚àíœÅ)2.
Remark 4 (Mixed strategies outperform employing GT only) .Comparing Corollary 3 and 4, we find
that using ED to update yandzwill lead to smaller ÀÜŒ¥yandÀÜŒ¥z, which improves the transient iteration
complexity compared to employing GT only in all optimization levels (see Corollary 3) .
3.5 Different topologies across optimization levels
In SPARKLE, we can utilize different topologies across levels. Theorem 1 and Corollary 1 have
clarified the influence of using different topologies across levels through the constants {Œ¥s,ÀÜŒ¥s}for
s‚àà {x, y, z}. For instance, when substituting {Œ¥s,ÀÜŒ¥s}established in (9)into(8), SPARKLE-ED has
the following transient iteration complexity:
max{n2(1‚àíœÅ(Wx))‚àí2, n3(1‚àíœÅ(Wy))‚àí2}
where Wxis the mixing matrix for updating x, while Wyis for updating yandz. As long as
(1‚àíœÅ(Wx))‚àí1‚â≤‚àön(1‚àíœÅ(Wy))‚àí1holds, SPARKLE-ED retains the transient iteration complexity
ofn3(1‚àíœÅ(Wy))‚àí2, which allows for the utilization of a sparser network topology when updating
x, thereby reducing communication overheads. Consequently, the ratio aof the communication
volume per round for the variables xandycan be significantly less than one. See Appendix C.2.3 for
discussion on how to use different topologies across levels in other SPARKLE variants.
3.6 Recovering single-level decentralized optimization
Previous works typically study single-level and bilevel optimization separately. By taking
Gi(x, y, Œæ )‚â° |y|2/2andFi(x, y, œï ) =Fi(x, œï)into (2), the decentralized SBO problem (1)re-
duces to stochastic single-level optimization. By setting zk‚â°0,yk‚â°0,uk
i=‚àá1fi(xk
i, Œæk
i),
SPARKLE reduces to the single-level framework (6), whose convergence can be naturally guaranteed
by Theorem 1. Please refer to Appendix C.4 for the detailed proof and results. This is the first
result demonstrating that bilevel optimization essentially subsumes the convergence of single-level
optimization.
4 Numerical experiments
In this section, we present experiments to validate our theoretical findings. We first explore how
update strategies and network structures influence the convergence of SPARKLE . Then we com-
pare SPARKLE to the existing decentralized SBO algorithms. Additional experiments about a
decentralized SBO problem with synthetic data are in Appendix D.1.
Hyper-cleaning on FashionMNIST dataset. We consider a data hyper-cleaning problem [ 44] on
a corrupted FashionMNIST dataset [ 48]. Problem formulations and experimental setups can be
found in Appendix D.2. Firstly, we equip SPARKLE with different decentralized strategies in
different optimization levels and then compare them with D-SOBA [ 29], MA-DSBO-GT [ 10], and
MDBO [ 21] using the corruption rate p= 0.1,0.2,0.3, respectively. As is shown in Figure 1, all
theSPARKLE -based algorithms generally achieve higher test accuracy than D-SOBA, while ED
and EXTRA especially outperform GT. Meanwhile, using mixed strategies ( i.e.,SPARKLE -ED-GT
andSPARKLE -EXTRA-GT) achieves similar test accuracy with SPARKLE -ED and SPARKLE -
EXTRA and outperform SPARKLE -GT, respectively. These observations match with the theoretical
results in Corollary 2-4 and Remark 3, 4.
90 250 500 750 1000 1250 1500 1750 2000
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
SPARKLE/uni00ADED/uni00ADGTSPARKLE/uni00ADEXTRA/uni00ADGT
D/uni00ADSOBA
MA/uni00ADDSBO/uni00ADGT
MDBO
0 500 1000 1500 2000 2500 3000
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
SPARKLE/uni00ADED/uni00ADGTSPARKLE/uni00ADEXTRA/uni00ADGT
D/uni00ADSOBA
MA/uni00ADDSBO/uni00ADGT
MDBO
0 500 1000 1500 2000 2500 3000 3500
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
SPARKLE/uni00ADED/uni00ADGTSPARKLE/uni00ADEXTRA/uni00ADGT
D/uni00ADSOBA
MA/uni00ADDSBO/uni00ADGT
MDBOFigure 1: The test accuracy on hyper-cleaning with various SPARKLE -based algorithms using different
corruption rates p. (Left: p= 0.1, Middle: p= 0.2, Right: p= 0.3.)
0 500 1000 1500 2000 2500 3000 3500
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy(a)SPARKLE-EXTRA; Fixed topo x; Varied topo of y,z
2200 2400 2600 2800 3000 3200 34000.650.700.750.80
=0.647
=0.828
=0.924
=0.990
0 500 1000 1500 2000 2500 3000 3500
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy(b)SPARKLE-EXTRA; Fixed topo y,z; Varied topo of x
2200 2400 2600 2800 3000 3200 34000.650.700.750.80
=0.647
=0.828
=0.924
=0.990
Figure 2: Test accuracy of SPARKLE-EXTRA on
hyper-cleaning. (Left: fixed graph for xand varying
graph for y, z; Right: fixed for y, zand varying for x)
0 250 500 750 1000 1250 1500 1750 2000
sample size0.00.51.01.52.02.53.03.54.0upper level loss1600 1700 1800 19000.270.280.29SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRASLDBO
MDBO
0 250 500 750 1000 1250 1500 1750 2000
sample size0.00.51.01.52.02.53.03.54.0upper level loss1600 1700 1800 19000.320.33SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRASLDBO
MDBOFigure 3: The upper-level loss against samples gener-
ated by one agent of different algorithms in the policy
evaluation. (Left: n= 10 , Right: n= 20 .)
Next, we test SPARKLE -EXTRA with two communication strategies including fixed topology for
updating xand varying topology for y, z, and fixed topology for updating y, zand varying topology
forx. As illustrated in Figure 2, maintaining a fixed topology for xwhile reducing the connectivity
of the topology for yandzwill deteriorate the algorithmic performance. Conversely, preserving the
topology for yandzwhile decreasing the connectivity for xhas little impact on the performance.
This suggests that the influence of the network topology for yandzon the algorithm dominates
over the topology for x, which is consistent with our discussion in Section 3.5. We also numerically
examine the influence of moving average on convergence, see discussions in Appendix D.2.
Distributed policy evaluation in reinforcement learning. We consider a multi-agent MDP problem
in reinforcement learning on a distributed setting with n‚àà {10,20}agents respectively, which can
be formulated as a decentralized SBO problems [ 52]. Here, we compare SPARKLE with existing
decentralized SBO approaches including MDBO [ 21] and the stochastic extension of SLDBO [ 16]
over a Ring graph. Figure 3 illustrates that SPARKLE converges faster and achieves a lower sample
complexity than the other baselines, especially when n= 20 , which shows the empirical benefits of
SPARKLE in decentralized SBO algorithms with a large number of agents and sparse communication
modes. More experimental details are in Appendix D.3.
Decentralized meta-learning. We investigate decentralized meta-learning on miniImageNet [ 47]
with multiple tasks [ 18], formulating it as a decentralized bilevel optimization problem. This approach
minimizes the validation loss with respect to shared parameters as the upper-level loss, while the
training loss is managed by task-specific parameters at the lower level. Additional details about
the experiment can be found in Appendix D.4. Our method, SPARKLE , is benchmarked against
D-SOBA [29] and MAML [18], demonstrating a significant improvement in training accuracy.
5 Conclusions and limitations
This paper proposes SPARKLE, a unified single-loop primal-dual framework for decentralized
stochastic bilevel optimization. Being highly versatile, SPARKLE can support different decentralized
mechanisms and topologies across optimization levels. Moreover, all SPARKLE variants have been
demonstrated to achieve state-of-the-art convergence rate compared to existing algorithms. However,
SPARKLE currently supports only strongly-convex problems in the lower-level optimization. Its
compatibility with generally-convex lower-level problems remains unknown. Additionally, the
condition number of the lower-level problem significantly impacts the performance, as is the case
with existing bilevel algorithms. We aim to address these limitations in future work.
106 Acknowledgment
The work of Shuchen Zhu, Boao Kong, and Kun Yuan is supported by Natural Science Foundation
of China under Grants 92370121, 12301392, and W2441021. This work is also supported by Open
Project of Key Laboratory of Mathematics and Information Networks, Ministry of Education, China.
No. KF202302.
References
[1]S. A. Alghunaim, E. K. Ryu, K. Yuan, and A. H. Sayed. Decentralized proximal gradient algorithms with
linear convergence rates. IEEE Transactions on Automatic Control , 66(6):2787‚Äì2794, 2020.
[2]S. A. Alghunaim and K. Yuan. A unified and refined convergence analysis for non-convex decentralized
learning. IEEE Transactions on Signal Processing , 2022.
[3]S. Arora, S. Du, S. Kakade, Y . Luo, and N. Saunshi. Provable representation learning for imitation learning
via bi-level optimization. In International Conference on Machine Learning , pages 367‚Äì376. PMLR, 2020.
[4]L. Bertinetto, J. Henriques, P. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers.
InInternational Conference on Learning Representations (ICLR), 2019 . International Conference on
Learning Representations, 2019.
[5]T.-H. Chang, M. Hong, and X. Wang. Multi-agent distributed optimization via inexact consensus admm.
IEEE Transactions on Signal Processing , 63(2):482‚Äì497, 2014.
[6]J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and learning over
networks. IEEE Transactions on Signal Processing , 60(8):4289‚Äì4305, 2012.
[7]T. Chen, Y . Sun, Q. Xiao, and W. Yin. A single-timescale method for stochastic bilevel optimization. In
International Conference on Artificial Intelligence and Statistics , pages 2466‚Äì2488. PMLR, 2022.
[8]T. Chen, Y . Sun, and W. Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods
for bilevel problems. Advances in Neural Information Processing Systems , 34:25294‚Äì25307, 2021.
[9]X. Chen, M. Huang, and S. Ma. Decentralized bilevel optimization. arXiv preprint arXiv:2206.05670 ,
2022.
[10] X. Chen, M. Huang, S. Ma, and K. Balasubramanian. Decentralized stochastic bilevel optimization with
improved per-iteration complexity. In International Conference on Machine Learning , pages 4641‚Äì4671.
PMLR, 2023.
[11] X. Chen, T. Xiao, and K. Balasubramanian. Optimal algorithms for stochastic bilevel optimization under
relaxed smoothness conditions. arXiv preprint arXiv:2306.12067 , 2023.
[12] M. Dagr√©ou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables
stochastic and global variance reduction algorithms. Advances in Neural Information Processing Systems ,
35:26698‚Äì26710, 2022.
[13] P. Di Lorenzo and G. Scutari. Next: In-network nonconvex optimization. IEEE Transactions on Signal
and Information Processing over Networks , 2(2):120‚Äì136, 2016.
[14] J. Domke. Generic methods for optimization-based modeling. In Artificial Intelligence and Statistics ,
pages 318‚Äì326. PMLR, 2012.
[15] J. Dong, Z. Cao, T. Zhang, J. Ye, S. Wang, F. Feng, L. Zhao, et al. Eflops: Algorithm and system co-design
for a high performance distributed training platform. In 2020 IEEE International Symposium on High
Performance Computer Architecture (HPCA) , pages 610‚Äì622, 2020.
[16] Y . Dong, S. Ma, J. Yang, and C. Yin. A single-loop algorithm for decentralized bilevel optimization. arXiv
preprint arXiv:2311.08945 , 2023.
[17] J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization: Convergence
analysis and network scaling. IEEE Transactions on Automatic control , 57(3):592‚Äì606, 2011.
[18] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
International Conference on Machine Learning , pages 1126‚Äì1135, 2017.
11[19] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter
optimization and meta-learning. In International Conference on Machine Learning , pages 1568‚Äì1577.
PMLR, 2018.
[20] B. Gao, Y . Yang, and Y . xiang Yuan. Lancbio: dynamic lanczos-aided bilevel optimization via krylov
subspace. arXiv preprint arXiv:2404.03331 , 2024.
[21] H. Gao, B. Gu, and M. T. Thai. On the convergence of distributed stochastic bilevel optimization algorithms
over a network. In International Conference on Artificial Intelligence and Statistics , pages 9238‚Äì9281.
PMLR, 2023.
[22] S. Ghadimi and M. Wang. Approximation methods for bilevel programming. arXiv preprint
arXiv:1802.02246 , 2018.
[23] R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation.
InInternational Conference on Machine Learning , pages 3748‚Äì3758. PMLR, 2020.
[24] Z. Guo, Q. Hu, L. Zhang, and T. Yang. Randomized stochastic variance-reduced methods for multi-task
stochastic bilevel optimization. arXiv preprint arXiv:2105.02266 , 2021.
[25] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A two-timescale stochastic algorithm framework for
bilevel optimization: Complexity analysis and application to actor-critic. SIAM Journal on Optimization ,
33(1):147‚Äì180, 2023.
[26] D. Jakoveti ¬¥c. A unification and generalization of exact distributed first-order methods. IEEE Transactions
on Signal and Information Processing over Networks , 5(1):31‚Äì46, 2018.
[27] K. Ji, J. Yang, and Y . Liang. Bilevel optimization: Convergence analysis and enhanced design. In
International Conference on Machine Learning , pages 4882‚Äì4892. PMLR, 2021.
[28] A. Koloskova, T. Lin, and S. U. Stich. An improved analysis of gradient tracking for decentralized machine
learning. Advances in Neural Information Processing Systems , 34:11422‚Äì11435, 2021.
[29] B. Kong, S. Zhu, S. Lu, X. Huang, and K. Yuan. Decentralized bilevel optimization over graphs: Loopless
algorithmic update and transient iteration complexity. arXiv preprint arXiv:2402.03167 , 2024.
[30] Z. Li, W. Shi, and M. Yan. A decentralized proximal-gradient method with network independent step-sizes
and separated convergence rates. IEEE Transactions on Signal Processing , July 2019. early acces. Also
available on arXiv:1704.07807.
[31] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized algorithms outperform
centralized algorithms? A case study for decentralized parallel stochastic gradient descent. In Advances in
Neural Information Processing Systems , pages 5330‚Äì5340, 2017.
[32] T. Lin, S. P. Karimireddy, S. U. Stich, and M. Jaggi. Quasi-global momentum: Accelerating decentralized
deep learning on heterogeneous data. In International Conference on Machine Learning , 2021.
[33] S. Lu, S. Zeng, X. Cui, M. Squillante, L. Horesh, B. Kingsbury, J. Liu, and M. Hong. A stochastic linearized
augmented lagrangian method for decentralized bilevel optimization. Advances in Neural Information
Processing Systems , 35:30638‚Äì30650, 2022.
[34] Y . Lu and C. De Sa. Optimal complexity in decentralized training. In International Conference on Machine
Learning , pages 7111‚Äì7123. PMLR, 2021.
[35] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible
learning. In International Conference on Machine Learning , pages 2113‚Äì2122. PMLR, 2015.
[36] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to
adversarial attacks. In International Conference on Learning Representations , 2018.
[37] A. Nedi ¬¥c, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation tradeoffs
in decentralized optimization. Proceedings of the IEEE , 106(5):953‚Äì976, 2018.
[38] A. Nedic, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimization over
time-varying graphs. SIAM Journal on Optimization , 27(4):2597‚Äì2633, 2017.
[39] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transac-
tions on Automatic Control , 54(1):48‚Äì61, 2009.
12[40] Y . Niu, J. Xu, Y . Sun, Y . Huang, and L. Chai. Distributed stochastic bilevel optimization: Improved
complexity and heterogeneity analysis. arXiv preprint arXiv:2312.14690 , 2023.
[41] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transactions on
Control of Network Systems , 5(3):1245‚Äì1260, 2018.
[42] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer
Vision , 115:211‚Äì252, 2015.
[43] A. H. Sayed. Adaptive networks. Proceedings of the IEEE , 102(4):460‚Äì497, 2014.
[44] A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization.
InThe 22nd International Conference on Artificial Intelligence and Statistics , pages 1723‚Äì1732. PMLR,
2019.
[45] W. Shi, Q. Ling, G. Wu, and W. Yin. EXTRA: An exact first-order algorithm for decentralized consensus
optimization. SIAM Journal on Optimization , 25(2):944‚Äì966, 2015.
[46] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu. D2: Decentralized training over decentralized data. In
International Conference on Machine Learning , pages 4848‚Äì4856, 2018.
[47] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning. Advances
in Neural Information Processing Systems , 29, 2016.
[48] H. Xiao, K. Rasul, and R. V ollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
[49] J. Xu, Y . Tian, Y . Sun, and G. Scutari. Distributed algorithms for composite optimization: Unified
framework and convergence analysis. IEEE Transactions on Signal Processing , 69:3555‚Äì3570, 2021.
[50] J. Xu, S. Zhu, Y . C. Soh, and L. Xie. Augmented distributed gradient methods for multi-agent optimization
under uncoordinated constant stepsizes. In IEEE Conference on Decision and Control (CDC) , pages
2055‚Äì2060, Osaka, Japan, 2015.
[51] J. Yang, K. Ji, and Y . Liang. Provably faster algorithms for bilevel optimization. Advances in Neural
Information Processing Systems , 34:13670‚Äì13682, 2021.
[52] S. Yang, X. Zhang, and M. Wang. Decentralized gossip-based stochastic bilevel optimization over
communication networks. Advances in Neural Information Processing Systems , 35:238‚Äì252, 2022.
[53] K. Yuan, S. A. Alghunaim, and X. Huang. Removing data heterogeneity influence enhances network
topology dependence of decentralized SGD. Journal of Machine Learning Research , 24(280):1‚Äì53, 2023.
[54] K. Yuan, S. A. Alghunaim, B. Ying, and A. H. Sayed. On the influence of bias-correction on distributed
stochastic optimization. IEEE Transactions on Signal Processing , 2020.
[55] K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal on
Optimization , 26(3):1835‚Äì1854, 2016.
[56] K. Yuan, B. Ying, X. Zhao, and A. H. Sayed. Exact dffusion for distributed optimization and learning ‚Äì
Part I: Algorithm development. IEEE Transactions on Signal Processing , 67(3):708 ‚Äì 723, 2018.
[57] Y . Zhang, M. T. Thai, J. Wu, and H. Gao. On the communication complexity of decentralized bilevel
optimization. arXiv preprint arXiv:2311.11342 , 2023.
13Appendix for ‚ÄúSPARKLE: A Unified Single-Loop
Primal-Dual Framework for Decentralized Bilevel
Optimization‚Äù
Contents
A More related works 15
B More details of SPARKLE 15
B.1 Primal-dual deviation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
B.2 Specific instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.3 Implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
C Convergence analysis 18
C.1 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.1.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.1.2 Basic transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C.1.3 Proof sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.1.4 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.1.5 Descent lemmas for the upper-level . . . . . . . . . . . . . . . . . . . . . 23
C.1.6 Descent lemmas for the lower- and auxiliary-level . . . . . . . . . . . . . 30
C.1.7 Consensus error analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.1.8 Proof of the main theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 47
C.2 Analysis of consensus error and transient iteration complexity . . . . . . . . . . . 54
C.2.1 Consensus Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
C.2.2 Essential matrix norms for analysis . . . . . . . . . . . . . . . . . . . . . 59
C.2.3 Theoretical gap between upper-level and lower-level . . . . . . . . . . . . 59
C.2.4 The transient iteration complexities of some specific examples in SPARKLE. 60
C.3 Convergence analysis in deterministic scenarios . . . . . . . . . . . . . . . . . . . 61
C.4 Degenerating to single-level algorithms . . . . . . . . . . . . . . . . . . . . . . . 62
D Experimental details 64
D.1 Synthetic bilevel optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
D.2 Hyper-cleaning on FashionMNIST dataset . . . . . . . . . . . . . . . . . . . . . . 65
D.3 Distributed policy evaluation in reinforcement learning . . . . . . . . . . . . . . . 68
D.4 Decentralized meta-learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
14A More related works
Bilevel optimization. Bilevel optimization presents substantial difficulties compared to single-level
optimization due to its nested structure. Estimating hyper-gradient ‚àáŒ¶(x)of the upper level involves
solving lower-level problems and estimating the Hessian inverse, which requires additional calcu-
lations. Many algorithms and techniques have been proposed to solve the challenge. Approximate
Implicit Differentiation (AID)-based algorithms [ 14,22,23,27] leverage the implicit gradient form of
‚àáŒ¶(x), which entails solving a linear system to obtain the Hessian-inverse-vector product. Similarly,
[8,25] utilize the Neumann series to handle the Hessian inverse. Iterative Differentiation (ITD)-based
algorithms [ 19,35,14,23,27] use iterative methods solving the lower-level problem and then es-
timate the hyper-gradient through automatic differentiation. However, these approaches introduce
inner steps, leading to extra computational overhead and memory spaces. [ 12] proposes a single-
level algorithm called SOBA, which approximating the Hessian-inverse-vector product by solving
a quadratic programming problem. A recent work [ 20] utilizes the Krylov subspace technique and
the Lanczos process to approximate it in deterministic scenarios. For stochastic bilevel optimization,
various methods have been employed to improve the convergence rate, such as momentum [ 7,11]
and variance reduction [51, 27, 24].
Decentralized optimization. Decentralized optimization is developed to deal with large-scale
optimization problems, where datasets are distributed among multiple agents. Without a central
server, each agent only gets access to its own local data and communications are limited to its
neighbors in a network. Compared with centralized algorithms, decentralized ones preserve data
privacy, and are more robust to contingencies in the communication network. However, due to the
absence of a central server, decentralized optimization requires communication among agents, posing
greater challenges for convergence, especially in the presence of severe data heterogeneity. To tackle
this issue, various algorithms have emerged, such as decentralized gradient descent [ 39,55], diffusion
strategies [ 6], dual averaging [ 17], EXTRA [ 45], Exact Diffusion (a.k.a. D2) [56,30,46], gradient
tracking [50, 13, 38], and decentralized ADMM [5]. In stochastic scenarios, a common method for
decentralized optimization is the decentralized stochastic gradient descent (DSGD), which has gained
a lot of attentions recently. It has been proved to achieve linear speedup asymptotically and shares
the same asymptotic rate with centralized stochastic gradient descent [31].
B More details of SPARKLE
B.1 Primal-dual deviation
Here we provide a detailed motivation of the update framework (6)for decentralized single-level
algorithms. First, we rewrite the single-level distributed optimization problem in the following
equivalent form:
min
xi‚ààRdf(x1, ..., x n) =1
nnX
i=1fi(xi),s.t.x1=...=xn, (10)
where each fiis smooth and possibly non-convex. To simplify the notation, we assume that d= 1
without loss of generality. Now we introduce three symmetric matrices A, B, D such that Ais a
doubly stochastic communication matrix with œÅ(A)<1, and B, D satisfy NullB= Null D=
Span{1n}. In general, B(D) determines the topology of a connected graph GB(GD) over agents.
The constraint Bx= 0(Dx= 0) is equivalent to:
xi=xjifxi, xjare adjacent in GB(GD).
To simplify the derivation, we additionally assume that A, B, D are pairwise commutative. Then for
x= (x1, ..., x n), we have:
x1=...=xn‚áîBx= 0‚áîDx= 0‚áîAx=x.
Therefore, (10) can be equivalently reformulated as
min
x‚ààRnf(Ax),s.t.Bx= 0. (11)
15We construct the augmented Lagrangian function of the problem (11) as follows:
LœÅ(x, d) =f(Ax) +‚ü®d, Bx‚ü©+œÅ
2‚à•Dx‚à•2,
where xdenotes the primal variable, ddenotes the dual variable or Lagrangian multiplier associated
with the consensus constraint, ‚à•Dx‚à•2serves as the penalty term measuring the deviation from
Dx= 0, or equivalently Bx= 0;œÅ >0is the penalty coefficient. Though the introduction of
matrices A, D is essentially a matter of equivalent substitution, it enhances the universality of the
algorithm framework we get.
Following classical primal-dual methods, we alternately perform gradient descent on xand gradient
ascent on din the k-th iteration:
xk+1=xk‚àíŒ±(A‚àáf(Axk) +Bdk+œÅD2xk), dk+1=dk+Œ≤Bxk+1,
where Œ±, Œ≤ denote the step-sizes. By making the change of variables
ÀÜxk=Axk,ÀÜdk=rŒ±
Œ≤Adk,bB=p
Œ±Œ≤B,bC=I‚àíŒ±œÅD2,bA=A2,
we obtain
ÀÜxk+1=bCÀÜxk‚àíŒ±bA‚àáf(ÀÜxk)‚àíbBÀÜdk,ÀÜdk+1=ÀÜdk+bBÀÜxk+1. (12)
One should note that the definition implies that bA,bCare doubly stochastic communication matrices
under appropriate selections of Œ±, œÅ. Finally, thanks to the introduction of moving-average iteration
of(12), we can obtain the framework (6)which serves as the foundation for our algorithm design.
See more details in Section 2.3.
B.2 Specific instances
Relation to some existing single-level algorithm frameworks According to (12), our framework
at single-level is
xk+1=Cxk‚àíŒ±Agk‚àíBdk,dk+1=dk+Bxk+1, k= 0,1, ... (13)
where Œ±is the step-size, gkdenotes the estimated gradient at the k-th iteration, dserves as the dual
variable.
Replacing CwithCA, we get UDA[1] , and equivalently, SUDA [2]:
xk+1=CAxk‚àíŒ±Agk‚àíBdk,dk+1=dk+Bxk+1, k= 0,1, ...
Therefore, following SUDA, we can also recover some common state-of-the-art heterogeneity
methods as follows by selecting specific A,B,C. First, from (13) we get
xk+2‚àíxk+1=C(xk+1‚àíxk)‚àíŒ±A 
gk+1‚àígk
‚àíB 
dk+1‚àídk
=C 
xk+1‚àíxk
‚àíŒ±A 
gk+1‚àígk
‚àíB2xk+1.
Thus, for k‚â•0we have
xk+2= 
I‚àíB2+C
xk+1‚àíCxk‚àíŒ±A 
gk+1‚àígk
,
withx1=Cx0‚àíŒ±Ag0.
Some specific instances We next show that how to choose A,B,Cto get some common hetero-
geneity methods.
‚Ä¢ ED: Taking A=W,B= (I‚àíW)1/2andC=W, we get ED:
xk+2=W 
2xk+1‚àíxk‚àíŒ± 
gk+1‚àígk
,
withx1=W 
x0‚àíŒ±g0
.
16‚Ä¢ EXTRA: Taking A=I,B= (I‚àíW)1/2withC=W, we get EXTRA:
xk+2=W 
2xk+1‚àíxk
‚àíŒ± 
gk+1‚àígk
,
andx1=Wx0‚àíŒ±g0.
‚Ä¢ Adapt-then-combine gradient tracking (ATC-GT): The iteration of ATC-GT is
xk+1=W 
xk‚àíŒ±hk
,hk+1=W 
hk+gk+1‚àígk
withh0=Wg0,x0=Wx0(x0
1=...=x0
n). It follows that for k‚â•0
xk+2‚àíWxk+1=Wxk+1‚àíW2xk‚àíŒ±W 
hk+1‚àíWhk
.
Then we obtain
xk+2= 2Wxk+1‚àíW2xk‚àíŒ±W2 
gk+1‚àígk
,
withx1=W2x0‚àíŒ±W2g0. Thus, we can take A=W2,B= (I‚àíW)2,C=W2to
implement ATC-GT.
‚Ä¢ Semi-ATC-GT: The iteration of Semi-ATC-GT is
xk+1=W 
xk‚àíŒ±hk
,hk+1=Whk+gk+1‚àígk
withh0=Wg0,x0=Wx0(x0
1=...=x0
n). Like ATC-GT, we have
xk+2= 2Wxk+1‚àíW2xk‚àíŒ±W 
gk+1‚àígk
,
withx1=W2x0‚àíŒ±Wg0. Thus, we can take A=W,B= (I‚àíW)2,C=W2to
implement semi-ATC-GT.
‚Ä¢ Non-ATC-GT: The iteration of Non-ATC-GT is
xk+1=Wxk‚àíŒ±hk,hk+1=Whk+gk+1‚àígk
withh0=Wg0,x0=Wx0(x0
1=...=x0
n). We have
xk+2= 2Wxk+1‚àíW2xk‚àíŒ± 
gk+1‚àígk
,
withx1=W2x0‚àíŒ±g0. Thus, we can take A=I,B= (I‚àíW)2,C=W2to implement
Non-ATC-GT.
B.3 Implementation details
Given the update method L, we update the lower-level variable yat the k-th (k‚â•0) iteration as
follows. For brevity, we define y‚àí1
i=y0
i, v‚àí1
i= 0, o0
i=Pn
j=1(Wy)ijv0
j.
Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥yk+1
i=Pn
j=1(Wy)ij 
2yk
j‚àíyk‚àí1
j‚àíŒ≤k 
vk
i‚àívk‚àí1
i
ifL=ED
yk+1
i=Pn
j=1(Wy)ij 
2yk
j‚àíyk‚àí1
j
‚àíŒ≤k 
vk
i‚àívk‚àí1
i
ifL=EXTRA
yk+1
i=Pn
j=1(Wy)ij 
yk
j‚àíŒ≤kok
j
, ok+1
i=Pn
j=1(Wy)ij 
ok
j+vk+1
i‚àívk
i
ifL=GT
¬∑¬∑¬∑ others
(14)
Similarly, we update the auxiliary variable zat the k-th (k‚â•0) iteration as follows. For brevity, we
define z‚àí1
i=z0
i, p‚àí1
i= 0, h0
i=Pn
j=1(Wz)ijp0
j. Note that we use the same method Lto update z
as we do for the lower-level variable y.
Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥zk+1
i=Pn
j=1(Wz)ij 
2zk
j‚àízk‚àí1
j‚àíŒ≥k 
pk
i‚àípk‚àí1
i
ifL=ED
zk+1
i=Pn
j=1(Wz)ij 
2zk
j‚àízk‚àí1
j
‚àíŒ≥k 
pk
i‚àípk‚àí1
i
ifL=EXTRA
zk+1
i=Pn
j=1(Wz)ij 
zk
j‚àíŒ≥khk
j
, hk+1
i=Pn
j=1(Wz)ij 
hk
j+pk+1
i‚àípk
i
ifL=GT
¬∑¬∑¬∑ others
(15)
Given the update method U, we update the upper-level variable xat the k-th (k‚â•0) iteration as
follows. For brevity, we define x‚àí1
i=x0
i, t0
i=Pn
j=1(Wy)ijr1
j.
17Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥xk+1
i=Pn
j=1(Wx)ij 
2xk
j‚àíxk‚àí1
j‚àíŒ±k 
rk+1
i‚àírk
i
ifU=ED
xk+1
i=Pn
j=1(Wx)ij 
2xk
j‚àíxk‚àí1
j
‚àíŒ±k 
rk+1
i‚àírk
i
ifU=EXTRA
xk+1
i=Pn
j=1(Wx)ij 
xk
j‚àíŒ±ktk
j
, tk+1
i=Pn
j=1(Wx)ij 
tk
j+rk+2
i‚àírk+1
i
ifU=GT
¬∑¬∑¬∑ others
(16)
Then the practical implementation of SPARKLE with mixed strategies is
Algorithm 2 SPARKLE- L-U
Require: Initialize x0
i=y0
i=z0
i=r0
i= 0, step-sizes Œ±k, Œ≤k, Œ≥k, Œ∏k.
fork= 0,1,¬∑¬∑¬∑, K‚àí1, each agent i(in parallel) do
Update yk+1
iaccording to (14);
Update zk+1
iaccording to (15);
rk+1
i= (1‚àíŒ∏k)rk
i+Œ∏kuk
i;
Update xk+1
iaccording to (16).
end for
C Convergence analysis
C.1 Proof of Theorem 1
C.1.1 Notations
We use lowercase letters to represent vectors and uppercase letters to represent matrices. Stacked
vectors [x‚ä§
1, ..., x‚ä§
n]‚ä§is denoted by col{x1, ..., x n}for brevity. We denote a block diagonal matrix
with diagonal block Mi(1‚â§i‚â§l)byblkdiag {M1, ..., M l}, and a diagonal matrix with diagonal
elements di(1‚â§i‚â§k)bydiag{d1, ..., d k}. The Kronecker product operator is denoted by ‚äó. For a
variable v, we use vk
ito represent its components at k-th iteration and i-th agent.
Moreover, we use an overbar above an iterator to denote the average over all agents. For example,
¬Øxk=Pn
i=1xk
i/n. Upright bold symbols are used to denote stacked vectors or matrices across
agents. For example, xk:=col{xk
1, ..., xk
n},¬Øxk:=col{¬Øxk, ...,¬Øxk}(ntimes ),Wx:=Wx‚äóIdim(x).
Denote the 2-norm of a matrix by ‚à• ¬∑ ‚à•.
Next, we define following œÉ-fields which will be used in our convergence analysis:
Fk=œÉ 
y0, . . . ,yk+1,z0, . . . ,zk+1,x0, . . . ,xk,r0, . . . ,rk
,
Uk=œÉ 
y0, . . . ,yk+1,z0, . . . ,zk,x0, . . . ,xk,r0, . . . ,rk
,
Gk=œÉ 
y0, . . . ,yk,z0, . . . ,zk,x0, . . . ,xk,r0, . . . ,rk
,
and denote E[¬∑|Fk]byEk,E[¬∑|Uk]byeEk,E[¬∑|Gk]bybEkfor brevity.
Define
z‚ãÜ(x) = nX
i=1‚àá2
22gi(x, y‚ãÜ(x))!‚àí1 nX
i=1‚àá2fi(x, y‚ãÜ(x))!
,
Then, for k= 0,1,¬∑¬∑¬∑, define:
zk+1
‚ãÜ= nX
i=1‚àá2
22gi 
¬Øxk, y‚ãÜ(¬Øxk)!‚àí1 nX
i=1‚àá2fi 
¬Øxk, y‚ãÜ(¬Øxk)!
.
For convenience, we define x‚àí1=x0,y‚àí1=y0, y‚ãÜ(¬Øx‚àí1) =y‚ãÜ(¬Øx0), z0
‚ãÜ=z1
‚ãÜ.
18C.1.2 Basic transformations
We begin with conducting SUDA-like [ 2] transformations, which is fundamental of the following
proofs.
Firstly, we define tkto track the averaged stochastic gradients among agents as follows
tk
y=By(dk
y‚àíByyk) +Œ≤Ay‚àá2g(¬Øxk,¬Øyk),
tk
z=Bz(dk
z‚àíBzzk) +Œ≥Azpk(¬Øxk,¬Øyk+1),
tk
x=Bx(dk
x‚àíBxxk) +Œ±Axe‚àáŒ¶(¬Øxk),(17)
where
pk(¬Øxk,¬Øyk+1) =col
‚àá2
22gi(¬Øxk,¬Øyk+1)zk
‚ãÜ‚àí ‚àá 2fi(¬Øxk,¬Øyk+1)	n
i=1,
e‚àáŒ¶(¬Øxk) =col
‚àá1fi(¬Øxk, y‚ãÜ(¬Øxk))‚àí ‚àá 12gi(¬Øxk, y‚ãÜ(¬Øxk))zk+1
‚ãÜ	n
i=1.
Then the iteration of y,z,xin Algorithm 1 can be written as:
iteration of y:(
yk+1= (Cy‚àíB2
y)yk‚àítk
y‚àíŒ≤Ay
vk‚àí ‚àá 2g(¬Øxk,¬Øyk)
,
tk+1
y=tk
y+B2
yyk+Œ≤Ay
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)
,(18)
iteration of z:(
zk+1= (Cz‚àíB2
z)zk‚àítk
z‚àíŒ≥Az
pk‚àípk(¬Øxk,¬Øyk+1)
,
tk+1
z=tk
z+B2
zzk+Œ≥Az
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)
,(19)
iteration of x:Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥xk+1= (Cx‚àíB2
x)xk‚àítk
x‚àíŒ±Axh
rk+1‚àíe‚àáŒ¶(¬Øxk)i
,
tk+1
x=tk
x+B2
xxk+Œ±Axh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)i
.(20)
Next, we present the transformation of the matrices A, B, C . For a communication matrix Wsfor the
variable s‚àà {x, y, z}satisfying Assumption 2, there exists an orthogonal matrix Usuch that:
W=UsÀÜŒõsU‚ä§
s=1‚àön1ÀÜUs
1 0
0 Œõ sÔ£Æ
Ô£∞1‚àön1‚ä§
ÀÜU‚ä§
sÔ£π
Ô£ª,
where Œõs=diag{Œªsi}n
i=2,ÀÜU‚ä§
s‚ààRn√ó(n‚àí1)satisfies ÀÜUsÀÜU‚ä§
s=In‚àí1
n1n1‚ä§
nand1‚ä§
nÀÜUs= 0. Then
it follows that:
Ws=UsÀÜŒõsU‚ä§
s=1‚àön1‚äóIdim(s)ÀÜUs
Idim(s)0
0 ŒõsÔ£Æ
Ô£∞1‚àön1‚ä§‚äóIdim(s)
ÀÜU‚ä§
sÔ£π
Ô£ª,
where dim(s)denotes the dimension of the corresponding variable, Œõs= Œõ s‚äóIdim(s)‚àà
Rd(n‚àí1)√ó[dim(s)¬∑(n‚àí1)],Us‚ààR[dim(s)¬∑n]√ó[dim(s)¬∑n]is an orthogonal matrix, and ÀÜUs=ÀÜUs‚äó
Idim(s)‚ààR[dim(s)¬∑n]√ó[dim(s)¬∑(n‚àí1)]satisfies:
ÀÜU‚ä§
sÀÜUs=Idim(s)¬∑(n‚àí1),ÀÜUsÀÜU‚ä§
s=
In‚àí1
n11‚ä§
‚äóIdim(s),(1‚ä§‚äóIdim(s))ÀÜUs=0.
Now we add subscript sforWs. Then, as As,B2
s,Cscan be expressed as a polynomial of Wsfor
s‚àà {x, y, z}according to Assumption 2, we have the orthogonal decomposition:
As=UsÀÜŒõsaU‚ä§
s=1‚àön1‚äóIdim(s)ÀÜUs
Idim(s) 0
0 ŒõsaÔ£Æ
Ô£∞1‚àön1‚ä§‚äóIdim(s)
ÀÜU‚ä§
sÔ£π
Ô£ª,
B2
s=UsÀÜŒõ2
sbU‚ä§
s=1‚àön1‚äóIdim(s)ÀÜUs
0 0
0Œõ2
sbÔ£Æ
Ô£∞1‚àön1‚ä§‚äóIdim(s)
ÀÜU‚ä§
sÔ£π
Ô£ª,
Cs=UsÀÜŒõscU‚ä§
s=1‚àön1‚äóIdim(s)ÀÜUs
Idim(s) 0
0 ŒõscÔ£Æ
Ô£∞1‚àön1‚ä§‚äóIdim(s)
ÀÜU‚ä§
sÔ£π
Ô£ª,(21)
19where
Œõsa=diag{Œªsa,i}n
i=2|{z }
Œõsa‚äóIdim(s),Œõsb=diag{Œªsb,i}n
i=2|{z }
Œõsb‚äóIdim(s),Œõsc=diag{Œªsc,i}n
i=2|{z }
Œõsc‚äóIdim(s).
Moreover, each Œõsbis positive definite because of the null space condition in Assumption 2. Then,
multiplying both sides of (18), (19) and (20) by U‚ä§
y,U‚ä§
z,U‚ä§
xrespectively, we get:
iter. of y:(
U‚ä§
yyk+1= (ÀÜŒõyc‚àíÀÜŒõ2
yb)U‚ä§
yyk‚àíU‚ä§
ytk
y‚àíŒ≤ÀÜŒõyaU‚ä§
y
vk‚àí ‚àá 2g(¬Øxk,¬Øyk)
,
U‚ä§
ytk+1
y=U‚ä§
ytk
y+ÀÜŒõ2
ybU‚ä§
yyk+Œ≤ÀÜŒõyaU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)
,
(22)
iter. of z:(
U‚ä§
zzk+1= (ÀÜŒõzc‚àíÀÜŒõ2
zb)U‚ä§
zzk‚àíU‚ä§
ztk
z‚àíŒ≥ÀÜŒõzaU‚ä§
z
pk‚àípk(¬Øxk,¬Øyk+1)
,
U‚ä§
ztk+1
z=U‚ä§
ztk
z+ÀÜŒõ2
zbU‚ä§
zzk+Œ≥ÀÜŒõzaU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)
.
(23)
iter. of x:Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥U‚ä§
xxk+1= (ÀÜŒõxc‚àíÀÜŒõ2
xb)U‚ä§
xxk‚àíU‚ä§
xtk
x‚àíŒ±ÀÜŒõxaU‚ä§h
rk+1‚àíe‚àáŒ¶(¬Øxk)i
,
U‚ä§
xtk+1
x=U‚ä§
xtk
x+ÀÜŒõ2
xbU‚ä§
xxk+Œ±ÀÜŒõxaU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)i
.(24)
Then, due to Eq. (17), we have:
(1‚ä§‚äóId)tk
y= (1‚ä§‚äóId) 
By(dk
y‚àíByyk) +Œ≤Ay‚àá2g(¬Øxk,¬Øyk)
=nŒ≤‚àá2g(¬Øxk,¬Øyk).(25)
(1‚ä§‚äóId)tk
z= (1‚ä§‚äóId) 
Bz(dk
z‚àíBzzk) +Œ≥Azpk(¬Øxk,¬Øyk+1)
=Œ≥nX
i=1
‚àá2
22gi(¬Øxk,¬Øyk+1)zk
‚ãÜ‚àí ‚àá 2fi(¬Øxk,¬Øyk+1)
.(26)
(1‚ä§‚äóId)tk
x= (1‚ä§‚äóId)
Bx(dk
x‚àíBxxk) +Œ±Axe‚àáŒ¶(¬Øxk)
=Œ±nX
i=1
‚àá1fi(¬Øxk, y‚ãÜ(¬Øxk))‚àí ‚àá 12gi(¬Øxk, y‚ãÜ(¬Øxk))zk+1
‚ãÜ
.(27)
Substituting (25),(26),(27) into (22),(23),(24), respectively. Then use (21) and the structure of
ÀÜUy,ÀÜUz,ÀÜUx, we have
iter. of y:Ô£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥¬Øyk+1= ¬Øyk‚àíŒ≤¬Øvk,
ÀÜU‚ä§
yyk+1= (Œõyc‚àíŒõ2
yb)ÀÜU‚ä§
yyk‚àíÀÜU‚ä§
ytk
y‚àíŒ≤ŒõyaÀÜU‚ä§
y
vk‚àí ‚àá 2g(¬Øxk,¬Øyk)
,
ÀÜU‚ä§
ytk+1
y=ÀÜU‚ä§
ytk
y+Œõ2
ybÀÜU‚ä§
yyk+Œ≤ŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)
,
iter. of z:Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥¬Øzk+1= ¬Øzk‚àíŒ≥¬Øpk,
ÀÜU‚ä§
zzk+1= (Œõzc‚àíŒõ2
zb)ÀÜU‚ä§
zzk‚àíÀÜU‚ä§
ztk
z‚àíŒ≥ŒõzaÀÜU‚ä§
z
pk‚àípk(¬Øxk,¬Øyk+1)
,
ÀÜU‚ä§
ztk+1
z=ÀÜU‚ä§
ztk
z+Œõ2
zbÀÜU‚ä§
zzk+Œ≥ŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)
,
iter. of x:Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥¬Øxk+1= ¬Øxk‚àíŒ±¬Ørk+1,
ÀÜU‚ä§
xxk+1= (ÀÜŒõxc‚àíÀÜŒõ2
xb)ÀÜU‚ä§
xxk‚àíÀÜU‚ä§
xtk
x‚àíŒ±ÀÜŒõxaÀÜU‚ä§
xh
rk+1‚àíe‚àáŒ¶(¬Øxk)i
,
ÀÜU‚ä§
xtk+1
x=ÀÜU‚ä§
xtk
x+ÀÜŒõ2
xbÀÜU‚ä§
xxk+Œ±ÀÜŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)i
.
20The above three equations are equivalent to:
"ÀÜU‚ä§
yyk+1
Œõ‚àí1
ybÀÜU‚ä§
ytk+1
y#
=
Œõyc‚àíŒõ2
yb‚àíŒõyb
Œõyb I"ÀÜU‚ä§
yyk
Œõ‚àí1
ybÀÜU‚ä§
yty
k#
‚àíŒ≤"
ŒõyaÀÜU‚ä§
y
vk‚àí ‚àá 2g(¬Øxk,¬Øyk)
Œõ‚àí1
ybŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)#
,(28)
ÀÜU‚ä§
zzk+1
Œõ‚àí1
zbÀÜU‚ä§
ztk+1
z
=
Œõzc‚àíŒõ2
zb‚àíŒõzb
Œõzb IÀÜU‚ä§
zzk
Œõ‚àí1
zbÀÜU‚ä§
ztz
k
‚àíŒ≥
ŒõzaÀÜU‚ä§
z
pk‚àípk(¬Øxk,¬Øyk+1)
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)
,(29)
ÀÜU‚ä§
xxk+1
Œõ‚àí1
xbÀÜU‚ä§
xtk+1
x
=
Œõxc‚àíŒõ2
xb‚àíŒõxb
Œõxb IÀÜU‚ä§
xxk
Œõ‚àí1
xbÀÜU‚ä§
xtx
k
‚àíŒ±Ô£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
rk+1‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª.(30)
Fors‚àà {x,y,z}, define:
ek
s=ÀÜU‚ä§
ssk
Œõ‚àí1
sbÀÜU‚ä§
stk
s
,Ms=
Œõsc‚àíŒõ2
sb‚àíŒõsb
Œõsb I
.
Then (28), (29), (30) are respectively equivalent to:
ek+1
y=Myek
y‚àíŒ≤"
ŒõyaÀÜU‚ä§
y
vk‚àí ‚àá 2g(¬Øxk,¬Øyk)
Œõ‚àí1
ybŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)#
,
ek+1
z=Mzek
z‚àíŒ≥
ŒõzaÀÜU‚ä§
z
pk‚àípk(¬Øxk,¬Øyk+1)
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)
,
ek+1
x=Mxex
k‚àíŒ±Ô£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
rk+1‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª.
Assumption 2, 3 imply that all eigenvalues of

diag{0,Œõsc‚àíŒõ2
sb} ‚àí diag{0,Œõsb}
diag{0,Œõsb} diag{0,1, ...,1}
=
U‚ä§
s
U‚ä§
s
Cs‚àí1
n1n1‚ä§
n‚àíB2
s ‚àíBs
Bs In‚àí1
n1n1‚ä§
n
Us
Us (31)
are strictly less than one in magnitude. Thus by symmetrically exchanging columns and rows of the
matrix, we know that equivalently, all eigenvalues of

Œõsc‚àíŒõ2
sb‚àíŒõsb
Œõsb In‚àí1
andMs=
Œõsc‚àíŒõ2
sb ‚àíŒõsb
Œõsb In‚àí1‚äóIdim(s)
(32)
are strictly less than one in magnitude,.
Then according to Lemma 3, for s‚àà {x, y, z},Mshas the similarity transformation:
Ms=OsŒìsO‚àí1
s,
where Osis invertible and ‚à•Œìs‚à•<1. Moreover, we define ÀÜek
s=O‚àí1
sek
s. It yields
ÀÜek+1
y=ŒìyÀÜek
y‚àíŒ≤O‚àí1
y"
ŒõyaÀÜU‚ä§
y
vk‚àí ‚àá 2g(¬Øxk,¬Øyk)
Œõ‚àí1
ybŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)#
, (33)
21ÀÜek+1
z=ŒìyÀÜek
z‚àíŒ≥O‚àí1
z
ŒõzaÀÜU‚ä§
z
pk‚àípk(¬Øxk,¬Øyk+1)
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)
, (34)
ÀÜek+1
x=ŒìxÀÜek
x‚àíŒ±O‚àí1
xÔ£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
rk+1‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª. (35)
Then, for s‚àà {x,y,z}, the consensus errors between different agents have the upper bound of:
sk‚àí¬Øsk2=‚à•ÀÜU‚ä§
ssk‚à•2‚â§ ‚à•ek
s‚à•2‚â§ ‚à•Os‚à•2‚à•ÀÜek
s‚à•2. (36)
Thus, we can define:
‚àÜk=Œ∫2‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+Œ∫2‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2+‚à•Oz‚à•2‚à•ÀÜek+1
z‚à•2
to measure the consensus error during the iteration.
We also define
Ik=‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2+Œ∫2‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2,
to measure the estimation accuracy of the lower- and auxiliary-level problems.
C.1.3 Proof sketch
Before proceeding with the formal proof, we first present the structure of the proof in Appendix C.
Bounded by each other
Descent of xPE‚à•¬Ørk‚à•2
Descent of yPE‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2PE[‚àÜk]PE[Ik]
Descent of zPE‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2
Consensus of yPE‚à•ÀÜek
y‚à•2PE‚àÜk
n+Ik
Consensus of zPE‚à•ÀÜek
z‚à•2
Consensus of xPE‚à•ÀÜek
x‚à•2PE‚à•Œ¶(¬Øxk)‚à•2
Hyper-gradient estimationPE‚à•Ekuk‚àíe‚àáŒ¶(¬Øxk)‚à•2
Hyper-gradient estimationPE‚à•Ekrk+1‚àíe‚àáŒ¶(¬Øxk)‚à•2Lemma 17
VariancePE‚à•Ekuk‚àíuk‚à•2Ô£º
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£Ω
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£æ
22C.1.4 Technical lemmas
Lemma 1. Suppose Assumptions 1 hold, we know ‚àáŒ¶(x),e‚àáŒ¶(x),z‚ãÜ(x)andy‚ãÜ(x)defined above
areL‚àáŒ¶,eL,Lz‚ãÜ,Ly‚ãÜ- Lipschitz continuous respectively with the constants satisfying:
L‚àáŒ¶‚â§Lf,1+2Lf,1Lg,1+Lg,2Lf,0
¬µg+2Lg,1Lf,0Lg,2+L2
g,1Lf,1
¬µ2g+Lg,2L2
g,1Lf,0
¬µ3g,
eL‚â§Lf,1+2Lf,1Lg,1+Lg,2Lf,0
¬µg+2Lg,1Lf,0Lg,2+L2
g,1Lf,1
¬µ2g+Lg,2L2
g,1Lf,0
¬µ3g,
Ly‚ãÜ‚â§Lg,1
¬µg,
Lz‚ãÜ‚â§q
1 +L2
y‚ãÜLf,1
¬µg+Lf,0Lg,2
¬µ2g
.
And we also have:
‚à•z‚ãÜ(x)‚à• ‚â§Lf,0
¬µg,‚àÄx‚ààRp.
Proof. See Lemma 2.2 in [22] and Lemma B.2 in [11].
Lemma 2. Suppose that g(x)is¬µ-strongly convex and L-smooth. Then for any xand0< Œ± <2
¬µ+L,
we have
‚à•x‚àíŒ±‚àág(x)‚àíx‚ãÜ‚à• ‚â§(1‚àíŒ±¬µ)‚à•x‚àíx‚ãÜ‚à•,
where x‚ãÜ= argmin g(x).
Proof. See Lemma 10 in [41].
Lemma 3. Given diagonal matrices A, B, C, D ‚ààR(n‚àí1)√ó(n‚àí1), and
M=
A‚äóIdB‚äóId
C‚äóIdD‚äóId
.
Suppose that the eigenvalues of Mare strictly less than one in magnitude. Then there exist an
invertible matrix Oand a matrix Œìwith‚à•Œì‚à•<1, such that Mhas the similarity transformation:
M=OŒìO‚àí1.
Proof. See Lemma 1 in [2].
Remark 5. Asserting the existence of Œìwith‚à•Œì‚à•<1, Lemma 3 only guarantees the convergence
ofSPARKLE . However, to obtain a precise non-asymptotic convergence rate, one must construct
appropriate OandŒì. See more details in Appendix C.2.2.
C.1.5 Descent lemmas for the upper-level
In this subsection, we estimate the upper bound of the errors induced by the moving average in
hyper-gradient estimation, as well as the upper bound of ‚à•‚àáŒ¶(x)‚à•2based on Ik,‚àÜk.
Lemma 4. Suppose Assumptions 1- 4 hold. We have:
Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)2‚â§20
nL2(‚àÜk+nIk),
Ekuk‚àíe‚àáŒ¶(¬Øxk)2
‚â§20L2(‚àÜk+nIk).(38)
23Proof. Cauchy Schwartz inequality implies that:
Ekuk‚àíe‚àáŒ¶ ¬Øxk2
‚â§5nX
i=1‚àá1fi 
xk
i, yk+1
i
‚àí ‚àá 1fi 
¬Øxk,¬Øyk+12+ 5nX
i=1‚àá1fi 
¬Øxk,¬Øyk+1
‚àí ‚àá 1fi 
¬Øxk, y‚ãÜ(¬Øxk)2
+ 5nX
i=1‚àá2
12gi 
xk
i, yk+1
i 
zk+1
i‚àízk+1
‚ãÜ2
+ 5nX
i=1 
‚àá2
12gi 
xk
i, yk+1
i
‚àí ‚àá2
12gi 
¬Øxk,¬Øyk+1
zk+1
‚ãÜ2
+ 5nX
i=1 
‚àá2
12gi 
¬Øxk,¬Øyk+1
‚àí ‚àá2
12gi 
¬Øxk, y‚ãÜ(¬Øxk)
zk+1
‚ãÜ2
‚â§10 
L2
f,1+Œ∫2L2
f,0xk‚àí¬Øxk2+yk+1‚àí¬Øyk+12+¬Øyk+1‚àíy‚ãÜ(¬Øxk)2
+ 10L2
g,1zk+1‚àí¬Øzk+12+¬Øzk+1‚àízk+1
‚ãÜ2
‚â§20L2(‚àÜk+nIk).
For the termEk¬Øuk‚àí ‚àáŒ¶ 
¬Øxk2, we have:
Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)2‚â§1
nEkuk‚àíe‚àáŒ¶ ¬Øxk2
‚â§20L2‚àÜk
n+Ik
.
Lemma 5. Suppose that Assumptions 1- 4 hold. We have
n2KX
k=0E
‚à•¬Øuk‚àíEk[¬Øuk]‚à•2
=KX
k=0E
‚à•uk‚àíEk[uk]‚à•2
‚â§9œÉ2
g,2KX
k=0 
E‚à•zk+1‚àí¬Øzk+1‚à•2+E‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2
+ 3(K+ 1)n 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
.(39)
Proof. Fork‚â•0, Cauchy Schwartz inequality implies that
1
3Ek
‚à•uk‚àíEk[uk]‚à•2
‚â§Ek"nX
i=1‚à•‚àá1fi(xk
i, yk+1
i, Œæk
i)‚àí ‚àá 1fi(xk
i, yk+1
i)‚à•2#
+Ek"nX
i=1 
‚àá12gi(xk
i, yk+1
i, Œ∂k
i)‚àí ‚àá 12gi(xk
i, yk+1
i)
zk+1
i2#
‚â§nœÉ2
f,1+œÉ2
g,2‚à•zk+1‚à•2
‚â§nœÉ2
f,1+ 3œÉ2
g,2 
‚à•zk+1‚àí¬Øzk+1‚à•2+‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2+‚à•zk+1
‚ãÜ‚à•2
‚â§nœÉ2
f,1+ 3œÉ2
g,2 
‚à•zk+1‚àí¬Øzk+1‚à•2+‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2+nL2
f,0
¬µ2g!
.
Then taking expectation and summation on both sides, we get
KX
k=0E
‚à•uk‚àíEk[uk]‚à•2
‚â§9œÉ2
g,2KX
k=0 
E‚à•zk+1‚àí¬Øzk+1‚à•2+E‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2
+ 3(K+ 1)n 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
.
24Since samples among agents are independent, it follows that
KX
k=0Ek
‚à•¬Øuk‚àíEk[¬Øuk]‚à•2
=1
n2KX
k=0Ek
‚à•uk‚àíEk[uk]‚à•2
.
Taking expectations, we get the conclusion.
Lemma 6. Suppose that Assumptions 1- 4, and Lemmas 4, 5 hold. If
Œ±‚â§1
2L‚àáŒ¶, (40)
we have
1
4KX
k=0E¬Ørk+12
‚â§Œ¶(¬Øx0)‚àíinf Œ¶
Œ±+ 10 
L2+Œ∏œÉ2
g,2
n!KX
k=0E‚àÜk
n+Ik
+3Œ∏
n(K+ 1) 
œÉ2
f,1+ 2œÉ2
g,2L2
f,0
¬µ2g!
.
(41)
Proof. TheL‚àáŒ¶-smoothness of Œ¶indicates that
Ek[Œ¶ 
¬Øxk+1
]‚àíŒ¶(¬Øxk)
‚â§
‚àáŒ¶(¬Øxk), 
‚àíŒ±Ek[¬Ørk+1]
+L‚àáŒ¶Œ±2
2Ek‚à•¬Ørk+1‚à•2
=
‚àáŒ¶(¬Øxk)‚àíEk[¬Øuk],‚àíŒ±Ek[¬Ørk+1]
+L‚àáŒ¶
2Œ±2Ek‚à•¬Ørk+1‚à•2‚àíŒ±
Ek[¬Øuk],Ek[¬Ørk+1]
.
Then, due to Ek[¬Øuk] =Œ∏‚àí1(Ek[¬Ørk+1]‚àí(1‚àíŒ∏)¬Ørk), we have:
Ek[Œ¶ 
¬Øxk+1
]‚àíŒ¶(¬Øxk)
‚â§Œ±
2‚à•‚àáŒ¶(¬Øxk)‚àíEk[¬Øuk]‚à•2+Œ±
2‚à•Ek[¬Ørk+1]‚à•2
+L‚àáŒ¶
2Œ±2Ek‚à•¬Ørk+1‚à•2‚àíŒ±
Ek[¬Øuk],Ek[¬Ørk+1]
=Œ±
2‚à•‚àáŒ¶(¬Øxk)‚àíEk[¬Øuk]‚à•2+ (‚àíŒ±
2+L‚àáŒ¶
2Œ±2)Ek‚à•¬Ørk+1‚à•2‚àíŒ±(1‚àíŒ∏)
2Œ∏‚à•Ek[¬Ørk+1]‚àí¬Ørk‚à•2
+Œ±(1‚àíŒ∏)
2Œ∏ 
‚à•¬Ørk‚à•2‚àíEk‚à•¬Ørk+1‚à•2
+Œ±
2Œ∏Ek‚à•¬Ørk+1‚àíEk[¬Ørk+1]‚à•2
‚â§Œ±
2‚à•‚àáŒ¶(¬Øxk)‚àíEk[¬Øuk]‚à•2+ (‚àíŒ±
2+L‚àáŒ¶
2Œ±2)Ek‚à•¬Ørk+1‚à•2+Œ±Œ∏
2Ek‚à•¬Øuk‚àíEk[¬Øuk]‚à•2
+Œ±(1‚àíŒ∏)
2Œ∏ 
‚à•¬Ørk‚à•2‚àíEk‚à•¬Ørk+1‚à•2
,
where the first equality uses 2
¬Ørk,Ek[¬Ørk+1]
=‚à•¬Ørk‚à•2+‚à•Ek[¬Ørk+1]‚à•2‚àí ‚à•¬Ørk‚àíEk[¬Ørk+1]‚à•2and
Ek‚à•¬Ørk+1‚à•2=‚à•Ek[¬Ørk+1]‚à•2+Ek‚à•¬Ørk+1‚àíEk[¬Ørk+1]‚à•2.
Taking expectation and summation, and using Œ±‚â§1
2L‚àáŒ®, we get
inf Œ¶‚àíŒ¶(¬Øx0)
‚â§Œ±
2KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚àíEk[¬Øuk]‚à•2‚àíŒ±
4KX
k=0E‚à•¬Ørk+1‚à•2+Œ±Œ∏
2KX
k=0E
Ek‚à•¬Øuk‚àíEk[¬Øuk]‚à•2
.(42)
Since samples of different agents are independent, we have
Ek‚à•¬Øuk‚àíEk[¬Øuk]‚à•2=1
n2Ek‚à•uk‚àíEk[uk]‚à•2.
25Combining it with the conclusion of Lemma 4 and 5, we get from (42) that
Œ±
4KX
k=0E¬Ørk+12
‚â§Œ¶(¬Øx0)‚àíinf Œ¶ +Œ±
2KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚àíEk[¬Øuk]‚à•2+Œ±Œ∏
2KX
k=0E
Ek‚à•¬Øuk‚àíEk[¬Øuk]‚à•2
‚â§Œ¶(¬Øx0)‚àíinf Œ¶ + 10 Œ± 
L2+Œ∏œÉ2
g,2
n!KX
k=0E‚àÜk
n+Ik
+3Œ±Œ∏
n(K+ 1) 
œÉ2
f,1+ 2œÉ2
g,2L2
f,0
¬µ2g!
.
Lemma 7. Suppose that Assumptions 1- 4 hold, then we have
KX
k=0EEk[rk+1]‚àíe‚àáŒ¶(¬Øxk)2
‚â§1‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 2KX
k=0EEk[uk]‚àíe‚àáŒ¶(¬Øxk)2
+2eL2(1‚àíŒ∏)2
Œ∏2K‚àí1X
k=0Eh¬Øxk+1‚àí¬Øxk2i
+ (1‚àíŒ∏)Œ∏KX
k=0Ehuk‚àíEk[uk]2i
.
Proof. We define u‚àí1=0for brevity. From the definition of Ek, we have :
Ek‚àí1rk‚àíe‚àáŒ¶(¬Øxk‚àí1)2
=Ek‚àí1Ek‚àí1[rk]‚àíe‚àáŒ¶(¬Øxk‚àí1)2
+Ek‚àí1hrk‚àíEk‚àí1[rk]2i
=Ek‚àí1Ek‚àí1[rk]‚àíe‚àáŒ¶(¬Øxk‚àí1)2
+Œ∏2Ek‚àí1huk‚àí1‚àíEk‚àí1[uk‚àí1]2i
.(43)
Jensen‚Äôs inequality implies that
Ekh
‚à•Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)‚à•2i
‚â§(1‚àíŒ∏)Ekrk‚àíe‚àáŒ¶(¬Øxk‚àí1)2
+Œ∏Ek
Ek[uk]‚àíe‚àáŒ¶(¬Øxk)
+Œ∏‚àí1(1‚àíŒ∏)
e‚àáŒ¶(¬Øxk‚àí1)‚àíe‚àáŒ¶(¬Øxk)2
‚â§(1‚àíŒ∏)Ekrk‚àíe‚àáŒ¶(¬Øxk‚àí1)2
+ 2Œ∏EkEk[uk]‚àíe‚àáŒ¶(¬Øxk)2
+2(1‚àíŒ∏)2
Œ∏Eke‚àáŒ¶(¬Øxk‚àí1)‚àíe‚àáŒ¶(¬Øxk)2
.(44)
Substituting (43) into (44) , and taking expectation and summation on both sides, we get:
Œ∏KX
k=0EEk‚àí1[rk]‚àíe‚àáŒ¶(¬Øxk‚àí1)2
‚â§Er0‚àíe‚àáŒ¶(¬Øx‚àí1)2
‚àíEEK[rK+1]‚àíe‚àáŒ¶(¬Øxk)2
+ 2Œ∏KX
k=0EEk[uk]‚àíe‚àáŒ¶(¬Øxk)2
+2(1‚àíŒ∏)2
Œ∏KX
k=0Ee‚àáŒ¶(¬Øxk‚àí1)‚àíe‚àáŒ¶(¬Øxk)2
+ (1‚àíŒ∏)Œ∏2KX
k=0Ehuk‚àíEk[uk]2i
.
26Finally, note that x‚àí1=x0,r0=0, andE‚àí1=E0. Subtracting Œ∏EE‚àí1[r0]‚àíe‚àáŒ¶(¬Øx‚àí1)2
=
Œ∏e‚àáŒ¶(¬Øx0)2
from both sides of this equation, we get:
KX
k=0EEk[rk+1]‚àíe‚àáŒ¶(¬Øxk)2
‚â§1‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 2KX
k=0EEk[uk]‚àíe‚àáŒ¶(¬Øxk)2
+2eL2(1‚àíŒ∏)2
Œ∏2K‚àí1X
k=0Eh¬Øxk+1‚àí¬Øxk2i
+ (1‚àíŒ∏)Œ∏KX
k=0Ehuk‚àíEk[uk]2i
.
Lemma 8 (Descent lemma) .Suppose that Assumptions 1- 4 and Lemmas 4, 5 hold. If
Œ±2
Œ∏2(1‚àíŒ∏)‚â§1
32L2
‚àáŒ¶, Œ±‚â§1
10L‚àáŒ¶, (45)
then we have
KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2‚â≤Œ¶(¬Øx0)‚àíinf Œ¶
Œ±+ 
L2+ 
Œ∏(1‚àíŒ∏) +L‚àáŒ¶Œ±Œ∏2
œÉ2
g,2KX
k=0E‚àÜk
n+Ik
+ (K+ 1) 
Œ∏(1‚àíŒ∏) +L‚àáŒ¶Œ±Œ∏2
(œÉ2
f,1+Œ∫2œÉ2
g,2) +(1‚àíŒ∏)2
Œ∏‚à•‚àáŒ¶(¬Øx0)‚à•2.
Proof. TheL‚àáŒ¶-smoothness of Œ¶indicates that
Ek[Œ¶(¬Øxk+1)]‚àíŒ¶(¬Øxk)
‚â§
‚àáŒ¶(¬Øxk),‚àíŒ±Ek[¬Ørk+1]
+L‚àáŒ¶Œ±2
2Ek¬Ørk+12
=‚àíŒ±
‚àáŒ¶(¬Øxk),Ek[¬Ørk+1]‚àí ‚àáŒ¶(¬Øxk)
‚àíŒ±‚à•‚àáŒ¶(¬Øxk)‚à•2+L‚àáŒ¶
2Œ±2Ek¬Ørk+12
‚â§ ‚àíŒ±
2‚à•‚àáŒ¶(¬Øxk)‚à•2+Œ±
2‚à•Ek[¬Ørk+1]‚àí ‚àáŒ¶(¬Øxk)‚à•2+L‚àáŒ¶
2Œ±2Ek¬Ørk+12.
Taking expectation and summation on both sides, we get:
KX
k=0Œ±E‚à•‚àáŒ¶(¬Øxk)‚à•2
‚â§2(Œ¶(¬Øx0)‚àíinf Œ¶) +KX
k=0Œ±E‚à•Ek[¬Ørk+1]‚àí ‚àáŒ¶(¬Øxk)‚à•2+KX
k=0L‚àáŒ¶Œ±2E¬Ørk+12.(46)
Define auxiliary series mkas:
m0= ¬Ør0= 0, mk+1= (1‚àíŒ∏)mk+Œ∏‚àáŒ¶(¬Øxk).
Note that
Ek¬Ørk+12=Ek¬Ørk+12+Ek¬Ørk+1‚àíEk¬Ørk+12
‚â§2‚à•Ek[¬Ørk+1]‚àí ‚àáŒ¶(¬Øxk)‚à•2+ 2‚à•‚àáŒ¶(¬Øxk)‚à•2+Œ∏2Ek‚à•¬Øuk‚àíEk¬Øuk‚à•2.(47)
Then using the Jenson‚Äôs Inequality, we get:
‚à•Ek¬Ørk+1‚àímk+1‚à•2=‚à•(1‚àíŒ∏)(¬Ørk‚àímk) +Œ∏(Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk))‚à•2
‚â§(1‚àíŒ∏)‚à•¬Ørk‚àímk‚à•2+Œ∏‚à•Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)‚à•2.
27It follows that for k‚â•0
E‚à•Ek¬Ørk+1‚àímk+1‚à•2
‚â§(1‚àíŒ∏)E‚à•Ek‚àí1[¬Ørk]‚àímk‚à•2+ (1‚àíŒ∏)Œ∏2E‚à•¬Øuk‚àí1‚àíEk‚àí1¬Øuk‚àí1‚à•2+Œ∏E‚à•Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)‚à•2,
where for brevity we define ¬Øu‚àí1= 0.
Taking the summation on both sides from k= 0toK, we get
KX
k=0Œ∏E‚à•Ek¬Ørk+1‚àímk+1‚à•2‚â§K‚àí1X
k=0Œ∏E‚à•Ek¬Ørk+1‚àímk+1‚à•2+E‚à•EK¬ØrK+1‚àímK+1‚à•2
‚â§KX
k=0Œ∏E‚à•Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)‚à•2+K‚àí1X
k=0(1‚àíŒ∏)Œ∏2E‚à•¬Øuk‚àíEk¬Øuk‚à•2.
(48)
On the other hand, due to the definition of mkand Jenson‚Äôs Inequality, we have:
‚à•mk+1‚àí ‚àáŒ¶(¬Øxk)‚à•2=‚à•(1‚àíŒ∏)(mk‚àí ‚àáŒ¶(¬Øxk))‚à•2
= (1‚àíŒ∏)2‚à•mk‚àí ‚àáŒ¶(¬Øxk‚àí1) +‚àáŒ¶(¬Øxk‚àí1)‚àí ‚àáŒ¶(¬Øxk)‚à•2
‚â§(1‚àíŒ∏)‚à•mk‚àí ‚àáŒ¶(¬Øxk‚àí1)‚à•2+(1‚àíŒ∏)2
Œ∏L2
‚àáŒ¶Œ±2‚à•¬Ørk‚à•2.
Taking the summation, we get
KX
k=0Œ∏‚à•mk+1‚àí ‚àáŒ¶(¬Øxk)‚à•2‚â§ ‚à•m0‚àí ‚àáŒ¶(¬Øx‚àí1)‚à•2+KX
k=0(1‚àíŒ∏)2
Œ∏L2
‚àáŒ¶Œ±2‚à•¬Ørk‚à•2
= (1‚àíŒ∏)2‚à•‚àáŒ¶(¬Øx0)‚à•2+KX
k=0(1‚àíŒ∏)2
Œ∏L2
‚àáŒ¶Œ±2‚à•¬Ørk‚à•2.(49)
Combining (48) and (49), we obtain:
KX
k=0Œ∏E‚à•Ek¬Ørk+1‚àí ‚àáŒ¶(¬Øxk)‚à•2
‚â§2KX
k=0Œ∏E‚à•Ek¬Ørk+1‚àímk+1‚à•2+ 2KX
k=0Œ∏‚à•mk+1‚àí ‚àáŒ¶(¬Øxk)‚à•2
‚â§2KX
k=0Œ∏E‚à•Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)‚à•2+ 2K‚àí1X
k=0(1‚àíŒ∏)Œ∏2E‚à•¬Øuk‚àíEk¬Øuk‚à•2
+ 2KX
k=0(1‚àíŒ∏)2
Œ∏L2
‚àáŒ¶Œ±2E‚à•¬Ørk‚à•2+ 2(1‚àíŒ∏)2‚à•‚àáŒ¶ 
¬Øx0
‚à•2
‚â§2KX
k=0Œ∏E‚à•Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)‚à•2+ 2K‚àí1X
k=0
1 + 21‚àíŒ∏
Œ∏L2
‚àáŒ¶Œ±2
(1‚àíŒ∏)Œ∏2E‚à•¬Øuk‚àíEk¬Øuk‚à•2
+ 2(1‚àíŒ∏)2‚à•‚àáŒ¶ 
¬Øx0
‚à•2+ 2K‚àí1X
k=0(1‚àíŒ∏)2
Œ∏L2
‚àáŒ¶Œ±2
2E‚à•Ek[¬Ørk+1]‚àí ‚àáŒ¶(¬Øxk)‚à•2+ 2E‚à•‚àáŒ¶(¬Øxk)‚à•2
,
(50)
where the last inequality uses (47).
(45) indicates that 41‚àíŒ∏
Œ∏L2
‚àáŒ¶Œ±2‚â§Œ∏
8. Subtracting
2K‚àí1X
k=0(1‚àíŒ∏)2
Œ∏L2
‚àáŒ¶Œ±2¬∑2E‚à•Ek[¬Ørk+1]‚àí ‚àáŒ¶(¬Øxk)‚à•2
28from both sides of (50), we have:
KX
k=0Œ∏E‚à•Ek¬Ørk+1‚àí ‚àáŒ¶(¬Øxk)‚à•2
‚â§4KX
k=0Œ∏E‚à•Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)‚à•2+ 8K‚àí1X
k=0(1‚àíŒ∏)Œ∏2E‚à•¬Øuk‚àíEk¬Øuk‚à•2+ 4(1‚àíŒ∏)2‚à•‚àáŒ¶(¬Øx0)‚à•2
+Œ∏
4K‚àí1X
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2.(51)
Substituting (47), (51) into (46), we get:
KX
k=0Œ±E‚à•‚àáŒ¶(¬Øxk)‚à•2
‚â§2(Œ¶(¬Øx0)‚àíinf Œ¶) +KX
k=0(Œ±+ 2L‚àáŒ¶Œ±2)E‚à•Ek[¬Ørk+1]‚àí ‚àáŒ¶(¬Øxk)‚à•2+KX
k=02L‚àáŒ¶Œ±2E‚à•‚àáŒ¶(¬Øxk)‚à•2
+KX
k=02L‚àáŒ¶Œ±2Œ∏2Ek‚à•¬Øuk‚àíEk¬Øuk‚à•2
‚â§2(Œ¶(¬Øx0)‚àíinf Œ¶) + 5 Œ±KX
k=0E‚à•Ek¬Øuk‚àí ‚àáŒ¶(¬Øxk)‚à•2+ 5Œ±
Œ∏(1‚àíŒ∏)2‚à•‚àáŒ¶ 
¬Øx0
‚à•2
+KX
k=0
10Œ±
Œ∏(1‚àíŒ∏) + 2L‚àáŒ¶Œ±2
Œ∏2E‚à•¬Øuk‚àíEk¬Øuk‚à•2+Œ±
2K‚àí1X
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2,
(52)
where the last inequality uses Œ±‚â§1
10L‚àáŒ¶.
SubtractingŒ±
2PK‚àí1
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2from both sides of (52), and substituting (38),(39) into it, we
get:
1
2KX
k=0Œ±E‚à•‚àáŒ¶(¬Øxk)‚à•2
‚â§2(Œ¶(¬Øx0)‚àíinf Œ¶) + 100 Œ±L2KX
k=0E‚àÜk
n+Ik
+ 5Œ±
Œ∏(1‚àíŒ∏)2‚à•‚àáŒ¶ 
¬Øx0
‚à•2
+ 
10Œ±
Œ∏(1‚àíŒ∏) + 2L‚àáŒ¶Œ±2
n2Œ∏2¬∑9œÉ2
g,2KX
k=0 
E‚à•zk+1‚àí¬Øzk+1‚à•2+E‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2
+ 
10Œ±
Œ∏(1‚àíŒ∏) + 2L‚àáŒ¶Œ±2
n2Œ∏2¬∑3(K+ 1)n 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
‚â§2(Œ¶(¬Øx0)‚àíinf Œ¶) + 
100Œ±L2+ 9 
10Œ±Œ∏(1‚àíŒ∏) + 2L‚àáŒ¶Œ±2Œ∏2œÉ2
g,2
n!KX
k=0E‚àÜk
n+Ik
+ 3(K+ 1) 
10Œ±(1‚àíŒ∏) + 2L‚àáŒ¶Œ±2Œ∏Œ∏
n 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
+ 5Œ±
Œ∏(1‚àíŒ∏)2‚à•‚àáŒ¶ 
¬Øx0
‚à•2.
29Finally, multiplying2
Œ±on both sides, we get:
KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2‚â≤Œ¶(¬Øx0)‚àíinf Œ¶
Œ±+ 
L2+ 
Œ∏(1‚àíŒ∏) +L‚àáŒ¶Œ±Œ∏2œÉ2
g,2
n!KX
k=0E‚àÜk
n+Ik
+K+ 1
n 
Œ∏(1‚àíŒ∏) +L‚àáŒ¶Œ±Œ∏2
(œÉ2
f,1+Œ∫2œÉ2
g,2) +(1‚àíŒ∏)2
Œ∏‚à•‚àáŒ¶(¬Øx0)‚à•2.
C.1.6 Descent lemmas for the lower- and auxiliary-level
The following lemmas present the error analysis of the estimation of y‚ãÜ(¬Øxk)andzk
‚ãÜ, i.e., the term Ik:
Lemma 9 (Estimation error of y‚ãÜ(x)).Suppose Assumptions 1- 4hold, and:
Œ≤‚â§¬µg
32L2
g,1. (53)
Then we have the estimation error of y‚ãÜ:
‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KX
k=0E[‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2]
‚â§4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KX
k=16Œ±2L2
y‚ãÜ
Œ≤2¬µ2gE‚à•¬Ørk‚à•2+KX
k=16
¬µ2gL2
g,1E"
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
n#
+4KŒ≤œÉ2
g,1
n¬µg,
and
KX
k=0E
‚à•¬Øyk+1‚àí¬Øyk‚à•2
‚â§Œ≤2L2
g,1
n 
4 +48L2
g,1
¬µ2g!KX
k=1E 
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
+48Œ±2L2
g,1
¬µ2gL2
y‚ãÜKX
k=1E‚à•¬Ørk‚à•2
+3(K+ 1)Œ≤2
nœÉ2
g,1+32Œ≤L2
g,1
¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2.
(54)
Proof. For each k‚â•0, due to the independence of samples, we have:
bEk[‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2] =bEk[‚à•¬Øyk‚àíŒ≤¬Øvk‚àíy‚ãÜ(¬Øxk)‚à•2]
=bEkÔ£Æ
Ô£∞¬Øyk‚àíŒ≤1
nnX
i=1‚àá2gi(xk
i, yk
i)‚àíy‚ãÜ(¬Øxk) +Œ≤1
nnX
i=1 
‚àá2gi(xk
i, yk
i)‚àívk
i2Ô£π
Ô£ª
‚â§¬Øyk‚àíŒ≤1
nnX
i=1‚àá2gi(¬Øxk,¬Øyk)‚àíy‚ãÜ(¬Øxk) +Œ≤1
nnX
i=1(‚àá2gi(¬Øxk,¬Øyk)‚àí ‚àá 2gi(xk
i, yk
i))2
+Œ≤2œÉ2
g,1
n.
30Then,
bEk[‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2]
‚â§
1 +Œ≤¬µg
2¬Øyk‚àíŒ≤1
nnX
i=1‚àá2gi(¬Øxk,¬Øyk)‚àíy‚ãÜ(¬Øxk)2
+Œ≤2
1 +2
Œ≤¬µg1
nnX
i=1(‚àá2gi(¬Øxk,¬Øyk)‚àí ‚àá 2gi(xk
i, yk
i))2
+Œ≤2œÉ2
g,1
n
‚â§
1 +Œ≤¬µg
2
(1‚àíŒ≤¬µg)2‚à•¬Øyk‚àíy‚ãÜ(¬Øxk)‚à•2
+Œ≤2
1 +2
Œ≤¬µg
L2
g,1‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
n
+Œ≤2œÉ2
g,1
n
‚â§(1‚àíŒ≤¬µg)
1 +Œ≤¬µg
2
‚à•¬Øyk‚àíy‚ãÜ(¬Øxk‚àí1)‚à•2+
1 +2
Œ≤¬µg
‚à•y‚ãÜ(¬Øxk)‚àíy‚ãÜ(¬Øxk‚àí1)‚à•2
+Œ≤2
1 +2
Œ≤¬µg
L2
g,1‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
n
+Œ≤2œÉ2
g,1
n
‚â§
1‚àíŒ≤¬µg
2
‚à•¬Øyk‚àíy‚ãÜ(¬Øxk‚àí1)‚à•2+3
Œ≤¬µgL2
y‚ãÜ‚à•¬Øxk‚àí¬Øxk‚àí1‚à•2
+3Œ≤
¬µgL2
g,1‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
n
+Œ≤2œÉ2
g,1
n,
where the first and the third inequality is due to the Jenson‚Äôs inequality, the second inequality holds
according to Lemma 2 and the fact that Œ≤‚â§¬µg
32L2
g,1‚â§1
3(¬µg+Lg,1), and the last inequality uses
Œ≤¬µg‚â§1
3. Taking the summation and expectation on the both sides, we get:
KX
k=0Œ≤¬µg
2E[‚à•¬Øyk‚àíy‚ãÜ(¬Øxk‚àí1)‚à•2] +E[‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2]
‚â§E‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KX
k=0E"
3Œ±2
Œ≤¬µgL2
y‚ãÜ‚à•¬Ørk‚à•2+3Œ≤
¬µgL2
g,1‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
n
+Œ≤2œÉ2
g,1
n#
.
Using (36) and the fact that x0,y0is consensual, it follows that:
K+1X
k=0Œ≤¬µg
2E[‚à•¬Øyk‚àíy‚ãÜ(¬Øxk‚àí1)‚à•2]‚â§2‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KX
k=13Œ±2
Œ≤¬µgL2
y‚ãÜE‚à•¬Ørk‚à•2
+KX
k=13Œ≤
¬µgL2
g,1E"
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
n#
+2KŒ≤2œÉ2
g,1
n.
(55)
On the other hand,
bEk
‚à•¬Øyk+1‚àí¬Øyk‚à•2
‚â§Œ≤21
nnX
i=1‚àá2gi(xk
i, yk
i)2
+Œ≤2
nœÉ2
g,1
‚â§2Œ≤2Ô£´
Ô£≠1
nnX
i=1 
‚àá2gi(xk
i, yk
i)‚àí ‚àá 2gi(¬Øxk,¬Øyk)2
+‚àá2g(¬Øxk,¬Øyk)‚àí ‚àá 2g(¬Øxk, y‚ãÜ(¬Øxk))2Ô£∂
Ô£∏
+Œ≤2
nœÉ2
g,1
‚â§2Œ≤2L2
g,1
n 
‚à•xk‚àí¬Øxk‚à•2+‚à•yk‚àí¬Øyk‚à•2+ 2‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2+ 2‚à•¬Øyk+1‚àí¬Øyk‚à•2
+Œ≤2
nœÉ2
g,1,
31where the second inequality uses ‚àá2g(¬Øxk, y‚ãÜ(¬Øxk)) = 0 .
Note that Œ≤2‚â§¬µ2
g
32L4
g,1‚â§1
8L2
g,1. Subtracting 2Œ≤2L2
g,1‚à•¬Øyk+1‚àí¬Øyk‚à•on both sides, and taking
expectation and summation, we get:
KX
k=0E
‚à•¬Øyk+1‚àí¬Øyk‚à•2
‚â§KX
k=0"
4Œ≤2L2
g,1
nE 
‚à•xk‚àí¬Øxk‚à•2+‚à•yk‚àí¬Øyk‚à•2
+8Œ≤2L2
g,1
nE‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2+2Œ≤2
nœÉ2
g,1#
‚â§4Œ≤2L2
g,1
nKX
k=1E 
‚à•Ox‚à•2‚à•ÀÜex
k‚à•2+‚à•Oy‚à•2‚à•ÀÜey
k‚à•2
+8Œ≤2L2
g,1
nKX
k=0E‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
+2(K+ 1)Œ≤2
nœÉ2
g,1
‚â§4Œ≤2L2
g,1
nKX
k=1E 
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
+2(K+ 1)Œ≤2
nœÉ2
g,1
+ 8Œ≤2L2
g,1 
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KX
k=16Œ±2
Œ≤2¬µ2gL2
y‚ãÜE‚à•¬Ørk‚à•2!
+ 8Œ≤2L2
g,1 KX
k=16
¬µ2gL2
g,1E"
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
n#
+4KŒ≤œÉ2
g,1
n¬µg!
‚â§Œ≤2L2
g,1
n 
4 +48L2
g,1
¬µ2g!KX
k=1E 
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
+48Œ±2L2
g,1
¬µ2gL2
y‚ãÜKX
k=1E‚à•¬Ørk‚à•2
+3(K+ 1)Œ≤2
nœÉ2
g,1+32Œ≤L2
g,1
¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2.
where the second inequality holds since x0,y0are consensual, the third inequality uses (55), and the
last inequality holds since Œ≤‚â§¬µg
32L2
g,1.
Lemma 10 (Estimation error of z‚ãÜ(x)).Suppose that Assumptions 1- 4hold, and
Œ≥ <min(
1
¬µg,nL2
g,1
¬µ2gœÉ2
g,2,n¬µg
36œÉ2
g,2)
. (56)
We have:
K+1X
k=0E‚à•¬Øzk‚àízk
‚ãÜ‚à•2
‚â§KX
k=09Œ±2L2
z‚ãÜ
Œ≥2¬µ2gE‚à•¬Ørk‚à•2
+ 72Œ∫2KX
k=1EŒ∫2‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oz‚à•2‚à•ÀÜek
z‚à•2
n
+ 72Œ∫2KX
k=0E"
Œ∫2‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2
n#
+KX
k=072Œ∫4E
‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
.
Proof. For each k‚â•0, note that zk
‚ãÜ=‚àá2
22g(¬Øxk, y‚ãÜ(¬Øxk))‚àí1‚àá2f2(¬Øxk, y‚ãÜ(¬Øxk)), we have:
eEk[¬Øzk+1]‚àízk+1
‚ãÜ= ¬Øzk‚àíŒ≥
nnX
i=1 
‚àá2
22gi(xk
i, yk+1
i)zk
i‚àí ‚àá 2fi(xk
i, yk+1
i)
‚àízk+1
‚ãÜ
32="
I‚àíŒ≥
nnX
i=1‚àá2
22gi(xk
i, yk+1
i)#
(¬Øzk‚àízk+1
‚ãÜ) +Œ≥
nnX
i=1
‚àá2fi(xk
i, yk+1
i)‚àí ‚àá 2fi(¬Øxk, y‚ãÜ(¬Øxk))
+Œ≥
nnX
i=1‚àá2
22gi(xk
i, yk+1
i)(¬Øzk‚àízk
i) +Œ≥
nnX
i=1
‚àá2
22gi(¬Øxk, y‚ãÜ(¬Øxk))‚àí ‚àá2
22gi(xk
i, yk+1
i)
zk+1
‚ãÜ.
Then we have:
eEk[¬Øzk+1]‚àízk+1
‚ãÜ2
‚â§(1 +Œ≥¬µg)"
I‚àíŒ≥
nnX
i=1‚àá2
22gi(xk
i, yk+1
i)#
(¬Øzk‚àízk+1
‚ãÜ)2
+ 3Œ≥2
1 +1
Œ≥¬µg1
nnX
i=1
‚àá2fi(xk
i, yk+1
i)‚àí ‚àá 2fi(¬Øxk, y‚ãÜ(¬Øxk))2
+ 3Œ≥2
1 +1
Œ≥¬µg1
nnX
i=1
‚àá2
22gi(¬Øxk, y‚ãÜ(¬Øxk))‚àí ‚àá2
22gi(xk
i, yk+1
i)
zk+1
‚ãÜ2
+ 3Œ≥2
1 +1
Œ≥¬µg1
nnX
i=1‚àá2
22gi(xk
i, yk+1
i)(¬Øzk‚àízk
i)2
‚â§(1 +Œ≥¬µg)(1‚àíŒ≥¬µg)2‚à•¬Øzk‚àízk+1
‚ãÜ‚à•2
+6Œ≥
¬µg 
L2
g,1‚à•zk‚àí¬Øzk‚à•
n+ 
L2
g,2L2
f,0
¬µ2g+L2
f,1!‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk+1‚àíy‚ãÜ(¬Øxk)‚à•2
n!
‚â§(1‚àíŒ≥¬µg)
1 +Œ≥¬µg
2
‚à•¬Øzk‚àízk
‚ãÜ‚à•2+
1 +2
Œ≥¬µg
‚à•zk
‚ãÜ‚àízk+1
‚ãÜ‚à•2+6Œ≥
¬µgL2
g,1‚à•zk‚àí¬Øzk‚à•
n
+12Œ≥
¬µg 
L2
g,2L2
f,0
¬µ2g+L2
f,1!‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk+1‚àí¬Øyk+1‚à•2
n+‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
‚â§(1‚àíŒ≥¬µg
2)‚à•¬Øzk‚àízk
‚ãÜ‚à•2+3Œ±2L2
z‚ãÜ
Œ≥¬µg‚à•¬Ørk‚à•2+6Œ≥
¬µgL2
g,1‚à•zk‚àí¬Øzk‚à•
n
+12Œ≥
¬µg 
L2
g,2L2
f,0
¬µ2g+L2
f,1!‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk+1‚àí¬Øyk+1‚à•2
n+‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
where the first and third inequality uses Jensen‚Äôs inequality and Cauchy Schwartz inequality, the
second inequality holds due to Assumption 1 and Œ≥¬µg<1, the last inequality holds since z‚ãÜ(x)is
Lz‚ãÜLipschitz continuous.
Moreover, the independence of samples implies that
eEk¬Øzk+1‚àíeEk[¬Øzk+1]2
=Œ≥2eEk1
nnX
i=1(Hk
i‚àíeEk[Hk
i])zk
i+1
nX
i(bk
i‚àíeEk[bk
i])2
‚â§2Œ≥2
n
œÉ2
g,2‚à•zk‚à•2
n+œÉ2
f,1
‚â§2Œ≥2
n 
3œÉ2
g,2 
‚à•zk‚àí¬Øzk‚à•2
n+‚à•¬Øzk‚àízk
‚ãÜ‚à•2+L2
f,0
¬µ2g!
+œÉ2
f,1!
.
AsŒ≥satisfies
6œÉ2
g,2Œ≥2
n‚â§6Œ≥L2
g,1
¬µ2g,6œÉ2
g,2Œ≥2
n‚â§Œ≥¬µg
6,
33we get:
eEk[‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2] =eEk‚à•eEk[¬Øzk+1]‚àízk+1
‚ãÜ‚à•2+eEk‚à•¬Øzk+1‚àíeEk[¬Øzk+1]‚à•2
‚â§
1‚àíŒ≥¬µg
3
‚à•¬Øzk‚àízk
‚ãÜ‚à•2+3Œ±2L2
z‚ãÜ
Œ≥¬µg‚à•¬Ørk‚à•2+12Œ≥
¬µgL2
g,1‚à•zk‚àí¬Øzk‚à•2
n+2Œ≥2
n 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+12Œ≥
¬µg 
L2
g,2L2
f,0
¬µ2g+L2
f,1!‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk+1‚àí¬Øyk+1‚à•2
n+‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
.
Taking expectation and summation on both sides, we get
KX
k=0Œ≥¬µg
3E‚à•¬Øzk‚àízk
‚ãÜ‚à•2+E‚à•¬ØzK+1‚àízK+1
‚ãÜ‚à•2
‚â§E‚à•¬Øz0‚àíz0
‚ãÜ‚à•2+KX
k=0"
3Œ±2L2
z‚ãÜ
Œ≥¬µgE‚à•¬Ørk‚à•2+12Œ≥
¬µgL2
g,1E‚à•zk‚àí¬Øzk‚à•2
n+2Œ≥2
n 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!#
+KX
k=012Œ≥
¬µg 
L2
g,2L2
f,0
¬µ2g+L2
f,1!
E‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk+1‚àí¬Øyk+1‚à•2
n+‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
.
It follows that
K+1X
k=0E‚à•¬Øzk‚àízk
‚ãÜ‚à•2
‚â§KX
k=09Œ±2L2
z‚ãÜ
Œ≥2¬µ2gE‚à•¬Ørk‚à•2
+ 72Œ∫2KX
k=1EŒ∫2‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oz‚à•2‚à•ÀÜek
z‚à•2
n
+ 72Œ∫2KX
k=0E"
Œ∫2‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2
n#
+KX
k=072Œ∫4E
‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
,
since z0
‚ãÜ=z1
‚ãÜandz0is consensual.
Then, we combine the results in Lemmas 9, 10 and give an upper bound of E[Ik]:
Lemma 11. Suppose that Lemmas 9 and 10 hold. Then we have:
KX
k=‚àí1E[Ik]‚â§9Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜKX
k=0E‚à•¬Ørk‚à•2+ 510 Œ∫4KX
k=0E‚àÜk
n
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥
+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+ 73Œ∫4 
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
n¬µgŒ≤!
.
(57)
Remark 6. Here I‚àí1=‚à•¬Øz0‚àíz0
‚ãÜ‚à•2+Œ∫2‚à•¬Øy0‚àíy‚ãÜ(¬Øx‚àí1)‚à•2. The aim of introducing this term is to
simplify the subsequent proofs of other lemmas.
34Proof. Lemma 10 implies that:
K+1X
k=0E‚à•¬Øzk‚àízk
‚ãÜ‚à•2
‚â§KX
k=09Œ±2L2
z‚ãÜ
Œ≥2¬µ2gE‚à•¬Ørk‚à•2
+ 72Œ∫2KX
k=1EŒ∫2‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oz‚à•2‚à•ÀÜek
z‚à•2
n
+ 72Œ∫2KX
k=0E"
Œ∫2‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2
n#
+KX
k=072Œ∫4E
‚à•¬Øyk+1‚àíy‚ãÜ(¬Øxk)‚à•2
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
Then, using Lemma 9, we have:
KX
k=‚àí1E[Ik] =K+1X
k=0E‚à•¬Øzk‚àízk
‚ãÜ‚à•2+Œ∫2K+1X
k=0E‚à•¬Øyk‚àíy‚ãÜ(¬Øxk‚àí1)‚à•2
‚â§KX
k=09Œ±2L2
z‚ãÜ
Œ≥2¬µ2gE‚à•¬Ørk‚à•2
+ 72Œ∫2KX
k=1EŒ∫2‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oz‚à•2‚à•ÀÜek
z‚à•2
n
+ 72Œ∫2KX
k=0E"
Œ∫2‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2
n#
+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+ 73Œ∫4"
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KX
k=16Œ±2
Œ≤2¬µ2gL2
y‚ãÜE‚à•¬Ørk‚à•2#
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+ 73Œ∫4"KX
k=16
¬µ2gL2
g,1E"
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
n#
+4KœÉ2
g,1
n¬µgŒ≤#
‚â§KX
k=09Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ
E‚à•¬Ørk‚à•2+KX
k=0510Œ∫4E‚àÜk
n
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥
+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+ 73Œ∫4 
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
n¬µgŒ≤!
.
C.1.7 Consensus error analysis
In this subsection we aim to bound the consensus errors of y, z, x (i.e. the terms ‚à•ÀÜek
y‚à•2,‚à•ÀÜek
z‚à•2, and
‚à•ÀÜek
x‚à•2).
Lemma 12 (Consensus error of y).Suppose that Assumptions 1- 4 hold, and
Œ≤2‚â§(1‚àí ‚à•Œìy‚à•)2
8L2
g,1‚à•O‚àí1y‚à•2‚à•Oy‚à•2‚à•Œõya‚à•2. (58)
We have
K+1X
k=0E‚à•ÀÜek
y‚à•2‚â§3KX
k=0Œ≤2‚à•O‚àí1
y‚à•2
(1‚àí ‚à•Œìy‚à•)2‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1E
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2
+‚à•Ox‚à•2
3‚à•Oy‚à•2KX
k=0E‚à•ÀÜek
x‚à•2+3(K+ 1)Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•.(59)
35Proof. Firstly, the term ‚à•ÀÜek+1
y‚à•2can be deformed as
‚à•ÀÜek+1
y‚à•2
=ŒìyÀÜek
y‚àíŒ≤O‚àí1
yÔ£Æ
Ô£∞ŒõyaÀÜU‚ä§
yh
bEk[vk]‚àí ‚àá 2g(¬Øxk,¬Øyk)i
Œõ‚àí1
ybŒõyaÀÜU‚ä§
yh
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)iÔ£π
Ô£ª‚àíŒ≤O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#2
=ŒìyÀÜek
y‚àíŒ≤O‚àí1
yÔ£Æ
Ô£∞ŒõyaÀÜU‚ä§
yh
bEk[vk]‚àí ‚àá 2g(¬Øxk,¬Øyk)i
Œõ‚àí1
ybŒõyaÀÜU‚ä§
yh
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)iÔ£π
Ô£ª2
+Œ≤2O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#2
‚àí2*
ŒìyÀÜek
y, Œ≤O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#+
+ 2Œ≤2*
O‚àí1
yÔ£Æ
Ô£∞ŒõyaÀÜU‚ä§
yh
bEk[vk]‚àí ‚àá 2g(¬Øxk,¬Øyk)i
Œõ‚àí1
ybŒõyaÀÜU‚ä§
yh
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)iÔ£π
Ô£ª,O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#+
(60)
due to Eq.(33). Then, for the first term in the right-hand side of (60), we have:
bEkÔ£Æ
Ô£∞ŒìyÀÜek
y‚àíŒ≤O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
bEk[vk]‚àí ‚àá 2g(¬Øxk,¬Øyk)i
Œõ‚àí1
ybŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)#2Ô£π
Ô£ª
‚â§‚à•Œìy‚à•‚à•ÀÜek
y‚à•2+Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•bEkÔ£Æ
Ô£∞"
ŒõyaÀÜU‚ä§
yh
bEk[vk]‚àí ‚àá 2g(¬Øxk,¬Øyk)i
Œõ‚àí1
ybŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)#2Ô£π
Ô£ª
‚â§‚à•Œìy‚à•‚à•ÀÜek
y‚à•2+Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•¬∑ ‚à•Œõya‚à•2‚à•‚àá2g(xk,yk)‚àí ‚àá 2g(¬Øxk,¬Øyk)‚à•2
+Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2bEk
‚à•‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)‚à•2
‚â§‚à•Œìy‚à•‚à•ÀÜek
y‚à•2+Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•¬∑ ‚à•Œõya‚à•2L2
g,1 
‚à•xk‚àí¬Øxk‚à•2+‚à•yk‚àí¬Øyk‚à•2
+Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1bEk
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2
,
(61)
where the first inequality uses the Jenson‚Äôs inequality, the second inequality hold since ‚à•ÀÜU‚ä§
y‚à• ‚â§1.
For the second term, we have:
bEkÔ£Æ
Ô£∞O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#2Ô£π
Ô£ª‚â§ ‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2nœÉ2
g,1. (62)
For the third them, we have:
bEk"*
ŒìyÀÜek
y, Œ≤O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#+#
= 0. (63)
36Next, for the last term, we have:
bEk*
O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
bEk[vk]‚àí ‚àá 2g(¬Øxk,¬Øyk)i
Œõ‚àí1
ybŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)#
,O‚àí1
y"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#+
‚â§1
2‚à•O‚àí1
y‚à•2bEk"
ŒõyaÀÜU‚ä§
yh
bEk[vk]‚àí ‚àá 2g(¬Øxk,¬Øyk)i
Œõ‚àí1
ybŒõyaÀÜU‚ä§
y
‚àá2g(¬Øxk+1,¬Øyk+1)‚àí ‚àá 2g(¬Øxk,¬Øyk)#2
+1
2‚à•O‚àí1
y‚à•2bEk"
ŒõyaÀÜU‚ä§
yh
vk‚àíbEk[vk]i
0#2
‚â§1
2Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2L2
g,1bEkh
‚à•xk‚àí¬Øxk‚à•2+‚à•yk‚àí¬Øyk‚à•2i
+1
2Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1bEkh
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2i
+1
2Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2nœÉ2
g,1.
(64)
Taking expectations on both sides of (60), and plugging (61), (62), (63), (64) into it, we obtain:
E
‚à•ÀÜek+1
y‚à•2
‚â§2Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1E
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2
+‚à•Œìy‚à•E‚à•ÀÜek
y‚à•2
+ 2Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•‚à•‚à•Œõya‚à•2L2
g,1E
‚à•xk‚àí¬Øxk‚à•2+‚à•yk‚àí¬Øyk‚à•2
+ 2Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2nœÉ2
g,1.
Taking summation over kand using ‚à•xk‚àí¬Øxk‚à•2‚â§ ‚à•Ox‚à•2‚à•ÀÜek
x‚à•2,‚à•yk‚àí¬Øyk‚à•2‚â§ ‚à•Oy‚à•2‚à•ÀÜek
y‚à•2, we
get:
(1‚àí ‚à•Œìy‚à•)KX
k=0E
‚à•ÀÜek
y‚à•2
‚â§2KX
k=0Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1E
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2
+E‚à•ÀÜe0
y‚à•2‚àíE‚à•ÀÜek+1
y‚à•2+ 2KX
k=0Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•‚à•‚à•Œõya‚à•2L2
g,1E
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
+ 2(K+ 1)Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2nœÉ2
g,1
‚â§2KX
k=0Œ≤2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1E
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2
+E‚à•ÀÜe0
y‚à•2‚àíE‚à•ÀÜek+1
y‚à•2+1‚àí ‚à•Œìy‚à•
4‚à•Oy‚à•2KX
k=0E
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek
y‚à•2
+ 2(K+ 1)Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2nœÉ2
g,1,
where the last inequality uses Œ≤2‚â§(1‚àí ‚à•Œìy‚à•)2
8L2
g,1‚à•O‚àí1y‚à•2‚à•Oy‚à•2‚à•Œõya‚à•2.
It follows that
K+1X
k=0E
‚à•ÀÜek
y‚à•2
‚â§3KX
k=0Œ≤2‚à•O‚àí1
y‚à•2
(1‚àí ‚à•Œìy‚à•)2‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1E
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2
+‚à•Ox‚à•2
3‚à•Oy‚à•2KX
k=0E
‚à•ÀÜek
x‚à•2
+3(K+ 1)Œ≤2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•.
37Lemma 13 (Consensus error of z).Suppose that Assumptions 1- 4 hold, and Œ≥satisfies
6Œ≥2‚à•O‚àí1
z‚à•2‚à•Oz‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•¬∑(2L2+ 2(1‚àí ‚à•Œìz‚à•)œÉ2
g,2)‚â§1‚àí ‚à•Œìz‚à•
4. (65)
We have
K+1X
k=0E
‚à•ÀÜek
z‚à•2
‚â§16Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0E
‚à•¬Øzk‚àízk
‚ãÜ‚à•2
+2E[‚à•ÀÜe0
z‚à•2]
1‚àí ‚à•Œìz‚à•
+8Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0E" 
L2
f,1+L2
g,2L2
f,0
¬µ2g+L2
g,1L2
z‚ãÜ!
‚à•¬Øxk+1‚àí¬Øxk‚à•2#
+8Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0E" 
L2
f,1+L2
g,2L2
f,0
¬µ2g!
‚à•¬Øyk+2‚àí¬Øyk+1‚à•2#
+ 16( K+ 1)nŒ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+Œ∫2
3‚à•Oz‚à•2KX
k=0E(‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2).(66)
Proof. Firstly, Eq. (34) implies that:
ÀÜek+1
z=ŒìzÀÜek
z‚àíŒ≥O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípk(¬Øxk,¬Øyk+1)i
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)#
+Œ≥O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípki
0#
.
Then using Cauchy Schwartz inequality, we get
‚à•ÀÜek+1
z‚à•2
‚â§ŒìzÀÜek
z‚àíŒ≥O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípk(¬Øxk,¬Øyk+1)i
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)#2
+Œ≥2O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípki
0#2
‚àí2*
ŒìzÀÜek
z, Œ≥O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípki
0#+
+Œ≥2O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípk(¬Øxk,¬Øyk+1)i
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)#2
+Œ≥2O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípki
0#2(67)
38To obtain the upper bound of the right-hand side of the above equation, we first estimate some
individual terms in it as follows. Note that:
eEk‚à•eEk[pk]‚àípk(¬Øxk,¬Øyk+1)‚à•2
=nX
i=1eEk‚àá2
22gi(xk
i, yk+1
i)zk
i‚àí ‚àá 2fi(xk
i, yk+1
i)‚àí 
‚àá2
22gi(¬Øxk,¬Øyk+1)z‚ãÜ
k‚àí ‚àá 2fi(¬Øxk,¬Øyk+1)2
‚â§3nX
i=1eEk‚àá2
22gi(xk
i, yk+1
i)(zk
i‚àízk
‚ãÜ)2+ 3nX
i=1eEk(‚àá2
22gi(xk
i, yk+1
i)‚àí ‚àá2
22gi(¬Øxk,¬Øyk+1))zk
‚ãÜ2
+ 3nX
i=1eEk‚àá2fi(¬Øxk,¬Øyk+1)‚àí ‚àá 2fi(xk
i, yk+1
i)2
‚â§6L2
g,1(‚à•zk‚àí¬Øzk‚à•2+‚à•¬Øzk‚àízk
‚ãÜ‚à•2) + 3 
L2
g,2L2
f,0
¬µ2g+L2
f,1!
 
‚à•xk‚àí¬Øxk‚à•2+‚à•yk+1‚àí¬Øyk+1‚à•2
(68)
and
eEk‚à•pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)‚à•2
=nX
i=1eEk‚à•‚àá2
22gi(¬Øxk+1,¬Øyk+2)zk+1
‚ãÜ‚àí ‚àá 2fi(¬Øxk+1,¬Øyk+2)‚àí ‚àá2
22gi(¬Øxk,¬Øyk+1)zk
‚ãÜ+‚àá2fi(¬Øxk,¬Øyk+1)‚à•2
‚â§3nX
i=1eEk‚à•(‚àá2
22gi(¬Øxk+1,¬Øyk+2)‚àí ‚àá2
22gi(¬Øxk,¬Øyk+1))zk+1
‚ãÜ‚à•2
+ 3nX
i=1eEk‚à•‚àá2
22gi(¬Øxk,¬Øyk+1)(zk+1
‚ãÜ‚àízk
‚ãÜ)‚à•2+ 3nX
i=1eEk‚à•‚àá2fi(¬Øxk+1,¬Øyk+2)‚àí ‚àá 2fi(¬Øxk,¬Øyk+1)‚à•2
‚â§3eEk" 
L2
f,1+L2
g,2L2
f,0
¬µ2g!
(‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+2‚àí¬Øyk+1‚à•2) +L2
g,1L2
z‚ãÜ‚à•¬Øxk‚àí¬Øxk‚àí1‚à•2#
.
(69)
Then we present the bound of the right-hand side of (67). For the first term, we have the following
evaluations:
eEk"ŒìzÀÜek
z‚àíŒ≥O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípk(¬Øxk,¬Øyk+1)i
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)##
‚â§‚à•Œìz‚à•‚à•ÀÜek
z‚à•2+Œ≥2‚à•O‚àí1
z‚à•2
1‚àí ‚à•Œìz‚à•eEkÔ£Æ
Ô£∞"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípk(¬Øxk,¬Øyk+1)i
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)#2Ô£π
Ô£ª
‚â§‚à•Œìz‚à•‚à•ÀÜek
z‚à•2+Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•eEkh
‚à•eEk[pk]‚àípk(¬Øxk,¬Øyk+1)‚à•2i
+Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
1‚àí ‚à•Œìz‚à•eEk
‚à•pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)‚à•2
(70)
where the first inequality use Jensen‚Äôs inequality and the second inequality use ‚à•ÀÜU‚ä§
z‚à• ‚â§1.
39For the second term, since zk,yk+1‚àà Uk, we have:
eEkO‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípki
0#2
‚â§‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2eEkh
‚à•eEk[pk]‚àípk2
]
‚â§2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2(‚à•zk‚à•2œÉ2
g,2+nœÉ2
f,1)
‚â§6‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2  
‚à•zk‚àí¬Øzk‚à•2+‚à•¬Øzk‚àízk
‚ãÜ‚à•2+nL2
f,1
¬µ2g!
œÉ2
g,2+nœÉ2
f,1!
.(71)
For the third term, we have:
2eEk*
ŒìzÀÜek
z, Œ≥O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípki
0#+
= 0, (72)
since ÀÜek
z‚àà Uk.
Next, for the last two terms, we have:
eEkÔ£Æ
Ô£∞O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípk(¬Øxk,¬Øyk+1)i
Œõ‚àí1
zbŒõzaÀÜU‚ä§
z
pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)#2Ô£π
Ô£ª
‚â§‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2eEkh
‚à•eEk[pk]‚àípk(¬Øxk,¬Øyk+1)‚à•2i
+‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2eEk
‚à•pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)‚à•2
.(73)
eEkÔ£Æ
Ô£∞O‚àí1
z"
ŒõzaÀÜU‚ä§
zh
eEk[pk]‚àípki
0#2Ô£π
Ô£ª‚â§ ‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2eEkh
‚à•eEk[pk]‚àípk‚à•2i
. (74)
Taking the expectation eEkon both sides of (67) and plugging (70),(71),(72),(73) and(74) into it,
we obtain:
eEk
‚à•ÀÜek+1
z‚à•2
‚â§‚à•Œìz‚à•‚à•ÀÜek
z‚à•2+2Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•eEkh
‚à•eEk[pk]‚àípk(¬Øxk,¬Øyk+1)‚à•2i
+2Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
1‚àí ‚à•Œìz‚à•eEk
‚à•pk+1(¬Øxk+1,¬Øyk+2)‚àípk(¬Øxk,¬Øyk+1)‚à•2
+ 2Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2eEk‚à•eEk[pk]‚àípk‚à•2
‚â§‚à•Œìz‚à•‚à•ÀÜek
z‚à•2+ 12nŒ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+12Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)eEk
‚à•Oz‚à•2‚à•ÀÜek
z‚à•2+‚à•¬Øzk‚àízk
‚ãÜ‚à•2
+6Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
g,2L2
f,0
¬µ2g+L2
f,1!
eEk
‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2
+6Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,1+L2
g,2L2
f,0
¬µ2g!
eEk
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+2‚àí¬Øyk+1‚à•2
+6Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•L2
g,1L2
z‚ãÜeEk
‚à•¬Øxk‚àí¬Øxk‚àí1‚à•2
,
40where the second inequality uses (36), (68), (69), and (71).
Thanks to
6Œ≥2‚à•O‚àí1
z‚à•2‚à•Oz‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•¬∑(2L2+ 2(1‚àí ‚à•Œìz‚à•)œÉ2
g,2)‚â§1‚àí ‚à•Œìz‚à•
4,
we have:
eEk
‚à•ÀÜek+1
z‚à•2
‚â§1 + 3‚à•Œìz‚à•
4‚à•ÀÜek
z‚à•2+12Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•eEk
‚à•¬Øzk‚àízk
‚ãÜ‚à•2
+ 12nŒ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+1‚àí ‚à•Œìz‚à•
4‚à•Oz‚à•2Œ∫2(‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2)
+6Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,1+L2
g,2L2
f,0
¬µ2g!
eEk
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+2‚àí¬Øyk+1‚à•2
+6Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•L2
g,1L2
z‚ãÜeEk
‚à•¬Øxk‚àí¬Øxk‚àí1‚à•2
.
Taking summation and expectation on both sides, we get:
3
4(1‚àí ‚à•Œìz‚à•)KX
k=0E
‚à•ÀÜek
z‚à•2
‚â§E‚à•ÀÜe0
z‚à•2‚àíE
‚à•ÀÜek+1
z‚à•2
+12Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•KX
k=0E
‚à•¬Øzk‚àízk
‚ãÜ‚à•2
+1‚àí ‚à•Œìz‚à•
4‚à•Oz‚à•2Œ∫2KX
k=0E(‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2)
+ 12nŒ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+6Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•KX
k=0E" 
L2
f,1+L2
g,2L2
f,0
¬µ2g+L2
g,1L2
z‚ãÜ!
‚à•¬Øxk+1‚àí¬Øxk‚à•2#
+6Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•KX
k=0E" 
L2
f,1+L2
g,2L2
f,0
¬µ2g!
‚à•¬Øyk+2‚àí¬Øyk+1‚à•2#
.
Thus,
K+1X
k=0E
‚à•ÀÜek
z‚à•2
‚â§16Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0E
‚à•¬Øzk‚àízk
‚ãÜ‚à•2
+2E[‚à•ÀÜe0
z‚à•2]
1‚àí ‚à•Œìz‚à•
+8Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0E" 
L2
f,1+L2
g,2L2
f,0
¬µ2g+L2
g,1L2
z‚ãÜ!
‚à•¬Øxk+1‚àí¬Øxk‚à•2#
+8Œ≥2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0E" 
L2
f,1+L2
g,2L2
f,0
¬µ2g!
‚à•¬Øyk+2‚àí¬Øyk+1‚à•2#
41+ 16( K+ 1)nŒ≥2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+Œ∫2
3‚à•Oz‚à•2KX
k=0E(‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2).
Lemma 14 (Consensus error of x).Suppose that Assumptions 1- 4 and Lemmas 4, 5, and 7 hold. We
have
K+1X
k=0E‚à•ÀÜek
x‚à•2
‚â§E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+6nŒ±2Œ∏(K+ 1)‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à• 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
+Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[‚àÜk+nIk]
+2eL2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2
‚à•Œõ‚àí1
xb‚à•2+2(1‚àíŒ∏)2
Œ∏2K‚àí1X
k=0Eh¬Øxk+1‚àí¬Øxk2i
.(75)
Proof. Firstly, the term ‚à•ÀÜek+1
x‚à•2can be deformed as
‚à•ÀÜek+1
x‚à•2
=ŒìxÀÜek
x‚àíŒ±O‚àí1
xÔ£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
rk+1‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª2
=ŒìxÀÜek
x‚àíŒ±O‚àí1
xÔ£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª2
+Œ±2Ek"O‚àí1
x
ŒõxaÀÜU‚ä§
x
Ek[rk+1]‚àírk+1
02#
‚àí2
ŒìxÀÜek
x, Œ±O‚àí1
x
ŒõxaÀÜU‚ä§
x
Ek[rk+1]‚àírk+1
0
+ 2Œ±2*
O‚àí1
xÔ£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª,O‚àí1
x
ŒõxaÀÜU‚ä§
x
Ek[rk+1]‚àírk+1
0+
(76)
due to Eq. (35).
42Then, for the first term of the right-hand side of (76), we use Jensen‚Äôs Inequality and get:
EkÔ£Æ
Ô£ØÔ£∞ŒìÀÜek
x‚àíŒ±O‚àí1
xÔ£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª2Ô£π
Ô£∫Ô£ª
‚â§‚à•Œìx‚à•‚à•ÀÜek
x‚à•2+Œ±2‚à•O‚àí1
x‚à•2
1‚àí ‚à•Œìx‚à•EkÔ£Æ
Ô£ØÔ£∞Ô£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª2Ô£π
Ô£∫Ô£ª
‚â§‚à•Œìx‚à•‚à•ÀÜek
x‚à•2+Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•EkEk[rk+1]‚àíe‚àáŒ¶(¬Øxk)2
,
+Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2
1‚àí ‚à•Œìx‚à•Eke‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)2
.(77)
For the second term in the right-hand side of (76), we have:
EkO‚àí1
x
ŒõxaÀÜU‚ä§
x
Ek[rk+1]‚àírk+1
02
‚â§‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2Ek‚à•Ek[rk+1]‚àírk+1‚à•2
=Œ∏2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2Ek‚à•Ek[uk]‚àíuk‚à•2.(78)
Like (72), we have:
Ek
ŒìxÀÜek
x, Œ±O‚àí1
x
ŒõxaÀÜU‚ä§
x
Ek[rk+1]‚àírk+1
0
= 0. (79)
Next, for the last term, we have:
2Œ±2Ek*
O‚àí1
xÔ£Æ
Ô£∞ŒõxaÀÜU‚ä§
xh
Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)i
Œõ‚àí1
xbŒõxaÀÜU‚ä§
xh
e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)iÔ£π
Ô£ª,O‚àí1
x
ŒõxaÀÜU‚ä§
x
Ek[rk+1]‚àírk+1
0+
‚â§Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2Ekh
‚à•Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)‚à•2+‚à•Ek[rk+1]‚àírk+1‚à•2i
+Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Ekh
‚à•e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)‚à•2i
.
(80)
Taking the expectation on both sides of (76), and plugging (77), (78), (79), (80) into it, we obtain:
Ek‚à•ÀÜek+1
x‚à•2
‚â§‚à•Œìx‚à•‚à•ÀÜek
x‚à•2+ 2Œ±2Œ∏2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2Ek‚à•Ek[uk]‚àíuk‚à•2
+2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•Ekh
‚à•Ek[rk+1]‚àíe‚àáŒ¶(¬Øxk)‚à•2+‚à•Œõ‚àí1
xb‚à•2‚à•e‚àáŒ¶(¬Øxk+1)‚àíe‚àáŒ¶(¬Øxk)‚à•2i
.
43Taking expectation and summation on both sides, we obtain:
(1‚àí ‚à•Œìx‚à•)KX
k=0E‚à•ÀÜek
x‚à•2
‚â§E‚à•ÀÜe0
x‚à•2‚àíE‚à•ÀÜek+1
x‚à•2+2eL2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõ‚àí1
xb‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•KX
k=0E‚à•¬Øxk+1‚àí¬Øxk‚à•2
+2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
1‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 2KX
k=0EEk[uk]‚àíe‚àáŒ¶(¬Øxk)2!
+2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•¬∑2eL2(1‚àíŒ∏)2
Œ∏2K‚àí1X
k=0Eh¬Øxk+1‚àí¬Øxk2i
+
2Œ±2Œ∏2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2+2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•Œ∏(1‚àíŒ∏)KX
k=0E‚à•Ek[uk]‚àíuk‚à•2
‚â§E‚à•ÀÜe0
x‚à•2‚àíE‚à•ÀÜek+1
x‚à•2+2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•1‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 6(K+ 1)nŒ±2Œ∏‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à• 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
+Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•280L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[‚àÜk+nIk]
+2eL2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•
‚à•Œõ‚àí1
xb‚à•2+2(1‚àíŒ∏)2
Œ∏2K‚àí1X
k=0Eh¬Øxk+1‚àí¬Øxk2i
.
where the first inequality uses eL- Lipschitz continuity of e‚àáŒ¶, Lemma 7, and the second inequality
uses Lemma 4 , Lemma 5 and
‚à•zk+1‚àí¬Øzk+1‚à•2‚â§‚àÜk,‚à•¬Øzk+1‚àízk+1
‚ãÜ‚à•2‚â§nIk.
Hence we get
K+1X
k=0E‚à•ÀÜek
x‚à•2
‚â§E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+6nŒ±2Œ∏(K+ 1)‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à• 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
+Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[‚àÜk+nIk]
+2eL2Œ±2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2
‚à•Œõ‚àí1
xb‚à•2+2(1‚àíŒ∏)2
Œ∏2K‚àí1X
k=0Eh¬Øxk+1‚àí¬Øxk2i
.
The following lemma gather the consensus analysis of x, y, z together:
Lemma 15. Take
Œ∑1=3Œ∫2Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2
(1‚àí ‚à•Œìy‚à•)2‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1+ 16Œ≥2L2 
2Œ∫2+L2
z‚ãÜ‚à•Oz‚à•2‚à•‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
(1‚àí ‚à•Œìz‚à•)2
44+ 4Œ∫2eL2
1 +(1‚àíŒ∏)2
Œ∏2‚à•Œõ‚àí1
xb‚à•2
Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2
(1‚àí ‚à•Œìx‚à•)2,
Œ∑2=3Œ∫2L2
g,1Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
(1‚àí ‚à•Œìy‚à•)2+ 16L2 
2Œ∫2+L2
z‚ãÜ
Œ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
(1‚àí ‚à•Œìz‚à•)2.
Suppose that Assumptions 1- 4 and Lemmas 12, 13, 14 hold, and Œ±, Œ≤ satisfy
Œ±2‚â§(1‚àí ‚à•Œìx‚à•)2
24Œ∫2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2h
80L2+ 18Œ∏(1‚àí ‚à•Œìx‚à•)
Œ∏+1‚àíŒ∏
1‚àí‚à•Œìx‚à•
œÉ2
g,2i,
Œ∑2Œ≤2‚â§1
1248L2
g,1.(82)
We have:
1
4KX
k=0E[‚àÜk] (83)
‚â§(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2KX
k=0E‚à•¬Ørk+1‚à•2+32L2
g,1Œ∑2Œ≤
¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+ 3(K+ 1)Œ∑2Œ≤2œÉ2
g,1
+Œ∫2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2Œ±2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[nIk]
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•¬∑16Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)
1‚àí ‚à•Œìz‚à•KX
k=‚àí1E[nIk]
+3Œ∫2Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+2Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+2‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•
+Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+2Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 16( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ 24( K+ 1)nŒ∫2Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
.
Proof. Adding (59), (66) and (75) together, we get:
Œ∫2‚à•Ox‚à•2K+1X
k=0E[‚à•ÀÜek
x‚à•2] +Œ∫2‚à•Oy‚à•2K+1X
k=0E[‚à•ÀÜek
y‚à•2] +‚à•Oz‚à•2K+1X
k=0E[‚à•ÀÜek
z‚à•2]
‚â§3Œ∫2KX
k=0Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2
(1‚àí ‚à•Œìy‚à•)2‚à•Œõ‚àí1
yb‚à•2‚à•Œõya‚à•2L2
g,1Eh
‚à•¬Øxk+1‚àí¬Øxk‚à•2+‚à•¬Øyk+1‚àí¬Øyk‚à•2i
+Œ∫2‚à•Ox‚à•2
3KX
k=0Eh
‚à•ÀÜek
x‚à•2i
+3Œ∫2(K+ 1)Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+2Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•
+16Œ≥2(L2
g,1+ (1‚àí ‚à•Œì‚à•)œÉ2
g,2)‚à•O‚àí1
z‚à•2‚à•Oz‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0Eh
‚à•¬Øzk‚àízk
‚ãÜ‚à•2i
+Œ∫2
3KX
k=0E(‚à•Ox‚à•2‚à•ÀÜek
x‚à•2+‚à•Oy‚à•2‚à•ÀÜek+1
y‚à•2)
+8Œ≥2(2Œ∫2+L2
z‚ãÜ)L2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0Eh
‚à•¬Øxk+1‚àí¬Øxk‚à•2i
+16Œ≥2Œ∫2L2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõ‚àí1
zb‚à•2‚à•Œõza‚à•2
(1‚àí ‚à•Œìz‚à•)2KX
k=0Eh
‚à•¬Øyk+2‚àí¬Øyk+1‚à•2i
45+ 16( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+2‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•
+Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+2Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+6nŒ∫2Œ±2Œ∏(K+ 1)‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à• 
œÉ2
f,1+ 3œÉ2
g,2L2
f,0
¬µ2g!
+Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0[‚àÜk+nIk]
+2Œ∫2eL2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2
‚à•Œõ‚àí1
xb‚à•2+2(1‚àíŒ∏)2
Œ∏2K‚àí1X
k=0E¬Øxk+1‚àí¬Øxk2
‚â§3
4K+1X
k=0E
Œ∫2‚à•Ox‚à•2‚à•ÀÜ ek
x‚à•2+Œ∫2‚à•Oy‚à•2‚à•‚à•ÀÜ ek
y‚à•2+‚à•Oz‚à•2‚à•‚à•ÀÜ ek
z‚à•2
+ (Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2KX
k=0E‚à•¬Ørk+1‚à•2+Œ∑232L2
g,1Œ≤
¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+ 3(K+ 1)Œ≤2œÉ2
g,1
+Œ∫2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2Œ±2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[nIk]
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•¬∑16Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)
1‚àí ‚à•Œìz‚à•KX
k=‚àí1E[nIk]
+3Œ∫2Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+2Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+2‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•
+Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+2Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 16( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ 24( K+ 1)nŒ∫2Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
.
where the second inequality uses (54) and
Œ∑2L2
g,1Œ≤2¬∑52 +Œ∫2Œ±2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2
‚â§1
12.
Hence:
1
4KX
k=0E[‚àÜk]
‚â§(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2KX
k=0E‚à•¬Ørk+1‚à•2+32L2
g,1Œ∑2Œ≤
¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+ 3(K+ 1)Œ∑2Œ≤2œÉ2
g,1
+Œ∫2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2Œ±2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[nIk]
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•¬∑16Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)
1‚àí ‚à•Œìz‚à•KX
k=‚àí1E[nIk]
+3Œ∫2Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+2Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+2‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•
+Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+2Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
46+ 16( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ 24( K+ 1)nŒ∫2Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
.
C.1.8 Proof of the main theorem
Before giving the final result of the convergence analysis, we present the following Lemma that
combines the results in the analysis of Ikand‚àÜk:
Lemma 16. Suppose that Assumptions 1- 4 and Lemmas 6, 11, 15 hold. If Œ±, Œ≤, Œ≥, Œ∏ satisfy
Œ±2‚â§(1‚àí ‚à•Œìx‚à•)2
16‚à•Ox‚à•2‚à•O‚àí1x‚à•2‚à•Œõxa‚à•2h
80L2+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí‚à•Œìx‚à•
(1‚àí ‚à•Œìx‚à•)œÉ2
g,2i
¬∑2040Œ∫6,
Œ≤2Œ∑2‚â§1
1024L2
g,1,
Œ≥2‚â§(1‚àí ‚à•Œìz‚à•)2
256‚à•Oz‚à•2‚à•O‚àí1z‚à•2‚à•Œõza‚à•2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)¬∑2040Œ∫4,(84)
and
40
4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ 
L2+Œ∏œÉ2
g,2
n!
‚â§1
4080Œ∫4,
(85)
then we have:
KX
k=0E[‚àÜk+nIk]‚â≤Œ∫4(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2n(Œ¶(¬Øx0)‚àíinf Œ¶)
Œ±+Œ∏(K+ 1)( œÉ2
f,1+Œ∫2œÉ2
g,2)
+Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜn(Œ¶(¬Øx0)‚àíinf Œ¶)
Œ±+Œ∏(K+ 1)( œÉ2
f,1+Œ∫2œÉ2
g,2)
+Œ∫6Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+Œ∫4Œ∑2Œ≤2(K+ 1)œÉ2
g,1
+Œ∫6‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+Œ∫4‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•+Œ∫6‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•
+Œ∫6Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2¬∑1‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ (K+ 1)Œ∫4Œ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2n
1‚àí ‚à•Œìz‚à•(œÉ2
f,1+Œ∫2œÉ2
g,2)
+ (K+ 1)Œ∫6Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2n
1‚àí ‚à•Œìx‚à•(œÉ2
f,1+Œ∫2œÉ2
g,2)
+(K+ 1)Œ≥
¬µg(œÉ2
f,1+Œ∫2œÉ2
g,2) +n‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+Œ∫4 
‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2
Œ≤¬µg+KœÉ2
g,1
¬µgŒ≤!
.
47Proof. Combining (57) and (83), we obtain
KX
k=0E[‚àÜk] +1
1020Œ∫4KX
k=‚àí1E[nIk]
‚â§4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2KX
k=0E‚à•¬Ørk+1‚à•2+128L2
g,1Œ∑2Œ≤
¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+ 12( K+ 1)Œ∑2Œ≤2œÉ2
g,1
+ 4Œ∫2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2Œ±2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[nIk]
+4‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•¬∑16Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)
1‚àí ‚à•Œìz‚à•KX
k=‚àí1E[nIk]
+12Œ∫2Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+8Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+8‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•
+4Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+8Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 64( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ 96( K+ 1)nŒ∫2Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜKX
k=0E‚à•¬Ørk‚à•2+1
2KX
k=0E[‚àÜk] +1
1020Œ∫4¬∑3n‚à•z1
‚ãÜ‚à•2
¬µgŒ≥
+(K+ 1)
1020Œ∫46Œ≥
¬µg 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+73Œ∫4
1020Œ∫4 
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
¬µgŒ≤!
.
(86)
Subtracting the term
4Œ∫2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2Œ±2
1‚àí ‚à•Œìx‚à•80L2
1‚àí ‚à•Œìx‚à•+ 18Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•
œÉ2
g,2KX
k=0E[nIk]
+4‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•16Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)
1‚àí ‚à•Œìz‚à•KX
k=‚àí1E[nIk] +1
2KX
k=0E[‚àÜk]
from both sides of (86) and using the restriction of Œ±, Œ≥ in (84), we can get:
1
2040Œ∫4 KX
k=0E[‚àÜk] +KX
k=‚àí1E[nIk]!
‚â§4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2KX
k=0E‚à•¬Ørk+1‚à•2+128L2
g,1Œ∑2Œ≤
¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+ 12( K+ 1)Œ∑2Œ≤2œÉ2
g,1
+12Œ∫2Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+8Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+8‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•
+4Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+8Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 64( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ 96( K+ 1)nŒ∫2Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
48+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜKX
k=0E‚à•¬Ørk‚à•2+(K+ 1)
1020Œ∫4¬∑6Œ≥
¬µg 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+1
1020Œ∫4¬∑3n‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+1
1020Œ∫4¬∑73Œ∫4 
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
¬µgŒ≤!
‚â§
4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ KX
k=0E‚à•¬Ørk+1‚à•2
+ 12( K+ 1)Œ∑2Œ≤2œÉ2
g,1+12Œ∫2Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+8Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•
+8‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•+4Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+8Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 64( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ 96( K+ 1)nŒ∫2Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ (K+ 1)18Œ≥
¬µg¬∑1020Œ∫4 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+1
1020Œ∫4¬∑3n‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+1
1020Œ∫4¬∑73Œ∫4 
8
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
¬µgŒ≤!
,
where the second inequality holds since
128L2
g,1Œ∑2Œ≤
¬µg‚â§1
8Œ≤¬µg.
Then taking (41) into the concern, we know:
1
2040Œ∫4 KX
k=0E[‚àÜk] +KX
k=‚àí1E[nIk]!
‚â§40
4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ
L2+Œ∏œÉ2
g,2
nKX
k=0E[‚àÜk+nIk]
+ 4
4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜn(Œ¶(¬Øx0)‚àíinf Œ¶)
Œ±
+ 12Œ∏(K+ 1)
4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ 
œÉ2
f,1+ 2œÉ2
g,2L2
f,0
¬µ2g!
+ 12( K+ 1)Œ∑2Œ≤2œÉ2
g,1+12Œ∫2Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+8Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•
+8‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•+4Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+8Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+ 64( K+ 1)nŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ 96( K+ 1)nŒ∫2Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à• 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+ (K+ 1)18Œ≥
¬µg¬∑1020Œ∫4 
L2
f,0
¬µ2gœÉ2
g,2+œÉ2
f,1!
+1
1020Œ∫4¬∑3n‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+1
1020Œ∫4¬∑73Œ∫48
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
¬µgŒ≤
,
49Since
40
4(Œ∑1+ 48Œ∫2L2
y‚ãÜŒ∑2)Œ±2+1
1020Œ∫49Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ 
L2+Œ∏œÉ2
g,2
n!
‚â§1
4080Œ∫4,
it follows that
KX
k=0E[‚àÜk+nIk]
‚â≤
Œ∫4(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜn(Œ¶(¬Øx0)‚àíinf Œ¶)
Œ±
+Œ∏(K+ 1)
Œ∫4(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ
(œÉ2
f,1+Œ∫2œÉ2
g,2)
+Œ∫6Œ≤2(K+ 1)‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+Œ∫4Œ∑2Œ≤2(K+ 1)œÉ2
g,1+Œ∫6‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•
+Œ∫4‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•+Œ∫6‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+Œ∫6Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)21‚àíŒ∏
Œ∏e‚àáŒ¶(¬Øx0)2
+
Œ∫4Œ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2n
1‚àí ‚à•Œìz‚à•+Œ≥
¬µg
(K+ 1)( œÉ2
f,1+Œ∫2œÉ2
g,2)
+ (K+ 1)Œ∫6Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2n
1‚àí ‚à•Œìx‚à•(œÉ2
f,1+Œ∫2œÉ2
g,2)
+n‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+Œ∫4 
1
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KœÉ2
g,1
¬µgŒ≤!
.
Then, we finish the proof of this lemma.
Finally, we can give the proof of Lemma 17, which is a detailed version of Theorem 1:
Lemma 17 (Detailed version of Theorem 1) .Suppose that Assumptions 1- 4 hold. Then there exist
constant step-sizes Œ±, Œ≤, Œ≥, Œ∏ , such that
1
K+ 1KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2
‚â≤Œ∫5œÉ‚àö
nK+Œ∫16
3"‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•1
3
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•1
3#
œÉ2
3
K2
3
+Œ∫7
2‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•
1‚àí ‚à•Œìy‚à•1
2œÉ1
2
K3
4
+Ô£Æ
Ô£∞Œ∫26
5 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
n(1‚àí ‚à•Œìy‚à•)2!1
5
+Œ∫6‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
n(1‚àí ‚à•Œìz‚à•)21
5Ô£π
Ô£ªœÉ2
5
K4
5
+Ô£Æ
Ô£∞Œ∫16
3 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
1‚àí ‚à•Œìy‚à•!1
3
+Œ∫14
3‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
1‚àí ‚à•Œìz‚à•1
3
+Œ∫8
3‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
1‚àí ‚à•Œìx‚à•1
3#
1
K+ 
Œ∫CŒ±+Œ∫4CŒ∏1
K,
where œÉ= max {œÉf,1, œÉg,1, œÉg,2},CŒ±, CŒ∏are defined as:
CŒ±=L‚àáŒ¶+Œ∫3‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•L
1‚àí ‚à•Œìx‚à•+Œ∫3L‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•‚à•Œõ‚àí1
xb‚à•
1‚àí ‚à•Œìx‚à•1
2
+Œ∫4L2
g,1
¬µg+œÉ2
g,1
n¬µg
+Œ∫4‚à•Oy‚à•‚à•O‚àí1
y‚à•‚à•Œõya‚à•Lg,1
1‚àí ‚à•Œìy‚à•+Œ∫9
2Lg,1 
‚à•Oy‚à•‚à•O‚àí1
y‚à•‚à•Œõya‚à•‚à•Œõ‚àí1
yb‚à•
1‚àí ‚à•Œìy‚à•!1
2
+Œ∫4
¬µg+¬µ2
gœÉ2
g,1
nL2
g,1
50+Œ∫6‚à•Oz‚à•‚à•O‚àí1
z‚à•‚à•Œõza‚à•q
L2+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2
1‚àí ‚à•Œìz‚à•+Œ∫11
2L‚à•Oz‚à•‚à•O‚àí1
z‚à•‚à•Œõza‚à•‚à•Œõ‚àí1
zb‚à•
1‚àí ‚à•Œìz‚à•1
2
,
CŒ∏=œÉ2
g,2
nL2
g,1+œÉ2
g,2
L2+ 1
Proof. Take L1=L2+ 
Œ∏(1‚àíŒ∏) +L‚àáŒ¶Œ±Œ∏2œÉ2
g,2
nand use the conclusion of Lemmas 8 and 16,
we get:
1
K+ 1KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2
‚â≤Œ¶(¬Øx0)‚àíinf Œ¶
Œ±(K+ 1)+1
n 
Œ∏(1‚àíŒ∏) +L‚àáŒ¶Œ±Œ∏2
(œÉ2
f,1+Œ∫2œÉ2
g,2) +(1‚àíŒ∏)2
Œ∏(K+ 1)‚à•‚àáŒ¶ 
¬Øx0
‚à•2
+L1
Œ∫4(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜŒ¶(¬Øx0)‚àíinf Œ¶
Œ±(K+ 1)+Œ∏
n(œÉ2
f,1+Œ∫2œÉ2
g,2)
+L1Œ∫6Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•œÉ2
g,1+L1Œ∫4Œ∑2Œ≤2œÉ2
g,1
n
+L1
K+ 1Œ∫6‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
n(1‚àí ‚à•Œìy‚à•)+Œ∫4‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
n(1‚àí ‚à•Œìz‚à•)+Œ∫6‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
n(1‚àí ‚à•Œìx‚à•)
+L1Œ∫6Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(K+ 1)(1 ‚àí ‚à•Œìx‚à•)2¬∑1‚àíŒ∏
Œ∏¬∑e‚àáŒ¶(¬Øx0)2
n
+L1
Œ∫4Œ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•+Œ≥
n¬µg
(œÉ2
f,1+Œ∫2œÉ2
g,2)
+L1Œ∫6Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•(œÉ2
f,1+Œ∫2œÉ2
g,2)
+L1‚à•z1
‚ãÜ‚à•2
(K+ 1)¬µgŒ≥+L1Œ∫41
Œ≤¬µg(K+ 1)‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+œÉ2
g,1
n¬µgŒ≤
.(87)
Define:
Œ∂y
0=1
nnX
i=1‚à•‚àá2gi(¬Øx0,¬Øy0)‚àí ‚àá 2g(¬Øx0,¬Øy0)‚à•2,
Œ∂z
0=1
nnX
i=1E
‚à•‚àá2
22gi(¬Øx0,¬Øy1)‚àí ‚àá2
22g(¬Øx0,¬Øy1)‚à•2‚à•z1
‚ãÜ‚à•2+‚à•‚àá2gi(¬Øx0,¬Øy1)‚àí ‚àá 2g(¬Øx0,¬Øy1)‚à•2
,
Œ∂x
0=1
nnX
i=1‚à•‚àá1fi(¬Øx0, y‚ãÜ(¬Øx0))‚àí ‚àá 1f(¬Øx0, y‚ãÜ(¬Øx0))‚à•2
+1
nnX
i=1‚à•‚àá2
12gi(¬Øx0, y‚ãÜ(¬Øx0))‚àí ‚àá2
12g(¬Øx0, y‚ãÜ(¬Øx0))‚à•2‚à•z1
‚ãÜ‚à•2,
ÀÜŒ∂0=1
ne‚àáŒ¶(¬Øx0)2
.
Then we take:
Œ±1=Œ∫‚àí4rn
KœÉ2, (88)
Œ±x,2=(1‚àí ‚à•Œìx‚à•)2
Œ∫10K‚à•Ox‚à•2‚à•O‚àí1x‚à•2‚à•Œõxa‚à•2œÉ21
4
Œ±y,2= 
1‚àí ‚à•Œìy‚à•
Œ∫13K‚à•Oy‚à•2‚à•O‚àí1y‚à•2‚à•Œõya‚à•2œÉ2
g,1!1
3
,
51Œ±y,3= 
n(1‚àí ‚à•Œìy‚à•)2
Œ∫21K‚à•Oy‚à•2‚à•O‚àí1y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2œÉ2
g,1!1
5
,
Œ±z,2=1‚àí ‚à•Œìz‚à•
Œ∫13K‚à•Oz‚à•2‚à•O‚àí1z‚à•2‚à•Œõza‚à•2œÉ21
3
,
Œ±z,3= 
n(1‚àí ‚à•Œìz‚à•)2
Œ∫25K‚à•Oz‚à•2‚à•O‚àí1z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2œÉ2
g,1!1
5
,
Œ±yb,2= 
1‚àí ‚à•Œìy‚à•
Œ∫13‚à•Oy‚à•2‚à•O‚àí1y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0!1
3
,
Œ±zb,2=1‚àí ‚à•Œìz‚à•
Œ∫11‚à•Oz‚à•2‚à•O‚àí1z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
01
3
,
Œ±xb,2=1‚àí ‚à•Œìx‚à•
Œ∫5‚à•Ox‚à•2‚à•O‚àí1x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
01
3
,
Œ∏1= 
nŒ∫2ÀÜŒ∂0
KœÉ2!1
2
,
Œ∏2=Œ∫3Œ±x,2,
and
Œ∏=
CŒ∏+1
Œ∏1+1
Œ∏2‚àí1
,
Œ±=Œò
CŒ±+‚àö
1‚àíŒ∏
Œ∏Œ∫3+1
Œ±1+1
Œ±y,2+1
Œ±y,3+1
Œ±z,3+1
Œ±yb,2+1
Œ±zb,2+1
Œ±xb,2+1
Œ±z,2+1
Œ±x,2‚àí1
,
Œ≤=Œò 
Œ∫4Œ±
,
Œ≥=Œò 
Œ∫4Œ±
,
(89)
52It yields L1= Œò( L2), and (45),(53),(56),(40),(58),(65),(82),(84), and (85) hold. It implies that
the restrictions on the step-sizes Œ±, Œ≤, Œ≥, Œ∏ in all previous lemma conditions hold. Thus all previous
lemmas hold. We obtain:
1
K+ 1KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2
‚â≤Œ¶(¬Øx0)‚àíinf Œ¶
Œ±K+Œ∏
n(œÉ2
f,1+Œ∫2œÉ2
g,2) +ÀÜŒ∂0
Œ∏K+Œ∫6Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•œÉ2
g,1+Œ∑2Œ∫4Œ≤2œÉ2
g,1
n
+1
K"
Œ∫6‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
n(1‚àí ‚à•Œìy‚à•)+Œ∫4‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
n(1‚àí ‚à•Œìz‚à•)+Œ∫6‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
n(1‚àí ‚à•Œìx‚à•)#
+
Œ∫4Œ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•+Œ≥
n¬µg
(œÉ2
f,1+Œ∫2œÉ2
g,2)
+Œ∫6Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•(œÉ2
f,1+Œ∫2œÉ2
g,2)
+‚à•z1
‚ãÜ‚à•2
(K+ 1)¬µgŒ≥+Œ∫4 
1
Œ≤¬µg(K+ 1)‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+œÉ2
g,1
n¬µgŒ≤!
‚â≤Œ∏
n(œÉ2
f,1+Œ∫2œÉ2
g,2) +1
Œ∏K+Œ∫
Œ±K+Œ∫9œÉ2
g,1
nŒ±+Œ∫14Œ±2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•œÉ2
g,1
+ 
Œ∫10‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
(1‚àí ‚à•Œìy‚à•)2+Œ∫14‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
(1‚àí ‚à•Œìz‚à•)2!
Œ∫12Œ±4œÉ2
g,1
n
+Œ±2Œ∫14‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
K(1‚àí ‚à•Œìy‚à•)+Œ±2Œ∫12‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
K(1‚àí ‚à•Œìz‚à•)
+Œ±2Œ∫6‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
K(1‚àí ‚à•Œìx‚à•)
+
Œ∫12Œ±2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•+Œ∫6Œ±2Œ∏‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2+Œ∫5Œ±
n
(œÉ2
f,1+Œ∫2œÉ2
g,2)
‚â≤Œ∏1
n(œÉ2
f,1+Œ∫2œÉ2
g,2) +Œ∫4
Œ∏1K+CŒ∏Œ∫4
K+Œ∫
Œ±1K+Œ∫9œÉ2
g,1
nŒ±1+Œ∫5Œ±1
n(œÉ2
f,1+Œ∫2œÉ2
g,2)
+Œ∫14Œ±2
y,2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•œÉ2
g,1+Œ∫
Œ±y,2K
+Œ∫10‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
(1‚àí ‚à•Œìy‚à•)2Œ∫12Œ±4
y,3œÉ2
g,1
n+Œ∫
Œ±y,3K
+Œ∫14‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
(1‚àí ‚à•Œìz‚à•)2Œ∫12Œ±4
z,3œÉ2
g,1
n+Œ∫
Œ±z,3K
+Œ±2
yb,2Œ∫14‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
K(1‚àí ‚à•Œìy‚à•)+Œ∫
Œ±yb,2K
+Œ±2
zb,2Œ∫12‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
K(1‚àí ‚à•Œìz‚à•)+Œ∫
Œ±zb,2K
+Œ±2
xb,2Œ∫6‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
K(1‚àí ‚à•Œìx‚à•)+Œ∫
Œ±xb,2K
+Œ∫12Œ±2
z,2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•(œÉ2
f,1+Œ∫2œÉ2
g,2) +Œ∫
Œ±z,2K
+Œ∫6Œ±2
x,2Œ∏2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2(œÉ2
f,1+Œ∫2œÉ2
g,2) +Œ∫
Œ±x,2K+Œ∫4
Œ∏2K,
53where the last inequality uses (89).
Finally, substituting (88) and (89) into the last inequality, we can get:
1
K+ 1KX
k=0E‚à•‚àáŒ¶(¬Øxk)‚à•2
‚â≤Œ∫5œÉ‚àö
nK+Œ∫16
3Ô£Æ
Ô£∞ 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•!1
3
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•1
3Ô£π
Ô£ªœÉ2
3
K2
3
+Œ∫7
2‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•
1‚àí ‚à•Œìy‚à•1
2œÉ1
2
K3
4
+Ô£Æ
Ô£∞Œ∫26
5 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
n(1‚àí ‚à•Œìy‚à•)2!1
5
+Œ∫6‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
n(1‚àí ‚à•Œìz‚à•)21
5Ô£π
Ô£ªœÉ2
5
K4
5
+Ô£Æ
Ô£∞Œ∫16
3 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
1‚àí ‚à•Œìy‚à•!1
3
+Œ∫14
3‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
1‚àí ‚à•Œìz‚à•1
3
+Œ∫8
3‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
1‚àí ‚à•Œìx‚à•1
3#
1
K+ 
Œ∫CŒ±+Œ∫4CŒ∏1
K,
where œÉ= max {œÉf,1, œÉg,1, œÉg,2}.
Remark 7. From the proof of Lemma 17, the impact of the moving average technique on variance
reduction becomes evident. The termŒ∏
nœÉ2absorb Œ±2Œ∑1œÉ2, which includes the high order term Œ±4œÉ2.
Additionally, compared to y, z, the quadratic term related to œÉ2ofxhas an extra term Œ∏multiplied in
the numerator ( Œ±2Œ∏œÉ2). These details reduce the impacts of noise to terms related to x, confirming
the conclusion that terms related to y, zdominate the rate in precious sections. Notably, taking Œ∏ <1
is indispensable our proof. If we take Œ∏= 1, there would be a constant term1
nœÉ2in the convergence
rate (see the first inequality of (87)), since the coefficient Œ±2/Œ≤2+Œ±2/Œ≥2=O(1). This would not
guarantee the convergence of SPARKLE .
C.2 Analysis of consensus error and transient iteration complexity
From Lemma 17, we can immediately obtain the transient time complexity of Algorithm 1. Here we
omit the impacts of the condition number Œ∫.
Lemma 18. The transient time complexity of Algorithm 1 has an upper bound of:
maxÔ£±
Ô£≤
Ô£≥n3 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•!2
‚à•Œõya‚à•2, n3‚à•Oz‚à•2‚à•O‚àí1
z‚à•2
1‚àí ‚à•Œìz‚à•2
‚à•Œõza‚à•2,
n2‚à•Ox‚à•‚à•O‚àí1
x‚à•
1‚àí ‚à•Œìx‚à•2
‚à•Œõxa‚à•2, n 
‚à•Oy‚à•‚à•O‚àí1
y‚à•‚à•Œõ‚àí1
yb‚à•
1‚àí ‚à•Œìy‚à•!4
3
‚à•Œõya‚à•,
n‚à•Oz‚à•‚à•O‚àí1
z‚à•‚à•Œõ‚àí1
zb‚à•
1‚àí ‚à•Œìz‚à•4
3
‚à•Œõza‚à•, n‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2
1‚àí ‚à•Œìx‚à•2
3
,
n‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•‚à•Œõ‚àí1
xb‚à•
1‚àí ‚à•Œìx‚à•, n
.(90)
Proof. According to lemma 17, SPARKLE achieves linear speedup if:
1‚àö
nK‚â≥Ô£Æ
Ô£∞ 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•!1
3
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•1
3Ô£π
Ô£ª1
K2
3
54+‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•
1‚àí ‚à•Œìy‚à•1
21
K3
4
+Ô£Æ
Ô£∞ 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
n(1‚àí ‚à•Œìy‚à•)2!1
5
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
n(1‚àí ‚à•Œìz‚à•)21
5Ô£π
Ô£ª1
K4
5
+Ô£Æ
Ô£∞ 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
1‚àí ‚à•Œìy‚à•!1
3
+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
1‚àí ‚à•Œìz‚à•1
3
+‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
1‚àí ‚à•Œìx‚à•1
3#
1
K+ (CŒ±+CŒ∏)1
K.
It holds when Ksatisfies:
 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•!1
31
K2
3‚â≤1‚àö
nK,
‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•1
31
K2
3‚â≤1‚àö
nK,
 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
n(1‚àí ‚à•Œìy‚à•)2!1
51
K4
5‚â≤1‚àö
nK,
‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•
1‚àí ‚à•Œìy‚à•1
21
K3
4‚â≤1‚àö
nK,
‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
n(1‚àí ‚à•Œìz‚à•)21
51
K4
5‚â≤1‚àö
nK,
 
‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
1‚àí ‚à•Œìy‚à•!1
31
K‚â≤1‚àö
nK,
‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
1‚àí ‚à•Œìz‚à•1
31
K‚â≤1‚àö
nK,
‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
1‚àí ‚à•Œìx‚à•1
31
K‚â≤1‚àö
nK,
(CŒ±+CŒ∏)1
K‚â≤1‚àö
nK.
Then we get:
K‚â≥max(
n3‚à•Oy‚à•2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•2
‚à•Œõya‚à•2, n3‚à•Oz‚à•2‚à•O‚àí1
z‚à•2
1‚àí ‚à•Œìz‚à•2
‚à•Œõza‚à•2,
n2‚à•Ox‚à•‚à•O‚àí1
x‚à•
1‚àí ‚à•Œìx‚à•2
‚à•Œõxa‚à•2, n 
‚à•Oy‚à•‚à•O‚àí1
y‚à•‚à•Œõ‚àí1
yb‚à•
1‚àí ‚à•Œìy‚à•!4
3
‚à•Œõya‚à•,
n‚à•Oz‚à•‚à•O‚àí1
z‚à•‚à•Œõ‚àí1
zb‚à•
1‚àí ‚à•Œìz‚à•4
3
‚à•Œõza‚à•, n‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2
1‚àí ‚à•Œìx‚à•2
3
,
n‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•‚à•Œõ‚àí1
xb‚à•
1‚àí ‚à•Œìx‚à•, n
.
55C.2.1 Consensus Error
Lemma 19. Suppose that Assumptions 1- 4 hold. Then there exist constant step-sizes Œ±, Œ≤, Œ≥, Œ∏ , such
that Lemma 17 holds and
1
KKX
k=0E‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
n
‚â≤Kn
K 
‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•+‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•!
,
where‚â≤Kdenotes the the asymptotic rate when K‚Üí ‚àû .
Proof. Suppose Œ±,Œ≤,Œ≥, and Œ∏satisfy the constraints given in (88) and(89), which ensures that
Theorem 1 (Lemma 17) holds.
For clarity, we define the constants:
c1=9Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+438Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜ, c 2= 10 
L2+Œ∏œÉ2
g,2
n!
.
Then there exist Œ±,Œ≤,Œ≥, and Œ∏that satisfy the constraints in (88) and (89), and also:
c1‚â§0.01L‚àí2, c 2‚â§11L2. (91)
We take such values for step-sizes in the following proof.
We proceed by substituting (41) into (57), yielding:
KX
k=‚àí1E[Ik]‚â§4c1 
Œ¶(¬Øx0)‚àíinf Œ¶
Œ±+c2K‚àí1X
k=0E‚àÜk
n+Ik
+3Œ∏
nK 
œÉ2
f,1+ 2œÉ2
g,2L2
f,0
¬µ2g!!
+ 510 Œ∫4KX
k=0E‚àÜk
n
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥
+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+ 73Œ∫4 
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
n¬µgŒ≤!
.
Subtracting 4c1c2PK‚àí1
k=0E[Ik]from both sides, we get:
KX
k=‚àí1E[Ik]‚â≤Œ¶(¬Øx0)‚àíinf Œ¶
Œ±+Œ∏
nK 
œÉ2
f,1+œÉ2
g,2L2
f,0
¬µ2g!
+Œ∫4KX
k=0E‚àÜk
n
+‚à•z1
‚ãÜ‚à•2
¬µgŒ≥
+KŒ≥
¬µgn 
œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+Œ∫4 
1
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KœÉ2
g,1
n¬µgŒ≤!
.
Substituting (57) into (41), we obtain:
1
4KX
k=0E¬Ørk+12
‚â§Œ¶(¬Øx0)‚àíinf Œ¶
Œ±+c2KX
k=0E‚àÜk
n
+c2c1KX
k=0E‚à•¬Ørk‚à•2
+c2"
510Œ∫4KX
k=0E‚àÜk
n
+3‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+6(K+ 1)Œ≥
¬µgn 
3œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+73Œ∫4 
4
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+4KœÉ2
g,1
n¬µgŒ≤!#
+3Œ∏
n(K+ 1) 
œÉ2
f,1+ 2œÉ2
g,2L2
f,0
¬µ2g!
.
56Subtracting c2c1PK
k=0E‚à•¬Ørk‚à•2from both sides, we get
KX
k=0E¬Ørk+12
‚â≤Œ¶(¬Øx0)‚àíinf Œ¶
Œ±+Œ∫4KX
k=0E‚àÜk
n
+Œ∏
nK 
œÉ2
f,1+œÉ2
g,2L2
f,0
¬µ2g!
+‚à•z1
‚ãÜ‚à•2
¬µgŒ≥+KŒ≥
¬µgn 
œÉ2
g,2L2
f,0
¬µ2g+œÉ2
f,1!
+Œ∫4 
1
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KœÉ2
g,1
n¬µgŒ≤!
.
Taking
Œ∑3= 
Œ∫2‚à•O‚àí1
x‚à•2‚à•Ox‚à•2‚à•Œõxa‚à•2Œ±2
(1‚àí ‚à•Œìx‚à•)2+‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•¬∑Œ≥2(L2
g,1+ (1‚àí ‚à•Œìz‚à•)œÉ2
g,2)
1‚àí ‚à•Œìz‚à•!
,
and combining previous results with (83), we obtain
KX
k=0E[‚àÜk]
‚â≤(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2KX
k=0E‚à•¬Ørk+1‚à•2+Œ∫Œ∑2Œ≤‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KŒ∑2Œ≤2œÉ2
g,1+Œ∑3KX
k=‚àí1E[nIk]
+Œ∫2Œ≤2K‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1+Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•
+Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•+Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
Œ∏(1‚àí ‚à•Œìx‚à•)2e‚àáŒ¶(¬Øx0)2
+KnŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
Œ∫2œÉ2
g,2+œÉ2
f,1
+KnŒ∫2Œ±2Œ∏‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2 
Œ∫2œÉ2
g,2+œÉ2
f,1
‚â≤
(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ∑3
¬∑Œ∫4KX
k=0E[‚àÜk] +Œ∫Œ∑2Œ≤‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2+KŒ∑2Œ≤2œÉ2
g,1
+n
(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ∑31
Œ±+Œ∏
nK 
œÉ2
f,1+Œ∫2œÉ2
g,2
+1
¬µgŒ≥+KŒ≥
¬µgn 
Œ∫2œÉ2
g,2+œÉ2
f,1
+Œ∫4 
1
Œ≤¬µg+KœÉ2
g,1
n¬µgŒ≤!#
+Œ∫2Œ≤2K‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•nœÉ2
g,1
+Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
Œ∏(1‚àí ‚à•Œìx‚à•)2e‚àáŒ¶(¬Øx0)2
+Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
1‚àí ‚à•Œìy‚à•+‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
1‚àí ‚à•Œìz‚à•+Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
1‚àí ‚à•Œìx‚à•
+KnŒ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
Œ∫2œÉ2
g,2+œÉ2
f,1
+KnŒ∫2Œ±2Œ∏‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2 
Œ∫2œÉ2
g,2+œÉ2
f,1
.
(88) and (89) imply that
Œ∑1‚â≤Œ∫2+Œ∫2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2, Œ∑ 2‚â≤Œ∫2,
(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2‚â≤Œ∫‚àí4, Œ∑ 3‚â≤Œ∫‚àí4
57where Œ∑1, Œ∑2are defined in Lemma 15.
Then taking Œ±, Œ≤, Œ≥, Œ∏ such that (88),(89),(91) hold and Œ∫4[(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ∑3]is a sufficiently
small constant, we can derive the following result:
1
KKX
k=0E‚àÜk
n
‚â≤Œ∫Œ∑2Œ≤
K+Œ∑2Œ≤2œÉ2
g,1
n
+
(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ∑31
Œ±K+Œ∏
n 
œÉ2
f,1+Œ∫2œÉ2
g,2
+1
¬µgŒ≥K+Œ≥
¬µgn 
Œ∫2œÉ2
g,2+œÉ2
f,1
+
(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ∑3
Œ∫4 
1
Œ≤¬µgK+œÉ2
g,1
n¬µgŒ≤!
+Œ∫2Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•œÉ2
g,1
+Œ∫2Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
Œ∏K(1‚àí ‚à•Œìx‚à•)2+Œ∫2‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
(1‚àí ‚à•Œìy‚à•)Kn+‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
(1‚àí ‚à•Œìz‚à•)Kn+Œ∫2‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
(1‚àí ‚à•Œìx‚à•)Kn
+Œ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
Œ∫2œÉ2
g,2+œÉ2
f,1
+Œ∫2Œ±2Œ∏‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2 
Œ∫2œÉ2
g,2+œÉ2
f,1
‚â≤Œ∫5Œ∑2Œ±
K+Œ∫10Œ±2œÉ2
g,1
n+Œ∫
Kh
(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±+Œ∑3
Œ±i
+
(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ∑3"
Œ∏
n 
œÉ2
f,1+Œ∫2œÉ2
g,2
+Œ∫5Œ±
n 
Œ∫2œÉ2
g,2+œÉ2
f,1
+Œ∫9œÉ2
g,1
nŒ±#
+Œ∫2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•œÉ2
g,1Œ∫8Œ±2+Œ∫‚àí1Œ±‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
K(1‚àí ‚à•Œìx‚à•)2
+Œ±2Œ∫10‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
K(1‚àí ‚à•Œìy‚à•)+Œ±2Œ∫8‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
K(1‚àí ‚à•Œìz‚à•)
+Œ±2Œ∫2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
K(1‚àí ‚à•Œìx‚à•)
+Œ∫8Œ±2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à• 
Œ∫2œÉ2
g,2+œÉ2
f,1
+Œ∫2Œ±2Œ∏‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
(1‚àí ‚à•Œìx‚à•)2 
Œ∫2œÉ2
g,2+œÉ2
f,1
.
From (88) and (89), we can determine the asymptotic orders for Œ±, Œ≤, Œ≥ andŒ∏when K‚Üí ‚àû
Œ±=O
Œ∫‚àí4rn
KœÉ2
, Œ≤ =Orn
KœÉ2
, Œ≥ =Orn
KœÉ2
, Œ∏ =O
Œ∫rn
KœÉ2
.
Then we get
1
KKX
k=0E‚àÜk
n
‚â≤KŒ∫2n
K 
‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•+‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•!
,
where‚â≤Kdenotes the the asymptotic rate when K‚Üí ‚àû .
Then using (36) and the definition of ‚àÜk, we get
1
KKX
k=0E‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
n
‚â≤Kn
K 
‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2
1‚àí ‚à•Œìz‚à•+‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2
1‚àí ‚à•Œìy‚à•!
.
58In particular, the corresponding result of SPARKLE variants that using EXTRA, ED or GT is
1
KKX
k=0E‚à•xk‚àí¬Øxk‚à•2
n+‚à•yk‚àí¬Øyk‚à•2
n
‚â≤Kn
K1
1‚àíœÅy+1
1‚àíœÅz
,
where œÅy, œÅzare spectrum gaps of relevant mixing matrices.
C.2.2 Essential matrix norms for analysis
Common heterogeneity-correction algorithms, including ED, EXTRA and GT, satisfy Assumption
3, according to transformations (31),(32) and discussions in [ 2, Appendix B.2]. Then Lemma 3
ensures that ‚à•Œì‚à•<1. From Lemma 18, the transient time complexity depends on the coefficients
‚à•O‚à•2,‚à•O‚àí1‚à•2,‚à•Œõa‚à•2,‚à•Œõ‚àí1
b‚à•2, and‚à•Œì‚à•2. The solution of these matrices is constructive. Table 4
presents the upper bounds of these coefficients with different communication modes. Please refer
to [2, Appendix B.2] for more details about the construction of these matrices and the computation
of relevant norms. It is required that Wis positive definite for ED, EXTRA, and we denote the
smallest nonzero eigenvalue of WbyœÅ.œÅcan view as a constant. Otherwise we replace Wwith
tI+ (1‚àít)Wfor some constant t‚àà(0,1)(e.g.t= 1/2).
Substituting values of ‚à•Os‚à•,‚à•O‚àí1
s‚à•,‚à•Œõsa‚à•,‚à•Œõ‚àí1
sb‚à•,‚à•Œìs‚à•into(90) , we obtain the explicit transient
iteration complexity for some specific examples of Algorithm 1, which are listed in Table 2. Note
that all GT variants exhibit the same transient iteration complexity.
Table 4: Upper bounds of coefficients for different heterogeneity-correction modes in Lemma 18,
where notation Ois omitted for ‚à•O‚à•and‚à•O‚àí1‚à•.
Mode A B C ‚à•O‚à• ‚à•O‚àí1‚à• ‚à•Œõa‚à• ‚à• Œõ‚àí1
b‚à• ‚à• Œì‚à•
ED W (I‚àíW)1
2W 1 œÅ‚àí1
2 œÅ (1‚àíœÅ)‚àí1
2‚àöœÅ
EXTRA I (I‚àíW)1
2W 1 œÅ‚àí1
2 1 (1 ‚àíœÅ)‚àí1
2‚àöœÅ
ATC-GT W2I‚àíW W21 1 œÅ2(1‚àíœÅ)‚àí1 1+œÅ
2
Semi-ATC-GT W I ‚àíW W21 1 œÅ (1‚àíœÅ)‚àí1 1+œÅ
2
Non-ATC-GT I I ‚àíW W21 1 1 (1 ‚àíœÅ)‚àí1 1+œÅ
2
C.2.3 Theoretical gap between upper-level and lower-level
Note that ‚à•Œõsa‚à• ‚â§1. We rewrite the upper bound of the transient iteration complexity in Lemma 18
as
max{n3Œ¥y, n3Œ¥z, n2Œ¥x, nÀÜŒ¥y, nÀÜŒ¥z, nÀÜŒ¥x} (92)
where
Œ¥y=‚à•Oy‚à•2‚à•O‚àí1
y‚à•2
1‚àí ‚à•Œìy‚à•2
‚à•Œõya‚à•2, Œ¥z=‚à•Oz‚à•2‚à•O‚àí1
z‚à•2
1‚àí ‚à•Œìz‚à•2
‚à•Œõza‚à•2, Œ¥x=‚à•Ox‚à•‚à•O‚àí1
x‚à•
1‚àí ‚à•Œìz‚à•‚à•Œõza‚à•2
,
ÀÜŒ¥y= 
‚à•Oy‚à•‚à•O‚àí1
y‚à•‚à•Œõ‚àí1
yb‚à•
1‚àí ‚à•Œìy‚à•!4
3
,ÀÜŒ¥z=‚à•Oz‚à•‚à•O‚àí1
z‚à•‚à•Œõ‚àí1
zb‚à•
1‚àí ‚à•Œìz‚à•4
3
,
ÀÜŒ¥x=‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõ‚àí1
xb‚à•2
1‚àí ‚à•Œìx‚à•2
3
+‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõ‚àí1
xb‚à•
1‚àí ‚à•Œìx‚à•.
(93)
Suppose that we use the same communication matrices and heterogeneity-correction methods for
updating x, y, z ,i.e.
‚à•Ox‚à•=‚à•Oy‚à•=‚à•Oz‚à•,‚à•O‚àí1
x‚à•=‚à•O‚àí1
y‚à•=‚à•O‚àí1
z‚à•,‚à•Œìx‚à•=‚à•Œìy‚à•=‚à•Œìz‚à•,
‚à•Œõxa‚à•=‚à•Œõya‚à•=‚à•Œõza‚à•,‚à•Œõ‚àí1
xb‚à•=‚à•Œõ‚àí1
yb‚à•=‚à•Œõ‚àí1
zb‚à•.
59Then we have
Œ¥x‚â≤Œ¥y=Œ¥z,ÀÜŒ¥x‚â≤ÀÜŒ¥y=ÀÜŒ¥z, (94)
Now we fix the update strategies for y, z.(94) implies that we can appropriately increase Œ¥x,ÀÜŒ¥xwhile
keeping the transient iteration complexity (92) unchanged (at most scaled by a constant factor). For
example, we can use a moderately sparser communication network for updating xthany, z. We
illustrate this point with three examples: SPARKLE -ED, SPARKLE -EXTRA and SPARKLE -GT
(variants), where y, zshare the same communication matrix Wy.
‚Ä¢ SPARKLE-ED, SPARKLE-EXTRA: From Table 4, we have
Œ¥x=O 
(1‚àíœÅ(Wx))‚àí2
, Œ¥y=Œ¥z=O 
(1‚àíœÅ(Wy))‚àí2
,
ÀÜŒ¥x=O
(1‚àíœÅ(Wx))‚àí3
2
,ÀÜŒ¥y=ÀÜŒ¥z=O 
(1‚àíœÅ(Wy))‚àí2
.
Substituting these values into (92), we get the transient iteration complexity is bounded by
max
n2(1‚àíœÅ(Wx))‚àí2, n3(1‚àíœÅ(Wy))‚àí2	
SPARKLE-ED will keep the transient iteration complexity n3(1‚àíœÅ(Wy))‚àí2(the dominated
term) if
(1‚àíœÅ(Wx))‚àí1‚â≤‚àön(1‚àíœÅ(Wy))‚àí1. (95)
‚Ä¢ SPARKLE-GT variants: Results in Table 4 imply that
Œ¥x=O 
(1‚àíœÅ(Wx))‚àí2
, Œ¥y=Œ¥z=O 
(1‚àíœÅ(Wy))‚àí2
,
ÀÜŒ¥x=O 
(1‚àíœÅ(Wx))‚àí2
,ÀÜŒ¥y=ÀÜŒ¥z=O
(1‚àíœÅ(Wy))‚àí8
3
.
Following the same argument as before, we have the following upper bound of the transient
iteration complexity of SPARKLE-GT
maxn
n2(1‚àíœÅ(Wx))‚àí2, n3(1‚àíœÅ(Wy))‚àí2, n(1‚àíœÅ(Wy))‚àí8
3o
.
we get the constraints of the spectral gap 1‚àíœÅ(Wx)that maintains the transient iteration
complexity maxn
n3(1‚àíœÅ(Wy))‚àí2, n(1‚àíœÅ(Wy))‚àí8
3o
:
(1‚àíœÅ(Wx))‚àí1‚â≤maxn‚àön(1‚àíœÅ(Wy))‚àí1, n‚àí1/2(1‚àíœÅ(Wy))‚àí4
3o
. (96)
Denote the communication times per agent of Wx,Wybycx, cyrespectively. For example, we have
cx= 2,cy=n‚àí1when taking Ring Graph for x(i.e.[Wx]ijÃ∏= 0iff|i‚àíj| ‚àà {0,1, n‚àí1}), and
Complete Graph for y(i.e.Wy=1
n1n1‚ä§
n).
Then for each agent, the communication cost per round is O(cxp+cyq). If we take a=cx/cyto
measure the relative sparsity of the two communication matrices, and consider cy=O(1), then for
each agent, the communication cost per round is O(ap+q).(95) and(96) theoretically provide the
range of the sparsity (connectivity) degree of Wxrelative to Wy. From (95) and(96), we can set
a‚â™1, while maintaining the transient iteration complexity for SPARKLE -GT, SPARKLE -ED,
SPARKLE-EXTRA.
C.2.4 The transient iteration complexities of some specific examples in SPARKLE.
Now we compute the transient iteration complexities of each SPARKLE- L-Ualgorithm, where
L,U‚àà {GT (variants) ,ED,EXTRA }. For brevity, here we assume that Wx=Wy=Wz, use the
same heterogeneity-correction method to y, z, and denote the spectral gap 1‚àíœÅ(Wx)by1‚àíœÅ.
Substituting the results in Table 4 into (92) and (93), we get
Œ¥x=O1
(1‚àíœÅ)2
, Œ¥y=Œ¥z=O1
(1‚àíœÅ)2
for any L,U‚àà {GT (variants) ,ED,EXTRA },
ÀÜŒ¥x=O1
(1‚àíœÅ)2
,O1
(1‚àíœÅ)3/2
,O1
(1‚àíœÅ)3/2
60forU={GT (variants) ,ED,EXTRA }respectively, and
ÀÜŒ¥y=ÀÜŒ¥z=O1
(1‚àíœÅ)8/3
,O1
(1‚àíœÅ)2
,O1
(1‚àíœÅ)2
forL={GT (variants) ,ED,EXTRA }respectively.
Combining the above results, we can directly obtain Table 2, the transient iteration complexities of
SPARKLE with mixed heterogeneity-correction techniques in different levels.
C.3 Convergence analysis in deterministic scenarios
The following lemma gives the convergence rate of Algorithm 1 without a moving average when
there is no sample noise:
Lemma 20. Suppose that Assumptions 1- 4 hold. If œÉ2= 0, then there exist Œ±, Œ≤, Œ≥ andŒ∏= 1such
that
1
K+ 1KX
k=0E‚à•Œ¶(¬Øxk)‚à•2
‚â≤ 
Œ∫16‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
1‚àí ‚à•Œìy‚à•!1
31
K+Œ∫14‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
1‚àí ‚à•Œìz‚à•1
31
K
+Œ∫8‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
1‚àí ‚à•Œìx‚à•1
31
K+eCŒ±1
K.
whereeCŒ±is a series of overheads which is defined below.
Proof. Note that œÉ2= 0implies that L1= Œò( L2)when Œ±=O(L‚àí1
‚àáŒ¶). Thus (87) implies that:
1
K+ 1KX
k=0E‚à•Œ¶(¬Øxk)‚à•2
‚â≤Œ¶(¬Øx0)‚àíinf Œ¶
Œ±(K+ 1)
+L2
Œ∫4(Œ∑1+Œ∫2L2
y‚ãÜŒ∑2)Œ±2+Œ±2L2
z‚ãÜ
Œ≥2¬µ2g+Œ∫4Œ±2
Œ≤2¬µ2gL2
y‚ãÜŒ¶(¬Øx0)‚àíinf Œ¶
Œ±(K+ 1)
+L2Œ∫6‚à•Oy‚à•2E‚à•ÀÜe0
y‚à•2
n(K+ 1)(1 ‚àí ‚à•Œìy‚à•)+L2Œ∫4‚à•Oz‚à•2E‚à•ÀÜe0
z‚à•2
n(K+ 1)(1 ‚àí ‚à•Œìz‚à•)+L2Œ∫6‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
n(K+ 1)(1 ‚àí ‚à•Œìx‚à•)
+L2‚à•z1
‚ãÜ‚à•2
¬µgŒ≥(K+ 1)+L2Œ∫4
K+ 11
Œ≤¬µg‚à•¬Øy0‚àíy‚ãÜ(¬Øx0)‚à•2.(97)
61Then we aim to choose the stepsize Œ±, Œ≤, Œ≥ . Define:
eCŒ±=L‚àáŒ¶+Œ∫3‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•L
1‚àí ‚à•Œìx‚à•+Œ∫3L‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•‚à•Œõ‚àí1
xb‚à•
1‚àí ‚à•Œìx‚à•1
2
+Œ∫4L2
g,1
¬µg+Œ∫4‚à•Oy‚à•‚à•O‚àí1
y‚à•‚à•Œõya‚à•Lg,1
1‚àí ‚à•Œìy‚à•+Œ∫4Lg,1 
Œ∫‚à•Oy‚à•‚à•O‚àí1
y‚à•‚à•Œõya‚à•‚à•Œõ‚àí1
yb‚à•
1‚àí ‚à•Œìy‚à•!1
2
+Œ∫6L‚à•Oz‚à•‚à•O‚àí1
z‚à•‚à•Œõza‚à•
1‚àí ‚à•Œìz‚à•+Œ∫11
2L‚à•Oz‚à•‚à•O‚àí1
z‚à•‚à•Œõza‚à•‚à•Œõ‚àí1
zb‚à•
1‚àí ‚à•Œìz‚à•1
2
,
eŒ±yb,2= 
1‚àí ‚à•Œìy‚à•
Œ∫13‚à•Oy‚à•2‚à•O‚àí1y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0!1
3
,
eŒ±zb,2=1‚àí ‚à•Œìz‚à•
Œ∫11‚à•Oz‚à•2‚à•O‚àí1z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
01
3
,
eŒ±xb,2=1‚àí ‚à•Œìx‚à•
Œ∫5‚à•Ox‚à•2‚à•O‚àí1x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
01
3
.
Then there exist
Œ±= Œò
eCŒ±+eŒ±‚àí1
xb,2+eŒ±‚àí1
yb,2+eŒ±‚àí1
zb,2‚àí1
, Œ≤= Œò 
Œ∫4Œ±
, Œ≥= Œò 
Œ∫4Œ±
such that (45), (53), (56), (40), (58), (65), (82), and (84) hold. Then all previous lemmas hold.
Then from (97) we have:
1
K+ 1KX
k=0E‚à•Œ¶(¬Øxk)‚à•2
‚â≤Œ∫
Œ±K+Œ±2Œ∫14‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
K(1‚àí ‚à•Œìy‚à•)
+Œ±2Œ∫12‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
K(1‚àí ‚à•Œìz‚à•)+Œ±2Œ∫6‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
K(1‚àí ‚à•Œìx‚à•)
‚â≤ 
Œ∫16‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2Œ∂y
0
1‚àí ‚à•Œìy‚à•!1
31
K+Œ∫14‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2Œ∂z
0
1‚àí ‚à•Œìz‚à•1
31
K
+Œ∫8‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
1‚àí ‚à•Œìx‚à•1
31
K+eCŒ±1
K.
C.4 Degenerating to single-level algorithms
We consider the bilevel problem with the following upper- and lower-level loss function on the i-th
agent:
Fi(x, y, œï ) =Fi(x, œï), G i(x, y, Œæ )‚â°‚à•y‚à•2
2.
Actually, this optimization problem with respect to xis single-level, since we have zk‚â°0,yk‚â°0,
uk
i=‚àá1fi(xk
i, Œæk
i)by induction. By taking Œ∏= 1, we get the following single-level algorithm
framework for decentralized stochastic single-level algorithm. As we discuss in previous sections, it
can recover various heterogeneity-correction algorithms, including GT, EXTRA and ED, by selecting
specific Ax,Bx,Cx.
62Algorithm 3 SPARKLE: degenerating to single-level decentralized stochastic algorithms
Require: Initialize x0=0,d0
x=0, learning rate Œ±k.
fork= 0,1,¬∑¬∑¬∑, K‚àí1do
xk+1=Cxxk‚àíŒ±kAxuk‚àíBxdk
x,dk+1
x=dk
x+Bxxk+1;
end for
In this case, we have z‚ãÜ
k‚â°0,y‚ãÜ
k‚â°0. Notice that Ly‚ãÜ= 0,Lz‚ãÜ= 0. It gives
Œ∑2=O 
Œ≤2‚à•Oy‚à•2‚à•O‚àí1
y‚à•2‚à•Œõya‚à•2‚à•Œõ‚àí1
yb‚à•2
(1‚àí ‚à•Œìy‚à•)2+Œ≥2‚à•Oz‚à•2‚à•O‚àí1
z‚à•2‚à•Œõza‚à•2‚à•Œõ‚àí1
zb‚à•2
(1‚àí ‚à•Œìz‚à•)2!
,
Œ∑1=O
Œ∑2+Œ±2
1 +(1‚àíŒ∏)2
Œ∏2‚à•Œõ‚àí1
xb‚à•2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2
(1‚àí ‚à•Œìx‚à•)2
.
If we take
Œ±‚â≤min(
1,1‚àí ‚à•Œìx‚à•
‚à•Ox‚à•‚à•O‚àí1x‚à•‚à•Œõxa‚à•,1‚àí ‚à•Œìx‚à•
‚à•Ox‚à•‚à•O‚àí1x‚à•‚à•Œõxa‚à•‚à•Œõ‚àí1
xb‚à•1
2)
andŒ∏= 1,Œ≤‚Üí0,Œ≥‚Üí0, then (45),(53),(56),(40),(58),(65),(82), and (84) hold. Thus all
previous lemmas hold. Then (87) transforms into
1
K+ 1KX
k=0E‚à•Œ¶(¬Øxk)‚à•2
‚â≤f(¬Øx0)‚àíinff
Œ±(K+ 1)+1
n 
Œ∏(1‚àíŒ∏) +Œ±Œ∏2
œÉ2
f,1+(1‚àíŒ∏)2
Œ∏(K+ 1)‚à•‚àáf 
¬Øx0
‚à•2
+Œ∑1Œ±2f(¬Øx0)‚àíinff
Œ±(K+ 1)+Œ∏
nœÉ2
f,1
+‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
n(K+ 1)(1 ‚àí ‚à•Œìx‚à•)
+Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
n(1‚àí ‚à•Œìx‚à•)2(K+ 1)"
1‚àíŒ∏
Œ∏nX
i=1‚àáfi(¬Øx0)2#
+1
n
Œ±2Œ∏
Œ∏+1‚àíŒ∏
1‚àí ‚à•Œìx‚à•‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2n
1‚àí ‚à•Œìx‚à•
œÉ2
f,1.
It follows that
1
K+ 1KX
k=0E‚à•Œ¶(¬Øxk)‚à•2
‚â≤f(¬Øx0)‚àíinff
Œ±(K+ 1)+Œ±œÉ2
f,1
n+
Œ±4‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2
(1‚àí ‚à•Œìx‚à•)2f(¬Øx0)‚àíinff
Œ±(K+ 1)+1
nœÉ2
f,1
+‚à•Ox‚à•2E‚à•ÀÜe0
x‚à•2
n(K+ 1)(1 ‚àí ‚à•Œìx‚à•)+Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•œÉ2
f,1
‚â≤f(¬Øx0)‚àíinff
Œ±(K+ 1)+Œ±œÉ2
f,1
n+Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2
1‚àí ‚à•Œìx‚à•œÉ2
f,1
+Œ±2‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
(K+ 1)(1 ‚àí ‚à•Œìx‚à•)+Œ±4‚à•O‚à•2
x‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2œÉ2
f,1
n(1‚àí ‚à•Œìx‚à•)2.
(98)
63Like (88), we take
C0= 1 +‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•
1‚àí ‚à•Œìx‚à•+‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõxa‚à•‚à•Œõ‚àí1
xb‚à•
1‚àí ‚à•Œìx‚à•1
2
,
Œ±1=s
n
KœÉ2
f,1, Œ± 2= 
1‚àí ‚à•Œìx‚à•
K‚à•Ox‚à•2‚à•O‚àí1x‚à•2‚à•Œõxa‚à•2œÉ2
f,1!1
3
,
Œ±3=1‚àí ‚à•Œìx‚à•
‚à•Ox‚à•2‚à•O‚àí1x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
01
3
,
Œ±4= 
n(1‚àí ‚à•Œìx‚à•)2
K‚à•O‚à•2x‚à•O‚àí1x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2œÉ2
f,1!1
5
,
Œ±= Œò
C0+1
Œ±1+1
Œ±2+1
Œ±3+1
Œ±4‚àí1
.
Substituting these values into (98), we get
1
K+ 1KX
k=0E‚à•Œ¶(¬Øxk)‚à•2‚â≤œÉf,1‚àö
nK+ 
‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2œÉ2
f,1
1‚àí ‚à•Œìx‚à•!1
3
K‚àí2/3+C0
K
+‚à•Ox‚à•2‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2Œ∂x
0
(1‚àí ‚à•Œìx‚à•)1
31
K+ 
‚à•O‚à•2
x‚à•O‚àí1
x‚à•2‚à•Œõxa‚à•2‚à•Œõ‚àí1
xb‚à•2œÉ2
f,1
n(1‚àí ‚à•Œìx‚à•)2!1
5
K‚àí4/5.
Like Lemma 18, we get the transient iterating complexity for Algorithm 3 is
(
n3‚à•Ox‚à•2‚à•O‚àí1
x‚à•2
1‚àí ‚à•Œìx‚à•2
‚à•Œõxa‚à•2, n‚à•Ox‚à•‚à•O‚àí1
x‚à•‚à•Œõ‚àí1
xb‚à•
1‚àí ‚à•Œìx‚à•4
3
‚à•Œõxa‚à•, n)
.
Substituting the value of relevant norms in Table 4, we get the transient iteration complexity for GT,
EXTRA, ED are
O
maxn3
(1‚àíœÅ)2,n
(1‚àíœÅ)8/3
,On3
(1‚àíœÅ)2
,On3
(1‚àíœÅ)2
respectively, where œÅ:=œÅ(Wx). These upper bounds are the same as the state-of-the-art results
shown in Table 1. It indicates that our analysis accurately captures the impacts of updates at each
level on the convergence results.
D Experimental details
In this section, we provide the details of our numerical experiments discussed in Section 4. We also
provide addition experimental results which are not mentioned in the main text due to the space
limitation. For all GT variants, we focus on one typical representative, ATC-GT, in our experiments,
which we denote as GT for brevity. All experiments described in this section were run on an NVIDIA
A100 server.
D.1 Synthetic bilevel optimization
Here, we consider problem (1)whose upper- and lower level loss functions on the i-th agents
(1‚â§i‚â§N) are denoted as:
fi(x, y) =EAi,bih
‚à•Aiy‚àíbi‚à•2i
,
gi(x, y) =EAi,bih
‚à•Aiy‚àíx‚à•2+Cr‚à•y‚à•2i
,
640 1000 2000 3000 4000 5000
number of gradient evaluations103
102
101
100101102||x(i)x*||2
(a) Fully connceted, severe heterogeneity
D/uni00ADSOBA
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
0 1000 2000 3000 4000 5000
number of gradient evaluations103
102
101
100101102||x(i)x*||2
(b) 2D Torus, severe heterogeneity
D/uni00ADSOBA
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
0 1000 2000 3000 4000 5000
number of gradient evaluations103
102
101
100101102||x(i)x*||2
(c) Adjusted ring, severe heterogeneity
D/uni00ADSOBA
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
0 1000 2000 3000 4000 5000
number of gradient evaluations103
102
101
100101102||x(i)x*||2
(d) Fully connceted, mild heterogeneity
D/uni00ADSOBA
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
0 1000 2000 3000 4000 5000
number of gradient evaluations103
102
101
100101102||x(i)x*||2
(e) 2D Torus, mild heterogeneity
D/uni00ADSOBA
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
0 1000 2000 3000 4000 5000
number of gradient evaluations103
102
101
100101102||x(i)x*||2
(f) Adjusted ring, mild heterogeneity
D/uni00ADSOBA
SPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRAFigure 4: The estimation error of D-SOBA, SPARKLE -GT, SPARKLE -ED, and SPARKLE -
EXTRA under different networks and data heterogeneity.
where x‚ààRD, y‚ààRKandCrdenotes a fixed regularization parameter. For each agent i, we
firstly generate the local solution y‚àó
i, x‚àó
iasy‚àó
i=y‚àó+Œ∂iandx‚àó
i=A‚àób‚àó+Œæi, where x‚àó‚àº N(0, IK)
is a randomly generated vector, each element of A‚àóis independently sampled from N(0,9). The
observation (Ai, bi)on agent iis generated in a streaming manner by Ai=A‚àó+œïi,bi=x‚àó
i+œài,
in which each element of œïi‚ààRK√óDandœài‚ààRDare independently generated by N(0, œÉ2
g). The
terms Œæi‚àº N(0, œÉ2
hIK)andŒ∂i‚àº N(0, œÉ2
hID)control the heterogeneity of data distributions across
different agents.
We set D= 20 , K= 10 , œÉg= 0.001, Cr= 0.001. Then we set œÉh= 0.5to represent severe
heterogeneity across agents and œÉh= 0.1for mild heterogeneity. We run D-SOBA, SPARKLE -GT,
SPARKLE -ED, and SPARKLE -EXTRA over Ring, 2D-Torus [ 37], and fully connected networks
withN= 64 agents. The moving-average term Œ∏= 0.1and the step-size at the t-th iteration are
Œ±t=Œ≤t=Œ≥t= 1/(500 + 0 .01t). The batch size is 10.
Fig. 4 illustrates the averaged estimation errorPN
i=1x(t)
i‚àíx‚àó2
of the mentioned algorithms with
different communication topology and data heterogeneity. It is observed that SPARKLE with ED,
EXTRA, GT achieve better convergence performances with decentralized communication networks.
Meanwhile, SPARKLE -ED and SPARKLE -EXTRA are more robust to data heterogeneity and the
sparsity of network topology than SPARKLE -GT. All the results are consistent with our theoretical
results.
D.2 Hyper-cleaning on FashionMNIST dataset
Here, we consider a data hyper-clean problem [ 44] on FashionMNIST dataset [ 48]. The FashionM-
NIST dataset consists of 60000 images for training and 10000 images for testing and we randomly
split 50000 training images into a training set and the other 10000 images into a validation set.
The data hyper-cleaning problem aims to train a classifier from a corrupted dataset, in which the label
of each training data is replaced by a random class number with a probability p(i.e. the corruption
rate). It can be considered as a stochastic bilevel problem (1)whose upper- and lower-level loss
functions on the i-th agents ( 1‚â§i‚â§n) are formulated as:
fi(x, y) =1D(i)
valX
(Œæe,Œ∂e)‚ààD(i)
valL(œï(Œæe;y), Œ∂e),
gi(x, y) =1D(i)
trX
(Œæe,Œ∂e)‚ààD(i)
trœÉ(xe)L(œï(Œæe;y), Œ∂e) +C‚à•y‚à•2,
650.50 0.55 0.60 0.65 0.70
test accuracy050010001500200025003000number of gradient evaluationsSPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
SPARKLE/uni00ADED/uni00ADGTSPARKLE/uni00ADEXTRA/uni00ADGT
D/uni00ADSOBA
MA/uni00ADDSBO/uni00ADGT
0.50 0.55 0.60 0.65 0.70
test accuracy05001000150020002500number of gradient evaluationsSPARKLE/uni00ADGT
SPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
SPARKLE/uni00ADED/uni00ADGTSPARKLE/uni00ADEXTRA/uni00ADGT
D/uni00ADSOBA
MA/uni00ADDSBO/uni00ADGTFigure 5: Hypergradient evaluation times for required test accuracy in hyper-cleaning problem. (Left:
p= 0.2; Right: p= 0.3)
where œïdenotes a training model while ydenotes its parameters, Ldenotes the cross-entropy loss
function and œÉ(x) = (1 + e‚àíx)‚àí1is the sigmoid function. D(i)
trandD(i)
valdenotes the training and
validation set of the i-th agent, respectively. C >0is a fixed regularization parameter.
Data generation and experiment settings. In this experiment, we let œïbe a two-layer MLP network
with a 300-dim hidden layer and ReLU activation while ydenotes its parameters. For 1‚â§i‚â§10,
we sample a probability distribution Pirandomly by Dirichlet distribution with parameters Œ±= 0.1.
The training and validation images with label iare sent to different agents according the probability
distribution Pi. Then D(i)
trandD(i)
valare generated sufficiently heterogeneous [ 32]. We set C= 0.001.
The batch size is set to 50.
Convergence performances with different corruption rates. We set the moving-average term
Œ∏k= 0.2and run D-SOBA [ 29], MA-DSBO-GT [ 10], MDBO [ 21]SPARKLE -GT, SPARKLE -ED,
SPARKLE -EXTRA, SPARKLE -ED-GT, and SPARKLE -EXTRA-GT on an Adjusted Ring graph
withn= 10 agents and p= 0.1,0.2,0.3separately. The step-sizes for all the algorithms are set to
Œ±k=Œ≤k=Œ≥k= 0.03and the term Œ∑in MDBO is set to 0.5. The weight matrix of Adjust Ring
W= [wij]n√ónsatisfies:
wij=Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥a, ifj=i,
1‚àía
2,if(j‚àíi)%n=¬±1,
0, else.
Moreover, we run SPARKLE with ED in the lower level and auxiliary variable and gradient tracking
in the upper level (i.e. SPARKLE -ED-GT) as well as SPARKLE with EXTRA in the lower level
and auxiliary variable and gradient tracking in the upper level (i.e. SPARKLE -EXTRA-GT) and
compare their test accuracy with the other four algorithms.
Figure 1 shows that SPARKLE -ED and SPARKLE -EXTRA outperforms in different cases than
SPARKLE -GT. Meanwhile, SPARKLE -EXTRA, SPARKLE -EXTRA-GT achieve similar test
accuracy, as do those for SPARKLE -ED and SPARKLE -ED-GT, which matches our theoretical
results in transient iteration analysis. Figure 5 presents the times of gradient evaluation for different
test accuracies of these algorithms at p= 0.2,0.3, demonstrating similar results.
Influence of network topology. We set the corruption rate p= 0.3, the step sizes Œ±k=Œ≤k=Œ≥k=
0.02, and the moving-average term Œ∏k= 0.2. Then we run SPARKLE-EXTRA and SPARKLE-
EXTRA-GT on a network containing n= 10 nodes with different topologies in the following two
cases:
‚Ä¢Fixed upper, varied lower :xcommunicates through a five-peer graph; y, zcommunicate through
different adjusted rings with œÅ= 0.647,0.828,0.924,0.990.
‚Ä¢Fixed lower, varied upper :y, zcommunicate through a five-peer graph; xcommunicates through
different adjusted rings with œÅ= 0.647,0.828,0.924,0.990.
660 500 1000 1500 2000 2500 3000 3500
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy(a)SPARKLE-EXTRA; Fixed topo x; Varied topo of y,z
2200 2400 2600 2800 3000 3200 34000.650.700.750.80
=0.647
=0.828
=0.924
=0.990
0 500 1000 1500 2000 2500 3000 3500
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy(b)SPARKLE-EXTRA; Fixed topo y,z; Varied topo of x
2200 2400 2600 2800 3000 3200 34000.650.700.750.80
=0.647
=0.828
=0.924
=0.990
0 500 1000 1500 2000 2500 3000 3500
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy(c)SPARKLE-EXTRA-GT; Fixed topo x; Varied topo of y,z
2200 2400 2600 2800 3000 3200 34000.650.700.750.80
=0.647
=0.828
=0.924
=0.990
0 500 1000 1500 2000 2500 3000 3500
number of gradient evaluations0.00.10.20.30.40.50.60.70.8test accuracy(d)SPARKLE-EXTRA-GT; Fixed topo y,z; Varied topo of x
2200 2400 2600 2800 3000 3200 34000.650.700.750.80
=0.647
=0.828
=0.924
=0.990
Figure 6: The average test accuracy of SPARKLE-EXTRA and SPARKLE-EXTRA-GT on hyper-
cleaning with different communicating strategy of x, y, z .
Table 5: Mean and standard deviation of the average test accuracy of last 40 iterations during 10 trials
with different moving-average terms
Algorithm Œ∏= 0.05 Œ∏= 0.2 Œ∏= 0.3
SPARKLE -GT 0.7080¬±0.0215 0.7045¬±0.0126 0 .7064¬±0.0113
SPARKLE -ED 0.7096¬±0.0074 0.7113¬±0.0047 0.7110¬±0.0081
SPARKLE -EXTRA 0.7190¬±0.0103 0.7277¬±0.0090 0.7243¬±0.0028
SPARKLE -ED-GT 0.7064¬±0.0063 0.7178¬±0.0037 0.7162¬±0.0041
SPARKLE -EXTRA-GT 0.7198¬±0.0051 0.7262¬±0.0058 0.7247¬±0.0048
The weight matrix of five-peer graph W= [wij]n√ónsatisfies:
wij=0.2,if(j‚àíi)%n= 0,¬±1,¬±2,
0, else.
Figure 6 shows the average test accuracy of both SPARKLE -EXTRA and SPARKLE -EXTRA-GT
over 10 trials. It indicates that the test accuracy decays with increasing spectral gap of topologies
related to y, zwhile the topology of xis fixed during the whole iterations. However, such convergence
gap becomes milder when the topologies of y, zare fixed and that of xvaries. This phenomenon
supports our theoretical findings, which suggest that the transient iteration complexity is more
sensitive to the network topologies of y, zthan to that of x.
Influence of moving-average iteration on convergence. Moreover, for Œ∏t= 0.05,0.2,0.3, we
runSPARKLE -GT, SPARKLE -ED, SPARKLE -EXTRA, SPARKLE -ED-GT, and SPARKLE -
EXTRA-GT on an Adjusted Ring graph with n= 10 agents, Œ±k=Œ≤k=Œ≥k= 0.03andp= 0.3
for 3000 iterations. We obtain the average test accuracy of the last 40 iterations over 10 trials, and
present the mean and standard deviation during the different trials in Table 5. We can observe that
most algorithms achieve the highest test accuracy when Œ∏= 0.2, which may prove that a suitable Œ∏
can benefit the test accuracy in hyper-cleaning problems.
670 250 500 750 1000 1250 1500 1750 2000
sample size0.00.51.01.52.02.53.03.54.0test lossSPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
SLDBOMDBO
Single/uni00ADlevel
0 250 500 750 1000 1250 1500 1750 2000
sample size0.51.01.52.02.53.0test lossSPARKLE/uni00ADED
SPARKLE/uni00ADEXTRA
SLDBOMDBO
Single/uni00ADlevelFigure 7: The test loss against samples generated by one agent of different algorithms in the policy
evaluation. (Left: n= 20 , Right: n= 10 .)
Table 6: The average training loss of the last 500 iterations for 10 independent trials in the distributed
policy evaluation.
Algorithm N= 10 N= 20
SPARKLE -ED 0.2781¬±1.09√ó10‚àí30.3198¬±3.21√ó10‚àí3
SPARKLE -EXTRA 0.2743¬±0.88√ó10‚àí30.3207¬±2.94√ó10‚àí3
MDBO 1.0408¬±4.51√ó10‚àí31.3293¬±8.38√ó10‚àí3
SLDBO 0.4132¬±1.18√ó10‚àí30.8374¬±2.47√ó10‚àí3
Single-level ED 0.2948¬±0.92√ó10‚àí30.3164¬±3.12√ó10‚àí3
D.3 Distributed policy evaluation in reinforcement learning
Following the result of [ 52], we consider a multi-agent MDP problem in reinforcement learning on a
distributed setting with nagents. Denote Sas the state space. Suppose that the value function in each
states‚àà S is a linear function V(s) =œï‚ä§
sx, where œïs‚ààRmis a feature and x‚ààRmis a parameter.
To obtain the optimal solution x‚àó, we consider the following Bellman minimization problem:
min
x‚ààRmF(x) =1
nnX
i=1"
1
2|S|X
s‚ààS 
œï‚ä§
sx‚àíEs‚Ä≤
ri(s, s‚Ä≤) +Œ≥œï‚ä§
s‚Ä≤xs2#
where ri(s, s‚Ä≤)denotes the reward incurred from transition stos‚Ä≤on the i-th agent, Œ≥‚àà(0,1)denotes
the discount factor. The expectation is taken over all random transitions from state stos‚Ä≤. It can be
viewed as a bilevel optimization problem with the following upper- and lower-level loss:
fi(x, y) =1
2|S|X
s‚ààS(œï‚ä§
sx‚àíys)2,
gi(x, y) =X
s‚ààS 
ys‚àíEs‚Ä≤
ri(s, s‚Ä≤) +Œ≥œï‚ä§
s‚Ä≤xs2,
where y= (y1,¬∑¬∑¬∑, y|S|)‚ä§‚ààR|S|. In our experiment, we set the number of states |S|= 200 and
m= 10 . For each s‚àà S, we generate its feature œïs‚àºU[0,1]m. The non-negative transition
probabilities are generated randomly and standardized to satisfyP
s‚Ä≤‚ààSps,s‚Ä≤= 1. The mean reward
¬Øri(s, s‚Ä≤)are independently generated from the uniform distribution U[0,1]. In each iteration, the
stochastic reward ri(s, s‚Ä≤)‚àº N(¬Øri(s, s‚Ä≤),0.022).
Forn= 10,20, we run SPARKLE -ED and SPARKLE -EXTRA as well as existing decentralized
SBO algorithms MDBO [ 21] and SLDBO [ 16] (here we use the stochastic gradient instead of
deterministic gradient) over a Ring graph. For MDBO, the number of Hessian-inverse estimation
iterations is set to 5. The step sizes are 0.03 for all methods. Figure 3 illustrates the upper-level loss
against samples generated by one agent for 10 independent trials. Table 6 shows the average training
loss of the last 500 iterations for 10 independent trials of the four decentralized SBO algorithms as
well as single-level ED [ 56] (For bilevel algorithms, training loss means the upper-level loss here).
680 2000 4000 6000 8000 10000 12000 14000
time(s)0.00.10.20.30.40.50.60.7training/uni00A0accuracy
SPARKLE/uni00ADED
D/uni00ADSOBA
Decentralized/uni00A0MAML
0 2000 4000 6000 8000 10000 12000 14000
time(s)0.00.10.20.30.40.50.60.7test/uni00A0accuracySPARKLE/uni00ADED
D/uni00ADSOBA
Decentralized/uni00A0MAMLFigure 8: The accuracy on training and testing set of different algorithms for the meta-learning
problem.
Both Figure 3 and Table 6 demonstrate that SPARKLE -ED and SPARKLE -EXTRA converge faster
than other methods.
Finally, we create a fixed "test set" with 10000 sample generated from S. Figure 7 shows the
loss on the test set of SPARKLE-ED, SPARKLE-EXTRA, SLDBO, MDBO and single-level ED
algorithm, demonstrating the superior performance of SPARKLE compared to other decentralized
SBO algorithms.
D.4 Decentralized meta-learning
We consider a meta-learning problem as described in [ 18]. There are Rtasks{Ts, s= 1,¬∑¬∑¬∑, R}.
Each task Tshas its own loss function L(x, ys, Œæ), where Œæsrepresents a stochastic sample drawn from
the data distribution Ds,ysdenotes the task-specific parameters and xdenotes the global parameters
shared by all the tasks. In meta-learning problem, we aim to find the parameters (x‚àó, y‚àó
1,¬∑¬∑¬∑, y‚àó
R)
that minimizes the loss function across all Rtasks, i.e.,
min
x,y1,¬∑¬∑¬∑,yRl(x, y1,¬∑¬∑¬∑, yR) =1
RRX
s=1EŒæ‚àºDs[L(x, ys, Œæ)]. (102)
The problem (102) can be formulated as a decentralized SBO problem with heterogeneous data
distributions across Nnodes. For i= 1,2,¬∑¬∑¬∑, N, letDtrain
s,iandDval
s,idenote the training and
validation datasets for the s-th task Tsreceived by node irespectively. We can then address the
meta-learning problem by minimizing (1), with the upper- and lower-level loss functions defined as:
fi(x, y) =1
RRX
s=1EŒæ‚àºDval
s,i[L(x, ys, Œæ)],
gi(x, y) =1
RRX
s=1h
EŒæ‚àºDtrain
s,i[L(x, ys, Œæ)] +R(ys)i
,
where Ldenotes the cross-entropy loss and R(ys) =Cr‚à•ys‚à•2is a strongly convex regularization
function.
In this experiment, we compare SPARKLE-ED with D-SOBA [ 29] and MAML [ 18] in a decentralized
communication setting over a 5-way 5-shot task across a network of N= 8 nodes connected by
Ring graph. The dataset used is miniImageNet [ 47], derived from ImageNet [ 42], which comprises
100 classes, each containing 600 images of size 84√ó84. We set R= 2000 and partition these
classes into 64 for training, 16 for validation, and 20 for testing. For the training and validation
classes, the data is split according to a Dirichlet distribution with parameter Œ±= 0.1[32]. We utilize
a four-layer CNN with four convolution blocks, where each block sequentially consists of a 3√ó3
convolution with 32 filters, batch normalizationm, ReLU activation, and 2√ó2max pooling. The batch
size is 32 , and Cr= 0.001. The parameters of the last linear layer are designated as task-specific,
while the other parameters are shared globally. For SPARKLE and D-SOBA, the step-sizes are
69Œ≤=Œ≥= 0.1andŒ±= 0.01. For MAML, the inner step-size is 0.1 and the outer step-size is 0.001,
and the number of inner-loop steps as 3. For all algorithms, the task number is set to 32. And we only
repeat the experiment only once due to the time limitation. Figure 8 shows the average accuracy on
the training dataset for all nodes, as well as the test accuracy of the three algorithms. We observe
that SPARKLE-ED outperforms other algorithms, demonstrating the efficiency of SPARKLE in
decentralized meta-learning problems.
70NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: Refer to Abstract and Introduction.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Refer to Section Conclusions.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
71Justification: Refer to Section Assumptions for our assumptions, and Appendix for detailed
proof.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: Refer to Appendix.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
72Answer: [No]
Justification: We may consider making data and code openly accessible when it is deemed
necessary.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run
to reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Refer to Appendix.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We show error bars in experiments where we consider them essential.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
73‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: Refer to Appendix.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification:Our research conforms, in every respect, with the NeurIPS Code of Ethics.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of our work.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
74‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: There is no such risk in the paper.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We comply with the licenses of existing assets used in the paper and provide
necessary references.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has curated
licenses for some datasets. Their licensing guide can help determine the license of a
dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
75‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
76