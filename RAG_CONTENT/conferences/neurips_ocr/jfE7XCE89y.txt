FuseMoE: Mixture-of-Experts Transformers for
Fleximodal Fusion
Xing Han
Department of Computer Science
Johns Hopkins University
xhan56@jhu.eduHuy Nguyenâˆ—
Department of Statistics and Data Sciences
The University of Texas at Austin
huynm@utexas.edu
Carl Harrisâˆ—
Department of Biomedical Engineering
Johns Hopkins University
charr165@jhu.eduNhat Ho+
Department of Statistics and Data Sciences
The University of Texas at Austin
minhnhat@utexas.edu
Suchi Saria+
Department of Computer Science
Johns Hopkins University
ssaria@cs.jhu.edu
Abstract
As machine learning models in critical fields increasingly grapple with multi-
modal data, they face the dual challenges of handling a wide array of modalities,
often incomplete due to missing elements, and the temporal irregularity and spar-
sity of collected samples. Successfully leveraging this complex data, while over-
coming the scarcity of high-quality training samples, is key to improving these
modelsâ€™ predictive performance. We introduce â€œFuseMoEâ€, a mixture-of-experts
framework incorporated with an innovative gating function. Designed to integrate
a diverse number of modalities, FuseMoE is effective in managing scenarios with
missing modalities and irregularly sampled data trajectories. Theoretically, our
unique gating function contributes to enhanced convergence rates, leading to bet-
ter performance in multiple downstream tasks. The practical utility of FuseMoE
in the real world is validated by a diverse set of challenging prediction tasks.
1 Introduction
Multimodal fusion is a critical and extensively studied problem in many significant domains
[78, 94, 90, 9], such as sentiment analysis [26, 54], image and video captioning [42, 41], and med-
ical prediction [33, 83]. Previous research has shown that embracing multimodality can improve
predictive performance by capturing complementary information across modalities, outperforming
single-modality approaches in similar tasks [71, 32]. However, an ongoing challenge lies in the
creation of scalable frameworks for fusing multimodal data under a variety of conditions, and in
creating reliable models that consistently surpass their single-modal counterparts.
âˆ—Equal Contribution,+Equal Advising.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Table 1: We evaluated the characteristics of FuseMoE against various benchmarks. The pipeline approach [83]
relies on a simple feature extraction scheme for each modality, followed by concatenation and classification. It
doesnâ€™t incorporate irregularities or missingness in its process, but its use of concatenation and zero-imputation
for missing modalities allows it to be adapted to FlexiModal settings. Both [101] and [96] tackle multi-modality
fusion, but as modalities increase, their method demands exponentially more cross-modal computations and
significant model architecture modifications. Finally, [58] presents MoE for language-image alignment, yet it
also requires substantial adjustments for the more intricate and universal FlexiModal context we explore.
Method Type Irregularity Missingness Num of
ModsTheory FlexiModal Adaptive?
Soenksen et al. [83] Data Pipeline âœ— âœ— â‰¥4 âœ— âœ“
Zhang et al. [101] Modality Fusion âœ“ âœ— 2 âœ— âœ—
Zadeh et al. [96] Modality Fusion âœ— âœ— 3 âœ— âœ—
Mustafa et al. [58] Multimodal MoE âœ— âœ— 2 âœ— âœ—
FuseMoE This Paper âœ“ âœ“ â‰¥4 âœ“ Adapted
Handling a variable number of input modalities remains an open challenge in multimodal fusion,
due to challenges with scalability and lack of unified approaches for addressing missing modali-
ties. Many existing multimodal fusion methods are designed for only two modalities [25, 102, 101],
rely on costly pairwise comparisons between modalities [90], or employ simple concatenation ap-
proaches [83], rendering them unable to scale to settings with a large number of input modalities
or adequately capture inter-modal interactions. Similarly, existing works are either unable to handle
missing modalities entirely [101, 98] or use imputation approaches [89, 51, 83] of varying sophis-
tication. The former methods restrict usage to cases where all modalities are completely observed,
significantly diminishing their utility in settings where this is often not the case (such as in clinical
applications); the latter can lead to suboptimal performance due to the inherent limitations of im-
puted data. In addition, the complex and irregular temporal dynamics present in multimodal data
have often been overlooked [101, 88], with existing methods often ignoring irregularity entirely
[83] or relying on positional embedding schemes [90] that may not be appropriate when modali-
ties display a varying degree of temporal irregularity. Consequently, there is a pressing need for
more advanced and scalable multimodal fusion techniques that can efficiently handle a broader set
of modalities, effectively manage missing and irregular data, and capture the nuanced inter-modal
relationships necessary for robust and accurate prediction. We use the term FlexiModal Data to
capture several of these key aspects, which havenâ€™t been well-addressed by prior works:
â€œFlexiâ€ suggests flexibility, indicating the possibility of having any combination
of modalities, even with arbitrary missingness or irregularity.
FlexiModal data is most evident in clinical scenarios, where extensive monitoring results in the
accumulation of comprehensive electronic health records (EHRs) for each patient. A typical EHR
encompasses diverse data types, including tabular (e.g., age, demographics, gender), images (X-
rays, magnetic resonance imaging, and photographs), clinical notes, physiological time series (ECG
and EEG), and vital signs (blood chemistry, heart rate). In this setting, we observe a variety of
modalities, sampled with varying irregularity and a high degree of missingness and sparsity.
Contributions In this paper, we introduce a novel mixture-of-experts (MoE) framework, which we
callFuseMoE , specifically designed to enhance the multimodal fusion of FlexiModal data. FuseMoE
incorporates sparsely gated MoE layers in its fusion component, which are adept at managing dis-
tinct tasks and learning optimal modality partitioning. In addition, FuseMoE surpasses previous
cross-attention-based methods in scalability, accommodating an unlimited array of input modal-
ities. Furthermore, FuseMoE routes each modality to designated experts that specialize in those
specific data types. This allows FuseMoE to effectively handle scenarios with missing modalities
by dynamically adjusting the influence of experts primarily responsible for the absent data, while
still utilizing the available modalities. Lastly, FuseMoE integrates a novel Laplace gating function,
which is theoretically proven to ensure better convergence rates compared to traditional Softmax
functions, thereby enhancing predictive performance. We have conducted comprehensive empirical
evaluations of FuseMoE across a range of application scenarios to validate its effectiveness.
2â€¦
â€¦
Per-Modality Encoders+Multi-Time AttentionVital SignsECGClinical NotesChest-X-Rays(missing)11:38 AM2:55 PM5:49 PM10:14 PM1/1/20232/15/20233/10/20234/20/2023â€¦Multi-HeadSelf-AttentionSparseGate/RouterFFN 1FFN 2FFN 3FFN 4FFN 5FFN 6FFN NMoE Fusion LayerÃ—ğ¿+Dense Encoder LayerMoEFusion LayerTask-Specific Loss+Load Balance LossAdd+NormalizeFigure 1: An example of addressing the challenge of FlexiModal Data: patients in ICUs often have extensive
and irregular health status measurements over time; patients with milder conditions only require monitoring
across fewer categories. FuseMoE is adept at handling inputs featuring any combination of modalities, including
those with missing elements. It starts by encoding inputs using modality-specific feature extractors, followed
by employing a multi-time attention mechanism [82] to address temporal irregularities. The core of FuseMoE
lies the MoE Fusion Layer, where a routing mechanism is trained to categorize multimodal inputs and direct
them to the appropriate combinations of MLPs. The outputs from these MLPs are weighted through a gating
function, resulting in fused embeddings, which are subsequently utilized for further processing.
2 FuseMoE: Enhance Predictive Performance for FlexiModal Data
In this section, we delve into the fundamental components of FuseMoE , illustrated in Figure 1. We
focus on two critical elements: the modality and irregularity encoder, and the MoE fusion layer.
2.1 Sparse MoE Backbone
The main components of a sparse MoE layer are a network Gas a sparse gate and an expert network
E. [79] proposed a Top- Kgating function that takes as an input a token representation xâˆˆRD
and then routes it to the Top- Kexperts out of the set {Ei}S
i=1. The gating network parameter
WâˆˆRDÃ—Nproduces logits hs(x) = Top K( xÂ·W), which are normalized via Softmax:
G(x)i=exp(hs(x)i)PK
jexp(hs(x)j). (1)
Each expert network ( Ei:RDâ†’RD) contains a feed-forward layer (FFN) and its parameters
are independent of other models. The final output of the expert network yis the linearly weighted
combination of each expertâ€™s output on the token by the gateâ€™s output: y=PS
i=1G(x)iEi(x).
Gating Network Design The gating networkâ€™s advantage lies in its capacity to be concurrently
trained with FFNs, facilitating the learning of an optimal sparse combination of experts. Essentially,
by evaluating the similarity between the input token and the experts, the gating network/router op-
timally matches the input partition with the most suitable experts. In many cases, variations in the
routing mechanism can greatly influence performance across diverse applications [48]. The Soft-
max gating is the most widely adopted across domains [77, 79]. We introduce a novel Laplace gating
function that offers enhanced convergence guarantees and delivers superior predictive performance,
particularly in FlexiModal applications. The function is formulated as follows:
hl(x) = Top K( âˆ’âˆ¥Wâˆ’xâˆ¥2). (2)
The Laplace gating function, characterized by its Euclidean term exp(âˆ’âˆ¥Wâˆ’xâˆ¥2), is less prone
to converge towards extreme weight distributions due to the bounded nature of this term. In sub-
sequent sections, we will illustrate how this gating function facilitates faster parameter estimation
rates compared to Softmax gating. Moreover, our empirical findings indicate that the Laplace gating
exhibits enhanced performance in managing FlexiModal data.
2.2 Modality and Irregularity Encoder
To encode the irregularity of sampling in each modality, we utilize a discretized multi-time attention
(mTAND) module [82], which leverages a time attention mechanism [43, 92] to discretize irregularly
sampled observations into discrete intervals. Specifically, given a set of lkcontinuous time points,
3RouterR1R2R3R1R2R3FFN 1FFN 2FFN 3FFN 4FFN 1FFN 2FFN 3FFN 4FFN 1FFN 2FFN 3FFN 4TSTXTIMGTSTXTIMGTSTXTIMGğ‘=0.45ğ‘=0.35(a) Joint experts & router(b) Per-modality routers(c) Disjoint experts & routersFigure 2: We present three exemplary designs of the Top- Krouter for effective multimodal fusion, considering
an input scenario with three modalities: Time-Series (TS), Text (TXT), and images (IMG). (a) The joint router
design utilizes a concatenated embedding of all modalities, directing this combined input to selected experts.
(b) In the modality-specific router design, each modalityâ€™s embedding is independently assigned to a shared
pool of experts. (c) The third design variant also uses modality-specific routers but assigns each modalityâ€™s
embedding to separate pools of experts, each pool uniquely tailored to process a specific modality type.
tâˆˆRlk, corresponding to the kthdimensionality of a given modality, we employ Hembedding
functions Ï•h(Ï„)to embed each Ï„kâˆˆtkin adhdimensional vector space (detailed definition and
examples can be found in Appendix B and E). The ithdimension of the hthembedding is defined as
Ï•h(Ï„)[i] =wiÏ„k, ifi= 1
sin(wiÏ„k+Ï•i),if1< iâ‰¤dh,
where{wi, Ï•i}dh
i=1are learnable parameters. By performing this for each continuous time point in tk,
we create a dhdimensional representation of each time point in Hdifferent embedding spaces. We
then leverage these embeddings to discretize the irregularly sampled observations into discretized
bins. Specifically, we seek to discretize xk(with lkcorresponding observation times tk) into Î³
regularly sampled intervals Î³. We do this via an attention mechanism, which, for each embedding
function Ï•h(Ï„), takes Î³as queries, tkas keys, and xkas values and produces Ë†xk,hâˆˆRÎ³embeddings
for each sequence. Formally,
Ë†xk,h=SoftmaxÏ•h(Î³)QhKâŠ¤
hÏ•h(tk)âŠ¤
âˆšlk
xk,
where QhandKhare learnable parameters. This formulation allows us to discretize univariate
observations xkintoÎ³regularly-sampled bins. To model irregularity across a multivariate set of
observations for a given modality with dmdimensions, we repeat this process for each dimension
of the input. This allows us to obtain an interpolation matrix Ë†Xh= [Ë†x1,h,Ë†x2,h, ...,Ë†xdm,h]âˆˆRÎ³Ã—dm
for each of the hembedding functions. We then concatenate the interpolation matrices across all H
embedding functions (i.e., I= [Ë†X1,Ë†X2, ...,Ë†Xh]âˆˆRÎ³Ã—(HÂ·dm)) and employ a linear projection to
achieve a final, discretized embedding for each modality, ZâˆˆRÎ³Ã—de, where dedenotes the desired
dimensionality of each modalityâ€™s representation. The discretization procedure offers a standard-
ized approach to managing irregularly sampled time series across various input types; however, it
can inevitably result in information loss. On the other hand, relying solely on the mTAND module
may yield suboptimal performance due to the potentially varying sampling rates of different vari-
ables [31], especially in scenarios where the sample sizes are small. To mitigate this, we combine
discretized outputs with continuous representations learned through the mTAND module.
Encoding Multiple Modalities The process described above allows us to discretize an arbitrarily
long irregular, multivariate sequence into a regularly sampled, discretized embedding with length
Î³and dimensionality de. We repeat this for each of the Mmodalities, to create Membeddings,
{Zj}M
j=1, which are then combined to generate predictions.
2.3 MoE Fusion Layer
Router Design Study Upon obtaining embeddings from each of the jmodalities, we propose mul-
tiple complementary approaches for processing multimodal inputs. Figure 2 illustrates a range of
router design options. The most straightforward strategy involves employing a common router that
handles the concatenated embeddings of all jmodalities, without imposing any gating constraints.
4As the complexity increases with additional modalities, we consider more sophisticated alterna-
tives: deploying separate routers for each modalityâ€™s embedding and assigning these embeddings
to a shared pool of experts. This allows for distinct processing while maintaining a unified expert
framework. Additionally, we further segregate these common expert pools, allowing each router to
direct its respective embedding to dedicated experts skilled in handling such specific inputs. These
varied router design choices offer users enhanced flexibility, enabling more fine-grained control of
both inter-modal and intra-modal relationships. Details of the respective advantages and challenges
of these router design mechanisms can be found in Appendix C.
We implement an entropy regularization loss to ensure balanced and stable expert utilization, a
concept supported by various previous studies [58, 57, 22]. It maximizes the mutual information
between modalities and experts and serves as an auxiliary loss function in addition to task-specific
loss. Given a total of Mmodalities, and denoting Has the entropy, we define the loss function Eas
E(x) =1
MMX
j=1H(Ë†pmj(E))âˆ’ H(1
MMX
j=1Ë†pmj(E)), (3)
where Ë†pmj(E)is the distribution over the experts {Ei}S
i=1for the jthmodality. This distribution can
be approximated by Ë†pmj(E) =1
ljPlj
i=1pmj(E|xmj
i), where ljis the number of observations of
thejthmodality. Intuitively, we actively encourage the input embeddings to diminish the uncertainty
in selecting experts. By incorporating the loss E, we aim to stabilize the expertsâ€™ preferences within
each modality, while promoting a diverse range of expert selections across different modalities.
Missing Modalities In scenarios where certain modalities are missing throughout the data trajec-
tories, we substitute the original embedding Zmissing with a learnable embedding Z, acting as a
generic â€œmissing indicatorâ€. This strategy is facilitated by employing per-modality routers, which,
in conjunction with entropy regularization, guide Zpredominantly toward a specific group of less-
utilized experts. The new embeddings Zare dynamically adjusted throughout the model training
process to minimize the task-specific loss and the entropy regularization loss. As a result, the router
will assign lower weights to the experts responsible for processing these embeddings.
3 Theoretical Contribution
In this section, we provide a theoretical guarantee of the benefits of the Laplace gating over the
standard Softmax gating in MoE. In particular, we conduct a convergence analysis for maximum
likelihood estimation (MLE) under the Lapace gating Gaussian MoE, and demonstrate that the MLE
under this model has better convergence behaviors than that under the softmax gating Gaussian MoE.
Problem Setup. Since the convergence analysis of MLE under the Top-K sparse gating MoE has
been studied in [63], we will focus on examining the Laplace gating solely in the sequel. Assume
that(X1, Y1), . . . , (Xn, Yn)âˆˆRdÃ—Rare i.i.d. samples drawn from the Laplace gating Gaussian
MoE of order kâˆ—whose conditional density function pGâˆ—(Y|X)is
pGâˆ—(Y|X) =kâˆ—X
i=1softmax( âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥+Î²âˆ—
i)Â·f(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i), (4)
where we define for any vectors v= (vi)kâˆ—
i=1thatsoftmax( vi) :=exp(vi)Pkâˆ—
j=1exp(vj). Above, f(Â·|Âµ, Î½)
denotes a univariate Gaussian density function with mean Âµand variance Î½. For ease of the pre-
sentation, we denote Gâˆ—:=Pkâˆ—
i=1exp(Î²âˆ—
i)Î´(Wâˆ—
i,aâˆ—
i,bâˆ—
i,Î½âˆ—
i)as a true but unknown mixing measure
associated with unknown parameters (Î²âˆ—
i, Wâˆ—
i, aâˆ—
i, bâˆ—
i, Î½âˆ—
i)foriâˆˆ {1,2, . . . , k âˆ—}. In the paper, we
specifically consider two settings of the true number of experts kâˆ—: (i)Exact-specified setting : when
kâˆ—is known; (ii) Over-specified setting : when kâˆ—is unknown, and we over-specify the model in
equation 4 by a Laplace gating MoE model with k > k âˆ—experts. However, due to the space limit,
we present only the latter setting, and defer the former setting to Appendix J.
5Maximum Likelihood Estimation. We use the maximum likelihood method to estimate the un-
known mixing measure Gâˆ—[91]. In particular, the MLE is given by
bGnâˆˆarg max
GâˆˆGk(Î˜)1
nnX
i=1log(pG(Yi|Xi)), (5)
where Gk(Î˜) := {G=Pkâ€²
i=1exp(Î²i)Î´(Wi,ai,bi,Î½i): 1â‰¤kâ€²â‰¤k,(Wi, ai, bi, Î½i)âˆˆÎ˜}denotes the
set of all mixing measures with at most kcomponents. Given the MLE defined in equation 5, we
are ready to present the main results. Before that, let us introduce some necessary notations for our
analysis.
Notations. We denote [n] :={1,2, . . . , n }for any nâˆˆN. For any vector vâˆˆRd,âˆ¥vâˆ¥stands for
its2-norm value. Additionally, the notation |S|indicates the cardinality of a given set S, while Î´
denotes the Dirac delta measure. Finally, for any two probability densities p, qdominated by the
Lebesgue measure Âµ, we denote V(p, q) =1
2R
|pâˆ’q|dÂµas their Total Variation distance.
Firstly, we demonstrate in Theorem 3.1 that the convergence rate of density estimation under the
Laplace gating Gaussian MoE is parametric on the sample size n.
Theorem 3.1 (Density estimation) .The density estimation pbGn(Y|X)converges to the true density
pGâˆ—(Y|X)under the Total Variation distance at the following rate:
EX[V(pbGn(Â·|X), pGâˆ—(Â·|X))] =O(p
log(n)/n).
Proof of Theorem 3.1 is in Appendix K.2. The parametric rate O(p
log(n)/n)of the conditional
density function pbGnindicates that if there exists a loss function among parameters Dsuch that
EX[V(pbGn(Â·|X), pGâˆ—(Â·|X))]â‰³D(bGn, Gâˆ—), then we will achieve the parameter and expert estima-
tion rates via the bound D(bGn, Gâˆ—) =O(p
log(n)/n).
Voronoi Loss. Following the above implication, we now define a loss function among parameters
based on a notion of V oronoi cells as in [55]. Given some mixing measure G, we distribute its com-
ponents Î¸i:= (Wi, ai, bi, Î½i)to the following V oronoi cells, which are generated by the components
Î¸âˆ—
j:= (Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j)of the true mixing measure Gâˆ—:
Ajâ‰¡ A j(G) :={iâˆˆ[k] :âˆ¥Î¸iâˆ’Î¸âˆ—
jâˆ¥ â‰¤ âˆ¥ Î¸iâˆ’Î¸âˆ—
â„“âˆ¥,âˆ€â„“Ì¸=j}, (6)
for any 1â‰¤jâ‰¤kâˆ—. Note that, the cardinality of the V oronoi cell Ajis exactly the number of fitted
components approximating Î¸âˆ—
j. For ease of the presentation, let us denote Î¦ij(Ï1, Ï2, Ï3, Ï4) :=
âˆ¥Wiâˆ’Wâˆ—
jâˆ¥Ï1+âˆ¥aiâˆ’aâˆ—
jâˆ¥Ï2+|biâˆ’bâˆ—
j|Ï3+|Î½iâˆ’Î½âˆ—
j|Ï4, for any (Ï1, Ï2, Ï3, Ï4)âˆˆR4. Then, the
V oronoi loss function D2(G, Gâˆ—)used for our analysis under the over-specified setting is given by:
D2(G, Gâˆ—) :=kâˆ—X
j=1X
iâˆˆAjexp(Î²i)âˆ’exp(Î²âˆ—
j)+X
jâˆˆ[kâˆ—]:|Aj|=1X
iâˆˆAjexp(Î²i)Î¦ij(1,1,1,1)
+X
jâˆˆ[kâˆ—]:|Aj|>1X
iâˆˆAjexp(Î²i)Î¦ij
2,2,Â¯r(|Aj|),Â¯r(|Aj|)
2
. (7)
The notation Â¯r(|Aj|)stands for the minimum value of râˆˆNsuch that the following sys-
tem of polynomial equations does not have any non-trivial solutions for the unknown variables
{(q1i, q2i, q3i)}|Aj|
i=1:
|Aj|X
i=1X
m1+2m2=s,
1â‰¤m1+m2â‰¤rq2
3iqm1
1iqm2
2i
m1!m2!= 0,for each s= 1,2, . . . , r, (8)
A solution to the above system is regarded as non-trivial if at least among variables q1iis different
from zero, whereas all the variables q3iare non-zero. It is worth noting that the function Â¯r(Â·)was
previously studied in [28] to characterize the convergence behavior of parameter estimation under
the location-scale Gaussian mixture models. [28] also gave some specific values of that function,
namely Â¯r(2) = 4 andÂ¯r(3) = 6 . Meanwhile, they claimed that it was non-trivial to determine the
6103104105
Sample size n10âˆ’3Loss
D1(/hatwideGn,Gâˆ—)
0.2nâˆ’0.49D1(/hatwideGn,Gâˆ—)
0.2nâˆ’0.49(a) Exact-specified setting
103104105
Sample size n10âˆ’310âˆ’2Loss
D2(/hatwideGn,Gâˆ—)
0.2nâˆ’0.47D2(/hatwideGn,Gâˆ—)
0.2nâˆ’0.47 (b) Over-specified setting
Figure 3: Log-log scaled plots illustrating simulation results under the exact-specified (left) and the
over-specified settings (right). The orange curves depict the mean discrepancy between the MLE bGn
and the true mixing measure Gâˆ—, accompanied by error bars signifying two empirical standard de-
viations. Additionally, the gray dash-dotted line represents the least-squares fitted linear regression
line for these data points. Finally, the loss functions D1andD2are defined in equations equation 9
and equation 7, respectively. See Appendix I for the experimental details.
value of Â¯r(m)when mâ‰¥4, and further techniques should be developed for that purpose. Since
Gaussian MoE models are generalization of the Gaussian mixture models, we also involve the func-
tionÂ¯r(Â·)in our convergence analysis. Now, we provide in the following theorem the convergence
rate of parameter estimation under the over-specified setting of the Laplace gating Gaussian MoE
model (see also Figure 3 for the empirical convergence rates justifying the theoretical rates in The-
orem 3.2).
Table 2: Parameter estimation rates under the Softmax and Laplace gating Gaussian MoE models. The func-
tioner(Â·)represents the solvability of a system of polynomial equations considered in [64] while er(Â·)â‰¤Â¯r(Â·)
ander(2) = 4 ,er(3) = 6 . Additionally, An
j:=Aj(bGn)denotes a V oronoi cell defined in equation 6.
Gates exp(Î²âˆ—
j) Wâˆ—
j aâˆ—
j bâˆ—
j Î½âˆ—
j
Softmax [64] O(nâˆ’1/2)O(nâˆ’1/2er(|An
j|))O(nâˆ’1/er(|An
j|))O(nâˆ’1/2er(|An
j|))O(nâˆ’1/er(|An
j|))
Laplace (Ours) O(nâˆ’1/2) O(nâˆ’1/4) O(nâˆ’1/4) O(nâˆ’1/2Â¯r(|An
j|))O(nâˆ’1/Â¯r(|An
j|))
Theorem 3.2 (Parameter Estimation) .When k > k âˆ—becomes unknown, the following Total Varia-
tion bound holds true for any mixing measure Gâˆˆ Gk(Î˜):
EX[V(pG(Â·|X), pGâˆ—(Â·|X))]â‰³D2(G, Gâˆ—).
Consequently, we obtain that D2(bGn, Gâˆ—) =O(p
log(n)/n).
Proof of Theorem 3.2 is in Appendix K.3. The results of Theorem 3.2 together with the formulation
of the loss function D2in equation 7 reveal that (see also Table 2):
(i)The parameters Wâˆ—
i, aâˆ—
i, bâˆ—
i, Î½âˆ—
iwhich are fitted by exactly one component, i.e. |An
i|:=
|Ai(bGn)|= 1, enjoy the same estimation rate of order O(nâˆ’1/2)(up to some logarithmic factor),
which match those in [64].
(ii)The rates for estimating the parameters Wâˆ—
i, aâˆ—
i, bâˆ—
i, Î½âˆ—
iwhich are fitted by more than one com-
ponent, i.e. |An
i|>1, are no longer homogeneous. On the one hand, the estimation rates for the
parameters bâˆ—
iandÎ½âˆ—
iare of orders O(nâˆ’1/2Â¯r(|An
i|))andO(nâˆ’1/Â¯r(|An
i|)), respectively, both of which
are determined by the function Â¯r(Â·)and vary with the number of fitted components |An
i|. Those rates
are comparable to their counterparts in [64]. On the other hand, the estimation rates for the gating
parameters Wâˆ—
iand the expert parameters aâˆ—
iare all of order O(nâˆ’1/4), which remains constant with
respect to the number of fitted components. Meanwhile, those rates in [64] depend on a different
system of polynomial equations from that in equation 8, which are significantly slower.
Advantage of Laplace Gating on FlexiModal Setting In the standard Softmax gating [64], the
similarity score is computed as the inner product of a tokenâ€™s hidden representation and an expert
7Table 3: MoE demonstrates improved performance averaged over 5 random experiments on the CMU-MOSI
and MOSEI datasets; the best results are highlighted in bold font and the second best results are underlined .
Method / DataMOSI Dataset MOSEI Dataset
MAEâ†“ Acc-2â†‘ Corrâ†‘ F1â†‘ MAEâ†“ Acc-2â†‘ Corrâ†‘ F1â†‘
TFN 0.90Â±0.02 80.81 Â±0.34 0.70 Â±0.04 80.70 Â±0.18 0.59Â±0.03 82.50 Â±0.58 0.68 Â±0.02 82.10 Â±0.41
MulT 0.86Â±0.01 84.10 Â±0.21 0.71 Â±0.02 83.90 Â±0.27 0.58Â±0.02 82.51 Â±0.41 0.71 Â±0.04 82.31 Â±0.27
MAG 0.71Â±0.04 86.10 Â±0.44 0.80 Â±0.03 86.00 Â±0.09 0.57Â±0.07 85.56 Â±0.22 0.79 Â±0.02 84.50 Â±0.18
Softmax-MoE 0.69Â±0.01 87.09 Â±0.18 0.82 Â±0.02 87.29Â±0.22 0.55Â±0.03 86.34 Â±0.23 0.76 Â±0.05 84.97 Â±0.32
Joint experts &router 0.67Â±0.02 87.28Â±0.35 0.82Â±0.03 87.35 Â±0.24 0.54Â±0.01 86.41 Â±0.36 0.81Â±0.05 85.43Â±0.25
Per-mod router 0.65Â±0.04 88.23 Â±0.57 0.84 Â±0.01 87.39 Â±0.13 0.56Â±0.02 86.12 Â±0.19 0.78Â±0.02 85.07 Â±0.14
Disjoint router 0.73Â±0.02 86.37 Â±0.33 0.81 Â±0.04 86.89 Â±0.21 0.55Â±0.02 85.67Â±0.31 0.81Â±0.01 85.21Â±0.22
(a)
 (b)
 (c)
Figure 4: (a) The Laplace gating mechanism enhances CIFAR-10 classification when integrated into Vision-
MoE [77]. We employed Vision Transformer (ViT) [17] and ViT-small as the backbone models and selectively
replaced their FFN layers with MoE layers; (b) FuseMoE improves prediction on PAM dataset over baseline
time series models; (c) Per-modality routers and the entropy loss Emitigate the impact of missing modalities.
embedding. However, this approach can lead to representation collapse [13, 69], where a subset
of experts dominates the decision-making process, resulting in the redundancy of other experts.
This issue likely contributes to the slow rates of estimating expert parameters aâˆ—
iin this setting (see
Table 2). By contrast, the Laplace gating function partially alleviates this problem by computing
the similarity score as the L2-distance between token representations and expert embeddings. This
approach does not inherently favor any expert based on magnitude, unlike inner product which can
be biased towards experts with larger norms. The Laplace gating ensures that all experts have a
more balanced opportunity to be selected based on how close they are to the token representation.
Therefore, Laplace gating is beneficial when dealing with heterogeneous inputs, such as multimodal
data, where its feature distributions can be very different across modalities. This is because it can
handle these differences without being overly sensitive to the scale and variance of the input features.
In addition, it can gracefully degrade in the presence of missing data, rather than causing abrupt
changes in gating probabilities that might occur with inner product-based measures. The improved
estimation rates for expert parameters aâˆ—
iunder the Laplace gating Gaussian MoE, along with our
empirical results on multiple large-scale datasets, substantiate these insights.
4 Experiments
Overview We demonstrate that FuseMoE can provide accurate and efficient predictions when ap-
plied to the FlexiModal setting. We tested FuseMoE on a diverse set of benchmarks, including
MIMIC-III [40] and MIMIC-IV [36], CMU-MOSI and MOSEI [97], the Physical Activity Moni-
toring (PAM) dataset [75], and CIFAR-10 [46]. Compared to CMU-MOSI and MOSEI, the MIMIC
ecosystem exhibits irregular and missing modality patterns and includes distinct modalities unlike
PAM and CIFAR-10. Evaluating FuseMoE across these diverse datasets provides various empirical
insights into critical aspects of our modelâ€™s performance. Comprehensive details on the datasets,
metrics, parameters, and additional results are thoroughly presented in the Appendices.
4.1 Main Results
CMU-MOSI and MOSEI Datasets We first apply our method to the CMU-MOSI and MOSEI
datasets [97], which utilize visual, acoustic, and textual data for sentiment analysis and emotion
recognition tasks. Our methodology employs pre-trained T5 [73] for text encoding, librosa [56] for
audio feature extraction, and EfficientNet [84] for video feature encoding. Table 3 details the per-
8Table 4: Comparison of FuseMoE -based methods (gray) and baselines, utilizing vital signs and clinical notes
of MIMIC-IV [36]. The best results are highlighted in bold font , and the second-best results are underlined .
All results are averaged across 5 random experiments.
Task\Method MISTS MulT MAG TFN HAIM Softmax Gaussian Laplace
48-IHMAUROC 75.06Â±1.03 75.95 Â±0.84 75.82 Â±0.73 78.76 Â±0.79 79.65 Â±0.00 79.49 Â±0.83 80.76 Â±0.56 81.03Â±0.25
F1 45.61Â±0.34 38.81 Â±0.22 42.55 Â±0.82 40.61 Â±0.41 39.79 Â±0.00 42.86 Â±0.44 46.86Â±0.24 46.53Â±0.57
LOSAUROC 80.56Â±0.33 81.36 Â±1.32 81.13 Â±0.66 80.71 Â±0.45 82.58 Â±0.00 82.11Â±0.39 81.92 Â±0.73 82.91Â±1.02
F1 73.01Â±0.52 73.45 Â±0.59 72.51 Â±0.27 73.84 Â±0.61 73.18 Â±0.00 74.43 Â±0.88 74.46 Â±0.52 74.58Â±0.63
25-PHEAUROC 69.45Â±0.72 66.58 Â±0.41 69.55 Â±0.67 69.18 Â±0.32 63.39 Â±0.00 70.54 Â±0.47 70.42Â±0.26 71.23Â±0.53
F1 28.59Â±0.46 28.55 Â±0.31 27.86 Â±0.29 28.52 Â±0.22 42.13Â±0.00 31.25Â±0.18 30.44 Â±0.27 31.33 Â±0.19
formance of various router design mechanisms within our MoE architecture, utilizing the Laplace
gating function, compared against representative baselines. The baselines include (1) the early fu-
sion method, Tensor Fusion Network (TFN) [96]; (2) the Multimodal Transformer (MulT), which
fuses modalities by modeling their interactions [90]; (3) the Multimodal Adaptation Gate (MAG),
which focuses on the consistency and differences across modalities [74]; and (4) multimodal fusion
using standard MoE with the Softmax gating function. Results indicate that employing an MoE
backboneâ€”regardless of the gating function chosen or whether utilizing per-modality routers or a
joint experts & router configurationâ€”significantly enhances performance on the multimodal task.
This improvement is attributed to the MoEâ€™s ability to effectively allocate specific components to
handle distinct input modalities, thus better addressing both inter- and intra-modal relationships.
CIFAR-10 Dataset Subsequently, we evaluate our method using the Vision-MoE framework [77]
on the CIFAR-10 classification task [46], with results illustrated in Figure 4(a). In this experiment,
we selectively replace the FFN layers with an even number in the Vision Transformer (ViT) mod-
els with MoE layers. These results, along with Table 3 on the CMU-MOSI and MOSEI datasets
comparing Softmax-gating MoE, indicate that the Laplace gating function surpasses the standard
Softmax gating function in performance. This outcome is consistent with our theoretical claims.
MIMIC-IV and PAM Datasets We then conduct comprehensive evaluations of FuseMoE on
MIMIC-IV [36], and the Physical Activity Monitoring (PAM) dataset [75]. These datasets feature
multiple input modalities, each characterized by varying degrees of irregular sampling or significant
levels of missingness . Our tasks of interest for MIMIC datasets include the 48-hour in-hospital mor-
tality prediction (48-IHM), 25-type phenotype classification (25-PHE), and length-of-stay (LOS)
prediction. In addition to the previously mentioned baselines, we have incorporated the HAIM
method [83], a data pipeline specifically designed for integrating multimodal data from the MIMIC-
IV dataset. We also include the cross-attention combined with irregular sequences modeling ap-
proach (MISTS) [101]. Table 4 shows the outcomes of combining irregular vital signs and clinical
notes from the MIMIC-IV dataset. In addition to the commonly used Softmax gating function, we
also evaluated the Gaussian gating function [93] as a comparative benchmark. The FuseMoE -based
methods surpass baselines in most scenarios, often by a non-trivial margin. Furthermore, we ob-
serve that HAIM shows considerable efficacy in extracting features from time series, resulting in a
strong performance in the 48-IHM and LOS tasks, which are heavily reliant on such data. However,
its performance appears more moderate on the 25-PHE task. The PAM dataset captures daily liv-
ing activities through 17 sensors, with data from each sensor treated as a separate modality. These
modalities are individually processed through time-series and irregularity encoders before being in-
tegrated into the FuseMoE framework. Our baselines include the Transformer [92], GRU-D [10],
SeFT [31], a mTAND-only configuration, and IP-Net [81]. We use the Laplace gating and its joint
experts & router structure in these experiments. The results in Figure 4(b) have again shown the
efficacy of integrating the irregularity encoder with the MoE fusion layer.
4.2 Ablation Studies
Scalability of FuseMoE with Increasing Modalities Table 5 presents the revised outcomes of
the MIMIC-IV dataset after integrating CXR and ECG of corresponding patients, employing the
per-modality router and the entropy loss Ewithin FuseMoE . This setup was chosen as it slightly
outperformed the joint router with an increase in modalities. Relative to their two-modality ver-
sions, FuseMoE has effectively harnessed additional information (notably from CXR), resulting in
9Table 5: Incorporating CXR and ECG into FuseMoE leads to a noticeable enhancement as compared to their
two-modality counterparts. All results are averaged across 5 random experiments.
Task\MethodVital & Notes & CXR Vital & Notes & CXR & ECG
HAIM Softmax Gaussian Laplace HAIM Softmax Gaussian Laplace
48-IHMAUROC 78.87Â±0.00 83.13 Â±0.36 83.64 Â±0.47 83.87Â±0.33 78.87Â±0.00 82.92 Â±0.22 83.03 Â±0.85 83.55Â±0.49
F1 39.78Â±0.00 46.82Â±0.28 38.87Â±0.26 45.36 Â±0.46 39.78Â±0.00 46.87 Â±0.17 44.04Â±0.26 46.88Â±0.42
LOSAUROC 82.46Â±0.00 83.76Â±0.59 83.64Â±0.52 83.51Â±0.51 82.46Â±0.00 83.53 Â±0.34 83.47Â±0.37 83.58Â±0.78
F1 72.75Â±0.00 74.32 Â±0.44 76.59Â±0.74 75.18Â±0.77 72.75Â±0.00 75.01 Â±0.63 74.43Â±0.64 75.11Â±0.65
25-PHEAUROC 63.57Â±0.00 73.87Â±0.71 72.68Â±0.61 73.65 Â±0.39 63.82Â±0.00 73.64 Â±0.89 73.74Â±0.41 73.67Â±0.71
F1 42.80Â±0.00 35.96Â±0.23 35.09 Â±0.15 36.01 Â±0.42 43.20Â±0.00 36.06Â±0.17 36.46 Â±0.55 35.81Â±0.34
a significant enhancement in performance. Conversely, the addition of new modalities did not ben-
efit the HAIM method, possibly due to its reliance on vital signs and clinical notes without ade-
quately addressing the dynamics between different modalities. Furthermore, HAIMâ€™s notably high
F1 scores on the 25-PHE task can be attributed to XGBoostâ€™s proficiency in managing missing mi-
nority classes. Note that, except for HAIM, other baselines were not designed to be agnostic to the
quantity and variety of input modalities. Therefore, adapting them to manage extra and missing
modalities requires considerable model changes, which might compromise their performance.
Missing Modalities Figure 4(c) illustrates the effectiveness of utilizing per-modality routers and
the entropy loss Ein addressing missing modalities. Initially, we compare the performance of
FuseMoE on patients with fully available modalities against those with missing components, em-
ploying a joint router mechanism with the importance loss function [79], to ensure load balancing.
The inclusion of datasets with missing modalities, while expanding the sample size, resulted in a de-
crease in performance due to the compromised data quality. However, a performance enhancement
was observed upon integrating per-modality or disjoint routers with E. Notably, the outcomes for
the 48-IHM and LOS tasks with missing modalities surpassed those obtained from datasets with-
out any missingness. This is because the per-modality approach can better separate the present and
missing modalities, reducing the influence of experts responsible for processing the absent inputs.
Therefore, this leads to a more efficient exploitation of a broader array of samples.
5 Discussions and Limitations
In this paper, we introduced FuseMoE , a model adept at managing multimodal data characterized
by random missingness or irregularityâ€”a crucial yet relatively unexplored challenge. FuseMoE
integrates MoE fusion layers with modal embeddings and offers multiple router configurations to
adeptly handle multimodal inputs across different complexity levels. FuseMoE also employs an
innovative Laplace gating function, which provides better theoretical results. Through empirical
evaluation, FuseMoE has demonstrated superior performance across diverse scenarios. However,
our current approach to encoding irregularities may potentially lead to over-parameterization when
the input size is small. In our future work, we aim to identify simpler and more efficient methods to
handle the irregularities of input samples while preserving the modelâ€™s overall performance.
Acknowledgement
Xing Han and Suchi Saria acknowledge support from the National Science Foundation (NSF) and
the Gordon and Betty Moore Foundation. Carl Harris acknowledges support from the NSF Graduate
Research Fellowship under Grant No. DGE 2139757. Nhat Ho acknowledges support from the
NSF IFML 2019844 and the NSF AI Institute for Foundations of Machine Learning. Any opinion,
findings, and conclusions or recommendations expressed in this material are those of the authors(s)
and do not necessarily reflect the views of the NSF, Gordon and Betty Moore Foundation.
References
[1] Vibhu Agarwal, Tanya Podchiyska, Juan M Banda, Veena Goel, Tiffany I Leung, Evan P
Minty, Timothy E Sweeney, Elsie Gyang, and Nigam H Shah. Learning statistical models of
10phenotypes using noisy labeled training data. Journal of the American Medical Informatics
Association , 23(6):1166â€“1173, 2016.
[2] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann,
and Matthew McDermott. Publicly available clinical bert embeddings. arXiv preprint
arXiv:1904.03323 , 2019.
[3] Aryan Arbabi, David R Adams, Sanja Fidler, Michael Brudno, et al. Identifying clinical
terms in medical text using ontology-guided machine learning. JMIR medical informatics ,
7(2):e12596, 2019.
[4] Zachi I Attia, Suraj Kapa, Francisco Lopez-Jimenez, Paul M McKie, Dorothy J Ladewig,
Gaurav Satam, Patricia A Pellikka, Maurice Enriquez-Sarano, Peter A Noseworthy,
Thomas M Munger, et al. Screening for cardiac contractile dysfunction using an artificial
intelligenceâ€“enabled electrocardiogram. Nature medicine , 25(1):70â€“74, 2019.
[5] Aya Awad, Mohamed Bader-El-Den, James McNicholas, and Jim Briggs. Early hospital
mortality prediction of intensive care unit patients using an ensemble learning approach. In-
ternational journal of medical informatics , 108:185â€“195, 2017.
[6] Dimitris Bertsimas, Jean Pauphilet, Jennifer Stevens, and Manu Tandon. Predicting inpatient
flow at a major hospital using interpretable analytics. Manufacturing & Service Operations
Management , 24(6):2809â€“2824, 2022.
[7] Serhat S Bucak, Rong Jin, and Anil K Jain. Multiple kernel learning for visual object
recognition: A review. IEEE Transactions on Pattern Analysis and Machine Intelligence ,
36(7):1354â€“1369, 2013.
[8] Rhonda R Butler. Icd-10 general equivalence mappings: Bridging the translation gap from
icd-9. Journal of AHIMA , 78(9):84â€“86, 2007.
[9] Bing Cao, Yiming Sun, Pengfei Zhu, and Qinghua Hu. Multi-modal gated mixture of local-
to-global experts for dynamic image fusion. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 23555â€“23564, 2023.
[10] Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Re-
current neural networks for multivariate time series with missing values. Scientific reports ,
8(1):6085, 2018.
[11] JunKai Chen, Zenghai Chen, Zheru Chi, and Hong Fu. Emotion recognition in the wild
with feature fusion and multiple kernel learning. In Proceedings of the 16th International
Conference on Multimodal Interaction , pages 508â€“513, 2014.
[12] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings
of the 22nd acm sigkdd international conference on knowledge discovery and data mining ,
pages 785â€“794, 2016.
[13] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Sing-
hal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representa-
tion collapse of sparse mixture of experts. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems , 2022.
[14] Joseph Paul Cohen, Joseph D Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo
Guarrera, Matthew P Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, et al.
Torchxrayvision: A library of chest x-ray datasets and models. In International Conference
on Medical Imaging with Deep Learning , pages 231â€“249. PMLR, 2022.
[15] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data
Via the EM Algorithm. Journal of the Royal Statistical Society: Series B (Methodological) ,
39(1):1â€“22, September 1977.
[16] Iman Deznabi, Mohit Iyyer, and Madalina Fiterau. Predicting in-hospital mortality by com-
bining clinical notes with time-series data. In Findings of the association for computational
linguistics: ACL-IJCNLP 2021 , pages 4026â€“4031, 2021.
11[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 , 2020.
[18] A Elixhauser. Clinical classifications software (ccs) 2009. http://www. hcug-us. ahrq.
gov/toolssoft-ware/ccs/ccs. jsp , 2009.
[19] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity. The Journal of Machine Learning Re-
search , 23(1):5232â€“5270, 2022.
[20] Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you
talking to a machine? dataset and methods for multilingual image question. Advances in
neural information processing systems , 28, 2015.
[21] Ashutosh Garg, Vladimir Pavlovic, and James M Rehg. Boosted learning in dynamic bayesian
networks for multimodal speaker detection. Proceedings of the IEEE , 91(9):1355â€“1369,
2003.
[22] Aude Genevay. Entropy-regularized optimal transport for machine learning . PhD thesis,
Paris Sciences et Lettres (ComUE), 2019.
[23] Ary L Goldberger, Luis AN Amaral, Leon Glass, Jeffrey M Hausdorff, Plamen Ch Ivanov,
Roger G Mark, Joseph E Mietus, George B Moody, Chung-Kang Peng, and H Eugene Stan-
ley. Physiobank, physiotoolkit, and physionet: components of a new research resource for
complex physiologic signals. circulation , 101(23):e215â€“e220, 2000.
[24] Brian Gow, Tom Pollard, Larry A Nathanson, Alistair Johnson, Benjamin Moody, Chrystinne
Fernandes, Nathaniel Greenbaum, Seth Berkowitz, Dana Moukheiber, Parastou Eslami, et al.
Mimic-iv-ecg-diagnostic electrocardiogram matched subset. 2022.
[25] Wei Han, Hui Chen, Alexander Gelbukh, Amir Zadeh, Louis-philippe Morency, and Soujanya
Poria. Bi-bimodal modality fusion for correlation-controlled multimodal sentiment analysis.
InProceedings of the 2021 International Conference on Multimodal Interaction , pages 6â€“15,
2021.
[26] Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchi-
cal mutual information maximization for multimodal sentiment analysis. arXiv preprint
arXiv:2109.00412 , 2021.
[27] Hrayr Harutyunyan, Hrant Khachatrian, David C Kale, Greg Ver Steeg, and Aram Galstyan.
Multitask learning and benchmarking with clinical time series data. Scientific data , 6(1):96,
2019.
[28] N. Ho and X. Nguyen. Convergence rates of parameter estimation for some weakly identifi-
able finite mixtures. Annals of Statistics , 44:2726â€“2755, 2016.
[29] Nhat Ho, Chiao-Yu Yang, and Michael I. Jordan. Convergence rates for Gaussian mixtures
of experts. Journal of Machine Learning Research , 23(323):1â€“81, 2022.
[30] Sepp Hochreiter and J Â¨urgen Schmidhuber. Long short-term memory. Neural computation ,
9(8):1735â€“1780, 1997.
[31] Max Horn, Michael Moor, Christian Bock, Bastian Rieck, and Karsten Borgwardt. Set func-
tions for time series. In International Conference on Machine Learning , pages 4353â€“4363.
PMLR, 2020.
[32] Shih-Cheng Huang, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew P Lungren.
Fusion of medical imaging and electronic health records using deep learning: a systematic
review and implementation guidelines. NPJ digital medicine , 3(1):136, 2020.
[33] Shih-Cheng Huang, Anuj Pareek, Roham Zamanian, Imon Banerjee, and Matthew P Lungren.
Multimodal fusion with deep neural networks for leveraging ct imaging and electronic health
record: a case-study in pulmonary embolism detection. Scientific reports , 10(1):22147, 2020.
12[34] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute,
Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large
chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of
the AAAI conference on artificial intelligence , volume 33, pages 590â€“597, 2019.
[35] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive
mixtures of local experts. Neural computation , 3(1):79â€“87, 1991.
[36] Alistair Johnson, Lucas Bulgarelli, Tom Pollard, Steven Horng, Leo Anthony Celi,
and Roger Mark. Mimic-iv. PhysioNet. Available online at: https://physionet.
org/content/mimiciv/1.0/(accessed August 23, 2021) , 2020.
[37] Alistair Johnson, Matt Lungren, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and
Steven Horng. Mimic-cxr-jpg-chest radiographs with structured labels. PhysioNet , 2019.
[38] Alistair Johnson, Tom Pollard, Steven Horng, Leo Anthony Celi, and Roger Mark. Mimic-
iv-note: Deidentified free-text clinical notes, 2023.
[39] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P
Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified pub-
licly available database of chest radiographs with free-text reports. Scientific data , 6(1):317,
2019.
[40] Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H Lehman, Mengling Feng, Moham-
mad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark.
Mimic-iii, a freely accessible critical care database. Scientific data , 3(1):1â€“9, 2016.
[41] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization
networks for dense captioning. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 4565â€“4574, 2016.
[42] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image de-
scriptions. In Proceedings of the IEEE conference on computer vision and pattern recogni-
tion, pages 3128â€“3137, 2015.
[43] Seyed Mehran Kazemi, Rishab Goel, Sepehr Eghbali, Janahan Ramanan, Jaspreet Sahota,
Sanjay Thakur, Stella Wu, Cathal Smyth, Pascal Poupart, and Marcus Brubaker. Time2vec:
Learning a vector representation of time. arXiv preprint arXiv:1907.05321 , 2019.
[44] Britt E Keuning, Thomas Kaufmann, Renske Wiersema, Anders Granholm, Ville Pettil Â¨a,
Morten Hylander MÃ¸ller, Christian Fynbo Christiansen, Jos Â´e Castela Forte, Harold Snieder,
Frederik Keus, et al. Mortality prediction models in the adult critically ill: A scoping review.
Acta Anaesthesiologica Scandinavica , 64(4):424â€“442, 2020.
[45] Swaraj Khadanga, Karan Aggarwal, Shafiq Joty, and Jaideep Srivastava. Using clinical notes
with time series data for icu management. arXiv preprint arXiv:1909.09702 , 2019.
[46] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny
images. 2009.
[47] Yann LeCun, L Â´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE , 86(11):2278â€“2324, 1998.
[48] Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, and
Ziwei Liu. Sparse mixture-of-experts are domain generalizable learners. arXiv preprint
arXiv:2206.04046 , 2022.
[49] Yikuan Li, Ramsey M Wehbe, Faraz S Ahmad, Hanyin Wang, and Yuan Luo. Clinical-
longformer and clinical-bigbird: Transformers for long clinical sequences. arXiv preprint
arXiv:2201.11838 , 2022.
[50] Ke Lin, Yonghua Hu, and Guilan Kong. Predicting in-hospital mortality of patients with
acute kidney injury in the icu using random forest model. International journal of medical
informatics , 125:55â€“61, 2019.
13[51] Jinghui Liu, Daniel Capurro, Anthony Nguyen, and Karin Verspoor. Attention-based mul-
timodal fusion with contrast for robust clinical prediction in the face of missing modalities.
Journal of Biomedical Informatics , 145:104466, 2023.
[52] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, Amir Zadeh,
and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific
factors. arXiv preprint arXiv:1806.00064 , 2018.
[53] Karla R Lovaasen and Jennifer Schwerdtfeger. ICD-9-CM Coding: Theory and Practice with
ICD-10, 2013/2014 Edition-E-Book . Elsevier Health Sciences, 2012.
[54] Navonil Majumder, Devamanyu Hazarika, Alexander Gelbukh, Erik Cambria, and Sou-
janya Poria. Multimodal sentiment analysis using hierarchical fusion with context modeling.
Knowledge-based systems , 161:124â€“133, 2018.
[55] T. Manole and N. Ho. Refined convergence rates for maximum likelihood estimation un-
der finite mixture models. In Proceedings of the 39th International Conference on Machine
Learning , volume 162 of Proceedings of Machine Learning Research , pages 14979â€“15006.
PMLR, 17â€“23 Jul 2022.
[56] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg,
and Oriol Nieto. librosa: Audio and music signal analysis in python. In SciPy , pages 18â€“24,
2015.
[57] Clara Meister, Elizabeth Salesky, and Ryan Cotterell. Generalized entropy regularization or:
Thereâ€™s nothing special about label smoothing. arXiv preprint arXiv:2005.00820 , 2020.
[58] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Mul-
timodal contrastive learning with limoe: the language-image mixture of experts. Advances in
Neural Information Processing Systems , 35:9564â€“9576, 2022.
[59] Ara V Nefian, Luhong Liang, Xiaobo Pi, Liu Xiaoxiang, Crusoe Mao, and Kevin Murphy. A
coupled hmm for audio-visual speech recognition. In 2002 IEEE International Conference
on Acoustics, Speech, and Signal Processing , volume 2, pages IIâ€“2013. IEEE, 2002.
[60] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng.
Multimodal deep learning. In Proceedings of the 28th international conference on machine
learning (ICML-11) , pages 689â€“696, 2011.
[61] Huy Nguyen, Pedram Akbarian, and Nhat Ho. Is temperature sample efficient for softmax
Gaussian mixture of experts? In Proceedings of the ICML , 2024.
[62] Huy Nguyen, Pedram Akbarian, TrungTin Nguyen, and Nhat Ho. A general theory for soft-
max gating multinomial logistic mixture of experts. In Proceedings of the ICML , 2024.
[63] Huy Nguyen, Pedram Akbarian, Fanqi Yan, and Nhat Ho. Statistical perspective of top-k
sparse softmax gating mixture of experts. In International Conference on Learning Repre-
sentations , 2024.
[64] Huy Nguyen, TrungTin Nguyen, and Nhat Ho. Demystifying softmax gating function in
Gaussian mixture of experts. In Advances in Neural Information Processing Systems , 2023.
[65] Huy Nguyen, TrungTin Nguyen, Khai Nguyen, and Nhat Ho. Towards convergence rates
for parameter estimation in Gaussian-gated mixture of experts. In Proceedings of The 27th
International Conference on Artificial Intelligence and Statistics , 2024.
[66] Xiaonan Nie, Xupeng Miao, Shijie Cao, Lingxiao Ma, Qibin Liu, Jilong Xue, Youshan Miao,
Yi Liu, Zhi Yang, and Bin Cui. Evomoe: An evolutional mixture-of-experts training frame-
work via dense-to-sparse gate. arXiv preprint arXiv:2112.14397 , 2021.
[67] Behnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, Tadas Baltru Ë‡saitis, and Louis-
Philippe Morency. Deep multimodal fusion for persuasiveness prediction. In Proceedings of
the 18th ACM International Conference on Multimodal Interaction , pages 284â€“288, 2016.
14[68] Raghavendra Pappagari, Piotr Zelasko, Jes Â´us Villalba, Yishay Carmiel, and Najim Dehak.
Hierarchical transformers for long document classification. In 2019 IEEE automatic speech
recognition and understanding workshop (ASRU) , pages 838â€“844. IEEE, 2019.
[69] Quang Pham, Giang Do, Huy Nguyen, TrungTin Nguyen, Chenghao Liu, Mina Sartipi,
Binh T. Nguyen, Savitha Ramasamy, Xiaoli Li, Steven Hoi, and Nhat Ho. Competes-
moe â€“ effective training of sparse mixture of experts via competition. arXiv preprint
arXiv:2402.02526 , 2024.
[70] Soujanya Poria, Erik Cambria, and Alexander Gelbukh. Deep convolutional neural network
textual features and multiple kernel learning for utterance-level multimodal sentiment anal-
ysis. In Proceedings of the 2015 conference on empirical methods in natural language pro-
cessing , pages 2539â€“2544, 2015.
[71] Gerasimos Potamianos, Chalapathy Neti, Guillaume Gravier, Ashutosh Garg, and Andrew W
Senior. Recent advances in the automatic recognition of audiovisual speech. Proceedings of
the IEEE , 91(9):1306â€“1326, 2003.
[72] Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft
mixtures of experts. arXiv preprint arXiv:2308.00951 , 2023.
[73] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. Journal of Machine Learning Research , 21(140):1â€“67, 2020.
[74] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-
Philippe Morency, and Ehsan Hoque. Integrating multimodal information in large pretrained
transformers. In Proceedings of the conference. Association for Computational Linguistics.
Meeting , volume 2020, page 2359. NIH Public Access, 2020.
[75] Attila Reiss and Didier Stricker. Introducing a new benchmarked dataset for activity moni-
toring. In 2012 16th international symposium on wearable computers , pages 108â€“109. IEEE,
2012.
[76] Stephan Reiter, Bjorn Schuller, and Gerhard Rigoll. Hidden conditional random fields for
meeting segmentation. In 2007 IEEE International Conference on Multimedia and Expo ,
pages 639â€“642. IEEE, 2007.
[77] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton,
Andr Â´e Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture
of experts. Advances in Neural Information Processing Systems , 34:8583â€“8595, 2021.
[78] Thanveer Shaik, Xiaohui Tao, Lin Li, Haoran Xie, and Juan D Vel Â´asquez. A survey of mul-
timodal information fusion for smart healthcare: Mapping the journey from data to wisdom.
Information Fusion , page 102040, 2023.
[79] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hin-
ton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-
experts layer. arXiv preprint arXiv:1701.06538 , 2017.
[80] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, and Yuxiong He. Scal-
ing vision-language models with sparse mixture of experts. arXiv preprint arXiv:2303.07226 ,
2023.
[81] Satya Narayan Shukla and Benjamin M Marlin. Interpolation-prediction networks for irreg-
ularly sampled time series. arXiv preprint arXiv:1909.07782 , 2019.
[82] Satya Narayan Shukla and Benjamin M Marlin. Multi-time attention networks for irregularly
sampled time series. arXiv preprint arXiv:2101.10318 , 2021.
[83] Luis R Soenksen, Yu Ma, Cynthia Zeng, Leonard Boussioux, Kimberly Villalobos Carballo,
Liangyuan Na, Holly M Wiberg, Michael L Li, Ignacio Fuentes, and Dimitris Bertsimas. In-
tegrated multimodal artificial intelligence framework for healthcare applications. NPJ digital
medicine , 5(1):149, 2022.
15[84] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural
networks. In International conference on machine learning , pages 6105â€“6114. PMLR, 2019.
[85] H. Teicher. On the mixture of distributions. Annals of Statistics , 31:55â€“73, 1960.
[86] H. Teicher. Identifiability of mixtures. Annals of Statistics , 32:244â€“248, 1961.
[87] H. Teicher. Identifiability of finite mixtures. Ann. Math. Statist. , 32:1265â€“1269, 1963.
[88] Sindhu Tipirneni and Chandan K Reddy. Self-supervised transformer for sparse and irregu-
larly sampled multivariate clinical time-series. ACM Transactions on Knowledge Discovery
from Data (TKDD) , 16(6):1â€“17, 2022.
[89] Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin. Missing modalities imputation via
cascaded residual autoencoder. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 1405â€“1414, 2017.
[90] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency,
and Ruslan Salakhutdinov. Multimodal transformer for unaligned multimodal language se-
quences. In Proceedings of the conference. Association for Computational Linguistics. Meet-
ing, volume 2019, page 6558. NIH Public Access, 2019.
[91] S. van de Geer. Empirical Processes in M-estimation . Cambridge University Press, 2000.
[92] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-
tion processing systems , 30, 2017.
[93] Lei Xu, Michael Jordan, and Geoffrey E Hinton. An alternative model for mixtures of experts.
Advances in neural information processing systems , 7, 1994.
[94] Bo Yang, Tao Mei, Xian-Sheng Hua, Linjun Yang, Shi-Qiang Yang, and Mingjing Li. Online
video recommendation based on multimodal fusion and relevance feedback. In Proceedings
of the 6th ACM international conference on Image and video retrieval , pages 73â€“80, 2007.
[95] Bo Yang and Lijun Wu. How to leverage multimodal ehr data for better medical predictions?
arXiv preprint arXiv:2110.15763 , 2021.
[96] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency.
Tensor fusion network for multimodal sentiment analysis. arXiv preprint arXiv:1707.07250 ,
2017.
[97] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria, and Louis-Philippe
Morency. Multi-attention recurrent network for human communication comprehension. In
Thirty-Second AAAI Conference on Artificial Intelligence , 2018.
[98] Xunlin Zhan, Yangxin Wu, Xiao Dong, Yunchao Wei, Minlong Lu, Yichi Zhang, Hang Xu,
and Xiaodan Liang. Product1m: Towards weakly supervised instance-level product retrieval
via cross-modal pretraining. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 11782â€“11791, 2021.
[99] Dongyu Zhang, Jidapa Thadajarassiri, Cansu Sen, and Elke Rundensteiner. Time-aware
transformer-based network for clinical notes series prediction. In Machine learning for
healthcare conference , pages 566â€“588. PMLR, 2020.
[100] Xiang Zhang, Marko Zeman, Theodoros Tsiligkaridis, and Marinka Zitnik. Graph-guided
network for irregularly sampled multivariate time series. arXiv preprint arXiv:2110.05357 ,
2021.
[101] Xinlu Zhang, Shiyang Li, Zhiyu Chen, Xifeng Yan, and Linda Ruth Petzold. Improving med-
ical predictions by irregular multimodal electronic health records modeling. In International
Conference on Machine Learning , pages 41300â€“41313. PMLR, 2023.
[102] Tongxue Zhou, Su Ruan, and St Â´ephane Canu. A review: Deep learning for medical image
segmentation using multi-modality fusion. Array , 3:100004, 2019.
16[103] Yanqi Zhou, Nan Du, Yanping Huang, Daiyi Peng, Chang Lan, Da Huang, Siamak Shakeri,
David So, Andrew M Dai, Yifeng Lu, et al. Brainformers: Trading simplicity for efficiency.
InInternational Conference on Machine Learning , pages 42531â€“42542. PMLR, 2023.
[104] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai,
Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in
Neural Information Processing Systems , 35:7103â€“7114, 2022.
17Appendix for
â€œFuseMoE: Mixture-of-Experts Transformers for
Fleximodal Fusionâ€
Contents
1 Introduction 1
2 FuseMoE: Enhance Predictive Performance for FlexiModal Data 3
2.1 Sparse MoE Backbone . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 Modality and Irregularity Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.3 MoE Fusion Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 Theoretical Contribution 5
4 Experiments 8
4.1 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
5 Discussions and Limitations 10
A Related Works 20
B FlexiModal Data and Tasks of Interest 20
B.1 MIMIC-IV and MIMIC-III Datasets . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2 MOSI and MOSEI Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 PAM Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.4 CIFAR-10 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C Mechanisms of Different Router Designs 23
C.1 Joint Experts & Routers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.2 Modality-Specific Router . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.3 Disjoint Experts & Routers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D Data Preprocessing 24
D.1 MIMIC-IV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.2 PAM Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E Modeling Irregularity 26
E.1 Unified Temporal Discretization Embeddings . . . . . . . . . . . . . . . . . . . . 26
E.2 Unifying imputation and mTAND embeddings . . . . . . . . . . . . . . . . . . . . 26
F Baseline Comparison 26
F.1 MISTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
18F.2 MulT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.3 MAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
F.4 TFN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
F.5 HAIM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
F.6 Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
G Computational Resources and Hyper-Parameters 27
G.1 Computational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
G.2 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
H Additional Results 27
H.1 FlexiModal Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
H.2 Ablation Study on FuseMoE Building Blocks . . . . . . . . . . . . . . . . . . . . 28
H.3 Ablation Study on MoE Architecture . . . . . . . . . . . . . . . . . . . . . . . . . 29
I Details on Numerical Experiments 31
I.1 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
I.2 Exact-specified Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
I.3 Over-specified Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
J Exact-Specified Setting 32
K Proof of Theoretical Results 33
K.1 Proof of Theorem J.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
K.2 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
K.2.1 Main Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
K.2.2 Proof of Lemma K.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
K.3 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
L Identifiability of the Laplace Gating Gaussian MoE 43
M Broader Impact 44
19A Related Works
Multimodal Fusion Initial approaches to multimodal fusion incorporated techniques such as
kernel-based methods [7, 11, 70], graphical models [59, 21, 76], and neural networks [60, 20, 67].
With the diverse evolution of deep learning models, numerous advanced methods have now been
employed in the fusion of multimodal data. In the realm of sentiment analysis, [96, 52] employ
a low-rank Tensor Fusion method that leverages both language and video content. Attention-
gating mechanisms are used by [74, 95] to generate displacement vectors through cross-modal
self-attention, which are then added to the input vectors from the primary modality. [90] takes an
alternative approach by integrating multiple layers of cross-modal attention blocks in a word-level
vision/language/audio alignment task.
In the context of clinical prediction, [45, 16] adopt a late fusion approach to combining vital sign
and text data by concatenating embeddings from pre-trained feature extractors. [83] developed a
generalizable data preprocessing and modeling pipeline for EHR encompassing four data modali-
ties, albeit through a direct concatenation of existing feature embeddings for each modality followed
by an XGBoost classifier [12]. Recently, [101] expanded on the work of [90] by introducing a dis-
cretized multi-time attention (mTAND) module [82] to encode temporal irregularities in time series
and text data. Their fusion approach involves layering sets of self- and cross-modal attention blocks.
However, this approach is limited to just two modalities and is not easily extendable to include
additional modal components or handle missing modalities. To the best of our knowledge, exist-
ing works are tailored to application-specific settings that necessitate the computation of pairwise
cross-modal relationships, which are not scalable to more general settings with arbitrary modalities.
Moreover, these studies typically do not account for scenarios where modalities are missing, or rely
on imputation approaches based on observed data.
Mixture-of-Experts MoE [35, 93] has gained significant popularity for managing complex tasks
since its introduction three decades ago. Unlike traditional models that reuse the same parameters
for all inputs, MoE selects distinct parameters for each specific input. This results in a sparsely
activated layer, enabling a substantial scaling of model capacity without a corresponding increase
in computational cost. Recent studies have demonstrated the effectiveness of integrating MoE with
cutting-edge models across a diverse range of tasks [79, 19, 103]. These works have also tackled
key challenges such as accuracy and training instability [66, 104, 72]. Given its ability to assign
input partitions to specialized experts, MoE naturally lends itself to multimodal applications. This
approach has been explored in fields such as vision-language modeling [58, 80] and dynamic image
fusion [9]. However, the application of MoE in complex real-world settings, such as those involving
FlexiModal Data, remains largely unexplored. This gap presents an opportunity to leverage MoEâ€™s
potential in handling its intricate and multifaceted nature such as multimodal EHR, where reliable
multimodal integration is crucial.
MoE Theory While MoE has been widely employed to scale up large models, its theoretical
foundations have remained nascent. Recently, [64] provided convergence rates for both density and
parameter estimation of Softmax gating Gaussian MoE. They connected these rates to the solvability
of systems of polynomial equations under V oronoi-based loss functions. Later, [63] extended these
theories to the top-K sparse softmax gating Gaussian MoE. Their theories further characterize the
effect of the sparsity of gating functions on the behaviors of parameter estimation and verify the
benefits of using top-1 sparse softmax gating MoE in practice. Other theoretical results for MoE
include estimation rates of parameters and experts for multinomial logistic MoE [62], for dense-to-
sparse gating MoE [61], for Gaussian gating MoE [65], and for input-independent gating MoE [29].
B FlexiModal Data and Tasks of Interest
Definition of FlexiModal Data We provide a generic definition for the FlexiModal Data as we
used throughout the paper. Let D={(xm1
i, tm1
i),(xm2
i, tm2
i), . . . , (xmj
i, tmj
i), yi}N
i=1to be the
FlexiModal dataset with Nunits, where xmj
irepresents the input sequence from the ithunit of the
jthmodality, tmj
idenotes the corresponding time points, and yiis the task-specific outcome. Take
multimodal EHR as an example, each jthmodality, which may vary from time-series data like heart
20rate, blood pressure, and glucose levels to high-dimensional inputs such as clinical notes and X-rays,
contains lmjobservations. Figure 5 is a more specific illustration of the FlexiModal example.
B.1 MIMIC-IV and MIMIC-III Datasets
Tasks In the ICU, where rapid and informed decisions are crucial, accurate mortality prediction
is essential to provide clinicians with advanced warnings of patient deterioration, aiding in critical
decision-making processes [5]. Similarly, the prediction of patient length-of-stay is indispensable
for optimizing treatment plans, resource allocation, and discharge processes [6]. Further, phenotyp-
ing of critical care conditions is highly relevant to comorbidity detection and risk adjustment and
presents a more challenging task than binary classification, due to the heterogeneous presentation
of conditions and the larger number of prediction tasks [101]. We concentrate on three critical care
tasks as highlighted in [44], performing extensive empirical analysis on each building block of the
proposed framework.
â€¢48-IHM In this binary classification task, we predict in-hospital mortality based on the
first 48 of the ICU stay for patients who stayed in the ICU for at least 48 hours.
â€¢LOS We formulate our length-of-stay task similar to that of 48-IHM: for patients who
spent at least 48 hours in the ICU, we predict ICU discharge without expiration within the
following 48 hours.
â€¢25-PHE In this multilabel classification problem, we attempt to predict one of 25 acute
care conditions [18, 53] (e.g., congestive heart failure, pneumonia, shock, etc.) at the end
each each patientâ€™s ICU stay. Because the original task was designed for diagnoses based
on ICD-9 codes, but MIMIC-IV includes both ICD-9 and ICD-10 codes, we map patients
with diagnoses coded using ICD-10 using the conversion database provided by [8].
Figure 5: Schematic of tasks of interest. Plotted are example vitals/labs, radiological notes, X-rays,
and ECGs sampled over the course of a patientâ€™s ICU stay. The first three rows represent example
observations from a single modality consisting of three irregularly sampled vital signs (HR, BP), and
lab values (Glucose). The following three rows represent irregularly sampled radiological notes, X-
rays, and ECGs. Opaque shapes denote observations falling within the observation window (i.e.,
observations that are used to generate predictions), while translucent shapes are not used to generate
predictions. For the 48-IHM task, we use the first 48 hours of observations to predict death at
any time during the ICU stay. For the LOS task, we use the first 48 hours of observations to predict
whether the patient will be discharged (alive) during the following 48 hours. And in the phenotyping
task ( PHE ), we use all observations to predict one of 25 critical care conditions.
We implement an in-hospital mortality prediction ( 48-IHM ) task to evaluate our methodâ€™s ability
to predict short-term patient deterioration. Similarly, an accurate determination of patient discharge
times is crucial for optimizing patient outcomes and hospital resource allocation [6], which motivates
our length-of-stay ( LOS ) task. We frame 48-IHM and LOS as binary classification problems and
use a 48-hour observation window (for patients who spent at least 48 hours in the ICU) to predict
in-hospital mortality (48-IHM) and discharge (without expiration) within the 48 hours following
the observation window (LOS). Lastly, identifying the presence of specific acute care conditions
in patient records is essential for various clinical objectives, including the construction of cohorts
for clinical studies and the detection of comorbidities [1]. Traditional methods, often reliant on
manual chart reviews or simple billing code-based definitions, are increasingly being supplemented
by machine learning techniques [27]; automating this process requires high-fidelity classifications,
motivating our 25-type phenotype classification ( 25-PHE ) task. In this multilabel classification
problem, we attempt to predict one of 25 acute care conditions using data from the entire ICU stay.
21Evaluation In our initial analysis, we focused on patients with no missing modalities, resulting
in a dataset comprised of 8,770 ICU stays for the 48-IHM and LOS tasks, and 14,541 stays for the
25-PHE task. For our analyses with missing observations, we include a total of 35,129 stays for
48-IHM and LOS, and 71.173 for 25-PHE. To evaluate the single-label tasks, 48-IHM and LOS, we
employ the F1-score and AUROC as our primary metrics. In line with previous studies [101, 50, 3],
we use macro-averaged F1-score and AUROC to assess the 25-PHE task.
Dataset Information We leveraged data from MIMIC-IV [36], a comprehensive database with
records from nearly 300kpatients admitted to a medical center from 2008 to 2019, focusing on the
subset of 73,181 ICU stays. We were able to link core ICU records (containing lab results and vital
signs) to corresponding chest X-rays [39], radiological notes [38], and electrocardiogram (ECG)
data [24] taking place during a given ICU stay. We allocated 70 percent of the data for model
training, with the remaining 30 percent evenly split between validation and testing.
Missingness Rates The total number of samples for each of our three tasks (i.e., those in which at
least one vital sign was recorded in the specified observation window), along with the total number
of observations per-modality, are shown in Table 6.
Table 6: We present the total number of ICU stays in each task, taking into account observations with
missing modalities. The total number of stays with at least one observation of the corresponding
modality are shown in the three right-most columns.
Task(s) Total Text CXR ECG
48-IHM & LOS 35,129 32,038 8,781 18,271
25-PHE 73,173 56,824 14,568 35,925
B.2 MOSI and MOSEI Datasets
Task We focus on the multimodal sentiment analysis (MSA) task which aims to predict sentiment
polarity âˆˆ {positive, negative, and neutral }and sentiment intensity, which is a real number ranging
from -3 to +3 under a multimodal setting.
Evaluation Following previous work such as [26], we adopt mean absolute error (MAE), Pearson
correlation (Corr), binary classification accuracy, F1 score computed for non-negative/negative class
as evaluation metrics.
Dataset Information The CMU-MOSI dataset contains 1284/229/686 train/validation/test sam-
ples, and the CMU-MOSEI dataset contains 16326/1871/4659 train/validation/test samples. They
are the largest dataset of multimodal sentiment analysis and emotion recognition to date. The
datasets contain utterance videos from numerous online YouTube speakers, which are transcribed
and properly punctuated, leading to multimodal input consisting of video frames, text, and audio
signals.
B.3 PAM Dataset
Task Physical Activity Monitoring (PAM) dataset measures the daily living activities of 9 subjects
with 3 inertial measurement units. PAM is labeled into 8 classes where each class represents an
activity of daily living.
Evaluation We choose common classification accuracy as the evaluation metric for this task.
Dataset Information The processed PAM dataset contains 5,333 segments (samples) of sensory
signals. Each sample is measured by 17 sensors and contains 600 continuous observations with the
sampling frequency 100 Hz. PAM does not include static attributes and the samples are approxi-
mately balanced across all 8 categories.
22Table 7: Dataset Summary
Dataset Research Area Modalities Sample Size Tasks
MIMIC-III Healthcare Time-Series, Text 36,212 Mortality, length-of-stay, phenotyping
MIMIC-IV Healthcare Time-Series, Text, Images, ECG 73,173 Mortality, length-of-stay, phenotyping
CMU-MOSI Affective Computing Text, Video, Audio 2,199 Sentiment
CMU-MOSEI Affective Computing Text, Video, Audio 22,777 Sentiment, emotions
PAM Healthcare Time-Series 5,333 Activity recognition
CIFAR-10 Multimedia Images 60,000 Image classification
B.4 CIFAR-10 Dataset
CIFAR-10 [46] is an established computer-vision dataset used for object recognition. It consists of
60,000 32x32 color images containing one of 10 object classes (â€planeâ€, â€carâ€, â€birdâ€, â€catâ€, â€deerâ€,
â€dogâ€, â€frogâ€, â€horseâ€, â€shipâ€, â€truckâ€), with 6000 images per class.
C Mechanisms of Different Router Designs
C.1 Joint Experts & Routers
In this approach, a concatenated embedding of all modalities is created, and this combined input is
directed to selected experts by the router. This method allows the model to capture interactions be-
tween modalities at the input level, as the concatenated embedding provides a unified representation
that includes all modalities. The router and experts work with this comprehensive view, enabling
the model to learn correlations and interactions directly from the fused data. However, this ap-
proach might not fully capture modality-specific nuances since the characteristics of each modality
are blended into a single representation.
Advantages
1. Captures inter-modal relationships by considering all modalities together.
2. Simplifies the routing mechanism by treating the concatenated embedding as a single input.
Challenges
1. May overlook modality-specific features due to the blending of all modalities into one
representation.
2. Could be less efficient if some modalities are irrelevant for certain tasks or experts.
C.2 Modality-Specific Router
Each modalityâ€™s embedding is independently assigned to a shared pool of experts by modality-
specific routers. This setup allows the model to maintain the distinctiveness of each modality while
still leveraging a common pool of expertise. By doing so, it can better capture modality-specific
nuances and how they contribute independently to the overall task. However, this approach might
be less effective in capturing complex inter-modal interactions since the initial routing is done inde-
pendently for each modality.
Advantages
1. Preserves modality-specific information by routing each modality independently.
2. Flexible in directing modalities to the most relevant experts, potentially improving effi-
ciency.
3. Captures interactions between modalities to some extent, but may not be as effective as
joint routing approaches.
Challenge needs additional coordination between independent routes to leverage cross-modal in-
sights.
23C.3 Disjoint Experts & Routers
In this configuration, modality-specific routers assign each modalityâ€™s embedding to separate pools
of experts, with each pool uniquely tailored to process a specific modality type. This method maxi-
mizes the ability of the model to capture and exploit modality-specific features and relationships, as
each pool of experts is optimized for a particular type of data. However, this setup might limit the
modelâ€™s ability to learn from the interactions between modalities, as each is processed in isolation.
Advantages
1. Allows for highly specialized processing of each modality, potentially improving perfor-
mance on modality-specific tasks.
2. Modality-specific experts can develop deeper insights into the characteristics and patterns
within their designated data type.
Challenges
1. Inter-modal relationships might be underutilized due to the segregated processing of each
modality.
2. Requires additional coordination or subsequent integration stages to combine insights from
different modality-specific experts.
Each router type offers unique benefits and faces specific challenges in capturing the subtle rela-
tionships between modalities. The choice among them depends on the specific requirements of the
application, including the importance of preserving modality-specific information versus capturing
inter-modal interactions, and the computational efficiency of managing multiple experts and routers.
For example, we found that modality-specific routers are more effective in ameliorating the effect of
missing modality in our experiments.
D Data Preprocessing
D.1 MIMIC-IV
In the preprocessing stage, we focused on 30 pertinent lab and chart events from each patientâ€™s
ICU record for vital sign measurements. For chest X-rays, we utilized a pre-trained DenseNet-121
model [14], which was fine-tuned on the CheXpert dataset [34], to extract 1024-dimensional image
embeddings. For radiological notes, we obtained 768-dimensional embeddings using the BioClini-
calBERT model [2]. ECG signals were processed using a convolutional autoencoder, adapted from
[4], to generate a 256-dimensional embedding for each ECG.
Time series We selected 30 time series events for inclusion, following [83]. Nine of these were
vital signs: heart rate, mean/systolic/diastolic blood pressure, respiratory rate, oxygen saturation,
and Glascow Coma Scale (GCS) verbal, eye, and motor response. We also included 21 lab values:
potassium, sodium, chloride, creatinine, urea nitrogram, bicarbonate, anion gap, hemoglobin, hema-
tocrit, magnesium, platlet count, phosphate, white blood cell count, total calcium, MCH, red blood
cell count, MCHC, MCV , RDW, platlet count, neutrophil count, and vancomycin. We standard scale
each time series value to have mean 0and standard deviation 1, based on the values in the training
set.
Chest X-rays To incorporate a medical imaging modality into our analyses, we use the MIMIC-
CXR-JPG [37] module available from Physionet [23], which includes 377,110 JPG format images
derived from the DICOM-based MIMIC-CXR database [39]. Following [83], for each image, we
resize each JPG image to 224 Ã—224 pixels and then extract embeddings from the last layer of the
Densenet121 model. We identify X-rays taken while the patient was in the ICU by first matching
subject IDs in MIMIC-CXR-JPG with the core MIMIC-IV database, then limiting these matched
X-rays to those with a chart time occuring between an ICU admission and discharge.
24Radiological notes To incorporate text data, we use the MIMIC-IV-Note module [38], which
contains 2,321,355 deidentified radiology reports for 237,427 patients that can be matched with
patients in the main MIMIC-IV via a similar approach to chest X-rays. We note that we were unable
to obtain intermediate clinical notes (i.e., notes made by clinicians throughout a patient stay), as
those have not yet been publicly released. We extract note embeddings using Bio-Clinical BERT
[2].
Electrocardiograms (ECGs) To include ECGs as an additional modality in our models, we uti-
lize the MIMIC-IV-ECG [24] module, which includes approximately 800,000 ECGs (10 seconds,
sampled at 500 Hz) collected from nearly 160,000 unique patients. To transform the ECGs so that
they are suitable for input to our model, we adopt a convolutional autoencoder approach, adapted
from [4], that compresses each ECG into a 256-dimensional vector. Specifically, each diagnostic
ECG contains a 5000Ã—12dimensional vector (5000 time points Ã—12 ECG leads). To prepare
the ECG for input to the autoencoder, we only include the first 4096 time points. We then train
the autoencoder to compress the ECG into a 256-dimensional latent vector, and then reconstruct
the original ECG using upsampling layers, using mean squared error as our loss function. The ar-
chitecture is shown in Figure 6. We train the autoencoder with 90% of the ECGs available in the
MIMIC-IV-ECG projection and use the rest for validation. We selected a batch size of 2048, and
reduced the learning rate by a factor of 0.5if the validation loss had plateaued for 3epochs. Training
stopped if the validation loss had not decreased for 6epochs. For our encoder, we use filter numbers
of[16,16,32,32,64,64], kernel widths of [5,5,5,3,3,3]and a dropout rate of 0.1. For the decoder,
we use the same filter numbers and kernel widths in reverse, and maintain a dropout rate of 0.1.
Figure 6: CNN Autoencoder Architecture The encoder consists of 6 convolutional blocks (tempo-
ral convolution, batch normalization, dropout, and max pooling layers), followed by a dense layer
that reduces the dimensionality of the representation of 256. The decoder reconstructs the input
ECG (dimensionality 4096Ã—12) from this latent vector via a dense layer, followed by 6 upsampling
convolutional blocks (upsampling, convolutional, batch normalization, and dropout layers).
D.2 PAM Dataset
We follow the preprocessing procedure from [100] as published from their official GitHub reposi-
tory1.
1https://github.com/mims-harvard/Raindrop/tree/main/PAMdata
25E Modeling Irregularity
E.1 Unified Temporal Discretization Embeddings
Unlike the embeddings in chest X-rays, clinical notes, and ECGs, vitals/lab/time-series values
present temporal irregularity across dimensions . That is, for the former three modalities, each di-
mension of the corresponding is observed at each irregular time point Ï„. By contrast, the sampling
for vitals/labs is irregular in both within andacross dimensions. For example, we might observe
heart rate values sampled at times Ï„HR={0,0.2,0.8,1.2,2.8}and glucose values sampled at time
Ï„Glu={0.1,0.7,3.4}. Given this unique challenge present in vitals/labs, we adapt the Unified Tem-
poral Discretization Embedding (UTDE) approach described in [101], which combines the mTAND
approach described in Section 2.2 with a simpler imputation-based discretization scheme. Specif-
ically, given a set of tobservations xâˆˆRtobserved at irregular times Ï„âˆˆRt, we a simple
imputation scheme to discretize xinto target bins Î³(e.g., Î³={0,1,2, ..., Î³}. Specifically, given
bin value Î³iâˆˆÎ³, we apply the following rules:
â€¢ If there exists a previously observed value of x(i.e.,âˆƒÏ„âˆˆÏ„st. Ï„â‰¤Î³i), we set the imputed
value of xat time Î³,Ë†xÎ³i, to the closest previously observed value.
â€¢ If no previously observed value exists, we set the value of Ë†xÎ³ito the global mean of x.
We do this for each possible vitals/lab, to generate a matrix of imputation embeddings IâˆˆRÎ³Ã—dvitals,
weredvitalsis the number of vitals/labs. We then input this embedding into a 1D causal convolutional
layer with stride 1 to obtain our final imputation embeddings with hidden dimension dh,EImpâˆˆ
RÎ³Ã—dh.
E.2 Unifying imputation and mTAND embeddings
We combined simple imputation and mTAND embeddings via a gating function g. Following [101],
we let EmTAND âˆˆRÎ³Ã—dhdenote the mTAND embeddings for vitals/labs derived from the process
described in Section 2.2 and let EImpâˆˆRÎ³Ã—dhdenote the simple imputations from the process
described above. We use each of these discretization embeddings to derive a final set of embeddings
for vitals/labs Evitals via a one-layer MLP gating function f. Specifically, we let g=f(EImpâŠ•
EmTAND ), where âŠ•denotes the concatenation operator. We then calculate Evitalsas
Evitals=gâŠ™EImp+ (1âˆ’g)âŠ™EmTAND âˆˆRÎ³Ã—dh,
where âŠ™denotes point-wise multiplication.
F Baseline Comparison
F.1 MISTS
This approach, from [101], casts time series and clinical notes as multivariate, irregularly-sampled
time series (MISTS) and uses layers of self- and cross-attention to fuse modalities. The method
uses a Time2Vec [43] encoding scheme to represent the irregularity of observation times. We use
the same hyperparameters as in the original paper (e.g., 3self- and cross-attention blocks, 128-
dimensional time embedding, etc.).
F.2 MulT
This model from [90] relies on multiple stacks of pairwise and bidirectional cross-modal attention
blocks (without a self-attention mechanism) to attend to low-level features. The results of cross-
modal attention are then sent to modality-specific transformers, concatenated, and used to make
predictions.
F.3 MAG
This method introduces the Multimodal Adaptation Gate (MAG) as an extension to BERT and XL-
Net, allowing these pre-trained models to incorporate visual and acoustic data during fine-tuning.
26By generating a modality-conditioned shift in their internal representations, MAG enables enhanced
sentiment analysis performance on multimodal datasets, achieving human-level accuracy in the field
[74].
F.4 TFN
The proposed Tensor Fusion Network approach (TFN) integrates three core components: Modality
Embedding Subnetworks for generating rich embeddings from unimodal inputs, a Tensor Fusion
Layer for capturing all levels of modality interactions through a 3-fold Cartesian product, and a
Sentiment Inference Subnetwork tailored to perform sentiment analysis based on the fusion layerâ€™s
output [96].
F.5 HAIM
The multimodal fusion approach detailed by [83] extracts a single set of features for each ICU stay,
and uses this to predict the outcome of interest (in-hospital mortality, etc.). For vitals/lab values, the
authors extract a set of 11 generic time series features: signal length, maximum, minimum, mean,
median, SD, variance, number of peaks, and average time-series slope and piece-wise change over
time of these metrics. This is done independently for each of the 30 events, leading to 30Ã—11 = 330
vital/lab features per ICU stay. To provide a fair comparison with our method, we only include the
most recent five notes and 128vitals measurements in calculating embeddings. We only include
entries for which all modalities are observed. For note/X-ray/ECG embeddings, we compute the
mean embedding across all observations occurring during the specified time frame (i.e., the first
48 hours of 48-IHM and LOS, the entire stay for PHE). As with our method, we standardize scale
values based on the training set. [83] uses an XGBoost [12] classifier to predict the outcomes of
interest. We follow the hyperparameter optimization approach described in the paper. Specifically,
we conduct a grid search across the following sets of hyperparameters: max depth ={5,6,7,8},
number of estimators ={200,300}, learning rate ={0.3,0.1,0.05}. Hyperparameters are selected
based on the maximum AU-ROC from five-fold cross-validation.
F.6 Implementation
We integrate F.1 through F.4 into our workflow using the implementation provided by [101]. For
F.5, we adapt the time series (e.g., series variance, mean, etc.) feature extraction and model fitting
code from the repository released by the corresponding paper. The original paper doesnâ€™t use ECG
waveforms, so we adopt a similar approach to ECG embeddings as with image and note embeddings,
and take the mean value of the latent vector across all included observations.
G Computational Resources and Hyper-Parameters
G.1 Computational Resources
We train models using a Lambda Workstation with four A550 GPUs with 24 GB of memory. We are
able to train models using a single GPU. An analysis of computation time and memory requirements
is shown in Figure 12.
G.2 Hyper-Parameters
The set of parameters we used for experiments can be found in Table 8.
H Additional Results
H.1 FlexiModal Experiments
We present additional results comparing FuseMoE to baselines using the MIMIC-III dataset, which
includes only vital signs and clinical notes (Table 9), and the MIMIC-IV dataset, featuring vital
signs and CXR (Table 10). All experiments utilize the â€œjoint experts and routerâ€ configuration. In
these settings, FuseMoE demonstrates noticeable advantages.
27Table 8: Hyperparameters used for MoE framework and general architecture.
Hyper-Parameter Type Parameter Name Value
MoENumber of experts 16
FFN hidden size 512
Top k 4
Disjoint top k 2
Hidden activation function GeLU
Number of MoE layers 3
Other ParametersRandom seed [32, 42, 52, 62, 72]
Training epochs 8
Training batch size 2
Eval batch size 8
CNN kernel size 1
Gradient accumulation steps 16
BERT update epochs 2
BERT learning rate 2.00E-05
Time series encoder learning rate 4.00E-04
Number of notes to include for a patient 5
Get notes from beginning or last Last
Attention embedding dimension 128
Number of attention heads 8
Maximum time for irregular time series 48
Time embedding dimension 64
Table 9: Comparison of FuseMoE -based methods (gray) and baselines, utilizing vital signs and
clinical notes of MIMIC-III. The best results are highlighted in bold font, and the second-best results
are underlined. All results are averaged across 5 random runs. Since HAIM is not designed for the
MIMIC-III dataset, we use the concatenation method from e.g. [45] as a replacement.
Task\Method MISTS MulT MAG TF Concat Softmax Gaussian Laplace
48-IHMAUROC 89.14Â±0.57 87.26 Â±0.35 86.53 Â±1.21 87.22 Â±0.89 86.72 Â±0.76 90.25 Â±0.74 90.77 Â±0.18 91.19Â±0.52
F1 56.45Â±1.30 54.13Â±1.20 53.20 Â±2.13 51.44 Â±0.66 52.77 Â±0.70 56.41 Â±0.98 56.21 Â±0.17 57.36Â±0.73
25-PHEAUROC 86.06Â±0.06 85.96 Â±0.07 85.94 Â±0.07 84.74 Â±0.16 85.94 Â±0.21 86.41 Â±0.75 85.96Â±0.64 86.72Â±0.27
F1 54.84Â±0.31 54.20 Â±0.33 53.73 Â±0.37 49.84 Â±0.83 53.30 Â±0.35 55.02 Â±0.23 55.29 Â±0.45 55.38Â±0.69
H.2 Ablation Study on FuseMoE Building Blocks
In Figure 7, we evaluate the impact of various irregularity encoders on the performance of the
FuseMoE framework. Our baseline approaches include the following methods:
1. employing only the imputation (discretization) module from the time-series irregularity
encoder, as detailed in Appendix E
2. utilizing solely the mTAND module [82] within the time-series irregularity encoder
3. implementing the SeFT method [31] as an irregularity encoder
4. adopting the RAINDROP method [100] as an irregularity encoder
Table 10: Comparison of FuseMoE -based methods (gray) and baselines, utilizing vital signs and
CXR of MIMIC-IV . The best results are highlighted in bold font, and the second-best results are
underlined. All results are averaged across 5 random experiments.
Task\Method MISTS MulT MAG TF HAIM Softmax Gaussian Laplace
48-IHMAUROC 81.36Â±0.24 77.70 Â±0.44 81.19 Â±1.25 76.92 Â±0.65 80.87 Â±0.00 82.08 Â±0.26 81.26Â±0.18 82.97Â±0.49
F1 43.35Â±0.39 28.40 Â±0.75 39.59 Â±0.43 46.59 Â±0.33 40.88Â±0.00 38.14 Â±0.31 44.59 Â±0.24 47.48Â±0.23
LOSAUROC 82.07Â±0.82 81.94 Â±0.26 81.86 Â±0.76 81.47 Â±0.89 81.69 Â±0.00 82.96 Â±0.47 82.74Â±0.85 83.22Â±0.68
F1 74.07Â±0.18 74.46 Â±0.17 73.89 Â±0.93 73.39 Â±0.14 72.93 Â±0.00 75.67Â±0.59 75.16Â±0.42 75.43 Â±0.19
25-PHEAUROC 71.50Â±0.22 71.20Â±0.76 70.89 Â±0.47 70.55 Â±0.29 63.43 Â±0.00 71.38 Â±0.31 70.87 Â±0.67 71.44 Â±0.24
F1 33.52Â±0.39 32.80 Â±0.18 33.14 Â±0.61 33.56 Â±0.74 42.45Â±0.00 33.49Â±0.15 31.94 Â±0.09 34.13 Â±0.56
28(a)
 (b)
Figure 7: The irregularity encoder employed by FuseMoE achieves the best average results compared
with 4 baseline irregularity encoders. The performance of these approaches is averaged over 5
random runs. We utilized the vital signs and clinical notes components of the MIMIC-IV dataset.
In Figure 8, we evaluate the impact of various time-series encoders on the performance of the Fuse-
MoE framework. The original FuseMoE framework feeds time-series embeddings obtained from
the irregularity encoder into the Transformer [92] and extracts the last hidden states of the Trans-
former output to pass through fully connected layers to make predictions. Our baseline approaches
include CNN [47] and LSTM [30] to encode time-series embeddings from the irregularity encoder.
(a)
 (b)
Figure 8: Transformer is more effective in acting as the time-series encoder than CNN and LSTM.
The performance outcomes of these approaches are derived from averages over 5 random runs. We
utilized the vital signs and clinical notes components of the MIMIC-IV dataset.
In Figure 9, we assess the effect of different CXR encoders on the FuseMoE framework. Currently,
the FuseMoE framework incorporates DenseNet-121 as the feature extractor for CXR images before
their integration into the mTAND module. This setup is compared with the application of the state-
of-the-art vision transformer (ViT-B) [17] as an alternative CXR encoder.
In Figure 10, we evaluate the influence of text encoders on the FuseMoE framework. Currently,
FuseMoE incorporates Clinical-Longformer [49] as the text encoder before integrating it into the
mTAND module. This setup is compared with other state-of-the-art text encoders: GRU-D [10],
FT-LSTM [99], and HierTrans [68].
Finally, in Figure 11, we investigate the effect of the mTAND module on each modality, while we
removed mTAND for a particular modality, the rest of FuseMoEâ€™s components remained constant.
H.3 Ablation Study on MoE Architecture
We then conducted ablation studies to explore the efficiency and effectiveness of MoE architecture
on model performance. We mainly use MIMIC-IV as our test bed. Figure 12(a) examines the com-
putational efficiency and resource utilization, positioning FuseMoE approximately in the middle of
29(a)
 (b)
Figure 9: ViT could further improve predictive results in some tasks by providing better CXR em-
beddings. The performance of these approaches is averaged over 5 random runs. We utilized all 4
modalities (vital signs, clinical notes, CXR, and ECG) of MIMIC-IV . Note that while we vary the
CXR encoders, the rest of our frameworkâ€™s components remain constant.
(a)
 (b)
Figure 10: Clinical-Longformer as the text encoder achieves the best performance compared with
baselines. The performance of these approaches is derived from averages over 5 random runs. We
utilized all 4 modalities (vital signs, clinical notes, CXR, and ECG) of the MIMIC-IV dataset, while
we varied the text encoders, the rest of our frameworkâ€™s components remained constant.
(a)
 (b)
Figure 11: Encoding irregularity using the mTAND module improves the overall performance. The
positive effect of the irregularity encoder is most evident in vital signs and clinical notes. The
performance outcomes of these approaches are averaged over 5 random runs. We utilized all 4
modalities (vital signs, clinical notes, CXR, and ECG) components of the MIMIC-IV dataset.
30(a)
 (b)
 (c)
Figure 12: Results of ablation studies on MoE architecture: (a) The computational efficiency and
resource utilization of each method when applied to vital signs and clinical notes from the MIMIC-
IV dataset; (b) The relationship between the number of experts and task performance across different
modalities, including vital signs, clinical notes, and CXR; (c) The impact of each modality on the
top-kexperts within a disjoint router structure.
Figure 13: Modality weight composition of CMU-MOSI and CMU-MOSEI datasets.
the comparison. Despite the increase in model parameters due to the incorporation of the MoE layer,
its sparse nature does not significantly escalate the computational load. Figure 12(b) illustrates the
correlation between the number of experts and task performance across different modalities. Gen-
erally, performance improves with the addition of more experts, plateauing once the count exceeds
16. To achieve a compromise between performance and computational expense, we opted to utilize
the top 4 experts out of 16 in our experiments. Figure 12(c) and Figure 13 study the influence of
each modality on the top- kchosen experts. For every expert selected, we calculate the number of
samples that include a specific modality, weighted by corresponding weight factors from the gating
functions. The outcomes are subsequently normalized across modalities. The analysis of Figure
12(c) reveals that predictions across all tasks heavily depend on vital signs and clinical notes. This
reliance is attributed to the abundant samples in these two modalities. Despite the notably smaller
quantity of CXR, they play more significant roles in the 25-PHE and 48-IHM tasks, which aligns
with our findings in Table 5. The results in Figure 13 demonstrate that the modality weight distribu-
tion in the MOSI and MOSEI datasets is more â€œspread outâ€, with the audio component carrying a
greater weight in the MOSEI dataset.
I Details on Numerical Experiments
We conduct multiple numerical experiments to illustrate the theoretical convergence rates of the
MLEbGnto the true mixing measure Gâˆ—under both exact-specified and over-specified settings.
I.1 Experimental Setup
Synthetic Data. Assume that the true mixing measure Gâˆ—=Pkâˆ—
i=1exp(Î²âˆ—
i)Î´(Wâˆ—
i,aâˆ—
i,bâˆ—
i,Î½âˆ—
i)is of
order kâˆ—= 2. The true parameters for the router, (Wâˆ—
i, Î²âˆ—
i)âˆˆRdÃ—R, are drawn independently
from an isotropic Gaussian distribution with zero mean and variance Ïƒ2
r= 0.01/dfor1â‰¤iâ‰¤6,
and otherwise are set to zero. Similarly, the true parameters of the experts, (aâˆ—
i, bâˆ—
i)âˆˆRdÃ—R, are
drawn independently of an isotropic Gaussian distribution with zero mean and variance Ïƒ2
e= 1/d
31for all experts. For the variances Î½âˆ—
i, we also sample from the Gaussian distribution N(0, Ïƒ2
e), and
then take the absolute value of the sample. These parameters remain unchanged for all experiments.
Then, we generate i.i.d samples {(Xi, Yi)}n
i=1by first sampling Xiâ€™s from the uniform distribution
Uniform[0 ,1]and then sampling Yiâ€™s from the true conditional density gGâˆ—(Y|X)of the Laplace
gating Gaussian mixture of experts (MoE) given in equation 4.
Maximum Likelihood Estimation (MLE). A popular approach to determining the MLE bGnfor
each set of samples is to use the EM algorithm [15]. However, since there are not any closed-
form expressions for updating the gating parameters Î²0i, Î²1iin the maximization steps, we have
to leverage an EM-based numerical scheme, which was previously used in [63]. In particular, we
utilize a simple coordinate gradient descent algorithm in the maximization steps. Additionally, we
select the convergence criterion of Ïµ= 10âˆ’6and run a maximum of 2000 EM iterations.
Initialization. For each kâˆˆ {kâˆ—, kâˆ—+ 1}, we randomly distribute elements of the set {1,2, ..., k}
intokâˆ—different V oronoi cells A1,A2, . . . ,Akâˆ—, each contains at least one element. Moreover, we
repeat this process for each replication. Subsequently, for each jâˆˆ[kâˆ—], we initialize parameters
Wiby sampling from a Gaussian distribution centered around its true counterpart Wâˆ—
jwith a small
variance, where iâˆˆ Aj. Other parameters Î²i, ai, bi, Î½iare also initialized in a similar fashion.
I.2 Exact-specified Setting
Under the exact-specified settings, we conduct 5 sample generations for each configuration, across
a spectrum of 10 different sample sizes nranging from 103to105. It can be seen from Figure 3
(left) that the MLE bGnempirically converges to the true mixing measure Gâˆ—under the V oronoi
metric D1at the rate of order O(nâˆ’0.49), which matches the theoretical parametric convergence rate
established in Theorem J.2.
I.3 Over-specified Setting
Under the over-specified settings, we continue to generate 5 samples of size nfor each setting, given
10 different choices of sample size nâˆˆ[103,105]. From Figure 3 (right), we observe that the MLE
bGnempirically converges to Gâˆ—under the V oronoi metric D2at the rate of order O(nâˆ’0.47), which
aligns with the theoretical parametric convergence rate established in Theorem 3.2.
J Exact-Specified Setting
In this appendix, we study the theoretical behaviors of the MLE under the exact-specified setting,
i.e.,k=kâˆ—, of the Laplace gating Gaussian MoE. We demonstrate that under the exact-specified
setting, the rate of estimated conditional density function pbGntopGâˆ—is parametric Onâˆ’1/2)(up to
some logarithmic factor).
Theorem J.1. The density estimation pbGn(Y|X)converges to the true density pGâˆ—(Y|X)under the
Total Variation distance Vat the following rate:
EX[V(pbGn(Â·|X), pGâˆ—(Â·|X))] =O(p
log(n)/n).
The proof of Theorem J.1 can be done similarly to that of Theorem 3.1 in Appendix K.2. The result
of Theorem J.1 indicates that as long as we can establish the lower bound of the total variation
distance between pbGnandpGâˆ—based on certain loss function between the MLE bGnand the true
mixing measure Gâˆ—, we directly achieve the rate of the MLE under that loss function.
Voronoi Loss We now define that loss function between the MLE and the true mixing measure for
the exact-specified setting:
D1(G, Gâˆ—) :=kâˆ—X
j=1X
iâˆˆAjexp(Î²i)âˆ’exp(Î²âˆ—
j)+X
jâˆˆ[kâˆ—]:|Aj|=1X
iâˆˆAjexp(Î²i)Î¦ij(1,1,1,1).(9)
Above, for any (Ï1, Ï2, Ï3, Ï4)âˆˆR4, we define Î¦ij(Ï1, Ï2, Ï3, Ï4) =âˆ¥Wiâˆ’Wâˆ—
jâˆ¥Ï1+âˆ¥aiâˆ’aâˆ—
jâˆ¥Ï2+
|biâˆ’bâˆ—
j|Ï3+|Î½iâˆ’Î½âˆ—
j|Ï4for any iâˆˆ Ajandjâˆˆ[kâˆ—]. We demonstrate in the following theorem that
32the rate of MLE to the true mixing measure under the V oronoi loss function D1isO(nâˆ’1/2)(up to
some logarithmic factor).
Theorem J.2 (Exact-specified setting) .When k=kâˆ—is known, the following Total Variation bound
holds guarantetrue for any Gâˆˆ Gk(Î˜):
EX[V(pG(Â·|X), pGâˆ—(Â·|X))]â‰³D1(G, Gâˆ—).
Therefore, we have D1(bGn, Gâˆ—) =O(p
log(n)/n).
Proof of Theorem J.2 is in Appendix K.1. The convergence rate of MLE under the V oronoi loss
function D1implies that the rates of estimating the true parameters Wâˆ—
i, aâˆ—
i, bâˆ—
i, Î½âˆ—
iare also O(nâˆ’1/2)
(up to logarithmic factors). These rates are comparable to those under the exact-specified setting of
softmax gating Gaussian MoE (cf. Theorem 1 in [64]).
K Proof of Theoretical Results
In this appendix, we provide proofs for all theoretical results in the paper. Throughout this appendix,
for any vector vâˆˆRdandÎ±:= (Î±1, Î±2, . . . , Î± d)âˆˆNd, we denote vÎ±=vÎ±1
1vÎ±2
2. . . vÎ±d
d,|v|:=
v1+v2+. . .+vdandÎ±! :=Î±1!Î±2!. . . Î± d!.
K.1 Proof of Theorem J.2
First of all, we need to establish the following bound:
EX[V(pG(Â·|X), pGâˆ—(Â·|X))]â‰³D1(G, Gâˆ—).
For that sake, it is sufficient to demonstrate two following inequalities:
â€¢Inequality A. infGâˆˆGkâˆ—(Î˜):D1(G,Gâˆ—)â‰¤Îµâ€²EX[V(pG(Â·|X), pGâˆ—(Â·|X))]
D1(G, Gâˆ—)>0;
â€¢Inequality B. infGâˆˆGkâˆ—(Î˜):D1(G,Gâˆ—)>Îµâ€²EX[V(pG(Â·|X), pGâˆ—(Â·|X))]
D1(G, Gâˆ—)>0,
for some constant Îµâ€²>0.
Proof of inequality A : The inequality A is equivalent to
lim
Îµâ†’0inf
GâˆˆGkâˆ—(Î˜):D1(G,Gâˆ—)â‰¤ÎµEX[V(pG(Â·|X), pGâˆ—(Â·|X))]
D1(G, Gâˆ—)>0.
Assume that the above inequality is not true, then, there exists a sequence of mixing
measure Gn:=Pkâˆ—
i=1exp(Î²n
i)Î´(Wn
i,an
i,bn
i,Î½n
i)âˆˆ G kâˆ—(Î˜) such that both D1(Gn, Gâˆ—)and
EX[V(pGn(Â·|X), pGâˆ—(Â·|X))]/D1(Gn, Gâˆ—)go to zero as nâ†’ âˆ . Now, we define
An
j=Aj(Gn) :={iâˆˆ[kâˆ—] :âˆ¥Î¸n
iâˆ’Î¸âˆ—
jâˆ¥ â‰¤ âˆ¥ Î¸n
iâˆ’Î¸âˆ—
Ï„âˆ¥,âˆ€Ï„Ì¸=j},
for any jâˆˆ[kâˆ—]as V oronoi cells with respect to the mixing measure Gn, where we denote Î¸n
i:=
(Wn
i, an
i, bn
i, Î½n
i)andÎ¸âˆ—
j:= (Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j). In this proof, since our arguments are assymptotic,
we can assume without loss of generality (WLOG) that these V oronoi cells does not depend on n,
that is, Aj=An
j. Next, it follows from the hypothesis D1n:=D1(Gn, Gâˆ—)â†’0asnâ†’ âˆ that
each V oronoi cell contains only one element. Therefore, we may assume WLOG that Aj={j}for
anyjâˆˆ[kâˆ—], which implies that (Wn
j, an
j, bn
j, Î½n
j)â†’(Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j)andexp(Î²n
j)â†’exp(Î²âˆ—
j)as
nâ†’ âˆ . Then, the loss function between GnandGâˆ—is given by
D1(Gn, Gâˆ—) =kâˆ—X
i=1h
exp(Î²n
i)
âˆ¥âˆ†Wn
iâˆ¥+âˆ¥âˆ†an
iâˆ¥+âˆ¥âˆ†bn
iâˆ¥+âˆ¥âˆ†Î½n
iâˆ¥
+exp(Î²n
i)âˆ’exp(Î²âˆ—
i)i
,
where we denote âˆ†Î²n
1i:=Î²n
1iâˆ’Î²âˆ—
1i,âˆ†an
i:=an
iâˆ’aâˆ—
i,âˆ†bn
i:=bn
iâˆ’bâˆ—
iandâˆ†Î½n
i:=Î½n
iâˆ’Î½âˆ—
i.
Now, we break the rest of our arguments into three steps:
33Stage 1 - Density decomposition :
In this step, we aim to decompose the term Qn:=hPkâˆ—
i=1exp(âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥+Î²âˆ—
i)i
Â·[pGn(Y|X)âˆ’
pGâˆ—(Y|X)], which can be represented as follows:
Qn=kâˆ—X
i=1exp(Î²n
i)h
F(Y|X;Wn
i, an
i, bn
i, Î½n
i)âˆ’F(Y|X;Wâˆ—
i, aâˆ—
i, bâˆ—
i, Î½âˆ—
i)i
âˆ’kâˆ—X
i=1exp(Î²n
i)h
H(Y|X;Wn
i)âˆ’H(Y|X;Wâˆ—
i)i
+kâˆ—X
i=1h
exp(Î²n
i)âˆ’exp(Î²âˆ—
i)ih
F(Y|X;Wâˆ—
i, aâˆ—
i, bâˆ—
i, Î½âˆ—
i)âˆ’H(Y|X, Wâˆ—
i)i
: =Anâˆ’Bn+En, (10)
where we denote F(Y|X;W, a, b, Î½ ) := exp( âˆ’âˆ¥Wâˆ’Xâˆ¥)f(Y|aâŠ¤X+b, Î½)andH(Y|X;W) =
exp(âˆ’âˆ¥Wâˆ’Xâˆ¥)pGn(Y|X). By applying the first-order Taylor expansion, we can rewrite Anas
An=kâˆ—X
i=1X
|Î±|=1exp(Î²n
i)
Î±!Â·(âˆ†Wn
i)Î±1(âˆ†an
i)Î±2(âˆ†bn
i)Î±3(âˆ†Î½n
i)Î±4
Ã—âˆ‚|Î±1|+|Î±2|+Î±3+Î±4F
âˆ‚WÎ±1âˆ‚aÎ±2âˆ‚bÎ±3âˆ‚Î½Î±4(Y|X;Wâˆ—
i, aâˆ—
i, bâˆ—
i, Î½âˆ—
i) +R1(X, Y)
=kâˆ—X
i=1X
|Î±|=1exp(Î²n
i)
Î±!Â·(âˆ†Wn
i)Î±1(âˆ†an
i)Î±2(âˆ†bn
i)Î±3(âˆ†Î½n
i)Î±4
Ã—âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
i)Â·âˆ‚|Î±2|+Î±3+Î±4f
âˆ‚aÎ±2âˆ‚bÎ±3âˆ‚Î½Î±4(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i) +R1(X, Y),
where R1(X, Y)is a Taylor remainder that satisfies R1(X, Y)/D1(X, Y)â†’0asnâ†’ âˆ and
g(X, W ) := exp( âˆ¥Wâˆ’Xâˆ¥). Recall that fis the univariate Gaussian density, then by denoting
h1(X;a, b) :=aâŠ¤X+b, we can verify that
âˆ‚Î±4f
âˆ‚Î½Î±4(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i) =1
2Î±4Â·âˆ‚2Î±4f
âˆ‚h2Î±4
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i).
Consequently, we get
An=kâˆ—X
i=1X
|Î±|=1exp(Î²n
i)
2Î±4Î±!Â·(âˆ†Wn
i)Î±1(âˆ†an
i)Î±2(âˆ†bn
i)Î±3(âˆ†Î½n
i)Î±4
Ã—XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
i)Â·âˆ‚|Î±2|+Î±3+2Î±4f
âˆ‚h|Î±2|+Î±3+2Î±4
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i) +R1(X, Y)
=kâˆ—X
i=11X
|Î±1|=01âˆ’|Î±1|X
|Î±2|=02(1âˆ’|Î±1|âˆ’|Î±2|)X
Î·=0X
Î±3+2Î±4=Î·,
0â‰¤Î±3+Î±4â‰¤1âˆ’|Î±1|âˆ’|Î±2|exp(Î²n
i)
2Î±4Î±!Â·(âˆ†Wn
i)Î±1(âˆ†an
i)Î±2(âˆ†bn
i)Î±3(âˆ†Î½n
i)Î±4
Ã—XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
i)Â·âˆ‚|Î±2|+Î·f
âˆ‚h|Î±2|+Î·
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i) +R1(X, Y),
(11)
where we denote Î·=Î±3+ 2Î±4âˆˆN.
34Subsequently, we also apply the first-order Taylor expansion to the term Bndefined in equation 10
and get that
Bn=kâˆ—X
i=1X
|Î³|=1exp(Î²n
i)
Î³!(âˆ†Wn
i)Î³Â·âˆ‚|Î³|H
âˆ‚WÎ³(Y|X;Wâˆ—
i) +R2(X, Y)
=kâˆ—X
i=1X
|Î³|=1exp(Î²n
i)
Î³!(âˆ†Wn
i)Î³Â·âˆ‚|Î³|g
âˆ‚WÎ³(X;Wâˆ—
i)pGn(Y|X) +R2(X, Y), (12)
where R2(X, Y)is a Taylor remainder such that R2(X, Y)/D1(Gn, Gâˆ—)â†’0asnâ†’ âˆ .
From the above results, the term Qncan be rewritten as
Qn=kâˆ—X
i=11X
|Î±1|=01âˆ’|Î±1|X
|Î±2|=02(1âˆ’|Î±1|âˆ’|Î±2|)X
Î·=0Sn
i,Î±1,Î±2,Î·Â·XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
i)Â·âˆ‚|Î±2|+Î·f
âˆ‚h|Î±2|+Î·
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i)
+kâˆ—X
i=11X
|Î³|=0Tn
i,Î³Â·âˆ‚|Î³|g
âˆ‚WÎ³(X;Wâˆ—
i)pGn(Y|X) +R1(X, Y) +R2(X, Y), (13)
in which we respectively define for each iâˆˆ[kâˆ—]that
Sn
i,Î±1,Î±2,Î·:=X
Î±3+2Î±4=Î·,
0â‰¤Î±3+Î±4â‰¤1âˆ’|Î±1|âˆ’|Î±2|exp(Î²n
i)
2Î±4Î±!Â·(âˆ†Wn
i)Î±1(âˆ†an
i)Î±2(âˆ†bn
i)Î±3(âˆ†Î½n
i)Î±4,
Tn
i,Î³:=exp(Î²n
i)
Î³!(âˆ†Wn
i)Î³,
for any (Î±1, Î±2, Î·)Ì¸= (0d,0d,0)andÎ³Ì¸=0d. Otherwise, Sn
i,0d,0d,0=Tn
i,0d:= exp( Î²n
i)âˆ’exp(Î²âˆ—
i).
Stage 2 - Non-vanishing coefficients :
Moving to the second step, we will show that not all the ratios Sn
i,Î±1,Î±2,Î·/D1(Gn, Gâˆ—)and
Tn
i,Î³/D1(Gn, Gâˆ—)tend to zero as nâ†’ âˆ . Assume by contrary that all of them approach zero
when nâ†’ âˆ , then for (Î±1, Î±2, Î·) = (0d,0d,0), it follows that
1
D1(Gn, Gâˆ—)Â·kâˆ—X
i=1exp(Î²n
i)âˆ’exp(Î²âˆ—
i)=kâˆ—X
i=1|Sn
i,Î±1,Î±2,Î·|
D1(Gn, Gâˆ—)â†’0. (14)
Additionally, for tuples (Î±1, Î±2, Î·)where Î±1âˆˆ { e1, e2, . . . , e d}with ej:=
(0, . . . , 0,1|{z}
jâˆ’th,0, . . . , 0),Î±2=0dandÎ·= 0, we get
1
D1(Gn, Gâˆ—)Â·kâˆ—X
i=1exp(Î²n
i)âˆ¥âˆ†Wn
iâˆ¥1=kâˆ—X
i=1|Sn
i,Î±1,Î±2,Î·|
D1(Gn, Gâˆ—)â†’0.
For(Î±1, Î±2, Î·)where Î±1=0d,Î±2âˆˆ {e1, e2, . . . , e d}andÎ·= 0, we have
1
D1(Gn, Gâˆ—)Â·kâˆ—X
i=1exp(Î²n
i)âˆ¥âˆ†an
iâˆ¥1=kâˆ—X
i=1|Sn
i,Î±1,Î±2,Î·|
D1(Gn, Gâˆ—)â†’0.
For(Î±1, Î±2, Î·)where Î±1=Î±2=0dandÎ·= 1, we have
1
D1(Gn, Gâˆ—)Â·kâˆ—X
i=1exp(Î²n
i)âˆ¥âˆ†bn
iâˆ¥1=kâˆ—X
i=1|Sn
i,Î±1,Î±2,Î·|
D1(Gn, Gâˆ—)â†’0.
For(Î±1, Î±2, Î·)where Î±1=Î±2=0dandÎ·= 2, we have
1
D1(Gn, Gâˆ—)Â·kâˆ—X
i=1exp(Î²n
i)âˆ¥âˆ†Î½n
iâˆ¥1=kâˆ—X
i=1|Sn
i,Î±1,Î±2,Î·|
D1(Gn, Gâˆ—)â†’0.
35As a result, we achieve that
1
D1(Gn, Gâˆ—)Â·kâˆ—X
i=1exp(Î²n
i)h
âˆ¥âˆ†Wn
iâˆ¥1+âˆ¥âˆ†an
iâˆ¥1+|âˆ†bn
i|+|âˆ†Î½n
i|i
â†’0.
Due to the topological equivalence between norm-1 and norm-2, the above limit implies that
1
D1(Gn, Gâˆ—)Â·kâˆ—X
i=1exp(Î²n
i)h
âˆ¥âˆ†Wn
iâˆ¥+âˆ¥âˆ†an
iâˆ¥+|âˆ†bn
i|+|âˆ†Î½n
i|i
â†’0. (15)
Combine equation 14 with equation 15, we deduce that D1(Gn, Gâˆ—)/D1(Gn, Gâˆ—)â†’0, which
is a contradiction. Consequently, at least one among the ratios Sn
i,Î±1,Î±2,Î·/D1(Gn, Gâˆ—)and
Tn
i,Î³/D1(Gn, Gâˆ—)does not vanish as ntends to infinity.
Stage 3 - Fatouâ€™s contradiction :
In this step, we use the Fatouâ€™s lemma to point out a contradiction to the results achieved in Step 2.
In particular, we denote by mnthe maximum of the absolute values of Sn
i,Î±1,Î±2,Î·/D1(Gn, Gâˆ—)and
Tn
i,Î³/D1(Gn, Gâˆ—). Since at least one of the previous ratios does not converge to zero, we deduce
that1/mnÌ¸â†’ âˆ .
Recall from the hypothesis that EX[V(pGn(Â·|X), pGâˆ—(Â·|X))]/D1(Gn, Gâˆ—)â†’0asnâ†’ âˆ . Ac-
cording to the Fatouâ€™s lemma, we have
0 = lim
nâ†’âˆEX[V(pGn(Â·|X), pGâˆ—(Â·|X))]
D1(Gn, Gâˆ—)â‰¥1
2Â·Z
lim inf
nâ†’âˆ|pGn(Y|X)âˆ’pGâˆ—(Y|X)|
D1(Gn, Gâˆ—)d(X, Y)â‰¥0.
This result indicates that |pGn(Y|X)âˆ’pGâˆ—(Y|X)|/D1(Gn, Gâˆ—)tends to zero as ngoes to infinity
for almost surely (X, Y). As a result, it follows that
lim
nâ†’âˆQn
mnD1(Gn, Gâˆ—)= lim
nâ†’âˆ|pGn(Y|X)âˆ’pGâˆ—(Y|X)|
mnD1(Gn, Gâˆ—)= 0.
Next, let us denote Sn
i,Î±1,Î±2,Î·/[mnD1(Gn, Gâˆ—)]â†’Î¾i,Î±1,Î±2,Î·andTn
i,Î³/[mnD1(Gn, Gâˆ—)]â†’Îºi,Î³
with a note that at least one among them is non-zero. From the formulation of Qnin equation 13,
we deduce that
kâˆ—X
i=11X
|Î±1|=01âˆ’|Î±1|X
|Î±2|=02(1âˆ’|Î±1|âˆ’|Î±2|)X
Î·=0Î¾i,Î±1,Î±2,Î·Â·XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
i)Â·âˆ‚|Î±2|+Î·f
âˆ‚h|Î±2|+Î·
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i)
+kâˆ—X
i=11X
|Î³|=0Îºi,Î³Â·âˆ‚|Î³|g
âˆ‚WÎ³(X;Wâˆ—
i)pGn(Y|X) = 0 ,
(16)
for almost surely (X, Y). The above equation is equivalent to
kâˆ—X
i=11X
|Î±1|=0ï£®
ï£°1âˆ’|Î±1|X
|Î±2|=02(1âˆ’|Î±1|âˆ’|Î±2|)X
Î·=0Î¾i,Î±1,Î±2,Î·Â·XÎ±2âˆ‚Î±2+Î·f
âˆ‚hÎ±2+Î·
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i) +Îºi,Î±1pGâˆ—(Y|X)ï£¹
ï£»
Ã—âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
i) = 0 ,
for almost surely (X, Y). It is worth noting that parameters Wâˆ—
1, . . . , Wâˆ—
Kare pair-wise distinct,
thus, the setnâˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
i) :iâˆˆ[kâˆ—],0â‰¤ |Î±1| â‰¤1o
is a linearly independent, which implies
that
1âˆ’|Î±1|X
|Î±2|=02(1âˆ’|Î±1|âˆ’|Î±2|)X
Î·=0Î¾i,Î±1,Î±2,Î·Â·XÎ±2âˆ‚Î±2+Î·f
âˆ‚hÎ±2+Î·
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i) +Îºi,Î±1pGâˆ—(Y|X) = 0 ,
36for any iâˆˆ[kâˆ—],0â‰¤ | Î±1| â‰¤ 1for almost surely (X, Y). Moreover,
since (aâˆ—
1, bâˆ—
1, Î½âˆ—
1), . . . , (aâˆ—
kâˆ—, bâˆ—
kâˆ—, Î½âˆ—
kâˆ—)have pair-wise distinct values, those of ((aâˆ—
1)âŠ¤X+
bâˆ—
1, Î½âˆ—
1), . . . , ((aâˆ—
kâˆ—)âŠ¤X+bâˆ—
kâˆ—, Î½âˆ—
kâˆ—)are also pair-wise different. Therefore, the set
n
XÎ±2âˆ‚Î±2+Î·f
âˆ‚hÎ±2+Î·
1(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i), pGâˆ—(Y|X) :
0â‰¤ |Î±2| â‰¤1âˆ’ |Î±1|,0â‰¤Î·â‰¤2(1âˆ’ |Î±1| âˆ’ |Î±2|)o
is also linearly independent. Consequently, we obtain that Î¾i,Î±1,Î±2,Î·=Îºi,Î³= 0 for any iâˆˆ[kâˆ—],
0â‰¤ |Î±1|+Î±2â‰¤1,0â‰¤Î·â‰¤2(1âˆ’ |Î±1| âˆ’ |Î±2|)and0â‰¤ |Î³| â‰¤1, which contradicts the fact that at
least one among those terms is different from zero.
Hence, we can find some constant Îµâ€²>0such that
inf
GâˆˆGkâˆ—(Î˜):D1(G,Gâˆ—)â‰¤Îµâ€²EX[V(pG(Â·|X), pGâˆ—(Â·|X))]
D1(G, Gâˆ—)>0.
Proof of inequality B : Assume by contrary that the inequality B does not hold, then there exists a
sequence of mixing measures Gâ€²
nâˆˆ Gkâˆ—(Î˜)such that D1(Gâ€²
n, Gâˆ—)> Îµâ€²and
lim
nâ†’âˆEX[V(pGâ€²n(Â·|X), pGâˆ—(Â·|X))]
D1(Gâ€²n, Gâˆ—)= 0.
This result leads to EX[V(pGâ€²n(Â·|X), pGâˆ—(Â·|X))]â†’0asnâ†’ âˆ . Recall that â„¦is a compact set,
therefore, we can replace the sequence Gâ€²
nby one of its subsequences that converges to a mixing
measure Gâ€²âˆˆ Gkâˆ—(Î˜). Since D1(Gâ€²
n, Gâˆ—)> Îµâ€², this result induces that D1(Gâ€², Gâˆ—)> Îµâ€².
Subsequently, by means of the Fatouâ€™s lemma, we achieve that
0 = lim
nâ†’âˆEX[2V(pGâ€²
n(Â·|X), pGâˆ—(Â·|X))]â‰¥Z
lim inf
nâ†’âˆpGâ€²
n(Y|X)âˆ’pGâˆ—(Y|X)d(X, Y).
It follows that pGâ€²(Y|X) =pGâˆ—(Y|X)for almost surely (X, Y). According to Lemma L.1, the
noisy top-K sparse softmax gating Gaussian mixture of experts is identifiable, thus, we obtain that
Gâ€²â‰¡Gâˆ—. As a consequence, we obtain that D1(Gâ€², Gâˆ—) = 0 , which contradicts to the fact that
D1(Gâ€², Gâˆ—)> Îµâ€²>0.
Hence, the proof is completed.
K.2 Proof of Theorem 3.1
In this appendix, we employ results for M-estimators in [91] to establish the density estimation rate
under the Laplace gating Gaussian mixture of experts (MoE).
Firstly, we introduce some necessary notations and fundamental results. In particular, let Pk(Î˜) :=
{pG(Y|X) :Gâˆˆ Gk(Î˜)}be the set of all conditional density functions w.r.t mixing measures in
Gk(Î˜). Next, we denote by N(Îµ,Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)the covering number of metric space (Pk(Î˜),âˆ¥ Â·
âˆ¥âˆ). Meanwhile, HB(Îµ,Pk(Î˜), h)stands for the bracketing entropy of Pk(Î˜)under the Hellinger
distance hwhere h(p, q) :=
1
2R
(âˆšpâˆ’âˆšq)2dÂµ1/2
for any probability densities p, qdominated
by the Lebesgue measure Âµ. Then, we provide in the following lemma the upper bounds of those
terms.
Lemma K.1. IfÎ˜is a bounded set, then the following inequalities hold for any 0< Î· < 1/2:
(i)logN(Î·,Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)â‰²log(1/Î·);
(ii)HB(Î·,Pk(Î˜), h)â‰²log(1/Î·).
Proof of Lemma K.1 is in Appendix K.2.2. Subsequently, we denote
ePk(Î˜) := {p(G+Gâˆ—)/2(Y|X) :Gâˆˆ Gk(Î˜)};
eP1/2
k(Î˜) := {p1/2
(G+Gâˆ—)/2(Y|X) :Gâˆˆ Gk(Î˜)}.
37In addition, for each Î´ > 0, we define a Hellinger ball centered around the conditional density
function pGâˆ—(Y|X)and intersected with the set eP1/2
k(Î˜)as
eP1/2
k(Î˜, Î´) :={p1/2âˆˆeP1/2
k(Î˜) : h(p, pGâˆ—)â‰¤Î´}.
To capture the size of the above Hellinger ball, [91] suggest using the following quantity:
JB(Î´,eP1/2
k(Î˜, Î´)) :=ZÎ´
Î´2/213H1/2
B(t,eP1/2
k(Î˜, t),âˆ¥ Â· âˆ¥ 2)dtâˆ¨Î´, (17)
where tâˆ¨Î´:= max {t, Î´}. Given those notations, let us recall a standard result for density estimation
in [91].
Lemma K.2 (Theorem 7.4, [91]) .Take Î¨(Î´)â‰¥ J B(Î´,eP1/2
k(Î˜, Î´))such that Î¨(Î´)/Î´2is a non-
increasing function of Î´. Then, for some sequence (Î´n)and universal constant cwhich satisfyâˆšnÎ´2
nâ‰¥cÎ¨(Î´), we obtain that
P
EXh
h(pbGn(Â·|X), pGâˆ—(Â·|X))i
> Î´
â‰¤cexp(âˆ’nÎ´2/c2),
for any Î´â‰¥Î´n
Proof of Lemma K.2 can be found in [91]. Now, we are ready to provide the proof for convergence
rate of density estimation in Theorem J.1 in Appendix K.2.1.
K.2.1 Main Proof
It is worth noting that for any t >0, we have
HB(t,eP1/2
k(Î˜, t),âˆ¥ Â· âˆ¥ 2)â‰¤HB(t,Pk(â„¦, t), h).
Then, the integral in equation 17 is upper bounded as follows:
JB(Î´,eP1/2
k(Î˜, Î´))â‰¤ZÎ´
Î´2/213H1/2
B(t,Pk(â„¦, t), h)dtâˆ¨Î´â‰²ZÎ´
Î´2/213log(1/t)dtâˆ¨Î´, (18)
where the second inequality follows from part (ii) of Lemma K.1.
As a result, by choosing Î¨(Î´) =Î´Â·p
log(1/Î´), we can verify that Î¨(Î´)/Î´2is a non-increasing
function of Î´. Furthermore, the inequality in equation 18 indicates that Î¨(Î´)â‰¥ JB(Î´,eP1/2
k(Î˜, Î´)).
Next, let us consider a sequence (Î´n)defined as Î´n:=p
log(n)/n. This sequence can be validated
to satisfy the conditionâˆšnÎ´2
nâ‰¥cÎ¨(Î´)for some universal constant c. Therefore, by Lemma K.2,
we reach the conclusion of Theorem J.1:
P
EX[h(pbGn(Â·|X), pGâˆ—(Â·|X))]> Cp
log(n)/n
â‰²nâˆ’c,
for some universal constant Cdepending only on Î˜.
K.2.2 Proof of Lemma K.1
Part (i). In this part, we will derive the following upper bound for the covering number of metric
space (Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)for any 0< Î· < 1/2given the bounded set â„¦:
logN(Î·,Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)â‰²log(1/Î·).
To start with, we denote â„¦ :={(a, b, Î½ )âˆˆRdÃ—RÃ—R+: (Î², W, a, b, Î½ )âˆˆâ„¦}. AsÎ˜is a bounded
set, the set â„¦is also bounded. Therefore, we can find an Î·-cover of â„¦, denoted by â„¦Î·. Additionally,
we also define âˆ† :={(Î², W)âˆˆRÃ—Rd: (Î², W, a, b, Î½ )âˆˆâ„¦}, and âˆ†Î·be an Î·-cover of âˆ†. Then,
it can be validated that
|â„¦Î·| â‰¤ O (Î·âˆ’(d+2)k),|âˆ†Î·| â‰¤ O (Î·âˆ’(d+1)k).
Next, for each mixing measure G=Pk
i=1exp(Î²i)Î´(Wi,ai,bi,Î½i)âˆˆ G k(Î˜), we take into ac-
count two other mixing measures. The first measure is Gâ€²=Pk
i=1exp(Î²i)Î´(Wi,ai,bi,Î½i), where
38(ai,bi,Î½i)âˆˆâ„¦Î·is the closest points to (ai, bi, Î½i)in this set for all iâˆˆ[k]. The second one is
G:=Pk
i=1exp(Î²i)Î´(Wi,ai,bi,Î½i)in which (Î²i,Wi)âˆˆâˆ†Î·for any iâˆˆ[k]. Next, let us define
T:={pGâˆˆ Pk(Î˜) : ( Î²i,Wi)âˆˆâˆ†Î·,(ai,bi,Î½i)âˆˆÎ˜Î·,âˆ€iâˆˆ[k]},
then it is obvious that pGâˆˆ T. Now, we will show that Tis anÎ·-cover of metric space (Pk(Î˜),âˆ¥ Â·
âˆ¥âˆ)with a note that it is not necessarily the smallest cover. Indeed, according to the triangle
inequality, we have
âˆ¥pGâˆ’pGâˆ¥âˆâ‰¤ âˆ¥pGâˆ’pGâ€²âˆ¥âˆ+âˆ¥pGâ€²âˆ’pGâˆ¥âˆ. (19)
Since the softmax function is no greater than one, the first term in the right hand side can be upper
bounded as follows:
âˆ¥pGâˆ’pGâ€²âˆ¥âˆâ‰¤kX
i=1sup
XâˆˆXsoftmax( âˆ’âˆ¥Wiâˆ’Xâˆ¥+Î²i)Â·f(Y|aâŠ¤
iX+bi, Î½i)âˆ’f(Y|aâŠ¤
iX+bi,Î½i)
â‰¤kX
i=1sup
XâˆˆXf(Y|aâŠ¤
iX+bi, Î½i)âˆ’f(Y|aâŠ¤
iX+bi,Î½i)
â‰²kX
i=1sup
XâˆˆX
âˆ¥aiâˆ’aiâˆ¥+âˆ¥biâˆ’biâˆ¥+âˆ¥Î½iâˆ’Î½iâˆ¥
=kX
i=1
âˆ¥aiâˆ’aiâˆ¥+âˆ¥biâˆ’biâˆ¥+âˆ¥Î½iâˆ’Î½iâˆ¥
â‰²Î·. (20)
Subsequently, we bound the second term âˆ¥pGâ€²âˆ’pGâˆ¥âˆas follows:
âˆ¥pGâ€²âˆ’pGâˆ¥âˆâ‰¤kX
i=1sup
XâˆˆXnsoftmax( âˆ’âˆ¥Wiâˆ’Xâˆ¥+Î²i)âˆ’softmax( âˆ’âˆ¥Wiâˆ’Xâˆ¥+Î²i)
Ã—f(Y|aâŠ¤
Ï„iX+bÏ„i,Î½Ï„i)o
â‰¤kX
i=1sup
XâˆˆXsoftmax( âˆ’âˆ¥Wiâˆ’Xâˆ¥+Î²i)âˆ’softmax( âˆ’âˆ¥Wiâˆ’Xâˆ¥+Î²i)
â‰¤kX
i=1sup
XâˆˆXâˆ’ âˆ¥Wiâˆ’Xâˆ¥+Î²i+âˆ¥Wiâˆ’Xâˆ¥ âˆ’Î²i
â‰¤kX
i=1sup
XâˆˆX[âˆ¥Wiâˆ’Wiâˆ¥+|Î²iâˆ’Î²i|]
â‰²Î·, (21)
It follows from the results in equation 19, equation 20 and equation 21 that âˆ¥pGâˆ’pGâˆ¥âˆâ‰²Î·. This
result indicates that Tis an Î·-cover of the metric space (Pk(Î˜),âˆ¥ Â· âˆ¥âˆ). As a consequence, we
obtain that
N(Î·,Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)â‰²|âˆ†Î·| Ã— |â„¦Î·| â‰¤ O (1/Î·(2d+3)k),
which leads to the conclusion of this part: logN(Î·,Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)â‰²log(1/Î·).
Part (ii). In this part, we provide an upper bound for the bracketing entropy of Pk(Î˜) under the
Hellinger distance h:
HB(Î·,Pk(Î˜), h)â‰²log(1/Î·).
Since Î˜andXare bounded sets, there exist positive constants Î³, â„“, u such that âˆ’Î³â‰¤aâŠ¤X+bâ‰¤Î³
andâ„“â‰¤Î½â‰¤u. Let us define
B(Y|X) :=(
1âˆš
2Ï€â„“exp
âˆ’Y2
8u
,for|Y| â‰¥2Î³
1âˆš
2Ï€â„“, for|Y|<2Î³
39Then, it can be validated that f(Y|aâŠ¤X+b, Î½)â‰¤B(Y|X)for any (X, Y)âˆˆ X Ã— Y .
Next, let Î¶â‰¤Î·which will be chosen later and {p1, . . . , p N}be an Î¶-cover of metric space
(Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)with the covering number N:=N(Î¶,Pk(Î˜),âˆ¥ Â· âˆ¥âˆ). Additionally, we also
consider brackets of the form [Î¨L
i(Y|X),Î¨U
i(Y|X)]where
Î¨L
i(Y|X) := max {pi(Y|X)âˆ’Î¶,0}
Î¨U
i(Y|X) := max {pi(Y|X) +Î¶, B(Y|X)}.
Then, we can check that Pk(Î˜)âŠ†SN
i=1[Î¨L
i(Y|X),Î¨U
i(Y|X)]andÎ¨U
i(Y|X)âˆ’Î¨L
i(Y|X)â‰¤
min{2Î¶, B(Y|X)}.
LetS:= max {2Î³,âˆš
8u}log(1/Î¶), we have for any iâˆˆ[N]that
âˆ¥Î¨U
iâˆ’Î¨L
iâˆ¥1=Z
|Y|<2Î³[Î¨U
i(Y|X)âˆ’Î¨L
i(Y|X)] d(X, Y) +Z
|Y|â‰¥2Î³[Î¨U
i(Y|X)âˆ’Î¨L
i(Y|X)] d(X, Y)
â‰¤SÎ¶+ exp
âˆ’S2
2u
â‰¤Sâ€²Î¶,
where Sâ€²is some positive constant. This inequality indicates that
HB(Sâ€²Î¶,Pk(Î˜),âˆ¥ Â· âˆ¥ 1)â‰¤logN(Î¶,Pk(Î˜),âˆ¥ Â· âˆ¥âˆ)â‰¤log(1/Î¶).
By setting Î¶=Î·/Sâ€², we obtain that HB(Î·,Pk(Î˜),âˆ¥Â·âˆ¥1)â‰²log(1/Î·). Finally, due to the inequality
h2â‰¤ âˆ¥ Â· âˆ¥ 1, we reach the conclusion of this part:
HB(Î·,Pk(Î˜), h)â‰²log(1/Î·).
Hence, the proof is completed.
K.3 Proof of Theorem 3.2
In order to establish the following Total Variation lower bound under the over-specified settings, i.e.
when k > k âˆ—is unknown:
EX[V(pG(Â·|X), pGâˆ—(Â·|X))]â‰³D2(G, Gâˆ—),
we need to prove two following inequalities:
â€¢Inequality A. infGâˆˆGk(Î˜):D2(G,Gâˆ—)â‰¤Îµâ€²EX[V(pG(Â·|X), pGâˆ—(Â·|X))]
D2(G, Gâˆ—)>0;
â€¢Inequality B. infGâˆˆGk(Î˜):D2(G,Gâˆ—)>Îµâ€²EX[V(pG(Â·|X), pGâˆ—(Â·|X))]
D2(G, Gâˆ—)>0,
for some constant Îµâ€²>0. As the inequality B can be achieved in the same fashion as in Ap-
pendix K.1, we concentrate on showing the inequality A in this proof. For that purpose, it suffices
to prove that
lim
Îµâ†’0inf
GâˆˆGk(Î˜):D2(G,Gâˆ—)â‰¤ÎµEX[V(pG(Â·|X), pGâˆ—(Â·|X))]
D2(G, Gâˆ—)>0. (22)
Assume that the above claim does not hold true, then there exists a sequence of mixing mea-
sures Gn:=Pkn
i=1exp(Î²n
i)Î´(Wn
i,an
i,bn
i,Î½n
i)âˆˆ G k(Î˜) such that both the terms D2(Gn, Gâˆ—)and
EX[V(pGn(Â·|X), pGâˆ—(Â·|X))]/D2(Gn, Gâˆ—)go to zero as nâ†’ âˆ . Let us recall the formulation of
the loss D2(Gn, Gâˆ—):
D2(Gn, Gâˆ—) =X
jâˆˆ[kâˆ—],
|Aj|>1X
iâˆˆAjexp(Î²n
i)h
âˆ¥âˆ†Wn
ijâˆ¥2+âˆ¥âˆ†an
ijâˆ¥2+|âˆ†bn
ij|Â¯rj+|âˆ†Î½n
ij|Â¯rj
2i
+X
jâˆˆ[kâˆ—],
|Aj|=1X
iâˆˆAjexp(Î²n
i)h
âˆ¥âˆ†Wn
ijâˆ¥+âˆ¥âˆ†an
ijâˆ¥+|âˆ†bn
ij|+|âˆ†Î½n
ij|i
+kâˆ—X
j=1X
iâˆˆAjexp(Î²n
i)âˆ’exp(Î²âˆ—
j).
(23)
40SinceD2(Gn, Gâˆ—)â†’0, we deduce thatP
iâˆˆAjexp(Î²n
i)â†’exp(Î²âˆ—
j)and(Wn
i, an
i, bn
i, Î½n
i)â†’
(Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j)for all iâˆˆ Ajandjâˆˆ[kâˆ—].
Now, we reuse the three-step framework in Appendix K.1.
Stage 1 - Density decomposition :
Firstly, by abuse of notations, let us consider the quantity
Qn:=hkâˆ—X
j=1exp(âˆ’âˆ¥Wâˆ—
jâˆ’Xâˆ¥+Î²âˆ—
j)i
Â·[pGn(Y|X)âˆ’pGâˆ—(Y|X)].
Similar to Step 1 in Appendix K.1, we can express this term as
Qn=kâˆ—X
j=1X
iâˆˆAjexp(Î²n
i)h
F(Y|X;Wn
i, an
i, bn
i, Î½n
i)âˆ’F(Y|X;Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j)i
âˆ’kâˆ—X
j=1X
iâˆˆAjexp(Î²n
i)h
H(Y|X;Wn
i)âˆ’H(Y|X;Wâˆ—
j)i
+kâˆ—X
j=1hX
iâˆˆAjexp(Î²n
i)âˆ’exp(Î²âˆ—
j)ih
F(Y|X;Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j)âˆ’H(Y|X, Wâˆ—
j)i
: =Anâˆ’Bn+En,
Next, we proceed to decompose Anbased on the cardinality of the V oronoi cells as follows:
An=X
j:|Aj|=1X
iâˆˆAjexp(Î²n
i)h
F(Y|X;Wn
i, an
i, bn
i, Î½n
i)âˆ’F(Y|X;Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j)i
+X
j:|Aj|>1X
iâˆˆAjexp(Î²n
i)h
F(Y|X;Wn
i, an
i, bn
i, Î½n
i)âˆ’F(Y|X;Wâˆ—
j, aâˆ—
j, bâˆ—
j, Î½âˆ—
j)i
.
By applying the Taylor expansions of order 1 and Â¯rjto the first and second terms of An, respectively,
and following the derivation in equation 11, we get that
An=X
j:|Aj|=1X
iâˆˆAj1X
|Î±1|=01âˆ’|Î±1|X
|Î±2|=02(1âˆ’|Î±1|âˆ’|Î±2|)X
Î·=0X
Î±3+2Î±4=Î·,
0â‰¤Î±3+Î±4â‰¤1âˆ’|Î±1|âˆ’|Î±2|exp(Î²n
i)
2Î±4Î±!Â·(âˆ†Wn
ij)Î±1(âˆ†an
ij)Î±2
Ã—(âˆ†bn
ij)Î±3(âˆ†Î½n
ij)Î±4Â·XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
j)Â·âˆ‚|Î±2|+Î·f
âˆ‚h|Î±2|+Î·
1(Y|(aâˆ—
j)âŠ¤X+bâˆ—
j, Î½âˆ—
j) +R3(X, Y)
+X
j:|Aj|>1X
iâˆˆAjÂ¯rjX
|Î±1|=0Â¯rjâˆ’|Î±1|X
|Î±2|=02(Â¯rjâˆ’|Î±1|âˆ’|Î±2|)X
Î·=0X
Î±3+2Î±4=Î·,
0â‰¤Î±3+Î±4â‰¤Â¯rjâˆ’|Î±1|âˆ’|Î±2|exp(Î²n
i)
2Î±4Î±!Â·(âˆ†Wn
ij)Î±1(âˆ†an
ij)Î±2
Ã—(âˆ†bn
ij)Î±3(âˆ†Î½n
ij)Î±4Â·XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
j)Â·âˆ‚|Î±2|+Î·f
âˆ‚h|Î±2|+Î·
1(Y|(aâˆ—
j)âŠ¤X+bâˆ—
j, Î½âˆ—
j) +R4(X, Y)
where Ri(X, Y)is a Taylor remainder such that Ri(X, Y)/D2(Gn, Gâˆ—)â†’0asnâ†’ âˆ foriâˆˆ
{3,4}. Next, we apply the Taylor expansions of order 1 and 2 to the first and second terms of Bn,
respectively, and following the derivation in equation 12, we get that
Bn=X
j:|Aj|=1X
iâˆˆAjX
|Î³|=1exp(Î²n
i)
Î³!(âˆ†Wn
ij)Î³Â·âˆ‚|Î³|g
âˆ‚WÎ³(X;Wâˆ—
j)pGn(Y|X) +R5(X, Y)
X
j:|Aj|>1X
iâˆˆAj2X
|Î³|=1exp(Î²n
i)
Î³!(âˆ†Wn
ij)Î³Â·âˆ‚|Î³|g
âˆ‚WÎ³(X;Wâˆ—
j)pGn(Y|X) +R6(X, Y),
41where R5(X, Y)andR6(X, Y)are Taylor remainders such that their ratios over D2(Gn, Gâˆ—)ap-
proach zero as nâ†’ âˆ . Subsequently, let us define
Sn
j,Î±1,Î±2,Î·:=X
iâˆˆAjX
Î±3+2Î±4=Î·,
0â‰¤Î±3+Î±4â‰¤Â¯rjâˆ’|Î±1|âˆ’|Î±2|exp(Î²n
i)
2Î±4Î±!Â·(âˆ†Wn
ij)Î±1(âˆ†an
ij)Î±2(âˆ†bn
ij)Î±3(âˆ†Î½n
ij)Î±4,
Tn
j,Î³:=X
iâˆˆAjexp(Î²n
i)
Î³!(âˆ†Wn
ij)Î³,
for any (Î±1, Î±2, Î·)Ì¸= (0d,0d,0)andÎ³Ì¸=0d. Otherwise, Sn
j,0d,0d,0=Tn
j,0d:=P
iâˆˆAjexp(Î²n
i)âˆ’
exp(Î²âˆ—
j). As a consequence, it follows that
Qn=kâˆ—X
j=1Â¯rjX
|Î±1|=0Â¯rjâˆ’|Î±1|X
|Î±2|=02(Â¯rjâˆ’|Î±1|âˆ’|Î±2|)X
Î·=0Sn
j,Î±1,Î±2,Î·Â·XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
j)Â·âˆ‚|Î±2|+Î·f
âˆ‚h|Î±2|+Î·
1(Y|(aâˆ—
j)âŠ¤X+bâˆ—
j, Î½âˆ—
j)
+kâˆ—X
j=11+1{|Aj|>1}X
|Î³|=0Tn
j,Î³Â·âˆ‚|Î³|g
âˆ‚WÎ³(X;Wâˆ—
j)pGn(Y|X) +R3(X, Y) +R4(X, Y) +R5(X, Y) +R6(X, Y).
(24)
Stage 2 - Non-vanishing coefficients :
In this step, we demonstrate that not all the ratios Sn
j,Î±1,Î±2,Î·/D2(Gn, Gâˆ—)andTn
j,Î³/D2(Gn, Gâˆ—)
converge to zero as nâ†’ âˆ . Assume by contrary that all these terms go to zero. Then, by employing
arguments for deriving equation 14 and equation 15, we get that
1
D2(Gn, Gâˆ—)Â·hkâˆ—X
j=1X
iâˆˆAjexp(Î²n
i)âˆ’exp(Î²âˆ—
j)
+X
j:|Aj|=1X
iâˆˆAjexp(Î²n
i)
âˆ¥âˆ†Wn
ijâˆ¥+âˆ¥âˆ†an
ijâˆ¥+|âˆ†bn
ij|+|âˆ†Î½n
ij|i
â†’0.
Taking the summation ofP
j:|Aj|>1|Sn
j,Î±1,Î±2,Î·|
D2(Gn,Gâˆ—)for all (Î±1, Î±2, Î·)where Î±1âˆˆ {2e1,2e2, . . . , 2ed},
Î±2=0dandÎ·= 0, we have
1
D2(Gn, Gâˆ—)Â·X
j:|Aj|>1X
iâˆˆAjexp(Î²n
i)âˆ¥âˆ†Wn
ijâˆ¥2â†’0.
Taking the summation ofP
j:|Aj|>1|Sn
j,Î±1,Î±2,Î·|
D2(Gn,Gâˆ—)for all (Î±1, Î±2, Î·)where Î±1=0d,Î±2âˆˆ
{2e1,2e2, . . . , 2ed}andÎ·= 0, we have
1
D2(Gn, Gâˆ—)Â·X
j:|Aj|>1X
iâˆˆAjexp(Î²n
i)âˆ¥âˆ†an
ijâˆ¥2â†’0.
Combine the above limit with the formulation of D2(Gn, Gâˆ—)in equation 23, we have that
1
D2(Gn, Gâˆ—)Â·X
j:|Aj|>1X
iâˆˆAjexp(Î²n
i)
|âˆ†bn
ij|Â¯rj+|âˆ†Î½n
ij|Â¯rj
2
Ì¸â†’0.
This result implies that we can find some index jâ€²âˆˆ[kâˆ—] :|Ajâ€²|>1that satisfies
1
D2(Gn, Gâˆ—)Â·X
iâˆˆAjâ€²exp(Î²n
i)
|âˆ†bn
ijâ€²|Â¯rjâ€²+|âˆ†Î½n
ijâ€²|Â¯rjâ€²
2
Ì¸â†’0.
For simplicity, we may assume that jâ€²= 1. Since Sn
1,0d,0d,Î·/D2(Gn, Gâˆ—)vanishes as nâ†’ âˆ for
any1â‰¤Î·â‰¤Â¯rj, we divide this term by the left hand side of the above equation and achieve that
P
iâˆˆA1P
Î±3+2Î±4=Î·,
1â‰¤Î±3+Î±4â‰¤Â¯r1exp(Î²n
i)
2Î±4Î±!(âˆ†bn
i1)Î±3(âˆ†Î½n
i1)Î±4
P
iâˆˆA1exp(Î²n
i)
|âˆ†bn
i1|Â¯r1+|âˆ†Î½n
i1|Â¯r1
2 â†’0, (25)
42for any 1â‰¤Î·â‰¤Â¯r1.
Subsequently, we define Mn:= max {|âˆ†bn
i1|,|âˆ†Î½n
i1|1/2:iâˆˆ A 1}andÏ€n:= max {exp(Î²n
i) :iâˆˆ
A1}. As a result, the sequence exp(Î²n
i)/Ï€nis bounded, which indicates that we can substitute it
with its subsequence that admits a positive limit z2
5i:= lim nâ†’âˆexp(Î²n
i)/Ï€n. Therefore, at least
one among the limits z2
5iequals to one. Furthermore, we also denote
(âˆ†bn
i1)/Mnâ†’z3i,(âˆ†Î½n
i1)/(2Mn)â†’z4i.
From the above definition, it follows that at least one among the limits z3iandz4iequals to either 1
orâˆ’1. By dividing both the numerator and the denominator of the term in equation 25 by Ï€nMÎ·
n,
we arrive at the following system of polynomial equations:
X
iâˆˆA1X
Î±3+2Î±4=Î·,
1â‰¤Î±3+Î±4â‰¤Â¯r1z2
5izÎ±3
3izÎ±4
4i
Î±3!Î±4!= 0,
for all 1â‰¤Î·â‰¤Â¯r1. Nevertheless, from the definition of Â¯r1, we know that the above system
does not admit any non-trivial solutions, which is a contradiction. Consequently, not all the ratios
Sn
j,Î±1,Î±2,Î·/D2(Gn, Gâˆ—)andTn
j,Î³/D2(Gn, Gâˆ—)tend to zero as nâ†’ âˆ .
Stage 3 - Fatouâ€™s contradiction :
Recall that EX[V(pGn(Â·|X), pGâˆ—(Â·|X))]/D2(Gn, Gâˆ—)â†’0asnâ†’ âˆ . Then, by applying the
Fatouâ€™s lemma, we get
0 = lim
nâ†’âˆEX[V(pGn(Â·|X), pGâˆ—(Â·|X))]
D2(Gn, Gâˆ—)â‰¥1
2Â·Z
lim inf
nâ†’âˆ|pGn(Y|X)âˆ’pGâˆ—(Y|X)|
D2(Gn, Gâˆ—)d(X, Y),
which implies that |pGn(Y|X)âˆ’pGâˆ—(Y|X)|/D2(Gn, Gâˆ—)â†’0asnâ†’ âˆ for almost surely (X, Y).
Next, we define mnas the maximum of the absolute values of Sn
j,Î±1,Î±2,Î·/D2(Gn, Gâˆ—). It follows
from Step 2 that 1/mnÌ¸â†’ âˆ . Moreover, by arguing in the same way as in Step 3 in Appendix K.1,
we receive that
Qn/[mnD2(Gn, Gâˆ—)]â†’0 (26)
asnâ†’ âˆ . By abuse of notations, let us denote
Sn
j,Î±1,Î±2,Î·/[mnD2(Gn, Gâˆ—)]â†’Î¾j,Î±1,Î±2,Î·,
Tn
j,Î³/[mnD2(Gn, Gâˆ—)]â†’Îºj,Î³.
Here, at least one among Î¾j,Î±1,Î±2,Î·, Îºj,Î³is non-zero. Then, by putting the results in equation 24 and
equation 26 together, we get
kâˆ—X
j=1Â¯rjX
|Î±1|=0Â¯rjâˆ’|Î±1|X
|Î±2|=02(Â¯rjâˆ’|Î±1|âˆ’|Î±2|)X
Î·=0Î¾j,Î±1,Î±2,Î·Â·XÎ±2Â·âˆ‚|Î±1|g
âˆ‚WÎ±1(X;Wâˆ—
j)Â·âˆ‚|Î±2|+Î·f
âˆ‚h|Î±2|+Î·
1(Y|(aâˆ—
j)âŠ¤X+bâˆ—
j, Î½âˆ—
j)
+KX
j=11+1{|Aj|>1}X
|Î³|=0Îºj,Î³Â·âˆ‚|Î³|g
âˆ‚WÎ³(X;Wâˆ—
j)pGn(Y|X) = 0 .
Arguing in a similar fashion as in Step 3 of Appendix K.1, we obtain that Î¾j,Î±1,Î±2,Î·=Îºj,Î³= 0for
anyjâˆˆ[kâˆ—],0â‰¤ |Î±1|+|Î±2| â‰¤2Â¯rj,0â‰¤Î·â‰¤2(Â¯rjâˆ’ |Î±1| âˆ’ |Î±2|)and0â‰¤ |Î³| â‰¤1 +1{|Aj|>1}.
This contradicts the fact that at least one among them is non-zero. Hence, the proof is completed.
L Identifiability of the Laplace Gating Gaussian MoE
Lemma L.1. For any mixing measures GandGâˆ—inGk(Î˜)that satisfy pG(Y|X) =pGâˆ—(Y|X)for
almost surely (X, Y)âˆˆ X Ã— Y , we have that Gâ‰¡Gâˆ—.
Proof of Lemma L.1. First, we assume that two mixing measures GandGâˆ—take the following
forms: G=Pk
i=1exp(Î²i)Î´(Wi,ai,bi,Î½i)andGâˆ—=Pkâˆ—
i=1exp(Î²âˆ—
i)Î´(Wâˆ—
i,aâˆ—
i,bâˆ—
i,Î½âˆ—
i). Recall that
43pG(Y|X) =pGâˆ—(Y|X)for almost surely (X, Y), then we have
kX
i=1softmax( âˆ’ âˆ¥Wiâˆ’Xâˆ¥+Î²i)Â·f(Y|aâŠ¤
iX+bi, Î½i)
=kâˆ—X
i=1softmax( âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥+Î²âˆ—
i)Â·f(Y|(aâˆ—
i)âŠ¤+bâˆ—
i, Î½âˆ—
i). (27)
Due to the identifiability of the location-scale Gaussian mixtures [85, 86, 87], we get that k=kâˆ—
and
n
softmax( âˆ’âˆ¥Wiâˆ’Xâˆ¥+Î²i) :iâˆˆ[k]o
â‰¡n
softmax( âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥+Î²âˆ—
i) :iâˆˆ[k]o
,
for almost surely X. WLOG, we may assume that
softmax( âˆ’âˆ¥Wiâˆ’Xâˆ¥+Î²i) = softmax( âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥+Î²âˆ—
i), (28)
for almost surely Xfor any iâˆˆ[k]. Since the softmax function is invariant to translations, it follows
from equation 28 that Wi=Wâˆ—
iandÎ²i=Î²âˆ—
i+v0for some v0âˆˆR. Notably, from the assumption
of the model, we have Î²k=Î²âˆ—
k= 0, which implies that v0= 0. As a result, we obtain that Î²i=Î²âˆ—
i
for any iâˆˆ[kâˆ—]. Then, equation 27 can be rewritten as
kâˆ—X
i=1exp(Î²i) exp(âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥)f(Y|aâŠ¤
iX+bi, Î½i)
=kâˆ—X
i=1exp(Î²i) exp(âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥)f(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i), (29)
for almost surely (X, Y). Next, we denote J1, J2, . . . , J mas a partition of the index set [kâˆ—], where
mâ‰¤kâˆ—, such that exp(Î²i) = exp( Î²iâ€²)for any i, iâ€²âˆˆJjandjâˆˆ[m]. On the other hand, when
iandiâ€²do not belong to the same set Jj, we let exp(Î²i)Ì¸= exp( Î²iâ€²). Thus, we can reformulate
equation 29 as
mX
j=1X
iâˆˆJjexp(Î²i) exp(âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥)f(Y|aâŠ¤
iX+bi, Î½i)
=mX
j=1X
iâˆˆJjexp(Î²i) exp(âˆ’âˆ¥Wâˆ—
iâˆ’Xâˆ¥)f(Y|(aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i),
for almost surely (X, Y). This results leads to {((ai)âŠ¤X+bi, Î½i) :iâˆˆJj} â‰¡ { ((aâˆ—
i)âŠ¤X+bâˆ—
i, Î½âˆ—
i) :
iâˆˆJj}, for almost surely Xfor any jâˆˆ[m]. Therefore, we have
{(ai, bi, Î½i) :iâˆˆJj} â‰¡ { (aâˆ—
i, bâˆ—
i, Î½âˆ—
i) :iâˆˆJj},
for any jâˆˆ[m]. As a consequence,
G=mX
j=1X
iâˆˆJjexp(Î²i)Î´(Wi,ai,bi,Î½i)=mX
j=1X
iâˆˆJjexp(Î²âˆ—
i)Î´(Wâˆ—
i,aâˆ—
i,bâˆ—
i,Î½âˆ—
i)=Gâˆ—.
Hence, we reach the conclusion of this lemma.
M Broader Impact
This paper presents research aimed at propelling advancements in the broad domain of machine
learning. The implications of our findings are wide-ranging, with potential applications in sectors
including healthcare, autonomous driving, and recommendation systems. Based on our current
understanding, this research does not warrant an ethics review, and a detailed discussion of the
potential societal impacts is not required at the current stage.
44NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: we have added a separate contribution paragraph in the introduction section
(section 1), along with the highly summarized contributions in the abstract.
Guidelines:
â€¢ The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢ The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢ The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: see section 5 on discussion on general limitations. Section 3 and 4 have also
thoroughly discussed the assumptions and experimental settings.
Guidelines:
â€¢ The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate â€Limitationsâ€ section in their paper.
â€¢ The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The au-
thors should reflect on how these assumptions might be violated in practice and what
the implications would be.
â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢ The authors should reflect on the factors that influence the performance of the ap-
proach. For example, a facial recognition algorithm may perform poorly when image
resolution is low or images are taken in low lighting. Or a speech-to-text system might
not be used reliably to provide closed captions for online lectures because it fails to
handle technical jargon.
â€¢ The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢ If applicable, the authors should discuss possible limitations of their approach to ad-
dress problems of privacy and fairness.
â€¢ While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
45Answer: [Yes]
Justification: see section 3 and Appendix J, K and L for assumptions and proofs.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢ All assumptions should be clearly stated or referenced in the statement of any theo-
rems.
â€¢ The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
â€¢ Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and/or conclu-
sions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: we have submitted the implementation of the proposed methods and all base-
lines in the supplementary material. Appendix B, D, F and G also contain all dataset
information and implementation details.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢ If the contribution is a dataset and/or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model with
the same dataset, or provide access to the model. In general. releasing code and data
is often one good way to accomplish this, but reproducibility can also be provided via
detailed instructions for how to replicate the results, access to a hosted model (e.g., in
the case of a large language model), releasing of a model checkpoint, or other means
that are appropriate to the research performed.
â€¢ While NeurIPS does not require releasing code, the conference does require all sub-
missions to provide some reasonable avenue for reproducibility, which may depend
on the nature of the contribution. For example
(a) If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b) If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c) If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to re-
produce the model (e.g., with an open-source dataset or instructions for how to
construct the dataset).
(d) We recognize that reproducibility may be tricky in some cases, in which case au-
thors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
465.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: we have submitted the implementation of the proposed methods and all base-
lines in the supplementary material. Appendix B, D, F and G also contain comprehensive
information to reproduce the experiments.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢ Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢ While we encourage the release of code and data, we understand that this might not
be possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢ The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢ The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢ The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢ At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢ Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: dataset information and preprocessing procedure can be found in Appendix
B and D.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
â€¢ The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropri-
ate information about the statistical significance of the experiments?
Answer: [Yes]
Justification: all results are averaged over 5 random experiments, as specified by the exper-
iment section 4 and additional results in Appendix H.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ The authors should answer â€Yesâ€ if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
47â€¢ The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢ The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢ It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should prefer-
ably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of
Normality of errors is not verified.
â€¢ For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢ If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: computational resources used to run the experiments can be found in Ap-
pendix G.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢ The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢ The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: we have reviewed the NeurIPS Code of Ethics and make sure the paper con-
forms to this.
Guidelines:
â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢ If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: we have attempted to discuss the impacts of this work in Appendix M.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
48â€¢ If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢ Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact spe-
cific groups), privacy considerations, and security considerations.
â€¢ The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢ The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢ If there are negative societal impacts, the authors could also discuss possible mitiga-
tion strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: our paper has no such risks.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢ Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by re-
quiring that users adhere to usage guidelines or restrictions to access the model or
implementing safety filters.
â€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢ We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: all the assets used in the paper have been properly credited.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢ The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
49â€¢ If assets are released, the license, copyright information, and terms of use in the pack-
age should be provided. For popular datasets, paperswithcode.com/datasets has
curated licenses for some datasets. Their licensing guide can help determine the li-
cense of a dataset.
â€¢ For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢ If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documenta-
tion provided alongside the assets?
Answer: [Yes]
Justification: we have provided documents in the code repository submitted in the supple-
mentary material.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢ Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢ The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢ At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the pa-
per include the full text of instructions given to participants and screenshots, if applicable,
as well as details about compensation (if any)?
Answer: [NA]
Justification: this paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢ The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
â€¢ Including this information in the supplemental material is fine, but if the main contri-
bution of the paper involves human subjects, then as much detail as possible should
be included in the main paper.
â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, cura-
tion, or other labor should be paid at least the minimum wage in the country of the
data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: this paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢ The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
â€¢ Depending on the country in which research is conducted, IRB approval (or equiva-
lent) may be required for any human subjects research. If you obtained IRB approval,
you should clearly state this in the paper.
50â€¢ We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢ For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
51