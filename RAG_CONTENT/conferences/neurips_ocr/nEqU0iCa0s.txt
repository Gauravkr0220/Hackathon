Self-Distilled Depth Refinement
with Noisy Poisson Fusion
Jiaqi Li1,âˆ—Yiran Wang1,âˆ—Jinghong Zheng1
Zihao Huang1Ke Xian2Zhiguo Cao1,â€ Jianming Zhang3
1School of AIA, Huazhong University of Science and Technology
2School of EIC, Huazhong University of Science and Technology
3Adobe Research
âˆ—Equal contributionâ€ Corresponding author
{lijiaqi_mail,wangyiran,deepzheng,zihaohuang,kxian,zgcao}@hust.edu.cn
jianmzha@adobe.com
https://github.com/lijia7/SDDR
Abstract
Depth refinement aims to infer high-resolution depth with fine-grained edges and
details, refining low-resolution results of depth estimation models. The prevailing
methods adopt tile-based manners by merging numerous patches, which lacks
efficiency and produces inconsistency. Besides, prior arts suffer from fuzzy depth
boundaries and limited generalizability. Analyzing the fundamental reasons for
these limitations, we model depth refinement as a noisy Poisson fusion problem
with local inconsistency and edge deformation noises. We propose the Self-distilled
Depth Refinement (SDDR) framework to enforce robustness against the noises,
which mainly consists of depth edge representation and edge-based guidance. With
noisy depth predictions as input, SDDR generates low-noise depth edge representa-
tions as pseudo-labels by coarse-to-fine self-distillation. Edge-based guidance with
edge-guided gradient loss and edge-based fusion loss serves as the optimization
objective equivalent to Poisson fusion. When depth maps are better refined, the
labels also become more noise-free. Our model can acquire strong robustness to
the noises, achieving significant improvements in accuracy, edge quality, efficiency,
and generalizability on five different benchmarks. Moreover, directly training an-
other model with edge labels produced by SDDR brings improvements, suggesting
that our method could help with training robust refinement models in future works.
1 Introduction
Depth refinement infers high-resolution depth with accurate edges and details, refining the low-
resolution counterparts from depth estimation models [ 30,51,1]. With increasing demands for high
resolutions in modern applications, depth refinement becomes a prerequisite for virtual reality [ 24,13],
bokeh rendering [ 27,28], and image generation [ 33,54]. The prevailing methods [ 25,21] adopt
two-stage tile-based frameworks. Based on the one-stage refined depth of the whole image, they
merge high-frequency details by fusing extensive patches with complex patch selection strategies.
However, numerous patches lead to heavy computational costs. Besides, as in Fig. 1 (a), excessive
integration of local information leads to inconsistent depth structures, e.g., the disrupted billboard.
Apart from efficiency and consistency, depth refinement [ 25,14,4,3,37,21] is restricted by noisy
and blurred depth edges. Highly accurate depth annotations with meticulous boundaries are necessary
to enforce fine-grained details. For this reason, prior arts [ 14,37,21] only use synthetic datasets [ 32,
38th Conference on Neural Information Processing Systems (NeurIPS 2024).0.892 0.904 0.898 0.910
One-stageTwo-stage
Ours
(one- stage)
16.7GOurs
(two -stage)
16.7GÃ—30
(a)Visualization ofDepth Refinement Approaches
0.20 0.23 0.22 0.21Edge Quality (ORD)Accuracy (ğœ¹ğœ¹ğŸğŸ)
(b)Performance andEfficiencyPatchFusion
810.8G Ã—177
Boost
286.1G Ã—63
Kim etal.
1138.3GGraph -GDSR
397.4GGBDF
10.4G
Boost
GBDF OursPatchFusion
BoostOursFigure 1: (a) Visual comparisons. We model depth refinement by noisy Poisson fusion with
the local inconsistency noise (representing the inconsistent billboard and wall in red box) and the
edge deformation noise (indicating blurred depth edges in the blue box and second row). Better
viewed when zoomed in. (b) Performance and efficiency. Circle area represents FLOPs. The
two-stage methods [ 25,21] are reported by multiplying FLOPs per patch with patch numbers. SDDR
outperforms prior arts in depth accuracy ( Î´1), edge quality (ORD), and model efficiency (FLOPs).
39,11,45,44] for the highly accurate depth values and edges. However, synthetic data falls
short of the real world in realism and diversity, causing limited generalizability with blurred depth
and degraded performance on in-the-wild scenarios. Some attempts [ 25,3] simply adopt natural-
scene datasets [ 35,47,49,5,20] for the problem. The varying characteristics of real-world depth
annotations, e.g., sparsity [ 2,5,7], inaccuracy [ 35,36,55], or blurred edges [ 48,47,43,10], make
them infeasible for supervising refinement models. Thus, GBDF [ 3] uses depth predictions [ 51] as
pseudo-labels, while Boost [ 25] leverages adversarial training [ 6] as guidance. Those inaccurate
pseudo-labels and guidance still lead to blurred edges as shown in Fig. 1 (a). The key problem is to
alleviate the noise of depth boundaries by constructing accurate edge representations and guidance.
To tackle these challenges, we dig into the underlying reasons for the limitations, instead of the
straightforward merging of local details. We model depth refinement as a noisy Poisson fusion
problem, decoupling depth prediction errors into two degradation components: local inconsistency
noise and edge deformation noise. We use regional linear transformation perturbation as the local
inconsistency noise to measure inconsistent depth structures. The edge deformation noise represents
fuzzy boundaries with Gaussian blur. Experiments in Sec. 3.1 showcase that the noises can effectively
depict general depth errors, serving as our basic principle to improve refinement results.
In pursuit of the robustness against the local inconsistency noise and edge deformation noise, we
propose the Self-distilled Depth Refinement (SDDR) framework, which mainly consists of depth
edge representation and edge-based guidance. A refinement network is considered as the Poisson
fusion operator, recovering high-resolution depth from noisy predictions of depth models [ 51,30,1].
Given the noisy input, SDDR can generate low-noise and accurate depth edge representation as
pseudo-labels through coarse-to-fine self-distillation. The edge-based guidance including edge-guided
gradient loss and edge-based fusion loss is designed as the optimization objective of Poisson fusion.
When depth maps are better refined, the pseudo-labels also become more noise-free. Our approach
establishes accurate depth edge representations and guidance, endowing SDDR with strong robustness
to the two types of noises. Consequently, as shown in Fig. 1 (b), SDDR significantly outperforms
prior arts [ 25,21,3] in depth accuracy and edge quality. Besides, without merging numerous patches
as the two-stage tile-based methods [21, 25], SDDR achieves much higher efficiency.
We conduct extensive experiments on five benchmarks. SDDR achieves state-of-the-art performance
on the commonly-used Middlebury2021 [ 34], Multiscopic [ 52], and Hypersim [ 32]. Meanwhile,
since SDDR can establish self-distillation with accurate depth edge representation and guidance on
natural scenes, the evaluations on in-the-wild DIML [ 15] and DIODE [ 40] datasets showcase our
superior generalizability. Analytical experiments demonstrate that these noticeable improvements
essentially arise from the strong robustness to the noises. Furthermore, the precise depth edge labels
produced by SDDR can be directly used to train another model [ 3] and yield improvements, which
indicates that our method could help with training robust refinement models in future works.
In summary, our main contributions can be summarized as follows:
2â€¢We model the depth refinement task through the noisy Poisson fusion problem with local inconsis-
tency noise and edge deformation noise as two types of depth degradation.
â€¢We present the robust and efficient Self-distilled Depth Refinement (SDDR) framework, which can
generate accurate depth edge representation by the coarse-to-fine self-distillation paradigm.
â€¢We design the edge-guided gradient loss and edge-based fusion loss, as the edge-based guidance to
enforce the model with both consistent depth structures and meticulous depth edges.
2 Related Work
Depth Refinement Models. Depth refinement refines low-resolution depth from depth estimation
models [ 30,51,1], predicting high-resolution depth with fine-grained edges and details. Existing
methods [ 3,14,21,25] can be categorized into one-stage [ 3,14] and two-stage [ 25,21] frameworks.
One-stage methods [ 3,14] conduct global refinement of the whole image, which could produce
blurred depth edges and details. To further enhance local details, based on the globally refined results,
the prevailing refinement approaches [ 25,21] adopt the two-stage tile-based manner by selecting and
merging numerous patches. For example, Boost [ 25] proposes a complex patch-sampling strategy
based on the gradients of input images. PatchFusion [ 21] improves the sampling by shifted and tidily
arranged tile placement. However, the massive patches lead to low efficiency. The excessive local
information produces inconsistent depth structures or even artifacts. In this paper, we propose the
Self-distilled Depth Refinement (SDDR) framework, which can predict both consistent structures
and accurate details with much higher efficiency by tackling the noisy Poisson fusion problem.
Depth Refinement Datasets. Depth datasets with highly accurate annotations and edges are necessary
for refinement models. Prior arts [ 21,14] utilize CG-rendered datasets [ 45,44,39,11,32] for
accurate depth, but the realism and diversity fail to match the real world. For instance, neither the
UnrealStereo4K [ 39] nor the MVS-Synth [ 11] contain people, restricting the generalizability of
refinement models. A simple idea for the problem is to leverage natural-scene data [ 35,47,49,5,20].
However, different annotation methods lead to varying characteristics, e.g., sparsity of LiDAR [ 2,5,7],
inaccurate depth of structured light [ 55,35,36], and blurred edges of stereo matching [ 49,47,43].
To address the challenge, Boost [ 25] adopts adversarial training as guidance only with a small amount
of accurately annotated real-world images. GBDF [ 3] employs depth predictions [ 51] with guided
filtering [ 9] as pseudo-labels. Due to the inaccurate pseudo-labels and guidance, they [ 3,25] produce
blurred edges and details. By contrast, SDDR constructs accurate depth edge representation and
edge-based guidance for self-distillation, leading to fine-grained details and strong generalizability.
3 SDDR: Self-Distilled Depth Refinement
We present a detailed illustration of our Self-distilled Depth Refinement (SDDR) framework. In
Sec. 3.1, we introduce the noisy Poisson fusion to model the depth refinement task and provide an
overview to outline our approach. SDDR mainly consists of depth edge representation and edge-based
guidance, which will be described in Sec. 3.2 and Sec. 3.3 respectively.
3.1 Noisy Poisson Fusion
Problem Statement. Based on depth maps of depth prediction models, i.e., depth predictor Nd, depth
refinement recovers high-resolution depth with accurate edges and details by refinement network Nr.
Some attempts in image super-resolution [ 31,56,26] and multi-modal integration [ 19,17,18,53]
utilize Poisson fusion to merge features and restore details. Motivated by this, we propose to model
depth refinement as a noisy Poisson fusion problem. The ideal depth Dâˆ—with completely accurate
depth values and precise depth edges are unobtainable in real world. A general depth prediction D,
whether produced by NdorNrfor an input image I, can be expressed as a noisy approximation of
Dâˆ—:
Dâ‰ˆDâˆ—+Ïµcons+Ïµedge. (1)
ÏµconsandÏµedgedenote local inconsistency and edge deformation noise to decouple depth prediction
errors. Local inconsistency noise Ïµconsrepresents inconsistent depth structures through regional linear
transformation perturbation. Based on masked Gaussian blur, edge deformation noise Ïµedgeshowcases
degradation and blurring of depth edges. Refer to Appendix A.4 for details of the noises. As in Fig. 2,
3RGB Ideal Depth ğ‘«ğ‘«âˆ—ğ‘«ğ‘«âˆ—+ğğğœğœğœğœğ§ğ§ğ§ğ§+ğğğğğğğğğğ Prediction ğ‘«ğ‘«
 Depth Error ğ‘«ğ‘«âˆ’ğ‘«ğ‘«âˆ—ğğğœğœğœğœğ§ğ§ğ§ğ§+ğğğğğğğğğğ
Figure 2: Depiction of depth errors. We utilize two samples of high-quality depth maps as ideal
depth Dâˆ—. For the predicted depth D, the combination of local inconsistency noise Ïµconsand edge
deformation noise Ïµedgecan approximate real depth error Dâˆ’Dâˆ—(the last two columns). Thus, as in
the third and fourth columns, prediction Dcan be depicted by the summation of Dâˆ—,Ïµcons, andÏµedge.
depth errors can be depicted by combinations of ÏµconsandÏµedge. Thus, considering refinement network
Nras a Poisson fusion operator, depth refinement can be defined as a noisy Poisson fusion problem:
D0=Nr(Nd(L),Nd(H)),
s.t.min
D0,â„¦ZZ
â„¦|âˆ‡D0âˆ’ âˆ‡Dâˆ—|âˆ‚â„¦ +ZZ
Iâˆ’â„¦|D0âˆ’Dâˆ—|âˆ‚â„¦.(2)
The refined depth of Nris denoted as D0.âˆ‡refers to the gradient operator. Typically for depth
refinement [ 3,25,21] task, input image Iis resized to low-resolution Land high-resolution Hfor
Nd.â„¦represents high-frequency areas, while Iâˆ’â„¦showcases low-frequency regions.
Motivation Elaboration. In practice, due to the inaccessibility of truly ideal depth, approximation of
Dâˆ—is required for training Nr. For this reason, the optimization objective in Eq. 2 is divided into â„¦
andIâˆ’â„¦. For the low-frequency Iâˆ’â„¦,Dâˆ—can be simply represented by the ground truth Dâˆ—
gtof
training data. However, as illustrated in Sec. 2, depth annotations inevitably suffer from imperfect
edge quality for the high-frequency â„¦. It is essential to generate accurate approximations of ideal
depth boundaries as training labels, which are robust to ÏµconsandÏµedge. Some prior arts adopts synthetic
depth [ 39,11,32] for higher edge quality, while leading to limited generalization capability with
blurred predictions in real-world scenes. To leverage real depth data [ 35,47,46,49,5,20], GBDF [ 3]
employs depth predictions [ 51] with guided filter as pseudo-labels, which still contain significant
noises and result in blurred depth. Besides, optimization of â„¦is also ignored. Kim et al. [14] relies
on manually annotated â„¦regions as input. GBDF [ 3,30,29] omits the selection of â„¦and supervises
depth gradients on the whole image. Inaccurate approximations of âˆ‡Dâˆ—and inappropriate division
ofâ„¦lead to limited robustness to local inconsistency noise and edge deformation noise.
Method Overview. To address the challenges, as shown in Fig. 3, we propose our SDDR framework
with two main components: depth edge representation and edge-based guidance. To achieve low-
noise approximations of âˆ‡Dâˆ—, we construct the depth edge representation Gsthrough coarse-to-fine
self-distillation, where sâˆˆ {1,2,Â·Â·Â·, S}refers to iteration numbers. The input image is divided
into several windows with overlaps from coarse to fine. For instance, we denote the high-frequency
area of a certain window win iteration sasâ„¦w
s, and the refined depth of NrasDw
s. In this way, the
self-distilled optimization of depth edge representation Gscan be expressed as follows:
Dw
sâ‰ˆDâˆ—+Ïµcons+Ïµedge,
min
GsX
wZZ
â„¦ws|Gw
sâˆ’ âˆ‡Dw
s|âˆ‚â„¦w
s.(3)
During training, depth edge representation Gw
sis further optimized based on the gradient of current
refined depth Dw
s. The final edge representation GSof the whole image will be utilized as the pseudo-
label to supervise the refinement network NrafterSiterations. SDDR can generate low-noise and
robust edge representation, mitigating the impact of ÏµconsandÏµedge(More results in Appendix A.1).
With GSas the training label, the next is to enforce Nrwith robustness to the noises, achieving
consistent structures and meticulous boundaries. To optimize Nr, we propose edge-based guidance
as an equivalent optimization objective to noisy Poisson fusion problem, which is presented by:
min
D0,â„¦ZZ
â„¦|âˆ‡D0âˆ’GS|âˆ‚â„¦ +ZZ
Iâˆ’â„¦D0âˆ’Dâˆ—
gtâˆ‚â„¦. (4)
4ïğ’…ğ’…
ïr
S=1S=2S=3Coarse -to-fine
Edge Refinement
S=1
S=2
S=3
Self-Distilled Depth Refinement
Depth Edge Representation
Edge -based GuidanceEdge -guided Gradient Error
Self-distillation
Edge -based 
GuidanceEdge -based Fusion ErrorRefined Depth ğ‘«ğ‘«ğŸğŸRefined Edge ğ‘®ğ‘®ğŸğŸInitial Step (s=0)
ğ‘«ğ‘«ğŸğŸğ’˜ğ’˜ğ‘«ğ‘«ğŸğŸğ’˜ğ’˜Final Step (s=S)
Refined Depth ğ‘«ğ‘«ğ‘ºğ‘º
ğ‘®ğ‘®ğ‘ºğ‘ºğ‘·ğ‘·ğ’ğ’ ğ‘®ğ‘®ğŸğŸğ‘·ğ‘·ğ’ğ’Region Mask ğœ´ğœ´Edge -based GuidanceDepth 
Predictorïğ’…ğ’…ïrRefinement 
Network
Self-Distillation Training
with Noisy Poisson Fusion
H
Ldï
ğŸ”’ğŸ”’
ğ‘®ğ‘®ğ‘ºğ‘º>ğ’•ğ’•ğŸğŸâˆ’ğ’‚ğ’‚
ğ‘®ğ‘®ğ‘ºğ‘º>ğ’•ğ’•ğŸğŸâˆ’ğ’ğ’âˆ—ğ’‚ğ’‚
ğ‘®ğ‘®ğ‘ºğ‘º<ğ’•ğ’•ğ’‚ğ’‚ğ‘®ğ‘®ğ‘ºğ‘º<ğ’•ğ’•ğ’ğ’âˆ—ğ’‚ğ’‚
Low Frequency High Frequency
Refined Edge ğ‘®ğ‘®ğ‘ºğ‘º
Refined Depth ğ‘«ğ‘«ğ‘ºğ‘º
ğœ´ğœ´ğ‘®ğ‘®ğ‘ºğ‘º
SupervisionClustering Quantile 
Samplings=1s=2s=3
High ğœºğœºğœğœğœğœğœğœğœğœ+ğœºğœºğğğğğğğğ Low ğœºğœºğœğœğœğœğœğœğœğœ+ğœºğœºğğğğğğğğ
Figure 3: Overview of self-distilled depth refinement. SDDR consists of depth edge representation
and edge-based guidance. Refinement network Nrproduces initial refined depth D0, edge representa-
tionG0, and learnable soft mask â„¦of high-frequency areas. The final depth edge representation GS
is updated from coarse to fine as pseudo-labels. The edge-based guidance with edge-guided gradient
loss and edge-based fusion loss supervises Nrto achieve consistent structures and fine-grained edges.
For the second term of Iâˆ’â„¦, we adopt depth annotations Dâˆ—
gtas the approximation of Dâˆ—. For the
first term, with the generated GSas pseudo-labels of âˆ‡Dâˆ—, we propose edge-guided gradient loss
and edge-based fusion loss to optimize D0andâ„¦predicted by Nr. The edge-guided gradient loss
supervises the model to consistently refine depth edges with local scale and shift alignment. The
edge-based fusion loss guides Nrto adaptively fuse low- and high-frequency features based on the
learned soft region mask â„¦, achieving balanced consistency and details by quantile sampling.
Overall, when depth maps are better refined under the edge-based guidance, the edge representation
also becomes more accurate and noise-free with the carefully designed coarse-to-fine manner. The
self-distillation paradigm can be naturally conducted based on the noisy Poisson fusion, enforcing
our model with strong robustness against the local inconsistency noise and edge deformation noise.
3.2 Depth Edge Representation
To build the self-distilled training paradigm, the prerequisite is to construct accurate and low-
noise depth edge representations as pseudo-labels. Meticulous steps are designed to generate the
representations with both consistent structures and accurate details.
Initial Depth Edge Representation. We generate an initial depth edge representation based on the
global refinement results of the whole image. For the input image I, we obtain the refined depth
results D0fromNras in Eq. 2. Depth gradient G0=âˆ‡D0is calculated as the initial representation.
An edge-preserving filter [ 38] is applied on G0to reduce noises in low-frequency area Iâˆ’â„¦. With
global information of the whole image, G0can preserve spatial structures and depth consistency. It
also incorporates certain detailed information from the high-resolution input H. To enhance edges
and details in high-frequency region â„¦, we conduct coarse-to-fine edge refinement in the next step.
Coarse-to-fine Edge Refinement. The initial D0is then refined from course to fine with Siterations
to generate final depth edge representation. For a specific iteration sâˆˆ {1,2,Â·Â·Â·, S}, we uniformly
divide input image Iinto(s+ 1)2windows with overlaps. We denote a certain window win iteration
sof the input image IasIw
s. The high-resolution Hw
sis then fed to the depth predictor Nd.Dw
sâˆ’1
represents the depth refinement results of the corresponding window win the previous iteration sâˆ’1.
The refined depth Dw
sof window win current iteration sas Eq. 3 can be obtained by NdandNr:
Dw
s=Nr(Dw
sâˆ’1,Nd(Hw
s)), sâˆˆ {1,2,Â·Â·Â·, S}, (5)
After that, depth gradient âˆ‡Dw
sis used to update the depth edge representation. The coarse-to-fine
manner achieves consistent spatial structures and accurate depth details with balanced global and
regional information. In the refinement process, only limited iterations and windows are needed.
Thus, SDDR achieves much higher efficiency than tile-based methods [ 25,21], as shown in Sec. C.1.
Scale and Shift Alignment. The windows are different among varied iterations. Depth results and
edge labels on corresponding window wof consecutive iterations could be inconsistent in depth scale
5RGB Depth Predictor Initial Depth ğ‘«ğ‘«ğŸğŸ Final Depth ğ‘«ğ‘«ğ‘ºğ‘º Pseudo -label ğ‘®ğ‘®ğ‘ºğ‘º
 Quantile -sampled ğœ´ğœ´
Quantile -sampled ğ‘®ğ‘®ğ‘ºğ‘º
Figure 4: Visualization of intermediate results. We visualize the results of several important steps
within the SDDR framework. The quantile sampling utilizes the same color map as in Fig. 3.
and shift. Therefore, alignment is required before updating the depth edge representation:
(Î²1, Î²0) = arg min
Î²1,Î²0âˆ¥(Î²1âˆ‡Dw
s+Î²0)âˆ’Gw
sâˆ’1âˆ¥2
2,
Gw
s=Î²1âˆ‡Dw
s+Î²0,(6)
where Î²1andÎ²0are affine transformation coefficients as scale and shift respectively. The aligned Gw
s
represents the depth edge pseudo-labels for image patch Iw
sgenerated from the refined depth Dw
s. At
last, after Siterations, we can obtain the pseudo-label GSas the final depth edge representation for
self-distillation. For better understanding, we showcase visualization of D0,DS, and GSin Fig. 4.
Robustness to Noises. In each window, we merge high-resolution Nd(Hw
s)to enhance details and
suppress Ïµedge. Meanwhile, coarse-to-fine window partitioning and scale alignment mitigate Ïµconsand
bring consistency. Thus, GSexhibits strong robustness to the two types of noises by self-distillation.
3.3 Edge-based Guidance
With depth edge representation GSas pseudo-label for self-distillation, we propose the edge-based
guidance including edge-guided gradient loss and edge-based fusion loss to supervise Nr.
Edge-guided Gradient Loss. We aim for fine-grained depth by one-stage refinement, while the
two-stage coarse-to-fine manner can further improve the results. Thus, edge-guided gradient loss
instructs the initial D0with the accurate GS. Some problems need to be tackled for this purpose.
AsNrhas not converged in the early training phase, GSis not sufficiently reliable with inconsistent
scales and high-level noises between local areas. Therefore, we extract several non-overlapping
regions Pn, nâˆˆ {1,2,Â·Â·Â·, Ng}with high gradient density by clustering [ 8], where Ngrepresents
the number of clustering centroids. The edge-guided gradient loss is only calculated inside Pnwith
scale and shift alignment. By doing so, the model can focus on improving details in high-frequency
regions and preserving depth structures in flat areas. The training process can also be more stable.
The edge-guided gradient loss can be calculated by:
Lgrad=1
NgNgX
n=1||(Î²1G0[Pn] +Î²0)âˆ’GS[Pn]||1, (7)
where Î²1andÎ²0are the scale and shift coefficients similar to Eq. 6. We use [Â·]to depict mask fetching
operations, i.e., extracting local area PnfromG0andGS. With the edge-guided gradient loss, SDDR
predicts refined depth with meticulous edges and consistent structures.
Edge-based Fusion Loss. High-resolution feature FHextracted from Hbrings finer details but could
lead to inconsistency, while the low-resolution feature FLfromLcan better maintain depth structures.
Nrshould primarily rely on FLfor consistent spatial structures within low-frequency Iâˆ’â„¦, while it
should preferentially fuse FHfor edges and details in high-frequency areas â„¦. The fusion of FLand
FHnoticeably influence the refined depth. However, prior arts [ 14,3,25] adopt manually-annotated
â„¦regions as fixed masks or even omit â„¦as the whole image, leading to inconsistency and blurring.
To this end, we implement â„¦as a learnable soft mask, with quantile sampling strategy to guide the
adaptive fusion of FLandFH. The fusion process is expressed by:
F= (1âˆ’â„¦)âŠ™FL+ â„¦âŠ™FH, (8)
where âŠ™refers to the Hadamard product. â„¦is the learnable mask ranging from zero to one. Larger
values in â„¦showcases higher frequency with denser edges, requiring more detailed information from
the high-resolution feature FH. Thus, â„¦can naturally serve as the fusion weight of FLandFH.
To be specific, we denote the lower quantile of GSasta,i.e.,P(X < t a|XâˆˆGS) =a.{GS< ta}
indicates flat areas with low gradient magnitude, while {GS> t1âˆ’a}represents high-frequency
6Predictor MethodMiddlebury2021 Multiscopic Hypersim
Î´1â†‘ RELâ†“ ORDâ†“ Î´1â†‘ RELâ†“ ORDâ†“ Î´1â†‘ RELâ†“ ORDâ†“
MiDaSMiDaS [30] 0.868 0 .117 0 .384 0 .839 0 .130 0 .292 0 .781 0 .169 0 .344
Kim et al. [14] 0.864 0 .120 0 .377 0 .839 0 .130 0 .293 0 .778 0 .175 0 .344
Graph-GDSR [4] 0.865 0 .121 0 .380 0 .839 0 .130 0 .292 0 .781 0 .169 0 .345
GBDF [3] 0.871 0 .115 0 .305 0 .841 0 .129 0 .289 0 .787 0 .168 0 .338
Ours 0.879 0.112 0.299 0.852 0.122 0.267 0.791 0.166 0.318
LeReSLeReS [51] 0.847 0 .123 0 .326 0 .863 0 .111 0 .272 0 .853 0 .123 0 .279
Kim et al. [14] 0.846 0 .124 0 .328 0 .860 0 .113 0 .286 0 .850 0 .125 0 .286
Graph-GDSR [4] 0.847 0 .124 0 .327 0 .862 0 .111 0 .273 0 .852 0 .123 0 .281
GBDF [3] 0.852 0 .122 0 .316 0 .865 0 .110 0 .270 0 .857 0 .121 0 .273
Ours 0.862 0.120 0.305 0.870 0.108 0.259 0.862 0.120 0.273
ZoeDepthZoeDepth [1] 0.900 0 .104 0 .225 0 .896 0 .097 0 .205 0 .927 0 .088 0 .198
Kim et al. [14] 0.896 0 .107 0 .228 0 .890 0 .099 0 .204 0 .923 0 .091 0 .204
Graph-GDSR [4] 0.901 0 .103 0 .226 0 .895 0 .096 0 .208 0 .926 0 .089 0 .199
GBDF [3] 0.899 0 .105 0 .226 0 .897 0 .096 0 .207 0 .925 0 .089 0 .199
Ours 0.905 0.100 0.218 0.904 0.092 0.199 0.930 0.086 0.191
Table 1: Comparisons with one-stage methods. As prior arts [ 14,4,3], we conduct evaluations
with different depth predictors [ 30,51,1]. For each predictor, we report the initial metrics and results
of refinement methods. Best performances with each depth predictors [30, 51, 1] are in boldface.
regions. â„¦should be larger in those high-frequency areas {GS> t1âˆ’a}and smaller in the flat regions
{GS< ta}. This suggests that GSandâ„¦should be synchronized with similar data distribution.
Thus, if we define the lower quantile of â„¦asTa,i.e.,P(X < T a|Xâˆˆâ„¦) = a, an arbitrary pixel
iâˆˆ {GS< ta}in flat regions should also belong to {â„¦< Ta}with a lower weight for FH, while
the pixel iâˆˆ {GS> t1âˆ’a}in high-frequency areas should be contained in {â„¦> T 1âˆ’a}for more
detailed information. The edge-based fusion loss can be depicted as follows:
Lfusion =1
NwNpNwX
n=1NpX
i=1max(0 ,â„¦iâˆ’Tnâˆ—a), i âˆˆ {GS< tnâˆ—a},
max(0 , T1âˆ’nâˆ—aâˆ’â„¦i), iâˆˆ {GS> t1âˆ’nâˆ—a},(9)
where Npis the pixel number. We supervise the distribution of â„¦with lower quantiles Tnâˆ—aand
T1âˆ’nâˆ—a, nâˆˆ {1,2,Â·Â·Â·, Nw}. Therefore, pixels with larger deviations between GSandâ„¦will
be penalized more heavily. Taking the worst case as an example, if iâˆˆ {GS< tNwâˆ—a}but
i /âˆˆ {â„¦< T Nwâˆ—a}, the error for the pixel will be accumulated for Nwtimes from atoNwâˆ—a.
Lfusion enforces SDDR with consistent structures (low Ïµconsnoise) in Iâˆ’â„¦and accurate edges (low
Ïµedgenoise) in â„¦. The visualizations of quantile-sampled GSandâ„¦are presented in Fig. 4.
Finally, combining Lgrad andLfusion as edge-based guidance for self-distillation, the overall loss L
for training Nris calculated as Eq. 10. Lgtsupervises the discrepancy between D0and ground truth
Dâˆ—
gtwith affinity-invariant loss [30, 29]. See Appendix A for implementation details of SDDR.
L=Lgt+Î»1Lgrad+Î»2Lfusion . (10)
4 Experiments
To prove the efficacy of Self-distilled Depth Refinement (SDDR) framework, we conduct extensive
experiments on five benchmarks [ 34,52,32,15,40] for indoor and outdoor, synthetic and real-world.
Experiments and Datasets. Firstly, we follow prior arts [ 3,25,14] to conduct zero-shot evaluations
on Middlebury2021 [ 34], Multiscopic [ 52], and Hypersim [ 32]. To showcase our superior generaliz-
ability, we compare different methods on DIML [ 15] and DIODE [ 40] with diverse natural scenes.
Moreover, we prove the higher efficiency of SDDR and undertake ablations on our specific designs.
Evaluation Metrics. Evaluations of depth accuracy and edge quality are necessary for depth
refinement models. For edge quality, we adopt the ORD andD3Rmetrics following Boost [ 25]. For
depth accuracy, we adopt the widely-used REL and Î´i(i= 1,2,3). See Appendix B for details.
4.1 Comparisons with Other Depth Refinement Approaches
Comparisons with One-stage Methods. For fair comparisons, we evaluate one-stage [ 14,4,3] and
two-stage tile-based [ 25,21] approaches separately. The one-stage methods predict refined depth
7Predictor MethodMiddlebury2021 Multiscopic Hypersim
Î´1â†‘ RELâ†“ ORDâ†“ Î´1â†‘ RELâ†“ ORDâ†“ Î´1â†‘ RELâ†“ ORDâ†“
MiDaSMiDaS [30] 0.868 0 .117 0 .384 0 .839 0 .130 0 .292 0 .781 0 .169 0 .344
Boost [25] 0.870 0 .118 0 .351 0 .845 0 .126 0 .282 0 .794 0 .161 0 .332
Ours 0.871 0.115 0.303 0.858 0.120 0.263 0.799 0.154 0.322
LeReSLeReS [51] 0.847 0 .123 0 .326 0 .863 0 .111 0 .272 0 .853 0 .123 0 .279
Boost [25] 0.844 0 .131 0 .325 0 .860 0 .112 0 .278 0.865 0.118 0.272
Ours 0.861 0.123 0.309 0.870 0.109 0.268 0.858 0 .123 0.271
ZoeDepthZoeDepth [1] 0.900 0 .104 0 .225 0 .896 0 .097 0 .205 0 .927 0 .088 0 .198
Boost [25] 0.911 0 .099 0 .210 0.910 0.094 0 .197 0 .926 0 .089 0 .193
PatchFusion [21] 0.887 0 .102 0 .211 0 .908 0 .095 0 .212 0 .881 0 .116 0 .258
Ours 0.913 0.096 0.202 0.908 0.091 0.197 0.933 0.083 0.189
Table 2: Comparisons with two-stage methods. PatchFusion [ 21] only adopts ZoeDepth [ 1] as the
fixed baseline, while other approaches are pluggable for different depth predictors [30, 51, 1].
RGB LeReS Kim et al. GBDF Ours(one-stage)
Figure 5: Qualitative comparisons of one-stage methods on natural scenes. LeReS [ 51] is used
as the depth predictor. SDDR predicts sharper depth edges and more meticulous details than prior
arts [3, 14], e.g., fine-grained predictions of intricate branches. Better viewed when zoomed in.
RGB ZoeDepth PatchFusion Boost Ours(two- stage)
Figure 6: Qualitative comparisons of two-stage methods on natural scenes. ZoeDepth [ 1] is
adopted as the depth predictor. The SDDR with coarse-to-fine edge refinement can predict more
accurate depth edges and more consistent spatial structures than the tile-based methods [21, 25].
based on the whole image. SDDR conducts one-stage refinement without the coarse-to-fine manner
during inference. Comparisons on Middlebury2021 [ 34], Multiscopic [ 52], and Hypersim [ 32] are
shown in Table 1. As prior arts [ 14,4,3], we use three depth predictors MiDaS [ 30], LeReS [ 51],
and ZoeDepth [ 1]. Regardless of which depth predictor is adopted, SDDR outperforms the previous
one-stage methods [ 14,4,3] in depth accuracy and edge quality on the three datasets [ 34,52,32]. For
instance, our method shows 6.6%and20.7%improvements over Kim et al. [14] for REL and ORD
with MiDaS [30] on Middlebury2021 [34], showing the efficacy of our self-distillation paradigm.
Comparisons with Two-stage Tile-based Methods. Two-stage tile-based methods [ 25,21] conduct
local refinement on numerous patches based on the global refined depth. SDDR moves away from the
tile-based manner and utilizes coarse-to-fine edge refinement to further improve edges and details. As
in Table 2, SDDR with the coarse-to-fine manner shows obvious advantages. For example, compared
with the recent advanced PatchFusion [ 21], SDDR achieves 5.2%and26.7%improvements for Î´1and
ORD with ZoeDepth [ 1] on Hypersim [ 32]. To be mentioned, PatchFusion [ 21] uses ZoeDepth [ 1] as
the fixed baseline, whereas SDDR is readily pluggable for various depth predictors [30, 51, 1].
Generalization Capability on Natural Scenes. We prove the superior generalization capability of
SDDR. In this experiment, we adopt LeReS [ 51] as the depth predictor. DIML [ 15] and DIODE [ 40]
8MethodDIML DIODE
Î´1â†‘ RELâ†“ORDâ†“D3Râ†“ Î´1â†‘ RELâ†“ORDâ†“D3Râ†“
LeReS [51] 0.902 0 .101 0 .242 0 .284 0 .892 0 .105 0 .324 0 .685
Kim et al. [14] 0.902 0 .100 0 .243 0 .301 0 .889 0 .105 0 .325 0 .713
Graph-GDSR [4] 0.901 0 .101 0 .243 0 .300 0 .890 0 .104 0 .326 0 .690
GBDF [3] 0.906 0 .100 0 .239 0 .267 0 .894 0 .105 0 .322 0 .673
Boost [25] 0.897 0 .108 0 .274 0 .438 0 .892 0 .105 0 .343 0 .640
Ours 0.926 0.098 0.221 0.220 0.900 0.098 0.293 0.637
Table 3: Comparisons of model generalizability. We con-
duct zero-shot evaluations on DIML [ 15] and DIODE [ 40]
datasets with diverse in-the-wild scenarios to compare the
generalization capability. We adopt LeReS [ 51] as the depth
predictor for all the compared methods in this experiment.
0.865
0.8250.8350.8450.855
Noise LevelDepth Accuracy( ğœ¹ğœ¹ğŸğŸ)
Ours
GBDFFigure 7: Robustness against noises.
X-axis shows noise level of Ïµcons+
Ïµedges. With higher noises, our SDDR
is more robust with less performance
degradation than the prior GBDF [ 3].
Method Î´1â†‘ RELâ†“ORDâ†“D3Râ†“
S= 0 0 .859 0 .125 0 .313 0 .235
S= 1 0 .860 0 .122 0 .309 0 .223
S= 2 0 .860 0 .120 0 .307 0 .219
S= 3 0.862 0.120 0.305 0.216
(a) Coarse-to-fine Edge RefinementLgtLgrad Lfusion Î´1â†‘ RELâ†“ORDâ†“D3Râ†“
âœ“ 0.854 0 .124 0 .313 0 .240
âœ“ âœ“ 0.858 0 .122 0 .307 0 .220
âœ“ âœ“ 0.859 0 .120 0 .311 0 .229
âœ“ âœ“ âœ“ 0.862 0.120 0.305 0.216
(b) Edge-based GuidanceMethod Training Data Î´1â†‘ RELâ†“ORDâ†“D3Râ†“
GBDF [3] HRWSI [49] 0.852 0 .122 0 .316 0 .258
Ours HRWSI [49] 0.860 0.121 0.309 0.222
(c) Effectiveness
Method Î´1â†‘ RELâ†“ORDâ†“D3Râ†“
GBDF [3] 0.852 0 .122 0 .316 0 .258
GBDF ( w/ G S)0.858 0.122 0.307 0.230
(d) Transferability
Table 4: Ablation Study. All ablations are on Middlebury2021 [ 34] with depth predictor LeReS [ 51].
datasets are used for zero-shot evaluations, considering their diverse in-the-wild indoor and outdoor
scenarios. As in Table 3, SDDR shows at least 5.7%and9.0%improvements for REL andORD
on DIODE [ 40]. On DIML [ 15] dataset, our approach improves D3R,ORD , and Î´1by over 17.6%,
7.5%, and 2.0%. The convincing performance proves our strong robustness and generalizability,
indicating the efficacy of our noisy Poisson fusion modeling and self-distilled training paradigm.
Qualitative Comparisons. We present visual comparisons of one-stage methods [ 14,3] on natural
scenes in Fig. 5. With our low-noise depth edge representation and edge-based guidance, SDDR
predicts sharper depth edges and details, e.g., the fine-grained predictions of intricate branches.
The visual results of two-stage approaches [ 25,21] are shown in Fig. 6. Due to the excessive fusion
of detailed information, tile-based methods [ 25,21] produce structure disruption, depth inconsistency,
or even noticeable artifacts, e.g., disrupted and fuzzy structures of the snow-covered branches. By
contrast, SDDR can predict more accurate depth edges and more consistent spatial structures.
Robustness against noises. As in Fig. 7, we evaluate SDDR and GBDF [ 3] with different levels
of input noises. As the noise level increases, our method presents less degradation. The stronger
robustness against the ÏµconsandÏµedges noises is the essential reason for all our superior performance.
Model Efficiency. SDDR achieves higher efficiency. Two-stage tile-based methods [ 25,21] rely on
complex fusion of extensive patches with heavy computational overhead. Our coarse-to-fine manner
noticeably reduces Flops per patch and patch numbers as in Fig. 1. For one-stage methods [ 4,3,14],
SDDR adopts a more lightweight Nrwith less parameters and faster inference speed over the previous
GBDF [3] and Kim et al. [14]. See Appendix C.1 for detailed comparisons of model efficiency.
4.2 Ablation Studies
Coarse-to-fine Edge Refinement. In Table 4a, we adopt the coarse-to-fine manner with varied
iterations. S= 0represents one-stage inference. Coarse-to-fine refinement brings more fine-grained
edge representations and refined depth. We set S= 3for the SDDR with two-stage inference.
Edge-based Guidance. In Table 4b, we evaluate the effectiveness of edge-based guidance. Lgrad
focuses on consistent refinement of depth edges. Lfusion guides the adaptive feature fusion of low-
and high-frequency information. With Lgtas the basic supervision of ground truth, adding Lgrad and
Lfusion improves D3R by 10.0%and REL by 3.2%, showing the efficacy of edge-based guidance.
Effectiveness of SDDR Framework. As in Table 4c, we train SDDR with the same HRWSI [ 49]
as GBDF [ 3] for fair comparison. Without the combined training data in Appendix B.1, SDDR still
improves D3R and ORD by 13.9%and2.2%over GBDF [3], proving our superiority convincingly.
9Transferability. We hope our depth edge representation GScan be applicable to other depth
refinement models. Therefore, in Table 4d, we directly train GBDF [ 3] combining the depth edge
representation produced by the trained SDDR. The depth accuracy and edge quality are improved
over the original GBDF [ 3], indicating the transferability of GSin training robust refinement models.
5 Conclusion
In this paper, we model the depth refinement task as a noisy Poisson fusion problem. To enhance the
robustness against local inconsistency and edge deformation noise, we propose Self-distilled Depth
Refinement (SDDR) framework. With the low-noise depth edge representation and guidance, SDDR
achieves both consistent spatial structures and meticulous depth edges. Experiments showcase our
stronger generalizability and higher efficiency over prior arts. The SDDR provides a new perspective
for depth refinement in future works. Limitations and broader impact are discussed in Appendix A.5.
Acknowledgement This work is supported by the National Natural Science Foundation of China
under Grant No. 62406120.
References
[1]Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias MÃ¼ller. Zoedepth: Zero-shot
transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288 , 2023. 1, 2, 3, 7, 8, 20,
21
[2]Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush Krishnan,
Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages
11618â€“11628, 2020. 2, 3
[3]Yaqiao Dai, Renjiao Yi, Chenyang Zhu, Hongjun He, and Kai Xu. Multi-resolution monocular depth map
fusion by self-supervised gradient-based composition. In Proceedings of the AAAI Conference on Artificial
Intelligence , volume 37, pages 488â€“496, 2023. 1, 2, 3, 4, 6, 7, 8, 9, 10, 15, 16, 18, 19, 20, 21
[4]Riccardo De Lutio, Alexander Becker, Stefano Dâ€™Aronco, Stefania Russo, Jan D Wegner, and Konrad
Schindler. Learning graph regularisation for guided super-resolution. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pages 1979â€“1988, 2022. 1, 7, 8, 9, 19,
20
[5]Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti
dataset. The International Journal of Robotics Research , 32(11):1231â€“1237, 2013. 2, 3, 4
[6]Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing
systems , volume 27, 2014. 2
[7]Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-
supervised monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 2482â€“2491, 2020. 2, 3
[8]John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal of
the royal statistical society. series c (applied statistics) , 28(1):100â€“108, 1979. 6, 15
[9]Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. IEEE transactions on pattern analysis
and machine intelligence , 35(6):1397â€“1409, 2012. 3
[10] Yiwen Hua, Puneet Kohli, Pritish Uplavikar, Anand Ravi, Saravana Gunaseelan, Jason Orozco, and Edward
Li. Holopix50k: A large-scale in-the-wild stereo image dataset. arXiv preprint arXiv:2003.11172 , 2020. 2,
18
[11] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning
multi-view stereopsis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2821â€“2830, 2018. 2, 3, 4, 18
[12] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.
Flownet 2.0: Evolution of optical flow estimation with deep networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) , pages 2462â€“2470, 2017. 18
10[13] John Kessenich, Graham Sellers, and Dave Shreiner. OpenGL Programming Guide: The official guide to
learning OpenGL, version 4.5 with SPIR-V . Addison-Wesley Professional, 2016. 1
[14] Soo Ye Kim, Jianming Zhang, Simon Niklaus, Yifei Fan, Simon Chen, Zhe Lin, and Munchurl Kim.
Layered depth refinement with mask guidance. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 3855â€“3865, 2022. 1, 3, 4, 6, 7, 8, 9, 14, 16, 19, 20, 21
[15] Youngjung Kim, Hyungjoo Jung, Dongbo Min, and Kwanghoon Sohn. Deep monocular depth estimation
via integration of global and local predictions. IEEE Transactions on Image Processing , 27(8):4131â€“4144,
2018. 2, 7, 8, 9, 18, 20, 21
[16] Tobias Koch, Lukas Liebel, Friedrich Fraundorfer, and Marco KÃ¶rner. Evaluation of cnn-based single-
image depth estimation methods. In Laura Leal-TaixÃ© and Stefan Roth, editors, Proceedings of the
European Conference on Computer Vision (ECCV) Workshops , volume 11131, pages 331â€“348. Springer,
2018. 18
[17] Guchong Li. An iggm-based poisson multi-bernoulli filter and its application to distributed multisensor
fusion. IEEE Transactions on Aerospace and Electronic Systems , 58(4):3666â€“3677, 2022. 3
[18] Jiaqi Li, Yiran Wang, Zihao Huang, Jinghong Zheng, Ke Xian, Zhiguo Cao, and Jianming Zhang. Diffusion-
augmented depth prediction with sparse annotations. In Proceedings of the 31st ACM International
Conference on Multimedia , pages 2865â€“2876, 2023. 3
[19] Jing Li, Hongtao Huo, Chenhong Sui, Chenchen Jiang, and Chang Li. Poisson reconstruction-based fusion
of infrared and visible images via saliency detection. IEEE Access , 7:20676â€“20688, 2019. 3
[20] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , June
2018. 2, 3, 4
[21] Zhenyu Li, Shariq Farooq Bhat, and Peter Wonka. Patchfusion: An end-to-end tile-based framework for
high-resolution monocular metric depth estimation. arXiv preprint arXiv:2312.02284 , 2023. 1, 2, 3, 4, 5,
7, 8, 9, 14, 19, 20, 21
[22] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for
high-resolution semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 1925â€“1934, 2017. 16
[23] Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature
pyramid networks for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 2117â€“2125, 2017. 16, 17
[24] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth
estimation. ACM Transactions on Graphics (ToG) , 39(4):71â€“1, 2020. 1
[25] S Mahdi H Miangoleh, Sebastian Dille, Long Mai, Sylvain Paris, and Yagiz Aksoy. Boosting monocular
depth estimation models to high-resolution via content-adaptive multi-resolution merging. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 9685â€“9694,
2021. 1, 2, 3, 4, 5, 6, 7, 8, 9, 14, 15, 16, 18, 19, 20, 21
[26] Antigoni Panagiotopoulou and Vassilis Anastassopoulos. Super-resolution image reconstruction techniques:
Trade-offs between the data-fidelity and regularization terms. Information Fusion , 13(3):185â€“195, 2012. 3
[27] Juewen Peng, Zhiguo Cao, Xianrui Luo, Hao Lu, Ke Xian, and Jianming Zhang. Bokehme: When neural
rendering meets classical rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 16283â€“16292, 2022. 1
[28] Juewen Peng, Jianming Zhang, Xianrui Luo, Hao Lu, Ke Xian, and Zhiguo Cao. Mpib: An mpi-based
bokeh rendering framework for realistic partial occlusion effects. In European Conference on Computer
Vision (ECCV) , pages 590â€“607. Springer, 2022. 1
[29] RenÃ© Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In
Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages 12179â€“12188,
2021. 4, 7
[30] RenÃ© Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust
monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on
pattern analysis and machine intelligence , 44(03):1623â€“1637, 2020. 1, 2, 3, 4, 7, 8, 20
11[31] Haoyu Ren, Amin Kheradmand, Mostafa El-Khamy, Shuangquan Wang, Dongwoon Bai, and Jungwon
Lee. Real-world super-resolution using generative adversarial networks. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , pages 436â€“437, 2020. 3
[32] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan,
Russ Webb, and Joshua M Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor scene
understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) ,
pages 10912â€“10922, 2021. 1, 2, 3, 4, 7, 8, 18
[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 10684â€“10695, 2022. 1
[34] Daniel Scharstein, Heiko HirschmÃ¼ller, York Kitajima, Greg Krathwohl, Nera NeÅ¡i Â´c, Xi Wang, and Porter
Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In Pattern Recognition:
36th German Conference, GCPR 2014, MÃ¼nster, Germany, September 2-5, 2014, Proceedings 36 , pages
31â€“42. Springer, 2014. 2, 7, 8, 9, 17, 19, 20, 21
[35] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support
inference from rgbd images. In European Conference on Computer Vision (ECCV) , pages 746â€“760.
Springer, 2012. 2, 3, 4
[36] JÃ¼rgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for
the evaluation of rgb-d slam systems. In IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS) , pages 573â€“580. IEEE, 2012. 2, 3
[37] Zhanghao Sun, Wei Ye, Jinhui Xiong, Gyeongmin Choe, Jialiang Wang, Shuochen Su, and Rakesh Ranjan.
Consistent direct time-of-flight video depth super-resolution. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages 5075â€“5085, 2023. 1
[38] Carlo Tomasi and Roberto Manduchi. Bilateral filtering for gray and color images. In Proceedings of the
IEEE/CVF International Conference on Computer Vision (ICCV) , pages 839â€“846. IEEE, 1998. 5
[39] Fabio Tosi, Yiyi Liao, Carolin Schmitt, and Andreas Geiger. Smd-nets: Stereo mixture density networks.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages
8942â€“8952, 2021. 2, 3, 4, 18
[40] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Z. Dai, Andrea F.
Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew R. Walter, and Gregory Shakhnarovich.
DIODE: A Dense Indoor and Outdoor DEpth Dataset. CoRR , 2019. 2, 7, 8, 9, 18, 20
[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing
systems , volume 30, 2017. 16
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. Adv. Neural Inf. Process. Syst. , pages
5998â€“6008, 2017. 16
[43] Chaoyang Wang, Simon Lucey, Federico Perazzi, and Oliver Wang. Web stereo video supervision for
depth prediction from dynamic scenes. In IEEE International Conference on 3D Vision (3DV) , pages
348â€“357. IEEE, 2019. 2, 3, 18
[44] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. Irs: A large
naturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation.
InIEEE International Conference on Multimedia and Expo (ICME) , pages 1â€“6. IEEE Computer Society,
2021. 2, 3, 18
[45] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish
Kapoor, and Sebastian Scherer. Tartanair: A dataset to push the limits of visual slam. In IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS) , pages 4909â€“4916. IEEE, 2020. 2, 3,
18
[46] Yiran Wang, Min Shi, Jiaqi Li, Chaoyi Hong, Zihao Huang, Juewen Peng, Zhiguo Cao, Jianming Zhang,
Ke Xian, and Guosheng Lin. Nvds+: Towards efficient and versatile neural stabilizer for video depth
estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence , pages 1â€“18, 2024. 4
12[47] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin.
Neural video depth stabilizer. In Proceedings of the IEEE/CVF International Conference on Computer
Vision (ICCV) , pages 9466â€“9476, 2023. 2, 3, 4, 18
[48] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative
depth perception with web stereo data supervision. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages 311â€“320, 2018. 2
[49] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao. Structure-guided ranking
loss for single image depth prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 608â€“617, 2020. 2, 3, 4, 9, 18, 19, 20, 21
[50] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:
Simple and efficient design for semantic segmentation with transformers. Advances in neural information
processing systems , 34:12077â€“12090, 2021. 16
[51] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen.
Learning to recover 3d scene shape from a single image. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages 204â€“213, 2021. 1, 2, 3, 4, 7, 8, 9, 19, 20, 21
[52] Weihao Yuan, Yazhan Zhang, Bingkun Wu, Siyu Zhu, Ping Tan, Michael Yu Wang, and Qifeng Chen.
Stereo matching by self-supervision of multiscopic vision. In IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS) , pages 5702â€“5709. IEEE, 2021. 2, 7, 8, 17
[53] Junrui Zhang, Jiaqi Li, Yachuan Huang, Yiran Wang, Jinghong Zheng, Liao Shen, and Zhiguo Cao.
Towards robust monocular depth estimation in non-lambertian surfaces. arXiv preprint arXiv:2408.06083 ,
2024. 3
[54] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion
models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages
3836â€“3847, 2023. 1
[55] Zhengyou Zhang. Microsoft kinect sensor and its effect. IEEE multimedia , 19(2):4â€“10, 2012. 2, 3, 18
[56] Changzhong Zou and Youshen Xia. Bayesian dictionary learning for hyperspectral image super resolution
in mixed poissonâ€“gaussian noise. Signal Processing: Image Communication , 60:29â€“41, 2018. 3
13RGB Depth Predictor ïğ’…ğ’…(ğ‘³ğ‘³)Depth Predictor ïğ’…ğ’…(ğ‘¯ğ‘¯)
Window Partitioning Window ğ’˜ğ’˜
 Depth ğ‘«ğ‘«ğŸğŸ
 Depth Edge ğ‘®ğ‘®ğŸğŸInitial Depth ğ‘«ğ‘«ğŸğŸ
s=0 s=1
Window Partitioning Window ğ’˜ğ’˜
 Depth ğ‘«ğ‘«ğŸğŸ
 Depth Edge ğ‘®ğ‘®ğŸğŸs=2
Window Partitioning Window ğ’˜ğ’˜
 Depth ğ‘«ğ‘«ğŸ‘ğŸ‘
 Depth Edge ğ‘®ğ‘®ğŸ‘ğŸ‘
s=3Depth Edge ğ‘®ğ‘®ğŸğŸ
Refined Window ğ’˜ğ’˜
Refined Window ğ’˜ğ’˜
Refined Window ğ’˜ğ’˜w
w
w
Figure 8: Visualizations of coarse-to-fine edge refinement. We present coarse-to-fine results of
steps s= 0,1,2,3. For s= 0, we showcase the low- and high-resolution predictions Nd(L)and
Nd(H)of the depth predictor, along with the initial refined depth D0and edge representation G0.
Fors= 1,2,3, we present the window partitioning on the previous Dsâˆ’1, the previous depth Dw
sâˆ’1
on a certain window w, refined depth Dw
son the window w, refined depth Dsof the whole image,
and the depth edge representation Gsgenerated on the current step.
A More Details on SDDR Framework
A.1 Depth Edge Representation
Coarse-to-fine Edge Refinement. In Sec. 3.2, line 169 of main paper, we propose the coarse-to-fine
edge refinement to generate accurate and fine-grained depth edge representation GS. Here, we provide
visualizations of the refinement process in Fig. 8. For the initial global refinement stage s= 0, we
showcase the results of the depth predictor at low and high inference resolutions, i.e.,Nd(L)and
Nd(H). Our refined depth D0presents both depth consistency and details. For s= 1,2,3, the refined
depth maps and edge representations are noticeably improved with finer edges and details. The final
depth edge representation GS(S= 3) with lower local inconsistency noise and edge deformation
noise is utilized as pseudo-label for the self-distillation training process.
Adaptive Resolution Adjustment. Adaptive resolution adjustment is applied to the low and high-
resolution input LandH. We denote the resolutions of LandHaslandh, which play a crucial
role in refined depth and need to be chosen carefully. Higher resolutions will bring finer details
but could lead to inconsistent depth structures due to the limited receptive field of Nd. Previous
works [ 14,25,21] upscale images or patches to excessively high resolutions for more details, resulting
in evident artifacts in their refined depth maps with higher levels of inconsistency noises Ïµcons. On the
other hand, if his too low, edge and detailed information cannot be sufficiently preserved in Nd(H),
leading to exacerbation of edge deformation noise Ïµedgewith blurred details in the refined depth. Such
errors and artifacts are unacceptable in depth edge representations for training models. Therefore, we
adaptively adjust resolutions landh, considering both the density of depth edges and the training
resolution of depth predictor Nd.
For image window Iw
s, we generally set the low-resolution input Lw
sas the training resolution Ë†rof
Nd. If we denote the original resolution of Iw
sasrw
s, SDDR adaptively adjusts the high resolution
14RGB Depth Predictor ïğ’…ğ’…(ğ‘³ğ‘³)Depth Predictor ïğ’…ğ’…(ğ‘¯ğ‘¯)
 Initial Depth ğ‘«ğ‘«ğŸğŸ
Boost
 Ours
Patch/Window Result Patch/Window Result
 Final Depth
448448 1568
1024
Figure 9: Adaptive resolution adjustment. We compare the effects of inference resolutions with
Boost [ 25]. The numbers in the corner of the second and third columns represent the chosen inference
resolution. We relieve the artifacts in Boost [25] by adaptive resolution adjustment.
RGB Initial ğ‘®ğ‘®ğŸğŸ Pseudo -label ğ‘®ğ‘®ğ‘ºğ‘º
 Clustered regions
ğ‘ƒğ‘ƒ1
ğ‘ƒğ‘ƒ4ğ‘ƒğ‘ƒ2 ğ‘ƒğ‘ƒ3
ğ‘ƒğ‘ƒ3ğ‘ƒğ‘ƒ1
ğ‘ƒğ‘ƒ4ğ‘ƒğ‘ƒ2 ğ‘ƒğ‘ƒ3ğ‘ƒğ‘ƒ1ğ‘ƒğ‘ƒ2
ğ‘ƒğ‘ƒ4
Figure 10: Edge-guided gradient error. Lgrad focuses on high-frequency areas Pnextracted by
clustering with more details. The flat regions are not constrained to preserve depth consistency.
hw
sfor the certain window as follows:
hw
s=mean (Ë†r, rw
s)âˆ—mean (|âˆ‡N d(Lw
s)|)
Î±âˆ—mean (|âˆ‡Dw
sâˆ’1|)
mean (|âˆ‡Dsâˆ’1|), (11)
where Î±is a priori parameter for depth predictor Nd, averaging the gradient magnitude of the depth
annotations on its sampled training data. The second term embodies adjustments according to depth
edges. Assuming mean (|âˆ‡N d(Lw
s)|)< Î±, it indicates that the current window area contains lower
edge intensity or density than the training data of Nd. In this case, we will appropriately decrease
hw
sfrom mean (Ë†r, rw
s)to maintain the similar density of detailed information as the training stage of
the depth predictor. The third term portrays adjustments based on the discrepancy of edge intensity
between the window area and the whole image. To be mentioned, for the generation of the initial
edge representation G0, the third term is set to ineffective as one. Lw
0is equivalent to Lwith the
whole image as the initial window w.
We present visual results with different resolutions to prove the effectiveness of our design. As shown
in Fig. 9, considering the training data distribution and the edge density, the inference resolution is
adaptively adjusted to a smaller one compared to Boost [ 25] (1024 versus 1568 ). In this way, our
SDDR achieves better depth consistency and alleviates the artifacts produced by prior arts [3, 25].
A.2 Edge-based Guidance
Edge-guided Gradient Error. In line 192, Sec 3.3 of the main paper, we mention that we use
clustering to obtain several high-frequency local regions to compute our edge-guided gradient loss.
Here, we elaborate on the details. K-means clustering [ 8] is utilized to obtain the edge-dense areas.
Specifically, we binarize the edge pseudo-label, setting the top 5%pixels to one and the rest to zero.
Next, we employ k-means clustering on the binarized labels to get several edge-dense areas with
the centroid value as one. The clustered areas are shown in the fourth column of the Fig. 10. Our
edge-guided gradient loss supervises these high-frequency regions to improve depth details. The
depth consistency in flat areas can be preserved without the constraints of depth edges.
15RGB Region Mask ğœ´ğœ´ Pseudo -label ğ‘®ğ‘®ğ‘ºğ‘º
 Quantile -sampled ğœ´ğœ´
 Quantile -sampled ğ‘®ğ‘®ğ‘ºğ‘º
ğ‘®ğ‘®ğ‘ºğ‘º>ğ’•ğ’•ğŸğŸâˆ’ğ’‚ğ’‚
Low -frequencyEdge Region
ğ’•ğ’•ğŸğŸâˆ’ğ’‚ğ’‚>ğ‘®ğ‘®ğ‘ºğ‘º>ğ’•ğ’•ğŸğŸâˆ’ğŸğŸğ’‚ğ’‚
ğ’•ğ’•ğŸğŸâˆ’ğŸğŸğ’‚ğ’‚>ğ‘®ğ‘®ğ‘ºğ‘º>ğ’•ğ’•ğŸğŸâˆ’ğŸ‘ğŸ‘ğ’‚ğ’‚
ğ‘®ğ‘®ğ‘ºğ‘º<ğ’•ğ’•ğ’‚ğ’‚Flat Region
ğ’•ğ’•ğ’‚ğ’‚<ğ‘®ğ‘®ğ‘ºğ‘º<ğ’•ğ’•ğŸğŸğ’‚ğ’‚
ğ’•ğ’•ğŸğŸğ’‚ğ’‚<ğ‘®ğ‘®ğ‘ºğ‘º<ğ’•ğ’•ğŸ‘ğŸ‘ğ’‚ğ’‚Figure 11: Edge-based fusion error. We present the region mask â„¦and pseudo-label GSbefore and
after quantile sampling. Different colors on the right represent the range of pixel values. Guiding
theâ„¦withGSensures that our model can predict balanced consistency and details by the simple
one-stage inference. The use of â„¦as a learnable soft mask achieves more fine-grained integration on
the feature level, enhancing the accuracy of Nr. This also leads to more accurate edge representation
GSin the iterative coarse-to-fine refinement process.
Shared
Encoder
Attention -based 
Feature Interaction
Ã—4
Conv
BlockWeight
ed
FusionFFMConv
Block
Conv
Block
Refined Depth MapFFMDecoderRefinement Network
ïğ’…ğ’…(ğ‘³ğ‘³)ïğ’…ğ’…(ğ‘¯ğ‘¯)
ğ‘­ğ‘­ğ’ğ’ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚ğ‘­ğ‘­ğ’‰ğ’‰ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚ğ’‚
Region
Mask
Conv
Block
Weighted
Fusion
Region
Mask
Weighted
Fusion
Figure 12: Architecture of refinement network. Some decoder layers are omitted for simplicity.
Edge-based Fusion Error. The proposed edge-based fusion loss aligns the data distribution of the
learnable region mask â„¦and the pseudo-label GSby quantile sampling (Sec 3.3, line 205, main
paper). Here, we provide additional visualizations for intuitive understanding. As shown in Fig. 11,
we visualize the soft region mask â„¦of high-frequency areas and the pseudo-label GSwith the same
color map in the second and third columns. The regions highlighted in GSwith stronger depth edges
and more detailed information naturally correspond to larger values in â„¦to emphasize features from
high-resolution inputs. We perform quantile sampling on â„¦andGS, as depicted in the fourth and
fifth columns. The legends on the right indicate the percentile ranking of the pixel values in the
whole image. Our edge-based fusion loss supervises that â„¦andGShave consistent distribution for
each color. In this way, â„¦tends to have smaller values in flat regions for more information from
low-resolution input, while the opposite is true in high-frequency regions. This is advantageous for
the model to balance the depth details and spatial structures.
A.3 Refinement Network
We provide the detailed model architecture of the refinement network Nr. As shown in Fig. 12,
the refinement network adopts the U-Net architecture similar to prior arts [ 25,3,14]. The depth
maps from the depth predictor Ndpredicted in different resolutions are up-sampled to a unified input
size. A shared Mit-b0 [ 50] serves as the encoder to extract feature maps of different resolutions.
The decoder gradually outputs the refined depth map with feature fusion modules (FFM) [ 22,23]
and skip connections. We make two technical improvements to the refinement network, including
attention-based feature interaction and adaptive weight allocation.
Attention-based Feature Interaction. To predict refined depth maps in high resolution ( e.g.,
2048Ã—2048 ), prior arts [ 25,3,14] adopt a U-Net with numerous layers ( e.g.,10layers or more) as
the refinement network for sufficient receptive field. This leads to heavy computational overhead. In
our case, we leverage the self-attention mechanism [41] to address this issue.
The features of low- and high-resolution inputs extracted by the encoder [ 50] are denoted as Fattn
l and
Fattn
h. We stack Fattn
l andFattn
h to obtain Finfor attention calculation. Positional embeddings [ 42]
16PEx,PEyare added to Finfor the height and width dimensions. An additional PEfis used to
distinguish the low- and high-resolution inputs. The attention-based feature interaction process can
be expressed as follows:
Fin= Stack( Fattn
l, Fattn
h) +PEx+PEy+PEf,
K=WkÂ·Fin, Q=WqÂ·Fin, V=WvÂ·Fin,
Fout= Softmax
KTQ/âˆš
d
V+Fin.(12)
Four attention layers are included in Nr. The interacted feature Foutis fed to the decoder to predict
refined depth. Attention-based feature interaction achieves large receptive field with fewer layers,
reducing model parameters and improving efficiency.
Adaptive Weight Allocation. The refinement network adopts adaptive weight allocation for the
fusion of low- and high-resolution features with the learnable mask â„¦. In each decoder layer, the
feature go through a convolutional block to generate â„¦with a single channel. The fused features F
(line 212, main paper) and the feature from the previous layer are fused by the FFM module [23].
A.4 Noise Implementation.
For our local inconsistency noise, we segment the ideal depth Dâˆ—into regular patches of size 64Ã—64,
with an overlap of half the patch size. Considering the depth discontinuities on the edges, instead
of applying a linear transformation to the entire patch, we extract the edges from Dâˆ—and apply a
linear transformation to each connected domain to simulate the local depth inconsistency. For edge
deformation noise, we first down-sample Dâˆ—to the inference resolution and then restore it to the
original resolution. Subsequently, we optimize a certain number of Gaussian distributions around the
edges of Dâˆ—to fit the edge deformation and blurring.
The local inconsistency noise and edge deformation noise can effectively model the degradation
of network prediction results compared to ideal depth maps. An additional experiment on the
Middlebury2021 [34] dataset also proves this point. We optimize the local inconsistency noise with
the least squares method and 50,000position-constrained Gaussian distributions as edge deformation
noise by gradient descent. The PSNR between the noisy depth (Dâˆ—+Ïµcons+Ïµedge)and model predicted
depth Dis over 40dB, which indicates that the difference between Dand(Dâˆ—+Ïµcons+Ïµedge)is
very small. The result further demonstrates that the noises can accurately model depth prediction
errors (Eq. 1, main paper), similar to the visualizations in Fig. 2 of the main text.
A.5 Broader Impacts and Limitations
Although SDDR works well in general, it still has limitations. For example, more advanced mecha-
nisms and structures can be explored for the refinement network in future work. For inputs under
conditions with specular surfaces, low light, or weak textures, the depth predictor tends to yield
sub-optimal results. Although SDDR improves upon these results, the outcomes are still not perfect.
Our approach exclusively utilizes publicly available datasets during the training process, thereby
having no broad societal impact, not involving AI ethics, and not involving any privacy-sensitive data.
B Detailed Experimental Settings
B.1 Datasets
Evaluation Datasets. We use five different benchmarks with diverse scenarios for comparisons. The
descriptions of our evaluation datasets are as follows:
â€¢Middlebury2021 [34] comprises 48 RGB-D pairs from 24 real indoor scenes for evaluating
stereo matching and depth refinement models. Each image in the dataset is annotated with dense
1920Ã—1080 disparity maps. We use the whole set of Middlebury2021 [34] for testing.
â€¢Multiscopic [52] includes a test set with 100synthetically generated indoor scenes. Each scene
consists of RGB images captured from 5different viewpoints, along with corresponding disparity
annotations. The resolution of images is 1280Ã—1080 . We adopt its official test set for testing.
17â€¢Hypersim [32] is a large-scale synthetic dataset. In our experiment, we follow the test set defined
by GBDF [ 3] for fair comparison, utilizing tone-mapped 286images generated by their released
code. Evaluation is performed using the corresponding 1024Ã—768depth annotations.
â€¢DIML [15] contains RGB-D frames from both Kinect v2 [ 55] and Zed stereo camera with different
resolutions. We conduct the generalization evaluation using the official test set, which includes real
indoor and outdoor scene images along with corresponding high-resolution depth annotations.
â€¢DIODE [40] contains high-quality 1024Ã—768LiDAR-generated depth maps of both indoor and
outdoor scenes. We use the whole validation set (771 images) for generalization testing.
Training Datasets. Our training data is sampled from diverse datasets, which can be categorized
into synthetic and natural-scene datasets. The synthetic datasets consist of TartanAir [ 45], Irs [ 44],
UnrealStereo4K [ 39] and MVS-Synth [ 11]. Among these, the resolutions of TartanAir [ 45] and
Irs [44] are below 1080p, while MVS-Synth [ 11] and UnrealStereo4K [ 39] reach resolutions of
1080p and 4k, respectively. Irs [ 44] and MVS-Synth [ 11] contain limited types of scenes, whereas
others include both indoor and outdoor scenes, some of which [ 45,39] present challenging conditions
like poor lighting. To enhance the generalization to natural scenes, we also sample from four
high-resolution real-world datasets, Holopix50K [ 10], iBims-1 [ 16], WSVD [ 43], and VDW [ 47].
IBims-1 [ 16] contains a small number of indoor scenes but provides high-precision depth annotations
from the capturing device. The remaining three datasets include large-scale diverse scenes, but their
depth annotations, obtained from stereo images [12], lack ideal edge precision.
B.2 Training Recipe
We leverage diverse training data to achieve strong generalizability. For each epoch, we randomly
choose 20,000images from natural-scene data [ 10,47,43,16] and 20,000images from synthetic
datasets [ 45,44,39,11]. For each sample, we adopt similar data processing and augmentation as
GBDF [ 3]. To enhance training stability, we first train Nrfor one epoch only with Lgt. In the next
two epochs, we involve Lgrad andLfusion for self-distillation. The aandNwinLfusion are set
to0.02and4. The learning rate is 1eâˆ’4.Î»1andÎ»2in Eq. 10 are 0.5and0.1. All training and
inference are conducted on a single NVIDIA A6000 GPU.
B.3 Evaluation Metrics
Depth Accuracy. Mdenotes numbers of pixels with valid depth annotations, while dianddâˆ—
iare
estimated and ground truth depth of pixel i. We adopt the widely-used depth metrics as follows:
â€¢Absolute relative error (Abs Rel):1
|M|P
dâˆˆM|dâˆ’dâˆ—|/dâˆ—;
â€¢Square relative error (Sq Rel):1
|M|P
dâˆˆMâˆ¥dâˆ’dâˆ—âˆ¥2/dâˆ—
â€¢Root mean square error (RMSE):q
1
|M|P
dâˆˆMâˆ¥dâˆ’dâˆ—âˆ¥2;
â€¢Mean absolute logarithmic error (log 10):1
|M|P
dâˆˆM|log (d)âˆ’log (dâˆ—)|;
â€¢Accuracy with threshold t: Percentage of disuch that
max(di
dâˆ—
i,dâˆ—
i
di) =Î´ < tâˆˆ
1.25,1.252,1.253
.
Edge Quality. For the edge quality, we follow prior arts [ 25,3,49] to employ the ordinal error
(ORD) and depth discontinuity disagreement ratio (D3R). The ORD metric is defined as:
ORD =1
NX
iÏ•(pi,0âˆ’pi,1),
Ï•(pi,0âˆ’pi,1) =log (1 + exp ( âˆ’l(pi,0âˆ’pi,1))), lÌ¸= 0,
(pi,0âˆ’pi,1)2, l = 0,
l=ï£±
ï£²
ï£³+1, pâˆ—
i,0/pâˆ—
i,1â‰¥1 +Ï„ ,
âˆ’1, pâˆ—
i,0/pâˆ—
i,1â‰¤1
1+Ï„,
0, otherwise ,(13)
18Method FLOPs ( G) Params ( M) Time ( s)
GBDF [3] 10.377 201 .338 0 .112
Kim et al. [14] 1138.342 61 .371 0 .128
Graph-GDSR [4] 397.355 32 .533 0 .832
Ours (one-stage) 16.733 16 .763 0 .035
Boost [25] 286.13Ã—63 79 .565 2 .183
PatchFusion [21] 810.813Ã—177 42 .511 5 .345
Ours (two-stage) 16.733Ã—30 16 .763 1 .050
Table 5: Model efficiency. We evaluate FLOPs, model parameters, and inference time of different
methods. The first four rows contain one-stage methods [ 3,14,4], while the last three rows are for
two-stage approaches [ 25,21]. FLOPs and inference time are tested on a 1024Ã—1024 image with
one NVIDIA RTX A6000 GPU. For the two-stage methods [ 25,21], their FLOPs are reported by
multiplying FLOPs per patch with the required patch numbers for processing the image.
0.2
0.205
0.21
0.215
0.22
0.225
0.230.89 0.8965 0.903 0.9
0.892 0.904 0.898
One -stageTwo -stage
PatchFusion
42M
Kimetal.
61MGBDF
201M
Graph -G
33MO
(one
1Our
(two -st
16M
Boost
80M
0.20 0.23 0.22 0.21Edge Quality (ORD)Accuracy (ğœ¹ğœ¹ğŸğŸ)
(b)Performance and Eff
0.240
0.234
0.2280.222
0.2160.862
0.860
0.858
0.856
0.854
0 10000 7500 5000 2500IterationAccuracy
Edge Error
Training IterationsDepth Accuracy ( ğ›…ğ›…ğŸğŸ)Edge Error ( ğƒğƒğŸ‘ğŸ‘ğ‘ğ‘)
Figure 13: Iterations for self-distillation. We report the depth accuracy and edge error metrics of
our SDDR model in the self-distillation training process.
Method Î´1â†‘ RELâ†“ORDâ†“D3Râ†“
Ours ( w/ D S)0.855 0 .129 0 .317 0 .237
Ours ( w/ G S)0.862 0.120 0.305 0.216
Table 6: Formats of Pseudo-labels. We compare the self-distilled training with refined depth DSand
depth edge representation GSas pseudo-labels. The experiment is conducted on Middlebury2021 [ 34]
dataset with LeReS [51] as the depth predictor.
where pi,0andpi,1represent pairs of edge-guided sampling points. pâˆ—
i,0andpâˆ—
i,1are the ground truth
values at corresponding positions. lis used to represent the relative ordinal relationship between pairs
of points. ORD characterizes the quality of depth edges by sampling pairs of points near extracted
edges using a ranking loss [ 49]. On the other hand, D3R[25] uses the centers of super-pixels
computed with the ground truth depth and compares neighboring super-pixel centroids across depth
discontinuities. It directly focuses on the accuracy of depth boundaries.
C More Experimental Results
C.1 Model Efficiency Comparisons.
In line 277 of the main paper, we mention that our method achieves higher model efficiency than prior
arts [ 4,3,14,25,21]. Here, we provide detailed comparisons of model efficiency in Table 5. For
one-stage methods [ 4,3,14], SDDR adopts a more lightweight refinement network, reducing model
parameters by 12.5times than GBDF [ 3] and improving inference speeds by 3.6times than Kim et
al.[14]. Compared with two-stage tile-based methods [ 25,21], our coarse-to-fine edge refinement
reduces the Flops per patch by 50.6times and the patch numbers by 5.9times than PatchFusion [ 21].
C.2 More Quantitative and Qualitative Results
Training Iterations of Self-distillation We investigate the iteration numbers of self-distillation in
Fig. 13. The iteration number of zero indicates the model after the training of the first epoch only with
19Predictor MethodDepth Edge
Abs Rel â†“Sq Rel â†“RMSE â†“log10â†“Î´1â†‘ Î´2â†‘ Î´3â†‘ ORDâ†“D3Râ†“
MiDaSMiDaS [30] 0.117 0 .576 3 .752 0 .052 0 .868 0 .973 0 .992 0 .384 0 .334
Kim et al. [14] 0.120 0 .562 3.558 0.053 0 .864 0 .973 0 .994 0 .377 0 .382
Graph-GDSR [4] 0.121 0 .566 3 .593 0 .053 0 .865 0 .973 0 .994 0 .380 0 .398
GBDF [3] 0.115 0 .561 3 .685 0 .052 0 .871 0 .973 0 .993 0 .305 0 .237
Ours 0.112 0.545 3.668 0.050 0.879 0.979 0.994 0.299 0.220
LeReSLeReS [51] 0.123 0 .464 3 .040 0 .052 0 .847 0 .969 0 .992 0 .326 0 .359
Kim et al. [14] 0.124 0 .474 3 .063 0 .052 0 .846 0 .969 0 .992 0 .328 0 .387
Graph-GDSR [4] 0.124 0 .467 3 .052 0 .052 0 .847 0 .969 0 .992 0 .327 0 .373
GBDF [3] 0.122 0.444 2.963 0.051 0 .852 0 .969 0 .992 0 .316 0 .258
Ours 0.120 0.452 2 .985 0.050 0.862 0.971 0.993 0.305 0.216
ZoedepthZoedepth [1] 0.104 0 .433 2 .724 0 .043 0 .900 0 .970 0 .993 0 .225 0 .208
Kim et al. [14] 0.107 0 .469 2 .766 0 .044 0 .896 0 .970 0 .992 0 .228 0 .243
Graph-GDSR [4] 0.103 0 .431 2 .725 0 .044 0 .901 0 .971 0 .993 0 .226 0 .233
GBDF [3] 0.105 0 .430 2 .732 0 .044 0 .899 0 .970 0 .993 0 .226 0 .200
Ours 0.100 0.406 2.674 0.042 0.905 0.973 0.994 0.218 0.187
Table 7: Comparisons with one-stage refinement approaches on Middlebury2021.
Predictor MethodDepth Edge
Abs Rel â†“Sq Rel â†“RMSE â†“log10â†“Î´1â†‘ Î´2â†‘ Î´3â†‘ ORDâ†“D3Râ†“
MiDaSMiDaS [30] 0.117 0 .576 3 .752 0 .052 0 .868 0 .973 0 .992 0 .384 0 .334
Boost [25] 0.118 0.544 3.758 0 .053 0 .870 0.979 0.997 0.351 0 .257
Ours 0.115 0.563 3.710 0.052 0.871 0.973 0 .993 0.303 0.248
LeReSLeReS [51] 0.123 0 .464 3 .040 0 .052 0 .847 0 .969 0.992 0.326 0 .359
Boost [25] 0.131 0 .487 3 .014 0 .054 0 .844 0 .960 0 .989 0 .325 0.202
Ours 0.123 0.459 3.005 0.052 0.861 0.969 0.991 0.309 0.214
ZoedepthZoedepth [1] 0.104 0 .433 2 .724 0 .043 0 .900 0 .970 0 .993 0 .225 0 .208
Patchfusion [21] 0.102 0 .385 2.406 0.042 0 .887 0 .977 0.997 0.211 0 .139
Boost [25] 0.099 0.349 2.502 0 .042 0 .911 0.979 0.995 0 .210 0 .140
Ours 0.096 0.350 2 .432 0.041 0.913 0.977 0 .995 0.202 0.125
Table 8: Comparisons with two-stage tile-based methods on Middlebury2021. PatchFusion [ 21]
can only adopt ZoeDepth [ 1] as the fixed baseline, while other approaches are reconfigurable and
pluggable for different depth predictors [1, 51, 30].
Dataset MethodDepth Edge
Abs Rel â†“Sq Rel â†“RMSE â†“log10â†“Î´1â†‘ Î´2â†‘ Î´3â†‘ ORDâ†“D3Râ†“
DIMLLeReS [51] 0.101 45 .607 325 .191 0 .043 0 .902 0 .990 0 .998 0 .242 0 .284
Kim et al. [14] 0.100 45 .554 325 .155 0 .042 0 .902 0 .990 0 .998 0 .243 0 .301
Graph-GDSR [4] 0.101 45 .993 326 .320 0 .043 0 .901 0 .989 0 .998 0 .243 0 .300
GBDF [3] 0.100 44 .038 318.874 0.042 0 .906 0.991 0.998 0 .239 0 .267
Boost [25] 0.108 50 .923 341 .992 0 .046 0 .897 0 .987 0 .998 0 .274 0 .438
Ours 0.098 41.328 320.193 0.042 0.926 0.990 0.998 0.221 0.230
DIODELeReS [51] 0.105 1 .642 9 .856 0 .041 0 .892 0 .968 0.989 0.324 0 .685
Kim et al. [14] 0.105 1 .654 9 .888 0 .044 0 .889 0 .964 0 .987 0 .325 0 .713
Graph-GDSR [4] 0.104 1 .626 9 .876 0 .044 0 .890 0 .967 0 .988 0 .326 0 .690
GBDF [3] 0.105 1 .625 9 .770 0.041 0.894 0 .968 0 .990 0 .322 0 .673
Boost [25] 0.105 1 .612 9 .879 0 .044 0 .892 0 .966 0 .987 0 .343 0 .640
Ours 0.098 1.529 9.549 0.042 0.900 0.968 0.988 0.293 0.637
Table 9: Comparisons with previous refinement approaches on DIML and DIODE.
ground truth for supervision, i.e., before self-distillation. Clearly, with the proposed self-distillation
paradigm, both the depth accuracy and edge quality are improved until convergence.
Formats of Pseudo-labels We compare the refined depth DSand the proposed depth edge represen-
tation GSas pseudo-labels. Using the accurate and meticulous depth DScould be a straightforward
idea. However, with depth maps as the supervision, the model cannot precisely focus on improving
edges and details. Thus, GSachieves stronger efficacy than DS, proving the necessity of our designs.
Quantitative Comparisons. In the main paper, only Î´1, REL, ORD, and D3Rare reported. Here, we
present the additional metrics of all the compared methods [ 14,4,3,25,21] on Middlebury2021 [ 34],
DIML [ 15], and DIODE [ 40] datasets in Table 7, Table 8, and Table 9. Our method outperforms
previous approaches on most evaluation metrics, showing the effectiveness of our SDDR framework.
Qualitative Comparison We provide more qualitative comparisons with one-stage [ 14,3] and
two-stage [ 21,25] methods in Fig. 14 and Fig. 15. These visual results further demonstrate the
excellent performance and generalization capability of SDDR on diverse scenes [34, 15, 49].
20RGB LeReS Kim et al.
 GBDF
 Ours
Figure 14: Qualitative comparisond with one-stage methods [ 14,3] on various datasets [ 15,49,
34]. We adopt LeReS [51] as the depth predictor. Better viewed when zoomed in.
RGB ZoeDepth PatchFusion
 Boost
 Ours
Figure 15: Qualitative comparisons with two-stage methods [ 21,25] on various datasets [ 15,49,
34]. We adopt Zoedepth [1] as the depth predictor. Better viewed when zoomed in.
21NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: The paper accurately conveys the contributions and scope of this work in the
abstract and introduction sections, and provides a bullet-point summary at the end.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The paper discusses the limitations of the method in Appendix A.5.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
22Justification: The paper does not include theoretical results.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: All experiments presented in this paper are reproducible. We will release the
code and model following the acceptance of the paper.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
23Answer: [No]
Justification: We will release the code and model after the acceptance of the paper.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We provide a detailed description of our experimental setup and results in
Sec. 4 of the main paper, as well as in Appendice B and C.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: Our experiments are stable across multiple runs.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
24â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: In Appendix C.1, we provide a detailed account of our computational overhead
and model efficiency.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The paper adheres to the NeurIPS Code of Ethics in all respects.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: In Appendix A.5, we elaborate on the lack of societal impact of our work.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
25â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks as elaborated in Appendix A.5.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The paper employs publicly available datasets and code for training and
comparative evaluation, adhering to all protocol restrictions that accompanied their release,
and cites the relevant literature.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
26â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Upon acceptance of the paper, we will release our model and code under the
CC BY-NC-SA 4.0 license.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
27