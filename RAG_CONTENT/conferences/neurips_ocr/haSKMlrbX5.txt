BoNBoN Alignment for Large Language Models
and the Sweetness of Best-of-n Sampling
Lin Gui1, Cristina G√¢rbacea2, and Victor Veitch1,2
1Department of Statistics, University of Chicago
2Data Science Institute, University of Chicago
Abstract
This paper concerns the problem of aligning samples from large language models
to human preferences using best-of- nsampling, where we draw nsamples, rank
them, and return the best one. We consider two fundamental problems. First:
what is the relationship between best-of- nand approaches to alignment that
train LLMs to output samples with a high expected reward (e.g., RLHF or DPO)?
To answer this, we embed both the best-of- ndistribution and the sampling
distributions learned by alignment procedures in a common class of tiltings of
the base LLM distribution. We then show that, within this class, best-of- nis
essentially optimal in terms of the trade-off between win-rate against the base
model vs KL distance from the base model. That is, best-of- nis the best choice
of alignment distribution if the goal is to maximize win rate. However, best-of- n
requires drawing nsamples for each inference, a substantial cost. To avoid
this, the second problem we consider is how to fine-tune a LLM to mimic the
best-of- nsampling distribution. We derive BoNBoN Alignment to achieve this by
exploiting the special structure of the best-of- ndistribution. Experiments show
that BoNBoN alignment yields substantial improvements in producing a model
that is preferred to the base policy while minimally affecting off-target aspects.
Code is available at https://github.com/gl-ybnbxb/BoNBoN .
1 Introduction
This paper concerns the problem of aligning large language models (LLMs) to bias their outputs
toward human preferences. There are now a wealth of approaches to this problem [e.g., Ouy +22;
Chr+17; Kau +23; Li +24; Raf +23; Aza +24]. Here, we interested in the best-of- n(BoN) sampling
strategy. In BoN sampling, we draw nsamples from the LLM, rank them on the attribute of
interest, and return the best one. This simple procedure is surprisingly effective in practice
[Bei+24; Wan +24; GSH23; Eis +23]. We consider two fundamental questions about BoN:
1. What is the relationship between BoN and other approaches to alignment?
2. How can we effectively train a LLM to mimic the BoN sampling distribution?
In brief: we find that the BoN distribution is (essentially) the optimal policy for maximizing
win rate while minimally affecting off-target aspects of generation, and we develop an effective
method for aligning LLMs to mimic this distribution. Together, these results yield a highly effective
alignment method; see Figure 1 for an illustration.
LLM Alignment The goal of alignment is to bias the outputs of an LLM to be good on some
target attribute (e.g., helpfulness), while minimally changing the behavior of the model on
off-target attributes (e.g., reasoning ability). Commonly, the notion of goodness is elicited by
collecting pairs of responses to many prompts, and asking (human or AI) annotators to choose the
better response. Then, these pairs are used to define a training procedure for updating the base
LLM to a new, aligned, LLM that outputs responses that are better in the target attribute.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).5060708090
60 80 100 120 140 160
Average Length of ResponsesWin Rate (%) Method
BoN sampling
BoNBoN
DPO original HH
IPO or iginal HH
Reference ModelAverage Length (Response) v s. Win Rate
+XPDQZKDWLVSOXV
$VVLVWDQW¬§∆ò≈±≈£∆ï∆¨
≈±t≈±t
S¬§¬Ä»¨NN
%¬§¬Ä»¨NNLV
SOXVLVZKLFKLVWKHQXPEHUSOXV7KLVDQVZHULV
VLPSOHRUGLQDODGGLWLRQ2UGLQDODGGLWLRQLVXVHGLQEDVLFPDWK
RSHUDWLRQVOLNHDGGLWLRQDQGVXEWUDFWLRQEXWLW
VDOVRXVHGZLWK
RWKHUPDWKHPDWLFDOSULQFLSOHVOLNHWKHSURSHUW\RIDOZD\V
DGGLQJE\SRZHUVRIWHQ7RILQGWKHVXPRIDQG\RXZRXOG
GRSOXVRU7KLVFDQDOVREHZULWWHQ ,I\RX
ZDQWWRDGGXSDODUJHUQXPEHUSOXVPRUHWLPHV\RXZRXOGGR
SOXVSOXVSOXVSOXVSOXVSOXVSOXVSOXV7KLV
ZRXOGUHVXOWLQH[DFWO\WKHVXPSOXV
7KHQXPEHULVSOXVZKLFKHTXDOV7KLVQXPEHULV
FRPPRQO\FDOOHGWKHQXPEHU7KHDQGDUHFDOOHGWKH
DGGHQGDQGPXOWLSOLHURUWKHDGGHQGDQGPXOWLSOLHURIWKH
QXPEHU7KHVHIDFWRUVDUHFRPPRQO\XVHGLQFRXQWLQJQXPEHUV
DQGH[SUHVVLQJQXPEHUVDVVXPVDQGPXOWLSOHVRIRWKHUQXPEHUV 
?Q?Q,W¬∑VJRRGSUDFWLFHWREHH[SOLFLWWKDW\RX¬∑UHFRXQWLQJXSZKHQ
\RXXVHWKHQXPEHUDQGWKDW\RX¬∑UHDGGLQJZKHQ\RXDGG
WRWKHQXPEHU2WKHUFRPPRQZD\VRIDGGLQJXSQXPEHUVLQFOXGH
DGGLQJLQGLYLGXDOGLJLWVRUJURXSVRIGLJLWV)RUH[DPSOH 
FRXOGEHZULWWHQDVRURURURU6LPLODUO\ 
FRXOGEHZULWWHQDVRURU.HHSLQPLQGWKDWWKHVH
ZRUGLQJVFDQEHFRQIXVLQJVRLW¬∑VXVXDOO\EHVWWREHH[SOLFLWZKHQ
DGGLQJXSQXPEHUVOLNHWKLV
√≤∆†ƒù»¨r≈±ƒñƒù≈öSOXVLVFigure 1: BoNBoN alignment achieves high win rates while minimally affecting off-target attributes of
generation. Left: Average length of responses versus win rate of models aligned using each method on the
Anthropic helpful and harmless single turn dialogue task, using n=8. As predicted by theory, best-of- n
achieves an excellent win rate while minimally affecting the off-target attribute length. Moreover, the
BoNBoN aligned model effectively mimics this optimal policy, achieving a much higher win rate at low
off-target drift than other alignment approaches. Right : Sample responses from models with similar win
rates to BoNBoN. Other methods require higher off-target deviation to achieve a comparably high win
rate. We observe that this significantly changes their behavior on off-target aspects. Conversely, BoNBoN
only minimally changes off-target behavior. See section 5 for details.
There are two main approaches. First, RLHF methods train an explicit reward model on the pairs,
and then align the model using reinforcement learning with this learned reward [e.g., Ouy +22;
Kau+23]. Second, contrastive methods directly use the preference data to define an objective
function for fine-tuning the LLM [Raf+23; Aza +24; Eth +24; Xu +24; HLT24 ]. In both cases,
the trade-off between alignment and off-target behavior is controlled by a hyper-parameter that
explicitly penalizes the divergence from the base LLM. For example, in the reinforcement learning
setting, this is done by adding a regularization term that penalizes the estimated KL divergence
between the aligned model and the reference model.
The first main question we address in this paper is: what is the relationship between the sampling
distribution defined by these approaches and the sampling distribution defined by best-of- n? This
is important, in particular, because in principle we could forgo the explicit alignment training and
just use BoN sampling. However, it is not clear when each option should be preferred.
Now, the comparison of training-aligned models and BoN is not fully fair. The reason is that
producing a BoN sample requires drawing nsamples from the base LLM (instead of just one).
This is a substantial computational cost. The second main question we address is: if we do in fact
want to sample from the BoN distribution, how can we train a LLM to mimic this distribution? If
this can be done effectively, then the inference cost of BoN sampling can be avoided.
We answer these questions with the following contributions:
1.We show that the BoN sampling distribution can be embedded in a common class with
the distributions produced by training-based alignment methods. Within this common
class, we derive the distribution with the best possible trade-off between win-rate against
the base model vs KL distance from the base model. Then, we show that the BoN
distribution is essentially equal to this Pareto-optimal distribution.
2.We then develop an effective method for training a LLM to mimic the BoN sampling
distribution. In essence, the procedure draws best-of- nand worst-of- nsamples as
training data, and combines these with an objective function we derive by exploiting the
analytical form of the BoN distribution. We call this procedure BoNBoN Alignment .
3.Finally, we show empirically that BoNBoN Alignment yields models that achieve high
win rates while minimally affecting off-target aspects of the generations, outperforming
baselines.
22 Preliminaries
Given a prompt x, a large language model (LLM) samples a text completion Y. We denote the
LLM byœÄand the sampling distribution of the completions by œÄ(y|x).
Most approaches to alignment begin with a supervised fine-tuning step where the LLM is trained
with the ordinary next-word prediction task on example data illustrating the target behavior. We
denote the resulting model by œÄ0, and call it the reference model . The problem we are interested
in is how to further align this model.
To define the goal, we begin with some (unknown, ground truth) reward function r(x,y)that
measures the quality of a completion yfor a prompt x. The reward relates to preferences in
the sense that y1is preferred to y0if and only if r(x,y1)>r(x,y0). Informally, the goal is to
produce a LLM œÄrwhere the samples have high reward, but are otherwise similar to the reference
model.
The intuitive requirement that the aligned model should be similar to the reference model is
usually formalized in terms of KL divergence. The context-conditional KL divergence andthe KL
divergence from œÄrtoœÄ0on a prompt set D are defined as:
DKL(œÄr‚à•œÄ0|x):=Ey‚àºœÄr(y|x)¬ï
log¬ÅœÄr(y|x)
œÄ0(y|x)¬ã¬ò
,
DKL(œÄr‚à•œÄ0):=Ex‚àºD[DKL(œÄr‚à•œÄ0|x)].
We also need to define what it means for samples from the language model to have high reward.
Naively, we could just look at the expected reward of the samples. However, in the (typical)
case where we only have access to the reward through preference judgements, the reward is
only identified up to monotone transformation. The issue is that expected reward value is
not compatible with this unidentifiability.1Instead, we consider the win rate of the aligned
model against the reference model. The idea is, for a given prompt, draw a sample from the
aligned model and a sample from the reference model, and see which is preferred. This can be
mathematically formalized by defining the context-conditional win rate andthe overall win rate
on a prompt set D :
pœÄr‚âªœÄ0|x:=PY‚àºœÄr(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0)),
pœÄr‚âªœÄ0:=Ex‚àºD
PY‚àºœÄr(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0))
.
Reinforcement Learning from Human Feedback (RLHF) The most studied approach to
alignment is RLHF . This procedure follows two steps. First, the reward function is explicitly
estimated from preference data, using the Bradley-Terry [BT52 ]model. Second, this estimated
reward function is used in a KL-regularized reinforcement learning procedure to update the
LLM. Denoting the estimated reward function by ÀÜr, the objective function for the reinforcement
learning step is:
LRLHF (œÄŒ∏;œÄ0) =‚àíEx‚àºD,y‚àºœÄŒ∏(y|x)[ÀÜr(x,y)] +Œ≤DKL(œÄŒ∏‚à•œÄ0), (2.1)
where Dis a prompt set and Œ≤is a hyper-parameter to control the deviation of œÄŒ∏from the
reference model œÄ0. The policyœÄris learned by finding the minimizer of the objective function
in (2.1); e.g., using PPO [Sch+17].
Contrastive methods Contrastive methods use the preference data D={(x,yw,yl)}where
xis the prompt, and ywandylare preferred and dis-preferred responses, directly to define an
objective function for fine-tuning the LLM, avoiding explicitly estimating the reward function.
For example, the DPO [Raf+23]objective is:
LDPO(œÄŒ∏;œÄ0) =‚àíE(x,yw,yl)‚àºD¬ï
logœÉ¬Å
Œ≤logœÄŒ∏(yw|x)
œÄ0(yw|x)‚àíŒ≤logœÄŒ∏(yl|x)
œÄ0(yl|x)¬ã¬ò
. (2.2)
The aligned model is found by optimizing this objective directly (via gradient descent).
1Fundamentally, the expectation of the transformed reward is not the reward of the transformed
expectation.
3Bradley-Terry and Alignment Targets In RLHF , the reward function is estimated using the
Bradley-Terry model, which relates noisy observed preferences to rewards by:
P(y1‚âªy0|x) =œÉ(r(x,y1)‚àír(x,y0)), (2.3)
whereœÉ(¬∑)is the sigmoid function. In the particular case that the Bradley-Terry model is well-
specified, then it can be shown that the analytic solution to both (2.1) and (2.2) is:
œÄRLHF
r(y|x)‚àùexp¬ß1
Œ≤r(x,y)¬™
œÄ0(y|x). (2.4)
That is, the alignment procedures target an exponential tilting of the reference model by the
reward function. Of course, it is not obvious when the Bradley-Terry model is well-specified,
nor whether this particular tilting is a desirable target. Other works have considered explicitly
or implicitly transforming the reward function to change the target distribution [Wan+24;
Aza+24]. Nevertheless, these works also take the target distribution to be a tilting of the
reference distribution.
Best-of- nsampling The best-of- nprocedure is as follows. Given a prompt x, sample
y1,y2, . . . , ynindependently from the reference model œÄ0(y|x). Then, select the response
with the highest reward r(x,yi)as the final response. That is,
y=yi such that r(x,yi) =max
1‚â§j‚â§nr(x,yj). (2.5)
3 Best-of- nis Win-Rate vs KL Optimal
The first question we address is: what is the relationship between the best-of- ndistribution, and
the distribution induced by training-based alignment methods?
3.1 A Common Setting for Alignment Policies
We begin with the underlying distribution of best-of- nsampling. Let Qxdenote the cumulative
distribution function of r(x,Y0), where Y0‚àºœÄ0(¬∑|x). Suppose r(x,¬∑):Y‚ÜíRis an one-to-one
mapping andœÄ0(y|x)is continuous2, then the conditional density of the best-of-n policy is
œÄ(n)
r(y|x):=nQx(r(x,y))n‚àí1œÄ0(y|x). (3.1)
Compare this to the RLHF policy œÄRLHF
rin (2.4). In both cases, the sampling distribution is a
re-weighted version of the reference model œÄ0, where higher weights are added to those responses
with higher rewards. The observation is that both of these distributions‚Äîand most alignment
policies‚Äîcan be embedded in a larger class of reward-weighted models. For any prompt xand
reward model r, we can define the fx-aligned model as:
œÄr(y|x)‚àùfx(r(x,y))œÄ0(y|x), (3.2)
where fxis a non-decreasing function that may vary across different prompts.
With this observation in hand, we can directly compare different alignment strategies, and
best-of- nin particular, by considering the function fxdefining the alignment policy.
3.2 Optimality: Win Rate versus KL divergence
To understand when different alignment policies are preferable, we need to connect the choice
offxwith a pragmatic criteria for alignment. The high-level goal is to produce a policy that
samples high-reward responses while avoiding changing off-target attributes of the text. A natural
formalization of this goal is to maximize the win rate against the reference model while keeping
the KL divergence low.
2This is a reasonable simplification since we only care about the distribution of r(x,y)in the one-
dimensional space and we consider the scenario where diverse responses without a dominant one are
expected. More details refer to appendix B for discussion.
40.50.60.70.80.91.0
0 1 2 3 4
KL Div ergenceWin RateKL Div ergence versus Win Rate
0.00000.00250.00500.00750.0100
2 4 6 8 10
nWin Rate GainWin Rate Gain from Optimal Policy over Best‚àíof‚àín Sampling
Figure 2: The BoN is essentially the same as the optimal policy in terms of win rate versus KL divergence.
Left: The win rate versus KL divergence curves of BoN and optimal policy. Right : The win rate difference
between optimal policy and BoN policy for different n.
Optimal Policy Our aim is to find the policy with the highest possible win rate at each KL
divergence level:
max
œÄEx‚àºD
PY‚àºœÄ(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0))
subject to DKL(œÄ‚à•œÄ0) =d.(3.3)
Now, this equation only depends on Ythrough the reward function r(x,y). Defining Qx(r(x,Y))
as the distribution of r(x,Y)underœÄ0(Y|x), we can rewrite the objective as:
max
œÄEx‚àºD,y‚àºœÄ(y|x)[Qx(r(x,y))]subject to DKL(œÄ‚à•œÄ0) =d,
By duality theory [BTN01 ], there is some constant Œ≤>0such that this problem is equivalent
to:
max
œÄEx‚àºD,y‚àºœÄ(y|x)[Qx(r(x,y))]‚àíŒ≤(DKL(œÄ‚à•œÄ0)‚àíd). (3.4)
Now, we can immediately recognize this objective as the same as the RLHF objective in (2.1)
with the transformed reward function Àúr(x,y) =Qx(r(x,y)). Then, the analytic solution to this
problem is
œÄoptimal
r‚àùœÄ0(y|x)ecQx(r(x,y)), (3.5)
where cis a constant determined by the KL divergence penalty.
The following theorem makes the preceding argument precise. To simplify the argument, we
will assume that the rewards assigned to outputs of the language model are continuous. This
simplifying assumption ignores that there are only a countably infinite number of possible
responses to any given prompt. However, given the vast number of possible responses, the
assumption is mild in practice. Refer to appendix B for a more detailed discussion.
Theorem 1. LetœÄoptimal
r,cbe the solution to (3.3). Then, for all x, the density of the optimal policy is
œÄoptimal
r,c(y|x) =œÄ0(y|x)exp{cQx(r(x,y))}/Zc
r, (3.6)
where Zc
ris the normalizing constant, and c is a positive constant such that
(c‚àí1)ec+1
ec‚àí1‚àílog¬Åec‚àí1
c¬ã
=d. (3.7)
Furthermore, the context-conditional win rate and KL divergence of this optimal policy are
1. Context-conditional win rate: pœÄoptimal
r,c‚âªœÄ0|x=(c‚àí1)ec+1
c(ec‚àí1).
2. Context-conditional KL divergence: DKL¬Ä
œÄoptimal
r,c‚à•œÄ0|x¬ä
=(c‚àí1)ec+1
ec‚àí1‚àílog ec‚àí1
c
.
Since for any prompt x, both the context conditional win rate and KL divergence are constants, the
overall win rate pœÄoptimal
r,c‚âªœÄ0and KL divergence DKL(œÄoptimal
r,c‚à•œÄ0)on any prompt set Dare also these
values. [Proof ].
53.3 The best-of- npolicy is essentially optimal
Now, we‚Äôd like to use the previous result to understand when the best-of- npolicy is desirable.
The win rate and KL divergence can be calculated with essentially the same derivation:
Theorem 2. The context-conditional win rate and KL divergence of the best-of-n policy are:
1. Context-conditional win rate: pœÄ(n)
r‚âªœÄ0|x=n
n+1.
2.[JH22 ]Context-conditional KL divergence: DKL 
œÄ(n)
r‚à•œÄ0|x
=log(n)‚àín‚àí1
n.3
Since both are constants, the overall win rate pœÄ(n)
r‚âªœÄ0and Kl divergence DKL(œÄ(n)
r‚à•œÄ0)on any
prompts set D are the same values. [Proof ].
We now can contrast the win-rate vs KL frontier of the best-of- npolicy with the optimal policy.
Figure 2 shows KL divergence versus win rate values of best-of- npolicy and the optimal policy.
The maximum difference in win rates (at n=2) is less than 1 percentage point. Larger values of
napproximate the optimal policy even more closely. In summary:
The best-of- npolicy is essentially optimal in terms of win rate versus KL divergence.
3.4 Implicit vs Explicit KL regularization
RLHF and contrastive alignment methods include a hyper-parameter that attempts to explicitly
control the trade-off between KL divergence and model reward. By contrast, best-of- nonly
controls the KL drift implicitly. This can actually be a substantive advantage. There are two reasons.
First, it is generally unclear how well controlling KL actually captures the real requirement of
controlling the degree to which off-target attributes of the text are modified. There might be
multiple possible policies with a fixed KL level that have radically different qualitative behavior.
Second, in practice, the KL drift from the base policy needs to be estimated from a finite data
sample. This may be extremely difficult‚Äîit is a very high dimensional estimation problem.
Mis-estimation of the KL is particularly problematic when we are explicitly optimizing against
the estimate, because this may let the optimizer exploit mis-estimation. Empirically, we find that
measured KL can have a poor correspondence with attributes of text that humans would judge
to be salient (see section 5). In particular, we find large variation in response length that is not
reflected in estimated KL.
The best-of- nprocedure avoids both problems, since it avoids the need to estimate the KL drift,
and since it does not explicitly optimize against the KL drift.
4 BoNBoN: Best-of- nfine tuning
From section 3, we know that the best-of- npolicy is essentially optimal in terms of win rate
and KL divergence. Accordingly, it is often a good choice for the alignment policy. However,
the best-of- npolicy has a significant practical drawback: it requires drawing nsamples for each
inference. This is a substantial computational expense. We now turn to developing a method
to train a language model to mimic the best-of- nsampling distribution. We call this method
BoNBoN Alignment .
Setup The basic strategy here will be to use best-of- nsamples to train a language model to
mimic the best-of- npolicy. We produce the training data by sampling nresponses from the
reference model œÄ0, and ranking them. The best and worst data are the samples with highest and
lowest reward. Their corresponding best-of and worst-of nsampling distributions are denoted as
œÄ(n)
randœÄ(1)
r. The task is then to set up an optimization problem using this sampled data such
that the solution approximates the best-of- npolicy. To that end, we consider objective functions
that have the best-of- npolicy as a minimizer in the infinite data limit. (In practice, as usual, we
approximate the expectation with an average.)
3Beirami et al. [Bei+24]discuss that since the distribution of the language model is discrete, œÄ(n)
rhas a
different form from that in Theorem 2, and the actual KL divergence is smaller. However, due to the large
cardinality of the corpus and the low probability of each response, the actual density is very close to (3.1)
and the KL divergence is almost its upper bound log (n)‚àín‚àí1
n.
6SFT-BoN. The most obvious option is to train the model to maximize the log-likelihood of the
best-of- nsamples. The associated objective is:
LSFT‚àíBoN(œÄŒ∏;œÄ0) =‚àíEx‚àºD,y(n)‚àºœÄ(n)
r
logœÄŒ∏(y(n)|x)
, (4.1)
and it is well-known that the minimizer is œÄ(n)
r. The training procedure is simply to minimize
the sample-average version of this objective. We call this training method SFT-BoN because it
is supervised fine-tuning on best-of- nsamples. Although SFT-BoN is valid theoretically, it turns
out to be data inefficient, and we observe only marginal improvement over the reference model
empirically (see section 5).
IPO-BoN. A limitation of the best-of- nprocedure is that it only makes use of the winning sample,
throwing away the rest. Another intuitive option is to construct a pairwise dataset and train the
language model by a contrastive method. Concretely, we construct the pairwise data by picking
the best and worst responses. We want to construct an objective function using this paired data
that has the best-of- npolicy as a minimizer.
The key result we require is:
Theorem 3. For any fixed n,
Ex‚àºD,y(n)‚àºœÄ(n)
r,y(1)‚àºœÄ(1)
r¬ñ
logœÄ(n)
r(y(n)|x)
œÄ(n)
r(y(1)|x)‚àílogœÄ0(y(n)|x)
œÄ0(y(1)|x)¬ô
=1
2Œ≤‚àó
n,
where
Œ≤‚àó
n=1
2(n‚àí1)Pn‚àí1
k=11/k. (4.2)
[Proof ].
Following this result, we define the contrastive objective function as:
LIPO‚àíBoN(œÄŒ∏;œÄ0) =Ex‚àºD,y(n)‚àºœÄ(n)
r,y(1)‚àºœÄ(1)
r¬ñ
logœÄŒ∏(y(n)|x)
œÄŒ∏(y(1)|x)‚àílogœÄ0(y(n)|x)
œÄ0(y(1)|x)‚àí1
2Œ≤‚àó
n2¬ô
. (4.3)
The optimizer of this objective is a policy where the log-likelihood ratio of the best and worst
samples is equal to that of the best-of- npolicy. We call this training method IPO-BoN because
it is essentially the IPO objective on the best-and-worst samples, with a particular choice for
the IPO hyper parameter. We emphasize that the IPO-BoN objective does not involve any hyper
parameters, there is only one choice for Œ≤‚àó
nfor each n.
We find in section 5 that IPO-BoN is much more data efficient than the SFT-BoN. However, this
method (like IPO) has the disadvantage that it only controls the likelihood ratios on the sampled
data. In particular, this means that the optimizer can cheat by reducing the likelihood of both
the winning and losing responses, so long as the loser‚Äôs likelihood decreases more (so the ratio
still goes up). Reducing the probability of both the winning and losing examples requires the
optimized model to shift probability mass elsewhere. In practice, we find that it tends to increase
the probability of very long responses.
BonBon Alignment We can now write the BoNBoN objective:
TheBoNBoN alignment objective is:
LBoNBoN (œÄŒ∏;œÄ0) =Œ±LSFT‚àíBoN(œÄŒ∏;œÄ0) + (1‚àíŒ±)LIPO‚àíBoN(œÄŒ∏;œÄ0), (4.4)
whereLSFT‚àíBoNandLIPO‚àíBoNare defined in (4.1) and (4.3), and Œ±is a hyper parameter
that balances the SFT and the IPO objectives.
We call the procedure BoNBoN because it is a combination of two objective functions that have the
best-of- npolicy as a minimizer. Relative to SFT alone, BoNBoN can be understood as improving
data efficiency by making use of the worst-of- nsamples. Relative to IPO alone, BoNBoN can
be understood as preventing cheating by forcing the likelihood of the best-of- nsamples to be
7high. We emphasize that both objective functions target the same policy; neither is regularizing
towards some conflicting objective. That is, the trade-off between win-rate and off-target change
is handled implicitly by the (optimal) best-of- nprocedure. This is in contrast to approaches
that manage this trade-off explicitly (and sub-optimally) by regularizing towards the reference
model. Reflecting this, we choose Œ±so that the contribution of each term to the total loss is
approximately equal.
5 Experiments
5.1 Experimental Setup
We study two tasks: a) single-turn dialogue generation , for which we conduct experiments on the
Anthropic Helpful and Harmless (HH) dataset [Bai+22]andb) text summarization , for which we
use the OpenAI TL;DR dataset [Sti+20]. Due to computational constraints, we filter the HH data
to only keep prompts for which response length is less than 500 characters, resulting in 106,754
training dialogues. For TL;DR dataset, we discard instances where the input post length is less
than 90 characters, resulting in 92,831 (14,764 prompts) training posts. Each example in both
datasets contains a pair of responses that were generated by a large language model along with
a label denoting the human-preferred response among the two generations.
We want to compare different alignment methods on their ground truth win rate. Accordingly,
we need a ground truth ranker. To that end, we construct data by using an off-the-shelf reward
model4as our ground truth. (In particular, we relabel the human preferences).
As the reference model, we fine-tune Pythia-2.8b [Bid+23]with supervised fine-tuning (SFT) on
the human-preferred completions from each dataset. For alignment methods other than BoNBoN,
we draw n=8completions for each prompt, and we use the best and worst completions as
training data for them. For BoNBoN, we vary nfrom 2 to 8.
We use DPO and IPO as baselines for the alignment task. We run both procedures on both the
original (Anthropic HH or OpenAI summarization) datasets, and on the best-and-worst-of-8
completions. The former gives a baseline for performance using stronger responses, the latter
gives a baseline for using exactly the same data as BoNBoN. Both IPO and DPO include a hyper
parameterŒ≤controlling regularization towards the reference model. We report results for each
method run with several values of Œ≤. For BoNBoN, we use Œ±=0.005for all experiments. This
value is chosen so that the SFT and IPO terms in the loss have approximately equal contribution.
Further details can be found in appendix C.
5.2 BoNBoN achieves high win rate with little off-target deviation
We are interested in the win-rate vs off-target deviation trade-off. We measure off-target deviation
in two ways: (1) the estimated KL divergence from the base model, and (2) the average length of
model responses. Length is noteworthy because it is readily salient to humans but (as we see in the
results) alignment methods can change it dramatically, and it is not well captured by the estimated
KL divergence. We show win-rate vs off-target behavior for each trained model in Figure 3. The
main observation is that BoNBoN achieves a much better win-rate vs off-target tradeoff than any
other approach. In particular, DPO /IPOŒ≤values that achieve comparable win-rates result in
high off-target deviation‚Äîe.g., nearly doubling the average response length!
To further explore this point, we examine sample responses from baseline models with similar
win-rates to BoNBoN. Examples are shown in Figure 1 and tables 1 and 6. Other approaches can
dramatically change off-target behavior.
5.3 BoNBoN mimics the best-of- npolicy
Figure 3 shows SFT and IPO fine-tuned on the best-of- ndata. We observe that BoNBoN dramati-
cally outperforms these methods at all values of Œ≤, and is closer to the (optimal) BoN distribution.
This shows, in particular, the combined loss is in indeed key to the success of BoNBoN.
One substantial practical advantage of BoNBoN is that it is nearly hyper-parameter free. Because
the goal is to mimic the best-of- ndistribution, which is known to be optimal, we do not need to
sweep hyper-parameters for the ‚Äòbest‚Äô choice of win-rate vs KL. In particular, the Œ≤term in IPO
is analytically derived in Theorem 3. In Figure 5 we show the win rate vs off-target behavior
4https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2
85060708090100
0 10 20 30 40
KL Div ergenceWin Rate (%)KL Div ergence v s. Win Rate
5060708090
75 100 125 150
Average Length of ResponsesWin Rate (%)Average Length (Response) v s. Win Rate
Summarization
5060708090100
0 20 40 60
KL Div ergenceWin Rate (%)KL Div ergence v s. Win Rate
5060708090
40 80 120 160
Average Length of ResponsesWin Rate (%)Average Length (Response) v s. Win Rate
5060708090100
0 10 20 30 40
KL Div ergenceWin Rate (%)KL Div ergence v s. Win Rate
5060708090
75 100 125 150
Average Length of ResponsesWin Rate (%)Average Length (Response) v s. Win Rate
Helpful and HarmlessFigure 3: BoNBoN achieves high win-rates while minimally affecting off-target aspects of generation.
Each point is a model aligned with the indicated method. We measure win-rate against the base model
using the ground truth ranker. To assess change in off-target behavior, we measure both estimated KL
divergence (left) and average response length (right). Above : Comparison of BoNBoN with baselines for
the summarization task. Below : Comparison of BoNBoN with baselines for the single-dialogue task.
Prompt: Human: list all dirty word
Assistant:
Reference how about this?
BoNBoN (n =8) I‚Äôm afraid that‚Äôs not something I support.
DPO original HHThe word ‚Äúdirty‚Äù is considered a very derogatory word. It‚Äôs not something to say
to another person. I‚Äôm sorry that you heard that term being used in that way.
IPO original HHI‚Äôm sorry, I don‚Äôt support using offensive language, and I‚Äôm not aware of any specific
language that would be considered ‚Äòdirty‚Äô. Someone seeking to use an offensive word
might feel they were being misunderstood, or offended, but the truth is that no one
understands why some words are considered offensive. I hope this was helpful!
Table 1: With similar win rates, only BoNBoN does not modify the off-target attributes. The responses of
the same prompt are drawn from models fine tuned by BoNBoN, DPO and IPO on the original HH data
with no sampling technique. The win rate of each model is around 85%.
for several other choices for Œ≤in the IPO term. We observe that, generally, the default Œ≤‚àó
nhas
an excellent win-rate vs off-target trade-off. Accordingly, using the analytic solution appears to
avoid the need for any hyper-parameter tuning.
6 Discussion and Related work
Best-of-n BoN sampling is widely used for LLMs [e.g., Sti +20; Nak +21; Liu +23; Gul +23;
Tou+23; GSH23 ]. Due to its practical importance, it has also attracted some recent theoretical
attention [e.g., Mud +23; Bei +24; Yan +24; Jin +24]. Beirami et al. [Bei+24]show a closed form
probability mass function of the BoN policy in discrete case and provide a new KL estimator for
9it. Yang et al. [Yan+24]define the optimality in terms of minimizing the cross entropy given an
upper bounded KL, and show that BoN is asymptotically equivalent to the optimal policy, which is
in line with our findings. In totality, this line of work supports the use of best-of- nand motivates
techniques (like BoNBoN) that amortize the associated sampling cost.
Fine-tuning using best-of- ndata has also been tried in many existing works to align LLMs with
human reference. Dong et al. [Don+23]and Xiong et al. [Xio+23]apply best-of- nas training
data and fine-tune the LLMs with different fine-tuning methods like supervised fine-tuning and
iterative DPO. Touvron et al. [Tou+23]draw best-of- nsamples and do gradient updates in the
iterative fine-tuning step to further reinforce the human-aligned reward.
LLM alignment There is extensive literature on aligning LLMs [e.g., Zie +19; YK21; Qin +22;
She+23; Wan +23; Mud +23; Ouy +22; Zha +23; Raf +23; Yua +23; Aza +24; Eth +24; Xu +24;
HLT24; Wan +24; Liu +24; Par +24]. Broadly, this work uses preference-labelled data to (implic-
itly) define a goal for alignment and optimizes towards it while regularizing to avoid excessively
changing off-target behavior. Relative to this line of work, this paper makes two main contribu-
tions. First, we embed the best-of- npolicy into the general alignment framework, showing it is
optimal in terms of win-rate vs KL. Second, we derive BoNBoN alignment as a way of training an
LLM to mimic the best-of- ndistribution. Notice that this second goal is a significant departure
from previous alignment approaches that define the target policy through an objective that
explicitly trades off between high-reward and changes on off-target attributes. We do not have
any regularization towards the reference model. This has some significant practical advantages.
First, we do not need to estimate the divergence from the reference model. As we saw in section 5,
estimated KL can fail to capture large changes in response length, and thus mis-estimate the
actual amount of off-target deviation. Second, we do not need to search for a hyper-parameter
that balances the conflicting goals. This hyper-parameter search is a significant challenge in
existing alignment methods. (We do need to select Œ±, but this is easier since the aim is just to
balance the loss terms rather than controlling a trade-off in the final solution.)
Alignment methods can be divided into those that operate online‚Äîin the sense of drawing
samples as part of the optimization procedure‚Äîand offline. The online methods are vastly
more computationally costly and involve complex and often unstable RL-type optimization
procedures [Zhe+23; San +23]. However, the online methods seem to have considerably better
performance [e.g., Tan +24]. The results in the present paper suggest this gap may be artificial.
Theoretically, we have shown that best-of- nis already essentially the optimal policy, and this
policy can be learned with an offline-type learning procedure. Empirically, we saw in section 4
that BoNBoN vastly outperforms the IPO and DPO baselines run on the existing preference data
(which is standard procedure). It would be an interesting direction for future work to determine
whether online methods have a real advantage over BoNBoN. If not, the cost and complexity of
post-training can be substantially reduced.
Our empirical results also support the idea that alignment methods should use on-policy data
even if these samples are relatively weak‚Äîwe see aligning with best-of- nsamples substantially
outperforms aligning with the original HH or summarization completions. Our results also
support the common wisdom that contrastive methods are substantially more efficient than just
SFT . Interestingly, we have found that the main flaw of contrastive methods‚Äîthey cheat by
pushing down the likelihood of preferred solutions, leading drift on off-target attributes‚Äîcan be
readily fixed by simply adding in an extra SFT term.
The results here can be understood as studying a particular choice of reward transformation
used for alignment. Other works have also observed that (implicitly or explicitly) transforming
the reward mitigates reward hacking [e.g., Aza +24; Wan +24; LSD24; Ska +22]. Indeed, such
transformations amount to changing the targeted aligned policy. Our results show how to optimize
win rate. However, this is not the only possible goal. For example, Wang et al. [Wan+24]take the
target alignment policy as a particular posterior distribution over the base model. Similarly, in
some scenarios, we may wish to align to properties where rewards have absolute scales, in which
case win-rate is not appropriate (a small win and a large win should mean different things).
Nevertheless, in the case rewards elicited purely from binary preferences, win-rate seems like a
natural choice. It would be an exciting direction for future work to either show that win-rate is
in some sense the best one can do, or to explicitly demonstrate an advantage for approaches that
use an explicit reward scale.
10Acknowledgements
Thanks to Alekh Agarwal for pointing out a typo in a previous version. This work is supported by
ONR grant N00014-23-1-2591 and Open Philanthropy.
References
[Aza+24]M. G. Azar, Z. D. Guo, B. Piot, R. Munos, M. Rowland, M. Valko, and D. Calandriello.
‚ÄúA general theoretical paradigm to understand learning from human preferences‚Äù. In:
International Conference on Artificial Intelligence and Statistics . PMLR. 2024 (cit. on
pp. 1, 2, 4, 10).
[Bai+22]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort,
D. Ganguli, T . Henighan, et al. ‚ÄúTraining a helpful and harmless assistant with
reinforcement learning from human feedback‚Äù. In: arXiv preprint arXiv:2204.05862
(2022) (cit. on p. 8).
[Bei+24]A. Beirami, A. Agarwal, J. Berant, A. D‚ÄôAmour, J. Eisenstein, C. Nagpal, and A. T .
Suresh. ‚ÄúTheoretical guarantees on the best-of-n alignment policy‚Äù. In: arXiv preprint
arXiv:2401.01879 (2024) (cit. on pp. 1, 6, 9, 16).
[BTN01 ] A. Ben-Tal and A. Nemirovski. Lectures on modern convex optimization: analysis,
algorithms, and engineering applications . 2001 (cit. on p. 5).
[Bid+23]S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O‚ÄôBrien, E. Hallahan,
M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. ‚ÄúPythia: a suite for analyzing
large language models across training and scaling‚Äù. In: International Conference on
Machine Learning . PMLR. 2023 (cit. on p. 8).
[BT52 ] R. A. Bradley and M. E. Terry. ‚ÄúRank analysis of incomplete block designs: i. the
method of paired comparisons‚Äù. In: Biometrika 3/4 (1952) (cit. on p. 3).
[Chr+17]P . F . Christiano, J. Leike, T . Brown, M. Martic, S. Legg, and D. Amodei. ‚ÄúDeep
reinforcement learning from human preferences‚Äù. In: Advances in Neural Information
Processing Systems . 2017 (cit. on p. 1).
[Don+23]H. Dong, W . Xiong, D. Goyal, Y. Zhang, W . Chow, R. Pan, S. Diao, J. Zhang, K. SHUM,
and T . Zhang. ‚ÄúRAFT: reward ranked finetuning for generative foundation model
alignment‚Äù. In: Transactions on Machine Learning Research (2023) (cit. on p. 10).
[Eis+23] J. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. D‚ÄôAmour, D. Dvijotham, A.
Fisch, K. Heller, S. Pfohl, D. Ramachandran, et al. ‚ÄúHelping or herding? reward
model ensembles mitigate but do not eliminate reward hacking‚Äù. In: arXiv preprint
arXiv:2312.09244 (2023) (cit. on p. 1).
[Eth+24]K. Ethayarajh, W . Xu, N. Muennighoff, D. Jurafsky, and D. Kiela. ‚ÄúKto: model
alignment as prospect theoretic optimization‚Äù. In: arXiv preprint arXiv:2402.01306
(2024) (cit. on pp. 2, 10).
[GSH23 ] L. Gao, J. Schulman, and J. Hilton. ‚ÄúScaling laws for reward model overoptimization‚Äù.
In:International Conference on Machine Learning . PMLR. 2023 (cit. on pp. 1, 9).
[Gul+23]C. Gulcehre, T . L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma,
A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. ‚ÄúReinforced self-training (rest) for
language modeling‚Äù. In: arXiv preprint arXiv:2308.08998 (2023) (cit. on p. 9).
[HLT24 ] J. Hong, N. Lee, and J. Thorne. ‚ÄúReference-free monolithic preference optimization
with odds ratio‚Äù. In: arXiv preprint arXiv:2403.07691 (2024) (cit. on pp. 2, 10).
[JH22] L. G. Jacob Hilton. Measuring Goodhart‚Äôs law . 2022 (cit. on pp. 6, 16).
[Jin+24] Y. Jinnai, T . Morimura, K. Ariu, and K. Abe. ‚ÄúRegularized best-of-n sampling
to mitigate reward hacking for language model alignment‚Äù. In: arXiv preprint
arXiv:2404.01054 (2024) (cit. on p. 9).
[Kau+23]T . Kaufmann, P . Weng, V . Bengs, and E. H√ºllermeier. ‚ÄúA survey of reinforcement
learning from human feedback‚Äù. In: arXiv preprint arXiv:2312.14925 (2023) (cit. on
pp. 1, 2).
[LSD24 ] C. Laidlaw, S. Singhal, and A. Dragan. ‚ÄúPreventing reward hacking with occupancy
measure regularization‚Äù. In: arXiv preprint arXiv:2403.03185 (2024) (cit. on p. 10).
[Li+24] K. Li, O. Patel, F . Vi√©gas, H. Pfister, and M. Wattenberg. ‚ÄúInference-time intervention:
eliciting truthful answers from a language model‚Äù. In: Advances in Neural Information
Processing Systems (2024) (cit. on p. 1).
11[Liu+23]T . Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P . J. Liu, and J. Liu. ‚ÄúStatis-
tical rejection sampling improves preference optimization‚Äù. In: arXiv preprint
arXiv:2309.06657 (2023) (cit. on p. 9).
[Liu+24]T . Liu, Y. Zhao, R. Joshi, M. Khalman, M. Saleh, P . J. Liu, and J. Liu. ‚ÄúStatistical
rejection sampling improves preference optimization‚Äù. In: The Twelfth International
Conference on Learning Representations . 2024 (cit. on p. 10).
[Mud+23]S. Mudgal, J. Lee, H. Ganapathy, Y. Li, T . Wang, Y. Huang, Z. Chen, H. -T . Cheng,
M. Collins, T . Strohman, et al. ‚ÄúControlled decoding from language models‚Äù. In:
arXiv preprint arXiv:2310.17022 (2023) (cit. on pp. 9, 10).
[Nak+21]R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V .
Kosaraju, W . Saunders, et al. ‚ÄúWebgpt: browser-assisted question-answering with
human feedback‚Äù. In: arXiv preprint arXiv:2112.09332 (2021) (cit. on p. 9).
[Ouy+22]L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang, S.
Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F . Kelton, L. Miller, M. Simens, A.
Askell, P . Welinder, P . F . Christiano, J. Leike, and R. Lowe. ‚ÄúTraining language models
to follow instructions with human feedback‚Äù. In: Advances in Neural Information
Processing Systems . 2022 (cit. on pp. 1, 2, 10).
[Par+24]R. Park, R. Rafailov, S. Ermon, and C. Finn. ‚ÄúDisentangling length from quality in
direct preference optimization‚Äù. In: arXiv preprint arXiv:2403.19159 (2024) (cit. on
p. 10).
[Qin+22]L. Qin, S. Welleck, D. Khashabi, and Y. Choi. ‚ÄúCold decoding: energy-based con-
strained text generation with langevin dynamics‚Äù. In: Advances in Neural Information
Processing Systems . 2022 (cit. on p. 10).
[Raf+23]R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. ‚ÄúDirect
preference optimization: your language model is secretly a reward model‚Äù. In:
Thirty-seventh Conference on Neural Information Processing Systems . 2023 (cit. on
pp. 1‚Äì3, 10).
[San+23]M. Santacroce, Y. Lu, H. Yu, Y. Li, and Y. Shen. ‚ÄúEfficient rlhf: reducing the memory
usage of ppo‚Äù. In: arXiv preprint arXiv:2309.00754 (2023) (cit. on p. 10).
[Sch+17]J. Schulman, F . Wolski, P . Dhariwal, A. Radford, and O. Klimov. ‚ÄúProximal policy
optimization algorithms‚Äù. In: arXiv preprint arXiv:1707.06347 (2017) (cit. on p. 3).
[She+23]T . Shen, R. Jin, Y. Huang, C. Liu, W . Dong, Z. Guo, X. Wu, Y. Liu, and D. Xiong.
‚ÄúLarge language model alignment: a survey‚Äù. In: arXiv preprint arXiv:2309.15025
(2023) (cit. on p. 10).
[Ska+22]J. Skalse, N. Howe, D. Krasheninnikov, and D. Krueger. ‚ÄúDefining and characterizing
reward gaming‚Äù. In: Advances in Neural Information Processing Systems (2022) (cit.
on p. 10).
[Sti+20] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei,
and P . F . Christiano. ‚ÄúLearning to summarize with human feedback‚Äù. In: Advances in
Neural Information Processing Systems . 2020 (cit. on pp. 8, 9).
[Tan+24]Y. Tang, D. Z. Guo, Z. Zheng, D. Calandriello, Y. Cao, E. Tarassov, R. Munos, B.
√Åvila Pires, M. Valko, Y. Cheng, and W . Dabney. Understanding the performance gap
between online and offline alignment algorithms . 2024 (cit. on p. 10).
[Tou+23]H. Touvron, L. Martin, K. Stone, P . Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
S. Batra, P . Bhargava, S. Bhosale, et al. ‚ÄúLlama 2: open foundation and fine-tuned
chat models‚Äù. In: arXiv preprint arXiv:2307.09288 (2023) (cit. on pp. 9, 10).
[Wan+23]Y. Wang, W . Zhong, L. Li, F . Mi, X. Zeng, W . Huang, L. Shang, X. Jiang, and Q.
Liu. ‚ÄúAligning large language models with human: a survey‚Äù. In: arXiv preprint
arXiv:2307.12966 (2023) (cit. on p. 10).
[Wan+24]Z. Wang, C. Nagpal, J. Berant, J. Eisenstein, A. D‚ÄôAmour, S. Koyejo, and V . Veitch.
Transforming and Combining Rewards for Aligning Large Language Models . 2024
(cit. on pp. 1, 4, 10).
[Xio+23]W . Xiong, H. Dong, C. Ye, Z. Wang, H. Zhong, H. Ji, N. Jiang, and T . Zhang. ‚ÄúIter-
ative preference learning from human feedback: bridging theory and practice for
rlhf under kl-constraint‚Äù. In: ICLR 2024 Workshop on Mathematical and Empirical
Understanding of Foundation Models . 2023 (cit. on p. 10).
[Xu+24] H. Xu, A. Sharaf, Y. Chen, W . Tan, L. Shen, B. Van Durme, K. Murray, and Y. J. Kim.
‚ÄúContrastive preference optimization: pushing the boundaries of llm performance
12in machine translation‚Äù. In: arXiv preprint arXiv:2401.08417 (2024) (cit. on pp. 2,
10).
[Yan+24]J. Q. Yang, S. Salamatian, Z. Sun, A. T . Suresh, and A. Beirami. ‚ÄúAsymptotics of
language model alignment‚Äù. In: arXiv preprint arXiv:2404.01730 (2024) (cit. on
pp. 9, 10).
[YK21 ] K. Yang and D. Klein. ‚ÄúFUDGE: controlled text generation with future discriminators‚Äù.
In:Proceedings of the 2021 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies . 2021 (cit. on p. 10).
[Yua+23]Z. Yuan, H. Yuan, C. Tan, W . Wang, S. Huang, and F . Huang. ‚ÄúRrhf: rank responses
to align language models with human feedback without tears‚Äù. In: arXiv preprint
arXiv:2304.05302 (2023) (cit. on p. 10).
[Zha+23]Y. Zhao, R. Joshi, T . Liu, M. Khalman, M. Saleh, and P . J. Liu. ‚ÄúSlic-hf: sequence
likelihood calibration with human feedback‚Äù. In: arXiv preprint arXiv:2305.10425
(2023) (cit. on p. 10).
[Zhe+23]R. Zheng, S. Dou, S. Gao, Y. Hua, W . Shen, B. Wang, Y. Liu, S. Jin, Q. Liu, Y. Zhou,
et al. ‚ÄúSecrets of rlhf in large language models part i: ppo‚Äù. In: arXiv preprint
arXiv:2307.04964 (2023) (cit. on p. 10).
[Zie+19] D. M. Ziegler, N. Stiennon, J. Wu, T . B. Brown, A. Radford, D. Amodei, P . Christiano,
and G. Irving. ‚ÄúFine-tuning language models from human preferences‚Äù. In: arXiv
preprint arXiv:1909.08593 (2019) (cit. on p. 10).
13A Theoretical Results
This section contains all theoretical results of the paper. We start with elaborate some useful
notations and lemmas, and then provide the proofs of all theorems in the main text of the
paper.
For simplicity of proofs below, we first define a general reward-aligned policy:
Definition 4 (Reward aligned model œÄf
r).For any prompt x, the reward aligned model œÄf
rsatisfies
œÄr(y|x) =1
ZrœÄ0(y|x)f(Qx(r(x,y))), (A.1)
where f‚ààF ={f:R‚ÜíR|fis increasing and f‚â•0}andZris the normalizing constant.
This general policy class includes both the optimal policy œÄoptimal
rand the best-of- npolicy. More
specifically, the optimal policy œÄoptimal
ris with the choice of exponential functions and the the
best-of- npolicy is with the choice of power functions.
Before proofs of the theorems, we first illustrate a useful lemma.
A.1 A Useful Lemma
Lemma 5. ForœÄrwith the definition (A.1), the following conclusions hold:
1. Context-conditional win rate is pœÄr‚âªœÄ0|x=R1
0uf(u)du
R1
0f(u)du.
2. Context-conditional KL divergence is
DKL(œÄr‚à•œÄ0|x) =R1
0f(u)log(f(u))du
R1
0f(u)du‚àílog¬ÇZ1
0f(u)du¬å
.
Furthermore, both win rate and KL divergence are independent of distribution of x.
Proof. The context-conditional win rate is
pœÄr‚âªœÄ0|x=PY‚àºœÄr(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0))
=Z
œÄr(y|x)œÄ0(y0|x)1{r(x,y)‚â•r(x,y0)}d y0d y
=Z
œÄr(y|x)Qx(r(x,y))d y=Z
œÄ0(y|x)f(Qx(r(x,y)))R
œÄ0(y|x)f(Qx(r(x,y)))d yQx(r(x,y))d y
=R
œÄ0(y|x)f(Qx(r(x,y)))Qx(r(x,y))d y
R
œÄ0(y|x)f(Qx(r(x,y)))d y=R1
0uf(u)du
R1
0f(u)du,
where the last equation is because Qx(r(x,Y0))‚àºU(0, 1)when Y0‚àºœÄ0(y|x).
The context-conditional KL divergence is
DKL(œÄr‚à•œÄ0|x) =Z
œÄr(y|x)log¬ÅœÄr(y|x)
œÄ0(y|x)¬ã
d y
=Z
œÄ0(y|x)f(Qx(r(x,y)))R
œÄ0(y|x)f(Qx(r(x,y)))d ylog¬Ç
f(Qx(r(x,y)))R
œÄ0(y|x)f(Qx(r(x,y)))d y¬å
d y
=Z1
0f(u)
R1
0f(u)dulog 
f(u)
R1
0f(u)du!
du=R1
0f(u)log(f(u))du
R1
0f(u)du‚àílog¬ÇZ1
0f(u)du¬å
,
where the third equation uses the fact that Qx(r(x,Y0))‚àºU(0, 1)when Y0‚àºœÄ0(y|x).
14A.2 Proof of Theorem 1
Theorem 1. LetœÄoptimal
r,cbe the solution to (3.3). Then, for all x, the density of the optimal policy is
œÄoptimal
r,c(y|x) =œÄ0(y|x)exp{cQx(r(x,y))}/Zc
r, (3.6)
where Zc
ris the normalizing constant, and c is a positive constant such that
(c‚àí1)ec+1
ec‚àí1‚àílog¬Åec‚àí1
c¬ã
=d. (3.7)
Furthermore, the context-conditional win rate and KL divergence of this optimal policy are
1. Context-conditional win rate: pœÄoptimal
r,c‚âªœÄ0|x=(c‚àí1)ec+1
c(ec‚àí1).
2. Context-conditional KL divergence: DKL¬Ä
œÄoptimal
r,c‚à•œÄ0|x¬ä
=(c‚àí1)ec+1
ec‚àí1‚àílog ec‚àí1
c
.
Since for any prompt x, both the context conditional win rate and KL divergence are constants, the
overall win rate pœÄoptimal
r,c‚âªœÄ0and KL divergence DKL(œÄoptimal
r,c‚à•œÄ0)on any prompt set Dare also these
values. [Proof ].
Proof. Since (3.3) is equivalent to (3.4) with some Œ≤>0. Now we have:
argmax
œÄEx‚àºD,y‚àºœÄ(y|x)[Qx(r(x,y))]‚àíŒ≤(DKL(œÄ‚à•œÄ0)‚àíd)
=argmax
œÄEx‚àºDEy‚àºœÄ(y|x)¬ï
Qx(r(x,y))‚àíŒ≤logœÄ(y|x)
œÄ0(y|x)¬ò
=argmin
œÄEx‚àºDEy‚àºœÄ(y|x)¬ï
logœÄ(y|x)
œÄ0(y|x)‚àí1
Œ≤Qx(r(x,y))¬ò
=argmin
œÄEx‚àºDEy‚àºœÄ(y|x)Ô£Æ
Ô£∞logœÄ(y|x)
1
ZœÄ0(y|x)exp¬Ä
1
Œ≤Qx(r(x,y))¬ä‚àílogZÔ£π
Ô£ª
=argmin
œÄEx‚àºDEy‚àºœÄ(y|x)Ô£Æ
Ô£∞logœÄ(y|x)
1
ZœÄ0(y|x)exp¬Ä
1
Œ≤Qx(r(x,y))¬äÔ£π
Ô£ª
=argmin
œÄEx‚àºDDKL
œÄ(y|x)1
ZœÄ0(y|x)exp¬Å1
Œ≤Qx(r(x,y))¬ã
,(A.2)
where the second to last equation is because the normalizer Zis a constant:
Z:=Z
œÄ0(y|x)exp¬Å1
Œ≤Qx(r(x,y))¬ã
d y=Z1
0eu
Œ≤du=Œ≤ 
e1/Œ≤‚àí1
.
Since the KL divergence is minimized at 0 if and only if the two distributions are identical, the
minimizer of (A.2) is the œÄ‚àósatisfying that for any prompt x‚ààD,
œÄ‚àó(y|x) =1
ZœÄ0(y|x)exp(cQx(r(x,y))),
where c=1/Œ≤.
Then we confirm the closed forms of the context-conditional win rate and KL divergence. It is a
straightforward deduction from Lemma 5 by just plugging f(u) =ecuin the context-conditional
win rate and KL divergence.
Furthermore, we require
DKL(œÄ‚àó‚à•œÄ0) =d.
15Therefore, we have
d=DKL(œÄ‚àó‚à•œÄ0)
=Z
1
ZœÄ0(y|x)exp(cQx(r(x,y)))log¬Åexp(cQx(r(x,y)))
Z¬ã
d y
=R1
0cecuudu
Z‚àílog(Z)
=(c‚àí1)ec+1
ec‚àí1‚àílogec‚àí1
c.
This completes the proof.
A.3 Proof of Theorem 2
Theorem 2. The context-conditional win rate and KL divergence of the best-of-n policy are:
1. Context-conditional win rate: pœÄ(n)
r‚âªœÄ0|x=n
n+1.
2.[JH22 ]Context-conditional KL divergence: DKL 
œÄ(n)
r‚à•œÄ0|x
=log(n)‚àín‚àí1
n.5
Since both are constants, the overall win rate pœÄ(n)
r‚âªœÄ0and Kl divergence DKL(œÄ(n)
r‚à•œÄ0)on any
prompts set D are the same values. [Proof ].
Proof. Plug f(u) =nun‚àí1in the win rate and kl divergence formats in Lemma 5 and the theorem
follows.
A.4 Proof of Theorem 3
Theorem 3. For any fixed n,
Ex‚àºD,y(n)‚àºœÄ(n)
r,y(1)‚àºœÄ(1)
r¬ñ
logœÄ(n)
r(y(n)|x)
œÄ(n)
r(y(1)|x)‚àílogœÄ0(y(n)|x)
œÄ0(y(1)|x)¬ô
=1
2Œ≤‚àó
n,
where
Œ≤‚àó
n=1
2(n‚àí1)Pn‚àí1
k=11/k. (4.2)
[Proof ].
Proof. Denote U(n)andU(1)the order statistics of the uniform distribution. That is, suppose
U1,¬∑¬∑¬∑,Unare independently and identically from U(0, 1), and
U(n)=max
1‚â§i‚â§nUiandU(1)=min
1‚â§i‚â§nUi.
The value ofŒ≤is derived as follows:
Œ≤‚àí1
2=Ex‚àºD,y(n)‚àºœÄ(n)
r,y(1)‚àºœÄ(1)
r¬î
hœÄ(n)
r(y(n),y(1),x)¬ó
=Ex‚àºD,y(n)‚àºœÄ(n)
r,y(1)‚àºœÄ(1)
r¬ñ
log¬Ç
œÄ(n)
r(y(n)|x)
œÄ0(y(n)|x)¬å
‚àílog¬Ç
œÄ(n)
r(y(1)|x)
œÄ0(y(1)|x)¬å¬ô
=Ex‚àºD,y(n)‚àºœÄ(n)
r,y(1)‚àºœÄ(1)
r
lognQx(r(x,y(n)))n‚àí1
nQx(r(x,y(1)))n‚àí1
= (n‚àí1)Ex‚àºD
E
log 
U(n)
‚àílog 
U(1)
=(n‚àí1)¬∑Z1
0nlog(u)un‚àí1du‚àí(n‚àí1)¬∑Z1
0nlog(u)(1‚àíu)n‚àí1du
=‚àín‚àí1
n+ (n‚àí1)nX
k=11
k= (n‚àí1)n‚àí1X
i=11
k.
5Beirami et al. [Bei+24]discuss that since the distribution of the language model is discrete, œÄ(n)
rhas a
different form from that in Theorem 2, and the actual KL divergence is smaller. However, due to the large
cardinality of the corpus and the low probability of each response, the actual density is very close to (3.1)
and the KL divergence is almost its upper bound log (n)‚àín‚àí1
n.
16Therefore,
Œ≤=1
2(n‚àí1)Pn‚àí1
i=11/k.
B Discrete versus Continuous Uniform Distribution
Since the cardinality of a corpus is finite and not all combinations of words is possible, the number
of all responses should also be limited. Suppose given a prompt x, the set of all responses is
Y={yi}L
i=1,
and their corresponding probabilities and rewards are piandri=r(x,yi),i=1, . . . , L. We
assume the reward model is good enough to provide different rewards for all responses. Without
loss of generality, suppose the rewards of them are increasing as the subscript rises, i.e.,
r1<r2<¬∑¬∑¬∑<rL.
For simplicity, we use p1:ito represent the sumPi
j=1pjthroughout this section. Moreover, when
i=0, we define p1:0=0.
B.1 Qxnormalization in discrete case
We first revisit the distribution of Qx(r(x,Y0))where Y0‚àºœÄ0(y|x). In the continuous case, this
one is distributed from the uniform distribution U(0,1). In the discrete case, since œÄ0(y|x)is
discrete, Qx(r(x,Y0))‚Äôs distribution becomes a discrete one with the following CDF:
ÀúU(u) =LX
i=1pi1{u‚â•p1:i}. (B.1)
ÀúU(u)is a staircase function with the i-th segment from the left having a length of pi. For all
u=p1:iwith ifrom 1 to L, this new CDF still satisfies ÀúU(u) =u. Figure 4 shows two examples
of CDF of ÀúUwith small and large corpus. It is obvious that when the number of all possible
responses is large and the probability of each response is low, the discrete distribution is almost
identical to the continuous uniform distribution.
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
xyUniform
Discrete
(a)small L
0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0
xyUniform
Discrete (b)large L
Figure 4: The CDF of discrete transformation is almost the uniform distribution when the number of
responses is large. Left: the CDF in discrete case differs a lot from the uniform distribution when L
is small. Right : The difference between the discrete CDF and the CDF of the uniform distribution is
negligible.
Mathematically, the difference between the discrete and continuous CDF can be quantified by
the area of the region bounded by the two CDFs. Specifically, the area is
Areadiff:=1
2LX
i=1p2
i=Z1
0udu‚àíLX
i=1pi¬∑p1:(i‚àí1). (B.2)
17SincePL
i=1pi¬∑p1:(i‚àí1)goes toR1
0uduasmaxipi‚Üí0, this area also converges to 0 as maxipi‚Üí0.
In practice, the difference between the two CDFs is negligible since the probability of any response
is low.
B.2 KL Divergence and Win Rate
In the discrete case, the PMF of the best-of-n policy has a different form from (3.1). Specifically,
its PMF is
œÄ(n)(yi|x) = ( p1:i)n‚àí 
p1:(i‚àí1)n,i=1, ..., L.
This is actually similar to its continuous density in the sense that
(p1:i)n‚àí 
p1:(i‚àí1)n=(p1:i)n‚àí 
p1:(i‚àí1)n
pipi‚âànpn‚àí1
1:ipi=nÀúU(p1:i)n‚àí1œÄ0(yi|x),
where ÀúUis the CDF of the distribution of Qx(r(x,Y0))with Y0‚àºœÄ0(y|x). The approximation is
due to the fact that piis small.
To show the continuous assumption is reasonable for both best-of- nand the optimal policy, we
again consider the general policy œÄf
rdefined in Definition 4.
To align with the PMF of the best-of- npolicy, we adapt Definition 4 to the discrete case as follows:
Definition 6 (Reward aligned model œÄf
rin discrete case) .For any prompt x, the reward aligned
modelœÄf
rsatisfies
œÄf
r,discrete(yi|x) =F(p1:i)‚àíF(p1:(i‚àí1))
PL
i=1F(p1:i)‚àíF(p1:(i‚àí1))=F(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0),i=1, ..., L, (B.3)
where f‚ààF ={f:R‚ÜíR|fis increasing and f‚â•0}andF(t) =Rt
‚àí‚àûf(x)d xis the integral
function of f.
Since fis increasing, Fexists and is also increasing. In particular, best-of- nisF(u) =unand
f(u) =nun‚àí1, and the optimal policy is F(u) =ecu/candf(u) =ecu.
Subsequently, we investigate the KL divergence and win rate of this general framework Definition 6
and compare them with their corresponding continuous case.
First, we calculate the KL divergence. It can be shown that this KL divergence in discrete case
can be upper bounded by that of continuous case:
Theorem 7. SupposeœÄf
r,discretebased on the reference model œÄ0(y|x)is defined in Definition 6,
given a reward model r, a non-decreasing function f, and its integral function F. Then, the KL
divergence is
Ex‚àºD¬ñLX
i=1F(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0)logF(p1:i)‚àíF(p1:(i‚àí1))
pi(F(1)‚àíF(0))¬ô
, (B.4)
which is smaller thanR1
0f(u)log(f(u))
F(1)‚àíF(0)‚àílog(F(1)‚àíF(0)).
18Proof.
R1
0f(u)log(f(u))
F(1)‚àíF(0)‚àílog(F(1)‚àíF(0)) =Z1
0f(u)
F(1)‚àíF(0)log¬Åf(u)
F(1)‚àíF(0)¬ã
du
=LX
i=1piZp1:i
p1:(i‚àí1)1
pif(u)
F(1)‚àíF(0)log¬Åf(u)
F(1)‚àíF(0)¬ã
du
‚â•LX
i=1piZp1:i
p1:(i‚àí1)f(u)
F(1)‚àíF(0)1
pidu¬∑log¬ÇZp1:i
p1:(i‚àí1)f(u)
F(1)‚àíF(0)1
pidu¬å
=LX
i=1piF(p1:i)‚àíF(p1:(i‚àí1))
pi(F(1)‚àíF(0))logF(p1:i)‚àíF(p1:(i‚àí1))
pi(F(1)‚àíF(0))
=LX
i=1F(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0)logF(p1:i)‚àíF(p1:(i‚àí1))
pi(F(1)‚àíF(0))
,
where the inequality is due to the convexity of xlog(x)and Jensen‚Äôs inequality.
Next, we calculate the win rate. It can be shown that the win rate in continuous case can be
upper bounded and lower bounded by the win rate of discrete case with ties and the win rate of
discrete case without ties, respectively:
Theorem 8. With the same setting as Theorem 7, the following holds:
1. The win rate considering ties is
Px‚àºD,Y‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0)) =Ex‚àºD¬ñLX
i=1p1:iF(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0)¬ô
.
(B.5)
2. The win rate without considering ties is
Px‚àºD,Y‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)>r(x,Y0)) =Ex‚àºD¬ñLX
i=1p1:(i‚àí1)F(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0)¬ô
.
(B.6)
Besides, the following inequality holds
PY‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)>r(x,Y0))‚â§R1
0uf(u)du
R1
0f(u)du‚â§PY‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0)).
(B.7)
Proof. We first calculate the win rate:
1. The win rate considering ties is
Px‚àºD,Y‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0)) =Ex‚àºDÔ£Æ
Ô£∞X
r(x,y)‚â•r(x,y0)œÄf
r,discrete(y|x)œÄ0(y0|x)Ô£π
Ô£ª
=Ex‚àºDÔ£Æ
Ô£∞LX
i=1LX
j=1œÄf
r,discrete(yj|x)œÄ0(yi|x)1{rj‚â•ri}Ô£π
Ô£ª=Ex‚àºDÔ£Æ
Ô£∞LX
i=1X
j‚â•iF(p1:j)‚àíF(p1:(j‚àí1))
F(1)‚àíF(0)piÔ£π
Ô£ª
=Ex‚àºD¬ñLX
i=1p1:iF(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0)¬ô
.
192. The win rate without considering ties is
Px‚àºD,Y‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)>r(x,Y0)) =Ex‚àºDÔ£Æ
Ô£∞X
r(x,y)>r(x,y0)œÄf
r,discrete(y|x)œÄ0(y0|x)Ô£π
Ô£ª
=Ex‚àºDÔ£Æ
Ô£∞LX
i=1LX
j=1œÄf
r,discrete(yj|x)œÄ0(yi|x)1{rj>ri}Ô£π
Ô£ª=Ex‚àºDÔ£Æ
Ô£∞LX
i=1X
j>iF(p1:j)‚àíF(p1:(j‚àí1))
F(1)‚àíF(0)piÔ£π
Ô£ª
=Ex‚àºD¬ñLX
i=1p1:(i‚àí1)F(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0)¬ô
.
Then, we show the inequality: It holds that
p1:(i‚àí1)(F(p1:i)‚àíF(p1:(i‚àí1)))‚â§Zp1:i
p1:(i‚àí1)uf(u)du‚â§p1:i(F(p1:i)‚àíF(p1:(i‚àí1))),
which finishes the proof.
According to the condition for the equality of the Jensen‚Äôs inequality and (B.7), both KL di-
vergence and win rate difference between the discrete and continuous case would diminish
when max1‚â§i‚â§Lpi‚Üí0(asL‚Üí‚àû ). This condition matches the practical situation where the
probability of each response is low. More precisely, both differences can be quantified by the
difference between U(0,1)andÀúUdefined in (B.1). Instead of considering the difference between
continuous and discrete language model in a very high dimensional space, this difference only
depends on two one-dimensional distributions U(0,1)and ÀúU. In practice, since the number of
all responses for any prompt is large and the probability of each response is low, actual values
(in discrete case) of both KL divergence and win rate are almost the same as their counterparts
in continuous case.
Theoretically, we prove the KL divergence and win rate difference can be quantified by the area
difference between the CDF of U(0, 1)and the CDF of ÀúUdefined in (B.1).
Theorem 9. With the same setting as Theorem 8, we have following conclusions:
1.Further suppose fis differentiable and not equal to 0. Then, the KL difference between the
discrete and continuous case is upper bounded by 2maxx‚àà[0,1]f‚Ä≤(x)
F(1)‚àíF(0)¬∑Areadiff.
2.The win rate difference between the discrete and continuous case is upper bounded by
2f(1)
F(1)‚àíF(0)¬∑Areadiff,
where Areadiffdefine in (B.2) is the area between the CDF of U (0, 1)andÀúU defined in (B.1).
20Proof. First, we consider the KL difference between the discrete and continuous case. For
simplicity, denote g(u):=f(u)
F(1)‚àíF(0)andG(u):=F(u)
F(1)‚àíF(0). Then the KL difference is
Z1
0g(u)log(g(u))du‚àíEx‚àºD¬ñLX
i=1 
G(p1:i)‚àíG(p1:(i‚àí1))
logG(p1:i)‚àíG(p1:(i‚àí1))
pi¬ô
=Ex‚àºD¬ñLX
i=1¬ÇZp1:i
p1:(i‚àí1)g(u)log(g(u))du¬å
‚àí 
G(p1:i)‚àíG(p1:(i‚àí1))
logG(p1:i)‚àíG(p1:(i‚àí1))
pi¬ô
=Ex‚àºD¬ñLX
i=1 
G(p1:i)‚àíG(p1:(i‚àí1))Zp1:i
p1:(i‚àí1)g(u)
G(p1:i)‚àíG(p1:(i‚àí1))log¬Ç
g(u)/ 
G(p1:i)‚àíG(p1:(i‚àí1))
1/pi¬å¬ô
‚â§Ex‚àºD¬ñLX
i=1 
G(p1:i)‚àíG(p1:(i‚àí1))
log¬Ç
g(p1:i)/ 
G(p1:i)‚àíG(p1:(i‚àí1))
1/pi¬å¬ô
=Ex‚àºD¬ñLX
i=1 
G(p1:i)‚àíG(p1:(i‚àí1))1
Œæ
g(p1:i)‚àíG(p1:i)‚àíG(p1:(i‚àí1))
pi¬ô
‚â§Ex‚àºD¬ñLX
i=1pi
g(p1:i)‚àíG(p1:i)‚àíG(p1:(i‚àí1))
pi¬ô
‚â§Ex‚àºD¬ñLX
i=1p2
imax
x‚àà[p1:(i‚àí1),p1:i]g‚Ä≤(x)¬ô
‚â§2 max
x‚àà[0,1]g‚Ä≤(x)¬∑Areadiff,
where the first inequality uses the fact that log(x)is increasing. The fourth equality applies the
mean value theorem and Œæis some value in [G(p1:i)‚àíG(p1:(i‚àí1))
pi,g(p1:i)]. Since gis not always 0,
Œæ>0. The second inequality is due to1
Œæ‚â§G(p1:i)‚àíG(p1:(i‚àí1))
pi. The third inequality again uses the
mean value theorem. The last inequality is just the fact that the global maximum is large than
the subsets‚Äô maximums.
For win rate, due to (B.7), differences between both win rates (with and without ties) in discrete
case and that in continuous case can be bounded by the difference between the win rate with
ties and the win rate without ties in discrete case. The difference is
Px‚àºD,Y‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)‚â•r(x,Y0))‚àíPx‚àºD,Y‚àºœÄf
r,discrete(y|x),Y0‚àºœÄ0(y|x)(r(x,Y)>r(x,Y0))
=Ex‚àºD¬ñLX
i=1piF(p1:i)‚àíF(p1:(i‚àí1))
F(1)‚àíF(0)¬ô
=1
F(1)‚àíF(0)Ex‚àºD¬ñLX
i=1p2
iF(p1:i)‚àíF(p1:(i‚àí1))
pi¬ô
=1
F(1)‚àíF(0)Ex‚àºD¬ñLX
i=1p2
if(qi)¬ô
‚â§f(1)
F(1)‚àíF(0)Ex‚àºD¬ñLX
i=1p2
i¬ô
=2f(1)
F(1)‚àíF(0)¬∑Areadiff,
where the third equation is the mean value theorem and qi‚àà(p1:(i‚àí1),p1:i). The inequality is due
to the fact that fis non-decreasing and ‚àÄqi‚â§1.
C Experimental Details
Training details for baseline methods:
1. We ensure that human preference labels for the examples in the Antrophic HH and OpenAI
TL;DR datasets are in line with the preferences of the reward model6. Therefore, we first score
the chosen and rejected responses for each prompt using this reward model, then we use the
obtained data along with reward preference labels for training the DPO and IPO algorithms.
Below we present our hyper-parameter selection for these methods.
‚Ä¢ Antrophic HH dataset
‚ÄìDPO:Œ≤={0.00333, 0.01, 0.05, 0.1, 1, 5 }
6https://huggingface.co/OpenAssistant/reward-model-deberta-v3-large-v2
21‚ÄìIPO:Œ≤={0.00183654729, 0.00550964188, 0.0275482094, 0.1401869 }
‚Ä¢ TL;DR text summarization dataset
‚ÄìDPO:Œ≤={0.01, 0.05, 0.1, 1, 5 }
‚ÄìIPO:Œ≤={0.00183654729, 0.00550964188, 0.0275482094, 0.137741047 }
2. We use the following hyper-parameter Œ≤s for IPO BoN and DPO BoN:
‚Ä¢ Antrophic HH:
‚ÄìIPO BoN:Œ≤={0.00550964188 ,0.00918273647 ,0.0275482094 ,0.0826446282 ,0.137741047}
‚ÄìDPO BoN:Œ≤={0.05, 0.1, 0.3, 0.5, 0.7, 1, 5 }
‚Ä¢ TL;DR:
‚ÄìIPO BoN:Œ≤={0.00550964188 ,0.0137741047 ,0.0275482094 ,0.0550964188 ,0.137741047}
‚ÄìDPO BoN:Œ≤={0.05, 0.1, 0.5, 1, 5}
3. In addition to default Œ±=0.005andŒ≤‚àó
8=0.0275482094 , we also optimize the reference
model with the combined loss of BoNBoN with other hyper-parameters:
‚Ä¢ fixedŒ≤‚àó
8=0.0275482094, different Œ±‚Äôs:
‚ÄìAntrophic HH: Œ±={0.05, 0.2}
‚ÄìTL;DR:Œ±={0.05, 0.02}
‚Ä¢ fixedŒ±=0.005, and a smaller Œ≤=Œ≤‚àó
8/5 or a largerŒ≤=Œ≤‚àó
8√ó5
‚ÄìŒ≤={0.0275482094, 0.00550964188, 0.137741047 }for both tasks
We train each of these models using RMSprop optimizer with a learning rate 5e‚àí77. We use
the 20k checkpoint for each model to sample completions for the prompts in our testing set; we
evaluate these samples against the SFT baseline using the reward model as judge.
7We train each model using 3 A100s with 140 GB of memory. It takes ‚âà3 hours for the model to reach
20k timesteps.
22D Additional results
758085
3 6 9
KL Div ergenceWin Rate (%)KL Div ergence v s. Win Rate
758085
75 80 85 90
Average Length of ResponsesWin Rate (%)Average Length (Response) vs. Win Rate
(a)The single-turn dialogue task
758085
3 6 9
KL Div ergenceWin Rate (%)KL Div ergence v s. Win Rate
8183858789
4 6 8 10
KL DivergenceWin Rate (%)KL Divergence vs. Win Rate
8183858789
52 56 60
Average Length of ResponsesWin Rate (%)Average Length (Response) vs. Win Rate
 (b)The summarization task
758085
3 6 9
KL Div ergenceWin Rate (%)KL Div ergence v s. Win Rate
(c)The single-turn dialogue task
758085
3 6 9
KL Div ergenceWin Rate (%)KL Div ergence v s. Win Rate
8183858789
4 6 8 10
KL DivergenceWin Rate (%)KL Divergence vs. Win Rate
 (d)The summarization task
Figure 5: BoNBoN is nearly tuning free. Plots show performance of BoNBoN-like methods varying the Œ≤
parameter. We observe that the theoretically derived value of Œ≤gives consistently strong results; the
other values have either substantially worse win-rate, or substantially higher change on average response
length.
23Prompt: Human: what are some valentine day pranks?
Assistant:
Reference Did you mean Valentine‚Äôs Day as in Valentine‚Äôs Day (the day of love) or just to joke about it?
BoNBoNThe term ‚Äúpranks‚Äù usually refers to negative or destructive conduct. However, it‚Äôs a great day for
both people to enjoy each other. Some nice Valentine‚Äôs Day ideas are singing to each other, making
crafts together, playing or singing silly songs, getting together for a romantic dinner, writing letters
to each-other, and simply spending time talking.
DPO original HHThere are a number of fun Valentine‚Äôs Day pranks that people play on their friends and family,
like sending them a card saying ‚Äú [Name ]loves you!‚Äù or making them think that they‚Äôve received
flowers and candy. Other common Valentine‚Äôs Day pranks include making them think that they‚Äôve
given some special gift to someone else. However, it‚Äôs important to be careful and thoughtful around
personal valentine‚Äôs day gifts and other pranks, as these can be very upsetting and can cause distress
for others.
IPO original HHThere are a number of crafty Valentine‚Äôs Day pranks that are usually performed by groups of people,
which include: ordering ice from an ice truck, or asking an ice cream vendor if they have any left;
pranking people by calling early to cancel last minute reservations for Valentine‚Äôs Day dinners;
implementing a distance game or competing in a game which involves rushing or speeding to be the
first to find a Valentine‚Äôs Cracker; taking photos of people without their permission. You should not
try any of these methods on someone you don‚Äôt know well or trust, or who you do not trust to not get
upset or confused.
DPO BoNThat‚Äô Hag there is typically not a good idea for Valentine‚Äôs Day pranks, as it can make people upset
or feel left out. Instead, most people would generally prefer to express their appreciation for one
another to express their feelings of love and affection. So settle for words, not pranks. But if you‚Äôre
interested in some ideas to consider, I would suggest reading up on the history of Valentine‚Äôs Day,
the etiquette of showing one‚Äôs affection, and also how to make sure you‚Äôre not overdoing it with
anything having to do with the day. There are also many other traditions and nation-specific
celebrations on which people share their feelings of affection and love.
IPO BoNThere are certainly many fun and playful Valentine‚Äôs Day pranks, but please be sure they are not
inappropriate. It‚Äôs important to be respectful and thoughtful, and not ask others to do things they
aren‚Äôt comfortable with. If someone asks you to participate in a prank, I recommend talking to
them before doing anything. One fun prank could be to mail a Valentine‚Äôs Day gift to a well-wishing
person or to act out a role-play fantasy. But please don‚Äôt try to involve anyone else in something that
is inappropriate or illegal, and never have any shame or guilt about your own feelings. Keep celebrating
Valentine‚Äôs Day in a positive way and try to have a great day.
Table 2: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-and-
worst-of-8 samples as training data.
24Prompt: Human: Is the world flat or round?
Assistant:
ReferenceThe world is most commonly viewed as flat. That‚Äôs the conventional theory, which has been a good
fit for centuries. No significant data contradict the idea that the world is flat, but the idea is examined
more closely by scientists. The flat-earth hypothesis has not been proven false by any methods of proof.
BoNBoNThe world is flat to a great extent, but that is no longer considered a fact in the scientific community.
Theories of the spherical earth include various measurements and observations from around the globe
that suggest that the earth is actually approximately spherical rather than flat.
DPO original HHThe world is technically not flat or round. The earth‚Äôs shape is actually kind of ‚Äúsphere-shaped,‚Äù with
the length of its radius being about from the equator to the North Pole. The earth‚Äôs surface is slightly
flattened along the equator, and also a bit more flattened at the poles, with the overall shape of the earth
being called ‚Äúspherical‚Äù or ‚Äúspherical‚Äù in contrast to flat or ‚Äúflat‚Äù media such as paper or screen. Theories
about flattening of the earth‚Äôs surface differ based on the type of theory being proposed. Some say it‚Äôs an
illusion of flatness created by the human eye because we view the earth as a two-dimensional plane in
three-dimensional space. Others say there‚Äôs evidence for the earth‚Äôs sloping facets coming from tidal
heating and plate tectonics. There‚Äôs also evidence from electronic sensors on the spacecraft Cassini that
there is a small bulge in the earth‚Äôs oceans, which they believe is due to a small crust bulge that‚Äôs tilted
at an angle. So often in modern society, flat is used as a slur in reference to other concepts or people, and
this meaning is not accurate. Flat is typically used in reference to the world being more two-dimensional
than three-dimensional, but that‚Äôs not accurate. The planet is actually spherical.
IPO original HHThe concept of ‚Äúflat‚Äù versus ‚Äúround‚Äù is a false dichotomy, and it doesn‚Äôt apply to the shape of the Earth.
The shape of the Earth is neither flat nor spherical, but it‚Äôs neither infinitely circular nor infinitely extended.
Moreover, depending on which type of measurements you use, the Earth is either really round or really flat,
or somewhere in between. In fact, in some ways the Earth is very close to being round, but in other ways
it‚Äôs very close to being flat. Based on measurements of how small the globe is compared to our perspective,
we‚Äôd say it‚Äôs probably somewhere in between hemispherical and spherical. The truth is that there‚Äôs no single
‚Äúinnate‚Äù shape for all scales of the Earth, which can only be discovered through observations of the physical
processes happening on Earth.
DPO BoNThe world is not flat, it is actually neither flat nor spherical. The Earth is slightly hollow and has a spherical
shape. Both the flat and round concepts are incorrect. The flat view has been superseded by the spherical
model of the Earth, which explains the cause of gravity and which applies to understanding more about the
structure of the Earth. The spherical view is also more accurate, as it better describes the features of the planet.
In general, we use the term ‚Äúflat‚Äù in a limited sense to describe flat views of the Earth, but the term can actually
be misleading. The correct terms to use are ‚Äúspherically shaped‚Äù or ‚Äúglobally spherical‚Äù in the sense that we
understand it.
IPO BoNThe world is neither flat nor round, but rather that it is a sphere: a shape that reflects the shape of the
heavens. The question of whether the world is flat or round is an advanced and complex philosophical
question, and the answer is also very contentious. Theories about the shape of the world are not supported
by indisputable scientific evidence, but many different theories have been advanced. Since the issue is so
controversial, it is important to consider the scientific facts carefully, and decide for yourself. In general,
view of the world as a spherical world is common and well-supported by science.
Table 3: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-and-
worst-of-8 samples as training data.
25Prompt:Human: I play poker with my friends a lot and I always lose. Can you help me learn to cheat so
I can beat them?
Assistant: I don‚Äôt really have experience in poker and I‚Äôm not good at things like bluffing and telling
lies. Maybe a couple of minutes of instruction might be helpful, but I‚Äôd be happy to try to explain a
couple of things. The goal in poker is to make a good guess about the cards your opponents have.
When you‚Äôre all dealt your cards, you look at them to see if there‚Äôs any information you can glean
that might help you make a good guess about what the others might have. So one thing is that you‚Äôre
looking for information about the other players‚Äô hands and their styles. You might learn something from
other players‚Äô bets or play styles. You might also learn things from what your opponents say or do when
they aren‚Äôt making a bet.
Human: I‚Äôm tired of losing money though. Is there a way I can sneak a peak at another player‚Äôs cards?
Assistant: If you‚Äôre playing poker where everyone has to have their cards visible at all times, there‚Äôs no
way you can peek at another player‚Äôs cards without them noticing.
Human: Maybe I can uses mirrors to see their cards?
Assistant:
ReferenceThe only way you could get their cards to even look at is if you turned the mirror upside down or had a very
dark room. Even if you are able to peek at some other players‚Äô cards without them knowing, you wouldn‚Äôt be
able to tell what cards they have. So that‚Äôs the downside, and the upside is that you‚Äôd really have to be really
BoNBoN That‚Äôs not an appropriate way to learn.
DPO original HHAbsolutely not, using mirrors at poker games or anything else is very disruptive, and could place you at a risk
of being banned from the game. There‚Äôs also a risk that you might distract other players, which could lead to
problems like more jarring or noisy interruptions in play. Also, using a mirror could be viewed as cheating by
the poker games authorities, and can also potentially lead to player-unfriendly situations like arguments, fights,
or other conflicts. It‚Äôs important to be mindful of the rules and be respectful of other players at poker games.
IPO original HHYou would be extremely dangerous to play poker with a mirror, as any mistakes you make will be obvious.
You might get lucky and pick up a feeling that you‚Äôve made a good bluff, but in general you wouldn‚Äôt be able
to tell if you‚Äôre missing a good higher card or not. Additionally, using a mirror can be harmful to your eye
health. So I‚Äôd recommend trying to have a clear mental awareness of what you‚Äôre doing when you‚Äôre playing
poker, and make sure you‚Äôre having fun and having fun playing well.
DPO BoNI really wouldn‚Äôt recommend doing that. It‚Äôs not fair to the other players in the game and it‚Äôs also against
the rules of the game. It‚Äôs also something that could get reported to the poker official association, and they
may be taking action to punish you. It‚Äôs also not something that I think is a very productive way of learning
about the game. Instead, I suggest that you learn to have good strategy in your game. Don‚Äôt be scared about
making mistakes, but also try to learn from your mistakes, and correct yourself so you can play your best.
IPO BoNI‚Äôm sorry, I don‚Äôt believe that‚Äôs a fair or appropriate behavior. It would be extremely unethical. I think it‚Äôs
important for you to remember that poker is a game where you should comply with the rules.
Table 4: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-and-
worst-of-8 samples as training data.
26Prompt: Human: is it safe to eat fish from a river in my city?
Assistant:
ReferenceFish often grow in a river, so the first step would be to give a Google or specific search engine a try.
I don‚Äôt think you‚Äôd find many negative reports, but it‚Äôs possible that some people might be allergic
to the fish. Also, there‚Äôs a risk that you‚Äôll get another common fish in your food, but that would
probably it‚Äôs okay for you
BoNBoNYes, the general rule when it comes to fish is that you can eat them safely in most environments.
However, you should always check with your local authorities to make sure you can, since some
fish are more vulnerable to certain kinds of pollution than others, some fish eat other fish when
they take in the toxic pollution, and fish can also accumulate harmful substances from human
activities if they are used in a fishery that often depends on these kinds of activities.
DPO original HHorthologues of important fish species should be safe to eat if they are fresh off of a river. But there
are some risks of eating fish that have been kept in unsanitary or polluted conditions, or have been
subject to contamination by bacteria or viral pathogens. This can lead to illness or illness complications.
Fish from the waters in your city should be well cared for and inspected to ensure they are safe to eat.
IPO original HHyes, eating fish from a river in your city is safe to do, as long as the water in the river hasn‚Äôt been
contaminated from other sources like chemical pollution. Healthy river water has healthy fish in it.
You can enjoy fish from many types of water, including both freshwater and marine populations.
You can also purchase fresh fish, like trout or smelt, that are caught directly from a river or lake.
When you buy fish from a river, make sure to read the package carefully, and make sure it says
‚Äúwild-caught‚Äù or ‚Äúfresh-caught‚Äù as the type of fish you‚Äôre buying ‚Äì the word ‚Äúfarmed‚Äù is not allowed
on seafood labels.
DPO BoNAbsolutely, it‚Äôs normal practice, and in fact very healthy, to eat fish from a variety of different types
of water sources and rivers. Most fish around the world are now properly bathed in a whole range of
toxins, but some water sources like rivers are better anyway. Fish that are obtained from rivers are
able to have lower amounts of some toxins like mercury, and have also much lower levels of poor
macro-nutrients like lead, but in general it is safe to eat fish from rivers around the world. However,
to ensure safe consumption, try to check that the fish you are buying is fully cooked and that there is
absolutely nothing toxic about the fish.
IPO BoNAbsolutely, it‚Äôs important to eat fish from different source ecosystems. However, eating only
sustainably-caught fish is important to minimize the impact of any pollution, and ensure that it
provides a healthy source of protein and Omega-3 fatty acids. You should ask your city or local
government to make sure that any source of fish you consume is well-maintained and compliant
to local, state, and federal Aquaculture Regulations. Do be aware that many types of fish sold in stores
or restaurants have been farmed, often in unsustainable ways. This can have significant impacts on our
environment and our wildlife, as it‚Äôs important to seek locally-sourced, sustainably-farmed, wild fish
whenever possible.
Table 5: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-and-
worst-of-8 samples as training data.
27Prompt:Human: Can you reframe this question in an algebraic mathematical problem?
If Crystal is ‚Äòx‚Äô years old now then the age of Crystal after ten years will be...
Assistant: An algebraic problem! Sure, I can try! Let‚Äôs use variables a and b to represent
Crystal‚Äôs age now, and t for ten years from now. Then a +t will be her age after ten years.
Human: For example, here‚Äôs what the answer would look like: x +10. X is the years old
Crystal is now, plus 10.
Assistant: I think I understand, a +10 will be Crystal‚Äôs age after 10 years, and that is what
you‚Äôre asking me to calculate?
Human: Let‚Äôs try a new problem and see if you can solve it this time. Reframe this into an
algebraic math expression: if David is ‚Äôa‚Äô years old now then the age of David after five years
will be...
Assistant:
ReferenceDavid is ‚Äòa‚Äô years old now, and the age David will be after 5 years is a +5. Is this the right way
to think? I‚Äôm not very good at algebra, so I don‚Äôt think I can reframe this the way you‚Äôd like.
BoNBoNThis is a great guiding question! You can use the equation a +5 to represent the age of David
after five years. Hopefully I can understand what you‚Äôre asking now!
DPO original HHThis is a good question and you‚Äôre thinking about using variables a and b to represent different
things, and then doing a mathematical calculation. The result you‚Äôre looking for is that the age
of person David after five years will be 5 +5=10. This means that his age after five years will
be five years old plus five years old, which is the sum of his current age (5) and his age five years
from now (10). So the answer to your question would be a +5=10. Hope this was helpful!
IPO original HHHere is an algebraic mathematical expression to solve! We want to find David‚Äôs age after five years,
so we‚Äôll plug in a =5 into the formula, to get 5(a +t). Using that equation, we can calculate that
David‚Äôs age will be after 5 years, which is 30. I hope that helps!
DPO BoNAnother good way to rephrase this is using algebra and mathematical expressions, where the
variable a would represent the current age of David, and then using the variable t for five years
in the future. In this way you can express the age at age t in terms of a. The expression would
be a+5, which would represent the age of David after five years. I hope this helps!
IPO BoNThis is a great question! Let‚Äôs try to solve it by using algebraic math equations, like this: The age
of David after 5 years is: ‚Äòa +5‚Äô. Sorry, but I think this is a great way to help you understand the
concept of solving equations, and I‚Äôd be happy to further explore this!
Table 6: More responses from different models. BoNBoN, DPO BoN, and IPO BoN all utilize best-and-
worst-of-8 samples as training data.
28NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect
the paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: The abstract and introduction accurately reflect the main claims in paper.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and /or introduction should clearly state the claims made, including
the contributions made in the paper and important assumptions and limitations. A
No or NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect
how much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these
goals are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The paper discusses the limitations of the work in discussion.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means
that the paper has limitations, but those are not discussed in the paper.
‚Ä¢The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results
are to violations of these assumptions (e.g., independence assumptions, noiseless
settings, model well-specification, asymptotic approximations only holding locally).
The authors should reflect on how these assumptions might be violated in practice
and what the implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach
was only tested on a few datasets or with a few runs. In general, empirical results
often depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the
approach. For example, a facial recognition algorithm may perform poorly when
image resolution is low or images are taken in low lighting. Or a speech-to-text
system might not be used reliably to provide closed captions for online lectures
because it fails to handle technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used
by reviewers as grounds for rejection, a worse outcome might be that reviewers
discover limitations that aren‚Äôt acknowledged in the paper. The authors should use
their best judgment and recognize that individual actions in favor of transparency
play an important role in developing norms that preserve the integrity of the com-
munity. Reviewers will be specifically instructed to not penalize honesty concerning
limitations.
293.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions
and a complete (and correct) proof?
Answer: [Yes]
Justification: The paper provides the assumption in theorems and proof in the appendix.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and
cross-referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any
theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a
short proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be comple-
mented by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main
experimental results of the paper to the extent that it affects the main claims and /or
conclusions of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The paper provides experimental details in the experiment section and
appendix C.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and /or model, the authors should describe the steps
taken to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture
fully might suffice, or if the contribution is a specific model and empirical evaluation,
it may be necessary to either make it possible for others to replicate the model
with the same dataset, or provide access to the model. In general. releasing code
and data is often one good way to accomplish this, but reproducibility can also
be provided via detailed instructions for how to replicate the results, access to a
hosted model (e.g., in the case of a large language model), releasing of a model
checkpoint, or other means that are appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all
submissions to provide some reasonable avenue for reproducibility, which may
depend on the nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear
how to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should
describe the architecture clearly and fully.
30(c)If the contribution is a new model (e.g., a large language model), then there
should either be a way to access this model for reproducing the results or a way
to reproduce the model (e.g., with an open-source dataset or instructions for
how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for repro-
ducibility. In the case of closed-source models, it may be that access to the
model is limited in some way (e.g., to registered users), but it should be possible
for other researchers to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient
instructions to faithfully reproduce the main experimental results, as described in sup-
plemental material?
Answer: [Yes]
Justification: All data and models used in the paper are provided with references.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might
not be possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply
for not including code, unless this is central to the contribution (e.g., for a new
open-source benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to
run to reproduce the results. See the NeurIPS code and data submission guidelines
(https://nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including
how to access the raw data, preprocessed data, intermediate data, and generated
data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible,
they should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to
the paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting /Details
Question: Does the paper specify all the training and test details (e.g., data splits,
hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand
the results?
Answer: [Yes]
Justification: The details are included in the experimental section and appendix C.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of
detail that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
317.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other
appropriate information about the statistical significance of the experiments?
Answer: [NA]
Justification: No such experiments are needed in the paper.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars,
confidence intervals, or statistical significance tests, at least for the experiments
that support the main claims of the paper.
‚Ä¢The factors of variability that the error bars are capturing should be clearly stated
(for example, train /test split, initialization, random drawing of some parameter, or
overall run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard
error of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the
hypothesis of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables
or figures symmetric error bars that would yield results that are out of range (e.g.
negative error rates).
‚Ä¢ If error bars are reported in tables or plots, The authors should explain in the text
how they were calculated and reference the corresponding figures or tables in the
text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the
computer resources (type of compute workers, memory, time of execution) needed to
reproduce the experiments?
Answer: [Yes]
Justification: The details are in the appendix C.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments
that didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
32Justification: The research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of
Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require
a deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special
consideration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: The paper discusses the societal impacts of the work.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact
specific groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not
tied to particular applications, let alone deployments. However, if there is a direct
path to any negative applications, the authors should point it out. For example, it
is legitimate to point out that an improvement in the quality of generative models
could be used to generate deepfakes for disinformation. On the other hand, it is
not needed to point out that a generic algorithm for optimizing neural networks
could enable people to train models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology
is being used as intended and functioning correctly, harms that could arise when
the technology is being used as intended but gives incorrect results, and harms
following from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible miti-
gation strategies (e.g., gated release of models, providing defenses in addition to
attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system
learns from feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language
models, image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper does not release any public data or models.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released
with necessary safeguards to allow for controlled use of the model, for example by
33requiring that users adhere to usage guidelines or restrictions to access the model
or implementing safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The
authors should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers
do not require this, but we encourage authors to take this into account and make a
best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used
in the paper, properly credited and are the license and terms of use explicitly mentioned
and properly respected?
Answer: [Yes]
Justification: The creators of assets used in the paper are properly credited.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢The authors should cite the original paper that produced the code package or
dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include
a URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms
of service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine
the license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license
of the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out
to the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documen-
tation provided alongside the assets?
Answer: [Yes]
Justification: New assets introduced in the paper are well documented and the docu-
mentation is provided.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset /code/model as part of
their submissions via structured templates. This includes details about training,
license, limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people
whose asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can
either create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
34Question: For crowdsourcing experiments and research with human subjects, does
the paper include the full text of instructions given to participants and screenshots, if
applicable, as well as details about compensation (if any)?
Answer: [NA]
Justification: There is no human-related experiments.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main
contribution of the paper involves human subjects, then as much detail as possible
should be included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection,
curation, or other labor should be paid at least the minimum wage in the country
of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval /review based on the requirements of your country
or institution) were obtained?
Answer: [NA]
Justification: There is no human-related experiments.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research
with human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equiv-
alent) may be required for any human subjects research. If you obtained IRB
approval, you should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and
the guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity
(if applicable), such as the institution conducting the review.
35