Data Debugging is NP-hard for Classifiers Trained
with SGD
Anonymous Author(s)
Affiliation
Address
email
Abstract
Data debugging is to find a subset of the training data such that the model obtained 1
by retraining on the subset has a better accuracy. A bunch of heuristic approaches 2
are proposed, however, none of them are guaranteed to solve this problem effec- 3
tively. This leaves an open issue whether there exists an efficient algorithm to find 4
the subset such that the model obtained by retraining on it has a better accuracy. 5
To answer this open question and provide theoretical basis for further study on 6
developing better algorithms for data debugging, we investigate the computational 7
complexity of the problem named DEBUGGABLE . Given a machine learning 8
model Mobtained by training on dataset Dand a test instance (xtest, ytest)where 9
M(xtest)Ã∏=ytest,DEBUGGABLE is to determine whether there exists a subset D‚Ä≤of 10
Dsuch that the model M‚Ä≤obtained by retraining on D‚Ä≤satisfies M‚Ä≤(xtest) =ytest. 11
To cover a wide range of commonly used models, we take SGD-trained linear 12
classifier as the model and derive the following main results. (1) If the loss function 13
and the dimension of the model are not fixed, DEBUGGABLE is NP-complete 14
regardless of the training order in which all the training samples are processed 15
during SGD. (2) For hinge-like loss functions, a comprehensive analysis on the 16
computational complexity of DEBUGGABLE is provided; (3) If the loss function is a 17
linear function, DEBUGGABLE can be solved in linear time, that is, data debugging 18
can be solved easily in this case. These results not only highlight the limitations of 19
current approaches but also offer new insights into data debugging. 20
1 Introduction 21
Given a machine learning model, data debugging is to find a subset of the training data such that 22
the model will have a better accuracy if retrained on that subset [ 1]. Data debugging serves as a 23
popular method of both data cleaning and machine learning interpretation. In the context of data 24
cleaning, data debugging ( a.k.a. training data debugging [ 2] or data cleansing [ 1]) can be used 25
to improve the quality of the training data by removing the flaws leading to mispredictions [ 3‚Äì5]. 26
When it comes to ML interpretation, data debugging locates the part of the training data responsible 27
for unexpected predictions of an ML model. Therefore it is also studied as a training data-based 28
(a.k.a. instance-based [ 6]) interpretation, which is crucial for helping system developers and ML 29
practitioners to debug ML system by reporting the harmful part of training data [7]. 30
To solve the data debugging problem, existing researches adopt a two-phase score-based heuristic 31
approach [ 2]. In the first phase, a score representing the estimated impact on the model accuracy is 32
assigned to each training sample in the training data. It is hoped that the harmful part of training 33
data gets a lower score than the other part. In the second phase, training samples with lower scores 34
are removed greedily and the model is retrained on the modified training data. The two phases are 35
carried out iteratively until a well-trained model is obtained. Most of the related works focus on 36
Submitted to 38th Conference on Neural Information Processing Systems (NeurIPS 2024). Do not distribute.developing algorithms to estimate the scores efficiently in the first phase [ 8‚Äì16], but rarely study the 37
effectiveness of the entire two-phase approach. 38
Since it is computationally intractable to estimate the score for all possible subsets of the training 39
data, it is often assumed that the score representing the impact of a subset is approximately equal 40
to the sum of the scores of each individual training samples from the subset. However, Koh et. al. 41
[10] showed this is not always the case. For a bunch of subsets sampled from the training data, 42
they empirically studied the difference between the estimated impact and the actual impact of each 43
subset by taking influence functions as the scoring method. The estimated impact is calculated by 44
summing up the score by influence function of each training samples in the subset, and the actual 45
impact is measured by the improvement of accuracy of the model retrained after removing the subset 46
from training data. They found that the estimated impact tends to underestimate the actual impact. 47
Removing a large number of training samples could result in a large deviation between estimated 48
and actual impacts. Although an upper bound of the deviation under certain assumptions has been 49
derived, it is still unknown whether the deviation can be reduced or eliminated efficiently. 50
The above deviation also poses challenges to the effectiveness of the entire approach. Suppose the 51
influence function is adopted as the scoring method, the accuracy of the model is not guaranteed 52
to improve due to the deviation reported in [ 10] if a large group of training samples are removed 53
during each iteration. Moreover, there is no theoretical analysis for the effectiveness of the greedy 54
approach in the second phase. Even if only one training sample is removed during each iteration 55
of the two-phase approach, the accuracy of the model is still not guaranteed to be improved. The 56
effectiveness of the entire two-phase approach is therefore not assured. This leaves the following 57
open problem: 58
Problem 1.1. Is there an efficient algorithm to find the subset of the training data, such that the 59
model obtained by retraining on it has a better accuracy? 60
The computational complexity results presented in this paper demonstrate that it is unlikely to solve 61
the data debugging problem efficiently in polynomial time. To figure out its hardness, we study the 62
problem DEBUGGABLE which is the decision version of data debugging when the test set consists of 63
only one instance. Formally, D EBUGGABLE is defined as follows: 64
Problem 1.2 (DEBUGGABLE ).Given a classifier M, its training data T, a test instance (x, y). Is 65
there a T‚Ä≤‚äÜT, such that Mpredicts yonxif retrained on T‚Ä≤? 66
Basically, we prove that DEBUGGABLE is NP-complete, which means data debugging is unlikely 67
to be solved in polynomial time. This result answers the open question mentioned above directly, 68
this is, the large deviation of estimated impacts [ 10] cannot be reduced or eliminated efficiently. This 69
is because if the impact of a subset of the training data could be accurately estimated as the sum of 70
the impact of each training sample in the subset, data debugging can be solved in polynomial time, 71
which is impossible unless P=NP. 72
Although DEBUGGABLE is generally intractable, we still hope to develop efficient algorithms tailored 73
to specific cases. Thus it is necessary to figure out the root cause of the hardness for DEBUGGABLE . 74
Previous research are always conducted based on the belief that the complexity of data debugging is 75
due to the chosen model architecture is complicated. However, we show that at least for models trained 76
by stochastic gradient descent (SGD), the hardness stems from the hyper-parameter configuration 77
selected for the SGD training, which was not yet aware of by previous work. To cover a wide range of 78
commonly used machine learning models, we take linear classifiers as the model and show that even 79
for linear classifiers, DEBUGGABLE is NP-hard as long as they are trained by SGD. Moreover, we 80
provided a comprehensive analysis on hyper-parameter configurations that affect the computational 81
complexity of DEBUGGABLE , including the loss function, the model dimension and the training 82
order. Training order, a.k.a. training data order [ 17] or order of training samples [ 18], refers to the 83
order in which each training sample is considered during the SGD. Detailed complexity results are 84
shown in Table 1. 85
Our contribution can be concluded as follows: 86
‚Ä¢We studied the computational complexity of data debugging and showed that data debugging 87
is NP-hard for linear classifiers in the general setting for all possible training orders . 88
‚Ä¢We studied the complexity of DEBUGGABLE when the loss is fixed as the hinge-like 89
function. For 2 or higher dimension, DEBUGGABLE is NP-complete when the training order 90
2Table 1: Computational complexity of the data debugging problem
Loss Function Dimension Training Order Complexity
Not Fixed Not Fixed - NP-hard
Hinge-like ‚â•2 Adversarially Chosen NP-hard
Hinge-like, Œ≤ <0 1 Adversarially Chosen NP-hard
Hinge-like, Œ≤‚â•0 1 - Linear Time
Linear - - Linear Time
is adversarially chosen; For one-dimensional cases, DEBUGGABLE can be NP-hard when 91
the interception Œ≤ <0, and is solvable in linear time when Œ≤‚â•0. 92
‚Ä¢ We proved that D EBUGGABLE is solvable in linear time when the loss function is linear. 93
Moreover, we have a discussion on the implications of these complexity results for machine learning 94
interpretability and data quality, as well as limitations of score-based greedy methods. Our results 95
suggest the further study as follows. (1) It is better to characterize the training sample and find the 96
criterion which can be used to decide the existence of efficient algorithms; (2) Designing algorithms 97
with CSP-solver is a potential way to solve data debugging more efficiently than the brute-force one; 98
(3) Developing random algorithms is a potential way to solve data debugging successfully with high 99
probability. 100
1.1 Related Works 101
The solution of data debugging has applications in database query results reliability enhancement 102
[2,19], training data cleaning [ 1] and machine learning interpretation[ 9,8,10,20,21]. Existing 103
works on data debugging mainly adopt a two-phase approach, which scores the training samples in the 104
first phase and greedily deletes training samples with lower scores in the second phase. Most of the 105
research focus on the first phase. There are mainly two ways of scoring adopted for data debugging in 106
practice. Leave-one-out (LOO) retraining is a widely studied way, which evaluates the contribution of 107
a training sample through the difference in the model‚Äôs accuracy trained without that training sample. 108
To avoid the cost of model retraining, Koh and Liang took influence functions as an approximation of 109
LOO [ 8]. After that, various extensions and improvements of the influence function based method 110
are proposed, such as Fisher kernel [ 9], influence function for group impacts [ 10], second-order 111
approximations [ 11] and scalable influence functions [ 12]. Another way is Shapley-based scoring, 112
where the impact of a training sample is measured by its average marginal contribution to all subsets 113
of the training data [ 13]. Since Shapley-base scoring suffers from expensive computational cost [ 22], 114
recent works focus on techniques that efficiently estimate the Shapley value, including Monte-Carlo 115
sampling [ 13], group testing [ 14,15] and using proxy models such as k-NN [ 16,3]. However, 116
those methods do not admit any theoretical guarantee on the effectiveness. This paper discusses the 117
limitations of the above methods and suggests some future directions on data debugging. 118
2 Preliminaries and Problem Definition 119
Linear classifiers. Formally, a (binary) linear classifier is a function Œªw:Rd‚Üí {‚àí 1,1}, where dis 120
called its dimension andw‚ààRdits parameter. Without loss of generality, the bias term of a linear 121
classifier is set as zero in this paper. All vectors in this paper are assumed to be column vectors. For 122
an input x, the value of Œªwis defined as 123
Œªw(x) =1 ifw‚ä§x‚â•0
‚àí1otherwise.
We denote the class of linear models as Œõ. 124
Training data. Atraining sample is a pair (x, y)in which x‚ààRdis the input and y‚àà {‚àí 1,1}is 125
the label of x. The training data is a multiset of training samples. We employ wT‚àí ‚Üíw‚Ä≤to denote 126
that the parameter w‚Ä≤is obtained by training the parameter won the training data T, and employ 127
w(x,y)‚àí ‚àí ‚àí ‚Üí w‚Ä≤to denote that w‚Ä≤is obtained by training won the training sample (x, y). 128
3Loss functions and learning rates. Binary linear classifiers typically use unary functions on yw‚ä§x 129
as their loss functions [ 23]. Therefore we only consider loss functions of the form L:yw‚ä§x7‚ÜíR 130
for the rest of the paper. 131
Thelinear loss is in the form of 132
Llin(yw‚ä§x) =‚àíŒ±(yw‚ä§x+Œ≤).
Thehinge-like loss function is defined as the following form 133
Lhinge(yw‚ä§x) =‚àíŒ±(yw‚ä§x+Œ≤), yw‚ä§x< Œ≤
0, otherwise.
We call Œ≤as the interception ofLhinge . We represent the learning rate of a model using a vector 134
Œ∑= (Œ∑1, . . . , Œ∑ d), where Œ∑i‚â•0and each parameter wican be updated with the corresponding 135
learning rate Œ∑i. 136
Stochastic gradient descent. The stochastic gradient descent (SGD) method updates parameter w 137
from its initial value w(0)through several epochs. During each epoch, the SGD goes through the 138
entire set of training samples in some training order through several iterations. The training order is 139
defined as a sequence of training samples, in the form of (x1, y1). . .(xn, yn). For 1‚â§i < j‚â§n, 140
(xi, yi)is considered before (xj, yj)during the SGD. We use wito denote the i-th coordinate of w. 141
We also use w(e,k)to denote the value of wat the end of k-th iteration of epoch eand use w(e)to 142
denote the value of wafter the end of epoch e. Assuming (x, y)to be the training sample considered 143
at iteration k, the stochastic gradient descent (SGD) method updates parameter wifor each iby 144
w(e,k)
i‚Üêw(e,k‚àí1)
i ‚àíŒ∑i¬∑‚àÇL(y(w(e,k‚àí1))‚ä§x)
‚àÇwi(1)
In other words, we have 145
w(e,k)‚Üêw(e,k‚àí1)‚àíŒ∑‚äó ‚àáL (y(w(e,k‚àí1))‚ä§x)
where Œ∑‚äó ‚àáL = (Œ∑1‚àÇL
‚àÇw1, . . . , Œ∑ d‚àÇL
‚àÇwd)is the Hadamard product. We say a training sample x 146
isactivated at iteration kduring epoch eif‚àáL(y(w(e,k‚àí1))‚ä§x)Ã∏= 0. The SGD terminates at 147
the end of epoch eif‚à•w(e‚àí1)‚àíw(e)‚à•< Œµ for threshold Œµorereached some predetermined 148
value. We denote w‚àó=w(e). A linear classifier trained by SGD with the meta-parameters 149
mentioned above is denoted as SGDŒõ(L,Œ∑, Œµ, T) =Œªw‚àó. With a slight abuse of notation, we define 150
SGDŒõ(L,Œ∑, Œµ, T, x) =Œªw‚àó(x). We also use SGDŒõ(T,x)to avoid cluttering when the context is clear. 151
Problem definition. With the above definitions, DEBUGGABLE for SGD-trained linear classifiers 152
can be formalized as follows: 153
DEBUGGABLE -LIN
Input: Training data T, loss function L, initial parameter w(0), learning
rateŒ∑, threshold Œµand instance (xtest, ytest).
Output: ‚ÄúYes‚Äù: if ‚àÉ‚àÜ‚äÜTsuch that SGDŒõ(L,Œ∑, Œµ, T\‚àÜ,xtest) =ytest;
‚ÄúNo‚Äù: otherwise.154
We say SGDŒõ(L,Œ∑, Œµ, T)isdebuggable on(xtest, ytest)if(L,w(0),Œ∑, Œµ, T, xtest, ytest)is a yes-instance 155
of D EBUGGABLE -LIN, and not debuggable on(xtest, ytest)otherwise. 156
3 Results for Unfixed Loss Functions 157
In this section, we prove the NP-hardness of DEBUGGABLE -LIN. Intuitively, DEBUGGABLE -LINis 158
to determine whether there exists a subset T‚Ä≤‚äÜTwhere activated training samples within T‚Ä≤drive 159
the parameter wtoward the region defined by ytestw‚ä§xtest>0. The activation of training samples 160
depends on the complex interaction between the training data and the model. 161
Theorem 3.1. DEBUGGABLE -LINis NP-hard for all training orders. 162
We only show the proof sketch and leave the details in the appendix. 163
4Proof Sketch. We build a reduction from an NP-hard problem M ONOTONE 1-IN-3 SAT [24]: 164
MONOTONE 1-IN-3 SAT
Input: A 3-CNF formula œÜwith no negation signs.
Output: ‚ÄúYes‚Äù: if œÜhas a 1-in-3 assignment, under which each clause
contains exactly one true literal;
‚ÄúNo‚Äù: otherwise.165
For example, œÜ1= (x1‚à®x2‚à®x3)‚àß(x2‚à®x3‚à®x4)is a yes-instance because (x1, x2, x3, x4) = 166
(T,F,F,T )is an 1-in-3 assignment; œÜ2= (x1‚à®x2‚à®x3)‚àß(x2‚à®x3‚à®x4)‚àß(x1‚à®x2‚à®x4)‚àß(x1‚à®x3‚à®x4) 167
is a no-instance. 168
Given a 3-CNF formula œÜ, our goal is to construct a configuration of the training process, such that 169
the resulting model outputs the correct answer if and only if its training data T‚Ä≤encodes an 1-in-3 170
assignment ŒΩofœÜ. This can be done by carefully designing the encoding so that for each xi‚ààœÜ, 171
ŒΩ(xi) =TRUE if and only if txi‚ààT‚Ä≤. Finally, we can construct some TwithT‚äáT‚Ä≤‚à™{txi|xi‚ààœÜ}, 172
such that some classifier trained on Tis a yes-instance of DEBUGGABLE -LINif and only if œÜis a 173
yes-instance of M ONOTONE 1-IN-3 SAT, thereby finishing our proof. 174
The reduction. Suppose œÜhasmclauses and nvariables, let N=n+2m+1. We set the dimension 175
of the linear classifier to N. 176
The input. Each coordinate of the input is named as 177
x= (xc1, . . . , x cm, xx1, . . . , x xn, xb1, . . . , x bm, xdummy )‚ä§
We also use xito denote the i-th coordinate of x. 178
The parameters. Each coordinate of the parameter is named as 179
w= (wc1, . . . , w cm, wx1, . . . , w xn, wb1, . . . , w bm, wdummy )‚ä§
We also use wito denote the i-th coordinate of w. Each wxjrepresents the truth value of variable xj, 180
where 1 represents TRUE and -1 represents FALSE . Similarly, each wcjrepresents the truth value of 181
clause cjbased on the value of its variables. wbjandwdummy are used for convenience of proof. 182
The initial value of the parameter is set to 183
w(0)= (mz}|{
1
2, . . . ,1
2,nz}|{
‚àí1, . . . ,‚àí1,mz}|{
‚àí1, . . . ,‚àí1,1)‚ä§
Loss function. We denote U(x0, Œ¥) :={x|x0‚àíŒ¥ < x < x 0+Œ¥}as the Œ¥-neighborhood of x0and 184
define U(¬±x0, Œ¥) =U(x0, Œ¥)‚à™U(‚àíx0, Œ¥). We define the local ramp function as 185
rx0,Œ¥(x) =Ô£±
Ô£≤
Ô£≥0 , x‚â§x0‚àíŒ¥;
x‚àíx0+Œ¥ , x ‚ààU(x0, Œ¥);
2Œ¥ , x ‚â•x0+Œ¥.
The loss function is defined as 186
L=‚àí12N
5r‚àí5,0.01(yw‚ä§x)‚àír‚àí1
2,0.26(yw‚ä§x)‚àí1
1000NX
x0‚àà{¬±1,¬±3}rx0,0.01(yw‚ä§x).
Lis monotonically decreasing with derivatives 187
‚àÇL
‚àÇwi=Ô£±
Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£≥‚àí12N
5¬∑yxi, yw‚ä§x‚ààU(‚àí5,0.01);
‚àíyxi , yw‚ä§x‚ààU(‚àí1
2,0.26);
‚àí1
1000Nyxi, yw‚ä§x‚ààS
x0‚àà{¬±1,¬±3}U(x0,0.01);
0 ,otherwise.(2)
5Table 2: Training data for var(i)
xxiy
5 1Table 3: Training data for clause( i, i1, i2, i3)
xcixxi1xxi2xxi3xbiy
1 1 1 11
21
Learning rate. The learning rate for SGD is set to be 188
Œ∑= (mz}|{
5, . . . , 5,nz}|{
1
6N, . . . ,1
6N,mz }| {
2000N, . . . , 2000N,1)‚ä§.
Training data. We define two gadgets, var(i)andclause( i, i1, i2, i3), as illustrated in Table 2 and 189
3. All the unspecified coordinates are set to zero. We use T0to denote the training data. var(i) 190
is contained in T0if and only if xi‚ààœÜ, and clause( i, i1, i2, i3)is contained in T0if and only if 191
ci= (xi1‚à®xi2‚à®xi3)‚ààœÜ. 192
Threshold and instance. The threshold Œµcan be any fixed value in R+. The instance is defined as 193
(xtest, ytest), where ytest= 1and 194
xtest= (mz}|{
1, . . . , 1,n+mz}|{
0, . . . , 0,‚àí11m+ 5
2)‚ä§.
The following reduction works for all possible training orders. Intuitively, during the training process, 195
each var(i)in the training data will set wxito around 1(that is, mark xiasTRUE) in the first epoch, 196
and each clause( i, i1, i2, i3)will set wcito near11
2in the second epoch, if and only if exactly one 197
ofwxi1, wxi2, wxi3is near 1and the others near ‚àí1(that is, mark cias satisfied if exactly one of 198
its literals is TRUE and the others FALSE ). The training process terminates at the end of the second 199
epoch. 200
4 Results for Fixed Loss Functions 201
We have proved the NP-hardness for DEBUGGABLE -LINwhen the loss function is not fixed. In 202
this section, we study the complexity when the loss function is fixed as linear and hinge-like 203
functions. Assuming that SGD terminates after only one epoch with a fixed order, we will show 204
thatDEBUGGABLE -LINis solvable in linear time for linear loss. For hinge-like loss functions, 205
DEBUGGABLE -LINcan be solved in linear time only when the dimension d= 1and the interception 206
Œ≤‚â•0. For the rest cases, D EBUGGABLE -LINbecomes NP-hard. 207
4.1 The Easy Case 208
We start with the linear loss function L=‚àíŒ±(yw‚ä§x+Œ≤), with which all the training data are 209
activated and w‚àó=w‚àó(T) =w(0)+P
(x,y)‚ààTŒ±yŒ∑‚äóx. Since ytest‚àà {‚àí 1,1},DEBUGGABLE -LIN 210
is equivalent to deciding whether 211
max
T‚Ä≤‚äÜT{ytest(w‚àó(T‚Ä≤))‚ä§xtest}>0.
A training sample (x, y)is ‚Äúgood‚Äù if ytest(Œ±yŒ∑‚äóx)‚ä§xtest>0and ‚Äúbad‚Äù otherwise. The good 212
training-sample assessment (GTA) algorithm, as shown in Algorithm 1, deals with this situation by 213
greedily picking all ‚Äúgood‚Äù training samples. 214
Denoting T‚àóas the set of all good data in T, it follows that 215
ytest(w‚àó(T‚àó))‚ä§xtest=ytest(w(0))‚ä§xtest+X
(x,y)‚ààT‚àóytest(Œ±yŒ∑‚äóx)‚ä§xtest
‚â•ytest(w(0))‚ä§xtest+X
(x,y)‚ààT‚Ä≤ytest(Œ±yŒ∑‚äóx)‚ä§xtest
for all T‚Ä≤‚äÜT. Hence max T‚Ä≤‚äÜT{ytest(w‚àó(T‚Ä≤))‚ä§xtest}=ytest(w‚àó(T‚àó))‚ä§xtestandDEBUGGABLE - 216
LINcan be solved by GTA in linear time. The following theorem is straightforward. 217
6Theorem 4.1. DEBUGGABLE -LINis linear time solvable for linear loss functions. 218
Algorithm 1: Good Training-sample Assessment (GTA)
Input: Training data T, loss function L, initial parameter w(0), learning rate Œ∑, threshold Œµand
test instance (xtest, ytest).
Output: TRUE, iff SGDŒõ(L,Œ∑, Œµ, T)is debuggable on (xtestytest).
1w‚Üêw(0);
2for(x, y)‚ààTdo
3 ifytest(Œ±yŒ∑‚äóx)‚ä§xtest>0then
4 w‚Üêw+Œ±yŒ∑‚äóx;
5 end
6end
7ifytestw‚ä§xtest‚â•0then
8 return TRUE;
9end
10return FALSE;219
GTA is still effective for one-dimensional classifiers trained with hinge-like losses when Œ≤‚â•0. 220
Theorem 4.2. DEBUGGABLE -LINis linear time solvable for hinge-like loss functions, when d= 1 221
andŒ≤‚â•0. 222
Proof. It suffices to prove that if ‚àÉT‚Ä≤‚äÜTsuch that SGDŒõ(T‚Ä≤, xtest) =ytest,SGDŒõ(T‚àó, xtest) =ytest. 223
a) Suppose all the data in T‚àóare activated, we have 224
ytestw‚àó(T‚àó)xtest=ytestw(0)xtest+X
(x,y)‚ààT‚àóytestŒ±yŒ∑xx test
‚â•ytestw(0)xtest+X
(x,y)‚ààT‚Ä≤‚à©T‚àóytestŒ±yŒ∑xx test+X
(x,y)‚ààT‚Ä≤\T‚àóytestŒ±yŒ∑xx test
=ytestw‚àó(T‚Ä≤)xtest‚â•0
b) Suppose (x, y)‚ààT‚àóis the first inactivated data during the training phase, and wis the current 225
parameter, we have ywx > Œ≤ . Since Œ±Œ∑¬∑(xy)¬∑(xtestytest)‚â•0, we have (xtestytest)¬∑w‚â•0. LetT‚Ä≤‚Ä≤be 226
the set of training data appeared before (x, y), we have ytestw‚àó(T‚àó)xtest‚â•ytestw‚àó(T‚Ä≤‚Ä≤)xtest‚â•0. 227
4.2 The Hard Case 228
The gradient of training data may not always be activated and could be affected by the training order. 229
When the training order is adversarially chosen, the following theorem shows that DEBUGGABLE -LIN 230
is NP-hard for all d‚â•2andŒ≤‚ààR. 231
Theorem 4.3. If the training order is adversarially chosen and d‚â•2,DEBUGGABLE -LINis NP-hard 232
foreach hinge-like loss function at every constant learning rate. 233
Proof sketch. Since the result can be easily extended for all d > 2by padding the other d‚àí2 234
dimensions with zeros, we only prove for the case of d= 2. We assume Œ≤‚â• ‚àí1and leave the 235
Œ≤ <‚àí1case to the appendix. To avoid cluttering, we further assume Œ∑=1andŒ±= 1. The proof 236
can be easily generalized by appropriately re-scaling the constructed vectors. 237
We build a reduction from the subset sum problem, which is well-known to be NP-hard: 238
SUBSET SUM
Input: A set of positive integer S, and a positive integer t.
Output: ‚ÄúYes‚Äù: if ‚àÉS‚Ä≤‚äÜSsuch thatP
a‚ààS‚Ä≤a=t;
‚ÄúNo‚Äù: otherwise.239
7Suppose n=|S|,m= max a‚ààS{a},Œ≥= max {Œ≤,1}andS={a1, a2, . . . , a n}. We further assume 240
n >1. Let the training data be 241
T={(x1, y1),(x2, y2), . . . , (xn, yn)} ‚à™ {(xc, yc),(xb, yb),(xa, ya)}
where xiyi= (‚àöŒ≥
n+1,3‚àöŒ≥ai)for all 1‚â§i‚â§n,xcyc= ((18 n2m2‚àí2)‚àöŒ≥,‚àí3t‚àöŒ≥),xbyb= 242
(‚àöŒ≥,‚àí‚àöŒ≥),xaya= (‚àöŒ≥,‚àöŒ≥). Let w(0)= (‚àí18n2m2‚àöŒ≥,0). Let the test instance (xtest, ytest) 243
satisfy xtestytest= (1,0). 244
Let the training order be (x1, y1),(x2, y2), . . . , (xn, yn),(xc, yc),(xb, yb),(xa, ya). 245
For each 1‚â§i < n , suppose w(0)T‚à©{(xi,yi)|1‚â§j‚â§i}‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí wi, we have 246
yi+1w‚ä§
ixi+1‚â§‚àöŒ≥
n+ 1(‚àí18n2m2‚àöŒ≥+‚àöŒ≥i
n+ 1) + 3‚àöŒ≥ai+1iX
j=13‚àöŒ≥aj
‚â§Œ≥
‚àín‚àí1
n+ 1¬∑9nm2+n
(n+ 1)2
<‚àí1‚â§Œ≤
This means all the T\ {(xc, yc),(xb, yb),(xa, ya)}can be activated. Thus the resulting parameter 247
trained by T\ {(xc, yc),(xb, yb),(xa, ya)}is 248
wc=w(0)+nX
i=1xiyi= 
‚àí18n2m2‚àöŒ≥+‚àöŒ≥|T‚àó|
n+ 1,3‚àöŒ≥nX
i=1ai!
.
It now suffices to prove that for all S‚Ä≤‚äÜS,P
a‚ààS‚Ä≤a=tif and only if ‚àÉT‚Ä≤‚äÜTsuch that 249
w:w(0)T‚Ä≤
‚àí ‚Üíwsatisfies ytestw‚ä§xtest>0. 250
If:Suppose ‚àÉS‚Ä≤‚äÜSsuch thatP
a‚ààSa=t, we prove that ‚àÉT‚Ä≤‚äÜTsuch that ytest(w‚àó)‚ä§xtest>0 251
forw‚àósatisfying w(0)T‚Ä≤
‚àí ‚Üíw‚àó. 252
LetT‚àó={(xi, yi)|ai‚ààS‚Ä≤},T‚Ä≤=T‚àó‚à™ {(xc, yc),(xb, yb),(xa, ya)}. We have 253
wc= (‚àí18n2m2‚àöŒ≥+‚àöŒ≥|T‚àó|
n+ 1,3‚àöŒ≥X
ai‚ààS‚Ä≤ai) = (‚àí18n2m2‚àöŒ≥+‚àöŒ≥|T‚àó|
n+ 1,3‚àöŒ≥t).
And therefore ycw‚ä§
cxc=Œ≥
(‚àí18n2m2+|T‚àó|
n+1)(18n2m2‚àí2)‚àí9t2
<‚àí1‚â§Œ≤, so 254
wc(xc,yc)‚àí ‚àí ‚àí ‚àí ‚Üí wb=wc+xcyc= (‚àöŒ≥(|T‚àó|
n+ 1‚àí2),0).
Note that ybw‚ä§
bxb=Œ≥(|T‚àó|
n+1‚àí2)<‚àí1‚â§Œ≤, we have 255
wb(xb,yb)‚àí ‚àí ‚àí ‚àí ‚Üí wa=wb+xaya= (‚àöŒ≥(|T‚àó|
n+ 1‚àí1),‚àí‚àöŒ≥)
Note also that yaw‚ä§
axa=Œ≥(|T‚àó|
n+1‚àí2)<‚àí1‚â§Œ≤, we have 256
wa(xa,ya)‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí w‚àó=wa+xaya= (|T‚àó|‚àöŒ≥
n+ 1,0)
Therefore, ytest(w‚àó)‚ä§xtest=|T‚àó|‚àöŒ≥
n+1>0. 257
Only if: For each T‚Ä≤‚äÜT, letT‚àó=T‚Ä≤\ {(xc, yc),(xb, yb),(xa, ya)}. Ifytest(w‚àó)‚ä§xtest>0for 258
w‚àósatisfying w(0)T‚Ä≤
‚àí ‚Üíw‚àó, we prove that ‚àÉS‚Ä≤‚äÜSsuch thatP
a‚ààS‚Ä≤a=t. We first show that for 259
eachT‚Ä≤‚äÜT, ifw(w(0)T‚Ä≤
‚àí ‚Üíw)satisfying ytestw‚ä§xtest>0, we have ‚àÄk‚àà {a, b, c},(xk, yk)‚àà 260
T‚Ä≤, ykw‚ä§
kxk< Œ≥, where w(0)T‚àó
‚àí ‚àí ‚Üíwc(xc,yc)‚àí ‚àí ‚àí ‚àí ‚Üí wb(xb,yb)‚àí ‚àí ‚àí ‚àí ‚Üí wa. Otherwise, suppose ‚àÉk‚àà {a, b, c} 261
such that (xk, yk)Ã∏‚ààT‚Ä≤orykw‚ä§
kxk‚â•Œ≥, we have 262
ytestw‚ä§xtest‚â§‚àöŒ≥(|T‚àó|
n+ 1‚àí1)<0
8which contradicts to the fact that ytestw‚ä§xtest‚â•0. 263
LetS‚Ä≤={ai|(xi, yi)‚ààT‚àó}andt‚Ä≤=P
a‚ààS‚Ä≤ai, it suffices to prove t‚Ä≤=t. Notice that 264
w(0)T‚àó‚à©{(xi,yi)|1‚â§j‚â§i}‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí wc= (‚àöŒ≥(‚àí18n2m2+|T‚àó|
n+ 1),3‚àöŒ≥X
ai‚ààS‚Ä≤ai)
= (‚àöŒ≥(‚àí18n2m2+|T‚àó|
n+ 1),3‚àöŒ≥t‚Ä≤)
Hence ycw‚ä§
cxc=Œ≥(‚àí18n2m2+|T‚àó|
n+1)(18n2m2‚àí2)‚àí9Œ≥tt‚Ä≤<‚àí1‚â§Œ≤, thus 265
wc(xc,yc)‚àí ‚àí ‚àí ‚àí ‚Üí wb=wc+xcyc= (‚àöŒ≥(|T‚àó|
n+ 1‚àí2),3‚àöŒ≥(t‚Ä≤‚àít))
(1) If t‚Ä≤‚â§t‚àí1, we have ybw‚ä§
bxb=Œ≥
|T‚àó|
n+1‚àí2 + 3( t‚àít‚Ä≤)
> Œ≥‚â•Œ≤, a contradiction. 266
(2) If t‚Ä≤‚â•t+ 1, we have yaw‚ä§
axa=Œ≥
|T‚àó|
n+1‚àí2 + 3( t‚Ä≤‚àít)
> Œ≥‚â•Œ≤, another contradiction. 267
Therefore t‚Ä≤=t, and this completes the proof. 268
Moreover, D EBUGGABLE -LINis NP-hard even when d= 1andŒ≤ <0. 269
Theorem 4.4. If the training order is adversarially chosen and d= 1,DEBUGGABLE -LINremains 270
NP-hard for each hinge-like loss function with Œ≤ <0atevery constant learning rate. 271
Remarks. The training order in this section can be arbitrary as long as the last three training 272
samples are (xc, yc),(xb, yb),(xa, ya), respectively. All the training samples are ‚Äúgood‚Äù since for 273
each(x, y)‚ààTwe have x‚ä§xtestyytest>0. This implies that DEBUGGABLE -LINis NP-hard even if 274
all the training data are ‚Äúgood‚Äù training samples, and exemplifies why the GTA algorithm fails for 275
higher dimensions. 276
5 Discussion and Conclusion 277
In this paper, we provided a comprehensive analysis on the complexity of DEBUGGABLE . We focus 278
on the linear classifier that is trained using SGD, as it is a key component in the majority of popular 279
models. 280
Since DEBUGGABLE is a special case of data debugging, the above results proved the intractability 281
of data debugging and therefore gives a negative answer to Problem 1.1 declared in the introduction. 282
The complexity results also demonstrated that it is not accurate to estimate the impact of subset of 283
training data by summing up the score of each training samples in the subset, as long as the scores 284
can be calculated in polynomial time . 285
In Section 4, a training sample is said to be ‚Äúgood‚Äù if it can help the resulting model to predict 286
correctly on the test instance. That is, it can increase ytest(w‚àó)‚ä§xtest. However, in our proof we 287
showed that D EBUGGABLE remains NP-hard even if all training samples are ‚Äúgood‚Äù. This suggests 288
that the quality of a training sample does not depend only on some properties of itself but also on 289
the interaction between the rest of the training data, which should be taken into consideration when 290
developing data cleaning approaches. 291
Moreover, the NP-hardness of DEBUGGABLE implies that, it is in general intractable to figure out the 292
causality between even the prediction of a linear classifier and its training data. This may be seem 293
surprising since linear classifiers have long been considered ‚Äúinherently interpretable‚Äù. As warned 294
in [25],a method being ‚Äúinherently interpretable‚Äù needs to be verified before it can be trusted , the 295
concept of interpretability must be rigorously defined , or at least its boundaries specified. 296
Our results suggests the following directions for future research. Firstly, characterizing the training 297
sample may be helpful in designing efficient algorithms for data debugging; Secondly, designing 298
algorithms using CSP-solver is a potential way to solve data debugging more efficiently than the brute- 299
force algorithms; Finally, developing random algorithms is a potential way to solve data debugging 300
successfully with high probability. 301
9References 302
[1]Satoshi Hara, Atsushi Nitanda, and Takanori Maehara. Data Cleansing for Models Trained with SGD . 303
Curran Associates Inc., Red Hook, NY , USA, 2019. 304
[2]Weiyuan Wu, Lampros Flokas, Eugene Wu, and Jiannan Wang. Complaint-driven training data debugging 305
for query 2.0. pages 1317‚Äì1334, 06 2020. doi: 10.1145/3318464.3389696. 306
[3]Bojan Karla≈°, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, and Ce Zhang. Data 307
debugging with shapley importance over end-to-end machine learning pipelines, 2022. 308
[4]Felix Neutatz, Binger Chen, Ziawasch Abedjan, and Eugene Wu. From cleaning before ml to cleaning for 309
ml.IEEE Data Eng. Bull. , 44:24‚Äì41, 2021. URL https://api.semanticscholar.org/CorpusID: 310
237542697 . 311
[5]Peng Li, Xi Rao, Jennifer Blase, Yue Zhang, Xu Chu, and Ce Zhang. Cleanml: A study for evaluating the 312
impact of data cleaning on ml classification tasks. In 2021 IEEE 37th International Conference on Data 313
Engineering (ICDE) , pages 13‚Äì24, 2021. doi: 10.1109/ICDE51399.2021.00009. 314
[6]Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse. If influence functions are 315
the answer, then what is the question? In Proceedings of the 36th International Conference on Neural 316
Information Processing Systems , NIPS ‚Äô22, Red Hook, NY , USA, 2024. Curran Associates Inc. ISBN 317
9781713871088. 318
[7]Romila Pradhan, Jiongli Zhu, Boris Glavic, and Babak Salimi. Interpretable data-based explanations for 319
fairness debugging. In Proceedings of the 2022 International Conference on Management of Data , 320
SIGMOD ‚Äô22, page 247‚Äì261, New York, NY , USA, 2022. Association for Computing Machinery. 321
ISBN 9781450392495. doi: 10.1145/3514221.3517886. URL https://doi.org/10.1145/3514221. 322
3517886 . 323
[8]Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In 324
Proceedings of the 34th International Conference on Machine Learning - Volume 70 , ICML‚Äô17, page 325
1885‚Äì1894. JMLR.org, 2017. 326
[9]Rajiv Khanna, Been Kim, Joydeep Ghosh, and Oluwasanmi Koyejo. Interpreting black box predictions 327
using fisher kernels. In International Conference on Artificial Intelligence and Statistics , 2018. URL 328
https://api.semanticscholar.org/CorpusID:53085397 . 329
[10] Pang Wei Koh, Kai-Siang Ang, Hubert Hua Kian Teo, and Percy Liang. On the accuracy of influence 330
functions for measuring group effects. In Neural Information Processing Systems , 2019. URL https: 331
//api.semanticscholar.org/CorpusID:173188850 . 332
[11] Samyadeep Basu, Xuchen You, and Soheil Feizi. On second-order group influence functions for black- 333
box predictions. In Proceedings of the 37th International Conference on Machine Learning , ICML‚Äô20. 334
JMLR.org, 2020. 335
[12] Han Guo, Nazneen Rajani, Peter Hase, Mohit Bansal, and Caiming Xiong. FastIF: Scalable influence 336
functions for efficient model interpretation and debugging. In Marie-Francine Moens, Xuanjing Huang, 337
Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods 338
in Natural Language Processing , pages 10333‚Äì10350, Online and Punta Cana, Dominican Republic, 339
November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.808. 340
URL https://aclanthology.org/2021.emnlp-main.808 . 341
[13] Amirata Ghorbani and James Y . Zou. Data shapley: Equitable valuation of data for machine learning. 342
ArXiv , abs/1904.02868, 2019. URL https://api.semanticscholar.org/CorpusID:102350503 . 343
[14] R. Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nicholas Hynes, Nezihe Merve G√ºrel, Bo Li, 344
Ce Zhang, Dawn Xiaodong Song, and Costas J. Spanos. Towards efficient data valuation based on the 345
shapley value. ArXiv , abs/1902.10275, 2019. URL https://api.semanticscholar.org/CorpusID: 346
67855573 . 347
[15] Ruoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao, Bhavya Kailkhura, Ce Zhang, Bo Li, and Dawn 348
Song. Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification? 349
In2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages 8235‚Äì8243, 350
2021. doi: 10.1109/CVPR46437.2021.00814. 351
[16] Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nezihe Merve Gurel, Bo Li, Ce Zhang, Costas 352
Spanos, and Dawn Song. Efficient task-specific data valuation for nearest neighbor algorithms. Proc. 353
VLDB Endow. , 12(11):1610‚Äì1623, jul 2019. ISSN 2150-8097. doi: 10.14778/3342263.3342637. URL 354
https://doi.org/10.14778/3342263.3342637 . 355
10[17] Jeremy Mange. Effect of training data order for machine learning. In 2019 International Conference 356
on Computational Science and Computational Intelligence (CSCI) , pages 406‚Äì407, 2019. doi: 10.1109/ 357
CSCI49370.2019.00078. 358
[18] Ernie Chang, Hui-Syuan Yeh, and Vera Demberg. Does the order of training samples matter? improving 359
neural data-to-text generation with curriculum learning. ArXiv , abs/2102.03554, 2021. URL https: 360
//api.semanticscholar.org/CorpusID:231846815 . 361
[19] Yejia Liu, Weiyuan Wu, Lampros Flokas, Jiannan Wang, and Eugene Wu. Enabling sql-based training 362
data debugging for federated learning. Proceedings of the VLDB Endowment , 15:388‚Äì400, 02 2022. doi: 363
10.14778/3494124.3494125. 364
[20] Marc-Etienne Brunet, Colleen Alkalay-Houlihan, Ashton Anderson, and Richard Zemel. Understanding 365
the origins of bias in word embeddings, 2019. 366
[21] Hao Wang, Berk Ustun, and Flavio P. Calmon. Repairing without retraining: Avoiding disparate impact 367
with counterfactual distributions, 2019. 368
[22] Xiaotie Deng and Christos H. Papadimitriou. On the complexity of cooperative solution concepts. Math. 369
Oper. Res. , 19:257‚Äì266, 1994. URL https://api.semanticscholar.org/CorpusID:12946448 . 370
[23] Qi Wang, Yue Ma, Kun Zhao, and Yingjie Tian. A comprehensive survey of loss functions in machine 371
learning. Annals of Data Science , 9, 04 2022. doi: 10.1007/s40745-020-00253-5. 372
[24] Erik D. Demaine, William Gasarch, and Mohammad Hajiaghayi. Computational Intractability: A Guide to 373
Algorithmic Lower Bounds . MIT Press, 2024. 374
[25] Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define and 375
evaluate faithfulness? In Annual Meeting of the Association for Computational Linguistics , 2020. URL 376
https://api.semanticscholar.org/CorpusID:215416110 . 377
[26] Victor Parque. Tackling the subset sum problem with fixed size using an integer representation scheme. 378
In2021 IEEE Congress on Evolutionary Computation (CEC) , pages 1447‚Äì1453, 2021. doi: 10.1109/ 379
CEC45853.2021.9504889. 380
11A Detailed Proofs for Section 3 381
Notations. Given some orderings {oe}of training data, where oe
tas the order of tin epoch e. We 382
usew(e,l)
xito denote the value of wxiafter the l-th iteration in epoch e. We also denote xtandytas 383
the feature and the label of training data t, respectively. We denote t(e,l)as the training sample being 384
considered during epoch e, iteration l. 385
Lemma A.1. Suppose T‚äÜT0is the training data and let Te
l,r={t(e,l),t(e,l+1), . . . ,t(e,r)} 386
be the set of consecutive training samples considered during epoch efrom iteration ltor. For 387
1‚â§l‚â§r‚â§ |T|, ifclause (Œ≥, i1, i2, i3)Ã∏‚ààTe
l,r, then w(e,l‚àí1)
cŒ≥ =w(e,r)
cŒ≥. 388
Proof. For each t‚ààTe
l,r, we have (xt)cŒ≥= 0. Therefore 389
‚àÇL
‚àÇcŒ≥
t‚â§max‚àí12N
5yxcŒ≥,| ‚àíyxcŒ≥|,‚àí1
1000NyxcŒ≥,0
= 0
Hence‚àÇL
‚àÇcŒ≥
t= 0, and 390
w(e,r)
cŒ≥=w(e,l‚àí1)
cŒ≥‚àíŒ∑cŒ≥X
t‚ààTe
l,r‚àÇL
‚àÇcŒ≥
t=w(e,l‚àí1)
cŒ≥
Similarly, (xt)bŒ≥= 0, and 391
‚àÇL
‚àÇbŒ≥
t‚â§max‚àí12N
5yxbŒ≥,| ‚àíyxbŒ≥|,‚àí1
1000NyxbŒ≥,0
= 0
Hence‚àÇL
‚àÇbŒ≥
t= 0, and 392
w(e,r)
bŒ≥=w(e,l‚àí1)
bŒ≥‚àíŒ∑bŒ≥X
t‚ààTe
l,r‚àÇL
‚àÇbŒ≥
t=w(e,l‚àí1)
bŒ≥
393
Lemma A.2. Suppose T‚äÜT0is the training data and Tl:={t(1,1), . . . ,t(1,l)}.‚àÄ1‚â§i‚â§n,1‚â§ 394
l‚â§ |T|,w(1,l)
xi‚ààU(1,l+1
6000N2)ifvar(i)‚ààTl; Otherwise w(1,l)
xi‚ààU(‚àí1,l+1
6000N2). 395
Proof. We prove this lemma by induction. 396
Basic Case: Note that for all 1‚â§i‚â§n,w(0)
xi=‚àí1, and for all 1‚â§Œ≥‚â§m, w(0)
cŒ≥= 1/2, w(0)
bŒ≥=‚àí1. 397
We denote t=t(1,1)to avoid cluttering. For any fixed i: 398
(1) Ift=var(i). We have yt(w(0))‚ä§x‚Ä≤
t= 5w(0)
xi=‚àí5, hence 399
‚àÇL
‚àÇwxi
t=‚àí12N
5yt(xt)i=‚àí12N
and 400
w(1,1)
xi=w(0)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t=‚àí1‚àí1
6N
‚àí12N
5
= 1‚ààU(1,2
6000N2)
(2) Ift=clause( Œ≥, i, i‚Ä≤, i‚Ä≤‚Ä≤). We have 401
yt(w(0))‚ä§x‚Ä≤
t=w(0)
xi+w(0)
xi‚Ä≤+w(0)
xi‚Ä≤‚Ä≤+w(0)
cŒ≥+1
2w(0)
bŒ≥=‚àí3
hence 402
‚àÇL
‚àÇwxi
t=‚àí1
1000Nyt(xt)xi=‚àí1
1000N
12and 403
w(1,1)
xi=w(0)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t=‚àí1‚àí1
6N
‚àí1
1000N
=‚àí1 +1
6000N2‚ààU(‚àí1,2
6000N2)
(3) Otherwise, wxiwill not be updated. Therefore w(1,1)
xi=w(0)
xi=‚àí1‚ààU(‚àí1,2
6000N2). 404
Hence this lemma is true for l= 1. 405
Induction Step: Suppose the lemma is true for l <|T|. We prove that this lemma remains true for 406
l+ 1. We denote t=t(1,l+1)to avoid cluttering. This makes sense since l+ 1‚â§ |T|and thus t‚ààT. 407
For any fixed i: 408
(1) Ift=var(i), then var(i)Ã∏‚ààTlbecause there are at most one var(i)inTfor each i. 409
Therefore w(1,l)
xi‚ààU(‚àí1,l+1
6000N2). We have yt(w(1,l))‚ä§x‚Ä≤
t= 5w(1,l)
xi‚ààU(‚àí5,0.01), and 410
‚àÇL
‚àÇwxi
t=‚àí12N
5yt(xt)i=‚àí12N. Hence 411
w(1,l+1)
xi=w(1,l)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t=w(1,l)
xi‚àí1
6N
‚àí12N
5
=w(1,l)
xi+ 2‚ààU(1,l+ 2
6000N2)
(2) If t=clause( Œ≥, i, i‚Ä≤, i‚Ä≤‚Ä≤). In this case, clause( Œ≥,¬∑,¬∑,¬∑)Ã∏‚ààT1
1,land by Lemma A.1 we have 412
w(1,l)
cŒ≥=w(0)
cŒ≥, w(1,l)
bŒ≥=w(0)
bŒ≥. From the induction hypothesis we have 413
w(1,l)
xi, w(1,l)
xi‚Ä≤, w(1,l)
xi‚Ä≤‚Ä≤‚ààU(¬±1,l+ 1
6000N2)
and thus 414
yt(w(1,l))‚ä§x‚Ä≤
t=w(1,l)
xi+w(1,l)
xi‚Ä≤+w(1,l)
xi‚Ä≤‚Ä≤+w(1,l)
cŒ≥+1
2w(1,l)
bŒ≥
=w(1,l)
xi+w(1,l)
xi‚Ä≤+w(1,l)
xi‚Ä≤‚Ä≤
‚àà[
x0‚àà{¬±1,¬±3}U(x0,3(l+ 1)
6000N2)‚äÜ[
x0‚àà{¬±1,¬±3}U(x0,0.01)
We have‚àÇL
‚àÇwxi
t=‚àí1
1000Nandw(1,l+1)
xi =w(1,l)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t=w(1,l)
xi+1
6000N2. Consider the 415
following cases: 416
‚Ä¢Ifvar(i)‚ààTl, then var(i)‚ààTl+1andw(1,l)
xi‚ààU(1,l+1
6000N2). Therefore w(1,l+1)
xi ‚àà 417
U(1,l+2
6000N2). 418
‚Ä¢Ifvar(i)Ã∏‚ààTl, then var(i)Ã∏‚ààTl+1andw(1,l)
xi‚ààU(‚àí1,l+1
6000N2). Therefore w(1,l+1)
xi‚àà 419
U(‚àí1,l+2
6000N2). 420
(3) Otherwise, wxiwill not be updated, and w(1,l+1)
xi =w(1,l)
xi. Ifvar(i)‚ààTlthenvar(i)‚ààTl+1and 421
w(1,l+1)
xi‚ààU(1,l+2
6000N2); Otherwise var(i)Ã∏‚ààTl+1andw(1,l+1)
xi‚ààU(‚àí1,l+2
6000N2). 422
Hence if the lemma is true for l <|T|, it is also true for l+ 1. Therefore, the lemma is true for all 423
1‚â§l‚â§ |T|. 424
Corollary A.1. Suppose T‚äÜT0is the training data. ‚àÄ1‚â§i‚â§n,1‚â§l‚â§ |T|, ifvar(i)‚ààT, then 425
w(1)
xi‚ààU(1,1
6000N). Otherwise w(1)
xi‚ààU(‚àí1,1
6000N). 426
13Proof. Note that w(1)
xi=w(1,|T|)
xi andN= 2m+n+ 1. By Lemma A.2, if var(i)‚ààTwe have 427
w(1,|T|)
xi‚ààU(1,|T|+ 1
6000N2)‚äÜU(1,m+n+ 1
6000N2)‚äÜU(1,1
6000N)
Ifvar(i)Ã∏‚ààT, we have 428
w(1,|T|)
xi‚ààU(‚àí1,|T|+ 1
6000N2)‚äÜU(‚àí1,m+n+ 1
6000N2)‚äÜU(‚àí1,1
6000N)
429
Lemma A.3. Suppose T‚äÜT0is the training data. ‚àÄ1‚â§Œ≥‚â§m, if‚àÉ1‚â§i1, i2, i3‚â§nsuch that 430
clause( Œ≥, i1, i2, i3)‚ààT, then w(1)
bŒ≥= 0, w(1)
cŒ≥=1
2+1
200N; Otherwise, w(1)
bŒ≥=‚àí1, w(1)
cŒ≥=1
2. 431
Proof. (1) If such tŒ≥=clause( Œ≥, i1, i2, i3)exists in T, by Lemma A.2 we have 432
w(1,o1
tŒ≥)
xi1+w(1,o1
tŒ≥)
xi2+w(1,o1
tŒ≥)
xi3‚àà[
x0‚àà{¬±1,¬±3}U(x0,3(o1
tŒ≥+ 1)
6000N2)‚äÜ[
x0‚àà{¬±1,¬±3}U(x0,0.01)
By Lemma A.1 we have w(1,o1
tŒ≥‚àí1)
cŒ≥ =w(0)
cŒ≥andw(1,o1
tŒ≥‚àí1)
bŒ≥=w(0)
bŒ≥because clause( Œ≥,¬∑,¬∑,¬∑)Ã∏‚àà 433
T1
1,otŒ≥‚àí1. Hence 434
ytŒ≥(w(1,o1
tŒ≥‚àí1))‚ä§x‚Ä≤
tŒ≥=w(1,o1
tŒ≥)
xi1+w(1,o1
tŒ≥)
xi2+w(1,o1
tŒ≥)
xi3+w(1,o1
tŒ≥‚àí1)
cŒ≥ +1
2w(1,o1
tŒ≥‚àí1)
bŒ≥
=w(1,o1
tŒ≥)
xi1+w(1,o1
tŒ≥)
xi2+w(1,o1
tŒ≥)
xi3+w(1,o1
tŒ≥‚àí1)
cŒ≥
‚àà[
x0‚àà{¬±1,¬±3}U(x0,0.01)
We have‚àÇL
‚àÇwcŒ≥
tŒ≥=‚àí1
1000N, and 435
w(1,o1
tŒ≥)
cŒ≥ =w(1,o1
tŒ≥‚àí1)
cŒ≥ ‚àíŒ∑cŒ≥‚àÇL
‚àÇwcŒ≥
tŒ≥=1
2+ 5√ó1
1000N=1
2+1
200N
Similarly,‚àÇL
‚àÇwbŒ≥
tŒ≥=‚àí1
2000Nand 436
w(1,o1
tŒ≥)
bŒ≥=w(1,o1
tŒ≥‚àí1)
bŒ≥‚àíŒ∑bŒ≥‚àÇL
‚àÇwcŒ≥
tŒ≥=‚àí1‚àí2000N√ó(‚àí1
2000N) = 0
Note also that clause( Œ≥,¬∑,¬∑,¬∑)Ã∏‚ààT1
otŒ≥,|T|, by Lemma A.1 we have 437
w(1)
cŒ≥=w(1,|T|)
cŒ≥ =w(1,o1
tŒ≥)
cŒ≥ =1
2+1
200Nandw(1)
bŒ≥=w(1,|T|)
bŒ≥=w(1,o1
tŒ≥)
bŒ≥= 0. 438
(2) If such tŒ≥=clause( Œ≥, i1, i2, i3)does not exist in T, by Lemma A.1 we have w(1)
cŒ≥=w(0)
cŒ≥=1
2439
andw(1)
bŒ≥=w(0)
bŒ≥=‚àí1. 440
Lemma A.4. Suppose T‚äÜT0andClbe the number of clause () inT2
1,l.‚àÄ1‚â§i‚â§n,1‚â§l‚â§ |T|, 441
w(2,l)
xi‚ààU(1,Cl+1/2
6N)ifvar(i)‚ààT; Otherwise w(2,l)
xi‚ààU(‚àí1,Cl+1/2
6N). 442
Proof. Similar to the proof of A.2, we prove this lemma by induction. 443
Basic Case: Note that for all 1‚â§i‚â§n,w(1)
xi=U(¬±1,1
6000N), and for all 1‚â§Œ≥‚â§m, w(1)
cŒ≥‚àà 444
{1
2,1
2+1
200N}, w(1)
bŒ≥‚àà {‚àí 1,0}. We denote t=t(2,1)to avoid cluttering. For any fixed i: 445
(1) Ift=var(i),C1= 0. By Corollary A.1, w(1)
xi=U(1,1
6000N). We have 446
yt(w(1))‚ä§x‚Ä≤
t= 5w(1)
xi‚ààU(5,1
1200N)
14hence‚àÇL
‚àÇwxi
t= 0, and 447
w(2,1)
xi=w(1)
xi‚ààU(1,1
6N) =U(1,Cl+ 1/2
6N)
(2) Ift=clause( Œ≥, i, i‚Ä≤, i‚Ä≤‚Ä≤),C1= 1. By Lemma A.3, we have w(1)
cŒ≥=1
2+1
200Nandw(1)
bŒ≥= 0. 448
Therefore, 449
yt(w(1))‚ä§x‚Ä≤
t=w(1)
xi+w(1)
xi‚Ä≤+w(1)
xi‚Ä≤‚Ä≤+w(1)
cŒ≥+1
2w(1)
bŒ≥
=w(1)
xi+w(1)
xi‚Ä≤+w(1)
xi‚Ä≤‚Ä≤+1
2‚àí1
200N
‚àà[
x0‚àà{1
2¬±1,1
2¬±3}U(x0,0.01)
hence‚àÇL
‚àÇwxi
t‚àà {0,‚àíyxxi}={‚àí1,0}, and Œ∑xi‚àÇL
‚àÇwxi
t‚àà {‚àí1
6N,0}. 450
By Corollary A.1, if var(i)‚ààT, we have 451
w(2,1)
xi=w(1)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t‚ààU(1,3/2
6N) =U(1,Cl+ 1/2
6N)
Ifvar(i)Ã∏‚ààT, we have 452
w(2,1)
xi=w(1)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t‚ààU(‚àí1,3/2
6N) =U(‚àí1,Cl+ 1/2
6N)
(3) Otherwise, wxiwill not be updated and C1‚â§1. Therefore if var(i)‚ààT, 453
w(2,1)
xi=w(1)
xi‚ààU(1,3/2
6N)‚äÜU(1,Cl+ 1/2
6N)
Ifvar(i)Ã∏‚ààT, 454
w(2,1)
xi=w(1)
xi‚ààU(‚àí1,3/2
6N)‚äÜU(‚àí1,Cl+ 1/2
6N)
Hence this lemma is true for l= 1. 455
Induction Step: Suppose the lemma is true for l <|T|. We prove that this lemma remains true for 456
l+ 1. We denote t=t(2,l+1)to avoid cluttering. This makes sense since l+ 1‚â§ |T|and thus t‚ààT. 457
For any fixed i: 458
(1) Ift=var(i),Cl+1=Cl. By Corollary A.1, w(2,l)
xi‚ààU(1,Cl+1/2
6N). 459
We have yt(w(2,l))‚ä§x‚Ä≤
t= 5w(2,l)
xi‚ààU(5,1/6)and‚àÇL
‚àÇwxi
t= 0.Hence w(2,l+1)
xi =w(2,l)
xi‚àà 460
U(1,Cl+1+1/2
6N). 461
(2) Ift=clause( Œ≥, i, i‚Ä≤, i‚Ä≤‚Ä≤),Cl+1=Cl+ 1. In this case, clause( Œ≥,¬∑,¬∑,¬∑)Ã∏‚ààT2
1,land by Lemma 462
A.1 and Lemma A.3 we have w(2,l)
cŒ≥=w(1)
cŒ≥=1
2+1
200N, w(2,l)
bŒ≥=w(1)
bŒ≥= 0. From the induction 463
hypothesis we have w(2,l)
xi, w(2,l)
xi‚Ä≤, w(2,l)
xi‚Ä≤‚Ä≤‚ààU(¬±1,Cl+1/2
6N). Noting that 464
Cl+ 1/2
6N‚â§m+ 1/2
6N=m+ 1/2
(n+ 2(m+ 1/2))‚â§1
12
we have 465
yt(w(2,l))‚ä§x‚Ä≤
t=w(2,l)
xi+w(2,l)
xi‚Ä≤+w(2,l)
xi‚Ä≤‚Ä≤+w(2,l)
cŒ≥+1
2w(2,l)
bŒ≥
=w(2,l)
xi+w(2,l)
xi‚Ä≤+w(2,l)
xi‚Ä≤‚Ä≤+1
2+1
200N
‚àà[
x0‚àà{1
2¬±1,1
2¬±3}U
x0,3(Cl+ 1/2)
6N+1
200N
‚äÜ[
x0‚àà{1
2¬±1,1
2¬±3}U(x0,0.26)
15And thus‚àÇL
‚àÇwxi
t‚àà {0,‚àíyxxi}={‚àí1,0}, and Œ∑xi‚àÇL
‚àÇwxi
t‚àà {‚àí1
6N,0}. 466
By Corollary A.1, if var(i)‚ààT,w(2,l+1)
xi =w(l)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t‚ààU(1,Cl+3/2
6N) =U(1,Cl+1+1/2
6N); 467
ifvar(i)Ã∏‚ààT,w(2,l+1)
xi =w(l)
xi‚àíŒ∑xi‚àÇL
‚àÇwxi
t‚ààU(‚àí1,Cl+3/2
6N) =U(‚àí1,Cl+1+1/2
6N). 468
(3) Otherwise, wxiwill not be updated. We have Cl+1‚â§Cl+ 1w(2,l+1)
xi =w(2,l)
xi. Ifvar(i)‚ààT 469
thenw(2,l+1)
xi‚ààU(1,Cl+1+1/2
6N); Ifvar(i)Ã∏‚ààTthenw(2,l+1)
xi‚ààU(‚àí1,Cl+1+1/2
6N). 470
Hence if the lemma is true for l <|T|, it is also true for l+ 1. Therefore, the lemma is true for all 471
1‚â§l‚â§ |T|. 472
Corollary A.2. Suppose T‚äÜT0is the training data. ‚àÄ1‚â§i‚â§n, ifvar(i)‚ààT, then w(2)
xi‚àà 473
U(1,0.1). Otherwise w(2)
xi‚ààU(‚àí1,0.1). 474
Proof. Note that w(2)
xi=w(2,|T|)
xi andC|T|‚â§m. By Lemma A.4, if var(i)‚ààTwe have 475
w(2,|T|)
xi‚ààU(1,C|T|+ 1/2
6N)‚äÜU(1,m+ 1/2
6N)‚äÜU(1,1
12)‚äÜU(1,0.1)
Ifvar(i)Ã∏‚ààT, we have 476
w(1,|T|)
xi‚ààU(‚àí1,C|T|+ 1/2
6N)‚äÜU(‚àí1,m+ 1/2
6N)‚äÜU(‚àí1,1
12)‚äÜU(‚àí1,0.1)
477
Lemma A.5. Suppose T‚äÜT0is the training data. ‚àÄ1‚â§i‚â§m, if‚àÉ1‚â§i1, i2, i3‚â§nsuch that 478
clause( i, i1, i2, i3)‚ààT, then 479
1.w(2)
bj= 1000 N; 480
2.w(2)
cj=11
2+1
200Nif exactly one of var(i1),var(i2),var(i3) is in T. Otherwise w(2)
cj= 481
1
2+1
200N. 482
Otherwise, w(2)
bi=‚àí1, w(2)
ci=1
2. 483
Proof. (1) If such tŒ≥=clause( Œ≥, i1, i2, i3)exists in T, by Lemma A.4 we have 484
w(2,o1
tŒ≥)
xi1, w(2,o1
tŒ≥)
xi2, w(2,o1
tŒ≥)
xi3‚ààU(¬±1,m+ 1/2
6N)‚äÜU(¬±1,1
12N)
By Lemma A.1 we have w(2,o1
tŒ≥‚àí1)
cŒ≥ =w(1)
cŒ≥=1
2+1
200Nandw(2,o1
tŒ≥‚àí1)
bŒ≥=w(1)
bŒ≥= 0 because 485
clause( Œ≥,¬∑,¬∑,¬∑)Ã∏‚ààT1
1,otŒ≥‚àí1. Consider the following two cases: 486
(a) If exactly one of var(i1),var(i2),var(i3) is in T, by Corollary A.2 we have 487
ytŒ≥(w(2,o1
tŒ≥‚àí1))‚ä§x‚Ä≤
tŒ≥=w(2,o1
tŒ≥‚àí1)
xi1+w(2,o1
tŒ≥‚àí1)
xi2+w(2,o1
tŒ≥‚àí1)
xi3+w(2,o1
tŒ≥‚àí1)
cŒ≥ +1
2w(2,o1
tŒ≥‚àí1)
bŒ≥
=w(2,o1
tŒ≥‚àí1)
xi1+w(2,o1
tŒ≥‚àí1)
xi2+w(2,o1
tŒ≥‚àí1)
xi3+1
2+1
200N
‚ààU(‚àí1
2,3
12N+1
200N)‚äÜU(‚àí1
2,0.26)
Hence‚àÇL
‚àÇwcŒ≥
tŒ≥=‚àí1, and 488
w(2,o1
tŒ≥)
cŒ≥ =w(2,o1
tŒ≥‚àí1)
cŒ≥ ‚àíŒ∑cŒ≥‚àÇL
‚àÇwcŒ≥
tŒ≥=1
2+1
200N+ 5 =11
2+1
200N
16Similarly, 489
w(2,o1
tŒ≥)
bŒ≥=w(2,o1
tŒ≥‚àí1)
bŒ≥‚àíŒ∑bŒ≥‚àÇL
‚àÇwbŒ≥
tŒ≥= 1000 N
Note also that clause( Œ≥,¬∑,¬∑,¬∑)Ã∏‚ààT1
otŒ≥,|T|, by Lemma A.1 we have w(2)
cŒ≥=w(2,|T|)
cŒ≥ =w(2,o1
tŒ≥)
cŒ≥ = 490
11
2‚àí1
200Nandw(2)
bŒ≥=w(2,|T|)
bŒ≥=w(2,o1
tŒ≥)
bŒ≥= 1000 N. 491
(b) Otherwise, we have 492
ytŒ≥(w(2,o1
tŒ≥‚àí1))‚ä§x‚Ä≤
tŒ≥=w(2,o1
tŒ≥‚àí1)
xi1+w(2,o1
tŒ≥‚àí1)
xi2+w(2,o1
tŒ≥‚àí1)
xi3+w(2,o1
tŒ≥‚àí1)
cŒ≥ +1
2w(2,o1
tŒ≥‚àí1)
bŒ≥
=w(2,o1
tŒ≥‚àí1)
xi1+w(2,o1
tŒ≥‚àí1)
xi2+w(2,o1
tŒ≥‚àí1)
xi3+1
2+1
200N
‚àà[
x0‚àà{‚àí7
2,1
2,5
2}U(x0,3
12N+1
200N)‚äÜ[
x0‚àà{‚àí7
2,1
2,5
2}U(x0,0.26)
Hence‚àÇL
‚àÇwcŒ≥
tŒ≥=‚àÇL
‚àÇwbŒ≥
tŒ≥= 0, sow(2,o1
tŒ≥)
cŒ≥ =w(2,o1
tŒ≥‚àí1)
cŒ≥ =1
2+1
200N, w(2,o1
tŒ≥)
bŒ≥=w(2,o1
tŒ≥‚àí1)
bŒ≥= 0. 493
Note also that clause( Œ≥,¬∑,¬∑,¬∑)Ã∏‚ààT1
otŒ≥,|T|, by Lemma A.1 we have w(2)
cŒ≥=w(2,|T|)
cŒ≥ =w(2,o1
tŒ≥)
cŒ≥ = 494
1
2+1
200Nandw(2)
bŒ≥=w(2,|T|)
bŒ≥=w(2,o1
tŒ≥)
bŒ≥= 0. 495
(2) If such tŒ≥=clause( Œ≥, i1, i2, i3)does not exist in T, by Lemma A.1 and Lemma A.3 we have 496
w(2)
cŒ≥=w(1)
cŒ≥=1
2andw(2)
bŒ≥=w(1)
bŒ≥=‚àí1. 497
Moreover, wreaches its fixpoint at the end of the second epoch and will no longer be updated. 498
Lemma A.6. w(2)=w(3). 499
Proof. Suppose w(2)Ã∏=w(3), then there exists 1‚â§i‚â§Nsuch that w(2)
iÃ∏=w(3)
i, and there 500
are some training sample tin the training data such that‚àÇL
‚àÇw(2)
i
tÃ∏= 0. Let t= (xt, yt)and 501
I=U(‚àí5,0.01)‚à™U(‚àí1
2,0.26)‚à™S
x0‚àà{¬±1,¬±3}U(x0,0.01)
. By (2) we have yt(w(2))‚ä§xt‚Ä≤‚ààI. 502
At least one of the following is true: 503
1.‚àÉ1‚â§i‚â§n,t=var(i). According to lemma A.2, yt(w(2))‚ä§xt‚Ä≤=yw(2)
xixi‚àà 504
U(5,0.5)‚äÜR\I, contradicting to yt(w(2))‚ä§xt‚Ä≤‚ààI. 505
2.‚àÉ1‚â§i‚â§mand1‚â§i1, i2, i3‚â§n, such that t=clause( i, i1, i2, i3). According to 506
lemma A.5, we have 507
yt(w(2))‚ä§xt‚Ä≤=w(2)
bi+w(2)
ci+w(2)
xi1+w(2)
xi2+w(2)
xi3
‚â•1000N+1
2+1
200N+ 3√ó(‚àí1‚àí0.1)
‚â•1000‚àí3.3‚â•996
We have yt(w(2))‚ä§xt‚Ä≤Ã∏‚ààI, another contradiction. 508
Therefore w(2)=w(3),wreaches its fixpoint at the end of the second epoch. In other words, 509
w‚àó=w(2). 510
We are now ready to give a rigorous proof of theorem 3.1. 511
17Proof of theorem 3.1. It only suffices to prove the correctness of the reduction in section 3. 512
If.Suppose œÜ‚ààMONOTONE 1-IN-3 SAT , then there is a truth assignment ŒΩ(¬∑)that assigns exactly 513
one variable in each clause of œÜis true. Let ‚àÜ ={var(i)|ŒΩ(xi) =FALSE}. Letw‚Ä≤be the parameter 514
ofSGDŒõ(T0\‚àÜ). By Lemma A.5, (w‚Ä≤)cŒ≥=11
2+1
200Nfor all 1‚â§Œ≥‚â§m, hence 515
(w‚Ä≤)‚ä§xtest=mX
Œ≥=1w‚Ä≤
cŒ≥‚â•11m
2+‚àí11m+ 5
2=5
2>0
andŒªw‚Ä≤(xtest) = 1 , thus SGDŒõ(T0)is thus debuggable. 516
Only if. Suppose SGDŒõ(T0)is debuggable, there will be a ‚àÜsuch that SGDŒõ(T0,xtest) =ytest. We 517
denote w‚Ä≤as the parameter trained by SGDonT0\‚àÜ. We have Œªw‚Ä≤(xtest) = 1 and(w‚Ä≤)‚ä§xtest‚â•0. 518
By Lemma A.5, w‚Ä≤
cŒ≥={1
2+1
200N,11
2+1
200N}. Suppose wc‚àó=1
2+1
200N, then 519
(w‚Ä≤)‚ä§xtest=wc‚àó+X
cŒ≥Ã∏=c‚àówcŒ≥
‚â§11
2(m‚àí1) +1
2+m
200N‚àí11m
2+5
2
=‚àí5
2+m
200N
‚â§ ‚àí5
2+1
200=‚àí2.495<0
leading to a contradiction. 520
As a consequence, w‚Ä≤
cŒ≥=11
2+1
200Nfor all 1‚â§Œ≥‚â§m. By Lemma A.5, exactly one of 521
var(i1),var(i2),var(i3) is in T0\‚àÜfor each cŒ≥= (xi1‚à®xi2‚à®xi3). Consider a truth assignment ŒΩ 522
that maps every xitoFALSE where var(i)‚àà‚àÜ, and maps the rest to TRUE. Then ŒΩassigns exactly 523
one variable true in each cŒ≥= (xi1‚à®xi2‚à®xi3)if and only if exactly one of var(i1),var(i2),var(i3) 524
is inT0\‚àÜ. Hence ŒΩis a truth assignment that assigns true to exactly one variable in each clause of 525
œÜ, and thus œÜis a yes-instance of M ONOTONE 1-IN-3 SAT. 526
B Detailed Proofs for Section 4 527
B.1 Proof of Theorem 4.4 528
Proof. We build a reduction from the SUBSET SUMproblem with a fixed size, which is NP-hard as a 529
particular case of the class of knapsack problems [26]. Formally, it is defined as: 530
SUBSET SUMwith a fixed size
Input: A set of positive integer S, and two positive integers t, k.
Output: ‚ÄúYes‚Äù: if ‚àÉS‚Ä≤‚äÜSof size ksuch thatP
a‚ààS‚Ä≤a=t;
‚ÄúNo‚Äù: otherwise.531
The ordered training data Tis constructed as 532
T={(x1, y1),(x2, y2), . . . , (xn, yn)} ‚à™ {(xa, ya)}
where xiyi=2
3+ai
3P
a‚ààSafor all 1‚â§i‚â§nandxaya= 1 +1
6P
a‚ààSa. LetŒ∑= 1, Œ±= 1, Œ≤=‚àí1, 533
w(0)=‚àí1‚àí2
3k‚àít
3P
a‚ààSaand let the test instance (xtest, ytest)satisfy xtestytest= 1. It now suffices 534
to prove that ‚àÉS‚Ä≤‚äÜSsuch that |S‚Ä≤|=kandP
a‚ààS‚Ä≤a=tif and only if ‚àÉT‚Ä≤‚äÜTsuch that 535
w:w(0)T‚Ä≤
‚àí ‚Üíwsatisfies ytestwx test>0. 536
If:Suppose ‚àÉS‚Ä≤‚äÜSsuch that |S‚Ä≤|=kandP
a‚ààSa=t. Let T‚àó={(xi, yi)|ai‚ààS‚Ä≤}, we prove 537
thatytestw‚àóxtest>0forw‚àósatisfying w(0)T‚Ä≤=T‚àó‚à™{(xa,ya)}‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí w‚àó. 538
18Since
w(0)+X
ai‚ààS‚Ä≤xiyi=‚àí1‚àí2
3k‚àít
3P
a‚ààSa+X
ai‚ààS‚Ä≤2
3+ai
3P
a‚ààSa
=‚àí1‚àí2
3k‚àít
3P
a‚ààSa+X
ai‚ààS‚Ä≤2
3+P
a‚ààS‚Ä≤a
3P
a‚ààSa=‚àí1
and‚àÄ1‚â§i‚â§n, xiyi>2
3, for each 1‚â§i < n , suppose w(0)T‚àó‚à©{(xj,yj)|1‚â§j‚â§i}‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí wi, we have
wixi+1yi+1<Ô£´
Ô£≠w(0)+X
aj‚ààS‚Ä≤xjyj‚àí2
3Ô£∂
Ô£∏¬∑2
3<‚àí10
9< Œ≤.
That is, each training sample in T‚àóis activated. Then for w(0)T‚àó
‚àí ‚àí ‚Üíwa, we have wa=‚àí1. Then, 539
since yawaxa=‚àí(1 +1
6P
a‚ààSa)< Œ≤ andwa(xa,ya)‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí w‚àówe have w‚àó=wa+xaya=1
6P
a‚ààSa. 540
Therefore, ytestw‚àóxtest=1
6P
a‚ààSa>0. 541
Only if: For each T‚Ä≤‚äÜT, letT‚àó=T‚Ä≤\ {(xa, ya)}andc(T‚àó)be the set of training samples in 542
T‚àóthat are activated. If ytestw‚àóxtest‚â•0forw‚àósatisfying w(0) T‚Ä≤
‚àí ‚Üíw‚àó, we prove that the set 543
S‚Ä≤={ai|(xi, yi)‚ààc(T‚àó)}satisfies |S‚Ä≤|=kandP
a‚ààS‚Ä≤a=t. 544
We first show that ytestwaxtest<0forw(0)c(T‚àó)‚àí ‚àí ‚àí ‚Üí wa. Otherwise, suppose ytestwaxtest‚â•0we 545
have wa‚â•0. Let (x, y)be the last training sample of c(T‚Ä≤), since2
3< xy ‚â§1, we have 546
w‚Ä≤‚â•wa‚àíxy‚â• ‚àí1forw‚Ä≤(x,y)‚àí ‚àí ‚àí ‚Üí wa. Thus yw‚Ä≤x‚â•Œ≤, which contradicts to the definition of c(T‚àó). 547
We next show that |S‚Ä≤|=k. Suppose |S‚Ä≤| ‚â§k‚àí1, we have
wa=w(0)+X
(xi,yi)‚ààc(T‚àó)xiyi=‚àí1‚àí2
3k‚àít
3P
a‚ààSa+X
ai‚ààS‚Ä≤2
3+P
a‚ààS‚Ä≤a
3P
a‚ààSa
<‚àí1‚àí2
3k+2
3(k‚àí1) +1
3=‚àí4
3
Thus w‚àó‚â§wa+xaya<‚àí4
3+ (1 +1
6P
a‚ààSa)<0and then ytestw‚àóxtest<0, which contradicts to 548
the fact that ytestw‚àóxtest‚â•0. Therefore |S‚Ä≤| ‚â•k. 549
Suppose |S‚Ä≤| ‚â•k+ 1, we have
wa=w(0)+X
(xi,yi)‚ààc(T‚àó)xiyi‚â• ‚àí1‚àí2
3k‚àí1
3+2
3(k+ 1) = ‚àí2
3
Then yawaxa‚â•(‚àí2
3)¬∑(1 +1
6P
a‚ààSa)‚â• ‚àí7
9‚â•Œ≤, that is, (xa, ya)is not activated and w‚àó=wa. 550
Then since ytestwaxtest<0, we have ytestw‚àóxtest=ytestwaxtest<0, which contradicts to the fact that 551
ytestw‚àóxtest‚â•0. Therefore |S‚Ä≤|=k. 552
It remains to prove thatP
a‚ààS‚Ä≤a=t. Otherwise, supposeP
a‚ààS‚Ä≤a‚â§t‚àí1, we have
wa=w(0)+X
(xi,yi)‚ààc(T‚àó)xiyi‚â§ ‚àí1‚àí2
3k‚àít
3P
a‚ààSa+2
3k+t‚àí1
3P
a‚ààSa
=‚àí1‚àí1
3P
a‚ààSa
Thus ytestw‚àóxtest‚â§ytest(wa+xaya)xtest‚â§ ‚àí1
6P
a‚ààSa<0, which contradicts to the fact that 553
ytestw‚àóxtest‚â•0. ThereforeP
a‚ààS‚Ä≤a‚â•t. 554
SupposeP
a‚ààS‚Ä≤a‚â•t+ 1we have
wa=w(0)+X
(xi,yi)‚ààc(T‚àó)xiyi‚â• ‚àí1‚àí2
3k‚àít
3P
a‚ààSa+2
3k+t+ 1
3P
a‚ààSa
=‚àí1 +1
3P
a‚ààSa
19Thus
yawaxa‚â•(‚àí1 +1
3P
a‚ààSa)¬∑(1 +1
6P
a‚ààSa)
‚â• ‚àí1 +1
6P
a‚ààSa+1
18(P
a‚ààSa)2‚â•Œ≤.
That is, (xa, ya)is not activated and w‚àó=wa. Then since ytestwaxtest<0, we have ytestw‚àóxtest= 555
ytestwaxtest<0, which contradicts to the fact that ytestw‚àóxtest‚â•0. ThereforeP
a‚ààS‚Ä≤a=t. 556
B.2 Proof of Theorem 4.3 for Œ≤ <‚àí1 557
Proof. To avoid cluttering, we still assume Œ∑=1andŒ±= 1. The proof can be generalized by 558
appropriately re-scaling the constructed vectors. 559
LetM=‚àíŒ≤(n+ 2) + 9 Œ≤nm2(n+ 1) + 3 . Suppose n=|S|>1,m= max a‚ààS{a}and 560
S={a1, a2, . . . , a n}. We further assume n >1. Let the ordered set of training samples be 561
T={(x1, y1),(x2, y2), . . . , (xn, yn)} ‚à™ {(xc, yc),(xb, yb),(xa, ya)}
where xiyi= (1
n+1,‚àí3Œ≤ai)for all 1‚â§i‚â§n,xcyc= (M+3
2Œ≤‚àí1, Œ≤(3t‚àí1
2)),xbyb= 562
(1,‚àí1),xaya= (‚àí3
2Œ≤,‚àí3
2Œ≤). Let w(0)= (‚àíM,0). Let the test instance (xtest, ytest)satisfy 563
xtestytest= (1,0). 564
For each 1‚â§i < n , suppose w(0)T‚à©{(xi,yi)|1‚â§j‚â§i}‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí wi, we have 565
yi+1w‚ä§
ixi+1‚â§ ‚àíM¬∑1
n+ 1+i
(n+ 1)2+ 9Œ≤2ai+1iX
j=1aj
‚â§ ‚àíM¬∑1
n+ 1+n
(n+ 1)2+ 9Œ≤2nm2< Œ≤
This means all the (xi, yi)‚ààT\ {(xc, yc),(xb, yb),(xa, ya)}can be activated and thus the resulting 566
parameter trained by T\ {(xc, yc),(xb, yb),(xa, ya)}is 567
wc=w(0)+nX
i=1xiyi= 
‚àíM+|T‚àó|
n+ 1,‚àí3Œ≤nX
i=1ai!
It now suffices to prove that for all S‚Ä≤‚äÜS,P
a‚ààS‚Ä≤a=tif and only if ‚àÉT‚Ä≤‚äÜTsuch that 568
w:w(0)T‚Ä≤
‚àí ‚Üíwsuch that ytestw‚ä§xtest>0. 569
If:Suppose ‚àÉS‚Ä≤‚äÜSsuch thatP
a‚ààSa=t, we prove that ‚àÉT‚Ä≤‚äÜTsuch that ytest(w‚àó)‚ä§xtest>0 570
forw‚àósatisfying w(0)T‚àó
‚àí ‚àí ‚Üíw‚àó. 571
LetT‚àó={(xi, yi)|ai‚ààS‚Ä≤},T‚Ä≤=T‚àó‚à™ {(xc, yc),(xb, yb),(xa, ya)}. We have 572
wc= (‚àíM+|T‚àó|
n+ 1,‚àí3Œ≤X
ai‚ààS‚Ä≤ai) = (‚àíM+|T‚àó|
n+ 1,‚àí3Œ≤t)
Andycw‚ä§
cxc= (‚àíM+|T‚àó|
n+1)(M+3
2Œ≤‚àí1)‚àí3tŒ≤2(3t‚àí1
2)< Œ≤, so 573
wc(xc,yc)‚àí ‚àí ‚àí ‚àí ‚Üí wb=wc+xcyc= (|T‚àó|
n+ 1+3
2Œ≤‚àí1,‚àí1
2Œ≤)
Note that Œ≤ <‚àí1, we have ybw‚ä§
bxb=|T‚àó|
n+1+ 2Œ≤ <(Œ≤+|T‚àó|
n+1) +Œ≤ < Œ≤ , and 574
wb(xb,yb)‚àí ‚àí ‚àí ‚àí ‚Üí wa=wb+xaya= (|T‚àó|
n+ 1+3
2Œ≤,‚àí1
2Œ≤‚àí1)
Note also that yaw‚ä§
axa=3
2(‚àíŒ≤)(|T‚àó|
n+1‚àí1 +Œ≤)< Œ≤, we have 575
wa(xa,ya)‚àí ‚àí ‚àí ‚àí ‚àí ‚Üí w‚àó=wa+xaya= (|T‚àó|
n+ 1,‚àí2Œ≤‚àí1)
20Therefore, ytest(w‚àó)‚ä§xtest=|T‚àó|
n+1‚â•0. 576
Only if: For each T‚Ä≤‚äÜT, letT‚àó=T‚Ä≤\ {(xc, yc),(xb, yb),(xa, ya)}, ifytest(w‚àó)‚ä§xtestforw‚àó577
satisfying w(0)T‚Ä≤
‚àí ‚Üíw‚àó, we prove that ‚àÉS‚Ä≤‚äÜSsuch thatP
a‚ààS‚Ä≤a=t. We first show that for 578
eachT‚Ä≤‚äÜT, ifw(w(0)T‚Ä≤
‚àí ‚Üíw)satisfying ytestw‚ä§xtest‚â•0, we have ‚àÄk‚àà {a, b, c},(xk, yk)‚àà 579
T‚Ä≤, ykw‚ä§
kxk< Œ≤, where w(0)T‚àó
‚àí ‚àí ‚Üíwc(xc,yc)‚àí ‚àí ‚àí ‚àí ‚Üí wb(xb,yb)‚àí ‚àí ‚àí ‚àí ‚Üí wa. Otherwise, suppose ‚àÉk‚àà {a, b, c} 580
such that (xk, yk)Ã∏‚ààT‚Ä≤orykw‚ä§
kxk‚â•Œ≤, we have 581
ytestw‚ä§xtest‚â§ ‚àíM+|T‚àó|
n+ 1+M+3
2Œ≤‚àí1 + 1‚àí3
2Œ≤‚àímin
1, M+3
2Œ≤‚àí1,‚àí3
2Œ≤
=|T‚àó|
n+ 1‚àí1<0
which contradicts to the fact that ytestw‚ä§xtest‚â•0. 582
LetS‚Ä≤={ai|(xi, yi)‚ààT‚àó}andt‚Ä≤=P
a‚ààS‚Ä≤ai, it suffices to prove t‚Ä≤=t. Notice that 583
w(0)T‚àó
‚àí ‚àí ‚Üíwc= (‚àíM+|T‚àó|
n+ 1,‚àí3Œ≤X
ai‚ààS‚Ä≤ai)
= (‚àíM+|T‚àó|
n+ 1,‚àí3Œ≤t‚Ä≤)
Hence ycw‚ä§
cxc= (‚àíM+|T‚àó|
n+1)(M+3
2Œ≤‚àí1)‚àí3t‚Ä≤Œ≤2(3t‚àí1
2)< Œ≤, thus 584
wc(xc,yc)‚àí ‚àí ‚àí ‚àí ‚Üí wb=wc+xcyc= (|T‚àó|
n+ 1+3
2Œ≤‚àí1,‚àí3Œ≤(t‚Ä≤‚àít)‚àí1
2Œ≤)
(1) If t‚Ä≤‚â§t‚àí1, we have 585
ybw‚ä§
bxb=|T‚àó|
n+ 1‚àí1 + 2 Œ≤+ 3Œ≤(t‚Ä≤‚àít)
‚â•|T‚àó|
n+ 1‚àí(1 +Œ≤)>0> Œ≤
a contradiction. Hence wa=wb(xb,yb)‚àí ‚àí ‚àí ‚àí ‚Üí wa= (|T‚àó|
n+1+3
2Œ≤,‚àí3Œ≤(t‚Ä≤‚àít)‚àí1
2Œ≤‚àí1). 586
(2) If t‚Ä≤‚â•t+ 1, we have 587
yaw‚ä§
axa=‚àí3Œ≤
2|T‚àó|
n+ 1‚àí1 +Œ≤‚àí3Œ≤(t‚Ä≤‚àít)
‚â• ‚àí3Œ≤
2|T‚àó|
n+ 1‚àí1‚àí2Œ≤
>‚àí3Œ≤
2|T‚àó|
n+ 1+ 1
>0> Œ≤
another contradiction. Therefore t‚Ä≤=t, and this completes the proof. 588
589
C Limitations 590
It is important to emphasize that the complexity results in section 4 requires the training order to 591
be adversarially chosen. The complexity of DEBUGGABLE for randomly chosen training order is 592
unclear and needs to be figured out in the future research. 593
21NeurIPS Paper Checklist 594
1.Claims 595
Question: Do the main claims made in the abstract and introduction accurately reflect the 596
paper‚Äôs contributions and scope? 597
Answer: [Yes] 598
Justification: The main results are discussed in section 3 and section 4. 599
Guidelines: 600
‚Ä¢The answer NA means that the abstract and introduction do not include the claims 601
made in the paper. 602
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the 603
contributions made in the paper and important assumptions and limitations. A No or 604
NA answer to this question will not be perceived well by the reviewers. 605
‚Ä¢The claims made should match theoretical and experimental results, and reflect how 606
much the results can be expected to generalize to other settings. 607
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals 608
are not attained by the paper. 609
2.Limitations 610
Question: Does the paper discuss the limitations of the work performed by the authors? 611
Answer: [Yes] 612
Justification: See section C in the appendix. 613
Guidelines: 614
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that 615
the paper has limitations, but those are not discussed in the paper. 616
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper. 617
‚Ä¢The paper should point out any strong assumptions and how robust the results are to 618
violations of these assumptions (e.g., independence assumptions, noiseless settings, 619
model well-specification, asymptotic approximations only holding locally). The authors 620
should reflect on how these assumptions might be violated in practice and what the 621
implications would be. 622
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was 623
only tested on a few datasets or with a few runs. In general, empirical results often 624
depend on implicit assumptions, which should be articulated. 625
‚Ä¢The authors should reflect on the factors that influence the performance of the approach. 626
For example, a facial recognition algorithm may perform poorly when image resolution 627
is low or images are taken in low lighting. Or a speech-to-text system might not be 628
used reliably to provide closed captions for online lectures because it fails to handle 629
technical jargon. 630
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms 631
and how they scale with dataset size. 632
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to 633
address problems of privacy and fairness. 634
‚Ä¢While the authors might fear that complete honesty about limitations might be used by 635
reviewers as grounds for rejection, a worse outcome might be that reviewers discover 636
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best 637
judgment and recognize that individual actions in favor of transparency play an impor- 638
tant role in developing norms that preserve the integrity of the community. Reviewers 639
will be specifically instructed to not penalize honesty concerning limitations. 640
3.Theory Assumptions and Proofs 641
Question: For each theoretical result, does the paper provide the full set of assumptions and 642
a complete (and correct) proof? 643
Answer: [Yes] 644
22Justification: The proof of theorem 3.1 is available in section A; The proof of theorem 4.1 645
and theorem 4.2 are available in section 4; The proof of theorem 4.3 is available in section 4 646
and section B; The proof of theorem 4.4 is available in section B. 647
Guidelines: 648
‚Ä¢ The answer NA means that the paper does not include theoretical results. 649
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross- 650
referenced. 651
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems. 652
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if 653
they appear in the supplemental material, the authors are encouraged to provide a short 654
proof sketch to provide intuition. 655
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented 656
by formal proofs provided in appendix or supplemental material. 657
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced. 658
4.Experimental Result Reproducibility 659
Question: Does the paper fully disclose all the information needed to reproduce the main ex- 660
perimental results of the paper to the extent that it affects the main claims and/or conclusions 661
of the paper (regardless of whether the code and data are provided or not)? 662
Answer: [NA] 663
Justification: This paper does not include experiments. 664
Guidelines: 665
‚Ä¢ The answer NA means that the paper does not include experiments. 666
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived 667
well by the reviewers: Making the paper reproducible is important, regardless of 668
whether the code and data are provided or not. 669
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken 670
to make their results reproducible or verifiable. 671
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways. 672
For example, if the contribution is a novel architecture, describing the architecture fully 673
might suffice, or if the contribution is a specific model and empirical evaluation, it may 674
be necessary to either make it possible for others to replicate the model with the same 675
dataset, or provide access to the model. In general. releasing code and data is often 676
one good way to accomplish this, but reproducibility can also be provided via detailed 677
instructions for how to replicate the results, access to a hosted model (e.g., in the case 678
of a large language model), releasing of a model checkpoint, or other means that are 679
appropriate to the research performed. 680
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis- 681
sions to provide some reasonable avenue for reproducibility, which may depend on the 682
nature of the contribution. For example 683
(a)If the contribution is primarily a new algorithm, the paper should make it clear how 684
to reproduce that algorithm. 685
(b)If the contribution is primarily a new model architecture, the paper should describe 686
the architecture clearly and fully. 687
(c)If the contribution is a new model (e.g., a large language model), then there should 688
either be a way to access this model for reproducing the results or a way to reproduce 689
the model (e.g., with an open-source dataset or instructions for how to construct 690
the dataset). 691
(d)We recognize that reproducibility may be tricky in some cases, in which case 692
authors are welcome to describe the particular way they provide for reproducibility. 693
In the case of closed-source models, it may be that access to the model is limited in 694
some way (e.g., to registered users), but it should be possible for other researchers 695
to have some path to reproducing or verifying the results. 696
5.Open access to data and code 697
23Question: Does the paper provide open access to the data and code, with sufficient instruc- 698
tions to faithfully reproduce the main experimental results, as described in supplemental 699
material? 700
Answer: [NA] 701
Justification: This paper does not include experiments requiring code. 702
Guidelines: 703
‚Ä¢ The answer NA means that paper does not include experiments requiring code. 704
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/ 705
public/guides/CodeSubmissionPolicy ) for more details. 706
‚Ä¢While we encourage the release of code and data, we understand that this might not be 707
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not 708
including code, unless this is central to the contribution (e.g., for a new open-source 709
benchmark). 710
‚Ä¢The instructions should contain the exact command and environment needed to run to 711
reproduce the results. See the NeurIPS code and data submission guidelines ( https: 712
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details. 713
‚Ä¢The authors should provide instructions on data access and preparation, including how 714
to access the raw data, preprocessed data, intermediate data, and generated data, etc. 715
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new 716
proposed method and baselines. If only a subset of experiments are reproducible, they 717
should state which ones are omitted from the script and why. 718
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized 719
versions (if applicable). 720
‚Ä¢Providing as much information as possible in supplemental material (appended to the 721
paper) is recommended, but including URLs to data and code is permitted. 722
6.Experimental Setting/Details 723
Question: Does the paper specify all the training and test details (e.g., data splits, hyper- 724
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the 725
results? 726
Answer: [NA] 727
Justification: This paper does not include experiments. 728
Guidelines: 729
‚Ä¢ The answer NA means that the paper does not include experiments. 730
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail 731
that is necessary to appreciate the results and make sense of them. 732
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental 733
material. 734
7.Experiment Statistical Significance 735
Question: Does the paper report error bars suitably and correctly defined or other appropriate 736
information about the statistical significance of the experiments? 737
Answer: [NA] 738
Justification: This paper does not include experiments. 739
Guidelines: 740
‚Ä¢ The answer NA means that the paper does not include experiments. 741
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi- 742
dence intervals, or statistical significance tests, at least for the experiments that support 743
the main claims of the paper. 744
‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for 745
example, train/test split, initialization, random drawing of some parameter, or overall 746
run with given experimental conditions). 747
‚Ä¢The method for calculating the error bars should be explained (closed form formula, 748
call to a library function, bootstrap, etc.) 749
24‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors). 750
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error 751
of the mean. 752
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should 753
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis 754
of Normality of errors is not verified. 755
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or 756
figures symmetric error bars that would yield results that are out of range (e.g. negative 757
error rates). 758
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how 759
they were calculated and reference the corresponding figures or tables in the text. 760
8.Experiments Compute Resources 761
Question: For each experiment, does the paper provide sufficient information on the com- 762
puter resources (type of compute workers, memory, time of execution) needed to reproduce 763
the experiments? 764
Answer: [NA] 765
Justification: This paper does not include experiments. 766
Guidelines: 767
‚Ä¢ The answer NA means that the paper does not include experiments. 768
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster, 769
or cloud provider, including relevant memory and storage. 770
‚Ä¢The paper should provide the amount of compute required for each of the individual 771
experimental runs as well as estimate the total compute. 772
‚Ä¢The paper should disclose whether the full research project required more compute 773
than the experiments reported in the paper (e.g., preliminary or failed experiments that 774
didn‚Äôt make it into the paper). 775
9.Code Of Ethics 776
Question: Does the research conducted in the paper conform, in every respect, with the 777
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ? 778
Answer: [Yes] 779
Justification: They authors have made sure that the research conducted in the paper conform 780
with the NeurIPS Code of Ethics. 781
Guidelines: 782
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. 783
‚Ä¢If the authors answer No, they should explain the special circumstances that require a 784
deviation from the Code of Ethics. 785
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid- 786
eration due to laws or regulations in their jurisdiction). 787
10.Broader Impacts 788
Question: Does the paper discuss both potential positive societal impacts and negative 789
societal impacts of the work performed? 790
Answer: [NA] 791
Justification: The impacts are discussed in section 5 792
Guidelines: 793
‚Ä¢ The answer NA means that there is no societal impact of the work performed. 794
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal 795
impact or why the paper does not address societal impact. 796
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses 797
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations 798
(e.g., deployment of technologies that could make decisions that unfairly impact specific 799
groups), privacy considerations, and security considerations. 800
25‚Ä¢The conference expects that many papers will be foundational research and not tied 801
to particular applications, let alone deployments. However, if there is a direct path to 802
any negative applications, the authors should point it out. For example, it is legitimate 803
to point out that an improvement in the quality of generative models could be used to 804
generate deepfakes for disinformation. On the other hand, it is not needed to point out 805
that a generic algorithm for optimizing neural networks could enable people to train 806
models that generate Deepfakes faster. 807
‚Ä¢The authors should consider possible harms that could arise when the technology is 808
being used as intended and functioning correctly, harms that could arise when the 809
technology is being used as intended but gives incorrect results, and harms following 810
from (intentional or unintentional) misuse of the technology. 811
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation 812
strategies (e.g., gated release of models, providing defenses in addition to attacks, 813
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from 814
feedback over time, improving the efficiency and accessibility of ML). 815
11.Safeguards 816
Question: Does the paper describe safeguards that have been put in place for responsible 817
release of data or models that have a high risk for misuse (e.g., pretrained language models, 818
image generators, or scraped datasets)? 819
Answer: [NA] 820
Justification: This paper only provides theoretical results and poses no such risks. 821
Guidelines: 822
‚Ä¢ The answer NA means that the paper poses no such risks. 823
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with 824
necessary safeguards to allow for controlled use of the model, for example by requiring 825
that users adhere to usage guidelines or restrictions to access the model or implementing 826
safety filters. 827
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors 828
should describe how they avoided releasing unsafe images. 829
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do 830
not require this, but we encourage authors to take this into account and make a best 831
faith effort. 832
12.Licenses for existing assets 833
Question: Are the creators or original owners of assets (e.g., code, data, models), used in 834
the paper, properly credited and are the license and terms of use explicitly mentioned and 835
properly respected? 836
Answer: [NA] 837
Justification: This paper does not use existing assets. 838
Guidelines: 839
‚Ä¢ The answer NA means that the paper does not use existing assets. 840
‚Ä¢ The authors should cite the original paper that produced the code package or dataset. 841
‚Ä¢The authors should state which version of the asset is used and, if possible, include a 842
URL. 843
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset. 844
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of 845
service of that source should be provided. 846
‚Ä¢If assets are released, the license, copyright information, and terms of use in the 847
package should be provided. For popular datasets, paperswithcode.com/datasets 848
has curated licenses for some datasets. Their licensing guide can help determine the 849
license of a dataset. 850
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of 851
the derived asset (if it has changed) should be provided. 852
26‚Ä¢If this information is not available online, the authors are encouraged to reach out to 853
the asset‚Äôs creators. 854
13.New Assets 855
Question: Are new assets introduced in the paper well documented and is the documentation 856
provided alongside the assets? 857
Answer: [NA] 858
Justification: This paper does not release new assets. 859
Guidelines: 860
‚Ä¢ The answer NA means that the paper does not release new assets. 861
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their 862
submissions via structured templates. This includes details about training, license, 863
limitations, etc. 864
‚Ä¢The paper should discuss whether and how consent was obtained from people whose 865
asset is used. 866
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either 867
create an anonymized URL or include an anonymized zip file. 868
14.Crowdsourcing and Research with Human Subjects 869
Question: For crowdsourcing experiments and research with human subjects, does the paper 870
include the full text of instructions given to participants and screenshots, if applicable, as 871
well as details about compensation (if any)? 872
Answer: [NA] 873
Justification: This paper does not involve crowdsourcing nor research with human subjects. 874
Guidelines: 875
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 876
human subjects. 877
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu- 878
tion of the paper involves human subjects, then as much detail as possible should be 879
included in the main paper. 880
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation, 881
or other labor should be paid at least the minimum wage in the country of the data 882
collector. 883
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human 884
Subjects 885
Question: Does the paper describe potential risks incurred by study participants, whether 886
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) 887
approvals (or an equivalent approval/review based on the requirements of your country or 888
institution) were obtained? 889
Answer: [NA] 890
Justification: This paper does not involve crowdsourcing nor research with human subjects. 891
Guidelines: 892
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with 893
human subjects. 894
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent) 895
may be required for any human subjects research. If you obtained IRB approval, you 896
should clearly state this in the paper. 897
‚Ä¢We recognize that the procedures for this may vary significantly between institutions 898
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the 899
guidelines for their institution. 900
‚Ä¢For initial submissions, do not include any information that would break anonymity (if 901
applicable), such as the institution conducting the review. 902
27