User Inference Attacks on Large Language Models
Nikhil Kandpal1Krishna Pillutla2Alina Oprea2,3
Peter Kairouz2Christopher A. Choquette-Choo2Zheng Xu2
1University of Toronto & Vector Institute2Google3Northeastern University
Abstract
Fine-tuning is a common and effective method for tailoring large language models (LLMs)
to specialized tasks and applications. In this paper, we study the privacy implications of fine-
tuning LLMs on user data. To this end, we consider a realistic threat model, called user
inference , wherein an attacker infers whether or not a userâ€™sdata was used for fine-tuning. We
design attacks for performing user inference that require only black-box access to the fine-tuned
LLM and a few samples from a user which need not be from the fine-tuning dataset. We find
that LLMs are susceptible to user inference across a variety of fine-tuning datasets, at times
with near perfect attack success rates. Further, we theoretically and empirically investigate the
properties that make users vulnerable to user inference, finding that outlier users, users with
identifiable shared features between examples, and users that contribute a large fraction of the
fine-tuning data are most susceptible to attack. Based on these findings, we identify several
methods for mitigating user inference including training with example-level differential privacy,
removing within-user duplicate examples, and reducing a userâ€™s contribution to the training
data. While these techniques provide partial mitigation of user inference, we highlight the need
to develop methods to fully protect fine-tuned LLMs against this privacy risk.
1 Introduction
Successfully applying large language models (LLMs) to real-world problems is often best achieved
by fine-tuning on domain-specific data (Liu et al., 2022; Mosbach et al., 2023). This approach is
seen in a variety of commercial products deployed today, e.g., GitHub Copilot (Chen et al., 2021),
Gmail Smart Compose (Chen et al., 2019), GBoard (Xu et al., 2023), etc., that are based on LLMs
trained or fine-tuned on domain-specific data collected from users. The practice of fine-tuning on
user dataâ€”particularly on sensitive data like emails, texts, or source codeâ€”comes with privacy
concerns, as LLMs have been shown to leak information from their training data (Carlini et al.,
2021), especially as models are scaled larger (Carlini et al., 2023). In this paper, we study the
privacy risks posed to users whose data are leveraged to fine-tune LLMs.
Most existing privacy attacks on LLMs can be grouped into two categories: membership in-
ference, in which the attacker obtains access to a sample and must determine if it was trained
on (Mireshghallah et al., 2022; Mattern et al., 2023; Niu et al., 2023); and extraction attacks , in
which the attacker tries to reconstruct the training data by prompting the model with different
prefixes (Carlini et al., 2021; Lukas et al., 2023). These threat models make no assumptions about
the origin of the training data and thus cannot estimate the privacy risk to a user that contributes
many training samples that share characteristics (e.g., topic, writing style, etc.). To this end, we
consider the threat model of user inference (Miao et al., 2021; Hartmann et al., 2023) for the first
1arXiv:2310.09266v2  [cs.CR]  23 Feb 2024Pre-trained LLMFinetuned LLM ğ‘!User-level finetuned dataTraining samplesSamples known by attackerQuery accessAdversary
Target User ğ‘ˆ2.For	each	ğ‘¥(#)	compute	ğ‘!(ğ‘¥(#))3. Test statistic 4ğ‘‡ğ‘¥(%),â€¦,ğ‘¥(&)=%&âˆ‘#'%&log(!()(#))(%&'()(#))4. ğ‘ˆ was in training if 4ğ‘‡ğ‘¥(%),â€¦,ğ‘¥(&)>ğœ1. Sample ğ‘¥(%),â€¦,ğ‘¥&	from	ğ·+
User ğ‘ˆUser ğ´User ğµ
Figure 1: The user inference threat model. An LLM is fine-tuned on user-stratified data. The
adversary can query samples on the fine-tuned model to compute likelihoods. The adversary can
access samples from a userâ€™s distribution (different than the user training samples) to compute a
likelihood score to determine if the user participated in training.
time for LLMs. We show that user inference is a realistic privacy attack for LLMs fine-tuned on
user data.
Inuserinference(seeFigure1), theattackeraimstodetermineifaparticularuserparticipatedin
LLM fine-tuning using only a few freshsamples from the user and black-box access to the fine-tuned
model. This threat model lifts membership inference from the privacy of individual samples to the
privacy of users who contribute multiple samples, while also relaxing the stringent assumption that
the attacker has access to the exact fine-tuning data. By itself, user inference could be a privacy
threat if the fine-tuning task reveals sensitive information about participating users (e.g., a model
is fine-tuned only on users with a rare disease). Moreover, user inference may also enable other
attacks extracting sensitive information about specific users, similar to how membership inference
is used as a subroutine in training data extraction attacks (Carlini et al., 2021).
In this work, we construct a simple and practical user inference attack that determines if a user
participated in LLM fine-tuning. It involves computing a likelihood ratio test statistic normalized
relative to a reference model (Section 3). This attack can be efficiently mounted even at the LLM
scale. We empirically study its effectiveness on the GPT-Neo family of LLMs (Black et al., 2021)
whenfine-tunedondiversedatadomains, includingemails, socialmediacomments, andnewsarticles
(Section 4.2). This study gives insight into the various parameters that affect vulnerability to user
inferenceâ€”such as uniqueness of a userâ€™s data distribution, amount of fine-tuning data contributed
by a user, and amount of attacker knowledge about a user.
We evaluate the attack on synthetically generated canary users to characterize the privacy
leakage for worst-case users (Section 4.3). We show that canary users constructed via minimal
modifications to the real usersâ€™ data increase the attackâ€™s effectiveness (in AUROC) by up to 40%.
This indicates that simple features shared across a userâ€™s samples like an email signature or a
characteristic phrase, can greatly exacerbate the risk of user inference.
Finally, we evaluate several methods for mitigating user inference, such as limiting the number
of fine-tuning samples contributed by each user, removing duplicates within a userâ€™s samples, early
stopping, gradient clipping, and fine-tuning with example-level differential privacy (DP). Our results
show that duplicates within a userâ€™s examples can exacerbate the risk of user inference, but are not
necessary for a successful attack. Additionally, limiting a userâ€™s contribution to the fine-tuning
set can be effective but is only feasible for data-rich applications with a large number of users.
Finally, example-level DP provides some defense but is ultimately designed to protect the privacy of
2individual examples, rather than users that contribute multiple examples. These results highlight
the importance of future work on scalable user-level DP algorithms that have the potential to
provably mitigate user inference (McMahan et al., 2018; Levy et al., 2021). Overall, we are the
first to study user inference against LLMs and provide key insights to inform future deployments of
LLMs fine-tuned on user data.
2 Related Work
There are many different ML privacy attacks with different objectives (Oprea and Vassilev, 2023):
membership inference attacks determine if a particular data sample was part of a modelâ€™s training
set (Shokri et al., 2017; Yeom et al., 2018; Carlini et al., 2022; Ye et al., 2022; Watson et al.,
2022; Choquette-Choo et al., 2021; Jagielski et al., 2023a); data reconstruction aims to exactly
reconstruct the training data of a model, typically for a discriminative model (Haim et al., 2022);
anddata extraction attacks aim to extract training data from generative models like LLMs (Carlini
et al., 2021; Lukas et al., 2023; Ippolito et al., 2023; Anil et al., 2023; Kudugunta et al., 2023; Nasr
et al., 2023).
Membership inference attacks on LLMs. Mireshghallah et al. (2022) introduce a likelihood
ratio-based attack on LLMs, designed for masked language models, such as BERT. Mattern et al.
(2023) compare the likelihood of a sample against the average likelihood of a set of neighboring
samples, and eliminate the assumption of attacker knowledge of the training distribution used in
prior works. Debenedetti et al. (2023) study how systems built on LLMs may amplify membership
inference. Carlini et al. (2021) use a perplexity-based membership inference attack to extract
training data from GPT-2. Their attack prompts the LLM to generate sequences of text, and then
uses membership inference to identify sequences copied from the training set. Note that membership
inference requires access to exact training samples while user inference does not.
Extraction attacks. Following Carlini et al. (2021), memorization in LLMs received much atten-
tion (Zhang et al., 2021; Tirumala et al., 2022; Biderman et al., 2023; Anil et al., 2023). These works
found that memorization scales with model size (Carlini et al., 2023) and data repetition (Kandpal
et al., 2022), may eventually be forgotten (Jagielski et al., 2023b), and can exist even on models
trained for specific restricted use-cases like translation (Kudugunta et al., 2023). Lukas et al. (2023)
develop techniques to extract PII information from LLMs and (Inan et al., 2021) design metrics to
measure how much of userâ€™s confidential data is leaked by the LLM. Once a userâ€™s participation is
identified by user inference, these techniques can be used to estimate the amount of privacy leakage.
User-level membership inference. Much prior work on inferring a userâ€™s participation in train-
ing makes the stronger assumption that the attacker has access to a userâ€™s exact training samples.
We call this user-level membership inference to distinguish it from user inference (which does not
require access to the exact training samples). Song and Shmatikov (2019) give the first such an
attack for generative text models. Their attack is based on training multiple shadow models and
does not scale to LLMs. This threat model has also been studied for text classification via reduction
to membership inference (Shejwalkar et al., 2021).
3User inference. This threat model was considered for speech recognition in IoT devices (Miao
et al., 2021), representation learning (Li et al., 2022) and face recognition (Chen et al., 2023).
Hartmann et al. (2023) formally define user inference for classification and regression but call it
distributional membership inference . These attacks are domain-specific or require shadow models.
Thus, they do not apply or scale to LLMs. Instead, we design an efficient user inference attack that
scales to LLMs and illustrate the user-level privacy risks posed by fine-tuning on user data. See
Appendix C for further discussion.
3 User Inference Attacks
Consider an autoregressive language model pÎ¸that defines a distribution pÎ¸(xt|x<t)over the next
token xtin continuation of a prefix x<t.= (x1, . . . , x tâˆ’1). We are interested in a setting where a
pretrained LLM pÎ¸0with initial parameters Î¸0is fine-tuned on a dataset DFTsampled i.i.d. from a
distribution Dtask. The most common objective is to minimize the cross entropy of predicting each
next token xtgiven the context x<tfor each fine-tuning sample xâˆˆDFT. Thus, the fine-tuned
model pÎ¸is trained to maximize the log-likelihoodP
xâˆˆDFTlogpÎ¸(x) =P
xâˆˆDFTP|x|
t=1logpÎ¸(xt|x<t)
of the fine-tuning set DFT.
Fine-tuning with user-stratified data. Much of the data used to fine-tune LLMs has a user-
level structure. For example, emails, messages, and blog posts can reflect the specific characteristics
of their author. Two text samples from the same user are more likely to be similar to each other
than samples across users in terms of language use, vocabulary, context, and topics. To capture
user-stratification, we model the fine-tuning distribution Dtaskas a mixture
Dtask=Pn
u=1Î±uDu (1)
ofnuser data distributions D1, . . . ,Dnwith non-negative weights Î±1, . . . , Î± nthat sum to one. One
can sample from Dtaskby first sampling a user uwith probability Î±uand then sampling a document
xâˆ¼ D ufrom the userâ€™s data distribution. We note that the fine-tuning process of the LLM is
oblivious to user-stratification of the data.
The user inference threat model. The task of membership inference assumes that an attacker
has access to a text sample xand must determine whether that particular sample was a part of the
training or fine-tuning data (Shokri et al., 2017; Yeom et al., 2018; Carlini et al., 2022). The user
inference threat model relaxes the assumption that the attacker has access to samples from the
fine-tuning data.
The attacker aims to determine if anydata from user uwas involved in fine-tuning the model
pÎ¸using mi.i.d. samples x(1:m):= (x(1), . . . ,x(m))âˆ¼ Dm
ufrom user uâ€™s distribution. Crucially, we
allow x(i)/âˆˆDFT, i.e., the attacker is not assumed to have access to the exact samples of user uthat
were a part of the fine-tuning set. For instance, if an LLM is fine-tuned on user emails, the attacker
can reasonably be assumed to have access to someemails from a user, but not necessarily the ones
used to fine-tune the model. We believe this is a realistic threat model for LLMs, as it does not
require exact knowledge of training set samples, as in membership inference attacks.
We assume that the attacker has black-box access to the LLM pÎ¸â€” they can only query the
modelâ€™s likelihood on a sequence of tokens and might not have knowledge of either the model
4architecture or parameters. Following standard practice in membership inference (Mireshghallah
et al., 2022; Watson et al., 2022), we allow the attacker access to a reference model prefthat is
similar to the target model pÎ¸but has not been trained on user uâ€™s data. This can simply be the
pre-trained model pÎ¸0or another LLM.
Attack strategy. The attackerâ€™s task can be formulated as a statistical hypothesis test. Letting
Pudenote the set of models trained on user uâ€™s data, the attacker aims to test:
H0:pÎ¸/âˆˆ Pu, H 1:pÎ¸âˆˆ Pu. (2)
There is generally no prescribed recipe to test for such a composite hypothesis. Typical attack
strategies involve training multiple â€œshadowâ€ models (Shokri et al., 2017); see Appendix B. This,
however, is infeasible at LLM scale.
The likelihood under the fine-tuned model pÎ¸is a natural test statistic: we might expect pÎ¸(x(i))
to be high if H1is true and low otherwise. Unfortunately, this is not always true, even for member-
ship inference. Indeed, pÎ¸(x)can be large for x/âˆˆDFTfor easy-to-predict x(e.g., generic text using
common words), while pÎ¸(x)can be small even if xâˆˆDFTfor hard-to-predict x. This necessitates
the need for calibrating the test using a reference model (Mireshghallah et al., 2022; Watson et al.,
2022).
We overcome this difficulty by replacing the attackerâ€™s task with surrogate hypotheses that are
easier to test efficiently:
Hâ€²
0:x(1:m)âˆ¼pm
ref, Hâ€²
1:x(1:m)âˆ¼pm
Î¸. (3)
By construction, Hâ€²
0is always false since prefis not fine-tuned on user uâ€™s data. However, Hâ€²
1
is more likely to be true if the user uparticipates in training andthe samples contributed by uto
the fine-tuning dataset DFTare similar to the samples x(1:m)known to the attacker even if they
are not identical. In this case, the attacker rejects Hâ€²
0. Conversely, if user udid not participate in
fine-tuning and no samples from DFTare similar to x(1:m), then the attacker finds both Hâ€²
0andHâ€²
1
to be equally (im)plausible, and fails to reject Hâ€²
0. Intuitively, to faithfully test H0vs.H1using Hâ€²
0
vs.Hâ€²
1, we require that x,xâ€²âˆ¼ D uare closer on average than xâˆ¼ D uandxâ€²â€²âˆ¼ D uâ€²for any other
uâ€²Ì¸=u.
The Neyman-Pearson lemma tells us that the likelihood ratio test is the most powerful for testing
Hâ€²
0vs.Hâ€²
1, i.e., it achieves the best true positive rate at any given false positive rate (Lehmann
et al., 1986, Thm. 3.2.1). This involves constructing a test statistic using the log-likelihood ratio
T(x(1), . . . ,x(m)) := log 
pÎ¸(x(1), . . . ,x(m))
pref(x(1), . . . ,x(m))!
=mX
i=1log 
pÎ¸(x(i))
pref(x(i))!
,(4)
where the last equality follows from the independence of each x(i), which we assume. Although
independence may be violated in some domains (e.g. email threads), it makes the problem more
computationally tractable. As we shall see, this already gives us relatively strong attacks.
Given a threshold Ï„, the attacker rejects the null hypothesis and declares that uhas participated
in fine-tuning if T(x(1), . . . ,x(m))> Ï„. In practice, the number of samples mavailable to the
attacker might vary for each user, so we normalize the statistic by m. Thus, our final attack
statistic is Ë†T(x(1), . . . ,x(m)) =1
mT(x(1), . . . ,x(m)).
5Dataset User Field #Users #ExamplesPercentiles of Examples/User
P0P25P50P75P100
Reddit Comments User Name 5194 1002 K 100 116 144 199 1921
CC News Domain Name 2839 660 K 30 50 87 192 24480
Enron Emails Senderâ€™s Email Address 136 91 K 28 107 279 604 4280
Table 1: Evaluation dataset summary statistics : The three evaluation datasets vary in their notion
of â€œuserâ€ (i.e. a Reddit comment belongs to the username that it was posted from whereas a CC News article
belongs to the web domain where the article was published). Additionally, these datasets span multiple
orders of magnitude in terms of number of users and number of examples contributed per user.
Analysis of the attack statistic. We analyze this attack statistic in a simplified setting to
gain some intuition. In the large sample limit as mâ†’ âˆ, the mean statistic Ë†Tapproximates the
population average
Â¯T(Du) :=Exâˆ¼Du
logpÎ¸(x)
pref(x)
. (5)
We will analyze this test statistic for the choice pref=Dâˆ’uâˆP
uâ€²Ì¸=uÎ±uâ€²Duâ€², which is the fine-
tuning mixture distribution excluding the data of user u. This is motivated by the results of Watson
et al. (2022) and Sablayrolles et al. (2019), who show that using a reference model trained on the
whole dataset excluding a single sample approximates the optimal membership inference classifier.
LetKL(Â·âˆ¥Â·)andÏ‡2(Â·âˆ¥Â·)denote the Kullbackâ€“Leibler and Ï‡2divergences. We establish a bound
(proved in Appendix A) assuming pÎ¸, prefperfectly capture their target distributions.
Proposition 1. Assume pÎ¸=Dtaskandpref=Dâˆ’ufor some user uâˆˆ[n]. Then, we have
log (Î±u) + KL( Duâˆ¥ Dâˆ’u)<Â¯T(Du)â‰¤Î±uÏ‡2(Duâˆ¥Dâˆ’u).
This suggests the attacker may more easily infer:
(a) users who contribute more data (so Î±uis large), or
(b) users who contribute unique data (so KL(Duâˆ¥Dâˆ’u)andÏ‡2(Duâˆ¥Dâˆ’u)are large).
Conversely, if neither holds, then a userâ€™s participation in fine-tuning cannot be reliably detected.
Our experiments corroborate these and we use them to design mitigations.
4 Experiments
Inthissection, weempiricallystudythesusceptibilityofmodelstouserinferenceattacks, thefactors
that affect attack performance, and potential mitigation strategies.
4.1 Experimental Setup
Datasets. We evaluate user inference attacks on three user-stratified text datasets representing
differentdomains: RedditComments(Baumgartneretal.,2020)forsocialmediacontent, CCNews1
1While CC News does not strictly have user data, it is made up of non-identical groups (as in Eq. (1)) defined by
the web domain. We treat each group as a â€œuserâ€ as in Charles et al. (2023).
6âˆ’1 0 1 2
Attack Statistic0200400600CountReddit Comments (AUC = 0.56)
âˆ’2 0 2 4
Attack Statistic0100200300400500CC News (AUC = 0.66)
Held-In Users
Held-Out Users0.0 0.5 1.0
Attack Statistic03691215Enron Emails (AUC = 0.88)
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateROC Curves
10âˆ’310âˆ’210âˆ’110âˆ’310âˆ’210âˆ’1
Reddit Comments
CC NewsEnron Emails
Random Classiï¬erFigure 2: Our attack can achieve significant AUROC , e.g., on the Enron emails dataset. Left
three: Histogramsoftheteststatisticsforheld-inandheld-outusersforthethreeattackevaluationdatasets.
Rightmost : Their corresponding ROC curves.
(Hamborg et al., 2017) for news articles, and Enron Emails (Klimt and Yang, 2004) for user emails.
These datasets are diverse in their domain, notion of a user, number of users, and amount of data
contributed per user (Table 1). We also report results for the ArXiv Abstracts dataset (Clement
et al., 2019) in Appendix E.
To make these datasets suitable for evaluating user inference, we split them into a held-in set of
users to fine-tune models, and a held-out set of users to evaluate attacks. Additionally, we set aside
10% of each userâ€™s samples as the samples used by the attacker to run user inference attacks; these
samples are not used for fine-tuning. For more details on the dataset preprocessing, see Appendix D.
Models. We evaluate user inference attacks on the 125M and 1.3B parameter decoder-only LLMs
from the GPT-Neo (Black et al., 2021) model suite. These models were pre-trained on The Pile
dataset (Gao et al., 2020), an 825GB diverse text corpus, and use the same architecture and pre-
training objectives as the GPT-2 and GPT-3 models. Further details on the fine-tuning are given
in Appendix D.
Attack and Evaluation. We implement the user inference attack of Section 3 using the pre-
trained GPT-Neo models as the reference pref. Following the membership inference literature, we
evaluate the aggregate attack success using the Receiver Operating Characteristic (ROC) curve
across held-in and held-out users; this is a plot of the true positive rate (TPR) and false positive
rate (FPR) of the attack across all possible thresholds. We use the area under this curve (AUROC)
as a scalar summary. We also report the TPR at small FPR (e.g., 1%) (Carlini et al., 2022).
Remarks on Fine-Tuning Data. Due to the size of pre-training datasets like The Pile, we found
it challenging to find user-stratified datasets that were not part of pre-training; this is a problem
with LLM evaluations in general (Sainz et al., 2023). However, we believe that our setup still
faithfully evaluates the fine-tuning setting for two main reasons. First, the overlapping fine-tuning
data constitutes only a small fraction of all the data in The Pile. Second, our attacks are likely
only weakened (and thus, underestimate the true risk) by this setup. This is because inclusion of
the held-out users in pre-training should only reduce the modelâ€™s loss on these samples, making the
loss difference smaller and thus our attack harder to employ.
70 100 200 300 400 500
Fine-Tuning Iteration ( Ã—103)2.752.802.852.902.953.00Validation Loss
0 50 100 150 200 250
Fine-Tuning Iteration ( Ã—103)2.83.03.2Validation Loss
0 10 20 30
Fine-Tuning Iteration ( Ã—103)2.42.62.83.0Validation Loss
50525456
Attack AUROC (%)Reddit Comments
50556065
Attack AUROC (%)CC News
60708090
Attack AUROC (%)Enron Emails
Attack AUROC Validation Loss of Held-In Users Validation Loss of Held-Out UsersFigure 3: Attack success over fine-tuning : User inference AUROC and the held-in/held-out validation
loss.
125M 1.3B
Model Size1.752.002.252.502.753.003.253.50Validation Lossâˆ† = 0.53
âˆ† = 0.57User-level Generalization vs. Model Size
Held-In
Held-Out
125M 1.3B
Model Size0.600.620.640.660.680.70Attack AUROCAttack AUROC vs. Model Size
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateROC Curves vs. Model Size
10âˆ’310âˆ’210âˆ’110âˆ’310âˆ’210âˆ’1125M
1.3B
Figure 4: Attack success vs. model scale : User inference attack performance in 125M and 1.3B
models trained on CC News. Left: Although the 1.3B model achieves lower validation loss, the difference
in validation loss between held-in and held-out users is the same as that of the 125M model. Center &
Right: User inference attacks against the 125M and 1.3B models achieve the same performance.
4.2 User Inference: Results and Properties
We examine how user inference is impacted by factors such as the amount of user data and attacker
knowledge, the model scale, as well as the connection to overfitting.
Attack Performance. We attack GPT-Neo 125M fine-tuned on each of the three fine-tuning
datasets and evaluate the attack performance. We see from Figure 2 that the user inference attacks
on all three datasets achieve non-trivial performance, with the attack AUROC varying between 88%
(Enron) to 66%(CC News) and 56%(Reddit).
Thedisparityinperformancebetweenthethreedatasetscanbeexplainedinpartbytheintuition
from Proposition 1, which points out two factors. First, a larger fraction of data contributed by a
user makes user inference easier. The Enron dataset has fewer users, each of whom contributes a
significant fraction of the fine-tuning data (cf. Table 1), while, the Reddit dataset has a large number
of users, each with few datapoints. Second, distinct user data makes user inference easier. Emails
are more distinct due to identifying information such as names (in salutations and signatures) and
addresses, while news articles or social media comments from a particular user may share more
subtle features like topic or writing style.
82123
Num. Attacker Docs0.550.560.57Attack AUROCReddit Comments
212325
Num. Attacker Docs0.6000.6250.6500.675CC News
212325
Num. Attacker Docs0.70.80.9Enron EmailsFigure 5: Attack performance vs. attacker knowledge : As we increase the number of examples
given to the attacker, the attack performance increases across all three datasets. The shaded area denotes
the std over 100random draws of attacker examples.
The Effect of the Attacker Knowledge. We examine the effect of the attacker knowledge (the
amount of user data used by the attacker to compute the test statistic) in Figure 5. First, we
find that more attacker knowledge leads to higher attack AUROC and lower variance in the attack
success. For CC News, the AUROC increases from 62.0Â±3.3%when the attacker has only one
document to 68.1Â±0.6%at 50 documents. The user inference attack already leads to non-trivial
results with an attacker knowledge of one document per user for CC News (AUROC 62.0%) and
Enron Emails (AUROC 73.2%). Overall, the results show that an attacker does not need much
data to mount a strong attack, and more data only helps.
UserInferenceandUser-levelOverfitting. Itiswell-establishedthatoverfittingtothetraining
data is sufficient for successful membership inference (Yeom et al., 2018). We find that a similar
phenomenon holds for user inference, which is enabled by user-level overfitting , i.e., the model
overfits not to the training samples themselves, but rather the distributions of the training users.
We see from Figure 3 that the validation loss of held-in users continues to decrease for all
3 datasets , while the loss of held-out users increases. These curves display a textbook example
of overfitting, not to the training data (since both curves are computed using validation data),
but to the distributions of the training users. Note that the attack AUROC improves with the
widening generalization gap between these two curves. Indeed, the Spearman correlation between
the generalization gap and the attack AUROC is at least 99.4%for all datasets. This demonstrates
the close relation between user-level overfitting and user inference.
Attack Performance and Model Scale. Next, we investigate the role of model scale in user
inference using the GPT-Neo 125M and 1.3B on the CC News dataset.
Figure 4 shows that the attack AUROC is nearly identical for the 1.3B model ( 65.3%) and 125M
model ( 65.8%). While the larger model achieves better validation loss on both held-in users ( 2.24vs.
2.64) and held-out users ( 2.81vs.3.20), the generalization gap is nearly the same for both models
(0.57vs.0.53). This shows a qualitative difference between user and membership inference, where
attack performance reliably increases with model size in the latter (Carlini et al., 2023; Tirumala
et al., 2022; Kandpal et al., 2022; Mireshghallah et al., 2022; Anil et al., 2023).
9100101102
Shared Substring Length0.50.60.70.80.9Attack AUROC
Shared Length â€” Reddit Comments
Canaries
Real Users
101102
Shared Substring Length0.650.700.750.800.850.90
Shared Length â€” CC News
100101102
Fine-Tuning Documents per User0.40.50.60.70.80.9
Finetuning Data Size â€” Reddit Comments
100101102
Fine-Tuning Documents per User0.450.500.550.600.650.700.750.80
Finetuning Data Size â€” CC NewsFigure6: Canaryexperiments. Lefttwo : Comparisonofattackperformanceonthenaturaldistributionof
users (â€œReal Usersâ€) and attack performance on synthetic canary users (each with 100 fine-tuning documents)
as the shared substring in a canaryâ€™s documents varies in length. Right two : Attack performance on canary
users (each with a 10-token shared substring) decreases as their contribution to the fine-tuning set decreases.
On all plots, we shade the AUROC std over 100bootstrap samples of held-in and held-out users.
4.3 User Inference in the Worst-Case
The disproportionately large downside to privacy leakage necessitates looking beyond the average-
case privacy risk to worst-case settings. Thus, we analyze attack performance on datasets containing
synthetically generated users, known as canaries. There is usually a trade-off between making the
canary users realistic and worsening their privacy risk. We intentionally err on the side of making
them realistic to illustrate the potential risks of user inference.
To construct a canary user, we first sample a real user from the dataset and insert a particular
substring into each of that userâ€™s examples. The substring shared between all of the userâ€™s examples
is a contiguous substring randomly sampled from one of their documents (for more details, see
Appendix D). We construct 180canary users with shared substrings ranging from 1-100tokens in
length and inject these users into the Reddit and CC News datasets. We do not experiment with
synthetic canaries in Enron Emails, as the attack AUROC already exceeds 88%for real users.
Figure 6 (left) shows that the attack is more effective on canaries than real users, and increases
with the length of the shared substring. A short shared substring is enough to significantly increase
the attack AUROC from 63%to69%(5 tokens) for CC News and 56%to65%for Reddit (10
tokens).
These results raise a question if canary gradients can be filtered out easily (e.g., using the â„“2
norm). However, Figure 7 (right) shows that the gradient norm distribution of the canary gradients
and those of real users are nearly indistinguishable. This shows that our canaries are close to real
users from the modelâ€™s perspective, and thus hard to filter out. This experiment also demonstrates
the increased privacy risk for users who use, for instance, a short and unique signature in emails or
characteristic phrases in documents.
4.4 Mitigation Strategies
Finally, we investigate existing techniques for limiting the influence of individual examples or users
on model fine-tuning as methods for mitigating user inference attacks.
Gradient Clipping. Since we consider fine-tuning that is oblivious to the user-stratification of
the data, one can limit the modelâ€™s sensitivity by clipping the gradients per batch (Pascanu et al.,
2013) or per example (Abadi et al., 2016). Figure 7 (left) plots its effect for the 125M model on CC
10News: neither batch nor per-example gradient clipping have any effect on user inference. Figure 7
(right) tells us why: canary examples do not have large outlying gradients and clipping affects real
and canary data similarly. Thus, gradient clipping is an ineffective mitigation strategy.
Early Stopping. The connection between user inference and user-level overfitting from Section 4.2
suggests that early stopping, a common heuristic used to prevent overfitting (Caruana et al., 2000),
could potentially mitigate user inference. Unfortunately, we find that 95%of the final AUROC is
obtained quite early in training: 15K steps ( 5%of the fine-tuning) for CC News and 90K steps ( 18%
of the fine-tuning) for Reddit, see Figure 3. Typically, the overall validation loss still decreases far
after this point. This suggests an explicit tradeoff between model utility (e.g., in validation loss)
and privacy risks from user inference.
Data Limits Per User. Since we cannot change the fine-tuning procedure, we consider limiting
the amount of fine-tuning data per user. Figure 6 (right two) show that this can be effective. For
CC News, the AUROC for canary users reduces from 77%at100fine-tuning documents per user
to almost random chance at 5documents per user. A similar trend also holds for Reddit.
Data Deduplication. Since data deduplication can mitigate membership inference (Lee et al.,
2022; Kandpal et al., 2022), we evaluate it for user inference. CC News is the only dataset in
our suite with within-user duplicates (Reddit and Enron are deduplicated in the preprocessing; see
Appendix D.1), so we use it for this experiment.2The deduplication reduces the attack AUROC
from 65.7%to59.1%. The attack ROC curve of the deduplicated version is also uniformly lower,
even at extremely small FPRs (Figure 8).
Thus, data repetition (e.g., due to poor preprocessing) can exacerbate user inference. How-
ever, the results on Reddit and Enron Emails (no duplicates) suggest that deduplication alone is
insufficient to fully mitigate user inference.
100101102
Shared Substring Length0.650.700.750.800.850.900.95Attack AUROC
Mitigations based on Gradient Clipping
Clipping Strategy
No clip
Batch
Per-Example
0 50 100 150
Gradient Norm10âˆ’410âˆ’310âˆ’210âˆ’1ProportionHistogram of Gradient Norms
Real Data
Canary
Figure 7: Mitigation with gradient clipping.
Left: Attack effectiveness for canaries with dif-
ferent shared substring lengths with gradient
clipping ( 125M model, CC News). Right: The
distribution of gradient norms for canary exam-
ples and real examples.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateEï¬€ect of Deduplication â€” CC News
10âˆ’310âˆ’210âˆ’110âˆ’310âˆ’210âˆ’1Original (AUROC = 65.7%)
Dedup. (AUROC = 59.1%)Figure 8: Effect of data deduplication per-user
on CC News. Table 5 in Appendix E gives TPR
values at low FPR.
2Although each article of CC News from HuggingFace Datasets has a unique URL, the text of 11%of the articles
has exact duplicates from the same domain. See Â§D.5 for examples.
11Example-level Differential Privacy (DP). DP (Dwork et al., 2006) gives provable bounds on
privacy leakage. We study how example-level DP, which protects the privacy of individual examples ,
impactsuserinference. We train the 125M model on Enron Emails using DP-Adam, a variant of
Adam that clips per-example gradients and adds noise calibrated to the privacy budget Îµ. We find
next that example-level DP can somewhat mitigate user inference while incurring increased compute
cost and a degraded model utility.
Obtaining good utility with DP requires large batches and more epochs (Ponomareva et al.,
2023), so we use a batch size of 1024, tune the learning rate, and train the model for 50epochs
(1.2Kupdates), so that each job runs in 24h (in comparison, non-private training takes 1.5h for 7
epochs). Further details of the tuning are given in Appendix D.4.
Table 2 shows a severe degradation in the validation loss under DP. For instance, a loss of 2.67
at the weak guarantee of Îµ= 32is surpassed after just 1/3rdof an epoch of non-private training;
this loss continues to reduce to 2.43after 3epochs. In terms of attack effectiveness, example-level
DP reduces the attack AUROC and the TPR at FPR = 5%, while the TPR at FPR = 1%remains
the same or gets worse. Indeed, while example-level DP protects individual examples, it can fail to
protect the privacy of users, especially when they contribute many examples. This highlights the
need for scalable algorithms and software for fine-tuning LLMs with DP at the user-level . Currently,
user-level DP algorithms have been designed for small models in federated learning, but do not yet
scale to LLMs.
Metric Îµ= 2 Îµ= 8 Îµ= 32Non-private
Val. Loss 2.77 2 .71 2 .67 2 .43
Attack AUROC 64.7% 66 .7% 67 .9% 88 .1%
TPR @ FPR = 1% 8 .8% 8 .8% 10 .3% 4 .4%
TPR @ FPR = 5% 11 .8% 10 .3% 10 .3% 27 .9%
Table 2: Example-level differential privacy : Training a model on Enron Emails under (Îµ,10âˆ’6)-DP
at the example-level (smaller Îµimplies a higher level of privacy).
Summary. Our results show that user inference is hard to mitigate with common heuristics.
Careful deduplication is necessary to ensure that data repetition does not exacerbate user inference.
Enforcing data limits per user can be effective but this only works for data-rich applications with a
large number of users. Example-level DP can offer moderate mitigation but at the cost of increased
data/compute and degraded model utility. Developing an effective mitigation strategy that also
works efficiently in data-scarce applications remains an open problem.
5 Discussion and Conclusion
When collecting data for fine-tuning an LLM, data from a companyâ€™s users is often the natural
choice since it closely resembles the types of inputs a deployed LLM will encounter. However fine-
tuning on user-stratified data also exposes new opportunities for privacy leakage. Until now, most
work on privacy of LLMs have ignored any structure in the training data, but as the field shifts
towards collecting data from new, potentially sensitive, sources, it is important to adapt our privacy
threat models accordingly. Our work introduces a novel privacy attack exposing user participation
12in fine-tuning, and future work should explore other LLM privacy violations beyond membership
inference and training data extraction. Furthermore, this work underscores the need for scaling
user-aware training pipelines, such as user-level DP, to handle large datasets and models.
6 Broader Impacts
This work highlights a novel privacy vulnerability in LLMs fine-tuned on potentially sensitive user
data. Hypothetically, our methods could be leveraged by an attacker with API access to a fine-tuned
LLM to infer which users contributed their data to the modelâ€™s fine-tuning set. To mitigate the risk
of data exposure, we performed experiments on public GPT-Neo models, using public datasets for
fine-tuning, ensuring that our experiments do not disclose any sensitive user information.
We envision that these methods will offer practical tools for conducting privacy audits of LLMs
beforereleasingthemforpublicuse. Byrunninguserinferenceattacks, acompanyfine-tuningLLMs
on user data can gain insights into the privacy risks exposed by providing access to the models and
assess the effectiveness of deploying mitigations. To counteract our proposed attacks, we evaluate
several defense strategies, including example-level differential privacy and restricting individual user
contributions, both of which provide partial mitigation of this threat. We leave to future work the
challenging problem of fully protecting LLMs against user inference with provable guarantees.
References
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep
learning with differential privacy. In Proceedings of the ACM SIGSAC Conference on Computer
and Communications Security , 2016.
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen, et al. Palm 2 technical report. arXiv:2305.10403 , 2023.
J. Baumgartner, S. Zannettou, B. Keegan, M. Squire, and J. Blackburn. The pushshift reddit
dataset. Proceedings of the International AAAI Conference on Web and Social Media , 14(1):
830â€“839, May 2020. doi: 10.1609/icwsm.v14i1.7347. URL https://ojs.aaai.org/index.
php/ICWSM/article/view/7347 .
S. Biderman, U. S. Prashanth, L. Sutawika, H. Schoelkopf, Q. Anthony, S. Purohit, and E. Raf.
Emergent and Predictable Memorization in Large Language Models. arXiv:2304.11158 , 2023.
S. Black, L. Gao, P. Wang, C. Leahy, and S. Biderman. GPT-Neo: Large Scale Autoregressive
Language Modeling with Mesh-Tensorflow, Mar. 2021.
N. Carlini, F. TramÃ¨r, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown,
D. Song, Ãš. Erlingsson, A. Oprea, and C. Raffel. Extracting training data from large language
models. In USENIX , 2021.
N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. TramÃ¨r. Membership inference attacks
from first principles. In IEEE Symposium on Security and Privacy , 2022.
N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memorization
across neural language models. In ICLR, 2023.
13R. Caruana, S. Lawrence, and C. Giles. Overfitting in Neural Nets: Backpropagation, Conjugate
Gradient, and Early Stopping . NeurIPS , 2000.
Z. Charles, N. Mitchell, K. Pillutla, M. Reneer, and Z. Garrett. Towards Federated Foundation
Models: Scalable Dataset Pipelines for Group-Structured Learning. arXiv:2307.09619 , 2023.
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda,
N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry,
P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Win-
ter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-
Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain,
W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford,
M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCan-
dlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. arXiv
2107.03374 , 2021.
M. Chen, Z. Zhang, T. Wang, M. Backes, and Y. Zhang. FACE-AUDITOR: Data auditing in facial
recognition systems. In 32nd USENIX Security Symposium (USENIX Security 23) , pages 7195â€“
7212, Anaheim, CA, Aug. 2023. USENIX Association. ISBN 978-1-939133-37-3. URL https:
//www.usenix.org/conference/usenixsecurity23/presentation/chen-min .
M. X. Chen, B. N. Lee, G. Bansal, Y. Cao, S. Zhang, J. Lu, J. Tsay, Y. Wang, A. M. Dai, Z. Chen,
T. Sohn, and Y. Wu. Gmail smart compose: Real-time assisted writing. In Proceedings of the
25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2019.
C. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot. Label-only membership inference
attacks. In ICML, 2021.
C. B. Clement, M. Bierbaum, K. P. Oâ€™Keeffe, and A. A. Alemi. On the use of arxiv as a dataset.
arXiv 1905.00075 , 2019.
E. Debenedetti, G. Severi, N. Carlini, C. A. Choquette-Choo, M. Jagielski, M. Nasr, E. Wallace,
and F. TramÃ¨r. Privacy side channels in machine learning systems. arXiv:2309.05610 , 2023.
C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating Noise to Sensitivity in Private Data
Analysis. In Proc. of the Third Conf. on Theory of Cryptography (TCC) , pages 265â€“284, 2006.
URL http://dx.doi.org/10.1007/11681878_14 .
K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov. Property Inference Attacks on Fully
Connected Neural Networks Using Permutation Invariant Representations. In Proceedings of the
2018 ACM SIGSAC Conference on Computer and Communications Security , page 619â€“633, 2018.
L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite,
N. Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.
arXiv:2101.00027 , 2020.
N. Haim, G. Vardi, G. Yehudai, michal Irani, and O. Shamir. Reconstructing training data from
trained neural networks. In NeurIPS , 2022.
14F. Hamborg, N. Meuschke, C. Breitinger, and B. Gipp. news-please: A generic news crawler and
extractor. In Proceedings of the 15th International Symposium of Information Science , 2017.
V.Hartmann,L.Meynent,M.Peyrard,D.Dimitriadis,S.Tople,andR.West. DistributionInference
Risks: Identifying and Mitigating Sources of Leakage. In 2023 IEEE Conference on Secure and
Trustworthy Machine Learning (SaTML) , pages 136â€“149, 2023.
H. A. Inan, O. Ramadan, L. Wutschitz, D. Jones, V. RÃ¼hle, J. Withers, and R. Sim. Training data
leakage analysis in language models. arxiv:2101.05405 , 2021.
D. Ippolito, F. Tramer, M. Nasr, C. Zhang, M. Jagielski, K. Lee, C. Choquette Choo, and N. Carlini.
Preventing generation of verbatim memorization in language models gives a false sense of privacy.
InINLG, 2023.
M. Jagielski, M. Nasr, C. Choquette-Choo, K. Lee, and N. Carlini. Students parrot their teachers:
Membership inference on model distillation. arXiv:2303.03446 , 2023a.
M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini, E. Wallace, S. Song, A. G.
Thakurta, N. Papernot, and C. Zhang. Measuring forgetting of memorized training examples. In
ICLR, 2023b.
P. Kairouz, S. Oh, and P. Viswanath. The Composition Theorem for Differential Privacy. In ICML,
pages 1376â€“1385, 2015.
P. Kairouz, B. McMahan, S. Song, O. Thakkar, A. Thakurta, and Z. Xu. Practical and private
(deep) learning without sampling or shuffling. In ICML, 2021.
N. Kandpal, E. Wallace, and C. Raffel. Deduplicating training data mitigates privacy risks in
language models. In ICML, 2022.
D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In ICLR, 2015.
B. Klimt and Y. Yang. Introducing the enron corpus. In International Conference on Email and
Anti-Spam , 2004.
S. Kudugunta, I. Caswell, B. Zhang, X. Garcia, C. A. Choquette-Choo, K. Lee, D. Xin, A. Kusupati,
R. Stella, A. Bapna, et al. Madlad-400: A multilingual and document-level large audited dataset.
arXiv:2309.04662 , 2023.
K.Lee, D.Ippolito, A.Nystrom, C.Zhang, D.Eck, C.Callison-Burch, andN.Carlini. Deduplicating
training data makes language models better. In ACL, 2022.
E. L. Lehmann, J. P. Romano, and G. Casella. Testing Statistical Hypotheses , volume 3. Springer,
1986.
D. A. N. Levy, Z. Sun, K. Amin, S. Kale, A. Kulesza, M. Mohri, and A. T. Suresh. Learning with
user-level privacy. In NeurIPS , 2021.
G. Li, S. Rezaei, and X. Liu. User-Level Membership Inference Attack against Metric Embedding
Learning. In ICLR 2022 Workshop on PAIR2Struct: Privacy, Accountability, Interpretability,
Robustness, Reasoning on Structured Data , 2022.
15H.Liu, D.Tam, M.Mohammed, J.Mohta, T.Huang, M.Bansal, andC.Raffel. Few-shotparameter-
efficient fine-tuning is better and cheaper than in-context learning. In NeurIPS , 2022.
N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and S. Zanella-Beguelin. Analyzing leakage
of personally identifiable information in language models. In IEEE Symposium on Security and
Privacy, 2023.
K. Luyckx and W. Daelemans. Authorship attribution and verification with many authors and lim-
ited data. In D. Scott and H. Uszkoreit, editors, Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008) , pages 513â€“520, Manchester, UK, Aug. 2008. Coling
2008 Organizing Committee. URL https://aclanthology.org/C08-1065 .
K. Luyckx and W. Daelemans. The effect of author set size and data size in authorship attribution.
Literary and Linguistic Computing , 26(1):35â€“55, 08 2010. ISSN 0268-1145. doi: 10.1093/llc/
fqq013. URL https://doi.org/10.1093/llc/fqq013 .
J. Mattern, F. Mireshghallah, Z. Jin, B. Schoelkopf, M. Sachan, and T. Berg-Kirkpatrick. Mem-
bership inference attacks against language models via neighbourhood comparison. In Findings of
ACL, 2023.
H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang. Learning differentially private recurrent
language models. In International Conference on Learning Representations , 2018.
Y. Miao, M. Xue, C. Chen, L. Pan, J. Zhang, B. Z. H. Zhao, D. Kaafar, and Y. Xiang. The Audio
Auditor: User-Level Membership Inference in Internet of Things Voice Services. In Privacy
Enhancing Technologies Symposium (PETS) , 2021.
F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick, and R. Shokri. Quantifying privacy
risks of masked language models using membership inference attacks. In EMNLP, 2022.
M. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot fine-tuning vs. in-
context learning: A fair comparison and evaluation. In Findings of ACL , 2023.
M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo,
E. Wallace, F. TramÃ¨r, and K. Lee. Scalable extraction of training data from (production)
language models. arXiv preprint arXiv:2311.17035 , 2023.
L. Niu, S. Mirza, Z. Maradni, and C. PÃ¶pper. CodexLeaks: Privacy leaks from code generation
language models in GitHub copilot. In USENIX Security Symposium , 2023.
A. Oprea and A. Vassilev. Adversarial machine learning: A taxonomy and terminology of attacks
and mitigations. NIST AI 100-2 E2023 report. Available at https://csrc.nist.gov/pubs/
ai/100/2/e2023/ipd , 2023.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In
ICML, 2013.
N. Ponomareva, H. Hazimeh, A. Kurakin, Z. Xu, C. Denison, H. B. McMahan, S. Vassilvitskii,
S. Chien, and A. G. Thakurta. How to DP-fy ML: A Practical Guide to Machine Learning with
Differential Privacy. Journal of Artificial Intelligence Research , 77:1113â€“1201, 2023.
16A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsu-
pervised multitask learners. 2019.
S. Ramaswamy, O. Thakkar, R. Mathews, G. Andrew, H. B. McMahan, and F. Beaufays. Training
production language models without memorizing user data. arxiv:2009.10031 , 2020.
S.J.Reddi, Z.Charles, M.Zaheer, Z.Garrett, K.Rush, J.KoneÄnÃ½, S.Kumar, andH.B.McMahan.
Adaptive Federated Optimization. In ICLR, 2021.
A. Sablayrolles, M. Douze, C. Schmid, Y. Ollivier, and H. JÃ©gou. White-box vs black-box: Bayes
optimal strategies for membership inference. In ICML, 2019.
O. Sainz, J. A. Campos, I. GarcÃ­a-Ferrero, J. Etxaniz, and E. Agirre. Did ChatGPT Cheat on Your
Test? https://hitz-zentroa.github.io/lm-contamination/blog/ , 2023.
V. Shejwalkar, H. A. Inan, A. Houmansadr, and R. Sim. Membership Inference Attacks Against
NLP Classification Models. In NeurIPS 2021 Workshop Privacy in Machine Learning , 2021.
R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine
learning models. In IEEE Symposium on Security and Privacy , 2017.
C. Song and V. Shmatikov. Auditing data provenance in text-generation models. In Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , 2019.
M. Song, Z. Wang, Z. Zhang, Y. Song, Q. Wang, J. Ren, and H. Qi. Analyzing User-Level Privacy
Attack Against Federated Learning. IEEE Journal on Selected Areas in Communications , 38(10):
2430â€“2444, 2020.
K. Tirumala, A. Markosyan, L. Zettlemoyer, and A. Aghajanyan. Memorization without overfitting:
Analyzing the training dynamics of large language models. In NeurIPS , 2022.
Z. Wang, M. Song, Z. Zhang, Y. Song, Q. Wang, and H. Qi. Beyond Inferring Class Representa-
tives: User-Level Privacy Leakage From Federated Learning. In IEEE INFOCOM 2019 - IEEE
Conference on Computer Communications , page 2512â€“2520, 2019.
L. Watson, C. Guo, G. Cormode, and A. Sablayrolles. On the importance of difficulty calibration
in membership inference attacks. In ICLR, 2022.
Z. Xu, Y. Zhang, G. Andrew, C. Choquette, P. Kairouz, B. Mcmahan, J. Rosenstock, and Y. Zhang.
Federated learning of gboard language models with differential privacy. In ACL, 2023.
J. Ye, A. Maddi, S. K. Murakonda, V. Bindschaedler, and R. Shokri. Enhanced membership infer-
ence attacks against machine learning models. In Proceedings of the ACM SIGSAC Conference
on Computer and Communications Security , 2022.
S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning: Analyzing the
connection to overfitting. In IEEE Computer Security Foundations Symposium , 2018.
C.Zhang, D.Ippolito, K.Lee, M.Jagielski, F.TramÃ¨r, andN.Carlini. Counterfactualmemorization
in neural language models. arXiv 2112.12938 , 2021.
17S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin,
T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and
L. Zettlemoyer. Opt: Open pre-trained transformer language models. arXiv 2205.01068 , 2022.
18Appendix
The outline of the appendix is as follows:
â€¢Appendix A: Proof of the analysis of the attack statistic (Proposition 1).
â€¢Appendix B: Alternate approaches to solving user inference (e.g. if the computational cost
was not a limiting factor).
â€¢Appendix C: Further details on related work.
â€¢Appendix D: Detailed experimental setup (datasets, models, hyperparameters).
â€¢Appendix E: Additional experimental results.
â€¢Appendix F: A discussion of user-level DP, its promises, and challenges.
A Theoretical Analysis of the Attack Statistic
We prove Proposition 1 here.
Recall of definitions. The KL and Ï‡2divergences are defined respectively as
KL(Pâˆ¥Q) =X
xP(x) logP(x)
Q(x)
and Ï‡2(Pâˆ¥Q) =X
xP(x)2
Q(x)âˆ’1.
Recall that we also defined
pref(x) =Dâˆ’u(x) =P
uâ€²Ì¸=uÎ±uâ€²Duâ€²
P
uâ€²Ì¸=uÎ±uâ€²=P
uâ€²Ì¸=uÎ±uâ€²Duâ€²
1âˆ’Î±u,and
pÎ¸(x) =nX
uâ€²=1Î±uâ€²Duâ€²(x) =Î±uDu(x) + (1 âˆ’Î±u)Dâˆ’u(x).
Proof of the upper bound. Using the inequality log(1 + t)â‰¤twe get,
Â¯T(Du) =Exâˆ¼Du
logpÎ¸(x)
pref(x)
=Exâˆ¼Du
logÎ±uDu(x) + (1 âˆ’Î±u)Dâˆ’u(x)
Dâˆ’u(x)
=Exâˆ¼Duh
log
1 +Î±u
Du(x)
Dâˆ’u(x)âˆ’1i
â‰¤Î±uExâˆ¼DuDu(x)
Dâˆ’u(x)âˆ’1
=Î±uÏ‡2(Duâˆ¥Dâˆ’u).
19Proof of the lower bound. Using log(1 + t)>log(t), we get
Â¯T(Du) =Exâˆ¼Du
logpÎ¸(x)
pref(x)
=Exâˆ¼Du
logÎ±uDu(x) + (1 âˆ’Î±u)Dâˆ’u(x)
Dâˆ’u(x)
= log(1 âˆ’Î±u) +Exâˆ¼Du
logÎ±uDu(x)
(1âˆ’Î±u)Dâˆ’u(x)+ 1
>log(1âˆ’Î±u) +Exâˆ¼Du
logÎ±uDu(x)
(1âˆ’Î±u)Dâˆ’u(x)
= log( Î±u) +Exâˆ¼Du
logDu(x)
Dâˆ’u(x)
= log( Î±u) + KL( Duâˆ¥Dâˆ’u).
B Alternate Approaches to User Inference
We consider some alternate approaches to user inference that are inspired by the existing literature
on membership inference. As we shall see, these approaches are impractical for the LLM user
inference setting where exact samples from the fine-tuning data are not known to the attacker and
models are costly to train.
A common approach for membership inference is to train â€œshadow modelsâ€, models trained in a
similar fashion and on similar data to the model being attacked (Shokri et al., 2017). Once many
shadow models have been trained, one can construct a classifier that identifies whether the target
model has been trained on a particular example. Typically, this classifier takes as input a modelâ€™s
loss on the example in question and is learned based on the shadow modelsâ€™ losses on examples that
were (or were not) a part of theirtraining data. This approach could in principle be adapted to
user inference on LLMs.
First,wewouldneedtoassumethattheattackerhasenoughdatafromuser utofine-tuneshadow
models on datasets containing user uâ€™s data as well as an additional set of samples used to compute
uâ€™s likelihood under the shadow models. Thus, we assume the attacker has nsamples x(1:n)
train :=
(x(1), . . . ,x(n))âˆ¼ Dn
uused for shadow model training and msamples x(1:m):= (x(1), . . . ,x(m))âˆ¼
Dm
uused to compute likelihoods.
Next, the attacker trains many shadow models on data similar to the target modelâ€™s fine-tuning
data, including x(1:n)
trainin half of the shadow modelsâ€™ fine-tuning data. This repeated training yields
samples from two distributions: the distribution of models trained with user uâ€™s data Pand the
distribution of models trained without user uâ€™s data Q. The goal of the user inference attack is to
determine which distribution the target model is more likely sampled from.
However, since we assume the attacker has only black-box access to the target model, they must
instead perform a different hypothesis test based on the likelihood of x(1:m)under the target model.
To this end, the attacker must evaluate the shadow models on x(1:m)to draw samples from:
Pâ€²:pÎ¸(x)where Î¸âˆ¼ P,xâˆ¼ D u,Qâ€²:pÎ¸(x)where Î¸âˆ¼ Q,xâˆ¼ D u. (6)
Finally, the attacker can classify user uas being part (or not part) of the target modelâ€™s fine-
tuning data based on whether the likelihood values of the target model on x(1:m)are more likely
under Pâ€²orQâ€².
20While this is the ideal approach to performing user inference with no computational constraints,
it is infeasible due to the cost of repeatedly training shadow LLMs and the assumption that the
attacker has enough data from user uto both train and evaluate shadow models.
C Further Details on Related Work
There are several papers that study the risk of user inference attacks, but they either have a different
threat model, or are not applicable to LLMs.
User-level Membership Inference. We refer to problems of identifying a userâ€™s participation in
training when given the exact training samples of the user as user-level membership inference . Song
and Shmatikov (2019) propose methods for inferring whether a userâ€™s data was part of the training
set of a language model, under the assumption that the attacker has access to the userâ€™s training
set. For their attack, they train multiple shadow models on subsets of multiple usersâ€™ training
data and a meta-classifier to distinguish users who participating in training from those who did
not. This meta-classifier based methodology is not feasible for LLMs due to its high computational
complexity. Moreover, the notion of a â€œuserâ€ in their experiments is a random i.i.d. subset of the
dataset; this does not work for the more realistic threat model of user inference, which relies on the
similarity between the attackerâ€™s samples of a user to the training samples contributed by this user.
Shejwalkar et al. (2021) also assume that the attacker knows the userâ€™s training set and perform
user-level inference for NLP classification models by aggregating the results of membership inference
for each sample of the target user.
User Inference. In the context of classification and regression, Hartmann et al. (2023) define
distributionalmembershipinference, withthegoalofidentifyingifauserparticipatedinthetraining
set of a model without knowledge of the exact training samples. This coincides with our definition
of user inference. Hartmann et al. (2023) use existing shadow model-based attacks for distribution
(or property) inference (Ganju et al., 2018), as their main goal is to analyze sources of leakage
and evaluate defenses. User inference attacks have been also studied in other applications domains,
such as embedding learning for vision (Li et al., 2022) and speech recognition for IoT devices (Miao
et al., 2021). Chen et al. (2023) design a black-box user-level auditing procedure on face recognition
systems in which an auditor has access to images of a particular user that are not part of the
training set. In federated learning, Wang et al. (2019) and Song et al. (2020) analyze the risk of
user inference by a malicious server. None of these works apply to our LLM setting because they
are either (a) domain-specific, or (b) computationally inefficient (e.g. due to shadow models).
Comparison to Related Tasks. User inference on text models is related to, but distinct from
authorship attribution, the task of identifying authors from a user population given access to mul-
tiple writing samples. We recall it definition and discuss the similarities and differences. The goal
of authorship attribution (AA) is to find which of the given population of users wrote a given text.
For user inference (UI), on the other hand, the goal is to figure out ifany data from a given user
was used to train a given model. Note the key distinction here: there is no model in the problem
statement of AA while the entire population of users is not assumed to be known for UI. Indeed,
UI cannot be reduced to AA or vice versa: Solving AA does not solve UI because it does not tell us
whether the userâ€™s data was used to train a given LLM (which is absent from the problem statement
21102103104
Number of Documents050010001500200025003000Number of UsersArXiv Abstracts
102103
Number of Documents0100200300400500600700Reddit Comments
102103104
Number of Documents0255075100125150CC News
102103104
Number of Documents0123456Enron EmailsFigure 9: Histogram of number of documents per user for each dataset.
of AA). Likewise, solving UI only tells us that a userâ€™s data was used to train a given model but it
does not tell us which user from a given population this data comes from (since the full population
of users is not assumed to be known for UI).
Author attribution assumes that the entire user population is known, which is not required in
user inference. Existing work on author attribution (e.g. Luyckx and Daelemans, 2008, 2010) casts
the problem as a classification task with one class per user, and does not scale to large number
of users. Interestingly, Luyckx and Daelemans (2010) identified that the number of authors and
the amount of training data per author are important factors for the success of author attribution,
also reflected by our findings when analyzing the user inference attack success. Connecting author
attribution with privacy attacks on LLM fine-tuning could be a topic of future work.
D Experimental Setup
In this section, we give the following details:
â€¢Appendix D.1: Full details of the datasets, their preprocessing, the models used, and the
evaluation of the attack.
â€¢Appendix D.2: Pseudocode of the canary construction algorithm.
â€¢Appendix D.3: Precise definitions of mitigation strategies.
â€¢Appendix D.4: Details of hyperparameter tuning for example-level DP.
â€¢Appendix D.5: Analysis of the duplicates present in CC News.
D.1 Datasets, Models, Evaluation
We evaluate user inference attacks on four user-stratified datasets. Here, we describe the datasets,
the notion of a â€œâ€˜userâ€â€™ in each dataset, and any initial filtering steps applied. Figure 9 gives a
histogram of data per user (see also Tables 1 and 3).
â€¢Reddit Comments3(Baumgartner et al., 2020) : Each example is a comment posted on
Reddit. We define a user associated with a comment to be the username that posted the
comment.
The raw comment dump contains about 1.8 billion comments posted over a four-year span
between 2012 and 2016. To make the dataset suitable for experiments on user inference, we
take the following preprocessing steps:
3https://huggingface.co/datasets/fddemarco/pushshift-reddit-comments
22â€“To reduce the size of the dataset, we initially filter to comments made during a six-month
period between September 2015 and February 2016, resulting in a smaller dataset of 331
million comments.
â€“As a heuristic for filtering automated Reddit bot and moderator accounts from the
dataset, we remove any comments posted by users with the substring â€œâ€˜botâ€â€™ or â€œâ€˜modâ€â€™
in their name and users with over 2000 comments in the dataset.
â€“We filter out low-information comments that are shorter than 250 tokens in length.
â€“Finally, we retain users with at least 100comments for the user inference task, leading
to around 5Kusers.
Reddit Small. We also create a smaller version of this dataset with 4 monthsâ€™ data (the rest
of the preprocessing pipeline remains the same). This gives us a dataset which is roughly half
the size of the original one after filtering â€” we denote this as â€œReddit Comments (Small)â€ in
Table 3.
Although the unprocessed version of the small 4-month dataset is a subset of the unprocessed
6-month dataset, this is not longer the case after processing. After processing, 2626 users of
the original 2774 users in the 4 month dataset were retained in the 6 month dataset. The other
148 users went over the 2000 comment threshold due to the additional 2 months of data and
were filtered out as a part of the bot-filtering heuristic. Note also that the held-in and held-out
split between the two Reddit datasets is different (of the 1324 users in the 4-month training
set, only 618 are in the 6-month training set). Still, we believe that a comparison between
these two datasets gives a reasonable approximation how user inference changes with the scale
of the dataset due to the larger number of users. These results are given in Appendix E.2.
â€¢CC News4(Hamborg et al., 2017; Charles et al., 2023): Each example is a news article pub-
lished on the Internet between January 2017 and December 2019. We define a user associated
with an article to be the web domain where the article was found (e.g., nytimes.com). While
CC News is not user-generated data (such as emails or posts used for the other datasets), it is
a large group-partitioned dataset and has been used as a public benchmark for user-stratified
federated learning applications (Charles et al., 2023). We note that this practice is common
with other group-partitioned web datasets such as Stack Overflow (Reddi et al., 2021).
â€¢Enron Emails5(Klimt and Yang, 2004): Each example is an email found in the account of
employees of the Enron corporation prior to its collapse. We define the user associated with
an email to be the email address that sent an email.
The original dataset contains a dump of emails in various folders of each user, e.g., â€œinboxâ€,
â€œsentâ€, â€œcalendarâ€, â€œnotesâ€, â€œdeleted itemsâ€, etc. Thus, it contains a set of emails sent and
received by each user. In some cases, each user also has multiple email addresses. Thus we
take the following preprocessing steps for each user:
â€“We list all the candidate senderâ€™s email address values on emails for a given user.
4https://huggingface.co/datasets/cc_news
5https://www.cs.cmu.edu/~enron/
23Dataset User Field #Users #ExamplesPercentiles of Examples/User
P0P25P50P75P100
ArXiv Abstracts Submitter 16511 625 K 20 24 30 41 3204
Reddit Comments (Small) User Name 2774 537K 100 115 141 194 1662
Table 3: Summary statistics for additional datasets.
â€“We filter and keep candidate email addresses that contain the last name of the user,
as inferred from the user name (assuming the user name is <last name>-<first
initial> ), also appears in the email.6
â€“We associate the most frequently appearing senderâ€™s email address from the remaining
candidates.
â€“Finally, this dataset contains duplicates (e.g. the same email appears in the â€œinboxâ€ and
â€œcalendarâ€ folders). We then explicitly deduplicate all emails sent by this email address
to remove exact duplicates. This gives the final set of examples for each user.
We verified that each of the remaining 138 users had their unique email addresses.
â€¢ArXiv Abstracts7(Clement et al., 2019): Each example is a scientific abstract posted to
the ArXiv pre-print server through the end of 2021. We define the user associated with an
abstract to be the first author of the paper. Note that this notion of author may not always
reflect who actually wrote the abstract in case of collaborative papers. As we do not have
access to perfect ground truth in this case, there is a possibility that the user labeling might
have some errors (e.g. a non-first author wrote an abstract or multiple users collaborated
on the same abstract). Thus, we postpone the results for the ArXiv Abstracts dataset to
Appendix E. See Table 3 for statistics of the ArXiv dataset.
Despite the imperfect ground truth labeling of the ArXiv datasets, we believe that evaluating the
proposed user inference attack reveals the risk of privacy leakage in fine-tuned LLMs for two reasons.
First, the fact that we have significant privacy leakage despite imperfect user labeling suggests that
the attack will only get stronger if we had perfect ground truth user labeling and non-overlapping
users. This is because mixing distributions only brings them closer, as shown in Proposition 2 below.
Second, our experiments on canary users are not impacted at all by the possible overlap in user
labeling, since we create our own synthetically-generated canaries to evaluate worst-case privacy
leakage.
Proposition 2 (Mixing Distributions Brings Them Closer) .LetP, Qbe two user distributions over
text. Suppose mislabeling leads to the respective mixture distributions of Pâ€²=Î»P+ (1âˆ’Î»)Qand
Qâ€²=ÂµQ+ (1âˆ’Âµ)Pfor some Î», Âµâˆˆ[0,1]. Then, we have, KL(Pâ€²âˆ¥Qâ€²)â‰¤KL(Pâˆ¥Q).
Proof.The proof follows from the convexity of the KL divergence in both its arguments. Indeed,
we have,
KL(Pâˆ¥ÂµQ+ (1âˆ’Âµ)P)â‰¤ÂµKL(P|Q) + (1 âˆ’Âµ) KL( Pâˆ¥P)â‰¤KL(P|Q),
6Thisprocessingomitssomeusers. Forinstance, themostfrequentlyappearingsenderâ€™semailoftheuserâ€œcrandell-
sâ€ with inferred last name â€œcrandellâ€ is sean.crandall@enron.com . It is thus omitted by the preprocessing.
7https://huggingface.co/datasets/gfissore/arxiv-abstracts-2021
24since 0â‰¤Âµâ‰¤1andKL(Pâˆ¥P) = 0. A similar reasoning for the first argument of the KL divergence
completes the proof.
Preprocessing. Before fine-tuning models on these datasets we perform the following preprocess-
ing steps to make them suitable for evaluating user inference.
1. Wefilteroutuserswithfewerthanaminimumnumberofsamples( 20,100,30,and 150samples
for ArXiv, Reddit, CC News, and Enron respectively). These thresholds were selected prior
to any experiments to balance the following considerations: (1) each user must have enough
data to provide the attacker with enough samples to make user inference feasible and (2) the
filtering should not remove so many users that the fine-tuning dataset becomes too small. The
summary statistics of each dataset after filtering are shown in Table 1.
2. We reserve 10%of the data for validation and test sets
3. We split the remaining 90%of samples into a held-in set and held-out set, each containing
half of the users. The held-in set is used for fine-tuning models and the held-out set is used
for attack evaluation.
4. For each user in the held-in and held-out sets, we reserve 10%of the samples as the attackerâ€™s
knowledge about each user. These samples are never used for fine-tuning.
Target Models. We evaluate user inference attacks on the 125M and 1.3B parameter models from
the GPT-Neo (Black et al., 2021) model suite. For each experiment, we fine-tune all parameters of
these models for 10epochs. We use the the Adam optimizer (Kingma and Ba, 2015) with a learning
rate of 5Ã—10âˆ’5, a linearly decaying learning rate schedule with a warmup period of 200steps, and
a batch size of 8. After training, we select the checkpoint achieving the minimum loss on validation
data from the users held in to training, and use this checkpoint to evaluate user inference attacks.
We train models on servers with one NVIDIA A100 GPU and 256GB of memory. Each fine-
tuningruntookapproximately 16hourstocompleteforGPT-Neo 125Mand 100hoursforGPT-Neo
1.3B.
Attack Evaluation. We evaluate attacks by computing the attack statistic from Section 3 for each
held-in user that contributed data to the fine-tuning dataset, as well as the remaining held-out set
of users. With these user-level statistics, we compute a Receiver Operating Characteristic (ROC)
curve and report the area under this curve (AUROC) as our metric of attack performance. This
metric has been used recently to evaluate the performance of membership inference attacks Carlini
et al. (2022), and it provides a full spectrum of the attack effectiveness (True Positive Rates at fixed
False Positive Rates). By reporting the AUROC, we do not need to select a threshold Ï„for our
attack statistic, but rather we report the aggregate performance of the attack across all possible
thresholds.
D.2 Canary User Construction
We evaluate worst-case risk of user inference by injecting synthetic canary users into the fine-tuning
data from CC News, ArXiv Abstracts, and Reddit Comments. These canaries were constructed by
taking real users and replicating a shared substring in all of that userâ€™s examples. This construction
25is meant to create canary users that are both realistic (i.e. not substantially outlying compared
to the true user population) but also easy to perform user inference on. The algorithm used to
construct canaries is shown in Algorithm 1.
Algorithm 1 Synthetic canary user construction
Input:Substring lengths L= [l1, . . . l n], canaries per substring length N, set of real users UR
Output: Set of canary users UC
UCâ† âˆ…
forlinLdo
foriup to Ndo
Uniformly sample user ufrom UR
Uniformly sample example xfrom uâ€™s data
Uniformly sample l-token substring sfrom x
ucâ† âˆ… â–·Initialize canary user with no data
forxinudo
xcâ†InsertSubstringAtRandomLocation (x, s)
Add example xcto user uc
Add user uctoUC
Remove user ufrom UR
D.3 Mitigation Definitions
In Section 4.2 we explore heuristics for mitigating privacy attacks. We give precise definitions of
the batch and per-example gradient clipping.
Batch gradient clipping restricts the norm of a single batch gradient to be at most C:
Ë†gt=min(C,âˆ¥âˆ‡Î¸tl(x)âˆ¥)
âˆ¥âˆ‡Î¸tl(x)âˆ¥âˆ‡Î¸tl(x).
Per-example gradient clipping restricts the norm of a single exampleâ€™s gradient to be at most C
before aggregating the gradients into a batch gradient:
Ë†gt=nX
i=1min(C,âˆ¥âˆ‡Î¸tl(x(i))âˆ¥)
âˆ¥âˆ‡Î¸tl(x(i))âˆ¥âˆ‡Î¸tl(x(i)).
The batch or per-example clipped gradient Ë†gt, is then passed to the optimizer as if it were the
true gradient.
For all experiments involving gradient clipping, we selected the clipping norm, C, by recording
the gradient norms during a standard training run and setting Cto the minimum gradient norm.
In practice this resulted in clipping nearly all batch/per-example gradients during training.
D.4 Example-Level Differential Privacy: Hyperparameter Tuning
Wenowdescribethehyperparametertuningstrategyfortheexample-levelDPexperimentsreported
in Table 2. Broadly, we follow the guidelines outlined by Ponomareva et al. (2023). Specifically, the
tuning procedure is as follows:
26101102103104
Batch Size103
102
101
Noise Scale in Avg. Gradient
Signal-to-noise ratio tuning --- Enron Emails
=2
=8
=32
=128
(a)The scale of the noise added to the average
gradients.
0 2500 5000 7500 10000 12500 15000 17500 20000
Fine-Tuning Iteration (Ã—103)2.652.702.752.802.852.902.95Validation Loss
Example-level DP (Batch size = 1024) | Enron Emails
Learning Rate
104
3Ã—104
5Ã—104
 (b)Tuning the learning rate with Îµ= 8.
Figure 10: Tuning the parameters for example-level DP on the Enron dataset.
â€¢The Enron dataset has n= 41000 examples from held-in users used for training. The Non-
private training of reaches its best validation loss in about 3epochs or T= 15Ksteps. We
keep this fixed for the batch size tuning.
â€¢Tuning the batch size : For each privacy budget Îµand batch size b, we obtain the noise
multiplier Ïƒsuch that the private sumPb
i=1gi+N(0, Ïƒ2)repeated Ttimes (one for each step
of training) is (Îµ, Î´)-DP, assuming that each âˆ¥giâˆ¥2â‰¤1. The noise scale per average gradient
is then Ïƒ/âˆš
b. This is the inverse signal-to-noise ratio and is plotted in Figure 10a.
We fix a batch size of 1024as the curves flatten out by this point for all the values of Îµ
considered. See also (Ponomareva et al., 2023, Fig. 1).
â€¢Tuning the number of steps : Now that we fixed the batch size, we train for as many steps
as possible in a 24 hour time limit (this is 12Ã—more expensive than non-private training).
Note that DP training is slower due to the need to calculate per-example gradients. This
turns out to be around 50 epochs or 1200 steps.
â€¢Tuning the learning rate : We tune the learning rate while keeping the gradient clipping
norm at C= 1.0(note that non-private training is not sensitive to the value of gradient
clip norm). We experiment with different learning rate and pick 3Ã—10âˆ’4as it has the best
validation loss for Îµ= 8(see Figure 10b). We use this learning rate for all values of Îµ.
D.5 Analysis of Duplicates in CC News
The CC News dataset from HuggingFace Datasets has 708241examples, each of which has the
following fields: web domain (i.e., the â€œuserâ€), the text (i.e. the body of the article), the date of
publishing, the article title, and the URL. Each example has a unique URL . However, the text of
the articles from a given domain are not all unique. In fact, there only 628801articles (i.e., 88.8%
of the original dataset) after removing exact text duplicates from a given domain. While all of the
duplicates have unique URLs, 43K out of the identified 80K duplicates have unique article titles).
We list some examples of exact duplicates below:
â€¢which.co.uk : â€œWe always recommend that before selecting or making any important deci-
sions about a care home you take the time to check that it is right for your or your relativeâ€™s
particular circumstances. Any description and indication of services and facilities on this page
27have been provided to us by the relevant care home and we cannot take any responsibility for
any errors or other inaccuracies. However, please email us on the address you will find on our
About us page if you think any of the information on this page is missing and / or incorrect.â€
has3K duplicates.
â€¢amarujala.com : â€œRead the latest and breaking Hindi news on amarujala.com. Get live
Hindi news about India and the World from politics, sports, bollywood, business, cities,
lifestyle, astrology, spirituality, jobs and much more. Register with amarujala.com to get
all the latest Hindi news updates as they happen.â€ has 2.2K duplicates.
â€¢saucey.com : â€œThank you for submitting a review! Your input is very much appreciated.
Share it with your friends so they can enjoy it too!â€ has 1K duplicates.
â€¢fox.com : â€œGet the new app. Now including FX, National Geographic, and hundreds of
movies on all your devices.â€ has 0.6K duplicates.
â€¢slideshare.net : â€œWe use your LinkedIn profile and activity data to personalize ads and
to show you more relevant ads. You can change your ad preferences anytime.â€ has 0.5K
duplicates.
â€¢ft.com: â€œ$11.77perweek*PurchaseaNewspaper+PremiumDigitalsubscriptionfor$11.77
per week. You will be billed $66.30 per month after the trial endsâ€ has 200duplicates.
â€¢uk.reuters.com : â€œBank of America to lay off more workers (June 15): Bank of America
Corp has begun laying off employees in its operations and technology division, part of the
second-largest U.S. bankâ€™s plan to cut costs.â€ has 52copies.
As shown in Figure 11, a small fraction of examples account for a large number of duplicates
(the right end of the plot). Most of such examples are typically web scraping errors. Some of the
web domains have legitimate news article repetitions, such as the last example above. In general,
these experiments suggest that exact or approximate deduplication for the data contributed by each
deduplication is a low cost preprocessing step that can moderately reduce the privacy risks posed
by user inference.
E Additional Experimental Results
We give full results on the ArXiv Abstracts dataset, provide further results for example-level DP,
and run additional ablations. Specifically, the outline of the section is:
â€¢Appendix E.1: Additional experimental results showing user inference on the ArXiv dataset.
â€¢Appendix E.2: Additional experiments on the effect of increasing the dataset size.
â€¢Appendix E.3: Tables of TPR statistics at particular values of small FPR.
â€¢Appendix E.4: ROC curves corresponding to the example-level DP experiment (Table 2).
â€¢Appendix E.5: Additional ablations on the aggregation function and reference model.
E.1 Results on the ArXiv Abstracts Dataset
Figure 12 shows the results for the ArXiv Abstracts dataset. Broadly, we find that the results are
qualitatively similar to those of Reddit Comments and CC News.
28100101102103
Number of duplicates of an article text100101102103104105Number of OccurencesCounts of text duplicates in CC NewsFigure 11: Histogram of number of duplicates in CC News. The right side of the plot shows a small
number of unique articles have a large number of repetitions.
âˆ’1.0âˆ’0.5 0.0 0.5 1.0 1.5 2.0
Attack Statistic050010001500200025003000CountHistogram of Attack Statistic
Held-In
Held-Out
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateROC Curve (AUC = 0.572)
10âˆ’410âˆ’310âˆ’210âˆ’1100
False Positive Rate10âˆ’410âˆ’310âˆ’210âˆ’1100True Positive RateROC Curve (log scale)
(a)Main attack results (cf. Figure 2): histograms of test statistics for held-in and held-out users and ROC
curve.
0 50 100 150 200 250 300 350
Fine-Tuning Iteration ( Ã—103)5051525354555657Attack AUROC (%)Attack Performance â€” ArXiv Abstracts
0 50 100 150 200 250 300 350
Fine-Tuning Iteration ( Ã—103)2.6002.6252.6502.6752.7002.7252.7502.7752.800Validation LossGeneralization â€” ArXiv Abstracts
Held-In Users
Held-Out Users
(b)Attack results over the course of training (cf.
Figure 3).
100101102
Shared Substring Length0.60.70.80.91.0Attack AUROC
Shared Length â€” Arxiv Abstracts
Canaries
Real Users
101102
Fine-Tuning Documents per User0.40.50.60.70.80.9
Finetuning Data Size â€” Arxiv Abstracts (c)Attack results with canaries (cf. Figure 6).
Figure 12: Results on the ArXiv Abstracts dataset.
29Quantitatively, the attack AUROC is 57%, in between Reddit ( 56%) and CC News ( 66%).
Figure 12b shows the user-level generalization and attack performance for the ArXiv dataset. The
Spearman rank correlation between the user-level generalization gap and the attack AUROC is at
least 99.8%, which is higher than the 99.4%of CC News (although the trend is not as clear visually).
Thisreiteratesthecloserelationbetweenuser-leveloverfittinganduserinference. Finally, theresults
of Figure 12c are also nearly identical to those of Figure 6, reiterating their conclusions.
E.2 Effect of Increasing the Dataset Size: Reddit
We now compare the effect increasing the size of the dataset has on user inference. To be precise,
we compare the full Reddit dataset that contains 6 months of scraped comments with a smaller
version that uses 4 months of data (see Appendix D.1 and Figure 13a for details).
We find in Figure 13b that increasing the size of the dataset leads to a uniformly smaller ROC
curve, including a reduction in AUROC ( 60%to56%) and a smaller TPR at various FPR values.
0.000 0.001 0.002 0.003 0.004 0.005 0.006 0.007
Fraction of Total Fine-Tuning Data From a Single User100101102103CountData Fractions â€” Reddit Dataset Size
Full / 6 Months
Small / 4 Months
(a)Histogram of fraction of data per user.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateROC Curves â€” Reddit Dataset Size
10âˆ’410âˆ’310âˆ’210âˆ’110âˆ’310âˆ’210âˆ’1Full / 6 months (AUROC = 55.8%)
Small / 4 months (AUROC = 60.1%) (b)The corresponding ROC curves.
Figure 13: Effect of increasing the fraction of data contributed by each user: Since Reddit Full
(6 Months) contains more users than Reddit Small (4 Months), each user contributes a smaller
fraction of the total fine-tuning dataset. As a result, the user inference attack on Reddit Full is less
successful, which agrees with the intuition from Proposition 1.
E.3 Attack TPR at low FPR
We give some numerical values of the attack TPR and specific low FPR values.
Main experiment. While Figure 2 summarizes the attack performance with the AUROC, we give
the attack TPR at particular FPR values in Table 4. This result shows that while Enronâ€™s AUROC
is large, its TPR at FPR = 1%at4.41%is comparable to the 4.41%of CC News. However, for
FPR= 5%, the TPR for Enron jumps to nearly 28%, which is much larger than the 11%of CC
News.
CC News Deduplication. The TPR statistics at low FPR are given in Table 5.
30FPR % TPR%
Reddit CC News Enron ArXiv
0.1 0 .28 1 .18N/A 0.38
0.5 0 .67 2 .76N/A 1.31
1 1 .47 4 .33 4 .41 2 .24
5 7 .05 11 .02 27 .94 8 .44
10 15 .45 18 .27 57 .35 15 .77
Table 4: Attack TPR at small FPR values corresponding to Figure 2.
CC News Variant AUROC % TPR% at FPR =
0.1% 0 .5% 1% 5% 10%
Original 65.73 1.18 2.76 4.33 11.02 18.27
Deduplicated 59.08 0.58 1.00 1.75 7.32 11.31
Table 5: Effect of within-user deduplication: Attack TPR at small FPR values corresponding to Figure 8.
E.4 ROC Curves for Example-Level Differential Privacy
The ROC curves corresponding to the example-level differential privacy is given in Figure 14. The
ROC curves reveal that while example-level differential privacy (DP) reduces the attack AUROC,
we find that the TPR at low FPR remains unchanged. In particular, for FPR = 3%, we have TPR
= 6%for the non-private version but TPR = 10%forÎµ= 32. This shows that example-level DP is
ineffective at fully thwarting the risk of user inference.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate0.00.20.40.60.81.0True Positive RateROC Curves (Linear Scale) vs. DP Îµ
10âˆ’1100
False Positive Rate10âˆ’1100True Positive RateROC Curves (Log Scale) vs. DP Îµ
Îµ= 2
Îµ= 32
Îµ=âˆ
Figure 14: ROC curves (linear and log scale) for the example-level differential privacy on the
Enron Emails dataset.
31E.5 Additional Ablations
The user inference attacks implemented in the main paper use the pre-trained LLM as a reference
model and compute the attack statistic as a mean of log-likelihood ratios described in Section 3.
In this section, we study different choices of reference model and different methods of aggregating
example-level log-likelihood ratios. For each of the attack evaluation datasets, we test different
choices of reference model and aggregation function for performing user inference on a fine-tuned
GPT-Neo 125M model.
In Table 6 we test three methods of aggregating example-level statistics and find that averaging
the log-likelihood ratio outperforms using the minimum or maximum per-example ratio. Addi-
tionally, in Table 7 we find that using the pre-trained GPT-Neo model as the reference model
outperforms using an independently trained model of equivalent size, such as OPT (Zhang et al.,
2022) or GPT-2 (Radford et al., 2019). However, in the case that an attacker does not know or
have access to the pre-trained model, using an independently trained LLM as a reference still yields
strong attack performance.
Attack Statistic
AggregationReddit Comments ArXiv Abstracts CC News Enron Emails
Mean 56.0Â±0.7 57.2Â±0.465.7Â±1.1 87.3Â±3.3
Max 54.5Â±0.8 56 .7Â±0.4 62 .1Â±1.1 71 .1Â±4.0
Min 54.6Â±0.8 55 .3Â±0.4 63 .3Â±1.0 57 .9Â±4.0
Table 6: Attack statistic design : We compare the default mean aggregation of per-document statistics
log(pÎ¸(x(i))/pref(x(i)))in the attack statistic (Section 3) with the min/max over documents i= 1, . . . , m.
We show the mean and std AUROC over 100 bootstrap samples of the held-in and held-out users.
Reference Model ArXiv Abstracts CC News Enron Emails
GPT-Neo 125Mâˆ—57.2Â±0.465.8Â±1.1 87.8Â±3.5
GPT-2 124M 53.1Â±0.565.7Â±1.2 74.1Â±4.5
OPT 125M 53.7Â±0.5 62 .0Â±1.2 77 .9Â±4.2
Table 7: Effect of the reference model : We show the user inference attack AUROC (%)for different
choices of the reference model pref, including the pretrained model pÎ¸0(GPT-Neo 125M, denoted byâˆ—). We
show the mean and std AUROC over 100 bootstrap samples of the held-in and held-out users.
F Discussion on User-Level DP
Differential privacy (DP) at the user-level gives quantitative and provable guarantees that the
presence or absence of one userâ€™s data is indistinguishable. Concretely, a training procedure is
(Îµ, Î´)-DP at the user level if the model pÎ¸trained on the data from set Uof users and a model
pÎ¸,utrained on data from users Uâˆª {u}satisfies
P(pÎ¸âˆˆA)â‰¤exp(Îµ)P(pÎ¸,uâˆˆA) +Î´ , (7)
and analogously with pÎ¸,pÎ¸,uinterchanged, for any outcome set Aof models, any user uand any U
of users. Here, Îµis known as the privacy budget and a smaller value of Îµdenotes greater privacy.
32In practice, this involves â€œclippingâ€ the user-level contribution and adding noise calibrated to
the privacy level (McMahan et al., 2018).
The promise of user-level DP. User-level DP is the strongest form of protection against user
inference. For instance, suppose we take
A=(
Î¸:1
mmX
i=1log 
pÎ¸(x(i))
pref(x(i))!
â‰¤Ï„)
to be set of all models whose test statistic calculated on x(1:m)âˆ¼ Dm
uis at most some threshold Ï„.
Then, the user-level DP guarantee (7) says that the test statistic between pÎ¸andpÎ¸,uare nearly
indistinguishable (in the sense of (7)). In other words, the attack AUROC is provably bounded as
function of the parameters (Îµ, Î´)(Kairouz et al., 2015).
User-level DP has successfully been deployed on industrial applications with user data (Ra-
maswamy et al., 2020; Xu et al., 2023). However, these applications are in the context of federated
learning with small on-device models.
The challenges of user-level DP. While user-level DP is a natural solution to mitigate user
inference, it involves several challenges, including fundamental dataset sizes, software/systems chal-
lenges, and a lack of understanding of empirical tradeoffs.
First, user-level DP can lead to a major drop in performance, especially if the number of users
in the fine-tuning dataset is not very large. For instance, the Enron dataset with O(150)users is
definitely too small while CC news with O(3000)users is still on the smaller side. It is common
for studies on user-level DP to use datasets with O(100K)users. For instance, the Stack Overflow
dataset, previously used in the user-level DP literature, has around 350Kusers (Kairouz et al.,
2021).
Second, user-aware training schemes including user-level DP and user-level clipping, require
sophisticated user-sampling schemes. For instance, we may require operations of the form â€œsample
4 users and return 2 samples from eachâ€. On the software side, this requires fast per-user data
loaders, which are not supported by standard training workflows, which are oblivious to the user-
level structure in the data.
Third, user-level DP also requires careful accounting of user contributions per round and bal-
ancing user contributions per-round and the number of user participations over all rounds. The
trade-offs involved here are not well-studied, and require a detailed investigation.
Finally, existing approaches require the datasets to be partitioned into disjoint user data subsets.
Unfortunately, this is not always true in applications such as email threads (where multiple users
contribute to the same thread) or collaborative documents. The ArXiv Abstracts dataset suffers
from this latter issue as well. This is a promising direction for future work.
Summary. In summary, the experimental results we presented make a strong case for user-level
DP at the LLM scale. Indeed, our results motivate the separate future research question on how to
effectively apply user-level DP given accuracy and compute constraints.
33