Teaching Embodied Reinforcement Learning Agents:
Informativeness and Diversity of Language Use
Jiajun Xi*Yinong He‚àóJianing Yang Yinpei Dai Joyce Chai
University of Michigan
{jiajunxi, heyinong, jianingy, daiyp, chaijy}@umich.edu
Abstract
In real-world scenarios, it is desirable for em-
bodied agents to have the ability to leverage
human language to gain explicit or implicit
knowledge for learning tasks. Despite recent
progress, most previous approaches adopt sim-
ple low-level instructions as language inputs,
which may not reflect natural human commu-
nication. It‚Äôs not clear how to incorporate rich
language use to facilitate task learning. To
address this question, this paper studies dif-
ferent types of language inputs in facilitating
reinforcement learning (RL) embodied agents.
More specifically, we examine how different
levels of language informativeness (i.e., feed-
back on past behaviors and future guidance)
anddiversity (i.e., variation of language ex-
pressions) impact agent learning and inference.
Our empirical results based on four RL bench-
marks demonstrate that agents trained with di-
verse and informative language feedback can
achieve enhanced generalization and fast adap-
tation to new tasks. These findings highlight
the pivotal role of language use in teaching em-
bodied agents new tasks in an open world.1
1 Introduction
Developing embodied agents that can understand
and communicate with humans in natural language
to learn and accomplish tasks is a long-standing
goal in artificial intelligence. In recent years, the
integration of human language and reinforcement
learning (RL) has seen significant advancements.
Unlike traditional RL methods that typically rely
on numerical reward signals to guide agent learn-
ing, recent works (Cheng et al., 2023; Lin et al.,
2023) explore using language as an intuitive and
useful signal to shape an agent‚Äôs behaviors. For ex-
ample, when the agent is making mistakes during
the task completion, providing language feedback
*Equal contribution.
1Source code available at https://github.com/
sled-group/Teachable_RL .can largely improve the instantaneous performance
thus enhancing the overall agent learning efficiency
and effectiveness (McCallum et al., 2023).
However, existing methods generally employ
simple instructions, such as "turn left" and"put
the apple to the table" to teach/control an agent
(Hanjie et al., 2021; Zhang and Chai, 2021; Lin
et al., 2023; McCallum et al., 2023; Shridhar et al.,
2021). While useful, these instructions may not
fully reflect the flexibility of language use in task
learning and collaboration (Chai et al., 2018, 2019;
Zhang et al., 2022, 2023; Dai et al., 2024a). In
the real world, humans often express complex lan-
guage instructions that are more informative . For
instance, when a student makes a mistake, a teacher
may help them to retrospect on what went wrong
(i.e., hindsight instructions ) and then guide them
on what should be done next to finish the goal ( i.e.,
foresight instructions ). In addition, humans are
likely to engage in conversations with more diverse
language patterns, describing the same goal with
different expressions and styles. Therefore, we ask
the following question:
How do the informativeness and diversity of
natural language used during RL training affect
an agent‚Äôs ability to learn tasks?
We take a popular offline RL model - decision
transformer (DT) (Chen et al., 2021) - as a back-
bone architecture and conduct a comprehensive
study to examine how informativeness and diver-
sity of language use may impact agents‚Äô learning
ability. To control informativeness, we leverage
expert agents‚Äô actions as a reference to generate
hindsight reflection and foresight guidance, using
hand-crafted language templates. To increase di-
versity, we construct a GPT-augmented language
pool, where GPT-4 (OpenAI, 2024) is used to aug-
ment hand-crafted templates into much more nat-
ural and richer expressions. We further extended
DT into a multi-modal Language-Teachable DTarXiv:2410.24218v1  [cs.CL]  31 Oct 2024(LTDT) and demonstrated that LTDT agents that
are trained with diverse and informative language
significantly outperform the counterpart agents that
are trained either with simple language alone or
with no language inputs. Notably, we found that
even with just one language template, combining
hindsight and foresight feedback together improves
agents‚Äô performance by an average of 9.86 points
(from 37.95% to 47.81%) on four popular offline
RL benchmarks compared to agents trained without
language. When more language diversity is incor-
porated into training, an additional 10.14 points
(from 47.81% to 57.95%) are obtained.
The contributions of this paper can be summa-
rized as follows:
‚Ä¢We investigate in detail, for the first time,
how language informativeness and diversity
affect offline RL agents in task learning, and
demonstrate their important roles in improv-
ing agents‚Äô performance, adaptability, and ro-
bustness.
‚Ä¢We show that training agents with informa-
tive and diverse instructions can intrinsically
improve the agent‚Äôs understanding of the task
and lead to better performance.
‚Ä¢We propose a simple framework to generate
both hindsight and foresight language feed-
back and enrich language variation without
any human annotators.
2 Related Work
Offline Reinforcement Learning Offline rein-
forcement learning (RL) has become a focal point
of research due to its ability to utilize pre-existing
datasets for training agents without real-time in-
teractions. Several algorithms address the unique
challenges of offline RL, such as mitigating extrap-
olation errors and ensuring robust policy evalua-
tion. A survey by Prudencio et al. (2023) outlines
the field‚Äôs taxonomy and open problems. Bench-
marking efforts by Fujimoto et al. (2019) assess
various batch deep RL algorithms. Key approaches
include Conservative Q-Learning (CQL) (Kumar
et al., 2020), Implicit Q-Learning (IQL) (Kostrikov
et al., 2021), and the Decision Transformer (DT)
(Chen et al., 2021), which treats RL as a sequence
modeling problem (Janner et al., 2021). Recent
work also explores generalization across tasks (Lee
et al., 2022; Reed et al., 2022; Schubert et al., 2023),
the use of exploratory data (Yarats et al., 2022), and
integrating large language models (LLMs) (Mir-chandani et al., 2023). Efficient online RL lever-
aging offline data is also a focus (Ball et al., 2023;
Modhe et al., 2023). Our research builds on the De-
cision Transformer (DT) by integrating language
feedback, creating the Language-Teachable Deci-
sion Transformer (LTDT). This novel approach in-
corporates rich, human-like language instructions,
improving agent learning through enhanced infor-
mativeness and diversity of language inputs.
Language in Reinforcement Learning The in-
tersection of natural language and RL offers new
ways to develop intuitive and effective learning
paradigms for embodied agents. Initial works uti-
lized language for feedback and task instructions
(She and Chai, 2017; Nguyen et al., 2017; Shrid-
har et al., 2020). Recent studies have explored
various methods for incorporating language feed-
back in RL, such as the LTC paradigm (Wang
et al., 2023), lifelong robot learning with human-
assisted language planners (Parakh et al., 2023),
and frameworks for rich information requests (Dai
et al., 2020; Tseng et al., 2021; Nguyen et al., 2022).
Language for corrections (Sharma et al., 2022; Liu
et al., 2023) and as reward signals (Xie et al., 2023;
Goyal et al., 2019; Yu et al., 2023) has shown
to enhance agent performance. Vision-language
joint training approaches, like CLIP (Radford et al.,
2021), BLIP-2 (Li et al., 2023), and InstructBLIP
(Dai et al., 2023), demonstrate the potential of com-
bining visual and language modalities for RL tasks
(Ma et al., 2023; Nguyen et al., 2019; Khandel-
wal et al., 2022). Further, multimodal prompts for
robotic manipulation (Jiang et al., 2023; Fan et al.,
2022) and LLMs for planning in robotics (Ahn
et al., 2022; Huang et al., 2022; Singh et al., 2023;
Yao et al., 2022; Dai et al., 2024b) highlight the
evolving role of language in RL. Other works, like
(Mehta et al., 2023), focus on generating problem-
specific language feedback templates. In contrast,
our work focuses on the informativeness and diver-
sity of language instructions, two problem-agnostic
yet easy-to-implement properties. By using both
hindsight and foresight language templates and en-
hancing diversity through GPT-4, we demonstrate
notable improvements in agent performance and
generalizability, showcasing the impact of complex
language inputs in offline RL training.
3 Problem Setting
In this section, we outline the problem setting by
defining the offline reinforcement learning problemHomeGridEnvironment
ALFWorld
MetaWorldTask Language Action
Messenger
Find  / Get  {obj}
Open  {bin_type }
Rearrange  {obj}
Pick {obj} and  put it in {place}
Clean  {obj} and put it in {place }
Get the  message and 
then send it to the goal .
Assembly: Pick up  the  wrench  
and put it on the pegLeft() Right()
Up() Down()Left() Right()
Up() Down()
PickUp (obj) Drop(obj)
Pedal(bin) Grasp(bin)
Lift(bin) Get(obj)
Open (gripper)Raise(gripper)
Close(gripper)MoveTo(gripper, pose)
Drop(gripper)Goto( recept ) Put(obj)
Open( recept ) Close( recept )
Take(obj) Look()
Clean(obj) Heat(obj)
H: You are too close to the enemy {name}.
F: Go {direction} to dodge the enemy {name}.
H: That‚Äôs a poor move since you are not 
avoiding the enemy {name}.
F: Please move {direction} to elude the 
enemy {name} on your track. 
H: You have gone to the wrong direction.
F: Pedal to open the recycling bin.
H: You seem to be heading away 
from the right route.
F: To access the recycling bin, you‚Äôll 
need to pedal.
H: You made a mistake by taking the
bad action {action}.
F: Take {action} in the next step.
H: The choice to implement {action} 
was misguided.
F: I suggest you try {action} for now.
H: Good job! You are correctly {action}.
F: It‚Äôs time to {action}.
H: That‚Äôs an excellent step to {action}.
F: To complete the task, you have to {action}Hammer:  Pick up the hammer  
and hit the nailGet to the goal  and then 
find the message .Pretrain
Heat {obj} and put it in {place}AdaptationPretrain
Adaptation
Clean -up
Pretrain
Adaptation
Pretrain
AdaptationFigure 1: An overview of four environments used for experiments. It shows tasks to be learned in each environment;
examples of hindsight (marked H) and foresight ( F) language feedback (next to the gear icon are hand-crafted
templates and next to the GPT icon are GPT-4 generated feedback); as well as low-level actions in each environment.
(Sec. 3.1), and a taxonomy of language feedback
(Sec. 3.2). Then we describe the instantiation of
such definitions in four different RL environments
we used for experiments (Sec. 3.3).
3.1 Offline Reinforcement Learning
To support a systematic study of language use, we
formulate the problem in the offline reinforcement
learning (RL) setting. At each time step t, the
agent receives an observation ot, a reward rt, and
a language feedback ltfor its previous action. The
agent then executes an action ataccording to a
policy œÄ, which is conditioned on the entire in-
teraction history htup to time t, i.e., œÄ(at|ht),
where ht={o‚â§t, r‚â§t, l‚â§t, a<t}represents the his-
tory of observations, rewards, language feedback,
and past actions up to time t. The agent‚Äôs goal is
to complete the task by maximizing the expected
discounted sum of rewards E[PT
t=1Œ≥trt]where T
is the episode length, and Œ≥is the discount fac-
tor. In offline RL, the training trajectories are pre-
collected with an expert agent (a well-trained agent
or a planner-based expert with privileged informa-
tion). The trained agents are evaluated interactively
with the environment.
3.2 Language Feedback: Informativeness and
Diversity
We aim to investigate how the informativeness and
diversity of language instructions used during the
training of an offline RL agent affect the agent‚Äôsperformance on seen tasks and adaptation to unseen
tasks.
3.2.1 Informativeness
Informativeness refers to the richness of infor-
mation content in language feedback. Following
Cheng et al. (2023), we categorize feedback into
two types: hindsight andforesight . Hindsight
feedback involves comments or critiques about
the agent‚Äôs past actions. For example, "Excellent,
you are moving towards the goal!" encourages the
agent to continue its current path, while "You are
getting too close to the enemy." alerts the agent
about a mistake. Hindsight feedback reflects on
incorrect actions taken in previous steps, which can
guide agents toward success by narrowing down the
search space for correct actions (See Appendix E
for more analysis). Conversely, foresight feedback
guides potential future actions. For instance, "You
should go right to get closer to the target." directs
the agent towards the goal, and "You should go left
to avoid the enemy on the right." helps the agent
make strategic decisions to avoid threats. Language
feedback is considered most informative when it
includes both hindsight and foresight elements, and
least informative when neither is present.
3.2.2 Diversity
Diversity in language feedback refers to the vari-
ety of ways the same information is conveyed. If
feedback is provided using only one template, itis less diverse. It becomes more diverse when the
same information is expressed in many different
ways. The goal is to expose the RL agent to vari-
ous expressions of the same feedback to enhance
its ability to generalize.
3.3 Environments
As shown in Figure 1, we conduct experiments
across four environments‚ÄîHomeGrid, ALFWorld,
Messenger, and MetaWorld‚Äîeach featuring dis-
crete action spaces, with hand-crafted hindsight
and foresight language instructions. More informa-
tion and examples of languages for each environ-
ment can be found in Appendix A.
HomeGrid (Lin et al., 2023) is a multitask grid
world designed to evaluate how well agents can
understand and use various types of language to
complete tasks. It includes five task types ( FIND ,
GET,CLEAN UP ,REARRANGE ,OPEN ), involving
interaction with objects and trash bins with a total
of 38 tasks. The agent receives a reward of 1 when
the task is completed and receives a reward of 0.5
if a subgoal is completed.
ALFWorld (Shridhar et al., 2021) is a text-game
environment that aligns with the embodied AL-
FRED benchmark (Shridhar et al., 2020) and pro-
vides simulation for household tasks. It includes six
types of tasks which require the agent to navigate
and interact with household objects by following
language instructions. The agent gets a reward
of 1 when the task is completed. We adopt the
hindsight and foresight language templates from
LLF-ALFWorld introduced in (Cheng et al., 2023),
which adds an extra language wrapper to the origi-
nal ALFWorld environment.
Messenger (Hanjie et al., 2021) is a grid world
with several entities. The agent‚Äôs task is to retrieve
a message from one entity and deliver it to another
goal entity, while avoiding enemies. At the start of
each episode, the agent is provided with a manual
describing the randomized roles of the entities and
their movement dynamics. The agent receives a
reward of 1 when the task is completed.
MetaWorld (Yu et al., 2019) is a benchmark that
consists of a variety of manipulation tasks per-
formed by a simulated Sawyer robot arm. It in-
cludes 50 types of common robot manipulation
tasks. We select two of them in our experiments:
ASSEMBLY and HAMMER . The agent receives a
reward of 1 when completing a task.Algorithm 1 Offline Data Collection
1: Initialize D ‚Üê ‚àÖ
2:foreach episode with seed ido
3: Initialize Di‚Üê ‚àÖ
4: Initialize environment env withseed i.
5: Append task description TdtoDi
6: Initialize the non-expert agent with a sub-optimal pol-
icyœÄ.
7: Initialize the expert agent with policy œÄ‚àó.
8: foreach time step do
9: at‚ÜêœÄ(ht)
10: a‚àó
t‚ÜêœÄ‚àó(ht)
11: rt, st, lhind
t, lfore
t‚Üêenv(at, a‚àó
t|ht).
12: ifUse GPT-augmented Pool then
13: lhind
t =GPT-augmented (lhind
t )
14: lfore
t =GPT-augmented (lfore
t)
15: end if
16: lt‚ÜêÔ£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥lhind
t+lfore
t if H + F
lhind
t if only H
lfore
t if only F
<empty> if No Lang
17: Append (rt, st, at, lt)toDi
18: end for
19: Aggregate Datasets D ‚Üê D ‚à™ D i
20:end for
4 Data Generation
To train an agent that can understand language feed-
back in an offline reinforcement learning manner,
we construct an offline dataset Dconsisting of two
parts:
‚Ä¢Agent trajectory consisting of task description
Tdand the tuples (ÀÜRt, st, at), where ÀÜRtrep-
resents the reward, stis the state, and atis the
action.
‚Ä¢language feedback ltconveying hindsight and
foresight information at each time step.
Algorithm 1 outlines the data generation process,
and we explain the algorithm in detail in the fol-
lowing sections.
4.1 Trajectory Generation
To improve model generalization and avoid overfit-
ting, it is essential to train on diverse, sub-optimal
trajectories rather than relying solely on optimal
ones generated by an expert agent (Kumar et al.,
2020; Chen et al., 2021). We achieve this by in-
troducing perturbations to an expert planner (see
Appendix B), allowing the non-expert agent to
produce sub-optimal trajectories. This promotes
broader exploration of the state-action space, en-
hancing the model‚Äôs ability to generalize to unseen
scenarios (Kumar et al., 2020; Chen et al., 2021).
During data collection, we begin by appending
the task description Tdto the trajectory sequence
and initializing the environment with a fixed seed.H: You seem to be heading away from 
    the right route . 
F: Make a 180 -degree turn right now.
ùëéùë°‚àí1‚àó‚Äúpedal‚ÄùExpert Agent ùúã‚àó 
prediction
Agent ùúã in 
environmentùëéùë°‚àó ùëéùë°+1‚àó‚Äúdown‚Äù ‚Äúpedal‚Äù
ùëéùë°‚àí1‚Äúup‚Äù
ùëéùë°‚Äúdown‚Äù
Time Stepùë°‚àí1 ùë° ùë°+1Task:
Open the bin
H: You have gone to the wrong direction .
F: Turn back.H: You are doing well so far.
F: Pedal to open the recycling bin.
Environment Simulator
GPT Template PoolExpert agent‚Äôs action
Compare ùúã with ùúã‚àóAgent‚Äôs action
H: So far, so good, you‚Äôre doing great!
F: To access the recycling bin, you‚Äôll
need to pedal.
Figure 2: A demonstration of hindsight and foresight language feedback generation. In our framework, the agent œÄ
executes the trajectory, while the expert agent œÄ‚àó, with access to privileged ground truth knowledge, is used solely
to provide information for generating language feedback to œÄ. At time step t, hindsight language is generated by
comparing the agent‚Äôs action at‚àí1with the expert agent‚Äôs action a‚àó
t‚àí1, whereas foresight language is generated by
referring to the expert agent‚Äôs action a‚àó
tto guide the agent on the next step. To increase the diversity of language
feedback, we construct a pool of language templates comprising GPT-augmented languages, and sample candidate
instructions as online language feedback.
A non-expert agent, using a sub-optimal policy œÄ
derived from the expert agent‚Äôs optimal policy œÄ‚àó,
interacts with the environment. At each time step,
the environment state ot, reward ÀÜRt, and the non-
expert agent‚Äôs action atare recorded to form the tra-
jectory sequence: (Td,ÀÜR1, s1, a1, . . . , ÀÜRt, st, at).
4.2 Language Feedback Generation
For the second part of the dataset D, we collect
the language feedback along the non-expert agent‚Äôs
trajectory. As shown in Figure 2, we follow a struc-
tured process to generate diverse and informative
language feedback. For the state at time step t, the
expert agent œÄ‚àóproposes an expert action a‚àó
t(e.g.
"down") at this state, which is further transformed
into a foresight template lfore
t(e.g. "Turn back.")
by the environment simulator, guiding the agent on
what should be done at this state. After the non-
expert agent œÄsteps the environment (into time step
t+ 1) with its generated action at(e.g. "down"),
the environment simulator generates a hindsight
template lhind
t+1(e.g. "You are doing well so far.")
based on the comparison between agent action at
and expert agent action a‚àó
tat the last time step t,
reflecting on whether the agent is on the right track.
For each foresight/hindsight template, we use
GPT-4 to augment it into more natural and variedexpressions. (e.g. We can augment "You are doing
well so far." into "Up until now, you‚Äôre doing won-
derfully." or"So far, so good, you‚Äôre doing great!". )
We compile all the rewritten sentences into a set
called the GPT-augmented language pool . At each
step of the non-expert agent, we randomly select
one candidate from the pool as the language instruc-
tion. This process ensures the feedback provided
to the agent has high level of diversity and enriches
the learning experience.
The level of informativeness and diversity of
the language feedback depends on the inclusion of
hindsight and foresight (e.g. concatenated when
both are required) and the use of GPT-augmented
language pool. The language feedback at each time
step will finally get concatenated with the trajectory
sequence into (Td,ÀÜR1, s1, a1, l1, . . .ÀÜRt, st, at, lt).
Algorithm 1 summarizes the data collection pro-
cess.
5 Model
Architecture. We extend the Decision Trans-
former (DT) architecture (Chen et al., 2021) to
create the Language-Teachable Decision Trans-
former (LTDT) by augmenting the input to in-
clude language feedback. This architecture is a
decoder-only transformer, similar to GPT-2 (Rad-Causal Transformer
Task Description‚Ä¶
MLP Embedding  & Positional EncodingLinear Decoder
‚Ä¶ùë†ùë°‚àí1‡∑†ùëÖùë°‚àí1ùëôùë°‚àí1ùëéùë°‚àí1ùë†ùë°‡∑†ùëÖùë°ùëôùë°ùëéùë°‡∑úùëéùë°‚àí1 ‡∑úùëéùë°Figure 3: Language-Teachable Decision Transformer.
ford et al., 2019), and models a trajectory sequence
(Td,ÀÜR1, s1, a1, l1, . . . , ÀÜRt, st, at, lt), with the lan-
guage feedback input appended at each step and
a task description (TD) input prefixed at the be-
ginning of the sequence. Like the original DT,
the embeddings of these inputs are passed through
the Causal Transformer, which encodes positional
information to maintain sequence order. The trans-
former‚Äôs output is used to predict the next action
in the sequence, conditioned on the state, return-
to-go, action, and language feedback in the last K
time steps, with the task description as the prefix
(4K+ 1tokens in total), as shown in Figure 3.
Training. Similar to the original DT training,
given an offline dataset of trajectory sequences, we
sample a sub-sequence of length K(with 4K+
1tokens), and the prediction head is trained to
predict discrete actions with the cross-entropy loss
or continuous actions with the MSE loss. More
training details can be found in Appendix G.
Language Embeddings. We use language em-
beddings from a frozen Sentence-BERT model
(Reimers and Gurevych, 2019) in all environments.
We find Sentence-BERT more sensitive to language
feedback changes, capturing nuanced semantic dif-
ferences better.
6 Experiment
In this section, we design experiments to answer
the following two research questions (RQs):
‚Ä¢RQ 1 : How do the informativeness anddiver-
sityof language affect agents‚Äô performance on
seen tasks?
‚Ä¢RQ 2 : How does the informativeness of the
language feedback affect pre-trained agents‚Äô
adaptability on unseen tasks?
For RQ1, we control agents trained with hind-
sight information, foresight information, or both
to investigate the function of informativeness. We
compare agents trained with language from both
hand-crafted templates and the GPT-augmented
language pool to examine the function of language
diversity.
For RQ2, agents are taught in languages from
the GPT-augmented language pool and tested onunseen tasks after fine-tuning with few-shot sam-
ples.
6.1 Experimental Setup
Setup for RQ 1. We compare performance on
seen tasks between agents trained with varying lev-
els of language informativeness and diversity: 1)
theNo Language agent is trained without any lan-
guage instructions; 2) the Template Foresight
agent is trained with hand-crafted foresight lan-
guage templates; 3) the Template Hindsight
agent is trained with hand-crafted hindsight lan-
guage templates; 4) the Template Hindsight +
Foresight agent is trained with hand-crafted fore-
sight and hindsight language templates; and 5) the
GPT-augmented Hindsight + Foresight agent
is trained with hindsight and foresight languages
from the GPT-augmented language pool. We train
on 100, 1,000, 20,000, and 10,000 trajectories
for HomeGrid, ALFWorld, Messenger, and Meta-
World environments, respectively. Evaluation is
performed over 5 runs, with 100 random seeds for
each run.
Setup for RQ 2. We pre-train different agents
on seen tasks and then compare adaptability
(how well an agent performs after few-shot learn-
ing) on unseen tasks: 1) the No Language
pre-trained agent is pre-trained without any
language instructions; 2) the GPT-augmented
hindsight pre-trained agent is pre-trained with
hindsight language from the GPT-augmented lan-
guage pool; 3) the GPT-augmented foresight
pre-trained agent is pre-trained with foresight
language from the GPT-augmented language pool;
4) the GPT-augmented hindsight + foresight
pre-trained agent is pre-trained with both hind-
sight and foresight language from the GPT-
augmented language pool. During the few-shot
adaptation stage, we choose to fine-tune the pre-
trained agents with both hindsight + foresight lan-
guage from the GPT-augmented language pool for
all settings, since this mimics a real-world few-shot
learning scenario, where humans likely provide di-
verse feedback, including both hindsight and fore-
sight, to guide the agent in new tasks. We pretrain
on 6,432, 1,000, 20,000, and 10,000 trajectories
for HomeGrid, ALFWorld, Messenger, and Meta-
World, respectively. For all environments, we adapt
on 5, 10, and 20 trajectories to 1 new task. Evalua-
tion is performed over 5 runs, with 100 seeds per
run.
Further details on task setup of RQ 1 and RQ0.10.20.30.40.5RewardHomeGrid
0.20.30.40.50.6ALFWorld
0.20.40.60.8Messenger
0.40.50.6Metaworld
No Language
T emplate Hindsight + ForesightT emplate Hindsight
GPT-augmented Hindsight + ForesightT emplate ForesightFigure 4: Comparison of agent performance in four environments (averaged across 100 seeds in each environment)
under varying levels of language feedback informativeness and diversity. Agents trained with more informative lan-
guage feedback exhibit progressively higher performance. Furthermore, given the same informativeness (Hindsight
+ Foresight), increasing diversity with the GPT-augmented language pool leads to the highest performance.
5 shot 10 shot 20 shot0.00.20.40.6RewardHomeGrid
5 shot 10 shot 20 shot0.00.20.4ALFWorld
5 shot 10 shot 20 shot0.00.20.40.6Messenger
5 shot 10 shot 20 shot0.00.20.40.6MetaWorld
No Language Pretrained
GPT-augmented Hindsight PretrainedGPT-augmented Foresight Pretrained
GPT-augmented Hindsight + Foresight Pretrained
Figure 5: Comparison of agent performance on unseen tasks in four environments (averaged across 100 seeds
in each environment) under varying language informativeness in agent pre-training. Agent trained with more
informative language adapts to new tasks faster and better.
2 can be found in Appendix C. Additional results
when training and adapting on same types of lan-
guage can be found in Appendix D.
Evaluation. At inference time, an agent is given a
short task description before it starts to act, and lan-
guage feedback along its execution. The language
feedback should ideally come from real humans,
who provide feedback varying in informativeness,
diversity, and frequency (how often feedback is pro-
vided). However, recruiting and moderating real
humans to generate online feedback is expensive
and difficult to scale. Therefore, we employ GPT-4
to provide online language feedback to mimic real
humans. Specifically, at each time step, we provide
all necessary context information to GPT-4 in its
prompt and let it decide ‚Äúwhether to speak‚Äù (fre-
quency), ‚Äúwhat to speak‚Äù (informativeness), and
‚Äúhow to speak‚Äù (diversity). The context informa-
tion, in this case, consists of the ground-truth envi-
ronment states, action/state history, and template-
based hindsight and foresight short text description
generated by comparing the actions of the expert
agent and the trained agent. GPT-4 then has the
freedom to rephrase, combine, shorten, and discard
such context information to utter diverse, coherent,
and natural language feedback, mimicking a real
human. See Appendix H for an example of such
GPT-generated online feedback.
Metric. We use the reward value as our main met-
ric. Agents receive a reward of 1 upon task com-
pletion for all environments and receive additional
rewards for achieving specific sub-goals for theHomeGrid and ALFWorld environments.
6.2 Experimental Results
Results for RQ 1. As we can see in Figure 4,
agents trained with both diverse and informative
language feedback ( GPT-augmented Hindsight
+ Foresight ) consistently achieve the highest per-
formance across all environments. The varied and
paraphrased instructions generated from GPT pro-
vide a richer set of linguistic inputs, enabling the
agents to develop a more robust language under-
standing for task execution during evaluation.
When examining the impact of informativeness,
we observe that agents trained with both hindsight
and foresight information (Template Hindsight +
Foresight) consistently achieve higher performance
across all environments compared to those trained
with only hindsight or foresight information. This
indicates that integrating both types of feedback
enhances the informativeness of the language, en-
abling the agents to develop a more comprehen-
sive understanding and leading to better decision-
making and overall performance. The only excep-
tion is in the Messenger environment, where the
no-language agent exhibits a surprisingly strong
performance. However, upon further investigation
of this exception, we find that if the hindsight-
only or foresight-only feedback is from the GPT-
augmented pool, the agent can still outperform the
No Language agent (refer to Appendix F).
In terms of diversity, the results show that agents
trained with diverse language feedback, as indi-0 20 40 60 80 1001.00
0.75
0.50
0.25
0.000.250.500.751.00HomeGrid
0 20 40 60 80 1000.75
0.50
0.25
0.000.250.500.751.00ALFWorld
0 20 40 60 80 1000.6
0.4
0.2
0.00.20.40.60.8Messenger
0 20 40 60 80 1001.00
0.75
0.50
0.25
0.000.250.500.751.00MetaWorld
T ask DifficultyEfficiency Gain
Efficiency Gain Fitted Efficiency Gain TrendFigure 6: Efficiency gain vs. task difficulty. We fit the scatter plots with a second-degree polynomial to visualize the
overall trend. As task difficulty increases, the general trend of the efficiency gain is to rise initially and then decline,
suggesting: (1) for tasks that are too easy or too hard, language feedback does not improve efficiency; (2) language
feedback is most helpful in increasing efficiency for moderate tasks.
0% 20% 40% 60% 80% 100%
Language Frequency0.30.40.50.60.70.8Reward
Messenger
MetaworldHomeGrid
ALFWorld
Figure 7: Performance vs. language frequency. Agents
perform better with more frequent language feedback
across four environments.
cated by the ‚ÄòGPT-augmented‚Äô bars, consistently
outperform those trained with less varied language
input. The rich set of augmented instructions gen-
erated by GPT helps agents develop a more flexible
and nuanced understanding of task instructions,
which translates to better performance during eval-
uation. This highlights the critical role of linguistic
diversity in enhancing the robustness and adapt-
ability of the agents‚Äô language comprehension, ul-
timately leading to improved task execution across
different environments.
Results for RQ 2. The results in Figure 5 re-
veal that agents pre-trained with more informa-
tive language can adapt to unseen tasks faster and
better . ‚ÄúAdapting faster‚Äù is evident by the fact
that agents pre-trained with GPT-augmented Hind-
sight + Foresight language in 5 or 10 shots can
already achieve a similar performance 20-shot per-
formance of agents trained with less informative
language. ‚ÄúAdapting better‚Äù is evident by the fact
that, at a given number of shots available for adap-
tation, the agent trained with the most informative
language performs the best compared to its less
informatively-pretrained counterparts. These re-
sults indicate that agents pre-trained with more
informative language can adapt and generalize to
new tasks faster and better.
6.3 Ablation Study
Efficiency Gain vs. Task Difficulty. Can lan-
guage feedback help the agent to achieve moredifferent tasks? To answer this question, we de-
fineefficiency gain as the difference in efficiency
between an agent trained with informative and di-
verse GPT languages, and an agent trained without
any languages. Efficiency is measured by a path-
weighted reward, as introduced in ALFRED (Shrid-
har et al., 2020). This reward, rp, is calculated as
rp=r√óL‚àó
max(L,L‚àó), where ris the total reward, L
is the agent‚Äôs trajectory length, and L‚àóis the ex-
pert agent‚Äôs trajectory length. Higher rpindicates
successful task completion with fewer steps.
We define task difficulty for each configuration
by calculating the average success rates of agents
trained without language feedback, ranking these
from lowest to highest. Configurations with lower
success rates are considered more difficult, indi-
cating greater challenges for agents learning from
these configurations without language assistance.
As shown in Figure 6, the efficiency gain gener-
ally rises with increasing learning difficulty, then
declines. This suggests that: (1) for tasks that are
too easy or too hard, language feedback does not
improve efficiency; (2) language feedback is most
helpful in increasing efficiency for moderate tasks.
Performance vs. Language Frequency. In the
main experiments, we utilize an online GPT model
to determine whether to provide language feedback
at each time step. However, it is important to ex-
plore how varying the frequency of language feed-
back influences agent performance. To investigate
this, we control the feedback frequency by sam-
pling according to pre-defined probabilities (e.g.,
20%, 40%). The language feedback is extracted
from the GPT-augmented language pool; if no lan-
guage is sampled, an empty string is provided to
the agent. The evaluation is conducted on agents
trained with both hindsight and foresight feedback
derived from the GPT-augmented language pool.
As illustrated in Figure 7, agents‚Äô performance im-0.10.20.30.40.5RewardHomeGrid
0.20.30.40.50.6ALFWorld
0.40.50.60.70.8Messenger
0.20.30.40.50.60.7Metaworld
Empty feedback
Disturbed feedbackNormal feedback
Baseline trained without languagesFigure 8: We investigate two special evaluation settings:
(1) no language feedback is provided during evaluation
and (2) disturbed language feedback is given at every
step. Results show that agents trained with the GPT-
augmented language still outperform the no-language
agent (the black dotted line) in the disturbed setting, and
also achieve better performance in some environments
while no language is given.
proves steadily across all environments with more
frequent language feedback during evaluation. This
finding suggests that agents trained with informa-
tive and diverse language feedback can continually
absorb and leverage new information when addi-
tional feedback is provided, leading to enhanced
performance.
Performance under Corrupted Language. This
ablation aims to evaluate how agents perform when
provided with incorrect instructions. We assess
the performance of an agent trained with GPT-4-
augmented informative and diverse language under
two conditions: (1) Empty Feedback: the absence
of language feedback during testing, and (2) Dis-
turbed Feedback: the provision of disturbed lan-
guage at each step. The disturbed language consists
of redundant, irrelevant, or misleading informa-
tion (e.g., incorrect actions or objects) and is gen-
erated using GPT-augmented templates with dis-
rupted content. The results in Figure 8 reveal two
interesting findings: (1) When tested without any
language feedback, the agent trained with informa-
tive and diverse language performs comparably or
even exceeds the performance of the agent trained
without any language (represented by the black dot-
ted line). This indicates that the agent develops a
robust intrinsic understanding of the task, demon-
strating that it does not overly rely on language
feedback; (2) When exposed to disturbed feedback,
the agent trained with informative and diverse lan-
guage maintains performance levels comparable to
the no-language agent. This showcases the agent‚Äôs
ability to withstand misleading information, a criti-
cal trait for real-world applications where human
feedback may be unreliable.
7 Conclusion
In this paper, we investigate how the informative-
ness and diversity of language feedback affectembodied agents. We introduce the Language-
Teachable Decision Transformer (LTDT), which
makes decisions based on human language feed-
back. To facilitate the training of LTDT agents,
we propose an easy-to-use pipeline for collecting
offline hindsight and foresight GPT templates. We
compare the performance of agents by varying the
informativeness and diversity of the training lan-
guages across four reinforcement learning environ-
ments and evaluate the agents‚Äô ability to understand
real-world human language using online GPT as a
proxy. Our results demonstrate that training with
more informative and diverse language feedback
significantly enhances agent performance and en-
ables fast adaptation to unseen tasks.
Limitations
Our study has several limitations. First, the investi-
gated environments are primarily game-based and
do not test the agents‚Äô ability to incorporate real-life
visual inputs. Future work will focus on evaluating
agents in more realistic and complex environments
that involve real-world visual inputs and challenges.
Second, while GPT language outputs can produce
diverse and contextually relevant language, they
may not fully cover all human language styles and
nuances. Specifically, GPT models might miss
certain idioms, dialects, or culturally specific ref-
erences that are prevalent in human communica-
tion. Future work will aim to incorporate a broader
spectrum of language variations and test agents in
scenarios involving more diverse linguistic inputs.
Ethical Impacts
Our study, conducted entirely within simulated en-
vironments, does not present immediate ethical
concerns. The teachable nature of our Language-
Teachable Decision Transformer (LTDT) method
is designed to make AI agents more controllable
and better aligned with human values, promoting
safer and more ethical interactions. By enhancing
agent performance through informative and diverse
language instructions, we aim to foster AI systems
that are more transparent and responsive to human
guidance, addressing ethical considerations in the
deployment of artificial intelligence. As AI be-
comes more mainstream, these considerations are
increasingly pertinent, and our work strives to ad-
vance AI technology responsibly.
Acknowledgements
This work was supported by NSF IIS-1949634 and
has benefited from the Microsoft Accelerate Foun-dation Models Research (AFMR) grant program.
We would like to thank the anonymous reviewers
for their valuable comments and suggestions.
References
Michael Ahn, Anthony Brohan, Noah Brown, Yev-
gen Chebotar, Omar Cortes, Byron David, Chelsea
Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol
Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu,
Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,
Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jes-
month, Nikhil Joshi, Ryan Julian, Dmitry Kalash-
nikov, Yuheng Kuang, Kuang-Huei Lee, Sergey
Levine, Yao Lu, Linda Luu, Carolina Parada, Pe-
ter Pastor, Jornell Quiambao, Kanishka Rao, Jarek
Rettinghouse, Diego Reyes, Pierre Sermanet, Nico-
las Sievers, Clayton Tan, Alexander Toshev, Vincent
Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,
Mengyuan Yan, and Andy Zeng. 2022. Do as i can
and not as i say: Grounding language in robotic af-
fordances. In arXiv preprint arXiv:2204.01691 .
Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey
Levine. 2023. Efficient online reinforcement learning
with offline data. arXiv preprint arXiv:2302.02948 .
Joyce Chai, Maya Cakmak, and Candy Sidner. 2019.
Teaching robots new tasks through natural interac-
tion. In K. A. Cluck and J. E. Laird, editors, Inter-
active Task Learning: Agents, Robots, and Humans
Acquiring New Tasks through Natural Interactions .
MIT Press.
Joyce Chai, Qiaozi Gao, Lanbo She, Shaohua Yang,
Sari Saba-Sadiya, and Guangyue Xu. 2018. Lan-
guage to action: Towards interactive task learning
with physical agents. In Proceedings of the Twenty-
Seventh International Joint Conference on Artificial
Intelligence, IJCAI 2018, July 13-19, 2018, Stock-
holm, Sweden , pages 2‚Äì9. ijcai.org.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,
Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind
Srinivas, and Igor Mordatch. 2021. Decision trans-
former: Reinforcement learning via sequence mod-
eling. Advances in neural information processing
systems , 34:15084‚Äì15097.
Ching-An Cheng, Andrey Kolobov, Dipendra Misra,
Allen Nie, and Adith Swaminathan. 2023. Llf-bench:
Benchmark for interactive learning from language
feedback. arXiv preprint arXiv:2312.06853 .
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
Boyang Li, Pascale Fung, and Steven Hoi.
2023. Instructblip: Towards general-purpose vision-
language models with instruction tuning. Preprint ,
arXiv:2305.06500.
Yinpei Dai, Jayjun Lee, Nima Fazeli, and Joyce Chai.
2024a. Racer: Rich language-guided failure recov-
ery policies for imitation learning. arXiv preprint
arXiv:2409.14674 .Yinpei Dai, Hangyu Li, Chengguang Tang, Yongbin
Li, Jian Sun, and Xiaodan Zhu. 2020. Learning low-
resource end-to-end goal-oriented dialog for fast and
reliable system deployment. In Proceedings of the
58th Annual Meeting of the Association for Compu-
tational Linguistics , pages 609‚Äì618.
Yinpei Dai, Run Peng, Sikai Li, and Joyce Chai. 2024b.
Think, act, and ask: Open-world interactive person-
alized robot navigation. In 2024 IEEE International
Conference on Robotics and Automation (ICRA) ,
pages 3296‚Äì3303. IEEE.
Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-
dlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,
De-An Huang, Yuke Zhu, and Anima Anandkumar.
2022. Minedojo: Building open-ended embodied
agents with internet-scale knowledge. Advances in
Neural Information Processing Systems , 35:18343‚Äì
18362.
Scott Fujimoto, Edoardo Conti, Mohammad
Ghavamzadeh, and Joelle Pineau. 2019. Benchmark-
ing batch deep reinforcement learning algorithms.
arXiv preprint arXiv:1910.01708 .
Prasoon Goyal, Scott Niekum, and Raymond J Mooney.
2019. Using natural language for reward shap-
ing in reinforcement learning. arXiv preprint
arXiv:1903.02020 .
Austin W. Hanjie, Victor Zhong, and Karthik
Narasimhan. 2021. Grounding language to entities
and dynamics for generalization in reinforcement
learning. In Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24
July 2021, Virtual Event , volume 139 of Proceedings
of Machine Learning Research , pages 4051‚Äì4062.
PMLR.
Peter E Hart, Nils J Nilsson, and Bertram Raphael. 1968.
A formal basis for the heuristic determination of min-
imum cost paths. IEEE transactions on Systems Sci-
ence and Cybernetics , 4(2):100‚Äì107.
Wanwei He, Yinpei Dai, Binyuan Hui, Min Yang, Zheng
Cao, Jianbo Dong, Fei Huang, Luo Si, and Yongbin
Li. 2022a. Space-2: Tree-structured semi-supervised
contrastive pre-training for task-oriented dialog un-
derstanding. arXiv preprint arXiv:2209.06638 .
Wanwei He, Yinpei Dai, Min Yang, Jian Sun, Fei Huang,
Luo Si, and Yongbin Li. 2022b. Unified dialog model
pre-training for task-oriented dialog understanding
and generation. In Proceedings of the 45th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval , pages 187‚Äì
200.
Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu,
Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei
Huang, Luo Si, et al. 2022c. Galaxy: A generative
pre-trained model for task-oriented dialog with semi-
supervised learning and explicit policy injection. In
Proceedings of the AAAI conference on artificial in-
telligence , volume 36, pages 10749‚Äì10757.Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
Igor Mordatch, Yevgen Chebotar, Pierre Sermanet,
Noah Brown, Tomas Jackson, Linda Luu, Sergey
Levine, Karol Hausman, and Brian Ichter. 2022. In-
ner monologue: Embodied reasoning through plan-
ning with language models. In arXiv preprint
arXiv:2207.05608 .
Michael Janner, Qiyang Li, and Sergey Levine. 2021.
Offline reinforcement learning as one big sequence
modeling problem. Advances in neural information
processing systems , 34:1273‚Äì1286.
Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi
Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, An-
ima Anandkumar, Yuke Zhu, and Linxi Fan. 2023.
Vima: General robot manipulation with multimodal
prompts. In Fortieth International Conference on
Machine Learning .
Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi,
and Aniruddha Kembhavi. 2022. Simple but effec-
tive: Clip embeddings for embodied ai. In Proceed-
ings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 14829‚Äì14838.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
2021. Offline reinforcement learning with implicit
q-learning. In International Conference on Learning
Representations .
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey
Levine. 2020. Conservative q-learning for offline
reinforcement learning. Advances in Neural Informa-
tion Processing Systems , 33:1179‚Äì1191.
Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang,
Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian
Fischer, Winnie Xu, Eric Jang, Henryk Michalewski,
et al. 2022. Multi-game decision transformers. Ad-
vances in Neural Information Processing Systems ,
35:27921‚Äì27936.
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
2023. Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large lan-
guage models. Preprint , arXiv:2301.12597.
Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner,
Pieter Abbeel, Dan Klein, and Anca Dragan. 2023.
Learning to model the world with language. arXiv
preprint arXiv:2308.01399 .
Zeyi Liu, Arpit Bahety, and Shuran Song. 2023.
Reflect: Summarizing robot experiences for fail-
ure explanation and correction. arXiv preprint
arXiv:2306.15724 .
Yecheng Jason Ma, William Liang, Vaidehi Som,
Vikash Kumar, Amy Zhang, Osbert Bastani, and Di-
nesh Jayaraman. 2023. Liv: Language-image repre-
sentations and rewards for robotic control. Preprint ,
arXiv:2306.00958.Sabrina McCallum, Max Taylor-Davies, Stefano Al-
brecht, and Alessandro Suglia. 2023. Is feedback all
you need? leveraging natural language feedback in
goal-conditioned rl. In NeurIPS 2023 Workshop on
Goal-Conditioned Reinforcement Learning .
Nikhil Mehta, Milagro Teruel, Patricio Figueroa Sanz,
Xin Deng, Ahmed Hassan Awadallah, and Julia Kisel-
eva. 2023. Improving grounded language understand-
ing in a collaborative environment by interacting
with agents through help feedback. arXiv preprint
arXiv:2304.10750 .
Suvir Mirchandani, Fei Xia, Pete Florence, Danny
Driess, Montserrat Gonzalez Arenas, Kanishka Rao,
Dorsa Sadigh, Andy Zeng, et al. 2023. Large lan-
guage models as general pattern machines. In 7th
Annual Conference on Robot Learning .
Nirbhay Modhe, Qiaozi Gao, Ashwin Kalyan, Dhruv
Batra, Govind Thattai, and Gaurav Sukhatme. 2023.
Exploiting generalization in offline reinforcement
learning via unseen state augmentations. arXiv
preprint arXiv:2308.03882 .
Khanh Nguyen, Hal Daum√© III, and Jordan Boyd-
Graber. 2017. Reinforcement learning for bandit
neural machine translation with simulated human
feedback. arXiv preprint arXiv:1707.07402 .
Khanh Nguyen, Debadeepta Dey, Chris Brockett, and
Bill Dolan. 2019. Vision-based navigation with
language-based assistance via imitation learning
with indirect intervention. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 12527‚Äì12537.
Khanh X Nguyen, Yonatan Bisk, and Hal Daum√© Iii.
2022. A framework for learning to request rich and
contextually useful information from humans. In In-
ternational Conference on Machine Learning , pages
16553‚Äì16568. PMLR.
OpenAI. 2024. Gpt-4 technical report. Preprint ,
arXiv:2303.08774.
Meenal Parakh, Alisha Fong, Anthony Simeonov, Tao
Chen, Abhishek Gupta, and Pulkit Agrawal. 2023.
Lifelong robot learning with human assisted language
planners. In CoRL 2023 Workshop on Learning Ef-
fective Abstractions for Planning (LEAP) .
Rafael Figueiredo Prudencio, Marcos ROA Maximo,
and Esther Luna Colombini. 2023. A survey on of-
fline reinforcement learning: Taxonomy, review, and
open problems. IEEE Transactions on Neural Net-
works and Learning Systems .
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from
natural language supervision. In International confer-
ence on machine learning , pages 8748‚Äì8763. PMLR.Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog .
Scott Reed, Konrad Zolna, Emilio Parisotto, Ser-
gio G√≥mez Colmenarejo, Alexander Novikov,
Gabriel Barth-maron, Mai Gim√©nez, Yury Sulsky,
Jackie Kay, Jost Tobias Springenberg, et al. 2022. A
generalist agent. Transactions on Machine Learning
Research .
Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
InProceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing . Associa-
tion for Computational Linguistics.
Ingmar Schubert, Jingwei Zhang, Jake Bruce, Sarah
Bechtle, Emilio Parisotto, Martin Riedmiller, Jost To-
bias Springenberg, Arunkumar Byravan, Leonard
Hasenclever, and Nicolas Heess. 2023. A gener-
alist dynamics model for control. arXiv preprint
arXiv:2305.10912 .
Pratyusha Sharma, Balakumar Sundaralingam, Valts
Blukis, Chris Paxton, Tucker Hermans, Antonio Tor-
ralba, Jacob Andreas, and Dieter Fox. 2022. Cor-
recting robot plans with natural language feedback.
Preprint , arXiv:2204.05186.
Lanbo She and Joyce Chai. 2017. Interactive learning
of grounded verb semantics towards human-robot
communication. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers , pages 1634‚Äì1644.
Association for Computational Linguistics.
Mohit Shridhar, Jesse Thomason, Daniel Gordon,
Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke
Zettlemoyer, and Dieter Fox. 2020. Alfred: A bench-
mark for interpreting grounded instructions for every-
day tasks. Preprint , arXiv:1912.01734.
Mohit Shridhar, Xingdi Yuan, Marc-Alexandre C√¥t√©,
Yonatan Bisk, Adam Trischler, and Matthew
Hausknecht. 2021. ALFWorld: Aligning Text and
Embodied Environments for Interactive Learning.
InProceedings of the International Conference on
Learning Representations (ICLR) .
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit
Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox,
Jesse Thomason, and Animesh Garg. 2023. Prog-
prompt: Generating situated robot task plans using
large language models. In 2023 IEEE International
Conference on Robotics and Automation (ICRA) ,
pages 11523‚Äì11530. IEEE.
Bo-Hsiang Tseng, Yinpei Dai, Florian Kreyssig, and
Bill Byrne. 2021. Transferable dialogue systems
and user simulators. In Proceedings of the 59th An-
nual Meeting of the Association for Computational
Linguistics and the 11th International Joint Confer-
ence on Natural Language Processing (Volume 1:Long Papers) , pages 152‚Äì166, Online. Association
for Computational Linguistics.
Kuan Wang, Yadong Lu, Michael Santacroce, Yeyun
Gong, Chao Zhang, and Yelong Shen. 2023. Adapt-
ing llm agents through communication. Preprint ,
arXiv:2310.01444.
Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu,
Qian Luo, Victor Zhong, Yanchao Yang, and Tao
Yu. 2023. Text2reward: Automated dense reward
function generation for reinforcement learning. arXiv
preprint arXiv:2309.11489 .
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik R Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. In The Eleventh International Conference
on Learning Representations .
Denis Yarats, David Brandfonbrener, Hao Liu, Michael
Laskin, Pieter Abbeel, Alessandro Lazaric, and Ler-
rel Pinto. 2022. Don‚Äôt change the algorithm, change
the data: Exploratory data for offline reinforcement
learning. In ICLR 2022 Workshop on Generalizable
Policy Learning in Physical World .
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,
Karol Hausman, Chelsea Finn, and Sergey Levine.
2019. Meta-world: A benchmark and evaluation
for multi-task and meta reinforcement learning. In
Conference on Robot Learning (CoRL) .
Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kir-
mani, Kuang-Huei Lee, Montse Gonzalez Arenas,
Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasen-
clever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu,
Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa
Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023. Lan-
guage to rewards for robotic skill synthesis. Arxiv
preprint arXiv:2306.08647 .
Yichi Zhang and Joyce Chai. 2021. Hierarchical task
learning from language instructions with unified
transformers and self-monitoring. In Findings of
the Association for Computational Linguistics: ACL-
IJCNLP 2021 , pages 4202‚Äì4213, Online. Association
for Computational Linguistics.
Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks,
Nikhil Devraj, Ziqiao Ma, Keunwoo Yu, Yuwei Bao,
and Joyce Chai. 2022. DANLI: Deliberative agent for
following natural language instructions. In Proceed-
ings of the 2022 Conference on Empirical Methods
in Natural Language Processing , pages 1280‚Äì1298,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Yichi Zhang, Jianing Yang, Keunwoo Yu, Yinpei Dai,
Shane Storks, Yuwei Bao, Jiayi Pan, Nikhil Devraj,
Ziqiao Ma, and Joyce Chai. 2023. Seagull: An em-
bodied agent for instruction following through situ-
ated dialog. In Alexa Prize SimBot Challenge Pro-
ceedings .AEnvironments and Language Feedback
A.1 Environments Overview
The Appendix Table 1 lists the information that is
inherently available within the environment. All
models, regardless of whether they are trained with
language input or not, will have access to this envi-
ronmental information.
Env Image Observation Instruction Manual Text State Description
HomeGrid Yes No No
AlfWorld No No Yes
Messenger No Yes No
MetaWorld No No No
Table 1: Information provided by each environment.
A.2 Language Feedback for Different
Environments
For each environment, we design multiple tem-
plates conveying different meanings, and then ap-
plied GPT-4 to augment the languages into a GPT-
augmented language pool. The number of tem-
plates and the corresponding GPT-augmented sen-
tences for each template are shown in Appendix
Table 2.
Env # Hind Templates # Fore Templates # AUG
HomeGrid 20 9 70
AlfWorld 4 4 200
Messenger 4 4 80
MetaWorld 2 6 180
Table 2: Number of templates and augmented sentences
for each environment, where ‚Äô# Hind Templates‚Äô refers
to the number of hindsight templates, ‚Äô# Fore Templates‚Äô
refers to the number of foresight templates, and ‚Äô# AUG‚Äô
refers to the number of GPT-augmented sentences per
template.
A.2.1 HomeGrid
HomeGrid is a multitask grid world designed
to evaluate how well agents can understand and
use various types of language to complete tasks.
Agents will receive both task specifications and
language hints, providing prior knowledge about
world dynamics, information about world states, or
corrections to assist the agents. We adopt the lan-
guage hints in HomeGrid as foresight and further
extend the environment to provide hindsight that
provides comments on agents‚Äô past performance.
Agents are expected to ground both hindsight and
foresight to the environment to achieve higher per-
formance. It includes five task types involving in-
teraction with objects and bins (find, get, clean up,
rearrange, open), with a total of 38 tasks. Objectlocations, bin locations, and bin dynamics are ran-
domized. The agent receives a reward of 1 when
the task is completed, and receives a reward of
0.5 if a subgoal exists (e.g., get the object in the
clean-up task) and gets completed. Each template
language is augmented to 70 sentences in the GPT
template pool. Examples of hindsight and foresight
languages are as follows:
‚Ä¢ Hindsight Examples:
Template:
‚ñ∑"You have gone to the wrong direc-
tion."
‚ñ∑"You are doing well so far."
GPT Template:
‚ñ∑"You seem to be heading away from
the right route."
‚ñ∑"So far, so good, you are doing
great!"
‚Ä¢ Foresight Examples:
Template:
‚ñ∑"Turn back."
‚ñ∑"Pedal to open the recycling bin."
GPT Template:
‚ñ∑"Make a 180-degree turn right now."
‚ñ∑"To access the recycling bin, you‚Äôll
need to pedal."
Language instructions are generated based on the
comparison of agent‚Äôs action and expert planer ac-
tion, considering distance, relative location, and
interaction between the agent and target objects.
A.2.2 ALFWorld
ALFWorld is a text-game environment that aligns
with the embodied ALFRED benchmark (Shridhar
et al., 2020) and provides simulation for house-
hold tasks. It includes six types of tasks where
agents need to navigate and interact with house-
hold objects through text actions. The location of
the task objects is randomly located among 50 loca-
tions in each episode, making the task challenging
for the agent to plan and for the subgoals. For
the experiment, we adopt LLF-ALFWorld (Cheng
et al., 2023), which provides an extra language
wrapper for hindsight and foresight language gen-
eration over the original ALFWorld. The languages
are generated based on both agents‚Äô past actions
and the optimal trajectory for the current episode.
Agent gets a reward of 1 when the task is completed.
Each template is augmented to 200 sentences inGPT template pool. Examples of hindsight and
foresight languages are as follows:
‚Ä¢ Hindsight Examples:
Template:
‚ñ∑"You made a mistake by taking the
bad action {action}."
‚ñ∑"It was a right decision to not take
the bad action {action}."
GPT Template:
‚ñ∑"The choice to implement {action}
was misguided."
‚ñ∑"You made a sensible choice by not
committing to the {avoid action}."
‚Ä¢ Foresight Examples:
Template:
‚ñ∑"You should now take the {action}
action."
‚ñ∑"Take {action} in the next step."
GPT Template:
‚ñ∑"Consider taking the {action} as your
next step."
‚ñ∑"Moving on, consider the {action}
action."
Language instructions are generated based on ex-
perts‚Äô next action and whether agent‚Äôs past actions
are aligned with expert past actions, considering
whether agents have moved to the target position
and conducted correct interaction with the objects.
A.2.3 Messenger
Messenger is a grid world with several entities.
The agent‚Äôs primary task is to retrieve a message
from one entity and deliver it to another goal entity,
all while avoiding enemies. At the start of each
episode, the agent is provided with a manual de-
scribing the randomized roles of the entities and
their movement dynamics. The challenge lies in
the fact that the agent does not have access to the
true identity of each entity and must ground the
text manual to the dynamics, necessitating multi-
hop reasoning. (For example, grounding the "an
approaching queen is a deadly enemy" to the obser-
vations of dynamics.) (Lin et al., 2023) The agent
receives a sparse reward of 1 when the task is com-
pleted. Each template language is augmented to 80
sentences in the GPT template pool. Examples of
hindsight and foresight languages are as follows:
‚Ä¢ Hindsight Examples:
Template:‚ñ∑"It‚Äôs good that you are getting close
to the {target} at {target direction}
by moving {direction}!"
‚ñ∑"Stepping {action direction}, yet you
ran into {enemy name}. Be more
cautious."
GPT Template:
‚ñ∑"Good job on approaching the {tar-
get} to the {target direction} by mov-
ing {direction}! "
‚ñ∑"Stepping {action direction} directly
met {enemy name}. Needs strategic
thinking."
‚Ä¢ Foresight Examples:
Template:
‚ñ∑"Move {optimal direction} to ap-
proach the {target name} located at
the {target direction}. "
‚ñ∑"Rest assured, there are no enemies
around."
GPT Template:
‚ñ∑"To get to the {target name} at {tar-
get direction}, go {optimal direc-
tion}. "
‚ñ∑"Not detecting any danger, it‚Äôs safe."
When generating the language instructions, we
compare the agent‚Äôs actions and the expert‚Äôs ac-
tions, considering the locations of the target and
nearest enemy, calculating the distance and gener-
ate the hindsight reflections based on some engi-
neered rules.
A.2.4 MetaWorld
MetaWorld is a simulated benchmark that includes
a variety of manipulation tasks performed using a
Sawyer robot arm. It includes 50 types of robot
manipulation tasks common in daily life. Since
our main goal is not meta-learning, we select the
"assembly" and "hammer" tasks for pretraining and
adaptation in our experiments. This requires the
agent to pick up the tool and aim at the specific tar-
get with high precision. To increase the challenge
of the tasks, we introduce random disturbances at
random steps. This requires the robot to actively re-
cover and return to its normal trajectory whenever
it deviates. The agent receives a sparse reward of 1
when completing the task. Each template language
is augmented to 180 template languages in the GPT
template pool. Examples of hindsight and foresight
languages are shown in the following:
‚Ä¢ Hindsight Examples:Template:
‚ñ∑"It‚Äôs excellent to raise the gripper."
‚ñ∑"You are making mistakes for not
opening your gripper."
GPT Template:
‚ñ∑"Good job for raising your gripper."
‚ñ∑"You make a regrettable mistake
since your gripper is closing."
‚Ä¢ Foresight Examples:
Template:
‚ñ∑"It‚Äôs time to grasp the wrench now."
‚ñ∑"Please raise the hammer."
GPT Template:
‚ñ∑"Can you grab the wrench with your
gripper?"
‚ñ∑"I think the hammer should be raised
now."
We compare the agent‚Äôs actions with the expert‚Äôs
actions, and tell the agent‚Äôs whether their decisions
at the previous step matches with the expert‚Äôs ac-
tions, and inform them of what an expert will do at
the next step.
B Agent for Offline Data Collection and
Language Feedback Generation
We use an expert agent and a non-expert agent with
sub-optimal policies during the data collection. The
sub-optimal policy is used for introducing some er-
rors or perturbations in the training data, and letting
the expert policy continue to recover. This helps
agents learn to recover from potential failures using
hindsight reflections and foresight instructions. In
our experiments, we introduced 10-20% random
noise in each trajectory as a sub-optimal policy. We
found that this level of perturbation aids learning,
but excessive disturbance (e.g., >50% per trajec-
tory) significantly degrades performance as agents
start learning suboptimal behaviors.
B.1 HomeGrid
For the HomeGrid environment, we design an ex-
pert planer to work as the expert agent. We first
divide the task into several sub-tasks (i.e. divide
"open the recycling bin" into 1. "navigate to the
bin", 2. "open the bin"). For navigation (move
to some place) sub-tasks, we implement breadth-
first search to find the optimal path; for inter-
action sub-task (interact with object), we output
the corresponding action. We implement the non-
expert agent by adding "perturbation" into the ex-
pert planer. For example, we randomly reverse thenext step of expert action and let the expert planner
recover from the error.
B.2 ALFWorld
For the ALFWorld environment, we use a pre-built
expert planer from LLF-Bench (Cheng et al., 2023)
to work as both the expert agent and the agent for
the data collection.
B.3 Messenger
As for the Messenger environment, we implement
an expert agent using the A* algorithm (Hart et al.,
1968). We define the cost by the distance to the
target and the distance to the nearest enemies, and
then heuristically search in the grid environment.
The non-expert agent in the data collection is im-
plemented by adding random disturbance to the
expert agent.
B.4 MetaWorld
We build the expert agent on the pre-defined policy
from the original MetaWorld codebase (Yu et al.,
2019) and adapt the policy to random disturbance
so that the expert planner can recover to a normal
trajectory in any situation.
C Task Settings for RQ 1 and 2
Task Setting for RQ 1. We evaluate the agents‚Äô
performance using the same tasks as in the train-
ing phase (but with different initialization of the
agents and object layout for different episodes).
Concretely, 1) in HomeGrid, we train and evalu-
ate on multi-tasks, including FIND ,GET,REAR -
RANGE and OPEN ; 2) in ALFWorld, we train and
evaluate on multi-tasks including PICK &PLACE ,
CLEAN &PLACE and HEAT &PLACE tasks; 3) in
Messenger, we train and evaluate on the task goal
‚Äúfirst retrieve the message and then deliver to target
entity‚Äù ; and 4) in MetaWorld, we train and evalu-
ate on the ASSEMBLY task, in which the robot arm
needs to pick up the wrench and put it on the peg.
Task Setting for RQ 2. We evaluate agents‚Äô perfor-
mance on unseen tasks by first pre-training agents
on certain tasks and then adapting agents to un-
seen tasks with few-shot episodes. Specifically, 1)
in HomeGrid, we take FIND ,GET,REARRANGE ,
OPEN tasks for pre-training and the CLEAN -UPtask
for adaptation and evaluation; 2) in ALFWorld, we
take PICK &PLACE and CLEAN &PLACE for pre-
training and HEAT &PLACE tasks for adaptation
and evaluation; 3) in Messenger, we take ‚Äúfirst re-
trieve the message and then deliver to target entity"as the pretraining task and ‚Äúfirst get to the target
entity and then retrieve the message" (where the
order of the goal is reversed compared to the pre-
training tasks) for adaptation and evaluation; 4) in
MetaWorld, we take the ASSEMBLY task for pre-
training, and the HAMMER task for adaptation and
evaluation.
D Performance under aligned language
type with training.
As stated in Section 6.1, we use online GPT for
all evaluations in RQ 1 and 2 to mimic real-life
human language environments. In this section, we
align the evaluation language type (and adaptation
language type in RQ 2) with each agent‚Äôs corre-
sponding training language type for further investi-
gation (e.g. No Language Agent is evaluated with
empty language; Template Hindsight Agent is
evaluated with Template Hindsight). Experiments
on RQ 1 and 2 are conducted on HomeGrid and
Messenger respectively, with the results presented
in Table 3.
HomeGrid Env on RQ 1
Training Language Aligned Eval Online GPT Eval
No Lang 0.235 0.212
Template H 0.260 0.246
Template F 0.305 0.262
Template H + F 0.325 0.285
GPT-augmented H + F 0.472 0.442
Messenger Env on RQ 2 (20 Shots)
Training Language Aligned Adapt & Eval Online GPT Eval
No Lang 0.323 0.270
GPT-augmented H 0.450 0.378
GPT-augmented F 0.512 0.464
GPT-augmented H + F 0.623 0.608
Table 3: Comparison of agents‚Äô performance adapted
(for RQ 2) and evaluated with aligned language type in
HomeGrid environment on RQ 1 and Messenger envi-
ronment on RQ 2. ‚ÄòAligned (Adapt &) Eval‚Äô refers to
(adaptation &) evaluation with same type of language
in training and ‚ÄòOnline GPT Eval‚Äô refers to online GPT
evaluation (results in Section 6.2). The results show that
GPT-augmented Hindsight + Foresight evaluated with
online GPT still outperforms other training settings even
with aligned language evaluation, indicating higher lan-
guage informativeness and diversity enhance intrinsic
task understanding.
The results Table 3 show that: (1) aligning
the informativeness and diversity levels between
training, adaptation and evaluation improves the
final performance for all types; (2) more impor-
tantly, even with aligned evaluation and adaptation
language, no other settings have outperformed
GPT-augmented Hindsight + Foresight evalu-ated with online GPT . This further demonstrates
that high informativeness and diversity in training
language help agents intrinsically understand tasks
to achieve better performance.
E Impact of hindsight on future steps
Compared to foresight feedback, which provides
instructions for the correct action in the next step,
hindsight feedback reflects on incorrect actions
taken in previous steps. This retrospective analysis
can still guide agents toward success by narrow-
ing down the search space for corrective actions.
To demonstrate the effectiveness of hindsight feed-
back, we conduct a quick comparative study be-
tween the No Language agent and the Template
Hindsight agent in HomeGrid. The study was
designed as follows:
1.Both agents are driven to the same state using
an expert policy.
2.A deliberate mistake is introduced for both
agents. Three types of mistakes are designed:
‚Ä¢Navigation Mistake : The agent moves
in the opposite direction compared to the
expert action.
‚Ä¢Object Pick/Drop Mistake : The agent
picks or drops an object when the expert
action is to drop or pick, respectively.
‚Ä¢Bin Manipulation Mistake : The
agent chooses the wrong action among
pedal/lift/grasp to open a specific trash
bin.
3.We use expert actions as the ground truth (GT)
actions and compare the performance of both
agents over 500 runs.
The results are shown in Appendix Table 4: The
Mistake Type No Lang (%) Template Hindsight (%)
Navigation 37.6 ¬±0.3 46.2 ¬±0.2
Object Pick/Drop 37.4 ¬±2.5 41.8 ¬±1.6
Bin manipulation 23.5 ¬±1.2 24.8 ¬±0.9
Table 4: Comparison of performance between No
Language Agent and Template Hindsight Agent on
different Mistake Types.
results indicate that for the navigation and object
pick/drop mistakes, hindsight feedback is highly
beneficial. This is because identifying a wrong ac-
tion usually directly implies the correct action for
those mistakes (e.g., if "turn left" is wrong, "turn
right" is correct; if "pick the object" is wrong, "drop
the object" is correct). However, for the bin manip-
ulation mistake, hindsight feedback is less helpful0.00.10.20.30.40.50.60.70.8RewardMessenger
No Language
GPT augmented hindsightGPT augmented foresight
GPT augmented Hindsight + ForesightFigure 9: In the Messenger environment, when trained
with more diverse foresight and hindsight languages,
the agents can perform better than those trained without
languages. Furthermore, agents trained with more infor-
mative languages demonstrate stronger performance.
since the action space grows larger (pedal/lift/grasp,
compared to binary opposite actions in Navigation
and Object Pick/Drop), and there are no clear im-
plications for the correct action.
F More results on the Messenger
environment
In the Messenger environment, models trained
with only template foresight or hindsight languages
struggle to generalize to diverse languages during
testing. Without exposure to diverse languages dur-
ing training, these models fail to extract the learned
hindsight or foresight information from mixed and
diverse languages. However, Figure 9 demonstrates
that models trained with more diverse hindsight or
foresight languages can overcome the generaliza-
tion problem, and outperform those trained without
language feedback, showcasing the importance of
diversity in the training languages. Furthermore,
the agents trained with both hindsight and foresight
information still perform the best, aligning with
results in other environments.
G Models and Training
We build our Language-Teachable Decision Trans-
former based on the code of the original Decision
Transformer (Chen et al., 2021). In this section, we
will show our training setup and model hyperpa-
rameters for each environment.
When selecting the data size, we prioritize the
efficient use of a small-scale dataset and examine
the impact of language feedback within the con-
straints of a limited budget and scarce data, as is
common in the field of robotics.G.1 HomeGrid
Estimated parameter size of the models: 12.191
MB. For research question 1, we train the model
with 100 trajectories. For research question 2, the
pretraining stages use 6432 trajectories. The mod-
els are trained on one Nvidia RTX A6000. For
research question 1, training takes 3 GPU hours.
For research question 2, pretraining takes 4 GPU
hours and adaptation takes 3 GPU hours. Hyperpa-
rameters shown in Appendix Table 5.
G.2 ALFWorld
Estimated parameter size of the models: 6.5 MB.
For research question 1, we train the model with
1000 trajectories. For research question 2, the pre-
training stages use 10000 trajectories. The models
are trained in one Nvidia RTX A6000. For re-
search question 1, training takes 3 GPU hours. For
research question 2, pretraining takes 4 GPU hours
and adaptation takes 3 GPU hours. Hyperparame-
ters shown in Appendix Table 6.
G.3 Messenger
Estimated parameters size of the models: 289.681
MB. We train the models with 10000 data trajecto-
ries during the pretraining stage for seen tasks. The
pretraining stage for seen tasks takes 5 GPU hours
on one Nvidia RTX A6000. The adaptation stage
for unseen tasks takes 1 GPU hour. Hyperparame-
ters are shown in Appendix Table 7.
G.4 MetaWorld
Estimated parameters size of the models: 289.681
MB. We train the models with 20000 data trajec-
tories during the pretraining stage for seen tasks.
The pretraining stage for seen tasks takes 2.5 GPU
hours on one Nvidia RTX A6000. The adaptation
stage for unseen tasks takes 1 GPU hour. Hyperpa-
rameters are shown in Appendix Table 8.
H Examples for Language Feedback in
Evaluation
As discussed in section 6.1, we feed template hind-
sight ( lhind) and template foresight ( lfore) into an
online GPT to generate language feedback as a
proxy for real-world human feedback, which can
be further extended into multi-turn human-machine
dialogue systems in task-oriented settings (He et al.,
2022a,b,c). In Figure 10, we demonstrate three ex-
amples of the GPT outcome. In example 1, we find
GPT can concatenate both hindsight and foresightHyperparameters Value
Number of transformer layers 3
Number of attention heads 1
Embedding dimension 128
Nonlinearity function ReLU
Batch size 64
Context length K 10
Return-to-go conditioning 1.5
Dropout 0.1
Optimizer AdamW
Learning Rate 1e‚àí4
Grad norm clip 0.25
Weight decay 1e‚àí4
Learning rate decay Linear warmup for first 1e5training steps
Table 5: Hyperparameters of Language-Teachable Deci-
sion Transformer for HomeGrid experiments.
Hyperparameters Value
Number of transformer layers 3
Number of attention heads 1
Embedding dimension 128
Nonlinearity function ReLU
Batch size 64
Context length K 10
Return-to-go conditioning 1.5
Dropout 0.1
Optimizer AdamW
Learning Rate 1e‚àí3
Grad norm clip 0.25
Weight decay 1e‚àí4
Learning rate decay Consine Annealing with minimum lr= 1e‚àí5
Table 6: Hyperparameters of Language-Teachable Deci-
sion Transformer for ALFWorld experiments.
and integrate them into a new fluent sentence. In
the second example, we observe that GPT decides
to discard the hindsight part and provides only fore-
sight as the outcome. In example 3, GPT chooses
not to respond when it thinks the current agent
doesn‚Äôt need help.Hyperparameters Value
Number of transformer layers 5
Number of attention heads 2
Embedding dimension 128
Nonlinearity function ReLU
Batch size 128 for pertaining and 1 for adaptation
Context length K 10
Return-to-go conditioning 1.5
Dropout 0.1
Optimizer AdamW
Learning Rate 1e‚àí3for pretraining and 1e‚àí4for adaptation
Grad norm clip 0.25
Weight decay 1e‚àí4
Learning rate decay Linear warmup for first 1e5training steps
Table 7: Hyperparameters of Language-Teachable Deci-
sion Transformer for Messenger experiments.
Hyperparameters Value
Number of transformer layers 5
Number of attention heads 2
Embedding dimension 256
Nonlinearity function ReLU
Batch size 128 for pertaining and 5 for adaptation
Context length K 12
Return-to-go conditioning 20
Return scale 10
Dropout 0.1
Optimizer AdamW
Learning Rate 1e‚àí5for pertaining and 1e‚àí6for adaptation
Weight decay 1e‚àí4
Learning rate decay Linear warmup for first 1e5training steps
Table 8: Hyperparameters of Language-Teachable Deci-
sion Transformer for MetaWorld experiments.
Good effort, but the fruit is in the kitchen area.
(Concatenate H and F into a fluent sentence.)H: Your efforts up to now haven't gone unnoticed.
F: The fruit is in the kitchen area.
You should turn around and face the opposite way.
(Discard the hindsight.)H: You seem to be veering off the right track.
F: Could you swivel to face the opposite way?
(empty)
(Decide not to respond.)H: So far, you're showing a lot of promise.
F: Check the living room for the plates.1
2
3
Figure 10: Examples for language feedback generated
by online GPT in evaluation.