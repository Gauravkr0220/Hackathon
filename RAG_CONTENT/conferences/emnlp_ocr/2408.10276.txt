FEDKIM: Adaptive Federated Knowledge Injection into Medical
Foundation Models
Xiaochen Wang1*, Jiaqi Wang1*, Houping Xiao2, Jinghui Chen1, Fenglong Ma1‚Ä†
1Pennsylvania State University,2Georgia State University
1{xcwang, jqwang,jzc5917,fenglong}@psu.edu ,2hxiao@gsu.edu
Abstract
Foundation models have demonstrated remark-
able capabilities in handling diverse modali-
ties and tasks, outperforming conventional ar-
tificial intelligence (AI) approaches that are
highly task-specific and modality-reliant. In
the medical domain, however, the development
of comprehensive foundation models is con-
strained by limited access to diverse modali-
ties and stringent privacy regulations. To ad-
dress these constraints, this study introduces
a novel knowledge injection approach, FED-
KIM , designed to scale the medical founda-
tion model within a federated learning frame-
work. FEDKIM leverages lightweight local
models to extract healthcare knowledge from
private data and integrates this knowledge into
a centralized foundation model using a de-
signed adaptive Multitask Multimodal Mixture
OfExperts (M3OE) module. This method
not only preserves privacy but also enhances
the model‚Äôs ability to handle complex medi-
cal tasks involving multiple modalities. Our
extensive experiments across twelve tasks in
seven modalities demonstrate the effectiveness
ofFEDKIM in various settings, highlighting its
potential to scale medical foundation models
without direct access to sensitive data. Source
codes are available at https://github.com/
XiaochenWang-PSU/FedKIM .
1 Introduction
Similar to large language models (Zhao et al., 2023)
and foundation models (Zhou et al., 2023a), med-
ical foundation models (Thirunavukarasu et al.,
2023; Moor et al., 2023) have achieved superior per-
formance of handling diverse modalities and tasks
within the medical domain. These models have
the potential to revolutionize medical diagnostics
and treatment by leveraging data-driven insights
from large volumes of multimodal healthcare data.
*Equal contribution.
‚Ä†Corresponding author.Table 1: Summary of medical foundation models.
Medical Foundation Model Modalities Tasks
MMedLM2 (Qiu et al., 2024) Text Question-answering
LLava-Med(Liu et al., 2023a) Text, Image Visual Question-answering
Med-Flamingo(Yang et al., 2023) Text, Image Visual Question-answering
PMC_LLAMA(Lee et al., 2023) Text Question-answering
BiomedGPT(Gu et al., 2021) Text, Image Visual Question-answering
BioMedLM(Lewis et al., 2020) Text Question-answering
GatorTron(Hao et al., 2020) TextClinical concept extraction
Medical relation extraction
Semantic textual similarity
Natural language inference
Question-answering
Med-PaLM(Singhal et al., 2022) Text Question-answering
ChatDoctor(Li et al., 2023) Text Question-answering
Due to the sensitive nature of medical data and the
complexity of medical tasks, most existing medical
foundation models usually rely on particular public
medical datasets. This nature results in limitations
of the existing medical foundation models, detailed
as follows:
(1)Unrealistic to conduct large-scale central-
ized training . The centralized training of med-
ical foundation models presents significant chal-
lenges, primarily due to the difficulties in aggre-
gating sensitive healthcare data. Regulations such
as the Health Insurance Portability and Account-
ability Act (HIPAA) in the United States and the
General Data Protection Regulation (GDPR) in the
European Union impose strict privacy restrictions
on the use of personal health information. This
regulatory environment makes it impractical to col-
lect and store large amounts of healthcare data in
a single location, which is typically required for
the effective training of high-performing medical
foundation models.
(2)Limited modality and task adaptability .
Current medical foundation models exhibit a high
degree of specialization, constraining their effec-
tiveness to a narrow range of downstream tasks
within specific modalities, as outlined in Table 1.
For instance, MMedLM (Qiu et al., 2024) is tai-
lored for text, while LLava-Med (Liu et al., 2023a)
focuses on both image and text modalities. In prac-arXiv:2408.10276v3  [cs.LG]  29 Oct 2024Client 1 Client n
‚Ä¶
ùììùüè ùììùíè ùììùëµ
Prompt ùí´ ùíï
Task ùíØ ùíïModality ‚Ñ≥ ùíï
Step 2: Multimodal Multi-tasking
Mixture Of Experts (ùë¥ùüëùë∂ùë¨)
Medical 
Foundation 
Model  ‚Ñ± 
ùú∂ùüèùíï
ùú∂ùíëùíï
ùú∂ùë∑ùíï ‚Ä¶ ‚Ä¶
Various Multimodal 
Medical Tasks
Client Updates (Sec. 2.2)
(a)Framework OverviewPlug
(b)Federated Knowledge Injection into Medical Foundation Model 
Client N
ExpertsEncoder ùúΩùíÜ Global Model
(ùúΩùíÜ , ùúΩùíÖ )‚Ä¶ ‚Ä¶
‚Ä¶
Medical 
Foundation 
Model  ‚Ñ± 
Injection
Update
Upload
DownloadServer Updates (Sec. 2.3)
ùììùíëùíï
EMBùê°‡Øù‡Øß
EmbeddingStep1: Feature Alignment
Public Data
EMBPlug
Step 3: LoRA-
ùêåùüëùêéùêÑbased PETF
Figure 1: Illustration of the proposed FEDKIM . (a) Framework overview, where the proposed FEDKIM contains
client and server updates. (b) Federated knowledge injection, where FEDKIM first aggregates models uploaded
from clients and then injects the aggregated model knowledge into medical foundation model Fwith three steps.
‚ÄúPEFT‚Äù in Step 3 denotes parameter-efficient fine-tuning.
tical settings, comprehensive medical decisions of-
ten require integrating multiple types of health data
across various tasks. Yet, by being task or modality-
specific, existing models fail to recognize and lever-
age the intricate relationships between different
healthcare data modalities and tasks.
The first limitation prevents training a medi-
cal foundation model from scratch in a central-
ized manner, while the second one exacerbates the
challenge of developing a multimodal, multi-task
medical foundation model. To overcome these
obstacles, a viable solution is to scale existing
medical foundation models and infuse them with
medical knowledge. Given that medical data is
stored on private clients, the federated learning (FL)
paradigm (McMahan et al., 2017; Che et al., 2023;
Zhou et al., 2022) offers a promising approach in
the medical domain (Wang et al., 2022a), which is
a decentralized and collaborative machine learning
method where participants do not need to share
data directly. Although several recent studies on
federated foundation models (Lu et al., 2023; Chen
et al., 2024a) have made progress, they primarily
focus on enhancing services to local clients us-
ing existing foundation models. Importantly, none
have specifically tackled the challenge of inject-
ing novel medical knowledge into existing medical
foundation models in a federated manner.
To tackle this new challenge, in this paper,
we propose a novel approach: Federated Knowl-edge Injection for Medical foundation models
(FEDKIM ), as shown in Figure 1. FEDKIM adopts
a flexible design, allowing it to incorporate vari-
ous types of medical modalities to handle a vari-
ety of medical tasks. Considering the real-world
scenarios, FEDKIM deploys the medical founda-
tion model only on the server side and leverages
lightweight local models along with classic fed-
erated learning approaches to extract healthcare
knowledge from private data.
To effectively inject extracted medical knowl-
edge into the foundation model, FEDKIM uses
knowledge-rich parameters from the modality-
specific encoders updated from the local end. To
be specific, FEDKIM integrates this knowledge us-
ing parameter-efficient fine-tuning technique with
a novel multitask multimodal mixture of expert
module, namely M3OE. M3OE adaptively selects
appropriate expert systems for handling specific
tasks in given modalities, enabling FEDKIM to
deal with tasks in complex medical contexts.
Our experiments across 12 healthcare tasks with
7 modalities demonstrate the effectiveness of FED-
KIM , providing a solid foundation for future ex-
ploratory research on the medical knowledge injec-
tion problem.
2 The proposed F EDKIM Framework
In this section, we first introduce the setup of the
medical knowledge injection task (Section 2.1).Next, we describe the proposed method, FED-
KIM . As depicted in Figure 1, FEDKIM consists
of two main components: knowledge extractors
(Section 2.2), which are deployed on local clients,
and a knowledge injector (Section 2.3), which is
deployed on the server.
2.1 Framework Setups
2.1.1 Client Setups
The goal of this work is to scale and enhance
the predictive ability of medical large language
models (LLMs) by incorporating medical knowl-
edge from private client data in a federated man-
ner. To achieve this, we employ Nclients,
each representing a hospital or a medical insti-
tute holding private medical data Dn. We assume
that the private dataset Dncontains all medical
modalities {M 1,¬∑¬∑¬∑,MM}and can perform all
tasks{T1,¬∑¬∑¬∑,TT}. Each client trains a model
fn= [ENC n();DEC n()]using the data Dn, where
ENC n()is the set of multimodal encoders and
DEC n()is the set of multi-task decoders/predictors.
Thus, the model parameters Œ∏noffncan be divided
intoŒ∏enc
nfor the encoder and Œ∏dec
nfor the decoder,
which will be further uploaded to the server.
2.1.2 Server Setups
We deploy a generative medical foundation
model on the server, denoted as F. We
aim to inject medical knowledge represented by
{Œ∏enc
1,¬∑¬∑¬∑,Œ∏enc
N}intoFand simultaneously up-
date{Œ∏enc
1,¬∑¬∑¬∑,Œ∏enc
N}by absorbing new knowl-
edge from F. These updated encoders and the
aggregated decoders will then be distributed to the
corresponding clients for learning in the next com-
munication round. To facilitate the updates of client
parameters, we place a small amount of public data
on the server, denoted as Dp.
2.2 Client Updates ‚Äì Knowledge Extraction
from Private Clients
This framework allows each client to handle T
tasks simultaneously. Although these tasks have
different training data, the modalities are partially
shared, which motivates us to design a simple client
model with Mmodality-specific encoders and T
task-specific decoders. Details of the encoders are
listed in Appendix D. We then use the followingloss to train each client model:
min
Œ∏nLn:=1
TTX
t=11
|Dtn|X
(xt
i,yt
i)‚ààDtn‚Ñìt(fn(xt
i;Œ∏n),yt
i),
(1)
fn(xi;Œ∏n) =DEC n,t(ENC n,m(xi;Œ∏enc
n,m);Œ∏dec
n,t),(2)
where Dt
nis the task-specific dataset, xt
iandyt
i
are the data features and the corresponding ground
truths, and ‚Ñìtis the loss function for a specific task,
such as cross-entropy. ENC n,m‚äÜENC nis the
encoder for modality Mmwith parameters Œ∏enc
n,m.
DEC n,t‚äÜDEC nis the decoder for the t-th task
with parameters Œ∏dec
n,t. The number of modality-
level encoders in ENC n,mis determined by the
input data, while amount of tasks determines the
number of task-oriented decoders. After the local
training, we will upload the encoder and decoder
parameters Œ∏enc
nandŒ∏dec
nto the server.
2.3 Server Updates ‚Äì Knowledge Injection
into Medical LLM
2.3.1 Knowledge Aggregation
We assume that the predictive ability of Fis bet-
ter than the uploaded decoders {Œ∏dec
1,¬∑¬∑¬∑,Œ∏dec
N},
and useful knowledge is primarily contained in
the encoders {Œ∏enc
1,¬∑¬∑¬∑,Œ∏enc
N}. Thus, on the
server side, we aim to inject medical knowledge
{Œ∏enc
1,¬∑¬∑¬∑,Œ∏enc
N}into the LLM Fwith the help
of public data Dp. Before the injection, we first
aggregate knowledge uploaded from each client
in traditional federated learning manners such as
FedAvg (McMahan et al., 2017) or FedProx (Li
et al., 2020), i.e.,
Œ∏e=fFL([Œ∏1
e,¬∑¬∑¬∑,Œ∏M
e]),
Œ∏d=fFL([Œ∏1
d,¬∑¬∑¬∑,Œ∏M
d]),(3)
where fFLcan be flexibly replaced with any fed-
erated learning methods, such as personalized FL
methods (Jiang et al., 2019; T Dinh et al., 2020),
differential privacy-based FL methods (Hu et al.,
2020; El Ouadrhiri and Abdelhadi, 2022), or adap-
tive FL methods (Reddi et al., 2020; Wang et al.,
2022c,b).
2.3.2 Knowledge Injection
Effectively injecting medical knowledge Œ∏eis chal-
lenging since the LLM Fcannot directly use these
diverse modality-specific encoders. To solve this
challenge, we leverage a straightforward yet ef-
fective feature alignment strategy that follows thetraining of LLaV A (Liu et al., 2024) by concatenat-
ing the modality embeddings with the task prompt.
Subsequently, we embed our original Multimodal
Multi-tasking Mixture OfExperts (M3OE) into
the medical foundation model. M3OE allows the
medical foundation model Fto adaptively select
specific expert system given different combination
of tasks and modalities. Next, we detail the process
of knowledge injection.
Step 1:Feature Align ment. For each input data
(xt
j,yt
j)‚àà Dt
pfrom the t-th task, we first obtain
its feature representations using the aggregated en-
coders Œ∏e, i.e.,et
j= [e1
j;¬∑¬∑¬∑;eM
j] =g(Œ∏e(xt
j)),
where g(¬∑)is the linear mapping function. We also
embed the task prompt Ptusing the encoder of F,
i.e.,pt=EMBF(Pt)., where EMBF()is the text
embedding layer of F. Then, the concatenation
of the data feature et
jand the task prompt feature
ptwill be used as the input of the encoder of F,
denoted as ht
j= [et
j;pt].
Step 2:Multimodal Multi -taskingMixture of
Experts (M3OE). A naive solution is directly using
the aligned feature ht
jto generate the output. How-
ever, such a naive end-to-end fine-tuning approach
not only has weak distinguishability of different
tasks but also ignores the generalization ability of
FEDKIM to unseen tasks, even though the modal-
ities have been encountered already. To address
this issue, we develop a Multimodal Multi-tasking
Mixture OfExperts (M3OE) module to allow FED-
KIM to distinguish tasks dynamically.
M3OE takes both the task description Ttand the
modality descriptions Mtassociated with Task Tt
as inputs to compute the relevance of each expert
for the given task and modality, where Mtis the
concatenation of descriptions of all modalities con-
cerning Task Tt.TtandMtare firstly encoded
by the embedding layer of the foundation model F,
and subsequently processed to output weights for
expert selection as follows:
Œ±t=softmax 
MLP 
Pooling 
Œ≤t
,
Œ≤t=(WqEMB F(Mt))(WkEMB F(Tt))‚ä§
‚àödkWvEMB F(Tt)
(4)
where Œ±t‚ààRPandPis the number of experts.
Wq,Wk, andWvdenote the attention matrices,
anddkis the dimension size.
The proposed M3OE effectively integrates the in-
jected knowledge managed by two separate routers,
resulting in a more streamlined and contextually
aware computation of weights. The output, Œ±t, rep-resents the attention-weighted selection of experts
optimized for both the modality and the specific
task. This approach provides the flexibility needed
to handle complex medical scenarios by selecting
the appropriate experts based on the context.
Step 3:LoRA -M3OEbased Parameter-Efficient
Fine -tuning.Finally, we generate the representa-
tion of each layer in Ffor the forward pass based
on LoRA (Hu et al., 2022) and the learned M3OE
weight using Eq. (4) as follows:
ct
j=WFht
j+PX
p=1Œ±t
p(BpApht
j), (5)
where WFdenotes the frozen parameters of F,
BpApdenotes the lower-rank adaptation module
serving as the p-th expert system. We will fine-tune
the proposed FEDKIM using the final output from
Fand the ground truth yt
j. The design balances
efficacy and efficiency during knowledge injection,
allowing FEDKIM to decently handle the complex
nature of medical applications.
During the training, modality-specific encoders
Œ∏egradually align with the medical LLM Fthat
contains abundant knowledge acquired through pre-
training. The alignment indicates prior knowledge
inFis also extracted during injection, in the form
of adjusted parameters restored in encoders. To
benefit local models and boost knowledge injection
in the next round, FEDKIM passes the updated
encoders Œ∏eand the aggregated decoders Œ∏dback
to local ends and performs the knowledge-driven
iterative training until convergence.
3 Experiment Setup
3.1 Task Introduction
In this study, we have training tasks and validation
tasks across different datasets and data modalities.
To provide a clear illustration, we present them in
Table 2.
Training Task. To examine the utility of the pro-
posed FEDKIM , we leverage four classification
tasks across sixmodalities to federatedly inject
medical knowledge into the selected foundation
model through multi-task training. Details regard-
ing these tasks are available in Appendix A. As
emphasized in Section 2, we perform training on
this suite of tasks in a multi-task pattern.
Validation Task. Typical medical foundation mod-
els, such as MMedLM2 (Qiu et al., 2024), of-
ten struggle with handling unseen tasks involvingnovel modalities. To evaluate the extent to which
knowledge injection enables the medical founda-
tion model to tackle unseen tasks, we compile five
classification tasks (ECD, SP, PED, AD, and EBD)
andthree generation tasks (MR, SNC, and MS).
Details on these tasks are provided in Appendix B.
3.2 Data Partition
For each training task, we divide the data into four
parts in a ratio of 7:1:1:1. Specifically, 70% of the
data,Dn, is private data evenly distributed to N
clients for training local models. Another 10% of
the data is public data, Dp, placed on the server for
tuning the foundation model. An additional 10%
of the data is development data, Dd, kept on the
server as a validation set. The remaining 10% of
the data, Dt, is used as testing data for these tasks.
More details regarding the data distribution can be
found in Appendix C.
3.3 Baselines
Since the task of medical knowledge injection is
novel and unexplored, there are no existing base-
lines. Therefore, we establish our own baselines,
detailed as follows:
FedPlug . FedPlug acquires modality-specific en-
coders through the federated learning process de-
scribed in Section 2.2 . These encoders are then
integrated into the foundation model for fine-tuning.
By aligning multimodal medical input with the se-
mantic space of the foundation model, FedPlug
enables the model to handle multiple modalities.
Throughout this process, only the aggregated en-
coders are trainable.
FedPlug L. Building on the FedPlug framework,
FedPlug Lincorporates the Low-Rank Adaptation
(LoRA) technique (Hu et al., 2022) to better inte-
grate multimodal features into the semantic space
of the large language model (LLM), thereby opti-
mizing the federated learning process. In addition
to the trainable encoders in FedPlug, each layer of
the LLM is equipped with a tunable LoRA module.
3.4 FL Backbone Approaches
We implement our FEDKIM based upon the fol-
lowing backbone approaches:
FedA VG (McMahan et al., 2017) is a conven-
tional federated learning method, producing a
global model by aggregating distributed models[Œ∏1,¬∑¬∑¬∑,Œ∏M]as follows:
favg([Œ∏1,¬∑¬∑¬∑,Œ∏M]) =1
NNX
n=1Œ∏n.
FedProx (Li et al., 2020) aims to extend FedAvg
by regularizing each local loss function with an L2
term as follows:
min
Œ∏nJn(Œ∏n;Œ∏‚àó) =Ln(Œ∏n) +Œª
2||Œ∏n‚àíŒ∏‚àó||2,(6)
where Œ∏‚àóis the global model, Ln(¬∑)is the corre-
sponding loss function, and Œªis the hyperparameter
for weighting.
MMedLM-21(Qiu et al., 2024) is an advanced
unimodal Large Language Model. Benefiting from
multilingual pre-training, MMedLM-2 achieves the
state-of-the-art performance in multiple question
answering tasks, thus selected as the backbone of
our foundation model deployed on the server.
3.5 Implementation Details
All experiments were conducted in an Ubuntu
20.04 environment using two NVIDIA A100 GPUs.
We utilized MMedLM-2, the aforementioned state-
of-the-art pre-trained medical language model, as
the target of medical knowledge injection. The
learning rate was set to 5√ó10‚àí4for the founda-
tion model and 1√ó10‚àí4for the local models. Œª
for FedProx was set to 1√ó10‚àí4. Cross-entropy
loss was used for training the local models, while
the foundation model was optimized using general
autoregressive loss. The number of clients Nwas
set to 5, and the number of experts Pwas set to 12
forFEDKIM . To ensure a fair comparison, we set
the number of communication rounds to 10 for all
methods involved in the comparison.
4 Performance Evaluation
We examine our proposed FEDKIM from the zero-
shot evaluation (subsection 4.1) and fine-tuning
evaluation (subsection 4.2) perspectives.
4.1 Zero-shot Evaluation
In the zero-shot evaluation, there is no overlap be-
tween the training tasks and evaluation tasks, which
targets at examining the zero-shot capability of the
medical foundation models enabled by FEDKIM .
The experiment results on unseen tasks are shown
in Figure 2, with FedAvg (Figure 2a) and FedProx
1https://huggingface.co/Henrychur/MMedLM2Table 2: Tasks and modalities in this study.
Task Type TaskModality
Image Signal Vital signs Lab events Input Output Text
TrainingCOVID-19 Detection ( CD) ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
Lung Opacity Detection ( LOD ) ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
ECG Abnormal Detection ( EAD ) ‚úó ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó
Mortality Prediction ( MP) ‚úó ‚úó ‚úì ‚úì ‚úì ‚úì ‚úó
ValidationEnlarged Cardiomediastinum Detection ( ECD )‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
Pleural Effusion Detection ( PED ) ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
Atelectasis Detection ( AD) ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úó
Ectopic Beats Detection ( EBD ) ‚úó ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó
Sepsis Prediction ( SP) ‚úó ‚úó ‚úì ‚úì ‚úì ‚úì ‚úó
MedVQA-RAD ( MR) ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úì
MedVQA-Slake ( MS) ‚úì ‚úó ‚úó ‚úó ‚úó ‚úó ‚úì
Signal Noise Clarification ( SNC ) ‚úó ‚úì ‚úó ‚úó ‚úó ‚úó ‚úì
ECD
(max: 64.53)SP
(max: 95.00)MR
(max: 17.94)
SNC
(max: 12.20)
MS
(max: 2.93)
PED
(max: 50.43)
AD
(max: 51.28)EBD
(max: 50.00)MMedLM2
FedPlug
FedPlugL
FedKIM
(a) FedAvg-based Knowledge Injection Performance.
ECD
(max: 62.39)SP
(max: 97.00)MR
(max: 33.85)
SNC
(max: 30.68)
MS
(max: 7.60)
PED
(max: 58.12)
AD
(max: 58.94)EBD
(max: 50.40)MMedLM2
FedPlug
FedPlugL
FedKIM (b) FedProx-based Knowledge Injection Performance.
Figure 2: Performance comparison between F EDKIM and baselines on the zero-shot evaluation.
(Figure 2b) as the backbone federated approaches.
We use black ‚Ä¢, orange, blue, and green curves
to denote MMedLM2, FedPlug, FedPlug L, and
FEDKIM , respectively. Accuracy for classification
tasks and BLEU for generation tasks are used for
visualization. Based on the experiment results, we
provide the observations and discussion below:
(1) The original foundation model MMedLM2
fails to do the zero-shot evaluation on the unseen
tasks in the training process. This is due to its
extremely limited multimodal capabilities.
(2) FedPlug, which only incorporates the feder-
ated encoder, performs the worst across all tasks,
regardless of the type. This observation under-
scores the necessity of effectively utilizing public
data to align the medical foundation model with
external knowledge. Without proper integration,
external knowledge‚Äîalthough derived through
federated approaches on vast amounts of private
data‚Äîcannot be directly assimilated into the medi-
cal foundation model.
(3) Even though FedPlug Lapproaches FED-KIM ‚Äôs performance on several tasks, it still falls
short, particularly in generation tasks like Med-
VQA. This indicates that the knowledge injected
through FedPlug+LoRA does not fully generalize
to unseen tasks, as training was exclusively per-
formed on classification tasks. In contrast, FED-
KIM , despite also being trained on classification
tasks, achieves better performance on these tasks
and maintains superior capability in handling un-
seen classification tasks. Comparing our FED-
KIM with FedPlug L,FEDKIM shows the supe-
rior performance on all the tasks, especially on
the tasks of SNC ( ‚Üë82.36% with FedAvg), PED
(‚Üë43.92% with FedAvg), and AD ( ‚Üë48.12%). On
the other tasks, such as MR, EBD, and ECD, these
approaches reach closed performance. This success
is attributed to the M3OE module, which enables
FEDKIM to adaptively select appropriate experts
to jointly handle novel tasks based on the context.
Furthermore, our proposed FEDKIM works well
with the federated backbones of FedAvg and Fed-
Prox. It also generally maintains the advantages ofa more advanced federated learning method (Fed-
Prox) over the vanilla approach. Comparing Fig-
ure 2a and Figure 2b, the performance with Fed-
Prox generally outperforms the one with FedAvg
on different tasks, such as PED ‚Üë15.25%.
These observations further show the adaptability
ofFEDKIM to enable medical foundation models
to have zero-shot capability across different tasks
and federated learning frameworks.
4.2 Fine-tuning Evaluation
While injecting medical knowledge into founda-
tion models demonstrates the potential for handling
unseen tasks, it remains uncertain whether the en-
hanced foundation model can also perform well on
previously encountered tasks. To address this, we
conducted a fine-tuning evaluation, with the train-
ing process detailed in Section 3.1 considered as
fine-tuning for these tasks. The test sets for these
tasks were used for evaluation, and the fine-tuning
results are presented in Table 3. For a compre-
hensive evaluation, we utilize accuracy, precision,
recall, and F1 score as metrics for these tasks.
Compared to the experiments on unseen tasks,
it is evident that the knowledge-injected medical
foundation model performs significantly better on
familiar tasks. This showcases the explicit utiliza-
tion of knowledge acquired through federated train-
ing. Similar to the zero-shot evaluation, approaches
combined with FedProx consistently outperform
those with FedAvg, underscoring the importance of
effective knowledge extraction during the injection
process.
Furthermore, FEDKIM consistently outperforms
the two baselines, FedPlug and FedPlug L. This
competitive performance validates the design and
effectiveness of the M3OE module.
4.3 Ablation Study
We conduct ablation studies on the COVID-19 de-
tection and enlarged cardiomediastinum detection
tasks to assess the impact of each module within
our proposed FEDKIM in both fine-tuning and zero-
shot settings. Retaining all other modules as in the
main experiments, we explore the following vari-
ant settings: (1) FEDKIMpub: Instead of utilizing
knowledge from private datasets Dn, this configura-
tion solely leverages public dataset Dpfor central-
ized training. Consequently, the federated training
module discussed in Section 2.2 is excluded, with
the encoder Œ∏eupdated exclusively through public
training as detailed in Section 2.3. (2) FEDKIMT:This variant omits the task description module that
guides the expert selection process, testing the im-
portance of task-specific information in routing the
mixture of experts. (3) FEDKIMM: Similarly, we
remove the modality description module to exam-
ine its influence on expert selection.
The results of the ablation studies are presented
in Table 4 and Table 5. They indicate that each
component significantly enhances F EDKIM‚Äôs per-
formance. Specifically, a substantial decline in
performance with FEDKIMpubhighlights the cru-
cial role of knowledge injected from local clients
through federated learning. This locally enriched
encoder allows the medical foundation model to
better adjust to unseen modalities, thereby enhanc-
ing its effectiveness compared to models trained
without this knowledge. Moreover, the absence
of task or modality descriptions diminishes FED-
KIM ‚Äôs ability to manage specific tasks through
multi-task training, validating the design of the
M3OE module. This module equips FEDKIM to
effectively navigate complex healthcare scenarios
that involve diverse tasks and modalities. In sum-
mary, the synergistic integration of local knowl-
edge, along with the task and modality description
modules, crucially bolsters the performance of our
proposed F EDKIM.
5 Related Work
Medical Foundation Models. Foundation mod-
els, known for their vast parameters and training
datasets, have demonstrated impressive capabili-
ties across domains (Touvron et al., 2023; Zhou
et al., 2023a; Yang et al., 2024; Li et al., 2024a;
Abbasian et al., 2024), and are becoming increas-
ingly prevalent in healthcare. Thirunavukarasu et
al. (Thirunavukarasu et al., 2023) highlight the po-
tential of large language models (LLMs) in clini-
cal settings. Moor et al. (Moor et al., 2023) pro-
pose a generalist medical AI for diverse tasks us-
ing multimodal data. Specialized medical foun-
dation models have been developed for disease
detection (Zhou et al., 2023b), cancer biomarker
identification (Pai et al., 2024), echocardiogram
interpretation (Christensen et al., 2024), image seg-
mentation (Zhang et al., 2023a), and precision on-
cology (Truhn et al., 2024). Despite these advance-
ments, this area remains relatively unexplored com-
pared to the general domain (Wang et al., 2024a),
primarily due to the complexity inherent in health-
care data (Wang et al., 2023, 2024b).Table 3: Fine-tuning evaluation for training tasks. ‚úódenotes incapacity.
TaskMethod LLM FedAvg FedProx
Metric MMedLM-2 FedPlug FedPlug LFEDKIM FedPlug FedPlug LFEDKIM
Covid-19
DetectionAccuracy ‚úó 98.34 94.21 98.48 86.11 95.73 98.98
Precision ‚úó 96.07 99.27 96.61 65.09 99.32 98.28
Recall ‚úó 97.44 77.78 97.44 97.72 83.76 97.72
F1 ‚úó 96.75 87.22 97.02 78.13 90.88 98.00
Lung
Opacity
DetectionAccuracy ‚úó 95.48 85.45 94.99 93.13 78.64 95.10
Precision ‚úó 98.22 99.85 98.32 93.74 72.20 97.15
Recall ‚úó 92.95 74.03 91.90 92.95 95.58 93.27
F1 ‚úó 95.51 83.69 95.00 93.35 82.26 95.17
ECG
Abnormal
DetectionAccuracy ‚úó 43.15 42.28 44.75 45.25 50.94 58.46
Precision ‚úó 56.97 82.61 61.11 60.85 58.01 58.46
Recall ‚úó 11.22 1.49 13.80 17.80 58.20 100.00
F1 ‚úó 18.74 2.93 22.52 27.55 58.10 73.78
Mortality
PredictionAccuracy ‚úó 84.11 53.63 90.01 82.41 91.42 89.99
Precision ‚úó 16.35 10.96 35.88 13.87 47.57 36.97
Recall ‚úó 21.43 63.04 23.29 16.64 15.22 27.33
F1 ‚úó 18.55 18.67 28.24 15.13 23.06 31.43
Table 4: Ablation study results in the fine-tuning setting.
Setting Accuracy Precision Recall F1
FEDKIMpub94.07 90.63 85.47 87.98
FEDKIMT98.34 96.28 97.12 96.70
FEDKIMM98.41 97.13 96.58 96.86
FEDKIM 98.48 96.61 97.44 97.02
Table 5: Ablation study results in the zero-shot setting.
Setting Accuracy Precision Recall F1
FEDKIMpub59.82 54.43 84.40 66.19
FEDKIMT61.11 55.80 79.82 65.66
FEDKIMM60.26 66.00 30.28 41.51
FEDKIM 61.54 55.13 93.58 69.39
Federated Fine-tuning with Foundation Mod-
els. Fine-tuning foundation models (FMs) with
task-specific data is essential for improved perfor-
mance in specialized tasks. Federated Learning
(FL) supports this by utilizing locally stored data
and distributed computational resources. Research
in this field includes full tuning (Deng et al., 2023;
Fan et al., 2023), partial tuning (Peng et al., 2024;
Marchisio et al., 2022; Khalid et al., 2023), and
parameter-efficient fine-tuning (PEFT) (Lu et al.,
2023; Zhang et al., 2023b). Notably, (Lu et al.,
2023) involves clients hosting FMs and exchang-
ing adapters with the server, which aggregates and
redistributes them. Similarly, FedPETuning (Zhang
et al., 2023b) shares parts of client models for pre-
trained language models in FL. Unlike these stud-
ies, which require clients to have FMs, our ap-
proach positions the medical FM on the server,
facilitating collaborative enhancement of medical
FM models without accessing local data.
Parameter-efficient Fine-tuning on Foundation
Model Full-parameter Fine-Tuning of foundation
models, while promising in terms of performanceenhancement, requires extremely extensive com-
putational resources. Consequently, researchers
have investigated Parameter-efficient Fine-tuning
(PEFT) techniques. PEFT methods aim to adapt
pre-trained models to specific tasks using a mini-
mal number of additional parameters. Low-Rank
Adaptation (LoRA) (Hu et al., 2022), a widely
recognized PEFT method, reduces the number of
trainable parameters by factorizing weight matri-
ces into low-rank representations, achieving signif-
icant parameter efficiency. Additionally, previous
studies have utilized modular approaches, such as
adapters (Gao et al., 2023) and the Perceiver Re-
sampler (Alayrac et al., 2022), to adapt new modal-
ities to foundation models.
Researchers have explored combining the Mix-
ture of Experts (Jacobs et al., 1991) concept
with Low-Rank Adaptation (LoRA) for Parameter-
efficient Fine-tuning (PEFT) (Li et al., 2024b; Wu
et al., 2023). To guide the selection of experts in
complex scenarios, they have leveraged modality
information (Luo et al., 2024; Li et al., 2024c),
instructions (Chen et al., 2023, 2024b; Wu et al.,
2023; Li et al., 2024b), or pre-defined task IDs (Liu
et al., 2023b). However, these MOE methodologies
do not specifically address the complex, modality-
diverse scenarios found in the healthcare domain.
6 Conclusion
This work introduces the concept of knowledge in-
jection into medical foundation models, emphasiz-
ing its critical role and potential in the development
of comprehensive medical models. We propose
a novel approach, FEDKIM , designed to extract
and inject healthcare knowledge into foundationmodels, thereby enhancing their ability to handle
multiple tasks and modalities. FEDKIM leverages
flexible federated learning techniques to extract
knowledge from distributed medical data. The ex-
tracted knowledge is then injected into the foun-
dation model using our proposed adaptive M3OE
module. Our exhaustive experimental results on 12
tasks and 7 modalities demonstrate the effective-
ness of FEDKIM in diverse settings, showcasing its
excellent capability in handling either encountered
or unseen healthcare tasks. This study validates the
potential of injecting knowledge into foundation
models using federated learning, providing a cru-
cial solution for developing a healthcare foundation
model without accessing sensitive data.
7 Limitations
This work explores the problem of medical knowl-
edge injection within the PEFT framework. Due
to current computational limitations, we have not
yet combined Full-parameter Fine-Tuning with our
proposed FEDKIM . Additionally, our study utilizes
MMedLM2, which has 7 billion parameters, but
injecting knowledge into larger foundation models
is restricted by available computational resources.
In future research, we plan to investigate the inte-
gration of knowledge injection with Full-parameter
Fine-Tuning. We also aim to evaluate the efficacy
of our approach on larger medical foundation mod-
els to further validate its scalability and potential.
Acknowledgements
This work is partially supported by the National
Science Foundation under Grant No. 2238275 and
2348541 and the National Institutes of Health under
Grant No. R01AG077016.
References
Rsna pneumonia detection challenge (2018). https:
//www.rsna.org/rsnai/ai-image-challenge/
rsna-pneumonia-detection-challenge-2018 .
Accessed: 2024-06-05.
Mahyar Abbasian, Elahe Khatibi, Iman Azimi, David
Oniani, Zahra Shakeri Hossein Abad, Alexander
Thieme, Ram Sriram, Zhongqi Yang, Yanshan Wang,
Bryant Lin, et al. 2024. Foundation metrics for eval-
uating effectiveness of healthcare conversations pow-
ered by generative ai. NPJ Digital Medicine , 7(1):82.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, MalcolmReynolds, et al. 2022. Flamingo: a visual language
model for few-shot learning. Advances in neural
information processing systems , 35:23716‚Äì23736.
Liwei Che, Jiaqi Wang, Yao Zhou, and Fenglong Ma.
2023. Multimodal federated learning: A survey. Sen-
sors, 23(15):6986.
Haokun Chen, Yao Zhang, Denis Krompass, Jindong
Gu, and V olker Tresp. 2024a. Feddat: An approach
for foundation model finetuning in multi-modal het-
erogeneous federated learning. In Proceedings of
the AAAI Conference on Artificial Intelligence , vol-
ume 38, pages 11285‚Äì11293.
Shaoxiang Chen, Zequn Jie, and Lin Ma. 2024b. Llava-
mole: Sparse mixture of lora experts for mitigating
data conflicts in instruction finetuning mllms. arXiv
preprint arXiv:2401.16160 .
Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu,
Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang,
Yu Qiao, and Jing Shao. 2023. Octavius: Mitigating
task interference in mllms via moe. arXiv preprint
arXiv:2311.02684 .
Matthew Christensen, Milos Vukadinovic, Neal Yuan,
and David Ouyang. 2024. Vision‚Äìlanguage founda-
tion model for echocardiogram interpretation. Nature
Medicine , pages 1‚Äì8.
Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, and
Yaoxue Zhang. 2023. Mutual enhancement of large
and small language models with cross-silo knowl-
edge transfer. arXiv preprint arXiv:2312.05842 .
Ahmed El Ouadrhiri and Ahmed Abdelhadi. 2022. Dif-
ferential privacy for deep and federated learning: A
survey. IEEE access , 10:22359‚Äì22380.
Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wen-
bin Wei, Lixin Fan, and Qiang Yang. 2023. Fate-
llm: A industrial grade federated learning frame-
work for large language models. arXiv preprint
arXiv:2310.10049 .
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie
Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui
He, Xiangyu Yue, et al. 2023. Llama-adapter v2:
Parameter-efficient visual instruction model. arXiv
preprint arXiv:2304.15010 .
Yu Gu, Robert Tinn, Hao Cheng, Youzheng Ben,
Zhuozhao Liu, Jingqi Zhou, Michael Wang, Shizhuo
Wang, Hongfang Zhou, and Yanshan Shen. 2021.
Biomedgpt: A large-scale biomedical generative pre-
trained transformer for biomedical text mining. arXiv
preprint arXiv:2104.07497 .
Tianxiao Hao, Junyi Jessy Wang, Chengyu Liu, Hao-
ran Zhang, Junjie Hu, Haomin Deng, Longxiang
Ding, Yitao Si, Yue Gong, Xinyu Han, et al. 2020.
Gatortron: A large clinical language model to un-
lock patient information from unstructured electronic
health records. arXiv preprint arXiv:2010.16114 .Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
et al. 2022. Lora: Low-rank adaptation of large lan-
guage models. In International Conference on Learn-
ing Representations .
Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and
Yanmin Gong. 2020. Personalized federated learning
with differential privacy. IEEE Internet of Things
Journal , 7(10):9530‚Äì9539.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu,
Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,
Behzad Haghgoo, Robyn Ball, Katie Shpanskaya,
et al. 2019. Chexpert: A large chest radiograph
dataset with uncertainty labels and expert comparison.
InProceedings of the AAAI conference on artificial
intelligence , volume 33, pages 590‚Äì597.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan,
and Geoffrey E Hinton. 1991. Adaptive mixtures of
local experts. Neural computation , 3(1):79‚Äì87.
Yihan Jiang, Jakub Kone Àácn`y, Keith Rush, and Sreeram
Kannan. 2019. Improving federated learning person-
alization via model agnostic meta learning. arXiv
preprint arXiv:1909.12488 .
Alistair EW Johnson, Tom J Pollard, Lu Shen, Li-wei H
Lehman, Mengling Feng, Mohammad Ghassemi,
Benjamin Moody, Peter Szolovits, Leo Anthony Celi,
and Roger G Mark. 2016. Mimic-iii, a freely accessi-
ble critical care database. Scientific data , 3(1):1‚Äì9.
Umar Khalid, Hasan Iqbal, Saeed Vahidian, Jing Hua,
and Chen Chen. 2023. Cefhri: A communication
efficient federated learning framework for recog-
nizing industrial human-robot interaction. In 2023
IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS) , pages 10141‚Äì10148.
IEEE.
Jason J Lau, Soumya Gayen, Asma Ben Abacha, and
Dina Demner-Fushman. 2018. A dataset of clini-
cally generated visual questions and answers about
radiology images. Scientific data , 5(1):1‚Äì10.
Yann LeCun, L√©on Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE ,
86(11):2278‚Äì2324.
Jinhyuk Lee, Wonjin Yoon, Donghyeon Kim,
Youngjoong Jo, and Jaewoo Kang. 2023. Pmc-llama:
Large language models for biomedical literature.
arXiv preprint arXiv:2304.07930 .
Michael Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Ves Stoyanov, and Luke Zettlemoyer. 2020.
Biomedlm: Large-scale biomedical language model.
arXiv preprint arXiv:2004.03982 .
Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei
Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al.2024a. Multimodal foundation models: From spe-
cialists to general-purpose assistants. Foundations
and Trends ¬Æin Computer Graphics and Vision , 16(1-
2):1‚Äì214.
Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan
Cheng, Lei Duan, Jie Zuo, Cal Yang, and Mingjie
Tang. 2024b. Mixlora: Enhancing large language
models fine-tuning with lora based mixture of experts.
arXiv preprint arXiv:2404.15159 .
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Vir-
ginia Smith. 2020. Fedprox: A federated learning
optimization algorithm. In Proceedings of the 2nd
MLSys Conference .
Xiang Li, Yifan Wang, Xiaoman Zhang, Haoran Lin,
Junyi Liu, Xiaoxuan Jiang, Weixiong Lin, and Yang
Zhang. 2023. Chatdoctor: A doctor-patient dialogue
system. arXiv preprint arXiv:2301.01285 .
Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue
Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, and
Min Zhang. 2024c. Uni-moe: Scaling unified multi-
modal llms with mixture of experts. arXiv preprint
arXiv:2405.11273 .
Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and
Xiao-Ming Wu. 2021. Slake: A semantically-labeled
knowledge-enhanced dataset for medical visual ques-
tion answering. In 2021 IEEE 18th International
Symposium on Biomedical Imaging (ISBI) , pages
1650‚Äì1654. IEEE.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2023a. Visual instruction tuning. arXiv preprint
arXiv:2304.08485 .
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae
Lee. 2024. Visual instruction tuning. Advances in
neural information processing systems , 36.
Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu,
Derong Xu, Feng Tian, and Yefeng Zheng. 2023b.
Moelora: An moe-based parameter efficient fine-
tuning method for multi-task medical applications.
arXiv preprint arXiv:2310.18339 .
Wang Lu, Xixu Hu, Jindong Wang, and Xing Xie.
2023. Fedclip: Fast generalization and personal-
ization for clip in federated learning. arXiv preprint
arXiv:2302.13485 .
Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xi-
aoshuai Sun, and Rongrong Ji. 2024. Cheap and
quick: Efficient vision-language instruction tuning
for large language models. Advances in Neural Infor-
mation Processing Systems , 36.
Kelly Marchisio, Patrick Lewis, Yihong Chen, and
Mikel Artetxe. 2022. Mini-model adaptation: Ef-
ficiently extending pretrained models to new lan-
guages via aligned shallow training. arXiv preprint
arXiv:2212.10503 .Brendan McMahan, Eider Moore, Daniel Ramage,
Seth Hampson, and Blaise Aguera y Arcas. 2017.
Communication-efficient learning of deep networks
from decentralized data. In Artificial intelligence and
statistics , pages 1273‚Äì1282. PMLR.
Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein
Abad, Harlan M Krumholz, Jure Leskovec, Eric J
Topol, and Pranav Rajpurkar. 2023. Foundation mod-
els for generalist medical artificial intelligence. Na-
ture, 616(7956):259‚Äì265.
Jungwoo Oh, Gyubok Lee, Seongsu Bae, Joon-myoung
Kwon, and Edward Choi. 2024. Ecg-qa: A compre-
hensive question answering dataset combined with
electrocardiogram. Advances in Neural Information
Processing Systems , 36.
Suraj Pai, Dennis Bontempi, Ibrahim Hadzic, Vasco Pru-
dente, Mateo Soka Àác, Tafadzwa L Chaunzwa, Simon
Bernatz, Ahmed Hosny, Raymond H Mak, Nicolai J
Birkbak, et al. 2024. Foundation model for cancer
imaging biomarkers. Nature machine intelligence ,
pages 1‚Äì14.
Zhaopeng Peng, Xiaoliang Fan, Yufan Chen, Zheng
Wang, Shirui Pan, Chenglu Wen, Ruisheng Zhang,
and Cheng Wang. 2024. Fedpft: Federated proxy
fine-tuning of foundation models. arXiv preprint
arXiv:2404.11536 .
Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weix-
iong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang,
and Weidi Xie. 2024. Towards building multilin-
gual language model for medicine. arXiv preprint
arXiv:2402.13963 .
Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey,
Anas Tahir, Serkan Kiranyaz, Saad Bin Abul Kashem,
Mohammad Tariqul Islam, Somaya Al Maadeed,
Susu M Zughaier, Muhammad Salman Khan, et al.
2021. Exploring the effect of image enhancement
techniques on covid-19 detection using chest x-
ray images. Computers in biology and medicine ,
132:104319.
Sashank J Reddi, Zachary Charles, Manzil Zaheer,
Zachary Garrett, Keith Rush, Jakub Kone Àácn`y, Sanjiv
Kumar, and Hugh Brendan McMahan. 2020. Adap-
tive federated optimization. In International Confer-
ence on Learning Representations .
Karan Singhal, Shekoofeh Azizi, Tong N Tu, Soroush S
Mahdavi, Jason T Wei, Hyung Won Chung, Nathan
Scales, Aneesh Tanwani, Heather Cole-Lewis,
Stephen R Pfohl, et al. 2022. Large language
models encode clinical knowledge. arXiv preprint
arXiv:2212.13138 .
Canh T Dinh, Nguyen Tran, and Josh Nguyen. 2020.
Personalized federated learning with moreau en-
velopes. Advances in Neural Information Processing
Systems , 33:21394‚Äì21405.
Arun James Thirunavukarasu, Darren Shu Jeng Ting,
Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan,and Daniel Shu Wei Ting. 2023. Large language
models in medicine. Nature medicine , 29(8):1930‚Äì
1940.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Fran-
cisco Massa, Alexandre Sablayrolles, and Herv√© J√©-
gou. 2021. Training data-efficient image transform-
ers & distillation through attention. In International
conference on machine learning , pages 10347‚Äì10357.
PMLR.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Daniel Truhn, Jan-Niklas Eckardt, Dyke Ferber, and
Jakob Nikolas Kather. 2024. Large language mod-
els and multimodal foundation models for precision
oncology. NPJ Precision Oncology , 8(1):72.
Robin van de Water, Hendrik Nils Aurel Schmidt, Paul
Elbers, Patrick Thoral, Bert Arnrich, and Patrick
Rockenschaub. 2024. Yet another icu benchmark:
A flexible multi-center framework for clinical ml. In
The Twelfth International Conference on Learning
Representations .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing
systems , 30.
Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot,
Dieter Kreiseler, Fatima I Lunze, Wojciech Samek,
and Tobias Schaeffter. 2020. Ptb-xl, a large publicly
available electrocardiography dataset. Scientific data ,
7(1):1‚Äì15.
Jiaqi Wang, Junyu Luo, Muchao Ye, Xiaochen Wang,
Yuan Zhong, Aofei Chang, Guanjie Huang, Ziyi Yin,
Cao Xiao, Jimeng Sun, and Fenglong Ma. 2024a. Re-
cent advances in predictive modeling with electronic
health records. In Proceedings of the Thirty-Third
International Joint Conference on Artificial Intelli-
gence , pages 8272‚Äì8280.
Jiaqi Wang, Cheng Qian, Suhan Cui, Lucas Glass, and
Fenglong Ma. 2022a. Towards federated covid-19
vaccine side effect prediction. In Joint European
Conference on Machine Learning and Knowledge
Discovery in Databases , pages 437‚Äì452. Springer.
Xiaochen Wang, Junyu Luo, Jiaqi Wang, Ziyi Yin,
Suhan Cui, Yuan Zhong, Yaqing Wang, and Fenglong
Ma. 2023. Hierarchical pretraining on multimodal
electronic health records. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing. Conference on Empirical Methods in Nat-
ural Language Processing , volume 2023, page 2839.
NIH Public Access.Xiaochen Wang, Junyu Luo, Jiaqi Wang, Yuan Zhong,
Xiaokun Zhang, Yaqing Wang, Parminder Bhatia,
Cao Xiao, and Fenglong Ma. 2024b. Unity in diver-
sity: Collaborative pre-training across multimodal
medical sources. In Proceedings of the 62nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 3644‚Äì3656.
Yujia Wang, Lu Lin, and Jinghui Chen. 2022b.
Communication-compressed adaptive gradient
method for distributed nonconvex optimization. In
International Conference on Artificial Intelligence
and Statistics , pages 6292‚Äì6320. PMLR.
Yujia Wang, Lu Lin, and Jinghui Chen. 2022c.
Communication-efficient adaptive federated learning.
InInternational Conference on Machine Learning ,
pages 22802‚Äì22838. PMLR.
Xun Wu, Shaohan Huang, and Furu Wei. 2023. Mole:
Mixture of lora experts. In The Twelfth International
Conference on Learning Representations .
Jianlin Yang, Yan Zhang, Li Li, Haifeng Wang,
Tianyu Liu, and Hua Wu. 2023. Med-flamingo: a
multimodal medical foundation model. Preprint ,
arXiv:2304.08100.
Weikai Yang, Mengchen Liu, Zheng Wang, and Shixia
Liu. 2024. Foundation models meet visualizations:
Challenges and opportunities. Computational Visual
Media , pages 1‚Äì26.
Yizhe Zhang, Tao Zhou, Shuo Wang, Peixian Liang,
Yejia Zhang, and Danny Z Chen. 2023a. Input aug-
mentation with sam: Boosting medical image seg-
mentation with segmentation foundation model. In
International Conference on Medical Image Com-
puting and Computer-Assisted Intervention , pages
129‚Äì139. Springer.
Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang,
Yue Yu, Lizhen Qu, and Zenglin Xu. 2023b.
Fedpetuning: When federated learning meets the
parameter-efficient tuning methods of pre-trained
language models. In Annual Meeting of the Asso-
ciation of Computational Linguistics 2023 , pages
9963‚Äì9977. Association for Computational Linguis-
tics (ACL).
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, et al. 2023. A
survey of large language models. arXiv preprint
arXiv:2303.18223 .
Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,
Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,
Lifang He, et al. 2023a. A comprehensive survey on
pretrained foundation models: A history from bert to
chatgpt. arXiv preprint arXiv:2302.09419 .
Yao Zhou, Jun Wu, Haixun Wang, and Jingrui He. 2022.
Adversarial robustness through bias variance decom-
position: A new perspective for federated learning.InProceedings of the 31st ACM International Con-
ference on Information & Knowledge Management,
Atlanta, GA, USA, October 17-21, 2022 , pages 2753‚Äì
2762. ACM.
Yukun Zhou, Mark A Chia, Siegfried K Wagner, Mu-
rat S Ayhan, Dominic J Williamson, Robbert R
Struyven, Timing Liu, Moucheng Xu, Mateo G
Lozano, Peter Woodward-Court, et al. 2023b. A
foundation model for generalizable disease detection
from retinal images. Nature , 622(7981):156‚Äì163.
A Details of Training Tasks
COVID-19 Detection (CD) involves identifying
COVID-19 symptoms from X-ray images using the
COVQU dataset (Rahman et al., 2021) to evaluate
the model‚Äôs ability to interpret medical images.
Lung Opacity Detection (LOD) uses chest X-
ray images to classify lung opacity based on data
from the RSNA Pneumonia Detection Challenge
2018 (rsn), annotated by medical practitioners.
ECG Abnormal Detection (EAD) is an unimodal
binary classification task that determines abnormal
patterns in 10-second, 12-lead ECG signals from
PTB-XL database (Wagner et al., 2020).
Mortality Prediction (MP) predicts ICU patient
survival or death using multimodal dynamic fea-
tures vital signs, lab tests, input and output, with
data sourced from MIMIC-III (Johnson et al.,
2016).
B Details of Validation Tasks
Enlarged Cardiomediastinum Detection
(ECD) (Irvin et al., 2019) aims to assess the
presence of an enlarged cardiomediastinum using
medical images from clinical evaluations. This
task measures the model‚Äôs capability to interpret
radiographic data effectively.
Sepsis Prediction (SP) aims to forecast the like-
lihood of sepsis during ICU stays, testing the
model‚Äôs ability to understand various clinical fea-
tures. These features are identical to those used
in the mortality prediction task, extracted from
the MIMIC-III database through the preprocess-
ing pipeline (van de Water et al., 2024).
Medical Visual Question Answering on RAD
(MR) involves using both visual images and textual
questions as inputs to generate answers. This task
evaluates the model‚Äôs ability to align text and image
modalities within the medical domain. The VQA-
RAD dataset is utilized for this task (Lau et al.,
2018).Table 6: Details about the datasets.
Task Type Task Total SamplesPrivate
ClientsPublic
(Server)Development Testing
Training
TasksLung Opacity Detection 18,406 12,880 1,849 1,841 1,836
COVID-19 Detection 13,808 9,665 1,380 1,380 1,383
ECG Abnormal Detection 21,797 15,259 2,179 2,180 2,179
Mortality Prediction 38,129 26,690 3,812 3,812 3,813
ValidationEnlarged Cardiomediastinum Detection 234 ‚úó ‚úó ‚úó 234
Pleural Effusion Detection 234 ‚úó ‚úó ‚úó 234
Atelectasis Detection 234 ‚úó ‚úó ‚úó 234
Sepsis Prediction 1,000 ‚úó ‚úó ‚úó 1,000
MedVQA-RAD 1,000 ‚úó ‚úó ‚úó 1,000
MedVQA-Slake 1,000 ‚úó ‚úó ‚úó 1,000
Signal Noise Clarification 1,000 ‚úó ‚úó ‚úó 1,000
Ectopic Beats Detection 2,000 ‚úó ‚úó ‚úó 2,000
Table 7: Single task fine-tuning evaluation.
TaskMethod LLM FedAvg FedProx
Metric MMedLM-2 FedPlug FedPlug LFEDKIM FedPlug FedPlug LFEDKIM
COVID-19
DetectionAccuracy ‚úó 92.34 99.56 99.57 84.16 95.66 98.91
Precision ‚úó 93.59 98.87 99.15 77.27 84.18 100.00
Recall ‚úó 74.92 99.43 99.15 53.27 98.68 95.73
F1 ‚úó 79.15 99.14 99.15 63.07 90.85 97.82
Lung
Opacity
DetectionAccuracy ‚úó 89.42 51.69 94.93 91.23 90.90 93.63
Precision ‚úó 84.69 51.74 93.60 87.76 88.06 92.55
Recall ‚úó 97.16 99.79 96.85 96.52 95.37 95.37
F1 ‚úó 90.50 68.10 95.19 91.93 91.57 93.94
ECG
Abnormal
DetectionAccuracy ‚úó 43.15 48.79 58.46 45.25 50.80 58.00
Precision ‚úó 56.97 65.02 58.46 60.85 58.47 58.34
Recall ‚úó 11.22 26.82 100.00 17.80 54.67 98.51
F1 ‚úó 18.74 37.98 73.78 27.55 56.51 73.28
Mortality
PredictionAccuracy ‚úó 84.11 91.67 64.16 82.41 90.98 57.38
Precision ‚úó 16.35 43.24 12.86 13.87 40.68 13.25
Recall ‚úó 21.43 14.91 56.21 16.64 14.91 72.98
F1 ‚úó 18.55 22.18 20.94 15.13 21.82 22.42
Signal Noise Clarification (SNC) is a generative
task that focuses on accurately describing noise in
ECG signals based on corresponding textual ques-
tions. The data is extracted from an existing ECG
question-answering dataset (Oh et al., 2024). The
signals are recorded in 12 channels and last for 10
seconds, similar to the ECG Abnormal Detection
task.
Pleural Effusion Detection (PED) (Irvin et al.,
2019) is derived from the CheXpert dataset and in-
volves using X-ray images to identify the presence
of pleural effusion, testing the model‚Äôs ability to
interpret radiographic data.
Atelectasis Detection (AD) (Irvin et al., 2019) also
uses X-ray images from the CheXpert dataset to
detect atelectasis, evaluating the model‚Äôs capability
in analyzing medical images.Medical Visual Question Answering on Slake
(MS) (Liu et al., 2021) utilizes both visual images
and textual questions from the SLAKE dataset to
generate answers, assessing the model‚Äôs proficiency
in aligning text and image modalities in the medical
domain.
Ectopic Beats Detection (EBD) aims to identify
ectopic beats in ECG signals sourced from the PTB-
XL database (Wagner et al., 2020).
C Dataset Details
For tasks involved in training, we adopt the data
partition setup detailed in Section 3.2. For tasks
utilized in zero-shot evaluation, we select a subset
of corresponding datasets to facilitate the inference
efficiency. We cover 1,000 randomly sampled sam-
ples for the tasks of Sepsis Prediction, MedVQA-Slake, MedVQA-RAD, Signal Noise Clarification.
For Ectopic Beats Detection, we cover 1,000 posi-
tive cases and 1,000 negative cases randomly sam-
pled from the dataset, as the original annotations
concerning ectopic beats are highly sparse. For
Enlarged Cardiomediastinum Detection, Pleural
Effusion Detection and Atelectasis Detection, we
leverage an existing validation set, which involves
234 samples. Statistics about the datasets leveraged
in this study are available in Table 6.
D Modality Encoding
We list all encoders along with corresponding
modalities in Table 8. Note that all these encoders
can be flexibly replaced with other qualified en-
coders under the framework of F EDKIM.
Table 8: Details of modality-specific encoders.
Modality Encoder
Image Deit-tiny (Touvron et al., 2021)
Signal CNN (LeCun et al., 1998)
Vital Sign Transformer (Vaswani et al., 2017)
Lab Results Transformer (Vaswani et al., 2017)
Input Transformer (Vaswani et al., 2017)
Output Transformer (Vaswani et al., 2017)
E Single-task Fine-tuning Evaluation
In addition to the evaluation discussed in Sec-
tion 4, another auxiliary topic worth investigat-
ing is whether FEDKIM can enhance a model‚Äôs
performance on a single task. This is particularly
meaningful for practitioners who aim to address
a specific task, given the scarcity and specializa-
tion often associated with medical data. Therefore,
we designed a single-task fine-tuning evaluation,
where FEDKIM is applied to the foundation model
for each individual training task. The experimental
results are presented in Table 7.
The results verify that FEDKIM , benefiting from
a well-designed knowledge injection strategy, out-
performs both baselines in most tasks. This explo-
ration demonstrates the applicability of FEDKIM
even when tasks for injection are limited, thereby
broadening its application scope in complex medi-
cal scenarios.