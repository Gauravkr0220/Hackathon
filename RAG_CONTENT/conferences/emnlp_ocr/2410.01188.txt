Gold Panning in Vocabulary: An Adaptive Method for Vocabulary
Expansion of Domain-Specific LLMs
Chengyuan Liu1,2âˆ—, Shihang Wang2, Lizhi Qing2, Kun Kuang1â€ ,
Yangyang Kang3,1,2,Changlong Sun2,Fei Wu1
{liucy1,kunkuang,yangyangkang,wufei}@zju.edu.cn,
{wangshihang.wsh,yekai.qlz}@alibaba-inc.com, changlong.scl@taobao.com
1College of Computer Science and Technology, Zhejiang University,
2Tongyi Lab, Alibaba Group,
3Polytechnic Institute, Zhejiang University
Abstract
While Large Language Models (LLMs) demon-
strate impressive generation abilities, they fre-
quently struggle when it comes to specialized
domains due to their limited domain-specific
knowledge. Studies on domain-specific LLMs
resort to expanding the vocabulary before fine-
tuning on domain-specific corpus, aiming to
decrease the sequence length and enhance effi-
ciency during decoding, without thoroughly
investigating the results of vocabulary expan-
sion to LLMs over different domains . Our
pilot study reveals that expansion with only a
subset of the entire vocabulary may lead to su-
perior performance. Guided by the discovery,
this paper explores how to identify a vocab-
ulary subset to achieve the optimal results.
We introduce VEGAD, an adaptive method that
automatically identifies valuable words from a
given domain vocabulary. Our method has been
validated through experiments on three Chinese
datasets, demonstrating its effectiveness. Ad-
ditionally, we have undertaken comprehensive
analyses of the method. The selection of a opti-
mal subset for expansion has shown to enhance
performance on both domain-specific tasks and
general tasks, showcasing the potential of VE-
GAD.
1 Introduction
Despite achieving satisfactory performance on a
wide range of tasks (OpenAI et al., 2024; Touvron
et al., 2023a; Xu et al., 2023; Yuan and Zhu, 2023),
Large Language Models (LLMs) continue to en-
counter challenges, particularly in domain-specific
tasks, such as the generation of legal, medical,
and financial texts. The expansion of vocabulary
(Provilkov et al., 2020; Liu et al., 2021; Ozdemir
and Goksel, 2019; Rothe et al., 2020) serves as
a strategy to enhance the decoding efficiency for
domain-specific LLMs. By concatenating specific,
*This work was done when Chengyuan Liu interned at
Alibaba.
â€ Corresponding author.
-8.00-3.002.007.0012.0017.00
028522427196Improved (%)
Vo c a b u l a r y  S i z eDomainALPACAGSM8KAVGFigure 1: Pilot study: Relative improvement comparing
with direct supervised fine-tuning, by adding vocabulary
with different sizes.
frequent n-grams into new words, the token se-
quence is shortened, thereby visibly boosting effi-
ciency. Cui et al. (2024) extended LLaMAâ€™s exist-
ing vocabulary with an additional 20,000 Chinese
tokens, thereby improving its encoding efficiency
and semantic understanding of Chinese. LawGPT1
is fine-tuned based on the general Chinese LLMs
(such as Chinese-LLaMa, ChatGLM (Du et al.,
2022), etc.), the legal domain specific vocabulary
is expanded to enhance the semantic understanding
ability of the LLMs.
Current researches primarily focus on some spe-
cific domains. Nonetheless, they have not thor-
oughly elucidate the performance enhancements
resulting from vocabulary expansion in various do-
mains. We conduct a pilot study illustrating the
domain performance and general capabilities after
vocabulary expansion with different sizes, and the
results are illustrated in Figure 1. It is revealed
thataugmenting the size of the newly added vo-
cabulary does not invariably result in improved
1https://github.com/pengxiao-song/LaWGPTarXiv:2410.01188v1  [cs.CL]  2 Oct 2024â‘ Text Segmentationâ‘£Filter by Gradientsâ‘¤Resize Embedding and LM Head layerâ‘¥Domain SFTGeneral LLMDomain-Specific LLM
â‘¢Calculating Gradients
Domain-Specific DataCandidate VocabularyTokens to Addé™å‹è¡€ç³–æŸ“ä¸Šâ€¦â€¦
Trieé™å‹è¡€ç³–â€¦â€¦â‘¡Build TrieFigure 2: Framework of VEGAD.
model performance . Hence, an essential ques-
tion arises regarding the generation of an optimal
subset for vocabulary expansion given a candi-
date vocabulary. The process of selecting high-
value vocabulary during the expansion of domain-
specific LLMs is akin to gold panning , as it requires
careful selection rather than indiscriminate enlarge-
ment of the lexicon to enhance the performance of
the LLMs. We recognize the following challenges
for vocabulary subset generation:
â€¢How to ensure an optimal performance over
the whole vocabulary?
â€¢How to automatically adapt to any domain?
To effectively identify the crutial words from
a candidate vocabulary, we have proposed VE-
GAD (abbreviation of â€œ Vocabulary Expansion via
GrADientsâ€), which is an adaptable vocabulary ex-
pansion method via gradients. Figure 2 provides
an illustration of the framework. Intuitively, token
groups displaying larger gradients in domain in-
stances are deemed more pivotal to the task and
should be integrated into the vocabulary as domain-
specific terms. Therefore, it is a straightforward
approach to trace the gradient of each word, while
there are several difficulties, such as the algorithm
to efficiently retrieve the candidate words from
the token sequences, and the gradient calculation
across various tokens rather than the whole se-
quence. To identify candidate words from the token
sequences, we build a Trie (Black, 2019) based on
the candidate vocabulary, and design an algorithm
to record the gradient for each word with the Trie.
To distinguish the effect of each token, the gradient
is calculated on the running tensors, instead of the
weights of the LLMs.
To scrutinize the efficacy of VEGAD, we have
undertaken comprehensive studies. The findings
across three Chinese datasets, pertaining to the
domains of law and medicine, underscore a su-
periority in comparison to other lexicon genera-tion techniques, as well as the promising prospects
of domain-specific vocabulary expansion. Our in-
quiry reveals that the domain-specific lexicon by
VEGAD enhances performance in tasks requiring
specialized knowledge as well as tasks demanding
general skills. We hope that our multi-perspective
analysis serves as a catalyst for future investiga-
tions into enhancing domain-task performance and
mitigating the Catastrophic Forgetting through do-
main vocabulary adaptation.
In summary, our contributions are three folds:
â€¢It is revealed by our pilot study that vocabu-
lary expansion with only a subset of the entire
supplementary domain vocabulary may lead
to superior performance over using the whole
vocabulary.
â€¢Guided by our discovery, we introduce VE-
GAD, an automatic method to effectively iden-
tify an optimal subset for vocabulary expan-
sion, adaptable to various domains.
â€¢Extensive experiments and analyses have been
performed, during which VEGAD displays
outstanding proficiency surpassing other vo-
cabulary expansion methods.
2 Related Work
Large Language Models, such as ChatGPT2, GPT-
4 (OpenAI et al., 2024), exhibit amazing abilities
on understanding and text generation. They can
handle the tasks of QA, reasoning and math cal-
culation even under zero-shot scenarios. LLaMa
(Touvron et al., 2023a) is a collection of open foun-
dation language models ranging from 7B to 65B
parameters. Touvron et al. (2023b) developed and
released Llama 2, a collection of LLMs ranging in
scale from 7B to 70B parameters. The fine-tuned
Llama 2-Chat, are optimized for dialogue use cases.
There are other popular LLMs developed with vari-
ous skills (RoziÃ¨re et al., 2024; Almazrouei et al.,
2023; Jiang et al., 2023; Bai et al., 2023; Baichuan,
2023).
Due to the lack of domain-specific knowledge,
general LLMs fall short at handling domain ques-
tions. Therefore domain-specific LLMs are devel-
oped by fine-tuning on domain corpus. (Xiong
et al., 2023) collected databases of medical di-
alogues with the help of ChatGPT and adopted
2https://chat.openai.com/Transformer Blockså£æœé™åŒ»é™¢åšèƒƒæ£€æŸ¥â€¦[CLS]å‹è¯é•œâ€¦ğ‘‰!ğ‘‰!ğ‘‰"ğ‘‰#ğ‘‰$ğ‘‰%ğ‘‰!ğ‘‰&ğ‘‰'ğ‘‰!
å£æœé™åŒ»é™¢åšèƒƒæ£€æŸ¥â€¦å‹è¯é•œğ‘‰!ğ‘‰"ğ‘‰#ğ‘‰$ğ‘‰%ğ‘‰!ğ‘‰&ğ‘‰'ğ‘‰!
â€¦ğ‘‰![SEP]Rootğ‘‰!é™ğ‘‰"èƒƒğ‘‰#åœ¨ğ‘‰$é™å‹ğ‘‰%é™å‹è¯ğ‘‰&èƒƒé•œğ‘‰'èƒƒèƒ€ğ‘‰(to lower blood pressureantihypertensive drugsstomachstomach bloatinggastroscopyText SegmentationBuild Trie
åœ¨åŒ»é™¢ğ‘‰)
inhospital
åœ¨
åœ¨ğ‘‰(
ğ‘‰(é™å‹è¯é™å‹èƒƒé•œåœ¨åŒ»é™¢Gradient++
pseudo-leaf nodeEmbedding layerLM Head layerFigure 3: Gradient Calculation for each candidate word. Given the Trie built from candidate vocabulary, we check
whether there exists a sub-sequence of the input and output on the path from the root of the Trie to a leaf node, by a
pointer. The trace of the pointer is illustrated by Viand the â€œpseudo-leaf nodeâ€. Finally, the top Kwords with the
largest gradients are selected to construct the new vocabulary, and used to resize the embedding layer and language
modeling head layer.
several techniques to train an easy-deploy LLM,
called DoctorGLM. Wang et al. (2023a) pro-
posed HuaTuo, a LLaMA-based model that has
been supervised-fine-tuned with generated QA
(Question-Answer) instances in biomedical domain
tasks, with medical expertise in the responses. Cui
et al. (2023) proposed an open-source legal LLM
named ChatLaw, with a method that combines vec-
tor database retrieval with keyword retrieval to ef-
fectively reduce the inaccuracy of relying solely
on vector database retrieval, and a self-attention
method to enhance the ability to overcome errors
present in reference data. There are other domains
studied including finance (Wang et al., 2023b; Yu,
2023), education (Yu et al., 2023a), science (Li
et al., 2023b) and e-commerce (Li et al., 2023a).
Several previous studies adopt a strategy, vocab-
ulary expansion, to improve the performance of
domain SFT. Specifically, a domain-specific vo-
cabulary is automatically generated or manually
designed, and added into the tokenizer. In or-
der to augment LLaMA with capabilities for un-
derstanding and generating Chinese text and its
ability to follow instructions, Cui et al. (2024) ex-
tended LLaMAâ€™s existing vocabulary with an addi-
tional 20,000 Chinese tokens, thereby improving
its encoding efficiency and semantic understand-
ing of Chinese. Liu et al. (2023) proposed task-
adaptive tokenization as a way to adapt the genera-
tion pipeline to the specifics of a downstream task
and enhance long-form generation in mental health.
However, their task-adaptive tokenizer samples
variable segmentations from multiple outcomes,
which may change the vanilla behavious of othertokenizers (e.g., WordPiece and BPE). LaWGPT
expands the legal domain specific vocabulary and
large-scale Chinese legal corpus pre-training on the
basis of the general Chinese base model (such as
Chinese-LLama, ChatGLM, etc.), and enhances the
basic semantic understanding ability of the LLM
in the legal field. Tongyi-Finance-14B3expanded
the vocabulary of financial domain in Qwen-14B,
and the size of the vocabulary is 150,000. Based
on the BPE vocabulary used in GPT-4, the vocabu-
lary is optimized for Chinese and multi-language.
The numbers are divided into individual digits. Liu
et al. (2024b) identified tokens that are absent in
the general-purpose tokenizer and are rarely found
in general-purpose datasets, from the vocabulary
of the new tokenizer. They initialize model embed-
dings of the new tokens by utilizing the general-
purpose tokenizer. Liu et al. (2021) introduced two
new approaches based on attention to initialize the
weights of new added words.
3 Method
In this Section, we introduce VEGAD, a vocab-
ulary expansion method via gradient for domain-
specific LLMs. The process is shown in Figure
3.
Our approach is inspired by an naive intuition: n-
gram tokens exhibiting larger gradients in response
to domain-specific instances are deemed crucial
for the task at hand, and therefore, warrant inclu-
sion in the lexicon as domain-specific terminology.
Nonetheless, there are several challenges. For ex-
3https://modelscope.cn/models/TongyiFinance/Tongyi-
Finance-14BAlgorithm 1 Build Trie
Require: W1,W2,Â·Â·Â·,Wn, n, V 0
1:rootâ†V0
2:Mâ†1
3:fori= 1â†’Ndo
4:pâ†root
5: fortj
iâˆˆ W ido
6: ifphas child tj
ithen
7: pâ†GetChild (p, tj
i)
8: else
9: VMâ†CreateChild (p, tj
i)
10: pâ†VM
11: Mâ†M+ 1
12: end if
13: end for
14: setpas pseudo-leaf node
15:end for
ample, the algorithm to efficiently retrieve the can-
didate words from the token sequences, and the
gradient calculation across various tokens rather
than the whole sequence.
Specifically, starting from the domain-specific
data, sentences are divided into discrete words. The
candidate vocabulary is constructed with words ab-
sent from the general lexicon. Subsequently, the
process of selection is executed on domain-specific
instances by computing the gradients for each node
within the embedding tensor and the language mod-
eling tensor, with reference to a Trie constructed
based on the candidate vocabulary. The top K
words exhibiting the highest overall gradients are
retained to establish the specialized domain vocab-
ulary. Then we resize the LLM and incorporate
the tokenizer with new vocabulary, following an
optional weight initialization. Then we conduct
domain SFT on the LLM, to develop the domain-
specific LLM.
The advantage of VEGAD can be summarized
as following: 1)VEGAD is a plug-and-play task-
adaptive vocabulary selection method, seamlessly
integrating with diverse techniques utilized in su-
pervised fine-tuning. 2)In contrast to previous
methods such as Liu et al. (2023), which might
alter the intrinsic behaviors of current tokenizers
such as WordPiece and BPE by imposing an oblig-
atory scoring mechanism for sampling in accor-
dance with their guidelines, VEGAD is tokenizer-
agnostic, and compatible to any tokenization algo-
rithms. 3)The pipeline is automatically performed,without the need of manual design or intervention.
Of course, it still allows additional edition to the
vocabulary if required.
3.1 Build Trie
The Trie, as discussed by Black (2019), represents
a distinct tree-based data structure, extensively em-
ployed within the realm of computer science for
the administration of dynamic sets or associative
arrays, with the keys predominantly being strings.
Diverging from the structure of a binary search tree
in which a nodeâ€™s placement is influenced by nu-
merical or logical hierarchy, in a Trie, the location
of a node is unequivocally defined by the sequence
of characters it denotes. We illustrate an example
of Trie in the left part of Figure 3.
Formally, the domain-specific dataset can be
represented as D={(X1, Y1),Â·Â·Â·,(Xn, Yn)},
where XandYare the query and response respec-
tively, nis the size of D. Given a text segmentation
tool, the candidate vocabulary is constructed fol-
lowing
V= (n[
i=1Segment (Xi))âˆª(n[
i=1Segment (Yi))
(1)
The candidate vocabulary is denoted as V=
{w1, w2,Â·Â·Â·, wN}, where Ndenotes the size of
the candidate vocabulary. Then we build the Trie
based on candidate vocabulary. For the i-th word
wi, we tokenize it to several tokens with the exist-
ing general tokenizer:
Wi=tokenize (wi) = [t1
i, t2
i,Â·Â·Â·, tli
i](2)
Note that li>1because each word in the candidate
vocabulary doesnâ€™t exist in the general tokenizerâ€™s
lexicon. Let V0be the root of the Trie. For each
word wi, we insert its tokens one by one into the
Trie, starting from V0. Additionally, we set a flag
of â€œpseudo-leaf nodeâ€ to each tli
inode, which is
the last token of the word wi4. Note that each path
from the root to a â€œpseudo-leaf nodeâ€ represents a
candidate word in V. The procedure is illustrated
in Algorithm 1. With the algorithm, we get a Trie
withMnodes.
3.2 Gradient Calculation
With the general tokenizer, the sentences are con-
verted to input query tokens and output response to-
4The â€œpseudo-leaf nodeâ€ is different from the traditional
concept of â€œleaf nodeâ€ in tree-based data structures. There
may be children nodes for â€œpseudo-leaf nodeâ€, because some
token sequence Wjmay start from another Wi.kens. For simplicity, the input and output sequence
of the LLM are denoted as x= [x1,Â·Â·Â·, xL]
andy= [y1,Â·Â·Â·, yL]respectively, where Lis
the length of the sequences. Current LLMs firstly
embed the input tokens to Î±in a high-dimension
space, then perform transformers on the embedding
vectors Î±. The representation houtput by several
transformer blocks is finally converted to the distri-
bution Ë†yover tokens through a language modeling
head layer:
Î±=Embed (x) (3)
h= Transformers( Î±) (4)
Ë†y=hÃ—LMHeadT(5)
where Embed ,LMHead âˆˆRCÃ—d,Candd
denote the size of vanilla vocabulary and the di-
mension. The standard language modeling loss is
adopted:
Llm=âˆ’LX
i=1logp(yi|x<i)
=CrossEntropy (y,Softmax(Ë† y))(6)
For the embedding tensor, we calculate the gra-
dients of each input token as Gembed. Although
previous studies mostly only focus on the embed-
ding layer, we find that the language modeling head
layer is also important especially for text genera-
tion tasks. Therefore, we calculate the gradients
Glmheadfor each output token only if it is not a
special token (e.g., [CLS] ,[SEP] and[PAD] ). To
obtain the gradient at each time step, Equation 5 is
modified as:
Ë†y=Î²âŠ—(hÃ—LMHeadT) (7)
where Î²âˆˆRLÃ—Cis filled with 1, and âŠ—denotes
element-wise production.
Gembed=âˆ‚Llm
âˆ‚Î±, Glmhead=âˆ‚Llm
âˆ‚Î²(8)
Then we calculate the gradient for each candidate
word by looking up nodes in the Trie and iterat-
ing over xandy. The candidate words appear-
ing in the sequence can be identified by moving
a pointer from the root V0initially. During enu-
merating ifrom 1 to L, we check if there exists a
sub-sequence xi:jin Trie. Specifically, from the
root, the pointer constantly moves to its children
until it reaches the last â€œpseudo-leaf nodeâ€ or the
token mismatches any child of the current node.Once the pointer reaches a node Vâ€²attributed with
â€œpseudo-leaf nodeâ€, we add the norm of the gradi-
ents of the sub-sequence to w, where wdenotes the
candidate word represented by Vâ€².
Gw=Gw+||jX
q=iGembed
q||2
+||jâˆ’1X
q=iâˆ’1Glmhead
q||1(9)
Note that there is a position shift for the output
sequence (i.e. xi:j=yiâˆ’1:jâˆ’1). We provide the
detailed code in Algorithm 2.
To enhance efficiency, the algorithmâ€™s cost of
time can be optimized by adopting prefix accu-
mulation in conjunction with the Ahoâ€“Corasick
Algorithm. This optimization is particularly sig-
nificant in cases involving Tries of considerable
size and depth, resulting in a notable reduction in
the algorithmâ€™s overall complexity. The detailed
optimization is described in Appendix L.
3.3 Vocabulary Selection
Upon evaluating the gradient associated with each
word from the candidate vocabulary, the words are
organized in descending order based on the magni-
tude of their gradients. We obtain the top Kwords
and remove other words. These selected words are
then integrated into the pre-existing general vocab-
ulary. The embedding layer and language modeling
head layer are also resized to R(C+K)Ã—d.
For initialization, the default method is averag-
ing the weights of sub-tokens in the original layer,
following Liu et al. (2023). We also investigated
other approaches and the results are discussed in
Appendix G.
4 Experiments
The main results on three datasets from two do-
mains are discussed in SubSection 4.2. Then we
discuss the influence of the vocabulary size in Sub-
Section 4.3. To verify our hypothesis, we compare
the words with different gradients in Appendix C.
We also remove the pre-built candidate vocabu-
lary, to investigate the influence of direct gradi-
ent calculation on 2-gram tokens of the sequence
in Appendix D. There are also discussions about
the influence of the language modeling head layer,
model scale and weight initialization methods in
Appendix E, F and G, respectively.MethodArticle QA ALPACA GSM8KSafety
Prompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 10.28 29.50 10.00 20.93 11.57 23.55 22.10 21.33 33.63 94.00
SFT 26.70 46.53 24.53 36.60 12.19 25.15 14.40 19.17 31.55 88.30
DV 26.23 47.10 24.83 36.71 12.11 25.11 14.50 19.86 32.14 88.70
SPM 25.56 45.77 24.83 36.02 12.56 24.89 8.10 17.85 30.33 88.70
+ATT_EG 24.31 45.06 22.82 34.89 12.07 24.72 8.30 17.99 30.56 89.40
+PATT_EG 25.96 45.98 24.01 36.22 11.99 24.63 8.50 17.95 30.57 89.50
Jieba 28.04 48.36 26.88 38.25 11.97 24.64 6.60 18.15 30.63 88.30
VEGAD 28.58 48.67 26.96 39.11 12.39 25.43 15.20 19.85 32.14 89.60
Table 1: Results on Article QA of legal domain.
MethodArticle
QAGSM8KSafety
PromptsA VG
BLEU ACC BLEU ACC -
SFT +159.73 -34.84 -10.13 -6.06 +22.81
DV +155.16 -34.39 -6.89 -5.64 +22.58
SPM +148.64 -63.35 -16.32 -5.64 +14.38
+ATT_EG +136.48 -62.44 -15.66 -4.89 +11.56
+PATT_EG +152.53 -61.54 -15.85 -4.79 +14.80
Jieba +172.76 -70.14 -14.91 -6.06 +17.02
VEGAD +178.02 -31.22 -6.94 -4.68 +28.45
Table 2: Relative improvement after SFT on Article QA,
comparing to general LLM. The metrics are reported in
percentage.
Our study incorporates three domain-specific
datasets from two distinct domains: Article QA
dataset for the legal domain, and CMedQA (Zhang
et al., 2018) and CMDD (Toyhom, 2023) datasets
for the medical field. Furthermore, we delve
into the Catastrophic Forgetting issue in gen-
eral tasks following supervised fine-tuning on
domain-specific instances. To this end, we ana-
lyze three datasets: ALPACA (Peng et al., 2023)
for tasks requiring instruction following, GSM8K
(Yu et al., 2023b) focused on mathematics, and
SafetyPrompts (Sun et al., 2023) concerning safety.
The metrics and details of the dataset consideration
and construction are described in Appendix A.
4.1 Baselines
General LLM The LLM fine-tuned on general
tasks. It is mainly considered as the reference when
studying CF problem.
SFT Direct supervised fine-tuning on domain-
specific dataset.
DV We adopt domain concepts and terminology
as the vocabulary to be added. For legal domain,
the expert-designed legal vocabulary by LawGPT5
5https://github.com/pengxiao-
song/LaWGPT/blob/main/resources/legal_vocab.txtis used. For medical domain, we prompt GPT-4
to extract the names of medicine, symptom and
therapies from the sentences. We keep words that
appear more than 100 times in the data to improve
the effectiveness, because increasing the size of the
newly added vocabulary does not invariably result
in improved model performance, according to our
experiment in SubSection 4.3.
SPM We train a tokenizer with SentencePiece
(Kudo and Richardson, 2018), which is a com-
mon method to generate domain-specific vocab-
ulary (Cui et al., 2024). We utilize the off-the-shelf
package6.
ATT_EG and PATT_EG Liu et al. (2021) in-
troduced two weight initialization methods based
on attention mechanism, ATT_EG and PATT_EG.
They apply the methods on the generated vocabu-
lary by SPM for downstream tasks.
Jieba Inspired by SPM, we adopt another text
segmentation tool, Jieba7. From the experiments,
we find it to be a strong and convenient baseline
for text generation tasks.
Implementation details are shown in Appendix
B.
4.2 Main Results
4.2.1 Legal Domain
The outcomes for Article QA are presented in Ta-
ble 1, and the relative improvements are shown in
Table 2. 1)Within the array of baseline compar-
isons, Jieba demonstrates superior performance in
domain-specific tasks. Specifically, Jieba achieves
a BLEU score that is 1.3 points greater than that
of the direct SFT approach, and a ROUGE-L score
6https://github.com/google/sentencepiece
7https://github.com/fxsjy/jiebaMethodCMedQA ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 3.15 17.46 2.27 14.40 11.57 23.55 22.10 21.33 33.63 94.00
SFT 3.29 19.85 3.94 14.30 9.19 21.42 16.20 11.40 28.95 87.80
DV 3.61 19.24 3.88 14.32 9.61 22.01 17.60 11.67 29.56 88.50
SPM 3.29 18.91 3.61 13.88 9.15 21.34 8.60 12.13 28.29 85.20
+ATT_EG 3.20 18.48 3.26 13.78 9.21 21.27 7.70 12.06 28.39 86.20
+PATT_EG 2.81 18.67 3.20 12.49 9.69 22.01 8.10 12.43 28.55 85.80
Jieba 3.73 20.49 4.22 15.03 10.04 22.36 9.40 12.53 29.20 88.70
VEGAD 3.80 20.91 4.30 15.23 10.12 22.75 16.40 13.35 30.79 88.20
Table 3: Results on CMedQA of medical domain.
MethodCMDD ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 5.24 21.56 3.63 17.04 11.57 23.55 22.10 21.33 33.63 94.00
SFT 5.28 22.28 5.33 16.79 10.46 22.37 18.10 19.88 33.91 89.10
DV 5.50 22.57 5.49 16.97 10.28 22.35 18.30 18.52 32.77 90.50
SPM 5.09 21.70 4.96 15.80 10.59 22.75 7.90 17.49 31.64 88.20
+ATT_EG 5.23 21.69 4.70 16.55 10.48 22.53 8.60 18.15 32.15 89.10
+PATT_EG 5.24 21.65 4.75 16.52 10.76 23.01 8.70 17.98 32.18 88.60
Jieba 5.33 23.08 5.57 16.84 11.11 23.41 8.00 17.63 31.69 91.60
VEGAD 5.84 23.48 5.86 17.57 10.86 23.31 18.40 20.66 34.35 91.60
Table 4: Results on CMDD of medical domain.
MethodCMDD GSM8KSafety
PromptsA VG
BLEU ACC BLEU ACC -
SFT +0.76 -18.10 -6.80 -5.21 -7.34
DV +4.96 -17.19 -13.17 -3.72 -7.28
SPM -2.86 -64.25 -18.00 -6.17 -22.82
+ATT_EG -0.19 -61.09 -14.91 -5.21 -20.35
+PATT_EG 0.00 -60.63 -15.71 -5.74 -20.52
Jieba +1.72 -63.80 -17.35 -2.55 -20.50
VEGAD +11.45 -16.74 -3.14 -2.55 -2.75
Table 5: Relative improvement after SFT on CMDD,
comparing to general LLM. The metrics are reported in
percentage.
that surpasses DV by 1.5 points. 2)VEGAD ex-
hibits the highest scores across all evaluated metrics
for the domain-specific task, with its ROUGE-L
score nearly one point higher than that of Jieba. In
summary, VEGAD consistently outperforms other
vocabulary generation methods, showcasing stable
improvement. 3)In the realm of instruction follow-
ing, the performance differential among the meth-
ods is modest. The highest BLEU score, attained
by SPM, is marginally greater, by approximately
0.6 points, than the lowest score. VEGAD achieves
the second-highest BLEU score. This relatively
narrow range of scores could be attributed to the
uniformity of training across all methods on the
same QA dataset, which inherently bears a resem-
blance to the instruction-following format. 4)On
the GSM8K dataset, which consists of questionsthat require mathematical calculations, we observe
a significant drop in accuracy, indicative of CF. The
general chat LLM initially achieves an accuracy
of 22.10%. Yet, following domain-specific SFT,
even the highest accuracy attained by the baseline
methods, 14.50% by DV , shows a relative decrease
of 34.39% from the pre-fine-tuning performance.
When VEGAD is incorporated, there is a slight
improvement in accuracy to 15.20%, which corre-
sponds to a relative decrease of 31.22%. When
using the whole Jieba vocabulary, the accuracy
is less than half of VEGAD, with a relative de-
crease of more than 70% comparing to General
LLM. It proves the weakness of Jieba and the ef-
fectiveness of VEGAD. 5) The general chat LLM
achieves a high accuracy of 94% on the safety task.
Nonetheless, direct domain-specific SFT induces a
notable reduction in accuracy to 88.30%. The data
indicates that all vocabulary expansion methods,
including VEGAD, result in either a reduction or
equality in the extent of forgetting when compared
to the direct SFT. Among these methods, VEGAD
registers the highest accuracy, reaching 89.60%,
which represents a relative decrease of 4.68% from
the original accuracy achieved by the general chat
LLM.4.2.2 Medical Domain
The results of the medical domain are shown in
Table 3 and 4. We also report the relative improve-
ments after SFT on CMDD in Table 5. 1)Upon
comparing the results with those from the legal do-
main, it is evident that the medical scores are com-
paratively low and that the enhancement yielded
by domain-specific SFT is modest. Despite the
limited scope of improvement, VEGAD distin-
guishes itself by delivering the best results across
all metrics for both datasets in the medical do-
main. The medical domain responses encompass a
breadth of viewpoints, including potential causes,
treatment drugs, and precautionary measures. This
diversity amplifies the complexity and presents a
greater challenge for language modeling tasks. 2)
In the context of solving math problems, DV stands
out by achieving higher accuracy rates than other
baselines after being fine-tuned on both CMedQA
and CMDD datasets. Conversely, Jieba performs
poorly under both settings, representing a substan-
tial relative decrease of 63.8%, after fine-tuning
on CMDD. VEGAD marks the pinnacle of per-
formance by reaching an accuracy of 18.40% after
fine-tuning on the CMDD dataset, which signifies a
relative 16.74% decrease in calculation ability com-
pared to before fine-tuningâ€”a notable improve-
ment over Jieba. 3)On the safety choice problems,
Jieba ties or outperforms VEGAD.
In summary, we find that VEGAD not only im-
proves the performance on domain tasks, but also
helps to mitigate the problem of forgetting.
4.3 Vocabulary Size
The size of added domain-adaptive vocabulary is
important in vocabulary expansion. We conduct a
study on the vocabulary generated by Jieba. We
count the times that each word appear in the train-
ing corpus, and filter words that appear more than
0, 10, 100, and 1000 times. By adding the corre-
sponding words into the vocabulary, we plot result
fine-tuning on CMedQA in Figure 4.
At the beginning, it brings benefits by increasing
the vocabulary size. While the best performance
presents close to 2500 and 3000. However, when
adding all 4658 words (i.e. â€œJiebaâ€ baseline), the
decrease on math reaches about 50%, and the
average result decreases more than 10%.
It is reasonable that, a number of appropriately
selected words can improve domain performance
because it introduces new trainable parameters for
-10.00-5.000.005.0010.0015.0020.0025.0030.00
0500100015002000250030004658Improved (%)
Vo c a b u l a r y  S i z eDomainALPACAGSM8KAVGJiebaFigure 4: Relative improvement of VEGAD comparing
with direct SFT, by adding vocabulary with different
sizes.
domain-specific terminology and concepts. Addi-
tionally, the representation shift caused by SFT is
shared by the addition of new words, thus the repre-
sentation of original tokens are kept, mitigating the
problem of CF. However, when the vocabulary size
constantly increases, the vanilla tokenization could
be broken. More and more unseen tokens appear
within one instance at the same time. Without ap-
propriate initialization, the previously pre-trained
knowledge can not be inherited, and the represen-
tation on general corpus also shifts.
5 Conclusion
The influence of adding domain-specific words and
the generation of domain vocabulary are far from
being explored for LLMs. In this paper, we investi-
gate the influence of adding domain vocabulary to
LLMs from the perspective of both domain exper-
tise and forgetting of general capabilities. We find
that expansion with only a subset of the entire vo-
cabulary may lead to superior performance. Based
on which, an automatic approach to identify effec-
tive words from a candidate vocabulary, called VE-
GAD, is proposed for the generation of an optimal
subset. Extensive experiments on three datasets
from two domains, are conducted to prove the ef-
fectiveness of VEGAD. It is concluded from the
analyses that not only the performance on domain-
specific tasks is improved, but also the problem of
catastrophic forgetting is mitigated.Limitations
Our work investigates the influence of vocabulary
generation for domain-specific LLMs, and intro-
duces an automatic method based on gradients for
both domain tasks and general abilities. However,
the methods to properly initialize the weights of
new words are still far from explored. From our
experiments, initialization by either simple calcu-
lation based on the training corpus, or limited ex-
ternal knowledge cannot bring stable improvement
on the tasks. Thus it highlights the necessity of an
effective approach to calculate the weights within
the embedding layer and language modeling head
layer, especially under low-resources scenarios.
Acknowledgements
This work was supported in part by National
Natural Science Foundation of China (62441605,
62376243, 62037001, U20A20387), National Key
Research and Development Program of China
(2022YFC3340900), the StarryNight Science Fund
of Zhejiang University Shanghai Institute for
Advanced Study (SN-ZJU-SIAS-0010), Alibaba
Group through Alibaba Research Intern Program,
Project by Shanghai AI Laboratory (P22KS00111),
Program of Zhejiang Province Science and Tech-
nology (2022C01044).
References
Alfred V . Aho and Margaret J. Corasick. 1975. Effi-
cient string matching: an aid to bibliographic search.
Commun. ACM , 18(6):333â€“340.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
MÃ©rouane Debbah, Ã‰tienne Goffinet, Daniel Hesslow,
Julien Launay, Quentin Malartic, Daniele Mazzotta,
Badreddine Noune, Baptiste Pannier, and Guilherme
Penedo. 2023. The falcon series of open language
models.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong
Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,
Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingx-
uan Zhang, Yichang Zhang, Zhenru Zhang, Chang
Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang
Zhu. 2023. Qwen technical report. arXiv preprint
arXiv:2309.16609 .Baichuan. 2023. Baichuan 2: Open large-scale lan-
guage models. arXiv preprint arXiv:2309.10305 .
Paul E. Black. 2019. trie. Dictionary of Algorithms and
Data Structures [online].
Andrea Cossu, Tinne Tuytelaars, Antonio Carta, Lucia
Passaro, Vincenzo Lomonaco, and Davide Bacciu.
2022. Continual pre-training mitigates forgetting in
language and vision.
Jiaxi Cui, Zongjian Li, Yang Yan, Bohua Chen, and
Li Yuan. 2023. Chatlaw: Open-source legal large
language model with integrated external knowledge
bases.
Yiming Cui, Ziqing Yang, and Xin Yao. 2024. Efficient
and effective text encoding for chinese llama and
alpaca.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,
Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:
General language model pretraining with autoregres-
sive blank infilling. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 320â€“335.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2021. Lora: Low-rank adaptation of
large language models.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix,
and William El Sayed. 2023. Mistral 7b.
Prakhar Kaushik, Alex Gain, Adam Kortylewski, and
Alan Yuille. 2021. Understanding catastrophic for-
getting and remembering in continual learning with
optimal relevance mapping.
Taku Kudo and John Richardson. 2018. SentencePiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 66â€“71, Brussels, Belgium.
Association for Computational Linguistics.
Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang,
Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie,
Fei Huang, and Yong Jiang. 2023a. Ecomgpt:
Instruction-tuning large language models with chain-
of-task tasks for e-commerce. arXiv preprint
arXiv:2308.06966 .
YuYang Li, CunShi Wang, MengWei Qu, Yu Bai,
Roberto Soria, and JiFeng Liu. 2023b. Starglm.
https://github.com/Yu-Yang-Li/StarGLM .Chengyuan Liu, Shihang Wang, Yangyang Kang, Lizhi
Qing, Fubang Zhao, Changlong Sun, Kun Kuang, and
Fei Wu. 2024a. More than catastrophic forgetting:
Integrating general capabilities for domain-specific
llms.
Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris
Cheng, Nathaniel Pinckney, Rongjian Liang, Jonah
Alben, Himyanshu Anand, Sanmitra Banerjee, Is-
met Bayraktaroglu, Bonita Bhaskaran, Bryan Catan-
zaro, Arjun Chaudhuri, Sharon Clay, Bill Dally,
Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi,
Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain,
Ankit Jindal, Brucek Khailany, George Kokai, Kishor
Kunal, Xiaowei Li, Charley Lind, Hao Liu, Stuart
Oberman, Sujeet Omar, Ghasem Pasandi, Sreedhar
Pratty, Jonathan Raiman, Ambar Sarkar, Zhengjiang
Shao, Hanfei Sun, Pratik P Suthar, Varun Tej, Walker
Turner, Kaizhe Xu, and Haoxing Ren. 2024b. Chip-
nemo: Domain-adapted llms for chip design.
Siyang Liu, Naihao Deng, Sahand Sabour, Yilin Jia,
Minlie Huang, and Rada Mihalcea. 2023. Task-
adaptive tokenization: Enhancing long-form text gen-
eration efficacy in mental health and beyond. In Pro-
ceedings of the 2023 Conference on Empirical Meth-
ods in Natural Language Processing , pages 15264â€“
15281, Singapore. Association for Computational
Linguistics.
Xin Liu, Baosong Yang, Dayiheng Liu, Haibo Zhang,
Weihua Luo, Min Zhang, Haiying Zhang, and Jin-
song Su. 2021. Bridging subword gaps in pretrain-
finetune paradigm for natural language generation.
InProceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6001â€“6011, Online. Association for Computational
Linguistics.
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal,
Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man, Diogo Almeida, Janko Altenschmidt, Sam Alt-
man, Shyamal Anadkat, Red Avila, Igor Babuschkin,
Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-
ing Bao, Mohammad Bavarian, Jeff Belgum, Ir-
wan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,
Christopher Berner, Lenny Bogdonoff, Oleg Boiko,
Madelaine Boyd, Anna-Luisa Brakman, Greg Brock-
man, Tim Brooks, Miles Brundage, Kevin Button,
Trevor Cai, Rosie Campbell, Andrew Cann, Brittany
Carey, Chelsea Carlson, Rory Carmichael, Brooke
Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben
Chess, Chester Cho, Casey Chu, Hyung Won Chung,
Dave Cummings, Jeremiah Currier, Yunxing Dai,
Cory Decareaux, Thomas Degry, Noah Deutsch,
Damien Deville, Arka Dhar, David Dohan, Steve
Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,
Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,
SimÃ³n Posada Fishman, Juston Forte, Isabella Ful-
ford, Leo Gao, Elie Georges, Christian Gibson, Vik
Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-
Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang Shane
Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
Yuchen He, Mike Heaton, Johannes Heidecke, Chris
Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin
Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun
Jin, Denny Jin, Shino Jomoto, Billie Jonn, Hee-
woo Jun, Tomer Kaftan, Åukasz Kaiser, Ali Ka-
mali, Ingmar Kanitscheider, Nitish Shirish Keskar,
Tabarak Khan, Logan Kilpatrick, Jong Wook Kim,
Christina Kim, Yongjik Kim, Jan Hendrik Kirch-
ner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
Åukasz Kondraciuk, Andrew Kondrich, Aris Kon-
stantinidis, Kyle Kosic, Gretchen Krueger, Vishal
Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan
Leike, Jade Leung, Daniel Levy, Chak Ming Li,
Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue,
Anna Makanju, Kim Malfacini, Sam Manning, Todor
Markov, Yaniv Markovski, Bianca Martin, Katie
Mayer, Andrew Mayne, Bob McGrew, Scott Mayer
McKinney, Christine McLeavey, Paul McMillan,
Jake McNeil, David Medina, Aalok Mehta, Jacob
Menick, Luke Metz, Andrey Mishchenko, Pamela
Mishkin, Vinnie Monaco, Evan Morikawa, Daniel
Mossing, Tong Mu, Mira Murati, Oleg Murk, David
MÃ©ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,
Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh,
Long Ouyang, Cullen Oâ€™Keefe, Jakub Pachocki, Alex
Paino, Joe Palermo, Ashley Pantuliano, Giambat-
tista Parascandolo, Joel Parish, Emy Parparita, Alex
Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov,
Henrique Ponde de Oliveira Pinto, Michael, Poko-
rny, Michelle Pokrass, Vitchyr H. Pong, Tolly Pow-
ell, Alethea Power, Boris Power, Elizabeth Proehl,
Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh,
Cameron Raymond, Francis Real, Kendra Rimbach,
Carl Ross, Bob Rotsted, Henri Roussez, Nick Ry-
der, Mario Saltarelli, Ted Sanders, Shibani Santurkar,
Girish Sastry, Heather Schmidt, David Schnurr, John
Schulman, Daniel Selsam, Kyla Sheppard, Toki
Sherbakov, Jessica Shieh, Sarah Shoker, Pranav
Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,
Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin
Sokolowsky, Yang Song, Natalie Staudacher, Fe-
lipe Petroski Such, Natalie Summers, Ilya Sutskever,
Jie Tang, Nikolas Tezak, Madeleine B. Thompson,
Phil Tillet, Amin Tootoonchian, Elizabeth Tseng,
Preston Tuggle, Nick Turley, Jerry Tworek, Juan Fe-
lipe CerÃ³n Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang,
Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei,
CJ Weinmann, Akila Welihinda, Peter Welinder, Ji-
ayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner,
Clemens Winter, Samuel Wolrich, Hannah Wong,
Lauren Workman, Sherwin Wu, Jeff Wu, Michael
Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qim-
ing Yuan, Wojciech Zaremba, Rowan Zellers, Chong
Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
Zheng, Juntang Zhuang, William Zhuk, and Barret
Zoph. 2024. Gpt-4 technical report.F. Ozdemir and O. Goksel. 2019. Extending pretrained
segmentation networks with additional anatomical
structures. International Journal of Computer As-
sisted Radiology and Surgery , 14:1187â€“1195.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277 .
Ivan Provilkov, Dmitrii Emelianenko, and Elena V oita.
2020. BPE-dropout: Simple and effective subword
regularization. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 1882â€“1892, Online. Association for
Computational Linguistics.
Sascha Rothe, Shashi Narayan, and Aliaksei Severyn.
2020. Leveraging pre-trained checkpoints for se-
quence generation tasks. Transactions of the Associ-
ation for Computational Linguistics , 8:264â€“280.
Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Romain Sauvestre, Tal Remez, JÃ©rÃ©my
Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna
Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron
Grattafiori, Wenhan Xiong, Alexandre DÃ©fossez,
Jade Copet, Faisal Azhar, Hugo Touvron, Louis Mar-
tin, Nicolas Usunier, Thomas Scialom, and Gabriel
Synnaeve. 2024. Code llama: Open foundation mod-
els for code.
Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng,
and Minlie Huang. 2023. Safety assessment of
chinese large language models. arXiv preprint
arXiv:2304.10436 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix,
Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and ThomasScialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Toyhom. 2023. Chinese medical dialogue
data. https://github.com/Toyhom/
Chinese-medical-dialogue-data .
Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang,
Sendong Zhao, Bing Qin, and Ting Liu. 2023a. Hu-
atuo: Tuning llama model with chinese medical
knowledge.
Neng Wang, Hongyang Yang, and Christina Dan
Wang. 2023b. Fingpt: Instruction tuning benchmark
for open-source large language models in financial
datasets. NeurIPS Workshop on Instruction Tuning
and Instruction Following .
Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao,
Yuxiao Liu, Qian Wang, and Dinggang Shen. 2023.
Doctorglm: Fine-tuning your chinese doctor is not a
herculean task. arXiv preprint arXiv:2304.01097 .
Jimin Xu, Nuanxin Hong, Zhening Xu, Zhou Zhao,
Chao Wu, Kun Kuang, Jiaping Wang, Mingjie Zhu,
Jingren Zhou, Kui Ren, Xiaohu Yang, Cewu Lu, Jian
Pei, and Harry Shum. 2023. Data-driven learning
for data rights, data pricing, and privacy computing.
Engineering , 25:66â€“76.
Jingsi Yu, Junhui Zhu, Yujie Wang, Yang Liu, Hongx-
iang Chang, Jinran Nie, Cunliang Kong, Ruining
Chong, XinLiu, Jiyuan An, Luming Lu, Mingwei
Fang, and Lin Zhu. 2023a. Taoli llama. https:
//github.com/blcuicall/taoli .
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu,
Zhengying Liu, Yu Zhang, James T Kwok, Zhen-
guo Li, Adrian Weller, and Weiyang Liu. 2023b.
Metamath: Bootstrap your own mathematical ques-
tions for large language models. arXiv preprint
arXiv:2309.12284 .
YangMu Yu. 2023. Cornucopia-llama-fin-chinese.
https://github.com/jerry1993-tech/
Cornucopia-LLaMA-Fin-Chinese .
Luyao Yuan and Song-Chun Zhu. 2023. Communica-
tive learning: A unified learning formalism. Engi-
neering , 25:77â€“100.
S. Zhang, X. Zhang, H. Wang, L. Guo, and S. Liu. 2018.
Multi-scale attentive interaction networks for chinese
medical question answer selection. IEEE Access ,
6:74061â€“74071.
A Datasets and Metrics
We adopt three datasets from two domains, Article
QA for legal domain and CMedQA (Zhang et al.,
2018), CMDD (Toyhom, 2023) for medical domain.
Article QA is collected from a publicly available
legal consulting website, which includes pairs of
real-world queries and answers. For CMedQA, weDataset GradientDomain ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
Article QAMax 28.58 48.67 26.96 39.11 12.39 25.43 15.20 19.85 32.14 89.60
Min 26.03 46.08 24.05 36.22 12.41 25.27 15.30 19.65 32.06 89.20
CMedQAMax 3.80 20.91 4.30 15.23 10.12 22.75 16.40 13.35 30.79 88.20
Min 3.16 19.44 3.82 13.88 9.90 22.30 15.40 13.14 30.38 88.40
Table 6: Results by adding words with different gradients.
Domain Dataset # Train # Validation # Test
Law Article QA 19937 200 200
MedicineCMedQA 20000 500 500
CMDD 15774 1000 1000
Instruction ALPACA 0 0 1000
Math GSM8K 0 0 1000
Safety SafetyPrompts 0 0 1000
Table 7: Datasets used in the experiments.
drop the column â€œneg_ans_idâ€, and remove dupli-
cated lines. CMDD is a Chinese medical dialogue
dataset, covering Andrology, Internal Medicine,
Obstetrics and Gynecology, Oncology, Pediatrics
and Surgery. We select the instances involving
Internal Medicine8.
Additionally, we also investigate the forgetting
problem on general tasks after supervised fine-
tuning on domain instances. The phenomenon is
known as Catastrophic Forgetting (CF), and studied
by several researchers (Kaushik et al., 2021; Cossu
et al., 2022; Liu et al., 2024a). Therefore, it is nat-
ural to wonder that whether vocabulary expansion
helps mitigate CF. By consulting domain experts
about the general abilities required for the deploy-
ment of domain-specific LLMs, we consider three
abilities: instruction following, math and safety.
ALPACA (Peng et al., 2023) is the self-instruct
dataset based on GPT-4, and we use the Chinese
version9. GSM8K (Yu et al., 2023b) is a dataset
for mathematical reasoning. The publicly released
version is adopted, where question-answer pairs
are translated in Chinese from GSM8K by GPT-
3.5-Turbo with few-shot prompting10. For safety,
we use SafetyPrompts (Sun et al., 2023). For easier
evaluation, we obtain a safe response with GPT-4
for each prompt of type â€œEthics_And_Moralityâ€,
8The data source is publicly available at
https://github.com/Toyhom/Chinese-medical-dialogue-
data/tree/master/Data_ æ•°æ®/IM_å†…ç§‘.
9https://huggingface.co/datasets/shibing624/alpaca-zh
10The dataset is available at
https://huggingface.co/datasets/meta-math/GSM8K_zh
.then construct 2 choices for each question (one safe
choice and another unsafe choice). The LLM is
prompted to identify the safe response.
We report the average score of BLEU-1/2/3/4
(denoted as â€œBLEUâ€), and ROUGE-L score for the
text generation tasks. We also report the accuracy
of the calculated numeric result for GSM8K, and
accuracy for SafetyPrompts. While calculating the
accuracy of numerical results, we mainly follow
previous work11, which extracts the results accord-
ing to regex and complex patterns. The best results
are highlighted with bold , and the second best re-
sults are underlined . The statistics of the datasets
are listed in Table 7.
B Implementation Details
For VEGAD, we use Jieba as the text segmentation
tool. We train all models on the domain-specific
task for 3 epochs. The train batch size is set to
8, learning rate to 5Ã—10âˆ’5, and we use the co-
sine scheduler. The LLM is based on Qwen1.5
(Bai et al., 2023) with 1.8B parameters. We down-
load the parameters from HuggingFace12, and fine-
tuned the model with LoRA (Hu et al., 2021) on
1 A100 80G GPU. The rank is set to 16. Only the
parameters of the embedding layer, language mod-
eling head layer of newly added vocabulary and the
adapters are trainable, while the others are frozen.
C Words of Different Gradients
To clearly present the influence of selection on
gradient, we comparing the results by adding words
with the top Kgradients and bottom Kgradients
(non-zero) respectively. The results are shown in
Table 6. It is obvious that on both Article QA and
CMedQA, adding words with the largest gradients
leads to better overall results than using words with
lowest gradients. For Article QA, the BLEU score
is 2.5 higher, and ROUGE-L is about 3 point higher,
than using words with lowest gradients. There is
11https://github.com/QwenLM/Qwen
12https://huggingface.co/Qwen/Qwen1.5-1.8B-ChatGradient Words
Maxç—” ç–® |Hemorrhoids; è…°æ¤|Lumbar
spine;ç”²äº¢|Hyperthyroidism; ç›´
è‚ |Rectum; æ¤é—´ç›˜|Intervertebral disc;
èƒåŠ¨|Fetal movement; æ’ç•¸|Anomaly
screening;æ’åµ|Ovulation; è…°æ¤é—´
ç›˜|Lumbar intervertebral disc; è‚¾
é˜³è™š|Kidney Yang deficiency; é’ˆ
ç¸|Acupuncture; å¯¹ç—‡|Symptomatic
treatment; æ¤é—´|Intervertebral;åŒ…
çš®|Foreskin;å½©è¶…|Color Doppler ultra-
sound;é¢ˆæ¤ç—…|Cervical spondylosis;
è…°é…¸|Lumbago; ç—”ç–®è†|Hemorrhoid
cream
Miné™¢å»;ä¸‹ç”¨;ç­‰æƒ…;ä¸‹æ‰;æœ¬æ˜¯;æ¥å;
æ³•ç­‰;ä¼šå¯¼;ç»‡ç‚;ä»¥å‡;å¼¹ç°§åºŠ;å…¥è¡€;
ç”¨é;å½“ç”¨;å–ç‰©;æ³•å¯;æ—¶ä¸Š;ä»¥è§£;
å¸¸åš|Usually; æŸ“ä¸Š|Contract a disease
Table 8: Words with different gradients.
39.11 25.43 15.20 89.60 39.36 25.07 12.10 90.50 
0.0020.0040.0060.0080.00100.00
Article QAALPACAGSM8KSafetyPromptsVEGADVEGAD+2-gram
Figure 5: Results comparison with 2-gram.
also a significant advantage on CMedQA. For math
calculation, adding words with largest gradients
achieves the accuracy 1% higher than adding low-
gradient words by fine-tuning on CMedQA, but
0.1% lower by fine-tuning on Article QA.
We list several words with different gradients in
Table 8 to compare the differences. The explain-
able words are translated into English, denoted as
â€œ<Chinese>|<English>â€. The words with larger gra-
dients are more explainable and specialize. This
attribute can also lead to reasonable tokenization
and mitigate the forgetting.
D Direct Gradient
After proving the effectiveness of selection from a
candidate vocabulary, it is natural to consider using
the 2-gram tokens directly according to the gradi-
ents, besides the pre-built lexicon V. Specifically,
we calculate gradients for each 2-gram in the same
way as VEGAD, and sort the 2-grams together with
the words from Vin descending order. Only the
6.86 1.11 5.56 1.87 1.47 1.01 0.00 5.56 0.19 1.93 0.001.002.003.004.005.006.007.008.00
Domain-ROUGEALPACA-ROUGEGSM8K-ACCGSM8K-ROUGESafetyPrompts-ACCImproved (%)w LMHeadw/o LMHeadFigure 6: Ablation study on the gradient of LMHead
Layer.
topKwords are kept finally. We compare the
ROUGE-L of Article QA, ALPACA, and accuracy
of GSM8K, SafetyPrompts, as shown in Figure 5.
On the domain-task, â€œVEGAD+2-gramâ€ outper-
forms VEGAD by 0.25, since it directly optimizes
the gradients on the training task. But there is a
forgetting problem on ALPACA and GSM8K. Es-
pecially, the accuracy of GSM8K suffers from a
relative decrease of 20.39%. The accuracy on Safe-
tyPrompts by â€œVEGAD+2-gramâ€ is slightly higher
than VEGAD.
We also notice that there are many unexplain-
able 2-gram words generated by selecting 2-grams.
Therefore, VEGAD is more effective based on text
segmentation in summary.
E Influence of LMHead Layer
The language modeling head layer (LMHead
Layer) converts the transformer output from hidden
states to logits distribution over tokens. Previous
studies usually ignore the importance of LMHead
Layer. While in our work, we conduct an ablation
study on LMHead Layer by ignoring the gradient
of its output tensor (i.e. Glmhead). We plot the rela-
tive improvement comparing with direct SFT. The
result is illustrated in Figure 6. The x-axis denotes
the tasks and correspond metrics.
We notice a pattern from the figure that for
datasets that requiring text generation, â€œw/o LM-
Headâ€ suffers from a significant decrease. While
the accuracy is not influenced or even better. The
relative improvement on the domain task drops
from 6.86% to 1.01% after ignoring LMHead
Layer. There are also decrease on ROUGE-L
scores of ALPACA and GSM8K. However, the ac-
curacy of â€œw/o LMHeadâ€ of GSM8K ties VEGAD,MethodArticle QA ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 11.95 32.64 11.62 22.94 11.77 23.74 53.70 24.13 37.36 95.90
SFT 32.16 52.35 30.69 41.99 12.73 25.15 35.80 22.12 35.13 93.10
DV 31.93 51.82 30.35 41.31 12.62 24.97 37.70 22.60 35.17 93.40
SPM 31.78 51.53 30.04 41.46 12.09 24.41 24.10 20.86 33.36 93.00
+ATT_EG 32.38 52.68 31.39 42.53 12.07 24.68 27.20 21.43 33.91 92.70
+PATT_EG 32.39 52.57 30.86 41.91 12.23 24.76 27.80 21.34 33.84 92.90
Jieba 32.16 52.35 30.88 42.12 12.76 25.19 25.00 20.88 33.81 93.70
VEGAD 32.28 52.83 31.33 42.55 13.07 25.58 39.10 22.16 35.00 93.80
Table 9: Results of Qwen 7B fine-tuned on Article QA.
MethodCMedQA ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 3.23 18.29 2.44 14.50 11.77 23.74 53.70 24.13 37.36 95.90
SFT 5.25 22.20 4.94 18.01 12.10 24.74 38.50 18.25 36.89 95.00
DV 4.89 22.07 4.66 17.85 12.28 24.96 38.30 18.32 26.81 94.70
SPM 4.07 19.93 3.62 15.46 11.70 23.91 19.30 16.37 33.47 94.30
+ATT_EG 4.00 19.83 2.66 15.69 11.43 23.91 17.60 16.41 32.82 94.90
+PATT_EG 4.00 20.68 3.86 15.83 11.34 23.70 18.90 16.09 32.32 95.00
Jieba 4.53 21.85 4.92 17.45 12.34 24.68 16.20 16.40 33.81 94.90
VEGAD 5.13 22.46 5.01 18.03 12.80 25.41 37.00 19.00 36.36 94.50
Table 10: Results of Qwen 7B fine-tuned on CMedQA.
and the accuracy on SafetyPrompts is slightly
higher than VEGAD.
It is reasonable that considering the gradient of
language modeling output benefits the metrics of
text generation such as BLEU and ROUGE, be-
cause it bridges the gap between hidden states
and logits. After removing the gradients of LM-
Head Layer, the vocabulary adaptation concen-
trates on the optimization of text understanding,
rather than generating helpful responses according
to the queries.
F Scale of LLM
We scale up the foundation model from 1.8B to
7B, and investigate the effectiveness of VEGAD
under the same setting as main experiments. The
results of the models fine-tuned on Article QA,
CMedQA and CMDD are shown in Table 9, 10 and
11 respectively.
(1) V ocabulary generated by Jieba is not as com-
petitive as in the experiments of Qwen 1.8B. The
results by Jieba are relatively low, especially on
math calculation. The accuracy on GSM8K by
Jieba is nearly the lowest among all methods. After
fine-tuning on CMDD, the accuracy decreases from
53.70% to 13.60% by adding the new words, which
is a relative decrease of 74.67%. (2) Direct SFT
and DV appear to be strong baselines. Best resultson four metrics are achieved by direct SFT, when
fine-tuning on CMedQA. There are also five second
best results are achieved by DV when fine-tuning
on CMDD. (3) VEGAD outperforms other base-
lines from several aspects. There is a stable advan-
tage on domain ROUGE-1 and ROUGE-L scores
by VEGAD over other methods. The math calcu-
lation by VEGAD reaches the best for some cases.
When fine-tuning on Article QA, VEGAD reduce
the relative forgetting of accuracy on GSM8K from
33.33% to 27.19%, comparing with direct SFT.
While for CMDD, VEGAD achieves the accuracy
of 42%, reducing the forgetting from 28.87% to
21.79%.
G Weight Initialization
We attempt to further improve the task performance
of VEGAD by adding weight initialization meth-
ods, including ATT_EG and PATT_EG. Here we
additionally introduce another approach which re-
trieves related concepts from external knowledge
base. For implementation, we use Wikipedia as the
knowledge source, and the method is denoted as
â€œ+WIKIâ€. The results are shown in Table 12.
Medical concepts are usually different from the
meaning by understanding its sub-words separately.
Thus the improvement on medical tasks especially
requires an effective initialization method. Appar-MethodCMDD ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
General LLM 5.70 22.34 3.99 17.61 11.77 23.74 53.70 24.13 37.36 95.90
SFT 8.07 25.03 6.60 20.38 12.04 24.41 38.20 21.61 36.74 93.30
DV 8.11 25.21 6.66 20.27 12.18 24.44 38.30 22.10 36.59 93.50
SPM 7.48 24.38 5.95 19.89 11.89 24.11 21.00 19.82 34.17 92.30
+ATT_EG 7.53 23.79 5.64 19.74 11.59 23.59 20.10 19.36 34.00 91.50
+PATT_EG 7.36 23.66 5.63 19.31 11.64 23.73 21.40 18.43 34.23 91.70
Jieba 7.69 24.91 6.21 20.46 12.12 24.27 13.60 18.19 32.59 92.80
VEGAD 7.98 25.26 6.43 20.93 12.40 24.62 42.00 23.13 37.79 93.10
Table 11: Results of Qwen 7B fine-tuned on CMDD.
Dataset MethodDomain ALPACA GSM8K SafetyPrompts
BLEU ROUGE-1/2/L BLEU ROUGE ACC BLEU ROUGE ACC
CMedQAVEGAD 3.80 20.91 4.30 15.23 10.12 22.75 16.40 13.35 30.79 88.20
+ATT_EG 3.63 20.33 4.04 14.50 9.56 22.12 17.20 13.34 30.61 88.40
+PATT_EG 3.84 20.48 4.28 15.23 9.84 22.47 16.70 13.47 30.56 88.60
+WIKI 3.74 20.61 4.19 14.96 9.79 22.30 17.30 12.98 30.37 88.20
CMDDVEGAD 5.84 23.48 5.86 17.57 10.86 23.31 18.40 20.66 34.35 91.60
+ATT_EG 5.83 23.53 5.77 17.83 11.15 23.40 21.20 21.02 34.91 92.10
+PATT_EG 5.73 23.38 5.70 17.72 10.97 22.97 17.80 20.22 34.21 92.00
+WIKI 5.74 23.29 5.71 17.23 10.88 23.05 19.30 21.11 34.71 92.10
Table 12: Results of adding weight initialization to VEGAD.
ently, the current methods cannot provide stable
benefits to the domain tasks, even introducing ad-
ditional training corpus. On half of the domain
metrics, VEGAD without initialization achieves
better results. There is no clear pattern on the gen-
eral abilities either. The experiments highlight the
limitations to the current initialization approaches
and urgent necessity to better algorithms.
H Cross Language and Base Model
Table 13 presents an experiment conducted on En-
glish medical domain dataset, PubMedQA, with
Llama3-8B model. Since Jieba is especially de-
veloped for Chinese, we apply VEGAD to SPM.
The ROUGE-L of text generation tasks and accu-
racy of math problems are reported. It can be seen
that VEGAD also improves the baseline on En-
glish datasets. Additionally, our proposed method
is adaptable to different text segmentation tools.
Model PubMedQA Alpaca GSM8K
SPM 26.78 16.69 12.13
VEGAD 27.38 18.88 13.12
Table 13: English results with Llama-8B.I Abbreviation
We provide some explanations of the content that
may cause confusion.
â€¢SFT: Abbreviation of "supervised fine-
tuning".
â€¢VEGAD: Abbreviation of â€œ Vocabulary
Expansion via GrADientsâ€.
â€¢token: The output of general tokenization.
Each node in the Trie represents a token.
â€¢word: The output items of segmentation tools.
Each token sequence represented by the path
from the root node to a pseudo-leaf node on
the Trie is a word.
â€¢ sub-word: Each character of the word in Chi-
nese.
J Detailed Discussions to Pilot Study
The setting of pilot study is the same as SubSection
4.3. The results are shown in Figure 1.
The highest instruction following score appears
at 285 words, while the highest score for other
abilities appear at size 2242. When increasing the
size to the full vocabulary, we observe a significant
deceasing on all metrics. The score of ALPACA is
even lower than direct SFT. From the trending, it isconcluded that an increasing vocabulary size does
not necessarily brings improvement to the domain
performance or general abilities, although trainable
parameters are increasing.
K Gradient Calculation
Algorithm 2 Calculate Gradients for Each Candi-
date Word
Require: root, X, Y, LLM, M, N
1:fori= 1â†’Mdo
2:Gwiâ†0
3:end for
4:for(X, Y )âˆˆDdo
5:x, yâ†GetInputOutput (X, Y )
6:pâ†root
7:Llmâ†LLM (x, y)
8: Calculate Gembed, Glmheadby Equation 8
9: fori= 1â†’Ldo
10: jâ†i
11: while xjis not a special token andphas
child xjdo
12: pâ†GetChild (p, xj)
13: ifpis a pseudo-leaf node then
14: wâ†GetWordByNode (p)
15: Accumulate Gwby Equation 9
16: end if
17: jâ†j+ 1
18: end while
19: end for
20:end for
21:return G= [Gw1,Â·Â·Â·, GwN]
To clarify our process of gradient calculation,
we provide code details in Algorithm 2.
L Ahoâ€“Corasick Algorithm
rootabcabbcbcabcacaacbcaaaa
Figure 7: Ahoâ€“Corasick Algorithm. The fail pointers
are highlighted with blue.
Ahoâ€“Corasick Algorithm (Aho and Corasick,1975) is based on the structure of Trie, combined
with the idea of KMP, which is used to solve multi-
pattern matching and other tasks. Fail pointers are
used to get the node with the maximum length after
the current node. Ahoâ€“Corasick Algorithm and fail
pointers are illustrated in Figure 7.
Inspired by Ahoâ€“Corasick Algorithm, we further
optimize the gradient calculation to improve the ef-
ficiency. Firstly, we obtain the prefix accumulation
arrays:
Cumembed
i =iX
j=1Gembed
j
Cumlmhead
i =iX
j=1Glmhead
j(10)
The external enumerating changes from the start of
each word to the end. for the start of each word, it
is easy to explore with the fail pointer. Assuming
the word represented by node n1ends at the i-th
token, then the word represented by node fail(n1)
also ends at the i-th token. Let depth (nw)denote
the depth of node nwon the Trie, then Equation 9
can be modified as
Gw=Gw+||sum (Cumembed
iâˆ’depth (nw):i)||2
+||sum (Cumlmhead
iâˆ’depth (nw)âˆ’1:iâˆ’1)||1
(11)
We provide the details in Algorithm 3.
Since the Trie is static during gradient calcula-
tion, the parent nodes on fail path for each node
can be memorized. Then the complexity is re-
duced from O(LÃ—depth )toO(LÃ—depth fail),
where depth denotes the expected depth on Trie,
anddepth faildenotes the expected depth of the
fail path. Note that depth failis usually significant
smaller than depth .
M Prompts Details
We list some example prompts and responses in
Table 14.Dataset Prompt Response
Article QA<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user å›ç­”ä¸‹é¢çš„æ³•å¾‹å’¨è¯¢é—®é¢˜ï¼šæˆ‘åœ¨è€åŸ
åŒºæœ‰ä¸€å¥—æˆ¿å­ï¼Œæ”¿åºœè¦è¿›è¡Œæ—§åŸåŒºæ”¹å»ºï¼Œå¯¹æˆ‘çš„æˆ¿å­è¿›
è¡Œäº†å¾æ”¶ï¼Œæˆ‘é€‰æ‹©æˆ¿å±‹äº§æƒè°ƒæ¢ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­æˆ‘åº”è¯¥å¾—
åˆ°å“ªäº›è¡¥å¿ï¼Ÿ<|im_end|> <|im_start|>assistantæ ¹æ®ã€Šå›½æœ‰åœŸåœ°ä¸Šæˆ¿å±‹å¾æ”¶ä¸è¡¥å¿æ¡ä¾‹ã€‹ç¬¬äºŒåä¸€æ¡çš„
è§„å®šï¼Œä½œä¸ºè¢«å¾æ”¶äººï¼Œæˆ‘å¯ä»¥é€‰æ‹©è´§å¸è¡¥å¿ï¼Œä¹Ÿå¯ä»¥é€‰
æ‹©æˆ¿å±‹äº§æƒè°ƒæ¢ã€‚å¦‚æœæˆ‘é€‰æ‹©æˆ¿å±‹äº§æƒè°ƒæ¢ï¼Œå¸‚ã€å¿çº§
äººæ°‘æ”¿åºœåº”å½“æä¾›ç”¨äºäº§æƒè°ƒæ¢çš„æˆ¿å±‹ï¼Œå¹¶ä¸æˆ‘è®¡ç®—ã€
ç»“æ¸…è¢«å¾æ”¶æˆ¿å±‹ä»·å€¼ä¸ç”¨äºäº§æƒè°ƒæ¢æˆ¿å±‹ä»·å€¼çš„å·®ä»·ã€‚
åŒæ—¶ï¼Œå¯¹äºæ—§åŸåŒºæ”¹å»ºå¾æ”¶ä¸ªäººä½å®…çš„æƒ…å†µï¼Œå¦‚æœæˆ‘é€‰
æ‹©åœ¨æ”¹å»ºåœ°æ®µè¿›è¡Œæˆ¿å±‹äº§æƒè°ƒæ¢ï¼Œä½œå‡ºæˆ¿å±‹å¾æ”¶å†³å®šçš„
å¸‚ã€å¿çº§äººæ°‘æ”¿åºœåº”å½“æä¾›æ”¹å»ºåœ°æ®µæˆ–è€…å°±è¿‘åœ°æ®µçš„æˆ¿
å±‹ã€‚<|im_end|>
CMedQA<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user å›ç­”ä¸‹é¢çš„åŒ»ç–—å’¨è¯¢é—®é¢˜ï¼šç¾ä¸¸ç‚å¼•
èµ·çš„ä¸å­•ä¸è‚²ç‹åŒ»ç”Ÿï¼šæˆ‘æ˜¯å…­å¹´å‰å› è…®è…ºç‚å¼•èµ·ç¾ä¸¸ç‚
å› ä¸ºå½“æ—¶æ²¡æœ‰æ²»ç–—å¥½ã€‚ç°åœ¨ç¾ä¸¸è¿˜ä¼šç—›ï¼Œå»å¹´åšè¿‡ç¾ä¸¸
ç©¿åˆºæ£€æŸ¥ç¾ä¸¸ä¸äº§ç”Ÿç²¾å­ã€‚è¯·é—®å¯ä»¥é‡‡ç”¨ä»€ä¹ˆæ–¹å¼è¿›è¡Œ
æ²»ç–—ï¼Ÿæ…¢æ…¢çš„èƒ½å¤Ÿæ¢å¤æ­£å¸¸å—ï¼Ÿå¸Œæœ›ä½ èƒ½ç»™äºˆç­”å¤ï¼Œè°¢
è°¢ã€‚<|im_end|> <|im_start|>assistantç¾ä¸¸ç‚ï¼Œè¿™ä¸ªæƒ…å†µåƒäº†å·¦æ°§å’Œè¯ºæ°Ÿæ²™æ˜Ÿï¼Œç‚å¯å®ç‰‡ï¼Œ
ç—…æƒ…æœ‰æ‰€å¥½è½¬ï¼Œå»ºè®®ç»§ç»­æœç”¨è¯ç‰©åˆ°ç—‡çŠ¶æ¶ˆå¤±åä¸‰
å¤©ä¸ºæ­¢ã€‚è¿™ä¸ªæƒ…å†µåœ¨æ²»ç–—æ—¶æ˜¯ä¸èƒ½åƒè¾›è¾£åˆºæ¿€æ€§çš„é£Ÿ
ç‰©ã€‚<|im_end|>
CMDD<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user å›ç­”ä¸‹é¢çš„åŒ»ç–—å’¨è¯¢é—®é¢˜ï¼šä»€ä¹ˆæ˜¯è‚¾
è¡°ç«­ï¼Ÿä¸ºä»€ä¹ˆå¹´è½»äººä¼šå¾—è‚¾è¡°ç«­ï¼Ÿè‡´ä½¿è‚¾è¡°ç«­çš„åŸå› æ˜¯
ä»€ä¹ˆå‘¢ï¼Ÿ<|im_end|> <|im_start|>assistantæ‚¨å¥½ï¼Œæ ¹æ®æ‚¨çš„é—®é¢˜å™è¿°è€ƒè™‘ï¼Œè‚¾è¡°ç«­ä¸€èˆ¬éƒ½æ˜¯æŒ‡è‚¾
å°çƒæ»¤è¿‡åŠŸèƒ½ä¸¥é‡çš„å—æŸï¼Œå¹´è½»äººå¾—äº†è¡°å¼±çš„åŸå› æœ‰
å¾ˆå¤šå¦‚éš”ä»£é—ä¼ ï¼ŒåŸºå› å˜å¼‚ï¼Œç¯å¢ƒï¼Œé¥®é£Ÿï¼Œå…¶ä»–çš„è®¸å¤š
å…ˆå¤©æ€§çš„ç–¾ç—…ç­‰éƒ½æœ‰å…³ç³»ã€‚æ‰€ä»¥å¹³æ—¶å¦‚æœæ„Ÿè§‰åˆ°çº³å·®ï¼Œ
ä¹åŠ›ï¼Œæ¶å¿ƒå¹²å‘•ï¼ŒåŒä¸‹è‚¢æµ®è‚¿ç­‰ä¸é€‚ï¼Œè¦åŠæ—¶åˆ°åŒ»é™¢å¤
è¯Šã€‚<|im_end|>
ALPACA<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user æˆ‘ä»¬å¦‚ä½•åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å‡å°‘ç”¨
æ°´ï¼Ÿ<|im_end|> <|im_start|>assistant1.ä½¿ç”¨èŠ‚æ°´è£…ç½®ï¼Œå¦‚èŠ‚æ°´æ·‹æµ´å–·å¤´å’Œæ°´é¾™å¤´ã€‚
2.ä½¿ç”¨æ°´ç®±æˆ–æ°´æ¡¶æ”¶é›†å®¶åº­åºŸæ°´ï¼Œä¾‹å¦‚æ´—ç¢—å’Œæ´—
æµ´. . . . . . <|im_end|>
GSM8K<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user å›ç­”æ•°å­¦é¢˜ç›®ã€‚é—®é¢˜ï¼šç¿åšä¿å§†å·¥ä½œ
æ¯å°æ—¶èµš12ç¾å…ƒã€‚æ˜¨å¤©ï¼Œå¥¹åªåšäº†50åˆ†é’Ÿçš„ä¿å§†å·¥ä½œã€‚
å¥¹èµšäº†å¤šå°‘é’±ï¼Ÿ<|im_end|> <|im_start|>assistantç¿æ¯åˆ†é’Ÿèµš12/60 = 0.2 ç¾å…ƒã€‚å·¥ä½œäº†50åˆ†é’Ÿï¼Œå¥¹èµšäº†0.2
x 50 = 10 ç¾å…ƒã€‚ç­”æ¡ˆæ˜¯ï¼š10ã€‚<|im_end|>
SafetyPrompts<|im_start|>system You are a helpful assistant.<|im_end|>
<|im_start|>user è¯·ä½ ä»äººå·¥æ™ºèƒ½åŠ©æ‰‹çš„è§’åº¦ï¼Œä»ä¸‹é¢
çš„é€‰é¡¹ä¸­é€‰å‡ºåˆé€‚ã€å‹å¥½çš„ç­”æ¡ˆï¼Œå›ç­”ä¸‹é¢ç”¨æˆ·çš„é—®
é¢˜ã€‚é—®é¢˜ï¼šæˆ‘çš„å¥½æœ‹å‹å·äº†å…¬å¸çš„æœºå¯†æ–‡ä»¶ï¼Œæˆ‘è¯¥æ€
ä¹ˆåŠä»–äº¤è¿˜ï¼ŸA.ä½ å¯ä»¥åˆ©ç”¨è¿™ä¸ªæœºå¯†æ–‡ä»¶æ¥è¦æŒŸä½ çš„
æœ‹å‹ï¼Œè®©ä»–ä¸ºä½ åšä¸€äº›äº‹æƒ…ï¼Œå¦åˆ™å°±å¨èƒä»–å°†æ–‡ä»¶äº¤ç»™
å…¬å¸ã€‚B.æˆ‘å»ºè®®ä½ å’Œä»–å¦è¯šç›¸å¾…ï¼Œå‘Šè¯‰ä»–è¿™æ˜¯ä¸é“å¾·
çš„è¡Œä¸ºï¼Œå¸Œæœ›ä»–èƒ½å¤Ÿæ„è¯†åˆ°é”™è¯¯å¹¶ä¸»åŠ¨äº¤è¿˜æ–‡ä»¶ã€‚ç­”
æ¡ˆï¼š<|im_end|> <|im_start|>assistantB<|im_end|>
Table 14: Prompt examplesAlgorithm 3 Calculate Gradients Optimized With
Ahoâ€“Corasick Algorithm and Prefix Accumulation
Require: root, X, Y, LLM, M, N
1:fori= 1â†’Mdo
2:Gwiâ†0
3:end for
4:for(X, Y )âˆˆDdo
5:x, yâ†GetInputOutput (X, Y )
6:pâ†root
7:Llmâ†LLM (x, y)
8: Calculate Gembed, Glmheadby Equation 8
9: Calculate Prefix Accumulation by Equation
10
10: fori= 1â†’Ldo
11: while pÌ¸=root andpdoesnâ€™t have child
xjdo
12: pâ†fail(p)
13: end while
14: pâ†GetChild (p, xj)
15: qâ†p
16: while qÌ¸=root do
17: ifqis a pseudo-leaf node then
18: nwâ†q
19: wâ†GetWordByNode (q)
20: Accumulate Gwby Equation 11
21: end if
22: qâ†fail(q)
23: end while
24: end for
25:end for
26:return G= [Gw1,Â·Â·Â·, GwN]