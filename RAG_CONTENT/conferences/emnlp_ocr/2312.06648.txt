EMNLP 2024 Main Conference
DenseXRetrieval: What Retrieval Granularity Should We Use?
Tong Chen♣*Hongwei Wang♢Sihao Chen♡Wenhao Yu♢
Kaixin Ma♢Xinran Zhao♠Hongming Zhang♢Dong Yu♢
♣University of Washington♢Tencent AI Lab
♡University of Pennsylvania♠Carnegie Mellon University
Abstract
Dense retrieval has become a prominent
method to obtain relevant context or world
knowledge in open-domain NLP tasks. When
we use a learned dense retriever on a retrieval
corpus at inference time, an often-overlooked
design choice is the retrieval unit in which the
corpus is indexed, e.g. document, passage, or
sentence. We discover that the retrieval unit
choice significantly impacts the performance of
both retrieval and downstream tasks. Distinct
from the typical approach of using passages or
sentences, we introduce a novel retrieval unit,
proposition , for dense retrieval. Propositions
are defined as atomic expressions within text,
each encapsulating a distinct factoid and pre-
sented in a concise, self-contained natural lan-
guage format. We conduct an empirical com-
parison of different retrieval granularity. Our
experiments reveal that indexing a corpus by
fine-grained units such as propositions signif-
icantly outperforms passage-level units in re-
trieval tasks. Moreover, constructing prompts
with fine-grained retrieved units for retrieval-
augmented language models improves the per-
formance of downstream QA tasks given a spe-
cific computation budget.
1 Introduction
Dense retrievers are a popular class of techniques
for accessing external information sources for open-
domain NLP tasks (Karpukhin et al., 2020). Before
we use a learned dense retriever to retrieve from a
corpus, an imperative design decision we have to
make is the retrieval unit – i.e. the granularity at
which we segment and index the retrieval corpus
for inference. In practice, the choice of retrieval
units, e.g. documents, fixed-length passage chunks
or sentences, etc, is usually pre-determined based
*Work was done during internship at Tencent AI Lab,
Bellevue.
/githubhttps://github.com/chentong0/
factoid-wikiQuestion: What is the angle of the Tower of Pisa?
Passage
RetrievalPrior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but thetower now
leans atabout 3.99 degrees. This means
the top of the Leaning Tower of Pisa is dis-
placed horizontally 3.9 meters (12 ft 10 in)
from the center.
Sentence
RetrievalPrior to restoration work performed be-
tween 1990 and 2001, the tower leaned at
an angle of 5.5 degrees, but thetower now
leans atabout 3.99 degrees.
Proposition
RetrievalThe Lean ingTower ofPisa now leans at
about 3.99 degrees.
Contriever GTR3545556575Recall@5 (%)43.065.2
47.366.7
52.768.0Passage Retrieval
Contriever GTR2530354045EM@500 (%)34.138.5
36.240.1
37.341.3Open-domain QAPassage Sentence Proposition
Figure 1: ( Top) An example of three granularities of
retrieval units of Wikipedia text when using dense re-
trieval. ( Bottom ) We observe that retrieving by proposi-
tions yields the best retrieval performance in both pas-
sage retrieval task and downstream open-domain QA
task, e.g. with Contriever (Izacard et al., 2022) or GTR
(Ni et al., 2022) as the backbone retriever. High light in-
dicates the part that contains the answer to the question.
on how the dense retrieval model is instantiated
or trained (Lewis et al., 2020; Lee et al., 2021a;
Santhanam et al., 2022; Ni et al., 2022).
In this paper, we investigate an overlooked re-
search question with dense retrieval inference – at
what retrieval granularity should we segment and
index the retrieval corpus? We aim to investigate
this question in two aspects.
•First, we examine how the granularity of the
index affects passage retrieval performance.
•Second, we investigate whether fine-grained units
1arXiv:2312.06648v3  [cs.CL]  4 Oct 20241. Prior to restoration work performed between 1990 and 2001, the Leaning Tower of Pisa leaned at an angle of 5.5 degrees.2. The Leaning Tower of Pisa now leans at about 3.99 degrees.3. The top of the Leaning Tower of Pisa is displaced horizontally 3.9 meters (12 ft 10 in) from the center.Prior to restoration work performed between 1990 and 2001, the tower leaned at an angle of 5.5 degrees , // but the tower now leans at about 3.99 degrees. // This means the top of the Learning Tower of Pisa is displaced horizontally 3.9 meters (12 ft 10 in) from the center.WIkipedia
FactoidWiki
?
✓AnswerQueryQA ModelRetrieval Units
Retrieval Units
Passage RetrievalBC
DPropositionizer
?QueryCorpus
PassagesSentencesPropositionsRetrieval UnitsRetrieverAFigure 2: We discover that segmenting and indexing a retrieval corpus on the proposition level can be a simple yet
effective strategy to increase dense retrievers’ generalization performance at inference time (A, B) . We empirically
compare the retrieval and downstream open-domain QA task performance when dense retrievers work with
Wikipedia indexed at the level of 100-word passages, sentences, or propositions (C, D) .
can replace passages in downstream QA tasks.
Based on our empirical experiments, we discover
that selecting the proper retrieval granularity at in-
ference time can be a simple yet effective strategy
for improving dense retrievers’ retrieval and down-
stream QA performance. We illustrate our intuition
with an example of open-domain QA in Table 1.
The example shows retrieved text by the same re-
triever at three different granularities. The pas-
sage, which represents a coarser retrieval unit with
a longer context, is theoretically able to provide
more relevant information for the question. How-
ever, a passage often includes extraneous details
(e.g., restoration period and horizontal displace-
ment in the example of Table 1) that could poten-
tially distract both the retriever and the language
model in downstream tasks (Shi et al., 2023; Yu
et al., 2023b). On the other hand, sentence -level in-
dexing provides a finer-grained approach but does
not entirely address the issue (Akkalyoncu Yilmaz
et al., 2019; Yang et al., 2020). This is because
sentences can still be complex and compounded,
and they are often not self-contained, lacking nec-
essary contextual information (e.g., in the example
of Table 1, “the tower” is the coreference of “Pisa
Tower”) for judging the query-document relevance.
To address these shortcomings of typical re-
trieval units such as passages or sentences, we pro-
pose using proposition as a novel retrieval unit for
dense retrieval. Propositions are defined as atomic
expressions within text, where each encapsulates
a distinct factoid and is presented in a concise,
self-contained natural language format. We show
an example proposition in Table 1. The proposi-
tion describes the information regarding the Towerof Pisa’s current leaning angle in a self-contained
way and precisely responds to what the question
is querying. We provide a more detailed definition
and description of proposition in §2. To validate
the efficacy of using proposition as a retrieval unit
for dense retrievers inference, we first process and
index an English Wikipedia dump with all docu-
ments segmented into propositions, which we refer
to as F ACTOID WIKI.
We conduct experiments on five different open-
domain QA datasets and empirically compare the
performance of four dual-encoder retrievers when
Wikipedia is indexed by passages, sentences, and
our proposed propositions. Notably, our findings in-
dicate that proposition-based retrieval outperforms
sentence and passage-based retrieval, especially in
terms of generalization, as discussed in §5. This
suggests that propositions, being both compact and
rich in context, enable dense retrievers to access
precise information while maintaining adequate
context. The average improvement over passage-
based retrieval of Recall@20 is +10.1 on unsuper-
vised dense retrievers and +2.7 on supervised re-
trievers, even though these retrievers were directly
trained on passage-level retrieval. Furthermore,
we observe a distinct advantage of proposition-
based retrieval in downstream QA performance
when using retrieval-augmented language models,
as elaborated in §6. Retrieval by finer-grained units
inherently provides a higher density of question-
relevant information. This finding implies using
finer-grained units in the prompts achieves the same
performance with a shorter input length, and hence,
a faster inference time.
Our main contributions are:
2•We provide a systemic study on how retrieval
granularity impacts retrieval and downstream
task performance. We observe that the retrieval
units have a significant impact on performance.
•We introduce FACTOID WIKI, a processed En-
glish Wikipedia dump, where each page is seg-
mented into multiple granularities: passages, sen-
tences, and our proposed propositions.
•We propose retrieval by proposition as an alter-
native strategy, which achieves better retrieval
and QA accuracy and generalization performance
(with unsupervised retriever), compared to pas-
sage or sentence as retrieval unit.
2 Proposition as a Retrieval Unit
The goal of our study is to understand how the gran-
ularity of a retrieval corpus influences the dense
retrieval models’ performance empirically. Aside
from commonly-used retrieval units such as 100-
word passages (Karpukhin et al., 2020) or sen-
tences, we propose using proposition as an alterna-
tive retrieval unit choice. Here, propositions repre-
sent atomic expressions of meanings in text (Min
et al., 2023) with three defining principles below.
1.Each proposition should correspond to a distinct
piece of meaning in text, where the composition
of all propositions would represent the seman-
tics of the entire text.
2.A proposition should be minimal , i.e. it cannot
be further split into separate propositions.
3.A proposition should be contextualized and self-
contained (Choi et al., 2021). A proposition
should include all the necessary context from the
text (e.g. coreference) to interpret its meaning.
The use of proposition as a retrieval unit is in-
spired by a recent line of work (Min et al., 2023;
Kamoi et al., 2023; Chen et al., 2023a,b), which
finds success in representing and evaluating text
semantics at the level of propositions. We demon-
strate the concept of proposition and how a passage
can be split into a set of propositions by an example
on the left side of Figure 2. The passage contains
three propositions, each of which corresponds to
a distinct factoid about the Leaning Tower of Pisa :
the angle before the restoration, the current angle,
and the horizontal displacement.
Within each proposition, necessary context from
the passage is incorporated so that the meaning of
the proposition can be interpreted independently of# units Avg. # words
Passages 41,393,528 58.5
Sentences 114,219,127 21.0
Propositions 256,885,003 11.2
Table 1: Statistics of text units in the English Wikipedia.
the original text, e.g. the reference of the tower is
resolved into its full mention, the Leaning Tower
of Pisa , in the first proposition. We expect each
proposition to describe exactly one atomic fact, and
so our intuition is that propositions would suitably
work as a retrieval unit for information-seeking
questions.
3 F ACTOID WIKI: Proposition-Level
Index and Retrieval for Wikipedia
We empirically compare passages, sentences, and
propositions as retrieval units on Wikipedia, a
commonly-used retrieval source for knowledge-
intensive NLP tasks (Petroni et al., 2021). To allow
a fair comparison across granularities, we process
an English Wikipedia dump from 2021-10-13, as
used by Bohnet et al. (2022). We segment each doc-
ument text into three different granularities: pas-
sages, sentences, and propositions. We include the
details on passage- and sentence-level segmenta-
tion of the corpus in Appendix A.
Parsing Passage to Propositions. To segment
the Wikipedia pages into propositions, we finetune
a text generation model, which we refer to as the
Propositionizer . The Propositionizer takes a pas-
sage as input and generates the list of propositions
within the passage.
Following Chen et al. (2023b), we train the
Propositionizer with a two-step distillation process.
We first prompt GPT-4 (OpenAI, 2023) with an
instruction containing the proposition definition
and 1-shot demonstration. We include the details
of the prompt in Figure 8. We start with a set of
42k passages and use GPT-4 to generate the seed
set of paragraph-to-proposition pairs. Next, we
use the seed set to finetune a Flan-T5-large model
(Chung et al., 2022). We refer to the processed
corpus as FACTOID WIKI. The statistics of FAC-
TOID WIKIare shown in Table 1.
Quality Analysis. We conduct a manual error
analysis to understand the quality of propositions
generated by GPT-4 and the Propositionizer. While
there does not exist a fixed standard on deciding
3GPT-4 Propositionizer
Not Faithful 0.7% (3/408) 1.3% (6/445)
Not Minimal 2.9% (12/408) 2.0% (9/445)
Not Stand-alone 4.9% (20/408) 3.1% (14/445)
Table 2: Frequency of errors occurred in the generated
propositions. Most generated propositions are faithful,
while a small portion of them are not stand-alone.
a ground truth set of propositions for a passage,
we estimate the frequency of error cases where
(1) a proposition is not fully supported by the pas-
sage, (2) a proposition can be further split into
separate propositions, and (3) propositions are not
self-contained, respectively (Table 2). On a random
sample of 50 passages, we observe that almost all
propositions generated by both models are faithful,
while a small portion of the propositions are not
stand-alone.
4 Experimental Settings
To evaluate the impact of the three retrieval unit
choices, we conduct experiments on five differ-
ent open-domain QA datasets with FACTOID WIKI.
With each dataset, we evaluate both passage re-
trieval and downstream QA performance when
dense retrievers work with Wikipedia indexed in
different granularities.
4.1 Open-Domain QA Datasets
We experiment on five different open-domain QA
datasets with Wikipedia as the retrieval source: Nat-
ural Questions (NQ, Kwiatkowski et al., 2019),
TriviaQA (TQA, Joshi et al., 2017), Web Ques-
tions (WebQ, Berant et al., 2013), SQuAD (Ra-
jpurkar et al., 2016), and Entity Questions (EQ,
Sciavolino et al., 2021).
4.2 Dense Retrieval Models
We compare the performance of the four following
supervised or unsupervised dense retriever mod-
els. Here, supervised models refer to ones that
have used human-labeled query-passage pairs as
supervision during training, and vice versa.
•SimCSE (Gao et al., 2021) is a BERT-base (De-
vlin et al., 2019) encoder trained on unlabeled
sentences sampled randomly from Wikipedia.
SimCSE can be transferred to use as an unsu-
pervised retriever (Chen et al., 2023b).
•Contriever (Izacard et al., 2022) is an unsuper-
vised retriever, instantiated with a BERT-baseencoder. Contriever is contrastively trained by
segment pairs constructed from unlabeled docu-
ments from Wikipedia and web crawl data.
•DPR (Karpukhin et al., 2020) is a dual-encoder
BERT-base model fine-tuned on passage retrieval
tasks directly using the question-passage pair la-
bels from NQ, TQA, WebQ and SQuAD.
•GTR (Ni et al., 2022) is a T5-base encoder (Raf-
fel et al., 2020) pretrained on online forum QA
data, and fine-tuned with question-passage pair
labels on MS MARCO (Nguyen et al., 2016) and
NQ datasets.
4.3 Passage Retrieval Evaluation
We evaluate the retrieval performance at the pas-
sage level when the corpus is indexed at the pas-
sage, sentence, or proposition level respectively.
For sentence and proposition level retrieval, we
follow the setting introduced in Lee et al. (2021b),
where the score of the passage is based on the max-
imum similarity score between the query and all
sentences or propositions in a passage. In practice,
we first retrieve a slightly larger number of text
units, then map each unit to the source passage,
and eventually return the top- kunique passages.
We use Passage Recall@ kas our evaluation metric,
which is defined as the percentage of questions for
which the correct answer is found within the top- k
retrieved passages.
To further understand how different retrieved
passages affect the downstream QA. We use Fusion-
in-Decoder (FiD, Izacard and Grave, 2021) model
to extract answers from retrieved passages. We use
a T5-large sized FiD model trained on NQ dataset
in our experiments. The exact match (EM) score
computes the percentage of questions for which the
predicted answer exactly matches the ground truth.
4.4 Open-domain QA Evaluation on
Retrieval-Augmented Language Models
Another aspect of the choice of granularity lies
in what units should be used in the prompt for
retrieval-augmented language models. For large
language models, retrieval-augmented generation
is achieved by prepending retrieved units to user in-
struction and taking them as the input for language
models. We aim to understand the implications of
using retrieved units of different granularity within
the same computational budget at inference time.
To fairly compare using different granularity in the
4Retriever GranularityNQ TQA WebQ SQuAD EQ Avg.
R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20 R@5 R@20
Unsupervised Dense Retrievers
SimCSE Passage 28.8 44.3 44.9 59.4 39.8 56.0 29.5 45.5 28.4 40.3 34.3 49.1
Sentence 35.5 53.1 50.5 64.3 45.3 64.1 37.1 52.3 36.3 50.1 40.9 56.8
Proposition 41.1 58.9 52.4 66.5 50.0 66.8 38.7 53.9 49.5 62.2 46.3 61.7
Contriever Passage 42.5 63.8 58.1 73.7 37.1 60.6 40.8 59.8 36.3 56.3 43.0 62.8
Sentence 46.4 66.8 60.6 75.7 41.7 63.1 45.1 63.5 42.7 61.3 47.3 66.1
Proposition 50.1 70.0 65.1 77.9 45.9 66.8 50.7 67.7 51.7 70.1 52.7 70.5
Supervised Dense Retrievers
DPR Passage 66.0 78.0 71.6 80.2 62.9 74.9 38.3 53.9 47.5 60.4 57.3 69.5
Sentence 66.0 78.0 71.8 80.5 64.1 74.4 40.3 55.9 53.7 66.0 59.2 71.0
Proposition 65.4 77.7 70.7 79.6 62.8 75.1 41.4 57.2 59.4 71.3 59.9 72.2
GTR Passage 66.3 78.4 70.1 79.4 63.3 76.5 54.4 68.1 71.7 80.5 65.2 76.6
Sentence 66.4 79.4 71.6 80.9 62.2 76.8 60.9 73.4 72.5 81.3 66.7 78.4
Proposition 66.5 79.6 72.2 80.9 63.2 77.4 63.3 75.0 74.9 83.0 68.0 79.2
Table 3: Passage retrieval performance (Recall@ k= 5, 20) on five different open-domain QA datasets when
pre-trained dense retrievers work with the three different granularity from the retrieval corpus. Underline denotes
cases where the training split of the target dataset was included in the training data of the dense retriever.
prompts under the same computation budget, we
set a token length limit for retrieved units.
For this reason, we follow an evaluation setup
where the maximum number of retrieved tokens
is capped at l= 100 or500, i.e. only the top
ltokens from passage, sentence, or proposition
level retrieval are fed into the language model as
input. We evaluate the percentage of questions for
which the predicted answer exactly matches (EM)
the ground truth. We denote our metric as EM @
ltokens. We use LLaMA-2-7B (Touvron et al.,
2023) in our evaluation. To ensure the model’s out-
put aligns with the format of each dataset, we em-
ploy in-context learning, incorporating four-shot
demonstrations as illustrated in Figure 9.
5 How Does Granularity Influence
Passage Retrieval?
In this section, we report and discuss how index-
ing the corpus at various granularity influences the
passage retrieval performance. Surprisingly, de-
spite all of the dense retrieval models being trained
on only passage-level documents, all the models
demonstrate on-par or superior performance when
the corpus is indexed at the proposition level. Our
results suggest that indexing the corpus at the finer-
grained units improves the cross-task generaliza-
tion on passage retrieval.
5.1 Passage Retrieval Performance
We report our evaluation results in Table 3. We
observe that retrieval by propositions outperformsretrieval by sentences or passages on most tasks for
both unsupervised and supervised retrievers.
With all dense retrievers tested, proposition-
level retrieval consistently outperforms sentence
and passage-level retrieval on average across the
five datasets. With the unsupervised retrievers, i.e.
SimCSE and Contriever, we see an averaged Re-
call@5 improvement of +12.0and+9.3(35.0%
and 22.5% relative improvement) on five datasets.
With the supervised retrievers, proposition-level
retrieval still shows an advantage on average, yet
the sizes of improvements are smaller. We hypothe-
size that this is due to these retrievers being trained
on query-passage pairs. For instance, with DPR,
which have been trained on NQ, TQA, WebQ, and
SQuAD, we observe that proposition and sentence
level retrieval perform slightly worse compared to
passage level on three out of the four datasets, with
the exception of SQuAD. As shown in Table 3, all
supervised retrievers demonstrate comparable per-
formance across three levels of retrieval granularity
in NQ, TQA, and WebQ.
However, on datasets that the retriever model has
notseen during training, we observe that retrieval
by proposition demonstrates a clear advantage. For
instance, most notably on SQuAD or EntityQues-
tions, we observe that proposition-based retrieval
significantly outperforms the other two granulari-
ties. We see 25% Recall@5 relative improvement
on EntityQuestions with relatively weak retrievers
like DPR. Furthermore, the Recall@5 of retrieval
by proposition on SQuAD improved most on GTR,
5101102103
Frequency204060Recall@5
SimCSE
101102103
Frequency204060Recall@5
Contriever
101102103
Frequency405060Recall@5
DPR
101102103
Frequency50607080Recall@5
GTRPassage Sentence PropositionFigure 3: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions
dataset. The frequency of each entity (i.e. smaller value ⇒less common entities, and vice versa) is estimated by
the frequency of the entity in its top-1000 passage retrieved by BM25. On queries with less common entities, we
observe that retrieving by proposition shows a larger advantage over retrieval by proposition.
Retriever GranularityNQ TQA WebQ SQuAD EQ Avg.
top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20 top-5 top-20
Unsupervised Dense Retrievers
SimCSE Passage 16.6 23.6 32.3 40.8 15.5 19.1 14.6 20.7 16.1 20.3 19.0 24.9
Sentence 20.7 28.1 36.0 44.5 18.5 21.9 19.6 25.8 19.9 25.1 23.0 29.1
Proposition 24.5 33.1 37.5 46.2 19.7 23.0 21.4 27.6 26.8 32.0 26.0 32.4
Contriever Passage 23.2 35.1 40.8 50.8 16.3 22.1 23.9 32.7 20.2 27.9 24.9 33.7
Sentence 26.0 36.8 43.4 52.9 18.4 23.9 26.7 34.7 23.7 30.3 27.6 35.7
Proposition 28.9 39.2 47.2 55.6 19.5 25.2 30.8 37.6 28.8 35.8 31.1 38.7
Supervised Dense Retrievers
DPR Passage 41.1 45.6 50.6 57.0 23.7 25.5 18.8 25.4 25.3 29.7 31.9 36.6
Sentence 40.3 45.6 51.7 57.6 24.0 26.9 21.1 27.4 28.6 32.9 33.1 38.1
Proposition 39.7 45.2 51.0 56.8 24.3 27.5 22.2 28.3 32.0 36.0 33.9 38.8
GTR Passage 39.8 46.1 49.7 55.9 23.0 25.9 29.9 35.1 37.8 39.6 36.0 40.5
Sentence 39.4 45.9 51.7 58.0 23.2 26.1 35.7 39.1 38.0 39.9 37.6 41.8
Proposition 40.0 46.9 52.5 58.4 24.2 26.5 37.8 40.4 39.2 41.0 38.7 42.6
Table 4: Open-domain QA performance (Exact Match) using Fusion-in-Decoder model (Izacard and Grave, 2021)
to extract answer from top-5 and top-20 passages retrieved on the index of passages, sentences, and propositions.
with 16% relative improvements.
5.2 Retrieval on Finer-grained Index ⇒
Better Cross-Task Generalization
Our results show the advantage of retrieval on
proposition-level index in cross-task generalization
settings. We observe that on SQuAD and Entity
Questions, retrieval on the proposition-level index
brings more performance gain over the passage-
level index and sentence-level index.
To better understand where the improvements
can be attributed, we conduct an additional analysis
on Entity Questions. As Entity Questions features
questions targeting the properties of longer-tail enti-
ties, we study how the retrieval performance under
three different granularities is affected by the occu-
rance of the target entity in question, i.e. whether
the entity appears frequently in Wikipedia or not.
We estimate the frequency of each entity with the
following method. Given the surface form of an en-
tity, we use BM25 to retrieve the top 1000 relevant
passages from Wikipedia. We use the number ofoccurrences of the entity in its relevant passages as
an estimate of its frequency. With the 20,000 test
queries, around 25% of the target entities have an
frequency value of less or equal to 3.
Figure 3 shows the passage retrieval perfor-
mance vs. the frequency of the target entity in
each question. Across all four dense retrievers,
we observe that retrieving by proposition shows a
much larger advantage over retrieving by passages
with questions targeting less common entities. As
the frequency of entities increases, the performance
gap decreases. Our findings indicate that the per-
formance gain from retrieval by proposition can
mostly be attributed to queries for long-tailed infor-
mation. This echoes our observation that retrieval
on proposition-level index improves the cross-task
generalization performance of dense retrievers.
5.3 Higher Passage Recall ⇒Higher
Downstream QA Accuracy
To further understand whether the passage retrieval
on a finer-grained index achieves higher down-
6stream QA performance, we extract the answer
from the retrieved passage by a QA reader, Fusion-
in-decoder. The results are shown in Table 4.
Retrieval by proposition-level index achieves the
highest average exact match (EM) on all four re-
triever models. Apart from limited exceptions, the
proposition-level index achieves the highest EM
for most retrieval tasks and on most datasets. We
observe that the trend of downstream QA perfor-
mance is highly consistent with passage retrieval
recall, suggesting higher passage recall implies bet-
ter downstream QA performance.
6 How Does Granularity Influence
Retrieval-Augmented LMs?
In this section, we study how the choice of different
granularity used in the prompts affects the retrieval-
augmented generation across open-domain QA
tasks. To fairly compare different granularity with
the same computation budget, we limit the num-
ber of retrieved tokens for input to the language
model at l= 100 or500tokens. Our results sug-
gest that retrieval by finer-grained units enables a
higher density of question-related information in
the prompts, leading to better performance.
6.1 Open-domain QA Performance
Table 5 shows the evaluation results with LLaMA-
2-7B as the language model. Across different re-
trievers, we observe higher QA performance in
terms of the EM@ lmetric on average when using
propositions as the retrieval unit.
Using propositions rather than passages in the
prompts, the four dense retrievers—SimCSE, Con-
Retriever, DPR, and GTR—improve by +4.1, +3.2,
+2.7, and +2.8 in the EM@500 score. The improve-
ments for using sentences over passages for the
four retrieval models are +2.4, +2.1, +2, and +1.6,
respectively. It is interesting to note that in the
LLaMA-2-7B model, the QA accuracy on TQA
and WebQ is not sensitive to retrieval type. The
highest improvements over the closed-book setting
are only +4.9 and +3.2, achieved by GTR with
propositions. Nevertheless, we observe that using
sentences and propositions in the prompts results
in higher performance than using passages for all
retrieval models on these two datasets. The results
suggest that using finer-grained units in the prompts
is beneficial to retrieval-augmented generation.6.2 Finer-grained Granularity ⇒Higher
Density of Question-Related Information
Intuitively, compared to sentences or passages as
retrieval units, the advantage of propositions is that
the retrieved propositions have a higher density
of relevant information to the query. With finer-
grained retrieval units, the correct answer to the
query would more likely appear in the top- lre-
trieved words by a dense retriever.
We illustrate this phenomenon by an analysis
shown in Figure 4. Here, we investigate the posi-
tion at which the ground truth answer appears in
the top- lretrieved words. Specifically, we calcu-
late the recall of the gold answer within the initial l
retrieved words with GTR working with Wikipedia
indexed in three different granularities.
We show the results in Figure 4 and 7 with l
ranging from 0 to 500 across all five datasets. For
a fixed word retrieval budget, proposition retrieval
shows a higher success rate than sentence and pas-
sage retrieval methods. The largest improvement of
proposition retrieval over passage retrieval occurs
within the range of 100-200 words, which corre-
sponds to roughly 10 propositions, 5 sentences, or
2 passages. As word count increases, the recall rate
of the three granularities converges, encompassing
all relevant information.
7 Related Work
Recent works on dense retrievers typically adopt
a dual-encoder architecture (Yih et al., 2011;
Reimers and Gurevych, 2019; Karpukhin et al.,
2020; Ni et al., 2022). With dual-encoders,
each query and document is encoded into a low-
dimensional feature vector respectively, and their
relevance is measured by a non-parametric similar-
ity function between the embedding vectors (Muss-
mann and Ermon, 2016). Due to the limited expres-
sivity from the similarity function, dual encoder
models often generalize poorly to new tasks with
scarce training data (Thakur et al., 2021). Previous
studies use techniques such as data augmentation
(Wang et al., 2022; Yu et al., 2023a; Izacard et al.,
2022; Gao and Callan, 2022; Lin et al., 2023; Dai
et al., 2023), continual pre-training (Chang et al.,
2020; Sachan et al., 2021; Oguz et al., 2022), task-
aware training (Xin et al., 2022; Cheng et al., 2023),
hybrid sparse-dense retrieval (Luan et al., 2021;
Chen et al., 2022), or mixed strategy retrieval (Ma
et al., 2022, 2023) and so on to improve cross-task
generalization performance of dense retrievers.
7Retriever GranularityNQ TQA WebQ SQuAD EQ Avg.
EM EM EM EM EM EM
@100 @500 @100 @500 @100 @500 @100 @500 @100 @500 @100 @500
Closed-book 23.4 57.4 25.9 13.0 23.2 28.6
Unsupervised Dense Retrievers
SimCSE Passage 20.5 22.9 49.7 52.9 24.5 24.6 13.7 16.6 20.7 25.5 25.8 28.5
Sentence 21.1 24.3 52.1 54.2 24.2 26.1 17.7 21.5 22.9 28.3 27.6 30.9
Proposition 22.0 26.0 51.0 53.9 23.5 27.0 18.6 22.7 25.9 33.6 28.2 32.6
Contriever Passage 24.5 28.7 54.7 57.9 25.7 26.9 17.7 24.2 25.6 32.5 29.6 34.1
Sentence 25.0 30.2 56.3 59.2 26.8 29.2 22.5 28.1 26.1 34.1 31.3 36.2
Proposition 25.8 30.3 56.8 60.0 26.8 29.9 24.8 29.7 27.1 36.5 32.3 37.3
Supervised Dense Retrievers
DPR Passage 30.6 33.7 56.5 60.3 25.0 26.8 14.2 18.9 26.4 31.6 30.6 34.3
Sentence 32.5 34.1 58.3 61.7 25.4 28.0 17.6 22.1 29.8 35.6 32.7 36.3
Proposition 31.5 33.8 57.6 60.6 27.1 28.2 18.2 22.6 32.9 39.7 33.5 37.0
GTR Passage 30.0 33.9 56.9 60.0 24.5 25.9 21.5 27.4 42.2 45.3 35.0 38.5
Sentence 30.9 34.0 58.9 61.9 24.5 27.0 29.8 31.7 42.9 45.9 37.4 40.1
Proposition 32.1 33.8 58.8 62.3 25.7 29.1 32.5 33.1 43.0 48.1 38.4 41.3
Table 5: Open-domain QA performance (EM = Exact Match) with LLaMA-2-7B model (Touvron et al., 2023). The
context in the prompts is constructed by passage, sentence, or propositions limiting at l= 100 or500tokens. We
prompt the LLaMA-2-7B model with four-shot demonstrations for each test case.
0 200 400
#Words506070Recall (%)GTR / NQ
0 200 400
#Words6070Recall (%)GTR / TQA
0 200 400
#Words506070Recall (%)GTR / WebQ
0 200 400
#Words405060Recall (%)GTR / SQuAD
0 200 400
#Words607080Recall (%)GTR / EQPassage Sentence Proposition
Figure 4: Recall of the gold answer in the retrieved text limited to first kwords for the GTR retriever. Finer-grained
retrieval has a higher recall across all numbers of words.
The motivation of our work echoes in part with
multi-vector retrieval, e.g. ColBERT (Khattab and
Zaharia, 2020), DensePhrase (Lee et al., 2021a,b),
ME-BERT (Luan et al., 2021), and MVR (Zhang
et al., 2022), where the retrieval model learns to
encode a candidate retrieval unit into multiple vec-
tors to increase model expressivity and improve
retrieval granularity (Seo et al., 2019; Humeau
et al., 2019). Our work instead focuses on the
setting where we do not update the dense retriever
model or its parameters. We show that indexing
the retrieval corpus by different granularity can be
a simple and orthogonal strategy for improving the
generalization of dense retrievers at inference time.
In line with generating retrieval units from the
original corpus, Sarthi et al. (2024) propose using
generative summaries as additional retrieval units
alongside the original text, enhancing queries with
document-level understanding. In contrast, our
work generates propositions to improve queries
related to long-tailed entities. These approaches arecomplementary, as they address different aspects
of retrieval enhancement.
The use of propositions as a unit of text rep-
resentation dates back to the Pyramid method in
summarization evaluation (Nenkova and Passon-
neau, 2004), where a model-generated summary
is evaluated by each proposition. Proposition ex-
traction from text has been a long-standing task,
with earlier formulations focusing on a structured
representation of propositions (Etzioni et al., 2008;
Gildea and Jurafsky, 2000). More recent studies
have found success in extracting free-text propo-
sitions via few-shot prompting with LLMs (Min
et al., 2023; Kamoi et al., 2023), or fine-tuning
compact-sized models (Chen et al., 2023b).
Retrieve-then-read , or more broadly retrieval
augmented generation, has recently emerged as
a popular paradigm for open-domain question an-
swering (Lewis et al., 2021; Jiang et al., 2023; Asai
et al., 2023). While earlier works provide up to
the top 100 retrieved passages for the downstream
8reader (Izacard and Grave, 2021; Kedia et al.,
2022), the amount of context allowed is signifi-
cantly reduced when using recent large language
models (Touvron et al., 2023; Yu et al., 2023b), due
to the limited context window length and inability
to reason over long context (Liu et al., 2023). Re-
cent efforts try to improve the quality of the reader
context by filtering or compressing the retrieved
documents (Wang et al., 2023; Xu et al., 2023).
Our work offers a new perspective by changing
the retrieval granularity, in order to achiev greater
information density with a fixed context length.
8 Conclusion
This paper studies how the choice of granularity
for indexing a corpus, as well as the granularity
used in the prompts, influences retrieval and down-
stream QA performance. Our results show that
retrieval by propositions outperforms passage-level
and sentence-level retrieval on passage retrieval
and downstream QA across five open-domain QA
datasets. Our analysis shows that indexing a corpus
with finer-grained units enhances the cross-task
generalization of dense retrievers and increases
the density of question-related information in the
prompts. We hope that FACTOID WIKIand our find-
ings will facilitate future research on information
retrieval and retrieval-augmented generation.
Limitations
The scope of our current study on the granular-
ity of retrieval corpus has the following limita-
tions. (1) Retrieval Corpus – Our study only fo-
cuses on Wikipedia as the retrieval corpus, due to
the fact that most open-domain QA datasets adopt
Wikipedia as the retrieval corpus. (2) Types of
dense retrievers evaluated – In the current version
of the paper, we evaluate 6 types of popular dense
retrievers, most of which follow the bi- or dual-
encoder architecture. In future versions, we will
include and discuss results on a broader range of
dense retrievers. (3) Language – Our current study
is limited to English Wikipedia only. We leave the
exploration on other languages to future work.
Ethical Considerations
This article follows the ACL Code of Ethics. Our
work is a foundational research on information
retrieval. To the best of our knowledge, we do
not find obvious risks related to malicious harmfuleffects, environmental impact, fairness considera-
tions, or privacy considerations.
Acknowledgements
The authors sincerely appreciate anonymous re-
viewers for helpful discussions and comments. The
authors would like to thank Xuanyu Ben Zhou,
Ruixin Hong, Ning Dai, and Linfeng Shen for valu-
able feedback on the project. Xinran Zhao is sup-
ported by the ONR Award N000142312840.
References
Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang,
and Jimmy Lin. 2019. Cross-domain modeling of
sentence-level evidence for document retrieval. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3490–
3496, Hong Kong, China. Association for Computa-
tional Linguistics.
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-rag: Learning to
retrieve, generate, and critique through self-reflection.
Preprint , arXiv:2310.11511.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
conference on empirical methods in natural language
processing , pages 1533–1544.
Bernd Bohnet, Vinh Q Tran, Pat Verga, Roee Aharoni,
Daniel Andor, Livio Baldini Soares, Jacob Eisenstein,
Kuzman Ganchev, Jonathan Herzig, Kai Hui, et al.
2022. Attributed question answering: Evaluation and
modeling for attributed large language models. arXiv
preprint arXiv:2212.08037 .
Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yim-
ing Yang, and Sanjiv Kumar. 2020. Pre-training tasks
for embedding-based large-scale retrieval. In Inter-
national Conference on Learning Representations .
Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, Dan
Roth, and Tal Schuster. 2023a. PropSegmEnt: A
large-scale corpus for proposition-level segmentation
and entailment recognition. In Findings of the As-
sociation for Computational Linguistics: ACL 2023 ,
pages 8874–8893, Toronto, Canada. Association for
Computational Linguistics.
Sihao Chen, Hongming Zhang, Tong Chen, Ben Zhou,
Wenhao Yu, Dian Yu, Baolin Peng, Hongwei Wang,
Dan Roth, and Dong Yu. 2023b. Sub-sentence en-
coder: Contrastive learning of propositional semantic
representations. arXiv preprint arXiv:2311.04335 .
Xilun Chen, Kushal Lakhotia, Barlas Oguz, Anchit
Gupta, Patrick Lewis, Stan Peshterliev, Yashar
9Mehdad, Sonal Gupta, and Wen-tau Yih. 2022.
Salient phrase aware dense retrieval: Can a dense
retriever imitate a sparse one? In Findings of the
Association for Computational Linguistics: EMNLP
2022 , pages 250–262, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng
Gao. 2023. Task-aware specialization for efficient
and robust dense retrieval for open-domain question
answering. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 2: Short Papers) , pages 1864–1875, Toronto,
Canada. Association for Computational Linguistics.
Eunsol Choi, Jennimaria Palomaki, Matthew Lamm,
Tom Kwiatkowski, Dipanjan Das, and Michael
Collins. 2021. Decontextualization: Making sen-
tences stand-alone. Transactions of the Association
for Computational Linguistics , 9:447–461.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .
Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo
Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall,
and Ming-Wei Chang. 2023. Promptagator: Few-
shot dense retrieval from 8 examples. In The Eleventh
International Conference on Learning Representa-
tions .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S Weld. 2008. Open information extrac-
tion from the web. Communications of the ACM ,
51(12):68–74.
Luyu Gao and Jamie Callan. 2022. Unsupervised cor-
pus aware language model pre-training for dense pas-
sage retrieval. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2843–2853,
Dublin, Ireland. Association for Computational Lin-
guistics.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse: Simple contrastive learning of sentence em-
beddings. arXiv preprint arXiv:2104.08821 .
Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics , pages 512–520, Hong Kong. Association
for Computational Linguistics.Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
and Jason Weston. 2019. Poly-encoders: Trans-
former architectures and pre-training strategies for
fast and accurate multi-sentence scoring. arXiv
preprint arXiv:1905.01969 .
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-
tian Riedel, Piotr Bojanowski, Armand Joulin, and
Edouard Grave. 2022. Unsupervised dense informa-
tion retrieval with contrastive learning. Transactions
on Machine Learning Research .
Gautier Izacard and Edouard Grave. 2021. Leveraging
passage retrieval with generative models for open do-
main question answering. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume ,
pages 874–880, Online. Association for Computa-
tional Linguistics.
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie
Callan, and Graham Neubig. 2023. Active retrieval
augmented generation. Preprint , arXiv:2305.06983.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. arXiv preprint arXiv:1705.03551 .
Ryo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and
Greg Durrett. 2023. Wice: Real-world entailment for
claims in wikipedia. In Proceedings of the 2023 Con-
ference on Empirical Methods in Natural Language
Processing .
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6769–6781,
Online. Association for Computational Linguistics.
Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee. 2022.
FiE: Building a global probability space by leverag-
ing early fusion in encoder for open-domain question
answering. In Proceedings of the 2022 Conference
on Empirical Methods in Natural Language Process-
ing, pages 4246–4260, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval , pages 39–
48.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics , 7:453–
466.
10Jaewoong Lee, Heejoon Lee, Hwanhee Lee, and Ky-
omin Jung. 2021a. Learning to select question-
relevant relations for visual question answering. In
Proceedings of the Third Workshop on Multimodal
Artificial Intelligence , pages 87–96, Mexico City,
Mexico. Association for Computational Linguistics.
Jinhyuk Lee, Alexander Wettig, and Danqi Chen. 2021b.
Phrase retrieval learns passage retrieval, too. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 3661–
3672, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems , 33:9459–9474.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2021.
Retrieval-augmented generation for knowledge-
intensive nlp tasks. Preprint , arXiv:2005.11401.
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz,
Jimmy Lin, Yashar Mehdad, Wen-tau Yih, and Xilun
Chen. 2023. How to Train Your DRAGON: Di-
verse Augmentation Towards Generalizable Dense
Retrieval. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing .
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023. Lost in the middle: How language mod-
els use long contexts. Preprint , arXiv:2307.03172.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2020.
Ro{bert}a: A robustly optimized {bert} pretraining
approach.
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and
Michael Collins. 2021. Sparse, dense, and attentional
representations for text retrieval. Transactions of the
Association for Computational Linguistics , 9:329–
345.
Kaixin Ma, Hao Cheng, Xiaodong Liu, Eric Nyberg,
and Jianfeng Gao. 2022. Open-domain question an-
swering via chain of reasoning over heterogeneous
knowledge. In Findings of the Association for Com-
putational Linguistics: EMNLP 2022 , pages 5360–
5374, Abu Dhabi, United Arab Emirates. Association
for Computational Linguistics.
Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric
Nyberg, and Jianfeng Gao. 2023. Chain-of-skills:
A configurable model for open-domain question an-
swering. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 1599–1618, Toronto,
Canada. Association for Computational Linguistics.Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
FActScore: Fine-grained atomic evaluation of factual
precision in long form text generation. In Proceed-
ings of the 2023 Conference on Empirical Methods
in Natural Language Processing .
Sewon Min, Julian Michael, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2020. AmbigQA: Answering am-
biguous open-domain questions. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 5783–
5797, Online. Association for Computational Lin-
guistics.
Stephen Mussmann and Stefano Ermon. 2016. Learning
and inference via maximum inner product search.
InInternational Conference on Machine Learning ,
pages 2587–2596. PMLR.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings of the Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics:
HLT-NAACL 2004 , pages 145–152, Boston, Mas-
sachusetts, USA. Association for Computational Lin-
guistics.
Benjamin Newman, Luca Soldaini, Raymond Fok, Ar-
man Cohan, and Kyle Lo. 2023. A controllable
qa-based framework for decontextualization. arXiv
preprint arXiv:2305.14772 .
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng
Gao, Saurabh Tiwary, Rangan Majumder, and
Li Deng. 2016. MS MARCO: A human gener-
ated machine reading comprehension dataset. CoRR ,
abs/1611.09268.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo
Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,
Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.
Large dual encoders are generalizable retrievers. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pages
9844–9855, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Barlas Oguz, Kushal Lakhotia, Anchit Gupta, Patrick
Lewis, Vladimir Karpukhin, Aleksandra Piktus,
Xilun Chen, Sebastian Riedel, Scott Yih, Sonal
Gupta, and Yashar Mehdad. 2022. Domain-matched
pre-training tasks for dense retrieval. In Findings
of the Association for Computational Linguistics:
NAACL 2022 , pages 1524–1534, Seattle, United
States. Association for Computational Linguistics.
OpenAI. 2023. Gpt-4 technical report. ArXiv ,
abs/2303.08774.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
Vassilis Plachouras, Tim Rocktäschel, and Sebastian
11Riedel. 2021. KILT: a benchmark for knowledge
intensive language tasks. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies , pages 2523–2544, Online.
Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. The Journal of Machine Learning Research ,
21(1):5485–5551.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. Squad: 100,000+ questions
for machine comprehension of text. arXiv preprint
arXiv:1606.05250 .
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Devendra Sachan, Mostofa Patwary, Mohammad
Shoeybi, Neel Kant, Wei Ping, William L. Hamil-
ton, and Bryan Catanzaro. 2021. End-to-end training
of neural retrievers for open-domain question answer-
ing. In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6648–6662, Online. Association for Computational
Linguistics.
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon,
Christopher Potts, and Matei Zaharia. 2022. Col-
BERTv2: Effective and efficient retrieval via
lightweight late interaction. In Proceedings of the
2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 3715–3734, Seat-
tle, United States. Association for Computational
Linguistics.
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh
Khanna, Anna Goldie, and Christopher D Manning.
2024. RAPTOR: Recursive abstractive processing
for tree-organized retrieval. In The Twelfth Interna-
tional Conference on Learning Representations .
Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,
and Danqi Chen. 2021. Simple entity-centric ques-
tions challenge dense retrievers. arXiv preprint
arXiv:2109.08535 .
Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur
Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019.
Real-time open-domain question answering with
dense-sparse phrase index. In Proceedings of the57th Annual Meeting of the Association for Computa-
tional Linguistics , pages 4430–4441, Florence, Italy.
Association for Computational Linguistics.
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan
Scales, David Dohan, Ed H Chi, Nathanael Schärli,
and Denny Zhou. 2023. Large language models can
be easily distracted by irrelevant context. In Inter-
national Conference on Machine Learning , pages
31210–31227. PMLR.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. Beir:
A heterogeneous benchmark for zero-shot evaluation
of information retrieval models. In Thirty-fifth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. Preprint , arXiv:2307.09288.
Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna
Gurevych. 2022. GPL: Generative pseudo labeling
for unsupervised domain adaptation of dense retrieval.
InProceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 2345–2360, Seattle, United States. Association
for Computational Linguistics.
Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan
Parvez, and Graham Neubig. 2023. Learning to filter
context for retrieval-augmented generation. Preprint ,
arXiv:2311.08377.
Ji Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita
Sharma, Damien Jose, and Paul Bennett. 2022. Zero-
shot dense retrieval with momentum adversarial do-
main invariant representations. In Findings of the As-
sociation for Computational Linguistics: ACL 2022 ,
pages 4008–4020, Dublin, Ireland. Association for
Computational Linguistics.
Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.
Recomp: Improving retrieval-augmented lms with
12compression and selective augmentation. Preprint ,
arXiv:2310.04408.
Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo,
Jax Law, Noah Constant, Gustavo Hernandez Abrego,
Steve Yuan, Chris Tar, Yun-Hsuan Sung, et al. 2020.
Multilingual universal sentence encoder for semantic
retrieval. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations , pages 87–94.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning , pages 247–256, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023a. Generate
rather than retrieve: Large language models are
strong context generators. In The Eleventh Inter-
national Conference on Learning Representations .
Wenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin
Ma, Hongwei Wang, and Dong Yu. 2023b. Chain-of-
note: Enhancing robustness in retrieval-augmented
language models. arXiv preprint arXiv:2311.09210 .
Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang,
and Nan Duan. 2022. Multi-view document repre-
sentation learning for open-domain dense retrieval.
InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , pages 5990–6000, Dublin, Ireland.
Association for Computational Linguistics.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. In International
Conference on Learning Representations .A Retrieval Corpus Processing
The English Wikipedia dump used in this study, re-
leased by Bohnet et al., 2022, was selected because
it has been filtered to remove figures, tables, and
lists, and is organized into paragraphs. The dump
dates back to October 13, 2021. We have seg-
mented Wikipedia into three retrieval units for this
study: 100-word passage chunks, sentences, and
propositions. Paragraphs are divided into 100-word
passage chunks using a greedy method. We divide
only at the end of sentences to ensure each passage
chunk contains complete sentences. As we process
the paragraph, we add sentences one by one. If in-
cluding the next sentence causes the passage chunk
to exceed 100 words, we start a new passage chunk
with that sentence. However, if the final passage
chunk is shorter than 50 words, we merge it with
the previous one to avoid overly small segments.
Each passage is further segmented into sentences
using the widely used Python SpaCy1en_cor
e_web_lg model. Additionally, each passage is
decomposed into propositions by our Proposition-
izer model. Decomposing the entire Wikipedia
corpus requires approximately 500 GPU hours on
NVIDIA P100 GPUs using the default implementa-
tion in the transformers2package. We decomposed
6 million pages into 41 million passages, 114 mil-
lion sentences, and 257 million propositions. On
average, a passage contains 6.3 propositions, and a
sentence contains 2.3 propositions.
B Training the Propositionizer
We generated a list of propositions from a given
paragraph using GPT-4 with a prompt, as shown in
Figure 8. After filtering, 42,857 pairs were used to
fine-tune a Flan-T5-Large model. We named the
model Propositionizer. The AdamW optimizer was
used with a batch size of 64, learning rate of 1e-4,
weight decay of 1e-4, and 3 epochs.
To compare the proposition generation perfor-
mance of different models, we set up a development
set and an evaluation metric. The development set
contains an additional 1,000 pairs collected by GPT-
4 using the same approach as the training set. We
evaluated the quality of the predicted propositions
by the F1 score of two sets of propositions. Mo-
tivated by the F1 score of two sets of tokens in
BertScore, we designed the F1 score for two sets of
1https://spacy.io/
2https://huggingface.co/docs/
transformers/en/index
13propositions. Let P={p1, ..., p n}denote the set
of labeled propositions and ˆP={ˆp1, ...,ˆpm}the
set of predicted propositions. We use sim(pi,ˆpj)
to represent the similarity between two proposi-
tions. Theoretically, any text similarity metric can
be used. We chose BertScore (Zhang et al., 2020)
with roberta-large (Liu et al., 2020) configuration
as our simfunction since we wanted our metric
to reflect the semantic difference between proposi-
tions. We define
Recall =1
|P|X
pi∈Pmax
ˆpj∈ˆPsim(pi,ˆpj)
Precision =1
|ˆP|X
ˆpj∈ˆPmax
pi∈Psim(pi,ˆpj)
F1 = 2 ·Precision ·Recall
Precision + Recall
Here is a figurative explanation of the F1 score:
Recall represents the percentage of propositions
in the labeled set that are similar to those in the
generated set, Precision represents the percentage
of propositions in the generated set that are similar
to the labeled set, and F1is the harmonic mean of
Recall andPrecision .F1is 1 if the two sets are
exactly the same, and 0 if any two propositions are
semantically different.
We conducted a comparative analysis of base-
size and large-size Flan-T5 models, which were
trained using varying amounts of data (shown in
Figure 5). Our findings suggest that larger models,
coupled with extensive training data, yield better
results. The Propositionizer presented in this paper
attained an F1 score of 0.822. Upon manually
reviewing the generated propositions, we found
them to be satisfactory.
5000 7500 10000 12500 15000 17500
Number of training samples767778798081F1
flan-t5-base flan-t5-large
Figure 5: Performance of proposition-level decompo-
sition by models with different sizes and number of
training data.C Quality Analysis of Generated
Propositions
We collected propositions generated from 50 ran-
domly selected passages. There are 408 and 445
propositions generated by GPT-4 and Proposition-
izer, respectively. The propositions and passages
were provided to an expert without knowing which
model generated each proposition. The expert an-
notated three scores from different perspectives for
each proposition: (1) whether the proposition is
fully supported by the passage, (2) whether the
proposition is minimal and cannot be further split
into separate propositions, and (3) whether the
proposition is self-contained. The scores range
from 1 to 3, where 1 means "no," 2 means "maybe,"
and 3 means "yes." We report the number of cases
where the annotation was "no." The detailed in-
structions are provided in Table 8.
D Offline Indexing
We used the pyserini andfaiss packages
to encode retrieval units into embeddings. We
exploited multiple GPUs to encode each text
unit in groups of 1M units with a batch size
of 64. After preprocessing the embeddings,
we used an exact search for the inner product
(faiss.IndexFlatIP ) in all experiments. The
plain index of FACTOID WIKIis approximately
768GB in size. To reduce memory pressure, the
embeddings are split into 8 shards. An approximate
nearest neighbor search is conducted per shard be-
fore aggregating all results.
Although the number of propositions is six times
that of passages, using efficient indexing tech-
niques can enable sub-linear search times relative
to the total count of vectors. Moreover, utilizing
GPU parallelism and distributed indexes signifi-
cantly decreases the online search time. As a result,
with proper implementation, we can make propo-
sition retrieval a practically viable and efficient
option.
E Retriever Models and QA Models
We used transformers andsentence-tra
nsformers packages for the model implementa-
tion. We used the following checkpoints released
on HuggingFace: SimCSE ( princeton-nlp/u
nsup-simcse-bert-base-uncased ), Con-
triever ( facebook/contriever ), DPR ( fac
ebook/dpr-ctx_encoder-multiset-ba
se,facebook/dpr-question_encoder-
14multiset-base ), GTR ( sentence-trans
formers/gtr-t5-base ).
We use T5-large size Fusion-in-decoder
model ( nq_reader_large ) released by
the authors in https://github.com/
facebookresearch/FiD . We use Hugging-
Face checkpoint ( meta-llama/Llama-2-7b )
for LLaMA-2-7B.
F Additional Results
In Section 5.2, we demonstrated the advantage
of retrieval by proposition over retrieval by sen-
tence, particularly as the population of the entity
decreases in EQ. We used the occurrence in the
top-1000 paragraphs retrieved by BM25 as a proxy
for frequency, rather than counting the number of
hyperlinks to the entity used in Sciavolino et al.,
2021. Therefore, the trend in the performance ver-
sus frequency plot shows some differences (Fig-
ure 6) between our results and those in Sciavolino
et al., 2021. For example, some entities are am-
biguous (e.g., 1992 , a TV series). In such cases,
the occurrence of the surface form of the entity is
large. Simultaneously, questions related to ambigu-
ous entities are challenging to answer, leading to
lower recall.
In Section 6.2, we discussed the recall of an-
swers in the retrieved text with respect to the con-
text length. We further illustrate the performance
trends of six dense retrievers, as detailed in Fig-
ure 7. The results indicate that the recall rate of
propositions consistently outperforms that of sen-
tences and passages. Our findings lead to the con-
clusion that question-related density is greater in
proposition units compared to sentences and pas-
sages.
G Error Case Study
To understand the source of errors from each type
of retrieval granularity, we present and discuss four
typical examples of mistakes in Table 6 and Table 7.
With each example, we show the question and its
corresponding top-1 retrieved text unit by the GTR
retriever across the three granularities.
We observe that with passage-level retrieval, the
ambiguity of an entity or its references presents a
challenge for dense retrievers, which echoes find-
ings from (Min et al., 2020). For instance, in exam-
ple Q1, the question asks for “ Super Bowl 50 ”, but
the retrieved passage and sentence refers to “ Super
Bowl 5 ”. In Example Q2, passage retrieval failsto identify the part referring to the correct “ atomic
number ”. Instead, the top-1 retrieved passage men-
tions “ atomic number ” in a different and irrelevant
context to the question. Retrieval by sentences can
also have a similar problem as retrieval by passages
like Example Q1. Also, retrieval by sentences faces
another challenge of lacking context. In Example
Q3 (shown in Table 7), sentence-based retrieval
fails as the correct sentence in the retrieved passage
uses “ it” to refer to the pericardial sac.
Retrieval by propositions tackles the aforemen-
tioned problems by ensuring each retrieval unit
contains one piece of fact only and necessary con-
text is incorporated in the propositions. However,
proposition-based retrieval faces challenges with
questions that involve multi-hop reasoning over
long-range textual analysis. In Example Q4 (shown
in Table 7), the retrieved passage separately de-
scribes the actor’s name and the character they por-
tray. There is not a single proposition that entails
both the question and the answer.
15Passage Retrieval Sentence Retrieval Proposition Retrieval
Q1: What was the theme of Super Bowl 50?
Title: Super Bowl X ✗
The overall theme of the Super Bowl enter-
tainment was to celebrate the United States
Bicentennial. Each Cowboys and Steelers
player wore a special patch with the Bicen-
tennial logo on their jerseys...Title: Super Bowl X ✗
The overall theme of the Super Bowl
entertainment was to celebrate the
United States Bicentennial.Title: Super Bowl XLV ✓
... As this was the 50th Super Bowl game,
the league [SuperBowl 50] emphasized
the"golden anniver sary" with various gold-
themed initiatives during the 2015 season, as
well as...
Q2: The atomic number of indium which belongs to 5th period is?
Title: Period 5 element ✗
The periodic table is laid out in rows to illus-
trate recurring (periodic) trends in the chemi-
cal behaviour of the elements as their atomic
number increases: ...Title: Period 5 element ✓
Indium is a chemical element with the
symbol In and atomic number 49.Title: Period 5 element ✓
Indium is a chemical element with the sym-
bol In and [Indium has a] atomic number 49.
This rare, very soft, malleable ...
Table 6: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct
answer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration
purpose only and not provided to the retrievers and downstream QA models.
Passage Retrieval Sentence Retrieval Proposition Retrieval
Q3: What is the function of the pericardial sac?
Title: Pericardium ✓
The pericardium, also called pericardial sac
... It separates the heart from interference of
other structures, protects itagainst infection
andblunt trauma, andlubricates theheart’s
movements.Title: Pericardium ✗
The pericardium, also called pericar-
dial sac, is a double-walled sac con-
taining the heart and the roots of the
great vessels.Title: Cardiac muscle ✓
On the outer aspect of the myocardium is the
epicardium which forms part of thepericar-
dialsacthatsurrounds, protects, andlubri-
cates theheart.
Q4: What is the main character’s name in layer cake?
Title: Layer Cake (film) ✓
... The film’s plot revolves around a London-
based criminal, played by Daniel Craig, ...
Craig’s character is unnamed in the film and
is listed in the credits as "XXXX" .Title: Angelic Layer ✗
The primary protagonist is Misaki
Suzuhara.Title: Plot twist ✗
Sometimes the audience may discover that
the true identity of a character is , in fact,
unknown [in Layer Cake] , as in Layer Cake
or the eponymous assassins in V for Vendetta
and The Day of the Jackal.
Table 7: Example cases where top-1 retrieved text unit of each retrieval granularity fails to provide the correct
answer. The underlined text is the correct answer. The gray text is the context of propositions, but it is for illustration
purpose only and not provided to the retrievers and downstream QA models.
101102
Frequency204060Recall@5
SimCSE
101102
Frequency30405060Recall@5
Contriever
101102
Frequency4060Recall@5
DPR
101102
Frequency7080Recall@5
GTRPassage Sentence Proposition
(a) Where was [X] born?
101102
Frequency204060Recall@5
SimCSE
101102
Frequency204060Recall@5
Contriever
101102
Frequency406080Recall@5
DPR
101102
Frequency708090Recall@5
GTRPassage Sentence Proposition
(b) Who was [X] created by?
Figure 6: Document retrieval recall vs. the frequency of the target entity in each question from the Entity Questions
dataset. We display the performance of two relations.
160 200 400
#Words203040Recall (%)SimCSE / NQ
0 200 400
#Words304050Recall (%)SimCSE / TQA
0 200 400
#Words304050Recall (%)SimCSE / WebQ
0 200 400
#Words203040Recall (%)SimCSE / SQuAD
0 200 400
#Words304050Recall (%)SimCSE / EQPassage Sentence Proposition
0 200 400
#Words20304050Recall (%)Contriever / NQ
0 200 400
#Words40506070Recall (%)Contriever / TQA
0 200 400
#Words20304050Recall (%)Contriever / WebQ
0 200 400
#Words304050Recall (%)Contriever / SQuAD
0 200 400
#Words20304050Recall (%)Contriever / EQPassage Sentence Proposition
0 200 400
#Words506070Recall (%)DPR / NQ
0 200 400
#Words6070Recall (%)DPR / TQA
0 200 400
#Words506070Recall (%)DPR / WebQ
0 200 400
#Words3040Recall (%)DPR / SQuAD
0 200 400
#Words405060Recall (%)DPR / EQPassage Sentence Proposition
0 200 400
#Words506070Recall (%)GTR / NQ
0 200 400
#Words6070Recall (%)GTR / TQA
0 200 400
#Words506070Recall (%)GTR / WebQ
0 200 400
#Words405060Recall (%)GTR / SQuAD
0 200 400
#Words607080Recall (%)GTR / EQPassage Sentence PropositionFigure 7: Recall of the gold answer in the retrieved text limited to first kwords. Finer-grained retrieval has a higher
recall across all numbers of words.
Is the proposition fully supported by the passage? No : The information provided relates to
the proposition, but there are some gaps or inconsistencies that prevent full support. Maybe
: The information provided supports the proposition adequately, covering most aspects well;
however, minor details or implications might not be fully explored or clarified. Yes: The
information provided clearly and comprehensively addresses all aspects of the proposition,
leaving no relevant details unexplained or ambiguous.
Should the given propositions be further split into separate propositions? No : The
proposition has a compound structure that could be separated into distinct propositions. Maybe :
The proposition is mostly straightforward with a single main idea and perhaps a minor additional
detail. Splitting might enhance clarity but is not strictly necessary. Yes: The proposition
is already concise and does not contain a compound structure. Splitting it into separate
propositions would likely reduce clarity.
Is the given proposition self-contained? No : The proposition contains pronouns, terms, or
references whose full names or meanings are not in the proposition. Maybe : The proposition is
almost entirely self-contained, with only a few minor terms that might be ambiguous without
additional context. Yes: The proposition is a self-contained claim without any ambiguities,
fully understandable on its own.
Table 8: Instructions for data annotation in analyzing the quality of generated propositions.
17Passage ⇒Propositions
Decompose the "Content" into clear and simple propositions, ensuring they are interpretable out of
context.
1.Split compound sentence into simple sentences. Maintain the original phrasing from the input
whenever possible.
2.For any named entity that is accompanied by additional descriptive information, separate this
information into its own distinct proposition.
3.Decontextualize the proposition by adding necessary modifier to nouns or entire sentences
and replacing pronouns (e.g., "it", "he", "she", "they", "this", "that") with the full name of the
entities they refer to.
4. Present the results as a list of strings, formatted in JSON.
Input : Title: ¯Eostre. Section: Theories and interpretations, Connection to Easter Hares. Content:
The earliest evidence for the Easter Hare (Osterhase) was recorded in south-west Germany in
1678 by the professor of medicine Georg Franck von Franckenau, but it remained unknown in
other parts of Germany until the 18th century. Scholar Richard Sermon writes that "hares were
frequently seen in gardens in spring, and thus may have served as a convenient explanation for the
origin of the colored eggs hidden there for children. Alternatively, there is a European tradition
that hares laid eggs, since a hare’s scratch or form and a lapwing’s nest look very similar, and
both occur on grassland and are first seen in the spring. In the nineteenth century the influence
of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular throughout Europe.
German immigrants then exported the custom to Britain and America where it evolved into the
Easter Bunny."
Output : [ "The earliest evidence for the Easter Hare was recorded in south-west Germany in
1678 by Georg Franck von Franckenau.", "Georg Franck von Franckenau was a professor of
medicine.", "The evidence for the Easter Hare remained unknown in other parts of Germany until
the 18th century.", "Richard Sermon was a scholar.", "Richard Sermon writes a hypothesis about
the possible explanation for the connection between hares and the tradition during Easter", "Hares
were frequently seen in gardens in spring.", "Hares may have served as a convenient explanation
for the origin of the colored eggs hidden in gardens for children.", "There is a European tradition
that hares laid eggs.", "A hare’s scratch or form and a lapwing’s nest look very similar.", "Both
hares and lapwing’s nests occur on grassland and are first seen in the spring.", "In the nineteenth
century the influence of Easter cards, toys, and books was to make the Easter Hare/Rabbit popular
throughout Europe.", "German immigrants exported the custom of the Easter Hare/Rabbit to
Britain and America.", "The custom of the Easter Hare/Rabbit evolved into the Easter Bunny in
Britain and America." ]
Input : <a new passage >
Output :
Figure 8: Prompt for generating propositions from a passage using GPT-4.
18Open-domain QA for LLaMA-2-7B
... [demonstrations] ...
Refer to the passages below and answer the following question with just a few words.
Title : 1972 in spaceflight. Passage : In 1972, humanity’s last crewed mission to the Moon of the
20th century was Apollo 17.
Title : 1970s. Passage : Apollo 17 Astronaut Gene Cernan becomes the last man on the Moon on
December 13, 1972.
Title : List of Apollo missions
Refer to the context above and answer the following question with just a few words.
Question: when was the last time anyone was on the moon
The answer is
Figure 9: Prompt for retrieval-augmented generation of open-domain QA for the LLaMA-2-7B model.
19