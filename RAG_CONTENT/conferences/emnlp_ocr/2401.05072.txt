Aligning Translation-Specific Understanding to General Understanding in
Large Language Models
Yichong Huangâ€ , Baohang Liâ€ , Xiaocheng Fengâ€ â€¡, Wenshuai Huoâ€ â€¡, Chengpeng Fuâ€ â€¡,
Ting Liuâ€ , Bing Qinâ€ â€¡B
â€ Harbin Institute of Technologyâ€¡Peng Cheng Laboratory
{ychuang,xcfeng,baohangli,cpfu,wshuo,tliu,qinb}@ir.hit.edu.cn
Abstract
Large Language models (LLMs) have exhib-
ited remarkable abilities in understanding com-
plex texts, offering a promising path towards
human-like translation performance. However,
this study reveals the misalignment between
the translation-specific understanding and the
general understanding inside LLMs. This
understanding misalignment leads to LLMs
mistakenly or literally translating some com-
plicated concepts that they accurately com-
prehend in the general scenarios ( e.g., QA).
To align the translation-specific understand-
ing to the general one, we propose a novel
translation process, DUAT ( Difficult words
Understanding Aligned Translation), explic-
itly incorporating the general understanding
on the complicated content incurring incon-
sistent understanding to guide the translation.
Specifically, DUAT performs cross-lingual in-
terpretation for the difficult-to-translate words
and enhances the translation with the gener-
ated interpretations. Furthermore, we reframe
the external tools to improve DUAT in detect-
ing difficult words and generating helpful in-
terpretations. We conduct experiments on the
self-constructed benchmark Challenge-WMT1,
consisting of samples that are prone to mis-
translation. Human evaluation results on high-
resource and low-resource language pairs in-
dicate that DUAT significantly facilitates the
understanding alignment, which improves the
translation quality (up to +3.85 COMET) and
reduces the literality of the translation by - 25%
âˆ¼- 51%.
1 Introduction
Recently, large language models (LLMs) have
demonstrated remarkable language understanding
and generation, paving the way for a higher level
of performance in machine translation (Zhao et al.,
2023; OpenAI, 2023; Jiang et al., 2023; Workshop,
1The dataset is available at: ChallengeWMT
Bmeans corresponding author.
Source Sentenceæ–‡ç« çš„å‰å¦»æ˜¯é©¬ä¼Šçã€‚Reference TranslationThe ex-wife of Wen Zhang is Ma Yili.LLMâ€™s TranslationThe ex-wife of the article is Ma Yili.QuestionIn this Chinese sentence: "æ–‡ç« çš„å‰å¦»æ˜¯é©¬ä¼Šç", whatisthe meaning of "æ–‡ç« "?LLMâ€™s AnswerItrefers to the Chinese actor and singer, Wen Zhang.
(b) LLM misunderstands the word "æ–‡ç« "asthe"creativework"whentranslatingthesentence.(a) LLM correctly understands the meaning of"æ–‡ç« "whenexplainingitsmeaning.
Figure 1: Illustration of the misalignment between
the general understanding (Fig a) and the translation-
specific language understanding (Fig b) inside the LLM
(gpt-3.5-turbo-0125 ). More examples are reported
in Appendix A.
2023). However, existing research reports that
LLMs have yet to achieve as significant advances in
machine translation as they have achieved in other
natural language processing fields (Hendy et al.,
2023; Pang et al., 2024; Zhang et al., 2023a; Jiao
et al., 2023; Zhu et al., 2023b; Lou et al., 2023).
In this study, we discover the misalignment be-
tween the general understanding and translation-
specific understanding inside LLMs, as illustrated
in Fig.1. This understanding misalignment leads to
LLMs mistakenly or literally translating some com-
plicated concepts that they accurately comprehend
in general scenarios. We refer to these failures as
language modelsâ€™ generalization failures on trans-
lation. Human evaluation on a total of 600 sam-
pled sentences across six language pairs show that
generalization failures account for a considerable
proportion of all mistranslations ( 16%-32% ), indi-
cating serious understanding misalignment (Â§6.1).
To align the translation-specific understanding
to the general one, we propose a novel transla-
tion process, DUAT (Difficult words Understand-
ing Aligned Translation), explicitly incorporating
the general understanding on the complicated con-arXiv:2401.05072v2  [cs.CL]  21 Oct 2024tent incurring inconsistent understanding to guide
the translation. Specifically, DUAT first detects the
difficult-to-translate words in the source sentence,
which could cover the generalization failures in-
tuitively. Next, the LLM is prompted to interpret
each difficult word with the target language, i.e.,
cross-lingual interpretation, unleashing the pow-
erful general understanding and transforming this
understanding into the target language space. After
that, DUAT conducts translation under the guid-
ance of these interpretations. Unlike the CoT-based
process mimicking junior translators to sequen-
tially translate all words (Peng et al., 2023), DUAT
works like senior translators to analyze the compli-
cate words, which helps the model deep understand
the source sentence and produces more nuanced
translations. Furthermore, we reframe the external
tool of token-level QE (Rei et al., 2023) to enhance
the detection of difficult words, and design a strat-
egy of interpretation quality control to filter hal-
lucinated interpretations based on sentence-level
QE (Rei et al., 2020).
To better analyze the understanding misalign-
ment, we proposed the Challenge-WMT bench-
mark, which contains more sentences prone to mis-
translation. These sentences were collected from
multi-year WMT datasets and represent difficult
samples that multiple state-of-the-art (SOTA) sys-
tems translate poorly. Human evaluation results
indicate that DUAT significantly facilitates the un-
derstanding alignment, reducing 80% âˆ¼88% of gen-
eralization failures. Moreover, this alignment im-
proves the translation quality, as evidenced by auto-
matic metrics (up to +3.85 COMET), and alleviates
translation literalness by -25% âˆ¼-51%.
2 Background
2.1 LLM-based MT
Considering the translation from source language
Lsto target language Lt, LLM-based machine
translation converts the source sentence xto an in-
struction using a translation-specific template and
generates the translation by feeding the instruction
to the LLM Î¸. To make the LLM better follow
the instruction, the in-context learning (ICL) strat-
egy (Brown et al., 2020; Dong et al., 2023) injects
a few examples/demonstrations of translation into
the instruction, which is shown as:
Request: Please translate the [Ls]sentence into [Lt].
# followed by [NDemonstrations Emt]
Source Sentence: [ Source Sentence x]Formally, the LLM-based MT generates the
translation with ICL as:
Ë†y=argmax P Î¸(Emt, x), (1)
where Emt={(xi, yi)}N
i=1is the demonstrations
set of translation.
2.2 Quality estimation (QE)
QE for machine translation, i.e.,reference-free MT
evaluation, aims to predict the quality of the given
translation only according to the source sentence,
which has shown auspicious correlations with hu-
man judgments (Rei et al., 2020, 2021). Given a
source sentence xand a translation y, QE score is
denoted as Ïˆ(y|x).
Thanks to the recent advance in the interpretabil-
ity of neural MT metrics (Rei et al., 2023), token-
level QE is proposed to score the error degree of
the given translation span by calculating the mis-
alignment of this span against the source sentence.
Given a source sentence xand the candidate trans-
lation ey, token-level QE Ï•(Â·)annotates the error
degree of the specific span wtin the translation,
i.e.,Ï•(wt|ey, x)where wtâˆˆey.
3 Approach: DUAT
In this section, we first introduce our translation
framework DUAT (Â§3.1). Specifically, DUAT con-
sists of three components: difficult word detection
(Â§3.2), cross-lingual interpretation (Â§3.3), and in-
terpretation quality control (Â§3.4). To make the
LLM follow the procedure of each component as
expected, we adopt the in-context learning strategy
and design an automatic method for constructing
demonstrations of DUAT (Â§3.5).
3.1 Framework
The progress of DUAT is illustrated in Fig.2. Given
the source sentence, DUAT first detects the diffi-
cult words or phrases in the source sentence. Once
the difficult words are identified, DUAT requests
the LLM to interpret each difficult word with the
target language, unleashing the powerful under-
standing capability inside the LLM and transform-
ing these understandings into the target language
space. Finally, to avoid the interference of incor-
rect and useless interpretations, DUAT removes
the negative interpretations through the interpreta-
tion quality control and outputs the final translation
guided by the helpful interpretations.â€¢å´©å¡Œ
â€¢åˆ¨å‘â€¢äººè®¾â€¢è§‚å¯Ÿå®¶
â€¢åˆ¨å‘Many  observers  point  out that his 
5-year rule has led to the collapse  
of his public  image,  mainly  due 
to his own "digging  holes "Preliminary Translation
è®¸å¤šè§‚å¯Ÿå®¶æŒ‡å‡ºï¼Œ ä»–æ‰§
æ”¿5å¹´äººè®¾å´©å¡Œï¼Œä¸»è¦
æ˜¯è‡ªå·±â€œåˆ¨å‘â€æ‰€è‡´ã€‚Source Sentence Difficult Words
äººè®¾ sample
difficult
wordsconduct
preliminary
translationdifficulty -
aware
selection
generate 
interpretation
Observers  point  out that the 
collapse  of his public image  after 
five years  in power  was primarily  
caused  by his own deeds .Final TranslationInterpretations in Target 
Language
äººè®¾ :Itrefers  to a person's  public  
image .
å´©å¡Œ :collapse  under  the action  of 
gravity .
åˆ¨å‘ :It indicates that someone's  
actions  have led to their own 
downfall  metaphorically(1)Difficult Word Detection
(2)Cross -Lingual InterpretationHelpful Interpretations
äººè®¾ :Itrefers  to a person's  
public  image .
åˆ¨å‘ :It indicates metaphorically  
that someone's  actions  have led 
to their own downfall(3)Interpretation Quality Controlğ’Ÿ1
ğ’Ÿ2 
ğ’Ÿ3 
ğ’œ1
ğ’œ2
ğ’œ3use ğ’œğ‘– ?
interpretation -guided translation
Translation ğ‘¦ğ‘¤/ğ‘œ Translation ğ‘¦ğ‘¤
QE Score ğ‘ ğ‘¤/ğ‘œ QE Score ğ‘ ğ‘¤ 
delete ğ’œğ‘– & update the best translationğ‘ ğ‘¤/ğ‘œ>ğ‘ ğ‘¤ ?ğ‘ ğ‘Œ 
ğ‘Œ å´©å¡Œ
åˆ¨å‘Figure 2: DUAT framework. The purple spans indicate the difficult-to-translate words, the green spans indicate
the correct translation/interpretation, and the red spans indicate the incorrect ones.
3.2 Step-1: Difficult Word Detection
In practice, we found that directly inquiring LLMs
to identify difficult-to-translate words is challeng-
ing. To tackle this challenge, we first conduct a
preliminary translation for the given source sen-
tence and then extract the mistranslated words
and phrases in the source sentence as the difficult
words . Concretely, we invent DUAT-I to do this
leveraging the Intrinsic ability of LLMs at first.
DUAT-I .Given source sentence x,DUAT-I first
obtains the preliminary translation ey(also known as
draft translation ) by prompting the LLM to trans-
latexwith the in-context learning strategy, which
is shown in Eq. (1). Next, the LLM is requested
to output the difficult words based on the source
sentence and the preliminary translation:
Request: Given a [Ls]sentence and its draft [Lt]trans-
lation, output the mistranslated words and phrases in
the[Ls]sentence.
# followed by [NDemonstrations Ediff]
Source Sentence: [ Given Sentence x]
Draft Translation: [ Draft Translation ey]
DUAT-I obtains the difficult word list Dvia
performing greedy decoding on the LLM:
D=argmax P Î¸(Ediff, x,ey), (2)
where Î¸is the LLM, which is prompted with N
demonstrations of difficult word detection Ediff=
{xi,eyi,Di}N
i=1.DUAT-E .It is frequently observed that the LLM
fails to recognize the mistranslated words, due to
their limitation in self-knowledge (Yin et al., 2023)
Therefore, we devise DUAT-E to boost the de-
tection with the External tool. First, DUAT-E
requests the LLM with the same prompt as DUAT-
Iwhile performing temperature sampling forK
times. Next, the union of all sampling results is
taken as the candidate set of difficult words Dcand:
Dcand=Kâˆª
k=1Dkâˆ¼PÎ¸(Ediff, x,ey, T),(3)
where Tis the hyperparameter of sampling tem-
perature, which is set to 0.5to capture more candi-
dates, and Kis set to 5 empirically.
Finally, DUAT-E annotates each candidate word
with its degree of misalignment with respect to
the draft translation, which reflects the translation-
specific difficulty. To implement this function, we
adopt an external tool of token-level QE Ï•(Â·). As
shown in Â§2.2, token-level QE is originally used to
score the mistranslation degree of the given trans-
lation span with respect to the source sentence, i.e.,
Ï•(wt|ey, x)where wtâˆˆey. Differently, we uti-
lize this tool in a dual manner. That is, we use
Ï•(Â·)to annotate the misalignment degree of the
given source span with respect to the translation,
i.e.,Ï•(ws|x,ey)where wsâˆˆx. Formally, the mis-
alignment score of each difficult word candidate is
calculated as:
Ï•(d) =Ï•(d|x,ey), dâˆˆ Dcand. (4)Then, DUAT-E selects candidates with misalign-
ment score Ï•(d)> Ï„, where Ï„is the hyperparame-
ter named the difficulty threshold. We refer to this
procedure as the difficulty-aware selection in Fig.2.
3.3 Step-2: Cross-Lingual Interpretation
After the difficult words in the source sentence
are detected, DUAT lets the LLM generate the
interpretation of each difficult word via requesting:
Request: Given a [Ls]sentence, provide the concise
interpretation for each difficult word with the [Lt].
# followed by [NDemonstrations Eintp]
Source Sentence: [ Given Sentence x]
Difficult Words: [ Difficult Words D]
Through access to the LLM, the interpretation
setAis obtained:
A=argmax P Î¸(Eintp, x,D), (5)
where Eintp={xi,Di,Ai}N
i=1, which is the
demonstrations of the cross-lingual interpretation.
Prob and cons. Under the guidance of generated
interpretations, DUAT can align the translation-
specific understanding to the general one. However,
LLMs may generate hallucinated interpretations
sometimes ( e.g., the interpretation of " å´©å¡Œ" in
Fig.2), which biases the resulting translation from
the original semantics. Besides, helpless interpreta-
tions that can not provide useful information also
pose a risk of disturbing the translation process.
3.4 Step-3: Interpretation Quality Control
To overcome the potential interference of the gener-
ated negative interpretations, DUAT removes them
through the interpretation quality control ( IQC )
and outputs the final translation guided by the help-
ful interpretations.
Concretely, given a set of interpretations A,
DUAT ablates each interpretation Aisequentially
and uses the remaining interpretations to guide the
translation. The interpretation-guided transla-
tion is implemented in a fashion of refinement :
Request: Given a [Ls]sentence and its draft [Lt]
translation, please revise the translation according to
the interpretations of the difficult words.
# followed by [NDemonstrations Eigt]
Source Sentence: [ Given Sentence x]
Draft Translation: [ Draft Translation ey]
Interpretations of Difficult Words:
[Interpretations A]Formally, the translation is obtained as:
Ë†y=argmax P Î¸(Eigt, x,ey,A), (6)
whereEigt={xi,eyi,Ai,Ë†yi}, which is the demon-
stration set of interpretation-guide translation.
If the better translation performance is achieved
by ablation, which is measured by the QE2tool due
to the unavailable access to the reference transla-
tion, the interpretation Aiis removed from Aand
the current translation is taken as the best transla-
tion. We also detail this process in Alg.1.
3.5 Demonstrations Synthesis for DUAT
To make the LLM follow the procedure of DUAT
as expected, we adopt the ICL strategy. Common
practice constructs demonstrations manually, neces-
sitating human translators proficient in NÃ—(Nâˆ’1)
language pairs for Nlanguages. To overcome this
considerable cost, we devise a method for syn-
thesizing high-quality demonstrations of DUAT
based on parallel data. This process of synthesiz-
ing demonstrations is accomplished in a manner of
post-explanation by asking the LLM to compare
the baseline translation and the reference transla-
tion. We describe this process in Appendix B.2.
4 Testbed: Challenge-WMT
To better analyze the understanding misalignment
problem of LLMs, we propose the benchmark
Challenge-WMT , which contains challenge sen-
tences that are prone to mistranslation. This bench-
mark is constructed by collecting samples that mul-
tiple SOTA systems translate poorly from multi-
year WMT testsets of six language pairs ( Chinese
(zh),Estonian (et), and Icelandic (is) to/from
English (en)). Additionally, we believe that this
dataset could promote future research in under-
standing the limitations of existing MT systems.
We select three SOTA MT systems: Google
Translate, ChatGPT, and NLLB (NLLB Team et al.,
2024). Due to the poor performance of NLLB
in the zh â‡”en translation, we additionally train a
zhâ†”en translation model based on DeltaLM (Ma
et al., 2021) on the parallel corpus from OPUS3.
Next, all of the system translations are scored with
COMET metric, and the Ïof samples with the
lowest score for each system are extracted as its
difficult samples set. We vary the value of Ïacross
different language pairs to ensure an appropriate
2We use wmt21-comet-qe-da as the QE scorer.
3https://opus.nlpl.eu/MethodsEnâ‡’Zh Zh â‡’En En â‡’Et Et â‡’En En â‡’Is Is â‡’En Average
COMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT COMET BLEURT
Existing Systems
Google 74.85 54.95 68.21 52.65 79.11 68.71 78.83 65.46 76.17 59.67 78.70 66.54 75.98 61.33
NLLB 68.77 47.77 60.09 45.14 74.20 63.13 74.35 60.87 69.37 52.08 72.55 59.66 69.89 54.78
GPT-4 76.15 55.37 70.77 58.85 80.25 70.80 77.83 65.48 77.33 61.15 79.39 68.40 76.95 63.34
Baselines
Zero-shot 74.89 54.25 71.27 58.24 80.67 69.10 74.93 61.75 71.17 53.12 76.22 64.05 74.86 60.09
ICL 75.47 55.79 72.22 59.56 80.9 69.93 79.40 66.63 73.19 54.87 77.52 65.81 76.45 62.10
+CoT 73.85 53.42 71.35 57.90 78.03 66.03 76.78 63.97 69.72 50.98 76.55 64.54 74.38 59.47
+Keywords 73.93 55.10 71.22 59.18 78.63 70.01 77.79 66.30 70.33 54.31 74.55 64.40 74.41 61.55
+Topic 75.83 53.60 72.46 57.98 80.98 66.78 79.20 64.64 72.77 51.98 76.49 62.06 76.29 59.51
+SimDems 75.22 55.10 72.20 59.16 81.24 70.42 79.11 66.29 72.70 54.04 76.78 64.43 76.21 61.57
Ours
DUAT-I 76.92 56.16 72.94 59.94 82.92 72.19 79.96 67.05 76.64 57.71 78.45 66.80 77.97 63.31
DUAT-E 77.57 56.86 73.25 60.16 83.07 72.30 80.01 66.91 77.04 57.38 78.70 66.93 78.27 63.42
Table 1: Main results on Challenge-WMT. The bold indicates the highest value. â€˜+SimDemsâ€™ represents the
translation strategy with demonstrations similar to the source sentence. The strategies â€˜+Topicâ€™, â€˜+Keywordsâ€™, and
â€˜+SimDemsâ€™ are proposed in MAPS. The baselines and our approaches are implemented based on GPT-3.5-turbo .
scale for each difficult sample set. Finally, the in-
tersection of all systemsâ€™ difficult sample sets is
taken as the Challenge-WMT testbed. Challenge-
WMTcomprises around 600+ sentence pairs for
each language pair, which is illustrated in Tab.6.
We equally split this dataset into the validation set
and the test set.
We report the translation performance measured
by COMET on Challenge-WMT and the com-
plete WMT set in Fig.8, which shows that the
performance decreases dramatically on Challenge-
WMT (84.5 â‡’73.6 averagely). Next, we conduct a
multi-aspect comparison for Challenge-WMT and
the complete set in Appendix C, and find that the
samples of Challenge-WMT have higher perplex-
ity (214 â‡’252 averagely). This result indicates that
sentences in Challenge-WMT are more complex .
5 Experiments
5.1 Experimental Setup
Comparative Methods. We verify the effective-
ness of our DUAT on the LLM GPT-3.5-turbo
for its promising capability in following compli-
cated instructions. Demonstrations of DUAT are
gained by performing our automatic method (Â§3.5)
on the validation set of Challenge-WMT. We com-
pare DUAT with the following methods:
â€¢Zero-shot , which asks the LLM to translate the
source sentence directly.
â€¢ICL (In-Context Learning), enhancing the trans-
lation with Krandomly selected exemplars fromthe validation set.
â€¢CoT (Wei et al., 2022), encouraging the LLM to
resolve the problem step by step. In this work,
we re-implement CoT by prompting the LLM to
translate the source sentence step by step.
â€¢MAPS (He et al., 2024), incorporating the knowl-
edge of keywords ,topic words , and demonstra-
tions similar to the given source sentence to en-
hance the translation process, respectively.
â€¢Commercial and open-source systems. We
also report the performance of Google Trans-
late, NLLB (in zhâ‡”en translation, we replace
NLLB with our trained MT model based on
DeltaLM), and zero-shot translation based on
GPT4 (GPT-4-turbo ).
ForDUAT and other ICL-based methods, we se-
lectK=8 demonstrations ( i.e.,8-shot) to achieve
a strong baseline performance. More details of
re-implementing the baselines under the few-shot
setting are illustrated in Appendix D.
Metrics. Following previous research of LLM-
based MT (Garcia et al., 2023; Chen et al.,
2023), we adopt COMET (Rei et al., 2020) and
BLEURT (Sellam et al., 2020) as the evaluation
metrics as their high correlations with human judg-
ment than BLEU (Papineni et al., 2002).
5.2 Results on Challenge-WMT
The main results are illustrated in Tab.1. From the
results, we have drawn the following observations:MethodsEnâ‡’Is Is â‡’En
COMET BLEURT COMET BLEURT
Zero-shot 77.33 61.15 79.39 68.40
ICL 80.1 61.99 81.02 70.20
DUAT-E 81.7 64.01 81.21 69.22
Table 2: Results in En â‡”Is translation based on GPT-4.
MethodsEnâ‡’Zh Zh â‡’En
COMET BLEURT COMET BLEURT
WMT22 Best 86.80 â€“ 81.00 â€“
Zero-shot 86.91 72.51 82.55 71.12
ICL 87.10 73.15 82.71 71.38
DUAT-E 87.60 72.41 82.75 71.75
Table 3: Results on the complete WMT2022 testset.
The result of WMT22 Best is reported for comparison.
(1)DUAT achieves significant improvements.
On average, DUAT-E surpasses the baseline
ICL by +1.82 COMET and +1.32 BLEURT,
and improves Zero-shot by +3.41 COMET and
+3.33 BLEURT. In the low-resource translation of
Enâ‡’Is,DUAT-E improves ICL by +3.85 COMET
and Zero-shot by +5.87 COMET. These improve-
ments show that, DUAT largely elicits the trans-
lation ability via aligning the translation-specific
understanding to the general one.
(2)DUAT achieves state-of-the-art performance
on Challenge-WMT. DUAT achieves the high-
est scores in En â‡”Zh and En â‡”Et translation in
terms of COMET and BLEURT. In En â‡”Is trans-
lation, GPT-4 achieves the best results due to its
superior multilingual capabilities. To verify the ef-
fectiveness of DUAT on LLMs with stronger mul-
tilingual capabilities, we implement DUAT based
on GPT-4 in En â‡”Is translation, as shown in Ta-
ble 2. The results suggest that DUAT can also
benefit stronger multilingual LLMs by facilitating
the understanding misalignment.
(3) CoT works poorly in machine translation.
In Table 1, CoT incurs a dramatic performance
drop over the baseline ICL. Our case studies reveal
that CoT produces extremely wordy translations,
which is also observed by Peng et al. (2023). We
conjecture that CoT makes LLMs imitate junior
translators rather than advanced translators.
(4) Difficult words are the bottleneck in trans-
lating complex sentences. The results show that
incorporating the analysis of keywords and top-
ics (He et al., 2024) has yet to gain significantimprovements as DUAT . It suggests that it is the
difficult words that lead to the performance bottle-
neck in translating intricate sentences. We also fol-
low He et al. (2024) to experiment under the rerank
setting as shown in Appendix.E, which shows the
effectiveness of our method further.
(5) The external tool helps LLMs better identify
difficult words. DUAT-I achieves an average im-
provement of +1.52 COMET, demonstrating the
effective performance of LLMs in recognizing dif-
ficult words. And DUAT-E gains a further improve-
ment of +0.3 COMET, showing the effectiveness
of the external token-level QE tool in this task.
5.3 Results on the complete WMT
Experimental results on Challenge-WMT show the
effectiveness of DUAT in translating complex sen-
tences, raising the question of its impact on trans-
lating simple sentences. Therefore, we conduct
additional experiments on the complete WMT2022
testset of Zh â‡”En translation. As the results shown
in Tab.3, our method achieves comparable results
to the baseline ICL in both translation directions.
These results show that DUAT has no negative im-
pact on translating simple sentences.
6 Analysis
6.1 Human Evaluation
To quantitatively analyze the understanding mis-
alignment problem, we employ one senior human
translator for each language direction to assess
generalization failures and translation literalness.
Specifically, in each direction, we randomly sam-
ple 100 sentences4from Challenge-WMT and ask
the senior translator to annotate the mistranslated
words and phrases ( i.e.,mistranslation) and score
the literalness (1 to 5 score) of the translations
generated by the strong baseline ( GPT-3.5-turbo
with ICL). Next, we ask the LLM to generate in-
terpretations of the mistranslated content. These
interpretations are presented to the translator, who
judges whether they contain the correct understand-
ing of the content. If the interpretation is accurate,
the content is annotated as a generalization failure.
Finally, the translator is asked to annotate the gener-
alization failures of DUATand score the literalness
of the translations produced by DUAT.
Analysis of generalization failures. As the re-
sults shown in Tab.4, generalization failures ac-
4It takes 1.4 dollars for annotating one sentence.Enâ‡’Zh Zh â‡’En En â‡’Et Et â‡’En En â‡’Is Is â‡’En
#Mistranslation 19 25 26 53 22 34
#Generalization Failure of Baseline 5 (26%) 4 (16%) 7 (27%) 17 (32%) 5 (23%) 10 (29%)
Translation Literalness of Baseline 4.11 3.53 4.75 4.46 3.76 3.70
Table 4: Human evaluation results.
MethodsEnâ‡’Zh Zh â‡’En
COMET âˆ† COMET âˆ†
DUAT-E 77.57 â€“ 73.23 â€“
w/o. Draft 76.94 -0.63 72.68 -0.55
w/o. IQC 76.54 -1.03 72.91 -0.32
DUAT-I 76.92 â€“ 72.94 â€“
w/o. Draft 76.68 -0.24 72.78 -0.16
w/o. IQC 76.45 -0.47 72.59 -0.35
Table 5: Ablation Study. âˆ†indicates the performance
drop after removing the specific component.
count for a considerable proportion of all mis-
translations (16% âˆ¼32%). Our method (DUAT-E)
largely resolves these failures by 71% âˆ¼88%. We
further study the unresolved failures and find that
most of these unresolved failures are because they
are not identified as difficult words by the LLMs
in the stage of difficult word detection (Eq.3) de-
spite actually hard to translate. It indicates that the
difficult word detection remains an open question.
Analysis of translation literalness. As the re-
sults shown in Tab.4, the baseline translations are
highly biased towards literal translation. DUAT-
significantly reduces the bias towards literal trans-
lation, indicating that the process of interpreting
the difficult words first and then translating aligns
better with sense-for-sense translation.
6.2 Ablation Study
DUAT introduces the processes of (1) draft trans-
lation to precisely detect the difficult words and (2)
IQC to improves the helpfulness of interpretations.
To clearly elucidate the contribution of these two
components, we conduct an ablation study in Ta-
ble 5. Specifically, we analyze the effect of the draft
translation by asking the LLM to detect difficult
words directly without the draft translation. The
impact of IQC is analyzed by evaluating the per-
formance of the generated translations guided by
the original noisy interpretations ( i.e.,without the
processing of IQC). The results show that remov-
ing either component leads to performance drops,
and IQC plays a more important role in DUAT .Specifically, the improvement of DUAT is halved
when ablating the IQC on the En â‡’Zh translation.
6.3 Analysis of Difficult Word Detection
To offer an in-depth insight into the process of dif-
ficult word detection, we illustrate the relation be-
tween the number of difficult words interpreted and
the resulting performance by adjusting the value
of the difficulty threshold ( Ï„), which is shown in
Fig.3. Concretely, a smaller value of Ï„allows more
difficult words to be interpreted. From the results,
we have the following observations:
Increasing the number of interpretations does
not necessarily lead to performance improve-
ments, but increasing high-quality ones can.
Specifically, without controlling the quality of the
interpretations ( i.e.,w/o. IQC), increasing the num-
ber of interpretations (the green lines ) yields un-
predictable performance changes (as shown by the
green bins ), as introducing either valuable infor-
mation or noise. Fortunately, with IQC filtering
negative interpretations, increasing the number of
interpretations (the blue lines ) leads to constant
improvements (as the blue bins show).
Interpreting words that are more difficult
brings larger improvements. Specifically, in the
Enâ‡’Zh translation, decreasing the value of Ï„from
0.19to0.17, the average number of helpful inter-
pretations is increased from 0.23to0.49(+0.26),
and the performance is increased from 76.52to
76.98(+0.46). However, decreasing the value of
Ï„from 0.15to0.10, the average number of help-
ful interpretations is increased from 0.91to1.47
(+0.56), and the performance is increased from
77.17to77.57(+0.40). It should be noted that in-
terpreting more words incurs more inference costs.
Therefore, a modest value of Ï„(i.e.,0.13âˆ¼0.15)
is recommended to reach a compromise between
efficiency and performance of DUAT.
6.4 Analysis of Interpretation Generation
Languages of interpretations. Given a difficult
word, DUAT generates the corresponding interpre-
tation with the target language ( i.e.,cross-lingual72.9172.8372.5672.9172.773.2573.2373.2573.2373.2501234572.272.572.873.173.40.100.140.160.180.20#DifficultWordsCOMETZhâ†’EnCOMET (w/o. IQC)COMET (w. IQC)#Difficult Words (w/o. IQC)#Difficult Words (w. IQC)
76.6776.776.5476.4976.3677.5777.4377.1776.9876.520123476.276.677.077.477.80.100.130.150.170.19#DifficultWordsCOMETDifficultyThreshold(ğœ)Enâ†’ZhFigure 3: Effect of different values of difficulty thresh-
old (Ï„) on DUAT-E.
77.5773.2576.8473.1376.572.9975.4772.22
7072747678Enâ†’ZhZhâ†’EnBaselineSRCSRCâ‡’TGTTGT
Figure 4: Effect of interpretationsâ€™ language for DUAT-
E.
interpretation), which implicitly comprises two
stages: (1) generating the interpretation in the
source language and (2) translating the interpre-
tation into the target language. Compared with con-
ducting these two stages explicitly, DUAT is more
efficient and avoids error accumulation, which is il-
lustrated in Fig.4. As demonstrated, interpretations
in the target language (the blue bins) are more
beneficial than the ones in the source language
(thepurple bins) owing to aligning the general un-
derstanding into the target language space, which
could provide more benefits for translation. And
the implicit two-stage process (the blue bins) is
better than the explicit one (the green bins).
7 Related Work
Evaluation of LLMsâ€™ translation capabilities.
With the remarkable progress of LLMs, researchers
have assessed their translation abilities in various
aspects. Zhang et al. (2023a); Vilar et al. (2023);
Garcia et al. (2023); Bawden and Yvon (2023) first
investigate LLM-based MT in terms of the prompttemplate and examples selection. Next, the eval-
uation is extended across more domains (Hendy
et al., 2023), more languages (Zhu et al., 2023a),
and document-level translation (Hendy et al., 2023;
Wang et al., 2023). Other lines of work have per-
formed in-depth assessments on the important at-
tributes beyond accuracy, like literalness (Raunak
et al., 2023) and culture awareness (Yao et al.,
2023). As existing studies have shown that LLMs
have achieved promising performance, our work
turns out to benchmark them on hard instances to-
wards detecting more underlying issues.
LLM-based translation strategies. Lu et al.
(2023) obtain the multilingual translations of key-
words in the source sentence via the translator
NLLB to augment the LLM, which improves the
translation of low-resource languages while hurting
the performance of high-source languages. Chen
et al. (2023) demonstrate that iterative refinement
reduces translationese significantly. He et al. (2024)
incorporate the knowledge of keywords, topics, and
reference demonstrations to enhance the translation
process, and use a rerank strategy to combine all
candidate translations. However, there is no sig-
nificant improvement to be observed when solely
utilizing each single type of knowledge. Differ-
ent from previous works that utilize the intrinsic
knowledge of LLM, DUAT focuses on dealing with
the difficult-to-translate words instead of the key-
words for the reason that we argue the difficult-to-
translate words lead to the performance bottleneck
due to the long-tail distribution of knowledge.
LLM-based Automatic Post-Editing (APE).
APE corrects the errors in the generated translation,
aiming to bias the translation towards the distri-
bution of the target language (Chen et al., 2023;
Koneru et al., 2024). Differently, our work aims
to leverage the powerful understanding abilities of
LLMs to correct the misunderstanding of compli-
cated concepts in the source sentence. Our work is
in parallel with APE.
8 Conclusion
In this work, we propose a novel translation pro-
cess, DUAT , to take the first step in resolving the
misalignment between the translation-specific un-
derstanding and the general understanding. Further-
more, we utilize the token-level QE tool to enhance
the detection of difficult words and the sentence-
level QE tool to remove harmful interpretations.Human evaluation results on high-resource and low-
resource language pairs indicate that DUAT sig-
nificantly facilitates the understanding alignment,
which improves the translation quality and allevi-
ates translation literalness.
9 Limitations
Even though DUAT elicits the translation abilities
of LLMs via unleashing the general understanding
(intrinsic knowledge) of LLMs, they still struggle
to translate concepts that require the incorporation
of extrinsic knowledge, such as the translation of
neologisms . However, Our approach lays the foun-
dation for researching when and how to incorporate
external knowledge. Besides, DUAT requires to
prompt the LLM for several times, leading to an
increase in latency. This latency is mainly caused
by our interpretation quality control (IQC) strategy,
which sequentially ablates each generated inter-
pretation. Concretely, if |D|difficult words are
identified, IQC needs to prompt the LLM for |D|
times.
Acknowledgements
Bing Qin is the corresponding author of this work.
We thank the anonymous reviewers for their insight-
ful comments. This work was supported by the Na-
tional Natural Science Foundation of China (NSFC)
(U22B2059, grant62276078), the Key R&D Pro-
gram of Heilongjiang via grant 2022ZX01A32,
the International Cooperation Project of PCL,
PCL2022D01, the Nature Scientific Foundation
of Heilongjiang Province(YQ2021F006), and the
Fundamental Research Funds for the Central Uni-
versities (Grant No.HIT.OCEF.2023018).
References
Rachel Bawden and FranÃ§ois Yvon. 2023. Investigating
the translation performance of a large multilingual
language model: the case of BLOOM. In Proceed-
ings of the 24th Annual Conference of the European
Association for Machine Translation , pages 157â€“170,
Tampere, Finland. European Association for Machine
Translation.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877â€“1901. Curran Associates,
Inc.
Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Ken-
neth Heafield. 2023. Iterative translation refinement
with large language models.
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong
Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and
Zhifang Sui. 2023. A survey on in-context learning.
Xavier Garcia, Yamini Bansal, Colin Cherry, George
Foster, Maxim Krikun, Melvin Johnson, and Orhan
Firat. 2023. The unreasonable effectiveness of few-
shot learning for machine translation. In Proceedings
of the 40th International Conference on Machine
Learning , ICMLâ€™23. JMLR.org.
Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng
Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shum-
ing Shi, and Xing Wang. 2024. Exploring Human-
Like Translation Strategy with Large Language Mod-
els. Transactions of the Association for Computa-
tional Linguistics , 12:229â€“246.
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita,
Young Jin Kim, Mohamed Afify, and Hany Hassan
Awadalla. 2023. How good are gpt models at ma-
chine translation? a comprehensive evaluation.
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, LÃ©lio Renard Lavaud,
Marie-Anne Lachaux, Pierre Stock, Teven Le Scao,
Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix,
and William El Sayed. 2023. Mistral 7b.
Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Xing
Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is chat-
gpt a good translator? yes with gpt-4 as the engine.
Sai Koneru, Miriam Exel, Matthias Huck, and Jan
Niehues. 2024. Contextual refinement of translations:
Large language models for sentence and document-
level post-editing. In Proceedings of the 2024 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (Volume 1: Long Papers) , pages
2711â€“2725, Mexico City, Mexico. Association for
Computational Linguistics.
Lianzhang Lou, Xi Yin, Yutao Xie, and Yang Xiang.
2023. CCEval: A representative evaluation bench-
mark for the Chinese-centric multilingual machine
translation. In Findings of the Association for Com-
putational Linguistics: EMNLP 2023 , pages 10176â€“
10184, Singapore. Association for Computational
Linguistics.Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Hao-
ran Yang, Wai Lam, and Furu Wei. 2023. Chain-
of-dictionary prompting elicits translation in large
language models.
Shuming Ma, Li Dong, Shaohan Huang, Dong-
dong Zhang, Alexandre Muzio, Saksham Singhal,
Hany Hassan Awadalla, Xia Song, and Furu Wei.
2021. Deltalm: Encoder-decoder pre-training for
language generation and translation by augmenting
pretrained multilingual encoders.
NLLB Team, Marta R. Costa-jussÃ , James Cross, Onur
Ã‡elebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
Gao, Vedanuj Goswami, Francisco GuzmÃ¡n, Philipp
Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.
2024. Scaling neural machine translation to 200 lan-
guages.
OpenAI. 2023. Gpt-4 technical report.
Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu,
Derek F. Wong, Shuming Shi, and Zhaopeng Tu.
2024. Salute the classic: Revisiting challenges of
machine translation in the age of large language mod-
els.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics , pages 311â€“318, Philadelphia,
Pennsylvania, USA. Association for Computational
Linguistics.
Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen,
Xuebo Liu, Min Zhang, Yuanxin Ouyang, and
Dacheng Tao. 2023. Towards making the most of
ChatGPT for machine translation. In Findings of the
Association for Computational Linguistics: EMNLP
2023 , Singapore. Association for Computational Lin-
guistics.
Vikas Raunak, Arul Menezes, Matt Post, and Hany Has-
san. 2023. Do GPTs produce less literal translations?
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
2: Short Papers) , pages 1041â€“1050, Toronto, Canada.
Association for Computational Linguistics.
Ricardo Rei, Ana C Farinha, Chrysoula Zerva, Daan
van Stigt, Craig Stewart, Pedro Ramos, Taisiya
Glushkova, AndrÃ© F. T. Martins, and Alon Lavie.
2021. Are references really needed? unbabel-IST2021 submission for the metrics shared task. In Pro-
ceedings of the Sixth Conference on Machine Trans-
lation , pages 1030â€“1040, Online. Association for
Computational Linguistics.
Ricardo Rei, Nuno M. Guerreiro, Marcos Treviso, Luisa
Coheur, Alon Lavie, and AndrÃ© Martins. 2023. The
inside story: Towards better understanding of ma-
chine translation neural evaluation metrics. In Pro-
ceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers) , Toronto, Canada. Association for Computa-
tional Linguistics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2685â€“2702, Online. Association
for Computational Linguistics.
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.
BLEURT: Learning robust metrics for text genera-
tion. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
7881â€“7892, Online. Association for Computational
Linguistics.
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
Viresh Ratnakar, and George Foster. 2023. Prompt-
ing PaLM for translation: Assessing strategies and
performance. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 15406â€“
15427, Toronto, Canada. Association for Computa-
tional Linguistics.
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,
Dian Yu, Shuming Shi, and Zhaopeng Tu. 2023.
Document-level machine translation with large lan-
guage models. In Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing , pages 16646â€“16661, Singapore. Association
for Computational Linguistics.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems .
BigScience Workshop. 2023. Bloom: A 176b-
parameter open-access multilingual language model.
Binwei Yao, Ming Jiang, Diyi Yang, and Junjie Hu.
2023. Empowering llm-based machine translation
with cultural awareness.
Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,
Xipeng Qiu, and Xuanjing Huang. 2023. Do large
language models know what they donâ€™t know? In
Findings of the Association for Computational Lin-
guistics: ACL 2023 , pages 8653â€“8665, Toronto,
Canada. Association for Computational Linguistics.Biao Zhang, Barry Haddow, and Alexandra Birch.
2023a. Prompting large language model for machine
translation: A case study. In Proceedings of the
40th International Conference on Machine Learning ,
ICMLâ€™23. JMLR.org.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex
Smola. 2023b. Automatic chain of thought prompt-
ing in large language models. In The Eleventh Inter-
national Conference on Learning Representations .
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen
Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,
Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,
Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A
survey of large language models.
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,
Shujian Huang, Lingpeng Kong, Jiajun Chen, and
Lei Li. 2023a. Multilingual machine translation with
large language models: Empirical results and analy-
sis.
Wenhao Zhu, Yunzhe Lv, Qingxiu Dong, Fei Yuan,
Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun
Chen, and Lei Li. 2023b. Extrapolating large lan-
guage models to non-english by aligning languages.A Generalization Failures on Translation
In this section, we first provide the illustration of
generalization failures on more LLMs, as shown in
Fig.5. As we can see, all of four LLMs accurately
comprehend the complex concept, which three out
of them mistranslate this concept. Then, we also
provide more examples of generalization failures,
as shown in Fig.6 and Fig.7.
B More details of DUAT
B.1 Details of IQC
We gives a formal description of our interpretation
quality control in Alg.1.
B.2 Details of Demonstration Synthesis
Inspired by the idea of Auto-CoT (Zhang et al.,
2023b), we utilize LLM to generate the difficult
wordsDand corresponding interpretations Abased
on the given bilingual sentence pair (x, y):
Request: Given a [Ls]sentence and its [Lt]translation,
please output the most difficult-to-translate words in
the source sentence and concisely analyze the meaning
of these words.
The input-output format is:
# the format description is omitted.
Source Sentence: [ Source Sentence x]
Target Translation: [ Target Translation y]
Then, the response is parsed via regular expres-
sion to extract the difficult words Dand interpreta-
tionsA. Next, we remove the noisy interpretations
through a process similar to IQC (Alg. 1). The only
difference is that the QE metric is replaced with
the reference-based COMET (Rei et al., 2020) due
to the available access to the reference translation.
Finally, the generated difficult words Dand inter-
pretations Acan be assembled with the source and
target sentence (x, y)as demonstrations for each
step of DUAT.
C Statistics of Challenge-WMT
We compare the complete WMT test set and the
Challenge-WMT subset in terms of the length of
source sentences, the length of target sentences,
the perplexity of source sentences, average number
of nouns, verbs and named entities in the source
sentence. The statistics is shown in Table 6.
D Details of Experiments
We conduct experiments under the few-shot setting.
To obtain the demonstrations of CoT, we ask theAlgorithm 1: IQC
Input : source sentence x, draft translation ey,
interpretations of difficult words A,
QE scorer Ïˆ(Â·)
Output : helpful interpretations Ë†A,
final translation Ë†y
1Ë†A â† A
2Ë†yâ†argmax P Î¸(Eigt, x,Ë†y,A)
3Ë†sâ†Ïˆ(Ë†y|x)
4foriâ†1to|A|do
5 yâ†argmax P Î¸(Eigt, x,Ë†y,A âˆ’ {A i}),
6 sâ†Ïˆ(y|x)
7 ifs >Ë†sthen
8 A â† A âˆ’ {A i},Ë†yâ†y,Ë†sâ†s
9 end
10end
LLM to output the step-by-step translation process
in a manner of post-explanation ( i.e.,, given the
source sentence and its translation, requesting the
LLM to generate the intermediate process). To
obtain the ones of MAPS, we let the LLM to per-
form translation with the specific strategy on the
validation set, and assemble the generated inter-
mediate process ( e.g., keywords) and the reference
translation as demonstrations.
E Results under the Rerank setting
We follow He et al. (2024) to conduct experiments
additionally under the rerank setting. For the base-
line ICL, we run for 4 times with different sets
of demonstrations, which are sampled randomly
with seeds {1,2,3,4}, and adopt QE to select the
best candidate as the final translation. For MAPS,
the final translation is selected from the candidates
generated by the three strategies (â€˜+topicâ€™, â€˜+Key-
wordsâ€™, and â€˜+SimDemsâ€™) and ICL (seed=1). For
DUAT , we select the final translation from the re-
sults of DUAT and ICL (seed=1). The results are
shown in Table 7.Modelgpt-3.5-turbo-0613gpt-3.5-turbo-0125gpt-4-turbo-2024-04-09gpt-4-0314QuestionIn this Chinese sentence: "æ–‡ç« çš„å‰å¦»æ˜¯é©¬ä¼Šç", whatisthe meaning of "æ–‡ç« "?LLMâ€™s AnswerItrefers to the Chinese actor and singer, Wen Zhang.In this sentence, "æ–‡ç« " refers to the Chinese actor and singer.Wen Zhang (æ–‡ç« ) is a Chinese actor."æ–‡ç« " is a person's name. It refers to a famous Chinese actor, Wen Zhang.Source Sentenceæ–‡ç« çš„å‰å¦»æ˜¯é©¬ä¼Šçã€‚Reference TranslationThe ex-wife of Wen Zhang is Ma Yili.LLMâ€™s TranslationThe ex-wife of the article is Ma Yili.The ex-wife of the article is Ma Yili.The ex-wife mentioned in the article is Ma Yili.The ex-wife of Wen Zhang is Ma Yili.Figure 5: Illustration of understanding misalignment in more LLMs.
Source Sentenceè®¸å¤šè§‚å¯Ÿå®¶æŒ‡å‡ºï¼Œä»–æ‰§æ”¿5å¹´äººè®¾å´©å¡Œï¼Œä¸»è¦æ˜¯è‡ªå·±â€œåˆ¨å‘â€æ‰€è‡´ã€‚Reference TranslationObservers point out that the collapse of his reputation after five years in power was primarily caused by his own deeds.LLMâ€™s TranslationMany observers point out that his 5-year rule has led to the collapse of his public image, mainly due to his own "digging holes"QuestionIn this Chinese sentence "è®¸å¤šè§‚å¯Ÿå®¶æŒ‡å‡ºï¼Œä»–æ‰§æ”¿5å¹´äººè®¾å´©å¡Œï¼Œä¸»è¦æ˜¯è‡ªå·±â€œåˆ¨å‘â€æ‰€è‡´", what is the meaning of "åˆ¨å‘"?LLMâ€™s ResponseIt is used metaphorically to indicate that someone's actions have led to their own downfall.(a) LLM misunderstands the word "åˆ¨å‘"as a physical activity duringtranslatingthesourcesentence .(b) LLM correctly understands the metaphoricalmeaning of"åˆ¨å‘"duringexplainingitsmeaning.
Figure 6: Illustration that understanding misalign-
ment leads to LLMs literally translating some com-
plicated concepts.
Source SentenceJudge Nigel Daly told Clifton, who admitted ABH, that the alcohol had "played a significant part" in the night's violence.Reference Translationæ³•å®˜å¥ˆæ°å°”Â·æˆ´åˆ©å‘Šè¯‰å¯¹é€ æˆå®é™…èº«ä½“ä¼¤å®³ï¼ˆç®€ç§°ABHï¼‰ä¾›è®¤ä¸è®³çš„å…‹åˆ©å¤«é¡¿ï¼Œé…’ç²¾åœ¨å½“æ™šçš„æ–½æš´ä¸­â€œå‘æŒ¥äº†é‡è¦ä½œç”¨â€ã€‚LLMâ€™s Translationæ‰¿è®¤çŠ¯æœ‰è½»ä¼¤ç½ªçš„å…‹åˆ©å¤«é¡¿è¢«æ³•å®˜å¥ˆæ°å°”Â·æˆ´åˆ©å‘ŠçŸ¥ï¼Œé…’ç²¾åœ¨å½“æ™šçš„æš´åŠ›äº‹ä»¶ä¸­â€œèµ·åˆ°äº†é‡è¦ä½œç”¨â€ã€‚Questionåœ¨è¿™å¥è‹±æ–‡ä¸­ï¼šJudge Nigel Daly told Clifton, who admittedABH, that the alcohol had "played a significant part" in the night's violence."ABH"æ˜¯ä»€ä¹ˆæ„æ€ï¼ŸLLMâ€™s ResponseABHæ˜¯Actual Bodily Harmçš„ç¼©å†™ï¼ŒæŒ‡çš„æ˜¯å®é™…èº«ä½“ä¼¤å®³ç½ªã€‚(a) LLM misunderstands the term "ABH".
(b) LLM correctly understands the termof"ABH"duringexplainingitsmeaning.
Figure 7: Illustration that understanding misalign-
ment leads to LLMs mistranslating some terminol-
ogy.
Language pair Enâ‡’Zh Zh â‡’En En â‡’Et Et â‡’En En â‡’Is Is â‡’En Average
Dataset Comp. Chal. Comp. Chal. Comp. Chal. Comp. Chal. Comp. Chal. Comp. Chal. Comp. Chal.
#Samples 6215 675 7207 615 4000 644 4000 602 3004 641 3004 694 4572 645
SRC-Len 22.4 24.2 47.4 52.0 19.6 20.3 14.9 15.1 21.4 24.6 18.9 20.8 24.1 26.2
TGT-Len 42.5 50.9 28.9 34.1 14.9 15.5 19.6 20.8 20.6 25.1 20.4 23.2 24.5 28.3
SRC-PPL 141 165 40 79 128 156 823 925 111 147 40 40 214 252
#Noun 4.2 4.9 3.4 4.1 4.6 4.7 1.7 1.6 5.8 7.1 2.1 2.0 3.6 4.1
#Verb 5.2 5.9 3.1 3.7 3.0 3.2 2.5 2.5 3.8 4.4 2.6 2.8 3.4 3.8
#NE 0.8 1.1 2.4 2.4 0.9 0.8 1.6 1.6 0.9 1.2 1.9 1.8 1.4 1.5
Table 6: Fine-grained comparison of the complete WMT test set ( Comp. ) and the Challenge-WMT subset ( Chal. ).
â€˜NEâ€™ is the abbreviation of "Named Entities".5060708090100
GoogleChatGPTDeltaLMGoogleChatGPTDeltaLMGoogleChatGPTNLLBGoogleChatGPTNLLBGoogleChatGPTNLLBGoogleChatGPTNLLB
72.6
76.2
78.7
69.4
71.2
76.2
74.4
74.9
78.8
74.2
80.7
79.1
60.1
71.3
68.2
68.8
74.9
74.9
82.6
84.4
86.8
81.8
82.3
86.0
86.1
85.0
88.2
88.1
90.9
90.5
79.0
83.2
82.5
79.2
81.3
82.3WMT Complete setComplexWMT
Enâ†’ZhZhâ†’EnEnâ†’EtEtâ†’EnEnâ†’IsIsâ†’EnFigure 8: Translation performance on the complete WMT test set and the Challenge-WMT test set.
MethodsEnâ‡’Zh Zh â‡’En En â‡’Et Et â‡’En En â‡’Is Is â‡’En Average
COMET QE COMET QE COMET QE COMET QE COMET QE COMET QE COMET QE
Baselines
ICL 76.79 3.94 72.67 0.43 82.10 9.37 79.98 7.44 73.42 -1.21 78.88 4.80 77.31 4.13
MAPS 77.24 4.56 73.17 1.70 83.05 10.57 80.12 8.28 75.67 2.61 78.47 5.13 77.95 5.48
Ours
DUAT-I 77.36 4.37 73.30 1.08 83.06 10.39 80.22 7.96 76.88 3.12 78.93 5.29 78.29 5.37
DUAT-E 77.78 5.04 73.36 0.88 83.21 10.93 80.10 8.06 77.39 3.97 79.22 5.31 78.51 5.70
Table 7: Experimental results under the rerank setting.