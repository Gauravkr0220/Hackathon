Consecutive Batch Model Editing with HooK Layers*
Shuaiyi Li1, Yang Deng2,â€ , Deng Cai3, Hongyuan Lu1, Liang Chen1, Wai Lam1
1The Chinese University of Hong Kong,2Singapore Management University,3Tencent AI Lab
sli@se.cuhk.edu.hk ,ydeng@smu.edu.sg ,thisisjcykcd@gmail.com ,
1155167604@link.cuhk.edu.hk ,lchen@se.cuhk.edu.hk ,wlam@se.cuhk.edu.hk
Abstract
As the typical retraining paradigm is unac-
ceptably time- and resource-consuming, re-
searchers are turning to model editing to find
an effective way that supports both consec-
utive and batch scenarios to edit the model
behavior directly. Despite all these practical
expectations, existing model editing methods
fail to realize all of them. Furthermore, the
memory demands for such sequential model
editing approaches tend to be prohibitive, fre-
quently necessitating an external memory that
grows incrementally over time. To cope with
these challenges, we propose CoachHooK, a
model editing method that simultaneously sup-
ports sequential and batch editing. CoachHooK
is memory-friendly as it only needs a small
amount of it to store several hook layers whose
size remains unchanged over time. Experi-
mental results demonstrate the superiority of
our method over other batch-supportive model
editing methods under both single-round and
consecutive batch editing scenarios. Extensive
analyses of CoachHooK have been conducted
to verify the stability of our method over a num-
ber of consecutive steps.
1 Introduction
Large Language Models (LLMs) (Chung et al.,
2022; OpenAI, 2023; Black et al., 2022; Touvron
et al., 2023) have been demonstrated to be capable
of recalling factual knowledge about the real world
(Brown et al., 2020; Petroni et al., 2020). Neverthe-
less, researchers also reveal that LLMs often fail
to recall the most up-to-date knowledge or infor-
mation and some specialized knowledge if they are
not periodically updated (Liska et al., 2022; Agar-
wal and Nenkova, 2022; Lazaridou et al., 2021).
*This research was substantially supported by the Center
for Perceptual and Interactive Intelligence (CPII) Ltd. un-
der the Innovation and Technology Commissionâ€™s InnoHK
scheme.
â€ Corresponding author.Despite the fact that fresh and customizable knowl-
edge is highly desired in many areas, such as text
generation, question-answering, reasoning, etc.,
updating the model via retraining is both time-
and resource-consuming. Additionally, researchers
have uncovered that well-trained LLMs do make
mistakes. One popular sort of mistake is called hal-
lucination (Tonmoy et al., 2024), which means that
LLMs generate text based on "hallucinated" fake
knowledge. Although many researchers have tried
to mitigate this issue (Qiu et al., 2023; MÃ¼ndler
et al., 2023; Kang et al., 2023; Varshney et al.,
2023), the strategy to fix this bug remains unclear.
Therefore, researchers have started to seek an effi-
cient approach that could edit LLMs in a customiz-
able, cost-effective way.
To this end, recent years have witnessed many ef-
forts in investigating the model-editing techniques
to bypass the retraining paradigm and edit the
LLMs directly (Meng et al., 2022; Hartvigsen et al.,
2022; Li et al., 2023b; Mitchell et al., 2022a,b). Ac-
cordingly, several new datasets ( e.g., ZsRE (Levy
et al., 2017) and COUNTERFACT (Meng et al.,
2023)) and corresponding metrics ( e.g., reliability,
generality, locality, portability (Yao et al., 2023))
are proposed to facilitate the development in this
field. However, these methods either require ex-
tra expensive training of a meta-network (Mitchell
et al., 2022a; De Cao et al., 2021), or a classifier
(Mitchell et al., 2022b), which causes time and re-
sources overhead, or demands an external memory
of explicit edit instances for reference (Mitchell
et al., 2022b; Hartvigsen et al., 2022), which in-
evitably escalates the memory requirement. Fur-
ther, most of existing methods are only evaluated
on single-round editing, where the model is rolled
back to the initial state after each edit step. This
deviates from the application scenario in reality
since most users anticipate an editing approach that
allows sequential and batch editing.
In light of these issues, we propose a novel
1arXiv:2403.05330v3  [cs.CL]  11 Oct 2024method, named CoachHooK1, which performs
Consecutive B atchModel Editing with HooK lay-
ers. Specifically, CoachHooK supports consecu-
tive batch editing and utilizes the hook layers to
separate weight change from the original model
weight. CoachHook does not need any training on
parameters or large explicit external memory that
stores editing instances. It only needs a reasonable
amount of memory to collect the optimized weight
in the hook layer. To achieve this, we propose
a new transformer memory updating mechanism
that supports consecutive batch editing settings and
design a simple yet effective local editing scope
identification technique used in the hook layer that
can accurately detect the inputs in the local editing
scope. We demonstrate the effectiveness of our
method via extensive experiments on ZsRE and
COUNTERFACT datasets using two popular au-
toregressive language models, GPT2-XL and GPT-
J (6B). Both the single-round batch settings and
consecutive batch settings are included, with the
total number of editing instances ranging from 1k
to 10k. An analysis of the editing scope identi-
fication has also been conducted to validate the
method. Beyond all these, we implement compre-
hensive ablation studies to verify the validity of
each component and discuss the optimal hyperpa-
rameter settings in the method.
2 Preliminaries of Model Editing
As defined by Yao et al. (2023), the task of model
editing is to efficiently modify an initial base model
fÎ¸into an edited model fÎ¸â€²whose responses to a
particular set of input instances Xtare adjusted
as desired without affecting the responses of the
model to other instances. The intended edit de-
scriptor is denoted as (xt, yt), where xtâˆˆ Xtand
fÎ¸(xt)Ì¸=yt. The post-edit model fÎ¸â€²is supposed
to produce the expected output to an intended edit
instance xt, while preserving the original output to
other instances:
fÎ¸â€²(x) =yt ifxâˆˆ Xt
fÎ¸(x)ifx /âˆˆ Xt(1)
In particular, there are three standard criteria for
model editing, namely Reliability, Generality, and
Locality (Yao et al., 2023; Mitchell et al., 2022a,b).
Suppose the prediction of the original model to
the prompt " What is the native language of Joe
1Code is released at https://github.com/Syon-Li/
CoachHook .Biden? " is " French ", and the expected post-edit
model prediction is " English ". To verify the Relia-
bility, we use the same original prompt as input and
then assess whether the post-edit model predicts
"English " as desired. For Generality, a rephrased
prompt " The mother tongue of Joe Biden is" could
be inputted into the edited model to assess whether
the output of the model remains as " English ". Lo-
cality suggests that the model output of an irrele-
vant prompt like " What is the native language of
Donald Trump? " should remain unaffected, which
means that the post-edit model should output what-
ever the initial model output to this prompt.
The current problem settings of model editing
can be generally categorized into three groups (Yao
et al., 2023):
1)Single instance Editing evaluates the post-edit
model performance when only one single knowl-
edge update is performed:
Î¸â€²â†argminÎ¸(âˆ¥fÎ¸(xt)âˆ’ytâˆ¥) (2)
2)Batch Editing evaluates the post-edit model per-
formance in a more realistic scenario where multi-
ple knowledge pieces are modified simultaneously:
Î¸â€²â†argminÎ¸Xn
t=1(âˆ¥fÎ¸(xt)âˆ’ytâˆ¥)(3)
where nâ‰¤| X t|is the batch size and it varies
for different methods (Meng et al., 2023; Mitchell
et al., 2022a,b; Meng et al., 2022).
3)Sequential Editing requires every single edit to
be performed successively, and evaluation has to
be conducted after a series of knowledge updates
(Hartvigsen et al., 2022):
Î¸|Xt|â€²â†argminÎ¸tX|Xt|
t=1(âˆ¥fÎ¸t(xt)âˆ’ytâˆ¥)(4)
In this work, we investigate a new and more prac-
tical setting for model editing, namely Consecutive
Batch Editing , which aims at executing the editing
in a consecutive batch editing way:
Î¸âŒˆ|X t|/nâŒ‰â€²â†argmin
Î¸sâŒˆ|X t|/nâŒ‰X
s=0h
min(( s+1)Â·n,|Xt|)X
t=sÂ·n(âˆ¥fÎ¸s(xt)âˆ’ytâˆ¥)i(5)
where srepresents the consecutive editing step.
23 Method
We first discuss our method under the single-layer
consecutive batch editing setting. Explicitly, we
first discuss the process of extending the single-
layer updating mechanism in MEMIT (Meng et al.,
2023) from a scenario of single-round batch editing
to consecutive batch editing. Then, we introduce
the hook layer and the local editing scope identifi-
cation operation employed in the hook layer. The
practicality of the operation is also clarified. Fi-
nally, we broaden the method from single-layer to
multi-layer scenarios.
3.1 Single-Layer Consecutive Batch Editing
3.1.1 Batch Editing Menchanism
Meng et al. (2023) demonstrate an effective single-
layer editing method using minimal squared er-
ror. Although it supports multiple edits on a single
round, the updates do not account for scenarios
involving consecutive updates. In this section, we
extend this approach to include consecutive sce-
narios. Following (Meng et al., 2023, 2022), we
analyse the model layer weights W0as a linear
associative memory (Kohonen, 1972a; Anderson,
1972a) that stores associations between a set of
keyskiand values viusing minimal squared error:
W0= argminWXn
i=1âˆ¥Wkiâˆ’viâˆ¥2(6)
In this work, W0is the weight of the second layer
of the modelâ€™s FFN part (denoted as Wl
proj). For
simplicity, we stack keys and values into matrices
K0= [k1|k2|...|kn]andV0= [v1|v2|...|vn], then
Eq. 6 can be optimized by solving (Strang, 2022):
W0K0KT
0=V0KT
0 (7)
Thanks to the well-conducted pre-training proce-
dure for most of the available LLMs, we can as-
sume that the pre-trained weight W0satisfies Eq.
7,i.e., serves as the optimal solution for Eq. 6.
Unlike Meng et al. (2023), we define a succes-
sive mass-editing objective:
Ë†W1= argminW(Xr
i=1âˆ¥Wkiâˆ’viâˆ¥2
+Xr+u
i=r+1âˆ¥Wkiâˆ’viâˆ¥2)(8)
Following Eq. 7, we conclude that Eq. 8 can be
optimized if we can solve:
Ë†W1[K1K2][K1K2]T= [V1V2][K1K2]T(9)where K1= [k1|k2|. . .|kr](râ‰¥n)andV1=
[v1|v2|. . .|vr]is the set of key-value pairs that have
been updated and K2= [kr+1|kr+2|...|kr+u]and
V2= [vr+1|vr+2|...|vr+u]is the set of key-values
that are going to be edited. Therefore, the objective
(Eq. 8) indicates that we want an optimal Ë†W1that
successfully updates the new associations while
maintaining the old key-value pairs.
Further expanding Eq. 9:
(W1+ âˆ†)( K1KT
1+K2KT
2) = (V1KT
1+V2KT
2)
(10)
W1K1KT
1+ âˆ†K1KT
1+W1K2KT
2
+ âˆ†K2KT
2=V1KT
1+V2KT
2(11)
Theâˆ†means the desired weight change to update
the new associations K2, V2andW1is the weight
that has been updated for the associations K1, V1
(Note that W1=W0if and only if r=n). In a
real consecutive editing scenario, rincreases and
starts with n, and each batch-editing iteration is
optimized through the objective (Eq. 8). Hence,
we can conclude that W1K1KT
1=V1KT
1. Sub-
tracting it from Eq. 11, we get:
âˆ†K1KT
1+W1K2KT
2+âˆ†K2KT
2=V2KT
2(12)
Further rearranging it, we have:
âˆ† =RKT
2Câˆ’1
accu (13)
where R= (V2âˆ’W1K2)is the residual error
evaluated on the most recent updated weights.
Caccu= (K1KT
1+K2KT
2)is the accumulation
sum of editing keysâ€™ outer product, and we have
K1KT
1=K0KT
0+Kâ€²Kâ€²T(14)
where K0is the set of pre-training keys that have
been contained in the pre-training weight, Kâ€²=
[kn+1|kn+2|...|kr]denotes the updated keys pro-
ceeding to current editing step. We follow (Meng
et al., 2023) to model K0KT
0as the uncentered
covariance of some randomly sampled inputs:
K0KT
0=Î»E[kkT] (15)
Note that the Î»represents a factor that balances
the pre-trained and the whole updated associations.
We follow the definitions of keys and values in
(Meng et al., 2023, 2022), where keys are the acti-
vations at the last token of the subject (such as "Joe
Biden" for example provided in Â§1) and values are
gradient-descent optimized vectors that maximize
the modelâ€™s prediction for the target object.
3ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’ğ‘¾ğ’‡ğ’„ğ’Attention ğ’‰ğ’ğ’‰ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’ğ‘·ğ’
ğ‘¶ğ’#
# ğ‘´ğ’=ğ‘·ğ’âˆ’ğ‘¶ğ’
ğ’ğ’= Î¤ (ğ‘´ğ’âˆ’ğ)ğˆ
ğ’‰ğ’Šğ’=àµğ‘¶ğ’Šğ’,ğ’ğ’Šğ’â‰¥ğœ¶
ğ‘·ğ’Šğ’,ğ’ğ’•ğ’‰ğ’†ğ’“ğ’˜ğ’Šğ’”ğ’†
àµğ’‰ğ’=ğ‘¶ğ’
After Updating:
Validated hook 
layerDuring Updating: 
Temporary hook 
layerğ’‰ğ’âˆ’ğŸ,ğ’‰ğ’,ğ‘·ğ’,ğ‘¶ğ’âˆˆğ‘¹ğ’Ã—ğ’
ğ’Œğ’Š,ğ’—ğ’Š,ğ’‰ğ’Šğ’âˆˆğ‘¹ğ’
ğ‘´ğ’,ğ’ğ’âˆˆğ‘¹ğ’
âˆ†=(ğ’—ğ’Šâˆ’ğ’‰ğ’Šğ’)ğ’Œğ’Šğ‘»ğ‘ªğ’‚ğ’„ğ’„ğ’–âˆ’ğŸHook layer
ğ’Œğ’Š ğ’—ğ’ŠFigure 1: Single layer update with hook layer (residual
connections are omitted). âˆ¥.âˆ¥means calculate the L2-
norm over the keysâ€™ dimension ( m). For each updating
of a single batch edits, the temporary hook layer is
used at the beginning to ensure âˆ†is computed based
onWl
h. After the weights update, the validated hook
layer is applied to determine whether to use the original
layer or hook layer for each token. This process can
be implemented iteratively to support consecutive batch
editing. Note that the temporary hook layer weight of
a new iteration is copied from the validated hook layer
weight of the previous iteration. So, the validated hook
layer keeps track of the updated layer from previous
edits by retaining the weight from the previous iteration.
3.1.2 Hook Layer
Yao et al. (2023) demonstrate that those editing
methods that directly modify the model parameter
in place struggle with sequential editing. Specif-
ically, the locality decreases drastically when the
number of iterations increases. Meanwhile, those
methods that freeze the model parameters show
more stable performance over iterations. This indi-
cates that it might be helpful to separate the editing
change from the model itself. However, directly ap-
plying an external memory (Mitchell et al., 2022b;
Hartvigsen et al., 2022) that grows over time for a
consecutive batch editing scenario is too memory-
costly. Therefore, we aim to seek an approach that
could store associations without regularly increas-
ing external memory while preserving the original
model parameters.
In light of these motivations, we introduce the
hook layer (Fig. 1), which takes the original model
layer weights as the weight initialization and is re-
sponsible for all editing weight alteration in the
whole editing process of CoachHooK. It is similar
to the forward hook function defined in popular ML
libraries like PyTorch, which adjusts the original
forward layer output based on predefined criteria.
Theoretically, the hook layer can be hung on any
target linear layer in the transformer. Nevertheless,
we mainly focus on the critical path identified in
(Meng et al., 2023, 2022) as they are verified tobe crucial for fact association storage in the autore-
gressive language model.
As shown in Fig. 1, there are generally two sorts
of hook layers in this work, namely, the Tempo-
rary hook layer and the Validated hook layer .
The temporary hook layer is temporarily applied
during the weight-updating process. It replaces the
original output with the output from the hook layer
so that the residual is computed based on the hook
layer weight. The hook layer weights are then up-
dated (Eq. 13) using the calculated residual and
the accumulated sum of the keysâ€™ outer product.
Validated hook layers inherit the updated weights
from the temporary hook layer, and are employed
after each single-batch weight updating process at
the layer.
3.1.3 Local Editing Scope Identification
Outlier Detection Given the original outputs pro-
duced by the model layer weights and the edited
outputs generated by hook layer weights, we need
to decide when and which part of the original out-
puts to swap over. The ideal solution is only to
switch those parts of outputs whose keys have
been updated to the hook layer weights and leave
other parts unchanged. To this end, we first de-
tect the output parts that have their keys updated.
Suppose kiâˆˆK1, viâˆˆV1is an association
that has been updated to W1, and kj/âˆˆK1is
a key that is not included in the updated associ-
ations. We show empirically in section 4.4 that
âˆ¥W1kiâˆ’W0kiâˆ¥â‰«âˆ¥ W1kjâˆ’W0kjâˆ¥holds.
This implies that when the hook layer with updated
weight Whreceives an input Ë†KâˆˆRmÃ—n(batch di-
mension is ignored for simplicity) that contains an
edited key kiâˆˆË†Kâˆ©K1, kiâˆˆRm, then we should
haveâˆ¥Whkiâˆ’W0kiâˆ¥â‰«âˆ¥ Whkjâˆ’W0kjâˆ¥
for all kjâˆˆË†Kâˆ’Ë†Kâˆ©K1, which means that
âˆ¥Whkiâˆ’W0kiâˆ¥would be outliers among {âˆ¥
Whkxâˆ’W0kxâˆ¥:âˆ€kxâˆˆË†K}. Hence, detecting
outputs of the updated keys can be transferred to
detecting the outliers in the L2-norm distribution
of inputs. We used the standardization to find the
outliers (Fig. 1), which applies the standardiza-
tion technique to L2-norm vectors of inputs and
determines outliers via a predefined threshold Î±.
Concretely, for the inputs Ë†K, we first compute the
L2-norm vector MlâˆˆRn:
Pl=W0Ë†K Ol=WhË†K (16)
Ml=âˆ¥(Olâˆ’Pl)âˆ¥ (17)
4ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ‘hl-2hl-3
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸ‘Pl-3
Ol-3# ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-2
Ol-2# hl-1ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-1
Ol-1# hl
ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ‘hl-2hl-3
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸ‘Pl-3
Ol-3# ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-2
Ol-2# hl-1ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-1
Ol-1# hl
ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ‘hl-2hl-3
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸ‘Pl-3
Ol-3# ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-2
Ol-2# hl-1ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-1
Ol-1# hl
ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ‘hl-2hl-3
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸ‘Pl-3
Ol-3# ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-2
Ol-2# hl-1ğ‘¾ğ’‘ğ’“ğ’ğ’‹ğ’âˆ’ğŸ
ğ‘¾ğ’‰ğ’Œğ’âˆ’ğŸPl-1
Ol-1# hlğ‘¾ğ’‰ğ’Œğ’ğ‘¾ğ’‰ğ’Œğ’ Validated hook layer: Temporary hook layer:
ğ’—ğ’Š
ğ’—ğ’Š
ğ’—ğ’Š
ğ’—ğ’Šğ‘¹ğ’âˆ’ğŸ‘=ğ’—ğ’Šâˆ’ğ’‰ğ’Šğ’
ğŸ‘ I) Addâˆ†ğ’âˆ’ğŸ‘
ğ‘¹ğ’âˆ’ğŸ=ğ’—ğ’Šâˆ’ğ’‰ğ’Šğ’
ğŸII) Addâˆ†ğ’âˆ’ğŸ
ğ‘¹ğ’âˆ’ğŸ=ğ’—ğ’Šâˆ’ğ’‰ğ’Šğ’III) Addâˆ†ğ’âˆ’ğŸ2. 1. 
3. Figure 2: Multiple layer update with hook layer (At-
tention module and the first layer of FFN are omitted).
The value vector viis first computed at the last editing
layer, and then we iteratively insert a fraction of the
residual to each editing layer (I, II, III) using Eq. 13.
Since changing one layer would affect the activations
of downstream layers, recollection of the activations is
conducted after each iteration. At the beginning, tem-
porary hook layers are initialized to all editing layers.
Once the hook layer weight is updated, it is replaced by
the validated hook layer (1, 2, 3).
Note that âˆ¥.âˆ¥here means computing the L2-norm
for each vector over the keysâ€™ dimension ( m). Then,
we standardize Mlto get the z-score vector Zland
select the swap location by comparing it with Î±.
The details of choosing Î±are discussed in the next
paragraph. Specifically, we do:
hl
i=(
Ol
iifZl
iâ‰¥Î±,
Pl
iortherwise .(18)
where iis the index over tokens.
Threshold Î±Determination We denote Zl
i=
max(( Mlâˆ’Âµ)/Ïƒ)as the maximum z-score entry
of an input Ë†K. Since the Zl
ivaries for different
instances (Â§4.4) and is likely to shift as the consec-
utive editing steps grow, it is unreasonable to set
Î±as a fixed real number. Therefore, we determine
theÎ±dynamically during the editing process:
Î±s=(
Î±z ifs= 1,
min(Î±c, Î±sâˆ’1)otherwise(19)
where sâ‰¥1. Specifically, the Î±is first initialized
to a pre-selected value Î±z. At each consecutive
editing step s, for the batch of inputs in this step,
we calculate Zl
i(the maximal z-score entry) for
each single instance and select the minimal Zl
iin
the batch ( i.e., the supremum) as the candidate Î±c.
TheÎ±sis finally determined to be the minimum
between the candidate Î±cand the previous value
Î±sâˆ’1. In practice, we set Î±z= 2.2.3.2 Multiple-layer Consecutive Batch Editing
Given the designed single-layer editing procedure,
there exists a risk that the single-layer hook fails to
detect the updated keys. Suppose kiis an updated
key; failure to detect kiindicates that the output cor-
responds to kiat this single layer would be the same
as the original output Wprojki, which consequently
leads to the failure update for ki. To tackle this
issue, one potential solution is to apply the hook to
multiple model layers rather than a single model
layer because the latter layer grasps the chance to
capture the edited keys missed by proceeding lay-
ers. Furthermore, (Zhu et al., 2020) showcased that
minimizing the magnitude of parameter change is
helpful for improving the robustness of the model.
Thus, we expand our work to multiple layers (Fig.
2).
We first find the desired object vector vifol-
lowing a similar procedure in (Meng et al., 2023).
However, the optimization is not based on the orig-
inal model, but the model hung with the validated
hook that inherits the most recently updated hook
weights from the previous editing step. After viis
found, the hook weight is updated at each layer.
At each batch editing step, all the hook layers
are initialized to temporary hook layers, which sub-
stitute the entire original output to output from
hook layers. The purpose of doing this is to ensure
that the residual regarding the hook layer weights
rather than the original model weights are calcu-
lated. Then, the residual is distributed evenly to
each layer, and the alteration âˆ†lto the parameter
at each layer is found in a layer-increasing iterative
manner with keys and residuals recomputed at each
iteration (Fig. 2). The reason for the recomputation
of keys and residuals is that the layer-increasing al-
teration approach will affect the keys and residuals
in the latter layer. For each layer, once the hook
layer weight is updated, the hook layer is changed
from a temporary hook layer to a validated hook
layer to facilitate the computation of the keys and
residuals in the latter layer. After the whole edit-
ing process is completed, the validated hook layers
with the ultimately updated weights are hung on
the model to shape the final edited model.
4 Experiments
4.1 Experiment Setups
Datasets & Evaluation Metrics We use the
ZsRE (Levy et al., 2017) and COUNTERFACT
(Meng et al., 2023) datasets with the split provided
5Method ModelZsRE COUNTERFACT
Reliability Generality Locality Average Reliability Generality Locality Average
FT-L (Meng et al., 2022)
GPT2-XL16.85 16.34 71.55 34.91 0.27 0.34 85.18 28.60
FT-M 17.95 17.32 71.26 35.51 0.36 0.42 82.81 27.86
LoRA (Hu et al., 2022) 30.10 29.08 80.54 46.57 5.64 3.46 69.45 26.18
MEND (Mitchell et al., 2022a) 2.16 2.11 20.34 8.20 0.13 0.03 4.22 1.46
SERAC (Mitchell et al., 2022b) 98.64 48.12 35.68 60.81 17.88 14.55 82.25 38.23
MEMIT (Meng et al., 2023) 61.19 49.97 97.51 69.56 81.01 27.67 95.80 68.16
CoachHooK 82.21 66.61 99.40 82.74 88.28 40.38 97.66 75.44
FT-L (Meng et al., 2022)
GPT-J22.57 21.77 99.19 47.84 0.37 0.34 99.57 33.43
FT-M 99.96 80.31 43.35 74.54 99.99 35.29 17.04 50.77
LoRA (Hu et al., 2022) 99.97 83.20 17.64 66.93 99.87 53.10 2.50 51.82
SERAC (Mitchell et al., 2022b) 87.46 63.64 77.35 76.15 16.67 15.93 99.99 44.20
MEMIT (Meng et al., 2023) 93.40 70.45 96.47 86.77 99.57 42.29 95.25 79.04
CoachHooK 97.59 72.41 99.10 89.70 87.94 42.76 98.17 76.29
Table 1: Single round batch editing results. The best two average scores are highlighted. All metrics are desired
higher values.
in EasyEdit2for evaluation. We employ three popu-
lar editing evaluation metrics defined in (Yao et al.,
2023; Huang et al., 2023; Cao et al., 2021), i.e.,
Reliability, Generality, and Locality, as well as the
average scores3over the three metrics. Further
details are provided in Appendix A.
Baselines & Implementation Details For base-
lines, we adopt several batch-supportive edit-
ing methods, including LoRA (Hu et al., 2022),
SERAC (Mitchell et al., 2022b), MEND (Mitchell
et al., 2022a), MEMIT (Meng et al., 2023) and
fine-tuning with specific layer (FT-L) technique
used in (Meng et al., 2022; Yao et al., 2023), which
only fine-tune a specific layer identified by Rome
(Meng et al., 2022) instead of all layers to ensure a
fair comparison. We also include a small variation
of FT-L called FT-M and a sequential supportive
editing method GRACE (Hartvigsen et al., 2022).
We choose large autoregressive language models
GPT2-XL and GPT-J (6B) as our base models.
For all consecutive editing experiments, the eval-
uation is conducted after the full set of consecutive
steps are finished. For example, in Fig. 4, we con-
duct experiments for sample sizes 200, 400, 600,
800, and 1000, so the evaluation is triggered right
after the first 200, 400, 600, etc, samples are edited
to the model. Unless specified, the batch size4for
consecutive editing is selected to be 10. Further de-
tails of the baselines and implementation are given
in the Appendix B.
2https://github.com/zjunlp/EasyEdit/tree/main
3Most of the application scenarios of model editing require
good performance in all three metrics.
4Since GRACE (Hartvigsen et al., 2022) does not support
batch editing, we set the batch size to 1 for GRACE.4.2 Evaluation on Single-round Batch Editing
We first test the effectiveness of our method under
basic single-round batch editing settings with batch
size 30, i.e., the model is rolled back to the initial
state after each batch editing. Both MEMIT and
CoachHooK need to set the parameter Î», the bal-
ance factor between pre-trained and newly updated
associations. According to (Meng et al., 2023),
higher Î»helps preserve the original model behav-
ior (locality) but could harm reliability and gener-
ality, and the best overall performance is found at
around Î»= 104. However, with the intent to ver-
ify whether our method comprehensively improves
the editing, that is, could accept lower Î»to assign
higher weight for new associations while not sacri-
ficing the locality, we deliberately set Î»= 5Ã—103
for CoachHooK and keep it as the optimized value
for MEMIT, which are 2Ã—104and1.5Ã—104for
GPT2-XL and GPT-J respectively.
The evaluation results are shown in Table 1. For
GPT2-XL, our method has the best result in almost
every metric. Specifically, despite the relatively
lowÎ», our method overwhelms other baselines in
generality metrics while maintaining a better lo-
cality. This indicates that lowering Î»or, in other
words, increasing the weight of the new associa-
tions does not sacrifice the locality in CoachHooK.
The improvement in GPT-J is less compared with
that in GPT2-XL. However, our method still has
the best average score for the ZsRE dataset and
a comparable average score with the best in the
COUNTERFACT dataset.
6Method ModelZsRE COUNTERFACT
Reliability Generality Locality Average Reliability Generality Locality Average
FT-L (Meng et al., 2022)
GPT2-XL3.79 2.48 6.60 4.29 1.00 1.00 6.00 2.67
FT-M 8.92 8.41 6.22 7.85 4.00 3.50 5.50 4.33
LoRA (Hu et al., 2022) 0.96 1.29 0.03 0.76 0.50 0.02 0.50 0.34
MEND (Mitchell et al., 2022a) 20.95 18.29 93.69 47.01 0.01 0.00 0.08 0.03
SERAC (Mitchell et al., 2022b) 100 36.03 35.95 57.33 15.41 12.96 81.00 36.46
GRACE (Hartvigsen et al., 2022) 100 0.04 100 66.68 100 0.40 100 66.80
MEMIT (Meng et al., 2023) 34.88 32.96 70.74 46.19 56.00 37.00 31.00 41.33
CoachHooK 66.91 56.11 97.23 73.42 86.00 38.00 59.00 61.00
FT-L (Meng et al., 2022)
GPT-J23.53 21.70 55.27 33.5 2.00 2.00 72.00 25.33
FT-M 64.33 55.63 17.59 45.85 25.50 5.00 2.00 10.83
LoRA (Hu et al., 2022) 1.43 1.39 0.02 0.95 0.50 0.50 0.10 0.37
SERAC (Mitchell et al., 2022b) 86.91 55.36 79.07 73.78 18.49 14.56 98.89 43.98
GRACE (Hartvigsen et al., 2022) 100 0.04 100 66.68 100 0.50 100 66.83
MEMIT (Meng et al., 2023) 63.36 48.90 74.80 62.35 75.00 45.00 42.00 54.00
CoachHooK 79.89 61.29 96.52 79.23 95.00 41.00 80.00 72.00
Table 2: Consecutive batch editing results.
4.3 Evaluation on Consecutive Batch Editing
We evaluate our methodâ€™s capability on 1k samples
from both datasets for consecutive batch editing,
i.e., there is no roll-back. The evaluation is con-
ducted after the end of the whole consecutive batch
editing process. We set Î»to15,000as the scenario
now is consecutive batch editing.
Results in Table 2 show that most of the methods
suffer from a great performance drop contrasted to
editing in a single round. Although our methodâ€™s
performance experiences a decrease as well, it sur-
passes other methods in 100 consecutive steps with
an even larger improvement margin for almost all
the metrics compared to the single-round batch edit-
ing. This demonstrates that our method does not
depend on simple regurgitation of the editing sam-
ples nor rely heavily on the trade-offs of lowering
the balancing factor Î»to increase the reliability and
locality. An interesting point is that the GRACE
performs perfectly in reliability and locality but
poorly in generality. As expected, GRACE is su-
perior in reliability since it maintains a codebook
to memorize the instances of editing that are en-
countered. However, its inferiority in generality
indicates that it suffers from the problem of regur-
gitation. (One may argue that increasing the de-
ferral radius could help improve the generality of
GRACE, we therefore provide further experiments
in Appendix C.)
We extend the data scale of the consecutive batch
editing experiment to 10k (1k consecutive steps)
to explore the limit of our method. Results can be
found in Fig. 6. Surprisingly, the locality experi-
ences a great fall from 100 to 200 steps but remains
0 20 40 60 80 1000.5
0.00.51.01.52.02.53.03.5
COUNTERFACT Reliability samples
0 20 40 60 80 10001234
COUNTERFACT Generality samples
Z-score entry corresponds to updated key Average of the overall z-score vectorFigure 3: Difference between the z-score entry to the
updated key Zl
keyand average of Zl. The x-axis repre-
sents the sample index.
steady from 200 to 1k editing steps, which proves
that the hook layer stably obstructs the out-scope
samples. Reliability and generality consistently fall
as the consecutive steps grow, indicating that there
is still room for improvement in this field.
4.4 Validation of Local Editing Scope
Given an updated hook layer with the weight Wh,
the original model weight W0, an updated key ki,
and an out-of-scope key kj, we conduct experi-
ments to verify whether âˆ¥Whkiâˆ’W0kiâˆ¥â‰«âˆ¥
Whkjâˆ’W0kjâˆ¥holds. We select 100 samples
from the COUNTERFACT dataset to edit GPT2-
XL using CoachHooK, then apply the edited model
to these 100 samples and record the z-score entries
of the L2-norm of the difference vector between
update keysâ€™ response from the last hook layer and
original model layer, namely, z-score entries of
âˆ¥Whkiâˆ’W0kiâˆ¥. Both reliability and generality
prompts are included for comprehensiveness.
The result is shown in Fig. 3. Almost all the z-
scores of the responses from updated keys exhibit a
great margin from the mean value, with the lowest
7200 400 600 800 10000.400.450.500.550.600.650.70
ZsRE Reliability
200 400 600 800 10000.400.450.500.55
ZsRE Generality
200 400 600 800 10000.700.750.800.850.900.95
ZsRE Locality
200 400 600 800 10000.60.70.80.9
COUNTERFACT Reliability
200 400 600 800 1000
x - the number of samples0.300.350.400.45
COUNTERFACT Generality
200 400 600 800 10000.30.40.50.60.7
COUNTERFACT Locality
CoachHooK w/o HK CoachHooK MEMITFigure 4: Ablation study.
around 1.5 in reliability samples and 2 in generality
samples. The discriminative z-score demonstrates
that the identification technique (section 3.1.3) can
effectively filter editing-irrelevant instances and
accept editing-relevant instances, which validates
the local editing scope.
4.5 Detailed Analysis and Discussions
Ablation Study of Update mechanism and Hook
layers The effectiveness of the derived consecu-
tive updating mechanism and the hook layers are
discussed in this part. We run three cases using
GPT2-XL, namely, MEMIT (no consecutive up-
dating mechanism, no hook layers), CoachHooK
without hook (CoachHooK w/o HK), and Coach-
HooK for consecutive batch editing on 1k samples
from both ZsRE and COUNTERFACT datasets.
The results are demonstrated in Fig. 4. In almost
all metrics of the two datasets except the generality
of COUNTERFACT, the CoachHooK w/o the hook
performs better than the vanilla MEMIT, and the
margin tends to increase as the consecutive steps as-
cend. This certifies the effectiveness of our derived
consecutive updating mechanism in consecutive
batch editing scenarios. For the ZsRE dataset, the
method with hook layers considerably outperforms
the one without hook in the locality without sacrific-
ing reliability and generality. This verifies that the
hook layer can efficiently and accurately block the
out-scope instances from the input without fraudu-
lently missing in-scope instances. For the COUN-
TERFACT dataset, the reliability of CoachHooK
is consistently higher than the other two, and the
generality surpasses that of MEMIT after 80 edit-
ing steps. Besides, the hook layer causes some side
effects in the locality of COUNTERFACT, but this
circumstance is not found in the ZsRE dataset. It
is worth noting that CoachHooK shows the most
250 500 750 10000.6000.6250.6500.6750.7000.7250.750
ZsRE Reliability
250 500 750 1000
x - the number of samples0.4500.4750.5000.5250.5500.575
ZsRE Generality
250 500 750 10000.900.920.940.96
ZsRE Locality
=20,000
=15,000
=10,000
=5,000
=1,000
Figure 5: Performance comparisons on initial five dif-
ferent values of Î».
2000 4000 6000 8000 100000.30.40.50.60.7
ZsRE Reliability
2000 4000 6000 8000 10000
x - the number of samples0.30.40.5
ZsRE Generality
2000 4000 6000 8000 100000.60.70.80.9
ZsRE Locality
=15,000
 =10,000
 =5,000
Figure 6: Extension on the best three values of Î».
stable performance as the number of consecutive
editing steps grows, showing the great potential of
our method for consecutive editing.
Effect of the Balance Factor Î»We test the ef-
fect of different Î», the balance factor between pre-
training and newly updated associations. We first
evaluate the CoachHooK with different Î»on 1k
samples from ZsRE (Fig. 5). It seems that a
small value of Î»= 1,000would cause signifi-
cant damage to all three metrics, especially the
reliability and generality, since they experience a
great drop as the consecutive steps increase. This
may result from the overly high magnitude of the
weight change caused by the low value of Î», which
severely distorts the previously updated associa-
tions. Meanwhile, a too-high value of Î»= 20,000
also seems not to be a good choice, which gives rise
to an overly small magnitude of the weight change
so that it fails to deliver the new optimized values
for keys. The cases of Î»= 5000 ,10000 ,15000
do not show an apparent difference, so we extend
further the sample size to 10k (Fig. 6).
Extended results show that 5000 is not a good
choice for large-consecutive editing steps, though
it performs no worse than the other two in early 1k
samples. The case of Î»= 15,000ranks first in reli-
ability and generality. Although it performs worse
in locality compared to Î»= 10,000, the margin
between them gradually narrows as the consecutive
steps rise. Overall, we conclude that 15,000 would
be a reasonable selection.
Effect of Editing Batch Size Does the batch size
parameter affect the performance of our method?
We investigate the effect of batch size by conduct-
8Reliability Generality locality0.40.60.81.0GPT2-XL
Reliability Generality locality0.60.70.80.91.0GPT-J
batch size 10 batch size 100 batch size 1000Figure 7: Performance comparisons on different editing
batch sizes.
ing single-round editing on 1k samples from ZsRE.
We tested batch sizes 10, 100, and 1000 (Fig. 7).
The results show that while the three metrics
decrease as the batch size rises, the margin could
be negligible, denoting that our method possesses
the mass-editing capacity.
More Analyses Other detailed analyses includ-
ing hyperparameters search, inference time and
memory analysis are presented in Appendix C.
5 Related Work
Linear Associative Memory Linear Associa-
tive Memory (Kohonen, 1972b; Anderson, 1972b)
treats the model weights Was a set of associations
between a set of keys and values. Many researchers
have adopted this technique as a memory compo-
nent in their methods. (Schlag and Schmidhuber,
2018; Shi et al., 2022; Li et al., 2023a; Chen et al.,
2020; Schlag et al., 2021). Recent research (Geva
et al., 2021) finds that the transformer feed-forward
layers are likely to be a key-value memory. This
significantly helps researchers to understand what
is happening inside the transformer black box and
inspires researchers in the field of model editing to
first locate where the knowledge is stored and then
edit the corresponding parameters (Meng et al.,
2023, 2022).
Model Editing Recent years have witnessed
prosperous development in the field of model edit-
ing. According to (Yao et al., 2023), the proposed
methods so far can be generally classified into
two groups, i.e., modify the modelâ€™s weight or not.
The methods that do not directly alter the model
weights generally follow two directions: they ei-
ther employ an external memory or introduce ad-
ditional adjustable parameters. Methods like T-
Patcher (Huang et al., 2023) and CaliNET (Dong
et al., 2022) apply new neurons that are respon-
sible for specific mistakes in the last layer of the
FFN model. GRACE (Hartvigsen et al., 2022) in-
troduces a timely adjusted code book to edit the
modelâ€™s behavior. Another group of methods like
SERAC (Mitchell et al., 2022b) integrates an ex-plicit external memory as edit descriptors to help
editing scope recognition. Some other techniques
like IKE (Zheng et al., 2023), MeLLo (Zhong et al.,
2023), and MemPrompt (Raffel et al., 2020) take
advantage of the recent development of in-context
learning and use carefully designed demonstration
context prompts to edit the model. Cai et al. (2024)
also discussed the possible transformation between
context and parameter update. On the other hand,
those directly altering the modelâ€™s weight either
train a hyper-network to predict the change re-
quired by the edits or first locate corresponding
parameters responsible for specific knowledge and
then edit the located parameters. For example,
Methods like KE (De Cao et al., 2021), MEND
(Mitchell et al., 2022a), and MALMEN (Tan et al.,
2024) follow the idea of meta-learning and use
the trained hypernetwork to predict the desired pa-
rameter change for editing. ROME (Meng et al.,
2022), MEMIT (Meng et al., 2023), PMET (Li
et al., 2023b), KN (Dai et al., 2022), and WilKE
(Hu et al., 2024) first locate the parameters respon-
sible for knowledge and then using a designed op-
timization mechanism to shift the parameters.
Despite the good performance of the proposed
methods, researchers investigated the harmful con-
sequences of these methods. Yang et al. (2024)
points out even few edits could result in the collapse
of a model. Gupta et al. (2024) shows that scaling
up the edits could lead to catastrophic forgetting
of the previous edits. Gu et al. (2024) argues that
editing could lead to the degradation of the modelâ€™s
general capabilities. Although the rationality of
these investigations is still under discussion, they
reveal the potential risk that model editing tech-
niques could bring to the base model.
6 Conclusion
This work introduces a novel model editing method,
CoachHooK, which advocates the more practical
consecutive batch model editing. CoachHooK uses
an expanded editing mechanism to support con-
secutive editing and newly proposed hook layers
to identify the editing scope. Compared to exist-
ing model editing methods, CoachHooK does not
require large external memory nor extra training
for meta-networks or classifiers. Instead, it adopts
hook layers whose size remains fixed over time for
storing associations. Comprehensive experiments
are conducted to verify the methodâ€™s effectiveness
over single-round and consecutive batch editing.
9Limitations
Several aspects remain to be further investigated.
Other types of tasks Notably, model editing
techniques could be applied to various types of
tasks. Specifically, besides factual knowledge edit-
ing, it can be applied to erase hallucinations, biases,
privacy information, etc. However, the concentra-
tion of this paper is to explore the practicability
of expanding the model editing application sce-
nario to consecutive batch editing and investigate
the potential bottleneck of corresponding methods
under this scenario. Therefore, our experiment fo-
cuses on varying the scale of editing samples in
factual knowledge editing tasks, as it is a relatively
well-studied and universal evaluation task in model
editing.
Model scale and architecture Due to the lim-
ited computational resources, we cannot verify our
methodâ€™s effectiveness in larger LLMs such as
Llama-2 (Touvron et al., 2023), and GPT-NEOX-
20B (Black et al., 2022). We focus on the decoder-
only autoregressive models and do not include
encoder-decoder structure models, as the autore-
gressive structures are the mainstream architecture
nowadays (OpenAI, 2023; Touvron et al., 2023).
Further, as stated by Yao et al. (2023), the weight
matrix in some models like OPT-13B (Zhang et al.,
2022) is not invertible. However, such an issue
can be relieved by adding a term Î²Ito the Eq. 14,
where Î²is a scalar expected to be small and Iis
the identity matrix.
The shrink of Î±As more and more associations
are integrated into the hook layer, the dynamically
determined hyperparameter Î±will gradually shrink,
meaning that an increasing number of vector entries
in the original layer output will be swapped by
the output from the hook layer, which is likely
to lead the drop in locality. Nevertheless, such a
problem can be alleviated by the newly designed
updated mechanism (Eq. 13), which considers both
previously updated and newly updated keys.
References
Oshin Agarwal and Ani Nenkova. 2022. Temporal ef-
fects on pre-trained models for language processing
tasks. Trans. Assoc. Comput. Linguistics , 10:904â€“
921.
James A Anderson. 1972a. A simple neural networkgenerating an interactive memory. Mathematical
biosciences , 14(3-4):197â€“220.
James A Anderson. 1972b. A simple neural network
generating an interactive memory. Mathematical
biosciences , 14(3-4):197â€“220.
Sid Black, Stella Biderman, Eric Hallahan, Quentin
Anthony, Leo Gao, Laurence Golding, Horace
He, Connor Leahy, Kyle McDonell, Jason Phang,
Michael Pieler, USVSN Sai Prashanth, Shivanshu
Purohit, Laria Reynolds, Jonathan Tow, Ben Wang,
and Samuel Weinbach. 2022. Gpt-neox-20b: An
open-source autoregressive language model. CoRR ,
abs/2204.06745.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Deng Cai, Huayang Li, Tingchen Fu, Siheng Li, Wei-
wen Xu, Shuaiyi Li, Bowen Cao, Zhisong Zhang,
Xinting Huang, Leyang Cui, Yan Wang, Lemao Liu,
Taro Watanabe, and Shuming Shi. 2024. On the trans-
formations across reward model, parameter update,
and in-context prompt. CoRR , abs/2406.16377.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-
ing factual knowledge in language models. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-
11 November, 2021 , pages 6491â€“6506. Association
for Computational Linguistics.
Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul
Smolensky, Kenneth D. Forbus, and Jianfeng Gao.
2020. Mapping natural-language problems to formal-
language solutions using structured neural representa-
tions. In Proceedings of the 37th International Con-
ference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event , volume 119 of Proceedings
of Machine Learning Research , pages 1566â€“1575.
PMLR.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, Albert Web-
son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
gun, Xinyun Chen, Aakanksha Chowdhery, Sharan
Narang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,
Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav
Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam
10Roberts, Denny Zhou, Quoc V . Le, and Jason Wei.
2022. Scaling instruction-finetuned language models.
CoRR , abs/2210.11416.
Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao
Chang, and Furu Wei. 2022. Knowledge neurons in
pretrained transformers. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 8493â€“
8502, Dublin, Ireland. Association for Computational
Linguistics.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-
ing factual knowledge in language models. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6491â€“
6506, Online and Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu,
Zhifang Sui, and Lei Li. 2022. Calibrating factual
knowledge in pretrained language models. In Find-
ings of the Association for Computational Linguistics:
EMNLP 2022, Abu Dhabi, United Arab Emirates, De-
cember 7-11, 2022 , pages 5937â€“5947. Association
for Computational Linguistics.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are key-
value memories. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP 2021, Virtual Event / Punta Cana,
Dominican Republic, 7-11 November, 2021 , pages
5484â€“5495. Association for Computational Linguis-
tics.
Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-
Hua Ling, Kai-Wei Chang, and Nanyun Peng. 2024.
Model editing can hurt general abilities of large lan-
guage models. arXiv preprint arXiv:2401.04700 .
Akshat Gupta, Anurag Rao, and Gopala Anu-
manchipalli. 2024. Model editing at scale leads to
gradual and catastrophic forgetting. In Findings of
the Association for Computational Linguistics, ACL
2024, Bangkok, Thailand and virtual meeting, Au-
gust 11-16, 2024 , pages 15202â€“15232. Association
for Computational Linguistics.
Thomas Hartvigsen, Swami Sankaranarayanan, Hamid
Palangi, Yoon Kim, and Marzyeh Ghassemi. 2022.
Aging with GRACE: lifelong model editing with dis-
crete key-value adaptors. CoRR , abs/2211.11031.
Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, and
Jun Zhao. 2024. Wilke: Wise-layer knowledge ed-
itor for lifelong knowledge editing. In Findings of
the Association for Computational Linguistics, ACL
2024, Bangkok, Thailand and virtual meeting, Au-
gust 11-16, 2024 , pages 3476â€“3503. Association for
Computational Linguistics.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. 2022. Lora: Low-rank adaptation of
large language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou,
Wenge Rong, and Zhang Xiong. 2023. Transformer-
patcher: One mistake worth one neuron. In The
Eleventh International Conference on Learning Rep-
resentations, ICLR 2023, Kigali, Rwanda, May 1-5,
2023 . OpenReview.net.
Haoqiang Kang, Juntong Ni, and Huaxiu Yao. 2023.
Ever: Mitigating hallucination in large language mod-
els through real-time verification and rectification.
CoRR , abs/2311.09114.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Teuvo Kohonen. 1972a. Correlation matrix memories.
IEEE Trans. Computers , 21(4):353â€“359.
Teuvo Kohonen. 1972b. Correlation matrix memories.
IEEE Trans. Computers , 21(4):353â€“359.
Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-
bovskaya, Devang Agrawal, Adam Liska, Tayfun
Terzi, Mai Gimenez, Cyprien de Masson dâ€™Autume,
TomÃ¡s KociskÃ½, Sebastian Ruder, Dani Yogatama,
Kris Cao, Susannah Young, and Phil Blunsom. 2021.
Mind the gap: Assessing temporal generalization
in neural language models. In Advances in Neural
Information Processing Systems 34: Annual Confer-
ence on Neural Information Processing Systems 2021,
NeurIPS 2021, December 6-14, 2021, virtual , pages
29348â€“29363.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017) , pages 333â€“342, Vancouver,
Canada. Association for Computational Linguistics.
Shuaiyi Li, Yang Deng, and Wai Lam. 2023a. Dep-
wignn: A depth-wise graph neural network for multi-
hop spatial reasoning in text. In Findings of the Asso-
ciation for Computational Linguistics: EMNLP 2023,
Singapore, December 6-10, 2023 , pages 6459â€“6471.
Association for Computational Linguistics.
Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun
Ma, and Jie Yu. 2023b. PMET: precise model editing
in a transformer. CoRR , abs/2308.08742.
Adam Liska, TomÃ¡s KociskÃ½, Elena Gribovskaya, Tay-
fun Terzi, Eren Sezener, Devang Agrawal, Cyprien
de Masson dâ€™Autume, Tim Scholtes, Manzil Zaheer,
Susannah Young, Ellen Gilsenan-McMahon, Sophia
Austin, Phil Blunsom, and Angeliki Lazaridou. 2022.
Streamingqa: A benchmark for adaptation to new
knowledge over time in question answering models.
InInternational Conference on Machine Learning,
ICML 2022, 17-23 July 2022, Baltimore, Maryland,
11USA, volume 162 of Proceedings of Machine Learn-
ing Research , pages 13604â€“13622. PMLR.
Kevin Meng, David Bau, Alex Andonian, and Yonatan
Belinkov. 2022. Locating and editing factual associ-
ations in GPT. In Advances in Neural Information
Processing Systems 35: Annual Conference on Neu-
ral Information Processing Systems 2022, NeurIPS
2022, New Orleans, LA, USA, November 28 - Decem-
ber 9, 2022 .
Kevin Meng, Arnab Sen Sharma, Alex J. Andonian,
Yonatan Belinkov, and David Bau. 2023. Mass-
editing memory in a transformer. In The Eleventh
International Conference on Learning Representa-
tions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 .
OpenReview.net.
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
Finn, and Christopher D. Manning. 2022a. Fast
model editing at scale. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Eric Mitchell, Charles Lin, Antoine Bosselut, Christo-
pher D. Manning, and Chelsea Finn. 2022b. Memory-
based model editing at scale. In International Con-
ference on Machine Learning, ICML 2022, 17-23
July 2022, Baltimore, Maryland, USA , volume 162 of
Proceedings of Machine Learning Research , pages
15817â€“15831. PMLR.
Niels MÃ¼ndler, Jingxuan He, Slobodan Jenko, and Mar-
tin T. Vechev. 2023. Self-contradictory hallucinations
of large language models: Evaluation, detection and
mitigation. CoRR , abs/2305.15852.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Fabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus,
Tim RocktÃ¤schel, Yuxiang Wu, Alexander H. Miller,
and Sebastian Riedel. 2020. How context affects
language modelsâ€™ factual predictions. In Conference
on Automated Knowledge Base Construction, AKBC
2020, Virtual, June 22-24, 2020 .
Yifu Qiu, Yftah Ziser, Anna Korhonen, Edoardo Maria
Ponti, and Shay B. Cohen. 2023. Detecting and miti-
gating hallucinations in multilingual summarisation.
InProceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2023, Singapore, December 6-10, 2023 , pages 8914â€“
8932. Association for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research ,
21(140):1â€“67.
Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR ,
abs/1910.01108.Imanol Schlag, Tsendsuren Munkhdalai, and JÃ¼rgen
Schmidhuber. 2021. Learning associative inference
using fast weight memory. In 9th International Con-
ference on Learning Representations, ICLR 2021, Vir-
tual Event, Austria, May 3-7, 2021 . OpenReview.net.
Imanol Schlag and JÃ¼rgen Schmidhuber. 2018. Learn-
ing to reason with third order tensor products. In
Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, December 3-8,
2018, MontrÃ©al, Canada , pages 10003â€“10014.
Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022.
Stepgame: A new benchmark for robust multi-hop
spatial reasoning in texts. In Thirty-Sixth AAAI Con-
ference on Artificial Intelligence, AAAI 2022, Thirty-
Fourth Conference on Innovative Applications of Ar-
tificial Intelligence, IAAI 2022, The Twelveth Sym-
posium on Educational Advances in Artificial In-
telligence, EAAI 2022 Virtual Event, February 22
- March 1, 2022 , pages 11321â€“11329. AAAI Press.
Gilbert Strang. 2022. Introduction to linear algebra .
SIAM.
Chenmien Tan, Ge Zhang, and Jie Fu. 2024. Massive
editing for large language models via meta learning.
InThe Twelfth International Conference on Learning
Representations, ICLR 2024, Vienna, Austria, May
7-11, 2024 . OpenReview.net.
S. M. Towhidul Islam Tonmoy, S. M. Mehedi Zaman,
Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha,
and Amitava Das. 2024. A comprehensive survey of
hallucination mitigation techniques in large language
models. CoRR , abs/2401.01313.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, AurÃ©lien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. CoRR , abs/2307.09288.
Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jian-
shu Chen, and Dong Yu. 2023. A stitch in time saves
nine: Detecting and mitigating hallucinations of llms
12by validating low-confidence generation. CoRR ,
abs/2307.03987.
Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi,
Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu
Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu,
Yuansheng Ni, Guozhou Zheng, and Huajun Chen.
2023. Easyedit: An easy-to-use knowledge edit-
ing framework for large language models. CoRR ,
abs/2308.07269.
Wanli Yang, Fei Sun, Xinyu Ma, Xun Liu, Dawei Yin,
and Xueqi Cheng. 2024. The butterfly effect of
model editing: Few edits can trigger large language
models collapse. In Findings of the Association
for Computational Linguistics, ACL 2024, Bangkok,
Thailand and virtual meeting, August 11-16, 2024 ,
pages 5419â€“5437. Association for Computational
Linguistics.
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,
Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu
Zhang. 2023. Editing large language models: Prob-
lems, methods, and opportunities. In Proceedings
of the 2023 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2023, Sin-
gapore, December 6-10, 2023 , pages 10222â€“10240.
Association for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin,
Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-
ter, Daniel Simig, Punit Singh Koura, Anjali Srid-
har, Tianlu Wang, and Luke Zettlemoyer. 2022.
OPT: open pre-trained transformer language mod-
els.CoRR , abs/2205.01068.
Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong
Wu, Jingjing Xu, and Baobao Chang. 2023. Can we
edit factual knowledge by in-context learning? In
Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2023, Singapore, December 6-10, 2023 , pages 4862â€“
4876. Association for Computational Linguistics.
Zexuan Zhong, Zhengxuan Wu, Christopher D. Man-
ning, Christopher Potts, and Danqi Chen. 2023.
Mquake: Assessing knowledge editing in language
models via multi-hop questions. In Proceedings of
the 2023 Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP 2023, Singapore,
December 6-10, 2023 , pages 15686â€“15702. Associa-
tion for Computational Linguistics.
Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh
Bhojanapalli, Daliang Li, Felix X. Yu, and Sanjiv
Kumar. 2020. Modifying memories in transformer
models. CoRR , abs/2012.00363.
A Experiment Details
All baselines are implemented using the EasyEdit
(Wang et al., 2023) library.
"subject": "Barbara Legrand ",
"src": "What  is Barbara Legrand's position on the field while 
playing football? ",
"pred": "midfielder" ,
"rephrase": "What is Barbara Legrand's position on the field 
during the football match? ",
"alt": "defender ",
"answers": ["goalkeeper "],
"loc": "nq question: who played donna in 2 pints of lager" ,
"loc_ans": "Natalie Casey" ,
"cond": "midfielder >> defender || What  is Barbara Legrand's 
position on the field while playing football?"Figure 8: A sample from ZsRE dataset.
"case_id ": 0,
"prompt": "The mother tongue of Danielle Darrieux is" ,
"target_new ": "English" ,
"subject": "Danielle Darrieux ",
"ground_truth ": "French" ,
"rephrase_prompt ": "Where Danielle Darrieux is from, people 
speak the language of" ,
"locality_prompt ": "Michel Rocard is a native speaker of" ,
"locality_ground_truth ": "French"
Figure 9: A sample from COUNTERFACT dataset.
Evaluation Metrics We employ three popular
editing evaluation metrics defined in (Yao et al.,
2023; Huang et al., 2023; Cao et al., 2021), i.e.,
reliability, generality, and locality. Given an initial
base model fÎ¸, a post-edit model fÎ¸â€², and a set of
edit instances {(xt, yt)}, the reliability is computed
as the average accuracy of the edit cases:
E(xt,yt)âˆˆ{(xt,yt)}{arg max yfÎ¸â€²(y|xt) =yt}.
(20)
The editing should also edit the equivalent neigh-
bor of the instance N(xt, yt)(e.g. rephrased de-
scriptions). This metric is named generality and is
evaluated by the average accuracy on the neighbors
of the edit cases:
E(xâ€²
t,yâ€²
t)âˆˆ{N(xt,yt)}{arg max yfÎ¸â€²(y|xâ€²
t) =yâ€²
t}.
(21)
Despite the editing, those instances that are irrel-
evant to the edit cases {O(xt, yt)}should not be
affected. This evaluation is called locality (also
known as specificity) and is measured by the pro-
portion of unchanged predictions between the ini-
tial model and the post-edit model:
E(xâ€²
t,yâ€²
t)âˆˆ{O(xt,yt)}{fÎ¸â€²(xâ€²
t) =fÎ¸(xâ€²
t)}.(22)
Datasets ZsRE is a question-answering dataset
that uses back-translation to generate equivalent
neighborhoods. It is initially used in factual knowl-
edge evaluation and later adopted in model editing
by (Mitchell et al., 2022a). COUNTERFACT is
13a challenging dataset focusing on counterfactual
information with a low prediction score compared
to correct facts. It builds out-of-scope data by re-
placing the subject entity with a similar description
that shares the same predicate.
Fig. 8 shows an example from the ZsRE dataset.
Each record in ZsRE contains the subject string,
the factual prompt used for testing reliability, the
rephrase prompt used for generality evaluation, and
the locality prompt used for evaluating the locality.
Note that what the locality demands the post-edit
model does is not to predict the ground truth but
whatever the initial base model predicts. Similarly,
the fact, rephrase, and locality prompts of each
record in COUNTERFACT are applied to the eval-
uation of the three metrics respectively (Fig. 9).
B Baselines and Implementation Details
Fine-tuning We implemented two fine-tuning
methods in the experiments. For FT-L, we fol-
low the procedure in (Meng et al., 2023, 2022) and
fine-tune the mlpprojparameter from layer 0 for
GPT2-XL and layer 21 for GPT-J since they are
found to have the optimal performance. FT-M5is a
small variation of FT-L, and it uses a different loss
computation procedure to optimize the parameters.
For both models, we conduct 25 optimization steps
using Adam optimizer (Kingma and Ba, 2015) and
use learning rate 5eâˆ’4. All other parameters of
both models follow default settings.
LoRA Hu et al. (2022) proposed a parameter-
efficient fine-tuning method that decomposes the
update gradient matrix into two small rank-n matri-
ces, which reduces the required memory for LLM
training to a large extent. In all experiments, we set
the learning rate and the rank number to 1eâˆ’3and
8, respectively. The Î±was chosen to be 32, and the
dropout rate was 0.1. The number of update steps
is 30 for GPT2-XL and 50 for GPT-J.
MEND MEND (Mitchell et al., 2022a) conducts
the editing by manipulating the language modelsâ€™
gradient. It trains a meta-network that employs a
rank-1 decomposition of the model gradients and
predicts a new rank-1 update to the corresponding
model weights. In this work, we train two meta-
networks using corresponding training split in the
ZsRE and COUNTERFACT datasets for GPT2-
XL following the default hyperparameter settings.
5https://github.com/zjunlp/EasyEdit/blob/main/
hparams/FT/gpt2-xl.yamlDue to the large required computation resource
for training GPT-J (6B) meta-network, we do not
perform training for GPT-J.
SERAC Mitchell et al. (2022b) designed a
memory-augmented editing method, which re-
quires an external cache to store explicit editing
cases. It also adopts a scope classifier that deter-
mines whether an input sample falls in the editing
scope and a small counterfactual model for edit-
ing the in-scope cases. For both GPT2-XL and
GPT-J, we train two separate models for the two
datasets, respectively. Following the original paper,
we apply distilbert-base-cased (Sanh et al., 2019)
for the scope classifier across all models. For the
small counterfactual model, we employ GPT2 for
GPT2-XL and gpt-j-tiny-random6for GPT-J. All
hyperparameters follow default settings.
MEMIT MEMIT (Meng et al., 2023) treats the
feedforward layer of transform as a linear asso-
ciative memory and uses a minimum square error
optimization to add new key-value associations to
layer weights. We follow the original paper to edit
the layers in the identified critical path and set the
balance factor Î»to the optimal value found in the
original work. Other parameters for the two models
are all set based on configurations in (Meng et al.,
2023, 2022).
GRACE Hartvigsen et al. (2022) proposed an
editing method that preserves the original model
parameters and adopts a codebook maintained by
adding, splitting, and expanding keys over time
to store relevant edits. We follow the optimized
settings in the original paper and set the value opti-
mizing learning rate to 1. The number of iterations
for optimizing the values is 100, and the initial Îµ
value is chosen to be 1. The codebook is employed
at layers 35 and 25, respectively.
CoachHooK CoachHooK expands the update
mechanism in MEMIT to consecutive cases and
applies hook layers to separate the weight change
from the original model layer. For both models, we
setÎ»= 15,000,Î±z= 2.2for consecutive batch
editing. Unless specified, we evaluate our method
on full critical path layers identified in (Meng et al.,
2023). We employ the same procedure in MEMIT
(Meng et al., 2023) to compute the updating keys
and the target values, except that the most recently
updated model during the process of consecutive
6https://huggingface.co/anton-l/gpt-j-tiny-random
14200 400 600 800 10000.5500.5750.6000.6250.6500.675
ZsRE Reliability
200 400 600 800 10000.440.460.480.500.520.540.560.58
ZsRE Generality
200 400 600 800 10000.880.900.920.940.960.98
ZsRE Locality
200 400 600 800 10000.600.650.700.750.800.850.90
COUNTERFACT Reliability
200 400 600 800 1000
x - the number of samples0.200.250.300.350.40
COUNTERFACT Generality
200 400 600 800 10000.600.620.640.660.680.700.72
COUNTERFACT Locality
All layers Three layers One layerFigure 10: Performance comparisons on the different
number of editing layers. Layers are selected from the
critical path identified in (Meng et al., 2023).
editing is applied for relevant computations. We
applied "torch.float16" for the GPT-J model for all
experiments.
C More detailed analysis and discussions
Effect of the Number of Editing Layers To in-
vestigate the necessity of applying the hook layer
onto multiple transformer layers, we conduct the
consecutive batch editing experiment on the ZsRE
dataset for GPT2-XL (Fig. 10). As the effect of
choosing different layers has already been stud-
ied in (Meng et al., 2023), we focus only on the
effect of the number of layers. We selected the
last one, three, and all layers from the critical path
identified in (Meng et al., 2023; Yao et al., 2023),
respectively.
As shown in Fig. 10, the one-layer case signifi-
cantly underperforms the other two cases in most
of the metrics for the two datasets, which directly
certifies the necessity of the expansion. In ZsRE,
the difference between the performance for one
layer and multiple layers tends to enlarge in reli-
ability and generality as the consecutive editing
steps increase. This may serve as evidence of our
assumption in section 3.2, which mentions that the
latter hook layer may capture the in-scope instances
missed in former hook layers. Additionally, the all-
layer case has slightly better generality than the
three-layer case, and they do not show a remark-
able difference in locality and reliability. A similar
situation could be found in the reliability and gen-
erality of the COUNTERFACT with an interesting
exception in the locality, where an adverse perfor-
mance order of the cases is shown. Nevertheless,
200 400 600 800 10000.00.20.40.60.8
COUNTERFACT Reliability
200 400 600 800 1000
x - the number of samples0.10.20.30.4
COUNTERFACT Generality
200 400 600 800 10000.50.60.70.80.91.0
COUNTERFACT Locality
initial =1
initial =2
initial =3
initial =4
fixed =3
Figure 11: Performance comparisons on different Î±z.
Model Type Inference Time (s)
GPT2-XLPre-edit 0.1187
Post-edit 0.1297
GPT-JPre-edit 0.0762
Post-edit 0.0863
Table 3: Inference time analysis.
the margin of the locality fall is not that manifest
in contrast with the advancement in reliability and
generality.
Effect of the Initial Threshold Î±zÎ±zis the ini-
tialization value of Î±used in the identification of
local editing scope (section 3.1.3). We study its in-
fluence in this part. According to Fig.11, although
theÎ±z= 1 case ranks the highest in the first 60
editing steps in generality, it consistently performs
the worst in locality, indicating that it fails to in-
tercept many out-scope inputs. This implies that 1
may be too low for the initialization. Other cases do
not show noticeable differences in the three metrics
since Î±zis just the initial value and Î±is determined
dynamically. It seems that overly low Î±zwould
damage the hook layerâ€™s capacity to discriminate
in-scope and out-scope samples. Considering the
unpredictable consecutive steps that our method
may be applied, we select a relatively low value
between 2 and 3, namely, Î±z= 2.2.
To verify the significance of the dynamical deter-
mination process, we also test the fix Î±case. We
chose the value of 3, the standard threshold used in
standardization to detect outliers. The results reveal
a dramatic decline in reliability and generality and
perfect fulfillment in the locality, indicating that
almost all instances are indiscriminately obstructed
by the hook layers regardless of the editing scope.
Besides, choosing an optimal fixed Î±before edit-
ing is practically unrealistic. Therefore, it would
be more reasonable to decide Î±dynamically.
15Model GranularityInstances
Reliability Generality Locality
GPT-JInstances 99.00 97.50 -
Overall tokens 9.69 12.51 11.19
Unwanted tokens 0.38 0.13 11.19
Table 4: Percentage of instances/tokens that used the
hook layer.
Deferral
RadiusModelCOUNTERFACT
Reliability Generality Locality Average
Îµ= 1
GPT2-XL100 0.40 100 66.80
Îµ= 3 100 0.42 100 66.81
Îµ= 5 100 0.65 99.50 66.72
Îµ= 10 100 1.80 93.70 65.17
Îµ= 20 100 18.30 56.60 58.30
Îµ= 30 100 83.90 7.40 63.77
Îµ= 1
GPT-J100 0.50 100 66.83
Îµ= 3 100 0.54 100 66.85
Îµ= 5 100 0.57 100 66.86
Îµ= 10 100 0.68 99.60 66.76
Îµ= 20 100 5.00 93.20 66.07
Îµ= 30 100 31.30 58.90 63.40
Table 5: Results of GRACE with increased Îµ.
Investigation on hook layer employment Al-
though the validation of the hook layer has been
proved in section 4.4, we conducted extra experi-
ments to survey how many entries that should apply
the hook layer indeed use the hook layer and vice
versa. We investigated three granularity: instances,
overall tokens, and unwanted tokens (Table 4). Sup-
pose the number of instances is A, the total number
of tokens for the set of instances is T, and there are
Tâ€²tokens that used the hook layer and Aâ€²instances
have their updated keys7(the last subject token)
use the hook layer. The instance granularity was
measured byAâ€²
A, the overall tokens granularity was
calculated byTâ€²
T, and the unwanted tokensTâ€²âˆ’Aâ€²
T.
The results show that almost all reliability and
generality instances apply the hook layer, and few
unwanted tokens mistakenly use the hook layer.
This again demonstrates the effectiveness of our
methodâ€™s editing scope identification.
GRACE with greater deferral radius Although
we followed the settings found in the original pa-
per of GRACE (Hartvigsen et al., 2022), one may
argue that the terrible generality performance of
GRACE in Table 2 is caused by the over small
deferral radius ( Îµ) and increasing it may help the
7Each instance only has one updated key.model reach a better balance between generality
and locality, then resulting in an improved over-
all average. Therefore, we further conducted the
consecutive batch editing experiments for GRACE
with several increased Îµon the COUNTERFACT
dataset, the result is shown in Table 5.
It is not hard to find from the results that, though
the results indeed show the trade-off between gen-
erality and locality, the average does not show great
improvement. This proves that merely increasing
the deferral radius for GRACE does not necessarily
improve its overall average performance.
Inference Time Analysis As our method will in-
troduce new hook layers to the model, we conduct
an experiment to investigate its influence on the
model inference. We run GPT2-XL on NVIDIA
Titan GPU and GPT-J on NVIDIA A6000. Table
3 shows the running result for the corresponding
pre-edit and post-edit models. The hook layersâ€™
employment does not seem to delay the model in-
ference too much. This may result from the fact
that the hook layers are only introduced for the
small proportion of layers in the critical path, and
the computation implemented in the hook layers is
relatively simple.
Memory Analysis Unlike GRACE (Hartvigsen
et al., 2022), whose memory requirement grows
over time and SERAC (Mitchell et al., 2022b),
which needs extra memory for counterfactual
model and scope classifier, the memory require-
ment of our method remains unchanged over time.
Therefore, the final memory requirement is fixed no
matter how many edits you make to the model. The
initial memory requirement is acceptable since it is
at maximum the copy of the 6 to 7 FFN projection
layer weights in the model. Specifically, the hook
layers are only applied to a set of identified layers,
which usually accounts for a small proportion of the
whole layers. For example, the number of identified
layers for GPT-J-6B is 6, which is [3, 4, 5, 6, 7, 8],
and 5 for GPT2-XL, which is [13, 14, 15, 16, 17].
Furthermore, it is not compulsory to hang hook lay-
ers to all the identified layers, user can decide how
many layers they want to edit. For convenience, we
assume to use all the identified layers here. Take
the GPTJ-6B as an example, a projection FFN layer
weight dimension is 16384 Ã—4096 , assuming the
data type is float32, then GPU memory required
by its hook layer (just a copy of itself) is approxi-
mately16384Ã—4096Ã—4
10243 = 0.25GB (ignore the bias).
Now, the editing layers used in our approach for
16GPT-J-6B is [3,4,5,6,7,8], so the required memory
is around 0.25GBÃ—6 = 1 .5GB.
17