MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction
Xiaolu Liu1, Song Wang1, Wentong Li1, Ruizi Yang1, Junbo Chen2*, Jianke Zhu1*
1Zhejiang University2Udeer.ai
{xiaoluliu, songw, liwentong, ruiziyang, jkzhu }@zju.edu.cn, junbo@udeer.ai
Abstract
Currently, high-definition (HD) map construction leans
towards a lightweight online generation tendency, which
aims to preserve timely and reliable road scene informa-
tion. However, map elements contain strong shape pri-
ors. Subtle and sparse annotations make current detection-
based frameworks ambiguous in locating relevant feature
scopes and cause the loss of detailed structures in predic-
tion. To alleviate these problems, we propose MGMap,
a mask-guided approach that effectively highlights the in-
formative regions and achieves precise map element lo-
calization by introducing the learned masks. Specifically,
MGMap employs learned masks based on the enhanced
multi-scale BEV features from two perspectives. At the in-
stance level, we propose the Mask-activated instance (MAI)
decoder, which incorporates global instance and struc-
tural information into instance queries by the activation
of instance masks. At the point level, a novel position-
guided mask patch refinement (PG-MPR) module is de-
signed to refine point locations from a finer-grained per-
spective, enabling the extraction of point-specific patch
information. Compared to the baselines, our proposed
MGMap achieves a notable improvement of around 10 mAP
for different input modalities. Extensive experiments also
demonstrate that our approach showcases strong robust-
ness and generalization capabilities. Our code can be found
at https://github.com/xiaolul2/MGMap.
1. Introduction
High-definition (HD) map plays an important role in au-
tonomous driving [19], as it provides centimeter-level road
information for self-positioning [18], path planning [7, 11]
and other downstream tasks [14, 15, 17]. Typically, offline
HD map constructions rely on manual annotations on global
LiDAR point clouds, which usually require tedious finan-
cial and manual investments. Moreover, it is challenging
for offline maps to reflect the ever-changing road conditions
due to the lack of real-time updates. To tackle the above is-
*Corresponding authors
w/o Mask Front View Image
Learned 
MaskOurs GT LabelFigure 1. For some detailed structures, our proposed MGMap
achieves effective map element localization by highlighting the in-
formative regions through the learned masks.
sues, a lightweight and real-time online map construction
paradigm [19] has gradually become a promising approach
by incorporating information from onboard sensors.
Some existing approaches [10, 19, 36, 43, 50] consider
online map construction as a segmentation task, where
pixel-level rasterized maps are learned in bird‚Äôs-eye-view
(BEV) space. Nevertheless, for vectorized expression, an
additional step of post-processing is required to cluster
and fit lane instances. Recently, efficient and straightfor-
ward approaches like VectorMapNet [26] and MapTR [23]
have been proposed for vectorized map construction, in
which map elements are represented by sparse point sets.
Transformer-based architectures are directly employed to
update instance queries and regress point locations.
Despite having achieved promising results, current map
vectorization frameworks are still constrained by inherent
issues. As shown in Figure 1, map elements, such as road
edges, dividing lines, and pedestrian crossings, always have
strong shape priors. Obscure features and coarse locations
can easily lead to the loss of detailed expressions in predic-
tion, especially for irregular boundaries and sudden changes
in corner angles. Besides, subtle and sparse annotations
pose significant challenges for deformable attention [51],
which is known as a sparse and local feature extraction strat-
egy employed in current vectorization frameworks. Consid-
ering the sparsity of detection targets, such under-sampling
techniques can readily lead to coarse localization and the
loss of effective information. These issues intensify the dif-
ficulties in determining the relevant feature scopes, confirm-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14812
ing lane line instances, and accurately locating lane points.
To tackle the above issues, in this paper, we propose a
fine-grained approach called MGMap, which aims to im-
prove localization and highlight specific features by incor-
porating the guidance of learned map masks. Initially, the
enhanced multi-level BEV features are dynamically con-
structed for richer semantic and position features. Based on
this, masks can be generated and adequately utilized at the
holistic instance level and the more granular point level. At
the instance level, we design the Mask-Activated Instance
(MAI) decoder to guide the construction and feature ag-
gregation of lane queries. By leveraging learned instance
masks, MAI decoder enables the activation of lane queries
to possess global instance structures and shape character-
istics upon activation. Furthermore, at the point level, a
Position-Guided Mask Patch Refinement (PG-MPR) mod-
ule is proposed to alleviate the difficulty in locating relevant
features due to the sparse and irregular detection targets.
By focusing on the specific patch regions, binary mask fea-
tures can be extracted to gather more detailed information
at lanes‚Äô surrounding locations, which is well-designed for
finer regression of the detailed structure and point locations.
Extensive experiments on nuScenes [1] and Argov-
erse2 [46] datasets demonstrate that MGMap achieves
state-of-the-art (SOTA) performance on the task of online
HD map construction. Besides, the promising experiment
results under different settings show the robustness and gen-
eralization capability of our presented model. The main
contributions of our work can be summarized as follows:
‚Ä¢ An effective approach for precise online HD map vector-
ization with the guidance of learned masks. The effective
features of instance masks and binary masks are extracted
for unique lane lines and shape learning.
‚Ä¢ A mask-activated instance decoder and a novel position-
guided mask patch refinement module to decode map ele-
ments from the instance level and the point level by fully
leveraging the potential of mask features.
‚Ä¢ Promising results on two testbeds show that our MGMap
outperforms the previous approaches at a large margin
and has strong robustness and generalization capability.
2. Related Work
Online HDMap Construction. Unlike traditional offline
HD map annotations on global LiDAR points [37, 38], re-
cent studies [19, 23, 26, 43] explore the online construction
directly from on-board sensor data to reduce the cost of la-
beling and provide up-to-date road information. Some ap-
proaches [10, 19, 20, 33, 43] treat HD map construction as
a segmentation task to predict pixel-level rasterized maps,
which requires post-processing for vectorized construction.
For more straightforward vectorized construction, Liu et al.
[26] propose a two-stage framework VectorMapNet with
an auto-regressive decoder to recurrently connect vertices.Jeong et al. [39] detect points first and employ an adjacent
matrix in InstaGraM to build the connections among in-
stance points. Further, Liao et al. [23, 24] propose to repre-
sent map elements as the ordered points with fixed numbers
in MapTR, so that transformer architecture can be used to
regress points‚Äô positions simultaneously. Later, Qiao et al.
[32] and Ding et al . [9] present new modeling strategies,
in which BeMapNet and PivotMap utilize Bezier curves
and dynamic pivot points to model map elements separately
for more detailed representations. In contrast to the above
methods, our MGMap approach introduces the guidance of
generated mask features to handle lane shapes in detail with
specific feature enhancement.
Camera-based BEV Perception. HD map construction re-
lies on high-quality BEV features, which are also the basis
for most 3D perception tasks [16, 25, 31, 44, 45]. Gen-
erally, BEV features are extracted and transformed from
perspective-view (PV) images. Initially, Reiher et al. [35]
utilize homography transformation to project PV images
into BEV space in IPM. After that, learning-based ap-
proaches are widely used to construct more reliable BEV
features [2, 44]. Pan et al. [29] employ a fully connected
layer in VPN to convert perspective view features into BEV
space. In [31] and [16], depth estimation is utilized to estab-
lish the connection between the surrounding view images
and BEV features. In order to enhance the robustness of
the model and the effect of detection, multi-modality and
temporal fusion strategies are used in BEVFormer [20] and
BEVFusion [22, 27]. Besides, transformer-based architec-
tures [20, 25, 30] are widely used for feature aggregation.
BEV features are represented by queries at different posi-
tions, which can be updated by interaction with PV image
features. To obtain a more reliable HD map, we employ a
pyramid-like network for multi-scale BEV features, which
enables to capture the rich semantic and location informa-
tion for online HD map construction.
Mask Refinement for Segmentations. Mask refinement
strategies are widely used for different segmentation tasks,
aiming to improve the quality of instance or semantic fea-
tures. In previous work [5] and [40], boundary-aware mask
features are constructed by an extra branch, which is used to
improve mask localization accuracy. Cheng et al. [6] utilize
the interaction between learned mask features with instance
activation maps for representing objects with instance fea-
tures. In [3, 4, 8, 21], mask features are integrated into the
transformer-based architecture, in which attentions are uti-
lized for feature extraction. Based on the initial prediction
outputs, Tang et al. [42] propose a refinement strategy with
small boundary patches to improve the mask quality. As for
map construction, our proposed MGMap approach takes ad-
vantage of the learned mask features from both the instance
level and point level to highlight and enhance informative
regions of subtle map annotations.
14813
BEV 
ExtractorMAI Decoder PG-MPR
Mask Feature
Shared  
BackboneConcatenate 
& Fuse
‚Ä¶Crop & ROIAlign
(ùíôùíôùíäùíäùíéùíé,ùíöùíöùíäùíäùíéùíé)(ùíôùíôùíãùíãùíèùíè,ùíöùíöùíãùíãùíèùíè)
(ùíôùíôùíåùíåùíçùíç,ùíöùíöùíåùíåùíçùíç)
Query Coordinates‚Ä¶ ‚Ä¶ ‚Ä¶
Instance Decoder
Final Map 
Prediction Query EmbeddingsInstance Decoder
Query Coordinates‚Ä¶Ins. Mask
Point Query 
(Predefined)Lane Query 
(MAI)
Hybrid QueryMask Feature 
Construction
Multi -LevelMulti -Head 
AttentionQuery Embeddings MLPPV2BEV 
TransformEnhanced BEV
Initial BEV
EML Neck
Figure 2. Overview of MGMap framework. MGMap mainly consists of three components: (1) BEV Extractor to obtain multi-scale
BEV features by transforming from perspective view (PV) to BEV with the enhanced multi-level neck; (2) Mask-Activated Instance (MAI)
Decoder is employed to construct and update queries at instance level; (3) Position-Guided Mask Patch Refinement (PG-MPR) module is
designed to refine points‚Äô positions from local patch features at point level.
3. MGMap
Given surround-view images captured from onboard cam-
eras, our goal is to distinguish local BEV map instances
while locating their corresponding structures. Each map el-
ement comprises a class label cand an ordered sequence of
points P={(xi, yi)}N
i=1that represents the lane structure,
withNdenoting the number of points for each lane.
Figure 2 illustrates the framework of our proposed
MGMap. To obtain multi-scale BEV features, a pyramid
network with fused attention is designed, which is built af-
ter initial PV-to-BEV feature transformations (Section 3.1).
Subsequently, a cascaded transformer decoder is utilized to
update mask-activated instance queries (Section 3.2). Fi-
nally, by interacting with the local semantic context derived
from the patch of mask features, we achieve fine-grained
position refinement at the point level (Section 3.3).
3.1. BEV Feature Extraction
Initially, we employ shared CNN backbones to extract 2D
features from PV images. PV features can be gathered
into a unified BEV representation via the strategy in [20],
which utilizes deformable attention [51] to update BEV
queries by interacting with surround-view image features.
The extracted BEV feature can be represented as F0‚àà
RD√óHBEV√óWBEV, where HBEV√óWBEV is the size ofthe BEV feature and Drepresents the dimension.
Enhanced Multi-Level Neck. To obtain BEV features with
rich semantic and location information, at BEV space, we
design a 3-layer Enhanced Multi-Level (EML) neck with
fused attention to construct the unified BEV features. Multi-
scale BEV features with larger receptive fields can be ob-
tained for a better understanding of the overall structures.
In the EML neck, we construct the cascaded layers us-
ing residual blocks based on the hybrid of channel atten-
tion (CA) and spatial attention (SA) [47], in which hybrid
multiplications among channels and spaces are designed to
capture both local and global contextual information. Such
dynamic selections at different BEV spaces are beneficial
to detect lane lines with irregular shapes. The calculation of
learnable attention maps can be formulated below
Fi+1= (CA(Fi)√óFi)√óSA(Fi). (1)
After that, multi-level BEV features {Fi}3
i=1can be ob-
tained with the shape of Di√óHBEV
2i+1√óWBEV
2i+1for each layer
i, where Diis the corresponding dimension. To preserve all
the information at different levels, we employ bilinear in-
terpolation to upsample 3-level outputs {Fi}3
i=1. All these
features are aligned to have the same resolution as the initial
F0. Finally, a 3√ó3convolutional layer after concatenation
is used to aggregate multi-level features and obtain the en-
hanced BEV features Fc. Such enhanced Fccontains local
14814
and semantic information across varying receptive fields,
enhancing the capability to detect fine-grained structures.
3.2. Mask-Activated Instance Decoder
For each lane instance, specific query embeddings with in-
stance and structure information are required for the re-
gression of lane shapes and positions. Based on enriched
BEV features, this section focuses on the design of mask-
activated lane queries and the subsequent update through
the cascaded deformable transformer decoder.
Mask-Activated Query. To achieve a more detailed and
specific representation, MGMap employs a hybrid approach
that combines lane queries Qlane and point queries Qpoint
to encode individual map instances.
For lane queries, in contrast to the fixed query de-
signs [23, 51], Qlane‚ààRM√óDis dynamically initialized
with the guidance of learned instance masks, which is more
flexible and contains specific shape prior and instance infor-
mation. Here Mis the number of instance queries and D
is the dimension. Specifically, we obtain a set of instance
segmentation maps Mins‚ààRM√óHBEV√óWBEV by apply-
ing basic convolution with the sigmoid function œÉ(¬∑)to the
enhanced BEV feature Fc, as illustrated in the lower half
of Figure 3. By leveraging Mins, we generate the mask-
activated instance query Qlane via multiplication between
instance masks Minsand the transpose of enhanced BEV
features
Qlane= (œÉ((Conv(Fc)))√óF‚ä§
c. (2)
During the training stage, Minsis supervised by instance
mask annotations, and bipartite matching is used to pair
each instance mask with the ground-truth label. This ac-
tivated instance query allows for a more tailored and pre-
cise representation of each map element, in which unique
features in instance masks can be aggregated to the specific
instance queries.
For point queries, Qpoint‚ààRN√óDis constructed using
a set of predefined learnable weights, which are broadcasted
by multi-layer perceptions (MLP) to fuse with each Qlane.
Thus, the hybrid query Q‚ààRM√óN√óDcan be obtained by
Q=Qlane+MLP(Qpoint). (3)
Deformable Decoder. To update the hybrid quires, an
L-layer decoder can be built by following the multi-scale
deformable DETR [51] structure. At each layer l, query
embeddings can be updated by interacting with structured
multi-level BEV features. Specifically, deformable DETR
assigns reference points Plas anchors to collect sparse fea-
tures from {{Fi}3
i=1,Fc}, in which Plis also the interme-
diate stage of the normalized lane point positions. Simulta-
neously, the updated lane point positions Pl+1can be ob-
tained by adding the learned offsets, which are derived from
ùêÖ‡ØñC
ùêå‡Øú‡Ø°‡Ø¶œÉ(MAI decoder) 
Lane Query ùêê‡Øü‡Øî‡Ø°‡Øò
ùêÜ‡Øï‡Øò‡Ø©
œÉ D
ùêå‡Øï(PG-MPR) 
Mask Feature
ùë≠‡Ø†(2√óH√óW)
(D√óH√óW)
(M√óH√óW) (M√óD)Positional Grid
Enhanced BEV(D√óH√óW)
Instance Mask
œÉ:Sigmoid :Concat.C :DensifyD :Multip.Binary Mask
Camera Feature
(Under BEV)
LiDAR Feature
(Under BEV)C Conv 3√ó3
C œÉFusion Feature
(Under BEV)
BEV
LiDARFBEV
CameraFBEV
FusionF
Conv 3√ó3ConvConvConv
Conv
Figure 3. Illustration of mask constructions at different stages.
In MAI decoder, instance masks are generated to activate lane
queries, while binary masks are extracted to provide fine-grained
patch features in PG-MPR.
MLP regression branch Regposon query Qlas follows
Ql=DeformAttn (Ql‚àí1,Pl,{Fi}3
i=1,Fc), (4)
Pl+1=œÉ(œÉ‚àí1(Pl) +Regpos(Ql)). (5)
The classification scores cl+1can also be updated by MLP
regression on Pl. With iterative interaction in the L-layer
decoder, MGMap can progressively update the semantic
and positional information to query embeddings and con-
stantly revise the shape and position of lane lines.
3.3. Position-Guided Mask Patch Refinement
Although the general shapes and structures can be regressed
from the instance level, some detailed expressions are still
hard to construct. Therefore, it is necessary to achieve fine-
grained refinement at a more specific point level. In this sec-
tion, we design a refinement module to utilize binary mask
features more specifically.
Mask Feature Construction. As shown in Figure 3, bi-
nary mask Mb‚ààR2√óHBEV√óWBEV can be obtained by ap-
plying sigmoid function after basic convolution on Fc, in
which auxiliary loss with rasterized supervision is used at
the training stage. With the guidance of binary segmenta-
tion learning, more highlighted features can be utilized to
distinguish lane line information from the background.
After that, the binary mask feature Fmcan be con-
structed based on the binary map Mb. We densify the di-
mension of Mbfrom 2 to 32 by D (¬∑). Then, we concatenate
the densified binary map with Fcand a 2-channel normal-
ized positional grid Gbev, which comprises spatial local in-
formation features for each pixel to provide relative location
information. Finally, these three kinds of characteristic fea-
tures are fused by the convolutional operation as below
Fm=Conv(Concat (Fc,D(Mb),Gbev)). (6)
Compared to Fc,Fmemphasizes specific location and se-
mantic information around lane lines to differentiate the
14815
GT Points Reference Points Sampling Positions
Learned Offsets
 Patch Regions
Sparse & Coarse
Sample  Dense & Reliable Sample  
(ùíôùíä,ùíöùíä)d
RegionforROI Align
(a) Original Deformable Attention (b) Mask Patch Refinement (Ours)Figure 4. (a) The conventional deformable attention extracts
sparse features from sampling points, which may select irrelevant
features; (b) Our proposed Mask Patch Refinement extracts more
relevant features from the region of reliable patch.
feature from ambiguous backgrounds, which provides more
clear location information.
Patch Extraction and Refinement. As shown in Figure 4
(b), for the i-th point of the lane line, the patch region is a
squared bounding box Bicentered at the point‚Äôs coordinates
(xi, yi), which is regressed from the last layer of the MAI
decoder. The size of the patch region is determined by the
hyper-parameter d.
Once the location and region scale are determined, we
apply the function fext, to extract local patch features vi
for each point in each map instance. This process is imple-
mented using ROIAlign [13], which is employed to extract
and unify the features as follows
vi=fext 
Fm,R(Bi),(xi, yi)), (7)
where R (¬∑)denotes the denormalized patch region compu-
tation. The extraction function fextutilizes basic convo-
lutional blocks after bilinear interpolation and pooling to
locate and align the corresponding values in the patch fea-
tures. Consequently, each extracted semantic patch feature
vihas the shape of D√ó5√ó5, which effectively mitigates the
issue of misalignment. Compared to the sparse deformable
attention design in Figure 4 (a), the mask patch design con-
tains dense and more reliable features.
With query embedding Qfrom the L-layer MAI decoder
and patch region feature V={vi}N
i=0, we leverage multi-
head attention to refresh the query features, which can be
written as follows
Qs+1=MultiHeadAtt QsVs
‚àö
D
, (8)
where srepresents the stage in the PG-MPR module and
Dis the dimension of features. Query embeddings can be
refreshed at a more detailed and specific point level.
Finally, point coordinates and the classification scores
can be regressed from query Qsthrough MLP branchesRegposand Regcls, which can be formulated as
Ps+1=œÉ(œÉ‚àí1(Ps) +Regpos(Qs)), (9)
cs+1=Regcls(Qs). (10)
3.4. Training Loss
MGMap is trained in an end-to-end manner. Bipartite
matching is employed to pair predicted map instances with
their ground-truth counterparts. With the regression of
points and class labels, auxiliary loss is required for mask
segmentation. Concretely, the total loss is the sum of detec-
tion loss and mask segmentation loss L=Ldet+Lmask .
Detection Loss. Lane detection aims to regress lane coor-
dinates and classification labels. As in [23], we employ L1
loss to calculate point-to-point Manhattan distance between
the predicted points ÀÜpijand ground-truth pij. The edge di-
rection loss is also considered in adjacent points by cosine
similarity. Lfocal calculates the classification loss, which
can be formulated as
Llane=M,NX
i,j=0 
ŒªdisDis( ÀÜpij, pij)+ŒªdirCosSim ( ÀÜeij, eij)
,(11)
Ldet=Llane+ŒªclsMX
i=0Lfocal(ÀÜci,ci), (12)
where pijis the j-th point of the i-th instance and eijis the
direction of two adjacent points. Œªdis,Œªdir, andŒªclsare the
weighted factors for the losses of point regression, direction
adjustment, and label regression, respectively.
Mask Construction Loss. Mask learning is able to re-
duce the risk of overfitting by pixel-level intensive supervi-
sion. We use the combination of cross-entropy loss and dice
loss [28] to deal with the unbalanced segmentation prob-
lems. Given the output masks ÀÜMinsandÀÜMb, we formulate
the auxiliary loss as below
Lmask =ŒªinsLins(ÀÜMins,Mins) +ŒªbLb(ÀÜMb,Mb),(13)
where ŒªinsandŒªbare the corresponding loss weights.
4. Experiments
4.1. Datasets and Benchmarks
We conduct extensive experiments on two public datasets,
including nuScenes [1] and Argoverse2 [46]. nuScenes
dataset contains 1000 driving scenes collected from Boston
and Singapore. 750 and 150 scene sequences are anno-
tated for training and validation, respectively. Each scene
sequence consists of 40-keyframe data with a sampling rate
of 2Hz. For each keyframe, there are 6 PV images and the
corresponding point clouds from 32-beam LiDAR. Argov-
erse2 contains 1000 scenes collected from six cities with 7
PV images. We use a subset of Argoverse2 , which is pro-
vided by the Online HD Map Construction Challenge1. We
1https://github.com/Tsinghua-MARS-Lab/Online-HD-Map-
Construction-CVPR2023
14816
APchamfer APrasterMethod Backbone Epochsped. div. bou. avg. ped. div. bou. avg.FPS
Camera-Based Methods
HDMapNet [ICRA22] [19] EB0 30 14.4 21.7 33.0 23.0 - - - - -
InstaGraM [CVPRW23] [39] EB4 30 33.8 47.2 44.0 41.7 - - - - -
MapTR [ICLR23] [23] R50 24 46.3 51.5 53.1 50.3 32.4 23.5 17.1 24.3 15.7
MapTR [ICLR23] [23] R50 30 45.2 53.8 54.3 51.1 32.9 24.9 18.9 25.6 15.7
MapVR [NeurIPS23] [49] R50 24 47.7 54.4 51.4 51.2 37.5 33.1 23.0 31.2 15.7
PivotNet [ICCV23] [9] R50 30 53.8 55.8 59.6 57.4 - - - - 9.6
BeMapNet [CVPR23] [32] R50 30 57.7 62.3 59.4 59.8 - - - - 4.4
MGMap(Ours) R50 30 57.4 63.5 63.3 61.4 46.5 36.5 28.7 37.2 11.6
MapTRV2 [arxiv23] [24] R50 24 59.8 62.4 62.4 61.5 - - - - -
MGMap*(Ours) R50 24 61.8 65.0 67.5 64.8 - - - - -
VectorMapNet [ICML23] [26] R50 110+ft 42.5 51.4 44.1 46.0 - - - - -
MapTR [ICLR23] [23] R50 110 56.2 59.8 60.1 58.7 43.6 35.7 25.8 35.0 15.7
MapVR [NeurIPS23] [49] R50 110 55.0 61.8 59.4 58.8 46.0 39.7 29.9 38.5 15.7
BeMapNet [CVPR23] [32] R50 110 62.6 66.7 65.1 64.8 - - - - 4.4
MGMap(Ours) R50 110 64.4 67.6 67.7 66.5 54.5 42.1 37.4 44.7 11.6
LiDAR-Based Methods
HDMapNet [ICRA22] [19] PP 30 10.4 24.1 37.9 24.1 - - - - -
VectorMapNet [ICML23] [26] PP 110 42.5 51.4 44.1 34.0 - - - - -
MapTR [ICLR23] [23] Sec 24 48.5 53.7 64.7 55.6 38.9 30.1 41.1 36.7 6.0
MGMap(Ours) Sec 24 63.5 66.7 73.6 67.9 53.4 44.4 52.6 50.1 5.5
Camera-LiDAR Fusion Methods
HDMapNet [ICRA22] [19] EB0 & PP 30 16.3 29.6 46.7 31.0 - - - - -
VectorMapNet [ICML23] [26] EB0 & PP 110+ft 48.2 60.1 53.0 53.7 - - - - -
MapTR [ICLR23] [23] R50 & Sec 24 55.9 62.3 69.3 62.5 46.4 38.4 49.2 44.7 5.2
MapVR [NeurIPS23] [49] R50 & Sec 24 60.4 62.7 67.2 63.5 52.4 46.4 54.4 51.1 5.2
MGMap(Ours) R50 & Sec 24 67.7 71.1 76.2 71.7 59.6 47.3 54.6 53.8 4.8
Table 1. Quantitative evaluation of map vectorization on nuScenes val . at60m√ó30mperception range under different input modalities
and backbone settings, ‚ÄúEB0‚Äù, ‚ÄúEB4‚Äù, ‚ÄúR50‚Äù, and ‚ÄúSec‚Äù correspond to the backbones Efficient-B0, Efficient-B4 [41], ResNet50 [12], and
SECOND [48] for LiDAR, respectively. ‚Äúft‚Äù means the two-stage fine-tune strategy. ‚ÄúMGMap*‚Äù means the reimplemented structure based
on stronger MapTRV2 [32]. The inference speed is measured on the same computer with a single NVIDIA Tesla V100 GPU.
mainly focus on three map elements, including lane-divider
(div.), ped-crossing (ped.), and road boundary (bou.).
4.2. Evaluation Metrics
To facilitate comprehensive evaluations, we employ the
Chamfer distance-based metrics, including average preci-
sion AP chamfer [19] and the IoU-based average precision
APraster [49], which evaluates the model from the point-
coordinate aspect considers each map element as a whole
unit separately. This ensures that the map vectorization
quality can be assessed from different perspectives.
Chamfer-Distance AP. For fair comparisons, AP chamfer is
inherited from previous map vectorization works [23, 26].
Specifically, the average Euclidean distances between sam-
pled points in each map line and the nearest points in the
ground-truth labels are measured. The AP is calculated un-
der the average of three distance thresholds, œÑchamfer ‚àà
{0.5m,1m,1.5m}. Each prediction is treated as true-
positive (TP) when the distance is below the threshold.
APchamfer evaluates map construction quality from point-
level perspectives, since it calculates the distances between
points as the measured errors.IoU-based AP. Following MapVR [49], AP raster is calcu-
lated by the Intersection over Union (IoU) among pixels.
Both predictions and ground truth labels are rasterized into
polylines in HD maps. Raster map size is set to 480√ó240,
and each map element is dilated by two pixels on each
side. Detection quality is evaluated by calculating the IoU
of the rasterized representations between the prediction and
ground truth. The calculation of AP is set under thresholds
œÑIoU‚àà {0.25 : 0 .5 : 0.05}for line-shaped elements and
œÑIoU‚àà {0.5 : 0.75 : 0 .05}for polygon-shaped pedestrian
crossings, where 0.05 represents the step size for the change
of threshold. AP raster is calculated under the average of all
the thresholds among the upper and lower bounds. Thus,
APraster considers each lane line at the pixel level and eval-
uates map quality from the whole instance perspective.
4.3. Implementation Details
To ensure fair comparisons, we choose ResNet50 [12] as
the image backbone. SECOND [48] is adopted as the
backbone for LiDAR modality. The BEV size, defined as
HBEV√óWBEV , is set to 200√ó100. The maximum num-
ber of instances and point queries are set to 50 and 20,
14817
Multi-View Surrounding Images MapTR Ours GT
Figure 5. The visual results of MapTR [23], our proposed MGMap approach and the corresponding ground truth.
respectively. In PG-MPR module, the hyper-parameter d
is set to 0.1, which is the normalized patch size. We use
AdamW [34] optimizer with a learning rate of 6e‚àí4. All
models are trained on 8 NVIDIA Tesla V100 GPUs with a
batch size of 6 per GPU. The weighted factors for detec-
tion loss are the same as those in MapTR [23]. For mask
branches, {Œªins, Œªb}are set to {2,15}, respectively.
4.4. Main Results
Performance on nuScenes Dataset. As shown in Table 1,
we compare MGMap approach against the SOTA meth-
ods on the validation set of nuScenes with different set-
tings. The experimental performance is evaluated under
APchamfer and AP raster . It can be seen that our proposed
approach outperforms previous methods and obtains the
best performance. Compared with baseline MapTR [23],
MGMap archives 10.3 mAP improvement with multi-view
camera-based input under the same settings of ResNet-50
and 30 epochs for training. It is worth noting that MGMap
achieves 67.9 mAP for LiDAR and 71.7 mAP for fusing
camera data with LiDAR, which demonstrates the strong
generalization capability of our scheme. Furthermore, the
visual results of MGMap in several driving scenarios are
shown in Figure 5. More comparison results under different
conditions are included in the supplementary material.
Performance on Argoverse2 Dataset. By following the
settings on the online HDMap construction challenge, we
re-implement MapTR and MGMap on the Argoverse2
dataset. Table 2 presents our experimental results. It can beMethod Backbone APped. APdiv. APbou. mAP
HDMapNet [19] EB0 5.7 13.1 37.6 18.8
VectorMapNet [26] R50 37.2 50.4 40.6 42.7
MapTR [23] R50 50.6 60.7 61.2 57.4
MGMap(Ours) R50 52.8 67.5 68.1 62.8
Table 2. Performance comparison with baseline methods at 60m√ó
30mperception range on a subset of Argoverse2 provided by the
Online HD Map Construction Challenge.
observed that our method achieves competitive performance
on the Argoverse2 dataset, MGMap achieves 5.4 mAP im-
provement compared with MapTR, which further shows the
effectiveness of our proposed approach.
Performance on Enlarged Perception Ranges. To eval-
uate the robustness of model, we conduct experiments on
the enlarged perception ranges. Under the same settings,
we re-implement MapTR and our MGMap with the percep-
tion ranges of 60m√ó60mand30m√ó90mat the X-axis
and Y-axis in BEV space, in which query numbers are pro-
portionally enlarged to account for the basic property. All
models are trained for 30 epochs. Table 3 reports the exper-
iment results. Compared to MapTR, our MGMap achieves
consistent performance improvements, with a 9.5 mAP im-
provement on the 60m√ó60mperceptions range setting and
a 10.2 mAP improvement on the 30m√ó90mrange setting.
4.5. Ablation Study
In this section, ablation experiments are conducted to exam-
ine the effectiveness of our proposed modules and designs.
14818
BEV range Method APped. APdiv. APbou. mAP
60m√ó60mMapTR [23] 37.9 44.4 41.6 41.3
MGMap(Ours) 48.7 53.4 50.0 50.8
‚àÜmAP +10.8 +9.0 +8.4 +9.5
90m√ó30mMapTR [23] 36.5 43.7 39.1 39.8
MGMap(Ours) 46.4 56.1 49.1 50.5
‚àÜmAP +9.9 +12.4 +10.0 +10.2
Table 3. Experimental results with enlarged perception range set-
tings on the nuScenes dataset. Our proposed MGMap approach
outperforms MapTR significantly on all evaluation metrics.
For fair comparisons, all experiments are conducted on the
camera modality of the nuScenes dataset with 30 epochs for
training. ResNet50 is employed as the image backbone.
Ablation on Mask-Guided Design. Our mask-guided de-
sign comprises two main components: the MAI decoder
and the PG-MPR module, which are utilized to handle the
overall shape structure and specific details from the instance
level and the point level. The MAI construction leverages
the masks to obtain global structure information, which
contributes to shape understanding. Furthermore, the patch
refinement considers the local level more precisely. This
strategy allows for more specific and fine-grained adjust-
ments to individual points, which contributes to precise lo-
calization. The ablation results presented in Table 4 demon-
strate the impact of each level design. When compared to
the absence of mask guidance, the inclusion of the MAI
decoder and PG-MPR module leads to individual improve-
ments of 1.9 mAP and 2.6 mAP, respectively. Furthermore,
the combination of these two components in the overall de-
sign achieves the highest performance, with a mAP of 61.4.
Ablation on EML Neck. To investigate the effectiveness
of the EML neck design, we conduct ablation experiments
to compare it with FPN in PV image space. Multi-scale
BEV features contain enriched semantic and location in-
formation with larger receptive fields, which is beneficial
for the positioning of irregular shapes. As shown in Ta-
ble 5, compared to baselines without multi-level features,
EML design at BEV space (referred to as ‚ÄúBEV‚Äù) achieves
a larger performance gain. While PV space design fails to
achieve the expected effect (‚ÄúPV‚Äù). Besides, enhanced BEV
features also contribute to mask generation and boost the
performance of the mask-guided design (‚ÄúMG‚Äù).
Ablation on PG-MPR Design. Finally, we investigate the
ablation experiment on the setting of the position-guided
mask patch refinement module. In our experiment settings,
we compare different selections of patch size dand the
number of refinement stages s. The evaluated normalized
patch sizes range from 0.08 to 0.12, and refinement stages
are set from 1 to 3. As illustrated in Table 6, applying the re-
finement strategy contributes to performance improvement.
Experiments also indicate that the oversized patches will
introduce irrelevant information and adversely affect per-
formance while undersized patches lead to information lossIns. Point APped. APdiv. APbou. mAP
53.1 60.1 59.5 57.6
‚úì 55.4 61.7 61.5 59.5
‚úì 54.9 63.4 62.3 60.2
‚úì ‚úì 57.4 63.5 63.3 61.4
Table 4. Ablation study of mask-guided design at the instance
and point levels. Ins. denotes the instance-level MAI decoder and
Point means the point-level PG-MPR module design.
Baseline PV BEV MG APped. APdiv. APbou. mAP
‚úì 45.2 53.8 54.3 51.1
‚úì ‚úì 41.7 49.9 52.6 48.1
‚úì ‚úì ‚úì 47.3 54.1 55.1 52.2
‚úì ‚úì 53.1 60.1 59.5 57.6
‚úì ‚úì ‚úì 57.4 63.5 63.3 61.4
Table 5. Ablation study of EML neck design by investigating the
performance of multi-level features at PV and BEV stages. Empir-
ical results show that the best performance is achieved by taking
advantage of the BEV-level EML neck for mask-guided design.
d s APped. APdiv. APbou. mAP
/ / 55.4 61.7 61.5 59.5
0.08 2 55.8 64.3 62.3 60.8
0.1 1 56.3 62.6 62.5 60.5
0.1 2 57.4 63.5 63.3 61.4
0.1 3 56.0 62.9 62.9 60.6
0.12 2 56.0 62.7 61.5 60.1
Table 6. Performance of point-level PG-MPR design with different
patch sizes ( d) and refinement stages ( s). The first row means the
result without the point-level refinement.
and also result in suboptimal results. The best performance
is achieved when the selected patch size is set to 0.1 along
with the two-stage refinement.
5. Conclusion
In this paper, we propose MGMap, an effective approach to
online HD map vectorization with the guidance of learned
masks. By taking advantage of masks at both the instance
and point levels, we alleviate the challenges of rough de-
tection and loss of details arising from subtle and sparse
annotations in HD maps. Our proposed MGMap not only
demonstrates state-of-the-art performance but also exhibits
strong robustness in online map vectorization across various
experimental settings. For future work, fusing with other
perceptual tasks to construct a more comprehensive repre-
sentation is still a direction worth exploring, which holds
promise for further advancements in autonomous driving.
Acknowledgments
This work is supported by National Natural Science
Foundation of China under Grants (62376244). It
is also supported by Information Technology Cen-
ter and State Key Lab of CAD&CG, Zhejiang University.
14819
References
[1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuScenes: A multi-
modal dataset for autonomous driving. In CVPR , 2020. 2,
5
[2] Yigit Baran Can, Alexander Liniger, Danda Pani Paudel, and
Luc Van Gool. Structured bird‚Äôs-eye-view traffic scene un-
derstanding from onboard images. In ICCV , pages 15661‚Äì
15670, 2021. 2
[3] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-
pixel classification is not all you need for semantic segmen-
tation. pages 17864‚Äì17875, 2021. 2
[4] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In CVPR ,
pages 1290‚Äì1299, 2022. 2
[5] Tianheng Cheng, Xinggang Wang, Lichao Huang, and
Wenyu Liu. Boundary-preserving mask r-cnn. In ECCV ,
pages 660‚Äì676. Springer, 2020. 2
[6] Tianheng Cheng, Xinggang Wang, Shaoyu Chen, Wenqiang
Zhang, Qian Zhang, Chang Huang, Zhaoxiang Zhang, and
Wenyu Liu. Sparse instance activation for real-time instance
segmentation. In CVPR , pages 4433‚Äì4442, 2022. 2
[7] Fang Da and Yu Zhang. Path-aware graph attention for hd
maps in motion prediction. In ICRA , pages 6430‚Äì6436.
IEEE, 2022. 1
[8] Jian Ding, Nan Xue, Gui-Song Xia, Bernt Schiele, and
Dengxin Dai. Hgformer: Hierarchical grouping transformer
for domain generalized semantic segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 15413‚Äì15423, 2023. 2
[9] Wenjie Ding, Limeng Qiao, Xi Qiu, and Chi Zhang. Piv-
otnet: Vectorized pivot learning for end-to-end hd map con-
struction. In ICCV , pages 3672‚Äì3682, 2023. 2, 6
[10] Hao Dong, Xianjing Zhang, Xuan Jiang, Jun Zhang, Jin-
tao Xu, Rui Ai, Weihao Gu, Huimin Lu, Juho Kannala, and
Xieyuanli Chen. Superfusion: Multilevel lidar-camera fu-
sion for long-range hd map generation and prediction. arXiv
preprint arXiv:2211.15656 , 2022. 1, 2
[11] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir
Anguelov, Congcong Li, and Cordelia Schmid. Vectornet:
Encoding hd maps and agent dynamics from vectorized rep-
resentation. In CVPR , pages 11525‚Äì11533, 2020. 1
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
pages 770‚Äì778, 2016. 6
[13] Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Gir-
shick. Mask r-cnn. In ICCV , pages 2961‚Äì2969, 2017. 5
[14] Yuzhe He, Shuang Liang, Xiaofei Rui, Chengying Cai,
and Guowei Wan. Egovm: Achieving precise ego-
localization using lightweight vectorized maps. arXiv
preprint arXiv:2307.08991 , 2023. 1
[15] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, et al. Planning-oriented autonomous driving. In
CVPR , pages 17853‚Äì17862, 2023. 1[16] Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong
Du. Bevdet: High-performance multi-camera 3d object de-
tection in bird-eye-view. arXiv preprint arXiv:2112.11790 ,
2021. 2
[17] Bo Jiang, Shaoyu Chen, Xinggang Wang, Bencheng Liao,
Tianheng Cheng, Jiajie Chen, Helong Zhou, Qian Zhang,
Wenyu Liu, and Chang Huang. Perceive, interact, predict:
Learning dynamic and static clues for end-to-end motion pre-
diction. arXiv preprint arXiv:2212.02181 , 2022. 1
[18] Jesse Levinson, Michael Montemerlo, and Sebastian Thrun.
Map-based precision vehicle localization in urban environ-
ments. In Robotics: science and systems , page 1. Atlanta,
GA, USA, 2007. 1
[19] Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. Hdmapnet:
An online hd map construction and evaluation framework. In
ICRA , pages 4628‚Äì4634. IEEE, 2022. 1, 2, 6, 7
[20] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-
hao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:
Learning bird‚Äôs-eye-view representation from multi-camera
images via spatiotemporal transformers. In ECCV , pages 1‚Äì
18. Springer, 2022. 2, 3
[21] Justin Liang, Namdar Homayounfar, Wei-Chiu Ma, Yuwen
Xiong, Rui Hu, and Raquel Urtasun. Polytransform: Deep
polygon transformer for instance segmentation. In CVPR ,
pages 9131‚Äì9140, 2020. 2
[22] Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia,
Zhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi
Tang. Bevfusion: A simple and robust lidar-camera fusion
framework. In NeurIPS , 2022. 2
[23] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng
Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. Maptr:
Structured modeling and learning for online vectorized hd
map construction. In ICLR , 2022. 1, 2, 4, 5, 6, 7, 8
[24] Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian
Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang.
Maptrv2: An end-to-end framework for online vectorized hd
map construction. arXiv preprint arXiv:2308.05736 , 2023.
2, 6
[25] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
Petr: Position embedding transformation for multi-view 3d
object detection. In ECCV , pages 531‚Äì548. Springer, 2022.
2
[26] Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and
Hang Zhao. Vectormapnet: End-to-end vectorized hd map
learning. In ICML , pages 22352‚Äì22369. PMLR, 2023. 1, 2,
6, 7
[27] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,
Huizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-
task multi-sensor fusion with unified bird‚Äôs-eye view repre-
sentation. In ICRA , pages 2774‚Äì2781. IEEE, 2023. 2
[28] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 3DV, pages 565‚Äì571. Ieee,
2016. 5
[29] Bowen Pan, Jiankai Sun, Ho Yin Tiga Leung, Alex Ando-
nian, and Bolei Zhou. Cross-view semantic segmentation
for sensing surroundings. IEEE Robotics and Automation
Letters , 5(3):4867‚Äì4873, 2020. 2
14820
[30] Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang,
and Erkang Cheng. Bevsegformer: Bird‚Äôs eye view semantic
segmentation from arbitrary camera rigs. In WACV , pages
5935‚Äì5943, 2023. 2
[31] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding
images from arbitrary camera rigs by implicitly unprojecting
to 3d. In ECCV , pages 194‚Äì210. Springer, 2020. 2
[32] Limeng Qiao, Wenjie Ding, Xi Qiu, and Chi Zhang. End-to-
end vectorized hd-map construction with piecewise bezier
curve. In CVPR , pages 13218‚Äì13228, 2023. 2, 6
[33] Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen, and Xi
Li. Unifusion: Unified multi-view fusion transformer for
spatial-temporal representation in bird‚Äôs-eye-view. In ICCV ,
pages 8690‚Äì8699, 2023. 2
[34] Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On
the convergence of adam and beyond. arXiv preprint
arXiv:1904.09237 , 2019. 7
[35] Lennart Reiher, Bastian Lampe, and Lutz Eckstein. A
sim2real deep learning approach for the transformation of
images from multiple vehicle-mounted cameras to a seman-
tically segmented image in bird‚Äôs eye view. In ITSC , pages
1‚Äì7. IEEE, 2020. 2
[36] Thomas Roddick and Roberto Cipolla. Predicting semantic
map representations from images using pyramid occupancy
networks. In CVPR , pages 11138‚Äì11147, 2020. 1
[37] Tixiao Shan and Brendan Englot. Lego-loam: Lightweight
and ground-optimized lidar odometry and mapping on vari-
able terrain. In IROS , pages 4758‚Äì4765. IEEE, 2018. 2
[38] Tixiao Shan, Brendan Englot, Drew Meyers, Wei Wang,
Carlo Ratti, and Daniela Rus. Lio-sam: Tightly-coupled li-
dar inertial odometry via smoothing and mapping. In IROS ,
pages 5135‚Äì5142. IEEE, 2020. 2
[39] Juyeb Shin, Francois Rameau, Hyeonjun Jeong, and Dong-
suk Kum. Instagram: Instance-level graph modeling for vec-
torized hd map learning. In CVPRW , 2023. 2, 6
[40] Towaki Takikawa, David Acuna, Varun Jampani, and Sanja
Fidler. Gated-scnn: Gated shape cnns for semantic segmen-
tation. In ICCV , pages 5229‚Äì5238, 2019. 2
[41] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In ICML , pages
6105‚Äì6114. PMLR, 2019. 6
[42] Chufeng Tang, Hang Chen, Xiao Li, Jianmin Li, Zhaoxi-
ang Zhang, and Xiaolin Hu. Look closer to segment bet-
ter: Boundary patch refinement for instance segmentation.
InCVPR , pages 13926‚Äì13935, 2021. 2
[43] Song Wang, Wentong Li, Wenyu Liu, Xiaolu Liu, and Jianke
Zhu. Lidar2map: In defense of lidar-based semantic map
construction using online camera distillation. In CVPR ,
pages 5186‚Äì5195, 2023. 1, 2
[44] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.
Fcos3d: Fully convolutional one-stage monocular 3d object
detection. In ICCV , pages 913‚Äì922, 2021. 2
[45] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang,
Yilun Wang, Hang Zhao, and Justin Solomon. Detr3d:
3d object detection from multi-view images via 3d-to-2d
queries. In CoRL , pages 180‚Äì191. PMLR, 2022. 2[46] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-
bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-
nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,
Deva Ramanan, Peter Carr, and James Hays. Argoverse 2:
Next generation datasets for self-driving perception and fore-
casting. In NeurIPS , 2021. 2, 5
[47] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So
Kweon. Cbam: Convolutional block attention module. In
ECCV , pages 3‚Äì19, 2018. 3
[48] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-
ded convolutional detection. Sensors , 18(10):3337, 2018. 6
[49] Gongjie Zhang, Jiahao Lin, Shuang Wu, Yilin Song, Zhipeng
Luo, Yang Xue, Shijian Lu, and Zuoguan Wang. Online map
vectorization for autonomous driving: A rasterization per-
spective. In NeurIPS , 2023. 6
[50] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,
Guan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified per-
ception and prediction in birds-eye-view for vision-centric
autonomous driving. arXiv preprint arXiv:2205.09743 ,
2022. 1
[51] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,
and Jifeng Dai. Deformable detr: Deformable transformers
for end-to-end object detection. In ICLR , 2020. 1, 3, 4
14821
