SmartEdit: Exploring Complex Instruction-based Image Editing
with Multimodal Large Language Models
Yuzhou Huang1,2‚àó#Liangbin Xie2,3,5‚àó#Xintao Wang2,4‚Ä†Ziyang Yuan2,7#Xiaodong Cun4
Yixiao Ge2,4Jiantao Zhou3Chao Dong5,6Rui Huang1Ruimao Zhang1‚Ä†Ying Shan2,4
1The Chinese University of Hong Kong, Shenzhen (CUHK-SZ)2ARC Lab, Tencent PCG3University of Macau4Tencent AI Lab
5Shenzhen Institute of Advanced Technology6Shanghai Artificial Intelligence Laboratory7Tsinghua University
https://github.com/TencentARC/SmartEdit
‚ÄúChange the left/middle/right apple to an orange‚Äù‚ÄúChange the left/right animal to a white fox‚Äù ‚ÄúChange thebigger/smaller bear toawolf‚Äù
‚ÄúChange thered/green apple toapeach‚Äù
‚ÄúChange the dog in mirror to a tiger‚Äù‚ÄúPlease replace the animal that is usually known 
as friend of human's with a tiger‚Äù‚ÄúPlease remove theobject that cantellthetime‚Äù
Figure 1. We propose SmartEdit, an instruction-based image editing model that leverages Multimodal Large Language Models (MLLMs)
to enhance the understanding and reasoning capabilities of instruction-based editing methods. With the specialized design, our SmartEdit
is capable of handling complex understanding (the instructions that contain various object attributes like location, relative size, color, and
in or outside the mirror) and reasoning scenarios.
Abstract
Current instruction-based image editing methods, such
as InstructPix2Pix, often fail to produce satisfactory re-
sults in complex scenarios due to their dependence on the
simple CLIP text encoder in diffusion models. To rectify
this, this paper introduces SmartEdit, a novel approach of
instruction-based image editing that leverages Multimodal
Large Language Models (MLLMs) to enhance its under-
standing and reasoning capabilities. However, direct in-
tegration of these elements still faces challenges in situ-
ations requiring complex reasoning. To mitigate this, we
‚àóEqual contribution‚Ä†Corresponding author#Interns in ARC
Lab, Tencent PCGpropose a Bidirectional Interaction Module (BIM) that en-
ables comprehensive bidirectional information interactions
between the input image and the MLLM output. During
training, we initially incorporate perception data to boost
the perception and understanding capabilities of diffusion
models. Subsequently, we demonstrate that a small amount
of complex instruction editing data can effectively stimu-
late SmartEdit‚Äôs editing capabilities for more complex in-
structions. We further construct a new evaluation dataset,
Reason-Edit, specifically tailored for complex instruction-
based image editing. Both quantitative and qualitative re-
sults on this evaluation dataset indicate that our SmartEdit
surpasses previous methods, paving the way for the practi-
cal application of complex instruction-based image editing.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
8362
1. Introduction
Text-to-image synthesis [8, 13, 23, 26, 27, 29] has experi-
enced significant advancements in recent years, thanks to
the development of diffusion models. These methods have
enabled the generation of images that are not only consis-
tent with natural language descriptions, but also align with
human perception and preferences, marking a substantial
leap forward in the field. Instruction-based image editing
methods [1, 36], represented by InstructPix2Pix, leverage
pre-trained text-to-image diffusion models as priors. This
allows users to conveniently and effortlessly modify images
through natural language instructions for ordinary users.
While existing instruction-based image editing methods
can handle simple instructions effectively, they often fall
short when dealing with complex scenarios, which require
the model to have a more powerful understanding and rea-
soning capabilities. As depicted in Fig. 1, there are two
common types of complex scenarios. The first is when the
original image contains multiple objects, and the instruc-
tion modifies only one of these objects through certain at-
tributes (such as location, relative size, color, in or outside
the mirror ). The other is when world knowledge is needed
to identify the object to be edited (such as the object that
can tell the time ). We define these two types as complex
understanding scenarios and complex reasoning scenarios,
respectively. Handling these two scenarios is crucial for
practical instruction editing, but existing instruction-based
image editing methods probably fail in these scenarios (as
shown in Fig. 2). In this paper, we attempt to identify the
reasons why existing instruction-based image editing meth-
ods fail in these scenarios, and try to tackle the challenge in
these scenarios.
The first reason why existing instruction-based image
editing methods fail in these scenarios is that they typi-
cally rely on a simple CLIP text encoder [25] in diffusion
models ( e.g., Stable Diffusion) to process the instructions.
Under this circumstance, these models struggle to 1) un-
derstand and reason through the instructions, and 2) inte-
grate the image to comprehend the instructions. To ad-
dress these limitations, we introduce the Multimodal Large
Language Models (MLLMs) ( e.g., LLaV A) [22, 39] into
instruction-based editing models. Our method, SmartEdit,
jointly optimizes MLLMs and diffusion models, leveraging
the powerful reasoning capabilities of MLLMs to facilitate
instruction-based image editing task.
While substituting the CLIP encoder in the diffusion
model with MLLMs can alleviate some problems, this ap-
proach still falls short when it comes to examples that ne-
cessitate complex understanding and reasoning. This is
because the input image to edit (original image) is inte-
grated into the UNet of the Stable Diffusion model through
a straightforward concatenation, which is further interacted
with MLLM outputs through a cross-attention operation. Inthis setup, the image features serve as the query, and MLLM
outputs act as the key and value. This means that the MLLM
outputs unilaterally modulate and interact with the image
feature, which affects the results. To alleviate this issue, we
further propose a Bidirectional Interaction Module (BIM).
This module reuses the image information extracted by the
LLaV A‚Äôs visual encoder from the input image. It also fa-
cilitates a comprehensive bidirectional information interac-
tion between this image and the MLLM output, enabling
the model to perform better in complex scenarios.
The second reason contributing to the failure of existing
instruction-based editing methods is the absence of specific
data. When solely training on editing datasets, such as the
datasets used in Instructpix2pix [1] and MagicBrush [36],
SmartEdit also struggles to handle scenarios requiring com-
plex reasoning and understanding. This is because SmartE-
dit has not been exposed to data from these scenarios. One
straightforward approach is to generate a substantial amount
of paired data similar to those scenarios. However, this
method is excessively expensive because the cost of gen-
erating data for these scenarios is high.
In this paper, we find that there are two keys to compen-
sate the insufficiency of specific editing data. The first is to
enhance the perception capabilities of UNet [28], and the
second is to stimulate the model capacity in those scenar-
ios with a few high-quality examples. Correspondingly, we
1) incorporate the perception-related data ( e.g., segmenta-
tion) into the model‚Äôs training. 2) synthesize a few high-
quality paired data with complex instructions to fine-tune
our SmartEdit (similar to LISA [19]). In this way, SmartE-
dit not only reduces the reliance on paired data under com-
plex scenarios but also effectively stimulates its ability to
handle these scenarios.
Equipped with both the model designs and the data uti-
lization strategy, SmartEdit can understand complex in-
structions, surpassing the scope that previous instruction
editing methods can do. To better evaluate the understand-
ing and reasoning ability of instruction-based image edit-
ing methods, we collect the Reason-Edit dataset, which
contains a total of 219 image-text pairs. Note that there
is no overlap between the Reason-Edit dataset and the
small-amount high-quality synthesized training data pairs.
Based on the Reason-Edit dataset, we evaluate existing
instruction-based image editing methods comprehensively.
Both the quantitative and qualitative results on the Reason-
Edit dataset indicate that SmartEdit significantly outper-
forms previous instruction-based image editing methods.
In summary, our contributions are as follows:
1. We analyze and focus on the performance of instruction-
based image editing methods in more complex instruc-
tions. These complex scenarios have often been over-
looked and less explored in past research.
2. We leverage MLLMs to better comprehend instructions.
8363
‚ÄúChange the dog in mirror to a lion‚Äù ‚ÄúChange the black raspberry to a tangerine‚Äù ‚ÄúPlease remove the tool that is used to cut cakes .‚ÄùFigure 2. For more complex instructions or scenarios, InstructPix2Pix fails to follow the instructions.
To further improve the performance, we propose a Bidi-
rectional Interaction Module to facilitate the interaction
of information between text and image features.
3. We propose a new dataset utilization strategy to enhance
the performance of SmartEdit in complex scenarios. In
addition to using conventional editing data, we introduce
perception-related data to strengthen the perceptual abil-
ity of UNet in the diffusion process. Besides, we also
add a small amount of synthetic editing data to further
stimulate the model‚Äôs reasoning ability.
4. An evaluation dataset, Reason-Edit, is specifically col-
lected for evaluating the performance of instruction-
based image editing tasks in complex scenarios. Both
qualitative and quantitative results on Reason-Edit
demonstrate the superiority of SmartEdit.
2. Related Work
2.1. Image Editing with Diffusion Models.
Pretrained text-to-image diffusion models [8, 13, 23, 26, 27,
29] can strongly assist image editing task. Instruction-based
image editing task [1, 4, 11, 12, 16, 17, 32, 36, 38] requires
users to provide an instruction, which converts the original
image to a newly designed image that matches the given
instruction. Some methods can achieve this by utilizing a
tuning-free approach. For example, Prompt-to-Prompt [12]
suggests modifying the cross-attention maps by comparing
the original input caption with the revised caption. MasaC-
trl [4] converts existing self-attention in diffusion models
into mutual self-attention, which can help query correlated
local contents and textures from source images for consis-
tency. In addition, due to the scarcity of paired image-
instruction editing datasets, the pioneering work Instruct-
Pix2Pix [1] introduces a large-scale vision-language im-
age editing datasets created by fine-tuned GPT-3 [2] and
Prompt-to-Prompt with stable diffusion, and further fine-
tunes the UNet [28], which can edit images by providing a
simple instruction. To enhance the editing effect of Instruct-
Pix2Pix on real images, MagicBrush [36] further provides a
large-scale and manually annotated dataset for instruction-
guided real image editing.
The recent work, InstructDiffusion [11], also adopts the
network design of InstructPix2Pix and focuses on unifying
vision tasks in a joint training manner. By taking advantage
of multiple different datasets, it can handle a variety of vi-
sion tasks, including understanding tasks (such as segmen-tation and keypoint detection) and generative tasks (such as
editing and enhancement). Compared with InstructDiffu-
sion, our primary focus is on the field of instruction-based
image editing, especially for complex understanding and
reasoning scenarios. In these scenarios, InstructDiffusion
typically generates inferior results.
2.2. LLM with Diffusion Models
The exceptional open-sourced LLaMA [6, 31] significantly
enhances the performance of vision tasks with the aid of
Large Language Models (LLMs). Pioneering works such
as LLaV A and MiniGPT-4 have improved image-text align-
ment through instruction-tuning. While numerous MLLM-
based [7, 22, 24, 39] studies have demonstrated their robust
capabilities across a variety of tasks, primarily those reliant
on text generation (e.g., human-robot interaction, complex
reasoning, science question answering, etc.), GILL [18]
serves as a bridge between MLLMs and diffusion models. It
learns to process images with LLMs and is capable of gen-
erating coherent images based on the input texts. SEED [9]
presents an innovative image tokenizer to enable LLM to
process and generate images and text concurrently. SEED-
2 [10] further refines the tokenizer by aligning the genera-
tion embedding with the image embedding of unCLIP-SD,
which allows for better preservation of rich visual semantics
and reconstruction of more realistic images. Emu [30] can
be characterized as a multimodal generalist, trained with the
next-token-prediction objective. CM3Leon [35] proposes
a multi-modal language model that is capable of execut-
ing text-to-image and image-to-text generation. It employs
the CM3 multi-modal architecture that is fine-tuned on di-
verse instruction-style data, and utilizes a training method
adapted from text-only language models.
3. Preliminary
The goal of instruction-based image editing is to make spe-
cific modifications to an input image xbased on instructions
cT, resulting in the target image y. InstructPix2Pix, which
is based on latent diffusion, is a seminal work in this field.
For the target image yand an encoder E, the diffusion pro-
cess introduces noise to the encoded latent z=E(y), result-
ing in a noisy latent zt, with the noise level increasing over
timesteps t‚ààT. A UNet œµŒ¥is then trained to predict the
noise added to the noisy latent zt, given the image condition
cxand text instruction condition cT, where cx=E(x). The
8364
image condition is incorporated by directly concatenating
cxandzt. The specific objective of latent diffusion is as
follows:
Ldiffusion =EE(y),E(x),cT,œµ‚àºN(0,1),t[‚à•œµ
‚àíœµŒ¥(t,concat[ zt,E(x)], cT)‚à•2
2 (1)
where œµis the unscaled noise, tis the sampling step, zt
is latent noise at step t,E(x)is the image condition, and cT
is the text instruction condition. The concat corresponds to
the concatenation operation.
Although InstructPix2Pix has some effectiveness in in-
struction editing, its performance is limited when dealing
with complex understanding and reasoning scenarios. To
address this issue, we introduce a Multimodal Large Lan-
guage Model (MLLM) into the network architecture and
propose a Bidirectional Interaction Module (BIM) to im-
plement bidirectional information interaction between the
MLLM output and image information. In addition, we also
explore the data utilization strategy and find that perception-
related data and a small amount of complex editing data are
crucial for enhancing model‚Äôs performance. We provide de-
tailed descriptions of these aspects in the next section.
4. Method
In this paper, we introduce SmartEdit, specifically designed
to handle complex instruction editing scenarios. In this sec-
tion, we first provide a detailed overview of the framework
of SmartEdit (Section 4.1). Then, we delve into the Bidirec-
tional Interaction Module (Section 4.2). In Section 4.3, we
discuss how to enhance the perception and understanding
capabilities of UNet in the diffusion model and stimulate
the ability of MLLMs to handle complex scenarios. Finally,
We introduce Reason-Edit, which is primarily used to eval-
uate the ability of instruction-based image editing methods
toward complex scenarios. (Section 4.4).
4.1. The Framework of SmartEdit
Given an image xand instruction c, which is tokenized as
(s1, ...,sT), our goal is to obtain the target image ybased
onc. As shown in Fig 3, the image xis first processed by
the image encoder and FC layer, resulting in v¬µ(x). Then
v¬µ(x)is sent into the LLM along with the token embed-
ding ( s1, ...,sT). The output of the LLM is discrete tokens,
which cannot be used as the input for subsequent modules.
Therefore, we take the hidden states corresponding to these
discrete tokens as the input for the following modules. To
jointly optimize LLaV A and the diffusion model, following
GILL [18], we expand the original LLM vocabulary with
rnew tokens [IMG 1],...,[IMG r]and append the r[IMG]
tokens to the end of instruction c. To be specific, we incor-
porate a trainable matrix Einto the embedding matrix of
the LLM, which represents the r[IMG] token embeddings.
Subsequently, we minimize the negative log-likelihood ofgenerated r[IMG] tokens, conditioned on tokens that have
been generated previously:
LLLM(c) =‚àírX
i=1logp{Œ∏‚à™E}([IMG i]|v¬µ(x),
s1, ..., s T,[IMG 1], . . . , [IMG i‚àí1]) (2)
The majority of parameters Œ∏in the LLM are kept frozen
and we utilize LoRA [15] to carry out efficient fine-tuning.
We take the hidden states hcorresponding to the r[IMG]
tokens as the input for the next module.
Considering the discrepancy between the feature spaces
of the hidden states in the LLM and the clip text encoder,
we need to align the hidden states hto the clip text encoder
space. Inspired by BLIP2 [20] and DETR [5], we adopt the
QFormer QŒ≤with 6‚àílayer transformer [33] and nlearn-
able queries, obtaining feature f. Subsequently, the image
feature voutput by the image encoder Eœïinteracts with f
through a bidirectional interaction module (BIM), resulting
inf‚Ä≤andv‚Ä≤. The mentioned process is represented as:
h= LLaVA( x, c),
f=QŒ≤(h),
v=Eœï(x),
f‚Ä≤, v‚Ä≤= BIM( f, v)(3)
For the diffusion model, following the design of Instruct-
pix2pix, we concat the encoded image latent E(x)and noisy
latent zt. Unlike Instructpix2pix, we use f‚Ä≤as the key and
value in UNet, and combine v‚Ä≤into the features before en-
tering UNet in a residual manner. The specific process can
be formulated as:
Ldiffusion =EE(y),E(x),cT,œµ‚àºN(0,1),t[‚à•œµ
‚àíœµŒ¥(t,concat[ zt,E(x)] +v‚Ä≤, f‚Ä≤)‚à•2
2 (4)
To keep consistency with equation 1, we omit the Conv
operation here.
4.2. Bidirectional Interaction Module
The design of BIM is depicted in Fig 4. It includes a self-
attention block, two cross-attention blocks, and an MLP
layer. The two inputs of BIM are the output ffrom
QFormer and the output vfrom the image encoder. After
bidirectional information interaction between fandv, BIM
will eventually output f‚Ä≤andv‚Ä≤. In BIM, the process be-
gins with fundergoing a self-attention mechanism. After
this,fserves as a query to interact with the input v, which
acts as both key and value, through a cross-attention block.
This interaction results in the generation of f‚Ä≤via a point-
wise MLP. Following the creation of f‚Ä≤, it then serves as
both key and value to interact with v, which now acts as a
query. This second cross-attention interaction leads to the
production of v‚Ä≤.
8365
Large Language Model
Diffusion ModelImage  
Encoder‚ÄúWhat is the object that gives people warning? Remove this object.‚Äù
QFormer
Bidirectional 
Interaction
ModuleLearnable queries
ùëì
ùë£ùëì‚Ä≤
ùúà‚Ä≤
FC
Decoder
Trainable
Frozen
Conv
concatEncoder
LoRA
‚Ä¶Figure 3. The overall framework of SmartEdit. For the instruction, we first append the r[IMG] tokens to the end of instruction c. Together
with image x, they will be sent into LLaV A, which can then obtain the hidden states corresponding to these r[IMG] tokens. Then the
hidden state is sent into the QFormer and gets feature f. Subsequently, the image feature voutput by the image encoder Eœïinteracts with
fthrough a bidirectional interaction module (BIM), resulting in f‚Ä≤andv‚Ä≤. The f‚Ä≤andv‚Ä≤are input into the diffusion models to achieve the
instruction-based image editing task.
Figure 4. The network design of the BIM Module. In this module,
the input information fandvwill undergo bidirectional informa-
tion interaction through different cross-attention.
As discussed in the introduction, the proposed BIM mod-
ule reuses the image feature and inputs it as supplementary
information into UNet. The implementation of two cross-
attention blocks in this module facilitates a robust bidirec-
tional information interaction between the image feature
and the text feature. Compared to not adopting the BIM
module or only fusing the image feature and text feature in
one direction, SmartEdit which is equipped with the BIM
module yields better results. The experimental comparison
of different designs is shown in Section 5.3.
4.3. Dataset Utilization Strategy
During the training process of SmartEdit, two primary chal-
lenges emerge when solely utilizing datasets gathered from
InstructPix2Pix and MagicBrush as the training set. The
first challenge is that SmartEdit has a poor perception of
position and concept. The second challenge is that, despite
being equipped with MLLM, SmartEdit still has limited ca-
pability in scenarios that require reasoning. In summary, the
effectiveness of SmartEdit in handling complex scenarios is
limited if it is only trained on conventional editing datasets.
After analysis, we have identified the causes of these issues.
The first issue stems from the UNet in the diffusion model
which lacks an understanding of perception and concepts,
leading to SmartEdit‚Äôs poor perception of position and con-cept. The second issue is that SmartEdit has limited expo-
sure to editing data that requires reasoning abilities, which
in turn limits its reasoning capabilities.
To tackle the first issue, we incorporate the segmenta-
tion data into the training set. Such modifications signifi-
cantly enhanced the perception capabilities of the SmartE-
dit model. Regarding the second issue, we take inspiration
from LISA [19] that a minimal amount of reasoning seg-
mentation data can efficiently activate MLLM‚Äôs reasoning
capacity. Guided by this insight, we establish a data pro-
duction pipeline and synthesize 476paired data (each sam-
ple contains original image, instruction, and synthetic tar-
get image) as a supplement to the training data. This syn-
thetic editing dataset includes two major types of scenarios:
complex understanding and reasoning scenarios. For com-
plex understanding scenarios, the original image contains
multiple objects and the corresponding instruction modifies
the specific object based on various attributes (i.e., location,
color, relative size, and in or outside the mirror). We specif-
ically consider the mirror attribute because it is a typical
example that requires a strong understanding of the scene
(both inside and outside the mirror) to perform well. For
reasoning scenarios, we involve complex reasoning cases
that need world knowledge to identify the specific object.
The effectiveness of this synthetic editing dataset and the
impact of different datasets on the model‚Äôs performance are
detailed in Section 5.4. The details of the data production
pipeline and some visual examples are described in the sup-
plementary material.
4.4. Reason-Edit for Better Evaluation
To better evaluate existing instruction editing methods and
SmartEdit‚Äôs capabilities in complex understanding and rea-
soning scenarios, we collect an evaluation dataset, Reason-
Edit. Reason-Edit consists of 219 image-text pairs. Con-
8366
sistent with the synthetic training data pairs, Reason-Edit is
also categorized in the same manner. Note that there is no
overlap between the data in Reason-Edit and the synthetic
training set. With Reason-Edit, we can thoroughly test
the performance of instruction-based image editing mod-
els in terms of understanding and reasoning scenarios. We
hope more researchers will pay attention to the capabili-
ties of instruction-based image editing models from these
perspectives, thereby fostering the practical application of
instruction-based image editing methods.
5. Experiments
5.1. Experimental Setting
Training Process and Implementation Details. The train-
ing process and implementation details of SmartEdit can be
found in the supplementary material.
Network Architecture. For the Large Language Model
with visual input (e.g., LLaV A), we choose LLaV A-1.1-7b
and LLaV A-1.1-13b as the base model. During training, the
weights of LLaV A are frozen and we add LoRA for efficient
fine-tuning. In LoRA, the values of the two parameters, dim
and alpha, are 16 and 27, respectively. We expand the orig-
inal LLM vocabulary with 32new tokens. The QFormer
is composed of 6-layer transformer [33] and 77learnable
query tokens. In the BIM module, there is a self-attention
block, two cross-attention blocks, and an MLP layer.
Training Datasets. For the training process of SmartE-
dit, the training data can be divided into 4categories: (1)
segmentation datasets, which include COCOStuff [3], Ref-
COCO [34], GRefCOCO [21], and the reasoning segmenta-
tion dataset from LISA [19]; (2) editing datasets, which in-
volve InstructPix2Pix and MagicBrush; (3) visual question
answering (VQA) dataset, which is the LLaV A-Instruct-
150k dataset [22]; (4) synthetic editing dataset, where we
collect a total of 476 paired data for complex understanding
and reasoning scenarios.
Evaluation Metrics. As we hope to only change the fore-
ground of the image while keeping the background un-
changed for editing, we adopt three metrics for the back-
ground area: PSNR, SSIM, and LPIPS [14, 37]. For the
foreground area, we calculate CLIP Score [25] between the
foreground area of the edited image and the GT label. The
GT label is annotated manually. Among these four metrics,
except for LPIPS where lower is better, the other three met-
rics are higher the better. While these metrics can reflect the
performance to a certain extent, they are not entirely accu-
rate. To provide a more accurate evaluation of the effects of
edited images, we propose a metric for assessing editing ac-
curacy. Specifically, we hire four workers to manually eval-
uate the results of these different methods on Reason-Edit.
The evaluation criterion is whether the edited image aligns
with the instruction. After obtaining the evaluation resultsfrom each worker, we average all the results to get the final
metric result, which is Instruction-Alignment (Ins-align).
5.2. Comparison with State-of-the-Art Methods
We compare SmartEdit with existing SOTA instruction-
based image editing methods, namely InstructPix2Pix,
MagicBrush, and InstructDiffusion. Considering that these
released models are trained on specific datasets, they would
inevitably perform poorly if directly evaluated on Reason-
Edit. To ensure a fair comparison, we fine-tune these meth-
ods on the same training set used by SmartEdit, and evaluate
the fine-tuned models on Reason-Edit. The experimental re-
sults are shown in Tab. 1. From the quantitative results of
the reasoning scenarios in the table, it can be observed that
when we replace the clip text encoder in the diffusion model
with LLaV A and adopt the proposed BIM module, both
SmartEdit-7B and SmartEdit-13B achieve better results on
these five metrics. This suggests that in scenarios requir-
ing reasoning from instructions, a simple clip text encoder
may struggle to understand the meaning of the instructions.
However, the MLLM can fully utilize its powerful reason-
ing ability and world knowledge to correctly identify the
corresponding objects and perform edits.
The qualitative results further illustrate this point. As
shown in Fig. 5, the first three examples are reasoning
scenarios. In the first example, both SmartEdit-7B and
SmartEdit-13B successfully identify the tool used for cut-
ting fruit (knife) and remove it, while keeping the rest of
the background unchanged. The second example can also
be handled well by both of them. However, in the third
example, we observe a difference in performance. Only
SmartEdit-13B can accurately locate the object and perform
the corresponding edits without altering other background
areas. This suggests that in instruction-based image editing
tasks that require reasoning, a more powerful MLLM model
can effectively generalize its reasoning ability to this task.
This observation aligns with the findings from LISA.
However, for understanding scenarios, we observe
a difference in performance between SmartEdit-7B and
SmartEdit-13B when compared to InstructDiffusion on
PSNR/SSIM/LPIPS. Specifically, SmartEdit-7B performs
worse than InstructDiffusion, while SmartEdit-13B outper-
forms InstructDiffusion on these metrics. Upon further
analysis of the qualitative results, as shown in the 4thand
5throws of Fig. 5, we find that from a visual perspective,
both SmartEdit-7B and SmartEdit-13B appear superior to
InstructDiffusion. This suggests that the three metrics do
not always align with human visual perception. We con-
firm this phenomenon in the supplementary material. From
the result of the Ins-align metric, it can be observed that
SmartEdit shows a significant improvement compared to
previous instruction-based image editing methods. Also,
when adopting a more powerful MLLM model, SmartEdit-
13B performs better than SmartEdit-7B on Ins-align.
8367
‚ÄúChange the cat in mirror to a tiger.‚Äù‚ÄúPlease replace food contains most vitamin with an orange. ‚Äù
‚ÄúPlease remove the object that can be used to have meals .‚Äù
‚ÄúChange the middle panda to a cat‚Äù‚ÄúWhat is the tool that is used to cut fruits . Remove this tool.‚Äù
Input Image InstructPix2Pix MagicBrush InstructDiffusion SmartEdit -7B SmartEdit -13BFigure 5. Qualitative comparison on Reason-Edit. When compared to several existing instruction editing methods that have undergone
further fine-tuning on our synthetic editing dataset, our approach demonstrates superior editing capabilities in complex scenarios.
MethodsUnderstanding Scenarios Reasoning Scenarios
PSNR(dB) ‚ÜëSSIM‚ÜëLPIPS‚ÜìCLIP Score ‚ÜëIns-align ‚ÜëPSNR(dB) SSIM LPIPS CLIP Score Ins-align ‚Üë
InstructPix2Pix 21.576 0.721 0.089 22.762 0.537 24.234 0.707 0.083 19.413 0.344
MagicBrush 18.120 0.68 0.143 22.620 0.290 22.101 0.694 0.113 19.755 0.283
InstructDiffusion 23.258 0.743 0.067 23.080 0.697 21.453 0.666 0.117 19.523 0.483
SmartEdit-7B 22.049 0.731 0.087 23.611 0.712 25.258 0.742 0.055 20.950 0.789
SmartEdit-13B 23.596 0.751 0.068 23.536 0.771 25.757 0.747 0.051 20.777 0.817
Table 1. Quantitative comparison (PSNR ‚Üë/SSIM‚Üë/LPIPS ‚Üì/CLIP Score ‚Üë(ViT-L/14)/Ins-align ‚Üë) on Reason-Edit. All the methods we com-
pared have been fine-tuned using the same training data as that used by SmartEdit.
‚ÄúWhat is the object that the dog's food should be put into ? Remove this object.‚Äù
‚ÄúPlease remove the object that can be used to eat the cake . ‚Äù
plain SimpleCA BIM Input Image
Figure 6. Demonstration of the effectiveness of the BIM Module.
5.3. Ablation Study on BIM
To validate the effectiveness of the bidirectional informa-
tion interaction in our proposed BIM module, we conduct
‚ÄúPlease replace the animal that is lying on the grass with a fox‚Äù
‚ÄúChange the red strawberry to a white pumpkin‚Äù
Input Image Edit Edit+Seg Total Edit+Reason -Edit
Figure 7. Demonstration of the significance of joint training with
multiple datasets.
comparative experiments on the SmartEdit-7B model. The
details are presented in Tab. 2. The first experiment, denoted
as Exp 1, aims to verify the necessity of the information in-
8368
Exp ID Plain SimpleCA BIMUnderstanding Scenarios Reasoning Scenarios
PSNR(dB) ‚ÜëSSIM‚ÜëLPIPS‚ÜìCLIP Score ‚ÜëIns-align ‚ÜëPSNR(dB) SSIM LPIPS CLIP Score Ins-align ‚Üë
1 ‚úì 20.975 0.713 0.108 23.36 0.695 23.848 0.725 0.074 20.33 0.694
2 ‚úì 19.557 0.692 0.126 23.66 0.692 23.508 0.716 0.081 20.17 0.722
3 ‚úì 22.049 0.731 0.087 23.61 0.712 25.258 0.742 0.055 20.95 0.789
Table 2. Quantitative comparison (PSNR ‚Üë/SSIM‚Üë/LPIPS ‚Üì/CLIP Score ‚Üë(ViT-L/14)/Ins-align ‚Üë) on Reason-Edit. These comparative exper-
iments are conducted based on the SmartEdit-7B.
Exp ID Edit Segmentation Synthetic editing datasetUnderstanding Scenarios Reasoning Scenarios
PSNR(dB) ‚ÜëSSIM‚ÜëLPIPS‚ÜìCLIP Score ‚ÜëIns-align ‚ÜëPSNR(dB) SSIM LPIPS CLIP Score Ins-align ‚Üë
1 ‚úì 17.568 0.664 0.171 22.79 0.201 22.400 0.706 0.102 19.22 0.233
2 ‚úì ‚úì 18.960 0.690 0.143 22.83 0.361 21.774 0.693 0.116 19.82 0.311
3 ‚úì ‚úì 19.562 0.702 0.111 22.32 0.440 23.595 0.715 0.079 20.43 0.567
4 ‚úì ‚úì ‚úì 22.049 0.731 0.087 23.61 0.712 25.258 0.742 0.055 20.95 0.789
Table 3. Quantitative comparison (PSNR ‚Üë/SSIM‚Üë/LPIPS ‚Üì/CLIP Score ‚Üë(ViT-L/14)/Ins-align ‚Üë) on Reason-Edit. These comparative exper-
iments are conducted based on the SmartEdit-7B.
teraction proposed in the BIM module. In this experiment,
we remove the BIM module from the SmartEdit-7B model
and directly apply the text feature output from QFormer to
the diffusion model. The second experiment, denoted as
Exp 2, aims to verify the necessity of the bidirectional in-
formation interaction proposed in the BIM module. Specif-
ically, all blocks are discarded except for the cross-attention
block on the image feature branch. Therefore, the infor-
mation from the text feature of QFormer is unidirectionally
applied to the image feature. These two experiments are de-
signed to test the impact of removing or altering the BIM
module on the performance of SmartEdit-7B in complex
understanding and reasoning scenarios. As shown in Tab. 2,
if the BIM module is removed, there is a significant decline
in all metrics for both understanding and reasoning scenar-
ios. When the BIM module is replaced with the SimpleCA
module, we observe a noticeable decline in all metrics, ex-
cept for the clip score in understanding scenarios. Further
comparison of the qualitative results in Fig. 6 confirms that
the introduction of the BIM indeed enhances SmartEdit‚Äôs
instruction editing performance. To be specific, when we
do not use the BIM module (i.e., plain), the dog bowl (first
row) turns into other objects (marked with a red circle), and
the fork (second row) does not change at all. After using
SimpleCA, it can be found that the dog bowl and fork have
been partially removed. When SmartEdit is equipped with
BIM, the dog bowl and fork can be well removed.
5.4. Ablation Study on Dataset Usage
In Section 4.3, we explore an efficient strategy for data uti-
lization, aiming to enhance SmartEdit‚Äôs capabilities in han-
dling complex understanding and reasoning scenarios. Dur-
ing the training process of SmartEdit, we employ the com-
mon editing dataset, segmentation dataset, and the synthetic
editing dataset. To validate the significance of these dif-
ferent data types in boosting SmartEdit‚Äôs performance, we
conduct a series of ablation studies, as detailed in Tab. 3.
These experiments are based on the SmartEdit-7B model.
In Exp 1, we train the model using only the editing data. In
Exp 2, we incorporate segmentation data into the trainingprocess, building upon Exp 1. In Exp 3, we further add the
synthetic editing data to the basis established in Exp 1. The
quantitative results of these experiments reveal that segmen-
tation data and synthetic editing data play complementary
roles in enhancing the model‚Äôs performance. This is further
corroborated by the visual comparison in Fig. 7. For rea-
soning scenarios, when adopting only the editing dataset or
combining the editing dataset and the segmentation dataset,
the performance of SmartEdit is inferior. When the syn-
thetic editing data is incorporated into the editing dataset,
SmartEdit can accurately locate the specific objects. How-
ever, the output of SmartEdit is also mediocre (the gener-
ated fox has obvious artifacts, and two pumpkins are gener-
ated). When all these datasets are combined as the training
set, the results generated by SmartEdit have a further signif-
icant improvement in visual effects.
6. Conclusion
In conclusion, this paper presents SmartEdit, a novel ap-
proach to instruction-based image editing that enhances un-
derstanding and reasoning capabilities by incorporating the
LLMs with visual inputs. By introducing the Bidirectional
Interaction Module (BIM), we have overcome challenges
associated with the direct integration of LLMs and diffusion
models in complex reasoning scenarios. Our data utilization
strategy, which incorporates perception data and complex
instruction editing data, effectively enhances SmartEdit‚Äôs
capabilities in handling complex understanding and reason-
ing scenarios. Evaluation on our newly constructed dataset,
Reason-Edit, shows that SmartEdit outperforms previous
methods, marking a significant step towards practical ap-
plications of complex instruction-based image editing.
Acknowledgement. The work is partially supported by the Young
Scientists Fund of the National Natural Science Foundation of
China under grant No. 62106154, the Natural Science Foundation
of Guangdong Province, China (General Program) under grant
No.2022A1515011524, Shenzhen Science and Technology Pro-
gram JCYJ20220818103001002 and JCYJ20220818103006012,
and the Guangdong Provincial Key Laboratory of Big Data Com-
puting, The Chinese University of Hong Kong (Shenzhen).
8369
References
[1] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-
structpix2pix: Learning to follow image editing instructions.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 18392‚Äì18402, 2023.
2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural in-
formation processing systems , 33:1877‚Äì1901, 2020. 3
[3] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 1209‚Äì1218, 2018. 6
[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-
aohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-
tual self-attention control for consistent image synthesis and
editing. arXiv preprint arXiv:2304.08465 , 2023. 3
[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In European confer-
ence on computer vision , pages 213‚Äì229. Springer, 2020. 4
[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, 2023. 3
[7] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N
Fung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning. Ad-
vances in Neural Information Processing Systems , 36, 2024.
3
[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems , 34:8780‚Äì8794, 2021. 2, 3
[9] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying
Shan. Planting a seed of vision in large language model.
arXiv preprint arXiv:2307.08041 , 2023. 3
[10] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li,
Xintao Wang, and Ying Shan. Making llama see and draw
with seed tokenizer. arXiv preprint arXiv:2310.01218 , 2023.
3
[11] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang
Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong
Chen, et al. Instructdiffusion: A generalist modeling in-
terface for vision tasks. arXiv preprint arXiv:2309.03895 ,
2023. 3
[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-
age editing with cross attention control. arXiv preprint
arXiv:2208.01626 , 2022. 3
[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 2, 3
[14] Alain Hore and Djemel Ziou. Image quality metrics: Psnrvs. ssim. In 2010 20th international conference on pattern
recognition , pages 2366‚Äì2369. IEEE, 2010. 6
[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685 , 2021. 4
[16] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and
Qiang Xu. Direct inversion: Boosting diffusion-based edit-
ing with 3 lines of code. arXiv preprint arXiv:2304.04269 ,
2023. 3
[17] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6007‚Äì6017, 2023. 3
[18] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-
erating images with multimodal language models. arXiv
preprint arXiv:2305.17216 , 2023. 3, 4
[19] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation
via large language model. arXiv preprint arXiv:2308.00692 ,
2023. 2, 5, 6
[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 4
[21] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gener-
alized referring expression segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 23592‚Äì23601, 2023. 6
[22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 2, 3, 6
[23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2, 3
[24] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu
Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: A
multi-modal open-ended embodied system in minecraft via
active perception. arXiv preprint arXiv:2312.07472 , 2023. 3
[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748‚Äì8763. PMLR, 2021. 2, 6
[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with clip latents. arXiv preprint arXiv:2204.06125 , 1
(2):3, 2022. 2, 3
[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj√∂rn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684‚Äì10695, 2022. 2, 3
8370
[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention‚ÄìMICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III
18, pages 234‚Äì241. Springer, 2015. 2, 3
[29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479‚Äì36494, 2022. 2, 3
[30] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong
Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun
Huang, and Xinlong Wang. Generative pretraining in multi-
modality. arXiv preprint arXiv:2307.05222 , 2023. 3
[31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste
Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 3
[32] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
Dekel. Plug-and-play diffusion features for text-driven
image-to-image translation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 1921‚Äì1930, 2023. 3
[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 4, 6
[34] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-
sions. In Computer Vision‚ÄìECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part II 14 , pages 69‚Äì85. Springer, 2016.
6
[35] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,
Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian
Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-
modal models: Pretraining and instruction tuning. arXiv
preprint arXiv:2309.02591 , 2023. 3
[36] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.
Magicbrush: A manually annotated dataset for instruction-
guided image editing. arXiv preprint arXiv:2306.10012 ,
2023. 2, 3
[37] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-
man, and Oliver Wang. The unreasonable effectiveness of
deep features as a perceptual metric. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 586‚Äì595, 2018. 6
[38] Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang,
Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Mine-
dreamer: Learning to follow instructions via chain-of-
imagination for simulated-world control. arXiv preprint
arXiv:2403.12037 , 2024. 3
[39] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-languageunderstanding with advanced large language models. arXiv
preprint arXiv:2304.10592 , 2023. 2, 3
8371
