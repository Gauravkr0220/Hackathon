Layout-Agnostic Scene Text Image Synthesis with Diffusion Models
Qilong Zhangli1,2Jindong Jiang1Di Liu1Licheng Yu2Xiaoliang Dai2
Ankit Ramchandani2Guan Pang2Dimitris N. Metaxas1Praveen Krishnan2
1Rutgers University2Meta AI
plant in a fancy pot with a 'do not touch' sign on it
A hand-drawn blueprint for a time machine, with the caption 'Time Traveling Device'a 3d model of a 1980s-style computer with the text 'my old habit' on the screentext 'balloons are flying' made from rainbow balloons, pastel background
A movie poster titled 'The Luminescent Jewel''stop racism' yellow sign'Science Project' Postage Stamp
a graffiti art of the text 'free the pink' on a wallA book cover with a title text of 'The Blessed Angels'A TV show poster named 'META'
'Only fail when you stop trying'a scene with a city in the background, and a single cloud in the foreground, with the text 'contemplate the clouds' in rounded cursive
Figure 1. Scene Text Generation. Qualitative samples of scene text images generated by our model are presented. These images contain
visually appealing texts that are coherent with the background and are created without relying on any spatial information or predefined
layouts as input, thereby enhancing the Text-to-Image (T2I) Diffusion Model‚Äôs capability to generate text.
Abstract
While diffusion models have significantly advanced the
quality of image generation, their capability to accurately
and coherently render text within these images remains a
substantial challenge. Conventional diffusion-based meth-
ods for scene text generation are typically limited by their
reliance on an intermediate layout output. This dependency
often results in a constrained diversity of text styles and
fonts, an inherent limitation stemming from the determinis-
tic nature of the layout generation phase. To address these
challenges, this paper introduces SceneTextGen, a novel
diffusion-based model specifically designed to circumvent
the need for a predefined layout stage. By doing so, Scene-
TextGen facilitates a more natural and varied representa-
tion of text. The novelty of SceneTextGen lies in its inte-
gration of three key components: a character-level encoder
for capturing detailed typographic properties, coupled with
a character-level instance segmentation model and a word-level spotting model to address the issues of unwanted text
generation and minor character inaccuracies. We validate
the performance of our method by demonstrating improved
character recognition rates on generated images across dif-
ferent public visual text datasets in comparison to both stan-
dard diffusion based methods and text specific methods.
1. Introduction
The text-to-image [50, 57] task has gained popularity with
advancements in diffusion models [41, 45, 49], significantly
enhancing the quality of image generation. However, seam-
lessly integrating clear and contextually appropriate text
into images is a persistent challenge. Text plays a vital
role in many domains, including media content creation and
artistic design, yet current diffusion models often struggle to
produce text that is lexically correct, in valid font style, and
that naturally complements the overall image aesthetics.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
7496
Prompt: A vintage postage stamp showing a painting of mountains and the text 'California'.
(a) Layout-Constrained Models
(b) Layout-Agnostic SceneTextGenFigure 2. Models reliant on predefined text layouts for input ex-
hibit limitations such as constrained font diversity and static text
positioning during each inference, leading to a lack of variability
in style and arrangement.
Traditional methods [54, 58, 62] of creating scene text
images typically formulate this problem as scene text edit-
ing which only involves editing or adding text to an ex-
isting scene image. These methods have challenges han-
dling complex backgrounds, font styles and lighting vari-
ations. While recent models [4, 8, 33, 45, 49] have made
strides in addressing these limitations by enhancing text en-
coding strategies [33] or employing predefined text layouts
[8], they still face significant constraints in generating visual
text. These constraints become apparent when observing
the limited diversity in font styles and the static positioning
of text, as shown in Fig. 2. Such rigidity in layout and font
selection hampers the capacity of generative models to pro-
duce text that is stylistically varied and contextually aligned
with the image content.
To address these issues, we propose SceneTextGen , a
novel framework that capitalizes on the capabilities of la-
tent diffusion models to infuse text into scene images with
greater diversity and authenticity. Our approach is specifi-
cally engineered to transcend the limitations of predefined
layouts, enabling more flexible text placement and an ex-
pansive assortment of text styles.
The two primary contributions of this work are: (i) the
integration of character-level encoder to capture the typo-
graphic properties of visual text and carefully injecting it
into the cross attention layers of the diffusion model, and (ii)
introducing a novel word spotting loss using a pre-trained
OCR along with a character segmentation loss to make the
network more faithful in generating visual text. The charac-
ter level encoder naturally blends with the existing diffusion
model architecture and therefore allows the network to learn
the layout of visual text implicitly rather than constraining
itself to some pre-defined form. Our comprehensive evalu-
ations confirm that SceneTextGen surpasses contemporary
methods, facilitating the generation of images with text thatis both aesthetically pleasing and rich in variety.
2. Related Work
2.1. Text 2 Image Generation Models
In the realm of text-to-image generation, diffusion mod-
els [11, 16, 27, 37, 41, 44, 49, 63, 64] represent a sig-
nificant leap forward. Different from GAN-based mod-
els [10, 17, 67], diffusion models employ a stochastic pro-
cess that iteratively adds noise to an image and learn to
reverse this process to generate images from textual de-
scriptions. Their capacity to produce high-quality, de-
tailed visual content from text prompts has been well doc-
umented. The Latent Diffusion Model (LDM)[42] further
enhances this approach by operating in a compressed im-
age latent space, improving both efficiency and image qual-
ity. It also facilitates high-quality conditional generation
through cross-attention text conditioning, leading to vari-
ous downstream applications [12, 13, 15, 19, 55]. DALL-E
[41], renowned for its novel approach of combining discrete
V AEs with transformer language models, has shown re-
markable ability in generating diverse and complex images
from textual descriptions, showcasing the potential of trans-
former architectures in creative generative tasks. Deepfloyd
[49], utilizing the robust T5 text encoders [40], not only
enhances image quality but also facilitates nuanced under-
standing of complex prompts, thereby allowing for more
accurate and context-aware visual representations. Control-
Net [63], has demonstrated exceptional performance in con-
ditioned image generation by providing the model with ref-
erences such as skeleton, canny edge images, and segmen-
tation maps. However, these models frequently encounter
difficulties when it comes to the generation of text within
images (see Fig. 4), a task requiring the text to be not
only visually integrated but also contextually pertinent to
the image content. This challenge stems from the complex-
ity of modeling the fine-grained interplay between visual
and textual elements, ensuring that the generated text is leg-
ible, aesthetically fitting, and semantically in sync with the
image. Prior attempts to refine this aspect have led to im-
provements [8, 49, 63], yet the generation of contextually
coherent text in images remains a largely unsolved problem,
underscoring the need for more focused research.
2.2. Scene Text Generation
The success of adversarial networks such as GANs [7, 10,
17, 56, 67] for image generation and style transfer [20, 21]
gave rise to scene text generation methods which can gen-
erate text at the granularity of glyphs [1, 26] or individual
words [23, 54, 58]. Many previous works focused on the
particular task of scene text editing where the model learns
a style from a reference image and renders the target content
in that style. Methods such as SRNet [54], SWAPText [58]
7497
decomposes the problem into: (1) learning the foreground
text using style and content, (2) background in-painting net-
work to remove existing text, and (3) a blending network
to merge foreground and background. These methods of-
ten fail in learning the correct style and removing existing
text from complex backgrounds. TextStyleBrush [23] pro-
poses a self-supervised approach to disentangle content and
style and generate word images in a one-shot manner. All
these previous methods are limited to generating individual
words, requires reference word style images and does not
generalize to generate the entire scene text image.
Spatial fusion GAN (SF-GAN) [62] generates text im-
ages by superimposing a foreground content image, trans-
formed to match the style and geometry of a background
image. This approach is aimed more at pure text synthesis
on image regions without text, whereas in this work we aim
to generate both image and text in a manner that reflects its
natural appearance in real-world contexts.
With the significant progress of diffusion models in text-
to-image generation, recent methods in scene text genera-
tion adapt these models towards producing more legible vi-
sual text. DIFFSTE [18] enhances scene text editing with
a dual encoder design in diffusion models, promoting text
legibility and style control. It is adept at mapping text in-
structions to images, showcasing zero-shot capabilities for
rendering text in novel font variations and interpreting infor-
mal natural language instructions. This method is however a
scene text editing method which generates single keywords
on specific regions defined by a mask, whereas we propose
a scene text generation network. One of the closest work
in this space is TextDiffuser [8] which address the problem
of scene text generation by decomposing the problem into
two-stages. In the first stage, a transformer model create
the layout mask for keywords extracted from text prompts.
This is then taken as condition data while formulating the
diffusion model to generate scene text images. They also
introduce the character segmentation loss which helps in
generating legible text. ControlNet [63], though not orig-
inally designed for scene text image rendering, has been ef-
fectively adapted for this purpose. It utilizes canny edge
maps as conditional inputs, sourced from printed text im-
ages generated by a layout model, to fine-tune the diffusion
model‚Äôs output. The use of pre-defined layout for visual text
generation in [8, 63], seriously limits the diversity of text
styles, fonts and even the layouts. We believe this is due to
the inherent difficulty in predicting layouts independently
without the general image guidance. In our work, we avoid
the need of pre-defined layout and make the network im-
plicitly learn layout along with image generation using our
novel way of injecting character level features. We also in-
troduce a word spotting loss which augments the character
segmentation loss proposed in [8] to generate more legible
visual texts.2.3. Scene Text Recognition
Advances in computer vision have laid a foundation for so-
phisticated analytical techniques in various domains [2, 9,
14, 30‚Äì32, 36, 46, 53, 60, 61, 65]. Especially in scene
text image recognition, most existing works split the pro-
cess into two stages: a text detection [3, 28, 29, 34, 48, 66]
module to detect words or characters from complex back-
grounds, and a text recognition [2, 24, 47, 52] module
which transcribes the text into unicode characters given
a cropped word image. More recently, end-2-end meth-
ods [6, 25, 38, 43, 51] have become popular due to the
benefits of joint training of detector and recognizer to share
contextual information.
In the nexus of scene text generation and recognition,
leveraging pre-trained scene text recognition or word spot-
ting models as guidance during diffusion-based text-to-
image synthesis has surfaced as an innovative strategy. For
simplicity, in this paper we refer to a scene text recogni-
tion module as OCR (optical character recognition). The
integration of OCR-derived losses enables the refinement
of generative models to produce text that is not just visually
coherent but also contextually accurate. This confluence of
generative modeling prowess with OCR accuracy paves the
way for novel research avenues to generate images with text
that is both authentic to read and visually integrated.
3. Methodology
3.1. Motivation
Our objective is to facilitate layout-free text image genera-
tion with diverse layouts and styles. A straightforward ap-
proach would be to directly fine-tune an existing latent dif-
fusion model with text images. However, our early inves-
tigation suggests that this strategy did not yield significant
improvements compared to the original LDM. We hypothe-
size that this limitation is due to two primary factors. First,
the language encoder of the LDM, primarily designed for
semantic interpretation, fails to capture character-specific
information adequately for text rendering. This encoder
tends to provide more semantic than structural information
about the text, necessitating a dedicated network for encod-
ing the text and guiding the model on its visual representa-
tion. Second, the conventional denoising loss used in diffu-
sion models seems insufficient for accurately rendering text
in images, often leading to text regions resembling textual
patterns without the distinct features of text strokes. To ad-
dress these challenges, we propose integrating a character-
level encoder and a hierarchical cross-attention mechanism
to learn character-level context information. Additionally,
we introduce two auxiliary losses at the word and character
levels to emphasize the text presentation. The subsequent
sections will detail each of these components and their con-
tribution to enhancing our model‚Äôs performance.
7498
OCR texts
‚Ä¶‚Ä¶LDMQKVKVQQKVKVQCharacter-Aware Diffusion Training
Image
Image CaptionsCaptionEncoder
Character Encoder√óùëáContextual Consistency Losses 
Recoveredùë•!Content Loss\Word Spotting ModelCharacter SegmentModel: Fixed: TrainableKV: Caption FeaturesKV: OCR Text FeaturesFigure 3. Model Framework: SceneTextGen employs a character-level encoder to extract detailed character-specific features. During loss
computation, the model leverages both word-level and character-level supervisions to guide the recovery of the image, in addition to the
standard denoising loss. This dual-level supervision enhances the model‚Äôs ability to accurately generate and refine text within scenes.
3.2. Preliminaries
The proposed method is trained on a corpus of visual text
images. Given an image Iwith textual caption c, we denote
wi, where i‚àà[0, N], as the visual text (words) present in
this image. We also assume that for each word, we know
its location, given in terms of the bounding box. For all
practical purposes, we assume this can be pre-computed us-
ing a pre-trained OCR detection [28] and recognition net-
work [5]. Please note, the OCR bounding box and tran-
scriptions are typically noisy. Fig. 3 presents an overview
of our proposed method. A latent diffusion model has three
key components: a CLIP encoder for semantic embedding
of text and images, an autoencoder for dimensionality re-
duction and feature extraction, and a UNet-based structure
for effective image-text synthesis and manipulation. The
core architecture of our method follows the latent diffu-
sion model, which is adapted to incorporate the proposed
character-level features and content-based loss functions.
3.3. Character-Level Encoding
Our approach begins by extracting and ordering the visual
text words from the image based on its locations as given
by the bounding box. We sort the word boxes following
the conventional reading pattern, i.e., from left to right and
top to bottom. This provides us the naive ordering of vi-
sual text which is then tokenized at the character level to
capture its typographic information. Then, a character en-
coder is used to encode these tokenized characters into a
high-dimensional feature space, allowing an accurate tran-
scription of the text‚Äôs spelling and appearance. The derived
text features encapsulate the typographical details and are
poised for the subsequent integration with the image‚Äôs la-
tent features, which have already been contextually primedby the initial cross-attention with the encoded caption fea-
tures obtained from the CLIP [39] text encoder.
3.4. Hierarchical Text Integration Process
We have employed a sequential cross-attention mechanism
to integrate character information into the U-Net architec-
ture effectively. This method is inspired by the observation
that the cross-attention of captioning features is pivotal in
shaping the overall spatial structure of an image, a notion
supported by prior studies [15]. Leveraging this concept,
our model initially constructs the general spatial layout of
the image content, guided by the caption, using the first
cross-attention layer. It then focuses on the precise render-
ing of characters in a subsequent cross-attention phase. This
approach establishes a hierarchical text integration process,
facilitating the development of a preliminary visual scaf-
fold that is both thematically and contextually coherent. It
ensures that the textual elements are accurately positioned
and seamlessly integrated into the overall image structure.
3.5. OCR-Guided Diffusion for Text Accuracy
To ensure the textual accuracy of generated images, our
model incorporates an OCR loss, utilizing the predictions
from the end to end pre-trained GLASS model [43]. Upon
each iteration of the diffusion process, the UNet predicts a
denoising step, from which we derive an estimation of clean
image x0. This estimated x0is then decoded through a Vari-
ational Autoencoder (V AE) [22] to reconstruct an image.
The GLASS OCR model performs inference on this re-
constructed image to produce word-level recognition re-
sults, which are represented as a tensor Pwith shape
[N, L, K ], where Nis the number of words detected, Lis
the maximum length of any word, and Kis the size of the
7499
character set. The ground truth for these detections is rep-
resented as a tensor Gwith shape [N, L], where each entry
is the character index for the corresponding position, and
non-character positions are marked with zero.
The OCR loss ( LOCR) is computed using a masked cross-
entropy function, which is formulated as follows:
LOCR=‚àí1PMNX
i=1LX
j=1Mij¬∑log 
exp(Pij[Gij])PK
k=1exp(Pij[k])!
(1)
Here, Mis a binary mask tensor that has the same shape
asGand indicates the valid character positions (i.e., where
GijÃ∏= 0).Mijrepresents the binary value of the mask at the
i-th word and j-th character position, Pijis the predicted
probability distribution over the character set, and Gijis
the ground truth character index at that position.
By integrating this OCR loss into the training regime, we
guide the diffusion model to produce text that is not only vi-
sually coherent but also textually accurate, as recognized by
the GLASS [43] OCR model, thereby enhancing the overall
fidelity of the generated images.
3.6. Refinement of Text Generation with Character-
Level Constraints
During the iterative refinement of our diffusion model, we
observed an unintended consequence of the OCR loss; the
model tended to generate images with repetitive words.
This issue is potentially attributed to the blurry nature of im-
ages at higher noise levels during training, which could ren-
der the OCR loss counterproductive. The word-level OCR
loss, while ensuring textual accuracy, imposes no explicit
constraint on the quantity of text within the image, inadver-
tently encouraging the model to generate excessive text.
To address this, we augmented our loss function with
a character-level segmentation loss, which acts directly on
the latent space rather than the recovered image. After ob-
taining the predicted latent features of a image x0from the
UNet, we proceed in two directions: we decode x0using
the V AE to compute the word-level OCR loss on the re-
covered image (as explained earlier), and we also apply x0
to a pre-trained character-level segmentation model based
on U-Net adapted from [8]. This model outputs a 96-
dimensional feature map (corresponding to the length of
the alphabet plus one for non-character pixels) with a spa-
tial resolution of 64√ó64. The character-aware loss is then
computed via cross-entropy between this feature map and a
resized character-level segmentation mask C.
Thus, the total loss function is a composite of the denois-
ing loss, the word-level recognition loss, and the character-
level segmentation loss, expressed as:
Ltotal=Ldenoising +ŒªwordLOCR-word +ŒªcharLOCR-char (2)where Ldenoising is the denoising loss, LOCR-word is the
word-level OCR loss, LOCR-char is the character-level seg-
mentation loss, and Œªword andŒªcharare weighting coeffi-
cients balancing the contribution of each term.
The character-level segmentation loss is formulated as:
LOCR-char =‚àí1
HWHX
h=1WX
w=196X
c=1yhwclog (ÀÜyhwc) (3)
where HandWare the height and width of the feature
map,yis the ground truth character segmentation mask, and
ÀÜyis the predicted character probability map from the seg-
mentation model.
By integrating character-level information directly in the
latent space, we impose a structured constraint on text gen-
eration, promoting both the accuracy and the appropriate
quantity of text in the generated images.
4. Experiments
4.1. Implementation Details
Datasets For the training of our model, we utilized
the publicly available MARIO dataset from [8], exclud-
ing MARIO-TMDB, and MARIO-OpenLibrary subsets as
they are not publicly accessible. Upon removing any
corrupted images, our final dataset comprised 7,249,449
image-caption pairs. In addition, to bolster our model‚Äôs ca-
pability in generating a broader range of concepts (beyond
text-centric images), we integrated 2,110,745 non-text im-
ages. These additional images, accompanied by text pairs,
were selected based on a minimum predicted aesthetics
score of 6.25, allowing for joint training to enhance over-
all performance.
Baselines We conducted quantitative comparisons of our
SceneTextGen method against several leading approaches,
including LDM[42], ControlNet[63], TextDiffuser[8],
GlyphControl[59], and DeepFloyd[49], utilizing the pub-
licly available code and pre-trained models for fairness.
Notably, DeepFloyd is distinguished by its dual super-
resolution modules, enabling it to produce high-resolution
images at 1024x1024 pixels, in contrast to the 512x512
pixel images generated by the other models. For Control-
Net comparisons, we employed Canny edge maps of printed
text images created by the initial model stage of TextDif-
fuser as conditioning inputs. However, due to the unavail-
ability of APIs, open-source code or checkpoints, we could
not extend our comparative analysis to include Imagen[45],
eDiff-i[4], or GlyphDraw[35].
Evaluation Criteria We assess text rendering qual-
ity using the MARIO-7M-Eval, MARIO-TMDB-Eval, and
7500
MARIO-OpenLibrary-Eval datasets. Our evaluation is
twofold: firstly, through CLIPScore, which measures the
cosine similarity between the image and text representa-
tions from CLIP; and secondly, via OCR Evaluation, which
leverages existing OCR tools to detect and recognize text
regions within the generated images. Metrics such as Av-
erage Precision, Average Recall, F1 Score, and Accuracy
are employed to determine the presence of keywords in the
generated images. During training, the input of the charac-
ter encoder is the ground truth OCR texts. During the in-
ference process, the character encoder receives as its input
the text from captions provided by the user. These captions
are enclosed in quotation marks as specified in the user‚Äôs
prompts, and the input of the CLIP text encoder is the full
caption. For each generated and ground truth image pair, we
utilize the easy-ocr library for OCR detection and recogni-
tion, followed by Hungarian Matching between the sets of
texts, applying the Levenshtein distance (or edit distance)
on the matched text pairs for the OCR evaluation.
Algorithm 1 T2I Visual Text Performance Evaluation
function METRICS (M)
P‚ÜêPRECISION (M)
R‚ÜêRECALL (M)
F‚ÜêF1(P, R )
A‚ÜêACCURACY (M)
return {P, R, F, A }
end function
1:GT‚ÜêOCR (Ground Truth Images )
2:Gen‚ÜêOCR (Generated Images )
3:Scores ‚Üêan empty list
4:foreachpair in Z IP(GT, Gen )do
5: C‚ÜêCOSTMAT(pair )
6: M‚ÜêHUNGARIAN (C)
7: Scores ‚ÜêScores ‚à™ {METRICS (M)}
8:end for
9:return AVERAGE (Scores )
Pseudo Code for OCR Performance Evaluation The
methodology described in Algorithm 1 demonstrates the
steps taken to assess the performance of the text-to-image
conversion. In the initial step, the algorithm applies OCR to
both the ground truth and the generated images. This pro-
cess results in two sets of text outputs, which are then paired
for comparison. The Hungarian algorithm is employed here
to find the optimal matching between elements (words) of
these two sets, minimizing the overall difference between
the matched pairs. This is crucial for an objective and accu-
rate comparison. For each matched pair, we calculate a cost
matrix, which serves as the input for the Hungarian algo-
rithm. The output of this step is a matching matrix Mwhich
represents the best possible alignment between the text el-
ements in the ground truth and the generated images. Sub-sequently, the algorithm computes key performance metrics
for each pair: Precision, Recall, F1 Score, and Accuracy.
Precision focuses on the accuracy of the replicated text, re-
call measures the completeness, F1 score provides a balance
between precision and recall, and accuracy gives an overall
effectiveness of the text replication. Finally, the algorithm
averages these scores across all image pairs to provide an
overall performance evaluation of the text-to-image conver-
sion process.
4.2. Quantitative Results
Our experimental analysis in Tab. 1 provides a direct com-
parison of the OCR based recognition scores among various
models as measured in terms of Precision, Recall, F1 scores,
and Accuracy. Our results indicate that SceneTextGen con-
sistently outperforms competing models in most metrics.
Latent Diffusion Model, lacking a sophisticated mechanism
for text comprehension, typically under-performs, leading
to lower OCR scores. In contrast, DeepFloyd[49] incor-
porates a T5 encoder which aids in textual understanding,
thereby enhancing the quality of the generated text. How-
ever, its performance is still limited due to an insufficient
character-level understanding.
ControlNet[63], TextDiffuser[8], and GlyphControl[59],
which utilize predefined text layouts or spatial information,
show mixed results. While the explicit introduction of text
information allows ControlNet to achieve high OCR scores,
the resulting text often appears artificial and lacks seamless
integration within the images (see Fig. 4).
Cross-dataset Generalization Ability As demonstrated
in Tab. 1, SceneTextGen-7M, despite being trained solely
on the MARIO-7M dataset, exhibits strong generalization
capabilities. It maintains robust OCR scores across evalu-
ation sets from both the TMDB and OpenLibrary datasets,
underscoring its adaptability and the efficacy of its training
methodology.
4.3. Measuring Font Style Diversity
To quantitatively assess the diversity of font styles gener-
ated by SceneTextGen, we utilized a pretrained VGG-based
font recognition model trained on synthesized text images.
This approach involved first extracting text image patches
using a pretrained Optical Character Recognition (OCR)
model. These patches were then processed through the font
recognition model to retrieve features from the penultimate
layer. By applying t-SNE for dimensionality reduction, we
visualized the feature space to examine the proximity and
diversity of text generated by different methods.
As illustrated in Fig. 5, the rendered text images ‚Äî
which serve as a baseline ‚Äî were created by printing text
onto a white canvas using the layout generator from [8] with
7501
Pre-defined
Text LayoutMARIO-7M TMDB OpenLibrary
AP(‚Üë) AR( ‚Üë) F1( ‚Üë) AC( ‚Üë) AP( ‚Üë) AR( ‚Üë) F1( ‚Üë) AC( ‚Üë) AP( ‚Üë) AR( ‚Üë) F1( ‚Üë) AC( ‚Üë)
TextDiffuser-10M [8] ‚úì 0.6135 0.4289 0.4683 0.3425 0.4401 0.4243 0.3994 0.2889 0.5617 0.3816 0.4242 0.2916
GlyphControl-10M* [59] ‚úì 0.5075 0.4118 0.4140 0.2883 0.3635 0.4082 0.3457 0.2362 0.4734 0.3762 0.3836 0.2534
Latent Diffusion Model 0.1482 0.1690 0.1296 0.0753 0.1717 0.2579 0.1703 0.0974 0.1911 0.2873 0.1923 0.1106
DeepFloyd [49] 0.2467 0.2788 0.2206 0.1366 0.2284 0.3738 0.2449 0.1500 0.2555 0.3910 0.2635 0.1610
ControlNet [63] ‚úì 0.5102 0.4444 0.4238 0.2981 0.3075 0.4667 0.3284 0.2194 0.4050 0.4273 0.3690 0.2415
TextDiffuser-7M [8] ‚úì 0.4778 0.3447 0.3682 0.2512 0.3198 0.3362 0.2961 0.1930 0.4257 0.3146 0.3318 0.2108
SceneTextGen-7M(Ours) 0.5274 0.4420 0.4424 0.3088 0.3813 0.4716 0.3790 0.2602 0.4136 0.4519 0.3945 0.2571
Table 1. Comparative analysis of OCR based text recognition scores across different models. Note, 7M denotes models that were trained on
the MARIO-7M dataset (7 million images with texts). In contrast, the TextDiffuser-10M category includes models trained on an expanded
dataset collection that encompasses MARIO-7M, MARIO-TMDB, and MARIO-OpenLibrary. * denotes LAION-Glyph dataset.
PromptSDSD-XLMidjourneyTextDiffusera bear holdsa board saying'hello world'
a meme of'Are you kidding'a cake of'Happy Birthdayto XYZ'ControlNet
a poster of'Monkey MusicFestival'
DALL  E
DeepFloyd
a book of'AI in Next Century'written by'AI Robot'a boy holds 'P'anda girl holds 'Q'
PromptSDSD-XLMidjourneyTextDiffusera bear holdsa board saying'hello world'
a meme of'Are you kidding'a cake of'Happy Birthdayto XYZ'ControlNet
a poster of'Monkey MusicFestival'
DALL  E
DeepFloyd
a book of'AI in Next Century'written by'AI Robot'a boy holds 'P'anda girl holds 'Q'
TextDiffuserPromptSDSD-XLMidjourneyTextDiffusera bear holdsa board saying'hello world'
a meme of'Are you kidding'a cake of'Happy Birthdayto XYZ'ControlNet
a poster of'Monkey MusicFestival'
DALL  E
DeepFloyd
a book of'AI in Next Century'written by'AI Robot'a boy holds 'P'anda girl holds 'Q'
SceneTextGen
LDMLDM-X
Figure 4. Comparative visualization of generated images. We present a side-by-side comparison of images generated from the same text
prompt across different existing methods (with results generated by [8] as a proxy. Human faces are blurred for ethical considerations.).
Each row corresponds to a unique prompt, showcasing the visual quality, text clarity, and contextual coherence achieved by each method.
the default ‚ÄôArial‚Äô font. This process establishes a refer-
ence for the feature distribution of ‚ÄôArtificial Texts.‚Äô Con-
trolNet [63], constrained by predefined text canny edges,
exhibits a feature distribution closely aligned with the ren-
dered text images. This proximity suggests its limitations
in integrating text seamlessly within images, a finding cor-
roborated by the qualitative results shown in Fig. 4. TextD-
iffuser [8], although performing well in most instances, still
shares some feature space with ControlNet, indicating oc-
casional production of artificial or unnatural texts. In con-
trast, SceneTextGen ‚Äî operating independently of any pre-
defined canny edges or layouts ‚Äî demonstrates a distinct
distribution with minimal overlap with the ‚Äôrendered textimages,‚Äô signifying its robustness in generating naturalistic
text within the context of scene images. Example visual-
izations are in Fig. 6. In addition, to better understand the
text layout given by each model, we also show in Fig. 7 the
overall distribution of visual text in the generated images
and ground truth images.
4.4. Weight of Each Loss
In this section, we present two ablation studies to evaluate
the impact of different loss function configurations on OCR
performance. Table 3 explores the effects of using combi-
nations of word-level and character-level losses. Table 4 ex-
amines the impact of varying the weights of these loss func-
7502
Pre-defined
Text Layout CLIP Score( ‚Üë)
TextDiffuser-10M [8] ‚úì 0.3436
GlyphControl-10M* [59] ‚úì 0.3450
Latent Diffusion Model 0.3015
DeepFloyd [49] 0.3267
ControlNet [63] ‚úì 0.3424
TextDiffuser-7M [8] ‚úì 0.3385
SceneTextGen-7M(Ours) 0.3455
Table 2. Comparison of CLIP scores reflecting the overall image
quality generated by various models. SceneTextGen-7M achieves
the highest CLIP score, indicating superior text-image alignment
without relying on text layouts. * denotes LAION-Glyph dataset.
Natural Texts
Unnatural Texts
Figure 5. t-SNE representation of text region embeddings derived
from the penultimate layer features of a font recognition model.
Caption: A sign that says 'Hello World'
Figure 6. Diversity of font styles. SceneTextGen is able to gener-
ate visually appealing and diverse font styles and layouts for text,
without any style or layout prompts.
tions. In both studies, we assess the Average Precision (AP)
and Accuracy (AC) scores to understand how these configu-
rations influence the model‚Äôs ability to accurately recognize
and generate text in images.
4.5. Limitations
While SceneTextGen demonstrates superior performance
in scene text image generation, challenges remain. Lim-
ited scene complexity: Models struggle to accurately gen-
erate complex elements in conjunction with text, such as
a car with a ‚Äôgreen‚Äô sticker specifically in the back win-
TextDiffuserSceneTextGenGround Truth
Figure 7. Distribution of visual text coordinates in the generated
image w.r.t. the area of each box. As expected from real-life ob-
servations, the visual text in the ground truth images tends to be
located at the center of each image.
Lword LcharAP AC
‚úì 0.5082 0.3109
‚úì 0.5083 0.3035
‚úì ‚úì 0.5274 0.3088
Table 3. Effects of loss
function combinations on the
MARIO-7M-Eval datasetWlamClamAP AC
1 2 0.4107 0.2782
0.1 0.2 0.4587 0.2895
0.01 0.02 0.5274 0.3088
Table 4. Impact of varying
loss weights on the MARIO-
7M-eval dataset
LDM          DeepFloydControlNet     TextDiffuserSceneTextGen
Caption: a green-colored luxury car with a 'green' sticker in the back windowCaption: photo of a sign with 'having a dog named shark at the beach was a mistake'Case 2               Case 1
Figure 8. Failure cases in complex scene interpretation and pro-
cessing of lengthy prompts for text.
dow.(Fig. 8, Case 1). Long text handling: Text accuracy
and coherence decrease with increasing length (Fig. 8, Case
2). These areas require further development.
5. Conclusion
SceneTextGen, incorporating a character-level encoder and
hierarchical text integration, offers advancements in scene
text image generation. Despite improved text rendering,
limitations arise in generating complex visuals and handling
lengthy text. These challenges highlight the ongoing dif-
ficulty in reconciling textual accuracy with broader image
synthesis. This work furthers our understanding of text-
image generation, paving the way for future exploration.
Acknowledgements This research project has been par-
tially funded by research grants to Dimitris N. Metaxas
through NSF: 2310966, 2235405, 2212301, 2003874, and
FA9550-23-1-0417.
7503
References
[1] Samaneh Azadi, Matthew Fisher, Vladimir G Kim, Zhaowen
Wang, Eli Shechtman, and Trevor Darrell. Multi-content
gan for few-shot font style transfer. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 7564‚Äì7573, 2018. 2
[2] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park,
Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwal-
suk Lee. What is wrong with scene text recognition model
comparisons? dataset and model analysis. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 4715‚Äì4723, 2019. 3
[3] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun,
and Hwalsuk Lee. Character region awareness for text de-
tection. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 9365‚Äì9374,
2019. 3
[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,
Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image
diffusion models with an ensemble of expert denoisers. arXiv
preprint arXiv:2211.01324 , 2022. 2, 5
[5] Darwin Bautista and Rowel Atienza. Scene text recognition
with permuted autoregressive sequence models. In European
Conference on Computer Vision , pages 178‚Äì196. Springer,
2022. 4
[6] Michal Bu Àásta, Yash Patel, and Jiri Matas. E2e-mlt-an uncon-
strained end-to-end method for multi-language scene text.
InComputer Vision‚ÄìACCV 2018 Workshops: 14th Asian
Conference on Computer Vision, Perth, Australia, Decem-
ber 2‚Äì6, 2018, Revised Selected Papers 14 , pages 127‚Äì143.
Springer, 2019. 3
[7] Qi Chang, Zhennan Yan, Mu Zhou, Di Liu, Khalid Sawalha,
Meng Ye, Qilong Zhangli, Mikael Kanski, Subhi Al‚ÄôAref,
Leon Axel, et al. Deeprecon: Joint 2d cardiac segmentation
and 3d volume reconstruction via a structure-specific gener-
ative method. In International Conference on Medical Image
Computing and Computer-Assisted Intervention , pages 567‚Äì
577. Springer, 2022. 2
[8] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng
Chen, and Furu Wei. Textdiffuser: Diffusion models as text
painters. arXiv preprint arXiv:2305.10855 , 2023. 2, 3, 5, 6,
7, 8
[9] Yunhe Gao, Mu Zhou, Di Liu, Zhennan Yan, Shaoting
Zhang, and Dimitris N Metaxas. A data-scalable transformer
for medical image segmentation: architecture, model effi-
ciency, and benchmark. arXiv preprint arXiv:2203.00131 ,
2022. 3
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. Advances in
neural information processing systems , 27, 2014. 2
[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 10696‚Äì10706, 2022. 2[12] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,
Dimitris Metaxas, and Feng Yang. Svdiff: Compact param-
eter space for diffusion fine-tuning. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 7323‚Äì7334, 2023. 2
[13] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng
Song, Mengwei Ren, Ruijiang Gao, Anastasis Stathopou-
los, Xiaoxiao He, Yuxiao Chen, Di Liu, Qilong Zhangli,
Jindong Jiang, Zhaoyang Xia, Akash Srivastava, and Dim-
itris Metaxas. Proxedit: Improving tuning-free real im-
age editing with proximal guidance. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision (WACV) , pages 4291‚Äì4301, 2024. 2
[14] Xiaoxiao He, Chaowei Tan, Bo Liu, Liping Si, Weiwu Yao,
Liang Zhao, Di Liu, Qilong Zhangli, Qi Chang, Kang Li,
et al. Dealing with heterogeneous 3d mr knee images: A fed-
erated few-shot learning method with dual knowledge dis-
tillation. In 2023 IEEE 20th International Symposium on
Biomedical Imaging (ISBI) , pages 1‚Äì5. IEEE, 2023. 3
[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image
editing with cross-attention control. In The Eleventh Inter-
national Conference on Learning Representations , 2022. 2,
4
[16] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high fidelity image generation. The Journal of
Machine Learning Research , 23(1):2249‚Äì2281, 2022. 2
[17] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125‚Äì1134,
2017. 2
[18] Jiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou,
Zhifei Zhang, Brian Price, and Shiyu Chang. Improving
diffusion models for scene text editing with dual encoders.
arXiv preprint arXiv:2304.05568 , 2023. 3
[19] Jindong Jiang, Fei Deng, Gautam Singh, and Sungjin Ahn.
Object-centric slot diffusion. In Advances in Neural Infor-
mation Processing Systems , pages 8563‚Äì8601, 2023. 2
[20] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. arXiv preprint arXiv:1710.10196 , 2017. 2
[21] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8110‚Äì8119, 2020. 2
[22] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 4
[23] Praveen Krishnan, Rama Kovvuri, Guan Pang, Boris Vas-
silev, and Tal Hassner. Textstylebrush: Transfer of text aes-
thetics from a single example. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 2023. 2, 3
[24] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets
with attention modeling for ocr in the wild. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 2231‚Äì2239, 2016. 3
7504
[25] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan
Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei.
Trocr: Transformer-based optical character recognition with
pre-trained models. In Proceedings of the AAAI Conference
on Artificial Intelligence , pages 13094‚Äì13102, 2023. 3
[26] Wei Li, Yongxing He, Yanwei Qi, Zejian Li, and Yongchuan
Tang. Fet-gan: Font and effect transfer via k-shot adaptive
instance normalization. In Proceedings of the AAAI confer-
ence on artificial intelligence , pages 1717‚Äì1724, 2020. 2
[27] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 22511‚Äì22521, 2023. 2
[28] Minghui Liao, Zhaoyi Wan, Cong Yao, Kai Chen, and Xiang
Bai. Real-time scene text detection with differentiable bina-
rization. In Proceedings of the AAAI conference on artificial
intelligence , pages 11474‚Äì11481, 2020. 3, 4
[29] Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman,
Shai Mazor, and R Manmatha. Scatter: selective con-
text attentional scene text recognizer. In proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11962‚Äì11972, 2020. 3
[30] Di Liu, Yunhe Gao, Qilong Zhangli, Ligong Han, Xiaox-
iao He, Zhaoyang Xia, Song Wen, Qi Chang, Zhennan Yan,
Mu Zhou, et al. Transfusion: multi-view divergent fusion
for medical image segmentation with transformers. In In-
ternational Conference on Medical Image Computing and
Computer-Assisted Intervention , pages 485‚Äì495. Springer,
2022. 3
[31] Di Liu, Xiang Yu, Meng Ye, Qilong Zhangli, Zhuowei Li,
Zhixing Zhang, and Dimitris N Metaxas. Deformer: Inte-
grating transformers with deformable models for 3d shape
abstraction from a single image. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 14236‚Äì14246, 2023.
[32] Di Liu, Anastasis Stathopoulos, Qilong Zhangli, Yunhe Gao,
and Dimitris Metaxas. Lepard: Learning explicit part dis-
covery for 3d articulated shape reconstruction. Advances in
Neural Information Processing Systems , 36, 2024. 3
[33] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan,
Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mo-
hammad Norouzi, and Noah Constant. Character-aware
models improve visual text rendering. arXiv preprint
arXiv:2212.10562 , 2022. 2
[34] Jianqi Ma, Weiyuan Shao, Hao Ye, Li Wang, Hong Wang,
Yingbin Zheng, and Xiangyang Xue. Arbitrary-oriented
scene text detection via rotation proposals. IEEE transac-
tions on multimedia , 20(11):3111‚Äì3122, 2018. 3
[35] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu,
Haonan Lu, and Xiaodong Lin. Glyphdraw: Learning to
draw chinese characters in image synthesis models coher-
ently. arXiv preprint arXiv:2303.17870 , 2023. 5
[36] Carlos Mart ¬¥ƒ±n-Isla, V ¬¥ƒ±ctor M Campello, Cristian Izquierdo,
Kaisar Kushibar, Carla Sendra-Balcells, Polyxeni Gkontra,
Alireza Sojoudi, Mitchell J Fulton, Tewodros Weldebirhan
Arega, Kumaradevan Punithakumar, et al. Deep learn-
ing segmentation of the right ventricle in cardiac mri: Them&ms challenge. IEEE Journal of Biomedical and Health
Informatics , 2023. 3
[37] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741 , 2021. 2
[38] Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa
Fujii, and Ying Xiao. Towards unconstrained end-to-end text
spotting. In Proceedings of the IEEE/CVF international con-
ference on computer vision , pages 4704‚Äì4714, 2019. 3
[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748‚Äì8763. PMLR, 2021. 4
[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research , 21(1):5485‚Äì5551, 2020. 2
[41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning , pages 8821‚Äì8831. PMLR, 2021.
1, 2
[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684‚Äì10695, 2022. 2, 5
[43] Roi Ronen, Shahar Tsiper, Oron Anschel, Inbal Lavi, Amir
Markovitz, and R Manmatha. Glass: Global to local at-
tention for scene-text spotting. In European Conference on
Computer Vision , pages 249‚Äì266. Springer, 2022. 3, 4, 5
[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22500‚Äì
22510, 2023. 2
[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479‚Äì36494, 2022. 1, 2, 5
[46] Lohrasb Ross Sayadi, Usama S Hamdan, Qilong Zhangli,
and Raj M Vyas. Harnessing the power of artificial intelli-
gence to teach cleft lip surgery. Plastic and Reconstructive
Surgery‚ÄìGlobal Open , 10(7):e4451, 2022. 3
[47] Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao,
and Xiang Bai. Robust scene text recognition with auto-
matic rectification. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 4168‚Äì4176,
2016. 3
7505
[48] Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan
Lyu, Cong Yao, and Xiang Bai. Aster: An attentional scene
text recognizer with flexible rectification. IEEE transactions
on pattern analysis and machine intelligence , 41(9):2035‚Äì
2048, 2018. 3
[49] Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva,
Christoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova.
Deepfloyd-if. https://github.com/deep-floyd/
IF, 2023. GitHub Repository. 1, 2, 5, 6, 7, 8
[50] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiaoyuan
Jing, Fei Wu, and Bingkun Bao. Deep fusion genera-
tive adversarial networks for text-to-image synthesis. arXiv
preprint arXiv:2008.05865 , 2020. 1
[51] Peng Wang, Hui Li, and Chunhua Shen. Towards end-to-end
text spotting in natural scenes. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 44(10):7266‚Äì7281,
2021. 3
[52] Peng Wang, Cheng Da, and Cong Yao. Multi-granularity
prediction for scene text recognition. In European Confer-
ence on Computer Vision , pages 339‚Äì355. Springer, 2022.
3
[53] Song Wen, Hao Wang, Di Liu, Qilong Zhangli, and Dimitris
Metaxas. Second-order graph odes for multi-agent trajectory
forecasting. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , pages 5101‚Äì5110,
2024. 3
[54] Liang Wu, Chengquan Zhang, Jiaming Liu, Junyu Han, Jing-
tuo Liu, Errui Ding, and Xiang Bai. Editing text in the wild.
InProceedings of the 27th ACM international conference on
multimedia , pages 1500‚Äì1508, 2019. 2
[55] Ziyi Wu, Jingyu Hu, Wuyue Lu, Igor Gilitschenski, and Ani-
mesh Garg. Slotdiffusion: Object-centric generative model-
ing with diffusion models. Advances in Neural Information
Processing Systems , 36:50932‚Äì50958, 2023. 2
[56] Zhaoyang Xia, Yuxiao Chen, Qilong Zhangli, Matt Huener-
fauth, Carol Neidle, and Dimitris Metaxas. Sign language
video anonymization. In Proceedings of the LREC2022 10th
Workshop on the Representation and Processing of Sign Lan-
guages: Multilingual Sign Language Resources, Marseille,
France, 25 June 2022 , 2022. 2
[57] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 1316‚Äì
1324, 2018. 1
[58] Qiangpeng Yang, Jun Huang, and Wei Lin. Swaptext:
Image based texts transfer in scenes. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 14700‚Äì14709, 2020. 2
[59] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang,
Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph
conditional control for visual text generation. Advances in
Neural Information Processing Systems , 36, 2024. 5, 6, 7, 8
[60] Jiachi Ye, Haoyan Kang, Hao Wang, Chen Shen, Belal Ja-
hannia, Elham Heidari, Navid Asadizanjani, Mohammad-Ali
Miri, V olker J Sorger, and Hamed Dalir. Demultiplexing oambeams via fourier optical convolutional neural network. In
Laser Beam Shaping XXIII , pages 16‚Äì33. SPIE, 2023. 3
[61] Jiachi Ye, Maria Solyanik, Zibo Hu, Hamed Dalir,
Behrouz Movahhed Nouri, and V olker J Sorger. Free-space
optical multiplexed orbital angular momentum beam identifi-
cation system using fourier optical convolutional layer based
on 4f system. In Complex Light and Optical Forces XVII ,
pages 70‚Äì80. SPIE, 2023. 3
[62] Fangneng Zhan, Hongyuan Zhu, and Shijian Lu. Spa-
tial fusion gan for image synthesis. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 3653‚Äì3662, 2019. 2, 3
[63] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836‚Äì3847, 2023. 2, 3, 5, 6, 7, 8
[64] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N
Metaxas, and Jian Ren. Sine: Single image editing with text-
to-image diffusion models. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 6027‚Äì6037, 2023. 2
[65] Qilong Zhangli, Jingru Yi, Di Liu, Xiaoxiao He, Zhaoyang
Xia, Qi Chang, Ligong Han, Yunhe Gao, Song Wen, Haim-
ing Tang, et al. Region proposal rectification towards robust
instance segmentation of biological images. In International
Conference on Medical Image Computing and Computer-
Assisted Intervention , pages 129‚Äì139. Springer, 2022. 3
[66] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang
Zhou, Weiran He, and Jiajun Liang. East: an efficient and
accurate scene text detector. In Proceedings of the IEEE con-
ference on Computer Vision and Pattern Recognition , pages
5551‚Äì5560, 2017. 3
[67] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A
Efros. Unpaired image-to-image translation using cycle-
consistent adversarial networks. In Proceedings of the IEEE
international conference on computer vision , pages 2223‚Äì
2232, 2017. 2
7506
