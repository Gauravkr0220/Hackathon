Neural Directional Encoding
for EfÔ¨Åcient and Accurate View-Dependent Appearance Modeling
Liwen Wu1*Sai Bi2Zexiang Xu2Fujun Luan2Kai Zhang2
Iliyan Georgiev2Kalyan Sunkavalli2Ravi Ramamoorthi1
1UC San Diego2Adobe Research
Abstract
Novel-view synthesis of specular objects like shiny met-
als or glossy paints remains a signiÔ¨Åcant challenge. Not
only the glossy appearance but also global illumination
effects, including reÔ¨Çections of other objects in the envi-
ronment, are critical components to faithfully reproduce a
scene. In this paper, we present Neural Directional En-
coding (NDE), a view-dependent appearance encoding of
neural radiance Ô¨Åelds (NeRF) for rendering specular ob-
jects. NDE transfers the concept of feature-grid-based spa-
tial encoding to the angular domain, signiÔ¨Åcantly improv-
ing the ability to model high-frequency angular signals. In
contrast to previous methods that use encoding functions
with only angular input, we additionally cone-trace spa-
tial features to obtain a spatially varying directional en-
coding, which addresses the challenging interreÔ¨Çection ef-
fects. Extensive experiments on both synthetic and real
datasets show that a NeRF model with NDE (1) outper-
forms the state of the art on view synthesis of specular
objects, and (2) works with small networks to allow fast
(real-time) inference. The source code is available at:
https://github.com/lwwu2/nde
1. Introduction
Some of the most compelling appearances in our visual
world arise from specular objects like metals, plastics,
glossy paints, or silken cloth. Faithfully reproducing these
effects from photographs for novel-view synthesis requires
capturing both geometry and view-dependent appearance.
Recent neural radiance Ô¨Åeld (NeRF) [ 37] methods have
made impressive progress on efÔ¨Åcient geometry represen-
tation and encoding using learnable spatial feature grids
[5,7,29,39,45,53]. However, modeling high-frequency
view-dependent appearance has achieved much less atten-
tion. EfÔ¨Åcient encoding of directional information is just
as important, for modeling effects such as specular high-
lights and glossy interreÔ¨Çections. In this paper, we present
a feature-grid-like neural directional encoding (NDE) that
can accurately model the appearance of shiny objects.
*This work was partially done during an internship at Adobe Research.NDE (ours) Ground truth
ENVIDR [ 26] Ref-NeRF [ 48]NDE (ours) Ground truth
0.52 FPS 0.02 FPS 75 FPS
Figure 1. Ours vs. analytical encoding. Methods like Ref-
NeRF [ 48] use an analytical function to encode viewing directions
in large MLPs, failing to model complex reÔ¨Çections (column 1-2
of the insets). Instead, we encode view-dependent effects into fea-
ture grids with better interreÔ¨Çection parameterization, successfully
reconstructing the details on the teapot and even multi-bounce re-
Ô¨Çections of the pink ball (3rd column of the insets) with little com-
putational overhead (75 FPS on an NVIDIA 3090 GPU).
View-dependent colors in NeRFs ( e.g. [48]) are com-
monly obtained by decoding spatial features and encoded
direction. This approach necessitates a large multi-layer
perceptron (MLP) and exhibits slow convergence with ana-
lytical directional encoding functions. To that end, we bring
feature-grid-based encoding to the directional domain, rep-
resenting reÔ¨Çections from distant sources via learnable fea-
ture vectors stored on a global environment map (Sec. 4.1).
Features localize signal learning, reducing the MLP size re-
quired to model high-frequency far-Ô¨Åeld reÔ¨Çections.
Besides far-Ô¨Åeld reÔ¨Çections, spatially varying near-Ô¨Åeld
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21157
interreÔ¨Çections are also key effects in rendering glossy
objects. These effects cannot be accurately modeled by
NeRF‚Äôs spatio-angular parameterization whose directional
encoding does not depend on the position. In contrast, we
propose a novel spatio-spatial parameterization by cone-
tracing a spatial feature grid (Sec. 4.2) to encode near-Ô¨Åeld
reÔ¨Çections. The cone tracing accumulates spatial encodings
along the queried direction and position, thus it is spatially
varying. While prior works consider only single-bounce
or diffuse interreÔ¨Çections [ 26], our representation is able to
model general multi-bounce reÔ¨Çection effects.
Overall, our neural directional encoding (NDE) achieves
both high-quality modeling of view-dependent effects and
fast evaluation. Figure 1demonstrates NDE incorpo-
rated into NeRF, showing (1) accurate rendering of spec-
ular objects‚Äîa difÔ¨Åcult challenge for the state of the art
(Sec. 5.1), and (2) high inference speed that can be pushed
to real-time without obvious quality loss (Sec. 5.2).
2. Related work
Novel-view synthesis aims to render a 3D scene from un-
seen views given a set of image captures with camera poses.
Neural radiance Ô¨Åelds (NeRF) [ 37] has recently emerged
as a promising solution to this task, utilizing an implicit
scene representation and volume rendering to synthesize
photorealistic images. Follow-up works achieve state-of-
the-art results in this area, for unbounded scenes [ 1,59],
in-the-wild captures [ 34], and sparse- or single-view recon-
struction [ 6,14,28,46,47,51]. While the original NeRF
method [ 37] is computationally inefÔ¨Åcient, it can be visu-
alized in real-time by baking the reconstruction into voxel-
[12,15,43,57] or feature-grid-based representations (dis-
cussed below). The volumetric representation has been ex-
tended to work with signed distance Ô¨Åelds (SDF) [ 50,55]
for better geometry acquisition, and the volume-rendering
concept has also been applied to other 3D-related tasks such
as object generation [ 4,5,27,30,42].
Feature-grid-based NeRF. NeRF‚Äôs positional encod-
ing [ 37] is a key component for the underlying multi-layer
perceptron (MLP) network to learn high-frequency spatial
and directional signals. However, the MLP size needs to
be large, which leads to slow training and inference. In-
stead, methods like NSVF [ 29] and DVGO [ 45] interpo-
late a 3D volume of learnable feature vectors to encode
the spatial signal, showing faster training and inference
with even better spatial detail. Addressing the sparsity
in typical scene geometry, later works avoid maintaining
a large dense 3D grid via volume-compression techniques
such as hash grids [ 39] and tensor factorization [ 5,7,11].
These methods are compact and scale up the feature grid to
large scenes [ 2,39] and even work with SDF-based mod-
els [25,56]. The essence of feature-grid encoding is to in-
terpolate feature vectors attached to geometry primitives,
and similar ideas have also been applied to irregular 3D
grids [ 22,44], point clouds [ 19,20,54,62], and meshes [ 8].
Operations like mip-mapping are trivial on feature grids, en-abling efÔ¨Åcient anti-aliasing and range query of NeRF mod-
els [2,16,53]‚Äîsomething we also leverage in this paper to
encode rough reÔ¨Çection.
Rendering specular objects. Apart from geometry,
view-dependent effects like reÔ¨Çections from rough surfaces
are a crucial component in photorealistic novel-view syn-
thesis. ReÔ¨Çections are conventionally modeled by Ô¨Åtting
local light-Ô¨Åeld functions [ 10,17,36]. A 4D light Ô¨Åeld
presents more degrees of freedom than the constraints from
input images, which necessitates additional regularization
to avoid overÔ¨Åtting. Inverse-rendering approaches intro-
duce such a constraint by solving for parametric BRDFs
and lighting, then using forward rendering to reconstruct
the light Ô¨Åeld. Spherical-basis lighting [ 60] or split-sum ap-
proximation [ 31,40] are usually used to tamper the Monte
Carlo variance of specular-reÔ¨Çection derivatives [ 3]. EN-
VIDR [ 26] and NMF [ 33] further explicitly consider global-
illumination effects by ray-tracing one or few bounces of
indirect lighting. On the other hand, Ref-NeRF [ 48] uses an
integrated directional encoding (IDE) to directly improve
NeRF‚Äôs view-dependent effects. IDE encodes the reÔ¨Çected
direction rather than viewing direction to let the network
learn an environment-map-like function and is pre-Ô¨Åltered
to account for rough reÔ¨Çection effects. Our neural direc-
tional encoding, similar to IDE, can model general view-
dependent appearance without assuming simpliÔ¨Åed lighting
or reÔ¨Çections but with smaller computation cost.
3. Preliminaries
We assume opaque objects with diffuse and specular com-
ponents and demonstrate our directional encoding using a
surface-based model that represents a scene using a signed
distance Ô¨Åeld (SDF) s(x)and a color Ô¨Åeld c(x,œâ)(depen-
dent on the viewing direction œâ). The SDF is converted to
NeRF‚Äôs density Ô¨Åeld œÉfollowing V olSDF [ 55] with a learn-
able parameter Œ≤controlling the boundary smoothness:
œÉ(x) =Ô£±
Ô£≤
Ô£≥1
2Œ≤exp/parenleftBig
s(x)
Œ≤/parenrightBig
ifs(x)‚â§0,
1
Œ≤/parenleftBig
1‚àí1
2exp/parenleftBig
‚àís(x)
Œ≤/parenrightBig/parenrightBig
otherwise .(1)
The color C(x,œâ)of a ray with origin xand direction œâ
can thus be volume-rendered [ 35]:
C(x,œâ)=/summationdisplay
iw(œÉ(xi))c(xi,œâ),where (2)
w(œÉ(xi)) =/parenleftBig
1‚àíe‚àíœÉ(xi)Œ¥i/parenrightBig/productdisplay
j<ie‚àíœÉ(xj)Œ¥j, (3)
withŒ¥i=‚à•xi‚àíxi‚àí1‚à•2andxidenoting the ithsample point
along the ray. Like Ref-NeRF [ 48], we decompose the color
cinto a diffuse color cd, specular tint ks, and specular color
csqueried in reÔ¨Çected direction œârwith surface normal n
given by the SDF gradient:
c(x,œâ) =cd(x)+ks(x)cs(x,œâr),where
œâr=reÔ¨Çect(œâ,n),n=normalize (‚àáxs(x)).(4)
21158
!
N'6-21%E.-'D7.5321%'3D5E.3F
>6?'@20 %155O60
*.0/@200'E%3'2- /B.'1E%B'276-'4%P;516@'Q
!
/RK /R: /RN
>53'%7-2D.3FS1'3E.3F
T'D5E'
T'D5E'T'D5E'
/ 0 123(+(+-'/B.17'-'E%B2- /B.'1E%B'276-'4%PD6?'@20 Q%
!RK !RKU: !R:!
Figure 2. Pipeline of our neural directional encoding (NDE). We encode far-Ô¨Åeld reÔ¨Çections into a cubemap and near-Ô¨Åeld interreÔ¨Çec-
tions into a volume. Both representations store learnable feature vectors to encode direction and are mip-mapped to account for rough
reÔ¨Çections. Given a reÔ¨Çected ray, the features are combined by tracing a cone of size proportional to the surface roughness to aggregate
spatial features with cubemap features blended as the background. The result is fed into an MLP to output the specular color (Eq. ( 5)).
Here, the specular color csis decoded from an MLP that
conditions on spatial feature f(x), directional encoding H
controlled by surface roughness œÅ, and the cosine term n¬∑œâ:
cs(x,œâr) =MLP(f(x),H(x,œâr,œÅ(x)),n¬∑œâ).(5)
cd,ks,f,œÅcome from a spatial MLP (Sec. 4.3).
Discussion on directional encoding. Previous works [ 37,
48] use an analytical function for Hdependent only on œâr
(and optionally œÅ), which has several limitations: (1) the en-
coding function is Ô¨Åxed (not learnable), and (2) the spatial
context only comes from f(x). Both require the decoder
MLP to be large to Ô¨Åt the spatio-angular details of the spec-
ular color, which can be expensive and slow.
4. Neural directional encoding
To minimize the MLP complexity, we use a learnable neu-
ral directional encoding that also depends on the spatial
location. SpeciÔ¨Åcally, our NDE encodes different types
of reÔ¨Çection by different representations, which include a
cubemap feature grid hffor far-Ô¨Åeld reÔ¨Çections and a spa-
tial volume hnthat models near-Ô¨Åeld interreÔ¨Çections. As
shown in Fig. 2, we compute Hby Ô¨Årst cone-tracing hn
accumulated along the reÔ¨Çected ray, yielding near-Ô¨Åeld fea-
tureHn(Sec. 4.2), and blending the far-Ô¨Åeld feature Hf
queried from hfin the same direction (Sec. 4.1):
H(x,œâr,œÅ) =Hn(x,œâr,œÅ)+(1‚àíŒ±n)Hf(œâr,œÅ),(6)
whereŒ±nis the cone-traced opacity [ 24], and both features
are mip-mapped with œÅdeciding the mip level.4.1. Far¬≠Ô¨Åeld features
Feature-grid-based representations [ 7,29,39,45,53] speed-
up spatial signal learning by storing feature vectors in vox-
els for local signal control. Similarly, we place feature vec-
torshfat every pixel of a global cubemap to encode ideal
specular reÔ¨Çections. The cubemap is pre-Ô¨Åltered to model
reÔ¨Çections under rough surfaces in the split-sum [ 18] style,
where the kthlevel mip-map hk
fis created by convolving the
downsampled hfusing a GGX kernel [ 49]Dwith canoni-
cal roughness œÅkevenly spaced in [0,1]:
hk
f=convolution (downsample (hf,k),D(œÅk)). (7)
Given the surface roughness, we perform a cubemap lookup
in the reÔ¨Çected direction and interpolate between mip levels
to get the far-Ô¨Åeld feature:
Hf(œâr,œÅ) =lerp/parenleftbigg
hk
f(œâr),hk+1
f(œâr),œÅ‚àíœÅk
œÅk+1‚àíœÅk/parenrightbigg
,(8)
where lerp (¬∑)denotes linear interpolation and œÅ‚àà[œÅk,œÅk+1].
The cubemap-based encoding allows signals in different
directions to be optimized independently by tuning the fea-
ture vectors. This is easier to optimize than globally solv-
ing the MLP parameters, making it more suitable to model
high-frequency details in the angular domain (Fig. 3). The
coarse level feature is a consistently Ô¨Åltered version of the
Ô¨Åne level, which is empirically found to be better con-
strained than using independent feature vectors at each mip
level [ 23,58].
4.2. Near¬≠Ô¨Åeld features
Parameterizing the specular color by a spatial and angular
feature is sufÔ¨Åcient for distant reÔ¨Çections, but lacks expres-
sivity for near-Ô¨Åeld interreÔ¨Çections: different points query
21159
IDE small IDE large Hfsmall (ours) Ground truth
Figure 3. Our cubemap-based feature encoding requires only a
small MLP (2 layers, 64 width) to model details in mirror reÔ¨Çec-
tions (3rd image) comparable with IDE [ 48] (2nd image; 8 layers,
256 width MLP) that fails when the MLP is small (1st image).
ùê±ùùé!
ùê±ùê±!
ùê±
Spatio-angular Spatio-spatial Cone-traced
Figure 4. Spatio-spatial encoding (middle) is equivalent to the
common spatio-angular encoding (left) of mirror reÔ¨Çections, but
it captures the variation of x‚Ä≤across different x. The idea can be
extended to model rough reÔ¨Çections by cone tracing mip-mapped
spatial features covered by the reÔ¨Çection cone (right).
the same hf, so spatially varying components can end up
being averaged out during optimization. Our insight is that
the spatio-angular reÔ¨Çection can also be parameterized as a
spatio-spatial function of current and next bounce location
(Fig. 4). Therefore, an MLP can decode the second bounce
spatial feature with f(x)in Eq. ( 5) to get mirror reÔ¨Çections.
For rough reÔ¨Çections, we aggregate the averaged second
bounce feature under the reÔ¨Çection lobe by cone tracing [ 9]
(Fig. 4, right), which volume renders the mip-mapped spa-
tial features hnusing the mip-mapped density œÉnalong
the reÔ¨Çected ray x+œârtwith mip level Œªi= log2(2ri)
at sample point x‚Ä≤
idecided by the cone‚Äôs footprint ri=‚àö
3œÅ2‚à•x‚àíx‚Ä≤
i‚à•2:
Hn(x,œâr,œÅ) =/summationdisplay
iwi
nhi
n,where
wi
n=w(œÉn(x‚Ä≤
i,Œªi)),hi
n=hn(x‚Ä≤
i,Œªi).(9)
The cone‚Äôs footprint is selected to cover the GGX lobe at x
(see supplemental document). Note that we do not use the
SDF-converted œÉin Eq. ( 1) as it cannot be mip-mapped;
instead, we optimize a separate œÉnto match œÉ(Sec. 4.3)
jointly with the indirect feature hn. Both are decoded from
a tri-plane [ 5]Tn, whose each 2D plane is mip-mapped
similar to Tri-MipRF [ 16]:
œÉn(x‚Ä≤
i,Œªi),hn(x‚Ä≤
i,Œªi)=MLP(mipmap(Tn(x‚Ä≤
i),Œªi)).(10)
The indirect rays are spatially varying, hence the cone-
traced near-Ô¨Åeld features are spatially varying too. This has
advantages over the angular-only feature for learning inter-
reÔ¨Çections and is empirically less likely to overÔ¨Åt (Fig. 5).
This is because the same hnis traced from different rays
in training, such that the underlying representation is well-
constrained. HnandHfare similar to the foreground
Ours without Hn Ours with Hn Ground truth
Figure 5. Our cone-traced near-Ô¨Åeld features successfully re-
construct the reÔ¨Çected spheres (2nd column) under novel views,
which are overÔ¨Åtted by the angular-only encoding (1st column).
Hash grid
Positional
Encoding
MLP
Cubemap
 MLP
Tri-plane
MLPResolution:
Levels:
Features:
Hash table:64   1
ReLU[16, 2048] 
16
2
524288
64   2
ReLU
256   8
Softplus
MLP
64   2
ReLU512 
9
16
Resolution:
Mip levels:
Features:
Resolution:
Mip levels:
Features:64 
6
16
Figure 6. Network architectures. N√óMdenotes an M-layer
MLP of width N.
and background colors in regular volume rendering, so Hf
can be naturally composited with Hnusing the opacity
Œ±n=1‚àí/producttext
ie‚àíœÉn(x‚Ä≤
i,Œªi)Œ¥i=/summationtext
iwi
nas in Eq. ( 6).
4.3. Optimization
Figure 6shows our network architectures. Stable geometry
optimization is essential for modeling specular objects, so
we use the positional-encoded MLP from V olSDF [ 55] to
output the SDF. To reduce computation cost, a hash grid
is used to encode other spatial features ( cd,ks,œÅ,f), and
all other MLPs are tiny. The representation is optimized
through the Charbonnier loss [ 1] between ground truth pixel
colorCgtand our rendering Cin tone-mapped space:
L=/summationdisplay
x,œâ/radicalBig
‚à•Œì(C(x,œâ))‚àíCgt(x,œâ)‚à•2
2+0.001,(11)
whereŒìis the tone-mapping function [ 40].
Occupancy-grid sampling. Eqs. ( 3) and ( 9) are acceler-
ated by an occupancy-grid estimator [ 24] to get rid of com-
putations in empty space. This is especially important for
the efÔ¨Åcient near-Ô¨Åeld feature evaluation, since we trace a
reÔ¨Çected ray for each primary ray sample. The primal ray
rendering uses a Ô¨Åxed ray marching step of 0.005. Follow-
ing [ 9], we choose the cone tracing step proportional to its
footprint: max(0.5ri,0.005) , and query a mip-mapped oc-
cupancy grid for the correct occupancy information.
Regularization. Given the primary samples xi, Eikonal
loss [ 55]Leikis applied to regularize the SDF, and we im-
plicitly regularize œÉnto matchœÉby encouraging the render-
ing using œÉnat mip level 0 to be close to the ground truth:
21160
LœÉ=/summationdisplay
x,œâ‚à•CœÉ(x,œâ)‚àíCgt(x,œâ)‚à•2
2,where
CœÉ(x,œâ) =/summationdisplay
iw(œÉn(xi,0))Àöc(xi,œâ),(12)
Àö‚ñ°denotes stop-gradient to prevent œÉnaffecting appearance.
The total loss is L+0.1Leik+0.01LœÉ.
Implementation details. We implement our code using
PyTorch [ 41], NerfAcc [ 24], and CUDA. The optimiza-
tion takes 400k steps using the Adam optimizer [ 21] with
0.0005 learning rate and dynamic batch size [ 39] target-
ing for 32k primary point samples. We use the scheduler
from BakedSDF [ 15] to anneal Œ≤in Eq. ( 1) for more stable
convergence. Because the SDF uses a positional-encoded
MLP, each scene still requires 10 ‚àº18 hours to train on an
NVIDIA 3090 GPU with 15GB GPU memory usage.
5. Experiments
We evaluate our method on view synthesis of specular ob-
jects using synthetic and real scenes. The synthetic scenes
include the Shinny Blender dataset [ 48] and the Materials
scene from the NeRF Synthetic dataset [ 37], all rendered
without background; the real scenes come from NeRO [ 31]
which contain backgrounds and reÔ¨Çections of the capturer
in the images. The rendering quality is compared in terms
of PSNR, SSIM [ 52], LPIPS [ 61], and the inference speed
in FPS is recorded on an NVIDIA 3090 GPU.
Background and capturer. For real scenes, we use a sep-
arate Instant-NGP [ 39] with coordinate contraction [ 1] to
render backgrounds. Similarly to NeRO [ 31], the reÔ¨Çection
of the capturer is encoded by blending a capturer plane fea-
turehcof opacity Œ±cbetweenHfandHn:
H=Hn+(1‚àíŒ±n)(Œ±chc+(1‚àíŒ±c)Hf),where
Œ±c,hc=MLP(mipmap(Tc(u),Œªc))(13)
are decoded from a mip-mapped 2D feature grid Tc;u,Œªc
are the ray-plane intersection coordinate and the mip-level
derived from the intersection footprint. Jointly optimiz-
ing foreground and background networks can be unstable,
so we apply stabilization loss from NeRO [ 31] and mod-
ify the specular color computation for the Ô¨Årst 200k steps:
hf,hn,hcare sampled and decoded into colors Ô¨Årst, then
the colors are blended to get cs. Compared to blending the
feature and decoding, we Ô¨Ånd the decoding-then-blending
strategy provides better geometry optimization.
5.1. View synthesis
We compare against NeRO [ 31], ENVIDR [ 26], and Ref-
NeRF [ 48] on synthetic scenes. All methods except for Ref-
NeRF use SDFs, and we evaluate NeRO after the BRDF
estimation as it shows better performance. Ideally, both
backgrounds and reÔ¨Çections from the capturer should beMethod Mat. Teapot Toaster Car Ball Coffee Helmet Mean
PSNR‚Üë
NeRO 24.85 40.29 27.31 26.98 31.50 33.76 29.59 30.61
ENVIDR 29.51 46.14 26.63 29.88 41.03 34.45 36.98 34.95
Ref-NeRF 35.41 47.90 25.70 30.82 47.46 34.21 29.68 35.88
NDE (ours) 31.53 49.12 30.32 30.39 44.66 36.57 37.77 37.19
SSIM‚Üë
NeRO 0.878 0.993 0.891 0.926 0.953 0.960 0.953 0.936
ENVIDR 0.971 0.999 0.955 0.972 0.997 0.984 0.993 0.982
Ref-NeRF 0.983 0.998 0.922 0.955 0.995 0.974 0.958 0.969
NDE (ours) 0.972 0.999 0.968 0.968 0.995 0.979 0.990 0.982
LPIPS‚Üì
NeRO 0.138 0.017 0.162 0.064 0.179 0.099 0.102 0.109
ENVIDR 0.026 0.003 0.097 0.031 0.020 0.044 0.022 0.035
Ref-NeRF 0.022 0.004 0.095 0.041 0.059 0.078 0.075 0.053
NDE (ours) 0.017 0.002 0.039 0.024 0.022 0.033 0.014 0.022
Table 1. Quantitative comparison on synthetic scenes showing
our encoding (NDE) is either the best orsecond best compared to
other methods for view synthesis of specular objects.
removed when evaluating renderings of specular objects,
which is difÔ¨Åcult for the real scenes. Therefore, we only
qualitatively compare real scenes against NeRO with PSNR
computed on the foreground zoom-ins without the capturer.
Results. Overall, our method gives the best rendering
quality on synthetic scenes with quantitative results either
better or comparable with the baselines (Tab. 1). This
is because our NDE gives the most detailed modeling of
both far-Ô¨Åeld reÔ¨Çections and interreÔ¨Çections, which also
helps improve the geometry reconstruction (Fig. 7bottom).
While ENVIDR‚Äôs SSIM is slightly better than ours in sev-
eral scenes, we not only achieve much better PSNRs (sur-
passing 2dB), but also higher LPIPS scores. The PSNR on
the Materials (Mat.) scene is worse than Ref-NeRF‚Äôs be-
cause the SDF is inefÔ¨Åcient at modeling the concave geom-
etry of the sphere base. However, our directional MLP is
much smaller (Sec. 5.2), and we still achieve perceptually
better appearance as shown in the insets of Fig. 7. The qual-
itative comparison in Fig. 8shows that NDE extends well
to real scenes, producing clearer specular reÔ¨Çections of the
complex real-world environments compared to NeRO.
Editability. The near- and far-Ô¨Åeld features provide a nat-
ural separation of different reÔ¨Çections, allowing us to ren-
der these effects separately by excluding HforHnduring
inference (Fig. 9). Because interreÔ¨Çections are spatially en-
coded in the near-Ô¨Åeld feature grid, an object and its Ô¨Årst-
bounce reÔ¨Çections can be removed by masking out both
œÉandœÉnfrom the corresponding regions (Fig. 10). This
does not work for multi-bounce reÔ¨Çections which are not
encoded on the deleted object.
5.2. Performance comparison
We compare the evaluation frames per second (FPS) on
an800√ó800resolution of the color network and its MLP
size (#Params.) with all baselines in Sec. 5.1on synthetic
scenes. The color MLPs include the decoder of œÉn,hn,cs
for our model (Fig. 6), lighting MLPs for NeRO [ 31] and
21161
ENVIDR [ 26] Ref-NeRF [ 48] NDE (ours) Ground truth ENVIDR Ref-NeRF NDE (ours) GT
Rendering
 Rendering/Normal
Figure 7. Qualitative results for synthetic scenes show our NDE successfully models the Ô¨Åne details of reÔ¨Çections from both environment
lights (mirror sphere and car top) and other objects (glossy interreÔ¨Çections on spheres; zoom in to see the difference). Ref-NeRF tends to use
wrong geometry to fake interreÔ¨Çections (2nd column on bottom). In contrast, our encoding has sufÔ¨Åcient capacity to model interreÔ¨Çections,
which enables more accurate normals (3rd column on bottom). Mean angular error of the normal is shown in the insets.
NeRO [ 31] NDE (ours) Ground truth
Figure 8. Qualitative comparison on real scenes. Our NDE gives better reconstruction of the interreÔ¨Çections (the bear‚Äôs plate and bottom
of the vase) and detailed highlights from the environment. Numbers in the insets are image PSNR values.
21162
Far-Ô¨Åeld reÔ¨Çections Near-Ô¨Åeld reÔ¨Çections Combined
Far-field
Near-field
Figure 9. ReÔ¨Çection separation. We can visualize different reÔ¨Çec-
tion effects by feeding corresponding features into the network.
Figure 10. Editability of our encoding. ReÔ¨Çections from the
deleted spheres can be removed by deleting the volume of their
indirect features (bottom).
ENVIDR [ 26], and the directional MLP for Ref-NeRF [ 48].
The spatial-network evaluation is excluded to eliminate the
difference caused by different geometry representations,
network architectures, and sampling strategies. For each
method, we choose the rendering batch size that maximizes
its performance.
Results. As shown in the top half of Tab. 2, our NDE takes
a fraction of a second to evaluate, because it requires sub-
stantially smaller MLPs to infer color without hurting the
rendering. In contrast, other baselines need large MLPs to
maintain rendering quality, which prevents them to be visu-
alized in real-time.
Real-time application. It is possible to create a real-time
version of our model by converting the SDF into a mesh
through marching cubes [ 32] and baking cd,ks,œÅ,finto
mesh vertices. The pixel color then can be computed us-
ing the rasterized vertex attributes and csdecoded from the
NDE, which takes only a single cubemap lookup and cone
tracing for each pixel. As a result, this process requires
about the same budget as evaluating a real-time NeRF
model [ 39,45,53]. We implement our real-time model
(NDE-RT) in WebGL and report the full rendering frame
rate (not just color evaluation) at the bottom of Tab. 2with
a real-time baseline 3DGS [ 19]. 3DGS is faster as it uses
spherical harmonics for color without network evaluation,
Ground truth Our ofÔ¨Çine model Our real-time model
Figure 11. Error near object boundaries in our real-time model
is caused by the marching-cube extraction of a triangle mesh and
its subsequent rasterization (squared error maps at the bottom).
This error does not lead to signiÔ¨Åcant qualitative differences (top).
Method FPS ‚Üë#Params‚ÜìPSNR‚ÜëSSIM‚ÜëLPIPS‚Üì
NeRO 0.11 454k 30.61 0.936 0.109
ENVIDR 0.55 206k 34.95 0.982 0.035
Ref-NeRF 0.08 521k 35.88 0.969 0.053
NDE (ours) 3.03 75k 37.19 0.982 0.022
3DGS 235 - 30.30 0.949 0.076
NDE-RT (ours) 66 75k 35.48 0.976 0.027
Table 2. Performance comparison. Our NDE achieves high ren-
dering quality, and its use of small MLPs enables fast color evalu-
ation and real-time rendering. We report only the evaluation time
and parameter counts of color MLPs except for 3DGS (no color
MLPs) and our NDE-RT, for which we report the total rendering
time. All metrics are averaged over the synthetic scenes in Tab. 1.
which leads to poor specular appearance reconstruction. In-
stead, our NDE-RT shows rendering quality comparable to
other baselines while achieving frame rates above 60. The
loss in PSNR is mainly due to error around object edges
which is cause by the marching-cube mesh extraction and
subsequent rasterization (Fig. 11). This error does not sig-
niÔ¨Åcantly affect the visual quality and can be resolved by
Ô¨Åne-tuning the mesh [ 8,40].
5.3. Ablation study
Different directional encodings. In Fig. 12we com-
pare different directional encodings on the Materials scene.
IDE [ 48] (analytical) with our tiny MLP yields blurry re-
Ô¨Çections. InterreÔ¨Çections cannot be reconstructed using
only the far-Ô¨Åeld feature, and if we volume-render rather
than cone-trace the near-Ô¨Åeld feature, mirror interreÔ¨Çections
can be recovered but reÔ¨Çections on rough surfaces look too
sharp. It is therefore necessary to use both the cubemap-
based far-Ô¨Åeld feature and the cone-traced near-Ô¨Åeld feature
to get the best specular appearance (Tab. 3).
Network architecture. Table 4shows the performance
trade-off between different network architectures of our
model on synthetic scenes. Using a smaller MLP width for
21163
Ground truth
Analytical Hf Cubemap Hf
V olume-rendered Hn Cone-traced Hn
Figure 12. Qualitative ablation of NDE components. Details
from the environment light fail to be reconstructed with an ana-
lytical encoding (mirror sphere on 2nd row). It is also necessary
to use the cone-traced near-Ô¨Åeld feature, otherwise rough surfaces
are rendered incorrectly (grey sphere on 3rd row).
Far-Ô¨Åeld feature Near-Ô¨Åeld feature PSNR ‚ÜëSSIM‚ÜëLPIPS‚Üì
Analytical - 28.54 0.944 0.029
Cubemap - 30.27 0.962 0.022
Cubemap V olume-rendered 29.31 0.951 0.034
Cubemap Cone-traced 31.53 0.972 0.017
Table 3. Ablation on directional encodings shows each compo-
nent of NDE is needed for the best rendering quality. The compar-
ison is made on the Materials scene.
Model MLP width PSNR ‚ÜëSSIM‚ÜëLPIPS‚ÜìFPS‚Üë
Our ofÔ¨Çine64 37.19 0.982 0.022 <1
32 36.69 0.979 0.026 <1
16 36.23 0.977 0.028 <1
Our real-time64 35.48 0.976 0.027 66
32 33.97 0.971 0.034 211
16 33.71 0.969 0.036 331
Table 4. Ablation on our network architecture. Using a smaller
MLP width introduces a minor loss in rendering Ô¨Ådelity but a no-
ticeable real-time performance boost.
the decoder of œÉn,hn,cshas only a slight negative impact
on the rendering quality but signiÔ¨Åcantly improves real-time
performance. The rendering quality reduction of the real-
time model is mainly caused by the error near object edges
as discussed in Sec. 5.2.
Spatial mip-mapping strategies. Besides mip-mapped
tri-plane [ 5,16], our architecture can also work with a mip-
mapped hash grid [ 39] for the near-Ô¨Åeld feature encoding.
Similar to [ 2,25], the hash-grid mip-mapping is imple-
mented by gradually masking out Ô¨Åne-resolution features as
the mip level increases. This results in limited model capac-
ity for rough surfaces where most of the features are maskedMat. Teapot Toaster Car Ball Coffee Helmet Mean
PSNR‚Üë
Hash grid 30.89 49.00 29.46 30.16 43.48 34.98 37.67 36.52
Tri-plane 31.53 49.12 30.32 30.39 44.66 36.57 37.77 37.19
SSIM‚Üë
Hash grid 0.968 0.999 0.953 0.967 0.990 0.974 0.990 0.977
Tri-plane 0.972 0.999 0.968 0.968 0.995 0.979 0.990 0.982
LPIPS‚Üì
Hash grid 0.019 0.002 0.058 0.025 0.031 0.043 0.014 0.027
Tri-plane 0.017 0.002 0.039 0.024 0.022 0.033 0.014 0.022
Table 5. Ablation on mip-mapping strategies suggests that the
mip-mapped tri-plane represents averaged near-Ô¨Åeld features and
density better than the mip-mapped hash grid.
ENVIDR [ 26]NDE (hash grid) NDE (MLP) Ground truth
Figure 13. Unstable geometry optimization of specular objects
prevents us from encoding the SDF using a hash grid [ 39] as it
gives incorrect surface normals (middle left). This is also the case
for other hash-grid-based methods (left).
out, such that a mip-mapped hash grid produces slightly
worse rendering than the tri-plane encoding (Tab. 5).
Limitations. Like previous works [ 26,31,48], NDE is
sensitive to the quality of the surface normal. This prevents
us from using more efÔ¨Åcient geometry representations such
as a hash grid, which tends to produce corrupted geometry
(Fig. 13). As a result, we use positional-encoded MLPs to
model the SDF, which leads to long training times and is
difÔ¨Åcult for modeling transparent objects. Meanwhile, the
editibility of our method is limited.
6. Conclusion
We have adapted feature-based NeRF encodings to the di-
rectional domain and introduced a novel spatio-spatial pa-
rameterization of view-dependent appearance. These im-
provements allow for efÔ¨Åcient modeling of complex re-
Ô¨Çections for novel-view synthesis and could beneÔ¨Åt other
applications that model spatially varying directional sig-
nals, such as neural materials [ 13,23,58] and radiance
caching [ 38].
Acknowledgements. This work was supported in part
by NSF grants 2110409, 2100237, 2120019, ONR grant
N00014-23-1-2526, gifts from Adobe, Google, Qualcomm,
Rembrand, a Sony Research Award, as well as the Ronald
L. Graham Chair and the UC San Diego Center for Visual
Computing. Additionally, we thank Jingshen Zhu for in-
sightful discussions.
21164
References
[1] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded
anti-aliased neural radiance Ô¨Åelds. In CVPR , 2022. 2,4,5
[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.
Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-
based neural radiance Ô¨Åelds. In ICCV , 2023. 2,8
[3] Yash Belhe, Bing Xu, Sai Praveen Bangaru, Ravi Ra-
mamoorthi, and Tzu-Mao Li. Importance sampling brdf
derivatives. In ACM TOG , 2024. 2
[4] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,
and Gordon Wetzstein. Pi-gan: Periodic implicit genera-
tive adversarial networks for 3d-aware image synthesis. In
CVPR , 2021. 2
[5] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,
Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J
Guibas, Jonathan Tremblay, Sameh Khamis, et al. EfÔ¨Å-
cient geometry-aware 3d generative adversarial networks. In
CVPR , 2022. 1,2,4,8
[6] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,
Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-
izable radiance Ô¨Åeld reconstruction from multi-view stereo.
InICCV , 2021. 2
[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
Hao Su. Tensorf: Tensorial radiance Ô¨Åelds. In ECCV , 2022.
1,2,3
[8] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-
drea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-
terization pipeline for efÔ¨Åcient neural Ô¨Åeld rendering on mo-
bile architectures. In CVPR , 2023. 2,7
[9] Cyril Crassin, Fabrice Neyret, Miguel Sainz, Simon Green,
and Elmar Eisemann. Interactive indirect illumination using
voxel cone tracing. In Computer Graphics Forum , 2011. 4
[10] John Flynn, Michael Broxton, Paul Debevec, Matthew Du-
Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and
Richard Tucker. Deepview: View synthesis with learned gra-
dient descent. In CVPR , 2019. 2
[11] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb√¶k
Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:
Explicit radiance Ô¨Åelds in space, time, and appearance. In
CVPR , 2023. 2
[12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie
Shotton, and Julien Valentin. Fastnerf: High-Ô¨Ådelity neural
rendering at 200fps. In ICCV , 2021. 2
[13] Alban Gauthier, Robin Faury, J ¬¥er¬¥emy Levallois, Th ¬¥eo
Thonat, Jean-Marc Thiery, and Tamy Boubekeur. Mipnet:
Neural normal-to-anisotropic-roughness mip mapping. In
ACM TOG , 2022. 8
[14] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,
Christian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.
Nerfdiff: Single-image view synthesis with nerf-guided dis-
tillation from 3d-aware diffusion. In ICML , 2023. 2
[15] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall,
Jonathan T Barron, and Paul Debevec. Baking neural ra-
diance Ô¨Åelds for real-time view synthesis. In ICCV , 2021. 2,
5
[16] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,
Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation
for efÔ¨Åcient anti-aliasing neural radiance Ô¨Åelds. In ICCV ,
2023. 2,4,8[17] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-
mamoorthi. Learning-based view synthesis for light Ô¨Åeld
cameras. In ACM TOG , 2016. 2
[18] Brian Karis. Real shading in unreal engine 4. In SIGGRAPH
2013 Course: Physically Based Shading in Theory and Prac-
tice, 2013. 3
[19] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¬®uhler,
and George Drettakis. 3d gaussian splatting for real-time
radiance Ô¨Åeld rendering. In ACM TOG , 2023. 2,7
[20] Leonid Keselman and Martial Hebert. Flexible techniques
for differentiable rendering with 3d gaussians. arXiv preprint
arXiv:2308.14737 , 2023. 2
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 ,
2014. 5
[22] Jonas Kulhanek and Torsten Sattler. Tetra-nerf: Represent-
ing neural radiance Ô¨Åelds using tetrahedra. In ICCV , 2023.
2
[23] Alexandr Kuznetsov, Krishna Mullia, Zexiang Xu, Milo Àás
HaÀásan, and Ravi Ramamoorthi. Neumip: Multi-resolution
neural materials. In ACM TOG , 2021. 3,8
[24] Ruilong Li, Matthew Tancik, and Angjoo Kanazawa. Ner-
facc: A general nerf acceleration toolbox. arXiv preprint
arXiv:2210.04847 , 2022. 3,4,5
[25] Zhaoshuo Li, Thomas M ¬®uller, Alex Evans, Russell H Tay-
lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.
Neuralangelo: High-Ô¨Ådelity neural surface reconstruction. In
CVPR , 2023. 2,8
[26] Ruofan Liang, Hui-Hsia Chen, Chunlin Li, Fan Chen, Sel-
vakumar Panneer, and Nandita Vijaykumar. Envidr: Implicit
differentiable renderer with neural environment lighting. In
ICCV , 2023. 1,2,5,6,7,8
[27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,
Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,
Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution
text-to-3d content creation. In CVPR , 2023. 2
[28] Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin,
Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer
for nerf-based view synthesis from a single input image. In
WACV , 2023. 2
[29] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and
Christian Theobalt. Neural sparse voxel Ô¨Åelds. In NeurIPS ,
2020. 1,2,3
[30] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang
Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh
in 45 seconds without per-shape optimization. In NeurIPS ,
2023. 2
[31] Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng
Wang, Lingjie Liu, Taku Komura, and Wenping Wang. Nero:
Neural geometry and brdf reconstruction of reÔ¨Çective objects
from multiview images. In ACM TOG , 2023. 2,5,6,8
[32] William E Lorensen and Harvey E Cline. Marching cubes:
A high resolution 3d surface construction algorithm. In SIG-
GRAPH , 1987. 7
[33] Alexander Mai, Dor Verbin, Falko Kuester, and Sara
Fridovich-Keil. Neural microfacet Ô¨Åelds for inverse render-
ing. In ICCV , 2023. 2
[34] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,
Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance Ô¨Åelds for uncon-
strained photo collections. In CVPR , 2021. 2
21165
[35] Nelson Max. Optical models for direct volume rendering.
IEEE Transactions on Visualization and Computer Graphics ,
1(2):99‚Äì108, 1995. 2
[36] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light Ô¨Åeld fusion: Practical view syn-
thesis with prescriptive sampling guidelines. In ACM TOG ,
2019. 2
[37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance Ô¨Åelds for view syn-
thesis. In ECCV , 2020. 1,2,3,5
[38] Thomas M ¬®uller, Fabrice Rousselle, Jan Nov‚Äôak, and Alexan-
der Keller. Real-time neural radiance caching for path trac-
ing. In ACM TOG , 2021. 8
[39] Thomas M ¬®uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. In SIGGRAPH , 2022. 1,2,3,5,7,
8
[40] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,
Wenzheng Chen, Alex Evans, Thomas M ¬®uller, and Sanja Fi-
dler. Extracting triangular 3d models, materials, and lighting
from images. In CVPR , 2022. 2,4,7
[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An
imperative style, high-performance deep learning library. In
NeurIPS , 2019. 5
[42] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,
2023. 2
[43] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas
Geiger. Kilonerf: Speeding up neural radiance Ô¨Åelds with
thousands of tiny mlps. In ICCV , 2021. 2
[44] Radu Alexandru Rosu and Sven Behnke. Permutosdf: Fast
multi-view reconstruction with implicit surfaces using per-
mutohedral lattices. In CVPR , 2023. 2
[45] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance Ô¨Åelds
reconstruction. In CVPR , 2022. 1,2,3,7
[46] Alex Trevithick and Bo Yang. Grf: Learning a general ra-
diance Ô¨Åeld for 3d representation and rendering. In ICCV ,
2021. 2
[47] Alex Trevithick, Matthew Chan, Michael Stengel, Eric Chan,
Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chan-
draker, Ravi Ramamoorthi, and Koki Nagano. Real-time
radiance Ô¨Åelds for single-image portrait view synthesis. In
ACM TOG , 2023. 2
[48] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-
tured view-dependent appearance for neural radiance Ô¨Åelds.
InCVPR , 2022. 1,2,3,4,5,6,7,8
[49] Bruce Walter, Stephen R Marschner, Hongsong Li, and Ken-
neth E Torrance. Microfacet models for refraction through
rough surfaces. In EGSR , 2007. 3
[50] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
InNeurIPS , 2021. 2
[51] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P
Srinivasan, Howard Zhou, Jonathan T Barron, RicardoMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-
net: Learning multi-view image-based rendering. In CVPR ,
2021. 2
[52] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-
moncelli. Image quality assessment: from error visibility to
structural similarity. In IEEE transactions on image process-
ing, 2004. 5
[53] Liwen Wu, Jae Yong Lee, Anand Bhattad, Yu-Xiong Wang,
and David Forsyth. Diver: Real-time and accurate neural ra-
diance Ô¨Åelds with deterministic integration for volume ren-
dering. In CVPR , 2022. 1,2,3,7
[54] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,
Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-
based neural radiance Ô¨Åelds. In CVPR , 2022. 2
[55] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. V ol-
ume rendering of neural implicit surfaces. In NeuRIPS , 2021.
2,4
[56] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,
Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,
and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-
time view synthesis. In SIGGRAPH , 2023. 2
[57] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and
Angjoo Kanazawa. Plenoctrees for real-time rendering of
neural radiance Ô¨Åelds. In ICCV , 2021. 2
[58] Tizian Zeltner, Fabrice Rousselle, Andrea Weidlich, Petrik
Clarberg, Jan Nov ¬¥ak, Benedikt Bitterli, Alex Evans,
Tom¬¥aÀás Davidovi Àác, Simon Kallweit, and Aaron Lefohn.
Real-time neural appearance models. arXiv preprint
arXiv:2305.02678 , 2023. 3,8
[59] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen
Koltun. Nerf++: Analyzing and improving neural radiance
Ô¨Åelds. arXiv preprint arXiv:2010.07492 , 2020. 2
[60] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and
Noah Snavely. Physg: Inverse rendering with spherical gaus-
sians for physics-based material editing and relighting. In
CVPR , 2021. 2
[61] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 5
[62] Xiaoshuai Zhang, Abhijit Kundu, Thomas Funkhouser,
Leonidas Guibas, Hao Su, and Kyle Genova. NerÔ¨Çets: Local
radiance Ô¨Åelds for efÔ¨Åcient structure-aware 3d scene repre-
sentation from 2d supervision. In CVPR , 2023. 2
21166
