Split to Merge: Unifying Separated Modalities for
Unsupervised Domain Adaptation
Xinyao Li1Yuke Li2*Zhekai Du1Fengling Li3Ke Lu1Jingjing Li1*
1University of Electronic Science and Technology of China
2Boston College3University of Technology Sydney
xinyao326@outlook.com, lidwh@bc.edu, zhekaid@std.uestc.edu.cn
fenglingli2023@gmail.com, kel@uestc.edu.cn, lijin117@yeah.net
Abstract
Large vision-language models (VLMs) like CLIP have
demonstrated good zero-shot learning performance in the
unsupervised domain adaptation task. Yet, most transfer
approaches for VLMs focus on either the language or visual
branches, overlooking the nuanced interplay between both
modalities. In this work, we introduce a Unified Modal-
ity Separation (UniMoS) framework for unsupervised do-
main adaptation. Leveraging insights from modality gap
studies, we craft a nimble modality separation network
that distinctly disentangles CLIPâ€™s features into language-
associated and vision-associated components. Our pro-
posed Modality-Ensemble Training (MET) method fosters
the exchange of modality-agnostic information while main-
taining modality-specific nuances. We align features across
domains using a modality discriminator. Comprehensive
evaluations on three benchmarks reveal our approach sets
a new state-of-the-art with minimal computational costs.
Code: https://github.com/TL-UESTC/UniMoS.
1. Introduction
Unsupervised domain adaptation (UDA) [2, 7, 29, 30] aims
to apply knowledge trained on a source domain to an un-
labeled target domain, a process invaluable in data-scarce
scenarios. Conventional methods often struggle with bridg-
ing the gap between source and target domains, finding it
challenging to develop consistent features across domains
[19, 39]. In image classification, aligning vision features
while neglecting semantic content can lead to difficulties in
differentiating complex samples [10, 41]. Vision-language
models (VLMs) such as CLIP [36] and ALIGN [13] circum-
vent these issues through joint multimodal pretraining on
images and texts. This extensive pretraining endows pub-
licly available VLMs with robust zero-shot transfer abil-
*Corresponding author.
Modality / Prob. couch chair (âˆš)
image modality 0.002 0.998
text modality 0.832 0.130Modality / Prob. shelf file cabinet (âˆš)
image modality 0.899 0.101
text modality 0.062 0.928
Figure 1. Examples of modality-specific information from task
Artâ†’RealWorld in Office-Home dataset. The digits are top-2
highest classification probabilities given by both modalities.
ities and a broad base of conceptual knowledge, making
them highly suitable for comprehensive UDA. They facil-
itate alignment across both visual and textual modalities,
enhancing adaptability and applicability in diverse contexts.
Previous studies have shown promising results by adapt-
ing VLMs like CLIP for unsupervised domain adaptation
(UDA). For instance, DAPrompt [10] introduces learn-
ing both domain-agnostic and domain-specific text embed-
dings, while PADCLIP [17] focuses on fine-tuning the vi-
sion branch of CLIP for adaptive visual feature extraction.
However, recent research [14, 24] highlights a modality gap
in VLMs, revealing that, despite training efforts, vision and
text features often remain distinctly distributed. We ar-
gue that adapting a single modality is less than ideal due
to the existence of unique, modality-specific cues in mis-
aligned textual and visual components. We suggest that
certain samples are best classified using specific modali-
ties, a hypothesis supported by empirical observations in
Fig. 1. This figure shows differing classification patterns
when each modality is adapted independently to the unla-
beled target data. Text modality results derive from CLIPâ€™s
zero-shot capabilities, while image results come from lin-
ear probing with target pseudo-labels. For instance, visually
straightforward items like a cushioned chair are accurately
classified by the vision linear classifier after tuning on tar-
get dataset. However, pretrained CLIP can erroneously cat-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
23364
egorize such items under visually similar classes. In con-
trast, complex items with nuanced semantic details, like a
file cabinet resembling a shelf, may confuse the vision clas-
sifier, while CLIPâ€™s broader knowledge base facilitates cor-
rect zero-shot predictions. In summary, while the vision
branch effectively discerns class-specific visual patterns,
the text branch leverages semantic information to clarify
ambiguous cases. This observation lays the groundwork
for a multimodal adaptation framework that synergistically
combines the strengths of both modalities.
A direct approach to domain adaptation involves con-
currently fine-tuning vision branch and crafting textual
prompts, which risks disturbing the image-text representa-
tion pairs in pretrained CLIP and is computationally inten-
sive [9, 57]. As a more efficient alternative, we propose
to explicitly disentangle CLIP-extracted visual features into
two complementary parts. The first component retains the
language-associated semantic knowledge inherent in CLIP,
while the second focuses on vision-specific attributes cru-
cial for distinguishing between nuanced visual categories.
We devised a set of modality separation networks with
dual branches to project CLIP-encoded visual features into
distinct language-associated components (LAC) and vision-
associated components (V AC). An orthogonal regulariza-
tion is employed to ensure these branches yield discrete,
disentangled representations. Each component is optimized
based on its inherent modality strengths. For the LAC
branch, we utilize knowledge distillation on target data to
harness the rich semantic content from the original pre-
trained CLIP model. Additionally, we implement a debi-
asing method to mitigate dataset bias in CLIPâ€™s zero-shot
results. For the V AC branch, the locality structure within
visual feature spaces [20, 53, 54] is leveraged to gener-
ate visual pseudo-labels for supervised learning on target
data. We then introduce a novel Modality-Ensemble Train-
ing (MET) strategy that synergistically merges outputs from
both modalities. A weight generator dynamically assembles
these predictions, supervised by V AC pseudo-labels on tar-
get data and actual labels on source data. Importantly, the
text modality output remains isolated during MET to pre-
serve independent training and maintain pretrained seman-
tics. Additionally, a modality discriminator is utilized to
align LAC and V AC across domains for unsupervised do-
main adaptation. Trained on source data to distinguish be-
tween LAC and V AC, this discriminator is frozen on the
target domain, directly updating the separation networks to
produce domain-invariant LAC and V AC. This approach en-
sures a consistent modality separation across domains, fa-
cilitating simultaneous adaptation in both modalities.
Contributions: 1. We investigate the modality gap phe-
nomenon in the context of applying Vision-Language Mod-
els (VLMs) to unsupervised domain adaptation, revealing
the limitations of adapting a single modality; 2. We intro-duce a novel framework, Unified Modality Separation (Uni-
MoS), which, coupled with a Modality-Ensemble Training
(MET) approach, facilitates effective multimodal adapta-
tion; 3. Our comprehensive analysis and validations under-
score and efficiency of the proposed UniMoS, demonstrat-
ing its ability to set new state-of-the-art benchmarks while
maintaining low computational demands.
2. Related work
Unsupervised domain adaptation (UDA). A core chal-
lenge in UDA is aligning representations between the
source domain and unlabeled target domain. Prior tech-
niques can be categorized as discrepancy-based [18, 28, 59]
and adversarial methods [7, 30, 39]. Discrepancy-based
methods explicitly minimizes divergence metrics including
MMD [18], MDD [59], etc. Adversarial methods extract
domain invariant features via a min-max game between the
feature extractor and domain discriminator [7, 30]. Re-
cent works focus on exploiting target data structures via
self-training techniques [15, 53â€“55, 60]. ICON [55] learns
an invariant classifier with consistent predictions to remove
the spurious correlation inconsistency in the target domain.
EIDCo [60] combines Mixup [56] with IDCo loss [5, 12]
to explore target data distribution. Vision transformer
(ViT) [6] and its variants have also gained popularity due
to their superior performance [51, 52, 63]. PMTrans [63]
mixes patch representations in SwinTransformer [27] as an
intermediate domain bridge. CDTrans [51] aligns features
extracted by DeiT [44] via cross-attention.
Vision-language models (VLMs) have shown great gen-
eralization abilities due to extensive multimodal pretrain-
ing [13, 36, 47, 48]. CLIP [36] is trained from 400 mil-
lion text-image pairs, while ALIGN [13] leverages more
than one billion text-image pairs. Subsequent works have
built on pretrained VLMs in various ways. Some learn
prompt texts to transfer VLMs to downstream tasks [10,
33, 38, 61, 62], while others incorporate additional tun-
able layers on the frozen pretrained encoder [9, 57]. Be-
yond utilizing existing VLMs, research also aims to im-
prove VLM training [14, 31]. Liang et al. [24] reveal that
VLMs exhibit a modality gap, failing to perfectly align mul-
timodal features. Jiang et al. [14] conduct theoretical anal-
ysis on modality gap and propose latent space regulariza-
tion to preserve modality-specific information. MaPLe [16]
utilize prompt learning on both modality branches to im-
prove alignment. Our approach is fundamentally differ-
ent since we disentangle VLM-extracted features posteri-
orly instead of training VLM from scratch, requiring far less
computation costs. Besides, our method requires no label-
ing on target domain. VLMs have also been adopted for
UDA [10, 17, 41]. DAPrompt [10] learns domain-specific
and domain-agnostic textual prompts for each class. AD-
CLIP [41] learns domain invariant prompts by conditioning
23365
â„’ð‘ð‘’â„’ð‘œð‘Ÿð‘¡â„Žð‘œâ„’ð‘˜ð‘™
â„’ð‘ð‘’a clipart photo of a car
a clipart photo of a bus
a clipart photo of a cupÂ·Â·Â·
ð‘“ð‘£ð‘Žð‘ð‘“ð‘™ð‘Žð‘ â„’ð‘ð‘ð‘’
frozen parametervision 
separatortext  
separator
modality separation
networksvision
encode r
classifierðœ‡1ðœ‡2Â·Â·Â·ðœ‡ð¾
ð‘“ð‘£ð‘“ð‘£ðœ‡1ð‘“ð‘£ðœ‡2Â·Â·Â·ð‘“ð‘£ðœ‡ð¾ðœ‡1ðœ‡2Â·Â·Â·ðœ‡ð¾
ð‘“ð‘™ð‘Žð‘ðœ‡1ð‘“ð‘™ð‘Žð‘ðœ‡2Â·Â·Â·ð‘“ð‘™ð‘Žð‘ðœ‡ð¾ ð‘“ð‘™ð‘Žð‘text
encode r
weight 
generatormodality
discriminator
ð‘¤à·œð‘¦ð‘‘ð‘–ð‘ 
à·œð‘¦ð‘£ð‘Žð‘à·œð‘¦ð‘™ð‘Žð‘
ð‘¦ð‘ â„’ð‘ð‘’ð‘¦ð‘ ð‘¦ð‘™ð‘Žð‘ð‘¡
ð‘¤Â·à·œð‘¦ð‘£ð‘Žð‘ð‘¡+
(1âˆ’ð‘¤)Â·à·œð‘¦ð‘™ð‘Žð‘ð‘¡
à·œð‘¦ð‘’ð‘›ð‘ ð‘¡ð‘¦ð‘£ð‘Žð‘ð‘¡ð‘¦ð‘™ð‘Žð‘ð‘¡
modality label
source data labelsource data labeltarget teacher 
knowledge
ð‘“ð‘£Figure 2. Framework of our method. We freeze the pretrained vision and text encoder of CLIP. CLIP-extracted vision features are
disentangled into language-associated components ( flac) and vision-associated components ( fvac) by the modality separation networks.
We obtain zero-shot results from CLIP as teacher knowledge, and distill the knowledge to LAC. We then introduce a weight generator to
assemble the modality outputs to train V AC. A modality discriminator is applied to align LAC and V AC from both domains.
on image style and content features. PADCLIP [17] dynam-
ically adjusts learning rate while tuning CLIP to prevent
catastrophic forgetting. However, these methods perform
adaptation on either the visual or textual modality in iso-
lation. Our work addresses this limitation by proposing a
unified adaptation framework for the multimodal features.
3. Method
3.1. Problem formulation
In this study, superscripts differentiate domains, with sym-
bols lacking superscripts applicable to both domains. We
consider a labeled source domain Ds={(xs
i, ys
i)}Ns
i=1and
aim to develop a model generalizable to an unlabeled tar-
get domain Dt={(xt
i)}Nt
i=1. CLIP [36] features a vision
encoder gvisand a text encoder gtxt. The vision feature
for an image input xis denoted as fv=gvis(x). Em-
ploying the zero-shot inference strategy from [17], we con-
struct naive prompts {(ti)}K
i=1asa [DOMAIN] photo of a
[CLASS] , with Krepresenting the number of classes, [DO-
MAIN] indicating domain specifics, and [CLASS] the class
name. Text features are then derived as Âµi=gtxt(ti).Âµi
andfvare both features with dvdimension. Classification
is based on the highest cosine similarity between fvandÂµi:
Ë†yzs= arg max
icos(Âµi, fv). (1)
Eq. (1) may not be ideal for unlabeled target data due
to the existence of modality gap [24]. To tackle this, we
conceptualize vision inputs as a composite of a vision-
associated component (V AC) and a language-associated
component (LAC), denoted as fv={fvac, flac}. This
leads us to obtain modality-specific classification results
yvacandylac, before constructing a cross-modality output:
yens=wÂ·yvac+ (1âˆ’w)Â·ylac, (2)where wis a set of learnable weights harmonizing the
contributions of V AC and LAC. This design seeks to bal-
ance modality-agnostic information sharing and modality-
specific information capturing.
Instead of separating LAC and V AC during training, we
utilize yensto guide V AC learning, incorporating comple-
mentary modality information for a holistic cross-modal
training. Furthermore, a fixed weight wmay lack flexi-
bility across diverse datasets and scenarios, potentially ob-
scuring the distinction between V AC and LAC. Addressing
this, we introduce a dynamic wthat adeptly discriminates
between modalities within yens, calibrating their influence
in the training. This strategy ensures tailored training ap-
proaches for different datasets or training stages, facilitating
modality-specific information utilization. Next we detail on
the training and aligning of LAC and V AC.
3.2. Modality separation networks
We first introduce the modality separation networks that
disentangles CLIP-extracted features, which comprise two
separators as shown in Fig. 2. These networks partition
CLIP-extracted vision features into LAC and V AC using
the text separator Gtxtand the vision separator Gvis, re-
spectively. The separated components are defined as flac=
Gtxt(fv)andfvac=Gvis(fv). Both separators are lin-
ear layers preserving the dimensionality of fv, such that
fvac, flacâˆˆRdv. Drawing on deep feature separation prin-
ciples [3], we apply an orthogonal loss to maintain the dis-
tinctness of LAC and V AC:
Lortho =|fs
lacÂ·fs
vacâŠ¤|2
F+|ft
lacÂ·ft
vacâŠ¤|2
F. (3)
Different outputs for LAC and V AC are then generated.
For the text modality, we utilize the zero-shot inference of
CLIP to classify LAC by calculating the cosine similarity
23366
between LAC and CLIPâ€™s text features, forming logits:
Ë†ylac= (Ë†l1,Ë†l2,Â·Â·Â·Ë†lk),Ë†li= cos( Âµi, flac)/T, (4)
where Tis temperature in pretrained CLIP. For V AC, we
route it through a linear classifier with layers Î¦1âˆˆRdvÃ—db
andÎ¦2âˆˆRdbÃ—K, producing the bottleneck feature with
dimension dband output via:
fb= Î¦ 1(fvac),Ë†yvac= Î¦ 2(fb). (5)
We provide implementation details in Supplementary.
3.3. Modality-ensemble training
Having obtained disentangled components, we design cus-
tomized training paradigm for each modality. A learnable
weight further connects both modalities, establishing a uni-
fied modality-ensemble training framework.
Learning LAC. To preserve the rich semantic content in
pretrained CLIP, we distill this knowledge to LAC. For the
target data, zero-shot similarity scores derived from pre-
trained CLIP serve as the teacher knowledge:
yt
lac= (l1âˆ’l, l2âˆ’l,Â·Â·Â·, lkâˆ’l), li= cos( Âµi, ft
v)/T, (6)
withl=1
KPK
k=1lknormalizing the CLIP outputs and T
the temperature of pretrained CLIP. The teacher knowledge
in Eq. (6) guides the distillation for the unlabeled target
LAC, while for the source data, cross-entropy loss is applied
directly using labeled source data. The overall training loss
for LAC combines Eq. (4) and Eq. (6) as follows:
Llac= KL(Ë† yt
lac, yt
lac) +Î±CE(Ë†ys
lac, ys), (7)
where Î±adjusts the influence of source data supervision,
KL(Â·,Â·)is the Kullback-Leibler divergence, and CE(Â·,Â·)
represents the standard cross-entropy loss.
Obtaining pseudo label for V AC. Focusing on image
modality, we aim to enhance the locality structure of vi-
sion representationsâ€”high inter-class discriminability and
tight intra-class distributionâ€”a feature that CLIP-extracted
vision features lack, as detailed in Fig. 4a. To instill these
locality structures within V AC, we utilize a K-means-based
deep clustering approach [4, 21] to generate pseudo-labels
for unlabeled target data. We calculate the clustering cen-
troids for class kas follows:
Ï•k=P
xtÎ´k(softmax(Ë† yt
ens))Â·ft
bP
xtÎ´k(softmax(Ë† ytens)), (8)
where Ë†yt
ensrepresents target ensemble outputs discussed be-
low, and Î´kselects the kthlogit. To mitigate imbalances in
text modality predictions from CLIP [17, 49], we imple-
ment Approximated Controlled Direct Effect (ACDE) [49]
to adjust similarity scores obtained in Eq. (4):
Ëœyt
lac= Ë†yt
lacâˆ’Ï„log Ë†p,Ë†pâ†mË†p+ (1âˆ’m)1
BBX
i=1pi,(9)where mis momentum, Ï„is a debiasing factor, Bis the
batch size, and pi= softmax(Ë† yt
lac)denotes classification
probability of LAC. The ensemble outputs, used in the cen-
troid calculation, are then defined as Ë†yt
ens=wÂ·Ë†yt
vac+ (1âˆ’
w)Â·Ëœyt
lac. For any given target bottleneck feature ft
b, we
compute its cosine similarity with all centroids, assigning
the class with the highest similarity as the pseudo-label:
yt
vac= arg max
kcos(ft
b, Ï•k). (10)
Learning V AC. We now train vision component on unifies
outputs from both modalities. Utilizing Eq. (9) and Eq. (5),
the target ensemble output Ë†yt
ensis formulated as:
Ë†yt
ens=wÂ·Ë†yt
vac+ (1âˆ’w)Â·Ëœyt
lac, (11)
with the weight w=W(V ACt)produced by the weight
generator W, as depicted in Fig. 2. Referring to Sec. 3.1,
we optimize Ë†yt
ensrather than Ë†yt
vacdirectly, with Ëœyt
lacserving
as an auxiliary in training V AC and thus detached from the
computational graph in Eq. (11).
To enhance individual discriminability and global diver-
sity, thereby preserving the locality structure of vision rep-
resentations, we follow state-of-the-art [1, 20, 21] to ap-
ply an information maximization loss Limcomprising two
components. The entropy loss Lentimproves individual
certainty:
Lent=âˆ’ExtâˆˆDt"KX
k=1Î´k(Ë†yt
ens) logÎ´k(Ë†yt
ens)#
,(12)
and the diversity loss fosters diverse class distributions:
Ldiv=âˆ’KX
k=1qklogqk, (13)
where qk=âˆ’ExtâˆˆDtÎ´k(Ë†yt
ens). Hence, Limis defined as:
Lim=Lentâˆ’ Ldiv. (14)
The training of V AC is supervised by target pseudo la-
bels for the vision modality, obtained through Eq. (10),
while source labels directly optimize Ë†ys
vac:
Lvac=CE(Ë†yt
ens, yt
vac) +Î²CE (Ë†ys
vac, ys) +Lim,(15)
where Î²modulates the impact of source data supervision.
3.4. Aligning source and target by discriminator
To achieve domain adaptation on both modalities, we in-
troduce a modality discriminator Dto align V AC and LAC
from both domains. Our approach utilizes a singular modal-
ity discriminator trained on the source domain to differen-
tiate LAC from V AC, and then assesses alignment on the
23367
target domain. Proper alignment across domains would en-
ableDto discern LAC and V AC on the target domain with-
out direct training. The modality discriminator is trained
using binary cross-entropy loss:
Lbce=âˆ’[ydislog Ë†ydis+ (1âˆ’ydis) log(1 âˆ’Ë†ydis)],(16)
where ydisrepresents the modality label (0 for V AC, 1 for
LAC) and Ë†ydisis the output of D.
Notably, Dis only trained on the source domain using
Eq. (16). On the target domain, only the separators Gvis
andGtxtare updated to minimize Eq. (16), aligning target
LAC and V AC with the source ones.
3.5. Training and inference
Training. As depicted in Fig. 2, the pretrained text en-
coder and vision encoder are frozen, and we optimize pa-
rameters of Gtxt,Gvis,Î¦1,Î¦2,W,D, denoted as Î¸Gtxt,
Î¸Gvis,Î¸Î¦1,Î¸Î¦2,Î¸W,Î¸D, respectively. Combining Eq. (7),
Eq. (3), Eq. (15), Eq. (16), we define the following opti-
mization problem:
Î¸Gtxt= arg min
Î¸GtxtLlac+Î³Lortho +Î³Lbce, (17)
Î¸Gvis= arg min
Î¸GvisLvac+Î³Lortho +Î³Lbce,
Î¸W, Î¸Î¦1, Î¸Î¦2= arg min
Î¸W,Î¸Î¦1,Î¸Î¦2Lvac,
Î¸D= arg min
Î¸DÎ³Lbce,
where Î³is hyperparameter controlling regularization terms
LbceandLortho . We present detailed training procedure of
UniMoS in Supplementary.
Inference. At inference, the final mixed prediction on tar-
get data is obtained using Ë†yt
ensfrom Eq. (11), with a fixed
mixup weight w. The objective is to maximize accuracy
by leveraging the strengths of both modalities for improved
classification, as supported by our observations (Fig. 1),
thus integrating outputs from both modalities to harness
their combined advantages.
4. Experiments
4.1. Datasets and implementation details
We extensively evaluate our method on three mainstream
UDA benchmarks. Office-Home [46] consists of 65 cat-
egories divided into 4 distinct domains. On VisDA-
2017 [34], the goal is to transfer knowledge from 152k syn-
thetic images (source domain) to 55k images of real items
(target domain). DomainNet [35] is the most challeng-
ing UDA benchmark so far, containing 0.6 million samples
from 345 categories divided into 6 distinct domains. Fol-
lowing previous works, we additionally provide results onMini-DomainNet [25, 40, 58], a subset of DomainNet with
4 domains and 126 categories.
We conduct all experiments on an NVIDIA RTX 2080Ti
GPU. Since our method does not involve updating CLIPâ€™s
pretrained parameters or prompts, the CLIP-extracted vi-
sion and text features are obtained via one single forward
and saved in memory, thus greatly saving computation
costs. For all tasks, we adopt SGD optimizer with batch
size 32, and set momentum min Eq. (9) to 0.99 and debias
factor Ï„in Eq. (9) to 0.5. For Office-Home and Domain-
Net, we train 50 epochs with initial learning rate 3e-3 and
adopt annealing strategy [8] for learning rate decay. We
train for 10 epochs with initial learning rate 9e-4 on VisDA
due to fast convergence. The fixed mixup weight described
in Sec. 3.5 is set to 0.3 for all tasks. We set regularization
weight Î³in Eq. (17) to 0.01 across all datasets.
4.2. Benchmark results
Office-Home. Tab. 1 gives classification accuracies on 12
adaptation tasks on Office-Home using the ResNet50 [11]
backbone. To ensure a fair comparison, we categorize
CLIP-based methods into two groups: â€˜none-tuningâ€™ and
â€˜full-tuningâ€™. The former involves learning prompts or addi-
tional modules without adjusting the pretrained CLIP back-
bones, while the latter optimizes the pretrained parame-
ters of CLIP for specific tasks. It is evident from the re-
sults that our proposed UniMoS consistently outperforms
both â€˜none-tuningâ€™ and â€˜full-tuningâ€™ methods. Notably, we
obtain +1.3% performance boost than the strong baseline
PADCLIP, which fine-tunes the CLIP vision backbone. Our
method requires no parameter update or data forwarding
of CLIP backbones, thus is much computationally cheaper.
Especially on tasks that take P as target domain, we achieve
up to +5.4% performance boost than PADCLIP, demonstrat-
ing the superiority of multimodal adaptation.
VisDA-2017. Tab. 2 shows class-wise classification accura-
cies on VisDA using ResNet101 [11]. Our method achieves
the best performance among â€˜none-tuningâ€™ CLIP methods,
while slightly falling behind PADCLIP. The reason is that
CLIP has not been trained on the synthetic images like those
from source domain of VisDA, resulting in incompatibilities
between the VisDA dataset and CLIP. Similar observations
are made by PADCLIP [17], which opts to fine-tune the vi-
sion branch of CLIP to address this challenge. Nevertheless,
our approach outperforms typical UDA methods.
DomainNet. Tab. 3 presents classification accuracies of
30 cross-domain adaptation tasks on the most challeng-
ing benchmark DomainNet. Rows represent source do-
mains and columns represent target domains. Our method
reaches comparable performance with the strong baseline
PADCLIP. One significant observation is that our UniMoS
obtains lower accuracy than PADCLIP (6.6% lower in av-
erage) on tasks with qdr as target. This discrepancy arises
23368
Table 1. UDA results on Office-Home. Best results are marked in bold font. Methods with â€˜*â€™ are based on CLIP.
Method Backbone Aâ†’C Aâ†’P Aâ†’R Câ†’A Câ†’P Câ†’R Pâ†’A Pâ†’C Pâ†’R Râ†’A Râ†’C Râ†’PAvg.
SourceOnly [11]
ResNet5034.9 50.0 58.0 37.4 41.9 46.2 38.5 31.2 60.4 53.9 41.2 59.9 46.1
ParetoDA [22] 56.8 75.9 80.5 64.4 73.5 73.7 65.6 55.2 81.3 75.2 61.1 83.9 70.6
SDAT [37] 58.2 77.1 82.2 66.3 77.6 76.8 63.3 57.0 82.2 74.9 64.7 86.0 72.2
MSGD [50] 58.7 76.9 78.9 70.1 76.2 76.6 69.0 57.2 82.3 74.9 62.7 84.5 72.3
Fixbi [32] 58.1 77.3 80.4 67.7 79.5 78.1 65.8 57.9 81.7 76.4 62.9 86.7 72.7
CST [26] 59.0 79.6 83.4 68.4 77.1 76.7 68.9 56.4 83.0 75.3 62.2 85.1 72.9
ATDOC [23] 60.2 77.8 82.2 68.5 78.6 77.9 68.4 58.4 83.1 74.8 61.5 87.2 73.2
KUDA [42] 58.2 80.0 82.9 71.1 80.3 80.7 71.3 56.8 83.2 75.5 60.3 86.6 73.9
EIDCo [60] 63.8 80.8 82.6 71.5 80.1 80.9 72.1 61.3 84.5 78.6 65.8 87.1 75.8
ICON [55] 63.3 81.3 84.5 70.3 82.1 81.0 70.3 61.8 83.7 75.6 68.6 87.3 75.8
PADCLIP* [17]ResNet50-
full-tuning57.5 84.0 83.8 77.8 85.5 84.7 76.3 59.2 85.4 78.1 60.2 86.7 76.6
CLIP* [36]
ResNet50-
none-tuning51.7 81.5 82.3 71.7 81.5 82.3 71.7 51.7 82.3 71.7 51.7 81.5 71.8
DAPrompt* [10] 54.1 84.3 84.8 74.4 83.7 85.0 74.5 54.6 84.8 75.2 54.7 83.8 74.5
ADCLIP* [41] 55.4 85.2 85.6 76.1 85.8 86.2 76.7 56.1 85.4 76.8 56.1 85.5 75.9
UniMoS* (ours) 59.5 89.4 86.9 75.2 89.6 86.8 75.4 58.4 87.2 76.9 59.5 89.7 77.9
Table 2. UDA results on VisDA-2017. Best results are marked in bold font. Methods with â€˜*â€™ are based on CLIP.
Method Backbone plane bicycle bus car horse knife mcycl person plant sktbrd train truck Avg.
SourceOnly [11]
ResNet10155.1 53.3 61.9 59.1 80.6 17.9 79.7 31.2 81.0 26.5 73.5 8.5 52.4
ParetoDA [22] 95.9 82.8 81.3 58.7 93.9 93.7 85.9 83.0 91.9 92.0 87.1 51.8 83.2
MSGD [50] 97.5 83.4 84.4 69.4 95.9 94.1 90.9 75.5 95.5 94.6 88.1 44.9 84.5
ATDOC [23] 95.3 84.7 82.4 75.6 95.8 97.7 88.7 76.6 94.0 91.7 91.5 61.9 86.3
CAN [15] 97.0 87.2 82.5 74.3 97.8 96.2 90.8 80.7 96.6 96.3 87.5 59.9 87.2
FixBi [32] 96.1 87.8 90.5 90.3 96.8 95.3 92.8 88.7 97.2 94.2 90.9 25.7 87.2
PADCLIP* [17]ResNet101-
full-tuning96.7 88.8 87.0 82.8 97.1 93.0 91.3 83.0 95.5 91.8 91.5 63.0 88.5
CLIP* [36]
ResNet101-
none-tuning98.2 83.9 90.5 73.5 97.2 84.0 95.3 65.7 79.4 89.9 91.8 63.3 84.4
DAPrompt* [10] 97.8 83.1 88.8 77.9 97.4 91.5 94.2 79.7 88.6 89.3 92.5 62.0 86.9
ADCLIP* [41] 98.1 83.6 91.2 76.6 98.1 93.4 96.0 81.4 86.4 91.5 92.1 64.2 87.7
UniMoS* (ours) 97.7 88.2 90.1 74.6 96.8 95.8 92.4 84.1 90.8 89.0 91.8 65.3 88.1
from a significant domain gap between qdr and other do-
mains, which is challenging to bridge without tuning the
CLIP backbones. However, despite this, we manage to
achieve superior performance to PADCLIP on all other
tasks, thereby mitigating the overall 6.6% performance
drop. We also surpass all competing methods significantly.
Tab. 4 compares UniMoS with available CLIP-based none-
tuning methods on Mini-DomainNet, where our method
achieves significant performance boost. Detailed results on
Mini-DomainNet are given in Supplementary.
4.3. Ablation study
In this section we validate the efficacy of each mod-
ule in UniMoS. Tab. 5 presents averaged accuracies on
Office-Home and VisDA-2017 by removing specific mod-
ules while maintaining other settings identical. The â€˜w/o
debiasingâ€™ is obtained by skipping the debias procedure in
Eq. (9). A primary observation is that the removal of any
module leads to a performance drop to varying degrees, un-
derscoring the positive contribution of each module to the
overall outcome. The â€˜w/o learnable weightâ€™ row is ob-
tained by replacing win Eq. (11) with a constant 0.5, re-sulting in the most pronounced performance decline. This
emphasizes the significance of dynamic weights, which en-
ables V AC to focus on vision-specific parts. Further insights
into the effects of dynamic ware detailed in Sec. 4.4.
Tab. 6 ablates on the choice of backbones. We ex-
periment with three backbones on Office-Home, and our
UniMoS consistently outperforms all competing methods,
proving that our method is generalizable across various
models. Detailed results are given in Supplementary.
4.4. Discussions
Effectiveness of learnable weight w.To better understand
how the learnable ensemble weight win Eq. (2) boosts per-
formance, Fig. 3 compares the training process with and
without dynamic w. We set a fixed weight of 0.5 for both
LAC and V AC in â€˜w/o learnable wâ€™, and learn dynamic
weights (shown as â€˜Learned weight wâ€™ in the figure) in other
settings. Our first observation is that in our full design, ac-
curacy of V AC increases steadily as the training progresses.
Leveraging complementary modality-specific information
from LAC, the final mixed outputs achieve higher accu-
racy than V AC alone. As stated in Sec. 3.1, the goal of
23369
Table 3. UDA results on DomainNet. Best results are marked in bold font. Methods with â€˜*â€™ are based on CLIP.
DeiT
-B [44]clp inf pnt qdr rel skt avgViT
-B [6]clp inf pnt qdr rel skt avgSSRT
-B [43]clp inf pnt qdr rel skt avg
clp - 24.3 49.6 15.8 65.3 52.1 41.4 clp - 27.2 53.1 13.2 71.2 53.3 43.6 clp 33.8 60.2 19.4 75.8 59.8 49.8
inf 45.9 - 45.9 6.7 61.4 39.5 39.9 inf 51.4 - 49.3 4.0 66.3 41.1 42.4 inf 55.5 - 54.0 9.0 68.2 44.7 46.3
pnt 53.2 23.8 - 6.5 66.4 44.7 38.9 pnt 53.1 25.6 - 4.8 70.0 41.8 39.1 pnt 61.7 28.5 - 8.4 71.4 55.2 45.0
qdr 31.9 6.8 15.4 - 23.4 20.6 19.6 qdr 30.5 4.5 16.0 - 27.0 19.3 19.5 qdr 42.5 8.8 24.2 - 37.6 33.6 29.3
rel 59.0 25.8 56.3 9.2 - 44.8 39.0 rel 58.4 29.0 60.0 6.0 - 45.8 39.9 rel 69.9 37.1 66.0 10.1 - 58.9 48.4
skt 60.6 20.6 48.4 16.5 61.2 - 41.5 skt 63.9 23.8 52.3 14.4 67.4 - 44.4 skt 70.6 32.8 62.2 21.7 73.2 - 52.1
avg 50.1 20.3 43.1 10.9 55.5 40.3 36.7 avg 51.5 22.0 46.1 8.5 60.4 40.3 38.1 avg 60.0 28.2 53.3 13.7 65.3 50.4 45.2
CDTrans
-DeiT [51]clp inf pnt qdr rel skt avgPMTrans
-Swin [63]clp inf pnt qdr rel skt avgCLIP
-B* [36]clp inf pnt qdr rel skt avg
clp - 29.4 57.2 26.0 72.6 58.1 48.7 clp - 34.2 62.7 32.5 79.3 63.7 54.5 clp - 70.1 70.1 70.1 70.1 70.1 70.1
inf 57.0 - 54.4 12.8 69.5 48.4 48.4 inf 67.4 - 61.1 22.2 78.0 57.6 57.3 inf 46.4 - 46.4 46.4 46.4 46.4 46.4
pnt 62.9 27.4 - 15.8 72.1 53.9 46.4 pnt 69.7 33.5 - 23.9 79.8 61.2 53.6 pnt 61.7 61.7 - 61.7 61.7 61.7 61.7
qdr 44.6 8.9 29.0 - 42.6 28.5 30.7 qdr 54.6 17.4 38.9 - 49.5 41.0 40.3 qdr 13.7 13.7 13.7 - 13.7 13.7 13.7
rel 66.2 31.0 61.5 16.2 - 52.9 45.6 rel 74.1 35.3 70.0 25.4 - 61.1 53.2 rel 82.9 82.9 82.9 82.9 - 82.9 82.9
skt 69.0 29.6 59.0 27.2 72.5 - 51.5 skt 73.8 33.0 62.6 30.9 77.5 - 55.6 skt 62.6 62.6 62.6 62.6 62.6 - 62.6
avg 59.9 25.3 52.2 19.6 65.9 48.4 45.2 avg 67.9 30.7 59.1 27.0 72.8 56.9 52.4 avg 53.5 58.2 55.1 64.7 50.9 55.0 56.2
DAPrompt
-B* [10]clp inf pnt qdr rel skt avgPADCLIP
-B* [17]clp inf pnt qdr rel skt avgUniMoS
-B*(ours)clp inf pnt qdr rel skt avg
clp - 73.0 73.8 72.6 73.9 73.5 73.4 clp - 73.6 75.4 74.6 76.4 76.3 75.3 clp - 76.5 77.2 76.6 77.5 77.8 77.1
inf 50.8 - 50.1 49.6 50.6 50.3 50.3 inf 55.1 - 54.3 53.6 54.9 54.9 54.6 inf 55.1 - 55.0 54.6 55.3 55.2 55.0
pnt 70.2 69.6 - 68.9 70.4 69.9 69.8 pnt 71.1 70.6 - 70.0 72.7 71.7 71.2 pnt 72.3 71.5 - 69.4 72.5 72.6 71.7
qdr 17.2 14.4 13.9 - 14.3 13.9 14.7 qdr 36.8 18.0 32.0 - 31.7 34.9 30.7 qdr 25.0 22.9 23.6 - 23.7 25.1 24.1
rel 84.9 84.8 84.9 84.7 - 84.6 84.8 rel 84.2 83.5 83.5 83.1 - 83.6 83.6 rel 86.0 85.9 85.8 85.5 - 85.9 85.8
skt 65.8 65.4 65.8 64.9 65.9 - 65.6 skt 68.1 66.6 67.2 66.1 67.5 - 67.1 skt 68.5 67.8 68.2 67.5 68.0 - 68.0
avg 57.8 61.4 57.7 68.1 55.0 58.4 59.8 avg 63.1 62.5 62.5 69.5 60.6 64.3 63.7 avg 61.4 64.9 62.0 70.7 59.4 63.3 63.6
Table 4. UDA results on Mini-DomainNet. Best results are
marked in bold font. All compared methods are CLIP-based.
Method Backbone Acc. Method Backbone Acc.
CLIP
ResNet5071.2 CLIP
ViT-B82.8
DAPrompt 74.8 DAPrompt 85.8
ADCLIP 75.2 ADCLIP 86.9
UniMoS (ours) 78.0 UniMoS (ours) 87.3
Table 5. Ablation study on Office-Home and VisDA-2017.
Method Office-Home VisDA-2017
w/oLortho 77.4 87.6
w/o debiasing 77.3 87.7
w/oLim 77.0 87.8
w/oLdistill 77.0 87.6
w/o learnable weight 76.9 86.2
w/o modality discriminator 77.6 87.9
UniMoS (full design) 77.9 88.1
dynamic ensemble weight is to adaptively identify and pre-
serve modality-specific information. We show in Fig. 3 that,
the learned weight changes in each epoch to fit the training
process. Without its support (w/o learnable w), the accu-
racy of V AC outputs drops significantly and finally con-
verges with LAC. This occurs because employing a static
weight would compromise the modality separation effects,
causing both modalities to collapse to poor performance.
In the example given by Fig. 3, accuracy of â€˜V AC output
w/o learnable wâ€™ is 3.5% lower than that of the full design
â€˜V AC outputâ€™. The phenomenon indicates the significance
of training with dynamic ensemble weights in our design.
More examples are given in Supplementary.
Computation analysis. Tab. 7 compares computation costs
of different methods on VisDA dataset. Our method ne-
cessitates training only a few linear layers without updat-
ing CLIP backbones, bringing great parameter efficiency.
0 10 20 30 40 50
Epoch50515253545556575859 Acc.
0.00.20.40.60.81.0
weightOffice-Home, P->C
LAC output
V AC output
V AC output w/o
learnable wMixed output
Learned weight w
Mixed output w/o
learnable wFigure 3. Effects of learnable ensemble weight won Office-Home.
Table 6. Results with different backbones on Office-Home. Best
results are in bold. All compared methods are based on CLIP.
Method Backbone Acc. Method Backbone Acc.
UniMoS (ours) ResNet50 77.9 CLIP
ViT-B82.4
CLIP
ViT-L87.0 DAPrompt 84.4
DAPrompt 88.7 ADCLIP 86.1
ADCLIP 90.5 PADCLIP 86.7
UniMoS (ours) 90.7 UniMoS (ours) 86.9
Furthermore, only one forward through CLIP is needed,
which allows UniMoS achieve more than 47Ã—training
speed boost than PADCLIP. Prompt learning methods like
DAPrompt also requires no training on CLIP backbones,
but they require extensive iterative forwarding of data
through CLIP to learn optimal prompts per class, leading to
low computing efficiency and scalability to larger datasets.
When running on DomainNet with 345 classes, DAPrompt
requires more than 22G GPU memory, while our method
requires less than 3G. More details are in Supplementary.
Feature distribution visualization. To demonstrate the ef-
ficacy of modality separation and feature alignment, we per-
23370
(a) CLIP-extracted feature distribution.
 (b) Modality separation effect.
 (c) Aligned bottleneck feature distribution.
Figure 4. T-sne visualization [45] of the effects of UniMoS on A â†’P task from Office-Home. UniMoS effectively disentangles CLIP-
extracted vision features (Fig. 4a) into LAC and V AC (Fig. 4b, obtained by randomly selecting 25 classes), and constructs clear cross-
domain locality structures (Fig. 4c).
Table 7. Computation analysis on VisDA-2017. Best results are
marked in bold font. Methods with â€˜*â€™ are based on CLIP.
Method Bkb. Param.Throughput
(imges/s)Train
timeFLOPs Acc.
DAPrompt* 1.2M 244 4.3H 11.3G 86.9
FixBi 86.1M 102 5.5H 15.73G 87.2
CAN 42.5M 31 10.5H 7.9G 87.2
PADCLIP* - - 23.5H * - 88.5
UniMoS* (ours)
ResNet1010.79M 2667 0.5H <0.01G 88.1
form t-sne [45] visualization on various features at different
phases of our method. Fig. 4a shows CLIP-extracted vision
features from source (green) and target (blue) domain, along
with CLIP-extracted text features (red) of naive prompts.
The text features distribute distantly with vision features,
proving the existence of modality gap. Besides, CLIP-
extracted vision features form poor class discriminability
and locality structures. Fig. 4b showcases the separated
LAC and V AC of our method. A clear boundary can be ob-
served between the features of both modalities, indicating
the effectiveness of modality separation. Additionally, the
text features distribute closer to LAC, proving that the sepa-
rated LAC is indeed more relevant to the text modality. Fol-
lowing feature alignment and V AC training, the bottleneck
features fbfrom both domains display a compact class-level
locality structure, as demonstrated in Fig. 4c. This compact
structure contributes significantly to the accuracy of the fi-
nal classification results.
Hyperparameter sensitivity. In the calibration of Uni-
MoS, we encounter three hyperparameters to determine: Î±
in Eq. (7), Î²in Eq. (15), and Î³in Eq. (17). We empiri-
cally discover that alternating Î³has little impact within each
dataset, so we focus on exploring the effects of Î±andÎ². As
illustrated in Fig. 5a, the performance on VisDA is notably
influenced by Î±, with smaller values leading to improved
accuracy. This outcome is attributed to the fact, as dis-
cussed in Sec. 4.2, that CLIP struggles to adequately iden-
*Cited from PADCLIP [17]. PADCLIP conducts the experiments on an
NVIDIA Tesla V100 GPU. Results on other metrics are unavailable since
the authors have not released the code implementations yet.
0.001 0.01 0.05 0.1 1
Î±0.001
0.01
0.05
0.1
1Î²87.9 87.6 87.7 87.5 87.6
87.9 87.7 87.7 87.6 87.6
88.0 87.7 87.7 87.7 87.6
88.1 87.8 87.8 87.7 87.6
87.7 87.6 87.5 87.5 87.3VisDA-2017
87.487.687.888.0(a) VisDA-2017
0.001 0.05 0.1 0.5 1 2.5
Î±0.001
0.05
0.1
0.5
1
2.5Î²75.2 75.8 75.8 76.3 76.5 76.6
75.9 76.5 76.4 76.9 77.0 76.9
76.2 76.8 76.8 77.1 77.2 77.1
76.6 77.3 77.4 77.6 77.7 77.2
76.8 77.5 77.5 77.8 77.9 77.3
77.0 77.7 77.7 77.7 77.6 77.1Office-Home
75.576.076.577.077.5 (b) Office-Home
Figure 5. Parameter sensitivity analysis on Î±andÎ²of UniMoS.
tify source synthetic images from VisDA. Consequently,
down-weighting source supervision on LAC proves bene-
ficial. Conversely, source supervision from both modalities
of Office-Home are important. They positively and equally
contributes to the adaptation process, so Î±= 1 andÎ²= 1
brings the best result, as shown in Fig. 5b.
5. Conclusions
Inspired by the theory of modality gap, in this paper we
propose a Unified Modality Separation framework for un-
supervised domain adaptation. The CLIP-extracted vision
features are explicitly disentangled into vision-associated
and language-associated components, which are trained dif-
ferently according to their modality strengths and further
aligned by a modality discriminator. A modality-ensemble
training paradigm unifies both components to leverage
modality-specific information while preserving modality-
shared contexts, contributing to successful classification.
This work is hope to inspire further analysis and exploita-
tion of the multimodal features in pretrained VLMs.
6. Acknowledgements
This work was supported in part by the National Natural
Science Foundation of China under Grant 62176042, and
in part by Sichuan Science and Technology Program under
Grant 2023NSFSC0483, and in part by Tencent Marketing
Solution Rhino-Bird Focused Research Program.
23371
References
[1] Sk Miraj Ahmed, Dripta S Raychaudhuri, Sujoy Paul, Samet
Oymak, and Amit K Roy-Chowdhury. Unsupervised multi-
source domain adaptation without access to source data. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 10103â€“10112, 2021.
[2] Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan.
A theory of learning from different domains. Machine learn-
ing, 79:151â€“175, 2010.
[3] Konstantinos Bousmalis, George Trigeorgis, Nathan Silber-
man, Dilip Krishnan, and Dumitru Erhan. Domain separa-
tion networks. Advances in neural information processing
systems , 29, 2016.
[4] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and
Matthijs Douze. Deep clustering for unsupervised learning
of visual features. In Proceedings of the European confer-
ence on computer vision (ECCV) , pages 132â€“149, 2018.
[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In International conference on ma-
chine learning , pages 1597â€“1607. PMLR, 2020.
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In International Con-
ference on Learning Representations , 2020.
[7] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain
adaptation by backpropagation. In International conference
on machine learning , pages 1180â€“1189. PMLR, 2015.
[8] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-
cal Germain, Hugo Larochelle, Franc Â¸ois Laviolette, Mario
Marchand, and Victor Lempitsky. Domain-adversarial train-
ing of neural networks. The journal of machine learning
research , 17(1):2096â€“2030, 2016.
[9] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
Clip-adapter: Better vision-language models with feature
adapters. International Journal of Computer Vision , pages
1â€“15, 2023.
[10] Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji
Song, Shuang Li, and Gao Huang. Domain adaptation via
prompt learning. IEEE Transactions on Neural Networks
and Learning Systems , pages 1â€“11, 2023.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770â€“778, 2016.
[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual rep-
resentation learning. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
9729â€“9738, 2020.
[13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and TomDuerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
conference on machine learning , pages 4904â€“4916. PMLR,
2021.
[14] Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing
Ping, Son Dinh Tran, Yi Xu, Belinda Zeng, and Trishul
Chilimbi. Understanding and constructing latent modality
structures in multi-modal representation learning. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 7661â€“7671, 2023.
[15] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Haupt-
mann. Contrastive adaptation network for unsupervised do-
main adaptation. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
4893â€“4902, 2019.
[16] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad
Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple:
Multi-modal prompt learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 19113â€“19122, 2023.
[17] Zhengfeng Lai, Noranart Vesdapunt, Ning Zhou, Jun Wu,
Cong Phuoc Huynh, Xuelu Li, Kah Kuen Fu, and Chen-Nee
Chuah. Padclip: Pseudo-labeling with adaptive debiasing
in clip for unsupervised domain adaptation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 16155â€“16165, 2023.
[18] Jingjing Li, Erpeng Chen, Zhengming Ding, Lei Zhu, Ke
Lu, and Heng Tao Shen. Maximum density divergence for
domain adaptation. IEEE transactions on pattern analysis
and machine intelligence , 43(11):3918â€“3930, 2020.
[19] Jingjing Li, Zhekai Du, Lei Zhu, Zhengming Ding, Ke Lu,
and Heng Tao Shen. Divergence-agnostic unsupervised do-
main adaptation by adversarial attacks. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 44(11):8196â€“
8211, 2021.
[20] Xinyao Li, Zhekai Du, Jingjing Li, Lei Zhu, and Ke Lu.
Source-free active domain adaptation via energy-based lo-
cality preserving transfer. In Proceedings of the 30th ACM
International Conference on Multimedia , pages 5802â€“5810,
2022.
[21] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need
to access the source data? source hypothesis transfer for un-
supervised domain adaptation. In International conference
on machine learning , pages 6028â€“6039. PMLR, 2020.
[22] Jian Liang, Kaixiong Gong, Shuang Li, Chi Harold Liu, Han
Li, Di Liu, Guoren Wang, et al. Pareto domain adapta-
tion. Advances in Neural Information Processing Systems ,
34:12917â€“12929, 2021.
[23] Jian Liang, Dapeng Hu, and Jiashi Feng. Domain adaptation
with auxiliary target domain-oriented classifier. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 16632â€“16642, 2021.
[24] Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Ser-
ena Yeung, and James Y Zou. Mind the gap: Understanding
the modality gap in multi-modal contrastive representation
learning. Advances in Neural Information Processing Sys-
tems, 35:17612â€“17625, 2022.
23372
[25] Mattia Litrico, Alessio Del Bue, and Pietro Morerio. Guid-
ing pseudo-labels with uncertainty estimation for source-
free unsupervised domain adaptation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7640â€“7650, 2023.
[26] Hong Liu, Jianmin Wang, and Mingsheng Long. Cycle self-
training for domain adaptation. Advances in Neural Infor-
mation Processing Systems , 34:22968â€“22981, 2021.
[27] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012â€“10022, 2021.
[28] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jor-
dan. Learning transferable features with deep adaptation net-
works. In International conference on machine learning ,
pages 97â€“105. PMLR, 2015.
[29] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I
Jordan. Unsupervised domain adaptation with residual trans-
fer networks. Advances in neural information processing
systems , 29, 2016.
[30] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and
Michael I Jordan. Conditional adversarial domain adapta-
tion. Advances in neural information processing systems ,
31, 2018.
[31] Norman Mu, Alexander Kirillov, David Wagner, and Sain-
ing Xie. Slip: Self-supervision meets language-image pre-
training. In European Conference on Computer Vision , pages
529â€“544. Springer, 2022.
[32] Jaemin Na, Heechul Jung, Hyung Jin Chang, and Wonjun
Hwang. Fixbi: Bridging domain spaces for unsupervised
domain adaptation. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
1094â€“1103, 2021.
[33] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar
Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count
to ten. arXiv preprint arXiv:2302.12066 , 2023.
[34] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman,
Dequan Wang, and Kate Saenko. Visda: The visual domain
adaptation challenge. arXiv preprint arXiv:1710.06924 ,
2017.
[35] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate
Saenko, and Bo Wang. Moment matching for multi-source
domain adaptation. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 1406â€“1415,
2019.
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748â€“8763. PMLR, 2021.
[37] Harsh Rangwani, Sumukh K Aithal, Mayank Mishra, Ar-
ihant Jain, and Venkatesh Babu Radhakrishnan. A closer
look at smoothness in domain adversarial training. In In-
ternational Conference on Machine Learning , pages 18378â€“
18399. PMLR, 2022.[38] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.
Denseclip: Language-guided dense prediction with context-
aware prompting. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
18082â€“18091, 2022.
[39] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tat-
suya Harada. Maximum classifier discrepancy for unsuper-
vised domain adaptation. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
3723â€“3732, 2018.
[40] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Dar-
rell, and Kate Saenko. Semi-supervised domain adaptation
via minimax entropy. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 8050â€“8058,
2019.
[41] Mainak Singha, Harsh Pal, Ankit Jha, and Biplab Banerjee.
Ad-clip: Adapting domains in prompt space using clip. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 4355â€“4364, 2023.
[42] Tao Sun, Cheng Lu, and Haibin Ling. Prior knowledge
guided unsupervised domain adaptation. In European Con-
ference on Computer Vision , pages 639â€“655. Springer, 2022.
[43] Tao Sun, Cheng Lu, Tianshuo Zhang, and Haibin Ling. Safe
self-refinement for transformer-based domain adaptation. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 7191â€“7200, 2022.
[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv Â´e JÂ´egou. Training
data-efficient image transformers & distillation through at-
tention. In International conference on machine learning ,
pages 10347â€“10357. PMLR, 2021.
[45] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of machine learning research , 9
(11), 2008.
[46] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,
and Sethuraman Panchanathan. Deep hashing network for
unsupervised domain adaptation. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 5018â€“5027, 2017.
[47] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang,
Xiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang.
Ufo: A unified transformer for vision-language representa-
tion learning. arXiv preprint arXiv:2111.10023 , 2021.
[48] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. In International Conference on Machine Learn-
ing, pages 23318â€“23340. PMLR, 2022.
[49] Xudong Wang, Zhirong Wu, Long Lian, and Stella X Yu.
Debiased learning from naturally imbalanced pseudo-labels.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 14647â€“14657, 2022.
[50] Haifeng Xia, Taotao Jing, and Zhengming Ding. Maximum
structural generation discrepancy for unsupervised domain
adaptation. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 45(3):3434â€“3445, 2022.
23373
[51] Tongkun Xu, Weihua Chen, WANG Pichao, Fan Wang, Hao
Li, and Rong Jin. Cdtrans: Cross-domain transformer for un-
supervised domain adaptation. In International Conference
on Learning Representations , 2021.
[52] Jinyu Yang, Jingjing Liu, Ning Xu, and Junzhou Huang.
Tvt: Transferable vision transformer for unsupervised do-
main adaptation. In Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision , pages 520â€“
530, 2023.
[53] Shiqi Yang, Joost van de Weijer, Luis Herranz, Shangling
Jui, et al. Exploiting the intrinsic neighborhood structure for
source-free domain adaptation. Advances in neural informa-
tion processing systems , 34:29393â€“29405, 2021.
[54] Shiqi Yang, Yaxing Wang, Joost van de Weijer, Luis Her-
ranz, Shangling Jui, and Jian Yang. Trust your good friends:
Source-free domain adaptation by reciprocal neighborhood
clustering. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence , 2023.
[55] Zhongqi Yue, Hanwang Zhang, and Qianru Sun. Make the
u in uda matter: Invariant consistency learning for unsuper-
vised domain adaptation. arXiv preprint arXiv:2309.12742 ,
2023.
[56] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. arXiv preprint arXiv:1710.09412 , 2017.
[57] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,
Kunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.
Tip-adapter: Training-free clip-adapter for better vision-
language modeling. arXiv preprint arXiv:2111.03930 , 2021.
[58] Wenyu Zhang, Li Shen, and Chuan-Sheng Foo. Rethinking
the role of pre-trained networks in source-free domain adap-
tation. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 18841â€“18851, 2023.
[59] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael
Jordan. Bridging theory and algorithm for domain adapta-
tion. In International conference on machine learning , pages
7404â€“7413. PMLR, 2019.
[60] Yixin Zhang, Zilei Wang, Junjie Li, Jiafan Zhuang, and Zi-
han Lin. Towards effective instance discrimination con-
trastive loss for unsupervised domain adaptation. In Pro-
ceedings of the IEEE/CVF International Conference on
Computer Vision , pages 11388â€“11399, 2023.
[61] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Conditional prompt learning for vision-language mod-
els. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 16816â€“16825,
2022.
[62] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. In-
ternational Journal of Computer Vision , 130(9):2337â€“2348,
2022.
[63] Jinjing Zhu, Haotian Bai, and Lin Wang. Patch-mix trans-
former for unsupervised domain adaptation: A game per-
spective. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3561â€“
3571, 2023.
23374
