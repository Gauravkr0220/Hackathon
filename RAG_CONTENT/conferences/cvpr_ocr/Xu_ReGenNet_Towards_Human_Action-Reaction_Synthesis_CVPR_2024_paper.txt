ReGenNet: Towards Human Action-Reaction Synthesis
Liang Xu1,2Yizhou Zhou3Yichao Yan1â€ Xin Jin2â€ Wenhan Zhu1Fengyun Rao3
Xiaokang Yang1Wenjun Zeng2
1MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China
2Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China
3WeChat, Tencent Inc.
https://liangxuy.github.io/ReGenNet/
Abstract
Humans constantly interact with their surrounding envi-
ronments. Current human-centric generative models mainly
focus on synthesizing humans plausibly interacting with
static scenes and objects, while the dynamic human action-
reaction synthesis for ubiquitous causal human-human in-
teractions is less explored. Human-human interactions
can be regarded as asymmetric with actors and reactors
in atomic interaction periods. In this paper, we compre-
hensively analyze the asymmetric, dynamic, synchronous,
and detailed nature of human-human interactions and pro-
pose the first multi-setting human action-reaction synthe-
sis benchmark to generate human reactions conditioned on
given human actions. To begin with, we propose to an-
notate the actor-reactor order of the interaction sequences
for the NTU120, InterHuman, and Chi3D datasets. Based
on them, a diffusion-based generative model with a Trans-
former decoder architecture called ReGenNet together with
an explicit distance-based interaction loss is proposed to
predict human reactions in an online manner, where the fu-
ture states of actors are unavailable to reactors. Quantita-
tive and qualitative results show that our method can gener-
ate instant and plausible human reactions compared to the
baselines, and can generalize to unseen actor motions and
viewpoint changes.
1. Introduction
Human-centric generative models have been widely studied
with numerous applications. Currently, there exists substan-
tial progress on generative models to synthesize how digital
humans actively interact with the environments with phys-
ical and semantic plausibility, e.g., conditioned on a given
scene [26, 27, 70, 82â€“84] and object [63, 64, 71, 85]. How-
ever, for human-human interactions, a man could be active
orpassive in atomic interaction periods. Existing works
â€ Corresponding authorsfor human motion generation mainly treat the actors and
reactors equally or limited on single human motion gen-
eration [44, 58, 72], while neglecting the reaction genera-
tion problem for ubiquitous human-human interactions (see
Fig. 1). In this paper, we focus on generative models for
human action-reaction synthesis, i.e., generating human re-
actions given the action sequence of another as conditions.
We believe this task will contribute to many applications in
AR/VR, games, human-robot interaction, and embodied AI.
Modeling human-human interactions is a challenging
task with the following features: 1) Asymmetric ,i.e., the
actor and reactor play asymmetric roles during a causal in-
teraction, where one person acts, and the other reacts [79];
2)Dynamic ,i.e., during the interaction period, the two peo-
ple constantly wave their body parts, move close/away, and
change relative orientations, spatially and temporally; 3)
Synchronous ,i.e., typically, one person responds instantly
with others such as an immediate evasion when someone
throws a punch, thus the online generation is required; 4)
Detailed ,i.e., the interaction between humans involves not
only coarse body movements together with relative position
changes but also local hand gestures and even facial expres-
sions. Thus, it is desirable to design a generative model that
simultaneously considers the above characteristics.
Directly applying previous human-centric generative
models [64, 82, 83, 85] for human action-reaction synthe-
sis is impractical, because existing models typically con-
sider static scenes or objects, yet dynamic humans are
more complicated. Moreover, online generation is also
not required for human scene/object interaction scenar-
ios, yet significant for action-reaction synthesis. On the
other hand, recent years have witnessed the rapid devel-
opment of single human motion generation conditioned on
action categories [51, 67], text descriptions [21, 37, 52], au-
dios [3, 25, 40â€“42] or sparse tracking signals [2, 14, 35].
However, very few works [44, 61, 62, 72, 73] have been
proposed to generate multi-person interactions, yet treating
the actor-reactor equally [44, 72] or focus on specific action
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1759
ReGenNet
ğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$Human MotionGenerated Human ReactionFigure 1. Illustration of our proposed ReGenNet ,i.e., given a human motion sequence and generate the plausible human reactions, which
will have broad applications in AR/VR and games.
categories, such as â€œmartial artsâ€ [62]. The sparse skeleton
joints or SMPL body models [46] are widely adopted, while
greatly limiting the fineness and details of hands-involved
interactions such as â€œplaying finger guessingâ€, letting alone
thesynchronous andasymmetric natures. To the best of
our knowledge, no previous works have been proposed to
deal with all the aforementioned patterns of human-human
interactions. There are no such human-human interaction
datasets with actor-reactor annotations.
In this paper, we comprehensively consider the intrinsic
features of human-human interactions and propose the first
human action-reaction synthesis benchmark with the fol-
lowing designs: 1) We adopt the SMPL-X [50] body model
as our data representation because it contains detailed ar-
ticulated hand poses. In terms of the datasets, we choose
Chi3D [15] with SMPL-X annotations from the body mark-
ers; we also extend the widely used NTU120 dataset [45]
to SMPL-X version by a state-of-the-art pose estimation
method [80]; we also adopt the human-human interaction
MoCap dataset InterHuman [44] for its accurate motion se-
quences. 2) For the absence of asymmetry nature in cur-
rent interaction datasets, we annotate the actors and reac-
tors of the above three datasets. Based on these annota-
tions, we propose, to our best knowledge, the first multi-
setting human action-reaction synthesis benchmark aiming
to generate physically and semantically plausible human re-
actions conditioned on a given personâ€™s action sequence.
3) To generate instant and synchronous human reactions,
we need to design an online model, i.e., future human mo-
tion is unavailable for the synthesis at the current moment.
We adopt a diffusion model together with the Transformer
architecture to model the spatiotemporal interactions, and
we choose the Transformer-decoder for its leftward prop-
erty via the masked multi-head attention and inference in an
auto-regressive manner. 4) To handle the highly dynamic
human-human interactions, we draw inspiration from the
previous human scene/object interaction counterparts which
model the contact/interaction using distance-based repre-
sentations [64, 82]. We thus design interaction losses that
explicitly measure the relative distances of the interacted
spatiotemporal body poses, orientations, and translations.Considering that in practical applications, the intention of
the actor could be agnostic to reactors, we also train our
model in an unconstrained fashion [53, 67]. With the above
designs, we name our reaction generation model as ReGen-
Net. Extensive experiments show that ReGenNet can syn-
thesize realistic human reactions with the lowest time delay
compared to the baselines, and can generalize to unseen ac-
tor motions and viewpoint changes. Our model is modular
and flexible to be trimmed for other practical applications
such as multi-person interaction generation tasks.
Our contributions can be summarized as follows. We
comprehensively analyze the asymmetric, dynamic, syn-
chronous, and detailed nature of human-human interactions.
Based on these analyses, we propose the first multi-setting
human action-reaction synthesis benchmark with three ded-
icated annotated datasets. To address this task, we present
ReGenNet , a diffusion-based generative model to synthe-
size plausible and instant human reactions. Our benchmark,
data, models, and code will be made publicly available.
2. Related Work
Human-scene/object interaction. Synthesizing human-
scene/object interactions is critical for games and AR/VR
applications [76â€“78]. The goal is to fit the human body
with scenes/objects as contexts , so as to plausibly navi-
gate/interact in the scenes or manipulate the objects with
geometric and semantic constraints. For human-scene in-
teractions , [18, 24, 27, 33, 38, 43, 56, 57, 82, 83] can gen-
erate static interactions with unseen environments. Recent
works [26, 70, 84] extended to produce dynamic human-
scene interactions, which is equivalent to generating motion
sequences with scene contexts. For human-object interac-
tions , earlier works [19, 36] focused on generating hand-
object interactions. [7, 32, 63] built databases of whole-
body interactions with daily objects and promoted the re-
search works on synthesizing whole-body manipulations
with objects [63, 64, 71, 85].
The core of these human-centric generative models
lies in understanding the semantics and affordances of
scenes/objects, together with training a conditional gener-
ative model based on the scene/object priors. Our proposed
1760
Dataset Year Verbs Motions Modality Asymmetry
SBU [79] 2012 8 300 Skel. âœ—
ShakeFive2 [69] 2016 8 153 Skel. âœ—
K3HI [6] 2020 8 312 Skel. âœ—
NTU120 [45] 2019 26 8,276 Skel. âœ—
You2Me [49] 2020 4 42 Skel. âœ—
Chi3D [15] 2020 8 373 SMPL-X âœ—
InterHuman [44] 2023 - 6,022 SMPL âœ—
Chi3D-AS(Ours) 2023 8 373 SMPL-X âœ“
InterHuman-AS(Ours) 2023 - 6,022 SMPL âœ“
NTU120-AS(Ours) 2023 26 8,118 SMPL-X âœ“
Table 1. Human-human interaction datasets . Skel. denotes
skeleton and AS denotes asymmetry.
benchmark of human action-reaction synthesis, where the
motion of the actor can also be viewed as the contexts , is
also significant yet not explored to the best of our knowl-
edge. The highly dynamic human motion is also more com-
plicated than static scenes and objects.
Human motion generation. The goal is to generate hu-
man motion conditioned on different guidances. Early
works [5, 16, 23, 28, 47, 48, 66] focused on the task of
future motion prediction, i.e., to predict the motion of fu-
ture frames given past motion as guidance. Besides, motion
synthesis from high-level semantic signals such as action
labels [9, 10, 20, 51, 67, 72, 75], text [1, 13, 21, 37, 52, 81],
music [4, 40â€“42], speech [3, 25] have emerged in recent
years. However, [53, 67] also proved that human motion
generation can also be learned without conditions in an un-
supervised manner. Furthermore, human-human interaction
synthesis has also been noticed [44, 58, 61, 62, 72]. How-
ever, these works treated the actor-reactor equally [44, 72]
or focused on specific action categories [62] in graphics.
Another line of research in AR/VR scenarios reverted the
full-body poses from the sparse tracking signal of head and
wrists [2, 8, 14, 34, 35], facilitating real-world applications.
The key component of these works is to learn generative
models such as GANs [17, 72, 75], V AEs [9, 20, 39, 51],
flow-based models [2, 55] and diffusion models [13, 31,
67, 81]. In this paper, we also adopt the diffusion models
for high-quality synthesis. However, most previous works
neglected the asymmetry of causal human-human interac-
tions. Concurrent works [12, 65] target at human reac-
tion generation, yet [12] adopts very sparse skeleton rep-
resentations and [65] only handles the â€œofflineâ€ and â€œun-
constrainedâ€ setting of human reaction generation without
generating instant and intention agnostic reactions.
Human-human interaction dataset. Human-human in-
teractions are indispensable components in our daily
lives. Many multi-person interaction datasets such as
UMPM [68], SBU [79], ShakeFive2 [69], K3HI [6],
You2Me [49], Chi3d [15], NTU120 [45], ExPI [22], In-
terHuman [44] have been produced with various sizes and
modalities as in Tab. 1. Especially, NTU120 [45] is a large-scale human motion dataset with 26 interactive action cat-
egories and concurrently, InterHuman [44] brings the cur-
rently largest multi-human interaction dataset with text de-
scription annotations.
However, all these previous datasets overlooked the
asymmetry property of causal human-human interactions.
Thus, we propose to annotate the actor-reactor order of the
Chi3D, NTU120 and InterHuman datasets. We also extend
the NTU120 dataset to the SMPL-X version to better de-
scribe the fine-grained interaction patterns.
3. Human Action-Reaction Synthesis
3.1. Modeling setup
Dataset Formulation. In this work, we tackle the prob-
lem of fine-grained human action-reaction synthesis. How-
ever, we notice that all the previous multi-person interac-
tion datasets ignored the asymmetry property of causal re-
lationships (see Tab. 1). Thus, we choose three datasets, i.e.,
NTU120 [45], InterHuman [44] and Chi3D [15], and anno-
tate the actor-reactor order of each interaction sequence. We
also extend the NTU120 dataset to SMPL-X representation
by a pose estimation method [80]. We name the datasets as
NTU120-AS, InterHuman-AS and Chi3D-AS, where â€œASâ€
denotes asymmetry. For the details of the asymmetric ac-
tion definition and the labeling process, please refer to the
supplementary materials.
Problem Formulation. In the setting of human action-
reaction synthesis, our goal is to generate the reaction con-
ditioned on an arbitrary action. Formally, we denote the re-
action as x1:N={xi}N
i=1and the action as y1:N={yi}N
i=1
of duration N. The intention acan be a signal of action la-
bel, text, and audio to dictate the interaction, which could
be optional for intention-agnostic scenarios.
To enhance the representational power of human-human
interactions, we adopt SMPL-X [50] human model to rep-
resent the human motion sequence. Thus, the reaction can
be represented as xi= [Î¸x
i,qx
i,Î³x
i]where Î¸x
iâˆˆR3K,
qx
iâˆˆR3,Î³x
iâˆˆR3are the pose parameters, global ori-
entation and the root translation of the person, respectively.
K= 54 is the number of body joints together with the jaw,
eyeballs, and fingers.
Motion Diffusion Model. Diffusion models [31, 59] have
been proven to serve as a powerful generative model for
human motion synthesis [13, 14, 67, 81], which can be re-
garded as learning a Markov chain-based progressive nois-
ing and denoising of human motions. Given the reaction
x1:N
0sampled from the real interaction data distribution, the
noising process can be written as
q(x1:N
t|x1:N
tâˆ’1) =N(x1:N
t;âˆšÎ±tx1:N
tâˆ’1,(1âˆ’Î±t)I),(1)
where tis the timestep of the diffusion process, Î±tâˆˆ(0,1)
is a constant hyper-parameter and Iis the identity matrix.
With sufficiently large T,Î±tbecomes small enough and
1761
TransformerDecoderUnitsLinear
...ğ‘¡ğ‘LinearMLPğ‘§!"â€œPush somebodyâ€+ActorReactorAddConcatDiffusedActionCategoryActorReactorFusedPE
â„“#$%
LinearForwardDiffusion
...
...
...Linear
......TimeSpatial
TimestepDirectionalAttention Mask(a) ReGenNetFramework(b) Decoder UnitMulti-HeadAttentionMaskedMulti-HeadAttentionAdd & NormFeedForward
Add & NormAdd & Norm
Attn.Mask
actor conditionedğ‘§"&:(Fusedğ‘§!"...
ğ‘¦&:(ğ‘¥&:(ğ‘¥')Figure 2. The architecture of our proposed ReGenNet which is formulated in a diffusion-based framework with Transformer Decoder
Units. The gray panel of (a) illustrates the whole diffusion model with the â€œForward Diffusionâ€ process and a stack of â„“decâ€œTransformer
Decoder Unitsâ€ as the denoising process, the blue panel of (a) is the actor feature as the condition. (b) shows the details of the â€œTransformer
Decoder Unitsâ€ with directional attention mask for online reaction synthesis.
x1:N
Tcan be approximated as a Gaussian noise N(0, I).
To generate the high-fidelity reaction, we need to reverse
denoise the xTback to x0forTtimesteps. In our set-
ting, the reverse diffusion process is conditionally formu-
lated as p(x1:N
tâˆ’1|x1:N
t, y1:N, a). We follow [14, 54, 67] to
use a neural network Fto directly predict the clean body
poses instead of predicting the residual noise, i.e.,Ë†x0=
F(xt, y1:N, t, a), so that we can add geometric losses di-
rectly on the predicted Ë†x0.Fcan be implemented by Trans-
formers or MLP networks for different applications. The
training objective of the diffusion model is formulated as
Ldm=Ex0âˆ¼q(x0),tâˆ¼[1,T][âˆ¥x0âˆ’F(xt, y1:N, t, a)âˆ¥2
2].(2)
3.2. Reaction Generation Network
In this section, we introduce our holistic diffusion-based re-
action generation framework as shown in Fig. 2, which con-
sists of a diffusion model Mand a Transformer decoder F.
Given a coupled action-reaction pair and the optional ac-
tion category (dotted lines in Fig. 2) <x1:N,y1:N,a>,y1:N
andaserve as conditions and x1:Nis the reaction to gener-
ate. For a sampled noising timestep t, we add noise to x1:N
through the forward diffusion process as Eq. (1) to produce
the noised x1:N
t. Then both the x1:N
tand the y1:Nare lin-
early projected to obtain the latent features through an FC
layer to dimension d, and concatenated together through the
feature dimension. We also apply another FC layer to fuse
the concatenated features and reduce the dimension to pro-
duce the final tokens z1:N
t. Experimental results show that
the conditioning scheme by concatenation is simple yet ef-
fective. The noising timestep tand the optional action label
aare all projected to dimension dby separate feed-forward
networks and then summed up to obtain the token zat.We implement Fby stacking â„“declayers of Transformer
decoder units to prevent future information leakage via the
masked multi-head attention for online generation. Ftakes
zatas input tokens and z1:N
tsummed with a standard po-
sitional embedding as output tokens together with a direc-
tional attention mask, which is critical to prevent the model
from seeing future actions at the current moment. The out-
put of the decoders is then projected back as the predicted
clean body poses Ë†x1:N
0. During inference time, we generate
human reactions in an auto-regressive manner.
Explicit interaction loss. Inspired by previous genera-
tive models of human scene/object interaction counterparts,
which designed delicate distance-based representations to
model interactions, we design explicit interaction losses to
measure the relative distances of the interacted spatiotem-
poral body pose Î¸, orientation qand translation Î³as
Î¸xâ†’y=FK(Î¸y)âˆ’FK(Î¸x);
qxâ†’y=RM(qy)âŠ¤Â·RM(qx);
Î³xâ†’y=Î³yâˆ’Î³x,(3)
where FK(Â·)denotes the forward kinematic function to
convert the rotation pose into joint positions, and RM(Â·)
converts the rotation poses to rotation matrices. The inter-
action loss is formulated as
Linter=1
NNX
i=1âˆ¥Î¸x0â†’yâˆ’Î¸Ë†x0â†’yâˆ¥2
2
+NX
i=1âˆ¥qx0â†’yâˆ’qË†x0â†’yâˆ¥2
2+NX
i=1âˆ¥Î³x0â†’yâˆ’Î³Ë†x0â†’yâˆ¥2
2
.
(4)
1762
MethodTrain conditioned Test conditioned
FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’ FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.09Â±0.001.000Â±0.000010.54Â±0.0626.71Â±0.620.09Â±0.000.867Â±0.000213.06Â±0.0925.03Â±0.23
cV AE [39] 77.52Â±7.250.899Â±0.000210.10Â±0.0219.38Â±0.1670.10Â±3.420.724Â±0.000211.14Â±0.0418.4Â±0.26
AGRoL [14] 38.04Â±1.450.932Â±0.000110.95Â±0.0721.44Â±0.3444.94Â±2.460.680Â±0.000112.51Â±0.0919.73Â±0.17
MDM [67] 40.13Â±3.650.955Â±0.000110.53Â±0.0421.15Â±0.2654.54Â±3.940.704Â±0.000311.98Â±0.0719.45Â±0.20
MDM-GRU [67] 5.31Â±0.180.993Â±0.000011.03Â±0.0625.04Â±0.3624.25Â±1.390.720Â±0.000213.43Â±0.0922.24Â±0.29
ReGenNet 0.90Â±0.011.000Â±0.000010.69Â±0.0526.25Â±0.3511.00Â±0.740.749Â±0.000213.80Â±0.1622.90Â±0.14
Table 2. Comparison to state-of-the-arts on the online, unconstrained setting for human action-reaction synthesis on NTU120-AS. Â±
indicates 95% confidence interval, â†’means that closer to Real is better. Bold indicates best result and underline indicates second best.
MethodTrain conditioned Test conditioned
FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’ FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.19Â±0.011.000Â±0.00005.36Â±0.0820.06Â±0.780.75Â±0.180.691Â±0.00937.15Â±1.2712.94Â±0.96
cV AE [39] 25.45Â±13.90.843Â±0.00059.02Â±0.3013.82Â±0.6417.33Â±17.140.552Â±0.00248.20Â±0.5711.44Â±0.35
AGRoL [14] 47.73Â±5.950.975Â±0.00017.43Â±0.2115.59Â±0.4964.83Â±277.80.644Â±0.00397.00Â±0.9511.33Â±0.65
MDM [67] 15.96Â±1.921.000Â±0.00005.98Â±0.1516.43Â±0.5018.40Â±7.950.647Â±0.00355.89Â±0.3310.96Â±0.27
MDM-GRU [67] 4.96Â±0.970.995Â±0.00006.36Â±0.2217.79Â±0.5818.63Â±25.870.574Â±0.00466.20Â±0.2410.49Â±0.32
ReGenNet 0.27Â±0.031.000Â±0.00005.39Â±0.1220.24Â±0.6413.76Â±4.780.601Â±0.00406.35Â±0.2412.02Â±0.33
Table 3. Comparison to state-of-the-arts on the online, unconstrained setting for human action-reaction synthesis on Chi3D-AS. Â±
indicates 95% confidence interval, â†’means that closer to Real is better. Bold indicates best result and underline indicates second best.
MethodsR Precision
(Top 3) â†‘FIDâ†“ MM Dist â†“ Diversity â†’ MModality â†‘
Real 0.722Â±0.0040.002Â±0.00023.503Â±0.0115.390Â±0.058-
T2M [21] 0.224Â±0.00332.482Â±0.09757.299Â±0.0164.350Â±0.0730.719Â±0.041
MDM [67] 0.370Â±0.0063.397Â±0.03528.640Â±0.0654.780Â±0.1172.288Â±0.039
MDM-GRU [67] 0.328Â±0.0126.397Â±0.21408.884Â±0.0404.851Â±0.0812.076Â±0.040
RAIG [65] 0.363Â±0.0082.915Â±0.02927.294Â±0.0274.736Â±0.0992.203Â±0.049
InterGen [44] 0.374Â±0.00513.237Â±0.035210.929Â±0.0264.376Â±0.0422.793Â±0.014
ReGenNet 0.407Â±0.0032.265Â±0.09696.860Â±0.0.0405.214Â±0.1392.391Â±0.023
Table 4. Comparison to state-of-the-arts on the online, un-
constrained setting for human action-reaction synthesis on the
InterHuman-AS dataset.
Method FID â†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.10Â±0.000.849Â±0.000212.98Â±0.1122.77Â±0.35
cV AE [39] 63.23Â±7.740.708Â±0.000411.15Â±0.0317.34Â±0.23
AGRoL [14] 35.83Â±1.130.592Â±0.000312.42Â±0.0618.67Â±0.21
MDM [67] 36.75Â±2.870.692Â±0.000411.73Â±0.0518.24Â±0.21
MDM-GRU [67] 25.57Â±1.710.636Â±0.000513.20Â±0.0920.49Â±0.33
ReGenNet 8.16Â±0.420.713Â±0.000213.88Â±0.1321.63Â±0.41
Table 5. Generalization results to different viewpoints on the
online, unconstrained setting on the NTU120-AS dataset.
Overall, the training loss is Lall=Ldm+Î»interÂ· Linter ,
andÎ»inter is the loss weight.
Customization Support. We tackle the most challenging
setting of online action-reaction synthesis without seeing
the future motions, even being agnostic to the actorâ€™s in-
tentions. There exist other scenarios that relax these restric-
tions, such as offline generative models if the time delay can
be tolerated. Our proposed ReGenNet is modular, flexible,
and can be trimmed for other practical usages. 1) The inten-tion branch can be enabled if the actorâ€™s intention is avail-
able to the reactor, or removed otherwise; 2) The directional
attention mask can be disabled for offline models.
4. Experiment
First, we give the definitions of our experiment settings.
We name the setting of instant human action-reaction syn-
thesis without seeing the future motions as online models,
and the opposite is offline models to relax the synchronic-
ity requirements. We also define unconstrained andcon-
strained settings depending on whether the intention of the
actor is visible to the reactor. We mainly focus on the chal-
lenging online, unconstrained setting due to its potential
for practical applications.
4.1. Datasets and Evaluation Metrics
We evaluate our model on our proposed NTU120-AS,
InterHuman-AS and Chi3D-AS datasets with SMPL-X [50]
body models and actor-reactor annotations. NTU120-AS
includes 8,118 human interaction sequences captured by 3
cameras of 26 action categories. We choose camera 1 and
follow the cross-subject protocol where half of the subjects
are used for training and the remaining ones for testing. We
also evaluate our model on camera 2 to examine the gen-
eralization ability for viewpoint changes. InterHuman-AS
presents 6,022 interaction sequences captured by a multi-
view motion capture studio. Chi3D-AS contains 373 inter-
action sequences and we randomly split the training and test
sets with a ratio of 4:1. In all our experiments, we adopt the
1763
Class SettingsTrain conditioned Test conditioned
FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’ FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.094Â±0.00031.000Â±0.0010.542Â±0.063226.709Â±0.61930.085Â±0.00030.867Â±0.000213.063Â±0.090825.032Â±0.2332
Modules1) Add 0.975Â±0.01861.000Â±0.0010.685Â±0.049326.272Â±0.366312.828Â±0.93350.747Â±0.000313.721Â±0.151322.771Â±0.1921
2) w.o. PE 0.892Â±0.01301.000Â±0.0010.717Â±0.056726.345Â±0.343212.916Â±1.28020.747Â±0.000113.775Â±0.152622.752Â±0.1330
Loss w.o. Linter 1.960Â±0.06210.999Â±0.0010.778Â±0.059726.024Â±0.322312.146Â±0.90440.751Â±0.000213.727Â±0.180822.846Â±0.1606
Num. of â„“dec2 11.445Â±0.87380.993Â±0.0010.972Â±0.054324.815Â±0.376929.015Â±3.77510.744Â±0.000213.107Â±0.127321.134Â±0.1150
4 3.218Â±0.11200.999Â±0.0010.856Â±0.052125.728Â±0.379018.148Â±1.74130.743Â±0.000213.418Â±0.104821.813Â±0.1671
12 1.967Â±0.02871.000Â±0.0010.752Â±0.053326.038Â±0.337013.348Â±0.74200.725Â±0.000214.090Â±0.162922.906Â±0.1197
16 2.382Â±0.05111.000Â±0.0010.757Â±0.050925.908Â±0.337011.089Â±1.39020.728Â±0.000214.173Â±0.132723.069Â±0.2492
Final Version ReGenNet 0.896Â±0.01091.000Â±0.0010.694Â±0.054226.247Â±0.347610.999Â±0.74250.749Â±0.000213.797Â±0.159322.902Â±0.1353
Table 6. Ablation studies on the online, unconstrained setting on the NTU120-AS dataset.
TimestepsTrain conditioned Test conditionedLatency(ms)
FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’ FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.094Â±0.00031.000Â±0.0010.542Â±0.063226.709Â±0.61930.085Â±0.00030.867Â±0.000213.063Â±0.090825.032Â±0.2332-
2 0.912Â±0.01101.000Â±0.0010.688Â±0.052926.249Â±0.350412.132Â±0.83010.751Â±0.000213.707Â±0.168322.797Â±0.11720.33
5 0.896Â±0.01091.000Â±0.0010.694Â±0.054226.247Â±0.347610.999Â±0.74250.749Â±0.000213.797Â±0.159322.902Â±0.13530.76
10 0.903Â±0.01081.000Â±0.0010.691Â±0.053626.250Â±0.350711.466Â±0.81990.749Â±0.000213.762Â±0.164722.855Â±0.12641.58
100 0.897Â±0.01091.000Â±0.0010.692Â±0.053726.248Â±0.349111.082Â±0.74400.748Â±0.000213.794Â±0.158122.890Â±0.122315.17
1000 0.908Â±0.01091.000Â±0.0010.692Â±0.054026.247Â±0.350211.719Â±0.80590.750Â±0.000113.738Â±0.166522.830Â±0.1220150.69
Table 7. Ablation Studies of the number of DDIM [60] sampling timesteps on the online, unconstrained setting on NTU120-AS.
Method FID â†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.09Â±0.000.867Â±0.000213.06Â±0.0925.03Â±0.23
Random 12.69Â±1.220.656Â±0.000213.97Â±0.0822.19Â±0.34
ReGenNet 11.00Â±0.740.749Â±0.000213.80Â±0.1622.90Â±0.14
Table 8. Ablation studies on the necessity of the explicit actor-
reactor order annotations on the NTU120-AS dataset.
6D rotation representation [86] as in [51].
For evaluation metrics, we follow prior works in hu-
man motion generation [20, 51, 67] and use Frechet Incep-
tion Distance (FID) [29], action recognition accuracy, diver-
sity, and multi-modality for quantitative evaluations. For all
these metrics, we train the action recognition model [74] to
extract motion features for calculating FID, diversity, and
multi-modality, or directly compute the action recognition
accuracy. Following [72], the root translation is considered
for the action recognition model since relative root trans-
lations matter for person-person interactions. We generate
the reactions by sampling actor motions from the training
or test sets as train-conditioned andtest-conditioned , re-
spectively. Evaluation results for test conditioned examine
the capability of reaction generation for unseen actor mo-
tions. We generate 1,000 samples 20 times with different
random seeds and report the average with the confidence
interval at 95% as previous works [20, 51, 67].
4.2. Implementation Details
We train our diffusion-based model with T= 1000 nois-
ing timesteps and a cosine noise schedule in a classifier-free manner [30]. The batch size is set as 64 for NTU120-
AS, InterHuman-AS and 16 for Chi3D-AS, the number of
decoder layers is 8 and the latent dimension of the Trans-
former tokens is 512. For the loss weights, we set Î»inter =
1. Each model is trained for 500K steps on 4 NVIDIA
A100 GPUs within 20 hours. During inference, we adopt
the DDIM [60] strategy to accelerate. Unless specified, we
run 5 sampling steps for all the diffusion-based models.
4.3. Comparison to State-of-the-arts
We evaluate our model on the most challenging online, un-
constrained setting, i.e., to generate instant reactions with-
out knowing the intention of the actors. Since there is
no previous work tackling the conditional human action-
reaction synthesis setting, we adopt and re-implement some
previous works for human-centric generative models as
baselines 1) cV AE [39], which is widely adopted in pre-
vious human-scene/object interaction generative models;
2) MDM [67], the state-of-the-art diffusion-based method
for human motion generation and MDM-GRU [67], a
diffusion-based model with a GRU [11] backbone; 3)
AGRoL [14], the state-of-the-art method to generate full-
body motions from sparse tracking signals, implemented
by diffusion models with MLPs as backbones. At the in-
ference stage, we employ a sliding-window strategy to it-
eratively generate the reactions for the online setting. For
a fair comparison, we also run 5 DDIM sampling steps for
AGRoL [14], MDM [67] and MDM-GRU [67]. For the
text-conditioned setting, we adopt T2M [21], MDM [67],
MDM-GRU [67], RAIG [65] and InterGen [44] baselines.
1764
MethodTrain conditioned Test conditioned
FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’ FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.09Â±0.001.000Â±0.000010.54Â±0.0626.71Â±0.620.09Â±0.000.867Â±0.000213.06Â±0.0925.03Â±0.23
cV AE [39] 81.62Â±14.430.902Â±0.000210.10Â±0.0219.38Â±0.1674.73Â±4.860.760Â±0.000211.14Â±0.0418.40Â±0.26
AGRoL [14] 10.87Â±1.070.983Â±0.000011.45Â±0.0723.80Â±0.4216.55Â±1.410.716Â±0.000213.84Â±0.1021.73Â±0.20
MDM [67] 1.61Â±0.020.999Â±0.000010.76Â±0.0626.02Â±0.307.49Â±0.620.775Â±0.000313.67Â±0.1824.14Â±0.29
MDM-GRU [67] 5.31Â±0.180.993Â±0.000011.03Â±0.0625.04Â±0.3624.25Â±1.390.720Â±0.000213.43Â±0.0922.24Â±0.29
ReGenNet 0.64Â±0.011.000Â±0.000010.70Â±0.0526.36Â±0.386.19Â±0.330.772Â±0.000314.03Â±0.0925.21Â±0.34
Table 9. Results on the offline , unconstrained setting on NTU120-AS. Bold indicates best result and underline indicates second best.
MethodTrain-conditioned Test-conditioned
FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’ FIDâ†“ Acc.â†‘ Div.â†’ Multimod. â†’
Real 0.09Â±0.001.000Â±0.000010.54Â±0.0626.71Â±0.620.09Â±0.000.867Â±0.000213.06Â±0.0925.03Â±0.23
ReGenNet-unconstrained 0.90Â±0.011.000Â±0.000010.69Â±0.0526.25Â±0.3511.00Â±0.740.749Â±0.000213.80Â±0.1622.90Â±0.14
ReGenNet-constrained 0.86Â±0.011.000Â±0.000010.69Â±0.0626.26Â±0.355.89Â±0.240.902Â±0.000111.90Â±0.0625.08Â±0.20
Table 10. Results on the online, constrained setting on NTU120-AS. Bold indicates best result and underline indicates second best.
Tab. 2, Tab. 3 and show the comparisons on the NTU120-
AS and Chi3D-AS dataset, respectively. For the two
datasets, ReGenNet notably outperforms baselines on the
FID metric, which shows that our generation is closest to the
real human reaction distributions. For the train-conditioned
setting where the actor motions are sampled from the train-
ing set, our method yields state-of-the-art performance
for FID, action recognition accuracy, diversity, and multi-
modality on NTU120-AS and Chi3D-AS datasets (except
for second best for the diversity of NTU120-AS), which
shows that our model learns the reaction patterns well.
For the test-conditioned setting, our method achieves the
best FID, action recognition accuracy, and multi-modality
for the NTU120-AS dataset and the best FID, and multi-
modality for the Chi3D-AS dataset. This verifies that our
method can generalize well to unseen actor motions. Due
to the limited scale of the Chi3D-AS test set, there might be
some fluctuations in the experimental results. Tab. 4 shows
the comparisons on the InterHuman-AS dataset, our method
also yields the best results over the baselines.
4.4. Generalization Experiments
To verify the generalization ability of our model to view-
point changes, we train the generative models on camera 1
and inference on camera 2 of the NTU120-AS dataset. As
reported in Tab. 5, ReGenNet achieves the best FID score,
action recognition accuracy, and multi-modality than other
baselines, which shows the robustness of our method.
4.5. Ablation Study
Basic Module Designs. As illustrated in Sec. 3.2, a sim-
ple yet effective conditioning scheme is to concatenate the
actor and reactor motion features as the input to the Trans-former decoders. We also tried to add these features to-
gether and the results are listed on the â€œModules - 1) Addâ€
setting in Tab. 6. However, the results become inferior in
terms of the FID and action recognition accuracy metrics.
We also verify that adding positional embedding is effective
to bring lower FID scores for the test-conditioned setting.
Explicit Interaction Loss. We design distance-based ex-
plicit interaction loss based on the relative interacted body
pose, head orientation, and positions as Eq. (4). From the
â€œLoss - w.o Linter â€ setting in Tab. 6, we derive that the ex-
plicit interaction loss contributes to lower FIDs with minor
action recognition accuracy drop for the test conditioned
setting ( 0.751â†’0.749). This confirms that the explicit
interaction loss enhances the reaction generation quality.
Layer of Decoders. We set the number of Transformer
decoder units layers â„“dec= 8 in our ultimate model, and
we also ablate â„“dec= 2,4,12,16. As represented on the
â€œNum. of â„“decâ€ setting of Tab. 6, we can observe that set-
tingâ„“dec= 8obtains the best overall performance.
Number of DDIM sampling timesteps. We report the
reaction generation results and the latency for per frame
reaction generation under different DDIM [60] sampling
timesteps, i.e., 2, 5, 10, 10, 100, and 1000. We take our
trained ReGenNet with 1,000 sampling timesteps and in-
ference with a subset of diffusion steps. From Tab. 7, we
can see that 5 DDIM timesteps inference yields the best
FID score with low latency. Thus, we adopt the DDIM-5
inference for all the reported results.
Necessity of actor-reactor annotations. To verify the ne-
cessity of explicitly annotating the actor-reactor orders, we
ablate it by randomly shuffling the actor-reactor roles in an
unsupervised way for human reaction generation. The re-
sults depicted in Tab. 8 show that random shuffling brings
1765
Chi3D-train: Handshake
Chi3D-test: Hitğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%ğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%
NTU120-train: Finger Guessingğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%
ğ‘¡!ğ‘¡"ğ‘¡#ğ‘¡$ğ‘¡%NTU120-test: Carry sth. with othersFigure 3. Visualization of human action-reaction synthesis results. Blue for actors and Orange for reactors.
inferior performance than ours. One possible explanation
could be that the action pattern and reaction pattern differ
a lot, yet directly randomly shuffling the actor-reactor order
may distract the reaction generation model.
4.6. Extension to other settings
Our proposed ReGenNet is modular and can be trimmed
for other scenarios. We show the experimental results of
our model adapting to offline (see Tab. 9) and constrained
(see Tab. 10) settings. For the offline setting, we re-
place the Transformer decoder units equipped with attention
masks with an 8-layer Transformer encoder architecture. As
shown in Tab. 9, our model achieves superior performance
on most of the metrics, which shows the effectiveness of our
method and flexibility for adaptation. For the constrained
setting where the actorâ€™s intention is available to the reactor,
we embed the action label ainto ReGenNet as described
in Sec. 3.2. As expected, the constrained setting achieves
superior performance than the unconstrained setting since
the action serves as a strong hint to generate the reactions.
4.7. Qualitative evaluation.
We visualize some human reaction generation examples
in Fig. 3, sampled from the train/test sets of Chi3D-AS and
NTU120-AS datasets. The visualization results show that
ReGenNet can synthesize human reactions with plausible
1) relative head orientations, i.e., handshaking face-to-face;
2) position change, i.e., step back for hitting; 3) body part
movements and hand gestures, i.e., realistic hand pose; and
4) semantics, i.e., the action between the actor and reactor
is visually reasonable. For more visualizations and videos
of the generated human reactions, please refer to the Suppl.5. Conclusion and Limitations
In this paper, we propose the first multi-setting human
action-reaction synthesis benchmark with a comprehen-
sive analysis of the asymmetric, dynamic, synchronous,
and detailed characteristics of human-human interactions.
For the first time, we annotate the actor-reactor order for
the NTU120, Chi3D, and InterHuman datasets. We pro-
pose ReGenNet , a conditional diffusion model with a
Transformer decoder architecture combined with an ex-
plicit distance-based interaction loss. Extensive experi-
ments demonstrated that ReGenNet can generate instant
and realistic reactions, even being agnostic to the actorâ€™s in-
tentions. We also verify that our method is generalizable to
viewpoint changes. Furthermore, experimental results show
that ReGenNet is modular and can be trimmed for different
settings of conditional action-reaction generations.
Limitations. The setup of our benchmark and datasets still
has some limitations: 1) Setup: Real-world human-human
interactions are much more complicated with longer du-
rations, interaction patterns, and actor-reactor transitions.
Currently, we only focus on the human action-reaction syn-
thesis within atomic action periods, which could be im-
proved in the future; 2) Datasets: The human motion of the
NTU120 dataset extracted by algorithms is noisy, even with
human-human inter-penetrations. The facial expressions for
these datasets are also not natural. Therefore, high-quality
human-human interaction datasets with actor-reactor anno-
tations are desired in the future.
Acknowledgments. This work was supported in part by
NSFC (62201342, 62101325), Shanghai Municipal Sci-
ence and Technology Major Project (2021SHZDZX0102),
NSFC under Grant 62302246 and ZJNSFC under Grant
LQ23F010008, and supported by High Performance Com-
puting Center at Eastern Institute of Technology, Ningbo.
1766
References
[1] Chaitanya Ahuja and Louis-Philippe Morency. Lan-
guage2pose: Natural language grounded pose forecasting. In
3DV, pages 719â€“728. IEEE, 2019. 3
[2] Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo,
Andrew Fitzgibbon, and Thomas J Cashman. Flag: Flow-
based 3d avatar generation from sparse observations. In
CVPR , pages 13253â€“13262, 2022. 1, 3
[3] Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, and
Libin Liu. Rhythmic gesticulator: Rhythm-aware co-speech
gesture synthesis with hierarchical neural embeddings. TOG ,
41(6):1â€“19, 2022. 1, 3
[4] Andreas Aristidou, Anastasios Yiannakidis, Kfir Aberman,
Daniel Cohen-Or, Ariel Shamir, and Yiorgos Chrysanthou.
Rhythm is a dancer: Music-driven motion synthesis with
global structure. arXiv preprint arXiv:2111.12159 , 2021. 3
[5] German Barquero, Sergio Escalera, and Cristina Palmero.
Belfusion: Latent diffusion for behavior-driven human mo-
tion prediction. arXiv preprint arXiv:2211.14304 , 2022. 3
[6] Murchana Baruah and Bonny Banerjee. A multimodal pre-
dictive agent model for human interaction generation. In
CVPR , pages 1022â€“1023, 2020. 3
[7] J Â´ulia Borras and Tamim Asfour. A whole-body pose tax-
onomy for loco-manipulation tasks. In IROS , pages 1578â€“
1585. IEEE, 2015. 2
[8] Angela Castillo, Maria Escobar, Guillaume Jeanneret, Al-
bert Pumarola, Pablo Arbel Â´aez, Ali Thabet, and Artsiom
Sanakoyeu. Bodiffusion: Diffusing sparse observations
for full-body human motion synthesis. arXiv preprint
arXiv:2304.11118 , 2023. 3
[9] Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi
Shinoda. Implicit neural representations for variable length
human motion generation. In ECCV , pages 356â€“372.
Springer, 2022. 3
[10] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu,
Tao Chen, Jingyi Yu, and Gang Yu. Executing your com-
mands via motion diffusion in latent space. arXiv preprint
arXiv:2212.04048 , 2022. 3
[11] Kyunghyun Cho, Bart Van Merri Â¨enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078 , 2014. 6
[12] Baptiste Chopin, Hao Tang, Naima Otberdout, Mohamed
Daoudi, and Nicu Sebe. Interaction transformer for hu-
man reaction generation. IEEE Transactions on Multimedia ,
2023. 3
[13] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav
Golyanik, and Christian Theobalt. Mofusion: A frame-
work for denoising-diffusion-based motion synthesis. arXiv
preprint arXiv:2212.04495 , 2022. 3
[14] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke,
Ali Thabet, and Artsiom Sanakoyeu. Avatars grow legs:
Generating smooth human motion from sparse tracking in-
puts with diffusion model. arXiv preprint arXiv:2304.08577 ,
2023. 1, 3, 4, 5, 6, 7[15] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut
Popa, Vlad Olaru, and Cristian Sminchisescu. Three-
dimensional reconstruction of human interactions. In CVPR ,
pages 7214â€“7223, 2020. 2, 3
[16] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Ji-
tendra Malik. Recurrent network models for human dynam-
ics. In ICCV , pages 4346â€“4354, 2015. 3
[17] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,
and Yoshua Bengio. Generative adversarial nets. In NIPS ,
pages 2672â€“2680, 2014. 3
[18] Helmut Grabner, Juergen Gall, and Luc Van Gool. What
makes a chair a chair? In CVPR 2011 , pages 1529â€“1536.
IEEE, 2011. 2
[19] Patrick Grady, Chengcheng Tang, Christopher D Twigg,
Minh V o, Samarth Brahmbhatt, and Charles C Kemp. Con-
tactopt: Optimizing contact to improve grasps. In CVPR ,
pages 1471â€“1481, 2021. 2
[20] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tion2motion: Conditioned generation of 3d human motions.
InACM Multimedia , pages 2021â€“2029. ACM, 2020. 3, 6
[21] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural
3d human motions from text. In CVPR , pages 5152â€“5161,
2022. 1, 3, 5, 6
[22] Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, and
Francesc Moreno-Noguer. Multi-person extreme motion pre-
diction. In CVPR , pages 13053â€“13064, 2022. 3
[23] Wen Guo, Yuming Du, Xi Shen, Vincent Lepetit, Xavier
Alameda-Pineda, and Francesc Moreno-Noguer. Back to
mlp: A simple baseline for human motion prediction. In
CVPR , pages 4809â€“4819, 2023. 3
[24] Abhinav Gupta, Scott Satkin, Alexei A Efros, and Martial
Hebert. From 3d scene geometry to human workspace. In
CVPR 2011 , pages 1961â€“1968. IEEE, 2011. 2
[25] Ikhsanul Habibie, Mohamed Elgharib, Kripasindhu Sarkar,
Ahsan Abdullah, Simbarashe Nyatsanga, Michael Neff, and
Christian Theobalt. A motion matching-based framework
for controllable gesture synthesis from speech. In ACM SIG-
GRAPH 2022 Conference Proceedings , pages 1â€“9, 2022. 1,
3
[26] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun
Saito, Jimei Yang, Yi Zhou, and Michael J Black. Stochas-
tic scene-aware motion prediction. In CVPR , pages 11374â€“
11384, 2021. 1, 2
[27] Mohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios
Tzionas, and Michael J Black. Populating 3d scenes by
learning human-scene interaction. In CVPR , pages 14708â€“
14718, 2021. 1, 2
[28] Alejandro Hernandez, Jurgen Gall, and Francesc Moreno-
Noguer. Human motion prediction via spatio-temporal in-
painting. In CVPR , pages 7134â€“7143, 2019. 3
[29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In NIPS , pages 6626â€“6637, 2017. 6
1767
[30] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 6
[31] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840â€“6851, 2020. 3
[32] Kaijen Hsiao and Tomas Lozano-Perez. Imitation learning
of whole-body grasps. In 2006 IEEE/RSJ international con-
ference on intelligent robots and systems , pages 5657â€“5662.
IEEE, 2006. 2
[33] Ruizhen Hu, Zihao Yan, Jingwen Zhang, Oliver Van Kaick,
Ariel Shamir, Hao Zhang, and Hui Huang. Predictive and
generative neural networks for object functionality. arXiv
preprint arXiv:2006.15520 , 2020. 2
[34] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J
Black, Otmar Hilliges, and Gerard Pons-Moll. Deep iner-
tial poser: Learning to reconstruct human pose from sparse
inertial measurements in real time. TOG , 37(6):1â€“15, 2018.
3
[35] Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa
Laich, Patrick Snape, and Christian Holz. Avatarposer: Ar-
ticulated full-body pose tracking from sparse motion sens-
ing. In ECCV , pages 443â€“460. Springer, 2022. 1, 3
[36] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang,
Michael J Black, Krikamol Muandet, and Siyu Tang. Grasp-
ing field: Learning implicit representations for human
grasps. In 3DV, pages 333â€“344. IEEE, 2020. 2
[37] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-
form language-based motion synthesis & editing. arXiv
preprint arXiv:2209.00349 , 2022. 1, 3
[38] Vladimir G Kim, Siddhartha Chaudhuri, Leonidas Guibas,
and Thomas Funkhouser. Shape2pose: Human-centric shape
analysis. TOG , 33(4):1â€“12, 2014. 2
[39] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 3, 5, 6,
7
[40] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun
Wang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz.
Dancing to music. NeurIPS , 32, 2019. 1, 3
[41] Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Dance-
former: Music conditioned 3d dance generation with para-
metric motion transformer. AAAI , 36(2):1272â€“1279, 2022.
[42] Ruilong Li, Shan Yang, David A Ross, and Angjoo
Kanazawa. Ai choreographer: Music conditioned 3d dance
generation with aist++. In CVPR , pages 13401â€“13412, 2021.
1, 3
[43] Xueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-
Hsuan Yang, and Jan Kautz. Putting humans in a scene:
Learning affordance in 3d indoor environments. In CVPR ,
pages 12368â€“12376, 2019. 2
[44] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and
Lan Xu. Intergen: Diffusion-based multi-human motion
generation under complex interactions. arXiv preprint
arXiv:2304.05684 , 2023. 1, 2, 3, 5, 6
[45] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,
Ling-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-
scale benchmark for 3d human activity understanding. T-
PAMI , 42(10):2684â€“2701, 2019. 2, 3[46] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. SMPL: a skinned multi-
person linear model. ACM Trans. Graph. , 34(6):248:1â€“
248:16, 2015. 2
[47] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. Weakly-
supervised action transition learning for stochastic human
motion prediction. In CVPR , pages 8151â€“8160, 2022. 3
[48] Julieta Martinez, Michael J Black, and Javier Romero. On
human motion prediction using recurrent neural networks. In
CVPR , pages 2891â€“2900, 2017. 3
[49] Evonne Ng, Donglai Xiang, Hanbyul Joo, and Kristen Grau-
man. You2me: Inferring body pose in egocentric video via
first and second person interactions. In CVPR , pages 9890â€“
9900, 2020. 3
[50] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and
Michael J Black. Expressive body capture: 3d hands, face,
and body from a single image. In CVPR , pages 10975â€“
10985, 2019. 2, 3, 5
[51] Mathis Petrovich, Michael J Black, and G Â¨ul Varol. Action-
conditioned 3d human motion synthesis with transformer
vae. In CVPR , pages 10985â€“10995, 2021. 1, 3, 6
[52] Mathis Petrovich, Michael J Black, and G Â¨ul Varol. Temos:
Generating diverse human motions from textual descriptions.
InECCV , pages 480â€“497. Springer, 2022. 1, 3
[53] Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman,
Olga Sorkine-Hornung, and Daniel Cohen-Or. Modi: Un-
conditional motion synthesis from diverse data. arXiv
preprint arXiv:2206.08010 , 2022. 2, 3
[54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 4
[55] Danilo Rezende and Shakir Mohamed. Variational inference
with normalizing flows. In International conference on ma-
chine learning , pages 1530â€“1538. PMLR, 2015. 3
[56] Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew
Fisher, and Matthias NieÃŸner. Scenegrok: Inferring action
maps in 3d environments. TOG , 33(6):1â€“10, 2014. 2
[57] Manolis Savva, Angel X Chang, Pat Hanrahan, Matthew
Fisher, and Matthias NieÃŸner. Pigraphs: learning interaction
snapshots from observations. ACM Transactions on Graph-
ics (TOG) , 35(4):1â€“12, 2016. 2
[58] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H
Bermano. Human motion diffusion as a generative prior.
arXiv preprint arXiv:2303.01418 , 2023. 1, 3
[59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In ICML , pages 2256â€“
2265. PMLR, 2015. 3
[60] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 6, 7
[61] Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Za-
man. Local motion phases for learning multi-contact charac-
ter movements. TOG , 39(4):54â€“1, 2020. 1, 3
1768
[62] Sebastian Starke, Yiwei Zhao, Fabio Zinno, and Taku Ko-
mura. Neural animation layering for synthesizing martial
arts movements. TOG , 40(4):1â€“16, 2021. 1, 2, 3
[63] Omid Taheri, Nima Ghorbani, Michael J Black, and Dim-
itrios Tzionas. Grab: A dataset of whole-body human grasp-
ing of objects. In ECCV , pages 581â€“600. Springer, 2020. 1,
2
[64] Omid Taheri, Vasileios Choutas, Michael J Black, and Dim-
itrios Tzionas. Goal: Generating 4d whole-body motion for
hand-object grasping. In CVPR , pages 13263â€“13273, 2022.
1, 2
[65] Mikihiro Tanaka and Kent Fujiwara. Role-aware interaction
generation from textual description. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 15999â€“16009, 2023. 3, 5, 6
[66] Jianwei Tang, Jieming Wang, and Jian-Fang Hu. Predicting
human poses via recurrent attention network. Visual Intelli-
gence , 1(1):18, 2023. 3
[67] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,
Daniel Cohen-Or, and Amit H Bermano. Human motion dif-
fusion model. arXiv preprint arXiv:2209.14916 , 2022. 1, 2,
3, 4, 5, 6, 7
[68] NP Van der Aa, Xinghan Luo, Geert-Jan Giezeman, Robby T
Tan, and Remco C Veltkamp. Umpm benchmark: A multi-
person dataset with synchronized video and motion capture
data for evaluation of articulated human motion and interac-
tion. In ICCV Workshops , pages 1264â€“1269. IEEE, 2011.
3
[69] Coert Van Gemeren, Ronald Poppe, and Remco C Veltkamp.
Spatio-temporal detection of fine-grained dyadic human in-
teractions. In Human Behavior Understanding: 7th Interna-
tional Workshop, HBU 2016, Amsterdam, The Netherlands,
October 16, 2016, Proceedings 7 , pages 116â€“133. Springer,
2016. 3
[70] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiao-
long Wang. Synthesizing long-term 3d human motion and
interaction in 3d scenes. In CVPR , pages 9401â€“9411, 2021.
1, 2
[71] Yan Wu, Jiahao Wang, Yan Zhang, Siwei Zhang, Otmar
Hilliges, Fisher Yu, and Siyu Tang. Saga: Stochastic whole-
body grasping with contact. In ECCV , pages 257â€“274.
Springer, 2022. 1, 2
[72] Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng
Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xi-
aokang Yang, Wenjun Zeng, and Wei Wu. Actformer: A gan-
based transformer towards general action-conditioned 3d hu-
man motion generation. arXiv e-prints , pages arXivâ€“2203,
2022. 1, 3, 6
[73] Liang Xu, Xintao Lv, Yichao Yan, Xin Jin, Shuwen Wu,
Congsheng Xu, Yifan Liu, Yizhou Zhou, Fengyun Rao,
Xingdong Sheng, Yunhui Liu, Wenjun Zeng, and Xiaokang
Yang. Inter-x: Towards versatile human-human interaction
analysis. arXiv preprint arXiv:2312.16051 , 2023. 1
[74] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-
ral graph convolutional networks for skeleton-based action
recognition. In AAAI , pages 7444â€“7452. AAAI Press, 2018.
6[75] Sijie Yan, Zhizhong Li, Yuanjun Xiong, Huahan Yan, and
Dahua Lin. Convolutional sequence generation for skeleton-
based action synthesis. In ICCV , pages 4393â€“4401. IEEE,
2019. 3
[76] Xingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing
knowledge in neural networks. In European Conference on
Computer Vision , pages 73â€“91. Springer, 2022. 2
[77] Xingyi Yang, Daquan Zhou, Songhua Liu, Jingwen Ye, and
Xinchao Wang. Deep model reassembly. Advances in neural
information processing systems , 35:25739â€“25753, 2022.
[78] Yan Yichao, Cheng Yuhao, Chen Zhuo, Peng Yicong, Wu
Sijing, Zhang Weitian, Li Junjie, Li Yixuan, Gao Jingnan,
Zhang Weixia, Zhai Guangtao, and Yang Xiaokang. A sur-
vey on generative 3d digital humans based on neural net-
works: representation, rendering, and learning. SCIENTIA
SINICA Informationis , pages 1858â€“, 2023. 2
[79] Kiwon Yun, Jean Honorio, Debaleena Chattopadhyay,
Tamara L Berg, and Dimitris Samaras. Two-person interac-
tion detection using body-pose features and multiple instance
learning. In CVPRW , pages 28â€“35. IEEE, 2012. 1, 3
[80] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng
Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: To-
wards well-aligned full-body model regression from monoc-
ular images. arXiv preprint arXiv:2207.06400 , 2022. 2, 3
[81] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-
fuse: Text-driven human motion generation with diffusion
model. arXiv preprint arXiv:2208.15001 , 2022. 3
[82] Siwei Zhang, Yan Zhang, Qianli Ma, Michael J Black, and
Siyu Tang. Place: Proximity learning of articulation and
contact in 3d environments. In 3DV, pages 642â€“651. IEEE,
2020. 1, 2
[83] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J
Black, and Siyu Tang. Generating 3d people in scenes with-
out people. In CVPR , 2020. 1, 2
[84] Kaifeng Zhao, Shaofei Wang, Yan Zhang, Thabo Beeler,
and Siyu Tang. Compositional human-scene interaction syn-
thesis with semantic control. In ECCV , pages 311â€“327.
Springer, 2022. 1, 2
[85] Keyang Zhou, Bharat Lal Bhatnagar, Jan Eric Lenssen, and
Gerard Pons-Moll. Toch: Spatio-temporal object-to-hand
correspondence for motion refinement. In ECCV , pages 1â€“
19. Springer, 2022. 1, 2
[86] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and
Hao Li. On the continuity of rotation representations in neu-
ral networks. In CVPR , pages 5745â€“5753. Computer Vision
Foundation / IEEE, 2019. 6
1769
