Exploiting Style Latent Flows for Generalizing Deepfake Video Detection
Jongwook Choi1,2Taehoon Kim1Yonghyun Jeong3Seungryul Baek4Jongwon Choi1,2*
1Dept. of Advanced Imaging, Chung-Ang Univ, Korea2GS. of AI, Chung-Ang Univ, Korea
3Image Vision, NA VER Cloud, Korea4AI Graduate School, UNIST, Korea
{cjw, kimth}@vilab.cau.ac.kr, yonghyun.jeong@navercorp.com, srbaek@unist.ac.kr, choijw@cau.ac.kr
Abstract
This paper presents a new approach for the detection of
fake videos, based on the analysis of style latent vectors and
their abnormal behavior in temporal changes in the gener-
ated videos. We discovered that the generated facial videos
suffer from the temporal distinctiveness in the temporal
changes of style latent vectors, which are inevitable during
the generation of temporally stable videos with various fa-
cial expressions and geometric transformations. Our frame-
work utilizes the StyleGRU module, trained by contrastive
learning, to represent the dynamic properties of style latent
vectors. Additionally, we introduce a style attention module
that integrates StyleGRU-generated features with content-
based features, enabling the detection of visual and tempo-
ral artifacts. We demonstrate our approach across various
benchmark scenarios in deepfake detection, showing its su-
periority in cross-dataset and cross-manipulation scenar-
ios. Through further analysis, we also validate the impor-
tance of using temporal changes of style latent vectors to
improve the generality of deepfake video detection.
1. Introduction
Recent generative algorithms are capable of producing
high-quality videos; while this advancement has led to so-
cial concerns, as it becomes increasingly challenging to dis-
tinguish between generated videos and authentic ones. Gen-
erative models have the potential to expedite industries such
as entertainment, gaming, fashion, design, and education.
However, their misuse can have adverse effects on society.
The high quality of the videos intensifies the potential for
inappropriate utilization of the technique. To resolve the
issue, researchers are actively engaged in developing the
deepfake video detection algorithms [19, 29,40].
Early deepfake detection research addressed spatial ar-
tifacts such as unnatural aspects [30] and frequency-level
checkerboard of the generative model [33] in the genera-
*Corresponding author
Style latent levelStyle latent varianceDFD -real
DFD -fake
Youtube -real      
Celeb -Synthesis1    2     3     4    5     6    7     8     9   10   11   12   13  14   15  16   17   18
1    2     3     4   5     6   7     8     9   10   11   12   13  14   15  16   17   180.4
0.2
0.00.00.51.01.5Figure 1. Variance of style flow for each style latent level. The x-
axis shows the level of style latent vectors for fine style representa-
tions. We noticed that the level-wise differences vary across deep-
fake domains, but the variance of style latent vectors is particularly
lower in certain levels of the style latent vectors for fake videos
than in real videos. This happens due to the temporal smooth-
ness of the style latent vectors to create temporally stable deepfake
videos, and our results demonstrate that deepfake videos have a
distinct variance in style flow compared to real videos.
tion of a single frame. While spatial artifact-based detec-
tion methods have shown reasonable performance for sin-
gle images, they failed in accounting for temporal artifacts
in deepfake videos that consist of multiple frames. To ad-
dress the limitation, recent studies [15, 41] integrate tempo-
ral cues such as flickering [14, 57] and discontinuity [16] to
enhance the accuracy of deepfake video detection task.
Nevertheless, existing methods [28, 51] that exploit
visual and temporal cues of generated videos encoun-
tered performance degradation when attempting to identify
videos produced by recent deepfake generation algorithms,
which effectively suppress visual and temporal artifacts.
Recent observations [36, 39] have presented a decline in
the effectiveness of visual artifact-based algorithms against
newly developed high-quality generation techniques [11,
37]. Temporal artifact-based algorithms [16] also experi-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1133
enced performance degradation due to the advanced quality
of video generation methods [41, 57].
In this paper, we propose to focus on the suppressed vari-
ance in the temporal flow of style latent features which are
extracted from generated videos, rather than conventional
visual and temporal artifacts. The style latent feature en-
codes the facial appearance and facial movements, such as
expressions and geometric transformations, to control the
synthesis of facial images. Fig. 1shows the clear differ-
ences in the flow of style latent vectors between the gen-
erated and synthetic videos. We noticed that the suppressed
variance of the style latent vector occurs due to the temporal
smoothness of facial appearance and movements in the gen-
erated videos. We believe that the flow of style latent vectors
has the potential to be a distinctive feature for generalizing
deepfake video detection across various generative models.
In order to encode the dynamics of style latent vectors,
we introduce the StyleGRU module in our video deepfake
detection framework. This module can effectively capture
the temporal variations within style latent vectors, conse-
quently enabling the extraction of robust style-based tempo-
ral features. To increase the representation power of style-
based temporal features, we further involve the supervised
contrastive learning [23] technique in StyleGRU module.
We develop the style attention module which effectively in-
tegrates the style-based temporal features with the content
features that encode the visual and temporal artifacts.
We perform various experiments to evaluate the pro-
posed algorithm for multiple scenarios such as cross-dataset
and cross-manipulation scenarios. From the experimental
results, our method notably surpasses the existing algo-
rithms, achieving a state-of-the-art performance. Ablation
studies are also conducted to show the module-wise effec-
tiveness, and the meaningful analysis experiments confirm
that the variations in style latent vector contribute effec-
tively to the robust detection to the various deepfake gen-
eration methods. The contributions of our work are summa-
rized as follows.
‚Ä¢ We propose a novel video deepfake detection framework
that is based on the unnatural variation of the style latent
vectors.
‚Ä¢ We introduce a StyleGRU module that can encode the
variations of style latent vectors with the contrastive
learning technique.
‚Ä¢ We propose a style attention module that effectively inte-
grates the features from StyleGRU with the content fea-
tures capturing conventional visual and temporal artifacts.
‚Ä¢ Our approach demonstrates state-of-the-art performance
in various deepfake detection scenarios, including cross-
dataset and cross-manipulation settings, which confirms
the effectiveness of facial attribute changes for deepfake
video detection.2. Related work
Deepfake detection. In deepfake detection research, there
are two main types: image-level and video-level detectors.
The image-level detectors detect fake images by recogniz-
ing spatial artifacts of a single frame. A straightforward
approach to detect spatial artifacts involves using Xcep-
tion model [40], a Convolutional Neural Network (CNN)
architecture along with MAT [56] that utilizes an atten-
tional mechanism. Face X-ray [27] proposed a method that
uses boundaries between forged faces and backgrounds to
capture spatial inconsistency. Several methods utilized the
frequency-level features to emphasize the artifacts from
the image generators [14]. FrePGAN [18] utilized the fre-
quency map transformed from the original RGB image, and
LRL [6] integrated RGB images with frequency-aware cues
to learn inconsistencies. Recently, algorithms utilize meth-
ods including blending artifacts [2, 5,12,43] or separating
elements relevant to detection from irrelevant ones [12, 55]
during their training. Image-level detectors demonstrate
high generalization performance. However, image-based
methods have the limitation of not utilizing the temporal
cues that inevitably occur during the generation of deepfake
videos.
In contrast to image-level detectors, video-level detec-
tors take advantage of temporal information by using multi-
ple frames to detect deepfake videos. CNN-GRU [42] cap-
tured temporal information from the features extracted by
CNNs, and LipForensics [16] utilized the information near
the mouth region by employing a network pre-trained on a
LipReading dataset. Recently, FTCN [57] directly extracted
temporal information using 3D CNNs with a spatial kernel
size of 1. AltFreeze [52] showcases strong generalization
capabilities through its unique approach of independently
training spatial and temporal information. However, the ex-
isting methods have focused on low-level features like pix-
els, so they ignore the temporal changes of high-level con-
texts such as facial attributes. To improve detection capabil-
ities, we propose a novel approach that considers temporal
features at both low-level and high-level, thereby enhanc-
ing the overall context and advancing the state-of-the-art in
deepfake detection.
Style Latent Vectors. Well-trained Generative Adversar-
ial Network (GAN) models [20‚Äì22] successfully generate
disentangled latent space. This advancement permits the
straightforward manipulation of style latent features to pro-
duce desired facial images. The GAN inversion, initially in-
troduced by [59], targets the identification of a latent code
within the latent space, which makes it possible to gener-
ate an image closely resembling a target one. Furthermore,
various encoders [1, 38,48,58] have been studied to prop-
erly utilize the prior knowledge of StyleGAN latent space.
Moreover, GAN inversion methods decoding the style latent
features of StyleGAN [21] are utilized for generating the
1134
Õµ‡µà›à‡µàÕ¥Õ¥Õ∂‡µàÕ¥Õ¥Õ∂Clip
 Style GRU
real/
fakeMLP
Linear Projection
n21*0
positional
encodingTransformer
Encoderclass
tokenTemporal Transformer
Encoder
‹•‡Øñ‡Ø¢‡Ø°‡Øß‡Øò‡Ø°‡Øßﬂ∂ﬂ∂‡Ø©
Style 
Attn.
Score
‹≥‡Æº
‹≠‡Ææ
‹∏‡ÆæStyle Attention Module
‹ß‡Ø¶‡Øß‡Ø¨‡Øü‡Øò
3D ResNet 50‡¢ô‡¨µ
‡¢ô‡¨∂
‡¢ô‡Øü»ü‡¢ô‡Øç‡µà ‡µà
‡µÖ‡¢ô‡Øü‡¨ø‡¨µ
pSp
»ü»ü»ü‡¢ô‡¨µ
»ü‡¢ô‡Øü‡¨ø‡¨µGRU GRU
GRU GRU
ﬂ∂‡Øû
ﬂ∂‡Ø§
Figure 2. Schematic diagram of our entire framework. For the video deepfake detection, we Ô¨Årst extract the style latent vectors from
pSp for each individual frame, and then encode their variations using the StyleGRU module into the style-based temporal feature Estyle.I n
parallel, the content feature Ccontent is extracted via the 3D ResNet-50 architecture from the video clip. Style Attention Module (SAM) in-
tegrates the style-based temporal feature Estyle with the content feature Ccontent via the attention mechanism. Finally, Temporal Transformer
Encoder (TTE) is applied to map the representation into the binary class of real and fake labels.
high-quality face-swap videos. MegaFS [ 60] also inverted
both source and target images into the Ô¨Ånely-tuned Style-GAN latent space for the purpose of face swap.
Face swapping through GAN inversion offers the ad-
vantage of separating facial identity in the latent spaceand swapping latent features, thereby preserving Ô¨Ådelity inmodels like StyleGAN [ 20]. Furthermore, it is employed for
various Deepfake generation techniques, including modify-ing facial attributes, extending beyond just altering facialidentity. These approaches surpassed the existing method,such as StyleRig [ 45], which involved direct modiÔ¨Åcations
to the style latent feature. The GAN inversion-based face
swap methods [ 31,53,54] continue to demonstrate good
performance. .0We also employ the style latent features ofGAN inversion models to represent the temporal changesof facial attributes. Given that most of the recent GAN in-version models, including those based on the StyleGAN
architecture [ 20], utilize style features, a similar situation
applies to the construction of deepfake generation models.Hence, we propose the use of StyleGRU, which is wellsuited for countering attacks on state-of-the-art deepfakedetection models.
3. Methodology
In this section, we propose the video deepfake detectionframework that exploits the style latent vectors and capturestheir characteristics in the generated videos. Especially, weproposed four distinct modules to achieve the goal: (1)StyleGRU module that extracts the pSp features for eachindividual frame and encode the temporal variations of thestyle latent vectors using the GRU into the style-based tem-
poral feature E
style, (2) 3D ResNet-50 module that encodes
the video clip into the content feature Ccontent , (3) Style At-
tention Module (SAM) that combines the style-based tem-poral feature E
style with the content feature Ccontent via theattention mechanism, (4) Temporal Transformer Encoder
(TTE) module that effectively maps the combined featuresinto the binary classes (i.e. real and fake). Fig. 2illustrates
the overall Ô¨Çow of our framework.
3.1. Overall Architecture
3.1.1 Input conÔ¨Åguration
We obtain the video clip Ithat contains a sequence of l
consecutive frames extracted from a single video. Then, webuild a clip I
pcomposed of all cropped faces within the
input clip Ithat have been preprocessed with FFHQ [ 20]
conditions. The preprocessed video clip Ipis used as an in-
put to StyleGRU after resizing it to be R256√ó256√ól, while
the raw clip Iis an input of 3D ResNet-50 module with the
resized shape of R224√ó224√ól.
3.1.2 StyleGRU module
The preprocessed video clip Ipserves as the input to ex-
tract style latent vectors via the pSp encoder [ 38]. The pSp
encoder ( pSp ) provides the capability to extract style la-
tent vectors for inverting given images within a well-trainedStyleGAN [ 21] latent space. We extract the style latent vec-
torsSas:
S=pSp(I
p)‚â°{s1,s2, ..., s l}, (1)
where sk‚ààRDis the D-dimensional style vector of the
k-th frame of the clip Ip,k‚àà[1,l], and lis the number of
frames in clip.
The style latent vector that we extract through the pSp
encoder is a feature for the GAN inversion task, so it isinsufÔ¨Åcient for direct application to the deepfake detectiontask. To enable the extension of style from the image do-
main to the video, we employ the difference of subsequent
1135
St yle 
G R U S A M 
real/ 
fa ke 
ùêøùë° ùëüùëñ +ùêøùëêùëô ùë† St a ge 1 
F a ke se q ue nce set Re al se q ue nce set 
A nc h or P ositi v e Ne g ati v e 
p S p 
St yle 
St yle 
Fl o w 
St yle 
G R U St yle 
G R U St a ge 2 
: Wei g ht -s hari n g 
: Trai n 
: Fr eeze : St yle Atte nti o n M o d ule S A M 
: Te m p or al Tra nsf or mer E nc o der T T E : p S p E nc o der p S p 
: Te m p or al 3 D Res N et 5 0 3 D 
C N N 3 D 
C N N ùëæ+s p ace 
... 
St yle Fl o w 
: St yle Fl o w T T E M L P 
ùêøùëè ùëê ùëí ùëÜ
ŒîùëÜŒî
Œî
ŒîŒî
Œî
ùëÜùëõùëÜ1ùëÜ3
ùëÜ2ùëÜùëõ‚àí1Cli ps 
fa ke 
real p S p St yle 
G R U St yle Fl o w 
Fi g ure 3. O ur tr ai ni n g pr oce d ures. I n sta ge 1, we trai n t he St yle G R U m o d ule usi n g t he s u per vise d c o ntrasti ve lear ni n g tec h ni q ue t o 
effecti vel y ca pt ure variati o ns of t he p S p feat ure a n d e nc o de it i nt o t he r o b ust st yle- base d te m p oral feat ure. I n sta ge 2, we trai n St yle 
Atte nti o n M o d ule ( S A M) a n d Te m p oral Tra nsf or mer E nc o der ( T T E) t o i nte grate t he st yle- base d te m p oral feat ure a n d c o nte nt feat ure a n d 
t he n ma p it t o war ds t he bi nar y classes (ie. real a n d fa ke), res pecti vel y. 
st yle late nt vect ors, e xtracte d fr o m fra mes wit hi n a cli p. T he 
st yle Ô¨Ç o w ‚àÜs, deri ve d fr o m differe nce of st yle late nt vec- 
t ors, is re prese nte d as ‚àÜsi=si+ 1 ‚àísi. T he n, t he i n p ut 
‚àÜS, w hic h is calle d style Ô¨Ç o w , is c o nstr ucte d as f oll o ws: 
‚àÜS={‚àÜs1,‚àÜs2,..., ‚àÜs(l‚àí1) }, ( 2) 
T he st yle Ô¨Ç o w ‚àÜSis a n i n p ut t o t he Gate d Rec urre nt 
U nit ( G R U) la yers [ 7]. T he n, we o btai n a te m p oral e m be d- 
de d st yle Ô¨Ç o w Est yle ‚àà R C√óB√óLt hr o u g h t he hi d de n state 
of t he Ô¨Å nal G R U la yer. Cre prese nts t he hi d de n u nit size of 
G R U, Bi n dicates t he n u m ber of directi o ns of G R U, a n d L
de n otes t he n u m ber of G R U la yers. We b uil d t w o c o nsec- 
uti ve G R U la yers of 4 0 9 6 hi d de n u nits wit h bi directi o nal 
c o n Ô¨Å g urati o n, s o C= 4 0 9 6 ,B= 2 , a n d L= 2 . T h us, we 
ca n deri ve t hat: 
Est yle =G R U ( ‚àÜ S), ( 3) 
w here G R U re prese nts t he o perati o n of bi directi o nal t w o 
G R U la yers e m be d de d i n St yle G R U m o d ule. 
3. 1. 3 3 D Res Net- 5 0 m o d ule 
T he c o nte nt feat ure Cc o nte nt is deri ve d fr o m t he feat ure e x- 
tracti o n perf or me d b y t he 3 D C N N as: 
Cc o nte nt =3 D C N N (I), ( 4) 
w here 3 D C N N is a n o perati o n f u ncti o n f or e xtracti n g a 
feat ure fr o m 3 D C N N m o del. T h us, Cc o nte nt c o ntai ns i nf or- 
mati o n a b o ut vis ual a n d te m p oral i nc o nsiste nc y. We utilize t he feat ure of R1 0 2 4 √ó1 6 b y a p pl yi n g a gl o bal a vera ge p o ol- 
i n g t o t he last c o n v ol uti o n la yer of 3 D- R 5 0 arc hitect ure [ 3]
pre-trai ne d i n F T C N [ 5 7 ]. 
3. 1. 4 St yle atte nti o n m o d ule 
We i ntr o d uce a St yle Atte nti o n M o d ule ( S A M) t hat le ver- 
a ges t he st yle e m be d di n g vect or o btai ne d fr o m t he St yle- 
G R U. To a p pl y t he atte nti o n mec ha nis m, we desi g nate 
Cc o nte nt as t he q uer y, w hile Est yle is all ocate d as t he ke y a n d 
val ue. We e m pl o y t he c o nte nt feat ure as t he q uer y base d o n 
t he i nt uiti o n t hat t hese feat ures h ol d t he p ote ntial t o i de ntif y 
a n o malies i n t he st yle Ô¨Ç o w, dri ve n b y t heir e nc o de d te m p o- 
ral i nc o nsiste nc y. 
T h us, we desi g n t he q uer y, ke y, a n d val ue of S A M as: 
QC=œïq(Cc o nte nt ), K E=œïk(Est yle ), V E=œïv(Est yle ),
( 5) 
w here œïq,œïk, a n d œïvare li near pr ojecti o n f u ncti o n usi n g 
f ull y c o n necte d la yer t o esti mate t he q uer y, ke y, a n d val ue, 
res pecti vel y. T he n, we calc ulate a St yle Atte nti o n ( S A) us- 
i n g t he q uer y a n d ke y as: 
S A (QC, K E) = sQCKE
dKE, ( 6) 
w here s(‚Ä¢)re prese nts a s oft ma x f u ncti o n. 
Ulti matel y, we c o m p ute t he res ult of S A M b y ta ki n g t he 
d ot- pr o d uct of t he S A wit h t he val ue c o m p o ne nt. T h us, 
S A M (E, C ) = œïS A (QC, K E)¬∑VE, ( 7) 
w here œïis t he last li near pr ojecti o n la yer of S A M. 
1136
3.1.5 Temporal transformer encoder module.
The output from the SAM is linearly projected into 16
time steps (n=16) and then fed into the Temporal Trans-
former Encoder (TTE) module. The core of TTE is com-
posed of standard transformer encoder blocks [49], with
each block including a multi-head self-attention module and
multi-layer perceptrons. The results of TTE are the input to
the following classification head, yielding the final deepfake
prediction through a sigmoid function œÉ. Thus,
ÀÜy=œÉ/parenleftbig
fcls(TTE (Ccontent+SAM (Estyle, Ccontent)))/parenrightbig
,(8)
where fclsis a function of fully-connected layer to repre-
sent the classification head. We refer to FTCN [57] for the
architecture of TTE.
3.2. Training Procedure
The training procedure consists of two following stages.
The first stage involves training a Gated Recurrent Unit
(GRU) layers to enable encoding of the temporal style la-
tent flow suitable for deepfake detection. In the following
stage, we train the 3D CNN model for content embedding,
the style attention module, and the subsequent modules for
the binary classification. Fig. 3provides a comprehensive
illustration of our training procedures.
3.2.1 Stage 1: style representation learning
In stage 1, we train StyleGRU to learn the representation of
style flow. We employ supervised contrastive learning [23]
to produce an effective representation of style flow. For
the supervised contrastive learning, we organized the style
flows used as inputs to the GRU layers into three categories:
anchor, positive, and negative style flows.
We utilize a different sampling strategy to build the clip
for anchor, positive, and negative style flows. At first, the
anchor style flow (‚àÜS a) is extracted from the clip sam-
pled from the video sequence by a sliding window sampling
strategy. On the other hand, the positive style flow (‚àÜS p)
is obtained by using a random sampling strategy. Both the
anchor and the positive style flows are extracted from the
preprocessed clips with the same label. In contrast, the neg-
ative style flow (‚àÜS n) is obtained from the clips with labels
distinct from those of the anchor and positive clips. The neg-
ative style flow is generated by randomly employing one of
the two sampling strategies.
As a batch, the anchor, positive, and negative style flows
are fed into the GRU layers, and their representation is
trained by using a triplet loss and a classification loss. When
we define the GRU‚Äôs final hidden states from ‚àÜSa,‚àÜSp,
and‚àÜSnbyEa,Ep, and En, respectively, the triplet loss
for training the GRU layers is derived as follows:
Ltri=max/parenleftbig
||Ea‚àíEp||2
2‚àí ||Ea‚àíEn||2
2+Œ±,0/parenrightbig
,(9)where Œ±is a margin parameter.
Additionally, we used classification loss (L cls), which is
designed by Binary Cross-Entropy (BCE) loss to learn ap-
propriate features for classification. To obtain the predicted
value ÀÜyfor the ground truth label y, the hidden states E
of the GRU are input into the auxiliary classifier. The calcu-
lated BCE(y ,ÀÜy) loss is utilized as Lcls. The auxiliary classi-
fier consists of three fully connected layers, with a residual
connection applied after the first two layers. The hidden size
of each fully connected layer is kept identical at 4096.
Then, the entire loss function for training StyleGRU is as
follows:
L=Ltri+ŒªLcls, (10)
where Œªis a user-defined hyperparameter to determine the
influence degree of Lcls. We minimize the loss by using
Adam optimizer [25].
3.2.2 Stage 2: style attention-based deepfake detection.
In stage 2, to perform the real/fake binary classification task,
we employ a conventional Binary Cross-Entropy (BCE)
loss to train the 3D ResNet-50, SAM, and TTE modules,
while fixing the StyleGRU module trained at stage 1. The
BCE loss (L bce) is applied to reduce the gap between the
fake prediction ÀÜyof Eq. 8and its corresponding ground truth
label, where 0 is real and 1 is fake. We used the momentum-
driven SGD optimizer for the training process of stage 2.
4. Experiments
We conduct experiments in this section to demonstrate
the superiority of our algorithm. Experiments under cross-
dataset and cross-manipulation conditions serve as bench-
mark settings to assess the generalization performance of
deepfake detection models.
Implementation Details. During the preprocessing phase,
we utilized Dlib [24] to extract facial landmarks and per-
form alignment, as well as Retinaface [ 10] to carry out
face cropping. Following this preprocessing, the real and
fake clips fed into the model consist of a total of l= 32
frames each. In stage 1, a dropout layer with a dropout rate
ofp= 0.2is included between the input style flow and
the StyleGRU. Furthermore, in StyleGRU, an RNN dropout
rate of p= 0.1is applied. As training parameters, a total of
50,000 samples are organized with a batch size of 256 over
100 epochs for contrastive learning. For optimization, the
ADAM optimizer is used with a learning rate of 5e‚àí4.
In stage 2, we use the momentum SGD optimizer and set
the weight decay to 1e‚àí4. The training is conducted with a
batch size of 16. For the first 10 epochs, the learning rate of
SGD gradually warms up from 0.01 to0.1. Subsequently, a
cosine learning rate scheduler is used to decrease the learn-
ing rate throughout 100 epochs.
1137
Method CDF DFD FSh
DFo Avg
Xception [8
] 73.7 - 72.0
84.5 -
CNN-aug [50
] 75.6 - 65.7
74.7 -
PatchForensics
[4] 69.6 - 57.8
81.8 -
Multi-task [35
] 75.7 - 66.0
77.7 -
FWA[
28] 69.5 - 65.5
50.2 -
Two-branch
[34] 76.7 - -
- -
DCL [44
] 82.3 91.6 -
- -
FaceX-ray
[27] 79.5 95.4 92.8 86.8 88.6
NoiseDF [51
] 75.9 - -
70.9 -
SLADD [5
] 79.7 - -
- -
SBI-R50 [43
] 85.7 94.0 78.2
91.4 87.3
CNN-GR U[
41] 69.8 - 80.8
74.1 -
STIL [15
] 75.6 - -
- -
LipForensics [
16] 82.4 - 97.1
97.6 -
RealF orensics* [
17] 85.6 82.2 99.6 99.8 91.8
FTCN* [57
] 86.9 94.4 98.8 98.8 94.7
AltFreeze* [52
] 89.0 93.7 99.2 99.0 95.2
StyleGR U 62.5 72.2 90.7
77.7 79.8
Ours 89.0 96.1 99.0 99.0 95.8
Table 1. Generalization to cross-dataset. We present the AUC
scores (%) on the CDF, DFD, FSh, and DFo datasets to assess
the cross-dataset performance of our framework. The highest-
performing score is marked in bold. The underlined values corre-
spond to the Top-2 best methods. The asterisk (*) denotes that we
have reproduced the results using officially provided weights.The
results of other methods were obtained from AltFreeze [52].
Datasets and Measurement. To ensure robustness under
various conditions, we used the following deepfake detec-
tion datasets: (1) FaceForensics++ (FF++) [40] comprises
1,000 original videos and 4,000 fake videos produced in
four manipulation methods, Deepfake (DF) [9], Face2Face
(F2F) [46], FaceSwap (FS) [32], NeuralTexture (NT) [47],
and we utilize the c23 version. (2) FaceShfiter (FSh) [ 26]
and (3) DeeperForensics [19] result from the employment
of optimized manipulation techniques to the original videos
of FF++. We executed our experiments using test videos ad-
hering to the same split as in FF++. (4) CelebDF-v2 (CDF)
[29] is composed of 590 real videos and 5,639 fake videos.
For our study, we engaged a subset of 518 test videos. (5)
DeepfakeDetection (DFD) [13] consists of 363 real videos
and 3071 synthetic videos.
To train our model, we used the training videos from the
FF++ [40] dataset with light compression, specifically iden-
tified as c23. The evaluation metric is the Area Under the
receiver operating characteristic Curve (AUC). Most deep-
fake detection algorithms have utilized AUC as their eval-
uation metric, and Lipforensics [16], which is a fundamen-
tal benchmark for video-based deepfake detection, also em-
ployed AUC.
Comparisons. We have compared both image-based and
video-based deepfake detection models. Xception [40] is
a method of training the Xception model [8], which is awidely used CNN architecture. CNN-aug [50] discovered
that CNN-generated images can be readily identified by a
CNN model. PatchForensics [4] indicated that the use of
patch-based classifiers can contribute to robust deepfake
detection. Multi-task [35] approach employed an architec-
ture similar to that of an auto-encoder for deepfake detec-
tion. FWA [28] introduced a training data generation tech-
nique that relies on deteriorating the quality of the GAN-
generated source images. Two-branch [34] method utilized
multi-task learning within the context of a deepfake de-
tection dataset. SLADD [5] introduced an algorithm that
automatically learns effective augmentation techniques for
deepfake detection. SBI [43] proposed a novel approach to
blending artifacts using self-images. DCL [44] applied con-
trastive learning to the task of deepfake detection in the im-
age domain. NoiseDF [51] compared the features of the
cropped face and background squares using noise to de-
tect deepfakes. Face X-ray [27] identified face forgery by
segmenting and delineating the blending border between
source and target images. CNN-GRU [41] highlighted the
potential of utilizing CNN and GRU to investigate temporal
features. STIL [15] specifically focused on learning spatio-
temporal inconsistency. LipForensics [16] exhibited robust
generalization performance through the use of a trained
model to capture the representation of natural mouth move-
ment. RealForensics [17] improved the ability to general-
ize using multimodal architecture. FTCN [57] argued that
leveraging temporal inconsistency is beneficial for enhanc-
ing the generalizability of deepfake detection algorithms.
AltFreeze [52] is an architecture that demonstrates good
generalization performance by separately training spatial
information and temporal information.
4.1. Generalization to cross-datasets
In the context of our framework, we have set the cross-
dataset performance as a proxy for the generalizability in
real-world situations. We train the proposed framework on
the FF++ and then evaluate its performance on the CDF,
DFD, FSh, and DFo datasets. Table 1shows that our frame-
work exhibits the best detection performance in the CDF,
DFD, and DFo datasets, which include high-quality deep-
fake videos. The performance on the FSh dataset also
reaches a top-2 level, with the highest average score, in-
dicating that our model is a deepfake detection algorithm
with generalization capability. ‚ÄòStyleGRU‚Äô represents the
cross-dataset score using the classification layer directly
connected after the StyleGRU for the Stage 1 training. The
score of StyleGRU is superior to that of CNN-GRU [41],
which is a similar structure that utilizes image features, on
the FSh and DFo datasets. This suggests the potential for
a better impact on generalization performance by utilizing
the flow of style latent vectors rather than simply relying on
the image-based feature.
1138
Trainon
remaining three
Method DF FS F2F
NT Avg
Xception [8
] 93.9 51.2 86.8
79.7 77.9
CNN-aug [50
] 87.5 56.3 80.1
67.8 72.9
PatchF orensics
[4] 94.0 60.5 87.3
84.8 81.6
FaceX-ray
[27] 99.5 93.2 94.5 92.5 94.9
CNN-GR U[
41] 97.6 47.6 85.8
86.6 79.4
LipForensics [
16] 99.7 90.1 98.8 98.3 97.1
FTCN* [57
] 99.8 99.3 95.9 95.3 97.5
AltFreeze [52
] 99.8 99.7 98.6 96.2 98.6
StyleGR U 94.0 68.5 88.8
80.6 83.0
Ours 99.7 98.8 98.6 96.4 98.4
Table 2. Generalization to cross-manipulation. In order to
quantitatively assess the performance of our method in cross-
manipulation scenarios, we utilized video-level AUC. The asterisk
(*) denotes results that we have reproduced. The underlined values
correspond to the Top-3 best methods. The results of other meth-
ods were obtained from AltFreeze [52].
4.2. Generalization to cross-manipulations
For deepfake detection tasks, the evaluation of cross-
manipulation performance targets fake videos generated
from the same original video but using a different scheme.
A common approach is to select three out of the four ma-
nipulations provided by FF++, and then the model trained
by the three chosen manipulation sets is evaluated by one
remaining manipulation set. DF [9] and FS [32] represent
identity swap manipulation methods, while F2F [46] and
NT [47] denote expression swap manipulation methods. Al-
tFreeze [52] does not officially provide weights for this ex-
periment setting. Therefore, it is not possible for us to re-
produce. Table 2presents a comparison of the performance
of the cross-manipulation between our model and various
models. For expression swap manipulation methods like
F2F and NT, they exhibit relatively lower performance com-
pared to Lipforensics [16]. Expression swap inherently lead
to inconsistencies in the mouth region, which is why Lip-
Forensics appears to perform well. However, our framework
displays outstanding performance across all manipulations.
4.3. Ablation Study
In this section, we conduct ablation studies to verify the ap-
proach used in our framework. we validate the necessity of
the proposed StyleGRU and SAM and conduct an ablation
study on the losses used in Stage 1.
4.3.1 Component-wise Ablation Study
As shown in Table 3, we conducted experiments using only
the StyleGRU by feeding the hidden states of StyleGRU
into an MLP classification head to detect the real/fake of
input video. Even though only the classification throughMethod Dataset
StyleGRU SAM TTE ‚àÜ CDF FSh Avg
‚úì - - ‚úì 62.5 90.7 76.6
- ‚úì ‚úì ‚úì 85.7 98.4 92.1
‚úì -‚úì ‚úì 87.8 98.3 93.1
‚úì ‚úì ‚úì -88.4 98.4 93.4
‚úì ‚úì ‚úì ‚úì 89.0 99.0 94.0
Table 3. Framework ablation. We evaluate our framework on the
CDF and FSh datasets using video-level AUC scores and conduct
ablation studies for each stage of the proposed methodology. All
models utilized the FF++ dataset for training.
Loss Dataset
Lcls Ltri FF++ CDF FSh
‚úì - 99.2 88.6 99.0
-‚úì 98.8 88.4 98.8
‚úì ‚úì 99.1 89.0 99.0
Table 4. Stage 1 loss ablation. We conduct an ablation study for
the losses employed in stage 1. Each experiment uses the different
combinations of training losses for StyleGRU, while preserving
the training process of stage 2. The model is trained by the FF++
dataset, and evaluated by video-level AUC.
StyleGRU was conducted, ours still demonstrates high
scores on the Fsh dataset. In addition, considering the sig-
nificant performance drop when StyleGRU is not used, it
can be observed that our proposed StyleGRU contributes to
improving cross-dataset performance. The influence of the
proposed SAM in Stage 2 on the outcome is also evaluated.
For experiments without StyleGRU, we processed style la-
tent vectors extracted from clips using an MLP to match
the dimension of the 3D CNN features. In the case of ex-
periments without the SAM, we simply added the Style-
GRU feature to the 3D CNN feature. The absence of the
SAM during the blending of content and style features leads
to a substantial decline in performance. This quantitatively
demonstrates the efficacy of the SAM trained during stage 2
in achieving the adapted feature for robust deepfake detec-
tion. Finally, the influence of differentiating (‚àÜ) the style
latent vectors is investigated by replacing the input of the
style flow with the original style latent vectors. During the
overall stage 1 training process, it was observed that the lack
of differencing in style latent vectors as a pre-processing to
be fed into the StyleGRU leads to a slower decrease in the
loss. Training without differentiation was confirmed to not
only lead to slower progression in stage 1 learning but also
adversely affect cross-dataset performance.
4.3.2 Ablation Study for Losses
Ablation studies are conducted on various options of train-
ing losses that can be employed at stage 1 training for Style-
GRU representation learning. As shown in Table 4, when
1139
3D CNN+TTE StyleGRU
Deepfakes: FaceSwap: Face2Face: NeuralTextures: Real:
Figure 4. t-SNE visualization. The t-SNE visualization was con-
ducted using the final layer features of a model trained on the FF++
test set. For hyperparameters, we utilized 1000 iterations with a
perplexity of 40 and PCA to 30.
only classification loss (L cls) is used, our model exhibits
best performance on the training dataset, while suffering
from the reduction of domain generality. Using only triplet
loss (L tri) results in relatively reduced performance due to
the lack of distinctiveness between the real and fake videos
for classification. Although employing both LclsandLtri
results in a slight performance decrease on the seen dataset,
we can see that their concurrent employment proves to be
beneficial on the unseen datasets.
4.4. Analytic Experiments
In this section, we perform visualizations for further anal-
ysis of the proposed style latent flow in our study. We also
analyze the perturbation robustness of our algorithm, which
is important to consider in real-world scenarios.
4.4.1 t-SNE for Domain Generalization
To evaluate the learning performance of StyleGRU, we con-
duct t-SNE visualizations on the FF++ test set using a model
trained on the FF++ dataset. As illustrated in Fig. 4, de-
spite StyleGRU utilizing significantly fewer parameters, our
model manages to extract more distinct features in the fea-
ture space. This characteristic represents our intuition that
the distinctiveness of style latent vectors aligns well with
the generality of deepfake video detection tasks across var-
ious generation algorithms.
4.4.2 Robustness to Unseen Perturbation
To evaluate the robustness of perturbation, experiments un-
der the conditions proposed by DFo [19] are conducted to
observe performance changes. In Fig. 5, performance under
specific perturbation conditions exhibits a slight deficiency,
which is attributed to the fact that the pSp encoder [38]
used for extracting style latent vectors was created without
considering noise. However, our model demonstrates a high
level of robustness against most perturbations.
707580859095100
1 2 3 4 5
ResizeXception FTCN AltFreeze Ours
707580859095100
1 2 3 4 5
Gaussian BlurFigure 5. Robustness to Unseen Perturbation. The performance
changes based on the video-level AUC metric when applying two
perturbations at five different degradation levels. The perturbation
follows the approach provided by DeeperForensics [19].
5. Conclusion
In this paper, we first demonstrate that the style latent flow
extracted from consecutive frames of a video, serves as a
effective cue for deepfake detection. Additionally, we con-
firm that leveraging an attention mechanism based on style
flow facilitates model generalization. The style flow feature
is trained via a supervised-contrastive representation learn-
ing process, and we develop a classification model using
the Style Attention Module. The superior performance of
the proposed algorithm validates the importance of tempo-
ral changes in style latent vectors to generalize the deep-
fake video detection task. While this paper focused on the
facial attributes extracted by StyleGAN, our future plans is
to broaden the scope of attributes for deepfake video detec-
tion to encompass various objects. We anticipate that utiliz-
ing StyleGAN pre-trained on other subjects, such as images
of dogs and scenery, could broaden the applicability of our
method to a wider range of deepfake subjects.
The model we propose includes an additional step of ex-
tracting style latent vectors through the pSp encoder. As a
result, the data preprocessing takes a relatively long time.
However, we confirms the positive impact of using style
flow on the generalization performance of deepfake video
detection models and provides a new direction for address-
ing synthesized videos created through GAN inversion.
Acknowledgements: This work was partly supported by Institute of Infor-
mation & communications Technology Planning & Evaluation (IITP) grant
funded by the Korea government(MSIT) (2021-0-01341, Artificial Intelli-
gence Graduate School Program(Chung-Ang University), 2020-0-01336,
AIGS program (UNIST), and 2021-0-01778 Development of human image
synthesis and discrimination technology below the perceptual threshold)
and Culture, Sports and Tourism R&D Program through the Korea Cre-
ative Content Agency grant funded by the Ministry of Culture, Sports and
Tourism in 2024 (Project Name : Development of high-freedom large-scale
user interaction technology using multiple projection spaces to overcome
low-light lighting environments. , Project Number : RS-2023-00222280).
1140
References
[1] Yuval Alaluf, Or Patashnik, and Daniel Cohen-Or. Restyle:
A residual-based stylegan encoder via iterative refinement.
InProceedings of the IEEE/CVF International Conference
on Computer Vision, pages 6711‚Äì6720, 2021. 2
[2] Weiming Bai, Yufan Liu, Zhipeng Zhang, Bing Li, and
Weiming Hu. Aunet: Learning relations between action units
for face forgery detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 24709‚Äì24719, 2023. 2
[3] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6299‚Äì6308, 2017. 4
[4] Lucy Chai, David Bau, Ser-Nam Lim, and Phillip Isola.
What makes fake images detectable? understanding proper-
ties that generalize. In Computer Vision‚ÄìECCV 2020: 16th
European Conference, Glasgow, UK, August 23‚Äì28, 2020,
Proceedings, Part XXVI 16, pages 103‚Äì120. Springer, 2020.
6,7
[5] Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and
Jue Wang. Self-supervised learning of adversarial example:
Towards good generalizations for deepfake detection. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 18710‚Äì18719, 2022. 2,6
[6] Shen Chen, Taiping Yao, Yang Chen, Shouhong Ding, Jilin
Li, and Rongrong Ji. Local relation learning for face forgery
detection. In Proceedings of the AAAI conference on artifi-
cial intelligence, pages 1081‚Äì1088, 2021. 2
[7] Kyunghyun Cho, Bart Van Merri ¬®enboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078, 2014. 4
[8] Franc ¬∏ois Chollet. Xception: Deep learning with depthwise
separable convolutions. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
1251‚Äì1258, 2017. 6,7
[9] Deepfakes. faceswap. https : / / github . com /
deepfakes/faceswap, 2019. [Accessed 27-07-2023].
6,7
[10] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kot-
sia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-
level face localisation in the wild. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 5203‚Äì5212, 2020. 5
[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural informa-
tion processing systems, 34:8780‚Äì8794, 2021. 1
[12] Shichao Dong, Jin Wang, Renhe Ji, Jiajun Liang, Haoqiang
Fan, and Zheng Ge. Implicit identity leakage: The stum-
bling block to improving deepfake detection generalization.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 3994‚Äì4004, 2023. 2
[13] Nick Dufour and Andrew Gully. Contributing Data to Deep-
fake Detection Research ‚Äî ai.googleblog.com. https://
ai.googleblog.com/2019/09/contributing-data-to-deepfake-detection.html, 2019. [Ac-
cessed 30-07-2023]. 6
[14] Qiqi Gu, Shen Chen, Taiping Yao, Yang Chen, Shouhong
Ding, and Ran Yi. Exploiting fine-grained face forgery clues
via progressive enhancement learning. In Proceedings of the
AAAI Conference on Artificial Intelligence, pages 735‚Äì743,
2022. 1,2
[15] Zhihao Gu, Yang Chen, Taiping Yao, Shouhong Ding, Jilin
Li, Feiyue Huang, and Lizhuang Ma. Spatiotemporal incon-
sistency learning for deepfake video detection. In Proceed-
ings of the 29th ACM international conference on multime-
dia, pages 3473‚Äì3481, 2021. 1,6
[16] Alexandros Haliassos, Konstantinos Vougioukas, Stavros
Petridis, and Maja Pantic. Lips don‚Äôt lie: A generalisable
and robust approach to face forgery detection. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 5039‚Äì5049, 2021. 1,2,6,7
[17] Alexandros Haliassos, Rodrigo Mira, Stavros Petridis, and
Maja Pantic. Leveraging real talking faces via self-
supervision for robust forgery detection. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 14950‚Äì14962, 2022. 6
[18] Yonghyun Jeong, Doyeon Kim, Youngmin Ro, and Jongwon
Choi. Frepgan: robust deepfake detection using frequency-
level perturbations. In Proceedings of the AAAI Conference
on Artificial Intelligence, pages 1060‚Äì1068, 2022. 2
[19] Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and
Chen Change Loy. Deeperforensics-1.0: A large-scale
dataset for real-world face forgery detection. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 2889‚Äì2898, 2020. 1,6,8
[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 4401‚Äì4410, 2019. 2,3
[21] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improv-
ing the image quality of stylegan. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 8110‚Äì8119, 2020. 2,3
[22] Tero Karras, Miika Aittala, Samuli Laine, Erik H ¬®ark¬®onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. Advances in Neural Infor-
mation Processing Systems, 34:852‚Äì863, 2021. 2
[23] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and
Dilip Krishnan. Supervised contrastive learning. Advances
in neural information processing systems, 33:18661‚Äì18673,
2020. 2,5
[24] Davis E King. Dlib-ml: A machine learning toolkit.
The Journal of Machine Learning Research, 10:1755‚Äì1758,
2009. 5
[25] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980,
2014. 5
[26] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang
Wen. Faceshifter: Towards high fidelity and occlusion aware
face swapping. arXiv preprint arXiv:1912.13457, 2019. 6
1141
[27] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong
Chen, Fang Wen, and Baining Guo. Face x-ray for more gen-
eral face forgery detection. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition,
pages 5001‚Äì5010, 2020. 2,6,7
[28] Yuezun Li and Siwei Lyu. Exposing deepfake videos by de-
tecting face warping artifacts. arxiv 2018. arXiv preprint
arXiv:1811.00656, 2018. 1,6
[29] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei
Lyu. Celeb-df: A large-scale challenging dataset for deep-
fake forensics. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition, pages 3207‚Äì
3216, 2020. 1,6
[30] Zhengzhe Liu, Xiaojuan Qi, and Philip H.S. Torr. Global
texture enhancement for fake face detection in the wild. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR), 2020. 1
[31] Zhian Liu, Maomao Li, Yong Zhang, Cairong Wang, Qi
Zhang, Jue Wang, and Yongwei Nie. Fine-grained face
swapping via regional gan inversion. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8578‚Äì8587, 2023. 3
[32] MarekKowalski. Faceswap. https://github.com/
MarekKowalski/FaceSwap, 2019. [Accessed 30-07-
2023]. 6,7
[33] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and
Giovanni Poggi. Do gans leave artificial fingerprints? In
2019 IEEE conference on multimedia information process-
ing and retrieval (MIPR), pages 506‚Äì511. IEEE, 2019. 1
[34] Iacopo Masi, Aditya Killekar, Royston Marian Mascaren-
has, Shenoy Pratik Gurudatt, and Wael AbdAlmageed. Two-
branch recurrent network for isolating deepfakes in videos.
InComputer Vision‚ÄìECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part
VII 16, pages 667‚Äì684. Springer, 2020. 6
[35] Huy H Nguyen, Fuming Fang, Junichi Yamagishi, and Isao
Echizen. Multi-task learning for detecting and segmenting
manipulated facial images and videos. In 2019 IEEE 10th
international conference on biometrics theory, applications
and systems (BTAS), pages 1‚Äì8. IEEE, 2019. 6
[36] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards uni-
versal fake image detectors that generalize across genera-
tive models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 24480‚Äì
24489, 2023. 1
[37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Confer-
ence on Machine Learning, pages 8821‚Äì8831. PMLR, 2021.
1
[38] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,
Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding
in style: a stylegan encoder for image-to-image translation.
InProceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition, pages 2287‚Äì2296, 2021. 2,3,
8[39] Jonas Ricker, Simon Damm, Thorsten Holz, and Asja Fis-
cher. Towards the detection of diffusion model deepfakes.
arXiv preprint arXiv:2210.14571, 2022. 1
[40] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Chris-
tian Riess, Justus Thies, and Matthias Nie√üner. Faceforen-
sics++: Learning to detect manipulated facial images. In
Proceedings of the IEEE/CVF international conference on
computer vision, pages 1‚Äì11, 2019. 1,2,6
[41] Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAl-
mageed, Iacopo Masi, and Prem Natarajan. Recurrent convo-
lutional strategies for face manipulation detection in videos.
Interfaces (GUI), 3(1):80‚Äì87, 2019. 1,2,6,7
[42] Ekraam Sabir, Jiaxin Cheng, Ayush Jaiswal, Wael AbdAl-
mageed, Iacopo Masi, and Prem Natarajan. Recurrent convo-
lutional strategies for face manipulation detection in videos.
Interfaces (GUI), 3(1):80‚Äì87, 2019. 2
[43] Kaede Shiohara and Toshihiko Yamasaki. Detecting deep-
fakes with self-blended images. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 18720‚Äì18729, 2022. 2,6
[44] Ke Sun, Taiping Yao, Shen Chen, Shouhong Ding, Jilin Li,
and Rongrong Ji. Dual contrastive learning for general face
forgery detection. In Proceedings of the AAAI Conference on
Artificial Intelligence, pages 2316‚Äì2324, 2022. 6
[45] Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian
Bernard, Hans-Peter Seidel, Patrick P ¬¥erez, Michael Zoll-
hofer, and Christian Theobalt. Stylerig: Rigging stylegan
for 3d control over portrait images. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6142‚Äì6151, 2020. 3
[46] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-
tian Theobalt, and Matthias Nie√üner. Face2face: Real-time
face capture and reenactment of rgb videos. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 2387‚Äì2395, 2016. 6,7
[47] Justus Thies, Michael Zollh ¬®ofer, and Matthias Nie√üner. De-
ferred neural rendering: Image synthesis using neural tex-
tures. Acm Transactions on Graphics (TOG), 38(4):1‚Äì12,
2019. 6,7
[48] Omer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and
Daniel Cohen-Or. Designing an encoder for stylegan image
manipulation. ACM Transactions on Graphics (TOG), 40(4):
1‚Äì14, 2021. 2
[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 5
[50] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew
Owens, and Alexei A Efros. Cnn-generated images are
surprisingly easy to spot... for now. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 8695‚Äì8704, 2020. 6,7
[51] Tianyi Wang and Kam Pui Chow. Noise based deepfake de-
tection via multi-head relative-interaction. In Proceedings of
the AAAI Conference on Artificial Intelligence, pages 14548‚Äì
14556, 2023. 1,6
[52] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun
Wang, and Houqiang Li. Altfreezing for more general video
1142
face forgery detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 4129‚Äì4138, 2023. 2,6,7
[53] Chao Xu, Jiangning Zhang, Miao Hua, Qian He, Zili Yi, and
Yong Liu. Region-aware face swapping. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7632‚Äì7641, 2022. 3
[54] Yangyang Xu, Bailin Deng, Junle Wang, Yanqing Jing, Jia
Pan, and Shengfeng He. High-resolution face swapping
via latent semantics disentanglement. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7642‚Äì7651, 2022. 3
[55] Zhiyuan Yan, Yong Zhang, Yanbo Fan, and Baoyuan Wu.
UCF: uncovering common features for generalizable deep-
fake detection. In IEEE/CVF International Conference on
Computer Vision, ICCV 2023, Paris, France, October 1-6,
2023, pages 22355‚Äì22366. IEEE, 2023. 2
[56] Hanqing Zhao, Tianyi Wei, Wenbo Zhou, Weiming Zhang,
Dongdong Chen, and Nenghai Yu. Multi-attentional deep-
fake detection. In 2021 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pages 2185‚Äì2194,
2021. 2
[57] Yinglin Zheng, Jianmin Bao, Dong Chen, Ming Zeng, and
Fang Wen. Exploring temporal coherence for more gen-
eral video face forgery detection. In Proceedings of the
IEEE/CVF international conference on computer vision,
pages 15044‚Äì15054, 2021. 1,2,4,5,6,7
[58] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-
domain gan inversion for real image editing. In European
conference on computer vision, pages 592‚Äì608. Springer,
2020. 2
[59] Jun-Yan Zhu, Philipp Kr ¬®ahenb ¬®uhl, Eli Shechtman, and
Alexei A Efros. Generative visual manipulation on the natu-
ral image manifold. In Computer Vision‚ÄìECCV 2016: 14th
European Conference, Amsterdam, The Netherlands, Octo-
ber 11-14, 2016, Proceedings, Part V 14, pages 597‚Äì613.
Springer, 2016. 2
[60] Yuhao Zhu, Qi Li, Jian Wang, Cheng-Zhong Xu, and Zhenan
Sun. One shot face swapping on megapixels. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition, pages 4834‚Äì4844, 2021. 3
1143
