Lodge: A Coarse to Fine Diffusion Network for
Long Dance Generation Guided by the Characteristic Dance Primitives
Ronghui Li1,2, YuXiang Zhang1,Yachao Zhang1,
Hongwen Zhang4,Jie Guo2,Yan Zhang3â€¡,Yebin Liu1, Xiu Li1â€ 
1Tsinghua University,2Peng Cheng Laboratory3Meshcapade,4Beijing Normal University
Lodge
TE
TE
Librosa
Music feature
Genre 
EmbeddingGGlobal Diffusion
Hard/Soft Guidance
LD LD LD LD
ConcatKeymotion 
for soft 
guidanceKeymotion
for hard 
guidance
Choreography 
AugmentGlobal music 
feature
Concatenate 
feature
Generated danceConcat Concat ConcatSplit
 
g 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
g 
g 
g 
g 
ğ‘šğ‘™ğ‘– 
ğ‘šğ‘” 
ğ‘‘â„ 
ğ‘‘ğ‘  
ğ‘‘ CA CA CA CA
CA
Concat
g 
ğ‘šğ‘™ TE
TE
Librosa
Music feature
Genre Embedding GGlobal Diffusion
LD LD LD LDConcat
Generated 
danceConcat Concat ConcatSplit
 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
ğ‘šğ‘” Choreography Augment OperationLodge
LD
L framesN frames
Text Control Module
M-Base Diffusion 
TimeStepsTMLPConcat
Transformer 
Encoder
Music 
FeatureGenre 
Embedding
Self-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLMSelf-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLMCond t
Forward Kinematics
Foot Contact, Position, Velocity 
ğ‘“ğ‘œğ‘œğ‘¡ğ‘£ 
ğ‘“ğ‘œğ‘œğ‘¡ğ‘ 
Transformer EncoderCond
Concat
CondFoot Refine Block Diffusion 
TimeStepsT
Cond
Global Average Pooling
Jazz
 Urban Korean
Self-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLM
TE
Condx Cond
Global Average 
Pooling
Jazz Urban KoreanConcat
g 
ğ‘šğ‘™ğ‘– 
Diffusion 
TimeStepsT
MLPConcat
TE
Music 
FeatureGenre 
Embedding
Cond t
Self-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLM
FK
ğ‘“ğ‘œğ‘œğ‘¡ğ‘£ 
ğ‘“ğ‘œğ‘œğ‘¡ğ¶ 
ğ‘“ğ‘œğ‘œğ‘¡ğ‘ Cond
ConcatFoot Refine BlockxSequence
Transformer BlockLinear
Real/Fake? Real/Fake? Real/Fake?
ğ‘‘ğ‘™ğ‘‡ 
ğ‘‘ğ‘™ğ‘‡âˆ’1 
Lodge Lodge
L framesN frames
Multi Genre 
Discriminator Characteristic
Dance primitives
Noise
ğ‘‘ğ‘™2
ğ‘‡   
ğ‘‘ğ‘™1
ğ‘‡   
ğ‘‘ğ‘™3
ğ‘‡   
ğ‘‘ğ‘™4
ğ‘‡   
4 frames 8 frames
n frames
ğ‘‘ğ‘™1
ğ‘‡âˆ’1 
ğ‘‘ğ‘™2
ğ‘‡âˆ’1 
ğ‘‘ğ‘™3
ğ‘‡âˆ’1 
ğ‘‘ğ‘™4
ğ‘‡âˆ’1 
ğ‘‘ğ‘    
ğ‘‘â„   
ğ‘‘â€²
ğ‘™3
ğ‘‡   
ğ‘‘â€²
ğ‘™4
ğ‘‡   
ğ‘‘â€²
ğ‘™2
ğ‘‡   
ğ‘‘â€²
ğ‘™1
ğ‘‡   
N frames
ğ‘‘ğ‘”ğ‘‡   Noise
4 framesAlign
 with
uneven 
music 
beats
ğ‘‘ğ‘    
T=T-1T=T-1
Denoising Denoising 
ğ‘‘ğ‘™ğ‘‡âˆ’1 
ğ‘‘ ğ‘™=ğ‘‘ğ‘™0 
music beatsMirror Mirror Mirror Mirror Mirror Mirror Mirror Mirror
TE
TE
Librosa
Music feature
Genre Embedding GGlobal Diffusion
LD LD LD LDConcat
Generated 
danceConcat Concat ConcatSplit
 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
ğ‘šğ‘” Diffusion 
TimeStepsT
Characteristic
Dance Primitives
Noise
ğ‘‘ğ‘™2
ğ‘‡   
ğ‘‘ğ‘™1
ğ‘‡   
ğ‘‘ğ‘™3
ğ‘‡   
ğ‘‘ğ‘™4
ğ‘‡   
4 frames 8 frames
n frames
ğ‘‘ğ‘™1
ğ‘‡âˆ’1 
ğ‘‘ğ‘™2
ğ‘‡âˆ’1 
ğ‘‘ğ‘™3
ğ‘‡âˆ’1 
ğ‘‘ğ‘™4
ğ‘‡âˆ’1 
ğ‘‘ğ‘    
ğ‘‘â„   
ğ‘‘â€²
ğ‘™3
ğ‘‡   
ğ‘‘â€²
ğ‘™4
ğ‘‡   
ğ‘‘â€²
ğ‘™2
ğ‘‡   
ğ‘‘â€²
ğ‘™1
ğ‘‡   
N frames
ğ‘‘ğ‘”ğ‘‡   Noise
4 framesAlign
 with
uneven 
music 
beats
ğ‘‘ğ‘    T=T-1
Denoising 
music beatsMirror Mirror Mirror Mirror Mirror Mirror Mirror MirrorChoreography Augment Operation
Choreography Augment Operation
Hard/Soft Diffusion Guidance
ğ‘“ğ‘œğ‘œğ‘¡ğ‘ Music
Generated 
danceParallel
Lodges
Number
of dance 
frames
Figure 1. Lodge can parallelly generate extremely long dance. The sections highlighted in green represent the characteristic dance
primitives. These are expressive 8-frame movements that not only support parallel generation but also contains choreographic patterns.
They guide the diffusion network to generate long, expressive dances in parallel while adhering to choreographic rules.
Abstract
We propose Lodge, a network capable of generating ex-
tremely long dance sequences conditioned on given music.
We design Lodge as a two-stage coarse to fine diffusion ar-
chitecture, and propose the characteristic dance primitives
that possess significant expressiveness as intermediate rep-
resentations between two diffusion models. The first stage
is global diffusion, which focuses on comprehending the
coarse-level music-dance correlation and production char-
acteristic dance primitives. In contrast, the second-stage is
the local diffusion, which parallelly generates detailed mo-
tion sequences under the guidance of the dance primitives
and choreographic rules. In addition, we propose a Foot
Refine Block to optimize the contact between the feet and
the ground, enhancing the physical realism of the motion.
Our approach can parallelly generate dance sequences of
extremely long length, striking a balance between global
â€ corresponding author
â€¡This work was done while YZ was at ETH Z Â¨urich.choreographic patterns and local motion quality and ex-
pressiveness. Extensive experiments validate the efficacy of
our method. Code, models, and demonstrative video results
are available at: https://li-ronghui.github.io/lodge
1. Introduction
Given a piece of long-term music, we aim at generating
high-fidelity and diverse 3D dance motions in an automatic
and efficient manner. An effective solution is desired not
only in many applications e.g. movie and game production,
but also of high potential to inspire dance designers with
novel movements, and improve their productivity.
With rapid advances in generative AI in recent years, ex-
isting methods [19, 23, 33, 39, 40] demonstrated the ability
to generate dance for seconds. However, dance in real ap-
plications often lasts for minutes. Dance performances and
social dance usually last 3 to 5 minutes. Dance theater can
last for more than 15 minutes or even an hour. Therefore,
the extremely long dance generation is becoming increas-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1524
ingly important as the demand for engaging dance content
continues to grow.
However, generating long dance sequences poses a no-
table challenge due to the substantial computational re-
sources needed for training. Therefore, many methods are
based on autoregressive models [14, 22, 39], and continu-
ously generate dance movements based on a relatively small
sliding window. This autoregressive nature accumulates the
model prediction errors as time progresses, and prevents the
model from learning global choreographic patterns. As a re-
sult, motion freezing often occurs after several seconds [52].
There are also some methods [39, 56, 60] maintain a latent
space to represent motion, and combine a autoregressive
based sequence model to learn music-dance paired relation-
ship. However, the compressed latent space with limited
representational capacity also makes these methods prone
to overfitting, resulting in poor generalization and diversity.
Recently, EDGE [46] proposed a diffusion-based dance
generation model. During the denoising process, EDGE
parallelly generate multiple dance segments with overlap
while maintaining consistency between these overlapping
parts using diffusion inpainting [26], and finally splices
these segments into a long dance by linear interpolation.
However, their dances lack an overall choreographic struc-
ture and shows incoherence at the splices points.
In summary, these existing methods regarding dance
generation solely as a sequence-to-sequence problem. They
struggle to enhance the dance quality of fine-grained local
details while neglect the coarse-level global choreography
patterns between music and dance. Referring to [1, 4, 5, 43],
dance is normally choreographed in a coarse-to-fine man-
ner. Provided the entire music, dance designers first ana-
lyze the music attributes such as rhythm, genre and emo-
tional tone, and create â€œdance phrasesâ€, i.e. some short-
term expressive movements, which possess powerful ex-
pressiveness and richer semantic information. During this
stage, dance designers can concentrate on design character-
istic dance phrases, such as â€œinversionsâ€™ and â€œmoonwalksâ€.
Arrange these characteristic dance phrases follow the struc-
tured information of the music, the overall dance structure
is laid down. Subsequently, the entire dance is created by
connecting dance phrases with transition movements.
Following the above insights, we think that the â€œdance
phrasesâ€ contains abundant distinctive movements and can
convey global choreographic patterns. Therefore, similar to
dance phrases, we propose characteristic dance primitives
suitable for network learning. These dance primitives are
expressive 8-frame key motions with high kinematic energy,
with the following main advantages: (1) They are sparse,
which reduce the computational demand. (2) They have rich
semantically information, and can transfer choreographic
patterns. (3) They possess expressive motion characteris-
tics, which can guide motion diffusion model to generatemore dynamic movements and avoiding monotony.
Next, we design a coarse-to-fine dance generation frame-
work with two motion diffusion models and employ the
characteristic dance primitives as their intermediate repre-
sentation. The first stage is coarse-grained global diffusion,
which takes as input long music and produces characteris-
tic dance primitives. According to the fundamental chore-
ographic rules, details in Sec. 3, these dance primitives are
further augmented to align with the beats and structural in-
formation of the music. Subsequently, we employ parallel
local diffusion to independently generate short dance seg-
ments. Based on some auto-selected dance primitives, we
utilize diffusion guidance to strictly constrain consistency
between the beginnings and ends of these segments. There-
fore, these dance segments can be concatenated into a con-
tinuous long dance. Simultaneously, under the guidance of
the other dance primitives, the quality, expressiveness, and
diversity of each dance segment are enhanced.
In addition, to improve the motion realism and eliminate
foot-skating artifacts, we introduce a foot refine block in-
spired by [57]. We find it is difficult to simply use foot-
related losses [54] to optimize the SMPL [25] format mo-
tion rotation data, especially in complex dance movements.
This is because the optimization objective exists in the lin-
ear joint position space while the SMPL format rotation data
is mainly in nonlinear rotation space, and there is a domain
gap hindering loss convergence. Therefore, we compute
foot contact information and utilize the foot refine block to
generate modification values addressing foot skating.
In summary, our main contributions are as follows:
â€¢ We introduce a coarse-to-fine diffusion framework that
can produce long dances in a parallel manner. Our
method is capable of learning the overall choreographic
patterns while ensuring the quality of local movements.
â€¢ We propose the characteristic dance primitives that pos-
sess significant expressiveness as intermediate represen-
tations between two diffusion models.
â€¢ We propose a foot refine block and employ a foot-ground
contact loss to eliminate artifacts such as skating, floating,
and ground inter-penetration.
2. Related Works
2.1. Human Motion Synthesis
Human motion generation is an important task in the
fields of computer vision and computer graphics. Re-
searchers make significant contributions in this direction.
For instance, MDM [45] successfully applies diffusion to
the Text2Motion task, yielding high-quality motion re-
sults; GestureDiffuCLIP Ao et al. [2] achieves coordi-
nated motion generation with speech and integrates style
control through text and video guidance; SAGA[48] and
Grasping[17] focuses on natural grasping motion genera-
1525
tion; [16, 55, 58] can produce human motions that inter-
act with 3D scenes while avoiding collisions. CALM [44]
and ASE [35] introduce reinforcement learning and physi-
cal simulation environments to enhance the physical realism
of generated movements. Despite substantial progress in as-
pects like motion quality, diversity, controllability, interac-
tivity, and physical realism, etc, generating dance motions
remains a challenging problem due to the inherent complex-
ity and long-duration nature of dance movements.
2.2. Music Driven Dance Generation
Numerous studies aim to generate high-quality dance that
synchronizes with the input music. These approaches en-
compass various categories, including motion-graph meth-
ods [6], sequence model based methods [19, 22, 39], VQ-
V AE based methods [39, 60], GAN-base methods [19], and
diffusion based methods [23, 46].
The traditional motion-graph based methods [3, 30, 33]
address this task as a similarity-based retrieval problem,
which limits the diversity and creativity of generations. In
recent years, deep learning models have gained significant
prominence, yielding aesthetically appealing outcomes. In
sequence-based methods, LSTM [13] and Transformer [47]
networks are commonly employed. These networks typi-
cally take as input music and the preceding dance sequence,
predicting the subsequent dance in an autoregressive man-
ner. Li et al. propose FACT[22], which inputs music and
seed motions into a Transformer network, generating new
dance frame by frame in an autoregressive manner, but chal-
lenges such as error accumulation and motion freezing [62]
phenomena persist. Based on VQ-V AE, Bailando incorpo-
rates a reinforcement learning-based action evaluator to op-
timize rhythm, while TM2D encodes the text-paired motion
and music-paired dance into a shared codebook to achieve
semantically controllable dance generation. The advantages
of VQ-V AE lie in its ability to maintain a pre-trained code-
book, ensuring the motion quality of decoded dance se-
quences. But the codebook also limits dance diversity and
hinders the networkâ€™s generalization. The Generative Ad-
versarial Network (GAN) consists of a generator and a dis-
criminator, engaged in adversarial training to produce real-
istic data. MNET [19] proposes a transformer-based dance
generator and a multi-genre dance discriminator network
to generate realistic dance clips and achieve genre control.
However, these GAN-based methods suffer from mode col-
lapse and training instability.
In recent years, with the rapid development of neural net-
works [7, 24, 37, 51, 56], Diffusion-based methods make
significant strides in tasks such as image, video, and mo-
tion generation [8, 9, 12, 27â€“29, 42, 50]. FineDance[23]
and EDGE[46] introduce Diffusion to generate diverse and
high-quality dance clips of seconds, but they only focus
on local motion quality of detailed dance clips and cannotquickly generate long-term dance movements that conform
to the overall choreography rules.
3. Method
3.1. Preliminaries
Music and Dance Representation. Given a music clip, we
follow [22] and employ Librosa [31] to extract the music 2D
feature map mâˆˆRLÃ—35, in which Lis the frame number
and35is the music feature channels with 1-dim envelope,
20-dim MFCC, 12-dim chroma, 1-dim one-hot peaks, and
1-dim one-hot beats. In addition, we follow EDGE [46] and
represent dance as dâˆˆRLÃ—139. This motion representation
obeys the SMPL[25] format (without fingers) and consists
of the following components: (1) 4-dim foot-ground contact
binary label, corresponding to left toe, left heel, right toe,
right heel, where 1 means contact with ground and 0 means
no contact; (2) 3-dim root translation; (3) 132-dim rotation
information in 6-dim rotation repersentation [59], the first
6-dim is global rotation and the remaining 126 dimensions
correspond to the relative rotations of 21 sub-joints propa-
gated along the kinematic chain.
The Diffusion Model. We follow DDPM [11] and
EDGE [46] to build our dance generation model. The dif-
fusion model consists of two main processes: a diffusion
process and a denoising process. The diffusion process per-
turbs the ground truth dance data d0intodtovertsteps, we
follow [11] to simplify this multi-step diffusion process into
one step, which can be formulated as:
q(dt|d0) =N(âˆšÂ¯Î±td0,(1âˆ’Â¯Î±t)I), (1)
where Â¯Î±tis within the range of (0,1)and follows a mono-
tonically decreasing schedule. Â¯Î±tconverges to 0astgoes
to infinity, making dtconverging to a sample from the stan-
dard normal distribution. The denoising process employs a
Transformer base-network fÎ¸to gradually recover the mo-
tion, generating Ë†d0conditioned on given music m. Instead
of predicting the noise [53], we directly predict the Ë†d0like
[46]. Therefore, the training process can be formulated as:
Lrecon= E d0,t[âˆ¥d0âˆ’fÎ¸(dt, t,m)âˆ¥2
2]. (2)
Choreography Rules. Based on suggestions from profes-
sional choreographers and existing literature[1, 4, 5, 43],
we want to generate long-duration dances that obeyed these
three basic choreographic rules: (1) The overall genre of the
music and the dance should be consistent, conveying similar
moods and tones. (2) The beat of the music and the dance
should be the same as far as possible. (3) The arrangement
of dance should align with the structure of the accompany-
ing music. For instance, identical meters in a musical phrase
often correspond to symmetrical movements.
1526
Lodge
TE
TE
Librosa
Music feature
Genre 
EmbeddingGGlobal Diffusion
Hard/Soft Guidance
LD LD LD LD
ConcatKeymotion 
for soft 
guidanceKeymotion
for hard 
guidance
Choreography 
AugmentGlobal music 
feature
Concatenate 
feature
Generated danceConcat Concat ConcatSplit
 
g 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
g 
g 
g 
g 
ğ‘šğ‘™ğ‘– 
ğ‘šğ‘” 
ğ‘‘â„ 
ğ‘‘ğ‘  
ğ‘‘ CA CA CA CA
CA
Concat
g 
ğ‘šğ‘™ Lodge
L framesN frames
Diffusion 
TimeStepsTMLPConcat
Transformer 
Encoder
Music 
FeatureGenre 
Embedding
Self-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLMSelf-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLMCond t
Forward Kinematics
ğ‘“ğ‘œğ‘œğ‘¡ğ‘£ 
ğ‘“ğ‘œğ‘œğ‘¡ğ‘ 
Transformer EncoderCond
Concat
CondFoot Refine Block Cond
Global Average Pooling
Jazz
 Urban KoreanSequence
Transformer BlockLinear
Real/Fake? Real/Fake? Real/Fake?
ğ‘‘ğ‘™ğ‘‡ 
ğ‘‘ğ‘™ğ‘‡âˆ’1 
Lodge Lodge
L framesN frames
Multi Genre 
Discriminator 
T=T-1
Denoising 
ğ‘‘ğ‘™ğ‘‡âˆ’1 
ğ‘‘ ğ‘™=ğ‘‘ğ‘™0 TE
TE
Librosa
Music feature
Genre Embedding GGlobal Diffusion
LD LD LD LDConcat
Generated 
danceConcat Concat ConcatSplit
 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
ğ‘šğ‘” Diffusion 
Time StepsT
Characteristic
Dance Primitives
Noise
ğ‘‘ğ‘™2
ğ‘‡   
ğ‘‘ğ‘™1
ğ‘‡   
ğ‘‘ğ‘™3
ğ‘‡   
ğ‘‘ğ‘™4
ğ‘‡   
4 frames 8 frames
n frames
ğ‘‘ğ‘™1
ğ‘‡âˆ’1 
ğ‘‘ğ‘™2
ğ‘‡âˆ’1 
ğ‘‘ğ‘™3
ğ‘‡âˆ’1 
ğ‘‘ğ‘™4
ğ‘‡âˆ’1 
ğ‘‘ğ‘    
ğ‘‘â„   
ğ‘‘â€²
ğ‘™3
ğ‘‡   
ğ‘‘â€²
ğ‘™4
ğ‘‡   
ğ‘‘â€²
ğ‘™2
ğ‘‡   
ğ‘‘â€²
ğ‘™1
ğ‘‡   
N frames
ğ‘‘ğ‘”ğ‘‡   Noise
4 framesAlign
 with
uneven 
music 
beats
ğ‘‘ğ‘    T=T-1
Denoising 
music beatsMirror Mirror Mirror Mirror Mirror Mirror Mirror MirrorChoreography Augment Operation
Hard/Soft Diffusion Guidance
ğ‘“ğ‘œğ‘œğ‘¡ğ‘ Music
Generated 
danceParallel
Lodges
Number
of dance 
frames
Linear
Linear
TE
TE
Librosa
Music feature
Genre Embedding GGlobal Diffusion
LD LD LD LDConcat
Generated 
danceConcat Concat ConcatSplit
 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
ğ‘šğ‘” Diffusion 
Time StepsT
Characteristic
Dance Primitives
Noise
ğ‘‘ğ‘™2
ğ‘‡   
ğ‘‘ğ‘™1
ğ‘‡   
ğ‘‘ğ‘™3
ğ‘‡   
ğ‘‘ğ‘™4
ğ‘‡   
4 frames 8 frames
n frames
ğ‘‘ğ‘™1
ğ‘‡âˆ’1 
ğ‘‘ğ‘™2
ğ‘‡âˆ’1 
ğ‘‘ğ‘™3
ğ‘‡âˆ’1 
ğ‘‘ğ‘™4
ğ‘‡âˆ’1 
ğ‘‘ğ‘    
ğ‘‘â„   
ğ‘‘â€²
ğ‘™3
ğ‘‡   
ğ‘‘â€²
ğ‘™4
ğ‘‡   
ğ‘‘â€²
ğ‘™2
ğ‘‡   
ğ‘‘â€²
ğ‘™1
ğ‘‡   
N frames
ğ‘‘ğ‘”ğ‘‡   Noise
4 framesAlign
 with
music 
beats
ğ‘‘ğ‘    T=T-1
Denoising 
music beatsMirror Mirror Mirror Mirror Mirror Mirror Mirror MirrorChoreography Augment Operation
Hard/Soft Diffusion GuidanceLinear
LinearFigure 2. An overview of our framework. â€œTEâ€ is Transformer Encoder, â€œGâ€ is the genre of dance, â€œLDâ€ is the Local Diffusion Model.
3.2. Two-stage Dance Generation
Given a extremely long music feature mâˆˆRLÃ—35, L=
kN, we first split minto segments of length Nwithout
overlaps, i.e.
mi
gâˆˆRNÃ—35	k
i=1. Our goal is to learn a
neural network Loge, di
g=Lodge (mi
g),dgâˆˆRNÃ—139,
d=concatenate (âŒˆmi
gâŒ‰, dim = 0) , which means Lodge
can parallelly generate extremely long dance sequences dâˆˆ
RkNÃ—139with a single inference.
Method Overview. In order to simultaneously consider
both the global choreographic rules and the local dance
details, we design a coarse to fine diffusion network with
two stages as shown in Figure 2. The first stage is the
global diffusion, which uses the global music feature mg
to learn the choreography patterns and produce character-
istic dance primitives. The dance primitives are expressive
key motions mkâˆˆR8Ã—139with a higher motion kinematic
energy, where 8 is the frame number. Then, we perform
choreographic augment operations on these dance primi-
tives by the following three steps: (1) We categorize them
into hard-cue key motions dhthat support parallel gener-
ation and soft-cue key motions dsthat enhance the dance
performance. (2) Based on the second choreographic rule,
we mirror these soft-cue key motions. (3) Based on the third
choreographic rule, we align soft-cue key motions to the
timing of the musical beats.
The second stage is the Local Diffusion (LD), which
focuses on the quality of short-duration nframes dance
generation, corresponding to several seconds. We furthersplit each mginton
mj
lâˆˆRnÃ—35oâŒˆN/nâŒ‰
j=1. As shown in
Figure 2, we use the characteristic dance primitives as an
intermediate-level representation of our two-stage diffusion
network. During the inference process, we replace the
movements at the beginning and end of dt, as well as those
at the timing of musical beats, with these dance primitives.
This way, we transfer globally learned choreographic pat-
terns and expressive dance primitives obtained by global
diffusion to local diffusion in a diffusion guidance man-
ner. Specifically, the hard-cue key motion uses the diffu-
sion inpainting technique [26, 46] to control the start and
end movements of the local diffusion. Meanwhile, during
the diffusion denoising process, soft-cue key motions only
serve as guidance in the initial 1000Ã—ssteps, where 1000 is
the diffusion denoising steps. By adjusting the hyperparam-
eter â€˜sâ€™, we can control the extent to which local diffusion is
influenced by these soft-cue key motions. Notably, thanks
to the hard cue motions, we can parallelly generate dance
sequences dmuch longer than N with a single inference.
3.3. Global Diffusion
Previous works overlook the global dependencies between
music and dance, focusing only on the music-dance rela-
tionship within a small window. To address this issue, we
only task global diffusion to generate sparse dance primi-
tives. Subsequently, multiple local diffusions work in par-
allel to generate complete long dances.
Given a global music features mgextracted by the
1527
Lodge
TE
TE
Librosa
Music feature
Genre 
EmbeddingGGlobal Diffusion
Hard/Soft Guidance
LD LD LD LD
ConcatKeymotion 
for soft 
guidanceKeymotion
for hard 
guidance
Choreography 
AugmentGlobal music 
feature
Concatenate 
feature
Generated danceConcat Concat ConcatSplit
 
g 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
g 
g 
g 
g 
ğ‘šğ‘™ğ‘– 
ğ‘šğ‘” 
ğ‘‘â„ 
ğ‘‘ğ‘  
ğ‘‘ CA CA CA CA
CA
Concat
g 
ğ‘šğ‘™ Lodge
L framesN frames
Diffusion 
TimeStepsTMLPConcat
Transformer 
Encoder
Music 
FeatureGenre 
Embedding
Self-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLMSelf-AttentionDense FiLMMLPDense FiLM
Cross -AttentionDense FiLMCond t
Forward Kinematics
ğ‘“ğ‘œğ‘œğ‘¡ğ‘£ 
ğ‘“ğ‘œğ‘œğ‘¡ğ‘ 
Transformer EncoderCond
Concat
CondFoot Refine Block Cond
Global Average Pooling
Jazz
 Urban KoreanSequence
Transformer BlockLinear
Real/Fake? Real/Fake? Real/Fake?
ğ‘‘ğ‘™ğ‘‡ 
ğ‘‘ğ‘™ğ‘‡âˆ’1 
Lodge Lodge
L framesN frames
Multi Genre 
Discriminator 
T=T-1
Denoising 
ğ‘‘ ğ‘™=ğ‘‘ğ‘™0 TE
TE
Librosa
Music feature
Genre Embedding GGlobal Diffusion
LD LD LD LDConcat
Generated 
danceConcat Concat ConcatSplit
 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
ğ‘šğ‘” Diffusion 
Time StepsT
Characteristic
Dance Primitives
Noise
ğ‘‘ğ‘™2
ğ‘‡   
ğ‘‘ğ‘™1
ğ‘‡   
ğ‘‘ğ‘™3
ğ‘‡   
ğ‘‘ğ‘™4
ğ‘‡   
4 frames 8 frames
n frames
ğ‘‘ğ‘™1
ğ‘‡âˆ’1 
ğ‘‘ğ‘™2
ğ‘‡âˆ’1 
ğ‘‘ğ‘™3
ğ‘‡âˆ’1 
ğ‘‘ğ‘™4
ğ‘‡âˆ’1 
ğ‘‘ğ‘    
ğ‘‘â„   
ğ‘‘â€²
ğ‘™3
ğ‘‡   
ğ‘‘â€²
ğ‘™4
ğ‘‡   
ğ‘‘â€²
ğ‘™2
ğ‘‡   
ğ‘‘â€²
ğ‘™1
ğ‘‡   
N frames
ğ‘‘ğ‘”ğ‘‡   Noise
4 framesAlign
 with
uneven 
music 
beats
ğ‘‘ğ‘    T=T-1
Denoising 
music beatsMirror Mirror Mirror Mirror Mirror Mirror Mirror MirrorChoreography Augment Operation
Hard/Soft Diffusion Guidance
ğ‘“ğ‘œğ‘œğ‘¡ğ‘ Music
Generated 
danceParallel
Lodges
Number
of dance 
frames
Linear
Linear
TE
TE
Librosa
Music feature
Genre Embedding GGlobal Diffusion
LD LD LD LDConcat
Generated 
danceConcat Concat ConcatSplit
 
g 
ğ‘šğ‘™1 
ğ‘šğ‘™2 
ğ‘šğ‘™3 
ğ‘šğ‘™4 
ğ‘šğ‘” Diffusion 
Time StepsT
Characteristic
Dance Primitives
Noise
ğ‘‘ğ‘™2
ğ‘‡   
ğ‘‘ğ‘™1
ğ‘‡   
ğ‘‘ğ‘™3
ğ‘‡   
ğ‘‘ğ‘™4
ğ‘‡   
4 frames 8 frames
n frames
ğ‘‘ğ‘™1
ğ‘‡âˆ’1 
ğ‘‘ğ‘™2
ğ‘‡âˆ’1 
ğ‘‘ğ‘™3
ğ‘‡âˆ’1 
ğ‘‘ğ‘™4
ğ‘‡âˆ’1 
ğ‘‘ğ‘    
ğ‘‘â„   
ğ‘‘â€²
ğ‘™3
ğ‘‡   
ğ‘‘â€²
ğ‘™4
ğ‘‡   
ğ‘‘â€²
ğ‘™2
ğ‘‡   
ğ‘‘â€²
ğ‘™1
ğ‘‡   
N frames
ğ‘‘ğ‘”ğ‘‡   Noise
4 framesAlign
 with
music 
beats
ğ‘‘ğ‘    T=T-1
Denoising 
music beatsMirror Mirror Mirror Mirror Mirror Mirror Mirror MirrorChoreography Augment Operation
Hard/Soft Diffusion GuidanceLinear
Linear
ğ‘‘ ğ‘™ Figure 3. Training process of Local Diffusion.
Librosa[31]. We feed mginto a Transformer downsample
network, which comprises a Linear layer and a Transformer
Encoder Layer. Next, the compressed global music feature
is sent to global diffusion. We adopte the EDGE framework
as the foundation for global diffusion, making a single mod-
ification by adjusting the training objective to output sparse
dance primitives. These primitives are key motions with
only 8 frames, categorized as dhandds.
There are key motions and transition motions in dance,
where key motions are those with velocity curves near lo-
cal minima, displaying greater expressiveness and richer se-
mantic information, while transition motions are relatively
monotonic. To ensure that global diffusion concentrates
solely on generating expressive key motions. We separated
the expressive key movements and monotonous transitional
movements in the dataset and trained global diffusion with
only the expressive key motions. Since the global diffu-
sion learns key motions on a global scale, it already im-
plicitly captures some choreographic patterns. To further
enhance the overall dance coherence, we do choreography
augment operation on ds, guiding the local diffusion to pro-
duce dance that more closely adheres to choreography rules.
3.4. Local Diffusion
Training Process. Thanks to our coarse to fine diffusion
architecture, the local diffusion only needs to train the net-
work on nframes, which greatly accelerates the training
speed and allows local diffusion to focus on the details
of the dance movements for a few seconds. The training
process of local diffusion can be seen in Figure 3. We
follow EDGE to build the Sequence Transformer Block,
which consists of self-attention layer[47], cross-attention
layer[38], multi-layer perception layer and the feature-wise
linear modulation (FilM)[36].In addition to the reconstruction loss, we introduce sev-
eral other losses to enhance training stability and physical
realism like previous works [45, 46]. We compute the po-
sitional coordinates d(i)
jointof the human body joints using
forward kinematics, and then get the joint velocity d(i)
j-vel
and joint acceleration d(i)
j-acc. We then add the following loss
functions: joint position Eq. (4), velocity Eq. (5), and ac-
celeration Eq. (6):
d(i)
joint=FK(d(i)), (3)
Ljoint=1
nnX
i=1d(i)
jointâˆ’Ë†d(i)
joint2
2, (4)
Lj-vel=1
nâˆ’1nâˆ’1X
i=1d(i)
j-velâˆ’Ë†d(i)
j-vel2
2, (5)
Lj-acc=1
nâˆ’2nâˆ’2X
i=1d(i)
j-accâˆ’Ë†d(i)
j-acc2
2. (6)
To optimize the contact between feet and the ground, we
follow LEMO[54, 57] in decoupling the horizontal and ver-
tical velocities of the feet, and optimizing the horizontal ve-
locity fhvand downward vertical velocity fdvto 0 when the
feet contact with the ground.
Lcontact =1
nâˆ’1nâˆ’1X
i=1(Ë†f(i)
hv+Ë†f(i)
dv)Â·Ë†b(i)2
2, (7)
where Ë†bis the predicted foot contact label. Our overall train-
ing object is the weighted sum of the losses:
Ltotal=Lrecon+Î»jointLjoint+Î»j-velLj-vel
+Î»j-accLj-acc+Î»contactLj-contact +Î»genreLgenre,(8)
where Î»genreis formulated as Eq. (9).
1528
Foot Refine Block. The motion is expressed in the SMPL
format, facilitating driven various human models and ren-
dering. However, representing motion in the SMPL for-
mat involves a sequence of relative rotations and motion
tree propagation. Small rotations near the root nodes, such
as the legs and knees, result in significant rotations at the
feet. Especially in dance movements, which involve a vari-
ety of foot actions, these challenges make it difficult for us
to straightforwardly resolve foot skating issues by simply
using foot-related loss functions. We argue the main issue
lies in the domain gap between the optimization objective
and the data representation. The contact status between the
feet and the ground is measured in a linear space based on
joint positions, while the motion in the SMPL format exists
in a nonlinear rotation space. To tackle this, we introduce
the Foot Refine Block inspired by [57]. This module first
computes the positions of foot keypoints foot pthrough
forward kinematics, as well as foot velocity foot v. Then
we calculate the foot-ground contact score foot cfollow
[57]. Building upon this, the Cross Attention mechanism is
employed to further optimize foot movements.
Multi Genre Discriminator. Local diffusion can produce
high-quality, diverse dance segments. As shown in the Fig-
ure 3, to ensure consistency with the overall musical style,
we also concatenate the genre embedding gwith the music
features, resulting in mg
las the condition for local diffu-
sion. We then use a multi-genre discriminator (MGD) to
control the dance genre following MNET[19]. The training
process of MGD can be formulated as:
Lgenre=EË†dlh
logMGD
Ë†dl, g,mli
+
EË†dl,th
log
1âˆ’MGD
LD
Ë†dl, g,ml
, g,mli
,(9)
Parallel Inference. Given the input mj
l, gand correspond-
ingdh, ds, the local diffusion outputs dj
l. By concatenat-
ingn
mj
loN/n
j=1along the time dimension, we obtain dg. For
simplicity in description, we omit â€˜ jâ€™ in subsequent writing.
To achieve parallel generation of long dance sequences, we
divide dhinto the first four frames and the last four frames.
The first four frames serve as the tail four frames of the pre-
vious dl, and the last four frames of dsserve as the lead-
ing four frames for the next dl. This approach requires
the local diffusion to generate the intervening dance mo-
tions coherently. However, directly using Diffusion inpaint-
ing techniques to control the first and last frames of each
segment results in incoherent motions. To address this is-
sue, we use a joint acceleration loss Lj-accand incorporate
a fine-tuning stage. In this stage, we mixture dtof Local
Diffusion and the ground truth dl0bydlâ€²
t[: 4] = dl0[: 4],
dlâ€²
t[âˆ’4 :] = dl0[âˆ’4 :],dlâ€²
t[4 :âˆ’4] =dlt[4 :âˆ’4]. The
Lreconloss in the fine-tuning stage is formulated as:
Lrecon= E dl0,t[dl0âˆ’fÎ¸ 
dlâ€²
t, g, t,ml2
2]. (10)4. Experiment
4.1. Experimental Setup
Datasets. We validate our method using the public music-
dance paired dataset FineDance[23] and AIST++[22].
FineDance employs professional dancers to perform the
dances and capture the data with an optical motion capture
system. The currently available dance data of FineDance
contains 7.7 hours, totaling 831,600 frames, with a frame
rate of 30 fps, and includes 16 different dance genres. The
average dance length of FineDance is 152.3 seconds com-
pared to 13.3 seconds for the AIST++ dataset, so we use the
FineDance dataset to train and test the long-term dance gen-
eration algorithm. We test the 20 pieces of music in the test
set of the FineDance dataset and generate dance sequences
with a length of 1024 frames (34.13 seconds).
AIST++ is also a widely used dance dataset, containing
5.2 hours of dance data, with a frame rate of 60 fps, and
includes 10 dance genres.
Implementation details. In the experiments on the
FineDance dataset, the global music feature length Nis
1024, corresponding to 34.13 seconds; the local music fea-
ture length nis 256, corresponding to 8.53 seconds. The
global diffusion output 13 characteristic dance primitives,
where 5 are dhand 8 are ds. After the choreography aug-
ments operation, dsis mirrored to produce 16 instances, and
it is aligned with the musicâ€™s beat. The optimizer of global
diffusion and local diffusion are Adan[49], we use the Ex-
ponential Moving Average(EMA) [20] strategy to make the
loss convergence process more stable. The learning rate
is1eâˆ’4. In the inference phase, we have two diffusion
sampling strategies DDPM [11] and DDIM [41] that can be
used to generate dance. On the AIST++ dataset, we down-
sampled the dance to 30 fps for training. Then we gener-
ated dances with 30 fps. Finally, we interpolated the output
dances to 60 fps and followed the experimental setup of Bai-
lando [39] for testing. The music-dance data from AIST++
has been segmented into numerous short clips. Therefore,
we change the global music feature length Nto be 256 and
the global music feature length nto be 128.
4.2. Comparisons on the FineDance dataset
As shown in Table 1, we compare our method with the ad-
vanced existing works. FACT [22] and MNET [19] are
auto-gressive dance generation methods. Bailando [39] is
an outstanding music-driven dance generation algorithm. It
employs VQ-V AE to transform dance movements into to-
kens. Subsequently, a GPT model forecasts this token se-
quence, which is then decoded to render the final dance.
To the best of our knowledge, EDGE [46] is a diffusion-
based dance generation algorithm, achieving the strongest
qualitative performance in short-duration dance generation.
During the diffusion denoising process, they assign the lat-
1529
ter half of the previous dance segment to the first half of the
subsequent segment, and utilize interpolation to maintain
consistency, thereby achieving long-term dance generation.
Motion Quality. To evaluate the motion quality of gener-
ated dance sequences, we follow the previous methods[22,
39] to calculate the Frechet Inception Distance ( FID)[10]
distance between motion features of the generated dance
and the ground truth dance sequences. The previous meth-
ods such as [39] calculate kinetic[34] and geometric[32]
motion features using the global coordinates of all the
SMPL[25] joints, which is suitable for measuring the qual-
ity of movements lasting only a few seconds. However, for
longer motion sequences, where trajectories become more
complex, this measurement approach focuses too heavily on
root positions, neglecting local movements and resulting in
data that lacks comparability. Therefore, we use the global
coordinates of the root joint and the relative distances of
other child joints to compute kinetic and geometric features.
The kinematic feature (subscript â€˜kâ€™), indicates the speed
and acceleration of the movement and reflects the physi-
cal characteristics of the dance. Therefore the FID distance
between kinematic features FIDkmeasures the physical re-
ality of the motion. The geometric feature (subscript â€˜gâ€™),
is calculated based on multiple predefined movement tem-
plates, thus the FID distance between geometric features
FIDgreflects the quality of the overall dance choreogra-
phy. In addition, we follow [18] to report the Foot Skating
Ratio (FSR) , which measures the proportion of frames in
which either foot skids more than a certain distance while
maintaining contact with the ground (foot height <5 cm).
Motion Diversity. To evaluate the motion diversity of gen-
erated dance sequences, we calculate the mean Euclidean
distance within the motion feature space, as outlined in the
works of Bailando[39]. DIV krepresents the motion diver-
sity in the kinematic feature space, while DIV gdenotes the
diversity in the geometric feature space. Table 1 reveals
that our Lodge approach achieved the highest DIV gscore,
which can be credited to our adoption of global diffusion
and characteristic dance primitives for mastering diverse
choreography patterns.
Beat Alignment Score (BAS). To evaluate the beat consis-
tency between the generated dance and the given music, we
follow [22] and use the BAS to evaluate our methods, our
approach demonstrated the highest Beat Alignment Score
of 0.2397.
Production efficiency. In our inference process, we eval-
uated the average Run time taken for model generation.
To ensure fairness in testing, we excluded data prepro-
cessing time from our calculations. All experiments were
conducted on the same computer equipped with an Nvidia
A100 GPU and 256GB of memory.
Run Time in Table 1 presents the average Run Time re-
quired to generate 1024 frames of dance movements. Bai-lando achieved a outstanding performance, but its runtime
increases linearly with the length of the sequence generated.
EDGE, using the ddim accelerated sampling strategy and
linear interpolation, also achieved a fast level for generat-
ing long dance sequences. Our method uses DDPM sam-
pling with a denoising step of 1000, taking 30.93 seconds.
Using DDIM with 100 denoising steps takes only 4.57 sec-
onds. Meanwhile, our parallel architecture ensures runtime
remains stable even with longer sequences.
User study. We conducted a user study where 20 partic-
ipants viewed 17 video pairs. Each pair consists of two
dance sequences: one created by Lodge (DDPM) and the
other by different methods or ground truth.
4.3. Comparisons on the AIST++ dataset
As Table 2 shows, we train Lodge on AIST++ and compare
it with SOTAs. Due to the lack of long-duration dance in
the AIST++ dataset, Lodgeâ€™s performance does not reach
the best metrics. However, compared to our baseline model
EDGE, Lodge shows improvement in multiple metrics.
4.4. Ablation Studies
In this section, we use DDPM sampling strategy and per-
form ablation experiments on the FineDance dataset to eval-
uate the different parts: (1) the characteristic dance primi-
tives, (2)the soft cue guidance, (3) the foot refine block.
Effect of the characteristic dance primitives. We con-
ducted a series of ablation experiments to validate the effect
of the characteristic dance primitives. In Table 3, â€˜Câ€™ in-
dicates we use characteristic dance primitives to guide the
local diffusion, â€˜Mâ€™ represents we mirror the characteristic
dance primitives. â€˜Bâ€™ denotes beat alignment, we align the
characterized dance primitives with the musicâ€™s beats, guid-
ing the Local diffusion to generate more expressive move-
ments at these beat points. If beat alignment is not applied,
then the characterized dance primitives are uniformly dis-
tributed across various timelines to guide the local diffusion.
The first row in Table 3 shows the results when not using
characteristic dance primitives, relying solely on some dh
for parallel long action generation but not using dswithin a
Local diffusion. This scenario leads to lower quality of mo-
tion (FID), diversity, and music rhythm alignment metrics.
In contrast, rows two and three, which incorporate guid-
ance from characteristic dance primitives, display signifi-
cant improvements in Div and BAS. This improvement is
because characteristic dance primitives are expressive key
motions; their inclusion helps prevent the neural network
from generating average, monotonous movements. The last
row, achieving the optimal results, demonstrates the effec-
tiveness of our strategy.
Effect of the Soft-cue Guidance. Our soft cue guidance
weight can be adjusted using the hyperparameter â€˜sâ€™, where
a larger â€˜sâ€™ value signifies a stronger effect. Table 4 demon-
1530
MethodMotion Quality Motion DiversityBASâ†‘Run Time â†“Winsâ†‘
FIDkâ†“FIDgâ†“Foot Skating Ratio â†“Divkâ†‘Divgâ†‘
Ground Truth / / 6.22 % 9.73 7.44 0.2120 / 42.6 %
FACT[22] 113.38 97.05 28.44 % 3.36 6.37 0.1831 35.88s 96.7 %
MNET[19] 104.71 90.31 39.36 % 3.12 6.14 0.1864 38.91s 92.3 %
Bailando[39] 82.81 28.17 18.76 % 7.74 6.25 0.2029 5.46s 68.2 %
EDGE[46] 94.34 50.38 20.04 % 8.13 6.45 0.2116 8.59s 80.6 %
Lodge (DDIM) 50.00 35.52 2.76% 5.67 4.96 0.2269 30.93s /
Lodge (DDPM) 45.56 34.29 5.01 % 6.75 5.64 0.2397 4.57 s /
Table 1. Compare with SOTAs on the FineDance dataset. Wins is the ratio of victories Lodge(DDPM) achieved in the user study.
MethodMotion Quality Motion DiversityBASâ†‘
FIDkâ†“FIDgâ†“Divkâ†‘Divgâ†‘
Ground Truth 17.10 10.60 8.19 7.45 0.2374
Liet al. [21] 86.43 43.46 6.85 3.32 0.1607
DanceNet [61] 69.18 25.49 2.86 2.85 0.1430
DanceRevolution [15] 73.42 25.92 3.52 4.87 0.1950
FACT [22] 35.35 22.11 5.94 6.18 0.2209
Bailando [39] 28.16 9.62 7.83 6.34 0.2332
EDGE [46] 42.16 22.12 3.96 4.61 0.2334
Lodge (DDPM) 37.09 18.79 5.58 4.85 0.2423
Table 2. Compare with SOTAs on the AIST++ dataset.
Ablations Metrics
C M B FIDkâ†“Divkâ†‘BASâ†‘
60.91 5.16 0.2090
âœ“ âœ“ 60.20 5.54 0.2132
âœ“ âœ“ 52.18 5.75 0.2139
âœ“ âœ“ âœ“ 45.56 6.75 0.2397
Table 3. Ablation study of the characteristic dance primitives.
strates the outcomes resulting from setting various â€˜sâ€™ val-
ues. With the increase in â€˜sâ€™, there is a corresponding en-
hancement in FIDkand Beat Alignment Score. The opti-
mal performance is achieved when â€˜sâ€™ is set to 1.
Method FIDkâ†“Divkâ†‘BASâ†‘
Ground Truth / 9.73 0.2120
s=0 60.91 5.16 0.2090
s=0.05 59.66 5.43 0.2131
s=0.25 60.51 5.41 0.2132
s=0.5 60.46 5.35 0.2196
s=0.75 59.89 5.32 0.2208
s=0.95 53.63 5.37 0.2239
s=1 45.56 6.75 0.2397
Table 4. Ablation study of the soft cue guidance.Effect of the Foot Refine Block. As shown in Table 5, af-
ter incorporating the Foot Refine Block, the motion quality
FIDkhad a large improvement, especially the Foot Skating
Ratio decreased from 5.94 %to 5.01 %, which proves that
our proposed Foot Refine Block can effectively improve the
foot-ground contact quality and reduce the probability of
foot skating phenomenon.
Method FIDkâ†“Divkâ†‘BASâ†‘Foot Skating Ratio â†“
Ground Truth / 9.73 0.2120 6.22 %
w/o Foot Refine Block 53.48 6.20 0.2216 5.94 %
w. Foot Refine Block 45.56 6.75 0.2397 5.01 %
Table 5. Ablation study of the foot refine block.
5. Conclusion and Limitation
In this work, we introduce Lodge, a two-stage coarse-to-fine
diffusion network, and propose characteristic dance primi-
tives as intermediate-level representations for the two diffu-
sion models. Lodge has been extensively evaluated through
user studies and standard metrics. Our generated samples
demonstrate that Lodge can parallelly generate dances that
conform to choreographic rules while preserving local de-
tails and physical realism. However, our method currently
cannot generate dance movements with hand gestures or fa-
cial expressions, which are also crucial for performances.
This limitation opens avenues for future research.
Acknowledgment
This work was supported in part by the Shenzhen Key
Laboratory of next generation interactive media inno-
vative technology (No.ZDSYS20210623092001004),
in part by the China Postdoctoral Science Foundation
(No.2023M731957), in part by the National Natural Sci-
ence Foundation of China under Grant 62306165, in part
by the the Peng Cheng Laboratory (PCL2023A10-
2), in part by the NSFC project No.62125107.
1531
References
[1] The three-phase choreographic process. https : / /
www.britannica.com/art/dance/The-three-
phase-choreographic-process . 2, 3
[2] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:
Gesture diffusion model with clip latents. arXiv preprint
arXiv:2303.14613 , 2023. 2
[3] Alexander Berman and Valencia James. Kinetic imagina-
tions: exploring the possibilities of combining ai and dance.
InTwenty-Fourth International Joint Conference on Artifi-
cial Intelligence , page 2431â€“2437, 2015. 3
[4] Lynne Anne Blom and L Tarin Chaplin. The intimate act of
choreography . University of Pittsburgh Pre, 1982. 2, 3
[5] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-
Chen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster:
choreography-oriented music-driven dance synthesis. ACM
Transactions on Graphics (TOG) , 40(4):1â€“13, 2021. 2, 3
[6] Marianela Ciolfi Felice, Sarah Fdili Alaoui, and Wendy E
Mackay. How do choreographers craft dance? designing for
a choreographer-technology partnership. In Proceedings of
the 3rd International Symposium on Movement and Comput-
ing, pages 1â€“8, 2016. 3
[7] Xiao Dong, Xunlin Zhan, Yunchao Wei, Xiaoyong Wei,
Yaowei Wang, Minlong Lu, Xiaochun Cao, and Xiaodan
Liang. Entity-graph enhanced cross-modal pretraining for
instance-level product retrieval. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence , 2023. 3
[8] Chunming He, Chengyu Fang, Yulun Zhang, Kai Li, Longx-
iang Tang, Chenyu You, Fengyang Xiao, Zhenhua Guo, and
Xiu Li. Reti-diff: Illumination degradation image restora-
tion with retinex-based latent diffusion model. arXiv preprint
arXiv:2311.11638 , 2023. 3
[9] Chunming He, Kai Li, Yachao Zhang, Yulun Zhang, Zhen-
hua Guo, Xiu Li, Martin Danelljan, and Fisher Yu. Strategic
preys make acute predators: Enhancing camouflaged object
detectors by generating camouflaged objects. 2024. 3
[10] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 7
[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems , 33:6840â€“6851, 2020. 3, 6
[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els.arXiv preprint arXiv:2210.02303 , 2022. 3
[13] Sepp Hochreiter and J Â¨urgen Schmidhuber. Long short-term
memory. Neural computation , 9(8):1735â€“1780, 1997. 3
[14] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang,
and Daxin Jiang. Dance revolution: Long-term dance gen-
eration with music via curriculum learning. arXiv preprint
arXiv:2006.06119 , 2020. 2
[15] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang,
and Daxin Jiang. Dance revolution: Long-term dance gen-eration with music via curriculum learning. arXiv preprint
arXiv:2006.06119 , 2020. 8
[16] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu
Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-
based generation, optimization, and planning in 3d scenes.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 16750â€“16761, 2023.
3
[17] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang,
Michael J Black, Krikamol Muandet, and Siyu Tang. Grasp-
ing field: Learning implicit representations for human
grasps. In 2020 International Conference on 3D Vision
(3DV) , pages 333â€“344. IEEE, 2020. 2
[18] Korrawe Karunratanakul, Konpat Preechakul, Supasorn
Suwajanakorn, and Siyu Tang. Guided motion diffusion for
controllable human motion synthesis. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2151â€“2162, 2023. 7
[19] Jinwoo Kim, Heeseok Oh, Seongjean Kim, Hoseok Tong,
and Sanghoon Lee. A brand new dance partner: Music-
conditioned pluralistic dancing controlled by multiple dance
genres. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 3490â€“
3500, 2022. 1, 3, 6, 8
[20] Frank Klinker. Exponential moving average versus moving
exponential average. Mathematische Semesterberichte , 58:
97â€“107, 2011. 6
[21] Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang,
Sanja Fidler, and Hao Li. Learning to generate diverse dance
motions with transformer. arXiv preprint arXiv:2008.08171 ,
2020. 8
[22] Ruilong Li, Shan Yang, David A Ross, and Angjoo
Kanazawa. Ai choreographer: Music conditioned 3d dance
generation with aist++. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 13401â€“
13412, 2021. 2, 3, 6, 7, 8
[23] Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su,
Zeping Ren, Han Zhang, Yansong Tang, and Xiu Li.
Finedance: A fine-grained choreography dataset for 3d full
body dance generation. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 10234â€“
10243, 2023. 1, 3, 6
[24] Ronghui Li, Yuqin Dai, Yachao Zhang, Jun Li, Jian Yang, Jie
Guo, and Xiu Li. Exploring multi-modal control in music-
driven dance generation. arXiv preprint arXiv:2401.01382 ,
2024. 3
[25] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J. Black. SMPL: A skinned multi-
person linear model. ACM Transactions on Graphics, (Proc.
SIGGRAPH Asia) , 34(6):248:1â€“248:16, 2015. 2, 3, 7
[26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting
using denoising diffusion probabilistic models. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 11461â€“11471, 2022. 2, 4
[27] Yue Ma, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li,
and Yu Qiao. Visual knowledge graph for human action rea-
1532
soning in videos. In Proceedings of the 30th ACM Interna-
tional Conference on Multimedia , pages 4132â€“4141, 2022.
3
[28] Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xin-
tao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Magic-
stick: Controllable video editing via control handle transfor-
mations. arXiv preprint arXiv:2312.03047 , 2023.
[29] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying
Shan, Xiu Li, and Qifeng Chen. Follow your pose:
Pose-guided text-to-video generation using pose-free videos.
arXiv preprint arXiv:2304.01186 , 2023. 3
[30] Adriano Manfr `e, Ignazio Infantino, Filippo Vella, and Salva-
tore Gaglio. An automatic system for humanoid dance cre-
ation. Biologically Inspired Cognitive Architectures , 15:1â€“9,
2016. 3
[31] Brian McFee, Colin Raffel, Dawen Liang, Daniel P Ellis,
Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa:
Audio and music signal analysis in python. In Proceedings
of the 14th python in science conference , pages 18â€“25, 2015.
3, 5
[32] Meinard M Â¨uller, Tido R Â¨oder, and Michael Clausen. Efficient
content-based retrieval of motion capture data. In ACM SIG-
GRAPH 2005 Papers , pages 677â€“685. 2005. 7
[33] Ferda Ofli, Engin Erzin, Y Â¨ucel Yemez, and A Murat Tekalp.
Learn2dance: Learning statistical music-to-dance mappings
for choreography synthesis. IEEE Transactions on Multime-
dia, 14(3):747â€“759, 2011. 1, 3
[34] Kensuke Onuma, Christos Faloutsos, and Jessica K Hodgins.
Fmdistance: A fast and effective distance function for mo-
tion capture data. In Eurographics (Short Papers) , pages 83â€“
86, 2008. 7
[35] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine,
and Sanja Fidler. Ase: Large-scale reusable adversarial
skill embeddings for physically simulated characters. ACM
Transactions On Graphics (TOG) , 41(4):1â€“17, 2022. 3
[36] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-
moulin, and Aaron Courville. Film: Visual reasoning with a
general conditioning layer. In Proceedings of the AAAI con-
ference on artificial intelligence , 2018. 5
[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj Â¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 10684â€“10695, 2022. 3
[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems , 35:36479â€“36494, 2022. 5
[39] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,
Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando:
3d dance generation by actor-critic gpt with choreographic
memory. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 11050â€“
11059, 2022. 1, 2, 3, 6, 7, 8
[40] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,
Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando++:3d dance gpt with choreographic memory. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 2023. 1
[41] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020. 6
[42] Teng Sun, Juntong Ni, Wenjie Wang, Liqiang Jing, Yinwei
Wei, and Liqiang Nie. General debiasing for multimodal
sentiment analysis. In Proceedings of the 31st ACM Interna-
tional Conference on Multimedia , pages 5861â€“5869. ACM,
2023. 3
[43] Red Bull Editorial Team. How to choreograph a dance: 10
tips from the pros. https://www.redbull.com/za-
en/how-to-choreograph-a-dance , 2020. 2, 3
[44] Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal
Chechik, and Xue Bin Peng. Calm: Conditional adversarial
latent models for directable virtual characters. In ACM SIG-
GRAPH 2023 Conference Proceedings , pages 1â€“9, 2023. 3
[45] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,
Daniel Cohen-Or, and Amit H Bermano. Human motion dif-
fusion model. arXiv preprint arXiv:2209.14916 , 2022. 2,
5
[46] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:
Editable dance generation from music. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 448â€“458, 2023. 2, 3, 4, 5, 6, 8
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 3, 5
[48] Yan Wu, Jiahao Wang, Yan Zhang, Siwei Zhang, Otmar
Hilliges, Fisher Yu, and Siyu Tang. Saga: Stochastic whole-
body grasping with contact. In European Conference on
Computer Vision , pages 257â€“274. Springer, 2022. 2
[49] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and
Shuicheng Yan. Adan: Adaptive nesterov momentum al-
gorithm for faster optimizing deep models. arXiv preprint
arXiv:2208.06677 , 2022. 6
[50] Zunnan Xu, Yachao Zhang, Sicheng Yang, Ronghui Li,
and Xiu Li. Chain of generation: Multi-modal gesture
synthesis via cascaded conditional control. arXiv preprint
arXiv:2312.15900 , 2023. 3
[51] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen,
Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using
human feedback to fine-tune diffusion models without any
reward model. arXiv preprint arXiv:2311.13231 , 2023. 3
[52] Siqi Yang, Zejun Yang, and Zhisheng Wang. Longdanced-
iff: Long-term dance generation with conditional diffusion
model. arXiv preprint arXiv:2308.11945 , 2023. 2
[53] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-
fuse: Text-driven human motion generation with diffusion
model. arXiv preprint arXiv:2208.15001 , 2022. 3
[54] Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys,
and Siyu Tang. Learning motion priors for 4d human body
capture in 3d scenes. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 11343â€“
11353, 2021. 2, 5
1533
[55] Yan Zhang and Siyu Tang. The wanderings of odysseus
in 3d scenes. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 20481â€“
20491, 2022. 3
[56] Yuxiang Zhang, Zhe Li, Liang An, Mengcheng Li, Tao Yu,
and Yebin Liu. Lightweight multi-person total motion cap-
ture using sparse multi-view cameras. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5560â€“5569, 2021. 2, 3
[57] Yuxiang Zhang, Hongwen Zhang, Liangxiao Hu, Hongwei
Yi, Shengping Zhang, and Yebin Liu. Real-time monocu-
lar full-body capture in world space via sequential proxy-to-
motion learning. arXiv preprint arXiv:2307.01200 , 2023. 2,
5, 6
[58] Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and
Siyu Tang. Synthesizing diverse human motions in 3d indoor
scenes. arXiv preprint arXiv:2305.12411 , 2023. 3
[59] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and
Hao Li. On the continuity of rotation representations in neu-
ral networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 5745â€“
5753, 2019. 3
[60] Haolin Zhuang, Shun Lei, Long Xiao, Weiqin Li, Liyang
Chen, Sicheng Yang, Zhiyong Wu, Shiyin Kang, and He-
len Meng. Gtn-bailando: Genre consistent long-term 3d
dance generation based on pre-trained genre token network.
InICASSP 2023-2023 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) , pages
1â€“5. IEEE, 2023. 2, 3
[61] Wenlin Zhuang, Congyi Wang, Siyu Xia, Jinxiang Chai, and
Yangang Wang. Music2dance: Music-driven dance genera-
tion using wavenet. arXiv preprint arXiv:2002.03761 , 3(4):
6, 2020. 8
[62] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang
Wang, Ming Shao, and Siyu Xia. Music2dance: Dancenet
for music-driven dance generation. ACM Transactions on
Multimedia Computing, Communications, and Applications
(TOMM) , 18(2):1â€“21, 2022. 3
1534
