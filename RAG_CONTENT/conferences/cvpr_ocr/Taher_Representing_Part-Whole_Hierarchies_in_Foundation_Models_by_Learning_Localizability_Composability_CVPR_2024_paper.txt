Representing Part-Whole Hierarchies in Foundation Models by Learning
Localizability, Composability, and Decomposability
from Anatomy via Self-Supervision
Mohammad Reza Hosseinzadeh Taher1Michael B. Gotway2Jianming Liang1
1Arizona State University2Mayo Clinic
{mhossei2,jianming.liang }@asu.edu Gotway.Michael@mayo.edu
Abstract
Humans effortlessly interpret images by parsing them into
part-whole hierarchies; deep learning excels in learning
multi-level feature spaces, but they often lack explicit cod-
ing of part-whole relations, a prominent property of med-
ical imaging. To overcome this limitation, we introduce
Adamâ€“v2, a new self-supervised learning framework ex-
tending Adam [69] by explicitly incorporating part-whole
hierarchies into its learning objectives through three key
branches: (1) Localizability, acquiring discriminative rep-
resentations to distinguish different anatomical patterns;
(2) Composability, learning each anatomical structure in
a parts-to-whole manner; and (3) Decomposability, com-
prehending each anatomical structure in a whole-to-parts
manner. Experimental results across 10 tasks, compared
to 11 baselines in zero-shot, few-shot transfer, and full fine-
tuning settings, showcase Adamâ€“v2â€™s superior performance
over large-scale medical models and existing SSL methods
across diverse downstream tasks. The higher generality and
robustness of Adamâ€“v2â€™s representations originate from its
explicit construction of hierarchies for distinct anatomical
structures from unlabeled medical images. Adamâ€“v2 pre-
serves a semantic balance of anatomical diversity and har-
mony in its embedding, yielding representations that are
both generic and semantically meaningful, yet overlooked
in existing SSL methods. All code and pretrained models
are available at GitHub.com/JLiangLab/Eden.
1. Introduction
Human perception effortlessly parses visual scenes into
part-whole hierarchies [39â€“41]. For instance, when inter-
preting a chest radiograph, even untrained observers can
quickly form a hierarchy by dividing the lower respiratory
tract into the left and right lungs, whereas more experienced
observers can invoke further sub-hierarchies (see Sec. 1).
Deep learning has enabled breakthroughs in learning visual
Lungs
Right Lung
Left Lung
Superior Lobe
MiddleLobe
InferiorLobe
Superior Lobe
InferiorLobe(a) Human perception(b) Our framework
Input imagesCo-segmented partsFigure 1. Human perception effortlessly organizes objects into
hierarchies to understand their part-whole relationships in images.
Taking lungs as an example in (a), even a non-radiologist can form
a hierarchy of the right and left lungs, whereas a radiologist can
further â€œseeâ€ the lobes in sub-hierarchies. To emulate this ability,
we introduce a self-supervised learning framework that explicitly
learns to encode inherent part-whole hierarchies within medical
images into an embedding space, leading to the development of a
powerful model (Adamâ€“v2) that is foundational to medical imag-
ing. Adam-v2 can transform each pixel in medical images (e.g.,
chest radiographs in (b)) into semantically meaningful embed-
dings (Eveâ€“v2), forming multiple â€œ echo chambers â€ (produced via
co-segmentation [1, 97])â€”different anatomical structures are as-
sociated with distinct embeddings, and the same anatomical struc-
tures have (nearly) identical embeddings across patients.
representation at multiple levels. However, the multi-level
feature space learned by deep models does not explicitly
code part-whole hierarchies with necessary semantic infor-
mation to indicate hierarchical relationships among wholes
and their constituent parts [39, 59].
To mimic the human ability to understand part-whole hi-
erarchies in images, Hinton, in his idea paper [39], intro-
duced an imag inary system ( i.e., GLOM), aiming to sig-
nify the importance of explicitly presenting part-whole hi-
erarchies in a neural network. Inspired by the conceptual
idea underlying GLOM, we devise a novel self-supervised
learning (SSL) framework, leading to a functioningsystem
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11269
that, from medical images, autodidactically constructs a hi-
erarchy of embeddings for distinct anatomical structures,
semantically balancing anatomical diversity and harmony
at each level and conveying parental â€œwholeâ€ at the higher
level and filial â€œpartsâ€ at the lower level.
Our framework, as illustrated in Fig. 2, comprises three
branches: (1) â€œlocalizabilityâ€, which compels the model to
learn a semantically structured embedding space by dis-
criminating between different anatomical structures, (2)
â€œcomposabilityâ€, which empowers the model to learn part-
whole relations by constructing each anatomical struc-
ture through the integration of its constituent parts, and
(3) â€œdecomposabilityâ€, which encourages the model to
learn whole-part relations by decomposing each anatomi-
cal structure into its constituent parts. Unifying these three
branches together in a coarse to fine learning approach, the
localizability branch enables the model to preserve harmony
in embeddings of semantically similar anatomical struc-
tures in a hierarchy of scales. Simultaneously, compos-
ability and decomposability branches empower the model
to not only convey hierarchical relationships but also pre-
serve diversity of semantically similar anatomical structures
across patients through encoding finer-grained anatomical
information of their constituent parts. We call our system
(i.e., pretrained model) Adamâ€“v2 because it represents a
significant advancement from our previous versionâ€”Adam
(autodidactic dense anatomical models) [69]â€”that learns
autodidactically and yields dense anatomical embedding,
nicknamed Eveâ€“v2 (embedding vectors) for semantic rich-
ness. We further coin our project site Eden (environment
fordense embeddings and networks), where all code, pre-
trained Adamâ€“v2, and Eveâ€“v2 are placed.
We extensively evaluate Adamâ€“v2 in (1)Zero-shot set-
ting(Â§4.1): Adamâ€“v2 yields more semantically meaningful
embeddings (Eveâ€“v2) compared with existing SSL meth-
ods with a set of unique properties essential for anatomy
understanding (Figs. 3 to 6); (2)Few-shot transfer set-
ting (Â§4.2): Adamâ€“v2 outperforms 2 large-scale medical
models, RadImageNet and LVM-Med, as well as a rep-
resentative set of 7 SSL methods by a remarkable mar-
gin in anatomical structure and disease segmentation tasks
(Tab. 1); and (3)Full fine-tuning setting (Â§4.3): Adamâ€“v2
provides more generalizable representations compared to
fully-supervised and SSL baselines across a myriad of tasks
(Fig. 7 & Tab. 2). Our main contributions are as follows:
â€¢ A new self-supervised learning strategy, called Adamâ€“
v2, that encodes inherent hierarchical relationships within
medical images, yielding discriminative representations
blended with semantics of part-whole relations.
â€¢ A comprehensive set of experiments proves higher gener-
alizability and robustness of Adamâ€“v2, particularly high-
lighting Adamâ€“v2â€™s proficiency in few-shot transfer and
achieving a new record in the ChestX-ray14 benchmark.â€¢ A set of quantitative and qualitative feature analyses that
opens up novel perspectives for assessing anatomy under-
standing from various viewpoints.
2. Method
Our framework, depicted in Fig. 2, aims to underpin the de-
velopment of powerful self-supervised models foundational
to medical imaging by constructing a hierarchy of embed-
dings learned from anatomy. Our framework comprises
three key branches: (1) localizability, aiming to acquire
discriminative representations for distinguishing different
anatomical structures; (2) composability, aiming to learn
each anatomical structure in a parts-to-whole manner; and
(3) decomposability, aiming to comprehend each anatom-
ical structure in a whole-to-parts manner. Seamlessly in-
tegrating these learning objectives into a unified framework
captures inherent part-whole hierarchies within medical im-
ages, yielding a powerful model (Adamâ€“v2) that can serve
not only as the foundation for myriad target tasks via adap-
tation (fine-tuning), but also its embedding vectors (Eveâ€“
v2) bear rich semantics, usable standalone without adap-
tation (zero-shot), for other tasks like landmark detection.
The following details our framework.
2.1. Learning Localizability
The localizability branch seeks to learn a semantically-
structured embedding space where similar anatomical struc-
tures are clustered together and are distinguished from dis-
similar anatomical structures. As illustrated in Fig. 2, the
localizability branch includes the student gÎ¸Sand teacher
gÎ¸Tencoders, and two projectors hÎ¸LSandhÎ¸LT, referred
to as localizability heads. The parameters of student gÎ¸S
and localizability head hÎ¸LSare learned with stochastic gra-
dient descent while the parameters of the teacher gÎ¸Tand
headhÎ¸LTare updated using an exponential moving aver-
age (EMA) on the weights of gÎ¸SandhÎ¸LS, respectively.
Given an anchor patch wrandomly sampled from the input
image I, we extract a set Cof multi-scale crops from w.
In particular, these crops exhibit diverse dimensions while
sharing the same or slightly shifted center as w, contribut-
ing to a comprehensive understanding of the same anatom-
ical structure at various resolutions. We then apply random
data augmentations T(.)onwand multi-scale crops in C.
The augmented view of wis passed to the teacher, while
the augmented views of the crops in Care passed to the
student network, generating the features yt=gÎ¸T(T(w))
andYs={gÎ¸S(T(c))|câˆˆC}, respectively. The localiz-
ability heads project the features to the output embeddings
zt=hÎ¸LT(yt)andZs={hÎ¸LS(ys)|ysâˆˆYs}, which are
normalized with a softmax function:
Pt(zt)(i)=exp(z(i)
t/Ï„t)
PK
k=1exp(z(k)
t/Ï„t)(1)
11270
WP1P2P3P4ğ‘Š!"#$âˆˆ	ğ¼!"#$,ğ¼!"#$4,ğ¼!"#$16,â€¦Coarse to fine
ğ‘Š=ğ‘ƒ!,ğ‘ƒ",ğ‘ƒ#,ğ‘ƒ$P1P2P3P4ğœ	~ğ’¯ğœ	~ğ’¯ğœ	~ğ’¯ğœ	~ğ’¯
â„’	Composabilityg	ğœƒ%Studentg	ğœƒ&TeacherEMAIEMAğœ	~ğ’¯ğœ	~ğ’¯g	ğœƒ%Studentg	ğœƒ&TeacherEMA
â„’	LocalizabilityWRandom anchorh	ğœƒ'&LocalizabilityHead
â„’	Decomposabilityg	ğœƒ%Studentg	ğœƒ&TeacherEMAh	ğœƒ'%LocalizabilityHeadh	ğœƒ(ComposabilityHeadh	ğœƒ)DecomposabilityHeadStop gradientConcatenationğœ	~ğ’¯Random augmentationsgğœƒ!	and gğœƒ"are sharedamong all three branchesFigure 2. Adamâ€“v2 learns hierarchical representations in a coarse-to-fine-manner via three branches: localizability, composability, and
decomposability. Given an anchor whole wrandomly sampled from image I, the localizability branch augment and process wand
its multi-scale views, and enforce consistency between their embeddings, yielding distinct features for different anatomical structures.
The composability branch decomposes winto a set of parts, and enforces consistency between the embedding of wand the aggregated
embeddings of its parts, encoding part-whole relations. The decomposability branch decomposes the embedding of wto acquire the
embeddings of its constituent parts, and enforce consistency between the embeddings of parts and their decomposed counterparts, capturing
whole-part relations.
where Ï„t>0is a temperature parameter controlling the
sharpness of the output distribution, and Kis the output
dimension of the localizability heads. A softmax function
Pswith temperature Ï„sis similarly employed to normalize
the features in Zs. The localizability branchâ€™s objective is
to maximize the consistency between the embeddings of the
input anchor and its augmented views. To do so, we employ
cross-entropy loss [12]:
LLocalizability =âˆ’1
|Zs|X
zsâˆˆZsPt(zt) logPs(zs) (2)
It is noteworthy that our framework offers flexibility in
utilizing various localizability loss functions. While we
opt for a self-distillation loss due to its simplicity and ef-
ficiency [12, 28, 67], alternative sophisticated objectives,
such as contrastive loss [16, 37], can also be employed.
2.2. Learning Composability
The composability branch seeks to learn the part-whole
anatomical hierarchies in a bottom-up manner by assem-
bling larger anatomical structures from their smaller con-
stituent subparts. As illustrated in Fig. 2, the composability
branch consists of the student gÎ¸Sand teacher gÎ¸Tencoders,
which are shared with the localizability branch, and a com-
posability head hÎ¸C. Given an anchor whole wrandomlysampled from the input image I, we decompose it into a
set of nnon-overlapping parts P ={pi}n
i=1. The parts
are augmented and processed by the student network, gen-
erating partsâ€™ embeddings Yps={yi=gÎ¸S(T(pi))}n
i=1.
The partsâ€™ embeddings are then concatenated and passed
to the composability head hÎ¸Cto produce the aggregated
embeddings of parts zps=hÎ¸C(âŠ•({yi}n
i=1)). Moreover,
the whole anatomical structure wis augmented and passed
to the teacher network to generate the wholeâ€™s embedding
zwt=gÎ¸T(T(w)). The composability branch is trained to
maximize the agreement between the wholeâ€™s embedding
and the the aggregated embeddings of its parts:
LComposability =â„“s(zwt, zps) (3)
where â„“s(zwt, zps)presents a function that measures sim-
ilarity between zwtandzps, such as MSE [28], cross-
entropy [12], or cosine similarity [18].
2.3. Learning Decomposability
The decomposability branch seeks to learn the whole-part
anatomical hierarchies in a top-down manner by decom-
posing larger anatomical structures into their smaller con-
stituent subparts. As shown in Fig. 2, the decomposability
branch comprises the student gÎ¸Sand teacher gÎ¸Tencoders,
11271
which are shared with the localizability and composabil-
ity branches, and a decomposability head hÎ¸D. Given an
anchor whole w, we decompose it into a set of nnon-
overlapping parts P ={pi}n
i=1. The anchor whole wis
augmented and fed into the student network, producing the
wholeâ€™s embedding zws=gÎ¸S(T(w)). The wholeâ€™s em-
bedding is then passed to the decomposability head hÎ¸D,
which decomposes it into a set of individual embeddings
corresponding to the constituent parts of the whole Zps=
hÎ¸D(zws). Additionally, the parts P ={pi}n
i=1are aug-
mented and processed by the teacher network, generating
partsâ€™ embeddings Zpt={gÎ¸T(T(pi))}n
i=1. The decom-
posability branch is trained to maximize the agreement be-
tween the embeddings of the individual parts and their de-
composed counterparts:
LDecomposability =1
|P||P|X
i=1â„“s(zpi, zpâ€²
i) (4)
where zpiâˆˆZptandzpâ€²
iâˆˆZps, and â„“s(zpi, zpâ€²
i)presents a
function that measures similarity between zpiandzpâ€²
i, such
as MSE, cross-entropy, or cosine similarity.
2.4. Training Pipeline
To guide the model in learning hierarchical representations,
we consider a hierarchy of diverse anatomical structures at
various scales. Specifically, the highest level of the hierar-
chy represents entire images (of spatial resolution (HÃ—W))
with complete anatomy, while each subsequent level m
âˆˆ {1,2...}represents anatomical structures wat a scale
of(H
2mÃ—W
2m), randomly sampled from the images. In a
coarse to fine manner, the anatomical structures wat each
level are fed as the input to the localizability, composability,
and decomposability branches, and are learned through the
following combined loss function:
L=Î»1âˆ— LLocalizability +Î»2âˆ— LComposability +Î»3âˆ— LDecomposability (5)
where Î»1,Î»2,Î»3are coefficients denoting the weight of
each loss term. Through our unified training scheme,
Adamâ€“v2 learns a rich embedding space that preserves har-
mony among similar anatomical structures and encoding
their hierarchical relations.
3. Implementation Details
Pretraining protocol. We use unlabeled chest radio-
graphs and color fundus photographs for pretraining Adamâ€“
v2 on two imaging modalities. Our SSL framework is
architecture-neutral and compatible with any ConvNet and
vision transformer backbones. As an illustration, we pre-
train Adamâ€“v2 with ResNet-50 [36], ViT-S [22], and
ConvNeXt-B [56] backbones. We follow [12] in optimiza-
tion settings (e.g. optimizer, learning rate schedule, Ï„t,Ï„s,etc), updating teacher weights, and architecture of hÎ¸LSand
hÎ¸LTheads. hÎ¸CandhÎ¸Dare two-layer MLP heads. We
use MSE as â„“s(.)in Eqs. (3) and (4). Î»1,Î»2,Î»3are set to
1,nto 4, and mup to 4. In localizability branch, follow-
ing [11, 12], we extract one 2242global view and eight 962
multi-scale crops from w. For other branches, we use input
resolution 2242. Augmentation T(.)includes color jittering,
Gaussian blur, and rotation. To prove the scalability of our
framework, we train a large-scale model using ConvNeXt-
B backbone and a large corpus of 926,028 images collected
from 13 different public chest X-ray datasets.
Evaluations. We evaluate our framework in zero-shot,
few-shot, and full transfer settings. We consider 10 down-
stream tasks on 9 publicly available datasets for fine-tuning
settings, including JSRT [73], VinDR-Rib [61], ChestX-
Det [53], SIIM-ACR [88], VinDr-CXR [62], NIH Shen-
zhen [46], ChestX-ray14 [76], DRIVE [9], and Drishti-
GS [66]. These tasks rigorously evaluate the generalizabil-
ity of our Adamâ€“v2 across a range of applications, diseases,
anatomical structures, and modalities.
Baselines. We compare Adamâ€“v2 with a representative set
of seven SOTA publicly-available SSL baselines, encom-
passing ConvNet- and transformer-based methods. These
baselines represent diverse objectives at instance-, patch-,
and pixel-level, among which TransVW [33], PCRL [94],
DiRA [34], and Medical-MAE [79] represent SOTA meth-
ods tailored for medical tasks. All SSL baselines are pre-
trained on the same datasets as our Adamâ€“v2 by follow-
ing their official settings. Moreover, we compare Adamâ€“v2
with the publicly available and official models of two recent
large-scale medical models: RadImageNet [57] and LVM-
Med [60], pretrained on 1.3 million medical images in fully-
supervised and self-supervised manners, respectively.
Fine-tuning protocol. Following the standard trans-
fer learning protocol [43], Adamâ€“v2â€™s pretrained teacher
model has been fine-tuned for (1) classification tasks by ap-
pending a task-specific head, and (2) segmentation tasks by
utilizing a U-Net network [65], where the encoder is initial-
ized with the pretrained weights. We run each method for
each task at least five times. We provide statistical analysis
using an independent two-sample t-test.
4. Results and Analysis
4.1. Adamâ€“v2 demonstrates zero-shot anatomy un-
derstanding, offering semantics-rich embed-
dings over existing SSL methods
This section showcases the anatomy understanding capabil-
ities of our framework by delving into the unique learned
and emergent properties of our Adamâ€“v2â€™s embeddings
(Eveâ€“v2) in various zero-shot settings.
(1)Localazability: We investigate Adamâ€“v2â€™s capability in
discriminating different anatomical structures to determine
11272
RadImageNet
LVM-Med
DINO
DenseCL
DiRA
Adam-v2 (ours)
RadImageNetLVM-MedDINODenseCLDiRAAdam-v2Intra-cluster distancesL2 distanceFigure 3. Adamâ€“v2 learns localizability of anatomical structures,
providing discriminative features for different landmarks. Same-
colored points are instances of the same landmark across images.
if the learned embeddings (Eveâ€“v2) preserve the locality
of anatomical structures. To do so, we create a dataset of
1,000 images (from the ChestX-ray14 dataset) with 10 dis-
tinct anatomical landmarks manually annotated by human
experts in each image (see Fig. 3). We extract patches of
size2242around each landmarkâ€™s location across images
and extract latent features of each landmark instance using
each pretrained model under study (with nofine-tuning).
We then visualize the embeddings with t-SNE [72] plot. We
compare Adamâ€“v2 with the RadImageNet, LVM-Med, and
a representative set of SSL methods. As seen in Fig. 3, the
baselines fall short in generating distinct features for dif-
ferent landmarks, leading to ambiguous embedding spaces
with mixed clusters. By contrast, our Adamâ€“v2 effectively
discriminates between various anatomical landmarks, re-
sulting in well-separated clusters within its learned embed-
ding space. We complement our qualitative results (t-SNE
plots) with quantitative results (box plots) by calculating
intra-cluster distance for each landmark class and visual-
izing the distances distributions with boxplots in Fig. 3. As
seen, our Adamâ€“v2 exhibits lower median distances, indi-
cating more cohesive clusters, compared to the baselines.
To showcase Adamâ€“v2â€™s capacity in balancing anatomical
diversity and harmony and conveying hierarchical relation-
ships, we randomly select four distinct anatomical land-
marks, extract three patches of different resolutions (la-
beled as levels 1, 2, and 3) around each landmark across
the images, and compute their embeddings with Adamâ€“v2â€™s
123123123123
123Each color represents one patientp11p43p83Figure 4. Adamâ€“v2 balances diversity and harmony in embed-
dings of similar anatomical structures across patients and scales.
Embedding similarity of whole patches vs. aggregated sub-parts
DINODenseCLDiRAAdam-v20.20.40.60.81.0
Cosine Similarity
Figure 5. Adamâ€“v2â€™s embeddings (Eveâ€“v2) encode part-whole
relations of anatomical structures.
Similarity (ğ”¼!"	ğ‘£ğ‘ .ğ”¼!)InterpolationExtrapolationSimilarity(ğ”¼!"	ğ‘£ğ‘ .ğ”¼!)ğ”¼!"=	ğ‘¡#	Ã—	ğ”¼$+1âˆ’ğ‘¡#	Ã—	ğ”¼%ğ‘¡!=	ğ‘‘"#ğ‘‘"$ğ”¼&"=	ğ‘¡'	Ã—	(ğ”¼%âˆ’ğ”¼$)+ğ”¼%ğ‘¡%=	ğ‘‘$&ğ‘‘"$
Cosine Similarity
Cosine Similarityt1=0.25t1=0.5t1=0.75t2=0.25t2=0.5t2=0.75Adam-v2Adam-v2
Figure 6. Adamâ€“v2 exhibits two emergent properties: Interpola-
tion & Extrapolation. For interpolation/extrapolation, similarity
has been computed between the interpolated/extrapolated embed-
dings ( Eâ€²
C/Eâ€²
D) and their corresponding ground truth ( EC/ED).
pretrained model. As seen in Fig. 4, the embeddings of
anatomical structures at levels 1, 2, and 3 for each landmark
are closely aligned, highlighting Adamâ€“v2â€™s capability to
preserve harmony in embeddings of semantically similar
anatomical structures across resolutions and patients. Also,
within each landmark, the embeddings of patches with lev-
els 1, 2, and 3 for the same patient (color-coded in Fig. 4)
are close, while those of different patients are well sepa-
rated, representing Adamâ€“v2â€™s capability to preserve diver-
sity of anatomical structures across patients.
(2)Composability &Decomposability :We explore Adamâ€“
v2â€™s ability to capture part-whole hierarchies, as imposed
by the composability and decomposability branches, in its
learned embeddings (Eveâ€“v2). To do so, we extract random
patches of varying sizes, called whole , from ChestX-ray14
test images. Each whole is decomposed into 2, 3, or 4 non-
overlapping parts with different sizes. We resize each whole
and its parts to 2242, extract features using pretrained mod-
11273
MethodAnatomical Structure Segmentation Disease Segmentation
JSRT-Clavicle (Dice%) JSRT-Heart (Dice%) SIIM-ACR (Dice%) ChestX-Det (IoU%)
3-shot 6-shot 12-shot 24-shot 3-shot 6-shot 12-shot 24-shot 5% 10% 5% 10%
RadImageNet [57] 55.52 71.26 82.57 83.29 73.12 75.42 89.22 91.00 54.56 61.48 64.22 67.10
LVM-Med [60] 56.87 72.99 83.48 84.10 79.45 86.94 89.98 90.78 54.13 62.31 65.11 67.14
DINO [12] 24.06 29.59 38.54 45.01 45.45 60.79 70.85 80.78 47.85 52.08 46.84 52.64
DenseCL [77] 36.43 51.31 63.03 69.13 64.88 74.43 75.79 80.06 48.07 52.32 60.18 65.76
DiRA [34] 31.42 38.59 66.81 73.06 63.76 64.47 76.10 81.42 42.44 48.27 61.63 64.86
Adamâ€“v2 (Ours) 73.59 79.57 84.00 85.96 86.88 89.87 90.47 91.39 55.61 68.11 65.92 68.17
âˆ†1 +16.7 +6.58 +0.52 +1.86 +7.43 +2.93 +0.49 +0.39 +1.05 +6.50 +0.81 +1.03
âˆ†2 +37.1 +28.2 +17.2 +12.9 +22.0 +15.4 +14.3 +9.97 +7.54 +15.7 +4.29 +2.41
Table 1. Adamâ€“v2 excels in few-shot transfer, outperforming large-scale medical models (RadImageNet and LVM-Med) and SSL baselines
across segmentation tasks. âˆ†1andâˆ†2show Adam-v2â€™s performance boosts over second-best large-scale and SSL baselines, respectively.
els, and calculate the cosine similarity between the embed-
ding of each whole and the aggregate of its parts. As seen in
Fig. 5, the box plot elements indicate that the median sim-
ilarity for our Adamâ€“v2 is significantly higher than that of
other SSL baselines. Additionally, the distribution of our
Adamâ€“v2â€™s similarity values is highly concentrated around
the 1.5x interquartile, situated at the top of the box plot.
This concentration suggests that, in most cases, the similar-
ity value between the embedding of entire wholes and their
aggregated parts is closer to 1 in our Adamâ€“v2 model.
(3)Interpolatation & (4) extrapolation :We investigate
the Adamâ€“v2â€™s capability to interpolate/extrapolate em-
beddings for a randomly chosen anatomical structure by
leveraging the embeddings of two other randomly selected
anatomical structures. For interpolation, we select two ran-
dom source coordinates (labeled as AandBin Fig. 6) and
use the established interpolation formula (refer to Fig. 6)
to interpolate a random point C. We extract 2242patches
around points A, B, and C and pass them through each pre-
trained model under study to extract their respective em-
beddings EA,EB, andEC, where ECserves as the ground
truth for evaluating the interpolated embeddings for C. Sub-
sequently, we apply the interpolation formula to generate
embeddings for C based on EAandEB, resulting in in-
terpolated embeddings Eâ€²
C. Finally, we compute the co-
sine similarity between the interpolated embeddings Eâ€²
C
and the ground truth EC. This process was repeated for
1,000 images selected from the test images of Chest X-ray
14, employing three different values of t1(i.e., 0.25, 0.5,
and 0.75). We use boxplots to illustrate the similarity dis-
tributions in each setting. We examine extrapolation of em-
beddings for a randomly selected point D in a similar man-
ner using the extrapolation formula. The boxplots in Fig. 6
reveal the consistent superiority of our Adamâ€“v2 in deliver-
ing higher similarity between interpolated/extrapolated em-
beddings and the ground truth (with a median close to 1)
compared to other baselines. This outstanding performance
is indicative of the Adamâ€“v2â€™s capability in establishing re-
lations between anatomical structures. Itâ€™s noteworthy that
our Adamâ€“v2 model was notexplicitly trained for these
properties, and their emergence underscores the Adamâ€“v2â€™scapabilities in understanding anatomy.
4.2. Adamâ€“v2 excels in few-shot transfer, outper-
forming SOTA fully/self-supervised methods
in segmentation tasks
This section highlights the effectiveness of Adamâ€“v2 as an
effective foundation for fine-tuning deep models in segmen-
tation tasks with limited labeled data. We compare Adamâ€“
v2 with 3 SSL methods, as well as RadImageNet and LVM-
Med models, which serve as performance upper bounds.
We conduct experiments on heart and clavicle segmentation
tasks, fine-tuning the pretrained models using a few shots of
labeled data (3, 6, 12, and 24) randomly sampled from JSRT
dataset. Moreover, we conduct experiments on various tho-
racic disease segmentation tasks, fine-tuning the pretrained
models on two randomly selected label fractions (5% and
10%) of the SIIM-ACR and ChestX-Det datasets. As seen
in Tab. 1, our Adamâ€“v2 outperforms both RadImageNet
and LVM-Med across all label fractions in all tasks. For
instance, in the 3-shot transfer for clavicle and heart seg-
mentation tasks, Adamâ€“v2 surpasses LVM-Med by at least
16% and 7%, respectively. Moreover, Adamâ€“v2 provides
outstandingly better few-shot transfer performance com-
pared with SSL methods across all tasks. For instance, in
the pneumothorax segmentation task within the SIIM-ACR
dataset, our Adamâ€“v2 surpasses the runner-up baseline by
7.54% and 15.7% in the 5% and 10% labeled data subsets,
respectively. Similarly, across the 5% and 10% fractions
of the ChestX-Det dataset, our Adamâ€“v2 demonstrates no-
tably higher averages of 4.29% and 2.41% in the thoracic
diseases segmentation task. Our attribution of Adamâ€“v2â€™s
superior representations for few-shot segmentation tasks is
grounded in the significance of anatomy learning through
our SSL approach and its profound impact on representa-
tion learning, which is neglected in existing methods.
4.3. Adamâ€“v2 stands out in full transfer, unleashing
generalizable representations for a variety of
tasks
This section demonstrates the generalizability of Adamâ€“
v2â€™s representations via transfer learning to a broad range of
11274
Clavicle segmentation [JSRT] 
20 ribs segmentation [VinDR-Rib]
Pneumothorax segmentation [SIIM-ACR] 
Heart segmentation [JSRT] 
13 thoracic diseases segmentation [ChestX-Det] 
Tuberculosis classification [NIH Shenzhen CXR] 
Thoracic diseases classification [VinDR-CXR] Random init.1SupervisedImageNet2Patch-level SSLTransVW5DenseCL69PCRLDiRAMedical-MAEPixel-level SSL78statistical significance analysisn.s.No Significanceâœ¶p < 0.05âœ¶âœ¶p < 0.01âœ¶âœ¶âœ¶p < 0.001>âœ¶âœ¶âœ¶p < 0.0001Instance-level SSLMoCo-v23DINO       4Adam-v2Adam-v2(large-scale)1011Figure 7. Adamâ€“v2 provides generalizable and robust representations, outperforming SOTA self-supervised methods across diverse down-
stream tasks. Statistical significance analysis ( p <0.05) was conducted between Adamâ€“v2 and the top SSL baseline in each task.
Method # Pretraining Data AUCâ€ 
RadImageNet [57] 1.3M 80.7
LVM-Med [60] 1.3M 82.0
Medical MAE [79] 0.5M 83.0â€¡
Adamâ€“v2 ( Large-scale ) âˆ¼1M 83.4
â€ We report mean AUC over 14 diseases on the official test split of ChestX-ray14 dataset.
â€¡We adopted this performance reported by the original authors [79]; All the rest performance is ours.
Table 2. Adamâ€“v2 outperforms previous SOTA methods (offi-
cially released large-scale medical vision models) on the public
ChestX-ray14 benchmark, yielding a new record mAUC of 83.4%.
downstream tasks in a full fine-tuning setting. We compare
Adamâ€“v2 with 7 SOTA ConvNet- and vision transformer-
based SSL methods designed for both computer vision
and medical applications. We include training downstream
models from random initialization (the lower-bound base-
line) and fully-supervised ImageNet model. As seen in
Fig. 7, our Adamâ€“v2 consistently achieves superior per-
formance compared with the fully-supervised ImageNet
model, as well as significant performance boosts ( p <0.05)
compared with all SSL counterparts across all tasks.
Comparison in Public ChestX-ray14 Benchmark. To
scrutinize the scalability of our framework, we pretrained
Adamâ€“v2 with the ConvNeXt-B backbone on nearly 1M
chest X-ray images and compared it against officially re-
leased large-scale medical vision models in the ChestX-
ray14 benchmark. As seen in Tab. 2, Adamâ€“v2 hits a
new record of 83.4 in the ChestX-ray14 benchmark. This
suggests that a meticulously crafted learning strategy that
comprehends human anatomy can fully harness large-scale
data, thereby paving the way for developing powerful self-
supervised models foundational to medical imaging.4.4. Ablation Experiments
Generalizability of our framework. Our framework can
seamlessly extend to other imaging modalities. To demon-
strate this, we consider fundus images and pretrain Adamâ€“
v2 using the EyePACS dataset and then fine-tune it for two
downstream tasks, considering both low-data regimes and
full fine-tuning settings. As seen in Tab. 3, Adamâ€“v2 ex-
hibits superior performance ( p < 0.05) across tasks in
both settings compared with SSL baselines that leverage the
same pretraining data as our Adamâ€“v2. Moreover, Adamâ€“
v2 outperforms ( p < 0.05) RadImageNet and LVM-Med
models in low-data regimes and achieves superior or equiv-
alent performance in full fine-tuning scenarios.
Effect of learning objectives. We assess the impact of each
learning branch in Adamâ€“v2 by starting from localizability
and incrementally adding composability and decomposabil-
ity learning. We fine-tune the models for two downstream
tasks. As seen in the top-row of Fig. 8, augmenting local-
izability with composability learning consistently improves
performance across tasks. Moreover, the inclusion of de-
composability further enhances the performance, resulting
in significant performance boosts ( p <0.05) in both tasks
compared to standalone localizability learning.
Effect of coarse-to-fine learning. We investigate the impact
of hierarchical learning of anatomical structures at various
scales (i.e. m) by initially training Adamâ€“v2 with the entire
anatomy ( m= 0) and then progressively delving deeper
into the higher levels of anatomy hierarchy (up to level 3),
representing finer anatomical structures. As seen in bottom-
row of Fig. 8, gradual increment of data granularity from
m= 0 tom= 2 consistently improves the downstream
performance. This highlights that our coarse-to-fine learn-
ing strategy incrementally deepens the modelâ€™s anatomical
knowledge, resulting in more generic representations for
11275
MethodDRIVE (Dice%) Drishti-GS (Dice%)
10% 100% 10% 100%
Random 74.03 (0.87) 78.27 (0.40) 70.17 (10.91) 94.53 (1.72)
RadImageNet 76.53 (0.49) 78.55 (0.17) 90.37 (1.48) 96.33 (0.15)
LVM-Med 77.19 (0.75) 79.46 (0.14) 91.60 (2.19) 97.02 (0.15)
DINO 75.89 (0.63) 78.36 (0.28) 85.90 (3.27) 96.44 (0.33)
DenseCL 75.76 (0.90) 78.36 (0.47) 86.04 (3.27) 96.60 (0.01)
DiRA 75.92 (0.90) 78.52 (0.38) 91.19 (1.86) 96.76 (0.16)
Adamâ€“v2 (Ours) 78.04 (0.14)â‹†â‹†â‹†79.91 (0.18)â‹†â‹†â‹†94.04 (0.56)â‹†â‹†â‹†97.02 (0.19)
Table 3. Adamâ€“v2 outperforms SSL methods in fundus down-
stream tasks. â‹†â‹†â‹†shows statistically significant ( p <0.05) boosts.
ğ‘š=1ğ‘š=2ğ‘š=3ğ‘š=0
ğ‘š=1ğ‘š=2ğ‘š=3ğ‘š=0
ğ¿ğ¿ğ¶ğ¿ğ¶ğ·
ğ¿ğ¿ğ¶ğ¿ğ¶ğ·
Figure 8. Ablation on the impact of (a) different branches of
Adamâ€“v2 (top-row) and (b) coarse-to-fine learning (bottom-row).
myriad tasks. Additionally, no significant change in per-
formance is observed at m= 3, suggesting that pretraining
up to level 2 yields sufficiently robust representations.
5. Related Work
Self-supervised learning. A large body of SSL meth-
ods seek to learn global features via instance discrimi-
nation pretext tasks. These methods align the features
of augmented views from the same image by employing
diverse learning objectives, including contrastive learn-
ing[11, 16, 17, 19â€“21, 29, 37, 52, 87, 90], self-distillation
[10, 12, 12, 18, 27, 28, 67], and feature decorrelation
[5, 7, 23, 89, 91]. Alternatively, dense SSL methods seek
to learn local features by encoding visual patterns embed-
ded at smaller image regions. Dense contrastive learning
methods [78, 86, 93] enforce consistency between pixels at
the same spatial location [6, 63, 82], similar pixels/patches
in a feature map [6, 77], or similar image regions [80, 81,
85, 92]. On the other hand, masked image modeling meth-
ods [4, 14, 24, 38, 48, 50, 54, 58, 71, 74, 75, 83, 84] mask
random portions of the images and reconstruct the missing
parts at pixel-level. Motivated by the success in computer
vision, a broad variety of instance discrimination [2, 3, 49]
and image reconstruction methods [15, 79], along with their
integration [34, 44, 70, 94], have been explored for medical
imaging. Given such advancements, the evolution of SSL
has empowered it to serve as the cornerstone for developing
foundation models with broad applicability [8]. However,existing SSL methods overlook anatomy hierarchies in their
learning objectives, thereby lacking anatomy understanding
capabilities. By contrast, Adamâ€“v2 exploits the hierarchical
nature of anatomy to learn semantics-rich features, leading
to more pronounced models tailored for medical tasks.
Learning from anatomy. Consistent anatomy in medi-
cal imaging provides strong yet free supervision signals
for deep models to learn common anatomical represen-
tations via self-supervision [95]. Existing works revolve
around recovering anatomical patterns from transformed
images [95, 96], learning semantics of recurrent anatomical
patterns across patients [32, 33] with subsequent enhance-
ments via adversarial learning [30, 31, 34, 35], exploit-
ing spatial relationships in anatomy [64], utilizing global
and local anatomical consistency [97], and incorporating
anatomical cues to improve contrastive learning [13, 25, 45,
47]. These existing works neglect hierarchical anatomy
relations. Although our earlier method Adam [69] uses
anatomy hierarchies as soft supervisory signals, our Adamâ€“
v2 explicitly encodes part-whole hierarchies via its learning
objectives. Compared with Adam [69], Adamâ€“v2 show-
cases two significant advancements: (1) enhancing the lo-
calizability branch by eliminating negative pairs pruning,
thereby improving computational efficiency for large-scale
pretraining, (2) introducing two novel components: com-
posability and decomposability, which are crucial for cap-
turing part-whole hierarchies.
Learning part-whole hierarchies. Hierarchical repre-
sentation learning is ingrained in architectures such as
ConvNets [36, 56] and hierarchical vision transformers
(ViT) [55]. But, the multi-scale feature hierarchy of com-
mon neural networks does not explicitly align with the part-
whole hierarchy in images, leading to the advent of new
architectures for encoding part-whole hierarchies [42, 51].
Notably, GLOM [39] introduced a conceptual framework
that utilizes attention to learn part-whole hierarchies, and
subsequent works proposed ViT-based architectures to im-
plement it [26, 68]. By contrast, Adamâ€“v2 goes beyond
architecture design by introducing a new learning strategy
that encodes the semantics of part-whole hierarchies into
the embedding space through three explicit training objec-
tives: localizability, composability, and decomposability.
6. Conclusion
We present a SSL framework Adamâ€“v2 that enhances vi-
sual representations by creating a hierarchy of embeddings
for different anatomical structures. The major novelty of
our work is explicitly enforcing part-whole hierarchies via
three learning objectives. Our experiments highlight the ef-
fectiveness of Adamâ€“v2 in various tasks, surpassing a range
of baselines. We also demonstrate the semantic richness of
our learned representations, which stem from explicitly ac-
quired or autonomously emerging unique properties.
11276
References
[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel.
Deep vit features as dense visual descriptors. ECCVW What
is Motion For? , 2022. 1
[2] Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary
Beaver, Jan Freyberg, Jonathan Deaton, Aaron Loh, Alan
Karthikesalingam, Simon Kornblith, Ting Chen, Vivek
Natarajan, and Mohammad Norouzi. Big self-supervised
models advance medical image classification. In 2021
IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 3458â€“3468, 2021. 8
[3] Shekoofeh Azizi, Laura Culp, Jan Freyberg, and et al. Ro-
bust and data-efficient generalization of self-supervised ma-
chine learning for diagnostic imaging. Nature Biomedical
Engineering , 7:756â€“779, 2023. 8
[4] Yutong Bai, Zeyu Wang, Junfei Xiao, Chen Wei, Huiyu
Wang, Alan L. Yuille, Yuyin Zhou, and Cihang Xie. Masked
autoencoders enable efficient knowledge distillers. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 24256â€“24265, 2023.
8
[5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vi-
creg: Variance-invariance-covariance regularization for self-
supervised learning. CoRR , abs/2105.04906, 2021. 8
[6] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicregl: Self-
supervised learning of local visual features, 2022. 8
[7] Adrien Bardes, Jean Ponce, and Yann LeCun. VI-
CReg: Variance-invariance-covariance regularization for
self-supervised learning. In International Conference on
Learning Representations , 2022. 8
[8] Rishi Bommasani and et al. On the opportunities and risks
of foundation models, 2021. 8
[9] A Budai, R Bock, A Maier, J Hornegger, and G Michelson.
Robust vessel segmentation in fundus images. International
Journal of Biomedical Imaging , 2013. 4
[10] Yun-Hao Cao, Peiqin Sun, and Shuchang Zhou. Three
guidelines you should know for universally slimmable self-
supervised learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 15742â€“15751, 2023. 8
[11] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-
otr Bojanowski, and Armand Joulin. Unsupervised learn-
ing of visual features by contrasting cluster assignments. In
Advances in Neural Information Processing Systems , pages
9912â€“9924. Curran Associates, Inc., 2020. 4, 8
[12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Â´e JÂ´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 9650â€“9660, 2021. 3, 4, 6, 8
[13] Krishna Chaitanya, Ertunc Erdil, Neerav Karani, and Ender
Konukoglu. Contrastive learning of global and local features
for medical image segmentation with limited annotations. In
Advances in Neural Information Processing Systems , pages
12546â€“12558. Curran Associates, Inc., 2020. 8
[14] Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhen-
guo Li, and Dit-Yan Yeung. Mixed autoencoder for self-supervised visual representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 22742â€“22751, 2023. 8
[15] Liang Chen, Paul Bentley, Kensaku Mori, Kazunari Misawa,
Michitaka Fujiwara, and Daniel Rueckert. Self-supervised
learning for medical image analysis using image context
restoration. Medical image analysis , 58:101539, 2019. 8
[16] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In Proceedings of the 37th Interna-
tional Conference on Machine Learning , pages 1597â€“1607.
PMLR, 2020. 3, 8
[17] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad
Norouzi, and Geoffrey Hinton. Big self-supervised models
are strong semi-supervised learners, 2020. 8
[18] Xinlei Chen and Kaiming He. Exploring simple siamese
representation learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 15750â€“15758, 2021. 3, 8
[19] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved baselines with momentum contrastive learning,
2020. 8
[20] Xinlei Chen, Saining Xie, and Kaiming He. An empiri-
cal study of training self-supervised vision transformers. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV) , pages 9640â€“9649, 2021.
[21] Ching-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav
Vineet, Neel Joshi, Antonio Torralba, Stefanie Jegelka,
and Yale Song. Robust contrastive learning against noisy
views. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 16670â€“
16681, 2022. 8
[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representa-
tions , 2021. 4
[23] Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto,
and Nicu Sebe. Whitening for self-supervised representation
learning. In Proceedings of the 38th International Confer-
ence on Machine Learning , pages 3015â€“3024. PMLR, 2021.
8
[24] Zhanzhou Feng and Shiliang Zhang. Evolved part mask-
ing for self-supervised learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 10386â€“10395, 2023. 8
[25] Zeyu Fu, Jianbo Jiao, Robail Yasrab, Lior Drukker, Aris T.
Papageorghiou, and J. Alison Noble. Anatomy-aware con-
trastive representation learning for fetal ultrasound. In Com-
puter Vision â€“ ECCV 2022 Workshops , pages 422â€“436, 2023.
8
[26] Nicola Garau, Niccol `o Bisagno, Zeno Sambugaro, and
Nicola Conci. Interpretable part-whole hierarchies and
conceptual-semantic relationships in neural networks. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
11277
sion and Pattern Recognition (CVPR) , pages 13689â€“13698,
2022. 8
[27] S. Gidaris, A. Bursuc, G. Puy, N. Komodakis, M. Cord,
and P. Perez. Obow: Online bag-of-visual-words generation
for self-supervised learning. In 2021 IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
6826â€“6836, Los Alamitos, CA, USA, 2021. IEEE Computer
Society. 8
[28] Jean-Bastien Grill, Florian Strub, Florent Altch Â´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,
Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-
laghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and
Michal Valko. Bootstrap your own latent - a new approach to
self-supervised learning. In Advances in Neural Information
Processing Systems , pages 21271â€“21284. Curran Associates,
Inc., 2020. 3, 8
[29] Yuanfan Guo, Minghao Xu, Jiawen Li, Bingbing Ni, Xuanyu
Zhu, Zhenbang Sun, and Yi Xu. Hcsc: Hierarchical con-
trastive selective coding. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 9706â€“9715, 2022. 8
[30] Zuwei Guo, Nahid U.I. Islam, Michael B. Gotway, and
Jianming Liang. Discriminative, restorative, and adversar-
ial learning: Stepwise incremental pretraining. In Domain
Adaptation and Representation Transfer , pages 66â€“76, 2022.
8
[31] Zuwei Guo, Nahid Ul Islam, Michael B. Gotway, and Jian-
ming Liang. Stepwise incremental pretraining for integrating
discriminative, restorative, and adversarial learning. Medical
Image Analysis , page 103159, 2024. 8
[32] Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher,
Zongwei Zhou, Michael B. Gotway, and Jianming
Liang. Learning semantics-enriched representation via self-
discovery, self-classification, and self-restoration. In Medi-
cal Image Computing and Computer Assisted Intervention â€“
MICCAI 2020 , pages 137â€“147, Cham, 2020. Springer Inter-
national Publishing. 8
[33] Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher,
Zongwei Zhou, Michael B. Gotway, and Jianming Liang.
Transferable visual words: Exploiting the semantics of
anatomical patterns for self-supervised learning. IEEE
Transactions on Medical Imaging , 40(10):2857â€“2868, 2021.
4, 8
[34] Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher,
Michael B. Gotway, and Jianming Liang. Dira: Discrimina-
tive, restorative, and adversarial learning for self-supervised
medical image analysis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 20824â€“20834, 2022. 4, 6, 8
[35] Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher,
Michael B. Gotway, and Jianming Liang. Self-supervised
learning for medical image analysis: Discriminative, restora-
tive, or adversarial? Medical Image Analysis , 94:103086,
2024. 8
[36] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE Conference on Computer Vision and Pattern
Recognition , pages 770â€“778, 2016. 4, 8[37] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual repre-
sentation learning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2020. 3, 8
[38] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
DollÂ´ar, and Ross Girshick. Masked autoencoders are scalable
vision learners. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
16000â€“16009, 2022. 8
[39] Geoffrey Hinton. How to represent part-whole hierarchies
in a neural network. Neural Computation , 35(3):413â€“452,
2023. 1, 8
[40] Geoffrey E. Hinton. Some demonstrations of the effects of
structural descriptions in mental imagery. Cogn. Sci. , 3:231â€“
250, 1979.
[41] Geoffrey E. Hinton. Mapping part-whole hierarchies into
connectionist networks. Artificial Intelligence , 46(1):47â€“75,
1990. 1
[42] Geoffrey E Hinton, Sara Sabour, and Nicholas Frosst. Matrix
capsules with EM routing. In International Conference on
Learning Representations , 2018. 8
[43] Mohammad Reza Hosseinzadeh Taher, Fatemeh Haghighi,
Ruibin Feng, Michael B. Gotway, and Jianming Liang. A
systematic benchmarking analysis of transfer learning for
medical image analysis. In Domain Adaptation and Rep-
resentation Transfer, and Affordable Healthcare and AI for
Resource Diverse Global Health , pages 3â€“13, Cham, 2021.
Springer International Publishing. 4
[44] Mohammad Reza Hosseinzadeh Taher, Fatemeh Haghighi,
Michael B. Gotway, and Jianming Liang. Caid: Context-
aware instance discrimination for self-supervised learning in
medical imaging. In Proceedings of The 5th International
Conference on Medical Imaging with Deep Learning , pages
535â€“551. PMLR, 2022. 8
[45] Hongyu Hu, Tiancheng Lin, Yuanfan Guo, and et al.
Anatomy-aware self-supervised learning for aligned multi-
modal medical data. In British Machine Vision Conference ,
2022. 8
[46] Stefan Jaeger, Sema Candemir, Sameer Antani, Y `Ä±-XiÂ´ang J
WÂ´ang, Pu-Xuan Lu, and George Thoma. Two public chest x-
ray datasets for computer-aided screening of pulmonary dis-
eases. Quantitative imaging in medicine and surgery , 4(6),
2014. 4
[47] Yankai Jiang, Mingze Sun, Heng Guo, Xiaoyu Bai, Ke Yan,
Le Lu, and Minfeng Xu. Anatomical invariance modeling
and semantic alignment for self-supervised learning in 3d
medical image analysis. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
15859â€“15869, 2023. 8
[48] Ziyu Jiang, Yinpeng Chen, Mengchen Liu, Dongdong Chen,
Xiyang Dai, Lu Yuan, Zicheng Liu, and Zhangyang Wang.
Layer grafted pre-training: Bridging contrastive learning and
masked image modeling for label-efficient representations.
InThe Eleventh International Conference on Learning Rep-
resentations , 2023. 8
[49] Aakash Kaku, Sahana Upadhya, and Narges Razavian. Inter-
mediate layers matter in momentum contrastive self super-
11278
vised learning. In Advances in Neural Information Process-
ing Systems , pages 24063â€“24074. Curran Associates, Inc.,
2021. 8
[50] Lingjing Kong, Martin Q. Ma, Guangyi Chen, Eric P.
Xing, Yuejie Chi, Louis-Philippe Morency, and Kun Zhang.
Understanding masked autoencoders via hierarchical latent
variable models. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 7918â€“7928, 2023. 8
[51] Adam Kosiorek, Sara Sabour, Yee Whye Teh, and Geof-
frey E Hinton. Stacked capsule autoencoders. In Advances in
Neural Information Processing Systems . Curran Associates,
Inc., 2019. 8
[52] Junnan Li, Pan Zhou, Caiming Xiong, and Steven C. H. Hoi.
Prototypical contrastive learning of unsupervised representa-
tions, 2021. 8
[53] Jie Lian, Jingyu Liu, Shu Zhang, Kai Gao, Xiaoqing Liu,
Dingwen Zhang, and Yizhou Yu. A structure-aware relation
network for thoracic diseases detection and segmentation.
IEEE Transactions on Medical Imaging , 40(8):2042â€“2052,
2021. 4
[54] Jihao Liu, Xin Huang, Jinliang Zheng, Yu Liu, and Hong-
sheng Li. Mixmae: Mixed and masked autoencoder for effi-
cient pretraining of hierarchical vision transformers. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 6252â€“6261, 2023. 8
[55] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
2021 IEEE/CVF International Conference on Computer Vi-
sion (ICCV) , pages 9992â€“10002, 2021. 8
[56] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2022. 4, 8
[57] Xueyan Mei, Zelong Liu, Philip M. Robson, Brett Marinelli,
Mingqian Huang, Amish Doshi, Adam Jacobi, Chendi
Cao, Katherine E. Link, Thomas Yang, Ying Wang, Hayit
Greenspan, Timothy Deyer, Zahi A. Fayad, and Yang Yang.
Radimagenet: An open radiologic deep learning research
dataset for effective transfer learning. Radiology: Artificial
Intelligence , 4(5):e210315, 2022. 4, 6, 7
[58] Shlok Mishra, Joshua Robinson, Huiwen Chang, David Ja-
cobs, Aaron Sarna, Aaron Maschinot, and Dilip Krishnan.
A simple, efficient and scalable contrastive masked autoen-
coder for learning visual representations, 2022. 8
[59] Ramy Mounir, Sujal Vijayaraghavan, and Sudeep Sarkar.
STREAMER: Streaming representation learning and event
segmentation in a hierarchical manner. In Thirty-seventh
Conference on Neural Information Processing Systems ,
2023. 1
[60] Duy M. H. Nguyen, Hoang Nguyen, Nghiem T. Diep, Tan N.
Pham, Tri Cao, Binh T. Nguyen, Paul Swoboda, Nhat Ho,
Shadi Albarqouni, Pengtao Xie, Daniel Sonntag, and Math-
ias Niepert. Lvm-med: Learning large-scale self-supervised
vision models for medical imaging via second-order graph
matching, 2023. 4, 6, 7[61] Hoang C. Nguyen, Tung T. Le, Hieu H. Pham, and Ha Q
Nguyen. Vindr-ribcxr: A benchmark dataset for automatic
segmentation and labeling of individual ribs on chest x-rays.
InMedical Imaging with Deep Learning , 2021. 4
[62] Ha Q. Nguyen and et al. Vindr-cxr: An open dataset of chest
x-rays with radiologistâ€™s annotations, 2020. 4
[63] Pedro O O. Pinheiro, Amjad Almahairi, Ryan Benmalek,
Florian Golemo, and Aaron C Courville. Unsupervised
learning of dense visual representations. In Advances in Neu-
ral Information Processing Systems , pages 4489â€“4500. Cur-
ran Associates, Inc., 2020. 8
[64] Jiaxuan Pang, Fatemeh Haghighi, DongAo Ma, Nahid Ul Is-
lam, Mohammad Reza Hosseinzadeh Taher, Michael B Got-
way, and Jianming Liang. POPAR: Patch order prediction
and appearance recovery for self-supervised medical image
analysis. In MICCAI Workshop on Domain Adaptation and
Representation Transfer , pages 77â€“87. Springer, 2022. 8
[65] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
net: Convolutional networks for biomedical image segmen-
tation. In Medical Image Computing and Computer-Assisted
Intervention â€“ MICCAI 2015 , pages 234â€“241, Cham, 2015.
Springer International Publishing. 4
[66] Jayanthi Sivaswamy, S. R. Krishnadas, Gopal Datt Joshi,
Madhulika Jain, and A. Ujjwaft Syed Tabish. Drishti-gs:
Retinal image dataset for optic nerve head(onh) segmen-
tation. In 2014 IEEE 11th International Symposium on
Biomedical Imaging (ISBI) , pages 53â€“56, 2014. 4
[67] Kaiyou Song, Jin Xie, Shan Zhang, and Zimeng Luo. Multi-
mode online knowledge distillation for self-supervised visual
representation learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 11848â€“11857, 2023. 3, 8
[68] Shuyang Sun, Xiaoyu Yue, Song Bai, and Philip Torr. Visual
parser: Representing part-whole hierarchies with transform-
ers, 2022. 8
[69] Mohammad Reza Hosseinzadeh Taher, Michael B. Gotway,
and Jianming Liang. Towards foundation models learned
from anatomy in medical imaging via self-supervision.
arXiv:2309.15358 , 2023. 1, 2, 8
[70] Yucheng Tang, Dong Yang, Wenqi Li, Holger R. Roth,
Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali
Hatamizadeh. Self-supervised pre-training of swin trans-
formers for 3d medical image analysis. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 20730â€“20740, 2022. 8
[71] Chenxin Tao, Xizhou Zhu, Weijie Su, Gao Huang, Bin
Li, Jie Zhou, Yu Qiao, Xiaogang Wang, and Jifeng Dai.
Siamese image modeling for self-supervised vision represen-
tation learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , pages
2132â€“2141, 2023. 8
[72] Laurens van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of Machine Learning Research , 9
(86):2579â€“2605, 2008. 5
[73] B. van Ginneken, M.B. Stegmann, and M. Loog. Segmenta-
tion of anatomical structures in chest radiographs using su-
pervised methods: a comparative study on a public database.
Medical Image Analysis , 10(1):19â€“40, 2006. 4
11279
[74] Haochen Wang, Kaiyou Song, Junsong Fan, Yuxi Wang, Jin
Xie, and Zhaoxiang Zhang. Hard patches mining for masked
image modeling. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 10375â€“10385, 2023. 8
[75] Haoqing Wang, Yehui Tang, Yunhe Wang, Jianyuan Guo,
Zhi-Hong Deng, and Kai Han. Masked image modeling
with local multi-scale reconstruction. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2122â€“2131, 2023. 8
[76] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mo-
hammadhadi Bagheri, and Ronald M Summers. Chestx-
ray8: Hospital-scale chest x-ray database and benchmarks
on weakly-supervised classification and localization of com-
mon thorax diseases. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 2097â€“
2106, 2017. 4
[77] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong,
and Lei Li. Dense contrastive learning for self-supervised
visual pre-training. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 3024â€“3033, 2021. 6, 8
[78] Zhaoqing Wang, Qiang Li, Guoxin Zhang, Pengfei Wan,
Wen Zheng, Nannan Wang, Mingming Gong, and Tongliang
Liu. Exploring set similarity for dense self-supervised repre-
sentation learning. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 16590â€“16599, 2022. 8
[79] Junfei Xiao, Yutong Bai, Alan Yuille, and Zongwei Zhou.
Delving into masked autoencoders for multi-label thorax dis-
ease classification. In Proceedings of the IEEE/CVF Win-
ter Conference on Applications of Computer Vision (WACV) ,
pages 3588â€“3600, 2023. 4, 7, 8
[80] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer,
and Trevor Darrell. Region similarity representation learn-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV) , pages 10539â€“10548, 2021.
8
[81] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang
Xu, Peize Sun, Zhenguo Li, and Ping Luo. Detco: Unsuper-
vised contrastive learning for object detection. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision (ICCV) , pages 8392â€“8401, 2021. 8
[82] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen
Lin, and Han Hu. Propagate yourself: Exploring pixel-level
consistency for unsupervised visual representation learn-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 16684â€“
16693, 2021. 8
[83] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple
framework for masked image modeling. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 9653â€“9663, 2022. 8
[84] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Yixuan
Wei, Qi Dai, and Han Hu. On data scaling in masked im-
age modeling. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR) , pages
10365â€“10374, 2023. 8
[85] Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Re-
gioncl: Exploring contrastive region pairs for self-supervised
representation learning. In Computer Vision â€“ ECCV 2022 ,
pages 477â€“494, Cham, 2022. Springer Nature Switzerland.
8
[86] Ceyuan Yang, Zhirong Wu, Bolei Zhou, and Stephen Lin. In-
stance localization for self-supervised detection pretraining.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 3987â€“3996,
2021. 8
[87] Mang Ye, Xu Zhang, Pong C. Yuen, and Shih-Fu Chang. Un-
supervised embedding learning via invariant and spreading
instance feature. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
2019. 8
[88] Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail
Fomitchev, Mohannad Hussain, ParasLakhani, Phil Culliton,
and Shunxing Bao. Siim-acr pneumothorax segmentation,
2019. 4
[89] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and
StÂ´ephane Deny. Barlow twins: Self-supervised learning via
redundancy reduction. arXiv:2103.03230 , 2021. 8
[90] Chaoning Zhang, Kang Zhang, Trung X. Pham, Axi Niu,
Zhinan Qiao, Chang D. Yoo, and In So Kweon. Dual temper-
ature helps contrastive learning without many negative sam-
ples: Towards understanding and simplifying moco. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 14441â€“14450, 2022.
8
[91] Shaofeng Zhang, Feng Zhu, Junchi Yan, Rui Zhao, and Xi-
aokang Yang. Zero-CL: Instance and feature decorrelation
for negative-free symmetric contrastive learning. In Interna-
tional Conference on Learning Representations , 2022. 8
[92] Shaofeng Zhang, Feng Zhu, Rui Zhao, and Junchi Yan.
Patch-level contrasting without patch correspondence for ac-
curate and dense contrastive representation learning. In The
Eleventh International Conference on Learning Representa-
tions , 2023. 8
[93] Tong Zhang, Congpei Qiu, Wei Ke, Sabine S Â¨usstrunk, and
Mathieu Salzmann. Leverage your local and global repre-
sentations: A new self-supervised learning strategy. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , pages 16580â€“16589, 2022.
8
[94] Hong-Yu Zhou, Chixiang Lu, Sibei Yang, Xiaoguang Han,
and Yizhou Yu. Preservational learning improves self-
supervised medical image models by reconstructing diverse
contexts. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 3499â€“3509,
2021. 4, 8
[95] Zongwei Zhou, Vatsal Sodha, Md Mahfuzur Rahman Sid-
diquee, Ruibin Feng, Nima Tajbakhsh, Michael B. Gotway,
and Jianming Liang. Models genesis: Generic autodidactic
models for 3d medical image analysis. In Medical Image
Computing and Computer Assisted Intervention â€“ MICCAI
11280
2019 , pages 384â€“393, Cham, 2019. Springer International
Publishing. 8
[96] Zongwei Zhou, Vatsal Sodha, Jiaxuan Pang, Michael B. Got-
way, and Jianming Liang. Models genesis. Medical Image
Analysis , 67:101840, 2021. 8
[97] Ziyu Zhou, Haozhe Luo, Jiaxuan Pang, Xiaowei Ding,
Michael Gotway, and Jianming Liang. Learning anatomi-
cally consistent embedding for chest radiography. In Pro-
ceedings of the 34th British Machine Vision Conference
(BMVC 2023) , 2023. 1, 8
11281
