Estimating Extreme 3D Image Rotations using Cascaded Attention
Shay Dekel
Bar Ilan University
Ramat-Gan, Israel
shaydekel@gmail.comYosi Keller
Bar Ilan University
Ramat-Gan, Israel
yosi.keller@gmail.comMartin Ë‡CadÂ´Ä±k
FIT, Brno University of Technology
Brno, Czech Republic
cadik@fit.vut.cz
Abstract
Estimating large, extreme inter-image rotations is crit-
ical for numerous computer vision domains involving im-
ages related by limited or non-overlapping fields of view.
In this work, we propose an attention-based approach with
a pipeline of novel algorithmic components. First, as ro-
tation estimation pertains to image pairs, we introduce an
inter-image distillation scheme using Decoders to improve
embeddings. Second, whereas contemporary methods com-
pute a 4D correlation volume (4DCV) encoding inter-image
relationships, we propose an Encoder-based cross-attention
approach between activation maps to compute an enhanced
equivalent of the 4DCV . Finally, we present a cascaded
Decoder-based technique for alternately refining the cross-
attention and the rotation query. Our approach outperforms
current state-of-the-art methods on extreme rotation estima-
tion. We make our code publicly available1.
1. Introduction
Estimating the relative pose between a pair of images is a
crucial task in computer vision, which is used in various ap-
plications such as indoor navigation, augmented reality, au-
tonomous driving, 3D reconstruction [40, 44], camera local-
ization [5, 45, 47], simultaneous localization and mapping
[12, 38], and novel view synthesis [35, 42]. The current
approach to image registration involves extracting features,
matching them, and establishing correspondence between
them. However, this approach is ineffective for input pairs
with little or no overlap, making it difficult to establish suf-
ficient feature correspondences for matching, such as in the
images shown in Fig. 1.
Numerous applications [1, 32, 49] necessitate precise es-
timation of inter-image rotations. The prevalent approach
for extreme 3D rotation estimation between images with
limited or no overlap, as in Fig. 1, relates to the semi-
nal work of Coughlan and Yuille [10]. They introduced a
technique premised on linear structures within an image,
primarily arising from three mutually orthogonal directions
Figure 1. The estimation of extreme 3D image rotations. First row:
Images pair with a small overlap. Second row: non-overlapping
image pairs. The proposed scheme estimates the relative rotation
between image pairs.
- one vertical (building walls) and two horizontal (ground
pavements, roads, etc.). Similarly, â€Single View Metrol-
ogyâ€ by Criminisi et al. [11] and extensions [26, 41, 61] uti-
lize parallel image lines and corresponding vanishing points
[19] for camera calibration. Furthermore, relative camera
rotation can be estimated via illumination cues [2], by ana-
lyzing lighting and shadow directions.
In this work, we propose a deep-learning approach for
estimating significant, extreme inter-image rotations. Un-
like classical formulations [10, 11] that explicitly detect
hand-crafted cues such as lines, shadows, and vanishing
points, our method directly regresses the relative rotation
from input images through a deep neural network. Inspired
by recent successful applications of Transformers [53] in
computer vision tasks including object detection [8] and im-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2588
age recognition [24], we adapt Transformers for multiple
tasks within the proposed pipeline shown in Fig. 2, expand-
ing beyond previous applications of Transformers.
First, we apply Transformers-Decoders to improve the
input image embeddings by distilling inter-image informa-
tion between the images by cross-decoding, where each em-
bedding uses the otherâ€™s embedding as a query. This bet-
ter encodes images with respect to each other. Second, a
Transformer-Encoder computes a stacked multihead atten-
tion to encode cross-attention between the latent representa-
tions of image pairs. Thus, it improves on the 4D correlation
volume (4DCV) used in prior works [15, 21, 30, 39, 51],
where 4DCVs were calculated by inner products. Instead
of a single layer of N2inner products as in 4DCV , the
proposed Transformer-Encoder-based approach leverages
multi-head attentionâ€™s advanced architecture to better en-
code interactions between activation map entries. Third, we
further improve the cross-attention encoding using a cas-
cade of two decoders and a learnt rotation query, to jointly
refine the cross-attention encoding and the rotation query.
The proposed scheme is a general-purpose attention-based
architecture for estimating attributes related to two input im-
ages such as optical flow, registration, relative pose regres-
sion, etc. This work was motivated by extreme rotation es-
timation, and we reserve other applications for future work,
as those will require additional task-specific modifications.
Interestingly, the attention maps computed by our
scheme, shown in Section 3.2, show that the Transformer-
Encoder assigns high attention scores to image regions con-
taining rotation-informative image cues, emphasizing verti-
cal and horizontal lines. We also observe that the proposed
approach can predict the rotation of non-overlapping image
pairs with state-of-the-art (SOTA) accuracy. Our framework
is end-to-end trainable and optimizes a regression loss. It is
evaluated on three dataset benchmarks: StreetLearn [36],
SUN360 [54] and InteriorNet [29], with different overlap
classes in indoor and outdoor locations and under varying
illumination. The experimental results in Section 4 show
our model to provide state-of-the-art (SOTA) accuracy.
In summary, our contributions are as follows.
â€¢ We propose a novel scheme for estimating extreme rota-
tions, including scenarios with minimal image overlap.
â€¢ Image embeddings are enhanced via cross-decoding, dis-
tilling inter-image information.
â€¢ A Transformer-Encoder cross-attention mechanism is
proposed to encode the latent space interactions between
image pairs.
â€¢ A decoder-decoder module infers relative rotation from
the cross-attention encoding by learning and applying
quaternionic rotation queries.
â€¢ Quantitative evaluations demonstrate favorable perfor-
mance compared to state-of-the-art rotation estimation
techniques on indoor and outdoor datasets.2. Related Work
Our rotation estimation approach represents a specific case
of the more general problem of relative pose estimation,
particularly relative pose regression (RPR). The prevalent
relative rotation estimation technique detects and matches
2D feature points [e.g., SIFT [34], SURF [4]] between im-
ages. For pose localization tasks [50], PnP schemes esti-
mate the relative 3D rotation, and the query image camera
pose is determined given the anchor imageâ€™s 3D coordinates
and pose. Other schemes for 3D rotation estimation utilized
3D Fourier transforms [22, 23], whose magnitude is invari-
ant to translations. Recent methods apply end-to-end train-
able deep networks to both images [3, 14]. Graph neural
networks (GNNs) enabled multi-image RPR via aggregat-
ing localization cues across video frames [52, 55]. Neu-
ral radiance fields (NeRFs) have been explored as an al-
ternative to traditional image or feature point storage for
RPR encoding. Some schemes employ rotation-specific
parametrizations, notably quaternions and Euler angles, to
estimate relative 3D rotations [59]. Such parametrizations,
especially quaternions, address the discontinuities intrinsic
to rotation representations, attributed to the Double-Cover
property. Levinson et al. [27] investigated the SVD orthog-
onalization approach for 3D rotation estimation via neural
networks. By projecting the inferred rotation matrices onto
the rotation group using SVD, they showcased that its inte-
gration supersedes conventional representations, advancing
the state-of-the-art in diverse deep learning paradigms. Fur-
ther, Mohlin et al. [37] introduced a neural network-based
estimation of the parameters for the Fisher distribution ma-
trix, representing the probability distributions of 3D rota-
tions. By optimizing the negative log-likelihood loss of
this distribution, they surpassed prior benchmarks in several
real-world datasets. Similarly, this optimization method-
ology was used by Liu et al. to estimate the head pose
[33]. As a noteworthy baseline, Rockwell et al. [43] de-
vised a Vision Transformer (ViT) to approximate the eight-
point algorithm for direct relative pose estimation between
two images, showing competitive performance across di-
verse scenarios. The methods above predominantly rely
on substantial overlap between input image pairs. A pro-
nounced rotation, resulting in limited overlap, could jeop-
ardize the accuracy of these estimations. Specifically, such
techniques are ineffective for aligning non-overlapping im-
ages. Caspi and Irani [9] demonstrated the feasibility of
aligning two image sequences with non-overlapping fields
of view in both temporal and spatial dimensions, provided
the cameras are in proximity. This alignment leverages
shared temporal variations within the sequences. In a par-
allel vein, Shakil [46] established that multiple nonover-
lapping video sequences, captured by uncalibrated video
cameras, can be synchronized through inherent temporal
fluctuations and inter-frame motion within the sequences.
2589
Extending beyond mere imagery, the challenge of register-
ing non-overlapping RGB-D scans [20, 48] serves as a no-
table derivative. In such cases, a holistic representation of
the scene is typically deduced. In our study, we adopt the
framework delineated by Cai et al. [7], where the task is to
estimate the extreme relative 3D rotation from a pair of in-
put images. Cai et al. introduced a scheme in which a CNN
is used to embed the images, followed by the computation
of a 4D correlation volume (4DCV) from the embeddings.
An MLP is then applied to this correlation volume, optimiz-
ing it using a cross-entropy loss, resulting in state-of-the-art
(SOTA) accuracy for non-overlapping images. Intrinsically,
4DCVs are an extension of Bilinear Pooling [25, 31], en-
coding pairwise correlations across all entities of the 2D
embedding maps corresponding to the image pair. Given
their encoding capability, 4DCVs have found applications
in tasks necessitating long-range spatial correspondences,
evident in the RAFT SOTA optical flow [51] and other op-
tical flow models [15, 21, 56]. In a related context, 3D cor-
relation volumes have been utilized in deep stereo match-
ing tasks [18, 28, 30, 39], where the pixels in one image
are matched with constrained spatial support in its counter-
part. Diverging from these methods, our proposal empha-
sizes the computation of an analogous 4DCV by evaluating
the cross-attention between the activation maps of the im-
age pair through multi-head Transformer-Encoder and an
associated activation mask. Specifically, this Transformer-
Encoder effectively realizes the functions of multiple ag-
gregated correlation volumes [53] via multi-head attention
(MHA). Moreover, we propose an inter-image embedding
distillation using Transformer-Decoders, and also improve
the rotation inference using a cascaded alternating rotation
decoding.
3. Rotation Estimation Using Cascaded Atten-
tion
The proposed methodology estimates the relative 3D rota-
tionRâˆˆR3Ã—3between input image pairs I1,I2âˆˆRHÃ—W,
outlined in Fig. 2. Siamese residual U-nets [57] with
weight sharing encode inputs into activation maps bI1,bI2âˆˆ
RcÃ—K1Ã—K2, where cis the number of channels and K1,
K2are spatial dimensions. To improve embeddings bI1,bI2
via cross-decoding, bI1,bI2are cross-propagated into weight-
sharing Transformer Decoder-0 units. Each input embed-
ding extracts task-relevant representationsÂ¯Â¯I1andÂ¯Â¯I2.
To further relate the two input images, we compute the
cross-attention Ë†T, an enhanced equivalent of the 4D cor-
relation volume (4DCV) used in prior works [15, 21, 30,
39, 51]. Therefore, we have vectorized the rowsÂ¯Â¯I1and
Â¯Â¯I2as two sequences âˆˆRcÃ—K1K2, concatenated as a single
tensor TâˆˆRcÃ—2K1K2, and apply a Transformer-Encoder
as in Section 3.2. The rotation is decoded on the basis of
cross-attention Ë†Tusing a novel attention-based architecturethat uses a cascade of two Transformer-Decoders. Initially,
Transformer Decoder-1 is utilized to augment Ë†Tby incor-
porating it as a query, with guidance provided by a learn-
able quaternion Â¯ qâˆˆR4as input. Subsequently, the output
of Transformer Decoder-1, denoted asÂ¯Â¯T, is introduced as
input to the subsequent Transformer Decoder-2, further re-
fining the query Â¯ q. Finally, a fully connected MLP layer
is applied to predict the relative rotation encoded as quater-
nion Ëœq. The 3D rotation is regressed using its quaternion
representation, q,as discussed in Section 3.4.
3.1. Image Embedding Distillation by Cross-
Decoding
Given the embeddings bI1,bI2âˆˆRcÃ—K1Ã—K2of the input im-
ages, we aim to refine the embeddings by distilling the in-
formation between the input images. For that, we apply
a Transformer-Decoder that is applied to each embedding,
where the other embedding is used as the decoderâ€™s query.
In order to transform activation maps into Transformer-
compatible inputs, we follow the same sequence prepara-
tion procedure as in [8]. The activation maps bI1,bI2âˆˆ
RcÃ—K1Ã—K2are first flattened to a sequential representation
bI1,bI2âˆˆRcÃ—K1K2. Each position in the activation map
is further assigned with a learned encoding to preserve the
spatial information of each location. To reduce the number
of parameters, two one-dimensional encodings are learned
separately for the X,Yaxes. Specifically, for an activation
mapbIwe define the sets of positional embedding vectors
EuâˆˆRK1Ã—C/2andEvâˆˆRK2Ã—C/2, such that a spatial
position (i, j), iâˆˆ1..K 1,jâˆˆ1..K 2, is encoded by con-
catenating the two corresponding embedding vectors:
Ei,j
pos=
Ei
u
Ej
v
âˆˆRC. (1)
The processed sequence, serving as input to the Trans-
former is thus given by:
bI=bI+EAâˆˆRK1K2Ã—C, (2)
where EAis the positional encoding of bI.
3.2. Cross-Attention Computation using a
Transformer-Encoder
The cross-attention between the refinement of represen-
tations of input imagesÂ¯Â¯I1andÂ¯Â¯I2is computed using a
Transformer-Encoder with l= 2 layers and h= 4 at-
tention heads for each layer. An ablation study of this
configuration is given in Section 4.6. The cross-attention
maps computed by the Transformer-Encoder are an im-
proved equivalent of the 4D correlation volumes [7, 51], en-
coding the interactions (inner-products) between allthe im-
1https://github.com/dekelshay/AttExtremeRotation
2590
MLP Input image pair
Concatenationð¼à¬µ
ð¼à¬¶ð¼áˆ˜à¬µ
ð¼áˆ˜à¬¶ð‘‡ð‘‡à· 
2Ã—ð¾à¬µð¾à¬¶,ð¶
[ð»,ð‘Š,3]
Embedding 
Embedding 
Transformer 
Encoder
MaskTransformer
Decoder-1ð‘žà´¤ 1Ã—4 
ð‘‡à´§ ð‘žà´§ð‘žà·¤ð¼à¬¶à´¨
ð¼à¬µà´¨Query
QueryQuery
1Ã—4 1Ã—4 1Ã—4 Transformer
Decoder-0
Transformer
Decoder-0Transformer
Decoder-2
ð¾à¬µ,ð¾à¬¶,ð¶Query
2Ã—ð¾à¬µð¾à¬¶,ð¶Figure 2. The proposed architecture utilizes weight-sharing Siamese CNNs to encode the input image pair (I1, I2)âˆˆRHÃ—Winto feature
maps (Ë†I1,Ë†I2). These feature maps are then cross-decoded by the weight sharing Transformer Decoder-0 layers, cross-distilling (Ë†I1,Ë†I2)
into the representationsÂ¯Â¯I1andÂ¯Â¯I2. The concatenated refined embeddings Tare input to the Transformer-Encoder alongside an attention
maskMto derive the cross-attention encoding Ë†T.Ë†Tenters a cascade of two Transformer Decoders, where the first, Transformer Decoder-
1, enhances the cross-attention asÂ¯Â¯T, guided by the learned quaternion rotation query Â¯q. The second, Transformer Decoder-2, encodes the
rotation as Â¯Â¯q, transformed via a multilayer perceptron (MLP) to predict the relative quaternion rotation Ëœq.
âˆ’âˆž
âˆ’âˆž0
0ð¼à¬µà´¨ ð¼à¬¶à´¨2Kà¬µKà¬¶
ð‘
ð¼à¬µà´¨
ð¼à¬¶à´¨âˆ’âˆž
âˆ’âˆžð¼Ì¿à¬µà¯„à°­,ð¼Ì¿à¬¶à¯„à°®ð¼à¬µà´¨ ð¼à¬¶à´¨
Mask2Kà¬µKà¬¶
Activation Mapð¼Ì¿à¬µà¯„à°­,ð¼Ì¿à¬¶à¯„à°®
Figure 3. Computing the cross-attention using a Transformer-
Encoder and the input mask, M.The mask Mzeros the self-
attention terms, retaining only the cross-attention terms.
age cues in the activation maps. By default, a Transformer-
Encoder computes the self-attention maps of the input se-
quence. Hence, the cross-attention Ë†Tof the vectorized and
concatenated activation maps Tis computed by applying
the attention mask Mgiven in Eq. 3. The mask Mnul-
lifies the self-attention terms in the attention maps com-
puted throughout the Transformer-Encoder, while retaining
the cross-attention terms,
M=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£°âˆ’âˆž âˆ’âˆž Â·Â·Â· 0 0
âˆ’âˆž âˆ’âˆž Â·Â·Â· 0 0
............
0 0 Â·Â·Â· âˆ’âˆž âˆ’âˆž
0 0 Â·Â·Â· âˆ’âˆž âˆ’âˆžï£¹
ï£ºï£ºï£ºï£ºï£ºï£»(3)
The use of the mask Mand the corresponding structure
of the attention maps is shown in Fig. 3. Any pair of im-
age patches could hold valuable information about the over-
all geometric relationships in an image. The Transformer-Encoder can uncover these hints implicitly. The regions in
the input images that contain rotation-related cues, explic-
itly or implicitly, receive higher attention scores, as seen
in Section 2 of the Supplementary Materials section. This
leads to a more meaningful and concise input for the distil-
lation and the subsequent MLP layer, ultimately improving
the estimation accuracy. The same as in [7], even when the
image pairs are non-overlapping, the Transformer-Encoder
formulation can predict the rotation using straight lines only
present in a single image, the same as human cognitive ca-
pabilities. For example, in the extreme scenario of non-
overlapping image pairs the roll angle can be estimated
from a single image, by implicitly assuming that buildings
and their edges are perpendicular to ground level. Simi-
larly, the relative elevation angle can be estimated by as-
suming that the streets and pavements are parallel to the
ground plane or by computing the corresponding vanishing
points. Most training and test datasets in this domain depict
urban scenes, adhering to these assumptions.
3.3. Cascaded Attention-based Decoding
Given the cross-attention tensor Ë†Tthat encodes interrela-
tions between the paired input images, our objective is infer-
ring the 3D relative rotation quaternion. To achieve this, we
propose an innovative cascaded decoding scheme that al-
ternately refines both the query rotation and cross-attention.
Initially, Transformer Decoder-1, enhances Ë†Tbased on the
learned quaternion qto compute T. Next, the refined cross-
attention Tis queried by qto deduce the rotated encoding
Â¯Â¯ qusing Transformer Decoder-2. This cascaded inference
approach could be extended via additional dual units. How-
ever, we observed no performance gains from additional
cascades. Since the decoder inputs are semantic representa-
tions, positional encodings are excluded.
2591
3.4. Relative Rotation Regression
The encoded quaternion vector Â¯Â¯ qis subsequently input to
a Multi-Layer Perceptron (MLP) regressor, computing the
quaternion output denoted Ëœ q. This resultant quaternion is
given by q= [qw, qx, qy, qz]. The training loss is formu-
lated as:
L=|q0âˆ’Ëœ q/||Ëœ q|| |2, (4)
where q0andËœqare the groundtruth and predicted quater-
nions, respectively. Normalization ensures that the quater-
nion is a valid 3D rotation representation.
4. Experimental Results
The proposed scheme was experimentally verified by apply-
ing it to contemporary benchmark datasets with overlapping
and nonoverlapping image pairs. Our experimental setup
rigorously adhered to the paradigm established by Cai et al.
[7], using identical datasets and image overlap categories.
Utilizing their provided source code2, to create perspective
views from panoramic images, ensuring that the input im-
ages were the same in both studies, allowing fair compar-
isons with previous SOTA results and other contemporary
schemes. For that, we also used the same Residual-Unet
backbone network [58] as in [7]. Section 4.1 details the im-
age datasets we used and their processing, according to Cai
et al. [7], to derive the training and test datasets. Training
details are given in Section 4.2. We compare with recent
SOTA schemes listed in Section 4.3 using the geodesic er-
ror measure used in previous work [7]
E= arccostr(RTRâˆ—)âˆ’1
2
, (5)
where Ris the predicted rotation matrix and Râˆ—is the
groundtruth relative rotation matrix for each image pair.
The experimental comparisons are reported in Section 4.4
and the attention maps are visualized in Section 2 of
the Supplementary Materials to provide an intuitive in-
terpretation of the cross-attention scores computed by the
Transformer-Encoder. We studied the cross-dataset gener-
alization properties of the proposed scheme in Section 4.5,
while ablation studies of the different parameters, design
choices and parameters are reported in Section 4.6.
4.1. Image Datasets and their Processing
We used the following datasets and train/test splits used in
previous works:
InteriorNet [29] is a synthetic data set to understand
and map interior scenes. A subset of 10,050 panoramas
from 112 different houses was used, where the images of
82 houses were used for training and those of 30 houses
were used for testing, respectively.
2https://github.com/RuojinCai/ExtremeRotation_codeStreetLearn [36] is an outdoor dataset consisting of ap-
proximately 140,000 panoramic views of Pittsburgh and
Manhattan. We used 56K panoramic views from Manhat-
tan, from which we randomly chose 1000 panoramic views
for testing.
SUN360 [54] is an indoor collection of high-resolution
panoramas that cover a full view of 360â—¦Ã—180â—¦for a variety
of environmental scenes downloaded from the Internet. It
also provides location category labels. We used 7K and 2K
panoramas for training and testing, respectively.
As these datasets contain panoramic images, we gen-
erated 200 perspective 128Ã—128 images by randomly
cropping 200 different locations in each panoramic im-
age. This sampling strategy ensures a consistent distri-
bution of ground-truth image pairs with pitch resolutions
spanning [âˆ’45â—¦,45â—¦]and yaw resolutions encompassing
[âˆ’180â—¦,180â—¦]. We estimate only the Yaw and the Pitch
angles, presuming a null roll between paired images. We
avoided generating textureless image pairs, that is, im-
ages that mainly contain ceilings or floors in a house or
skies in an outdoor scenes, by limiting the pitch rang to
[âˆ’45â—¦,45â—¦]for the outdoor dataset and [âˆ’30â—¦,30â—¦]for the
indoor datasets. There is no overlap between the train and
test datasets. To compare our results with prior research
and to analyze the influence of camera translation on our
rotation estimation approach, we partitioned the Interior-
Net and StreetLearn datasets into two groups: images with
and without camera translations. The non-translated images
were acquired by randomly selecting pairs of cropped im-
ages from a single panorama. In contrast, datasets that in-
clude translations (known as StreetLearn-T and InteriorNet-
T) were generated by randomly selecting pairs of cropped
images from different panoramas, where translations are
less than 3m. However, our method was not used to es-
timate these translations. We evaluated our performance
in overlapping and nonoverlapping pairs and use the setup
of Cai et al. [7] by dividing the datasets into three overlap
classes:
Large , contains highly overlapping pairs up to relative
rotations of 45â—¦
Small , contains pairs that partially overlap with relative
rotation angles âˆˆ[45â—¦,90â—¦]
None , contains pairs without overlap with relative rota-
tions >90â—¦.
4.2. Training Details
We use a pre-trained Residual-Unet [58] (same as in Cai
et al. [7]) as a backbone to compute the feature maps of
the two input images (Ë†I1,Ë†I2)âˆˆRcÃ—k1Ã—k2=R128Ã—32Ã—32.
According to Fig. 2, subsequently, these feature maps are
cross-propagated into dedicated decoder units, resulting in
the refinement of representationsÂ¯Â¯I1andÂ¯Â¯I2. The refinements
of the representationsÂ¯Â¯I1andÂ¯Â¯I2were reshaped and con-
2592
catenated along the axis of the samples to form the tensor
TâˆˆR(2Â·32Â·32)Ã—128=R2048Ã—128.Twas the input to the
Transformer-Encoder, consisting of l= 2layers with ReLU
nonlinearity and a dropout of p= 0.1. Each encoder layer
usesh= 4 MHA heads and a hidden dimension of Ch=
768. An ablation study of the Transformer-Encoder param-
eters is given in Section 4.6. The Transformer-Encoderâ€™s
output Ë†Tis then fed into a dual-path structure comprising
two concatenated decoders. The primary decoder receives a
learnt quaternion vector initialized by white Gaussian noise,
Â¯q, as input and the cross-attention Ë†Tas a query, and pro-
ducesÂ¯Â¯T, enhancing contextual nuances, while the subse-
quent decoder getsÂ¯Â¯Tas an input and the same empty quater-
nion vector, Â¯qas a query, and generates Â¯Â¯q, encapsulating
pivotal rotational attributes. The two sequential attention-
based decoders use l= 2 layers with h= 2 MHA heads
and a hidden dimension of Ch= 768 . Finally, the MLP
regressor that computes the quaternion representation for
the regression loss in Eq. 4. The MLP regressor contains
two fully connected layers. Throughout all experiments, the
model is optimized using an Adam optimizer with an initial
learning rate of Î»= 5eâˆ’4, with Î²1= 0.9,Î²2= 0.999,
Ïµ= 10âˆ’10,and a batch size of 20. Our model is imple-
mented in PyTorch, it is end-to-end trainable, and all exper-
iments were performed on an 8GB NVIDIA GeForce GTX
2080 GPU.
4.3. Comparative baselines
In line with Cai et al. [7], we compare our method with
contemporary schemes using the datasets in Section 4.1:
A SIFT-based approach [6] . A method for matching
SIFT features [34] using RANSAC [17] in image pairs of
the same panorama, and estimating the relative rotation ma-
trix using Homography equations or the Essential matrix.
CNN-based methods [13] . Deep learning schemes that
detect and encode local image features using SuperPoint-
Net [13] and D2-Net [16].
Self-supervised interest point [59]. A scheme by Zhou
et al. [59] (Reg6D) that applies a CNN to approximate the
mappings between various rotation representations and fits
continuous 5D and 6D rotation representations, instead of
the commonly used Euler and quaternion representations.
Extreme rotation estimation [7] . A deep learning tech-
nique to estimate the relative 3D rotation of image pairs in
an extreme setting [7] where the images have little or no
overlap. They proposed a network that automatically learns
implicit visual cues by computing a 4D correlation volume.
Attention-based methods. We also compare to recent
work by Rockwell et al. [43] (8PointVit) using a Vi-
sion Transformer (ViT) to estimate the relative pose. Al-
though Rockwell et al. achieve competitive results in mul-
tiple settings, their approach is less suited for extreme view
changes.4.4. Experimental comparisons
The results of the comparison of the proposed scheme with
baselines and SOTA schemes are reported in Table 1. We
report the mean and median of the geodesic error given in
Eq. 5 and the percentage of image pairs whose estimated
relative rotation error was less than 10â—¦. We compared the
accuracy of our proposed model to the schemes detailed in
Section 4.3. The proposed approach is shown to be accu-
rate for both indoor and outdoor scenes and significantly
outperforms the baseline schemes in all overlap categories.
For nonoverlapping pairs, correspondence-based methods
such as SIFT [6], SuperPointNet [13], Reg6D [59] and
8PointViT [43] failed to provide any estimates, as they re-
quire feature correspondence. The DenseCorrV ol approach
[7] provides accurate results in extreme cases, but our ap-
proach outperforms it. Qualitative experimental results are
given in Section 1 of the Supplementary Materials section.
The qualitative results of the rotation estimation are shown
in Fig. 4 for the StreetLearn and SUN360 datasets, for the
large, small and nonoverlapping cases. We show the full
panoramas, the footprints of the cropped images that were
used as inputs for the proposed scheme and the footprint of
the estimated image crop based on the estimated rotation.
In all cases, we achieve high estimation accuracy.
4.5. Cross-Dataset Generalization
The cross-dataset generalization properties of our approach
were evaluated using the Holicity dataset [60]. The Man-
hattan dataset was used to train the models, while the Lon-
don dataset was used for testing. The test images were di-
vided into three overlap classes according to Section 4.1.
We compared the generalization of our approach with Cai
et al.â€™s [7]. The results, in Table 2, show that our approach
outperformed Cai et al in all overlap classes.
4.6. Ablation Study
Transformer-Encoder parameters. Table 3 summarizes
multiple Transformer-Encoder configurations for each over-
lap category. The expressive power of the Transformer-
Encoder depends on the number of heads and layers. The
more are used, the better the expressive power. However,
using an excessive number might lead to overfitting, and
the optimal constellation, in terms of accuracy in Table 3, is
given by h= 4,l= 2. In particular, this constellation is a
sweetspot so that increasing the number of heads or layers
results in reduced accuracy.
Backbone ablation. In Table 4, we examine the depth
of the Residual-Unet [58] backbone by altering the num-
ber of residual blocks it contains. Increasing the number
of residual blocks enhances the backboneâ€™s expressive ca-
pability, but an excessively deep architecture may result in
overfitting. We found that using three residual blocks is the
optimal choice, which aligns with our original decision.
2593
InteriorNet InteriorNet-T SUN360 StreetLearn StreetLearn-T
Overlap Method Avg(â—¦â†“) Med(â—¦â†“) 10â—¦(%â†‘) Avg(â—¦â†“) Med(â—¦â†“) 10â—¦(%â†‘) Avg(â—¦â†“) Med(â—¦â†“) 10â—¦(%â†‘) Avg(â—¦â†“) Med(â—¦â†“) 10â—¦(%â†‘) Avg(â—¦â†“) Med(â—¦â†“) 10â—¦(%â†‘)
LargeSIFT* [34] 6.09 4.00 84.86 7.78 2.95 55.52 5.46 3.88 93.10 5.84 3.16 91.18 18.86 3.13 22.37
SuperPoint* [13] 5.40 3.53 87.10 5.46 2.79 65.97 4.69 3.18 92.12 6.23 3.61 91.18 6.38 1.79 16.45
Reg6D [59] 9.05 5.90 68.49 17.00 11.95 41.79 16.51 12.43 40.39 11.70 8.87 58.24 36.71 24.79 23.03
DenseCorrV o [7] 1.53 1.10 99.26 2.89 1.10 97.61 1.00 0.94 100.00 1.19 1.02 99.41 9.12 2.91 87.50
8PointViT [43] 0.48 0.40 100.00 2.90 1.83 97.91 - - - 0.62 0.52 100.00 4.08 2.43 90.13
Ours 0.43 0.38 99.65 1.75 0.95 98.8 0.85 0.45 99.95 0.58 0.48 99.31 3.88 1.69 87.20
SmallSIFT* [34] 24.18 8.57 39.73 18.16 10.01 18.52 13.71 6.33 56.77 16.22 7.35 55.81 38.78 13.81 5.68
SuperPoint* [13] 16.72 8.43 21.58 11.61 5.82 11.73 17.63 7.70 26.69 19.29 7.60 24.58 6.80 6.85 0.95
Reg6D [59] 25.71 15.56 33.56 42.93 28.92 23.15 42.55 32.11 9.40 24.77 15.11 30.56 46.61 34.33 13.88
DenseCorrV ol [7] 6.45 1.61 95.89 10.24 1.38 89.81 3.09 1.41 98.50 2.32 1.41 98.67 13.04 3.49 84.23
8PointViT [43] 1.84 0.94 99.32 4.48 2.38 96.30 - - - 1.46 1.09 100.00 9.19 3.25 87.7
Ours 1.55 0.872 99.85 4.25 0.777 97.55 2.109 0.831 98.99 1.21 0.718 99.122 7.48 1.8666 88.996
NoneSIFT* [34] 109.30 92.86 0.00 93.79 113.86 0.00 127.61 129.07 0.00 83.49 90.00 0.38 85.90 106.84 0.38
SuperPoint* [13] 120.28 120.28 0.00 â€“ â€“ 0.00 149.80 165.24 0.00 â€“ â€“ 0.00 â€“ â€“ 0.00
Reg6D [59] 48.36 32.93 10.82 60.91 51.26 11.14 64.74 56.55 3.77 28.48 18.86 24.39 49.23 35.66 11.86
8PointViT [43] - - - - - - - - - - - - - - -
DenseCorrV ol [7] 37.69 3.15 61.97 49.44 4.17 58.36 34.92 4.43 61.39 5.77 1.53 96.41 30.98 3.50 72.69
Ours 35.13 2.814 65.20 45.32 4.05 59.56 32.46 4.19 63.17 5.33 1.20 96.22 28.13 3.25 72.43
Table 1. Relative rotation estimation results. We utilized the InteriorNet, SUN360, and StreetLearn datasets and show the average and
median of geodesic errors. We also present the percentage of image pairs with relative rotation error below 10â—¦, for the overlap categories
in Section 4.1. The gray numbers indicate errors exceeding 50% . The asterisk âˆ—signifies that the mean and median errors did not lead to
pose estimation, and their calculations are performed only on successful image pairs.
Large
 Small
 No
StreetLearn SUN360
Figure 4. Rotation estimation results. The panoramic and cropped groundtruth images are marked with green and yellow dots. The
predicted footprint of one of the cropped images is marked by the red-dotted line. The first row shows the results of the matching of images
with large overlaps. The second and last rows show the matching of small overlap and non-overlapping images.
Overlap Ours [â—¦] Cai et al. [7] [â—¦]
Large 9.55 11.23
Small 16.33 20.87
None 38.48 40.82
Table 2. Cross-dataset generalization. We trained the models on
the Manhattan dataset and tested them on the London dataset. The
average geodesic error is reported.
3D rotation encoding and training losses. The abla-
tions of different rotation encodings and their corresponding
training losses were evaluated and are presented in Table 5.
The evaluation was performed by applying the Residual-Unet backbone [58] and a proposed Transformer-Encoder-
based cross-attention method to image pairs from the Stree-
Learn dataset with large overlaps. For the discrete formu-
lation, in line with Cai et al. [7], the pitch and yaw angles
were discretized into 360 bins âˆˆ[âˆ’180â—¦,180â—¦], and a cross-
entropy loss was used to train the network. These results
were compared to those obtained using the L2regression
loss, as described in Eq. 4 in our scheme. The results in Ta-
ble 5 show that our L2regression outperforms the discrete
Euler angle approach proposed by Cai et al. [7].
Architecture Ablations To evaluate the proposed archi-
tecture and assess the contribution of each proposed com-
2594
Overlap Heads LayersRotational error
Avg [â—¦] Med [â—¦] 10â—¦[%]
Large1 1 1.21 0.97 99.1
4 1 0.82 0.65 99.1
2 2 0.87 0.71 99.12
4 2 0.58 0.48 99.31
4 4 0.65 0.59 99.2
Small1 1 7.13 3.26 94.99
4 1 6.42 2.46 95.32
2 2 5.44 2.05 96.55
4 2 1.21 0.718 99.122
4 4 4.61 0.98 95.43
None1 1 7.43 2.88 91.55
4 1 6.15 2.55 92.55
2 2 6.23 3.02 91.35
4 2 5.33 1.20 96.22
4 4 5.78 1.87 95.22
Table 3. Ablation of the Transformer-Encoder parameters using
the StreeLearn dataset. For each overlap class, there is an optimal
configuration that balances the Transformer-Encoderâ€™s expressive
power and overfitting.
Backbone LayersRotational error
Avg [â—¦] Med [â—¦] 10â—¦[%]
1Conv(k=7, s=2, d=64)
2.95 1.53 94.131Ã—Residual blocks
Conv(k=3, s=1, d=512)
Conv(k=3, s=1, d=256)
2Conv(k=7, s=2, d=64)
1.75 1.05 96.132Ã—Residual blocks
Conv(k=3, s=1, d=512)
Conv(k=3, s=1, d=256)
3Conv(k=7, s=2, d=64)
1.21 0.7818 99.1223Ã—Residual blocks
Conv(k=3, s=1, d=512)
Conv(k=3, s=1, d=256)
4Conv(k=7, s=2, d=64)
1.45 0.86 97.124Ã—Residual blocks
Conv(k=3, s=1, d=512)
Conv(k=3, s=1, d=256)
Table 4. Backbone Ablation. We evaluate the depth of the
Residual-Unet backbone network [58], used in our scheme, by
changing the number of residual blocks.
ponent, the use of the different Transformer Decoders in
particular, we conducted a series of experiments employing
various architectural variations of the proposed architecture
introduced in Section 3 and Fig. 2. The results are shown in
Table 6 and the corresponding architectures are shown in theRepresentation Loss functionRotational error
Avg [â—¦] Med [â—¦] 10â—¦[%]
Quaternions L2Regression 1.21 0.78 99.122
Euler angles Cross-Entropy 2.37 1.46 98.13
Euler angles L2Regression 2.22 1.32 98.99
Table 5. Ablation of 3D rotation encoding and training losses. We
compare the 3D rotations encodings by Euler angles and quater-
nions in discrete and continuous domains and the corresponding
training losses.
Supplementary Materials. In each experiment, we used a
particular partial configuration of the proposed Transformer
Decoders and evaluated the resulting estimation error us-
ing the StreetLearn dataset with large overlaps between the
input images. The results in Table 6 show that the pro-
posed configuration outperforms all other configurations. In
particular, configuration #1 shows that using the sequential
attention-based decoders, TD1 and TD2 improves the ac-
curacy significantly. The cross-decoding by TD0 provides
additional, but not as significant improvement.
TD0 TD1 TD2 Avg [â—¦] Med [â—¦] 10â—¦[%]
0 + + + 0.58 0.48 99.3
1 - + + 0.98 0.81 95.15
2 - - - 3.35 2.44 88.16
3 - + - 1.76 1.55 93.12
4 + + - 0.86 0.72 95.64
5 + - - 1.97 1.65 92.82
Table 6. Architectural ablation study. We compare the esti-
mation accuracy of different configurations of Transformers De-
coders (TDs). The corresponding architectures are shown in the
Supplementary Materials, and the first configuration is shown in
Section 3 and Fig. 2.
5. Conclusion
We present a novel formulation for estimating the relative
rotation between a pair of images. In particular, we study
the estimation of rotations between images with small and
no overlap. We propose an attention-based approach us-
ing a Transformer-Encoder to calculate the cross-attention
between image pair embedding maps, which outperforms
the previous use of 4D correlation volumes [7, 51] and a
decoder-decoder mechanism to estimate the output quater-
nion. Our framework can be trained end-to-end and opti-
mizes a regression loss. It has been experimentally shown
to outperform previous SOTA schemes [7] on multiple
datasets used in contemporary work. In particular, for the
challenging small and nonoverlapping cases.
2595
References
[1] Hadar Averbuch-Elor and Daniel Cohen-Or. Ringit: Ring-
ordering casual photos of a temporal event. ACM Trans.
Graph. , 34(3), 2015. 1
[2] Alexandru O. Balan, Michael J. Black, Horst Haussecker,
and Leonid Sigal. Shining a light on human pose: On shad-
ows, shading and the estimation of pose and shape. In IEEE
International Conference on Computer Vision (ICCV) , pages
1â€“8, 2007. 1
[3] Vassileios Balntas, Shuda Li, and Victor Prisacariu. Reloc-
net: Continuous metric learning relocalisation using neural
nets. In European Conference on Computer Vision (ECCV) ,
September 2018. 2
[4] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf:
Speeded up robust features. In European Conference on
Computer Vision (ECCV) , pages 404â€“417. Springer, 2006.
2
[5] Eric Brachmann, Alexander Krull, Sebastian Nowozin,
Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten
Rother. Dsac-differentiable ransac for camera localization.
InIEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 6684â€“6692, 2017. 1
[6] Matthew Brown, Richard I Hartley, and David Nist Â´er. Mini-
mal solutions for panoramic stitching. In IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) ,
pages 1â€“8. IEEE, 2007. 6
[7] Ruojin Cai, Bharath Hariharan, Noah Snavely, and Hadar
Averbuch-Elor. Extreme rotation estimation using dense cor-
relation volumes. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2021. 3, 4, 5, 6, 7,
8
[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Andrea Vedaldi,
Horst Bischof, Thomas Brox, and Jan-Michael Frahm, ed-
itors, European Conference on Computer Vision (ECCV) ,
pages 213â€“229, Cham, 2020. Springer International Publish-
ing. 1, 3
[9] Yaron Caspi and Michal Irani. Aligning non-overlapping
sequences. International Journal of Computer Vision ,
48(1):39â€“51, 2002. 2
[10] J.M. Coughlan and A.L. Yuille. Manhattan world: compass
direction from a single image by bayesian inference. In IEEE
International Conference on Computer Vision (ICCV) , vol-
ume 2, pages 941â€“947 vol.2, 1999. 1
[11] Antonio Criminisi, Ian Reid, and Andrew Zisserman. Sin-
gle view metrology. In IEEE International Conference on
Computer Vision (ICCV) , pages 434â€“441. IEEE Computer
Society, 1999. 1
[12] Andrew J Davison, Ian D Reid, Nicholas D Molton, and
Olivier Stasse. Monoslam: Real-time single camera slam.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , 29(6):1052â€“1067, 2007. 1
[13] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
novich. Superpoint: Self-supervised interest point detection
and description. In IEEE Conf. Comput. Vis. Pattern Recog.
Worksh. , 2018. 6, 7
[14] Mingyu Ding, Zhe Wang, Jiankai Sun, Jianping Shi, andPing Luo. Camnet: Coarse-to-fine retrieval for camera re-
localization. In IEEE International Conference on Computer
Vision (ICCV) , October 2019. 2
[15] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip
Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van
Der Smagt, Daniel Cremers, and Thomas Brox. Flownet:
Learning optical flow with convolutional networks. In IEEE
International Conference on Computer Vision (ICCV) , pages
2758â€“2766, 2015. 2, 3
[16] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Polle-
feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net:
A Trainable CNN for Joint Detection and Description of Lo-
cal Features. In IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) , 2019. 6
[17] Martin A Fischler and Robert C Bolles. Random sample
consensus: a paradigm for model fitting with applications to
image analysis and automated cartography. Communications
of the ACM , 24(6):381â€“395, 1981. 6
[18] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong
Tan, and Ping Tan. Cascade cost volume for high-resolution
multi-view stereo and stereo matching. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 2495â€“2504, 2020. 3
[19] R. I. Hartley and A. Zisserman. Multiple View Geometry
in Computer Vision . Cambridge University Press, ISBN:
0521540518, second edition, 2004. 1
[20] Shengyu Huang, Zan Gojcic, Mikhail Usvyatsov, and Kon-
rad Schindler Andreas Wieser. Predator: Registration of 3D
point clouds with low overlap. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2021. 3
[21] Di Jia, Kai Wang, ShunLi Luo, TianYu Liu, and Ying Liu.
Braft: Recurrent all-pairs field transforms for optical flow
based on correlation blocks. IEEE Signal Processing Letters ,
28:1575â€“1579, 2021. 2, 3
[22] Y . Keller, A. Averbuch, and Y . Shkolnisky. Algebraically ac-
curate volume registration using eulerâ€™s theorem and the 3D
pseudopolar FFT. In IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition (CVPR) , volume 2, pages 795â€“
800 vol. 2, 2005. 2
[23] Y . Keller, Y . Shkolnisky, and A. Averbuch. V olume
registration using the 3D pseudopolar fourier transform.
IEEE Transactions on Signal Processing , 54(11):4323â€“
4331, 2006. 2
[24] Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weis-
senborn, Georg Heigold, Jakob Uszkoreit, Lucas Beyer,
Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Syl-
vain Gelly, Thomas Unterthiner, and Xiaohua Zhai. An im-
age is worth 16x16 words: Transformers for image recogni-
tion at scale. In International Conference on Learning Rep-
resentations , 2021. 2
[25] S. Kong and C. Fowlkes. Low-rank bilinear pooling for fine-
grained classification. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 7025â€“
7034, Los Alamitos, CA, USA, jul 2017. IEEE Computer
Society. 3
[26] Jinwoo Lee, Minhyuk Sung, Hyunjoon Lee, and Junho Kim.
Neural geometric parser for single image camera calibration.
In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-
Michael Frahm, editors, European Conference on Computer
2596
Vision (ECCV) , pages 541â€“557, Cham, 2020. Springer Inter-
national Publishing. 1
[27] Jake Levinson, Carlos Esteves, Kefan Chen, Noah Snavely,
Angjoo Kanazawa, Afshin Rostamizadeh, and Ameesh
Makadia. An analysis of SVD for deep rotation estimation.
Advances in Neural Information Processing Systems (NIPS) ,
33, 2020. 2
[28] Minhua Li, Qingling Chang, Yuhan Wang, Xinglin Liu, Shit-
ing Xu, and Yan Cui. Stereo matching with multiscale hybrid
cost volume. IEEE Access , 10:100128â€“100136, 2022. 3
[29] Wenbin Li, Sajad Saeedi, John McCormac, Ronald Clark,
Dimos Tzoumanikas, Qing Ye, Yuzhong Huang, Rui Tang,
and Stefan Leutenegger. Interiornet: Mega-scale multi-
sensor photo-realistic indoor scenes dataset. In British Ma-
chine Vision Conference , 2018. 2, 5
[30] Zhengfa Liang, Yulan Guo, Yiliu Feng, Wei Chen, Linbo
Qiao, Li Zhou, Jianfeng Zhang, and Hengzhu Liu. Stereo
matching using multi-level cost volume and multi-scale fea-
ture constancy. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 2019. 2, 3
[31] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji.
Bilinear cnn models for fine-grained visual recognition. In
IEEE International Conference on Computer Vision (ICCV) ,
2015. 3
[32] Chenxi Liu, Alex Schwing, Kaustav Kundu, Raquel Urtasun,
and Sanja Fidler. Rent3d: Floor-plan priors for monocular
layout estimation. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2015. 1
[33] Hai Liu, Shuai Fang, Zhaoli Zhang, Duantengchuan Li, Ke
Lin, and Jiazhang Wang. Mfdnet: Collaborative poses per-
ception and matrix fisher distribution for head pose esti-
mation. IEEE Transactions on Multimedia , 24:2449â€“2460,
2022. 2
[34] David G Lowe. Distinctive image features from scale-
invariant keypoints. International Journal of Computer Vi-
sion, 60(2):91â€“110, 2004. 2, 6, 7
[35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view
synthesis. In European Conference on Computer Vision
(ECCV) , 2020. 1
[36] Piotr Mirowski, Andras Banki-Horvath, Keith Anderson,
Denis Teplyashin, Karl Moritz Hermann, Mateusz Mali-
nowski, Matthew Koichi Grimes, Karen Simonyan, Koray
Kavukcuoglu, Andrew Zisserman, et al. The StreetLearn
environment and dataset. arXiv preprint arXiv:1903.01292 ,
2019. 2, 5
[37] David Mohlin, Josephine Sullivan, and G Â´erald Bianchi.
Probabilistic orientation estimation with matrix fisher distri-
butions. Advances in Neural Information Processing Systems
(NIPS) , 33, 2020. 2
[38] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D
Tardos. Orb-slam: a versatile and accurate monocular slam
system. IEEE Transactions on Robotics , 31(5):1147â€“1163,
2015. 1
[39] Guang-Yu Nie, Ming-Ming Cheng, Yun Liu, Zhengfa Liang,
Deng-Ping Fan, Yue Liu, and Yongtian Wang. Multi-level
context ultra-aggregation for stereo matching. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition(CVPR) , pages 3283â€“3291, 2019. 2, 3
[40] Onur Ozyesil, Vladislav V oroninski, Ronen Basri, and Amit
Singer. A survey of structure from motion. arXiv preprint
arXiv:1701.08493 , 2017. 1
[41] Yiming Qian and James H. Elder. A reliable online method
for joint estimation of focal length and camera rotation. In
European Conference on Computer Vision (ECCV) , page
249â€“265, Berlin, Heidelberg, 2022. Springer-Verlag. 1
[42] Gernot Riegler and Vladlen Koltun. Free view synthesis. In
European Conference on Computer Vision (ECCV) , 2020. 1
[43] Chris Rockwell, Justin Johnson, and David F. Fouhey. The
8-point algorithm as an inductive bias for relative pose pre-
diction by vits. In International Conference on 3D Vision
(3DV) , 2022. 2, 6, 7
[44] Johannes L Schonberger and Jan-Michael Frahm. Structure-
from-motion revisited. In IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , 2016. 1
[45] Johannes L Sch Â¨onberger, Marc Pollefeys, Andreas Geiger,
and Torsten Sattler. Semantic visual localization. In
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 6896â€“6906, 2018. 1
[46] O. Shakil. An efficient video alignment approach for non-
overlapping sequences with free camera movement. In
ICASSP , volume 2, pages IIâ€“II, 2006. 2
[47] Yoli Shavit and Yosi Keller. Camera pose auto-encoders
for improving pose regression. In European Conference
on Computer Vision (ECCV) , pages 140â€“157, Cham, 2022.
Springer Nature Switzerland. 1
[48] Che Sun, Yunde Jia, Yi Guo, and Yuwei Wu. Global-
aware registration of less-overlap rgb-d scans. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 6357â€“6366, June 2022. 3
[49] Dekel (Basha) T., Moses Y ., and Avidan S. Photo sequenc-
ing.International Journal of Computer Vision , 110(3):275 â€“
289, 2014. Cited by: 8. 1
[50] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea
Cimpoi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, and Ak-
ihiko Torii. Inloc: Indoor visual localization with dense
matching and view synthesis. In IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , pages
7199â€“7209, 2018. 2
[51] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow. In European Conference on
Computer Vision (ECCV) , page 402â€“419, Berlin, Heidel-
berg, 2020. Springer-Verlag. 2, 3, 8
[52] M. Turkoglu, E. Brachmann, K. Schindler, G. J. Brostow,
and A. Monszpart. Visual camera re-localization using graph
neural networks and relative pose supervision. In Interna-
tional Conference on 3D Vision (3DV) , pages 145â€“155, Los
Alamitos, CA, USA, dec 2021. 2
[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Il-
lia Polosukhin. Attention is all you need. In I. Guyon,
U. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Infor-
mation Processing Systems (NIPS) , volume 30. Curran As-
sociates, Inc., 2017. 1, 3
[54] Jianxiong Xiao, Krista A Ehinger, Aude Oliva, and Anto-
nio Torralba. Recognizing scene viewpoint using panoramic
2597
place representation. In IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , pages 2695â€“2702.
IEEE, 2012. 2, 5
[55] Fei Xue, Xin Wu, Shaojun Cai, and Junqiu Wang. Learn-
ing multi-view camera relocalization with graph neural net-
works. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 11372â€“11381, 2020. 2
[56] Gengshan Yang and Deva Ramanan. V olumetric corre-
spondence networks for optical flow. In H. Wallach, H.
Larochelle, A. Beygelzimer, F. d'Alch Â´e-Buc, E. Fox, and R.
Garnett, editors, Advances in Neural Information Processing
Systems (NIPS) , volume 32. Curran Associates, Inc., 2019. 3
[57] Zhengxin Zhang, Qingjie Liu, and Yunhong Wang. Road
extraction by deep residual u-net. IEEE Geoscience and Re-
mote Sensing Letters , 15:749â€“753, 2018. 3
[58] Zhengxin Zhang, Qingjie Liu, and Yunhong Wang. Road
extraction by deep residual u-net. IEEE Geoscience and Re-
mote Sensing Letters , 15(5):749â€“753, 2018. 5, 6, 7, 8
[59] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao
Li. On the continuity of rotation representations in neural
networks. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 5745â€“5753, 2019. 2, 6,
7
[60] Yichao Zhou, Jingwei Huang, Xili Dai, Linjie Luo, Zhili
Chen, and Yi Ma. HoliCity: A city-scale data platform for
learning holistic 3D structures. 2020. arXiv:2008.03286
[cs.CV]. 6
[61] Rui Zhu, Xingyi Yang, Yannick Hold-Geoffroy, Federico
Perazzi, Jonathan Eisenmann, Kalyan Sunkavalli, and Man-
mohan Chandraker. Single view metrology in the wild.
In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-
Michael Frahm, editors, European Conference on Computer
Vision (ECCV) , pages 316â€“333, Cham, 2020. Springer Inter-
national Publishing. 1
2598
