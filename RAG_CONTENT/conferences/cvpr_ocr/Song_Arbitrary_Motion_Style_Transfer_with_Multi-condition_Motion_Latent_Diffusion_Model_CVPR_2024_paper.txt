Arbitrary Motion Style Transfer with Multi-condition Motion Latent Diffusion
Model
Wenfeng Song1, Xingliang Jin1, Shuai Li2,3*, Chenglizhao Chen4â€ , Aimin Hao3,5,
Xia Hou1, Ning Li1, Hong Qin6
1Beijing Information Science and Technology University, China2Zhongguancun Laboratory, China
3State Key Laboratory of Virtual Reality Technology and Systems, Beihang University
4College of Computer Science and Technology, China University of Petroleum (East China)
5Research Unit of Virtual Human and Virtual Surgery (2019RU004), Chinese Academy of Medical Sciences
6Department of Computer Science, Stony Brook University (SUNY at Stony Brook), Stony Brook, New York 11794-2424, USA
MCM -LDM
Trajectory
ContentStyle
 B. Content Motion
A. Style Motion C. Stylized Motion
Figure 1. Arbitrary motion style transfer with our MCM-LDM. The black arrows point to the highlighted style features. These results
illustrate our methodâ€™s capability to maintain the essence of original content while seamlessly infusing it with new stylistic characteristics
and trajectory considerations.
Abstract
Computer animationâ€™s quest to bridge content and style
has historically been a challenging venture, with previous
efforts often leaning toward one at the expense of the other.
This paper tackles the inherent challenge of content-style
duality, ensuring a harmonious fusion where the core nar-
rative of the content is both preserved and elevated through
stylistic enhancements. We propose a novel Multi-condition
Motion Latent Diffusion Model (MCM-LDM) for Arbitrary
Motion Style Transfer (AMST). Our MCM-LDM signifi-
cantly emphasizes preserving trajectories, recognizing their
fundamental role in defining the essence and fluidity of mo-
*,â€  Corresponding authorstion content. Our MCM-LDMâ€™s cornerstone lies in its abil-
ity first to disentangle and then intricately weave together
motionâ€™s tripartite components: motion trajectory, motion
content, and motion style. The critical insight of MCM-
LDM is to embed multiple conditions with distinct priori-
ties. The content channel serves as the primary flow, guid-
ing the overall structure and movement, while the trajec-
tory and style channels act as auxiliary components and
synchronize with the primary one dynamically. This mech-
anism ensures that multi-conditions can seamlessly inte-
grate into the main flow, enhancing the overall animation
without overshadowing the core content. Empirical evalu-
ations underscore the modelâ€™s proficiency in achieving fluid
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
821
and authentic motion style transfers, setting a new bench-
mark in the realm of computer animation. The source code
and model are available at https://github.com/
XingliangJin/MCM-LDM.git .
1. Introduction and Motivation
Computer animation, an intricate melding of computational
prowess and artistic flair, has continually pushed the bound-
aries of what is conceivable in digital realms. Among its
myriad ventures, Arbitrary Motion Style Transfer (AMST)
stands out as an area of heightened intrigue and profound
challenge. The vision it encapsulates is tantalizing: melding
distinct motion styles onto varied content, much like casting
the intense fervor of martial arts onto the delicate pirouettes
of a ballet dancer or infusing the serenity of a meandering
stream with the tumultuous dynamism of a waterfall. How-
ever, the road to actualizing this vision is fraught with com-
plexities that have stymied even advanced methodologies.
Previous methods in motion style transfer, including Mo-
tion Puzzle [26] and others [1, 21, 22, 36, 42, 43], have
made significant strides in AMST. However, two main chal-
lenge still exists. Content-Style Duality: The critical chal-
lenge in AMST is the dual imperative of maintaining con-
tent integrity while seamlessly integrating a distinct, of-
ten contrasting, style. This intricate process involves not
just superimposing stylistic elements but intricately weav-
ing them into the fabric of the original content. As ex-
emplified in Fig. 1-C, the goal is to capture the essence
of the style from style motions (Fig. 1-A) while preserv-
ing the core attributes and dynamics of the content motion
(Fig. 1-B). Achieving this preservation is difficult due to the
complexities of disentangling the intertwined latent spaces
representing content and style.
Granularity of Details: Beyond the broader motion pat-
terns, the devil lies in the details. The style patterns mostly
ignore a critical factor: trajectory. A significant challenge
arises due to the inherent discrepancies between the tra-
jectories characteristic of the original content and the de-
sired style. As illustrated in Fig. 2-A, conventional meth-
ods [1, 26, 36, 42] often directly transpose the content mo-
tionâ€™s trajectory onto the stylized motion. The copy-based
methods, while straightforward, frequently result in unnat-
ural artifacts, such as the common issue of â€˜foot slidingâ€™.
In addressing the content-style duality, we introduce the
Multi-condition Motion Latent Diffusion Model (MCM-
LDM), benefiting from the generative capabilities of dif-
fusion models, known for their effectiveness in captur-
ing complex data distributions. MCM-LDM systemati-
cally segments motion into tripartite components â€” con-
tent, style, and trajectory â€” and employs a multi-condition
guidance mechanism in the denoising process. This allows
the model to generate new styles that are coherent and seam-
lessly integrated with the content, overcoming the common
Multi -condition Motion 
Latent Diffusion Model
Style
TrajectoryCopy of Raw Trajectory from ContentStyle
Content
B. Learning -based Method (Ours)A. Copy -based Methods
Foot Sliding
No Foot Sliding
Content
Learnable Trajectory Based on Content
AutoEncoderFigure 2. Comparisons of trajectory. Our method (B) learns to
preserve motion trajectory during style transfer, while other meth-
ods [1, 26, 36] (A) copy content trajectory directly onto stylized
motions, resulting in foot sliding issue.
pitfall of disjointed or unnatural style transfers.
To tackle the challenge of Granularity of Details, we pro-
pose a custom-designed Multi-condition Denoiser, to skill-
fully balance these conditions, ensuring the natural dynam-
ics of the original motion are preserved while integrating
new stylistic elements. Unlike previous works, we aim for
the learning-based manner as shown in Fig. 2-B. The de-
noiser embeds multiple conditions with distinct priorities
to preserve primary content while dynamically integrating
style and trajectory as secondary conditions, enabling a so-
phisticated balance in guiding the diffusion process. This
mechanism leads to more authentic and cohesive AMST
outcomes (as despite in Fig. 1-C), setting a new standard
in the realm of computer animation.
To summarize, our contributions are listed as follows.
â€¢ We present the first diffusion-based approach in AMST
that integrates trajectory awareness, providing a nuanced
solution that addresses previously unexplored aspects of
motion style transfer.
â€¢ Our innovative MCM-LDM systematically extracts and
guides motion through content, style, and trajectory con-
ditions during the diffusion process, effectively address-
ing the complex challenges of content-style duality and
the granularity of motion details.
â€¢ We propose a novel Multi-condition Denoiser, which pri-
marily serves the content while adapting style and tra-
jectory as secondary conditions, enabling a sophisticated
balance in guiding the diffusion process. This mechanism
leads to authentic and cohesive AMST outcomes, setting
a new standard in the realm of computer animation.
2. Related Work
2.1. Motion Style Transfer
Initial approaches [3, 5, 7, 23, 25, 31, 33, 46, 48, 51] to mo-
tion style transfer predominantly utilized machine learning
techniques. However, these methods often result in subop-
822
timal performance and limited transfer scope. Recent deep
learning-based methods [1, 9, 13, 21, 22, 26, 29, 35, 36,
43, 47] have significantly enhanced the quality and scope
of motion style transfer. For instance, Aberman et al. [1]
introduced a 1D temporal convolution-based network with
AdaIN for motion style control in the latent space. Park et
al. [36] expanded this to spatio-temporal graph convolution
(STGCN) for effective motion content preservation. These
methods, however, are constrained by the need for anno-
tated style data and specific styles. Addressing this, Jang et
al. [26] introduced AMST for individual body parts using
STGCN and a novel body-part style fusion network. Our
approach similarly tackles AMST with our MCM-LDM.
We aim to extend the network to learn to preserve the con-
tent motion trajectory without artifacts.
2.2. Motion Generation via Diffusion Model
Diffusion model has rapidly evolved in the field of motion
generation [8, 12, 14, 30, 39]. Initially, methods [2, 27,
38, 40, 45, 52â€“54] focused on generating motions with dif-
fusion model in the original motion space. For instance,
Zhang et al. [52] was the first to use the diffusion model
for generating motions from text. Later, Tevet et al. [45]
utilized the transformer network structure for the diffusion
model to learn condition-guided motion generation. How-
ever, applying the diffusion model directly to the original
motion space incurs high computational costs and slow in-
ference times. More recent approaches [4, 6, 11, 28, 32, 50]
have shifted towards applying diffusion models in the mo-
tion latent space. MLD [11] first introduced the use of
diffusion models in the continuous latent space of a mo-
tion Variational AutoEncoder (V AE). Kong et al. [28] pro-
posed a discrete diffusion model in the Vector Quantised-
Variational AutoEncoder (VQ-V AE) latent space for text-
to-motion generation. Inspired by these methods, We
present a multi-condition motion latent diffusion model, de-
signed for AMST with enhanced efficiency and versatility.
3. New Method
3.1. Method Overview
Our approach achieves AMST by utilizing motion content,
style, and trajectory as guiding conditions in the denoising
process of our MCM-LDM. As illustrated in Fig. 3, our
method begins with extraction and encoding these condi-
tions using our Multi-condition Extraction module, as de-
tailed in Sec. 3.2. To generate stylized motion guided by
content, trajectory, and style conditions, we introduce our
MCM-LDM, a motion latent diffusion model optimized
for multi-condition guidance, described in Sec. 3.3. In
Sec. 3.4, we provide a detailed description of our Multi-
condition Denoiser (Fig. 4), designed specifically for the
multi-condition guided denoising process.3.2. Multi-condition Extraction
In contrast to conventional motion style transfer meth-
ods [1, 26, 36] that use separate inputs for content and style
during the training stage, our approach takes the same mo-
tion as both the style and content input. Thus, our training
task shifts motion style transfer to self-reconstruction. To
effectively disentangle and encode condition features from
a single motion ( x1:L, where Lis the motion length), we
have designed the Multi-condition Extraction module.
In particular, for the trajectory condition, we employ a
transformer-based Trajectory Encoder Etrato extract and
encode the trajectory t1:Lofx1:L, resulting in trajectory
features ft. However, separating content and style poses
a unique challenge due to their inherent overlap within a
single motion sequence. We introduce Style Extractor Esty
and Content Encoder Econspecifically designed to isolate
the style and content conditions.
ForEsty, inspired by image style transfer methods [10,
15, 24] using pre-trained VGG [41] to extract style fea-
tures, we utilize a pre-trained MotionCLIP [44] as our mo-
tion Style Extractor to extract the style features of motion.
After training MotionCLIP with our specific data format,
the style features fsare derived from the output of Motion-
CLIPâ€™s encoder. Due to the alignment between the latent
space of MotionCLIP and text/image, our Estycan better
capture the style features of the motion.
ForEcon, we initially feed x1:Lthrough the motion en-
coderEof a pre-trained motion V AE, yielding the raw con-
tent features zcin the latent space. Drawing inspiration
from ArtFusion[10], our aim is to prevent the model from
overly relying on the content information. To achieve this
goal, we employ a StyleRemover, which eliminates the style
from the content. Specifically, we apply the Instance Nor-
malization layer to zcbefore subjecting it to transformer
encoding. Finally, we employ linear dimensionality reduc-
tion to obtain the final content feature fc. Given that Esty
extracts the style features, our StyleRemover can naturally
learn how to remove the style from the content. Conse-
quently, the resulting content features effectively preserve
the content information while eliminating the style. The
above extraction of condition features can be expressed as:
ft=Etra(t1:L),
fs=Esty(x1:L),
fc=StyleRemover (E(x1:L)).(1)
By encoding each condition separately, we obtain con-
dition features (fc, ft, fs), ensuring that they remain unaf-
fected by other conditions. These independent conditions
(fc, ft, fs)facilitate individual guidance for the denoising
process in subsequent steps of our MCM-LDM.
823
Styleâ€œbendingâ€Multi -condition Extraction (Sec. 3.2) 
Style 
Remover
Trajectory
Contentâ€œwalkingâ€Trajectory
Encoder
ğ’‡ğ’”ğ’‡ğ’•
ğ’‡ğ’„
Style MotionContent Motion
Multi -condition Motion Latent Diffusion Model (Sec. 3.3)
ğ’›ğŸ ğ’›ğ‘µForward Process
ğ’›ğ‘µ
Style
Extractorğ‘¬ğœ½
ğ’‡ğ’”ğ’‡ğ’•
ğ‘¬ğœ½
ğ’‡ğ’”ğ’‡ğ’•
Denoising Process
Concatenation
Primary
Condition
Secondary
ConditionsÆŠğ’›ğ‘µâˆ’ğŸ
ÆŠğ’›ğŸ
ÆŠğ’›ğŸ
ÆŠFigure 3. Method overview. We have two components: (1) The Multi-condition Extraction obtains content features fcand trajectory
features ftfrom the content motion, while the style features fsare obtained from the style motion. (2) MCM-LDM contains forward
process and denosing process. The condition features guide the denoising process through Multi-condition Denoiser.
3.3. MCM-LDM
As demonstrated in Fig. 3-B, extended from the motion la-
tent diffusion model [11], we employ MCM-LDM for mo-
tion style transfer. Our MCM-LDM leverages both for-
ward and reverse diffusion processes within a motion la-
tent space, which is defined by the same pre-trained motion
V AE used in Econ, under the guidance of multiple condi-
tions: content fc, trajectory ft, and style fs. By encoding
the original motion x1:Lusing the V AE encoder E, we ob-
tain the motion latent feature z0=E(x1:L).
The overall diffusion process is modeled as a Markov
noising process. Starting from a latent motion feature z0,
the forward process progressively adds Gaussian noise to
z0until its distribution approximates a Gaussian distribu-
tionN(0,I)with a mean of 0 and a covariance matrix of
the identity matrix I, indicating uncorrelated variables with
equal variances. The forward diffusion process is governed
by the conditional probability distribution:
q(zn|znâˆ’1) =N(âˆšÎ±nznâˆ’1,(1âˆ’Î±n)I), (2)
where znis the noisy latent feature sampled at diffusion
stepn,nâˆˆ {1, ..., N};q(zn|znâˆ’1)denotes the distribu-
tion of zngiven znâˆ’1; The parameter Î±ncontrols the level
of noise added to znâˆ’1, gradually transforming it until its
distribution approaches N(0,I). For the reverse process,
or the denoising process, starting from a random latent fea-
turezN, The denoising process then progressively predicts
and eliminates the noise at each diffusion step, ultimately
reconstructing the original motion latent feature z0.
To incorporate multi-condition guidance into the denois-
ing process, we designed our Multi-condition Denoiser EÎ¸
to predict noise based on the noisy latent feature zn, the dif-
fusion step n, and the guided conditions (fc, ft, fs). The
process of predicting noise can be represented as:
Eâˆ—
n=EÎ¸(zn, n, f c, ft, fs), (3)where Eâˆ—
ndenotes the predicted noise at step n. For sim-
plicity, we use EÎ¸(zn, fc, fs, ft)to represent the time-
dependent version EÎ¸(zn, n, f c, fs, ft). Further details re-
garding the specific network design and guided strategy can
be found in Sec. 3.4. The objective [20] of our MCM-LDM
can be defined as:
J=EE,n, (fc,ft,fs)
âˆ¥Eâˆ’EÎ¸(zn, fc, ft, fs)âˆ¥2
2
,(4)
where Erepresents the Gaussian noise. In addition, we em-
ploy classifier-free guidance [19] during the training of EÎ¸.
Specifically, we use shared weights for training both the full
condition model EÎ¸(zn, fc, ft, fs)and the dual condition
model EÎ¸(zn, fc, ft,âˆ…)without style condition. âˆ…is a zero
null style feature. During training, we randomly set fs=âˆ…
by 25% chance to train these two models.
During the inference phase, the style condition fsis de-
rived from the style motion, while the content condition fc
and trajectory condition ftare provided by the content mo-
tion. The final stylized motion latent feature Ë†z0is obtained
by progressively predicting the noise in the initial random
noise and denoising it. Using the motion decoder Din the
pre-trained V AE, the final stylized motion can be obtained
asË†x1:L=D( Ë†z0). By utilizing the classifier-free guidance,
the predicted noise in the ndiffusion step is computed using
Eâˆ—
n=Î»EÎ¸(zn, fc, ft, fs) + (1 âˆ’Î»)EÎ¸(zn, fc, ft,âˆ…)(5)
instead of Equ. 3, where Î»is the guidance scale. By adjust-
ing the size of Î», we can adjust the degree of style during
style transfer. Despite being trained for self-reconstruction,
our MCM-LDM effectively incorporates content, trajec-
tory, and style features for significant style transfer dur-
ing inference, even with varying content and style motions.
Moreover, our MCM-LDM ensures that the stylized motion
maintains alignment with the original content motion, ow-
ing to the trajectory condition.
824
3.4. Multi-condition Denoiser
Existing methods [11, 45, 52] for diffusion-based motion
generation primarily focus on single-condition guidance.
However, in our work, we address the challenge of multi-
condition diffusion guidance by introducing our Multi-
condition Denoiser EÎ¸(Fig. 4). EÎ¸effectively achieves an
adaptive balance in guiding the denoising process for our
condition features (fc, ft, fs)by distinguishing them into
primary and secondary components. Specifically, through
both experiments and everyday observations, we have rec-
ognized the significance of motion content compared to tra-
jectory and style. Therefore, we assign the primary condi-
tion designation to the content fc, while the trajectory ft
and style fsserve as secondary conditions for guiding the
denoising process in our EÎ¸. Our EÎ¸utilizes a transfer-
based structure consisting of Klayers. We then introduce
the guiding strategies of these conditions.
Primary Condition Guidance. The primary condition
fcis integrated as follows:
zâ€²
n=Concat (zn, fc), (6)
where zâ€²
nis the concatenated feature vector; Concat (Â·)rep-
resents concatenation. By doing so, we combine the pri-
mary condition with the noisy latent feature znbefore in-
putting it into EÎ¸, ensuring that the primary condition is
involved throughout the entire denoising process, exerting
its influence on the network.
Secondary Conditions and Their Optimization. For
secondary conditions (trajectory ftand style fs), we first
get the corresponding parameters through:
Î³s, Î²s, Î±s=MLP s(fs),
Î³t, Î²t, Î±t=MLP t(ft),(7)
where Î³s,Î²s,Î±s,Î³t,Î²t, andÎ±trepresent the parameters for
the corresponding fsandftconditions; MLP s(Â·), MLP t(Â·)
denotes the different multi-layer perceptron. These param-
eters are then integrated into EÎ¸using AdaLN-Zero [37]:
Ë†zn,kâ€²= Ë†zn,kâˆ’1+Î±sMSA(LN(Ë†zn,kâˆ’1)Î³s+Î²s),
Ë†zn,k= Ë†zn,kâ€²+Î±tMLP(LN(Ë†zn,kâ€²)Î³t+Î²t),(8)
where MSA (Â·)denotes multi-head self-attention, LN (Â·)de-
notes layer normalization; Ë†zn,kâˆ’1andË†zn,krepresent the
output of (kâˆ’1)-th and k-th layer of EÎ¸;Ë†zn,kâ€²represents
the intermediate variable in the k-th layer of EÎ¸; MLP (Â·)de-
notes the multi-layer perceptron. By applying the secondary
conditions at each layer of our EÎ¸through AdaLN-Zero,
they guide the denoising process in a secondary manner.
By prioritizing the primary condition and incorporating
the secondary condition in intermediate layers, our EÎ¸ef-
fectively learns the importance of different conditions, guid-
ing the denoising process. The understanding enables the
LN
MSA
LN
MLPğ’›ğ’âˆ’ğŸğœ¸ğ’”,ğœ·ğ’”
ğœ¸ğ’•,ğœ·ğ’•
ğœ¶ğ’•ğœ¶ğ’”ğ’›ğ’
ğ’‡ğ’„
ğ’‡ğ’”ğ’‡ğ’•
Multi
 -
condition Denoiser
ğ‘²Ã—
ğ‘´ğ‘³ğ‘·ğ’”ğ‘´ğ‘³ğ‘·ğ’•
: Concatenation : Secondary Conditions: Primary ConditionFigure 4. Architecture of Multi-conditon Denoiser. We incor-
porate the content features fcas a primary condition by concate-
nating it with the noisy latent feature zn, achieving a leading role.
In contrast, the trajectory features ftand style features fsserve as
secondary conditions, embedded into content flow dynamically.
network to preserve the significance of motion content, re-
sulting in desirable style transfer results. Consequently, our
approach achieves desirable results in style transfer, as it re-
tains the majority of the motion content, exhibits the speci-
fied style accurately, and preserves the trajectory intact.
4. Experiments
In this section, we conduct a series of experiments to eval-
uate the effectiveness of our MCM-LDM. Firstly, we pro-
vide an overview of the dataset setting and implementation
details. Secondly, we present the quantitative metrics to as-
sess the quality of the stylized motions. Next, we compare
our MCM-LDM with other state-of-the-art methods. Fol-
lowing that, we conduct ablation studies to analyze the im-
pact of our main components. Additionally, we include a
user study to evaluate the performance of our MCM-LDM.
More results and details are provided in the supplements.
4.1. Dataset and Implementation Details
Dataset. As our MCM-LDM aims to achieve arbitrary mo-
tion style transfer, our training data do not need any further
annotated style labels. Therefore, we use a large 3D human
motion dataset HumanML3D [17] to train our model, which
consists of 14,616 diverse motion sequences. The motions
within the dataset are originally taken from AMASS [34]
and HumanAct12 [16] datasets with pre-processing.
Implementation Details . We use an off-the-shelf pre-
trained V AE model from MLD [11], with a latent space size
of 7Ã—256. Following [17], the motion is represented as a
combination of 3D joint rotations, positions, velocities, and
foot contact, and the trajectory is obtained by the rotation
and velocity of the root node. For our Content Encoder, we
employ a dimension reduction from 7 to 6. The classifier-
free guidance scale Î»is set to 2.5. Our Multi-condition De-
noiser EÎ¸utilizes a 9-layer architecture with a dimension of
1,024 and 4 heads. We train our model with a batch size of
128 for 400 epochs, requiring a total training time of 6.67
hours with a single RTX 3090.
825
4.2. Quantitative Metrics
In this section, we present five metrics that we employ to
quantitatively assess the quality of the stylized motions:
FrÂ´echet Motion Distance (FMD), Content Recognition Ac-
curacy (CRA), Style Recognition Accuracy (SRA), Trajec-
tory Similarity Index (TSI), and Foot Sliding Factor (FSF).
The first three metrics are used to evaluate the overall mo-
tion quality, content preservation, and style expression, re-
spectively. They have been widely used in previous motion
style transfer methods [26, 36, 42, 43]. We further propose
TSI and FSF to evaluate trajectory similarity between the
stylized motion and the content motion, and the foot sliding
factors of the stylized motions. We then provide a detailed
introduction of these metrics.
FMD, CRA, and SRA. To evaluate the quality of styl-
ized motions, we employ FMD as a quantitative metric,
which is a variant of Fr Â´echet Inception Distance (FID) [18].
We train a content classifier as a feature extractor using [49]
on a subset of the HumanML3D test set with annotated con-
tent labels. The FMD is computed based on the feature
vectors obtained from the final pooling layer of the clas-
sifier, comparing the real and generated motion sequences.
A lower FMD value indicates higher motion quality. For
the CRA metric, we utilize the same content classifier to
evaluate this metric. A higher CRA value implies that the
generated motion has a higher potential to preserve the con-
tent of the original content motion. Similarly, for the SRA
metric, we train a style classifier in another subset of the
HumanML3D test set with our annotated style label. The
recognition accuracy of style is calculated to obtain the SRA
value. A higher SRA value indicates better style perfor-
mance of the stylized motions.
TSI and FSF. To better evaluate the trajectory preserva-
tion and the degree of foot sliding in the stylized motions,
we employ the TSI and FSF metrics. Specifically, we calcu-
late the distance between the stylized motion trajectory and
the original content motion trajectory using the Euclidean
distance to obtain the TSI metric. For FSF metric, we cal-
culate the foot sliding displacement generated by the feet
during ground contact for each stylized motion. For more
details, please refer to our supplementary material.
Methods FMD â†“CRAâ†‘(%) SRAâ†‘(%) TSIâ†“FSFâ†“
Real Motions â€“ 99.24 100.00 â€“ â€“
1DConv+AdaIN [1] 42.68 31.18 57.00 0.22 2.05
STGCN+AdaIN [36] 129.44 60.43 17.66 0.11 0.93
Motion Puzzle [26] 113.31 26.31 46.33 0.22 2.43
Ours 27.69 35.75 58.00 0.40 1.28
Table 1. Quantitative evaluation. â€˜â†‘â€™ (â€˜â†“â€™) indicates that the value
is better if the metric is larger (smaller); The bold fonts denote
best performers. The results demonstrate that our MCM-LDM
achieves balanced performance in all metrics.4.3. Comparison with State-of-the-art Methods
In this section, we qualitatively and quantitatively
compare four models, including Motion Puzzle [26],
Conv1D+AdaIN from Aberman et al. [1], STGCN+AdaIN
from Park et al. [36] and ours. To ensure a fair compar-
ison, we retrain Motion Puzzle using the HumanML3D
dataset [17], which is the same dataset used for training
our model. Since the original methods of Aberman et
al.[1] and Park et al. [36] are designed for style-labeled
motion data, we retrain two models, Conv1D+AdaIN and
STGCN+AdaIN, using their key components along with
the Motion Puzzle [26]â€™s loss function for arbitrary style
transfer. The Conv1D+AdaIN model corresponds to Aber-
man et al. [1]â€™s method, which utilizes 1D convolution and
AdaIN. On the other hand, the STGCN+AdaIN model rep-
resents Park et al. [36]â€™s method, which incorporates spatio-
temporal graph convolution and AdaIN.
Qualitative Evaluation. As shown in Fig. 5, our MCM-
LDM demonstrates the ability to generate style transfer re-
sults with attractive style features while avoiding the foot
sliding issue. Specifically, in the first row of Fig. 5, our
stylized motion exhibits more pronounced hand movements
that align with the style motion compared to other methods.
Notably, Motion Puzzle [26] and 1DConv+AdaIN [1] in the
first row suffer from significant foot sliding, as indicated by
the purple line, while STGCN+AdaIN fails to transfer the
style. In contrast, our results maintain appropriate foot con-
tact, as we successfully incorporate the motion trajectory
as an additional diffusion condition in our network. This
approach ensures the preservation of the motion trajectory
without directly copying it from the content motion, leading
to improved foot contact alignment.
Quantitative Evaluation. We also provide multidimen-
sional quantitative evaluations, primarily focusing on the
modelâ€™s generated quality, content preservation, style per-
formance, trajectory preservation, and foot sliding degree.
The results of the quantitative evaluations are presented in
Table 1. From the results, our MCM-LDM significantly
outperforms other methods in the FMD metric, indicating
better quality in stylized motions. This can be attributed to
the powerful generation capability of the diffusion model
and the effectiveness of our multi-condition guidance. Ad-
ditionally, we achieve a remarkable balance between con-
tent preservation and style performance. Our MCM-LDM
has the best SRA and the second-best CRA. Though our
CRA is slightly lower than STGCN+AdaIN model, our
method focuses on harmonizing style with content, which
may result in minor content modifications for a more in-
tegrated style. Notably, STGCN+AdaIN, despite its higher
CRA, has the lowest SRA of 17.66, as it tends to reconstruct
the original content motion. Our excellent performance in
style performance and content preservation is attributed to
our successful application of style and content conditional
826
Ours 1DConv+AdaIN [1] STGCN+AdaIN  [36] Motion Puzzle [26] Content Style
Figure 5. Qualitative evaluation. We provide two groups of style transfer cases. The purple line denotes the foot contact with the floor.
We zoom in on the details of foot contact as well as stylistic features for a more straightforward evaluation. The results show that our
MCM-LDM performs better style performance while avoiding the foot sliding issue.
Methods FMD â†“CRAâ†‘(%) SRAâ†‘(%) TSIâ†“
w/o StyleRemover 34.78 93.43 16.88 0.10
w/oft 29.48 30.18 65.11 0.93
w/o MotionCLIP 138.55 28.18 18.00 0.67
Ours 27.69 35.75 58.00 0.40
Table 2. Ablation study. The results validate the importance of
StyleRemover in Econ, pre-trained MotionCLIP in Esty, and tra-
jectory condition ftto our approach.
Methods FMD â†“ CRAâ†‘(%) SRAâ†‘(%) TSIâ†“
w Con. 32.56 33.62 58.66 0.46
w AdaIN 33.43 30.25 59.55 0.51
w Pri. fs 30.09 31.75 58.44 0.46
w Pri. ft 32.81 32.93 55.44 0.49
Ours 27.69 35.75 58.00 0.40
Table 3. Experiments of four guidance strategies in EÎ¸.â€˜w
Con.â€™ and â€˜w AdaINâ€™ represent the fusion mechanisms of concate-
nation and AdaIN for incorporating the secondary conditions into
EÎ¸. â€˜w Pri. fsâ€™ and â€˜w Pri. ftâ€™ respectively represent treating style
or trajectory as a primary condition.
adaptive guidance for diffusion-based motion generation.
We further evaluate the trajectory preservation and
foot sliding using TSI and FSF. Disregarding the
STGCN+AdaIN [36] model, which tends to reconstruct the
original content motion, our MCM-LDM achieves the low-
est FSF. As for the TSI metric, our TSI scores are lower
compared to other methods. This is because other meth-
ods directly replicate the trajectory from the original con-
tent motion to preserve the trajectory, naturally resulting in
high trajectory similarity but inevitably leading to foot slid-
ing issues. In contrast, our MCM-LDM treats trajectories
as an additional condition, allowing the network to learn
trajectory preservation. This achieves a trade-off between
trajectory accuracy and avoiding foot sliding.
4.4. Ablation Study
In this section, we conduct several ablation experiments on
trajectory condition (Table 2), components in the Multi-
OursOurs w/o 
StyleRemoverContent StyleOurs w/o 
Trajectory
Figure 6. Visualization of ablation study . We present the visual-
ization results of two ablation experiments: without our StyleRe-
move and without the trajectory condition. The results showcase
their importance.
condition Extraction (Table 2), and Guidance Strategy in
EÎ¸(Table 3).
Importance of the Trajectory Condition. To assess the
impact of the trajectory condition, we design a denoising
network EÎ¸(zn, fc, fs)that excludes the trajectory condi-
tion (Table 2: â€˜w/o ftâ€™), relying solely on content fcand
style fsfor guidance. The results show an improvement
in the SRA score from 58.00 to 65.11, indicating enhanced
style performance. However, the TSI score experienced a
significant decrease from 0.40 to 0.93. Fig. 6 visualizes
this decline, revealing the networkâ€™s failure to preserve the
original motion trajectories adequately. Such a deficiency is
unacceptable for motion-style transfer. Therefore, by incor-
porating trajectories as an additional condition to EÎ¸, our
MCM-LDM effectively retains the motion trajectories.
Importance of the Components in Multi-condition
Extraction. We conduct separate experiments to evalu-
ate the importance of the StyleRemover in Econand the
pre-trained MotionCLIP in Esty. Firstly, we remove the
StyleRemover module from Econ(Table 2: â€˜w/o StyleRe-
moverâ€™). The results show that the SRA score decreases
Methods Realism Content Preservation Style Performance
1DConv+AdaIN [1] 3.91Â±0.563.91Â±0.653.86Â±0.63
STGCN+AdaIN [36] 3.65Â±0.783.89Â±0.763.01Â±0.74
Motion Puzzle [26] 3.79Â±0.793.90Â±0.703.85Â±0.69
Ours 4.48Â±0.434.45Â±0.454.43Â±0.51
Table 4. User study. The results show that our MCM-LDM out-
performs other methods in terms of realism, content preservation,
and style performance.
827
from 58.00 to 16.88, while the CRA score increases from
35.75 to 93.43. Fig. 6 further visualizes the style trans-
fer result of this experiment. We find that our MCM-LDM
without StyleRemover fails to transfer the style to the con-
tent, resulting in a direct reconstruction of the content mo-
tion. These results demonstrate that our StyleRemover ef-
fectively prevents the model from excessively relying on
content, thereby achieving successful motion style trans-
fer. Secondly, we experiment with the exclusion of the pre-
trained MotionCLIP and the use of a transformer-based en-
coder to extract style features (Table 2: â€˜w/o MotionCLIPâ€™).
The results demonstrate a significant decrease in all metrics,
indicating that the encoder involved in the training process
is ineffective in extracting style features, leading to unsuc-
cessful style transfer. This underscores the importance of
our pre-trained MotionCLIP to capture style characteristics.
Efficiency of the Condition Mechanisms in Multi-
condition Denoiser. We further conduct experiments with
multi-condition settings in our Multi-condition Denoiser
EÎ¸. First, we conduct experiments to explore using vari-
ous conditions as the primary condition. When style fsis
considered the primary condition (Table 3: â€˜w Pri. fsâ€™),
the SRA metric slightly increases, while other metrics ex-
hibit a significant decrease. Conversely, when trajectory ft
is treated as the primary condition (Table 3: â€˜w Pri. ftâ€™), all
metrics decrease noticeably. This decrease can be attributed
to the lack of trajectory information, which negatively im-
pacts the performance of style transfer. These findings high-
light the crucial role of treating content as the primary con-
dition to guide the style transfer process effectively. Sec-
ondly, we conduct experiments involving using two other
fusion mechanisms where the secondary conditions are in-
corporated into our Multi-condition Denoiser EÎ¸. These
fusion mechanisms include concatenation and AdaIN (Ta-
ble 3: â€˜w Con.â€™ and â€˜w AdaINâ€™). The results indicate that
both fusion mechanisms lead to a slight increase in the SRA
metric but a decrease in other metrics. To achieve a more
balanced style transfer effect, we utilize the AdaLN-Zero as
our fusion mechanism.
4.5. User Study
In this section, we present a user study evaluating stylized
motions of our MCM-LDM in comparison with other meth-
ods, including Conv1D+AdaIN [1], STGCN+AdaIN [36],
and Motion Puzzle [26]. Participants are asked to rate re-
sults generated by these methods on a scale of 1 (signif-
icantly inaccurate) to 5 (significantly accurate), based on
three metrics: (1) Realism: the naturalness of the stylized
motion, (2) Content Preservation: the level of the stylized
motion to preserve the content information from content
motion, and (3) Style Performance: the level of the stylized
motion to perform the style features from style motion.
As shown in Table 4, our method achieves the highestscore in three metrics. Moreover, we conduct an ANOV A
test to statistically examine the differences. The overall
ANOV A establishes considerable distinctions among Real-
ism (F=11.749, p<0.01), Content Preservation ( F=6.864,
p<0.01), and Style Performance ( F=30.619, p<0.01). The
post-hoc analysis suggests that our method is significantly
higher than other methods across three metrics (all p<0.01).
These results further validate the effectiveness of our MCM-
LDM in style transfer, making it more favored by users.
4.6. Limitation and Discussion
Although MCM-LDM could transfer arbitrary motion style
with multi-conditions, it still has some limitations. First,
our MCM-LDM could not generate animations with arbi-
trary trajectories, which is the same as the content, and the
user study may have been biased due to the statistic com-
putation. Second, MCM-LDM tends to be less effective
with motions extending beyond the training datasetâ€™s tem-
poral scope. Another limitation is the modelâ€™s capability
to handle content actions that involve intricate interactions
with the environment. Possible directions include explor-
ing advanced trajectory modification techniques, expanding
the training datasets to encompass longer motion sequences,
and enhancing the modelâ€™s ability to understand and repli-
cate environmental interactions.
5. Conclusion and Future Work
We introduced a pioneering approach to AMST through
our MCM-LDM. Our model marks a significant advance-
ment in the field of computer animation, particularly in its
nuanced handling of motion trajectory, content, and style.
We proposed a Multi-condition Denoiser to disentangle and
harmoniously integrate the tripartite components of mo-
tionâ€”trajectory, content, and style. This ensures a seam-
less integration of various conditions, thereby maintaining
the authenticity of the animation and enhancing its overall
appeal. In the future, we aim to extend our modelâ€™s capabil-
ities to handle more nuanced expressions and subtle human
gestures, thereby enhancing its utility in creating emotion-
ally resonant animations. The potential integration of facial
and finger movements within our framework could lead to
more comprehensive and lifelike character animations.
Acknowledgments
This paper is supported by Beijing Natural Science Foun-
dation (L232102, 4222024), National Natural Science
Foundation of China (62102036, 62272021, 62172246),
R&D Program of Beijing Municipal Education Commis-
sion (KM202211232003), Beijing Science and Technology
Plan Project Z231100005923039, National Key R&D Pro-
gram of China (No. 2023YFF1203803), the Youth Inno-
vation and Technology Support Plan of Colleges and Uni-
versities in Shandong Province (2021KJ062), USA NSF
IIS-1715985 and USA NSF IIS-1812606 (awarded to Hong
QIN).
828
References
[1] Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-
Or, and Baoquan Chen. Unpaired motion style transfer from
video to animation. ACM Transactions on Graphics , 39(4):
64:1â€“64:12, 2020. 2, 3, 6, 7, 8
[2] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and
Gustav Eje Henter. Listen, denoise, action! audio-driven
motion synthesis with diffusion models. ACM Transactions
on Graphics , 42(4):1â€“20, 2023. 3
[3] Kenji Amaya, Armin Bruderlin, and Tom Calvert. Emotion
from motion. In Graphics Interface , pages 222â€“229, 1996.
2
[4] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:
Gesture diffusion model with clip latents. ACM Transactions
on Graphics , 42(4):1â€“18, 2023. 3
[5] Andreas Aristidou, Qiong Zeng, Efstathios Stavrakis,
KangKang Yin, Daniel Cohen-Or, Yiorgos Chrysanthou,
and Baoquan Chen. Emotion control of unstructured dance
movements. In Proceedings of the Eurographics Symposium
on Computer Animation , pages 1â€“10, 2017. 2
[6] German Barquero, Sergio Escalera, and Cristina Palmero.
Belfusion: Latent diffusion for behavior-driven human mo-
tion prediction. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision , pages 2317â€“2327,
2023. 3
[7] Matthew Brand and Aaron Hertzmann. Style machines. In
Proceedings of the Annual Conference on Computer Graph-
ics and Interactive Techniques , pages 183â€“192, 2000. 2
[8] Joao Carvalho, An T Le, Mark Baierl, Dorothea Koert, and
Jan Peters. Motion planning diffusion: Learning and plan-
ning of robot motions with diffusion models. In IEEE/RSJ
International Conference on Intelligent Robots and Systems ,
pages 1916â€“1923, 2023. 3
[9] Ziyi Chang, Edmund J. C. Findlay, Haozheng Zhang, and
Hubert P. H. Shum. Unifying human motion synthesis and
style transfer with denoising diffusion probabilistic mod-
els. In Proceedings of the International Joint Conference on
Computer Vision, Imaging and Computer Graphics Theory
and Applications , pages 64â€“74, 2023. 3
[10] Dar-Yen Chen. Artfusion: Arbitrary style transfer using
dual conditional latent diffusion models. arXiv preprint
arXiv:2306.09330 , 2023. 3
[11] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, and Gang Yu. Executing your commands via motion
diffusion in latent space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 18000â€“18010, 2023. 3, 4, 5
[12] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav
Golyanik, and Christian Theobalt. Mofusion: A framework
for denoising-diffusion-based motion synthesis. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 9760â€“9770, 2023. 3
[13] Han Du, Erik Herrmann, Janis Sprenger, Klaus Fischer, and
Philipp Slusallek. Stylistic locomotion modeling and syn-
thesis using variational generative models. In Proceedings
of the ACM SIGGRAPH Conference on Motion, Interaction
and Games , pages 1â€“10, 2019. 3[14] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke,
Ali Thabet, and Artsiom Sanakoyeu. Avatars grow legs:
Generating smooth human motion from sparse tracking in-
puts with diffusion model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 481â€“490, 2023. 3
[15] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.
Image style transfer using convolutional neural networks. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2414â€“2423, 2016. 3
[16] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tion2motion: Conditioned generation of 3d human motions.
InProceedings of the International Conference on Multime-
dia, pages 2021â€“2029, 2020. 5
[17] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5152â€“5161, 2022. 5, 6
[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equi-
librium. In Proceedings of the International Conference on
Neural Information Processing Systems , pages 6629â€“6640,
2017. 6
[19] Jonathan Ho and Tim Salimans. Classifier-free diffusion
guidance. arXiv preprint arXiv:2207.12598 , 2022. 4
[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840â€“6851, 2020. 4
[21] Daniel Holden, Jun Saito, and Taku Komura. A deep learning
framework for character motion synthesis and editing. ACM
Transactions on Graphics , 35(4):1â€“11, 2016. 2, 3
[22] Daniel Holden, Ikhsanul Habibie, Ikuo Kusajima, and Taku
Komura. Fast neural style transfer for motion data. IEEE
Computer Graphics and Applications , 37(4):42â€“49, 2017. 2,
3
[23] Eugene Hsu, Kari Pulli, and Jovan Popovi Â´c. Style translation
for human motion. ACM Transactions on Graphics , 24(3):
1082â€“1089, 2005. 2
[24] Xun Huang and Serge Belongie. Arbitrary style transfer in
real-time with adaptive instance normalization. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 1510â€“1519, 2017. 3
[25] Leslie Ikemoto, Okan Arikan, and David Forsyth. Generaliz-
ing motion edits with gaussian processes. ACM Transactions
on Graphics , 28(1):1â€“12, 2009. 2
[26] Deok-Kyeong Jang, Soomin Park, and Sung-Hee Lee. Mo-
tion puzzle: Arbitrary motion style transfer by body part.
ACM Transactions on Graphics , 41(3):1â€“16, 2022. 2, 3, 6,
7, 8
[27] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-
form language-based motion synthesis & editing. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
pages 8255â€“8263, 2023. 3
829
[28] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi,
and Xinchao Wang. Priority-centric human motion genera-
tion in discrete latent space. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 14806â€“
14816, 2023. 3
[29] Shigeru Kuriyama, Tomohiko Mukai, Takafumi Taketomi,
and Tomoyuki Mukasa. Context-based style transfer of tok-
enized gestures. In Computer Graphics Forum , pages 305â€“
315, 2022. 3
[30] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and
Lan Xu. Intergen: Diffusion-based multi-human motion
generation under complex interactions. arXiv preprint
arXiv:2304.05684 , 2023. 3
[31] C. Karen Liu, Aaron Hertzmann, and Zoran Popovi Â´c. Learn-
ing physics-based motion style with nonlinear inverse op-
timization. ACM Transactions on Graphics , 24(3):1071â€“
1081, 2005. 2
[32] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang,
and Yi Yang. Diversemotion: Towards diverse human
motion generation via discrete diffusion. arXiv preprint
arXiv:2309.01372 , 2023. 3
[33] Wanli Ma, Shihong Xia, Jessica K Hodgins, Xiao Yang,
Chunpeng Li, and Zhaoqi Wang. Modeling style and vari-
ation in human motion. In Proceedings of the Eurographics
Symposium on Computer Animation , pages 21â€“30, 2010. 2
[34] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black. Amass: Archive of
motion capture as surface shapes. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 5442â€“5451, 2019. 5
[35] Ian Mason, Sebastian Starke, He Zhang, Hakan Bilen, and
Taku Komura. Few-shot learning of homogeneous human
locomotion styles. Computer Graphics Forum , 37(7):143â€“
153, 2018. 3
[36] Soomin Park, Deok-Kyeong Jang, and Sung-Hee Lee. Di-
verse motion stylization for multiple style domains via
spatial-temporal graph-based generative model. Proceedings
of the ACM on Computer Graphics and Interactive Tech-
niques , 4(3):1â€“17, 2021. 2, 3, 6, 7, 8
[37] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision , pages 4195â€“4205,
2023. 5
[38] Sigal Raab, Inbal Leibovitch, Guy Tevet, Moab Arar,
Amit Haim Bermano, and Daniel Cohen-Or. Single motion
diffusion. In International Conference on Learning Repre-
sentations , 2024. 3
[39] Zhiyuan Ren, Zhihong Pan, Xin Zhou, and Le Kang. Diffu-
sion motion: Generate text-guided 3d human motion by dif-
fusion model. In IEEE International Conference on Acous-
tics, Speech and Signal Processing , pages 1â€“5, 2023. 3
[40] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H
Bermano. Human motion diffusion as a generative prior.
arXiv preprint arXiv:2303.01418 , 2023. 3
[41] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations , 2015.
3[42] Wenfeng Song, Xingliang Jin, Shuai Li, Chenglizhao Chen,
Aimin Hao, and Xia Hou. Finestyle: Semantic-aware fine-
grained motion style transfer with dual interactive-flow fu-
sion. IEEE Transactions on Visualization and Computer
Graphics , 29(11):4361â€“4371, 2023. 2, 6
[43] Tianxin Tao, Xiaohang Zhan, Zhongquan Chen, and Michiel
van de Panne. Style-erd: Responsive and coherent online
motion style transfer. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6583â€“6593, 2022. 2, 3, 6
[44] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,
and Daniel Cohen-Or. Motionclip: Exposing human motion
generation to clip space. In Proceedings of the European
Conference on Computer Vision , pages 358â€“374, 2022. 3
[45] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
Cohen-or, and Amit Haim Bermano. Human motion diffu-
sion model. In International Conference on Learning Repre-
sentations , 2023. 3, 5
[46] Munetoshi Unuma, Ken Anjyo, and Ryozo Takeuchi. Fourier
principles for emotion-based human figure animation. In
Proceedings of the Annual Conference on Computer Graph-
ics and Interactive Techniques , pages 91â€“96, 1995. 2
[47] Yu-Hui Wen, Zhipeng Yang, Hongbo Fu, Lin Gao, Yanan
Sun, and Yong-Jin Liu. Autoregressive stylized motion syn-
thesis with generative flow. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 13607â€“13607, 2021. 3
[48] Shihong Xia, Congyi Wang, Jinxiang Chai, and Jessica Hod-
gins. Realtime style transfer for unlabeled heterogeneous
human motion. ACM Transactions on Graphics , 34(4):1â€“10,
2015. 2
[49] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial tempo-
ral graph convolutional networks for skeleton-based action
recognition. In Proceedings of the AAAI Conference on Ar-
tificial Intelligence , pages 7444â€“7452, 2018. 6
[50] Siyue Yao, Mingjie Sun, Bingliang Li, Fengyu Yang, Junle
Wang, and Ruimao Zhang. Dance with you: The diversity
controllable dancer generation via diffusion models. In Pro-
ceedings of the ACM International Conference on Multime-
dia, pages 8504â€“8514, 2023. 3
[51] M Ersin Yumer and Niloy J Mitra. Spectral style transfer for
human motion between independent actions. ACM Transac-
tions on Graphics , 35(4):1â€“8, 2016. 2
[52] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-
fuse: Text-driven human motion generation with diffusion
model. arXiv preprint arXiv:2208.15001 , 2022. 3, 5
[53] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,
Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Re-
modiffuse: Retrieval-augmented motion diffusion model. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 364â€“373, 2023.
[54] Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, and
Nicu Sebe. Modiff: Action-conditioned 3d motion gener-
ation with denoising diffusion probabilistic models. arXiv
preprint arXiv:2301.03949 , 2023. 3
830
