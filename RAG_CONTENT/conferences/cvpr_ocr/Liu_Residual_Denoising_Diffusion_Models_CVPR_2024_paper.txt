Residual Denoising Diffusion Models
Jiawei Liu1,2,3, Qiang Wang1,4, Huijie Fan1,2*, Yinong Wang5, Yandong Tang1,2, Liangqiong Qu5*
1State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences
2Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Science
3University of Chinese Academy of Sciences4Shenyang University5The University of Hong Kong
{liujiawei,wangqiang,fanhuijie,ytang }@sia.cn, liangqqu@hku.hk
Abstract
We propose residual denoising diffusion models
(RDDM), a novel dual diffusion process that decouples
the traditional single denoising diffusion process into
residual diffusion and noise diffusion. This dual diffusion
framework expands the denoising-based diffusion models,
initially uninterpretable for image restoration, into a
uniï¬ed and interpretable model for both image generation
and restoration by introducing residuals. Speciï¬cally, our
residual diffusion represents directional diffusion from the
target image to the degraded input image and explicitly
guides the reverse generation process for image restoration,
while noise diffusion represents random perturbations in
the diffusion process. The residual prioritizes certainty,
while the noise emphasizes diversity, enabling RDDM to
effectively unify tasks with varying certainty or diversity re-
quirements, such as image generation and restoration. We
demonstrate that our sampling process is consistent with
that of DDPM and DDIM through coefï¬cient transforma-
tion, and propose a partially path-independent generation
process to better understand the reverse process. Notably,
our RDDM enables a generic UNet, trained with only
an L1 loss and a batch size of 1, to compete with state-
of-the-art image restoration methods. We provide code
and pre-trained models to encourage further exploration,
application, and development of our innovative framework
(https://github.com/nachifur/RDDM ).
1. Introduction
In real-life scenarios, diffusion often occurs in complex
forms involving multiple, concurrent processes, such as the
dispersion of multiple gases or the propagation of different
types of waves or ï¬elds. This leads us to ponder whether
the denoising-based diffusion models [ 17,51] have limita-
tions in focusing solely on denoising. Current diffusion-
*Corresponding author.
Noise domainTarget 
domain
(a) DDPMInput domainNoise 
domainTarget 
domain
Residual + NoiseResidual
Noise-carrying 
input domain
(b) OursFigure 1. Denoising diffusion process - DDPM [ 17] (a) and our
residual denoising diffusion process (b). For image restoration,
we introduce residual diffusion to represent the diffusion direction
from the target image to the input image.
based image restoration methods [ 22,39,48,49,82] extend
the diffusion model to image restoration tasks by using de-
graded images as a condition input to implicitly guide the
reverse generation process, without modifying the original
denoising diffusion process [ 17,51]. However, the reverse
process starting from noise seems to be unnecessary, as the
degraded image is already known. The forward process
is non-interpretability for image restoration, as the diffu-
sion process does not contain any information about the de-
graded image, as shown in Fig. 1(a).
In this paper, we explore a novel dual diffusion pro-
cess and propose Residual Denoising Diffusion Models
(RDDM), which can tackle the non-interpretability of a sin-
gle denoising process for image restoration. In RDDM, we
decouple the previous diffusion process into residual diffu-
sion and noise diffusion. Residual diffusion prioritizes cer-
tainty and represents a directional diffusion from the target
image to the conditional input image, and noise diffusion
emphasizes diversity and represents random perturbations
in the diffusion process. Thus, our RDDM can unify dif-
ferent tasks that require different certainty or diversity, e.g.,
image generation and restoration. Compared to denoising-
based diffusion models for image restoration, the residuals
in RDDM clearly indicate the forward diffusion direction
and explicitly guide the reverse generation process for im-
age restoration, as shown in Fig. 1(b).
Speciï¬cally, we redeï¬ne a new forward process that al-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
2773
0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0
First remove residualsThen remove noiseSimultaneous removal 
of residuals and noiseÒ§ğ›½ğ‘¡
à´¤ ğ›¼ğ‘¡Figure 2. Decoupled dual diffusion framework. The previous for-
ward diffusion process is decoupled into residual diffusion and
noise diffusion, while in the reverse process, the simultaneous
sampling can be decoupled into ï¬rst removing the residuals and
then removing noise.
lows simultaneous diffusion of residuals and noise, wherein
the target image progressively diffuses into a purely noisy
image for image generation or a noise-carrying input image
for image restoration. Unlike the previous denoising diffu-
sion model [ 17,51], which uses one coefï¬cient schedule to
control the mixing ratio of noise and images, our RDDM
employs two independent coefï¬cient schedules to control
the diffusion speed of residuals and noise. We found that
this independent diffusion property is also evident in the
reverse generation process, e.g., readjusting the coefï¬cient
schedule within a certain range during testing does not af-
fect the image generation results, and removing the residu-
als ï¬rstly, followed by denoising (see Fig. 2), can also pro-
duce semantically consistent images. Our RDDM is com-
patible with widely used denoising diffusion models, i.e.,
our sampling process is consistent with that of DDPM [ 17]
and DDIM [ 51] by transforming coefï¬cient schedules. In
addition, our RDDM natively supports conditional inputs,
enabling networks trained with only an â„“1loss and a batch
size of 1 to compete with state-of-the-art image restoration
methods. We envision that our models can facilitate a uni-
ï¬ed and interpretable image-to-image distribution transfor-
mation methodology, highlighting that residuals and noise
are equally important for diffusion models, e.g., the residual
prioritizes certainty while the noise emphasizes diversity.
The contributions of this paper are summarized as follows:
â€¢ We propose a novel dual diffusion framework to tackle
the non-interpretability of a single denoising process for
image restoration by introducing residuals. Our residual
diffusion represents a directional diffusion from the target
image to the conditional input image.
â€¢ We introduce a partially path-independent generation pro-
cess that decouples residuals and noise, highlighting their
roles in controlling directional residual shift (certainty)and random perturbation (diversity), respectively.
â€¢ We design an automatic objective selection algorithm to
choose whether to predict residuals or noise for unknown
new tasks.
â€¢ Extensive experiments demonstrate that our method can
be adapted to different tasks, e.g., image generation,
restoration, inpainting and translation, focusing certainty
or diversity, and involving paired or unpaired data.
2. Related Work
Denoising diffusion models (e.g., DDPM [ 17], SGM [ 52,
53], and DDIM [ 51]) were initially developed for image
generation. Subsequent image restoration methods [ 14,39,
48] based on DDPM and DDIM feed a degraded image as a
conditional input to a denoising network, e.g., DvSR [ 62],
SR3 [ 49], and WeatherDiffusion [ 82], which typically re-
quire large sampling steps and batch sizes. Additionally,
the reverse process starting from noise in these methods
seems unnecessary and inefï¬cient for image restoration
tasks. Thus, SDEdit [ 41], ColdDiffusion [ 2], InDI [ 11],
and I2SB [ 29] propose generating a clear image directly
from a degraded image or noise-carrying degraded image.
InDI [ 11] and I2SB [ 29], which also present uniï¬ed image
generation and restoration frameworks, are the most closely
related to our proposed RDDM. Speciï¬cally, the forward
diffusion of InDI, I2SB, and our RDDM consistently em-
ploys a mixture of three terms (i.e., input images Iin, target
imagesI0, and noise Ïµ), extending beyond the denoising-
based diffusion model [ 17,51] which incorporates a mix-
ture of two terms (i.e., I0andÏµ). However, InDI [ 11] and
I2SB [ 29] opt for estimating the target image or its linear
transformation term to replace the noise estimation, akin to
a special case of our RDDM (SM-Res). In contrast, we
introduce residual estimation while also embracing noise
for both generation and restoration tasks. Our RDDM can
further extend DDPM [ 17], DDIM [ 51], InDI [ 11], and
I2SB [ 29] to independent double diffusion processes, and
pave the way for the multi-dimensional diffusion process.
We highlight that residuals and noise are equally impor-
tant, e.g., the residual prioritizes certainty while the noise
emphasizes diversity. In addition, our work is related to
coefï¬cient schedule design [ 44,48], variance strategy op-
timization [ 3,4,24,44], superimposed image decomposi-
tion [ 12,81], curve integration [ 47], stochastic differential
equations [ 53], and residual learning [ 15] for image restora-
tion [ 1,32,56,70,72,75]. See Appendix A.5for detailed
comparison.
3. Background
Denoising diffusion models [ 17,50] aim to learn a distri-
butionpÎ¸(I0) :=/integraltext
pÎ¸(I0:T)dI1:T1to approximate a tar-
1To understand diffusion from an image perspective, we use Iinstead
ofxin DDPM [ 17].
2774
0ğ¼ğ‘‡ ğ¼ğ‘¡âˆ’1 ğ¼ğ‘¡ ğ¼0 â‹¯ğ‘ğœƒ(ğ¼ğ‘¡âˆ’1|ğ¼ğ‘¡)
ğ‘(ğ¼ ğ‘¡|ğ¼ğ‘¡âˆ’1, ğ¼ğ‘Ÿğ‘’ğ‘ )â‹¯ ğ¼ğ‘–ğ‘›+ ğœ– =
ForwardReverse
Restoration GenerationFigure 3. The proposed residual denoising diffusion model (RDDM) is a uniï¬ed framework for image generation and restoration (a shadow
removal task is shown here). We introduce residuals ( Ires) in RDDM, redeï¬ning the forward diffusion process to involve simultaneous
diffusion of residuals and noise. The residuals ( Ires=Iinâˆ’I0) diffusion represents the directional diffusion from the target image I0
to the degraded input image Iin, while the noise ( Ïµ) diffusion represents the random perturbations in the diffusion process. In RDDM, I0
gradually diffuses into IT=Iin+Ïµ,Ïµâˆ¼ N(0,I). In the third columns, ITis a purely noisy image for image generation since Iin= 0,
and a noise-carrying degraded image for image restoration as Iinis the degraded image.
get data distribution q(I0), whereI0are target images and
I1,...,I T(T= 1000 ) are latent images of the same dimen-
sion asI0. In the forward process, q(I0)is diffused into a
Gaussian noise distribution using a ï¬xed Markov chain,
q(I1:T|I0) :=/producttextT
t=1q(It|Itâˆ’1), (1)
q(It|Itâˆ’1) :=N(It;âˆšÎ±tItâˆ’1,(1âˆ’Î±t)I), (2)
whereÎ±1:Tâˆˆ(0,1]T.q(It|Itâˆ’1)can also be written as
It=âˆšÎ±tItâˆ’1+âˆš1âˆ’Î±tÏµtâˆ’1. In fact, it is simpler to
sampling ItfromI0by reparameterization [ 25,26],
It=âˆšÂ¯Î±tI0+âˆš
1âˆ’Â¯Î±tÏµ, (3)
whereÏµâˆ¼N(0,I),Â¯Î±t:=/producttextt
s=1Î±s. The reverse process is
also a Markov chain starting at pÎ¸(IT)âˆ¼N(IT;0,I),
pÎ¸(I0:T) :=pÎ¸(IT)/producttextT
t=1pÎ¸(Itâˆ’1|It), (4)
pÎ¸(Itâˆ’1|It) :=N(Itâˆ’1;ÂµÎ¸(It,t),Î£tI), (5)
wherepÎ¸(Itâˆ’1|It)is a learnable transfer probability (the
variance schedule Î£tis ï¬xed). A simpliï¬ed loss func-
tion [ 17] is derived from the maximum likelihood of pÎ¸(I0),
i.e.,L(Î¸) :=EI0âˆ¼q(I0),Ïµâˆ¼N(0,I)/bracketleftBig
âˆ¥Ïµâˆ’ÏµÎ¸(It,t)âˆ¥2/bracketrightBig
. The es-
timated noise ÏµÎ¸can be used to represent ÂµÎ¸inpÎ¸(Itâˆ’1|It),
thusItâˆ’1can be sampled from pÎ¸(Itâˆ’1|It)step by step.
4. Residual Denoising Diffusion Models
Our goal is to develop a dual diffusion process to unify
and interpret image generation and restoration. We mod-
ify the representation of IT=Ïµin traditional DDPM to
IT=Iin+Ïµin our RDDM, where Iinis a degraded im-
age (e.g., a shadow, low-light, or blurred image) for image
restoration and is set to 0for image generation. This modi-
ï¬cation is compatible with the widely used denoising diffu-
sion model, e.g., IT= 0+Ïµis the pure noise ( Ïµ) for genera-
tion. For image restoration, ITis a noisy-carrying degraded
image (Iin+Ïµ), as shown in the third column in Fig. 3.The modiï¬ed forward process from I0toIT=Iin+Ïµin-
volves progressively degrading I0toIin, and injecting noise
Ïµ. This naturally results in a dual diffusion process, a resid-
ual diffusion to model the transition from I0toIinand a
noise diffusion. For example, the forward diffusion process
from the shadow-free image I0to the noisy carrying shadow
imageITinvolves progressively adding shadows and noise,
as shown in the second row in Fig. 3.
In the following subsections, we detail the underlying
theory and the methodology behind our RDDM. Inspired
by residual learning [ 15,31,32], we redeï¬ne each forward
diffusion process step in Section 4.1. For the reverse pro-
cess, we present a training objective to predict the residuals
and noise injected in the forward process in Section 4.2.
In Section 4.3, we propose three sampling methods, i.e.,
residual prediction (SM-Res), noise prediction (SM-N), and
â€œresidual and noise predictionâ€ (SM-Res-N).
4.1. Directional Residual Diffusion Process with
Perturbation
To model the gradual degradation of image quality and the
increment of noise, we deï¬ne the single forward process
step in our RDDM as follows:
It=Itâˆ’1+It
res, It
resâˆ¼N(Î±tIres,Î²2
tI),(6)
whereIt
resrepresents a directional mean shift (residual dif-
fusion) with random perturbation (noise diffusion) from
stateItâˆ’1to stateIt, the residuals IresinIt
resis the dif-
ference between IinandI0(i.e.,Ires=Iinâˆ’I0), and
two independent coefï¬cient schedules Î±tandÎ²tcontrol the
residual and noise diffusion, respectively. In fact, it is sim-
pler to sample ItfromI0(like Eq. 3),
It=Itâˆ’1+Î±tIres+Î²tÏµtâˆ’1,
=Itâˆ’2+(Î±tâˆ’1+Î±t)Ires+(/radicalBig
Î²2
tâˆ’1+Î²2
t)Ïµtâˆ’2
=...
=I0+ Â¯Î±tIres+Â¯Î²tÏµ,(7)
2775
whereÏµtâˆ’1,...Ïµâˆ¼ N(0,I),Â¯Î±t=/summationtextt
i=1Î±iandÂ¯Î²t=/radicalBig/summationtextt
i=1Î²2
i. Ift=T,Â¯Î±T= 1andIT=Iin+Â¯Î²TÏµ.Â¯Î²Tcan
control the intensity of noise perturbation for image restora-
tion (e.g., Â¯Î²2
T= 0.01for shadow removal), while Â¯Î²2
T= 1
for image generation. From Eq. 6, the joint probability dis-
tributions in the forward process can be deï¬ned as:
q(I1:T|I0,Ires) :=/producttextT
t=1q(It|Itâˆ’1,Ires), (8)
q(It|Itâˆ’1,Ires) :=N(It;Itâˆ’1+Î±tIres,Î²2
tI).(9)
Eq. 7deï¬nes the marginal probability distribution
q(It|I0,Ires) =N(It;I0+ Â¯Î±tIres,Â¯Î²2
tI). In fact, the for-
ward diffusion of our RDDM is a mixture of three terms
(i.e.,I0,Ires, andÏµ), extending beyond the widely used de-
noising diffusion model that is a mixture of two terms, i.e,
I0andÏµ. A similar mixture form of three terms can be seen
in several concurrent works, e.g., InDI [ 11], I2SB [ 29], IR-
SDE [ 40], and ResShift [ 67].
4.2. Generation Process and Training Objective
In the forward process (Eq. 7), residuals ( Ires) and noise
(Ïµ) are gradually added to I0, and then synthesized into It,
while the reverse process from ITtoI0involves the esti-
mation of the residuals and noise injected in the forward
process. We can train a residual network IÎ¸
res(It,t,Iin)
to predict Iresand a noise network ÏµÎ¸(It,t,Iin)to esti-
mateÏµ. Using Eq. 7, we obtain the estimated target images
IÎ¸
0=Itâˆ’Â¯Î±tIÎ¸
resâˆ’Â¯Î²tÏµÎ¸. IfIÎ¸
0andIÎ¸
resare given, the
generation process is deï¬ned as,
pÎ¸(Itâˆ’1|It) :=qÏƒ(Itâˆ’1|It,IÎ¸
0,IÎ¸
res), (10)
where the transfer probability qÏƒ(Itâˆ’1|It,I0,Ires)2fromIt
toItâˆ’1is,
qÏƒ(Itâˆ’1|It,I0,Ires) =N(Itâˆ’1;I0+ Â¯Î±tâˆ’1Ires
+/radicalBig
Â¯Î²2
tâˆ’1âˆ’Ïƒ2
tItâˆ’(I0+ Â¯Î±tIres)
Â¯Î²t,Ïƒ2
tI),(11)
whereÏƒ2
t=Î·Î²2
tÂ¯Î²2
tâˆ’1/Â¯Î²2
tandÎ·controls whether the gen-
eration process is random ( Î·= 1) or deterministic ( Î·= 0).
Using Eq. 10and Eq. 11,Itâˆ’1can be sampled from Itvia:
Itâˆ’1=Itâˆ’(Â¯Î±tâˆ’Â¯Î±tâˆ’1)IÎ¸
res
âˆ’(Â¯Î²tâˆ’/radicalBig
Â¯Î²2
tâˆ’1âˆ’Ïƒ2
t)ÏµÎ¸+ÏƒtÏµt,(12)
whereÏµtâˆ¼N(0,I). WhenÎ·= 1, our RDDM has the sum-
constrained variance, while DDPM has preserving variance
(see Appendix A.4). When Î·= 0 (i.e.,Ïƒt= 0), the sam-
pling process is deterministic,
Itâˆ’1=Itâˆ’(Â¯Î±tâˆ’Â¯Î±tâˆ’1)IÎ¸
resâˆ’(Â¯Î²tâˆ’Â¯Î²tâˆ’1)ÏµÎ¸.(13)
2Eq.11does not change q(It|I0,Ires)in Appendix A.2.We derive the following simpliï¬ed loss function for training
(Appendix A.1):
Lres(Î¸) :=E/bracketleftBig
Î»res/vextenddouble/vextenddoubleIresâˆ’IÎ¸
res(It,t,Iin)/vextenddouble/vextenddouble2/bracketrightBig
,(14)
LÏµ(Î¸) :=E/bracketleftBig
Î»Ïµâˆ¥Ïµâˆ’ÏµÎ¸(It,t,Iin)âˆ¥2/bracketrightBig
, (15)
where the hyperparameters Î»res,Î»Ïµâˆˆ{0,1}, and the train-
ing input image Itis synthesized using I0,Ires, andÏµby
Eq.7.Itcan also be synthesized using Iin(replaceI0in
Eq.7byI0=Iinâˆ’Ires),
It=Iin+(Â¯Î±tâˆ’1)Ires+Â¯Î²tÏµ. (16)
4.3. Sampling Method Selection Strategies
For the generation process (from IttoItâˆ’1),ItandIinare
known, and thus IresandÏµcan represent each other by
Eq.16. From Eq. 14,15,16, we propose three sampling
methods as follows.
SM-Res. WhenÎ»res= 1andÎ»Ïµ= 0, the residuals IÎ¸
resare
predicted by a network, while the noise ÏµÎ¸is represented as
a transformation of IÎ¸
resusing Eq. 16.
SM-N. WhenÎ»res= 0andÎ»Ïµ= 1, the noise ÏµÎ¸is predicted
by a network, while the residuals IÎ¸
resare represented as a
transformation of ÏµÎ¸using Eq. 16.
SM-Res-N. WhenÎ»res= 1andÎ»Ïµ= 1, both the residuals
and the noise are predicted by networks.
To determine the optimal sampling method for real-world
applications, we give empirical strategies and automatic se-
lection algorithms in the following.
Empirical Research. Table 1presents that the SM-Res
shows better results for image restoration but offers a poorer
FID for generation. On the other hand, the SM-N yields
better frechet inception distance (FID in [ 16]) and incep-
tion scores (IS), but is ineffective in image restoration (e.g.,
PSNR 11.34 for shadow and 16.30 for low-light). This may
be due to the inadequacy of using ÏµÎ¸to represent IÎ¸
resin
Eq.16for restoration tasks. We attribute these inconsistent
results to the fact that residual predictions prioritize cer-
tainty, whereas noise predictions emphasize diversity . In
our experiments, we use SM-N for image generation, SM-
Res for low-light (LOL [ 61]), and SM-Res-N for other im-
age restoration tasks. For an unknown new task, we empiri-
cally recommend using SM-N for those requiring greater di-
versity and SM-Res for tasks that demand higher certainty.
Automatic Objective Selection Algorithm (AOSA). To
automatically choose between SM-Res or SM-N for an un-
known task, we develop an automatic sampling selection
algorithm in Appendix B.2. This algorithm requires only
a single network and learns the hyperparameter in Eq. 15,
enabling a gradual transition from combined residual and
noise training (akin to SM-Res-N) to individual prediction
(SM-Res or SM-N). This plug-and-play training strategy re-
quires less than 1000 additional training iterations and is
2776
Sampling MethodGeneration (CelebA) Shadow removal (ISTD) Low-light (LOL) Deraining (RainDrop)
FID (â†“) IS (â†‘) MAE(â†“) PSNR(â†‘) SSIM(â†‘)PSNR(â†‘) SSIM(â†‘)PSNR(â†‘) SSIM(â†‘)
SM-Res 31.47 1.73 4.76 30.72 0.959 25.39 0.937 31.96 0.9509
SM-N 23.25 2.05 81.01 11.34 0.175 16.30 0.649 19.15 0.7179
SM-Res-N 28.90 1.78 4.67 30.91 0.962 23.90 0.931 32.51 0.9563
Table 1. Sampling method analysis. The sampling steps are 10 on the CelebA 64 Ã—64 [36] dataset, 5 on the ISTD [ 57] dataset, 2 on the
LOL [ 61] dataset, and 5 on the RainDrop [ 45] dataset.
fully compatible with the current denoising-based diffusion
methods [ 17]. Our RDDM using AOSA has the potential to
provide a uniï¬ed and interpretable methodology for mod-
eling, training, and inference pipelines for unknown target
tasks.
Comparison with Other Prediction Methods. Our
SM-N is similar to DDIM [ 51] (or DDPM [ 17]), which
only estimates the noise, and is consistent with DDPM and
DDIM by transforming the coefï¬cient/variance schedules
in Eq. 12(the proof in Appendix A.3),
Â¯Î±t= 1âˆ’/radicalBig
Â¯Î±t
DDIM3,Â¯Î²t=/radicalBig
1âˆ’Â¯Î±t
DDIM,
Ïƒ2
t=Ïƒ2
t(DDIM).(17)
In fact, current research has delved into numerous dif-
fusion forms that extend beyond noise estimation. For ex-
ample, IDDPM [ 44] proposes that it is feasible to estimate
noise (ÏµÎ¸), clean target images ( IÎ¸
0), or the mean term ( ÂµÎ¸)
to represent the transfer probabilities (i.e., pÎ¸(Itâˆ’1|It)in
Eq.5). The score-based generative model (SGM) [ 52] and
Schr Â¨odinger Bridge (I2SB [ 29]) estimate the score of noisy
data (i.e., the sum of residuals and noise/summationtextt
i=1It
res). Cold-
Diffusion [ 2] and InDI [ 11] estimate the clean target im-
ages (I0). Rectiï¬ed Flow [ 35] predicts the residuals ( Ires)
to align with the image linear interpolation process without
noise diffusion (i.e., IT=Iin). A detailed comparison can
be found in Appendix A.5.
These previous/concurrent works choose to estimate the
noise, the residual, the target image, or its linear transfor-
mation term. In contrast, we introduce residual estimation
while also embracing noise for both generation and restora-
tion. Residuals and noise have equal and independent sta-
tus, which is reï¬‚ected in the forward process (Eq. 7), the re-
verse process (Eq. 13), and the loss function (Eq. 15). This
independence means that the noise diffusion can even be
removed and only the residual diffusion retained to model
the image interpolation process (when Â¯Î²T= 0 in Eq. 7,
RDDM degenerates to Rectiï¬ed Flow [ 35]). In addition,
this property derives a decoupled dual diffusion framework
in Section 5.
5. Decoupled Dual Diffusion Framework
Upon examining DDPM from the perspective of RDDM,
we discover that DDPM indeed involves the simultaneous
3Â¯Î±t
DDIMhere isÎ±tof DDIM [ 51].Schedules FID ( â†“) IS (â†‘)
Linear (DDIM [ 51]) 28.3942.05
Scaled linear [ 48] 28.15 2.00
Squared cosine [ 44] 47.21 2.64
Î±t(mean),Î²2
t(mean) 38.35 2.22
Î±t(linearly increasing), Î²2
t(linearly increasing) 40.03 2.45
Î±t(linearly decreasing), Î²2
t(linearly decreasing) 27.82 2.26
Î±t(linearly decreasing), Î²2
t(linearly increasing) 23.25 2.05
Table 2. Coefï¬cient schedules analysis on CelebA ( 64Ã—64) [36].
In our RDDM, the residual diffusion and noise diffusion are de-
coupled, so one may design a better schedule in the decoupled
coefï¬cient space, e.g., Î±t(linearly decreasing), Î²2
t(linearly in-
creasing). To be fair, all coefï¬cient schedules were retrained using
the same network structure, training, and evaluation. The sampling
method is SM-N with 10 sampling steps using Eq. 13.
diffusion of residuals and noise, which is evident as Eq. 48
becomes equivalent to Eq. 44in Appendix A.3. We ï¬nd that
it is possible to decouple these two types of diffusion. Sec-
tion 5.1presents a decoupled forward diffusion process. In
Section 5.2, we propose a partially path-independent gener-
ation process and decouple the simultaneous sampling into
ï¬rst removing the residuals and then removing noise (see
Fig.6(d) and Fig. 17). This decoupled dual diffusion frame-
work sheds light on the roles of deresidual and denoising in
the DDPM generation process.
5.1. Decoupled Forward Diffusion Process
Our deï¬ned coefï¬cients ( Î±t,Î²2
t) offer a distinct physical in-
terpretation. In the forward diffusion process (Eq. 7),Î±t
controls the speed of residual diffusion and Î²2
tregulates the
speed of noise diffusion. In the reverse generation process
(Eq. 13),Â¯Î±tandÂ¯Î²tare associated with the speed of remov-
ing residual and noise, respectively. In fact, there are no
constraints on Î±tandÎ²2
tin Eq. 7, meaning that the resid-
ual diffusion and noise diffusion are independent of each
other. Utilizing this decoupled property and the difference
between these two diffusion processes, we should be able
to design a better coefï¬cient schedule, e.g., Î±t(linearly de-
creasing) and Î²2
t(linearly increasing) in Table 2. This aligns
with the intuition that, during the reverse generation pro-
4Our RDDM is implemented based on the popular diffusion repos-
itory github.com/lucidrains/denoising-diffusion-pytorch. Differences in
network structure and training details may lead to poorer FID. We have
veriï¬ed sampling consistency with DDIM [ 51] in Table 3(a) and Ap-
pendix A.3.
2777
0 200 400 600 800 1000
/uni000000570.900.920.940.960.981.00t
DDIM
/uni0000000b/uni00000044/uni0000000c/uni00000003t
DDIM/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000046/uni00000052/uni00000056/uni0000000c
0 200 400 600 800 1000
/uni000000570.00.51.01.52.02.5 t1e 3
/uni0000000b/uni00000045/uni0000000c/uni00000003t
DDIM t/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000046/uni00000052/uni00000056/uni0000000c
0 200 400 600 800 1000
/uni000000570.00.51.01.52.02.53.03.54.02
t1e 3
/uni0000000b/uni00000046/uni0000000c/uni00000003t
DDIM2
t/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000047/uni00000003/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000000c
/uni00000027/uni00000027/uni0000002c/uni00000030/uni00000003/uni0000000b/uni00000056/uni00000054/uni00000058/uni00000044/uni00000055/uni00000048/uni00000047/uni00000003/uni00000046/uni00000052/uni00000056/uni0000000c
0 200 400 600 800 1000
/uni000000570.00.51.01.52.02.53.0t,2
t1e 3
/uni0000000b/uni00000047/uni0000000c/uni00000003 t,2
t/uni00000050/uni00000048/uni00000044/uni00000051
/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000003/uni00000047/uni00000048/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni0000004c/uni00000051/uni0000004a
/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni0000004f/uni0000005c/uni00000003/uni0000004c/uni00000051/uni00000046/uni00000055/uni00000048/uni00000044/uni00000056/uni0000004c/uni00000051/uni0000004a
0 200 400 600 800 1000
/uni000000570.00.51.01.52.0 t1e 3
/uni0000000b/uni00000048/uni0000000c/uni00000003P(x,a)P(1 x, 0.3)
P(1 x, 0.5)
P(1 x, 0.8)
P(1 x, 0.9)
P(1 x, 1.0)
P(1 x, 1.1)
P(1 x, 1.2)
P(1 x, 1.5)
Figure 4. Coefï¬cient transformation from DDIM [ 51] to RDDM
using Eq. 17. (a) We show several schedules for Î±t
DDIM , e.g., lin-
ear [51], scaled linear [ 48], and squared cosine [ 44]. (b) We trans-
formÎ±t
DDIM intoÎ±tin our RDDM. (c) We transform Î±t
DDIM
intoÎ²2
tin our RDDM. (d) A few simple schedules. (e) P(x,a)
is a normalized power function (see Eq. 18). â€meanâ€, â€linearly
increasingâ€, and â€linearly decreasingâ€ in (d) can be denoted as
P(x,0),P(x,1)andP(1âˆ’x,1), respectively. See Algorithm 1
in Appendix A.3for more details of (b) and (c).
ğ‘ƒ(1 âˆ’ ğ‘¥, 1.0) ğ‘ƒ(1 âˆ’ ğ‘¥, 1.2) ğ‘ƒ(1 âˆ’ ğ‘¥, 1.5)
Score:9.1 Score:9.3 Score:8.2
(f) convert ğ›¼ğ·ğ·ğ¼ğ‘€ğ‘¡to ğ›¼ğ‘¡, ğ›½ğ‘¡2and readjust the converted ğ›¼ğ‘¡without touching the ğ›½ğ‘¡2
ğ‘ƒ(1 âˆ’ ğ‘¥, 0.3) ğ‘ƒ(1 âˆ’ ğ‘¥, 0.5) ğ‘ƒ(1 âˆ’ ğ‘¥, 0.8)
Score:9.8 Score:9.8 Score:9.7
(a) DDIM (linear) (b) ğ›¼ğ·ğ·ğ¼ğ‘€ğ‘¡â†’ ğ›¼ ğ‘¡, ğ›½ğ‘¡2
Score:9.4 Score:9.4
(c) ğ›¼ğ·ğ·ğ¼ğ‘€ğ‘¡â†’
scaled linear(e) ğ›¼ğ‘¡â†’ ğ›¼ ğ‘¡
ğ›½ğ‘¡2â†’ ğ‘ƒ(1 âˆ’ ğ‘¥, 1)(d) ğ›¼ğ·ğ·ğ¼ğ‘€ğ‘¡â†’
squared cosine
Figure 5. Analysis of readjusting coefï¬cient schedules. We ï¬nd
that changing the Î±tschedule barely affects the denoising process
in (f) and edited faces may have higher face scores when assessed
using AI face scoring software5. These images were generated
using a pre-trained UNet on the CelebA ( 256Ã—256) dataset [ 36]
with 10 sampling steps.
cess (from Tto0), the estimated residuals become increas-
ingly accurate while the estimated noise should also weaken
progressively. Therefore, when tis close to 0, the deresid-
ual pace should be faster and the denoising pace should be
slower. Since our Î±tandÎ²2
trepresent the speed of diffu-
sion, we name the curve in Fig. 4(b-d) the diffusion speed
curve .
5.2. Partially PathÂ­independent Generation Process
In the original DDPM [ 17] or DDIM [ 51], when the
Î±t
DDIM schedule changes, it is necessary to retrain the
denoising network because this alters the diffusion pro-
cess [ 44,48]. As shown in Fig. 5(c)(d), directly changing
theÎ±t
DDIM schedule causes denoising to fail. Here, wepropose a path-independent generation process, i.e., mod-
ifying the diffusion speed curve does not cause the image
generation process to fail. We try to readjust the diffusion
speed curve in the generation process. First, we convert the
Î±t
DDIM schedule of a pre-trained DDIM into the Î±tand
Î²2
tschedules of our RDDM using Eq. 17(from Fig. 5(a)
to Fig. 5(b). We then readjust the converted Î±tschedules
using the normalized power function ( P(x,a)in Fig. 5(f)),
without touching the Î²2
tschedule that controls noise diffu-
sion, as shown in Fig. 5(f).P(x,a)is deï¬ned as ( ais a
parameter of the power function),
P(x,a) :=xa//integraldisplay1
0xadx,wherex=t/T. (18)
These schedule modiï¬cations shown in Fig. 5lead to the
following key ï¬ndings.
1.Fig. 5(f) shows that modifying the residual diffusion
speed curve ( Î±t) leads to a drastic change in the generation
results, probably due to IÎ¸
resbeing represented as a trans-
formation of ÏµÎ¸using Eq. 16.
2.As the time condition trepresents the current noise
intensity in the denoising network ( ÏµÎ¸(It,t,0)), modifying
the noise diffusion speed curve ( Î²2
t) causestto deviate from
accurately indicating the current noise intensity, leading to
denoising failure, as shown in Fig. 5(e).
Nonetheless, we believe that, corresponding to the de-
coupled forward diffusion process, there should also be a
path-independent reverse generation process. To develop a
path-independent generation process, we improve the gen-
eration process based on the above two key ï¬ndings:
1.Two networks are used to estimate IÎ¸
resandÏµÎ¸sepa-
rately, i.e., SM-Res-N-2Net in Appendix B.2.
2.Â¯Î±tandÂ¯Î²tare used for the time conditions embed-
ded in the network, i.e., IÎ¸
res(It,t,0)â†’IÎ¸
res(It,Â¯Î±tÂ·T,0),
ÏµÎ¸(It,t,0)â†’ÏµÎ¸(It,Â¯Î²tÂ·T,0).
These improvements lead to a partially path-independent
generation process, as evidenced by the results shown in
Fig.6(c).
Analysis of Partially Path-independence via Greenâ€™s
Theorem. â€œPath-independenceâ€ reminds us of Greenâ€™s the-
orem in curve integration [ 47]. From Eq. 13, we have:
Itâˆ’Itâˆ’1=(Â¯Î±tâˆ’Â¯Î±tâˆ’1)IÎ¸
res+(Â¯Î²tâˆ’Â¯Î²tâˆ’1)ÏµÎ¸, (19)
dI(t) =IÎ¸
res(I(t),Â¯Î±(t)Â·T,0)dÂ¯Î±(t)
+ÏµÎ¸(I(t),Â¯Î²(t)Â·T,0)dÂ¯Î²(t),(20)
whereI(t) =I(0)+Â¯Î±(t)Ires+Â¯Î²(t)Ïµ. Given inputs I(t)and
Â¯Î±(t), the denoising network learns to approximate the noise
ÏµinI(t)by estimating ÏµÎ¸. If this network is trained well and
robust enough, it should be able to avoid the interference of
5https://ux.xiaoice.com/beautyv3
2778
(a1) Training: 
DDIM (linear)
(a3) ğ›¼ğ‘¡â†’
ğ‘ƒğ‘¥, 0(a2) ğ›½ğ‘¡2â†’
ğ‘ƒ1 âˆ’ ğ‘¥, 1
(a) Denoising ( ğœ–ğœƒğ¼ğ‘¡,Ò§ğ›½ğ‘¡âˆ™ ğ‘‡, 0 ) 
(b) ğœ–ğœƒğ¼ğ‘¡, ğ‘¡, 0 +ğ¼ğ‘Ÿğ‘’ğ‘ ğœƒğ¼ğ‘¡, ğ‘¡, 0(b1) Training: 
DDIM (linear)
(b2) ğ›¼ğ‘¡, ğ›½ğ‘¡2â†’
ğ‘ƒğ‘¥, 0
(b3) ğ›¼ğ·ğ·ğ¼ğ‘€ğ‘¡â†’
squared cos
(c6) ğ›¼ğ·ğ·ğ¼ğ‘€ğ‘¡â†’
squared cos
(c) Path Independence 
(ğœ–ğœƒğ¼ğ‘¡,Ò§ğ›½ğ‘¡ğ‘‡, 0+ğ¼ğ‘Ÿğ‘’ğ‘ ğœƒğ¼ğ‘¡, à´¤ ğ›¼ğ‘¡ğ‘‡, 0) 
(c2) ğ›¼ğ‘¡, ğ›½ğ‘¡2â†’
ğ‘ƒğ‘¥, 0
(c3) ğ›¼ğ‘¡, ğ›½ğ‘¡2â†’
ğ‘ƒ1 âˆ’ ğ‘¥, 1
(c4) ğ›¼ğ‘¡, ğ›½ğ‘¡2â†’
ğ‘ƒ1 âˆ’ ğ‘¥, 1.5
(c5) ğ›¼ğ·ğ·ğ¼ğ‘€ğ‘¡â†’
scaled linear
(c1) Training: 
DDIM (linear)
(d) Decoupled Sampling (Denoising ( ğœ–ğœƒğ¼ğ‘¡,Ò§ğ›½ğ‘¡ğ‘‡, 0),Deresidual ( ğ¼ğ‘Ÿğ‘’ğ‘ ğœƒğ¼ğ‘¡, à´¤ ğ›¼ğ‘¡ğ‘‡, 0) (d1) Remove 
residuals and noise 
(d3) First remove 
noise then residuals 
First remove 
residuals
Then remove 
noise
(d2) Figure 6. Partially path-independent generation process. (a1) We
trained a denoising network using the DDIM linear schedule [ 51].
(a2-a3) We modiï¬ed the Î±tandÎ²2
tschedules during testing. (b)
We trained two networks to remove noise and residuals. In contrast
to the sharply varying images in (a2-a3) and the noisy images in
(b2-b3), (c) shows that we constructed a path independent genera-
tion process where modiï¬cations to the diffusion speed curve can
generate a noise-free image with little variation in image seman-
tics. (d) The simultaneous sampling in (d1) or (c) can be decom-
posed into ï¬rst removing residuals and then noise (d2), or remov-
ing noise and then residuals (d3). In (d3), diversity is signiï¬cantly
reduced because noise is removed ï¬rst.
the residual terms Â¯Î±(t)IresinI(t). This also applies to a
robust residual estimation network. Thus, we have
âˆ‚IÎ¸
res(I(t),Â¯Î±(t)Â·T)
âˆ‚Â¯Î²(t)â‰ˆ0,âˆ‚ÏµÎ¸(I(t),Â¯Î²(t)Â·T)
âˆ‚Â¯Î±(t)â‰ˆ0.(21)
If the equation in Formula 21holds true, it serves as a nec-
essary and sufï¬cient condition for path independence in
curve integration, which provides an explanation for why
Fig. 6(c) achieves a partially path-independent generation
process. The path-independent property is related to the
networkâ€™s resilience to disturbances and applies to distur-
bances that vary within a certain range. However, exces-
sive disturbances can lead to visual inconsistencies, e.g.,
readjusting Î±tandÎ²2
ttoP(x,5). Thus, we refer to this
generative property as partially path-independent. We also
investigated two reverse paths to gain insight into the im-
plications of the proposed partial path independence. In
the ï¬rst case, the residuals are removed ï¬rst, followed by
the noise: I(T)âˆ’Iresâ†’I(0) +Â¯Î²TÏµâˆ’Â¯Î²TÏµâ†’I(0). The second
case involves removing the noise ï¬rst and then the residuals:
I(T)âˆ’Â¯Î²TÏµâ†’Iinâˆ’Iresâ†’I(0). The ï¬rst case (Fig. 6(d2)) shows(a)CelebA (FID) 5 steps 10 steps 15 steps 20 steps 100 steps
DDIM 69.60 40.45 32.67 30.61 23.66
DDIMâ†’RDDM 69.60 40.41 32.71 30.77 24.92
(b)Shadow MAE(â†“) SSIM(â†‘) PSNR(â†‘)
Removal SNS ALL S NS ALL S NS ALL
DSC [ 19]Â¶ 9.48 6.14 6.67 0.967 - -33.45 - -
FusionNet [ 13]7.77 5.56 5.92 0.975 0.880 0.945 34.71 28.61 27.19
BMNet [ 79] 7.60 4.59 5.02 0.988 0.976 0.959 35.61 32.80 30.28
DMTN [ 31] 7.00 4.28 4.72 0.990 0.979 0.965 35.83 33.01 30.42
Ours (RDDM) 6.67 4.27 4.67 0.988 0.979 0.962 36.74 33.18 30.91
(c)Low-light PSNR(â†‘)SSIM(â†‘)LPIPS (â†“)(d)Deraining PSNR(â†‘)SSIM(â†‘)
KinD++ [ 76] 17.752 0.760 0.198 AttnGAN [ 45] 31.59 0.9170
KinD++-SKF [ 68]20.363 0.805 0.201 DuRN [ 34] 31.24 0.9259
DCC-Net [ 77] 22.72 0.81 - RainAttn [ 46] 31.44 0.9263
SNR-Aware [ 66]24.608 0.840 0.151 IDT [ 64] 31.87 0.9313
LLFlow [ 59] 25.19 0.93 0.11 RainDiff64 [ 82]32.29 0.9422
LLFormer [ 58] 23.649 0.816 0.169 RainDiff128 [ 82]32.43 0.9334
Ours (RDDM) 25.392 0.937 0.116 Ours (RDDM) 32.51 0.9563
Table 3. Quantitative comparison results of image generation on
the CelebA ( 256Ã—256) dataset [ 36], shadow removal on the ISTD
dataset [ 57], low-light enhancement on the LOL [ 61] dataset, and
deraining on the RainDrop [ 45] dataset. â€œS, NS, ALLâ€ in (b) de-
note shadow area (S), non-shadow area (NS) and whole image
(ALL). The sampling steps are 5 for shadow removal and derain-
ing, 2 for low-light.
that removing residuals controls semantic transitions, while
the second case (Fig. 6(d3)) shows that diversity is signif-
icantly reduced because noise is removed ï¬rst. Fig. 6(d)
validates our argument that residuals control directional se-
mantic drift (certainty) and noise controls random perturba-
tion (diversity). See Appendix B.4for more details.
6. Experiments
Image Generation. We can convert a pre-trained6
DDIM [ 51] to RDDM by coefï¬cient transformation using
Eq.17, and generate images by Eq. 12. Table 3(a) veriï¬es
that the quality of the generated images before and after the
conversion is nearly the same7. We show the generated face
images with 10 sampling steps in Fig. 7(a).
Image Restoration. We extensively evaluate our
method on several image restoration tasks, including
shadow removal, low-light enhancement, deraining, and de-
blurring on 5 datasets. Notably, our RDDM uses an iden-
tical UNet and is trained with a batch size of 1 for all
these tasks. In contrast, SOAT methods often involve elabo-
rate network architectures, such as multi-stage [ 13,59,80],
multi-branch [ 10], Transformer [ 58], and GAN [ 27], or so-
phisticated loss functions like the chromaticity [ 20], texture
similarity [ 74], and edge loss [ 70]. Table 3and Fig. 7(b-c)
show that our RDDM is competitive with the SOTA restora-
6https://huggingface.co/google/ddpm-celebahq-256
7The subtle differences in larger sampling steps may stem from errors
introduced by numerical representation limitations during coefï¬cient trans-
formation, which may accumulate and amplify in larger sampling steps.
2779
Input DSC FusionNet BMNet DMTN Ours (RDDM) Ground Truth
Input KinD++  SNR-Aware LLFormer LLFlow Ours (RDDM) Ground Truth
(b)
(c)
(a)
Input Input+Noise Ours Ground Truth
 Input Input+Noise Ours Ground Truth(d)
(e)
First remove residuals Then remove noise Input+NoiseFigure 7. Application of our RDDM. (a) Image generation on the CelebA dataset [ 36]. (b) Shadow removal on the ISTD dataset [ 57]. (c)
Low-light enhancement on the LOL dataset [ 61]. (d) Image inpainting (center and irregular mask). (e) The image translation process can
be regarded as ï¬rst translating the semantics and then generating the details. These images in (b) are magniï¬ed using MulimgViewer [ 30].
tion methods. See Appendix Bfor more training details and
comparison results.
We extend DDPM [ 17]/DDIM [ 51], initially uninter-
pretable for image restoration, into a uniï¬ed and inter-
pretable diffusion model for both image generation and
restoration by introducing residuals. However, the resid-
ual diffusion process represents the directional diffusion
from target images to conditional input images, which does
not involve a priori information about the image restoration
task, and therefore is not limited to it. Beyond image gener-
ation and restoration, we show examples of image inpaint-
ing and image translation to verify that our RDDM has the
potential to be a uniï¬ed and interpretable methodology for
image-to-image distribution transformation. We do not in-
tend to achieve optimal performance on all tasks by tun-
ing all hyperparameters. The current experimental results
show that RDDM 1) achieves consistent image generation
performance with DDIM after coefï¬cient transformation, 2)
competes with state-of-the-art image restoration methods
using a generic UNet with only an â„“1loss, a batch size of
1, and fewer than 5 sampling steps, and 3) has satisfactory
visual results of image inpainting andimage translation
(see Fig. 7(d-e), Fig. 14, or Fig. 15in Appendix B.3), whichvalidates our RDDM.
7. Conclusions
We present a uniï¬ed dual diffusion model called Residual
Denoising Diffusion Models (RDDM) for image restora-
tion and image generation. This is a three-term mixture
framework beyond the previous denoising diffusion frame-
work with two-term mixture. We demonstrate that our sam-
pling process is consistent with that of DDPM and DDIM
through coefï¬cient schedule transformation, and propose
a partially path-independent generation process. Our ex-
perimental results on four different image restoration tasks
show that RDDM achieves SOTA performance in no more
than ï¬ve sampling steps. We believe that our model and
framework hold the potential to provide a uniï¬ed method-
ology for image-to-image distribution transformation and
pave the way for the multi-dimensional diffusion process.
8. Acknowledgments
This work was supported by National Natural Science
Foundation of China under Grants 61991413, 62273339,
62073205, 62306253.
2780
References
[1] Saeed Anwar and Nick Barnes. Densely residual laplacian
super-resolution. IEEE TPAMI , 44(3):1192â€“1204, 2020. 2
[2] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li,
Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas
Geiping, and Tom Goldstein. Cold diffusion: Inverting
arbitrary image transforms without noise. arXiv preprint
arXiv:2208.09392 , 2022. 2,5,16
[3] Fan Bao, Chongxuan Li, Jiacheng Sun, Jun Zhu, and Bo
Zhang. Estimating the optimal covariance with imperfect
mean in diffusion probabilistic models. In Proc. ICML , 2022.
2
[4] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-
dpm: an analytic estimate of the optimal reverse variance in
diffusion probabilistic models. In Proc. ICLR , 2022. 2
[5] Christopher M Bishop and Nasser M Nasrabadi. Pattern
recognition and machine learning . Springer, 2006. 13
[6] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr Â´edo
Durand. Learning photographic global tonal adjustment with
a database of input / output image pairs. In Proc. CVPR ,
2011. 21
[7] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun.
Learning to see in the dark. In Proc. CVPR , pages 3291â€“
3300, 2018. 21
[8] Zipei Chen, Chengjiang Long, Ling Zhang, and Chunxia
Xiao. CANet: A Context-Aware Network for Shadow Re-
moval. In Proc. ICCV , pages 4723â€“4732, 2021. 19,20
[9] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.
Stargan v2: Diverse image synthesis for multiple domains.
InProc. CVPR , pages 8188â€“8197, 2020. 23,28
[10] Xiaodong Cun, Chi Man Pun, and Cheng Shi. Towards
ghost-free shadow removal via dual hierarchical aggregation
network and shadow matting GAN. In Proc. AAAI , pages
10680â€“10687, 2020. 7,17,19,20
[11] Mauricio Delbracio and Peyman Milanfar. Inversion by di-
rect iteration: An alternative to denoising diffusion for image
restoration. Transactions on Machine Learning Research ,
2023. 2,4,5,16,23,26
[12] Huiyu Duan, Wei Shen, Xiongkuo Min, Yuan Tian, Jae-
Hyun Jung, Xiaokang Yang, and Guangtao Zhai. Develop
then rival: A human vision-inspired framework for superim-
posed image decomposition. IEEE TMM , 2022. 2
[13] Lan Fu, Changqing Zhou, Qing Guo, Felix Juefei-Xu,
Hongkai Yu, Wei Feng, Yang Liu, and Song Wang. Auto-
Exposure Fusion for Single-Image Shadow Removal. In
Proc. CVPR , pages 10566â€“10575, 2021. 7,17,19,20
[14] Lanqing Guo, Chong Wang, Wenhan Yang, Siyu Huang,
Yufei Wang, Hanspeter Pï¬ster, and Bihan Wen. Shadowd-
iffusion: When degradation prior meets diffusion model for
shadow removal. In Proc. CVPR , 2023. 2,16,28
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proc.
CVPR , pages 770â€“778, 2016. 2,3
[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. In Proc. NeurIPS , 2017. 4[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Proc. NeurIPS , pages 6840â€“
6851, 2020. 1,2,3,5,6,8,12,13,14,15,16,19
[18] Weipeng Hu and Haifeng Hu. Domain discrepancy elimi-
nation and mean face representation learning for nir-vis face
recognition. IEEE Signal Processing Letters , 28:2068â€“2072,
2021. 26
[19] Xiaowei Hu, Chi Wing Fu, Lei Zhu, Jing Qin, and
Pheng Ann Heng. Direction-aware spatial context features
for shadow detection and removal. IEEE TPAMI , 42(11):
2795â€“2808, 2020. 7,19,20
[20] Yeying Jin, Aashish Sharma, and Robby T. Tan. DC-
ShadowNet: Single-Image Hard and Soft Shadow Removal
Using Unsupervised Domain-Classiï¬er Guided Network. In
Proc. ICCV , pages 5007â€“5016, 2021. 7,17
[21] Yeying Jin, Wenhan Yang, Wei Ye, Yuan Yuan, and Robby T
Tan. Des3: Attention-driven self and soft shadow removal
using vit similarity and color convergence. arXiv preprint
arXiv:2211.08089 , 2022. 25
[22] Yeying Jin, Wenhan Yang, Wei Ye, Yuan Yuan, and Robby T
Tan. Shadowdiffusion: Diffusion-based shadow removal
using classiï¬er-driven attention and structure preservation.
arXiv preprint arXiv:2211.08089 , 2022. 1
[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of gans for improved quality, stability,
and variation. In Proc. ICLR , 2018. 23,26,28
[24] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan
Ho. Variational diffusion models. In Proc. NeurIPS , pages
21696â€“21707, 2021. 2
[25] Diederik P. Kingma and Max Welling. Auto-encoding vari-
ational bayes. In Proc. ICLR , 2014. 3
[26] Diederik P. Kingma and Max Welling. An introduction to
variational autoencoders. Found. Trends Mach. Learn. , 12
(4):307â€“392, 2019. 3
[27] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang
Wang. Deblurgan-v2: Deblurring (orders-of-magnitude)
faster and better. In Proc. ICCV , pages 8878â€“8887, 2019.
7,17,23
[28] Hieu Le and Dimitris Samaras. Shadow removal via shadow
image decomposition. In Proc. ICCV , pages 8578â€“8587,
2019. 17
[29] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evange-
los A Theodorou, Weili Nie, and Anima Anandkumar. I2sb:
Image-to-image schr Â¨odinger bridge. In Proc. ICML , 2023.
2,4,5,16,23
[30] Jiawei Liu. MulimgViewer: A multi-image viewer for image
comparison and image stitching. 8
[31] Jiawei Liu, Qiang Wang, Huijie Fan, Wentao Li, Liangqiong
Qu, and Yandong Tang. A decoupled multi-task network for
shadow removal. IEEE TMM , 2023. 3,7,19,20
[32] Jiawei Liu, Qiang Wang, Huijie Fan, Jiandong Tian, and
Yandong Tang. A shadow imaging bilinear model and three-
branch residual network for shadow removal. IEEE TNNLS ,
pages 1â€“15, 2023. 2,3
[33] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongx-
uan Luo. Retinex-inspired unrolling with cooperative prior
architecture search for low-light image enhancement. In
Proc. CVPR , pages 10561â€“10570, 2021. 21
2781
[34] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki
Okatani. Dual residual networks leveraging the potential
of paired operations for image restoration. In Proc. CVPR ,
pages 7007â€“7016, 2019. 7
[35] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow
straight and fast: Learning to generate and transfer data with
rectiï¬ed ï¬‚ow. In Proc. ICLR , 2023. 5,16
[36] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proc. ICCV ,
2015. 5,6,7,8,15
[37] Zhihao Liu, Hui Yin, Yang Mi, Mengyang Pu, and Song
Wang. Shadow removal by a lightness-guided network with
training on unpaired data. IEEE TIP , 30:1853â€“1865, 2021.
19,20
[38] Gunter Lofï¬‚er, Grigori Yourganov, Frances Wilkinson, and
Hugh R Wilson. fmri evidence for the neural representation
of faces. Nature neuroscience , 8(10):1386â€“1391, 2005. 26
[39] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher
Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpaint-
ing using denoising diffusion probabilistic models. In Proc.
CVPR , pages 11461â€“11471, 2022. 1,2
[40] Ziwei Luo, Fredrik K Gustafsson, Zheng Zhao, Jens Sj Â¨olund,
and Thomas B Sch Â¨on. Image restoration with mean-reverting
stochastic differential equations. In Proc. ICML , 2023. 4
[41] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided
image synthesis and editing with stochastic differential equa-
tions. In Proc. ICLR , 2021. 2
[42] Qiang Meng, Shichao Zhao, Zhida Huang, and Feng Zhou.
Magface: A universal representation for face recognition and
quality assessment. In Proc. CVPR , pages 14225â€“14234,
2021. 26
[43] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep
multi-scale convolutional neural network for dynamic scene
deblurring. In Proc. CVPR , pages 3883â€“3891, 2017. 22,23,
28
[44] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In Proc. ICML ,
pages 8162â€“8171, 2021. 2,5,6,16
[45] Rui Qian, Robby T Tan, Wenhan Yang, Jiajun Su, and Jiay-
ing Liu. Attentive generative adversarial network for rain-
drop removal from a single image. In Proc. CVPR , pages
2482â€“2491, 2018. 5,7,17,22,23,28
[46] Yuhui Quan, Shijie Deng, Yixin Chen, and Hui Ji. Deep
learning for seeing through window with raindrops. In Proc.
ICCV , pages 2463â€“2471, 2019. 7
[47] K. F. Riley, M. P. Hobson, and S. J. Bence. Mathemati-
cal Methods for Physics and Engineering: A Comprehensive
Guide . Cambridge University Press, 3 edition, 2006. 2,6
[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj Â¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In Proc. CVPR , pages
10684â€“10695, 2022. 1,2,5,6
[49] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-
mans, David J Fleet, and Mohammad Norouzi. Image super-
resolution via iterative reï¬nement. IEEE TPAMI , 45(4):
4713â€“4726, 2022. 1,2,16,27,28[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In Proc. ICML , pages
2256â€“2265, 2015. 2
[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In Proc. ICLR , 2021. 1,2,5,
6,7,8,13,14,15,16,17,24
[52] Yang Song and Stefano Ermon. Generative modeling by es-
timating gradients of the data distribution. 32, 2019. 2,5
[53] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In Proc. ICLR , 2021. 2,15,16
[54] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. Consistency models. In Proc. ICML , 2023. 29
[55] Maitreya Suin, Kuldeep Purohit, and AN Rajagopalan.
Spatially-attentive patch-hierarchical network for adaptive
motion deblurring. In Proc. CVPR , pages 3606â€“3615, 2020.
23
[56] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,
Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxim:
Multi-axis mlp for image processing. In Proc. CVPR , pages
5769â€“5780, 2022. 2
[57] Jifeng Wang, Xiang Li, and Jian Yang. Stacked Condi-
tional Generative Adversarial Networks for Jointly Learning
Shadow Detection and Shadow Removal. In Proc. CVPR ,
pages 1788â€“1797, 2018. 5,7,8,17,18,19,20,26,27,28
[58] Tao Wang, Kaihao Zhang, Tianrun Shen, Wenhan Luo, Bjorn
Stenger, and Tong Lu. Ultra-high-deï¬nition low-light image
enhancement: A benchmark and transformer-based method.
InProc. AAAI , pages 2654â€“2662, 2023. 7,27
[59] Yufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-
Pui Chau, and Alex Kot. Low-light image enhancement with
normalizing ï¬‚ow. In Proc. AAAI , pages 2604â€“2612, 2022. 7,
27
[60] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang
Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general
u-shaped transformer for image restoration. In Proc. CVPR ,
pages 17683â€“17693, 2022. 23,24,28
[61] Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu.
Deep retinex decomposition for low-light enhancement. In
Proc. BMVC , 2018. 4,5,7,8,20,21,28
[62] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan
Saharia, Alexandros G Dimakis, and Peyman Milanfar. De-
blurring via stochastic reï¬nement. In Proc. CVPR , pages
16293â€“16303, 2022. 2,16,23
[63] Hugh R Wilson, Gunter Lofï¬‚er, and Frances Wilkinson.
Synthetic faces, face cubes, and the geometry of face space.
Vision research , 42(27):2909â€“2923, 2002. 26
[64] Jie Xiao, Xueyang Fu, Aiping Liu, Feng Wu, and Zheng-Jun
Zha. Image de-raining transformer. IEEE TPAMI , 2022. 7
[65] Ke Xu, Xin Yang, Baocai Yin, and Rynson WH Lau.
Learning to restore low-light images via decomposition-and-
enhancement. In Proc. CVPR , pages 2281â€“2290, 2020. 17,
21,23,28
[66] Xiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia.
Snr-aware low-light image enhancement. In Proc. CVPR ,
pages 17714â€“17724, 2022. 7,21,22
2782
[67] Zongsheng Yue, Jianyi Wang, and Chen Change Loy.
Resshift: Efï¬cient diffusion model for image super-
resolution by residual shifting. In Proc. NeurIPS , 2023. 4
[68] Wu Yuhui, Pan Chen, Wang Guoqing, Yang Yang, Wei Jiwei,
Li Chongyi, and Heng Tao Shen. Learning semantic-aware
knowledge guidance for low-light image enhancement. In
Proc. CVPR , 2023. 7,21
[69] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for real image restoration
and enhancement. In Proc. ECCV , pages 492â€“511, 2020. 21
[70] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Multi-stage progressive image restoration. In Proc.
CVPR , pages 14821â€“14831, 2021. 2,7,17,23
[71] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar
Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling
Shao. Learning enriched features for fast image restoration
and enhancement. IEEE TPAMI , 45(2):1934â€“1948, 2022. 21
[72] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and
Lei Zhang. Beyond a gaussian denoiser: Residual learning of
deep cnn for image denoising. IEEE TIP , 26(7):3142â€“3155,
2017. 2
[73] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In Proc. CVPR , pages 586â€“
595, 2018. 17,21
[74] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling
the darkness: A practical low-light image enhancer. In Proc.
ACMMM , pages 1632â€“1640, 2019. 7,17
[75] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and
Yun Fu. Residual dense network for image restoration. IEEE
TPAMI , 43(7):2480â€“2495, 2020. 2
[76] Yonghua Zhang, Xiaojie Guo, Jiayi Ma, Wei Liu, and Jiawan
Zhang. Beyond brightening low-light images. IJCV , 129:
1013â€“1037, 2021. 7,21
[77] Zhao Zhang, Huan Zheng, Richang Hong, Mingliang Xu,
Shuicheng Yan, and Meng Wang. Deep color consistent
network for low-light image enhancement. In Proc. CVPR ,
pages 1899â€“1908, 2022. 7
[78] Chuanjun Zheng, Daming Shi, and Wentian Shi. Adaptive
unfolding total variation network for low-light image en-
hancement. In Proc. ICCV , pages 4439â€“4448, 2021. 21
[79] Yurui Zhu, Jie Huang, Xueyang Fu, Feng Zhao, Qibin Sun,
and Zheng-Jun Zha. Bijective mapping network for shadow
removal. In Proc. CVPR , pages 5627â€“5636, 2022. 7,17,19,
20
[80] Yurui Zhu, Zeyu Xiao, Yanchi Fang, Xueyang Fu, Zhiwei
Xiong, and Zheng-Jun Zha. Efï¬cient model-driven network
for shadow removal. In Proc. AAAI , 2022. 7,17,19,21
[81] Zhengxia Zou, Sen Lei, Tianyang Shi, Zhenwei Shi, and
Jieping Ye. Deep adversarial decomposition: A uniï¬ed
framework for separating superimposed images. In Proc.
CVPR , pages 12806â€“12816, 2020. 2
[82] Ozan Â¨Ozdenizci and Robert Legenstein. Restoring vision in
adverse weather conditions with patch-based denoising dif-
fusion models. IEEE TPAMI , pages 1â€“12, 2023. 1,2,7,16,
23
2783
