Learning to Select Views for Efficient Multi-View Understanding
Yunzhong Hou, Stephen Gould, and Liang Zheng
Australian National University
{firstname.lastname }@anu.edu.au
Abstract
Multiple camera view (multi-view) setups have proven
useful in many computer vision applications. However,
the high computational cost associated with multiple views
creates a significant challenge for end devices with lim-
ited computational resources. In modern CPU, pipelining
breaks a longer job into steps and enables parallelism over
sequential steps from multiple jobs. Inspired by this, we
study selective view pipelining for efficient multi-view un-
derstanding, which breaks computation of multiple views
into steps, and only computes the most helpful views/steps
in a parallel manner for the best efficiency. To this end, we
use reinforcement learning to learn a very light view selec-
tion module that analyzes the target object or scenario from
initial views and selects the next-best-view for recognition
or detection for pipeline computation. Experimental results
on multi-view classification and detection tasks show that
our approach achieves promising performance while using
only 2 or 3 out of Navailable views, significantly reduc-
ing computational costs while maintaining parallelism over
GPU through selective view pipelining1.
1. Introduction
Multiple camera views (multi-view) are popular in com-
puter vision systems for their ability to address challenges
such as occlusions, ambiguities, and limited field-of-view
(FoV) coverage. Tasks like classification [30, 37] and de-
tection [6, 17] have shown significant benefits from using
multiple cameras. With reduced hardware cost and easy
deployment, real-world products now include more cam-
eras at larger scales. However, the use of multiple cam-
eras comes at a high computational cost, which can be
a significant challenge for end devices with limited com-
putational resources, especially with higher image resolu-
tions and deeper neural network backbones. Limiting im-
age resolution or using lighter networks [19, 27] are current
options to reduce computation, but they may impede the
1Code available at https://github.com/hou-yz/MVSelect .
step 1
step 1
step 1
step 1step 2
step 2
step 2
step 2step 3
step 3
step 3
step 3step 4
step 4
step 4
step 4
ğ‘¡ğ‘¡
ğ‘¡ğ‘¡
ğ‘¡ğ‘¡
(a) Piplining in modern CPU
step 1
step 1
step 1
step 1step 2
step 2
step 2
step 2step 3
step 3
step 3
step 3step 4
step 4
step 4
step 4
ğ‘¡ğ‘¡
ğ‘¡ğ‘¡
ğ‘¡ğ‘¡
(b) Parallel in modern GPU
step 1
step 1
step 1
step 1step 2
step 2
step 2
step 2step 3
step 3
step 3
step 3step 4
step 4
step 4
step 4
ğ‘¡ğ‘¡
ğ‘¡ğ‘¡
ğ‘¡ğ‘¡
â€¦
â€¦
â€¦
(c) Selective view pipelining
Figure 1. Demonstration of the proposed selective view pipelin-
ing. Inspired by instruction pipelining in modern CPU (a), se-
lective view pipelining (c) breaks the multi-view perception task
into sequential steps, and only computes the chosen views (see
Fig. 2) to minimize the computation. At the same time, computa-
tion across multiple objects (different border colors) can still enjoy
parallelism in GPU (b). In this example, for a N= 6 view sys-
tem, traditional computation (b) on GPU hardware that only sup-
ports two images at the same time will produce a throughput of
2 instances during 6 time steps. In contrast, by taking advantage
of the parallelism, selective view pipelining computes 6 instances
within 7 time steps.
progress in camera sensors or neural network architecture.
In modern CPU, instruction pipelining [10] breaks indi-
vidual instructions into steps to enable parallelism across
multiple instructions within the same CPU (see Fig. 1a).
For multi-view perception, although computations for dif-
ferent images can be paralleled over GPU [9], the level of
parallelism is still limited by hardware. As an example, the
GPU in Fig. 1b might only support parallel computing for
two images at a time, resulting in at least 3 time steps for
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20135
Q: guitar                or mandolin        ? A: guitar!initial view selected view
MVSelectğ‘ğ‘total views
Figure 2. Efficient multi-view understanding with two glances.
Instead of using all Nviews at once, a more efficient approach is
to first examine one view and then select another view to resolve
ambiguities from the initial glance. If the initial side view cannot
distinguish between a guitar and a mandolin (a round-shaped in-
strument that may also have a flat back), we can then query the
front view. This sequential process can work in a pipeline to en-
able parallelism between multiple
computing one object with 6 views.
In this work, we propose selective view pipelining,
which breaks the multi-view perception task into a sequence
of steps. Based on initial views, we estimate the most help-
ful views for this task, and only compute the selected views
for maximum efficiency. The example in Fig. 1c shows that
only selecting one additional view can greatly reduce the
computation from 6 views to 2 views. By further taking ad-
vantage of the parallelism in GPUs, we can roughly improve
the overall throughput by 3 Ã—.
To this end, this paper introduces a new angle to effi-
cient multi-view understanding by selecting only the most
useful views. To identify the best views, this approach
leverages camera layouts, which is a key aspect overlooked
by existing alternatives. With known camera layouts, net-
works should be able to infer what each camera view looks
like and then choose accordingly, as previous works [20]
have shown that networks can associate images with cam-
era poses. Existing work on active vision [1, 8, 12] indicates
that it is possible to find next-best-view (NBV) for recon-
struction with 3D sensors. In this paper, we focus on recog-
nition and detection problems using multiple RGB cameras,
where a side view may confuse a guitar with a mandolin
(Fig. 2). However, prior knowledge of the camera layout
should inform the system that the front view can clear the
ambiguities and should be queried.
To achieve this goal, this paper proposes a novel view
selection module, MVSelect, that chooses the best camera
views from any initial view at minimal computational costs.
Using the same feature extracted for the final task of classifi-
cation or detection, the proposed module analyzes the target
object or scenario from existing views and then selects the
max pooling
feature extraction multiple RGB views feature vectors  classification resultclassifierFigure 3. Multi-view classification with MVCNN [37].
next view that best helps the task (classification or detec-
tion) network. To navigate through the non-differentiable
view selection (the system only looks at selected cameras,
so not-selected views cannot back propagate gradients to
the controller), following Mnih et al. [26], we train the con-
troller via trial-and-error . This process is formulated as a
reinforcement learning problem aiming to maximize the fi-
nal recognition or detection accuracy.
On both multi-view classification and detection tasks,
our experimental results show that MVSelect can provide
a good strategy for fixed task networks, while also capa-
ble of joint training with the task network for further per-
formance improvements. Specifically, when joint training
both MVSelect and the task network, the resulting system
can achieve competitive performance to using all Ncam-
eras, while using only 2 views for classification tasks and 3
views for detection tasks, respectively.
The computational overhead of MVSelect is very small,
as it shares the feature extraction backbone with the task
network and only has a few learnable layers. For the en-
tire system, the computational cost is roughly proportional
to the number of views used, e.g., approximately 2/Nof the
total computation when 2 views are used, a significant effi-
ciency boost.
The MVSelect policy also enables study on multi-view
camera layout. In fact, we find that many of the Ncameras
are rarely chosen and can be shut off for further operational
cost improvements, which can serve as a starting point for
future study on multi-view camera layout optimization.
2. Background
Multi-view classification. One effective way for 3D shape
recognition is to capture the object in multiple camera
views. MVCNN [37] extracts feature vectors from the in-
put views, and then uses max pooling to aggregate across
multiple views for classification. Based on MVCNN, many
alternative approaches are proposed. Qi et al. [30] propose
sphere rendering at different volume resolutions. GVCNN
[11] investigates hierarchical information between different
20136
feature extraction
 multiple RGB viewsprojected 
feature maps  max pooling
pedestrian 
occupancy mapdetection headFigure 4. Multi-view detection with MVDet [17].
views by grouping the image features before the final ag-
gregation. RotationNet [20] introduces a multi-task objec-
tive by jointly considering classification and camera poses.
ViewGCN [42] uses a Graph Convolution Network (GCN)
[23] instead of the max pooling layer to aggregate across
views. Hamdi et al. [14] propose the MVTN network to
estimate the best viewpoints for 3D point cloud models.
Multi-view detection estimate pedestrian occupancy
from the birdâ€™s-eye-view (BEV). For this task, some meth-
ods [13, 33, 45] aggregate single-view detection results.
Others find single-view detection results unreliable and
instead aggregate the features. Hou et al. [17] intro-
duce MVDet, the first fully deep-learning approach, which
projects feature maps from each camera view to the BEV .
Based on MVDet, researchers develop other deep meth-
ods. SHOT [36] projects the image feature map at different
heights and stacks them together to improve performance.
MVDeTr [16] deals with distinct distortion patterns from
the projection. Qiu et al. [32] investigate data augmentation
with simulated occlusion over multiple views.
Camera viewpoint study. Cao and Snavely [5] investi-
gate this for reconstruction with structure-from-motion. Ac-
tive vision community investigates next-best-view planning
with 3D sensors like RGBD cameras or LiDAR for scene
reconstruction [4, 8, 35]. In multi-view classification, Rota-
tionNet [20] makes some pioneering work on limited view
numbers by taking a random partial set of all Nviews of the
image. MVTN [14] uses the 3D point cloud as initial input
and then estimates the best camera layout for multi-view
classification, but its moving camera assumption is hard to
meet in real-world systems. In multi-view detection, V ora
et al. [41] investigate camera layout generalization by ran-
domly dropping camera views from both training and test-
ing. Hou et al. [18] investigate how to place cameras. For
3D human pose estimation, Pirinen et al. [28] actively select
cameras over a dome.
Reinforcement learning (RL) directs an agent to inter-
act with the environment in a manner that maximizes cu-
mulative rewards. State sâˆˆ S, action aâˆˆ A, and reward
râˆˆ R are key concepts to model the interaction between
agent and environment. In a certain state s, policy Ï€(a|s)
records the probability for each action, and state value func-tionV(s)estimates the future rewards when following the
corresponding policy. To learn the best policy, Q-learning
and DQN [25] optimize the action value function Q(s, a),
which describes the estimated future return for a specific ac-
tionaat state s. Policy gradient methods like REINFORCE
[43] and PPO [34] directly optimize for the polity Ï€(a|s).
3. Multi-view Network Revisit
3.1. Multi-view Classification with MVCNN
MVCNN [37] (Fig. 3) is a classic architecture which many
multi-view classification networks build upon. Given Nin-
put images xn, nâˆˆ {1, . . . , N }, first, MVCNN uses its fea-
ture extractor f(Â·)to calculate the feature vectors,
hn=f(xn), (1)
where the feature vector hnâˆˆRDisD-dimensional. Sec-
ondly, it uses max pooling to aggregate multiple views into
an overall feature descriptor Ë†hâˆˆRD,
Ë†h= max
n{hn}, (2)
where max{Â·Â·Â·} takes the maximum along each of the D
dimensions. Lastly, it applies the output head g(Â·)to pro-
duce the classification result Ë†y,
Ë†y=g
Ë†h
. (3)
In training, the original design by Su et al. [37] adopts a
2-stage paradigm by first training on individual views and
then considering multiple views. In this paper, we skip the
first stage and directly train MVCNN on all Nviews,
LMVCNN =LCE(Ë†y,y), (4)
where LCE(Â·,Â·)denotes the cross-entropy loss and yde-
notes the ground truth one-hot label.
3.2. Multi-view Detection with MVDet
MVDet [17] (Fig. 4) is a multi-view detection architecture
that is followed by many recent works. To estimate human
occupancy in birdâ€™s-eye-view (BEV), given input images
xn, nâˆˆ {1, . . . , N }, MVDet first extracts D-channel fea-
ture maps for each view and uses perspective transforma-
tion to project the camera views to the BEV . Together, these
operations can be considered as the BEV feature extraction
step under Eq. 1, with the exception that hnâˆˆRDÃ—HÃ—W
now denotes the D-channel feature map for the BEV sce-
nario of shape HÃ—W. Secondly, for multi-view aggre-
gation, instead of the concatenation in the original design,
we choose element-wise max pooling in Eq. 2, producing
the overall feature description Ë†hâˆˆRDÃ—HÃ—Wthat fits arbi-
trary numbers of views. Lastly, we apply the output head as
20137
ğ‘“ğ‘“
ğ‘“ğ‘“
ğ‘“ğ‘“
ğ‘‘ğ‘‘ ğ‘‘ğ‘‘
ğ‘”ğ‘”ï¿½ğ’‰ğ’‰
ï¿½ğ’šğ’š
ğ‘ ğ‘ 1 ğ‘ ğ‘ 2 ğ‘ ğ‘ 3 ğ‘ğ‘2 ğ‘ğ‘3 ğ‘Ÿğ‘Ÿ3=MODAï¿½ğ’šğ’š,ğ’šğ’š ğ‘ğ‘1Figure 5. Efficient multi-view understanding with selective view pipelining using T= 3glances. Solid lines indicate the network forward
pass and dashed lines indicate the interaction between agent (MVSelect) and environment (multi-view system). The approach starts with
a random initial view a1, and the feature extractor f(Â·)computes its feature (Eq. 1). The state for this initial time step is recorded as s1
(Eq. 6). Next, the proposed MVSelect module d(Â·)is used to choose a second view a2, and the state is updated as s2. Then, by repeating
the last step, a third view a3is chosen and the state is updated as s3. Finally, three views are aggregated into an overall descriptor Ë†h(Eq. 2),
and the task network output Ë†yis calculated using the output head g(Â·)(Eq. 3). The final reward r3=MODA (Ë†y,y)is set as the accuracy
fir the task network (Eq. 7), and all other rewards are set as zero.
Eq. 3 to generate a heatmap Ë†yâˆˆ(0,1)HÃ—Wthat indicates
the likelihood of human occupancy in each BEV location.
The loss for MVDet can be written as,
LMVDet =LBEV(Ë†y,y) +1
NNX
n=1Ln, (5)
whereLBEV(Â·,Â·)denotes BEV output loss; yâˆˆ {0,1}HÃ—W
denotes binary ground truth map; and Lndenotes the aux-
iliary per-view loss with 2D bounding boxes.
4. Efficient Multi-view Understanding
In a multi-view system with Nviews, selective view
planning uses a total of T < N camera views atâˆˆ
{1, . . . , N }, tâˆˆ {1, . . . , T }for efficient understanding. To
achieve this, we propose a view selection module, MVSe-
lect, denoted as d(Â·), that sequentially selects camera views.
Starting from a random initial view a1, MVSelect chooses
the remaining Tâˆ’1cameras by observing the target object
or scene from existing views a1, . . . , a tat each time step t
and deciding which camera view at+1to select next for best
performance. Once Tcamera views have been gathered, we
aggregate them into an overall description Ë†husing Eq. 2,
and calculate the final output Ë†yusing Eq. 3. Fig. 5 gives an
overview of the proposed efficient multi-view approach.
4.1. Problem Formulation
While iteratively selecting camera views, it is impossible
to know what is in the not-selected camera views. If viewselection was formulated as a prediction task in a super-
vised manner, the loss (final target metric at t=T) could
not be back propagated to the not-selected views, making
itnon-differentiable and impossible to update the selection
probability. Reinforcement learning, on the other hand, can
update the controller even with the non-differentiable view
selection. Therefore, we formulate this non-differentiable
process as a reinforcement learning problem, where MVS-
elect is the agent and learns through trial and error , and the
multi-view system is the environment .
State. In order to get a Markovian representation, we
record the chosen camera views a1, . . . , a tand the observa-
tionsha1, . . . ,hatas state st. We use the extracted features
to represent the observations rather than the RGB camera
views to reduce dimensionality and maximize efficiency,
since these features will be used in the task network later
(Eq. 2). Mathematically, we formulate the state stas,
st=
scam
t, sobs
t
,
scam
t=tX
Ï„=1onehot ( aÏ„),
sobs
t=tmax
Ï„=1{haÏ„},(6)
where onehot ( Â·)is the one-hot function over Ncameras.
This representation reflects both chosen cameras scam
tâˆˆRN
and their observations sobs
tâˆˆRD, and maintains the same
dimensionality across different time steps. The observation
partsobs
talso matches the overall representation in Eq. 2.
Action. For state st, tâˆˆ {1, . . . , T âˆ’1}, MVSelect
takes the next camera view at+1as action.
20138
Algorithm 1 Joint training of MVSelect and task network.
1:input : camera views xn, nâˆˆ {1, . . . , N }, ground truth
y, random initial view a1, number of total views T, hyper-
parameter Ïµ.
2:update : feature extractor f(Â·)and output head g(Â·)of task
network, and MVSelect controller d(Â·).
3:initialize the total loss Ltotal= 0;
4:fortâˆˆ {1, . . . , T âˆ’1}do
5: select and apply the next action using Ïµ-greedy: with prob-
ability Ïµadopt a random action, or else choose the action
with highest value at+1= arg maxaQ(st, a);
6: observe next state st+1(Eq. 6) and reward rt+1(Eq. 7);
7: calculate the RL loss LRL(Eq. 9) and update the total loss
Ltotal=Ltotal+LRL;
8:end for
9:calculate the task loss Ltask(Eq. 4 or Eq. 5) and update the
total loss Ltotal=Ltotal+Ltask;
10:optimize for the total loss Ltotal.
Reward. Upon taking action at+1, the system receives
reward rt+1and transitions into the next state st+1. To
achieve high task network performance, we consider the
following as reward,
rt= 0, tâˆˆ {1, . . . , T âˆ’1},
rMVCNN
T = 1(Ë†y=y), rMVDet
T =MODA (Ë†y,y),(7)
where 1(Â·)denotes the binary indicator function,
MODA (Â·,Â·)is the evaluation metric for multi-view
detection [21]. We also experiment with other reward
designs in Section 5.4.
4.2. MVSelect Architecture
We design MVSelect architecture d(Â·)with two branches.
The first branch expands the camera selection result scam
tâˆˆ
RNintoD-dimensional learnable camera embeddings, and
then sums over the selected embeddings to formulate a hid-
den vector. The second branch converts the observation
sobs
tâˆˆRDinto another hidden vector. By combining
the two hidden vectors, the controller network outputs the
action-value Q(s, a), which measures the expected cumu-
lative rewards for taking an action ain a given state s. See
Supplementary Materials for figure illustrations.
During testing, MVSelect outputs the next action as,
at+1= arg max
aQ(st, a),
which maximizes the expected cumulative rewards.
4.3. Training Scheme
We adopt Q-learning [38] for training MVSelect. Specifi-
cally, action-value function Q(Â·,Â·)should estimate the cu-mulative future rewards after taking action at+1at state st,
Q(st, at+1) =E TX
Ï„=t+1Î³Ï„âˆ’tâˆ’1rÏ„!
,
where E(Â·)denotes the expectation, and Î³âˆˆ[0,1]denotes
the discount factor. We take the temporal difference (TD)
[38] target as supervision for the action value,
qt=(
rt+1+Î³max aQ(st+1, a),ift < Tâˆ’1
rT, otherwise,(8)
and calculate the loss using the L2distance,
LRL=Tâˆ’1X
t=1LMSE(Q(st, at+1), qt), (9)
where the next action at+1is chosen using Ïµ-greedy for
exploration-exploitation trade offs.
In joint training, the task network takes supervision from
Ltask(Eq. 4 and Eq. 5), and the selection module takes su-
pervision from LRL(Eq. 9). A step-by-step demonstration
of this process can be found in Algorithm 1.
5. Experiments
5.1. Experiment Settings
Datasets. For multi-view classification, we use syn-
thetic dataset ModelNet40 [44] under two different cam-
era layouts (12 views and 20 views) and real-world dataset
ScanObjectNN (12 views) [39]. For multi-view detection,
we use real-world dataset Wildtrack (7 views) [6] and syn-
thetic dataset MultiviewX (6 views) [17].
Evaluation metrics. For multi-view classification,
we report instance-averaged accuracy. Regarding multi-
view detection, we report multi-object detection accuracy
(MODA), which is calculated as 1âˆ’FP+FN
GT[21]. All met-
rics are reported in percentages.
Implementation details. For multi-view classification,
we input images of size 224Ã—224to the MVCNN model.
For multi-view detection, we use a resolution of 720Ã—1280
with view-coherent data augmentation [16], and downsam-
ple the BEV grid by a factor of 4. In terms of architecture,
we use ResNet-18 [15] as feature extractor f(Â·).
We train all networks for 10 epochs using the Adam opti-
mizer [22]. We use learning rates of 5Ã—10âˆ’5and5Ã—10âˆ’4,
with batch sizes of 8 and 1 for MVCNN and MVDet, re-
spectively. The MVSelect module is trained using a learn-
ing rate of 1Ã—10âˆ’4. For joint training, we decrease the
learning rate for the task network to 1/5of its original value.
20139
Table 1. Performance comparison with state-of-the-art methods. Results are averaged from 5 runs. * indicates that the camera poses are
dynamically chosen and do not follow a pre-defined layout.â€ indicates our implementation.
view selection ( T= 2)ModelNet40ScanObjNN12 views 20 views
MVCNN [37] N/A 90.1 92.0 -
GVCNN [11] N/A 92.6 - -
MHBN [47] N/A 93.4 - -
RotationNet [20] N/A - 94.7 -
RelationNet [46] N/A 94.3 97.3 -
ViewGCN [42] N/A - 97.6 -
MVCNNâ€ N/A 94.5 96.5 86.1
PointNet [31] + ViewGCN MVTN* ( T= 12 or20) [14] 93.8 93.5 92.6
MVCNNâ€ SEE++â€ [3] 79.2 70.3 75.5
MVCNNâ€ Potthast and Sukhatmeâ€ [29] 76.5 66.2 72.1
MVCNNâ€ MVSelect 88.2 79.6 80.0
MVCNNâ€ (joint training) MVSelect 94.3 94.4 84.1view selection ( T= 3) Wildtrack MultiviewX
RCNN & cluster [45] N/A 11.3 18.7
POM-CNN [13] N/A 23.2 -
DeepMCD [7] N/A 67.8 70
Deep-Occlusion [2] N/A 74.1 75.2
MVDet [17] N/A 88.2 83.9
SHOT [36] N/A 90.2 88.3
MVDeTr [16] N/A 91.5 93.7
MVDetâ€ N/A 90.0 93.0
MVDetâ€ SEE++â€ [3] 76.1 77.8
MVDetâ€ Potthast and Sukhatmeâ€ [29] 78.3 78.0
MVDetâ€ max FoVâ€ [35] 78.0 73.9
MVDetâ€ MVSelect 80.0 78.7
MVDetâ€ (joint training) MVSelect 88.6 88.1
Figure 6. Example of selected views on ModelNet40 (left) and Wildtrack (right).
Regarding hyperparameters, we set the future reward dis-
count factor Î³= 0.99, and the exploration ratio Ïµto grad-
ually decrease from 0.95to0.05during training. All ex-
periments are conducted on a single RTX-3090 GPU and
averaged across 5 repetitive runs.
Experimental setups. First, we fix the task net-
work. We compare with state-of-the-art methods including
1) NBV policy with depth-sensing enabled point-based oc-
clusion estimation [3], 2) NBV policy with depth-sensing
enabled voxel-based probabilistic representation [29], and
3) NBV policy to maximize field-of-view (FoV) coverage
[35] (only applicable to multi-view detection). We also re-
port two baselines: 1) random selection, 2) the best policy
on the validation set; and two oracles: for a certain initial
view, 1) choosing the overall best-performing camera for all
instances in the dataset ( dataset-level oracle ) and 2) choos-
ing specifically for each instance ( instance-level oracle ).
For the two oracles, the former reflects the upper bound
of performance achievable by human-designed heuristics,
and the latter represents the theoretical performance ceiling
when keeping the task network fixed.
Second, we jointly train the proposed view selection
module with the task networks. For this experiment, our
goal is to achieve the highest possible performance using T
views. If not specified, we use a total of T= 2 views for
multi-view classification and T= 3views for detection.
5.2. Evaluation of MVSelect
Formulti-view classification , as in Table 1, MVSelect with
fixed multi-view classification network achieves competi-tive performance compared to other state-of-the-art view
selection methods. When compared with baselines and or-
acles in Table 2, the proposed view selection module, can
choose the views effectively. Compared to dataset-level ora-
cles (same policy for all instances with the same initial cam-
era), MVSelect also turns out to be advantageous by 3.0%,
9.7%, and 1.5% across the two settings. This verifies that
MVSelect can take the target object into consideration (see
Fig. 2) and select different cameras for different instances
under the same initial camera.
When joint training MVCNN with MVSelect, we wit-
ness large improvements. In fact, on two settings, the results
are only 0.2%, 2.1%, and 2.0% behind compared to the full
N-view system. Overall, we believe that MVSelect and its
joint training capabilities enable us to consider only 2 views
without major performance drawbacks. We demonstrate an
example of the selected views in Fig. 6, and the MVSelect
policy in Fig. 7.
Formulti-view detection , MVSelect achieves competi-
tive performance compared to existing NBV methods, some
of them even require additional depth sensing (Table 1). Al-
though the raw improvements are not as substantial as those
in multi-view classification, they are statistically highly
significant (p-value <0.001). This shows human designed
heuristics like minimal occlusion or maximum FoV cover-
age cannot guarantee the best detection results.
When compared to baselines and oracles, MVSelect also
shows to be competitive (see Table 2). Specifically, we ob-
serve that the instance-level oracle remains relatively low
compared to that of multi-view classification tasks. This is
20140
Table 2. Comparison with view selection baselines and oracles on multi-view classification and detection datasets.
view selection ( T= 2)ModelNet40ScanObjNN12 views 20 views
N/A: all Nviews 94.5 96.5 86.1
dataset-lvl oracle 85.2Â±1.4 69.9Â±1.9 78.5Â±1.2
instance-lvl oracle 96.5Â±0.4 98.1Â±1.9 93.4Â±0.6
random selection 71.5Â±2.5 48.1Â±3.1 74.2Â±3.5
validation best policy 85.1Â±0.8 69.5Â±1.0 77.5Â±1.1
MVSelect 88.2Â±0.4 79.6Â±1.8 80.0Â±0.8
MVSelect + joint training 94.3Â±0.2 94.4Â±0.2 84.1Â±0.2view selection ( T= 3) Wildtrack MultiviewX
N/A: all Nviews 90.0 93.0
dataset-lvl oracle 82.5Â±0.4 80.2Â±0.3
instance-lvl oracle 87.4Â±0.4 82.3Â±0.8
random selection 74.9Â±1.3 76.2Â±1.4
validation best policy 79.5Â±1.1 78.0Â±0.4
MVSelect 80.0Â±0.8 78.7Â±0.5
MVSelect + joint training 88.6Â±0.2 88.1Â±0.2
(a) Classification accuracy
 (b) MVSelect policy
Figure 7. Multi-view classification with T= 2 views on the 12-view setup of
ModelNet40. The task network is fixed once trained. Left: test set accuracy of
using two views. Right : MVSelect policy for the test set.Table 3. Computation efficiency.
viewsFLOPs throughput
f(Â·)g(Â·)d(Â·)(instance/s)
ModelNet4020 36.5G 20.5k N/A 119.6
2 3.6G 20.5k 1.1M 361.8
12 21.9G 20.5k N/A 196.2
2 3.6G 20.5k 530.4k 507.9
Wildtrack7 1.2T 19.1G N/A 8.4
3 511.6G 19.1G 3.2G 16.2
MultiviewX6 1.0T 17.7G N/A 9.8
3 511.6G 17.7G 2.9G 16.9
Table 4. Ablation study.
ModelNet40Wildtrack12 views
MVSelect 88.2 80.0
w/o camera branch 88.2 79.1
w/o feature branch 85.0 79.7
w/ transformer 87.0 78.7
reward= âˆ†task loss 88.0 80.1
likely due to the target scenarios not being fully captured
by any single view, and the multi-view detection network
needs multiple views to collaborate with each other for op-
timal results. Compared to the dataset-level oracles, we find
the MVSelect policy lose its edge. In fact, we find that
for multi-view detection, MVSelect tends to select the same
camera for a given initial view, since it is notaware of the
situation outside of the FoV coverage. As a result, it can-
not choose cameras based on uncovered areas in different
frames. Without this instance-aware advantage, the fixed
camera policy learned during training (MVSelect) cannot
outperform the dataset-level oracle, whose policy is com-
puted on the test set.
Joint training with MVDet once again leads to substan-
tial performance improvements over keeping the task net-
work fixed. In fact, the results even exceed the instance-
level oracle for fixed task networks. Using T= 3 views,
the joint training approach provides competitive results to
using all Nviews, and exceeds the reported performance in
the original MVDet paper [17]. We present an example of
the selected views in the Wildtrack dataset in Fig. 6.5.3. Efficiency Analysis
In Table 3, following previous works in efficient inference
[19, 24], we detail the computational cost in FLOPs for task
networks and MVSelect. Specifically, we find feature ex-
traction f(Â·)to take up the majority of the computation,
while everything else is lighter by at least an order of mag-
nitude. Overall, we verify that using T= 2 or 3 out of N
views can reduce the computational cost to roughly T/N.
In terms of inference speed, we find reduction in FLOPs
results in monotonically increasing throughputs, ranging
from 1.72Ã—to3.03Ã—. Due to factors such as imple-
mentation, parallelization, and hardware limitations, actual
speedups cannot actually reach the level of computational
cost reduction, as suggested by previous study [27].
5.4. Variant Study
Ablation study. Regarding the MVSelect architecture de-
sign, Table 4 shows that removing the camera branch and
feature branch primarily affects multi-view detection and
multi-view classification performance, respectively. This
aligns with the policy we learned for the two tasks. For
multi-view detection, since the system has no clue about ar-
20141
406080100
2 3 4 5
number of total views TModelNet40 (20 views)
all N views
random selection
dataset-lvl oracle
instance-lvl oracle
MVSelect only
joint training
5565758595
2 3 4 5
number of total views TWildtrack
all N views
random selection
dataset-lvl oracle
instance-lvl oracle
MVSelect only
joint trainingFigure 9. System performance under different number of total views T. We report
classification accuracy and MODA for the two tasks, respectively (same for Fig. 10
and Table. 4). Circle and triangle markers indicate whether the task network is fixed
or not, respectively. Dotted lines represent oracle performance (not achievable).
94.594.0 94.094.5
71.570.671.5 71.585.2
81.685.1
73.096.2
93.195.7 96.2
88.2
84.988.1
79.494.393.7 93.993.4
708090100
default random
shut-offpolicy
shut-offrandom
pose*ModelNet40 (12 views)
all N views random selection
dataset-lvl oracle instance-lvl oracle
MVSelect only joint trainingFigure 10. System evaluation under different
settings. Except for the random pose* setting
(where MVSelect variants are re-trained), all
models are trained on the default setting.
eas outside camera FoVs, the camera branch plays a more
important role for encoding prior knowledge of the scene
layout. For multi-view classification, however, target ob-
jects are fully observable, and MVSelect can make differ-
ent decisions for each instance. Thus, the feature branch is
more important, as it enables per-instance decision making.
When changing the MVSelect network architecture into
transformer [40], we find that more parameters do not trans-
late into better performance.
Another option for the reward is the change in task loss.
In our variant study, this reward design does not show any
significant advantages over the current design.
Influence of total view count. The performance curves
in Fig. 9 demonstrate that the joint training variant achieves
competitive performance with as few as T= 2 views for
multi-view classification and T= 3 views for multi-view
detection, beyond which point performance plateaus. By
contrast, learning MVSelect for fixed task networks shows a
less steep curve. The performance increases up to a total of
T= 5views, at which point the MVSelect policy performs
comparably to the full N-view system.
Camera layout optimization. Determining the optimal
camera locations is crucial for setting up an effective multi-
view system. In Fig.7, we observe that not all cameras are
equally useful according to the MVSelect policy. To address
this, we allocate a validation partition of the data to identify
the more useful camera views and then disable half of the
N= 12 cameras that are not frequently utilized. In testing,
we find that some cameras are not as useful (â€œpolicy shut-
offâ€ in Fig. 10) and the proposed method can generalize
to test-time disturbances including malfunctioning cameras
(â€œrandom shut-offâ€ in Fig.10). Although it is necessary to
set up all Ncameras for the analysis, optimizing the multi-
view camera layout can be a crucial step towards achieving
optimal performance, and merits further investigation.Random object pose. In real-world applications of
multi-view systems, such as those found in iPhones and
Teslas, the cameras may remain fixed in their relative po-
sitions while the entire system is in motion. To simulate
this scenario in our experiments, we introduce the random
object pose setting (Fig. 10) and re-train MVSelect. While
there is no exact object pose as supervision, the reinforce-
ment learning approach is able to roughly infer the relative
poses between the object and the multi-view system, result-
ing in improved performance compared to random selection
and dataset-level oracle (which is arguably inappropriate for
this setup). In the future, we plan to estimate camera poses
with respect to the object or the environment as an auxiliary
supervision for moving setups.
6. Conclusion
In conclusion, this paper proposes selective view pipelining,
an efficient approach for multi-view understanding by lim-
iting the number of views. To this end, a camera view selec-
tion module, MVSelect, is proposed along with a reinforce-
ment learning based training scheme that can learn from the
non-differentiable selection process. When jointly trained
with the task network, the proposed approach demonstrates
competitive performance on multi-view classification and
detection tasks at fractions of the computational cost. Over-
all, the proposed efficient approach provides an alternative
to reducing image resolution and using lighter networks,
and paves ways for future multi-view camera layout opti-
mization.
Acknowledgement
This work was supported by the ARC Discovery Project
(DP210102801).
20142
References
[1] John Aloimonos, Isaac Weiss, and Amit Bandyopadhyay.
Active vision. International journal of computer vision , 1:
333â€“356, 1988. 2
[2] Pierre Baqu Â´e, Franc Â¸ois Fleuret, and Pascal Fua. Deep oc-
clusion reasoning for multi-camera multi-target detection. In
Proceedings of the IEEE International Conference on Com-
puter Vision , pages 271â€“279, 2017. 6
[3] Rowan Border and Jonathan D Gammell. Proactive estima-
tion of occlusions and scene coverage for planning next best
views in an unstructured representation. In 2020 IEEE/RSJ
International Conference on Intelligent Robots and Systems
(IROS) , pages 4219â€“4226. IEEE, 2020. 6
[4] Rowan Border, Jonathan D Gammell, and Paul Newman.
Surface edge explorer (see): Planning next best views di-
rectly from 3d observations. In 2018 IEEE International
Conference on Robotics and Automation (ICRA) , pages
6116â€“6123. IEEE, 2018. 3
[5] Song Cao and Noah Snavely. Minimal scene descriptions
from structure from motion models. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recogni-
tion, pages 461â€“468, 2014. 3
[6] Tatjana Chavdarova, Pierre Baqu Â´e, St Â´ephane Bouquet, An-
drii Maksai, Cijo Jose, Timur Bagautdinov, Louis Lettry,
Pascal Fua, Luc Van Gool, and Franc Â¸ois Fleuret. Wild-
track: A multi-camera hd dataset for dense unscripted pedes-
trian detection. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition , pages 5030â€“
5039, 2018. 1, 5
[7] Tatjana Chavdarova et al. Deep multi-camera people detec-
tion. In 2017 16th IEEE International Conference on Ma-
chine Learning and Applications (ICMLA) , pages 848â€“853.
IEEE, 2017. 6
[8] Shengyong Chen, Youfu Li, and Ngai Ming Kwok. Ac-
tive vision in robotic systems: A survey of recent develop-
ments. The International Journal of Robotics Research , 30
(11):1343â€“1377, 2011. 2, 3
[9] Wikipedia Contributors. Graphics processing unit, 2019. 1
[10] Wikipedia Contributors. Instruction pipelining, 2020. 1
[11] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and
Yue Gao. Gvcnn: Group-view convolutional neural networks
for 3d shape recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
264â€“272, 2018. 2, 6
[12] John M Findlay and Iain D Gilchrist. Active vision: The
psychology of looking and seeing . Number 37. Oxford Uni-
versity Press, 2003. 2
[13] Francois Fleuret, Jerome Berclaz, Richard Lengagne, and
Pascal Fua. Multicamera people tracking with a probabilistic
occupancy map. IEEE transactions on pattern analysis and
machine intelligence , 30(2):267â€“282, 2007. 3, 6
[14] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem.
Mvtn: Multi-view transformation network for 3d shape
recognition. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 1â€“11, 2021. 3, 6
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-ings of the IEEE conference on computer vision and pattern
recognition , pages 770â€“778, 2016. 5
[16] Yunzhong Hou and Liang Zheng. Multiview detection with
shadow transformer (and view-coherent data augmentation).
InProceedings of the 29th ACM International Conference on
Multimedia , pages 1673â€“1682, 2021. 3, 5, 6
[17] Yunzhong Hou, Liang Zheng, and Stephen Gould. Multiview
detection with feature perspective transformation. In Euro-
pean Conference on Computer Vision , pages 1â€“18. Springer,
2020. 1, 3, 5, 6, 7
[18] Yunzhong Hou, Xingjian Leng, Tom Gedeon, and Liang
Zheng. Optimizing camera configurations for multi-view
pedestrian detection. arXiv preprint arXiv:2312.02144 ,
2023. 3
[19] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017. 1, 7
[20] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi
Nishida. Rotationnet: Joint object categorization and pose
estimation using multiviews from unsupervised viewpoints.
InProceedings of the IEEE conference on computer vision
and pattern recognition , pages 5010â€“5019, 2018. 2, 3, 6
[21] Rangachar Kasturi, Dmitry Goldgof, Padmanabhan
Soundararajan, Vasant Manohar, John Garofolo, Rachel
Bowers, Matthew Boonstra, Valentina Korzhova, and Jing
Zhang. Framework for performance evaluation of face, text,
and vehicle detection and tracking in video: Data, metrics,
and protocol. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 31(2):319â€“336, 2008. 5
[22] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR (Poster) , 2015. 5
[23] Thomas N Kipf and Max Welling. Semi-supervised clas-
sification with graph convolutional networks. In J. In-
ternational Conference on Learning Representations (ICLR
2017) , 2016. 3
[24] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and
Hans Peter Graf. Pruning filters for efficient convnets. In In-
ternational Conference on Learning Representations , 2017.
7
[25] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex
Graves, Ioannis Antonoglou, Daan Wierstra, and Martin
Riedmiller. Playing atari with deep reinforcement learning.
arXiv preprint arXiv:1312.5602 , 2013. 3
[26] V olodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recur-
rent models of visual attention. Advances in neural informa-
tion processing systems , 27, 2014. 2
[27] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
and Jan Kautz. Pruning convolutional neural networks for re-
source efficient inference. arXiv preprint arXiv:1611.06440 ,
2016. 1, 7
[28] Aleksis Pirinen, Erik G Â¨artner, and Cristian Sminchisescu.
Domes to drones: Self-supervised active triangulation for 3d
human pose reconstruction. Advances in Neural Information
Processing Systems , 32, 2019. 3
[29] Christian Potthast and Gaurav S Sukhatme. A probabilis-
tic framework for next best view estimation in a cluttered
20143
environment. Journal of Visual Communication and Image
Representation , 25(1):148â€“164, 2014. 6
[30] Charles R Qi, Hao Su, Matthias NieÃŸner, Angela Dai,
Mengyuan Yan, and Leonidas J Guibas. V olumetric and
multi-view cnns for object classification on 3d data. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 5648â€“5656, 2016. 1, 2
[31] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652â€“660,
2017. 6
[32] Rui Qiu, Ming Xu, Yuyao Yan, Jeremy S Smith, and Xi
Yang. 3d random occlusion and multi-layer projection for
deep multi-camera pedestrian localization. arXiv preprint
arXiv:2207.10895 , 2022. 3
[33] Gemma Roig, Xavier Boix, Horesh Ben Shitrit, and Pascal
Fua. Conditional random fields for multi-camera object de-
tection. In 2011 International Conference on Computer Vi-
sion, pages 563â€“570. IEEE, 2011. 3
[34] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347 , 2017. 3
[35] William R Scott, Gerhard Roth, and Jean-Franc Â¸ois Rivest.
View planning for automated three-dimensional object re-
construction and inspection. ACM Computing Surveys
(CSUR) , 35(1):64â€“96, 2003. 3, 6
[36] Liangchen Song, Jialian Wu, Ming Yang, Qian Zhang, Yuan
Li, and Junsong Yuan. Stacked homography transformations
for multi-view pedestrian detection. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 6049â€“6057, 2021. 3, 6
[37] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik
Learned-Miller. Multi-view convolutional neural networks
for 3d shape recognition. In Proceedings of the IEEE in-
ternational conference on computer vision , pages 945â€“953,
2015. 1, 2, 3, 6
[38] Richard S Sutton and Andrew G Barto. Reinforcement learn-
ing: An introduction . MIT press, 2018. 5
[39] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua,
Duc Thanh Nguyen, and Sai-Kit Yeung. Revisiting point
cloud classification: A new benchmark dataset and classifi-
cation model on real-world data. In International Conference
on Computer Vision (ICCV) , 2019. 5
[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Å ukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neu-
ral Information Processing Systems . Curran Associates, Inc.,
2017. 8
[41] Jeet V ora, Swetanjal Dutta, Kanishk Jain, Shyamgopal
Karthik, and Vineet Gandhi. Bringing generalization to deep
multi-view detection. arXiv preprint arXiv:2109.12227 ,
2021. 3
[42] Xin Wei, Ruixuan Yu, and Jian Sun. View-gcn: View-based
graph convolutional network for 3d shape analysis. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 1850â€“1859, 2020. 3, 6[43] Ronald J Williams. Simple statistical gradient-following al-
gorithms for connectionist reinforcement learning. Rein-
forcement learning , pages 5â€“32, 1992. 3
[44] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-
guang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
shapenets: A deep representation for volumetric shapes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 1912â€“1920, 2015. 5
[45] Yuanlu Xu, Xiaobai Liu, Yang Liu, and Song-Chun Zhu.
Multi-view people tracking via hierarchical trajectory com-
position. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 4256â€“4265,
2016. 3, 6
[46] Ze Yang and Liwei Wang. Learning relationships for multi-
view 3d object recognition. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 7505â€“
7514, 2019. 6
[47] Tan Yu, Jingjing Meng, and Junsong Yuan. Multi-view har-
monized bilinear network for 3d object recognition. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 186â€“194, 2018. 6
20144
