Adapting Visual-Language Models for
Generalizable Anomaly Detection in Medical Images
Chaoqin Huang1,2,3 *, Aofan Jiang1,3‚àó, Jinghao Feng1,3, Ya Zhang1,3, Xinchao Wang2,‚Ä†, Yanfeng Wang1,3,‚Ä†
1Shanghai Jiao Tong University,2National University of Singapore
3Shanghai Artificial Intelligence Laboratory
{huangchaoqin,stillunnamed,fjh1345528968,ya zhang,wangyanfeng622 }@sjtu.edu.cn; {xinchao }@nus.edu.sg
Abstract
Recent advancements in large-scale visual-language
pre-trained models have led to significant progress in zero-
/few-shot anomaly detection within natural image domains.
However, the substantial domain divergence between nat-
ural and medical images limits the effectiveness of these
methodologies in medical anomaly detection. This paper
introduces a novel lightweight multi-level adaptation and
comparison framework to repurpose the CLIP model for
medical anomaly detection. Our approach integrates mul-
tiple residual adapters into the pre-trained visual encoder,
enabling a stepwise enhancement of visual features across
different levels. This multi-level adaptation is guided by
multi-level, pixel-wise visual-language feature alignment
loss functions, which recalibrate the model‚Äôs focus from ob-
ject semantics in natural imagery to anomaly identification
in medical images. The adapted features exhibit improved
generalization across various medical data types, even in
zero-shot scenarios where the model encounters unseen
medical modalities and anatomical regions during train-
ing. Our experiments on medical anomaly detection bench-
marks demonstrate that our method significantly surpasses
current state-of-the-art models, with an average AUC im-
provement of 6.24% and 7.33% for anomaly classification,
2.03% and 2.37% for anomaly segmentation, under the
zero-shot and few-shot settings, respectively. Source code is
available at: https://github.com/MediaBrain-
SJTU/MVFA-AD
1. Introduction
Medical anomaly detection (AD), which focuses on identi-
fying unusual patterns in medical data, is central to prevent-
ing misdiagnoses and facilitating early interventions [14,
*Equal Contribution
‚Ä†Corresponding authors: Yanfeng Wang and Xinchao Wang
MedicalImage DomainRetinalOCTChestX-rayBrainMRI
Histopathology
Natural ImageDomain
LiverCTMulti-Level Feature Adaptation
CLIPPredictionPre-trained VLMTest: ComparisonUnseen Modality
A photo of a {dog}.A photo of a {cat}.A photo of a {car}.
‚úî
normal liverabnormal liverZero-ShotFew-ShotText PromptsReference ImagesACASTrain: Adaptation
SeenUnseenLJDomainGapTaskGapUnseenAnatomical RegionscomparecompareGeneralizableMedicalADFigure 1. The overview of adaptation in pre-trained visual-
language models for zero-/few-shot medical anomaly classifica-
tion (AC) and anomaly segmentation (AS).
47, 61]. The vast variability in medical images, both in
terms of modalities and anatomical regions, necessitates a
model that is versatile across various data types. The few-
shot AD approaches [12, 22, 46, 57] strive to attain model
generalization with scarce training data, embodying a pre-
liminary attempt for a universal AD model, despite the need
for lightweight re-training [12, 46, 57] or distribution ad-
justment [22] for each new AD task.
Contemporary large-scale pre-trained visual-language
models (VLMs) have recently paved the way for robust and
generalizable anomaly detection. A notable initial effort is
to directly adopt CLIP [38], a representative open-source
VLM for natural imagery, for AD, simply by carefully craft-
ing artificial text prompts [26]. By further employing an-
notated training data, Chen et al. [9] introduces extra lin-
ear layers to map the image features to the joint embedding
space to the text features, facilitating their comparison. De-
spite the promise of the above two approaches, their exten-
sion to the medical domain has not been explored.
This paper attempts to develop a universal generalizable
AD model for medical images, designed to be adaptable to
previously unseen modalities and anatomical regions. The
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
11375
creation of such a model holds significant practical impor-
tance, but tailoring the CLIP model for this purpose presents
a triad of challenges. Firstly, re-purposing CLIP for AD
signifies a substantial shift in task requirements. The visual
encoder in CLIP is known to primarily capture image se-
mantics, yet a universal AD model must discern irregular-
ities across diverse semantic contexts. Secondly, the tran-
sition from using CLIP in the realm of natural imagery to
medical imagery constitutes a significant domain shift. Fi-
nally, the task of extending the AD model‚Äôs applicability to
unencountered imaging modalities and anatomical regions
during the training phase is notably demanding.
This paper proposes a lightweight Multi-level Adaptation
and Comparison framework to re-purpose CLIP for AD in
medical images as shown in Figure 1. A multi-level visual
feature adaptation architecture is designed to align CLIP‚Äôs
features to the requirements of AD in medical contexts. The
process of visual feature adaptation merges adapter tuning
with multi-level considerations. This is achieved by inte-
grating multiple residual adapters into the pre-trained vi-
sual encoder. This stepwise enhancement of visual features
across different levels is guided by multi-level, pixel-wise
visual-language feature alignment loss functions. These
adapters recalibrate the model‚Äôs focus, shifting it from ob-
ject semantics to identifying anomalies in images, utilizing
text prompts that broadly categorize images as ‚Äònormal‚Äô or
‚Äòanomalous‚Äô. During testing, comparison is performed be-
tween the adapted visual features and text prompt features,
and additional referenced image features if available, en-
abling the generation of multi-level anomaly score maps.
The methods are evaluated on a challenging medical
AD benchmark, encompassing datasets from five distinct
medical modalities and anatomical regions: brain MRI
[1, 2, 35], liver CT [7, 29], retinal OCT [21, 28], chest X-
ray [51], and digital histopathology [4]. Our method outper-
forms state-of-the-art approaches, showcasing an average
improvement of 6.24% and 7.33% in anomaly classifica-
tion, and 2.03% and 2.37% in anomaly segmentation under
zero-shot and few-shot scenarios, respectively.
The main contributions are summarized below:
‚Ä¢ A novel multi-level feature adaptation framework is pro-
posed, which is, to the best of our knowledge, the first
attempt to adapt pre-trained visual-language models for
medical AD in zero-/few-shot scenarios.
‚Ä¢ Extensive experiments on a challenging benchmark for
AD in medical images have demonstrated its excep-
tional generalizability across diverse data modalities and
anatomical regions.
2. Related Works
Vanilla Anomaly Detection. Given the limited availabil-
ity and high cost of abnormal images, a portion of current
research on AD focuses on unsupervised methods relyingexclusively on normal images [6, 10, 17, 19, 31, 41, 43, 44,
50, 54, 58]. Approaches such as PatchCore [41] create a
memory bank of normal embeddings and detect anomalies
based on the distance from a test sample to the nearest nor-
mal embedding. Another method, CflowAD [19], projects
normal samples onto a Gaussian distribution using normal-
izing flows. However, relying solely on normal samples
can result in an ambiguous decision boundary and reduced
discriminability [5]. In practical scenarios, a small number
of anomaly samples are usually available, and these can be
used to enhance detection effectiveness.
Zero-/Few-Shot Anomaly Detection. The utilization of
a few known anomalies during training can present chal-
lenges, potentially biasing the model and hindering gener-
alization to unseen anomalies. DRA [12] and BGAD [57]
introduce methods to mitigate this issue. Beyond simply
maximizing the separation of abnormal features from nor-
mal patterns [37, 42], DRA [12] learns disentangled rep-
resentations of anomalies to enable generalizable detec-
tion, accounting for unseen anomalies. BGAD [57] pro-
poses a boundary-guided semi-push-pull contrastive learn-
ing mechanism to further alleviate the bias issue. Recent
advancements like WinCLIP [26] explore the use of foun-
dation models for zero-/few-shot AD, leveraging language
to assist in AD. Building upon WinCLIP, April-GAN [9]
maps visual features extracted from CLIP to the linear space
where the text features are located, supervised by pixel-level
annotated data. This paper concentrates on medical AD, a
more challenging area than traditional industrial AD due to
the larger gap between different data modalities.
Medical Anomaly Detection. Current medical AD meth-
ods typically treat AD as a one-class classification issue,
relying on normal images for training [3, 8, 27, 61, 64‚Äì
66]. These methods, which identify anomalies as devi-
ations from the normal distribution, often require a large
number of normal samples per class, making them imprac-
tical for real-world diagnosis. Many of these techniques
are designed for a particular anatomical region [13, 55]
or are restricted to handling one specific data type per
model [24, 30, 53]. These methods often fall short in terms
of generalizing across diverse data modalities and anatomi-
cal regions [62], a pivotal aspect our paper aims to address.
Visual-Language Modeling. Recently, VLMs have wit-
nessed substantial advancements, applied to many different
scenarios [20, 32, 34, 56, 59, 60]. Trained on a vast amount
of image-text data, CLIP [38] excels in generalizability and
robustness, notably enabling language-driven zero-shot in-
ference [16, 48]. To broaden the application of VLMs, re-
sources like the extensive LAION-5B dataset [45] and the
OpenCLIP codebase [25] have been made available openly.
Subsequent research has underscored CLIP‚Äôs potential for
zero-/few-shot transfer to downstream tasks beyond mere
classification [18, 40, 67]. Several studies [23, 39, 63] have
11376
TextEncoderZero-Shot BranchL‚àí,+!√ó#
‚Ñ±$%&$ASLabelsalignment
VisualEncoderAdapter‚®ÅElement-wise Sum‚äóMatrixProductAdapter ùê¥'
ùëÜ'ùëÜ'()
‚ãØ
‚®Å‚®Å‚ãØ
‚Ñ±*'+‚Ñ±+%,TestImageFew-Shot Branch
ùíÆ!"#ùíÆ$"%&AC{‚àí,+}
Upsamplenormal [obj.]abnormal [obj.]normal [obj.]abnormal [obj.]
for featurealignment(a) Train: Adaptation(b) Multi-level VisualFeature Adapter
MVFA(c) Test: Comparison
Frozen
Learnable
ReferencedNormalImagesMulti-levelAdaptedFeaturesMulti-levelFeatureMemoryBank‚Ñ±$%&$TextEncoder‚äóNearestNeighborSearch
abnormalACASPredictionensembleTwo-branchProjector
J
‚Ñ±!"#,"‚Ñ±#%&,"‚Ñ±$%&,(‚Ñ±)*$,(‚Ñ±$%&,+‚Ñ±)*$,+‚Ñ±$%&,,‚Ñ±)*$,,‚Ñ±$%&,-‚Ñ±)*$,-ùëÜ)ùê¥)
MVFA‚Ñ±'ùëÜ-ùê¥-
‚Ñ±(‚Ñ±'‚àóùëÜ.ùê¥.
‚Ñ±*‚Ñ±(‚àóùëÜ/
ùëÉ‚Ñ±+,#‚Ñ±*‚àó‚äó‚äó‚äó‚äó‚®ÅFigure 2. The architecture of multi-level adaptation and comparison framework for zero-/few-shot medical anomaly detection.
leveraged pre-trained CLIP models for language-guided de-
tection and segmentation, achieving promising outcomes.
This research extends the application of VLMs, initially
trained on natural images, to AD in medical images, in-
troducing a unique approach of multi-level visual feature
adaptation and comparison framework.
3. Problem Formulation
We aim to adapt a visual-language model, initially trained
on natural images (denoted as Mnat), for anomaly de-
tection (AD) in medical images, resulting in a medically
adapted model ( Mmed). This adaptation utilizes a medi-
cal training dataset Dmed, which consists of annotated sam-
ples from the medical field, enabling the transformation of
MnatintoMmed. Specifically, Dmed is defined as a set
of tuples {(xi, ci,Si)}K
i=1, where Kis the total number of
image samples in the dataset. Each tuple includes a training
image xi, its corresponding image-level anomaly classifica-
tion (AC) label ci‚àà {‚àí ,+}, and pixel-level anomaly seg-
mentation (AS) annotations Si‚àà {‚àí ,+}h√ówfor images of
sizeh√ów. The label ‚Äò +‚Äô indicates an anomalous sample,
while ‚Äò ‚àí‚Äô denotes a normal one. For a given test image xtest,
the model aims to accurately predict both image-level and
pixel-level anomalies for AC and AS, respectively.To model the detection of anomalies from unseen imag-
ing modalities and anatomical regions, we approach the
problem in a zero-shot learning context. Here, Dmed is a
pre-training dataset that is composed of medical data from
different modalities and anatomical regions than those in
the test samples, which assesses the model‚Äôs generalization
to unseen scenarios. Considering the practicality of obtain-
ing a limited number of samples from the target scenario,
we also extend the method to the few-shot learning context.
Here,Dmedincludes a small collection of Kannotated im-
ages that are of the same modality and anatomical region as
those in the test samples, with Ktypically representing a
small numerical value, such as {2,4,8,16}in this study.
Below we introduce our proposed multi-level adaptation
and comparison framework for AD in medical images, com-
prising (i) multi-level feature adaptation (Sec. 4), and (ii)
multi-level feature comparison (Sec. 5).
4. Train: Multi-Level Feature Adaptation
To adapt a pre-trained natural image visual-language model
for anomaly detection (AD) in medical imaging, we intro-
duce a multi-level feature adaptation framework specifically
designed for AD in medical images, utilizing minimal data
and lightweight multi-level feature adapters.
11377
Multi-level Visual Feature Adapter (MVFA). Address-
ing the challenge of overfitting due to a high parameter
count and limited training examples, we apply the CLIP
adapter across multiple feature levels. This method appends
a small set of learnable bottleneck linear layers to the visual
branches of CLIP while keeping its original backbone un-
changed, thus enabling adaptation at multiple feature levels.
As shown in Figure 2 (a), for an image x‚ààRh√ów√ó3, a
CLIP visual encoder with four sequential stages ( S1toS4)
transforms the image xinto a feature space Fvis‚ààRG√ód.
Here, Grepresents the grid number, and dsignifies the fea-
ture dimension. The output of the first three visual encoder
stages ( S1toS3), denoted as Fl‚ààRG√ód, l‚àà {1,2,3},
represents the three middle-stage features.
The visual feature adaptation involves three feature
adapters, Al(¬∑),l‚àà {1,2,3}, and one feature projector,
P(¬∑), at different levels. At each level l‚àà {1,2,3}, a learn-
able feature adapter Al(¬∑)is integrated into the feature Fl,
encompassing two (the minimum number) layers of linear
transformations. This integration transforms the features for
adaptation, represented as:
Al(Fl) =ReLU (FT
lWl,1)Wl,2,where l‚àà {1,2,3}.(1)
Here, Wl,1andWl,2denote the learnable parameters of the
linear transformations. Consistent with [15], a residual con-
nection is employed in the feature adapter to retain the orig-
inal knowledge encoded by the pre-trained CLIP. Specifi-
cally, a constant value Œ≥serves as the residual ratio to ad-
just the degree of preserving the original knowledge for im-
proved performance. Therefore, the feature adapter at the
l-th feature level can be expressed as:
F‚àó
l=Œ≥Al(Fl)T+ (1‚àíŒ≥)Fl,where l‚àà {1,2,3},(2)
withF‚àó
lserving as the input for the next encoder stage
Sl+1. By default, we set Œ≥= 0.1. Moreover, as shown
in Figure 2 (b), to simultaneously address both global and
local features for AC and AS respectively, a dual-adapter
architecture replaces the single-adapter used in Eq. (1), pro-
ducing two parallel sets of features at each level, Fcls,l
andFseg,l. For the final visual feature Fvisgenerated by
the CLIP visual encoder, a feature projector P(¬∑)projects
it using linear layers with parameters WclsandWseg, ob-
taining global and local features as Fcls,4=FT
visWcls
andFseg, 4=FT
visWseg. Utilizing the multi-level adapted
features, the model is equipped to effectively discern both
global anomalies for classification and local anomalies for
segmentation, through the following visual-language fea-
ture alignment.
Language Feature Formatting. To develop an effective
framework for anomaly classification and segmentation, we
adopt a two-tiered approach for text prompts, inspired by
methodologies used in [9, 26]. These methods leverage de-
scriptions of both normal and abnormal objects. At the statelevel, our strategy involves using straightforward, generic
text descriptions for normal and abnormal states, focusing
on clarity and avoiding complex details. Moving to the
template level, we conduct a thorough examination of the
35 templates referenced in [11] (detailed in the supplemen-
tary material). By calculating the average of the text fea-
tures extracted by the text encoder for normal and abnor-
mal states separately, we obtain a text feature represented
asFtext‚ààR2√ód, where dis the feature dimension.
Visual-Language Feature Alignment. For the image-level
anomaly annotation c‚àà {‚àí ,+}and the corresponding
pixel-level anomaly map S ‚àà {‚àí ,+}h√ów, we optimize
the model at each feature level, l‚àà {1,2,3,4}, by align-
ing the adapted-visual features given by MVFA and the text
features. This is achieved through a loss function that com-
bines different components:
Ll=Œª1Dice(softmax (Fseg,lFT
text),S)+
Œª2Focal (softmax (Fseg,lFT
text),S)+
Œª3BCE(max
h√ów(softmax (Fcls,lFT
text)), c),(3)
where Dice(¬∑,¬∑),Focal (¬∑,¬∑), and BCE(¬∑,¬∑)are dice loss [36],
focal loss [33], and binary cross-entropy loss, respectively.
Œª1,Œª2andŒª3are the individual loss weights where we set
Œª1=Œª2=Œª3= 1.0as default. The overall adaptation
lossLadapt is then calculated as the sum of losses at each
feature level, expressed as Ladapt =P4
l=1Ll.
Discussion. WinCLIP [26] relies on the class token from
pre-trained VLMs for natural image AD, with no adaptation
performed. In contrast, MVFA introduces multi-level adap-
tation, freezing the main backbone while adapting features
at each level via adapters in line with corresponding visual-
language alignments. The resulting adapted features are
residually integrated into subsequent encoder blocks, mod-
ifying input features of these blocks. This unique approach
enables collaborative training of adapters across different
levels via gradient propagation, enhancing the overall adap-
tation of the backbone model. As a result, unlike APRIL-
GAN [9], which utilizes isolated feature projections that do
not adapt the main backbone, MVFA leads to robust gen-
eralization in medical AD. The difference between MVFA
and feature projection in [9] is also evaluated in Sec. 6.3.
5. Test: Multi-Level Feature Comparison
During testing, to accurately predict anomalies at the image
level (AC) and pixel level (AS), our approach incorporates
a two-branch multi-level feature comparison architecture,
comprising a zero-shot branch and a few-shot branch, as
illustrated in Figure 2 (c).
Zero-Shot Branch. A test image xtestis processed through
MVFA to produce multi-level adapted features. These fea-
tures are then compared with the text feature Ftext. Zero-
11378
Table 1. Comparisons with state-of-the-art few-shot anomaly detection methods with K=4. The AUCs (in %) for anomaly classification
(AC) and anomaly segmentation (AS) are reported. The best result is in bold, and the second-best result is underlined.
Setting Method SourceHIS ChestXray OCT17 BrainMRI LiverCT RESC
AC AC AC AC AS AC AS AC AS
full-normal-shotCFlowAD [19] WACV 2022 54.54 71.44 85.43 73.97 93.52 49.93 92.78 74.43 93.75
RD4AD [10] CVPR 2022 66.59 67.53 97.24 89.38 96.54 60.02 95.86 87.53 96.17
PatchCore [41] CVPR 2022 69.34 75.17 98.56 91.55 96.97 60.40 96.58 91.50 96.39
MKD [44] CVPR 2022 77.74 81.99 96.62 81.38 89.54 60.39 96.14 88.97 86.60
few-normal-shotCLIP [25] OpenCLIP 63.48 70.74 98.59 74.31 93.44 56.74 97.20 84.54 95.03
MedCLIP [52] EMNLP 2022 75.89 84.06 81.39 76.87 90.91 60.65 94.45 66.58 88.98
WinCLIP [26] CVPR 2023 67.49 70.00 97.89 66.85 94.16 67.19 96.75 88.83 96.68
few-shotDRA [12] CVPR 2022 68.73 75.81 99.06 80.62 74.77 59.64 71.79 90.90 77.28
BGAD [57] CVPR 2023 - - - 83.56 92.68 72.48 98.88 86.22 93.84
APRIL-GAN [9] arXiv 2023 76.11 77.43 99.41 89.18 94.67 53.05 96.24 94.70 97.98
MVFA Ours 82.71 81.95 99.38 92.44 97.30 81.18 99.73 96.18 98.97
shot AC and AS results, denoted as czeroandSzero, are cal-
culated using average softmax scores across the four levels,
czero=1
4P4
l=1max
G(softmax (Fcls,lFT
text)),
Szero=1
4P4
l=1BI(softmax (Fseg,lFT
text)).(4)
Here, BI (¬∑)reshapes the anomaly map to‚àö
G√ó‚àö
Gand re-
stores it to the original input image resolution using bilinear
interpolation, with Grepresenting the grid number.
Few-Shot Branch. All the multi-level visual features of a
few labeled normal images in Dmedcontribute to construct-
ing a multi-level feature memory bank Gfacilitating the fea-
ture comparison. The few-shot AC and AS scores, denoted
ascfewandSfew, are derived from the minimum distance be-
tween the test feature and the memory bank features at each
level, through a nearest neighbor search process,
cfew=1
4P4
l=1max
G(min
m‚ààGDist(Fcls,l, m)),
Sfew=1
4P4
l=1BI(min
m‚ààGDist(Fseg,l, m)).(5)
Here, Dist(¬∑,¬∑)represents the cosine distance, calculated as
1‚àícosine (¬∑,¬∑). The final predicted AC and AS results com-
bine the outcomes from both branches:
cpred=Œ≤1czero+Œ≤2cfew,Spred=Œ≤1Szero+Œ≤2Sfew.(6)
Here, Œ≤1andŒ≤2are weighting factors for the zero-shot and
few-shot branches, respectively, set to 0.5 by default.
6. Experiments
6.1. Experimental Setups
Datasets. We consider a medical anomaly detection (AD)
benchmark based on BMAD [3], covering five distinct med-
/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000036/uni0000004b/uni00000052/uni00000057/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
(a) BrainMRI
/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000036/uni0000004b/uni00000052/uni00000057/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
 (d) HIS
/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000036/uni0000004b/uni00000052/uni00000057/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
(b) LiverCT
/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000036/uni0000004b/uni00000052/uni00000057/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
 (e) ChestXray
/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000036/uni0000004b/uni00000052/uni00000057/uni00000018/uni00000013/uni00000019/uni00000013/uni0000001a/uni00000013/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
(c) RESC
/uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000036/uni0000004b/uni00000052/uni00000057/uni0000001b/uni00000013/uni0000001c/uni00000013/uni00000014/uni00000013/uni00000013/uni00000024/uni00000038/uni00000026/uni00000003/uni0000000b/uni00000008/uni0000000c
/uni00000024/uni00000026/uni0000001d/uni00000003/uni00000024/uni00000053/uni00000055/uni0000004c/uni0000004f/uni00000010/uni0000002a/uni00000024/uni00000031
/uni00000024/uni00000026/uni0000001d/uni00000003/uni00000027/uni00000035/uni00000024
/uni00000024/uni00000026/uni0000001d/uni00000003/uni00000025/uni0000002a/uni00000024/uni00000027
/uni00000024/uni00000026/uni0000001d/uni00000003/uni00000032/uni00000058/uni00000055/uni00000056
/uni00000024/uni00000036/uni0000001d/uni00000003/uni00000024/uni00000053/uni00000055/uni0000004c/uni0000004f/uni00000010/uni0000002a/uni00000024/uni00000031
/uni00000024/uni00000036/uni0000001d/uni00000003/uni00000027/uni00000035/uni00000024
/uni00000024/uni00000036/uni0000001d/uni00000003/uni00000025/uni0000002a/uni00000024/uni00000027
/uni00000024/uni00000036/uni0000001d/uni00000003/uni00000032/uni00000058/uni00000055/uni00000056 (f) OCT17
Figure 3. Comparisons with state-of-the-art few-shot anomaly
detection methods on datasets of (a) BrainMRI, (b) LiverCT, (c)
RESC, (d) HIS, (e) ChestXray, and (f) OCT17, with the shot num-
berK={2,4,8,16}. The AUCs (in %) for anomaly classifi-
cation (AC) and anomaly segmentation (AS) are reported. More
details are included in the supplementary material.
11379
ical domains and resulting in six datasets. These include
brain MRI [1, 2, 35], liver CT [7, 29], retinal OCT [21, 28],
chest X-ray [51], and digital histopathology [4]. Among
these, BrainMRI [1, 2, 35], LiverCT [7, 29], and RESC [21]
datasets are used for both anomaly classification (AC) and
segmentation (AS), while OCT17 [28], ChestXray [51], and
HIS [4] are solely for AC. Detailed descriptions of these
datasets are provided in the supplementary material.
Competing Methods and Baselines. In this study, we
consider various state-of-the-art AD methods within dis-
tinct training settings as competing methods. These set-
tings encompass (i) vanilla methods that use all normal
data (CFlowAD [19], RD4AD [10], PatchCore [41], and
MKD [44]), (ii) few-normal-shot methods (CLIP [25],
MedCLIP [52], WinCLIP [26]), and (iii) few-shot methods
(DRA [12], BGAD [57], and April-GAN [9]). We evaluate
these methods for AC and AS, excluding BGAD, which is
exclusively applied for segmentation due to its requirement
for pixel-level annotations during training.
Evaluation Protocols. The area under the Receiver Oper-
ating Characteristic curve metric (AUC) is used to quantify
the performance. This metric is a standard in AD evalua-
tion, with separate considerations for image-level AUC in
AC and pixel-level AUC in AS.
Model Configuration and Training Details. We utilize
the CLIP with ViT-L/14 architecture, with input images at a
resolution of 240. The model comprises a total of 24 layers,
which are divided into 4 stages, each encompassing 6 lay-
ers. We use the Adam optimizer at a constant learning rate
of 1e-3 and a batch size of 16, conducting 50 epochs for
training on one single NVIDIA GeForce RTX 3090 GPU.
6.2. Comparison with State-of-the-art Methods
Few-Shot Setting. In Table 1, we compare the perfor-
mance of MVFA under the few-shot setting with K= 4
against other state-of-the-art AD methods. For an in-depth
analysis of MVFA‚Äôs performance across various few-shot
scenarios ( K‚àà {2,4,8,16}), please refer to Figure 3.
MVFA demonstrates superior performance over competing
methods like DRA [12], BGAD [57], and April-GAN [9].
Notably, MVFA surpasses April-GAN, the winner of the
V AND workshop at CVPR 2023 [68], by an average of
7.33% in AUC for AC and 2.37% in AUC for AS across
all datasets. Compared to BGAD [57], MVFA shows an av-
erage improvement of 9.18% in AUC for AC and 3.53% in
AUC for AS, in datasets with pixel-level annotations.
MVFA outperforms few-normal-shot CLIP-based meth-
ods such as CLIP [25] and WinCLIP [26], which also use
visual-language pre-trained backbones and employ feature
comparisons for AD. MVFA‚Äôs advantage lies in its ability
to effectively utilize a few abnormal samples, leading to
significant gains over these methods. For example, against
WinCLIP [26], MVFA achieves an average improvement ofTable 2. Comparisons with state-of-the-art zero-shot anomaly de-
tection methods with in-/out-domain evaluation. The AUCs (in %)
for AC and AS are reported.
Datasets WinCLIP [26] April-GAN [9] MVFA (ours)
HIS 69.85 / - 72.36 / - 77.90 / -
ChestXray 70.86 / - 57.49 / - 71.11 / -
OCT17 46.64 / - 92.61 / - 95.40 / -
BrainMRI 66.49 / 85.99 76.43 / 91.79 78.63 / 90.27
LiverCT 64.20 / 96.20 70.57 / 97.05 76.24 /97.85
RESC 42.51 / 80.56 75.67 / 85.23 83.31 /92.05
Table 3. Comparisons with state-of-the-art few-shot anomaly de-
tection methods with K=4 for in-/out-domain evaluation. The
AUCs (in %) for AC/AS are reported.
AC/AS (avg AUC%) WinCLIP [26] April-GAN [9] MVFA
in-domain (MVTec) 95.16/96.27 92.77/95.89 96.19 /96.32
out-domain (medical) 76.38/95.86 81.65/96.30 88.97 /98.67
12.60% in AUC for AC and 2.81% in AUC for AS across
all datasets. While MedCLIP [52] shows superior results
on ChestXray because it was trained on large-scale over-
lapping ChestXray data in our medical AD benchmark, it
lacks broad generalization capabilities, as evidenced by its
performance on other datasets.
Moreover, MVFA exhibits substantial improvements
over full-normal-shot vanilla AD methods such as
CFlowAD [19], RD4AD [10], PatchCore [41], and
MKD [44], which rely on much larger datasets than those
employed in this study. This highlights the value of incor-
porating a few abnormal samples as supervision, especially
in medical diagnostics where acquiring a limited number of
abnormal data can be more practical.
Zero-Shot Setting. The experiments for zero-shot AD were
conducted under the leave-one-out setting . In this config-
uration, a designated target dataset was chosen for test-
ing, while the remaining datasets with different modalities
and anatomical regions were employed for training. The
aim of this approach was to gauge the performance when
confronted with unseen modalities and anatomical regions,
thereby assessing the model‚Äôs capacity for generalization.
Table 2 provides a comprehensive overview of the results
pertaining to zero-shot medical AC and AS, offering a com-
parative evaluation alongside two state-of-the-art methods
that also harness the capabilities of the CLIP backbone.
MVFA shows remarkable superiority; for instance, it out-
performs WinCLIP [26] with an average AUC improvement
of 20.34% for AC and 5.81% for AS across all datasets.
Similarly, against April-GAN [9], MVFA achieves an av-
erage AUC improvement of 6.24% for AC and 2.03% for
AS across all datasets, underscoring its effectiveness in the
challenging zero-shot medical AD setting.
In-Domain Evaluation. The MVTec AD benchmark [5],
11380
Table 4. Ablation studies compared with feature alignment with multi-level projectors and feature adaptation with multi-level adapters
under the zero-shot setting. The AUCs (in %) for AC and AS are reported.
MethodHIS ChestXray OCT17 BrainMRI LiverCT RESC
AC AC AC AC AS AC AS AC AS
feature alignment (projectors) 66.32 58.06 49.85 76.66 89.39 75.85 97.64 74.44 89.17
feature adaptation (adapters) 77.90 71.11 95.40 78.63 90.27 76.24 97.85 83.31 92.05
Table 5. Ablation studies of multi-level feature ensemble under
single-level training/multi-level training with the same model ar-
chitecture. The average AUCs (in %) of all six datasets for AC and
AS under the few-shot setting (K=4) are reported. Results for each
dataset are included in the supplementary material.
Layer 1 Layer 2 Layer 3 Layer 4 All
AC 80.96/83.39 88.83/88.84 81.25/84.32 83.33/84.62 88.97
AS 97.70/97.98 98.58/98.62 96.03/98.54 97.19/97.44 98.67
consisting of 15 industrial defect detection sub-datasets,
is considered as in-domain evaluation. As shown in Ta-
ble 3, although our main focus is not in-domain scenarios,
MVFA shows comparable performance to state-of-the-art
methods in a few-shot setting (K=4). Detailed results for
each sub-dataset are included in the supplementary mate-
rial. In our main focus of out-domain evaluations on the
medical AD benchmark, MVFA significantly outperforms
competing methods, highlighting its superior generalization
capabilities.
6.3. Ablation Studies
Feature Adaptation vs. Feature Alignment. We conduct
ablation studies in the zero-shot setting for AC and AS to
assess the effectiveness of multi-level feature adapters in
enhancing cross-modal generalization. For this purpose,
we substitute the multi-level feature adapters with distinct
multi-level feature projectors, while keeping the parameters
and feature alignment loss functions the same. The primary
distinction is that each projector is optimized independently,
in contrast to the collective optimization approach used for
adapters. This ensures that the only variable under consid-
eration is the architecture, while all other factors, including
model parameters and training loss functions, remain con-
stant for a valid comparison.
The results, as shown in Table 4, reveal significant im-
provements with feature adapters. They lead to substantial
improvements in AC, with image-level AUC increasing by
11.58%, 13.05%, 45.55%, 1.97%, 0.39%, and 8.87% for
HIS, ChestXay, OCT17, BrainMRI, LiverCT, and RESC,
respectively, with an average improvement of 13.57%. For
AS, improvements were observed, with pixel-level AUC in-
creasing by 0.88%, 0.21%, and 2.88% for BrainMRI, Liv-
addMedical ImageLayer1Layer2Layer3Layer4FinalPredictionSingle-Layer HeatmapsGroundTruthFigure 4. Considering features across multiple levels significantly
enhances segmentation performance. The white dashed boxes de-
marcate regions that have been missed or erroneously segmented.
More cases are included in the supplementary material.
erCT, and RESC, respectively. These findings highlight the
critical role of multi-level feature adapters in boosting the
model‚Äôs generalization capabilities.
Feature Ensemble in Multi-Level. In this study, as shown
in Table 5, we evaluate the effectiveness of an ensemble ap-
proach that integrates features from different layers. The ap-
proaches are compared against our comprehensive model,
both using the multi-level adapter architecture. Our eval-
uation specifically focused on features from four distinct
layers under two training scenarios: (i) single-layer train-
ing, where only one layer‚Äôs adapter is optimized in each
experiment, and (ii) multi-level training, aligning with the
methodology of our comprehensive model. The objective of
this comparison was to determine the benefits of combining
features from multiple layers compared to optimizing each
layer‚Äôs features separately.
The results presented in Table 5 demonstrate that among
individual layers, Layer 2 yielded the highest performance,
achieving 88.84% in AC and 98.62% in AS. However, the
ensemble method, which integrates features from all lay-
ers, outperformed single-layer approaches, recording AUCs
of 88.97% in AC and 98.67% in AS. This underscores the
effectiveness of combining features from multiple levels.
Moreover, multi-level training consistently exceeds the per-
formance of single-layer training, reinforcing the benefits
of our multi-level adaptation approach across all layers.
The visualizations of retina anomaly segmentation from
various layers, as depicted in Figure 4, further reinforce our
11381
Image
Ground TruthHeatmapResultHeatmapResult
April-GANOursBrainMRILiverCTRetinalOCT(a)(b)(c)(d)(e)(f)Figure 5. Visualization of AS for MVFA on brain MRI, liver CT, and retinal OCT, compared with state-of-the-art method April-GAN.
Results from (e) show better performance than results from (c), showing the effectiveness of the proposed multi-level feature adaptation.
(a) Without Adapter
/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f
/uni00000024/uni00000045/uni00000051/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f (b) With Adapter
Figure 6. Visualization, using t-SNE, of the features learned from
the RESC test set, using (a) pretrained visual encoder, and (b)
multi-level feature adapters. The same t-SNE optimization iter-
ations are used in each case. Results show that features extracted
by adapters are separated between normal and abnormal samples.
findings. These visualizations distinctly show that single-
layer methods are less effective compared to the ensem-
ble approaches. By synergistically integrating features from
different layers, we achieve a marked improvement in AD,
which aligns with and supports our quantitative results.
6.4. Visualization Analysis
To qualitatively analyze how the proposed multi-level fea-
ture adaptation approach improves anomaly segmentation
performance, we visualize the results of several cases from
the BrainMRI, LiverCT, and RESC datasets. It can be seen
from the results in Figure 5 that the segmentation produced
by our MVFA (column e) is closer to the ground truth (col-
umn f) than that produced by the state-of-the-art method
April-GAN (column c). This illustrates the effectiveness ofthe proposed multi-level visual feature adaptation.
We employ t-SNE [49] to visualize the learned features
from RESC, as depicted in Figure 6. Each dot corresponds
to features of a normal or abnormal sample from the test set.
It can be seen that adapter enhances the separation between
samples belonging to distinct states, which is beneficial for
the identification of AD decision boundaries.
7. Conclusion
This paper adapts the pretrained visual-language models
in natural domain to medical AD, with cross-domain gen-
eralizability among various modalities and anatomical re-
gions. The adaptation involves not only from natural do-
main to medical domain, but also from high-level semantics
to pixel-level segmentation. To achieve such goals, a collab-
orative multi-level feature adaptation method is introduced,
where each adaptation is guided by the corresponding
visual-language alignments, facilitating segmenting anoma-
lies of diverse forms from medical images. Coupled with
a comparison-based AD strategy, the method enables flex-
ible adaptation for datasets with substantial modality and
distribution differences. The proposed method outperforms
existing methods on zero-/few-shot AC and AS tasks, indi-
cating promising research avenues for future exploration.
Acknowledgment. This work is supported by the Na-
tional Key R&D Program of China (No. 2022ZD0160703),
STCSM (No. 22511106101, No. 18DZ2270700, No.
21DZ1100100), 111 plan (No. BP0719010), State Key
Laboratory of UHD Video and Audio Production and Pre-
sentation, and the Ministry of Education, Singapore, under
its Academic Research Fund Tier 2 (Award Number: MOE-
T2EP20122-0006).
11382
References
[1] Ujjwal Baid, Satyam Ghodasara, Suyash Mohan, Michel
Bilello, Evan Calabrese, Errol Colak, Keyvan Farahani,
Jayashree Kalpathy-Cramer, Felipe C Kitamura, Sarthak
Pati, et al. The rsna-asnr-miccai brats 2021 benchmark on
brain tumor segmentation and radiogenomic classification.
arXiv preprint arXiv:2107.02314 , 2021. 2, 6
[2] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel
Bilello, Martin Rozycki, Justin S Kirby, John B Freymann,
Keyvan Farahani, and Christos Davatzikos. Advancing the
cancer genome atlas glioma mri collections with expert seg-
mentation labels and radiomic features. Scientific Data , 4(1):
1‚Äì13, 2017. 2, 6
[3] Jinan Bao, Hanshi Sun, Hanqiu Deng, Yinsheng He, Zhaox-
iang Zhang, and Xingyu Li. Bmad: Benchmarks for medical
anomaly detection. arXiv preprint arXiv:2306.11876 , 2023.
2, 5
[4] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes
Van Diest, Bram Van Ginneken, Nico Karssemeijer, Geert
Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen,
Quirine F Manson, Maschenka Balkenhol, et al. Diagnos-
tic assessment of deep learning algorithms for detection of
lymph node metastases in women with breast cancer. Jama ,
318(22):2199‚Äì2210, 2017. 2, 6
[5] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Mvtec ad‚Äìa comprehensive real-world
dataset for unsupervised anomaly detection. In CVPR , 2019.
2, 6
[6] Paul Bergmann, Michael Fauser, David Sattlegger, and
Carsten Steger. Uninformed students: Student-teacher
anomaly detection with discriminative latent embeddings. In
CVPR , 2020. 2
[7] Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene
V orontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin,
Colin Jacobs, Gabriel Efrain Humpire Mamani, Gabriel
Chartrand, et al. The liver tumor segmentation benchmark
(lits). Medical Image Analysis , 84:102680, 2023. 2, 6
[8] Yu Cai, Hao Chen, Xin Yang, Yu Zhou, and Kwang-Ting
Cheng. Dual-distribution discrepancy with self-supervised
refinement for anomaly detection in medical images. Medi-
cal Image Analysis , 86:102794, 2023. 2
[9] Xuhai Chen, Yue Han, and Jiangning Zhang. A zero-/few-
shot anomaly classification and segmentation method for
cvpr 2023 vand workshop challenge tracks 1&2: 1st place
on zero-shot ad and 4th place on few-shot ad. arXiv preprint
arXiv:2305.17382 , 2023. 1, 2, 4, 5, 6
[10] Hanqiu Deng and Xingyu Li. Anomaly detection via reverse
distillation from one-class embedding. In CVPR , 2022. 2, 5,
6
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , 2009. 4
[12] Choubo Ding, Guansong Pang, and Chunhua Shen. Catching
both gray and black swans: Open-set supervised anomaly
detection. In CVPR , 2022. 1, 2, 5, 6
[13] Zhiyuan Ding, Qi Dong, Haote Xu, Chenxin Li, Xinghao
Ding, and Yue Huang. Unsupervised anomaly segmentationfor brain lesions using dual semantic-manifold reconstruc-
tion. In International Conference on Neural Information
Processing , 2022. 2
[14] Tharindu Fernando, Harshala Gammulle, Simon Denman,
Sridha Sridharan, and Clinton Fookes. Deep learning for
medical anomaly detection‚Äìa survey. ACM Computing Sur-
veys (CSUR) , 54(7):1‚Äì37, 2021. 1
[15] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.
Clip-adapter: Better vision-language models with feature
adapters. International Journal of Computer Vision , pages
1‚Äì15, 2023. 4
[16] Gabriel Goh, Nick Cammarata, Chelsea V oss, Shan Carter,
Michael Petrov, Ludwig Schubert, Alec Radford, and Chris
Olah. Multimodal neurons in artificial neural networks. Dis-
till, 6(3):e30, 2021. 2
[17] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha,
Moussa Reda Mansour, Svetha Venkatesh, and Anton
van den Hengel. Memorizing normality to detect anomaly:
Memory-augmented deep autoencoder for unsupervised
anomaly detection. In ICCV , 2019. 2
[18] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. In ICLR , 2021. 2
[19] Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka.
Cflow-ad: Real-time unsupervised anomaly detection with
localization via conditional normalizing flows. In WACV ,
2022. 2, 5, 6
[20] Keji He, Chenyang Si, Zhihe Lu, Yan Huang, Liang Wang,
and Xinchao Wang. Frequency-enhanced data augmentation
for vision-and-language navigation. In Advances in Neural
Information Processing Systems , 2023. 2
[21] Junjie Hu, Yuanyuan Chen, and Zhang Yi. Automated seg-
mentation of macular edema in oct using deep neural net-
works. Medical Image Analysis , 55:216‚Äì227, 2019. 2, 6
[22] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang,
Michael Spratling, and Yan-Feng Wang. Registration based
few-shot anomaly detection. In ECCV , 2022. 1
[23] Chaoqin Huang, Aofan Jiang, Ya Zhang, and Yanfeng Wang.
Multi-scale memory comparison for zero-/few-shot anomaly
detection. arXiv preprint arXiv:2308.04789 , 2023. 2
[24] Weikai Huang, Yijin Huang, and Xiaoying Tang. Lesion-
paste: One-shot anomaly detection for medical images.
arXiv preprint arXiv:2203.06354 , 2022. 2
[25] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. 2, 5, 6
[26] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang,
Avinash Ravichandran, and Onkar Dabeer. Winclip: Zero-
/few-shot anomaly classification and segmentation. In
CVPR , 2023. 1, 2, 4, 5, 6
[27] Aofan Jiang, Chaoqin Huang, Qing Cao, Shuang Wu,
Zi Zeng, Kang Chen, Ya Zhang, and Yanfeng Wang.
Multi-scale cross-restoration framework for electrocardio-
gram anomaly detection. In MICCAI , 2023. 2
11383
[28] Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Car-
olina CS Valentim, Huiying Liang, Sally L Baxter, Alex
McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al.
Identifying medical diagnoses and treatable diseases by
image-based deep learning. Cell, 172(5):1122‚Äì1131, 2018.
2, 6
[29] Bennett Landman, Zhoubing Xu, J Igelsias, Martin Styner,
T Langerak, and Arno Klein. Miccai multi-atlas la-
beling beyond the cranial vault‚Äìworkshop and challenge.
InProc. MICCAI Multi-Atlas Labeling Beyond Cranial
Vault‚ÄîWorkshop Challenge , page 12, 2015. 2, 6
[30] Chenxin Li, Yunlong Zhang, Jiongcheng Li, Yue Huang,
and Xinghao Ding. Unsupervised anomaly segmentation
using image-semantic cycle translation. arXiv preprint
arXiv:2103.09094 , 2021. 2
[31] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas
Pfister. Cutpaste: Self-supervised learning for anomaly de-
tection and localization. In CVPR , 2021. 2
[32] Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen,
and Xinchao Wang. Graphadapter: Tuning vision-language
models with dual knowledge graph. In Advances in Neural
Information Processing Systems , 2023. 2
[33] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and
Piotr Doll ¬¥ar. Focal loss for dense object detection. In ICCV ,
2017. 4
[34] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache:
Accelerating diffusion models for free. In IEEE/CVF Con-
ference Computer Vision and Pattern Recognition , 2024. 2
[35] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree
Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya
Burren, Nicole Porz, Johannes Slotboom, Roland Wiest,
et al. The multimodal brain tumor image segmentation
benchmark (brats). IEEE Transactions on Medical Imaging ,
34(10):1993‚Äì2024, 2014. 2, 6
[36] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In International Conference
on 3D Vision (3DV) , pages 565‚Äì571. IEEE, 2016. 4
[37] Guansong Pang, Chunhua Shen, and Anton van den Hengel.
Deep anomaly detection with deviation networks. In Pro-
ceedings of the 25th ACM SIGKDD International Confer-
ence on Knowledge Discovery & Data Mining , pages 353‚Äì
362, 2019. 2
[38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 1, 2
[39] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong
Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu.
Denseclip: Language-guided dense prediction with context-
aware prompting. In CVPR , 2022. 2
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj ¬®orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 2
[41] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard
Sch¬®olkopf, Thomas Brox, and Peter Gehler. Towards totalrecall in industrial anomaly detection. In CVPR , 2022. 2, 5,
6
[42] Lukas Ruff, Robert A Vandermeulen, Nico G ¬®ornitz, Alexan-
der Binder, Emmanuel M ¬®uller, Klaus-Robert M ¬®uller, and
Marius Kloft. Deep semi-supervised anomaly detection. In
ICLR , 2020. 2
[43] Mohammad Sabokrou, Mohammad Khalooei, Mahmood
Fathy, and Ehsan Adeli. Adversarially learned one-class
classifier for novelty detection. In CVPR , 2018. 2
[44] Mohammadreza Salehi, Niousha Sadjadi, Soroosh
Baselizadeh, Mohammad Hossein Rohban, and Hamid R
Rabiee. Multiresolution knowledge distillation for anomaly
detection. In CVPR , 2021. 2, 5, 6
[45] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for training
next generation image-text models. NeurIPS , 2022. 2
[46] Shelly Sheynin, Sagie Benaim, and Lior Wolf. A hierarchical
transformation-discriminating generative model for few shot
anomaly detection. In ICCV , 2021. 1
[47] Jianpo Su, Hui Shen, Limin Peng, and Dewen Hu. Few-
shot domain-adaptive anomaly detection for cross-site brain
images. IEEE Transactions on Pattern Analysis and Machine
Intelligence (Early Access) , 2021. 1
[48] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Car-
lini, Benjamin Recht, and Ludwig Schmidt. Measuring ro-
bustness to natural distribution shifts in image classification.
NeurIPS , 2020. 2
[49] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of Machine Learning Research , 9
(11), 2008. 8
[50] Shenzhi Wang, Liwei Wu, Lei Cui, and Yujun Shen. Glanc-
ing at the patch: Anomaly localization with global and local
feature comparison. In CVPR , 2021. 2
[51] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mo-
hammadhadi Bagheri, and Ronald M Summers. Chestx-
ray8: Hospital-scale chest x-ray database and benchmarks
on weakly-supervised classification and localization of com-
mon thorax diseases. In CVPR , 2017. 2, 6
[52] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng
Sun. Medclip: Contrastive learning from unpaired medical
images and text. In EMNLP , 2022. 5, 6
[53] Julia Wolleb, Florentin Bieder, Robin Sandk ¬®uhler, and
Philippe C Cattin. Diffusion models for medical anomaly
detection. In MICCAI , 2022. 2
[54] Jhih-Ciang Wu, Ding-Jie Chen, Chiou-Shann Fuh, and Tyng-
Luh Liu. Learning unsupervised metaformer for anomaly
detection. In ICCV , 2021. 2
[55] Haote Xu, Yunlong Zhang, Liyan Sun, Chenxin Li, Yue
Huang, and Xinghao Ding. Afsc: Adaptive fourier
space compression for anomaly detection. arXiv preprint
arXiv:2204.07963 , 2022. 2
[56] Xingyi Yang and Xinchao Wang. Diffusion model as repre-
sentation learner. In IEEE International Conference on Com-
puter Vision , 2023. 2
11384
[57] Xincheng Yao, Ruoqi Li, Jing Zhang, Jun Sun, and
Chongyang Zhang. Explicit boundary guided semi-push-
pull contrastive learning for supervised anomaly detection.
InCVPR , 2023. 1, 2, 5, 6
[58] Fei Ye, Chaoqin Huang, Jinkun Cao, Maosen Li, Ya Zhang,
and Cewu Lu. Attribute restoration framework for anomaly
detection. IEEE Transactions on Multimedia , 24:116‚Äì127,
2022. 2
[59] Jingwen Ye, Ruonan Yu, Songhua Liu, and Xinchao Wang.
Mutual-modality adversarial attack with semantic perturba-
tion. In AAAI Conference on Artificial Intelligence , 2024.
2
[60] Tao Yu, Zhihe Lu, Xin Jin, Zhibo Chen, and Xinchao
Wang. Task residual for tuning vision-language models. In
IEEE/CVF Conference Computer Vision and Pattern Recog-
nition , 2023. 2
[61] Jianpeng Zhang, Yutong Xie, Zhibin Liao, Guansong Pang,
Johan Verjans, Wenxin Li, Zongji Sun, Jian He, and Chun-
hua Shen Yi Li. Viral pneumonia screening on chest x-ray
images using confidence-aware anomaly detection. IEEE
Transactions on Medical Imaging , 40(3):879‚Äì890, 2021. 1,
2
[62] Ruipeng Zhang, Ziqing Fan, Qinwei Xu, Jiangchao Yao, Ya
Zhang, and Yanfeng Wang. Grace: A generalized and per-
sonalized federated learning method for medical imaging. In
MICCAI , 2023. 2
[63] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan
Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang
Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based
language-image pretraining. In CVPR , pages 16793‚Äì16803,
2022. 2
[64] Kang Zhou, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen
Liu, Weixin Luo, Zaiwang Gu, Jiang Liu, and Shenghua
Gao. Encoding structure-texture relation with p-net for
anomaly detection in retinal images. In ECCV , 2020. 2
[65] Kang Zhou, Jing Li, Weixin Luo, Zhengxin Li, Jianlong
Yang, Huazhu Fu, Jun Cheng, Jiang Liu, and Shenghua Gao.
Proxy-bridged image reconstruction network for anomaly
detection in medical images. IEEE Transactions on Medi-
cal Imaging , 41(3):582‚Äì594, 2021.
[66] Kang Zhou, Jing Li, Yuting Xiao, Jianlong Yang, Jun Cheng,
Wen Liu, Weixin Luo, Jiang Liu, and Shenghua Gao. Mem-
orizing structure-texture correspondence for image anomaly
detection. IEEE Transactions on Neural Networks and
Learning Systems , 33(6):2335‚Äì2349, 2021. 2
[67] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
Liu. Learning to prompt for vision-language models. In-
ternational Journal of Computer Vision , 130(9):2337‚Äì2348,
2022. 2
[68] Yang Zou, Taewan Kim, Latha Pemula, and Onkar Dabeer.
Visual anomaly and novelty detection (vand) challenge in
cvpr 2023 workshop, 2023. https://sites.google.
com/view/vand-cvpr23/challenge . 6
11385
