OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers
Han Liang1Jiacheng Bao1Ruichi Zhang1Sihan Ren1Yuecheng Xu1
Sibei Yang1Xin Chen2Jingyi Yu1Lan Xu1
1ShanghaiTech University2Tencent PCG
Abstract
We have recently seen tremendous progress in realistic
text-to-motion generation. Yet, the existing methods of-
ten fail or produce implausible motions with unseen text
inputs, which limits the applications. In this paper, we
present OMG, a novel framework, which enables com-
pelling motion generation from zero-shot open-vocabulary
text prompts. Our key idea is to carefully tailor the pretrain-
then-finetune paradigm into the text-to-motion generation.
At the pre-training stage, our model improves the gener-
ation ability by learning the rich out-of-domain inherent
motion traits. To this end, we scale up a large uncondi-
tional diffusion model up to 1B parameters, so as to utilize
the massive unlabeled motion data up to over 20M motion
instances. At the subsequent fine-tuning stage, we intro-
duce motion ControlNet, which incorporates text prompts
as conditioning information, through a trainable copy of
the pre-trained model and the proposed novel Mixture-
of-Controllers (MoC) block. MoC block adaptively rec-
ognizes various ranges of the sub-motions with a cross-
attention mechanism and processes them separately with
the text-token-specific experts. Such a design effectively
aligns the CLIP token embeddings of text prompts to var-
ious ranges of compact and expressive motion features. Ex-
tensive experiments demonstrate that our OMG achieves
significant improvements over the state-of-the-art meth-
ods on zero-shot text-to-motion generation. Project page:
https://tr3e.github.io/omg-page.
1. Introduction
Generating vivid motions of human characters is a long-
standing task in computer vision, with numerous applica-
tions in movies, games, robotics, and VR/AR. The text-to-
motion setting aims to democratize motion generation for
novices and has recently received substantive attention.
Benefited from the existing text-annotated motion
datasets [19, 47], recent advances generate diverse motions
from text prompts, equipped with various generative mod-
els like V AEs [56, 76], autoregressive models [33, 87], and
diffusion models [12, 13,77,89]. However, these meth-
Figure 1. Our Open-vocabulary Motion Generation (OMG) ap-
proach is capable of generating high-quality motions in response
to unseen text prompts.
ods heavily rely on the paired text-motion data with lim-
ited text annotations, and hence fall short of generalizing
to unseen open-vocabulary text inputs. Imagine generating
â€œGollum breakdance footwork â€, it requires out-of-domain
generation ability with rich human knowledge to understand
the motion traits in various words like characters (Gollum,
ninja, etc.) or skills (breakdance, spinkick, etc.). To tackle
the open-vocabulary texts, recent works [8, 28,42] utilize
the zero-shot text-image alignment ability of the large-scale
pre-trained models, e.g., CLIP [57]. They adopt text-pose
alignments and hence remove the reliance on pair-wise text
and continuous motions. Yet, the discrete poses are insuf-
ficient to represent the rich motion traits, leading to unre-
alistic motion generation. On the other hand, recent ad-
vances [9, 21,63,88] enable impressive and controllable
text-driven image and even video generation, even from
open-vocabulary text inputs. Revisiting their huge success,
both the pretrain-then-finetune paradigm as well as scaling
up the model have played an important role. Yet, adopting
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
482
these strategies for text-to-motion generation is challenging.
First, there exists a huge imbalance between the quantities
of unpaired motion data and paired text-motion data. Sec-
ond, for open-vocabulary texts, various tokens correspond
to various motion traits, constituting a complex many-to-
many problem.
To tackle the challenges, we propose OMG â€“ a novel
scheme to generate motions of human characters from
open-vocabulary texts (see Fig. 1). Our key idea is to care-
fully tailor the pretrain-then-finetune paradigm into text-to-
motion generation. For pre-training, we adopt a minimalis-
tic design to scale up a large unconditional diffusion model,
so as to utilize the massive unlabeled motion data. For fine-
tuning, we first freeze the pre-trained large model. We then
adopt the trainable copy and treat the text prompts as ex-
tra conditions through a novel design, named mixture-of-
controllers, to learn to predict the conditional residuals. It
adaptively fuses the motion traits corresponding to various
tokens from the input text prompts, so as to handle the am-
biguity between the text and motion modalities.
Specifically, at the pre-training stage, we adopt the
diffusion transformer [52] as the backbone with over-
parameterized motion representation. We then scale up the
models with parameters ranging from 88M to 1B, leverag-
ing over 20M unlabeled motion instances from a diverse
range of 13 publicly available datasets. During training, we
also adopt a sliding random window strategy to crop various
motion sequences, to improve the generation ability for ar-
bitrary lengths of motion. To this end, the pre-trained model
learns the rich inherent motion features with a large motion
manifold to ensure the realism of the generated motions.
For the subsequent stage, to incorporate the text prompts
as conditioning information, we adopt a fine-tuning scheme
called motion ControlNet, analogous to the famous Con-
trolNet [88] for the text-image generation task. It includes
a trainable copy of the large-scale pre-trained model, which
serves as a strong backbone to retain the motion features, as
well as a novel block called Mixture-of-Controllers (MoC).
Such MoC block can effectively inject residual information
into the pre-trained model, based on the motion features and
the CLIP-based text embeddings. Our key design in the
MoC block is the cross-attention mechanism between text
and motion, as well as the text-token-specific Mixture-of-
Experts [16, 67], which are to be detailed in later sections.
Such a design effectively aligns the token embeddings of
text prompts to various ranges of compact and expressive
motion features, which are warmed up from a powerful pre-
trained model. As a result, our OMG approach achieves
compelling generation from open-vocabulary texts, signif-
icantly outperforming prior arts. In particular, only fine-
tuned on the HumanML3D dataset [19], it can generate
vivid motions of various human characters with diverse mo-
tion skills from the Mixamo dataset [1], as shown Fig. 1.To summarize, our main contributions include:
â€¢ We propose a text-to-motion generation approach with a
pretrain-then-finetune paradigm to scale up both data and
model, achieving state-of-the-art zero-shot performance.
â€¢ We experimentally demonstrate that pre-training on large-
scale unlabeled motion data improves the generation re-
sults from diverse and open-vocabulary texts.
â€¢ We propose a fine-tuning scheme for text conditioning,
utilizing a mixture of controllers to effectively improve
the alignment between text and motion.
2. Related Work
Conditional Motion Synthesis. Being able to generate re-
alistic and contextually relevant motion sequences based
on various types of conditions, conditional motion synthe-
sis has received increasing attention in the field of mo-
tion generation. Common types of conditions include
text [3, 4,12,20,34,41,43,56,66,76,84], action [18, 55],
music [2, 35,37,39], speech [5, 6,22,92], control sig-
nals [54, 71,73,81], action labels [18, 55,82], incom-
plete motion sequences [14, 23], images [11, 25,40,60,61]
and scene [91]. The advent of Diffusion Models [26, 70]
has given a big boost to text-driven motion synthesis. Kim
et al. [34] develops a transformer-based diffusion model
which could generate and edit motions well aligned with
the given text. Motion Diffusion Model (MDM) [77] com-
bines insights already well established in the motion gener-
ation domain and in each diffusion step predicts the sample
rather than the noise. Motion Latent-based Diffusion model
(MLD) [12] performs a diffusion process on the motion la-
tent space. Besides the diffusion model, T2M-GPT [87]
investigates a framework that combines VQ-V AE [79] and
autoregressive model. However, since these models are
trained only based on paired text-motion datasets such as
HumanML3D [20], they cannot well handle unseen text
prompts.
Open-vocabulary Generation. Instead of relying on pre-
existing data, zero-shot text-driven generation leverages
general knowledge learned during training to create novel
content from text prompts. Reed et al. [59] uses DC-GAN
architecture to steer for zero-shot text-to-image synthesis.
CLIP [57], pre-trained on four hundred million image-text
pairs, possesses the remarkable ability to effectively com-
prehend and generate meaningful associations between im-
ages and text. With CLIPâ€™s strong ability, many works
are able to generate high-quality zero-shot text-driven im-
ages [17, 51,83] or 3D objects [31, 32,49,53,65,80]. In
the motion synthesis field, some works have attempted to
investigate open-vocabulary text-to-motion generation and
achieved good results. MotionCLIP [76] utilizes a decoder
to decode the CLIP motion embeddings. AvatarCLIP [28]
synthesizes a key pose and then retrieves the closest mo-
tion from the database. Lin et al. [42] pre-train a motion
483
â€¦ â€¦
ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡
1B parameters
Diffusion
Transformer
t
-
step 
diffusionMotion Diffusion Pre-training
Large
 -
scale 
Unlabeled Motion DataZero-shot Inference
Diffusion
Transformer
t
-
step 
diffusion
ttt
retarget
ğ‘¡ğ‘¡ â† ğ‘‡ğ‘‡ğ‘¡ğ‘¡ â† ğ‘¡ğ‘¡ âˆ’ 1ifğ‘¡ğ‘¡= 1
â€œGollum jumps forward.â€
CLIP
Motion
ControlNet 
x(ğ‘¡ğ‘¡)ï¿½x(0)
xâ„’
ï¿½x(0)
x(ğ‘¡ğ‘¡)ğ‘¡ğ‘¡~ğ’°ğ’°1,ğ‘‡ğ‘‡Overview
3D asset
x(ğ‘¡ğ‘¡)
â€¦ â€¦
ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡
ï¿½x(0)xâ„’
â€¦ â€¦
â€œHuman jumps forward.â€
CLIP
Motion 
ControlNet 
Motion ControlNet Training
Diffusion
TransformerFigure 2. Method overview. We train our OMG model in two stages. First, we leverage large-scale unlabeled motion data to pre-
train an unconditional diffusion model with up to 1B parameters (Sec. 3.1). Then, we adopt a conditional fine-tuning scheme called motion
ControlNet to condition the pre-trained diffusion model on text prompts (Sec. 3.2). During inference, the pre-trained unconditional denoiser
and the fine-tuned conditional denoiser are combined with classifier-free guidance, generating realistic motions with zero-shot text inputs.
generator that learns to reconstruct the full motion from the
masked motion with the key pose. Make-An-Animation [8]
pre-trains generative model with diverse in-the-wild text-
pose pair. However, since these approaches are based on
text-pose alignment, the generated motion is deficient in re-
alism due to a lack of global rotation and translation. Re-
cently, MotionGPT [33] pre-trains and fine-tunes the large
language model with tokenized motion and text data. How-
ever, it still struggles to generate novel motion from open-
vocabulary text prompts.
Mixture-of-Experts. In the deep learning field, Mixture-
of-Experts (MoE) [16, 30,67] in a neural network architec-
ture divides specific model parameters into distinct groups,
referred to as â€œexpertsâ€. Eigen et al. [15] use a different gat-
ing network at each layer in a multi-layer network, increas-
ing the number of effective experts by introducing an expo-
nential number of paths through different combinations of
experts at each layer. Other research that regards the entire
model as an expert [38] also achieves good results. More-
over, endeavors are underway to enhance the training and
inference speed within the MoE framework [24, 36]. The
MoE paradigm is applicable not only to language process-
ing tasks but also extends to visual models [62], Mixture-
of-Modality-Experts in multi-modal transformers [68], as
well as motion synthesis. Holden et al. [27] develop phase-
functioned experts blended by pre-defined blending weights
to control character at a specific phase, and Starke et al.
[72, 73], Zhang et al. [85] uses a gating network to pre-
dict the blending weights, achieving impressive results of
character control. Inspired by them, we devise text-token-
specific experts to control the corresponding sub-motions.3. Methods
We aim to generate realistic and diverse human motions that
are conditioned on text prompts, which capture complex
and abstract motion characteristics with zero-shot open-
vocabulary descriptions. To this end, we adopt pretrain-
then-finetune paradigm to enhance the capability of our
model. For the unconditional denoiser (Fig. 2left), we
leverage a large-scale unlabeled motion dataset for motion
diffusion pre-training and scale the model size up (Sec. 3.1).
For the conditional denoiser (Fig. 2middle), we devise
a specific conditional fine-tuning scheme called motion
ControlNet, including a novel conditioning design called
Mixture-of-Controllers, to exploit the zero-shot capability
of the CLIP text embeddings (Sec. 3.2). During inference
(Fig. 2right), we further use classifier-free guidance to com-
bine the unconditional denoiser and the conditional one.
3.1. Motion Diffusion Pre-training
Large-scale Unlabeled Motion Dataset. To facilitate
large-scale pre-training in the motion domain, we collected
a large amount of high-quality human motion data total-
ing over 20M frames from various sources, such as char-
acter animation datasets, marker-based, IMU-based, and
multi-view markerless mocap datasets. We then standard-
ize the frame rate and re-target them to the unified paramet-
ric skeleton of SMPL body model [45] using off-the-shelf
re-targeting algorithms [86]. After that, following the pre-
vious successful works [19, 77] on motion generation, we
enrich the SMPL skeletons with an over-parameterized mo-
tion representation that facilitates network learning.
484
Model Name nlayers dmodel nheads dhead nparams
OMG-Base 8 1024 8 128 88M
OMG-Large 12 1280 10 128 201M
OMG-Huge 16 1664 13 128 405M
OMG-Giant 24 2048 16 128 1B
Table 1. Sizes and architectures of our 4 models.
Model Scaling Up. We design our pre-trained diffusion
model with minimalism, only preserving the essential mod-
ules that are scalable and efficient to adapt to expanding
data. To this end, we adopt the Diffusion Transformer
(DiT) [52] architecture as our network backbone, due to
its scalability and impressive performance with increasing
input tokens and training data. The only difference is that
we use rotary positional embedding [74] to encourage the
model to capture the relative temporal relations among the
frames. To study the dependence of performance on model
size, we train 4 different sizes of our OMG model, rang-
ing from 88 million parameters to 1 billion parameters.
Tab. 1 shows the configuration details of our 4 models. Here
nlayers is the total number of layers, dmodel is the number
of units in each bottleneck layer (we always have the feed-
forward layer two times the size of the bottleneck layer, dff
= 2dmodel ), and dhead is the dimension of each attention
head, and nparams is the total number of trainable parame-
ters.
Training. We train our unconditional denoiser Duto pre-
dict the clean motion Ë† x(0)given diffusion timestep twith
the simple objective:
Lsimple =Ex,t,Ïµ[Î»t||xâˆ’ Du(x(t), t)||2
2], (1)
where x(t)=x+ÏƒtÏµist-step noised motion, noise Ïµâˆ¼
N(0,I),Î»tis the loss weighting factor. Furthermore, we
introduce geometric losses, analogous to MDM [77], that
contain velocity loss Lvel, and foot contact loss Lfoot to
enforce physical properties and anti-sliding. Overall, the
unconditional denoiser Duis thus trained with the following
objective:
L=Lsimple +Î»velLvel+Î»footLfoot. (2)
In our experiments, Î»vel= 30 andÎ»foot= 30 . The
number of diffusion timesteps Tis set to 1,000 with co-
sine noise level schedule [50]. We train all sizes of the
model for 1M iterations with a batch size of 256using the
AdamW [46] optimizer with a weight decay of 0, a maxi-
mum learning rate of 10âˆ’4, and a cosine LR schedule with
10K linear warmup steps. The models are trained using
Pytorch with ZeRO [58] memory redundancy strategy on
8 NVIDIA A100 GPUs. The largest model OMG-G takes
1500 GPU hours for pre-training.
ğğ(0)
ğğ(2)Gollum jumps forward
expert poolâ€¦â€¦
ğğ1
ğğ(ğ’Šğ’Š)=ï¿½
ğ‘—ğ‘—=1ğ¾ğ¾
ğğğ‘—ğ‘—(ğ‘–ğ‘–)ğğğ‘—ğ‘—ğğ2
ğğ3 ğğ4
attention maskassigning <eos > token
Ada-IN
ğğ(1)
GateNetMixture of Controllers
ğğ(2)
ğğ(1)
ğğ(0)
down zero conv
transformer
layer
hğ‘™ğ‘™
trainable 
copy
Motion ControlNet 
cross 
attention
up zero convhğ‘™ğ‘™+1text tokensğ¡ğ¡ğ‘™ğ‘™+1â€²
ğ¡ğ¡ğ‘™ğ‘™+1â€²ğ«ğ«
ğŸğŸ ğŸğŸâ€²Figure 3. Motion ControlNet (top) freezes the parameters of the
pre-trained transformer layer and combines a trainable copy of the
layer with the proposed Mixture-of-Controllers (bottom) block.
The MoC block first fuses the text features and motion features
and simultaneously determines the sub-motion ranges for each text
token with the cross-attention mechanism. Then it performs fine-
grained control of sub-motions with text-token-specific experts.
Sliding Random Windows. During training, we propose
sliding random windows that iterate over each motion state
frame xiin the motion dataset to sample motion sequences
starting from xi. Specifically, for the ithframe, we ran-
domly crop a sub-sequence xof length lwith a random
window [i, i+l), where lâˆ¼ U[1, L]is a uniform variable,
andLis the hyper-parameter that determines the maximum
length of the generated motion sequence of our model. This
pushes the model to learn the relations among both temporal
frames and spatial features of a single motion state to pro-
cess arbitrary lengths of motions even a single keyframe, to
facilitate the following fine-grained control of sub-motions
of arbitrary lengths. In our experiments, for balancing res-
olution and duration, the maximum length Lof sliding ran-
dom windows is set to 300with the framerate set to 30.
485
A graceful dancer, moves in ballet steps with rhythm. A powerful boxer , unleashes strikes with his fists while dodging. Helen playing violin with a soothing melody.Snatch workout Zombie Monkey
A graceful 
 dancer, moves in ballet steps 
 with rhythm.
 A powerful 
 boxer
 , unleashes 
 strikes with his fists 
 while dodging.
 Helen 
 playing violin 
 with a soothing melody.
Snatch workout
 Zombie
 Monkey
Figure 4. Qualitative results generated by our model given various unseen text prompts. Our model effectively captures the motion
characteristics from either a single phrase or longer natural sentences.
3.2. Motion ControlNet
To incorporate the text prompts c={ci}n
i=1into the pre-
trained motion model, We adopt a fine-tuning scheme we
call motion ControlNet to condition on text prompts, in-
spired by image ControlNet [88], including a trainable copy
of original transformer layers and the proposed condition-
ing blocks called Mixture-of-Controllers (MoC).
As illustrated in Fig. 3 top, we freeze the parameters of
the pre-trained transformer layer and combine a trainable
copy of the layer with a MoC block that injects nresiduals
corresponding to ntext tokens into the output of the original
transformer layer. Specifically, the frozen parameters retain
the pre-trained denoising ability, while the trainable copy
reuses the large-scale pre-trained model as a strong initial
backbone to learn to extract semantic motion features from
intermediate representation hlof the last layer. And based
on that, the MoC block is employed to predict the ncon-
ditional residuals rcorresponding to ntext tokens, all of
which are then added to the original output hl+1. To do so,
a pre-trained CLIP text encoder E(c)is employed to extract
the text token embeddings.
Mixture-of-Controllers. In the Mixture-of-Controllers
block (Fig. 3 bottom), our key idea is to control the sub-
motion sequences separately with different expert con-
trollers in an MoE fashion [16, 67], so as to better align
them to the corresponding text token embeddings in CLIP
space. For instance, for the given text prompt â€œGollum
jumps forwardâ€ , the â€œGollumâ€ token corresponds to the
entire sequence while â€œjumpsâ€ corresponds only to the sub-
sequence when he is jumping.
To this end, we introduce two cooperative designs. The
first one is a cross-attention mechanism to fuse the text
features and motion features and simultaneously determine
the sub-motion ranges for each text token. The second one
is the text-token-specific experts selected from an expert
pool to perform fine-grained control of sub-motions.
For the cross-attention mechanism , a sequence of mo-tion features fâˆˆRlÃ—dmis input into a cross-attention layer
along with text embeddings E(c)âˆˆRnÃ—dc, where lde-
notes the length of the sequence, dmrepresents the dimen-
sion of the motion features, ndenotes the number of the
text tokens, and dcrepresents the dimension of text embed-
ding. The cross-attention layer projects fto a query ma-
trixQ=fÂ·WQ
m, where WQ
mâˆˆRdmÃ—dmis the motion
projection matrix. And E(c)is projected to a key matrix
K=E(c)WK
cand a value matrix V=E(c)WV
c, where
WK
c, WV
câˆˆRdcÃ—dmare the text projection matrices. Then
the text values Vare distributed into the motion sequence
with the attention mechanism:
fâ€²=f+softmax (QKT
âˆš
d)V. (3)
Simultaneously an attention map between text tokens
and motion sequence is produced, denoted by A=
softmax (QKT
âˆš
d)âˆˆRlÃ—n. Within this attention map, the
ithcolumn Aâˆ—,iindicates a correspondence between a sub-
motion sequence and the ithtext token. Besides, we em-
ploy the adaptive instance normalization (Ada-IN) condi-
tioning on âŸ¨eosâŸ©(end of sequence) token to normalize fâ€²
before sending them to the experts. Since CLIP âŸ¨eosâŸ©token
summarizes the entire text, we intuitively use it to unify the
distribution of the entire motion sequence.
For the text-token-specific experts , we define an expert
controller corresponding to ithtext token E(ci)as a two-
layer feed-forward network (denoted by F) with parame-
terse(i)={W0âˆˆRdmÃ—2dm,W1âˆˆR2dmÃ—dm,b0âˆˆ
R2dm,b1âˆˆRdm}. Furthermore, the parameters e(i)are
generated by blending Kexpert parameters {e1, ...,eK}in
an expert pool, each of which is in a form of the same pa-
rameter configuration:
e(i)=KX
jÏ‰(i)
jej, (4)
486
He performs spin kick with taekwondo skills. Snatch workoutMotionCLIP MotionDiffuse MDM MLD T2M -GPT MotionGPT OMG (ours)
Gollum jumps forward
T2M
 MotionGPT
He performs 
 spin kick 
 with 
 taekwondo
 skills.
 Snatch workout
 Gollum jumps forward
Figure 5. Qualitative comparison. Our method can generate high-quality human motions that better align with text prompts than previous
state-of-the-art methods.
where jindicates the index of expert pool, Kis expert pool
size that can be adjusted, and Ï‰(i)={Ï‰(i)
j}K
j=1âˆˆRKis
the blending weights vector controlled by the text token ci
with a three-layer fully-connected gating network G:
Ï‰(i)=softmax (G(E(ci))). (5)
Then, the nexperts take motion features fâ€²as input and
output nconditional masked residuals r={ri}n
i=1:
ri=Mâˆ—,iâ—¦ F(fâ€²|e(i)), (6)
where Mâˆ—,i=sigmoid (Î³(Aâˆ—,iâˆ’Î²max(Aâˆ—,i)))is an at-
tention mask ranging from 0 to 1, and Î³andÎ²are hyper-
parameters to control the sharpness and threshold respec-
tively.
Additionally, a down-projection and up-projection 1-D
convolution pair is used to process input and output respec-
tively to reduce the control latent dimension and thus the
trainable parameters. Moreover, similar to image Control-
Net [88], we use zero-initialized convolution parameters to
protect the trainable copy from the harmful gradient noises
in the initial training steps.Training and Inference. The conditional denoiser Dcis
also trained using the same objective L(Eq. (2)) with train-
able parameters in motion ControlNet. In our experiments,
we employ the frozen CLIP-VIT-L/14 [57] text encoder to
extract text embeddings at the final layer normalization with
dimension dc= 768 and we truncate the text tokens with
the maximum token number 77. We set the down-projection
latent dimension dmof MoC blocks to 256, the expert pool
sizeK= 12 , and attention mask sharpness Î³= 24 and
threshold Î²= 0.25. We train all the motion ControlNets
for500epochs with a batch size of 64using the AdamW
optimizer with a weight decay of 1Ã—10âˆ’5, a maximum
learning rate of 3Ã—10âˆ’5, and a cosine LR schedule with
1K linear warmup steps. In the training process, we ran-
domly replace âŸ¨eosâŸ©token with the empty token with 50%
probability to increase the ability to capture global seman-
tics in text tokens as a replacement.
During inference, classifier-free guidance is used to com-
bine unconditional denoiser and conditional denoiser:
Ë†x(0)= (1âˆ’s)Â· Du(x(t), t) +sÂ· Dc(x(t), t,c). (7)
In our experiments, DDIM [69] sampling strategy with 200
timesteps is employed and the guidance strength s= 4.5.
487
MethodsHumanML3D [
20] Mixamo [1] (zero-shot)
FIDâ†“ R-Precisionâ†‘ Div
ersityâ†’ FIDâ†“ CLIP-scoreâ†‘ Diversity â†’
Real 0.002Â±.0000.797Â±.0029.503Â±.0650.106Â±.0030.648Â±.0012.665Â±.022
MotionCLIP [
76] - - - 2.542Â±.0120.511Â±.0042.205Â±.012
MAA [8] 0.774Â±.0020.676Â±.0018.230Â±.064- - -
MotionDiffuse [89] 0.630Â±.0010.782Â±.0019.410Â±.0492.363Â±.0100.505Â±.0022.411Â±.016
MDM [
77] 0.544Â±.0440.611Â±.0079.559Â±.0861.297Â±.0040.536Â±.0042.594Â±.011
MLD [
12] 0.473Â±.0130.772Â±.0029.724Â±.0821.229Â±.0040.556Â±.0032.583Â±.018
T2M-GPT [
87] 0.116Â±.0040.775Â±.0029.844Â±.0951.420Â±.0030.541Â±.0022.590Â±.022
MotionGPT [33] 0.232Â±.0080.778Â±.0029.528Â±.0711.365Â±.0030.552Â±.0022.589Â±.018
OMG (ours) 0
.381Â±.0080.784Â±.0029.657Â±.0851.164Â±.0090.588Â±.0022.632Â±.021
Table 2. Comparison of text-to-motion generation on Hu-
manML3D [20] and Mixamo [1] test set. We ran all the evalua-
tions 20 times, with the average reported alongside a 95% confi-
dence interval. The right arrow â†’means the closer to real motion
the better. Bold and underline indicate the best and the second
best result. The term (Zero-shot) implies that the dataset contains
unseen open-vocabulary texts.
4. Experiments
We show qualitative results in Fig. 4. Our method enables
fine-grained control of complex and abstract motion trait
descriptions. We encourage the reader to appreciate more
qualitative results provided in supplements and the video.
4.1. Datasets and Evaluation Metrics
Training Datasets. At the pre-training stage, we utilize
various publicly available human motion datasets, such
as artist-created datasets [23, 48], marker-based [7, 29,
44,47,75], IMU-based [40, 78] and multi-view marker-
less [10, 39,41,90] motion capture datasets, totaling over
20 million frames. In the subsequent conditional fine-tuning
stage, we train our motion ControlNet using the text-motion
HumanML3D [20] dataset, for fair comparisons with previ-
ous methods.
Evaluation Datasets. We test on two benchmarks, the
HumanML3D [20] and the Mixamo [1] test set. The Hu-
manML3D test set evaluates the in-domain performance.
And the Mixamo dataset consists of abundant artist-created
animations and human-annotated descriptions, offering a
wide variety of diverse and dynamic motions, utilized to
compare the zero-shot performance across the domains.
Evaluation Metrics are summarized as four parts. (1)
Frechet Inception Distance (FID) is our principal metric to
evaluate the feature distributions between the generated and
real motions. (2) Motion-retrieval precision (R-Precision)
calculates the text and motion Top 3 matching accuracy un-
der feature space. (3) CLIP-score. Borrowing from text-to-
image synthesis [64], we use CLIP-score to evaluate zero-
shot text-motion consistency by measuring cosine similar-
ity in CLIP space. To do so, we train a motion encoder
using the training set of HumanML3D and Mixamo to ex-
tract motion features that are aligned to text embeddings in
CLIP space as same as [76]. (4) Diversity is assessed by
calculating variance through features.
Figure 6. Quantitative evaluation on pre-training, model size, and
expert pool size. (a) Models wpre-training show consistently
improved performance over w/opre-training, and wpre-training
models, which benefit from large-scale motion data, improve with
increasing model size. (b) Larger expert pool sizes improve the
performance.
4.2. Comparison
We compare our approach with various state-of-the-art
methods. Specifically, we apply conditional motion diffu-
sion models, including MDM [77], MLD [12], MotionDif-
fuse [89], V AE-based model MotionCLIP [76], and auto-
regressive model T2M-GPT [87]. Besides, we also apply
MAA [8] based on the text-pose alignment method, and
MotionGPT [33] utilizing motion-language pre-training.
Some results on HumanML3D are borrowed from their own
paper. The quantitative results are presented in Tab. 2.
Our method demonstrates the best text-to-motion align-
ment (R-Precision) in the in-domain evaluation. Moreover,
among all diffusion-based methods, our model achieves the
best FID. In terms of zero-shot performance, our approach
achieves the best FID and CLIP-score, outperforming pre-
vious methods. This suggests a superior capability for high-
quality motion generation and effective matching with zero-
shot text prompts. As depicted in Fig. 5of the qualitative
results, our model demonstrates the capability to generate
motions that show more realism, and better alignment with
each motion characteristic description, whether in the sen-
tence or phrase.
4.3. Ablation Study
To examine the specific contributions of our novel OMG
model architecture, we conduct a series of ablation stud-
ies focusing on the roles of pre-training, model scale, and
multiple expert controllers. Additionally, we undertake an
in-depth analysis of our MoC Block. These evaluations uti-
lize the out-of-domain dataset Mixamo [1], providing a ro-
bust testbed to ascertain the effectiveness of the technical
designs on the zero-shot performance of our model.
Effect of Pre-training and Model Scale. Our investigation
into the impact of pre-training involves training several vari-
488
Methods FIDâ†“ CLIP-score â†‘Di
versity â†’
Real 0.106Â±.0030.648Â±.0012.665Â±.022
(1)Cross-attn
+ FFN 1.252Â±.0060.552Â±.0032.576Â±.025
(2) w/o Zero Conv 1.339Â±.0050.535Â±.0042.695Â±.014
(3) w/o Attention Mask 1.246Â±.0080.557Â±.0032.647Â±.026
ours (complete) 1.164Â±
.0090.588Â±.0022.632Â±.021
Table 3. Quantitative evaluation on MoC block. The damping
performance of the three variants of our model highlights the ef-
fectiveness of our MoC block technical designs.
ant models without pre-training (w/o pre-training) across
four distinct scales (see Tab. 1). These models are then
compared against counterparts with the pre-training process
(wpre-training). As illustrated in Fig. 6a,wpre-training
consistently outperforms w/opre-training, achieving lower
FID across all sizes. This indicates that our modelâ€™s per-
formance on zero-shot motion generation is effectively en-
hanced through pre-training. Meanwhile, we observe that
the performance of wpre-training improves with increas-
ing model size. Qualitative analysis, as shown in Fig. 7a, re-
veals that the â€œGollumâ€ characteristic of the generated mo-
tions becomes more pronounced with larger models. This
suggests that larger model sizes enhance the overall quality
and alignment of the generated motions.
Effect of Expert Pool Size. To systematically analyze the
impact of multiple experts, we train four distinct variants
of the OMG model, sweeping over different expert pool
sizes (K = 1,4,8, and 12respectively). The results, as de-
picted in Fig. 6b, demonstrate a discernible decrease in FID
as expert pool size increases. This trend suggests that the
larger expert hypothesis space enhances the expertsâ€™ ability
to align sub-motions with their respective text embeddings.
Evaluation of the MoC Block. In order to assess the ef-
fectiveness of our proposed MoC block, we explore its per-
formance through several variant configurations: (1)Cross-
attn + FFN, where the architecture is pruned to include
only a cross-attention layer and a feed-forward network; (2)
w/o Zero Conv, substituting zero convolutions with stan-
dard convolutional layers initialized using Gaussian distri-
butions; and (3)w/o Attention Mask, omitting the mul-
tiplication of attention masks with the output of the ex-
pert layer. These variant models are rigorously compared
against our original OMG model. Quantitative results, as
shown in Tab. 3, demonstrate that the exclusion of specific
components within the MoC Block leads to a worse result in
both the FID and CLIP-score. This observation underscores
the integral contributions of these components to the mo-
tion generation process. Qualitative analysis, presented in
Fig.7b, indicates noticeable deficiencies in the motion gen-
eration of all three variants: variant (1)is not able to clearly
depict â€œspinâ€ motions; variant (2)exhibits less expressive
He performs spin kick with taekwondo skills Gollum jumps forwardours( 3 )( 2 )( 1 )
(a) (b)
OMG -G OMG -H OMG -L OMG -BFigure 7. Qualitative evaluation on model sizes (a) and MoC block
(b). Models with larger sizes effectively comprehend richer out-
of-domain motion features to present better motion expressions.
Besides, our technical designs effectively improve the alignment
with the input texts.
motion features; and variant (3)inadequately captures the
â€œkickâ€ motion. In contrast, our OMG model demonstrates
a superior ability to more accurately align motion with the
text prompt.
5. Conclusion
In this paper, we present a novel text-to-motion generation
framework, OMG, that combines the advantages of condi-
tional generative models and text-pose alignment methods.
It carefully tailors pretrain-then-finetune paradigm into text-
to-motion generation. The pre-training stage leverages a
large amount of unlabeled motion data to train a powerful
unconditional diffusion model that ensures the realism and
diversity of the generated motions. The fine-tuning stage
introduces motion ControlNet including the proposed novel
text conditioning block called Mixture-of-Controllers. With
the cross-attention mechanism and text-token-specific ex-
perts, it adaptively aligns the sub-motion features to the text
embeddings in the CLIP space in an MoE fashion. The ex-
tensive experiments demonstrate that OMG achieves state-
of-the-art zero-shot performance on text-to-motion gener-
ation. We believe it is a significant step towards open-
vocabulary motion generation of human characters, with
wide potential applications in movies, games, robotics, and
VR/AR.
Acknowledgement
This work was supported by National Key R&D Program of
China (2022YFF0902301), Shanghai Local college capac-
ity building program (22010502800). We also acknowledge
support from Shanghai Frontiers Science Center of Human-
centered Artificial Intelligence (ShangHAI).
489
References
[1] Adobe. https://www.mixamo.com/. Accessed:2023-
9-28. 2,7
[2] Gunjan Aggarwal and Devi Parikh. Dance2music: Au-
tomatic dance-driven music generation. arXiv preprint
arXiv:2107.06252, 2021. 2
[3] Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and
Songhwai Oh. Text2action: Generative adversarial synthesis
from language to action. In 2018 IEEE International Confer-
ence on Robotics and Automation (ICRA), pages 5915â€“5920.
IEEE, 2018. 2
[4] Chaitanya Ahuja and Louis-Philippe Morency. Lan-
guage2pose: Natural language grounded pose forecasting.
In2019 International Conference on 3D Vision (3DV), pages
719â€“728. IEEE, 2019. 2
[5] Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, and
Libin Liu. Rhythmic gesticulator: Rhythm-aware co-speech
gesture synthesis with hierarchical neural embeddings. ACM
Transactions on Graphics (TOG), 41(6):1â€“19, 2022. 2
[6] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:
Gesture diffusion model with clip latents. ACM Transactions
on Graphics (TOG), 42(4):1â€“18, 2023. 2
[7] Joao Pedro Ara Â´ujo, Jiaman Li, Karthik Vetrivel, Rishi Agar-
wal, Jiajun Wu, Deepak Gopinath, Alexander William Clegg,
and Karen Liu. Circle: Capture in rich contextual environ-
ments. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 21211â€“21221,
2023. 7
[8] Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh,
and Sonal Gupta. Make-an-animation: Large-scale text-
conditional 3d human motion generation. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 15039â€“15048, 2023. 1,3,7
[9] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue,
Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luh-
man, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya
Ramesh. Video generation models as world simulators.
2024. 1
[10] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao
Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang
Pan, et al. Humman: Multi-modal 4d human dataset for ver-
satile sensing and modeling. In European Conference on
Computer Vision, pages 557â€“577. Springer, 2022. 7
[11] Xin Chen, Zhuo Su, Lingbo Yang, Pei Cheng, Lan Xu,
Bin Fu, and Gang Yu. Learning variational motion
prior for video-based motion capture. arXiv preprint
arXiv:2210.15134, 2022. 2
[12] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, and Gang Yu. Executing your commands via motion
diffusion in latent space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 18000â€“18010, 2023. 1,2,7
[13] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav
Golyanik, and Christian Theobalt. Mofusion: A framework
for denoising-diffusion-based motion synthesis. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 9760â€“9770, 2023. 1[14] Yinglin Duan, Tianyang Shi, Zhengxia Zou, Yenan Lin,
Zhehui Qian, Bohan Zhang, and Yi Yuan. Single-
shot motion completion with transformer. arXiv preprint
arXiv:2103.00776, 2021. 2
[15] David Eigen, Marcâ€™Aurelio Ranzato, and Ilya Sutskever.
Learning factored representations in a deep mixture of ex-
perts. arXiv preprint arXiv:1312.4314, 2013. 3
[16] William Fedus, Barret Zoph, and Noam Shazeer. Switch
transformers: Scaling to trillion parameter models with sim-
ple and efficient sparsity. The Journal of Machine Learning
Research, 23(1):5232â€“5270, 2022. 2,3,5
[17] Kevin Frans, Lisa Soros, and Olaf Witkowski. Clipdraw:
Exploring text-to-drawing synthesis through language-image
encoders. Advances in Neural Information Processing Sys-
tems, 35:5207â€“5218, 2022. 2
[18] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tion2motion: Conditioned generation of 3d human motions.
InProceedings of the 28th ACM International Conference on
Multimedia, pages 2021â€“2029, 2020. 2
[19] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5152â€“5161, 2022. 1,2,3
[20] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 5152â€“5161, 2022. 2,7
[21] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang,
Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin,
and Bo Dai. Animatediff: Animate your personalized text-
to-image diffusion models without specific tuning. In The
Twelfth International Conference on Learning Representa-
tions, 2023. 1
[22] Ikhsanul Habibie, Mohamed Elgharib, Kripasindhu Sarkar,
Ahsan Abdullah, Simbarashe Nyatsanga, Michael Neff, and
Christian Theobalt. A motion matching-based framework
for controllable gesture synthesis from speech. In ACM SIG-
GRAPH 2022 Conference Proceedings, pages 1â€“9, 2022. 2
[23] F Â´elix G Harvey, Mike Yurick, Derek Nowrouzezahrai, and
Christopher Pal. Robust motion in-betweening. ACM Trans-
actions on Graphics (TOG), 39(4):60â€“1, 2020. 2,7
[24] Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong
Zhai, and Jie Tang. Fastmoe: A fast mixture-of-expert train-
ing system. arXiv preprint arXiv:2103.13262, 2021. 3
[25] Yannan He, Anqi Pang, Xin Chen, Han Liang, Minye Wu,
Yuexin Ma, and Lan Xu. Challencap: Monocular 3d capture
of challenging human performances using multi-modal refer-
ences. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 11400â€“11411,
2021. 2
[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. Advances in neural information
processing systems, 33:6840â€“6851, 2020. 2
490
[27] Daniel Holden, Taku Komura, and Jun Saito. Phase-
functioned neural networks for character control. ACM
Transactions on Graphics (TOG), 36(4):1â€“13, 2017. 3
[28] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang
Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-
driven generation and animation of 3d avatars. ACM Trans-
actions on Graphics (TOG), 41(4):1â€“19, 2022. 1,2
[29] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3. 6m: Large scale datasets and pre-
dictive methods for 3d human sensing in natural environ-
ments. IEEE transactions on pattern analysis and machine
intelligence, 36(7):1325â€“1339, 2013. 7
[30] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and
Geoffrey E Hinton. Adaptive mixtures of local experts. Neu-
ral computation, 3(1):79â€“87, 1991. 3
[31] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter
Abbeel, and Ben Poole. Zero-shot text-guided object genera-
tion with dream fields. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
867â€“876, 2022. 2
[32] Nikolay Jetchev. Clipmatrix: Text-controlled creation of 3d
textured meshes. arXiv preprint arXiv:2109.12922, 2021. 2
[33] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao
Chen. Motiongpt: Human motion as a foreign language. Ad-
vances in Neural Information Processing Systems, 36, 2024.
1,3,7
[34] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-
form language-based motion synthesis & editing. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence,
pages 8255â€“8263, 2023. 2
[35] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun
Wang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz.
Dancing to music. Advances in neural information process-
ing systems, 32, 2019. 2
[36] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models
with conditional computation and automatic sharding. In In-
ternational Conference on Learning Representations, 2020.
3
[37] Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Dance-
former: Music conditioned 3d dance generation with para-
metric motion transformer. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, pages 1272â€“1279, 2022. 2
[38] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike
Lewis, Tim Althoff, Noah A Smith, and Luke Zettlemoyer.
Branch-train-merge: Embarrassingly parallel training of ex-
pert language models. In First Workshop on Interpolation
Regularizers and Beyond at NeurIPS 2022, 2022. 3
[39] Ruilong Li, Shan Yang, David A Ross, and Angjoo
Kanazawa. Ai choreographer: Music conditioned 3d dance
generation with aist++. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision, pages 13401â€“
13412, 2021. 2,7
[40] Han Liang, Yannan He, Chengfeng Zhao, Mutian Li, Jingya
Wang, Jingyi Yu, and Lan Xu. Hybridcap: Inertia-aid
monocular capture of challenging human motions. In Pro-ceedings of the AAAI Conference on Artificial Intelligence,
pages 1539â€“1548, 2023. 2,7
[41] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and
Lan Xu. Intergen: Diffusion-based multi-human motion
generation under complex interactions. arXiv preprint
arXiv:2304.05684, 2023. 2,7
[42] Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang
Lin, Qi Tian, and Chang-wen Chen. Being comes from
not-being: Open-vocabulary text-to-motion generation with
wordless training. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
23222â€“23231, 2023. 1,2
[43] Xiao Lin and Mohamed R Amer. Human motion modeling
using dvgans. arXiv preprint arXiv:1804.10652, 2018. 2
[44] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng,
Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. Beat:
A large-scale semantic and emotional multi-modal dataset
for conversational gestures synthesis. In European Confer-
ence on Computer Vision, pages 612â€“630. Springer, 2022.
7
[45] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. In Seminal Graphics Papers: Pushing
the Boundaries, Volume 2, pages 851â€“866. 2023. 3
[46] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
cay regularization. In International Conference on Learning
Representations, 2018. 4
[47] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black. Amass: Archive
of motion capture as surface shapes. In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 5442â€“5451, 2019. 1,7
[48] Ian Mason, Sebastian Starke, and Taku Komura. Real-time
style modelling of human locomotion via feature-wise trans-
formations and local motion phases. Proceedings of the
ACM on Computer Graphics and Interactive Techniques, 5
(1), 2022. 7
[49] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and
Rana Hanocka. Text2mesh: Text-driven neural stylization
for meshes. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 13492â€“
13502, 2022. 2
[50] Alexander Quinn Nichol and Prafulla Dhariwal. Improved
denoising diffusion probabilistic models. In International
Conference on Machine Learning, pages 8162â€“8171. PMLR,
2021. 4
[51] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,
and Dani Lischinski. Styleclip: Text-driven manipulation of
stylegan imagery. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 2085â€“2094,
2021. 2
[52] William Peebles and Saining Xie. Scalable diffusion models
with transformers. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 4195â€“4205,
2023. 2,4
[53] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan
Zhang, Qing Shuai, Xiaowei Zhou, and Hujun Bao. Ani-
matable neural radiance fields for modeling dynamic human
491
bodies. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 14314â€“14323, 2021. 2
[54] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and
Angjoo Kanazawa. Amp: Adversarial motion priors for styl-
ized physics-based character control. ACM Transactions on
Graphics (ToG), 40(4):1â€“20, 2021. 2
[55] Mathis Petrovich, Michael J Black, and G Â¨ul Varol. Action-
conditioned 3d human motion synthesis with transformer
vae. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 10985â€“10995, 2021. 2
[56] Mathis Petrovich, Michael J Black, and G Â¨ul Varol. Temos:
Generating diverse human motions from textual descriptions.
InEuropean Conference on Computer Vision, pages 480â€“
497. Springer, 2022. 1,2
[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748â€“8763. PMLR, 2021. 1,2,6
[58] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and
Yuxiong He. Zero: Memory optimizations toward training
trillion parameter models. In SC20: International Confer-
ence for High Performance Computing, Networking, Storage
and Analysis, pages 1â€“16. IEEE, 2020. 4
[59] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In International conference
on machine learning, pages 1060â€“1069. PMLR, 2016. 2
[60] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,
Srinath Sridhar, and Leonidas J Guibas. Humor: 3d human
motion model for robust pose estimation. In Proceedings of
the IEEE/CVF international conference on computer vision,
pages 11488â€“11499, 2021. 2
[61] Yiming Ren, Chengfeng Zhao, Yannan He, Peishan Cong,
Han Liang, Jingyi Yu, Lan Xu, and Yuexin Ma. Lidar-aid
inertial poser: Large-scale human motion capture by sparse
inertial and lidar sensors. IEEE Transactions on Visualiza-
tion and Computer Graphics, 29(5):2337â€“2347, 2023. 2
[62] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim
Neumann, Rodolphe Jenatton, Andr Â´e Susano Pinto, Daniel
Keysers, and Neil Houlsby. Scaling vision with sparse mix-
ture of experts. Advances in Neural Information Processing
Systems, 34:8583â€“8595, 2021. 3
[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj Â¨orn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684â€“10695, 2022. 1
[64] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. Photorealistic text-to-image diffusion models with deep
language understanding. Advances in Neural Information
Processing Systems, 35:36479â€“36494, 2022. 7
[65] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,
Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-shan. Clip-forge: Towards zero-shot text-to-shape genera-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 18603â€“18613,
2022. 2
[66] Yoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim
Bermano. Human motion diffusion as a generative prior.
InThe Twelfth International Conference on Learning Rep-
resentations, 2023. 2
[67] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-
geously large neural networks: The sparsely-gated mixture-
of-experts layer. In International Conference on Learning
Representations, 2016. 2,3,5
[68] Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt
Keutzer, and Yuxiong He. Scaling vision-language models
with sparse mixture of experts. In The 2023 Conference on
Empirical Methods in Natural Language Processing, 2023.
3
[69] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations, 2020. 6
[70] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In International Conference on Learning Represen-
tations, 2020. 2
[71] Sebastian Starke, He Zhang, Taku Komura, and Jun Saito.
Neural state machine for character-scene interactions. ACM
Trans. Graph., 38(6):209â€“1, 2019. 2
[72] Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Za-
man. Local motion phases for learning multi-contact charac-
ter movements. ACM Transactions on Graphics (TOG), 39
(4):54â€“1, 2020. 3
[73] Sebastian Starke, Ian Mason, and Taku Komura. Deepphase:
Periodic autoencoders for learning motion phase manifolds.
ACM Transactions on Graphics (TOG), 41(4):1â€“13, 2022. 2,
3
[74] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen
Bo, and Yunfeng Liu. Roformer: Enhanced transformer with
rotary position embedding. Neurocomputing, 568:127063,
2024. 4
[75] Omid Taheri, Nima Ghorbani, Michael J Black, and Dim-
itrios Tzionas. Grab: A dataset of whole-body human grasp-
ing of objects. In Computer Visionâ€“ECCV 2020: 16th Eu-
ropean Conference, Glasgow, UK, August 23â€“28, 2020, Pro-
ceedings, Part IV 16, pages 581â€“600. Springer, 2020. 7
[76] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,
and Daniel Cohen-Or. Motionclip: Exposing human motion
generation to clip space. In European Conference on Com-
puter Vision, pages 358â€“374. Springer, 2022. 1,2,7
[77] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
Cohen-or, and Amit Haim Bermano. Human motion diffu-
sion model. In The Eleventh International Conference on
Learning Representations, 2022. 1,2,3,4,7
[78] Matthew Trumble, Andrew Gilbert, Charles Malleson,
Adrian Hilton, and John Collomosse. Total capture: 3d
human pose estimation fusing video and inertial sensors.
492
InProceedings of 28th British Machine Vision Conference,
pages 1â€“13, 2017. 7
[79] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. Advances in neural information pro-
cessing systems, 30, 2017. 2
[80] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,
and Jing Liao. Clip-nerf: Text-and-image driven manip-
ulation of neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 3835â€“3844, 2022. 2
[81] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and
Huaizu Jiang. Omnicontrol: Control any joint at any time
for human motion generation. In The Twelfth International
Conference on Learning Representations, 2023. 2
[82] Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng
Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xi-
aokang Yang, et al. Actformer: A gan-based transformer
towards general action-conditioned 3d human motion gener-
ation. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 2228â€“2238, 2023. 2
[83] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue
Cao, Han Hu, and Xiang Bai. A simple baseline for open-
vocabulary semantic segmentation with pre-trained vision-
language model. In European Conference on Computer Vi-
sion, pages 736â€“753. Springer, 2022. 2
[84] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan
Kautz. Physdiff: Physics-guided human motion diffusion
model. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 16010â€“16021, 2023. 2
[85] He Zhang, Sebastian Starke, Taku Komura, and Jun Saito.
Mode-adaptive neural networks for quadruped motion con-
trol. ACM Transactions on Graphics (TOG), 37(4):1â€“11,
2018. 3
[86] Jiaxu Zhang, Junwu Weng, Di Kang, Fang Zhao, Shaoli
Huang, Xuefei Zhe, Linchao Bao, Ying Shan, Jue Wang, and
Zhigang Tu. Skinned motion retargeting with residual per-
ception of motion semantics & geometry. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 13864â€“13872, 2023. 3
[87] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli
Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi
Shen. T2m-gpt: Generating human motion from textual de-
scriptions with discrete representations. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), 2023. 1,2,7
[88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 3836â€“3847, 2023. 1,2,5,6
[89] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou
Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-
fuse: Text-driven human motion generation with diffusion
model. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2024. 1,7
[90] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein
Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Ego-
body: Human body shape and motion of interacting peoplefrom head-mounted devices. In European Conference on
Computer Vision, pages 180â€“200. Springer, 2022. 7
[91] Yan Zhang, Mohamed Hassan, Heiko Neumann, Michael J.
Black, and Siyu Tang. Generating 3d people in scenes with-
out people. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), 2020. 2
[92] Qingcheng Zhao, Pengyu Long, Qixuan Zhang, Dafei Qin,
Han Liang, Longwen Zhang, Yingliang Zhang, Jingyi Yu,
and Lan Xu. Media2face: Co-speech facial animation
generation with multi-modality guidance. arXiv preprint
arXiv:2401.15687, 2024. 2
493
