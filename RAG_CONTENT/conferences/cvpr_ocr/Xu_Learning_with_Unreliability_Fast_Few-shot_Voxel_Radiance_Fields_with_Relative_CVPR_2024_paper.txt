Learning with Unreliability: Fast Few-shot Voxel Radiance Fields
with Relative Geometric Consistency
Yingjie Xu1,2*Bangzhen Liu2*Hao Tang1,3Bailin Deng4Shengfeng He1‚Ä†
1Singapore Management University2South China University of Technology
3Nanjing University of Science and Technology4Cardiff University
(a) Input Views
 (b) Warped Images
 (c) w/o Unreliability
 (d) Ours
 (e) Comparisons on the Realistic Synthetic 360‚ó¶dataset [24]
Figure 1. We present ReV oRF, a voxel-based framework designed to capitalize on the unreliability inherent in warped novel views. (b)
demonstrates the warping outcomes, where black holes signify unmatched pixels from the original view. (c) illustrates the results of training
when these holes are masked out, which unfortunately results in ambiguous geometric structures. In contrast, (d) showcases our approach‚Äôs
ability to maintain correct geometric consistency. ReV oRF achieves this by leveraging relational depth prior knowledge within these
unreliable hole regions. Our approach demonstrates the best reconstruction quality while being one of the fastest few-shot approaches in (e).
Abstract
We propose a voxel-based optimization framework,
ReVoRF , for few-shot radiance fields that strategically ad-
dress the unreliability in pseudo novel view synthesis. Our
method pivots on the insight that relative depth relationships
within neighboring regions are more reliable than the ab-
solute color values in disoccluded areas. Consequently, we
devise a bilateral geometric consistency loss that carefully
navigates the trade-off between color fidelity and geometric
accuracy in the context of depth consistency for uncertain
regions. Moreover, we present a reliability-guided learning
strategy to discern and utilize the variable quality across syn-
thesized views, complemented by a reliability-aware voxel
smoothing algorithm that smoothens the transition between
reliable and unreliable data patches. Our approach allows
for a more nuanced use of all available data, promoting en-
hanced learning from regions previously considered unsuit-
able for high-quality reconstruction. Extensive experiments
across diverse datasets reveal that our approach attains
significant gains in efficiency and accuracy, delivering ren-
dering speeds of 3 FPS, 7 mins to train a 360‚ó¶scene, and
*The first two authors contributed equally.
‚Ä†Corresponding author ( shengfenghe@smu.edu.sg ).a 5% improvement in PSNR over existing few-shot methods.
Code is available at https://github.com/HKCLynn/ReVoRF.
1. Introduction
Neural Radiance Fields (NeRF) have revolutionized the
fields of novel view synthesis and 3D reconstruction by lever-
aging an implicit function optimized from a collection of
2D images [2, 13, 24, 27]. Despite their remarkable ren-
dering capabilities, NeRFs are hampered by the substantial
cost and time required to gather dense image sets for a given
scene [6,41,47]. This challenge has spurred the development
of Few-shot NeRF, an emerging domain focused on recon-
structing 3D scenes with minimal image data [6, 8, 12, 18].
The performance of NeRF in accurately reconstructing
geometry and texture diminishes when faced with sparse
observations, as it tends to overfit the limited views avail-
able [8, 49]. To address this issue, there has been a push in
recent studies to fortify NeRF with additional priors [46,47],
including semantic relations [14], depth cues [49], and en-
tropy constraints [17]. These enhancements strive to ex-
tract maximum information from limited data. However,
the reconstructions are inherently limited by the insufficient
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
20342
coherence of the sparse views provided.
Recent research has explored overcoming the challenges
posed by very limited observations through pseudo-view
synthesis [3, 18]. By using known camera poses and coarse
depth estimates, these methods generate warped images from
sparse viewpoints to enhance cross-view consistency. How-
ever, as shown in Fig. 1b, these generated images often
include noisy areas with artifacts, which, if used for learning,
can lead to inconsistent training signals and compromise
scene integrity. To address this, Kwak et al. [18] implement
self-occlusion aware masking to exclude unreliable regions.
While this selective masking successfully filters out areas
of uncertainty, it also introduces voids, presenting a conun-
drum: refining these images can bring in inconsistent noise
and floaters, as illustrated in Fig. 1c, yet the limited number
of usable samples necessitates using all available pseudo
supervision for quality reconstruction.
In light of the issues identified with unreliable warped
areas, our paper proposes a novel method for fully exploiting
these uncertain regions to achieve multi-view consistency
learning. The rationale behind our method is that, although
absolute supervision is not reliable in those disoccluded
regions, we observe that they maintain consistent relative
depth relationships. The unreliability of certain regions in
warped images can still bear geometric resemblances to their
original view counterparts. We find that local depth informa-
tion within these images can indicate geometric disparities,
offering a self-supervised signal that aids in discerning the
geometry of regions lacking precise textural information.
While reliable regions offer more accurate supervision, our
approach seeks to fully utilize all the information present,
exploiting depth cues in coarsely warped images to inform
the learning process across both reliable and traditionally
discarded unreliable areas.
Drawing from the insights above, we propose a novel
voxel-based optimization framework, ReV oRF, tailored for
fast and multi-view consistent reconstruction of few-shot
radiance fields, which incorporates the relative depth priors
from several aspects. The objective of this work is to con-
currently explore information from both dependable and less
reliable regions within the warped novel view images. In the
first step, we randomly warp the sparse images onto a series
of novel views, subsequently delineating reliable and unreli-
able regions based on the pixel-wise correlation between the
input and novel view. We then introduce a bilateral geometric
consistency loss to enable self-training on novel synthesized
images. This loss encompasses a reconstruction term in a
bilateral manner, including a color and density regulariza-
tion term for reliable regions and a relative depth consistency
term for unreliable regions, respectively. While the former
term aims at explicitly learning the geometric context of the
reliable regions, the relative depth regularization is applied
for implicitly exploring the geometric consistency guidedby relative depth. Moreover, we integrate unreliability into
our voxelization of scene features: 1) a reliability-guided
learning strategy that dynamically adjusts learning priorities
towards more reliable regions; 2) a reliability-aware voxel
smoothing procedure that preserves structural integrity in
reliable zones and mitigates inconsistencies in less reliable
ones, ensuring a balanced and coherent scene reconstruction.
As illustrated in Fig. 1e, assisted by both bilateral geomet-
ric consistency loss and reliability-aware regularization, our
method is the second fastest while achieving the best recon-
struction fidelity, with a large margin over the others.
Our contributions can be summarized as follows:
‚Ä¢We present the first attempt to explore pseudo-views
unreliability within few-shot radiance fields, presenting
the first framework to incorporate these areas for en-
hanced multi-view consistency learning with a bilateral
geometric consistency loss.
‚Ä¢We introduce a reliability-guided learning strategy and
voxelization smoothing procedure that tailors the learn-
ing process to the reliability of data, thus optimizing the
training emphasis for improved reconstruction quality
in few-shot radiance fields.
‚Ä¢We demonstrate superior performance of our approach
against existing state-of-the-art few-shot methods in
efficiency and accuracy, through extensive experiments
on both synthetic and real-world datasets.
2. Related Work
Neural Radiance Fields (NeRF). NeRF [2, 5, 13, 22, 24]
have emerged as a significant advancement in 3D reconstruc-
tion and novel view synthesis. These methods employ an
implicit function to represent a 3D scene, enabling the ex-
traction of detailed geometric and textural information from
a set of multi-view images. Subsequent researchers have
broadened the scope of NeRF applications, including genera-
tive modeling [11, 40], video synthesis [9, 20, 34], and scene
editing [21,48]. Despite the impressive rendering quality, the
training of vanilla NeRF often spans several days for a single
scene reconstruction. Recent advancements [10, 27, 37, 38]
have endeavored to mitigate this computational burden. Ap-
proaches such as DVGO [4,37,38] employ dense voxel grids
in conjunction with shallow multilayer perceptrons to expe-
dite the reconstruction process. Similarly, Plenoxels [10]
utilizes sparse voxel grids, and Instant-NGP [27] employs
a multi-resolution hash table to delineate the radiance field
more efficiently. Diverse from these methods, which typi-
cally require dense inputs, we aim to address the challenge
of achieving fast and high-fidelity scene reconstruction in
the case where only a few observed views are available.
Few-Shot NeRFs. Recent advances [5, 14, 17, 47, 49, 51]
have sought to reduce the dependency on densely collected
20343
data for scene reconstruction, leveraging sparse inputs and
scene priors. Notably, PixelNeRF [47] and StereoRF [7]
utilize local semantic relationships across multiple scenes,
while MVSNeRF [5] incorporates cost volume to enhance
performance. These methodologies, however, require pre-
training on numerous scenes to acquire necessary scene pri-
ors. Further developments [14,17,31,39,46] have introduced
various regularization techniques to maximize the utility of
sparse input views. InfoNeRF [17] enhances ray adjacency
consistency through entropy regularization. DietNeRF [14]
facilitates cross-view semantic consistency by harnessing the
semantic space of the pretrained CLIP [31], while DiffusioN-
eRF [44] explores the diffusion prior of pretrained diffusion
models. Additionally, FreeNeRF [46] applies frequency
regularization, and VGOS [39] introduces voxel regular-
ization to optimize both feature representation and density.
Another group of research focuses on augmenting sparse
inputs with synthetically generated views. RapNeRF [49]
utilizes geometric re-projection for novel view extrapolation,
while VmNeRF [3] employs depth maps for view-morphing.
GeCoNeRF [18] aims to refine geometric consistency by sep-
arating reliable regions from warped images and discarding
unreliable areas prone to self-occlusion. Our work diverges
from these approaches by considering the inherent informa-
tion of both reliable and unreliable regions of the novel view
images, facilitating cross-view geometric consistency.
Unreliability/Uncertainty Modeling. In the rapid devel-
opment of NeRF, the incorporation of uncertainty modeling
has become crucial for achieving robustness in 3D recon-
struction from sparse views. Previous efforts have employed
diverse strategies, including Bayesian approaches [15, 28]
and evidential neural networks [1,33], to quantify uncertainty
in neural networks. In the context of NeRF, uncertainty has
been harnessed to enhance rendering and guide input capture.
Some methods assume Gaussian noise in RGB space for
pixel-wise uncertainty [22, 30], employ volumetric entropy
for scene geometry [19, 45], or adopt variational inference
or Latent Variable Modeling for radiance field uncertainty
as seen in S-NeRF [36] and CF-NeRF [35]. However, these
approaches have not comprehensively addressed uncertainty
quantification in unseen regions. Our approach differs from
these methods by not only capturing uncertainty in the ge-
ometry and appearance of visible areas but also explicitly
accounting for unseen spaces, including occluded points,
which previous methods have not considered. This distinc-
tion allows for a more nuanced and accurate reconstruction,
promising to elevate the fidelity of few-shot NeRF models.
3. Methodology
3.1. Preliminaries
NeRF [24] represents a scene as a continuously differ-
entiable function fvia a Multi-Layer Perceptron (MLP).Given a 3D position x‚ààR3and the associated 2D view-
ing directions d‚ààR2, NeRF maps them into a volume
density œÉ‚ààRand an RGB value c‚ààR3, such that:
(c, œÉ) =f(Œ≥(x), Œ≥(d)),where the Œ≥is a positional encod-
ing that projects xanddinto a higher dimensional feature
space [42]. With a ray parameterized as rp(t) =o+tdp
cast from the camera‚Äôs optical center oalong direction dp,
the expected color ÀÜC(rp)of pixel pis rendered as follows:
ÀÜC(rp) =Ztf
tnT(t)œÉ(rp(t))c(rp(t),dp)dt, (1)
where tnandtfare the near and far bounds of the ray for
sampling, and T(t) = exp
‚àíRt
tnœÉ(r(s))ds
denotes the
cumulative transparency along the ray from tntot. There-
fore, the NeRF can be optimized by a reconstruction loss
between the rendered color ÀÜC(r)and the real color C(r):
Lrgb=X
r‚ààR‚à•ÀÜC(r)‚àíC(r)‚à•2, (2)
where Rdenotes the set of training rays.
3.2. Overview
We propose ReV oRF, a novel voxel-based optimization
framework tailored for fast and multi-view consistent scene
reconstruction from sparse input views. Our key idea is to
incorporate the unreliability information for fully exploring
the warped novel view images. By treating depth priors
as the unreliability metrics, ReV oRF facilitates the recon-
struction of the few-shot radiance field from several aspects,
including the multi-view consistency learning (Sec. 3.3) and
the regularization of voxel features (Sec. 3.4). The overall
pipeline of ReV oRF is displayed in Fig. 2.
3.3. Unreliability for Multi-view Consistency
In this section, we explore the potential of unreliability
in facilitating multi-view geometric consistency via the pro-
posed bilateral geometric consistency loss.
Novel View Warping. Starting from a set of sparse in-
put images Ii
rfori‚àà1, ..., N r, where irepresents the
view number and Nris a small number, e.g.,Nr= 3
orNr= 4, we propose to synthesize novel view images
Ii,j
s‚Üêrthrough a fast and flexible warping process on sev-
eral novel views j‚àà1, ..., N s. To preserve the cross-view
consistency, the warping is guided by a coarse depth map
Di
r, where the depth value on each pixel pis accumulated
by the density along each ray rpomitted from the camera:
Di
r(p) =Rtn
tfT(t)œÉ(rp(t))t dt. Subsequently, we obtain
the warped image Ii,j
s‚Üêrthrough a cross-view transformation
Hs‚Üêr, which deforms each pixel profIi
rto its correspond-
20344
Input view ùíì
Depth estimationWarp
Novel view ùíî
Unreliable voxelReliable voxel
Smoothing voxel
Voxel density ùùà
Voxel feature ùíáùíÑ
ùë∞ùíî‚Üêùíì
 Warped images
ùë´ùíî‚Üêùíì
 Estimated DepthRGB Regularization
Depth Smoothing Loss
Reliability -aware Voxelization Smoothing
‚áÅùë¥ùíî‚Üêùíì
 Reliability Mask
Relative Depth Regularization
Bilateral Geometric Consistency Loss 
Unreliable depth
Reliable RGB
Figure 2. Overview of our proposed ReV oRF. Specifically, we first warp the sparse images onto several novel views and determine both the
dependable and unreliable regions. Based on the dependability of each image region, we introduce a bilateral geometric consistency loss
for multi-view consistent learning, which is composed of a color and density regularization term for reliable regions and a relative depth
consistency term for unreliable regions. These two terms are responsible for explicitly learning the reliable geometric contents and implicitly
exploring the geometric consistency via the guidance of relative depth, respectively. For voxel feature regularization, we integrate the
unreliability through a reliability-guided learning strategy and a reliability-aware voxel smoothing procedure. By prioritizing the learning of
more reliable regions and mitigating the inconsistencies in less reliable ones, ReV oRF ensures a more balanced and coherent reconstruction.
ing position pson the target view:
ps‚Üêr=Hs‚Üêr(pr)
=fs‚Üêw(fw‚Üêr(pr)),(3)
fw‚Üêris a mapping matrix from the pixel coordinate of Ii
r
to the world coordinate and fs‚Üêwis the inverse operation to
the coordinate of Ii,j
s‚Üêr, such that:
fw‚Üêr(pr) =Di
r(pr)T‚àí1
rK‚àí1
rpr, (4)
fs‚Üêw(pw) =KsTs(pw), (5)
where KandTrepresent the camera‚Äôs intrinsic and extrinsic
parameter matrices in their corresponding views, respec-
tively. Since the warping function is not surjective, voids
may occur in Ii,j
s‚Üêr. We empirically obtain a binary mask
Mwarp , where the pixels of void areas are set as 1 to identify
the initial unreliable regions. To further refine the mask,
we employ the cross-view pixel correspondences within the
world coordinate of Ii
randIi,j
s‚Üêr. To achieve this, we ob-
tain the pseudo depth Dj
sof view jby rendering from the
radiance fields. Following Eq. 4, we map the pixel of Ii
r
andIi,j
s‚Üêrinto the same coordinate and obtain the correlation
mapMcorby comparing the distance between each pixelpair(pr, ps‚Üêr):
Mcor(ps‚Üêr) =(
1,‚à•fw‚Üêr(pr)‚àífw‚Üês(ps‚Üêr)‚à•2> œµ.
0,otherwise .
(6)
In this way, the final unreliability mask can be calculated as
follows:
Ms‚Üêr=Mcor‚à™Mwarp. (7)
Bilateral Geometric Consistency Loss. According to the
obtained unreliability mask Ms‚Üêr, we categorize the warped
novel view image into reliable regions Rreland unreliable
regions Runr. Subsequently, we propose a bilateral geomet-
ric consistency loss to facilitate the self-training. Since the
contents within Rrelare considered reliable, we explicitly
constrain the appearance of rendered image Ij
son view jvia
a reconstruction loss defined as:
Lrel=X
p‚ààRrel‚à•Ii,j
s‚Üêr(p)‚àíIj
s(p)‚à•2. (8)
ForRunr, we propose to leverage the relative depth prior
of the warped Ii,j
s‚Üêrto improve the geometric consistency.
Specifically, we extract the depth map Ds‚Üêrwith a powerful
pretrained depth estimation model DPT [32]. By analyzing
the semantics of the surrounding context, we could inpaint
20345
the voids occurring in Ds‚Üêr. Subsequently, a relative depth
regularization loss [23] is introduced to constrain the geomet-
ric consistency on the rendered depth Ds, which is defined
as:
Lunr=X
p‚ààRunrX
ÀÜp‚ààN(Dp
s‚Üêr)h(p,ÀÜp),(9)
where
h(p,ÀÜp) =
Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥max(|DÀÜp
s‚àíDp
s| ‚àím,0) if(DÀÜp
s‚Üêr‚àíDp
s‚Üêr)
√ó(DÀÜp
s‚àíDp
s)<0,
0 otherwise .(10)
Herepdenotes any pixel within the unreliable region Runr;
ÀÜprepresents each pixel within a neighborhood N(Dp
s‚Üêr)of
p, where N(Dp
s‚Üêr)is obtained by calculating pixels that
have close depth values with Dp
s‚Üêrin the warped depth map;
Dp
s, DÀÜp
sandDp
s‚Üêr, DÀÜp
s‚Üêrdenote the depth values of pand
ÀÜpwithin the rendered depth map Dsand estimated depth
mapDs‚Üêr, respectively. The function h(p,ÀÜp)penalizes
inconsistent relative ordering between the depth values of
pandÀÜpin the two depth maps. Specifically, if Dp
s< DÀÜp
s
butDp
s‚Üêr> DÀÜp
s‚Üêr, orDp
s> DÀÜp
sbutDp
s‚Üêr< DÀÜp
s‚Üêr, then
h(p,ÀÜp)penalizes the depth difference |DÀÜp
s‚àíDp
s|beyond
a threshold m, to prevent the depth values from shifting
dramatically.
Our bilateral geometric consistency loss is then defined
as a weighted sum of LrelandLunr:
Lbgc=ŒªrelLrel+ŒªunrLunr. (11)
This loss enables us to thoroughly explore the information
from both reliable and unreliable regions, facilitating the
learning of cross-view consistency. Note that we also apply
Eq. 9 on input views Iras a depth regularization between the
rendered depth and Drfor fully exploring the depth prior.
3.4. Unreliability for Voxel Feature Regularization
In this section, we incorporate the unreliability on regu-
larizing the feature of each position of the voxel grid. With
a proper design of reliability-aware voxel smoothing and
reliability-aware learning adjustment, we further improve
the quality of the rendered image, avoiding suboptimal scene
reconstruction.
Reliability-aware Voxel Smoothing. Employing vox-
elized feature representations [37, 38] can significantly im-
prove the training and rendering speed of NeRF, by storing
the RGB features fcand density œÉin a voxel grids. To fa-
cilitate the learning of voxel representation, DVGO [37, 38]
propose a differentiable voxel smoothing loss, which regu-
larizes the difference of fcandœÉbetween a given voxel vwith its six adjacent points Vas follows:
L(v) =X
ÀÜv‚ààV‚àÜfc(v,ÀÜv) + ‚àÜ œÉ(v,ÀÜv), (12)
where ‚àÜ(¬∑,¬∑)denotes an error metric for the difference ( e.g.,
L1,L2, or Huber loss).
However, under the few-shot scenario, learning on the
sparse input views can easily overfit the view-specific image,
which could lead to a degenerated voxel grid that may con-
tain fluctuant features. To address this issue, we propose to
regularize the voxel features for balanced and smooth learn-
ing. By taking the unreliability of each synthesized novel
view image into consideration, we mitigate the influence of
unreliable regions while promoting learning in more reliable
areas. We design a reliability-aware smooth factor œÅ(v)for
each voxel in the grid. Specifically, given a warped image,
we cast Oraysrthrough each pixel of the reliable regions
Rrel. For each voxel v, we obtain a reliability score by accu-
mulating the number of rays that pass this voxel, denoted by
S(v). The maximum number of times being passed by rays
is denoted as S(v)max. Then the reliability-aware smooth
factor is defined as œÅ(v) =S(v)
S(v)max. Finally, we formulate
the reliability-aware voxel smoothing losses on fcandœÉas:
Lfc=X
vX
ÀÜv‚ààV(1 +e‚àíœÅ(v))‚àÜfc(v,ÀÜv),
LœÉ=X
vX
ÀÜv‚ààV(1 +e‚àíœÅ(v))‚àÜœÉ(v,ÀÜv).(13)
The final regularization loss is defined as follows:
Lrs=ŒªfLfc+ŒªdLœÉ. (14)
In this case, the unreliable regions will have smoother super-
vision during training, mitigating the inconsistency caused
by potential overfitting.
Reliability-guided Learning Adjustment. To further fa-
cilitate the learning of geometric and appearance informa-
tion, we apply a reliability-guided learning strategy to dy-
namically prioritize the learning towards the reliable zones,
while eliminating the false supervision of unreliable re-
gions at the beginning of training. Concretely, we adjust
the importance of each voxel vwith a reliability weight
wv= 1 + œÅ(v), to control the gradients of each voxel dur-
ing the back-propagation.
3.5. Optimization of ReVoRF
To avoid dramatic variation of the unreliable depth, we
further adopt a depth smoothness loss function [29] as an
extra regularization for better relative depth supervision:
Lds=1
|R|X
r‚ààRX
(x,y)‚ààD‚à•d(x, y)‚àíd(x, y+ 1)‚à•2
2
+‚à•d(x, y)‚àíd(x+ 1, y)‚à•2
2,(15)
20346
Diet-NeRF [14]
 InfoNeRF [17]
 VGOS [39]
 ReV oRF (Ours)
 GT
Figure 3. 4-views reconstructions on Realistic Synthetic 360 ¬∞[26]. ReV oRF enables more consistent reconstruction with detailed appearance.
Realistic Synthetic 360¬∞ dataset
Methods PSNR ‚ÜëSSIM ‚ÜëLPIPS ‚Üì Training Time ‚Üì
NeRF [24] 15.93 0.780 0.320 2 hrs
PixelNeRF [47] 16.09 0.738 0.390 3-4 days* + 10 hrs
DietNeRF [14] 16.06 0.793 0.306 19 hrs
3DGS [16] 17.55 0.701 0.250 3 mins
InfoNeRF [17] 18.62 0.811 0.230 4 hrs
VGOS [39] 18.91 0.825 0.205 3 mins
GeCoNeRF [18] 19.78 0.880 0.185 >2 hrs
Ours 20.72 0.848 0.179 7 mins
Table 1. Quantitative comparison for 4-views setting in the Realistic
Synthetic 360‚ó¶dataset [24]. The best and the second-best results
are highlighted in bold andunderlined , respectively. (*) denotes
the time cost of pre-training.
where Rrepresents the set of rays emanating from the sam-
pled views, Drefers to the depth patch that is centered
around r, andd(x, y)is of the depth value in position (x, y).
The final objective of ReV oRF is formulated as:
Ltotal=Lrgb+Lbgc+Lrs+ŒªdsLds. (16)
4. Experiments
In this section, we demonstrate the superiority of the pro-
posed ReV oRF through extensive experiments. The details
of experiment settings are discussed in Sec. 4.1. Analysis on
comparison experiments and ablation study are performed
in Sec. 4.2 and Sec. 4.3, respectively.
4.1. Experiment Settings
Datasets. The experiments are conducted on inward-
facing scenes from the Realistic Synthetic 360 ¬∞dataset [26]
and forward-facing scenes from the LLFF dataset [25].
Realistic Synthetic 360 ¬∞comprises path-traced images
from 8 synthetic scenes, which are characterized by theircomplex geometry and realistic rendering of non-Lambertian
materials. Each scene is represented by 400 images, ren-
dered by inward-facing virtual cameras positioned at varying
viewpoints. We adhere to the protocol established by InfoN-
eRF [17] and randomly select 4 views from 100 training
images as sparse inputs. The model‚Äôs performance is then
evaluated on a set of 200 testing images.
LLFF consists of 8 real-world scenes captured with a
handheld cellphone, featuring 20 to 62 forward-facing im-
ages per scene. These scenes encompass a range of complex
environments. In line with the standard protocol [26], we
reserve 1/8 of these images for testing purposes. The remain-
ing images are used for training, from which we randomly
sample three views for input into our model.
Implementation Details. Following DVGO [37, 38], we
adopt a coarse-to-fine optimization scheme to stabilize the
training of ReV oRF and gradually improve the geometric
details. During the whole training period, we set the val-
ues of ŒªrelandŒªunrin Eq. 11 as 10‚àí1and10‚àí2, respec-
tively. The values of Œªd, Œªfin Eq. 14, and Œªdsin Eq. 15 are
set as 5¬∑10‚àí4,5¬∑10‚àí5, and 5¬∑10‚àí5in the coarse stage, and
decreased to 5¬∑10‚àí5,10‚àí5, and 5¬∑10‚àí5in the fine stage.
The warping poses collected for the Realistic Synthetic
360¬∞dataset [24] and the LLFF dataset [20] are different.
For Realistic Synthetic 360 ¬∞, we randomly vary the polar
angle Œ∏and azimuthal angle œïin the range of [5‚ó¶,10‚ó¶]
based on the input view, and subsequently warp each in-
put sparse view to its four neighboring views defined by
{(Œ∏, œï),(‚àíŒ∏, œï),(Œ∏,‚àíœï),(‚àíŒ∏,‚àíœï)}. For the LLFF dataset,
the warped views are obtained by randomly interpolating
between every adjacent input view. To speed up the train-
ing stage and improve the quality of depth supervision, the
warping is performed periodically, which updates the warped
depth maps and RGB images every 1000 training steps.
20347
(a) RegNeRF [29]
 (b) SparseNeRF [12]
 (c) VGOS [39]
 (d) ReV oRF (Ours)
 (e) GT
Figure 4. Comparisons on the LLFF dataset [25] in 3-view setting. The red and blue boxes denote compared regions. Our approach achieves
better results in reconstructing fine details with enhanced clarity. Please zoom in for details.
NeRF LLFF dataset
Methods PSNR ‚ÜëSSIM ‚ÜëLPIPS ‚Üì Training Time ‚Üì
PixelNeRF [47] 16.17 0.438 0.512 3-4 days* + 10 hrs
SRF [7] 17.07 0.436 0.529 2-3 days* + 43mins
MVSNeRF [5] 17.88 0.584 0.327 1-2 days* + 10mins
Mip-NeRF [2] 14.62 0.351 0.495 14 hrs
DietNeRF [14] 14.94 0.370 0.496 18 hrs
3DGS [16] 13.05 0.407 0.388 13 mins
RegNeRF [29] 19.08 0.587 0.336 4 hrs
VGOS [39] 19.35 0.620 0.432 5 mins
SparseNeRF [12] 19.86 0.624 0.328 >2 hrs
Ours 19.26 0.644 0.316 11 mins
Table 2. Quantitative comparison for 3-views setting on LLFF [20].
The best and the second-best results are highlighted in bold and
underlined , respectively. (*) signifies the pre-training time.
Evaluation Metrics. To assess the effectiveness of our
method, we employ several established metrics, including
PSNR (Peak Signal-to-Noise Ratio) for assessing image re-
construction accuracy, SSIM (Structural Similarity Index
Measure) [43] for evaluating changes in luminance and con-
trast that affect structural integrity, and LPIPS (Learned
Perceptual Image Patch Similarity) [50], which uses deep
learning to approximate human visual perception. These
metrics provide a comprehensive analysis of our model‚Äôs
performance, covering aspects of accuracy, perceptual qual-
ity, and structural fidelity in the reconstructed images.
4.2. Comparisons
On the Realistic Synthetic 360 ¬∞dataset [24], we compare
our method with state-of-the-art approaches, including Reg-
NeRF [29], DietNeRF [14], infoNeRF [17], VGOS [39],Pix-
elNeRF [47], and GeCoNeRF [18], in a 4-view setting. On
the LLFF dataset [20], we implement our method in a 3-
view setting and compare with SRF [7], MVSNeRF [5],
mip-NeRF [2], DietNeRF [14], RegNeRF [29],VGOS [39],
SparseNeRF [12] and GeCoNeRF [18]. We adopt the re-
ported results from VGOS [39], sparseNeRF [12], and
GeCoNeRF [18]. Besides, we also compare with the ad-
vanced reconstruction method 3DGS [16].Qualitative Experiments. Fig. 3 compares our approach
with some recent methods on the Realistic Synthetic 360 ¬∞
dataset [24]. Diet-NeRF [14] performs poorly in the setting
of 4 views, while infoNeRF [17] and VGOS [39] are inferior
to our method in terms of both geometric shapes and detail
resolution. Our approach demonstrates superior performance
in both geometry and details.
Fig. 4 shows a qualitative comparison on a scene from
the LLFF dataset [20]. While all methods can recover the
overall structure of the scene, our approach excels at the
quality of details as shown in the magnified regions. Our
method incorporates smoothness while retaining fine details,
achieving the most natural results.
Quantitative Experiments. Table 1 shows quantitative re-
sults from different methods on the Realistic Synthetic 360 ¬∞
dataset [24]. In terms of training time, our method is at least
an order of magnitude faster than all other methods except
for VGOS [39]. Although our method is slightly lower than
VGOS [39], it significantly enhances the PSNR, LPIPS [50],
and SSIM [43] of the rendered images. Our method achieves
state-of-the-art accuracy in PSNR and LPIPS [50]. Addi-
tionally, despite not utilizing a pre-trained model or per-
ceptual loss for high-level semantic information extraction,
our method still achieves the second-best performance in
perceived SSIM [43].
Table 2 shows a quantitative comparison on the LLFF
dataset [20]. Our method achieves the highest scores in both
SSIM [43] and LPIPS [50], indicating that our images exhibit
the best performance in terms of human perceptual recon-
struction. We also achieve the third-highest PSNR, which,
together with our state-of-the-art performance in SSIM [43]
and LPIPS [50], demonstrates that our approach has made
improvements in certain aspects of image rendering.
4.3. Ablation Study
Our ablation study is segmented into five distinct
groups with Table 3, with DVGO [37, 38] serving
20348
Baseline
 Baseline+ Lrel
 Baseline+ Lrel+Lunr
 Baseline+ Lrel+Lunr+Lds
 Full Model
Figure 5. Visualizations of the ablation on Chair scene from the Realistic Synthetic 360 ¬∞[26] dataset in 4 views setting. With the proposed
losses, our methods could gradually improve the cross-view consistency and reduce the noise compared with the baseline.
Lrs Lds Lunr Lrel PSNR ‚ÜëSSIM ‚ÜëLPIPS ‚Üì
17.19 0.767 0.223
‚úì 17.79 0.780 0.243
‚úì ‚úì 19.01 0.805 0.228
‚úì ‚úì ‚úì 19.23 0.811 0.220
‚úì ‚úì ‚úì ‚úì 20.72 0.848 0.179
Table 3. Ablation study on the Realistic Synthetic 360‚ó¶dataset [24]
in the 4-view setting. The best and the second-best results are
highlighted in bold and underlined , respectively.
as the baseline. We incrementally introduce our pro-
posed contributions along with various regularization
methods to enhance the model‚Äôs rendering quality.
The groups are delineated as: baseline, baseline+ Lrel,
baseline+ Lrel+Lunr, baseline+ Lrel+Lunr+Lds, and
baseline+ Lrel+Lunr+Lds+Lrs. The ablation results reveal
that each incremental contribution positively impacts the
rendering quality in various aspects. After the addition of
Lrel, our PSNR increased by 0.6. Subsequent inclusion
ofLunrled to a further rise in PSNR by 1.22. With the
incorporation of Lds, the PSNR went from 19.01 to 19.23.
Finally, after adding Lrs, our PSNR peaked at 20.72 ,
SSIM [43] reached 0.848 , and LPIPS [50] arrived at 0.179 ,
marking a significant enhancement.
Besides, we explore the potential of ReV oRF under the
settings where more input views are available. We report ex-
tra results for 6-view and 9-view settings in Table 4, demon-
strating that increased input views generally enhance perfor-
mance. ReV oRF maintains superiority across these settings.
Fig. 5 presents the visualization of our ablation study on
the Chair scene. We incorporated Lrelto enhance the tex-
ture quality of the model, which, however, introduced some
noise artifacts. To mitigate these artifacts, LunrandLds
were subsequently integrated. These adjustments success-
fully reduced noise but at the cost of blurring the geometric
structures in the process. The issue of maintaining geometric
consistency while eliminating noise was addressed through
the implementation of Lrs. We observe that our method
effectively prevents the collapse of new views caused by
overfitting due to a limited number of viewpoints.MethodsPSNR ‚Üë SSIM ‚Üë LPIPS ‚Üì
3-view6-view9-view 3-view6-view9-view 3-view6-view9-view
SRF [7] 17.07 16.75 17.39 0.436 0.438 0.465 0.529 0.521 0.503
PixelNeRF [47] 16.17 17.03 18.92 0.438 0.473 0.535 0.512 0.477 0.430
MVSNeRF [5] 17.88 19.99 20.47 0.584 0.660 0.695 0.327 0.264 0.244
DVGO [37] 16.60 21.25 22.89 0.560 0.704 0.746 0.422 0.246 0.228
VGOS [39] 19.35 21.55 22.39 0.620 0.671 0.692 0.432 0.328 0.325
Ours 19.26 22.21 23.04 0.644 0.720 0.753 0.316 0.269 0.225
Table 4. Comparison of 3, 6, and 9 input views on LLFF [20].
The best and the second-best results are highlighted in bold and
underlined , respectively.
5. Conclusion, Limitation, and Future Work
In this paper, we address the challenge of view deforma-
tion by discerning reliable and unreliable areas, subsequently
introducing a bilateral geometric consistency regularization.
This approach maximizes the use of reliable regions while
delicately exploring the depth in unreliable areas, applying a
more lenient constraint to these zones. Further extending our
method into voxel space, we transform 2D reliable areas into
3D space through a reliability-aware voxelization smoothing
process. Our method, when applied to various datasets, has
proven to be highly precise, significantly bolstering geomet-
ric consistency and demonstrating its efficacy in intricate 3D
reconstruction tasks.
Our method shares a common limitation of the voxel-
based method: the tendency to produce smoothed results,
leading to a loss in fine details. Besides, the exceptionally
challenging context for NeRF with sparse input also limits
its application in more complex scenes, such as large-scale
scene reconstruction. For future work, we aim to refine the
voxelization technique to better preserve details, potentially
exploring hybrid models that combine voxel-based methods
with alternative geometric representations for a more detailed
reconstruction.
Acknowledgement. The work is supported by the Guang-
dong Natural Science Funds for Distinguished Young
Scholar (No. 2023B1515020097), Singapore MOE Tier
1 Funds (MSS23C002), and the NRF Singapore under the
AI Singapore Programme (No. AISG3-GV-2023-011).
20349
References
[1]Alexander Amini, Wilko Schwarting, Ava Soleimany, and
Daniela Rus. Deep evidential regression. In NeurIPS , vol-
ume 33, pages 14927‚Äì14937, 2020. 3
[2]Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
Mip-nerf: A multiscale representation for anti-aliasing neural
radiance fields. In ICCV , pages 5855‚Äì5864, 2021. 1, 2, 7
[3]Matteo Bortolon, Alessio Del Bue, and Fabio Poiesi. Vm-
nerf: Tackling sparsity in nerf with view morphing. In ICIAP ,
pages 63‚Äì74. Springer, 2023. 2, 3
[4]Ang Cao and Justin Johnson. Hexplane: A fast representation
for dynamic scenes. In CVPR , pages 130‚Äì141, 2023. 2
[5]Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,
Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-
izable radiance field reconstruction from multi-view stereo.
InICCV , pages 14124‚Äì14133, 2021. 2, 3, 7, 8
[6]Di Chen, Yu Liu, Lianghua Huang, Bin Wang, and Pan Pan.
Geoaug: Data augmentation for few-shot nerf with geometry
constraints. In ECCV , pages 322‚Äì337. Springer, 2022. 1
[7]Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard
Pons-Moll. Stereo radiance fields (srf): Learning view syn-
thesis for sparse views of novel scenes. In CVPR , pages
7911‚Äì7920, 2021. 3, 7, 8
[8]Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan.
Depth-supervised nerf: Fewer views and faster training for
free. In CVPR , pages 12882‚Äì12891, 2022. 1
[9]Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B. Tenen-
baum, and Jiajun Wu. Neural radiance flow for 4d view
synthesis and video processing. In ICCV , 2021. 2
[10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:
Radiance fields without neural networks. In CVPR , pages
5501‚Äì5510, 2022. 2
[11] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,
Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja
Fidler. Get3d: A generative model of high quality 3d tex-
tured shapes learned from images. NeurIPS , 35:31841‚Äì31854,
2022. 2
[12] Zhaoxi Chen Guangcong, Chen Change Loy, and Ziwei Liu.
Sparsenerf: Distilling depth ranking for few-shot novel view
synthesis. In ICCV , 2023. 1, 7
[13] Wenbo Hu, Yuling Wang, Lin Ma, Bangbang Yang, Lin Gao,
Xiao Liu, and Yuewen Ma. Tri-miprf: Tri-mip representation
for efficient anti-aliasing neural radiance fields. In ICCV ,
pages 19774‚Äì19783, 2023. 1, 2
[14] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf
on a diet: Semantically consistent few-shot view synthesis.
InICCV , pages 5885‚Äì5894, 2021. 1, 2, 3, 6, 7
[15] Alex Kendall and Yarin Gal. What uncertainties do we need
in bayesian deep learning for computer vision? In NeurIPS ,
pages 5574‚Äì5584, 2017. 3
[16] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¬®uhler, and
George Drettakis. 3d gaussian splatting for real-time radiance
field rendering. ACM Transactions on Graphics , 42(4), July
2023. 6, 7[17] Mijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray
entropy minimization for few-shot neural volume rendering.
InCVPR , pages 12912‚Äì12921, 2022. 1, 2, 3, 6, 7
[18] Min-Seop Kwak, Jiuhn Song, and Seungryong Kim. Gecon-
erf: Few-shot neural radiance fields via geometric consistency.
InICML . JMLR.org, 2023. 1, 2, 3, 6, 7
[19] Soomin Lee, Le Chen, Jiahao Wang, Alexander Liniger,
Suryansh Kumar, and Fisher Yu. Uncertainty guided pol-
icy for active robotic 3d reconstruction using neural radiance
fields. IEEE Robotics Autom. Lett. , 7(4):12070‚Äì12077, 2022.
3
[20] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
Neural scene flow fields for space-time view synthesis of
dynamic scenes. In CVPR , pages 6498‚Äì6508, 2021. 2, 6, 7, 8
[21] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang,
Jun-Yan Zhu, and Bryan Russell. Editing conditional radiance
fields. In ICCV , pages 5773‚Äì5783, 2021. 2
[22] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,
Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-
worth. Nerf in the wild: Neural radiance fields for uncon-
strained photo collections. In CVPR , pages 7210‚Äì7219, 2021.
2, 3
[23] Alican Mertan, Damien Jade Duff, and G ¬®ozde ¬®Unal. Siralama
sorunu olarak nispi derinlik tahmini relative depth estimation
as a ranking problem. In 2020 28th Signal Processing and
Communications Applications Conference (SIU) , pages 1‚Äì6.
IEEE, 2020. 5
[24] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra-
mamoorthi, and R Ng. Nerf: Representing scenes as neural
radiance fields for view synthesis. In ECCV , 2020. 1, 2, 3, 6,
7, 8
[25] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz Cayon,
Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and
Abhishek Kar. Local light field fusion: practical view syn-
thesis with prescriptive sampling guidelines. ACM TOG ,
38(4):29:1‚Äì29:14, 2019. 6, 7
[26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In ECCV , volume 12346, pages 405‚Äì421, 2020. 6,
8
[27] Thomas M ¬®uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM TOG , 41(4):1‚Äì15, 2022. 1,
2
[28] Radford M. Neal. Bayesian learning for neural networks .
PhD thesis, University of Toronto, Canada, 1995. 3
[29] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,
Mehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan.
Regnerf: Regularizing neural radiance fields for view synthe-
sis from sparse inputs. In CVPR , 2022. 5, 7
[30] Xuran Pan, Zihang Lai, Shiji Song, and Gao Huang. Activen-
erf: Learning where to see with uncertainty estimation. In
ECCV , volume 13693, pages 230‚Äì246, 2022. 3
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
20350
transferable visual models from natural language supervision.
InICML , pages 8748‚Äì8763. PMLR, 2021. 3
[32] Ren¬¥e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. IEEE TPAMI , 2020. 4
[33] Murat Sensoy, Lance Kaplan, and Melih Kandemir. Eviden-
tial deep learning to quantify classification uncertainty. In
NeurIPS , volume 31, 2018. 3
[34] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hong-
wen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d
decomposition for high-fidelity dynamic reconstruction and
rendering. In CVPR , pages 16632‚Äì16642, 2023. 2
[35] Jianxiong Shen, Antonio Agudo, Francesc Moreno-Noguer,
and Adria Ruiz. Conditional-flow nerf: Accurate 3d mod-
elling with reliable uncertainty quantification. In ECCV , vol-
ume 13663, pages 540‚Äì557, 2022. 3
[36] Jianxiong Shen, Adria Ruiz, Antonio Agudo, and Francesc
Moreno-Noguer. Stochastic neural radiance fields: Quantify-
ing uncertainty in implicit 3d representations. In 3DV, pages
972‚Äì981, 2021. 3
[37] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel
grid optimization: Super-fast convergence for radiance fields
reconstruction. In CVPR , pages 5459‚Äì5469, 2022. 2, 5, 6, 7,
8
[38] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Improved di-
rect voxel grid optimization for radiance fields reconstruction.
arXiv preprint arXiv:2206.05085 , 2022. 2, 5, 6, 7
[39] Jiakai Sun, Zhanjie Zhang, Jiafu Chen, Guangyuan Li, Boyan
Ji, Lei Zhao, and Wei Xing. Vgos: V oxel grid optimization
for view synthesis from sparse inputs. In IJCAI , pages 1414‚Äì
1422, 8 2023. 3, 6, 7, 8
[40] Feitong Tan, Sean Fanello, Abhimitra Meka, Sergio Orts-
Escolano, Danhang Tang, Rohit Pandey, Jonathan Taylor,
Ping Tan, and Yinda Zhang. V olux-gan: A generative model
for 3d face synthesis with hdri relighting. In ACM TOG , pages
1‚Äì9, 2022. 2
[41] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-
han, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,
and Henrik Kretzschmar. Block-nerf: Scalable large scene
neural view synthesis. In CVPR , pages 8248‚Äì8258, 2022. 1
[42] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara
Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-
mamoorthi, Jonathan Barron, and Ren Ng. Fourier features let
networks learn high frequency functions in low dimensional
domains. In NeurIPS , volume 33, pages 7537‚Äì7547, 2020. 3
[43] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.
Simoncelli. Image quality assessment: from error visibility
to structural similarity. IEEE TIP , 13(4):600‚Äì612, 2004. 7, 8
[44] Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF:
Regularizing Neural Radiance Fields with Denoising Diffu-
sion Models. In CVPR , 2023. 3
[45] Dongyu Yan, Jianheng Liu, Fengyu Quan, Haoyao Chen,
and Mengmeng Fu. Active implicit object reconstruction
using uncertainty-guided next-best-view optimization. IEEE
Robotics Autom. Lett. , 8(10):6395‚Äì6402, 2023. 3[46] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-
proving few-shot neural rendering with free frequency regu-
larization. In CVPR , pages 8254‚Äì8263, 2023. 1, 3
[47] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance fields from one or few images. In
CVPR , pages 4578‚Äì4587, 2021. 1, 2, 3, 6, 7, 8
[48] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,
Rongfei Jia, and Lin Gao. Nerf-editing: geometry editing of
neural radiance fields. In CVPR , pages 18353‚Äì18364, 2022.
2
[49] Jian Zhang, Yuanqing Zhang, Huan Fu, Xiaowei Zhou,
Bowen Cai, Jinchi Huang, Rongfei Jia, Binqiang Zhao, and
Xing Tang. Ray priors through reprojection: Improving neu-
ral radiance fields for novel view extrapolation. In CVPR ,
pages 18376‚Äì18386, 2022. 1, 2, 3
[50] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , pages 586‚Äì595,
2018. 7, 8
[51] Chenxi Zheng, Bangzhen Liu, Xuemiao Xu, Huaidong Zhang,
and Shengfeng He. Learning an interpretable stylized sub-
space for 3d-aware animatable artforms. IEEE TVCG , 2024.
2
20351
