Action Detection via an Image Diffusion Process
Lin Geng Foo1Tianjiao Li1Hossein Rahmani2Jun Liu1â€ 
1Singapore University of Technology and Design2Lancaster University
{lingeng_foo,tianjiao_li}@mymail.sutd.edu.sg,
h.rahmani@lancaster.ac.uk, jun_liu@sutd.edu.sg
Abstract
Action detection aims to localize the starting and ending
points of action instances in untrimmed videos, and predict
the classes of those instances. In this paper, we make the
observation that the outputs of the action detection task can
be formulated as images. Thus, from a novel perspective, we
tackle action detection via a three-image generation process
to generate starting point, ending point and action-class
predictions as images via our proposed Action Detection
Image Diffusion (ADI-Diff) framework. Furthermore, since
our images differ from natural images and exhibit special
properties, we further explore a Discrete Action-Detection
Diffusion Process and a Row-Column Transformer design
to better handle their processing. Our ADI-Diff framework
achieves state-of-the-art results on two widely-used datasets.
1. Introduction
The goal of action detection is to localize the starting and
ending points of action instances in untrimmed videos, while
also predicting the classes of those actions. Action detection
is important across many video analysis applications, includ-
ing healthcare monitoring [ 44,47], sports analysis [ 17,24]
and security surveillance [ 1,60], and has attracted a lot of re-
search attention. A common approach [ 10,16,29,62,63] is
to first extract proposals of action instances, before process-
ing each of these proposals individually to produce refined
starting point, ending point, and action-class predictions.
Many works focus on improving the action proposal local-
ization process (e.g., with a better starting and ending point
regression head [ 30,31,36]), or designing better model ar-
chitectures (e.g., graph models [ 26,65,70] and Transformers
[35,69]). Nevertheless, action detection still remains chal-
lenging, since actions often contain complex motions with
high intra-class variability [ 21,73], and difficulties also arise
due to the varying lighting conditions, different viewpoints
and background clutter [21, 29, 48].
â€  Corresponding author
ğ‘framesğ¶classesAction -class AD image ( ğ’™ğŸğ’‚) Input video clip Starting Point AD image ( ğ’™ğŸğ’”)
Is starting point? (Yes/No)Ending Point AD image ( ğ’™ğŸğ’†)
Is ending point? (Yes/No)ğ‘frames
ğ‘framesImagify
ğ‘framesFigure 1. Illustration of our formulated AD images, which allow
us to tackle action detection by generating three images. The
action-class AD image ( xa) has a shape of NÃ—C, while the
starting and ending point AD images ( xsandxe) both have a shape
ofNÃ—2, where we show N= 5 andC= 5 in this figure for
illustration. Specifically, the pixel values in a row of the image form
the probabilities of a discrete distribution regarding a specific video
frame, e.g., the n-th row of the action-class AD image represents
the probability distribution over the action classes for the n-th
frame. We depict the ground truth AD images ( xa
0, xs
0, xe
0) in this
figure, thus each row contains a single white pixel (with value 1)
in each row depicting the correct prediction, while the other pixels
are black in color (with value 0).
On the other hand, image diffusion models [ 22,56] have
recently undergone rapid development and show an excellent
capability to generate high-quality images. Image diffusion
models aim to obtain a high-quality image from an noisy
and uncertain image, and achieve this via step-by-step pro-
gressive denoising. Intuitively, the diffusion modelâ€™s process
of progressive denoising helps to bridge the large gap be-
tween the high-quality target images and the noisy images by
breaking it down into smaller intermediate steps [ 55], which
assists the model in converging towards generating the high-
quality target images. Thus, image diffusion models [ 22,56]
can improve image quality and training stability, and possess
a strong ability to generate high-quality target images that
align well with the provided input conditions.
In this work, inspired by the efficacy of image diffusion
models, we make the following observation: the three out-
puts (starting point, ending point and action-class) for the
action detection task can be formulated as images . For in-
stance, the action-class predictions can be represented by a
NÃ—Cimage (where Nis the number of frames and Cis
the number of action-classes), while the starting and ending
point predictions can each be represented by a NÃ—2image,
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
18351
as shown in Fig. 1. Hence, from a new perspective, we can
re-cast action detection as a three-image generation task,
tackled by generating these three â€œaction detectionâ€ images
â€“ which we call AD images â€“ as output. Then, in order to
generate these AD images with a high level of quality, we
can leverage image diffusion models [ 22,53,54] with their
strong image generation capabilities.
To this end, we propose an AD Image Diffusion (ADI-
Diff) framework for action detection, as shown in Fig. 2.
Our ADI-Diff framework learns to generate the target high-
quality action-class, starting point and ending point AD
images via diffusion. Following previous works on diffusion
models [ 22,54], our ADI-Diff framework comprises two
opposite diffusion processes: the forward process and the
reverse process . Specifically, the forward process aims to
generate supervisory signals of intermediate steps during
training, through progressively adding noise to the ground
truth AD images. Conversely, the reverse process aims to
learn to reverse the forward process, i.e, by learning to de-
noise and produce high-quality AD images, which is the
main part of our action detection pipeline.
However, directly using standard diffusion models [ 22,
54] for our ADI-Diff framework can be sub-optimal since
they learn to generate natural images, while our proposed
AD images differ from natural images, because AD images
also represent a set of discrete probability distributions . For
instance, our AD images are used to tackle a classification
problem (either among Caction-classes or 2 classes for
starting/ending point predictions), which is a problem of pre-
dicting discrete probability distributions. Thus, our AD im-
ages also represent a set of discrete probability distributions,
where the pixels in each row of the image represent the proba-
bilities of a discrete distribution. Hence, instead of following
the standard diffusion process to map between a high-quality
image and a totally uncertain image, our diffusion process
should learn to map between the ground truth â€“ which is
anideal discrete probability distribution â€“ and a totally un-
certain discrete probability distribution . In other words, the
standard diffusion process, which progressively introduces
Gaussian noise in the forward process and converges towards
Gaussian noise, is not well-suited for our needs. Therefore,
we propose a novel Discrete Action-Detection Diffusion Pro-
cess that constrains each forward diffusion step to produce
discrete probability distributions, which enables us to gener-
ate the desired high-quality AD images from the noisy and
uncertain probability distributions more effectively.
Moreover, in contrast to traditional images which contain
rich local spatial correlations in both dimensions, our AD
images exhibit different relationship patterns along each of
the two dimensions. Specifically, in our AD images, there
is a strong sequential ordering between adjacent rows (i.e.,
between temporal frames), which differs from the inter-class
relationships between adjacent columns (e.g., between actionclasses). Hence, since our AD images differ from traditional
images, existing diffusion network designs, which tend to
focus on 2D spatial processing in local neighbourhoods,
are not suitable for our use. Thus, we further propose our
Row-Column Transformer design for our diffusion model
to effectively extract class information across the columns
while encoding temporal relationships across the rows.
In summary, our contributions are as follows: (1)From
a novel perspective, we re-cast action detection as a three-
image generation problem and generate the AD image pre-
dictions via our AD Image Diffusion (ADI-Diff) framework.
(2)We propose a Discrete Action-Detection Diffusion Pro-
cess that constrains the forward diffusion process to produce
discrete probability distributions, which provides a good
mapping between the input noisy distribution and the ground
truth distribution. (3)To handle our AD images which are
different from traditional images, we further introduce a
Row-Column Transformer design for our diffusion network.
2. Related Work
Inaction detection , many approaches [ 10,16,29,62,63]
first extract action proposals before processing them indi-
vidually to predict action classes and refined starting and
ending points. These methods are generally split into two
categories: anchor-based and anchor-free. Anchor-based
methods [ 10,16,63] such as multi-tower networks [ 9] and
temporal feature pyramid networks [ 33,36] generate a dense
set of anchors with pre-defined lengths throughout the video
that act as action proposals. On the other hand, anchor-free
methods [ 29,40,42,62,69] often predict actionness scores
[61,72] or action boundary confidence scores [ 29,30] for
video frames in order to generate action proposals. Besides,
some existing works explore different architectures to encode
spatio-temporal information, including RNNs [ 5,67], graph
models [ 3,26,65,68,70], or Transformers [ 8,35,43,58].
Moreover, some proposal-free methods [ 40,41] have also
been explored recently, and our ADI-Diff also falls into this
category. Different from previous works, we re-cast action
detection as a three-image generation problem, and lever-
age the diffusion modelâ€™s strong image generation capability
to generate the three AD image predictions. Our proposed
ADI-Diff framework effectively handles the challenging ac-
tion detection task by generating high-quality starting/ending
point and action-class AD images, achieving good results.
Diffusion models have emerged as an effective way to
sample from a data distribution by learning to estimate
the gradients of the data distribution [ 55]. Originally in-
troduced in the context of image generation [ 53], diffu-
sion models have seen much development in recent years
[2,15,22,54,64], and have been explored across vari-
ous generation tasks, including video [52], point cloud [39]
and text [ 27] generation. Diffusion models have also been
adopted for human activity analysis [ 14,20,32,49], e.g., for
18352
Imagify
frames
ğ‘¥0ğ‘ğ‘¥0ğ‘ ğ‘¥0ğ‘’
frames
ğ‘¥1ğ‘ğ‘¥1ğ‘ ğ‘¥1ğ‘’
frames
ğ‘¥ğ‘‡ğ‘ğ‘¥ğ‘‡ğ‘ ğ‘¥ğ‘‡ğ‘’â€¦ğ‘ğ‘§1|ğ‘§0 ğ‘ğ‘§ğ‘‡|ğ‘§ğ‘‡âˆ’1
ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ 1
ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ 2
ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ ğ‘
â€¦â€¦â€¦
ğ‘ğœ™ğ‘§0|ğ‘§1 ğ‘ğœ™ğ‘§ğ‘‡âˆ’1|ğ‘§ğ‘‡
: Forward Process
: Reverse ProcessSpat io-temporal features ğ‘“ğ‘†ğ‘‡à·œğ‘¥0ğ‘à·œğ‘¥1ğ‘à·œğ‘¥0ğ‘ à·œğ‘¥0ğ‘’à·œğ‘¥1ğ‘ à·œğ‘¥1ğ‘’
â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦â€¦
Class esProbs1Class esProbs1
Class esProbs1Figure 2. Illustration of the proposed AD Image Diffusion (ADI-Diff) framework. The forward process (represented with orange arrows)
progressively diffuses the ground truth AD images xa
0, xs
0, xe
0towards a noisy outcome, which generates supervisory signals for intermediate
steps. On the other hand, the reverse process (represented with green arrows) is trained to denoise the noisy inputs xa
T, xs
T, xe
Twhile
conditioned on extracted spatio-temporal features fSTfrom the input video, to obtain the output AD images Ë†xa
0,Ë†xs
0,Ë†xe
0.
pose estimation [ 20] where a GMM-based forward process
is proposed. Moreover, some works [ 11] adopt diffusion
models for regressing the bounding box of an object, which
has also inspired a similar process for regressing temporal
boundaries [ 42]. In this work, considering that action de-
tection requires starting point, ending point and action-class
outputs which can be treated as three AD images, and that
diffusion models naturally have a strong image generation
capability, we propose to cast action detection as a three-
image diffusion process. At the same time, since AD images
are not natural images and have their own properties, we
propose modifications to the image diffusion process, which
attain good performance.
3. ADI-Diff Framework
In this paper, we tackle the action detection task by casting
it as an image generation problem (as described in Sec. 3.1),
and leverage a diffusion model via our proposed ADI-Diff
framework to generate AD images that encode the required
starting point, ending point and action-class information.
Moreover, in Sec. 3.2, we design a Discrete Action-Detection
Diffusion Process, constraining the pixels along each row
of our AD images to form a discrete probability distribution
in the forward process. Besides, in order to perform diffu-
sion for our AD images which exhibit different relationship
patterns along each of the two dimensions (i.e., between
frames and between classes), we propose a Row-Column
Transformer architecture in Sec. 3.3.
3.1. Formulation of AD images
In this work, we observe that we can reformulate the three
outputs of the action detection task (starting point, ending
point and action-class prediction) as three images. Hence,
from a new perspective, we can cast action detection as
an image diffusion process to generate these three AD im-
ages. Below, we describe how we formulate our action-class,
starting point and ending point AD images to encode the
corresponding predictions.Action-class AD image xa.As shown in Fig. 1, to cap-
ture the action-class predictions at each time step, we set the
action-class AD image to be a matrix xawith shape NÃ—C,
where Nis the number of frames in the video and Crepre-
sents the number of action classes. The matrix xacan be
seen as an image, where the pixel value at the n-th row and
c-th column is the probability that action cis happening at
then-th frame of the video, and is constrained to be in [0,1].
Thus, xais a grayscale image with shape NÃ—C, which can
be generated via an image diffusion process.
Starting Point and Ending Point AD images xs, xe.
Next, in order to encode temporal boundary predictions, we
produce two AD images: the starting point AD image xsâˆˆ
[0,1]NÃ—2and the ending point AD image xeâˆˆ[0,1]NÃ—2,
which respectively encode predictions of the starting points
and ending points of action instances. Specifically, the N
rows of the matrix xs(orxe) encode information regarding
theNframes, with the 2pixels in each row respectively
encoding the probabilities for the presence and absence of a
starting (or ending) point. For example, for the starting point
AD image xs, the pixel value in the first column of the n-th
row represents the probability of a starting point occurring
at the n-th frame. The same goes for the ending point AD
image xe, except that the pixel value in the first column
encodes the probability of an ending point occurring instead.
In summary, both xsandxecan be seen as grayscale images
with shape NÃ—2, as shown in Fig. 1.
3.2. Discrete Action-Detection Diffusion Process
After re-casting action detection as a three-image generation
task, we seek to generate high-quality AD images to han-
dle action detection effectively. To achieve this, we derive
inspiration from the strong image generation capabilities
of diffusion models [ 22,54], and adopt a diffusion-based
approach to generate the three AD images for action de-
tection. Overall, diffusion models [ 22,54] aim to obtain a
high-quality image from a totally noisy and uncertain image ,
and do so by progressively removing the noise and uncer-
tainty over multiple steps. Specifically, to learn a mapping
18353
between a noisy image (that is totally random and uncer-
tain) and a high-quality image, standard diffusion models
consist of a forward process where Gaussian noise is pro-
gressively added to high-quality images. Meanwhile, the
reverse process learns to reverse the forward process, i.e., to
denoise the noisy and uncertain inputs to obtain high-quality
images. These processes enable diffusion models to bridge
the large gap between the input noisy images and the target
high-quality images, and obtain high-quality images from
noisy and uncertain images .
Nevertheless, using standard image-based diffusion mod-
els [22,54] directly can be sub-optimal. Specifically, these
diffusion models aim to generate natural images from noisy
images, so they add Gaussian noise during the forward pro-
cess to naturally obtain intermediate noisy images. However,
our AD images differ from natural images, since we are us-
ing our AD images to deal with a classification problem, e.g.,
the action-class AD images are used to tackle a C-way action
classification task, while the starting point AD images are
used to predict starting points as a binary classification task
(yes/no), and the same goes for ending point AD images. In
other words, our AD images are in fact a set of discrete prob-
ability distributions . Thus, using the Gaussian noise is not
suitable, because we want our diffusion process to learn to
map between the ground truth â€“ an ideal discrete probability
distribution â€“ and a totally uncertain discrete probability dis-
tribution , with intermediate discrete distributions to bridge
the gap. In order to form intermediate discrete probability
distributions, we cannot simply apply Gaussian noise dur-
ing the forward process, instead we need to constrain each
step of the forward process to produce a discrete probability
distribution , and also converge towards a totally uncertain
discrete probability distribution . Hence, below we design
our own Discrete Action-Detection Diffusion Process.
Firstly, following the standard diffusion model that adds
random noise during their forward process to obtain a totally
noisy and uncertain image, we would like to add random
noise to obtain a totally noisy and uncertain discrete distribu-
tion in our forward diffusion process. We note that, when the
classification predictions are totally uncertain, there should
be an equal probability of predicting any class, which cor-
responds to the Uniform distribution . Since the Uniform
distribution is the most uncertain, we would like to add noise
to converge towards the Uniform distribution in our forward
process ( Property 1 ).
Apart from fulfilling Property 1 above, we find that pre-
vious diffusion models [ 22] also have two other important
properties that help to facilitate the training of the step-by-
step diffusion process. Therefore, to maintain the efficacy of
the diffusion framework, we here also need to satisfy these
two properties, which are as follows: Property 2) Following
previous diffusion models [ 22], there should be a formula
to efficiently jump over tforward steps, i.e., a formulationforq(zt|z0)(as in Eq. 4). This formula facilitates training
and allows us to randomly sample multiple time steps, with-
out having to iterate through many forward steps to get to
a specified step. Property 3) Following previous diffusion
models [ 22], we also need a formulation for the forward
process posterior, i.e., q(ztâˆ’1|zt, z0)in Eq. 5, which allows
us to generate ztâˆ’1fromztto form a ztâˆ’1, ztpair, enabling
a direct step-wise comparison against the reverse process
step during training [22].
Therefore, in order to fulfill these properties, and learn to
generate the three AD images via our diffusion, we design
our forward and reverse process as described below.
Forward Process. In the forward diffusion process, we
aim to create the supervisory signals of intermediate steps
for training. Notably, the forward process of previous works
add Gaussian noise at every forward step, which eventually
corrupts a natural image (at step 0) into Gaussian noise (at
stepT). Here, we instead wish to add a specific type of noise
to fulfill Property 1. Thus, we initialize the ground truth
AD images, and then progressively diffuse (the rows of) the
ground truth AD images towards the Uniform distribution
overTsteps.
Next, we formally introduce some definitions. For sim-
plicity, we describe the diffusion process for a single row of
the action-class AD image as an example, which is a discrete
probability distribution with Cclasses. Specifically, we de-
fineztto be a discrete probability distribution at step tof the
diffusion process, where ztis a vector of length C. At step 0
of the diffusion process, z0represents the ground truth, and
is a one-hot vector with value 1 at the index of the ground
truth category, and 0 elsewhere. The forward process spans
Tsteps, where noise is gradually added to z0, such that after
Tsteps, zTis approximately a Uniform distribution.
Concretely, to create intermediate distributions
{z1, ..., z T}from z0, we add random noise vtat each t-th
forward step, which progressively makes the prediction more
uncertain. This addition of random noises ( {vt}T
t=1) is an
important component [ 22,55,56] that facilitates exploration
of the low-density regions of the data distribution. Here, we
addvtat each step according to a Multinomial distribution
with uniform probability parameters, which crucially allows
us to converge towards the Uniform distribution and satisfy
Property 1 (as explained in further detail later).
Specifically, at each step t, to obtain ztfrom ztâˆ’1, we
increase the uncertainty by introducing a small chance to
randomly select any class (which might not be the correct
ground truth class) with equal probability. Here, we intro-
duce Î²tas a small positive hyperparameter to control the
small increase in randomness at step t. Following the above
intuition, we can formulate each t-th forward step as:
zt= (1âˆ’Î²t)ztâˆ’1+Î²tvt, (1)
where vtis a random vector of length Cwhose elements are
non-negative and add up to 1, i.e., forming the probabilities
18354
of a discrete distribution. We obtain vtvia sampling from
aMN K(K,1
C1)distribution, where MN stands for the
Multinomial distribution, Kis a hyperparameter for the
number of trials,1
C1gives a uniform probability of selecting
each class in each trial (where 1is a vector of 1â€™s with length
C), and we further divide the resulting sample by Kto let
elements of vtsum to 1 (denoted by the subscript K).
Therefore, the likelihood q(zt|ztâˆ’1)of observing ztgiven
ztâˆ’1can be formulated as:
q(zt|ztâˆ’1) =MN K(ztâˆ’(1âˆ’Î²t)ztâˆ’1)
Î²t(zt;K,1
C1, ztâˆ’1),(2)
where, with slight abuse of notation, MN(zt;Â·)is the Multi-
nomialâ€™s likelihood of observing zt, and the subscript is the
substitution formula ( Kvtin terms of zt) which is used to
formulate the exact likelihood, such that Kvtfollows the
Multinomial distribution (more details in Supp).
Next, expanding upon Eq. 1 which represents a single
forward step, the formula for tsteps of the forward process
starting from z0can be derived as:
zt= Â¯Î±tz0+(tY
Ï„=2Î±Ï„)Î²1v1+(tY
Ï„=3Î±Ï„)Î²2v2+....+Î²tvt,(3)
where Î±t= 1âˆ’Î²tandÂ¯Î±t=Qt
Ï„=1Î±Ï„. Then, the corre-
sponding likelihood q(zt|z0)of observing zt(after tsteps)
can be approximately formulated as the following (with the
proof and analysis in Supp):
q(zt|z0) =MN BtK(ztâˆ’Â¯Î±tz0)
1âˆ’Â¯Î±t(zt;BtK,1
C1, z0),(4)
where Bt=(1âˆ’Â¯Î±t)2
((Qt
Ï„=2Î±Ï„)2Î²2
1+(Qt
Ï„=3Î±Ï„)2Î²2
2+....+Î²2
t).
Note that, we can show that our diffusion process fulfills
Property 1 . Specifically, when we set our Î²tâ€™s to be relatively
high such that Â¯Î±tconverges to 0 as tâ†’T, the likelihood in
Eq. 4 converges towards q(zT|z0) =MN BTK(BTK,1
C1).
Then, using the properties of the Multinomial distribution,
we observe that zTapproximately converges to a Uniform
distribution in expectation, i.e., E[zT] =1
C1. This explic-
itly shows that our diffusion framework satisfies Property
1. At the same time, we also fulfill Property 2 , since Eq. 4
enables us to directly generate the intermediate distributions
{z1, ..., z T}from z0for efficient training.
Next, we would like to fulfill Property 3, which will
provide a way to obtain ztâˆ’1fromztin the forward process,
giving us a pair of ztâˆ’1, ztto facilitate the step-wise training
of the reverse process [ 2,22,54]. Specifically, this requires a
formulation of the forward process posterior q(ztâˆ’1|zt, z0).
We can formulate a tractable expression for q(ztâˆ’1|zt, z0)
by using the properties of the Markov chain, as follows:
q(ztâˆ’1|zt, z0) =1
Ïƒt 
MN K(ztâˆ’(1âˆ’Î²t)ztâˆ’1)
Î²t(ztâˆ’1;K,1
C1, zt)
Â· 
MN Btâˆ’1K(ztâˆ’1âˆ’Â¯Î±tâˆ’1z0)
1âˆ’Â¯Î±tâˆ’1(ztâˆ’1;Btâˆ’1K,1
C1, z0)
,(5)
where Ïƒt=P
ztâˆ’1 
MN K(ztâˆ’(1âˆ’Î²t)ztâˆ’1)
Î²t(ztâˆ’1;K,1
C1, zt)
Â· 
MN Btâˆ’1K(ztâˆ’1âˆ’Â¯Î±tâˆ’1z0)
1âˆ’Â¯Î±tâˆ’1(ztâˆ’1;Btâˆ’1K,1
C1, z0)(more de-
tails in Supp). Note that, in practice we can fix Ïƒtas a
hyperparameter, since it is constant for all observed ztâˆ’1.
Reverse Process. Using the forward process pre-
sented above, we can generate the intermediate distributions
{z1, ..., z T}. Then, we can use these intermediate distribu-
tions to optimize our diffusion model d(parameterized by
Ï•) to learn the reverse diffusion process. As shown in Fig. 2,
the reverse process aims to generate the AD image outputs
afterTdiffusion steps.
First, we extract information from the input video to fa-
cilitate the diffusion process. To do this, we follow previous
works [ 29,40,50,51,60,69] and extract features from video
snippets with a pre-trained feature extractor. Specifically, we
extract a feature fSTâˆˆRNÃ—CST, where Nis the number
of frames and CSTis the number of channels. Each reverse
step will be conditioned on this extracted feature fST.
Next, we perform the reverse diffusion process. We first
initialize the input noisy distribution zTthat is approxi-
mately a Uniform distribution â€“ more precisely, zTfollows
aMN BTK(BTK,1
C1)distribution to match with the zTof
our forward process. Then, we perform the reverse diffusion
process zTâ†’Ë†zTâˆ’1â†’...â†’Ë†z0to generate the predic-
tions Ë†z0, where the hat (Ë†)operator denotes that these are
estimates produced by our diffusion model d(and not the
forward process). Specifically, we define our reverse process
in a step-by-step manner as follows:
Ë†ztâˆ’1=dÏ•(Ë†zt, fST, ft), tâˆˆ {1, ..., T}, (6)
where ftis the unique step embedding to represent the tth
diffusion step, which we generate via the sinusoidal function.
By performing the Tsteps of the reverse process via Eq. 6,
we can produce output predictions Ë†z0from the uncertain dis-
tribution zT. Moreover, to better represent the intermediate
and target distributions, we initialize Msamples during train-
ing and perform the reverse process on Msamples, instead
of using a single sample only.
Multi-row Processing. For simplicity, above we describe
the forward and reverse process in terms of a single row zt
of the action-class AD image. However, the same diffusion
process can be simultaneously applied to all the rows of the
action-class AD image xa, where the forward process adds
noise to all rows at once (to generate {xa
1, ..., xa
T}from the
ground truth xa
0), and the reverse process aims to reverse
the addition of noise in all rows (i.e., produce xa
tâˆ’1fromxa
t
at the t-th step). We further note that this Discrete Action-
Detection Diffusion Process is also used for the starting
point and ending point AD images, since they all tackle a
frame-wise classification task.
3.3. Row-Column Transformer Architecture
Moreover, different from natural images which contain rich
local 2D spatial relationships, our AD images possess differ-
ent relationship patterns along each of the two dimensions
18355
(i.e., between frames vs. between classes). Specifically, there
is a strong natural sequential ordering between adjacent rows
(i.e., between adjacent temporal frames), which yet differs
from the inter-class relationships between adjacent columns
(e.g., between adjacent action classes). Hence, existing dif-
fusion network designs [ 22,54], which tend to focus on 2D
spatial processing in neighbourhoods, are not well-suited for
our use. To overcome this, we propose to handle the two
dimensions in different ways, such that our Row-Column
Transformer design can effectively extract class information
across the columns while encoding temporal information
across the rows. Below, for simplicity, we describe the ar-
chitecture of our diffusion network dto handle a single AD
image, using the action-class AD image as an example.
Next, we describe the inputs to the diffusion network
dat the t-th diffusion step. We denote the input image
asxaâˆˆRNÃ—C. In order to derive predictions specific to
the input video, we also extract spatio-temporal features
fSTâˆˆRNÃ—CSTfrom the input video. By conditioning on
fST, our reverse process receives important video-specific
information to perform the denoising. Besides, to better cap-
ture the distribution characteristics at each t-th step, we also
condition the diffusion process on step index t, and generate
a diffusion step embedding ftâˆˆRNÃ—1via the sinusoidal
function to represent the t-th diffusion step. Then, we con-
catenate xa, fSTandftto form input xâˆˆRNÃ—(C+CST+1).
Our Row-Column Transformer design for our diffusion
network dconsists of Lstacks of Row-Column Blocks , which
we introduce below. Refer to Supp for more details.
Row-Column Block. The first part of the block encodes
information across columns, i.e., class information. Cru-
cially, relationships between the columns (i.e., inter-class
relationships) can exist over long ranges. Hence, in or-
der to encode the relationships between columns (classes)
across a long range, we perform a Multi-Head Self-Attention
(MHSA) between the columns of the input image. Note that,
for starting and ending point AD images, the two neigh-
bouring columns (yes/no) are highly correlated, and these
correlations can still be learned with the diffusion design in
the previous section and the MHSA operation. Specifically,
we treat each column of the input xâˆˆRNÃ—(C+CST+1)as
a token, thus obtaining C+CST+ 1tokens of length N.
Next, a learnable positional embedding is added to each to-
ken, which encodes the positional information of each token.
Then, we perform MHSA among all the C+CST+1tokens,
to obtain an intermediate output ucolâˆˆRNÃ—(C+CST+1).
This is followed by 2 MLP layers, where we eventually
output xcolâˆˆRNÃ—(C+CST+1).
In the next part of the Row-Column Block, we encode
temporal information across rows (frames). Notably, there
exist strong local relationships and sequential correlations
between neighbouring rows (frames). Furthermore, actions
often provide context information for other actions in thesame sequence, which we can exploit by considering the
longer-term temporal relationships between rows (frames).
Thus, to effectively encode local relationships between neigh-
bouring rows (frames), we apply a Temporal Convolution
(TC), which has a strong inductive bias for encoding lo-
cal sequential relationships [ 12,13]. We also combine the
TC with a MHSA conducted between the rows (frames)
to encode long-range temporal relationships. Specifically,
we first process the local relationships with a 1Ã—3TC,
to yield urowâˆˆRNÃ—(C+CST+1). Then, to encode long-
range temporal relationships, we first add a learnable posi-
tional embedding to each token, before performing MHSA
across the rows (frames) by treating each row of urowas
a token (i.e., there are Ntokens of length C+CST+ 1).
This is followed by 2 MLP layers to obtain a final output
xrowâˆˆRNÃ—(C+CST+1).
Combined Image Processing. Above, we describe both
the Discrete Action-Detection Diffusion Process and Row-
Column Transformer for the action-class AD image xa. Yet,
these methods can also handle other AD images, since the
three AD images (action-class xa, starting point xs, ending
point xe) are all similarly designed to perform classification.
To produce the three AD images, one possible way is to
perform our methods once for each AD image, i.e., produc-
ing them separately. Another option is to stitch the images
together into a combined image and perform a combined
processing for all three AD images. Specifically, we can
concatenate the three AD images {xa
t, xs
t, xe
t}horizontally
to obtain the combined image as xcombined
t âˆˆRNÃ—(C+4),
where each row consists of three discrete distributions. At
the same time, we can still handle the three discrete distribu-
tions separately during the diffusion process. To process the
stitched image with the diffusion network, we only need to
modify the column dimensionality, i.e., by adding 4 columns
to the Row-Column Block design above.
There are two advantages in such combined processing.
Firstly, it is more efficient and allows us to produce the three
AD images in parallel at one go. Secondly, the combined
processing also facilitates more sharing of knowledge be-
tween the classification and starting/ending point localization
sub-tasks, that can be learned via the combined end-to-end
update. For instance, by producing the three AD images si-
multaneously, our model learns to tackle action classification
with the knowledge of the global temporal structure of all
action instances in the video (gained from the localization
sub-tasks), which leads to better performance.
3.4. Inference and Training Pipeline
Inference Pipeline. After obtaining the outputs Ë†xa
0,Ë†xs
0,Ë†xe
0,
we use them to obtain the final action instances. We largely
follow the post-processing pipeline of [ 30,31], as follows:
First, we find the frames where actions are likely to start or
end, by finding pixels in the left column of Ë†xs
0,Ë†xe
0that are
18356
above a pre-defined threshold Î´. Next, because these pixels
are often found in clusters, we group up the pixels that are
connected, and identify their average position as the starting
or ending point. Then, following previous approaches [ 30,
31] to generate candidate proposals, each starting point Ë†xs
0is
coupled with all the ending points behind to identify action
candidates, with the action duration ranging between the
identified starting and ending points. To obtain the action
candidateâ€™s class, we average the rows of Ë†xa
0corresponding
to the duration of the action candidate, and take the class with
the highest value. After obtaining all the action candidates
for the video sequence, we utilize Soft-NMS [ 4] to remove
overlapping candidates and produce the final results.
Training Pipeline. We use an off-the-shelf model to
extract video features fST, which is kept frozen throughout.
At the start, we randomly initialize the diffusion model d
with the architecture in Sec. 3.3. During training, we obtain
supervision signals for the intermediate steps via the forward
process (Eq. 4). Then, we perform the reverse process (Eq. 6)
with our diffusion model dto obtain AD image predictions
for the intermediate steps and output. We apply the MSE
loss between our AD image predictions and the supervision
signals at each step, to update the parameters of the diffusion
model d.
4. Experiments
4.1. Implementation Details
Following existing works [ 51,69], we use an off-the-shelf
I3D [ 7] and R(2+1)D [ 59] models to extract video features
fSTfor THUMOS14 and ActivityNet-1.3 respectively. The
diffusion network dÏ•is randomly initialized following the
Xavier initialization scheme [ 18]. Following previous image
diffusion works [ 22,54], we adopt MSE loss for training.
We use AdamW [ 38] as the optimizer. The initial learning
rate is set to 2Ã—10âˆ’5and decays following cosine rule. The
training batch size is set to 16. Following previous work
[69], all training videos are padded to be 2,304 frames and
mask operations are added accordingly for redundant padded
frames. We set the hyperparameters T= 50 ,L= 3,Î´=
0.9,M= 10 .K= 200 for THUMOS14 and K= 2000 for
ActivityNet-1.3. All experiments are conducted on Nvidia
V100 GPUs. Refer to Supp for more implementation details.
4.2. Datasets and Evaluation Metric
Following previous works [ 31,40,42,50,51,62,65,69], we
evaluate our method on the THUMOS14 and ActivityNet-1.3
datasets. THUMOS14 [23] includes 413 untrimmed videos
containing 20 classes of actions. The THUMOS14 dataset
is split into a validation set with 200 videos and a test set
with 213 videos. We follow existing settings [ 3,29,61,69]
to train our model on the validation set and test on the test
set.ActivityNet-1.3 [6] contains over 20K videos and 200Table 1. Results on THUMOS14 and ActivityNet-1.3 datasets.
THUMOS14 ActivityNet-1.3
Methods Feature 0.3 0.4 0.5 0.6 0.7 Avg Feature 0.5 0.75 0.95 Avg
BMN [31] TSN 56.0 47.4 38.8 29.7 20.5 38.5 TSN 50.1 34.8 8.3 33.9
DBG [28] TSN 57.8 49.4 39.8 30.2 21.7 39.8 - - - - -
G-TAD [65] TSN 54.5 47.6 40.3 30.8 23.4 39.3 TSN 50.4 34.6 9.0 34.1
BC-GNN [3] TSN 57.1 49.1 40.4 31.2 23.1 40.2 TSN 50.6 34.8 9.4 34.3
TAL-MR [71] I3D 53.9 50.7 45.4 38.0 28.5 43.3 I3D 43.5 33.9 9.2 30.2
P-GCN [68] R(2+1)D 69.1 63.3 53.5 40.5 26.0 50.5 I3D 48.3 33.2 3.3 31.1
TSA-Net [19] P3D 61.2 55.9 46.9 36.1 25.2 45.1 P3D 48.7 32.0 9.0 31.9
MUSES [34] I3D 68.9 64.0 56.9 46.3 31.0 - I3D 50.0 35.0 6.6 34.0
TCANet [45] TSN 60.6 53.2 44.6 36.8 26.7 44.3 TSN 52.3 36.7 6.9 35.5
BMN-CSA [57] TSN 64.4 58.0 49.2 38.2 27.8 47.7 TSN 52.4 36.2 5.2 35.4
ContextLoc [73] I3D 68.3 63.8 54.3 41.8 26.2 50.9 I3D 56.0 35.2 3.6 34.2
VSGN [70] TSN 66.7 60.4 52.4 41.0 30.4 50.2 R(2+1)D 53.3 36.8 8.1 35.9
RTD-Net [58] I3D 68.3 62.3 51.9 38.8 23.7 49.0 I3D 47.2 30.7 8.6 30.8
A2Net [66] I3D 58.6 54.1 45.5 32.5 17.2 41.6 I3D 43.6 28.7 3.7 27.8
GTAN [37] P3D 57.8 47.2 38.8 - - - P3D 52.6 34.1 8.9 34.3
PBRNet [33] I3D 58.5 54.6 51.3 41.8 29.5 - I3D 54.0 35.0 9.0 35.0
TadTR [35] I3D 62.4 57.4 49.2 37.8 26.3 46.6 I3D 49.1 32.6 8.5 32.3
AFSD [29] I3D 67.3 62.4 55.5 43.7 31.1 52.0 I3D 52.4 35.3 6.5 34.4
TAGS [40] I3D 68.6 63.8 57.0 46.3 31.8 52.8 I3D 56.3 36.8 9.6 36.5
STPT [62] STPT 70.6 65.7 56.4 44.6 30.5 53.6 STPT 51.4 33.7 6.8 33.4
ReAct [50] 3DCNN 69.2 65.0 57.1 47.8 35.6 55.0 3DCNN 49.6 33.0 8.6 32.6
ActionFormer [69] I3D 82.1 77.8 71.0 59.4 43.9 66.8 R(2+1)D 54.7 37.8 8.4 36.6
DiffTAD [42] I3D 74.9 72.8 71.2 62.9 58.5 68.0 I3D 56.1 36.9 9.0 36.1
Self-DETR [25] I3D 74.6 69.5 60.0 47.6 31.8 56.7 I3D 52.2 33.6 8.4 33.7
TriDet [51] I3D 83.6 80.1 72.9 62.4 47.4 69.3 R(2+1)D 54.7 38.0 8.4 36.8
Ours I3D 84.9 81.5 76.5 63.0 48.0 70.8 R(2+1)D 56.9 38.9 9.1 38.3
action categories. It comprises of training, validation and test
splits containing 10,024, 4,926 and 5,044 videos respectively.
Following previous settings [ 30,31,65,69], our model is
optimized on the training set and tested on the validation set.
Evaluation Metric. Following previous works [ 40,50,
62,69], we report the mean average precision (mAP) at
different temporal intersection over union (tIoU) thresholds.
The tIoU threshold determines how much overlap is required
between the prediction and the ground truth to be considered
an accurate prediction. We also report the average mAP
(Avg), where we average across several tIoUs.
4.3. Main Experimental Results
We compare with state-of-the-art action detection methods
on THUMOS14 and ActivityNet-1.3 datasets in Tab. 1. Our
proposed method achieves the best results on average mAP
among existing methods, showing its efficacy.
4.4. Ablation Studies
Following previous works [ 50,51,62,69], we conduct abla-
tion experiments on THUMOS14.
Impact of Main Components of ADI-Diff Framework.
First, we evaluate the efficacy of our proposed Discrete
Action-Detection Diffusion process by comparing against
the following baselines: (A) Stand. Diff. + Model Archi-
tecture from [ 22]: We apply the standard diffusion process
[22] with the image diffusion model architecture from [ 22].
(B) Disc. AD Diff. + Model Architecture from [ 22]: We
adopt our Discrete Action-Detection Diffusion, but use the
image diffusion model architecture from [ 22].(C) Stand.
Diff. + Row-Col : We apply the standard diffusion process
[22] with our Row-Column Transformer. As observed in
Tab. 2, Baseline B which uses the proposed diffusion pro-
cess obtains a much better result than Baseline A which
uses standard diffusion, showing the efficacy of the proposed
diffusion process. Notably, this trend also persists when
the Row-Column Transformer is used, where our method
18357
significantly outperforms Baseline C. This improvement is
because our Discrete Action-Detection Diffusion allows us
to effectively map the noisy distributions to the underlying
target distribution.
Next, we also validate the efficacy of our proposed Row-
Column Transformer design. First, we compare Baseline
A vs Baseline C, as well as Baseline B vs our method, and
find that the proposed Row-Column Transformer leads to
performance improvements in both cases, no matter if the
standard diffusion or our proposed diffusion process is used.
This shows the efficacy of the proposed Row-Column Trans-
former. Besides, we further ablate the design of the Row-
Column Transformer by comparing against the following
alternative designs while applying our proposed diffusion
process: (D) Disc. AD Diff. + Model Architecture from
[46]adopts the model architecture from [ 46];(E) Disc. AD
Diff. + Row-Col (w/ Col design only) adopts an alternative
network design (with approximately same network size) that
processes both the columns and rows the same way, with
MHSA layers only; (F) Disc. AD Diff. + Row-Col (w/ Row
design for both) adopts an alternative network design (with
approximately same network size) that processes both the
columns and rows the same way, with TC+MHSA layers;
(G) Disc. AD Diff. + Row-Col (w/o Learnable PE) re-
places the learnable positional embedding with a fixed one
generated using the sinusoidal function. Overall, as shown
in Tab. 2, our proposed design performs the best, showing its
efficacy in capturing both class-wise (across columns) and
temporal relationships (across rows).
Table 2. Ablation study for main components of ADI-Diff.
Method 0.3 0.5 0.7 Avg
(A) Stand. Diff. + Model Architecture from [22] 80.4 68.2 44.1 66.0
(B) Disc. AD Diff. + Model Architecture from [22] 82.6 74.3 45.2 69.0
(C) Stand. Diff. + Row-Col 82.1 73.9 45.0 68.1
(D) Disc. AD Diff. + Model Architecture from [46] 82.0 74.1 45.2 67.5
(E) Disc. AD Diff. + Row-Col (w/ Col design for both) 81.8 75.5 45.0 69.0
(F) Disc. AD Diff. + Row-Col (w/ Row design for both) 82.0 75.3 45.1 68.8
(G) Disc. AD Diff. + Row-Col (w/o Learnable PE) 82.4 75.2 45.9 69.1
Ours (Disc. AD Diff. + Row-Col) 84.9 76.5 48.0 70.8
Visualization of Diffusion Process. In Fig. 3, we visualize
the action-class AD images produced throughout the reverse
diffusion process. We observe that our ADI-Diff framework
progressively denoises the original noisy discrete probability
distributions, to produce high-quality discrete action-class
distributions. See Supp for more results.
ğ‘ ğ‘¡ğ‘’ğ‘=1 ğ‘ ğ‘¡ğ‘’ğ‘=10 ğ‘ ğ‘¡ğ‘’ğ‘=20 ğ‘ ğ‘¡ğ‘’ğ‘=40 ğ‘ ğ‘¡ğ‘’ğ‘=50
GroundTruth
Figure 3. Visualization of diffusion process.
We also compare against a Standard baseline, which
applies a standard diffusion process [ 22]. As observed in
Fig. 4, our proposed method produces better predictions qual-
itatively as compared to the Standard baseline. Specifically,
the baselineâ€™s action-class AD image (right of Fig. 4) shows
much ambiguity and confusion, where the class predictions
can be spread over multiple columns (i.e., white pixels
(a) Ours (b) Standard
Figure 4. Comparison
between the action-class
AD image generated by
our method (left) and
standard diffusion (right).are not concentrated in a consistent
column). In contrast, the action-
class AD image produced by our
method (left of Fig. 4) tends to con-
sistently provide the correct action-
class â€“ here, the predictions are con-
centrated on two separate columns,
indicating that two action-classes are
captured in this video clip, with one
action happening after the other.
Impact of Image Stitching. Next, we explore the impact
of stitching three of our AD images into a combined image
for processing. Results are shown in Tab. 3. We find that
stitching the images leads to an efficiency gain and some
accuracy gains, as it allows us to produce the AD images in
parallel while also facilitating the sharing of knowledge.
Table 3. Ablation study for stitching AD images.
Setting 0.3 0.5 0.7 Avg Speed (seconds per clip)
w/o stitching 84.7 76.0 47.8 70.3 0.158
w/ stitching 84.9 76.5 48.0 70.8 0.113
Impact of Temporal Boundary AD Images xs, xe.We
also investigate the impact of introducing xs, xeby evalu-
ating the performance without them, where here we follow
previous approaches [51, 69] to directly regress the starting
and ending points. In Tab. 4, we observe that the perfor-
mance is much better when we add xs, xe, which shows
the importance of introducing the temporal boundary AD
images to handle the action detection task.
Table 4. Ablation study for temporal boundary AD images.
Setting 0.3 0.5 0.7 Avg
w/o temporal boundary AD images 80.8 71.5 43.9 67.3
w/ temporal boundary AD images 84.9 76.5 48.0 70.8
Inference Speed. In Tab. 5, we compare our methodâ€™s speed
against existing methods in terms of seconds per video clip.
Our method achieves comparable speed to the state-of-the-
art [51], yet significantly outperforms it.
Table 5. Inference speed.
Setting 0.3 0.5 0.7 Avg Speed (seconds per clip)
DiffTAD [42] 74.9 71.2 58.5 68.0 0.397
TriDet [51] 83.6 72.9 47.4 69.3 0.110
Ours 84.9 76.5 48.0 70.8 0.113
5. Conclusion
In this paper, we tackle action detection by casting it as an
image generation problem, and propose an AD Image Dif-
fusion (ADI-Diff) framework to generate target AD images
via diffusion. With a Discrete Action-Detection Diffusion
Process and a Row-Column Transformer design, we attain
state-of-the-art performance on two widely-used datasets.
Acknowledgements. This project is supported by the
Ministry of Education, Singapore, under the AcRF Tier 2
Projects (MOE-T2EP20222-0009 and MOE-T2EP20123-
0014), National Research Foundation Singapore under its
AI Singapore Programme (AISG-100E-2023-121).
18358
References
[1]CV Amrutha, C Jyotsna, and J Amudha. Deep learning
approach for suspicious activity detection from surveillance
video. In 2020 2nd International Conference on Innovative
Mechanisms for Industry Applications (ICIMIA) , pages 335â€“
339. IEEE, 2020. 1
[2]Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow,
and Rianne van den Berg. Structured denoising diffusion mod-
els in discrete state-spaces. Advances in Neural Information
Processing Systems , 34:17981â€“17993, 2021. 2, 5
[3]Yueran Bai, Yingying Wang, Yunhai Tong, Yang Yang, Qiyue
Liu, and Junhui Liu. Boundary content graph neural network
for temporal action proposal generation. In European Confer-
ence on Computer Vision , pages 121â€“137. Springer, 2020. 2,
7
[4]Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S
Davis. Soft-nmsâ€“improving object detection with one line of
code. In Proceedings of the IEEE international conference
on computer vision , pages 5561â€“5569, 2017. 7
[5]Shyamal Buch, Victor Escorcia, Chuanqi Shen, Bernard
Ghanem, and Juan Carlos Niebles. Sst: Single-stream tempo-
ral action proposals. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition , pages 2911â€“
2920, 2017. 2
[6]Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and
Juan Carlos Niebles. Activitynet: A large-scale video bench-
mark for human activity understanding. In Proceedings of the
ieee conference on computer vision and pattern recognition ,
pages 961â€“970, 2015. 7
[7]Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 6299â€“6308, 2017. 7
[8]Shuning Chang, Pichao Wang, Fan Wang, Hao Li, and Ji-
ashi Feng. Augmented transformer with adaptive graph
for temporal action proposal generation. arXiv preprint
arXiv:2103.16024 , 2021. 2
[9]Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold,
David A Ross, Jia Deng, and Rahul Sukthankar. Rethinking
the faster r-cnn architecture for temporal action localization.
Inproceedings of the IEEE conference on computer vision
and pattern recognition , pages 1130â€“1139, 2018. 2
[10] Guo Chen, Yin-Dong Zheng, Limin Wang, and Tong Lu.
Dcan: Improving temporal action detection via dual context
aggregation. In Proceedings of the AAAI Conference on
Artificial Intelligence , pages 248â€“257, 2022. 1, 2
[11] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Diffu-
siondet: Diffusion model for object detection. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 19830â€“19843, 2023. 3
[12] Rui Dai, Srijan Das, Kumara Kahatapitiya, Michael S Ryoo,
and FranÃ§ois BrÃ©mond. Ms-tct: multi-scale temporal con-
vtransformer for action detection. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 20041â€“20051, 2022. 6
[13] StÃ©phane dâ€™Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S
Morcos, Giulio Biroli, and Levent Sagun. Convit: Improvingvision transformers with soft convolutional inductive biases.
InInternational Conference on Machine Learning , pages
2286â€“2296. PMLR, 2021. 6
[14] Lin Geng Foo, Jia Gong, Hossein Rahmani, and Jun Liu.
Distribution-aligned diffusion for human mesh recovery. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 9221â€“9232, 2023. 2
[15] Lin Geng Foo, Hossein Rahmani, and Jun Liu. Ai-generated
content (aigc) for various data modalities: A survey. arXiv
preprint arXiv:2308.14177 , 2, 2023. 2
[16] Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and Ram
Nevatia. Turn tap: Temporal unit regression network for
temporal action proposals. In Proceedings of the IEEE inter-
national conference on computer vision , pages 3628â€“3636,
2017. 1, 2
[17] Silvio Giancola, Mohieddine Amine, Tarek Dghaily, and
Bernard Ghanem. Soccernet: A scalable dataset for action
spotting in soccer videos. In Proceedings of the IEEE confer-
ence on computer vision and pattern recognition workshops ,
pages 1711â€“1721, 2018. 1
[18] Xavier Glorot and Yoshua Bengio. Understanding the dif-
ficulty of training deep feedforward neural networks. In
Proceedings of the thirteenth international conference on
artificial intelligence and statistics , pages 249â€“256. JMLR
Workshop and Conference Proceedings, 2010. 7
[19] Guoqiang Gong, Liangfeng Zheng, and Yadong Mu. Scale
matters: Temporal scale aggregation network for precise ac-
tion localization in untrimmed videos. In 2020 IEEE Inter-
national Conference on Multimedia and Expo (ICME) , pages
1â€“6. IEEE, 2020. 7
[20] Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein
Rahmani, and Jun Liu. Diffpose: Toward more reliable 3d
pose estimation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) , 2023.
2, 3
[21] Hongji Guo, Zhou Ren, Yi Wu, Gang Hua, and Qiang Ji.
Uncertainty-based spatial-temporal attention for online action
detection. In Computer Visionâ€“ECCV 2022: 17th European
Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceed-
ings, Part IV , pages 69â€“86. Springer, 2022. 1
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in Neural Information
Processing Systems , 33:6840â€“6851, 2020. 1, 2, 3, 4, 5, 6, 7, 8
[23] Haroon Idrees, Amir R Zamir, Yu-Gang Jiang, Alex Gorban,
Ivan Laptev, Rahul Sukthankar, and Mubarak Shah. The
thumos challenge on action recognition for videos â€œin the
wildâ€. Computer Vision and Image Understanding , 155:1â€“23,
2017. 7
[24] Haohao Jiang, Yao Lu, and Jing Xue. Automatic soccer video
event detection based on a deep neural network combined
cnn and rnn. In 2016 IEEE 28th International Conference
on Tools with Artificial Intelligence (ICTAI) , pages 490â€“494.
IEEE, 2016. 1
[25] Jihwan Kim, Miso Lee, and Jae-Pil Heo. Self-feedback
detr for temporal action detection. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 10286â€“10296, 2023. 7
18359
[26] Jin Li, Xianglong Liu, Zhuofan Zong, Wanru Zhao, Mingyuan
Zhang, and Jingkuan Song. Graph attention based proposal
3d convnets for action detection. In Proceedings of the AAAI
Conference on Artificial Intelligence , pages 4626â€“4633, 2020.
1, 2
[27] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang,
and Tatsunori B Hashimoto. Diffusion-lm improves control-
lable text generation. arXiv preprint arXiv:2205.14217 , 2022.
2
[28] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo,
Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang, and
Rongrong Ji. Fast learning of temporal action proposal via
dense boundary generator. In Proceedings of the AAAI con-
ference on artificial intelligence , pages 11499â€“11506, 2020.
7
[29] Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang,
Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei
Fu. Learning salient boundary feature for anchor-free tem-
poral action localization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 3320â€“3329, 2021. 1, 2, 5, 7
[30] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and
Ming Yang. Bsn: Boundary sensitive network for temporal
action proposal generation. In Proceedings of the European
conference on computer vision (ECCV) , pages 3â€“19, 2018. 1,
2, 6, 7
[31] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen.
Bmn: Boundary-matching network for temporal action pro-
posal generation. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 3889â€“3898,
2019. 1, 6, 7
[32] Daochang Liu, Qiyue Li, Anh-Dung Dinh, Tingting Jiang,
Mubarak Shah, and Chang Xu. Diffusion action segmentation.
InProceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) , pages 10139â€“10149, 2023. 2
[33] Qinying Liu and Zilei Wang. Progressive boundary refine-
ment network for temporal action detection. In Proceedings of
the AAAI Conference on Artificial Intelligence , pages 11612â€“
11619, 2020. 2, 7
[34] Xiaolong Liu, Yao Hu, Song Bai, Fei Ding, Xiang Bai, and
Philip HS Torr. Multi-shot temporal event localization: a
benchmark. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 12596â€“
12606, 2021. 7
[35] Xiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei
Zhang, Song Bai, and Xiang Bai. End-to-end temporal action
detection with transformer. IEEE Transactions on Image
Processing , 31:5427â€“5441, 2022. 1, 2, 7
[36] Yuan Liu, Lin Ma, Yifeng Zhang, Wei Liu, and Shih-Fu
Chang. Multi-granularity generator for temporal action pro-
posal. In Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 3604â€“3613, 2019.
1, 2
[37] Fuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo
Luo, and Tao Mei. Gaussian temporal awareness networks
for action localization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 344â€“353, 2019. 7[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 7
[39] Shitong Luo and Wei Hu. Diffusion probabilistic models for
3d point cloud generation. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2837â€“2845, 2021. 2
[40] Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, and Tao Xiang.
Proposal-free temporal action detection via global segmenta-
tion mask learning. In European Conference on Computer
Vision , pages 645â€“662. Springer, 2022. 2, 5, 7
[41] Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, and Tao Xiang.
Semi-supervised temporal action detection with proposal-free
masking. In European Conference on Computer Vision , pages
663â€“680. Springer, 2022. 2
[42] Sauradip Nag, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song,
and Tao Xiang. Difftad: Temporal action detection with pro-
posal denoising diffusion. In Proceedings of the IEEE/CVF
International Conference on Computer Vision (ICCV) , pages
10362â€“10374, 2023. 2, 3, 7, 8
[43] Megha Nawhal and Greg Mori. Activity graph trans-
former for temporal action localization. arXiv preprint
arXiv:2101.08540 , 2021. 2
[44] Henry Friday Nweke, Ying Wah Teh, Ghulam Mujtaba, and
Mohammed Ali Al-Garadi. Data fusion and multiple classifier
systems for human activity detection and health monitoring:
Review and open research directions. Information Fusion , 46:
147â€“170, 2019. 1
[45] Zhiwu Qing, Haisheng Su, Weihao Gan, Dongliang Wang,
Wei Wu, Xiang Wang, Yu Qiao, Junjie Yan, Changxin Gao,
and Nong Sang. Temporal context aggregation network for
temporal action proposal refinement. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recog-
nition , pages 485â€“494, 2021. 7
[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and BjÃ¶rn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10684â€“10695, 2022. 8
[47] Supriya Sathyanarayana, Ravi Kumar Satzoda, Suchitra
Sathyanarayana, and Srikanthan Thambipillai. Vision-based
patient monitoring: a comprehensive review of algorithms
and technologies. Journal of Ambient Intelligence and Hu-
manized Computing , 9:225â€“251, 2018. 1
[48] Muhammad Bilal Shaikh and Douglas Chai. Rgb-d data-
based action recognition: A review. Sensors , 21(12):4246,
2021. 1
[49] Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang,
Kai Han, Shanshe Wang, Siwei Ma, and Wen Gao. Diffusion-
based 3d human pose estimation with multi-hypothesis ag-
gregation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV) , pages 14761â€“14771,
2023. 2
[50] Dingfeng Shi, Yujie Zhong, Qiong Cao, Jing Zhang, Lin Ma,
Jia Li, and Dacheng Tao. React: Temporal action detection
with relational queries. In European conference on computer
vision , pages 105â€“121. Springer, 2022. 5, 7
[51] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and
Dacheng Tao. Tridet: Temporal action detection with relative
18360
boundary modeling. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
18857â€“18866, 2023. 5, 7, 8
[52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.
Make-a-video: Text-to-video generation without text-video
data. In The Eleventh International Conference on Learning
Representations , 2023. 2
[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning , pages 2256â€“2265. PMLR, 2015.
2
[54] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
diffusion implicit models. In International Conference on
Learning Representations , 2021. 2, 3, 4, 5, 6, 7
[55] Yang Song and Stefano Ermon. Generative modeling by
estimating gradients of the data distribution. Advances in
Neural Information Processing Systems , 32, 2019. 1, 2, 4
[56] Yang Song and Stefano Ermon. Improved techniques for
training score-based generative models. Advances in neural
information processing systems , 33:12438â€“12448, 2020. 1, 4
[57] Deepak Sridhar, Niamul Quader, Srikanth Muralidharan,
Yaoxin Li, Peng Dai, and Juwei Lu. Class semantics-based at-
tention for action detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 13739â€“
13748, 2021. 7
[58] Jing Tan, Jiaqi Tang, Limin Wang, and Gangshan Wu. Re-
laxed transformer decoders for direct action proposal gener-
ation. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 13526â€“13535, 2021. 2,
7
[59] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 6450â€“6459, 2018. 7
[60] Elahe Vahdani and Yingli Tian. Deep learning-based action
detection in untrimmed videos: a survey. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2022. 1, 5
[61] Limin Wang, Yu Qiao, Xiaoou Tang, and Luc Van Gool. Ac-
tionness estimation using hybrid fully convolutional networks.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 2708â€“2717, 2016. 2, 7
[62] Yuetian Weng, Zizheng Pan, Mingfei Han, Xiaojun Chang,
and Bohan Zhuang. An efficient spatio-temporal pyramid
transformer for action detection. In European Conference on
Computer Vision , pages 358â€“375. Springer, 2022. 1, 2, 7
[63] Huijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region
convolutional 3d network for temporal activity detection. In
Proceedings of the IEEE international conference on com-
puter vision , pages 5783â€“5792, 2017. 1, 2
[64] Li Xu, Haoxuan Qu, Yujun Cai, and Jun Liu. 6d-diff: A
keypoint diffusion framework for 6d object pose estimation.
arXiv preprint arXiv:2401.00029 , 2023. 2
[65] Mengmeng Xu, Chen Zhao, David S Rojas, Ali Thabet, and
Bernard Ghanem. G-tad: Sub-graph localization for temporalaction detection. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 10156â€“
10165, 2020. 1, 2, 7
[66] Le Yang, Houwen Peng, Dingwen Zhang, Jianlong Fu, and
Junwei Han. Revisiting anchor mechanisms for temporal
action localization. IEEE Transactions on Image Processing ,
29:8535â€“8548, 2020. 7
[67] Serena Yeung, Olga Russakovsky, Greg Mori, and Li Fei-Fei.
End-to-end learning of action detection from frame glimpses
in videos. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 2678â€“2687, 2016. 2
[68] Runhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin
Zhao, Junzhou Huang, and Chuang Gan. Graph convolutional
networks for temporal action localization. In Proceedings of
the IEEE/CVF International Conference on Computer Vision ,
pages 7094â€“7103, 2019. 2, 7
[69] Chen-Lin Zhang, Jianxin Wu, and Yin Li. Actionformer: Lo-
calizing moments of actions with transformers. In European
Conference on Computer Vision , pages 492â€“510. Springer,
2022. 1, 2, 5, 7, 8
[70] Chen Zhao, Ali K Thabet, and Bernard Ghanem. Video self-
stitching graph network for temporal action localization. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 13658â€“13667, 2021. 1, 2, 7
[71] Peisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang,
and Qi Tian. Bottom-up temporal action localization with
mutual regularization. In Computer Visionâ€“ECCV 2020: 16th
European Conference, Glasgow, UK, August 23â€“28, 2020,
Proceedings, Part VIII 16 , pages 539â€“555. Springer, 2020. 7
[72] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xi-
aoou Tang, and Dahua Lin. Temporal action detection with
structured segment networks. In Proceedings of the IEEE
International Conference on Computer Vision , pages 2914â€“
2923, 2017. 2
[73] Zixin Zhu, Wei Tang, Le Wang, Nanning Zheng, and Gang
Hua. Enriching local and global contexts for temporal action
localization. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 13516â€“13525, 2021. 1,
7
18361
