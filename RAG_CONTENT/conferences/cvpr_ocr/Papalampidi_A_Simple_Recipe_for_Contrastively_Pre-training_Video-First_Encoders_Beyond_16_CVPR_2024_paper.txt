A Simple Recipe for Contrastively Pre-training Video-First Encoders
Beyond 16 Frames
Pinelopi Papalampidi* Skanda Koppula* Shreya Pathak*
Justin Chiu Joe Heyward Viorica Patraucean Jiajun Shen Antoine Miech
Andrew Zisserman Aida Nematzdeh
Google DeepMind
{pinelopi,skandak,shreyapa }@google.com
Abstract
Understanding long, real-world videos requires model-
ing of long-range visual dependencies. To this end, we
explore video-first architectures, building on the common
paradigm of transferring large-scale, imageâ€“text models to
video via shallow temporal fusion. However, we expose two
limitations to the approach: (1) decreased spatial capabil-
ities, likely due to poor videoâ€“language alignment in stan-
dard video datasets, and (2) higher memory consumption,
bottlenecking the number of frames that can be processed.
To mitigate the memory bottleneck, we systematically an-
alyze the memory/accuracy trade-off of various efficient
methods: factorized attention, parameter-efficient image-
to-video adaptation, input masking, and multi-resolution
patchification. Surprisingly, simply masking large portions
of the video (up to 75%) during contrastive pre-training
proves to be one of the most robust ways to scale encoders
to videos up to 4.3 minutes at 1 FPS. Our simple approach
for training long video-to-text models, which scales to 1B
parameters, does not add new architectural complexity and
is able to outperform the popular paradigm of using much
larger LLMs as an information aggregator over segment-
based information on benchmarks with long-range tempo-
ral dependencies (YouCook2, EgoSchema).
1. Introduction
Long-video understanding requires modeling of the tempo-
ral dynamics and long-range visual dependencies of real-
world scenes [63, 64]. However, capturing long-range vi-
sual content is challenging, even when equipped with large
language models. In this paper, we overcome hardware
memory limitations and demonstrate how to extend video
encoders to directly process minutes-long visual content us-
ing language grounding, and simple, established techniques
*Equal contribution.
Perceiver
Resampler<output summary> 
Step 1: Two-stage Contrastive Pre-training 
(a) image-to-short video, (b) short-to-long video  Step 2: Video-to-Text Fine-tuning 
Cross-attention 
Cross-attention 
Cross-attention Pretrained LM ðŸ¥¶
 
Temporal Pooling
Video ViT
(Joint Space-Time Attention) 
NCE Lossvideo
representationcaption
representation
Text Encoder (BERT)
<caption> 
<long video> (a) ðŸ”¥
â†’(b) semi- ðŸ¥¶
(a) ðŸ”¥
â†’(b) ðŸ¥¶
used during pre-trainingFigure 1. Two main training steps: (1) training a video encoder
via Noise Contrastive Estimation and (2) using this frozen video
encoder with a pre-trained, frozen LM and visual adapter layers
for video-to-text generation (e.g., video summarization and Q/A).
without additional architectural complexity [24, 64]. We fo-
cus on long videos through the lens of language, assessing
our models on the widely applicable tasks of visual summa-
rization and question-answering.
Recent work on visionâ€“language models have yielded
impressive results, predominantly focusing on understand-
ing images or short clips of 16 frames or less [1, 13, 30,
72, 73]. This work recycles strong pre-trained image en-
coders, performs late temporal fusion [1, 71, 73], and em-
ploys mostly-frozen, powerful LLMs. The lack of video-
first encoders, equipped with early temporal aggregation,
may handicap the ability to process complex visual depen-
dencies, and this is usually reflected in prior workâ€™s focus
on short video benchmarks ( <30seconds) in which sixteen
frames are sufficient for competitive performance [4, 29].
In this work, we systematically explore video-first mod-
els starting from a standard imageâ€“language recipe using
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14386
two-step training and large pre-trained LMs (Figure 1; [1]).
This baseline enables us to start from a demonstrably
scalable, simpler-to-tune, widely-used recipe that performs
competitively [15, 30]. Through our analysis, we are able
to scale this method in a memory-efficient manner to longer
sequences of frames, up to 4.3 minutes of video at 1 FPS.
We first explore video-first models on short-video bench-
marks (MSR-VTT [67], V ATEX [60], YouCook2 [81], Ac-
tivityNet [28]) and compare against the SoTA VideoCoCa
model [71]. We show that simple joint space-time atten-
tion significantly improves performance over frame-level
encodings on benchmarks with rich temporal dependencies
(YouCook2, V ATEX). Overall, our models are able to reach
VideoCoCa performance, while requiring fewer parameters
and lower frame resolution.
This performance gain incurs extra compute and mem-
ory costs that grow quadratically with the video length.
To address this, we provide one of the first systematic
analyses of the memory/accuracy pareto-front of popular
memory-efficient methods; this includes factorized atten-
tion, parameter-efficient image-to-video adaptation, input
masking, and multi-resolution patchification. Through this
analysis, we find that among all these options, simple to-
ken masking (up to 75%) during contrastive pre-training
incurs only a 1% Recall@1 drop on zero-shot text-video
retrieval, and no drop in zero-shot video captioning. At
the same time, such high masking offers 2-3x memory
savings and allows us to generalize to longer video con-
texts. The alternatives we explore ( e.g., efficient backbone
architectures, more sophisticated TubeViT-style patchifica-
tion [49]), do not maintain the same robustness against
noisy video inputs and present a 25% relative decrease in
performance for text-video retrieval on challenging bench-
marks (YouCook2, V ATEX). Finally, although parameter-
efficient methods [21, 22] fail to adapt image encoders to
video-first models without suffering performance drops, we
find that they can adapt video models trained on short con-
texts (e.g., 16 second videos) to longer temporal horizons.
Based on the above learnings, we extend our best per-
forming short-video encoder to longer contexts of 256
frames (4.3 minutes at 1 FPS). We use the full-length videos
of HowTo100M [42] accompanied by LLM-generated sum-
maries based on the ASR to further contrastively train our
LONG VIVIT while masking 75% of the input video to-
kens and freezing most parameters of the encoder. L ONG -
VIVIT-to-text ( âˆ¼1B parameters) is able to outperform
modular methods that use LLM assistance and PALI-
3 [9] for frame captioning on temporally rich benchmarks
(YouCook2, EgoSchema). Even modular methods that con-
sider frame selection (SeViLA [74]) or an oracle segmen-
tation of the video for localizing and captioning key events
(on YouCook2) cannot reach L ONG VIVITâ€™s performance.
An interesting byproduct of our work is that we can gleanwhich videoâ€“language benchmarks have strong temporal
dependencies, and thus are suitable for testing long video
models; we find that papers often use benchmarks in which
short video or even blind models perform well [5, 41, 67].
In short, we provide the following contributions:
â€¢ We explore the memory/accuracy pareto-frontier of
video-first visionâ€“language models, and systematically
evaluate many architectural, data, and training alterna-
tives. In the end, we identify a simple recipe that enables
scaling to 4.3 minutes at 1 FPS, many times longer than
comparable videoâ€“language models [1, 71].
â€¢ We identify short and long video benchmarks with sub-
stantial temporal dependencies, for which we demon-
strate that the traditional image-first, late-temporal fusion
recipe is convincingly weaker than a video-first approach.
â€¢ Finally, we compare our long video models to a variety
of strong baselines and show competitive performance
with far fewer parameters; this includes baselines that
use LLM-based aggregation over visual captions, and we
quantitatively evaluate this common approach for the first
time on standard video benchmarks.
2. Related Work
We base our recipes on [1, 30], which provide a strong
two-step videoâ€“language recipe that leverages pre-trained
LLMs and works at scale. Similar work at smaller scale
has additionally included captioning losses [32, 76], more
contrastive losses [10, 38, 43, 66], masking/masked autoen-
coding [15, 16, 18, 19, 33, 35, 40, 55], and combinations
thereof [13, 23, 54, 58, 61, 72, 73, 75, 82]. This work fo-
cuses on imageâ€“text modeling and extends to <30 seconds
via image-to-video transfer, selective fine-tuning, or tempo-
ral fusion of frame encodings [1, 71, 73].
A volume of work focuses on video-first learning. This
includes some of the very early work in image-to-video ker-
nel inflation [6, 53, 56], transformer-based video architec-
tures [2, 3, 37], image-to-video parameter-efficient adap-
tion [7, 36, 46], and multiple spatiotemporal resolutions
along different network paths [14, 39, 68, 70]. These have
still only been demonstrated on short videos, so other works
have broached the challenge of temporal scalability: [24, 51,
64] propose alternative encoders, and [27, 48, 59] propose
more exotic attention mechanisms. TubeViT [49] proposes
multi-granularity patchification. We systematically dissect
what works and scales among some of these alternatives,
electing options that enable us to re-use strong pre-trained
models and use standard, more easily-tuned architectures.
Specifically in video-to-text generation, approaches that
handle longer videos are very limited and mostly target
images or short videos [15, 31, 61]. A dominant ap-
proach is to summarize frames and aggregate information
via LLMs [31, 34, 62, 77]. To the best of our knowl-
edge, we are the first to attempt to train large-scale video-
14387
to-text models on longer sequences of frames and directly
test them against LLM-assisted modular methods on chal-
lenging temporal benchmarks [41, 81].
3. The Video-to-Text Architecture
We base our approach on the successful two-step recipe
that combines pre-trained vision and language models [ e.g.,
1, 30, 72, 73] as shown in Figure 1: (1) we first pre-train a
vision encoder, and then (2) fuse the frozen vision represen-
tations into a pre-trained, frozen LM.
3.1. Videoâ€“Language Contrastive Pre-training
Following common practice [1, 30], we use a dual visionâ€“
language architecture with a Noise Contrastive Estimation
(NCE) loss [17, 45, 65] to pre-train our vision encoder, sim-
ilar to CLIP [50], ALIGN [26] and VideoCLIP [66]. Both
encoders are transformers [57]: a BERT-medium (77M) or
base (117M) language encoder and ViT-Base (86M param-
eters) or Large (307M parameters) vision encoder. On the
language side, caption representations are computed by av-
eraging across the corresponding token representations. On
the vision side, video frames are patchified into a sequence
of visual tokens, fed into a vision encoder, and then average
pooled to produce a final video representation.
Most prior larger-scale videoâ€“language models use pre-
trained image encoders and patchify frames individually
via 2D convolutions [ e.g., 1, 66, 71]. Instead, we create
spatiotemporal tubelets via 3D convolutions as done in re-
cent vision-only models [2, 49, 55]. Using 3D tubelets in-
stead of flat patches has the dual advantage of higher in-
put compression and more explicit temporal contextualiza-
tion; our early experiments yielded improved performance.
The tubelet embedding sequence is then flattened, added to
learnable positional embeddings, and fed into the vision en-
coder. The vision encoder uses spatio-temporal attention
as in ViViT [2]: Joint space-time attention does not add any
new parameters to vanilla image ViT [12], facilitating trans-
fer between image and video models.
Training a large-scale transformer-based video encoder
can be challenging because self-attention across thousands
of visual tokens is both compute and memory intensive.
Memory bottlenecks a model in two ways: (1) limiting the
number of frames, and (2) limiting the contrastive batch size
during training, negatively impacting performance. To ad-
dress (2), we use a pre-trained image encoder trained with
large batch sizes, and further tune it on videos, instead of
jointly training from scratch on images and videos. For
initializing the 3D convolution, we repeat the pre-trained
weights across the temporal dimension similarly to [2]
(see Appendix A). During videoâ€“language pre-training, we
maintain different embedding paths for images vs. videos:
images are embedded with the original 2D convolution and
videos with a separate 3D convolution (no weight sharing).3.2. Video-to-Text Tuning
We follow prior work [ e.g., 1, 72, 73] by plugging the frozen
pre-trained vision encoder into a frozen pre-trained LM.
We first temporally mean pool the video representations to
keep a fixed number of tokens independently of the num-
ber of frames and next use a randomly initialized Perceiver-
resampler [25] to project the representations to the LM em-
bedding space (Appendix A). We add new randomly initial-
ized cross-attention layers at each layer of the LM to ground
generation on the visual content. We train the new lay-
ers and Perceiver resampler with a standard auto-regressive
video captioning loss: âˆ’logp(wt|w < t ;V), where wtis
itstthtoken, and Vis the video representation.
4. Memory-Efficient Encoder Design Space
Device memory is a key bottleneck for video training with
joint space-time attention. To overcome this, we explore
four broad categories of solutions: (1) efficient attention,
(2) parameter-efficient image-to-video adaptation, (3) input
token masking, and (4) multi-resolution patchification.
1. Attention mechanism. Factorized attention [2, 3] sep-
arates the temporal and spatial dimensions over which self-
attention is applied, reducing both memory and computa-
tional costs. However, this modification introduces a new
temporal block within each transformer layer making ini-
tialization and model tuning more challenging. In con-
trast to [2], that initializes the new blocks with zeroes, we
find that we achieve best performance when initializing
the temporal blocks with the same self-attention weights
of ViT. However, we add a gating mechanism which acts
as a residual connection between the self-attention blocks:
h=h+tanh(Î±)htemporal . Here, Î±is a trainable parame-
ter initialized to zero, that helps maintain the capabilities of
the original ViT during training.
2. Parameter-efficient adaptation. We explore using
parameter-efficient methods from NLP [8] to adapt im-
age encoders to video, while only tuning a small percent-
age of model parameters. Most prior work adapts image-
based models by freezing an image backbone and adding
late, trainable temporal-fusion layers [10, 71, 78]. In con-
trast, we explore ways to use pre-trained image encoders
and adapt them to video-first architectures [7, 36, 46].
Inspired by the success of parameter-efficient adaptation
in NLP [79], we consider using MLP Adapters [21] and
LoRA [22] (details in Appendix A). We also explore tun-
ing only temporal self-attention blocks [7], effectively as
adapter layers, in factorized attention. In all variants, we
still tune the video-specific 3D patch convolution.
3. Token masking. Most existing work samples videos at
a fixed frames per second (FPS) rate [ e.g., 1, 2, 55, 74].
However, semantics required for many videoâ€“language
tasks vary slowly in the temporal dimension [80] and videos
14388
present high degree of redundancy between consecutive
frames [55]. We explore ways to sparsely sample the video
input to reduce the number of input visual tokens. Specifi-
cally, we test random masking of input tubelet embeddings.
Since consecutive frames are largely redundant, the same
semantic signals could potentially be extracted even with
high masking rates. For example, [55] masks up to 95%
of the input video to reach optimal performance on the task
of video-masked autoencoding. We demonstrate similar re-
sults in a videoâ€“language setting.
4. Multi-resolution patchification. Finally, we test a
simple approach to reduce redundancy in videos via more
coarse-grained patchification in the temporal or spatial di-
mension, as commonly done in multiple-view video mod-
els [14, 39, 70]. However, this decreases frame resolution,
and may lose fine-grained information. As a result, we also
experiment with TubeViT [49] variant that combines flat
patches and tubelets of different granularity to mitigate in-
formation loss. Following [49], we use four different con-
volution kernels that can encode either coarse-grained tem-
poral or spatial information; details are in Appendix A.
5. Datasets and Benchmarks
For contrastive pre-training, we use: (1) 27M video-text
pairs (VTP) as described in [1], (2) HowTo100M [42]
(HT100M; 100M instructional YouTube clips aligned with
ASR using their timestamps, called HowTo100M Clips),
and (3) VideoCC3M [44] (3M video-text pairs based on
Conceptual Captions [52]). Unfortunately, we find the textâ€“
video alignment in VideoCC3M to be of poor quality; in-
stead, we use a modified variant with generated pseudo-
labeled captions of every video by PALI [9] (see Appen-
dices B, C). To pre-train with longer videos, we use a long
version of HowTo100M (referred to as HowTo100M Sum-
mary) consisting of (1) the full-length videos with an aver-
age duration of 6.5 minutes and (2) their textual summaries
generated by automatically cleaning and summarizing the
ASR transcripts using an LLM [20]. We also include the
image datasets of [1]. For video-to-text tuning, we use the
same mixture of datasets but exclude HowTo100M Clips,
since the noisy video-text alignments hurt performance.
We report text-video retrieval and captioning results on
short video benchmarks , with average video length â‰¤30
seconds: MSR-VTT [67], YouCook2 [81], ActivityNet
Captions [28], and V ATEX [60]. To evaluate performance
on longer videos, we consider video summarization on full-
length versions of YouCook2 and ActivityNet Captions,
with a video duration of up to 5 minutes, and multiple-
choice video question answering (QA) on EgoSchema [41].
6. Experimental Results
In Section 6.1, we describe our results evaluating alterna-
tives in memory-efficient video encoder design; options de-MSR-VTT V ATEX YC2 AN
T2V V2T T2V V2T T2V V2T T2V V2T
Joint ST-ViViT 39.6 38.1 23.8 26.3 12.3 13.6 6.7 6.4
Factorized ST-ViViT 40.2 36.9 25.3 25.4 11.6 12.7 6.6 7.4
Avg Frame-level 39.3 34.8 24.8 25.0 9.1 7.9 6.8 7.1
Att-pool Frame-level 38.4 37.5 21.9 26.1 9.0 8.9 6.1 6.2
Table 1. Text-video retrieval results (% Recall@1) when consid-
ering different visual backbones.
scribed in Section 4. For this analysis, we use ViT-B/BERT-
medium, with training details in Appendix B and ablations
on experimental design in Appendix C.
In Section 6.2, we combine our most competitive de-
sign choices from 6.1 and test our models on short and long
video understanding benchmarks. We scale our best model
variants to ViT-L/BERT-base with a 400M (or 1B) language
decoder. We test our short video models on text-video re-
trieval and video captioning, and our long video models on
video summarization and QA on 256-frame videos.
In Section 6.3, we share our experience working across
short and long video benchmarks [5, 11, 41, 60, 67], offer-
ing insights about which ones yield robust temporal signal.
6.1. Exploration of Memory-Efficient Designs
We explore memory-efficient methods to train video-first
encoders as described in Section 4. We first consider short
video inputs of 16 frames at 1 FPS and report peak train-
time memory consumption vs. performance on text-video
retrieval on short video benchmarks [5]. Then, we test
whether our main findings hold for longer inputs (128+
frames) on video summarization on full-length YouCook2.
Base architectures. We explore the memory/accuracy
trade-off of different visual backbones in Table 1: ViViT
with joint space-time attention ( i.e., Joint ST-ViViT), ViViT
with factorized attention ( i.e., Factorized ST-ViViT) [2],
and frame-level (ViT-based) image encodings with average
or attentional pooling (â€˜att-poolâ€™) [1, 71]. Different methods
perform similarly, especially on MSR-VTT and ActivityNet
(AN). Interestingly, attentional pooling on top of frame-
level encodings does not improve performance. ViViT
with either joint or factorized attention performs best and
presents higher gains for YouCook2 (YC2), the more tem-
porally challenging benchmark [6.3]. In contrast to prior
work [ e.g., 10, 71] which tests frozen image-to-video trans-
fer and claims joint attention to be inferior, we find it to be
competitive in this fully fine-tuned setting.
Architectures and token masking. We now test robust-
ness of backbones when masking part of the input tubelets
(0-75%). We report Recall@1 on text-to-video retrieval for
YouCook2 and V ATEX1per backbone for different masking
1We do not observe significant sensitivity to input masking for MSR-
VTT and ActivityNet Captions across all configurations (Section 6.3).
14389
6 7 8 9 10 11 12 13 14
Memory consumption (GB)678910111213T ext-to-video R@1 - YouCook2JST 0%JST 25%JST 50%
JST 75%
Adapters
LoRAFrame 0%
Frame 25%
Frame 50%
Frame 75%FST 0%
FST 25%
FST 50% FST 75%
FST Adaptation
6 7 8 9 10 11 12 13 14
Memory consumption (GB)1920212223242526T ext-to-video R@1 - VATEXJST 0% JST 25% JST 50% JST 75%
Adapters
LoRAFrame 0%
Frame 25%
Frame 50%
Frame 75%FST 0%
FST 25%
FST 50%
FST 75%FST AdaptationFigure 2. Trade-offs between performance (% text-to-video Recall@1; y axis) and train-time memory consumption (x axis) for different
backbones (joint space-time (JST), factorized space-time (FST), and drame-level encodings) with random input masking (0% up to 75%)
or parameter-efficient methods for training (Adapters, LoRA, factorized temporal (FST) adaptation; lower opacity).
Memory increase
from Base to LargeDrop (R@1)
at BaseDrop (R@1)
at Large020406080100% DifferenceJoint ST 75% masking
Frame-level 75% masking
Factorized ST 75% masking
Joint ST - Adapters
Joint ST - LoRA
Factorized T emp Adaptation
Figure 3. Difference (%) in memory consumption for different
model scales: (ViT-B vs ViT-L). We also report performance drop
of efficient methods presented in Figure 2 in comparison with the
vanilla approach (i.e., no input masking and full fine-tuning) at
different model scales to test whether behavior is similar.
ratios in Figure 2. Joint space-time attention (JST) is robust
against noise from masking up to 75% during pre-training.
The same does not hold for frame-level encodings and fac-
torized attention (FST), where performance drops consis-
tently as we increase masking. We conclude that JST can
better handle noisy inputs and use it in further exploration.
Parameter-efficient adaptation. We next report perfor-
mance of parameter-efficient image-to-video adaptation in
Figure 2. We consider (1) JST with (a) MLP Adapters at ev-
ery layer of the encoder, (b) LoRA with rank decomposition
matrices in the self-attention and feed-forward transformer
blocks, and (2) factorized temporal adaptation where we
tune the temporal self-attention. No adaptation method can
reach the memory savings provided by high input masking,
since we tune parameters depthwise and gradient computa-
tion still requires backpropagation through the model. At
the same time, we see significant performance drop, sug-
gesting that adaptation of spatial-only models to the tem-
poral dimension cannot be sufficiently addressed in semi-
frozen fashion. Comparing parameter-efficient methods, we
find MLP Adapters to be more competitive than LoRA,which is now canonical for LLMs. We hypothesize that
LoRA is successful for tuning very small portions of the
network and performing â€œeasierâ€ in-modality transfer.
Adaptation at scale. We next scale from ViT-B/86M to
ViT-L/307M in Figure 3 and test whether observations hold
with different model scales. We present the % memory in-
crease from base to large (left bar set) and % performance
decrease of each method at each scale2. Joint ST exhibits
a similar memory pattern to frame-level, while leading to
smaller accuracy drops, whereas factorized ST presents sig-
nificant memory overhead with model scale due to the extra
temporal parameters. For this reason, we exclude factor-
ized ST from further experimentation. Finally, parameter-
efficient methods are unable to achieve competitive perfor-
mance at both model scales, although their memory require-
ments scale better with model size.
Multi-resolution patchification. Given the outsized
memory impact of input token count in Figure 4, we ad-
ditionally analyze: (1) coarse-grained patchification in the
temporal (convolution over 4 instead of 2 frames) and/or
spatial (convolution over 32x32 instead of 16x16 pixel
spaces) dimension, and (2) the TubeViT [49] approach of
multiple tube kernels of different spatiotemporal size and
strides. For all benchmarks, masking the input at high
ratios while maintaining a fine granularity of tubelets de-
creases performance significantly less than other input pro-
cessing methods. Temporal coarse-grained patchification
negatively affects benchmarks with richer temporal depen-
dencies (i.e., YouCook2, V ATEX) more than spatial. The
opposite trend holds for datasets depending on spatial un-
derstanding (i.e., MSR-VTT, ActivityNet Captions3). Tube-
ViT acts as the middle ground between the two by em-
ploying multiple kernels, with some performance degrada-
tion across all benchmarks. However, it is not able to al-
leviate the negative effects caused by considering coarser-
2Performance drop for factorized ST is omitted since the variant with-
out masking leads to out of memory issues.
3Omitted from Figure 4 but follows same patterns as MSR-VTT.
14390
4 6 8 10 12 14
Memory consumption (GB)10.010.511.011.512.0YouCook20%
25%
50%
75%
TubeViTCoarse temp
Coarse spaceMemory Budget
4 6 8 10 12 14
Memory consumption (GB)22.523.023.524.0VATEX0%25% 50%75%
TubeViTCoarse tempCoarse space
Memory Budget
4 6 8 10 12 14
Memory consumption (GB)353637383940MSR-VTT0% 25%
50%75%
TubeViTCoarse   
temp    
Coarse spaceMemory
BudgetFigure 4. Trade-offs between performance (text-to-video Recall@1; y axis) and memory consumption (x axis) for input sampling methods:
(1) high input masking ratios (0% to 75%) with joint space-time attention, (2) coarse-grained temporal (Coarse temp) and/or spatial (Coarse
space) patchification with a fixed kernel and TubeViT which samples parts of the video with multiple 3D kernels of different granularity.
Short ViViT
w/o MaskingLong
ViViT
LongViViT w/ temp
coarse-grained 
+ 35% MaskingLongViViT
w/ spatial coarse-grainedLongViViT
+ Adapters
+ 75% MaskingLongViViT
+ 75% Masking
 + tuning last 4 layersImage  
ViT   4 6 8 10 12 14 1651015
202224262830Rouge-L
Memory Consumption (GB) during short-to-long video contrastive tuningMemory Consumption
 video-to-text
Figure 5. Scaling memory-efficient methods to more frames (i.e., 128 frames) for ViViT-B and variants. We measure performance for
video-to-text summarization on the full-length YouCook2 videos via Rouge-L (color-coded) while keeping track of memory consumption
during short-to-long video contrastive tuning ( x-axis) and video-to-text tuning ( y-axis).
grained information and presents higher memory require-
ments due to the multiple convolutions. Overall, we find
that high masking with Joint ST and small tubelets yields
the strongest memory/performance curves.
Scaling to longer videos. We now test the best methods
from Figure 4 on 128 input frames (32.7k visual tokens).
We select methods that are within a memory budget (red
vertical lines) and would fit on a 16GB device when ex-
panded to long videos (128+ frames). We contrastively fine-
tune [3.1] our best performing video model (i.e., Joint ST
referred to as S HORT VIVIT) on sequences of 128 frames on
HowTo100M Summary [5], as detailed in Appendix B. We
refer to this model as L ONG VIVIT. Finally, we fine-tune
LONG VIVIT for text generation (Section 3.2) on the full-
length YouCook2, and report Rouge-L in Figure 5, mea-
suring memory consumption during both long-context con-
trastive ( x-axis) and video-to-text ( y-axis) tuning.
Validating our previous results, I MAGE VIT (frame-level
encodings) trained on longer videos with 75% masking4
significantly under-performs video-first models (10 R-L
drop). S HORT VIVIT without further HT100M Summary
training performs better than I MAGE VIT, but cannot match
models adapted to longer videos. L ONG VIVIT improves
performance by 1.8 Rouge-L points over S HORT VIVIT.
Comparing input masking with coarser-grained patchifica-
tion5provides similar insights to the previous paragraph.
4We start from I MAGE VIT trained on short videos with no masking.
5Using the same fine-grained S HORT VIVIT model for initialization.Finally, we test MLP Adapters [21] for tuning S HORT -
VIVIT to longer videos and observe no performance drop
in comparison with full fine-tuning. This provides further
evidence that parameter-efficient methods can be used for
â€œeasier transfersâ€ but not temporal adaptation of spatial-
only models. One downside of MLP Adapters is that it in-
creases parameter count during video-to-text tuning ( y-axis
in Figure 5). Thus, we also experiment with contrastively
tuning only the last four layers of the model. With this,
we observe a further 3x decrease in memory, since we tune
the network widthwise and excise early layer gradient com-
putation. At the same time, there is no memory increase
for video-to-text and no performance degradation. We con-
clude that this combination (high input masking and tuning
the last layers) is an effective setting for longer video adap-
tation. Given the observed robustness to masking, to further
decrease video-to-text memory, we also mask 30% of the
input video during training and inference without observing
any drop in summarization performance (see Appendix C).
6.2. Main Results
Short video benchmarks. We present our main results
on short video benchmarks in Table 2. We use ViT-L with
BERT-base for contrastive pre-training (Section 3.1) and a
400M frozen LM for video-to-text tuning (Section 3.2). Our
entire video-to-text model accounts for âˆ¼900M parameters,
although we additionally test scaling the frozen LM to 1B
parameters ( âˆ¼1.5B total count). We report Recall@1 for
zero-shot text-video retrieval and CIDEr for zero-shot and
14391
MSR-VTT V ATEX YouCook2 ActivityNet
Zero-shot FT Zero-shot FT Zero-shot FT Zero-shot FT
T2V/V2T C1/C2 C1 T2V/V2T C1/C2 C1 T2V/V2T C1/C2 C1 T2V/V2T C1/C2 C1
IMAGE VIT-L 30.9/ 41.6 24.6/25.1 63.6 36.2/ 42.9 37.9/39.4 61.1 18.2/16.8 14.5/16.5 95.9 20.6/18.2 16.3/17.7 41.1
SHORT VIVIT-L 31.9/38.9 32.7/32.9 63.1 37.8/42.8 43.6 /43.0 67.5 20.4 /20.5 21.0 /22.1 131.9 21.3 /18.9 25.2/26.1 44.8
EffS HORT VIVIT-L 29.9/38.3 33.8/33.9 63.8 34.4/42.7 41.3/42.7 64.7 20.5/20.3 21.1 /21.7 127.1 20.1/17.7 27.0/26.5 41.1
VideoCoCa-L [71] 33.3/â€“ 24.3 â€“ â€“ â€“ â€“ 18.9/â€“ 20.7 â€“ 31.5*/â€“ 17.4 â€“
VideoCoCa-2.1B 34.3 /64.7 27.1 73.2 53.2/73.6 22.8 77.8 20.3/â€“ 34.3 128.0 34.5*/33.0* 19.3 39.3
Flamingo-3B [1] â€“ â€“ â€“ â€“ 40.1 â€“ â€“ 55.8 â€“ â€“ â€“ â€“
Table 2. We present three model variants: I MAGE VIT-L, that uses frame-level encodings with a late temporal fusion trained on im-
ages and videos, S HORT VIVIT-L, our best performing video-first model with joint space-time attention, and Efficient S HORT VIVIT-L
(EffS HORT VIVIT-L) where we apply 75% train-time masking for 3x memory savings. We also report performance for SoTA image-first
models: VideoCoCa-L and Flamingo-3B, although they are bigger and not directly comparable. We report Recall@1 for zero-shot text-
to-video (T2V) and video-to-text (V2T) retrieval, and CIDEr for zero-shot and fine-tuned (FT) captioning when considering a 400M (C1)
or 1B (C2) frozen LM for generation. ActivityNet retrieval results marked with * are not directly comparable, as these models uniformly
sample frames, whereas we use the first frames of the long video with a fixed FPS of 1 to match experimental settings across benchmarks.
fine-tuned video captioning. We consider three model vari-
ants: frame-level encodings I MAGE VIT, S HORT VIVIT,
and S HORT VIVIT with 75% masking that uses 2-3x less
memory (referred to as Efficient SHORT VIVIT). We also
report results for VideoCoCa [71] and Flamingo [1]6.
Our results remain consistent with our earlier observa-
tions. Contextualizing only intra-frame dependencies cou-
pled with late temporal fusion (I MAGE VIT) leads to inferior
performance for retrieval and captioning on benchmarks
with richer temporal dependencies (YouCook2, V ATEX)
but performs better on retrieval on MSR-VTT which relies
on spatial understanding. Video-first architectures further
tuned on video datasets (substantially noisier than curated
image ones) improve temporal capabilities at the expense
of spatial. For Efficient S HORT VIVIT, we find that mask-
ing 75% of the input video causes a performance drop: an
average of 1% absolute difference on zero-shot retrieval and
no significant difference on zero-shot captioning across all
benchmarks. The efficient model still performs similarly or
better than I MAGE VIT, especially on captioning and tem-
porally rich benchmarks (e.g., YouCook2, V ATEX), while
consuming significantly less memory. Finally, when scal-
ing the frozen LM component from 400M to 1B (C1 â†’C2)
for zero-shot video-to-text generation, we observe moderate
improvements across benchmarks and variants.
We compare our results against large image-based mod-
els with SoTA performance on video benchmarks (second
block of Table 2). Although results are not directly com-
parable due to different experimental settings, we are com-
petitive and achieve even better results for temporally rich
benchmarks (i.e., YouCook2) on text-video retrieval for
models of similar parameter count7. Moreover, our models
6Models are not directly comparable due to different pre-training
datasets, model sizes, training regimes, and input resolution. For in-
stance, [71] fully fine-tune the LM and report results for 576 Ã— 576 frame
resolution instead of 256 Ã— 256.
7Video-text retrieval results on ActivityNet Captions are not compara-significantly outperform VideoCoCa on most video caption-
ing benchmarks even when considering their much larger
versions in the zero-shot setting. Finally, when fine-tuning
our video-to-text models with the 400M LM, we are again
able to match and surpass the performance of the larger
VideoCoCa-2.1B in two out of four benchmarks.
Long video understanding. We further tune L ONG -
VIVIT-L on 256-frame HT100M Summary videos and
evaluate zero-shot/fine-tuned summarization (YouCook2,
ActivityNet) and QA (EgoSchema released subset); this is
shown in Table 3. We additionally report results of L ONG -
VIVIT on Perception Test [47] in Appendix D, where
videos are short but can benefit from higher FPS.
We consider two families of models. 1. Models that take
as input 256 frames (first block of Table 3): I MAGE VIT and
SHORT VIVIT pre-trained on 16-frame clips, and L ONG -
VIVIT further trained on 256-frame clips. 2. Modular ap-
proaches from prior work (second block of Table 3): (a)
SeViLA Localizer [74] for localizing important frames in
the long video given a textual query which are then fed into
SHORT VIVIT for performing the task8, and (b) the popular
paradigm of captioning video segments or frames and using
an LLM to aggregate information and form coherent sum-
maries or answer questions [31, 34, 77]. We try the latter
approach with I MAGE VIT and S HORT VIVIT, generating
captions over 16-second video segments and then feeding
the captions to the September 2023 release of Bard, a much
larger LLM than the ones used in previous results. We cap-
tion clips using uniform video segmentation (every 16 sec-
onds) or an oracle segmentation when available (i.e., we
consider ground-truth start and end timestamps for differ-
ent events within ActivityNet and YouCook2 videos). We
ble since we are only considering the first 16 seconds of the video, whereas
[71] uniformly sample frames from the entire video ( âˆ¼180 seconds).
8We select 16 frames using the pre-trained localizer provided by [74].
For video summarization, we use synthetic summaries of the video gener-
ated by PALI+Bard as the textual query for retrieving frames.
14392
Zero-shot Fine-tuned
AN YC2 ES AN YC2
Inference with 256 frames
IMAGE VIT 14.4 4.6 40.8 23.8 29.4
SHORT VIVIT 15.4 7.0 47.9 24.3 29.5
LONG VIVIT 15.2 20.3 56.8 24.0 30.6
Modular approaches with 16-frame video models
SeViLA-to-S HORT VIVIT 16.2 4.2 49.6 24.4 28.3
IMAGE VIT-to-Bard 18.1 15.8 35.0 22.9 19.1
+ oracle segments 16.3 16.2 â€“ 22.7 22.1
SHORT VIVIT-to-Bard 19.3 18.1 42.0 22.7 20.8
+ oracle segments 18.3 18.2 â€“ 22.7 24.7
PALI [9] 5B-to-Bard 22.0 19.9 44.8 â€“ â€“
Blind Bard â€“ â€“ 27.0 â€“ â€“
SoTA [69] â€“ â€“ â€“ 36.9 34.6
Table 3. Results on long video-to-text benchmarks. We report
Rouge-L for zero-shot and fine-tuned video summarization on Ac-
tivityNet Captions (AN) and YouCook2 (YC2) and zero-shot ac-
curacy (%) for multiple choice QA on EgoSchema (ES).
also test substituting our small video models with PALI-
3 (5B parameters) for frame captioning9. Finally, we ref-
erence the SoTA fine-tuned performance on ActivityNet
and YouCook2, when using specialized models with pre-
computed features by multiple networks, object detectors,
and domain-specific vocabulary [69].
Looking through Table 3, we find that on ActivityNet,
which contains less temporal dependencies [6.3], modular
approaches via frame selection or LLM-based aggregation
of information (second block) perform well. Frame caption-
ing via PALI combined with the power of LLMs is enough
for the task in a zero-shot setting. For fine-tuned models,
feeding either the long input or selected frames into S HORT -
VIVIT perform better than utilizing Bard. On ActivityNet,
we see no benefit from training further on longer videos.
In contrast, we find that short video and modular models
are insufficient for addressing video tasks with longer-range
temporal dependencies (YouCook2, EgoSchema). Adapt-
ing S HORT VIVIT to longer contexts (L ONG VIVIT) signif-
icantly improves performance and achieves the best scores
across all comparison approaches. Using Bard as an in-
formation aggregator over individual clip captions cannot
achieve competitive performance, even when considering
an oracle video segmentation for YouCook2 (Lines 3 and 5
in the second block of Table 3). Surprisingly, even using a
much larger and more powerful image-based model (PALI)
cannot reach L ONG VIVIT on YouCook2 and EgoSchema.
Interestingly, selecting 16 key frames and feeding them
into S HORT VIVIT also outperforms Bard-based methods
on EgoSchema and fine-tuned YouCook2. This suggests
there can be temporal dependencies in long videos that can-
not be resolved even with an optimal event segmentation for
the video, or be aggregated by LLMs given inprecise visual
9We consider captions of key frames per 8 seconds of video.
MSR-VTT VATEX YouCook2 ActivityNet60
40
20
0% performance difference
(video-to-text R@1)Removing video data
Removing image dataFigure 6. Performance difference (%) per benchmark when we
remove (1) video or (2) image data from the training mixture.
information. On such benchmarks, L ONG VIVIT demon-
strates strong performance even without LLM assistance.
6.3. Brief Notes on Video Evaluations
We briefly describe some of our findings on video evalu-
ations. Firstly, we find that blind Bard is able to achieve
SoTA results on the full set of EgoSchema (no visual in-
put; 33.9% accuracy vs. 32.1% for the best model in [41]).
Adding visual information from PALI into Bard increases
performance to just 39.2%. However, on EgoSchemaâ€™s re-
leased subset , performance of blind Bard is 27%, which is
much lower than PALI-to-Bard (44.8%), suggesting that the
subset contains questions that rely more on visual ground-
ing than pure language reasoning, so we report numbers on
the subset in Table 3 and on the full set in Appendix ??.
Figure 6 details a simple ablation across other video
benchmarks to quantify temporal richness. We test re-
moving either video or image data from the training mix
and measure the effect on performance (video-to-text Re-
call@1). We see a dramatic performance drop when re-
moving video data for YouCook2 and V ATEX (up to 75%).
ActivityNet and MSRVTT suffer more from the absence of
image data, whereas non-video training influences perfor-
mance in lesser degree (as little as 18% for MSR-VTT).
We believe thereâ€™s room for more fine-grained, temporal-
focused videoâ€“language benchmarks in the community.
7. Conclusions
In short, we systematically analyze memory-efficient meth-
ods to scale video-first architectures to longer sequences of
frames and demonstrate that just masking high percentages
of the video ( â‰¤75%) yields competitive results on long
videoâ€“language tasks. Such masking shows a very small
performance drop on short videos, provides 2-3x memory
savings and allows scaling up to 4.3 minutes at 1 FPS
(LONG VIVIT) when freezing part of the short video
network in our two-stage training. L ONG VIVIT outper-
forms modular approaches with LLM assistance on video
summarization and QA on benchmarks with richer tem-
poral dependencies (YouCook2, EgoSchema). We overall
demonstrate that encoding longer-range visual dependen-
cies can make a difference in downstream performance
and corrects mistakes that LLMs are unable to rectify.
14393
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
Neural Information Processing Systems , 35:23716â€“23736,
2022. 1, 2, 3, 4, 7
[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen
Sun, Mario Lu Ë‡ciÂ´c, and Cordelia Schmid. Vivit: A video
vision transformer. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 6836â€“6846,
2021. 2, 3, 4
[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
space-time attention all you need for video understanding?
InICML , page 4, 2021. 2, 3
[4] Shyamal Buch, Crist Â´obal Eyzaguirre, Adrien Gaidon, Jiajun
Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting theâ€
videoâ€ in video-language understanding. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 2917â€“2927, 2022. 1
[5] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
and Juan Carlos Niebles. Activitynet: A large-scale video
benchmark for human activity understanding. In Proceed-
ings of the ieee conference on computer vision and pattern
recognition , pages 961â€“970, 2015. 2, 4
[6] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition , pages 6299â€“6308, 2017. 2
[7] Dongsheng Chen, Chaofan Tao, Lu Hou, Lifeng Shang, Xin
Jiang, and Qun Liu. Litevl: Efficient video-language learn-
ing with enhanced spatial-temporal modeling. In Proceed-
ings of the 2022 Conference on Empirical Methods in Natu-
ral Language Processing , pages 7985â€“7997, 2022. 2, 3
[8] Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex
Smola, and Diyi Yang. Parameter-efficient fine-tuning de-
sign spaces. arXiv preprint arXiv:2301.01821 , 2023. 3
[9] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,
Jialin Wu, Paul V oigtlaender, Basil Mustafa, Sebastian
Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski, et al.
Pali-3 vision language models: Smaller, faster, stronger.
arXiv preprint arXiv:2310.09199 , 2023. 2, 4, 8
[10] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit
Bansal, and Gedas Bertasius. Vindlu: A recipe for ef-
fective video-and-language pretraining. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 10739â€“10750, 2023. 2, 3, 4
[11] Pradipto Das, Chenliang Xu, Richard F Doell, and Jason J
Corso. A thousand frames in just a few words: Lingual
description of videos through latent topics and sparse ob-
ject stitching. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 2634â€“2641,
2013. 4
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-formers for image recognition at scale. In International Con-
ference on Learning Representations , 2020. 3
[13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-
e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378 , 2023. 1, 2
[14] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and
Kaiming He. Slowfast networks for video recognition. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 6202â€“6211, 2019. 2, 4
[15] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang
Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end
video-language transformers with masked visual-token mod-
eling. arXiv preprint arXiv:2111.12681 , 2021. 2
[16] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang
Wang, Lijuan Wang, and Zicheng Liu. An empirical study
of end-to-end video-language transformers with masked vi-
sual modeling. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 22898â€“
22909, 2023. 2
[17] Michael Gutmann and Aapo Hyv Â¨arinen. Noise-contrastive
estimation: A new estimation principle for unnormalized
statistical models. In Proceedings of the thirteenth inter-
national conference on artificial intelligence and statistics ,
pages 297â€“304. JMLR Workshop and Conference Proceed-
ings, 2010. 3
[18] Tengda Han, Weidi Xie, and Andrew Zisserman. Turbo train-
ing with token dropout. arXiv preprint arXiv:2210.04889 ,
2022. 2
[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
Dollâ€™ar, and Ross B Girshick. Masked autoencoders are scal-
able vision learners. 2022 ieee. In CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 15979â€“
15988, 2021. 2
[20] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,
Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego
de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan
Clark, et al. Training compute-optimal large language mod-
els.arXiv preprint arXiv:2203.15556 , 2022. 4
[21] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona
Attariyan, and Sylvain Gelly. Parameter-efficient transfer
learning for nlp. In International Conference on Machine
Learning , pages 2790â€“2799. PMLR, 2019. 2, 3, 6
[22] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International
Conference on Learning Representations , 2021. 2, 3
[23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
Owais Khan Mohammed, Qiang Liu, et al. Language is
not all you need: Aligning perception with language mod-
els.arXiv preprint arXiv:2302.14045 , 2023. 2
[24] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip
classification with state-space video models. In European
Conference on Computer Vision , pages 87â€“104. Springer,
2022. 1, 2
14394
[25] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira. Perceiver: General
perception with iterative attention. In International confer-
ence on machine learning , pages 4651â€“4664. PMLR, 2021.
3
[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representa-
tion learning with noisy text supervision. In International
Conference on Machine Learning , pages 4904â€“4916. PMLR,
2021. 3
[27] Nikita Kitaev, Åukasz Kaiser, and Anselm Levskaya.
Reformer: The efficient transformer. arXiv preprint
arXiv:2001.04451 , 2020. 2
[28] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
Proceedings of the IEEE international conference on com-
puter vision , pages 706â€“715, 2017. 2, 4
[29] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single
frame bias for video-and-language learning. arXiv preprint
arXiv:2206.03428 , 2022. 1
[30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 1, 2, 3
[31] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai
Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.
Videochat: Chat-centric video understanding. arXiv preprint
arXiv:2305.06355 , 2023. 2, 7
[32] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng
Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-
language understanding as masked language modeling. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 23119â€“23129, 2023. 2
[33] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichten-
hofer, and Kaiming He. Scaling language-image pre-training
via masking. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 23390â€“
23400, 2023. 2
[34] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin,
Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin
Liang, Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang.
Mm-vid: Advancing video understanding with gpt-4v(ision),
2023. 2, 7
[35] Yuanze Lin, Chen Wei, Huiyu Wang, Alan Yuille, and Ci-
hang Xie. Smaug: Sparse masked autoencoder for effi-
cient video-language pre-training. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2459â€“2469, 2023. 2
[36] Ruyang Liu, Jingjia Huang, Ge Li, Jiashi Feng, Xinglong
Wu, and Thomas H Li. Revisiting temporal modeling for
clip-based image-to-video knowledge transferring. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 6555â€“6564, 2023. 2, 3
[37] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang,
Stephen Lin, and Han Hu. Video swin transformer. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 3202â€“3211, 2022. 2[38] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
Nan Duan, and Tianrui Li. Clip4clip: An empirical study of
clip for end to end video clip retrieval and captioning. Neu-
rocomputing , 508:293â€“304, 2022. 2
[39] Chuofan Ma, Qiushan Guo, Yi Jiang, Ping Luo, Zehuan
Yuan, and Xiaojuan Qi. Rethinking resolution in the context
of efficient video recognition. Advances in Neural Informa-
tion Processing Systems , 35:37865â€“37877, 2022. 2, 4
[40] Yue Ma, Tianyu Yang, Yin Shan, and Xiu Li. Simvtp: Sim-
ple video text pre-training with masked autoencoders. arXiv
preprint arXiv:2212.03490 , 2022. 2
[41] Karttikeya Mangalam, Raiymbek Akshulakov, and Jiten-
dra Malik. Egoschema: A diagnostic benchmark for very
long-form video language understanding. arXiv preprint
arXiv:2308.09126 , 2023. 2, 3, 4, 8
[42] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2630â€“2640, 2019. 2, 4
[43] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan
Laptev, Josef Sivic, and Andrew Zisserman. End-to-end
learning of visual representations from uncurated instruc-
tional videos. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 9879â€“
9889, 2020. 2
[44] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja
Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid.
Learning audio-video modalities from image captions. In
Computer Visionâ€“ECCV 2022: 17th European Conference,
Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part
XIV, pages 407â€“426. Springer, 2022. 4
[45] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-
sentation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748 , 2018. 3
[46] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hong-
sheng Li. St-adapter: Parameter-efficient image-to-video
transfer learning. Advances in Neural Information Process-
ing Systems , 35:26462â€“26477, 2022. 2, 3
[47] Viorica P Ë˜atrË˜aucean, Lucas Smaira, Ankush Gupta, Adri `a Re-
casens Continente, Larisa Markeeva, Dylan Banarse, Skanda
Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang,
et al. Perception test: A diagnostic benchmark for mul-
timodal video models. arXiv preprint arXiv:2305.13786 ,
2023. 7
[48] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico
Shippole. Yarn: Efficient context window extension of large
language models. arXiv preprint arXiv:2309.00071 , 2023. 2
[49] AJ Piergiovanni, Weicheng Kuo, and Anelia Angelova. Re-
thinking video vits: Sparse video tubes for joint image and
video learning. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 2214â€“
2224, 2023. 2, 3, 4, 5
[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
14395
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748â€“8763. PMLR, 2021. 3
[51] Michael S Ryoo, Keerthana Gopalakrishnan, Kumara Kahat-
apitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Ju-
lian Ibarz, and Anurag Arnab. Token turing machines. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 19070â€“19081, 2023. 2
[52] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) , pages
2556â€“2565, 2018. 4
[53] Karen Simonyan and Andrew Zisserman. Two-stream con-
volutional networks for action recognition in videos. Ad-
vances in neural information processing systems , 27, 2014.
2
[54] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guil-
laume Couairon, Wojciech Galuba, Marcus Rohrbach, and
Douwe Kiela. Flava: A foundational language and vision
alignment model. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
15638â€“15650, 2022. 2
[55] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
Videomae: Masked autoencoders are data-efficient learners
for self-supervised video pre-training. In Advances in Neural
Information Processing Systems , 2022. 2, 3, 4
[56] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann
LeCun, and Manohar Paluri. A closer look at spatiotemporal
convolutions for action recognition. In Proceedings of the
IEEE conference on Computer Vision and Pattern Recogni-
tion, pages 6450â€“6459, 2018. 2
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 3
[58] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Lu-
owei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang
Jiang, and Lu Yuan. Omnivl: One foundation model for
image-language and video-language tasks. Advances in neu-
ral information processing systems , 35:5696â€“5710, 2022. 2
[59] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv preprint arXiv:2006.04768 , 2020. 2
[60] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang
Wang, and William Yang Wang. Vatex: A large-scale, high-
quality multilingual dataset for video-and-language research.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 4581â€“4591, 2019. 2, 4
[61] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun
Wang, et al. Internvideo: General video foundation models
via generative and discriminative learning. arXiv preprint
arXiv:2212.03191 , 2022. 2
[62] Zhenhailong Wang, Manling Li, Ruochen Xu, Luowei Zhou,
Jie Lei, Xudong Lin, Shuohang Wang, Ziyi Yang, Chen-
guang Zhu, Derek Hoiem, et al. Language models withimage descriptors are strong few-shot video-language learn-
ers.Advances in Neural Information Processing Systems , 35:
8483â€“8497, 2022. 2
[63] Chao-Yuan Wu and Philipp Krahenbuhl. Towards long-form
video understanding. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1884â€“1894, 2021. 1
[64] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi
Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer.
Memvit: Memory-augmented multiscale vision transformer
for efficient long-term video recognition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13587â€“13597, 2022. 1, 2
[65] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 3733â€“3742,
2018. 3
[66] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko,
Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and
Christoph Feichtenhofer. Videoclip: Contrastive pre-training
for zero-shot video-text understanding. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Lan-
guage Processing , pages 6787â€“6800, 2021. 2, 3
[67] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5288â€“5296, 2016. 2, 4
[68] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,
Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-
vancing high-resolution video-language representation with
large-scale video transcriptions. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 5036â€“5045, 2022. 2
[69] Kashu Yamazaki, Khoa V o, Quang Sang Truong, Bhiksha
Raj, and Ngan Le. Vltint: visual-linguistic transformer-in-
transformer for coherent video paragraph captioning. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
pages 3081â€“3090, 2023. 8
[70] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu,
Mi Zhang, Chen Sun, and Cordelia Schmid. Multiview
transformers for video recognition. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 3333â€“3343, 2022. 2, 4
[71] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, So-
ham Ghosh, Yonghui Wu, and Jiahui Yu. Video-text model-
ing with zero-shot transfer from contrastive captioners. arXiv
preprint arXiv:2212.04979 , 2022. 1, 2, 3, 4, 7
[72] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Zero-shot video question answering via
frozen bidirectional language models. Advances in Neural
Information Processing Systems , 35:124â€“141, 2022. 1, 2, 3
[73] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 1, 2, 3
14396
[74] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.
Self-chained image-language model for video localization
and question answering. arXiv preprint arXiv:2305.06988 ,
2023. 2, 3, 7
[75] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, et al. Florence: A new
foundation model for computer vision. arXiv preprint
arXiv:2111.11432 , 2021. 2
[76] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,
Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Mer-
lot: Multimodal neural script knowledge models. Advances
in Neural Information Processing Systems , 34:23634â€“23651,
2021. 2
[77] Andy Zeng, Maria Attarian, Krzysztof Marcin Choroman-
ski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek
Purohit, Michael S Ryoo, Vikas Sindhwani, Johnny Lee,
et al. Socratic models: Composing zero-shot multimodal
reasoning with language. In The Eleventh International Con-
ference on Learning Representations , 2022. 2, 7
[78] Bowen Zhang, Xiaojie Jin, Weibo Gong, Kai Xu, Zhao
Zhang, Peng Wang, Xiaohui Shen, and Jiashi Feng. Mul-
timodal video adapter for parameter efficient video text re-
trieval. arXiv preprint arXiv:2301.07868 , 2023. 3
[79] Qingru Zhang, Minshuo Chen, Alexander Bukharin,
Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.
Adaptive budget allocation for parameter-efficient fine-
tuning. arXiv preprint arXiv:2303.10512 , 2023. 3
[80] Zhang Zhang and Dacheng Tao. Slow feature analysis for hu-
man action recognition. IEEE transactions on pattern anal-
ysis and machine intelligence , 34(3):436â€“450, 2012. 3
[81] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards
automatic learning of procedures from web instructional
videos. In Proceedings of the AAAI Conference on Artificial
Intelligence , 2018. 2, 3, 4
[82] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592 , 2023. 2
14397
