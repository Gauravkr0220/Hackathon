A Unified Diffusion Framework for Scene-aware Human Motion Estimation from
Sparse Signals
Jiangnan Tang1,2Jingya Wang1,2,* Kaiyang Ji1,2Lan Xu1,2Jingyi Yu1,2Ye Shi1,2,*
1ShanghaiTech University, China
2MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration, China
{tangjn2022, wangjingya, jiky, xulan1, yujingyi, shiye }@shanghaitech.edu.cn
Abstract
Estimating full-body human motion via sparse tracking
signals from head-mounted displays and hand controllers in
3D scenes is crucial to applications in AR/VR. One of the
biggest challenges to this task is the one-to-many mapping
from sparse observations to dense full-body motions, which
endowed inherent ambiguities. To help resolve this ambigu-
ous problem, we introduce a new framework to combine rich
contextual information provided by scenes to benefit full-
body motion tracking from sparse observations. To estimate
plausible human motions given sparse tracking signals and
3D scenes, we develop S2Fusion, a unified framework fus-
ingScene and sparse Signals with a conditional dif Fusion
model. S2Fusion first extracts the spatial-temporal relations
residing in the sparse signals via a periodic autoencoder,
and then produces time-alignment feature embedding as
additional inputs. Subsequently, by drawing initial noisy
motion from a pre-trained prior, S2Fusion utilizes condi-
tional diffusion to fuse scene geometry and sparse track-
ing signals to generate full-body scene-aware motions. The
sampling procedure of S2Fusion is further guided by a spe-
cially designed scene-penetration loss and phase-matching
loss, which effectively regularizes the motion of the lower
body even in the absence of any tracking signals, mak-
ing the generated motion much more plausible and coher-
ent. Extensive experimental results have demonstrated that
ourS2Fusion outperforms the state-of-the-art in terms of
estimation quality and smoothness. Code is available at
https://github.com/jn-tang/S2Fusion .
1. Introduction
With the emergence of advanced AR/VR technologies, there
is an increasing demand for generating realistic human
avatars in applications such as virtual conferencing and gam-
ing. However, common AR/VR devices, e.g., HTC Vive and
*Corresponding authorMeta Quest Pro, provide only sparse tracking signals from
inertial measurement units (IMU) embedded in single head-
mounted displays (HMD) and hand controllers. Using sparse
tracking signals to generate dense full-body motions is a
one-to-many mapping with inherent ambiguities, making
this problem a challenging task.
A natural method to generate full-body motion is leverag-
ing data-driven methods that utilize both large-scale motion
capture data [ 40] and sparse tracking signals by AR/VR de-
vices. Recently, various data-driven models ‚Äì ranging from
simple regression-based methods [ 2,14,28,63,76] to prob-
abilistic generative models such as V AE [ 16], normalizing
flow [ 1] and diffusion model [ 17] ‚Äì are deployed. Although
these methods show various ways of finding the most proba-
ble human motion estimation, they still fail to narrow down
the distribution of possible motion space, leaving the crux
of one-to-many ambiguity unresolved. Existing methods
[1,14,16,17,28,60,64] also failed to pose constraints on
the pose of the lower body, making the generated motion
disastrous as the legs are free to penetrate through scene ge-
ometries; the generated motion of legs may also uncorrelated
with hands motion, resulting in the moving of hands and feet
on the same side. The incurring of implausible leg motions
is due to the lack of observations on the lower body.
Since human motion is highly related to the surrounding
environments, introducing the scene modality can greatly
reduce the ambiguities in estimating full-body motion from
sparse tracking signals. The rich contextual information pro-
vides valuable cues to infer the lower body motion, even
though no direct observations are available. Therefore, we
propose to combine the scene information with sparse track-
ing signals for human motion estimation. To handle the
uncorrelated issues between the generated motion of legs
and hands, we use the temporal movement pattern presented
in the upper body observations to control the generation of
lower body motions.
To estimate plausible human motions given sparse track-
ing signals and 3D scenes, we develop S2Fusion, a unified
framework fusing Scene and sparse Signals with a condi-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21251
Figure 1. Given sparse tracking signals from only the head and
left/right hands, our method accurately estimates full-body motion
in the 3D scene.
tional dif Fusion model. S2Fusion models this specific task by
a conditional diffusion model, in light of the recent success
of diffusion models in motion generation [ 9,51]. Diffusion
models support flexible control on generating samples, by
incorporating various loss functions in the loss-guided sam-
pling process [ 11,46]. To tackle the lack of paired motion-
scene datasets and to generate more diverse motion, the
reverse diffusion process starts from a non-Gaussian motion
distribution, by adopting a pre-trained motion prior on a
large-scale motion dataset [ 40]. To coordinate the human
motion in 3D scenes, we extract the periodic motion feature
of the sparse tracking signals using a periodic autoencoder
[49]; the periodic motion feature represents the alignment
of full-body motions in time and space, resulting in more
effective positional embeddings. To facilitate the mitigating
of unrealistic lower body motions, we guide the diffusion
sampling process by the gradient of our specially designed
loss functions, under the framework of loss-guided sampling
[46]. Our loss functions include scene-penetration loss and
phase-matching loss, which regularizes the lower body mo-
tioneven in the absence of any tracking signals .
In summary, our paper makes the following contributions:
‚Ä¢We introduce a new framework that combines scene in-
formation and sparse tracking signals for human motion
estimation to greatly reduce the inherent ambiguities in
estimating full-body motion. To further enhance human
motion estimation, we propose to extract periodic mo-
tion features from sparse tracking signals to improve the
coordination between the upper and lower body.
‚Ä¢We develop a unified diffusion method, i.e., S2Fusion,
tailored for the scene-aware human motion estimation
with sparse signals. S2Fusion integrates three main com-
ponents: 1) a motion prior that provides initial value
for the reverse diffusion process, which significantly
improves generation quality and inference speed; 2) a
periodic motion feature extractor that learns the spatial-
temporal alignment of input signals varies in unit and
scale; the extracted feature is then combined with sparsetracking signals and scene geometry as conditional in-
puts, fed to motion generation process; 3) a set of spe-
cially designed loss functions, which effectively intro-
duce regularization on the motion of the lower body
during the loss-guided diffusion sampling process.
‚Ä¢We conduct extensive experiments for the scene-aware
human motion estimation and verify S2Fusion can suc-
cessfully reduce the inherent ambiguities posed by the
sparse-to-dense problem in contrast to other pure sensor-
based methods and achieve a notable performance boost.
Moreover, comprehensive ablation studies also demon-
strate the effectiveness of the developed three main com-
ponents of S2Fusion.
2. Related Work
2.1. Motion Estimation from Sparse Sensors
There is a surging interest in studying reconstructing human
motion from sparse sensor inputs, as it is not only economi-
cal compared to traditional marker-based optical solutions
or vision-based solutions [ 69,71], but also suits the needs of
AR/VR applications. A major problem in reconstructing full-
body motions where only sparse observations are available is
the ambiguities inherent in this one-to-many mapping prob-
lem. One line of work reconstructs full-body motion from six
IMUs located on the head, left/right wrists, left/right knees,
and torso, pioneered by von Marcard et al., [ 56]. Yi et al.
proposed a real-time RNN-based method TransPose [ 65] to
predict full-body motions, and their follow-up works [ 66,67]
refines the predicted motions by physics-based optimization
and egocentric SLAM system, respectively. Other backbones
are also explored, DIP [ 26] is powered by biRNN, and TIP
[29] deployed Transformer encoder architecture. Another
line of work reconstructs full-body motion from three 6D
trackers located on the head and left/right hands, providing
accurate translation and rotation measurements. [ 1,16,17]
tried various generative probabilistic models to solve the
unconstrained problem, while [ 2,28,76] used Transformer
backbone to regress full-body motion directly. Our method
fits this line of research of estimating full-body motion from
three 6D trackers. In addition, our approach brings scene
information to resolve the inherent ambiguities induced by
sparse-to-dense mapping, resulting in notable estimation
improvement.
2.2. Diffusion-based Probabilistic Models
Diffusion-based probabilistic models (DPMs) [ 22,47] have
recently shown promising results in image generation [ 13],
video generation [ 18,23], and speech synthesis [ 33], demon-
strating their powerful probabilistic modeling capabilities.
The success of DPMs is mainly attributed to their ability
to support versatile conditioning and are highly control-
lable. Increasing interest grows to further control the genera-
21252
tion process of DPMs by adjusting the denoising trajectory,
such as classifier guidance [ 13], imputation and inpainting
[10,11,48,79], which can be collectively termed as loss-
guided diffusion (LGD) [ 46]. The loss-guided diffusion en-
ables flexible conditioning without retraining existing mod-
els, effectively rendering DPMs a powerful generation tool.
OurS2Fusion is a specifically designed diffusion model for
scene-aware human motion estimation from sparse signals.
It differentiates from standard diffusion models mainly in
three components: 1) a motion prior module for diffusion
initialization; 2) a comprehensive condition acquisition mod-
ule; and 3) a set of specifically designed loss functions in the
diffusion sampling stage.
2.3. Human Motion Generation
Human motion can be generated by any signal that de-
scribes the motion, including text [ 4,9,20,31,44,51,52,
61,68,72], audio [ 35,54], action labels [ 9,19,42,51],
or unconditioned [ 44,74,75]. With increasingly available
scene-motion datasets [ 6,21,24,50,70], there are shifts in
trends to generate human motions in 3D scenes, ranging from
holistically generating human motion with scene-awareness
[12,25,27,57‚Äì59,73] to fine-grained reconstructing accu-
rate human-object interactions [ 8,15,30,34,36,41,53,62].
Our method is the first attempt to fuse scene information
with sparse tracking signals to estimate full-body physical-
plausible human motion with a sophisticated diffusion
model.
2.4. Motion Frequency Analysis
The characteristic of motion in the frequency domain has
been utilized for motion synthesis [ 37], editing [ 7,32], styl-
ization [ 55], and compression [ 5]. The features in the fre-
quency domain present a holistic view of the underlying
motion and are consistent over a period of time, hence are a
great medium for summarizing the movement pattern. Starke
et al. [ 49] have recently achieved success in using a deep
neural network to learn the periodic feature from motion
datasets and synthesizing high-quality motions, they demon-
strate the power of a new network architecture called periodic
autoencoder (PAE) in extracting the periodic behavior of the
motions. Shi et al. [ 45] built upon PAE a conditional V AE,
which also leverages phase features in robustly generating
motions. Instead of only leveraging the phase feature resid-
ing on the frequency domain, our method explores using
both the temporal and frequency domain features; the tempo-
ral domain features are useful in aligning motions time axis,
and the frequency domain features enabled us to correlate
the upper and lower body motions.
3. Human Motion Estimation with S2Fusion
It is not trivial to estimate full-body motion from scene
geometry and sparse signals, as it is an ill-conditioned one-to-many problem, and care must be taken to deal with spatial
scene geometry and temporal sparse signals. In light of the
recent success of the diffusion model in text-to-motion gen-
eration [ 51], we design a conditional diffusion model to
generate full-body poses given scene geometry and sparse
tracking signals. However, we observed several issues that
hinder the motion generation quality from directly applying
the conditional diffusion model. First, due to the limited
data volume of the existing motion-scene datasets, the gen-
erated motion is less diverse and unrealistic. Second, the
tracking signals only come from the upper body, so it is
hard to generate correlated upper and lower body motions.
To tackle these challenges, we designed a novel diffusion
method, i.e., S2Fusion that leverages a pre-trained motion
prior and augments scene and sparse tracking signals with
periodic alignment features to generate more diverse and
realistic motions.
Given a sequence of sparse tracking signals p1:N‚àà
RN√ócand scene geometry S‚ààRP√ó3, we aim to predict
full-body motion x1:N‚ààRN√ón. Here candnrespectively
denote the dimension of input and output, and Pdenotes
the number of points in the scene point cloud. The popular
SMPL [ 38] model is used to represent human poses and only
the first 22 joints of the SMPL model [ 28] are considered.
We use the 6D representation [78] to represent rotations for
effectively learning rotation quantities. Leveraging the scene
geometry S, sparse tracking signals p1:Nand the periodic
alignment features f1:N, we obtain the final clean motion
x1:N
0by a conditional diffusion model, given initial noisy
sample Àúx1:N. We depict our full-body motion generation
pipeline in Figure 2.
3.1. Motion Prior for S2Fusion Initialization
To circumvent the limited volume of scene-motion datasets
and generate more diverse and realistic motions, we pro-
pose to draw initial motion distribution from a pre-trained
motion prior for the reverse diffusion process [ 39]. We
build a V AE-based generative model that resembles an
encoder-decoder architecture and trained on large-scale mo-
tion dataset AMASS [40].
Once the V AE-based generative model is trained, we can
sample initial motion unaware of the scene by conditioning
on the tracking signals,
Àúx1:N=fœï(z,p1:N), z‚àº N(0,I), (1)
the sampled motion Àúx1:Nis then refined by our conditional
diffusion process Sec. 3.3.
Compared to Gaussian noise, the pre-trained generative
prior captures the complex structure of the motion manifold,
therefore improving the sample quality. Meanwhile, by jump-
starting to a noisy motion (as compared to standard DDPM
settings which require T= 1000 denoising steps [ 39]), we
can also achieve accelerated inference speed.
21253
t=1t=2t=T
‚Ä¶
3.1 Motion prior for ùë∫ùüêFusion initializationsampling
Sparse tracking signals Initial noisy motions
Conditional VAE
PointNet ++
t=1t=2t=T
‚Ä¶
Periodic motion feature
3.2 Conditioning acquisitionScene geometry
Sparse tracking signalsScene feature
Sparse tracking feature1D Conv
Periodic autoencoder
FFT FCF, A, B
S
3.3 Conditional denoiser of ùë∫ùüêFusionùë¨ùë∫
ùíôùüèùíëùüè
MLP
Linear
Transformer Encoder‡∑ùùíôùüè +
+
+
+
Linear
t +Full-body motion
ùíáùüè
ùíôùüêùíëùüêùíáùüê
ùíôùüëùíëùüëùíáùüë
ùíôùëµùíëùëµùíáùëµ‡∑ùùíôùüê
‡∑ùùíôùüë
‡∑ùùíôùëµ
ùíôùüè:ùëµ
ùíëùüè:ùëµ
ùíáùüè:ùëµ
ùë¨ùë∫Noisy motion
ConditionsDenoiser
denoise guidance from ‚Ñìsample
3.4ùë∫ùüêFusion sampling with lower body regularizationùíôùüéùüè:ùëµ
√óùëá
ùë¨ùë∫
ùíáùüè:ùëµ
ùíëùüè:ùëµFigure 2. Illustration of S2Fusion pipeline. Given the sparse tracking signals p1:Nand scene geometry S,S2Fusion generates full-body
motion with scene awareness and coherent upper and lower body movements. (1) The pre-trained motion prior fœïfirst samples the initial
noisy motion Àúx1:Nfor the reverse diffusion process; (2) then the periodic motion features f1:Nare extracted by a periodic autoencoder, and
combined with encoded scene feature ESand the sparse tracking signals p1:Nto form the final conditioning input cto the reverse diffusion
process; (3) the conditional diffusion model predicts the clean motion x1:N
0from noisy motion Àúx1:Nconditioned on c; (4) the diffusion
sampling process is further guided by the gradient of ‚Ñìpenetration and‚Ñìphaseto generate scene-aware and physically plausible motions.
3.2. Condition Acquisition for S2Fusion
The overall conditioning input to our diffusion model is
c= (p1:N,f1:N,ES), (2)
consists of raw tracking signals, periodic alignment features,
and encoded scene features. We introduce the acquisition of
the conditioning inputs as follows.
Scene conditioning. During human-scene interaction, only a
small region around the human provides meaningful contex-
tual information. Hence we crop a 2m√ó2m√ó2mbounding
boxBSaround the human, given the global translation mea-
sured by HMD. The cropped point cloud is then fed into a
scene encoder [ 43]hŒ∏, obtaining the scene feature vector
ES.
Periodic motion feature. Though the observed tracking sig-
nals consist of varying sources with different units and scales,
i.e., rotation from the gyroscope, and position from the in-
frared optical sensor, the changes in these signals reflect
the temporal movement pattern of the underlying motions.
As suggested by [ 49], full-body movements can happen as a
composition of multiple local periodic movements, and these
movements can be decomposed into multiple latent chan-
nels that capture the non-linear periodicity of different bodysegments, which are aligned in time. To effectively extract
the temporal alignment feature of these signals, we resort
to the frequency domain and use a periodic autoencoder
(PAE) [ 49]gœÑto compute the frequency domain parameters
frequency F,amplitude A,offset Bandphase shift S. Intu-
itively, the phase shift indicates the time-alignment of the
motions, while the amplitude resembles the momentum.
The PAE gœÑconsists of a combination of 1D convolution
and a fast Fourier transformation (FFT) layer to compute the
amplitudes A, offsets B, and frequencies Fof a temporal
signal p1:N,
[A,B,F] =FFT(Conv (p1:N)). (3)
The phase-shifts Sare obtained by a separate fully-
connected network,
(sx, sy) =FC(Conv (p1:N)),S= arctan( sy, sx).(4)
After extracting parameters in the frequency domain, we
can reconstruct a smoothed periodic alignment feature in the
temporal domain,
ft=A¬∑sin(2œÄ¬∑(F¬∑t‚àíS)) +B, (5)
21254
Figure 3. A visualization of the periodic motion features of the
upper and lower body extracted from randomly selected motion se-
quences in AMASS[ 40]. The phase shift of the sinusoidal functions
indicates the time-alignment of the upper and lower body motions,
while the amplitude resembles the momentum. It can be shown that
the periodic motion features of the upper body are correlated with
that of the lower body.
where t‚àà {1, . . . , N }. The reconstructed feature in essence
is a multi-resolution sinusoidal function, which summarizes
the alignment of full-body motion in both time and space.
Sparse tracking signals. We take the position and rotation
of the head and left/right hands as input signals, and com-
pute angular and linear velocities as extra input following
previous work [28].
3.3. Conditional Denoiser of S2Fusion
Diffusion models consist of a forward diffusion process
and a reverse diffusion process. To model a distribution
x0‚àºq(x0), the forward process follows a Markov chain of
Tsteps which progressively adds Gaussian noise and pro-
duces a series of time-dependent distributions q(xt|xt‚àí1).
Formally,
q(xt|xt‚àí1) =N(xt;p
1‚àíŒ≤txt‚àí1, Œ≤tI), (6)
q(x1:T|x0) =TY
t=1q(xt|xt‚àí1), (7)
where Œ≤t‚àà(0,1)is the variance schedule at timestep t.
To generate full-body motion, we model the conditioned
motion generation problem by the reverse diffusion process,
which gradually cleans corrupted signal xT. Instead of pre-
dicting residual Gaussian noise œµtadded through each step t,
we predict the signal ÀÜxitself following [ 51] with the simple
objective ,
Lsimple =Et‚àº[1,T]G(x1:N
t, t,c)‚àíx0 (8)
This iterative process at timestep tcan be formulated as
x1:N
t‚àí1=‚àö¬ØŒ±t‚àí1G(x1:N
t, t,c) +p
1‚àí¬ØŒ±t‚àí1œµ, (9)where Gis a network that learns to generate clean motion
ÀÜx1:Nat timestep t,¬ØŒ±t=Qt
i=1(1‚àíŒ≤i)andœµ‚àº N (0,I)
is the injected Gaussian noise. The denoising network Gis
trained with Lsimple and geometric loss [ 51] that regulates
the generated motion to lie on motion manifold,
Ltrain=Lsimple +Lgeometric , (10)
Lgeometric =‚à•FK(ÀÜx)‚àíFK(x0)‚à•, (11)
where the FK(¬∑)denotes the forward kinematic process that
converts joint rotations into positions.
More details about the training details of the V AE-based
motion prior and periodic autoencoder are provided in the
supplementary.
3.4. S2Fusion Sampling with Lower Body Motion
Regularization
Given the sparse tracking signals of the upper body , it is
already possible to reconstruct the upper body motion faith-
fully [ 17,28,76]. However, the lack of lower body tracking
signals makes the generated leg motions often possess un-
realistic behavior and are inconsistent with the upper body.
Special treatment to the lower body is required to generate
more realistic motions. To tackle the challenge presented
by the absence of observations, we designed two loss func-
tions that regularize the leg motions which do not require
any tracking signals from the lower body . We incorporate
the designed loss functions in the sampling procedure of
ourS2Fusion, enabling flexible control of the lower body
motions.
Scene-penetration loss resolves the incurring of implausible
human-scene penetration,
‚Ñìpenetration (x0) =X
i‚ààCX
b‚ààKNN(x0,i,k)max( r‚àí ‚à•x0,i‚àíb‚à•,0),
(12)
where Cis the set of joints that we want to avoid contact
with the scene, kis the number of neighbors to the KNN
query, and points in the scene within radius rare considered
to be in contact. We empirically set Cas the left/right ankles
and the left/right knees; we set r= 0.02m and k= 4in our
evaluation.
Phase-matching loss forces the upper body and the lower
body to move in a coherent manner,
‚Ñìphase(x0) =‚à•Pupper‚àíPlower‚à•, (13)
where Pupper is the phase feature computed using the ampli-
tude and phase shift as defined in Sec. 3.2,
Pupper= [sin(2 œÄ¬∑S),cos(2 œÄ¬∑S),A]. (14)
21255
Algorithm 1: Sampling procedure of S2Fusion for
full-body human motion estimation.
Input: sparse tracking signals p1:N, scene point
cloudS
Output: full-body motion x1:N
0
/*preprocessing */
1z‚àº N(0,I)
2Draw initial noisy motion from pre-trained prior
Àúx1:N‚àºfœï(z,p1:N)
3Extract periodic motion feature f1:N:=gœÑ(p1:N)
4Extract scene feature ES:=hŒ∏(BS)
5Form conditioning input c:= (p1:N,f1:N,ES)
/*reverse diffusion process */
6x1:N
T:=Àúx1:N
7fort=T, . . . , 1do
8 ÀÜx1:N
0:=G(x1:N
t, t,c) // predict clean
signal
9 ¬Øx1:N
0:=ÀÜx1:N
0‚àíŒ∑‚àáÀÜx1:N
0‚Ñì(ÀÜx1:N
0) // loss
guidance
10 x1:N
t‚àí1:=‚àö¬ØŒ±t‚àí1¬Øx1:N
0+‚àö1‚àí¬ØŒ±t‚àí1œµ
11return x1:N
0
The effectiveness of the phase-matching loss can be justified
by the observation that the upper and lower bodies always
move in a coordinated way: humans tend to synchronize the
upper and lower body movements to achieve balance during
everyday activities such as walking, running, dancing, and
so on. The movement of the upper body provides a valuable
cue to the movement of the lower body . We plot the periodic
motion features of the upper and lower body in Figure 3. We
observe there is a clear correlation between the upper and
lower body movements.
To compute the phase feature Plowerof the lower body, we
select the anchor joints that can represent the motion of the
lower body: the pelvis and left/right ankles, resembling the
tracking signals sent from the head and left/right hands. Then
we collect the position, rotation, linear velocity, and angular
velocity of the generated lower body motions, following the
same procedure in Sec. 3.2. We then compute the phase shift
and amplitude parameter and obtain Plower.
Sampling with guidance from loss functions. The overall
loss guidance in the sampling stage of S2Fusion is as follows,
‚Ñìsample =Œ±¬∑‚Ñìpenetration +Œ≤¬∑‚Ñìphase, (15)
where Œ±andŒ≤are scaling factors controlling the strength of
applying guidance.
After obtaining a clean motion sample ÀÜx1:N
0as in Sec.
3.3, we guide the sampling process by injecting the gradient
of‚Ñìsample to regularize the motion of lower body,
¬Øx1:N
0‚ÜêÀÜx1:N
0‚àíŒ∑‚àáÀÜx1:N
0‚Ñìsample(ÀÜx1:N
0). (16)The full pipeline of the sampling procedure is summarized
in Algorithm 1. We chose Œ±= 0.1, Œ≤= 0.01, Œ∑= 1empiri-
cally.
4. Experiments
We first evaluate and compare our method to others [ 17,28,
76] in commonly used metrics for motion reconstruction.
Then we conduct ablation studies to showcase the effective-
ness of the design choices of our model.
4.1. Experiment Setup
4.1.1 Datasets
We consider two motion-scene datasets to train and evalu-
ate our method. Both datasets consist of rich human-scene
interactions and provide fine-grained scene meshes.
CIRCLE [3] contains 10 hours of full-body motion in 9
diverse scenes paired with egocentric information of the
environment. We randomly split the training and testing data
by a 70:30 ratio, and discarded sequences shorter than 1.0s.
GIMO [77] consists of body pose sequences, scene scans,
and eye gaze information. The dataset is collected by using
Hololens and IMU-based motion capture suits for motion
acquisition; and iPhone 12 for scene scanning.
4.1.2 Evaluation Metrics
We evaluate the performance of our method against others
using the commonly used evaluation metrics, which measure
motion estimation accuracy, motion smoothness, and scene
awareness, respectively.
MPJRE (mean per joint rotation error) measures the average
relative rotation error of all body joints in deg.MPJPE
(mean per joint position error) measures the pose accuracy
of each frame in mm.MPJVE (mean per joint velocity
error) measures the average velocity error of all body joints
inmm/s .Jitter measures the average jerk, which is the
time derivative of acceleration, of all body joints and reflects
the smoothness of motion. FSrepresents the accumulated
drift of foot joints during contact, it is computed by the
average horizontal displacement between the grounding feet
in adjacent frames.
We also conduct per-body part evaluations, as Hand PE ,
Upper PE andLower PE denote the hand position error,
upper body position error and lower body position error,
respectively.
4.2. Comparison
We compare our method with the state-of-the-art methods,
[17,28,76] on CIRCLE [ 3] and GIMO [ 77] datasets. For a
fair comparison, we re-train these methods on CIRCLE and
GIMO until convergence.
21256
GIMO [77] CIRCLE [3]
Method MPJRE ‚ÜìMPJPE ‚ÜìMPJVE ‚ÜìJitter‚ÜìFS‚ÜìMPJRE ‚ÜìMPJPE ‚ÜìMPJVE ‚ÜìJitter‚ÜìFS‚Üì
AvatarPoser [28] 7.02 91.3 324.0 16.4 2.04 2.68 30.5 184.6 11.5 2.11
AGRoL [17] 6.58 88.6 269.4 12.5 1.70 2.62 28.7 141.1 8.2 1.72
AvatarJLM [76] 4.95 70.7 258.1 10.7 1.41 2.49 24.6 128.5 7.0 1.53
Ours 4.65 57.8 235.7 10.1 1.39 2.32 19.2 117.6 5.8 1.48
Table 1. Full-body motion estimation results evaluated on GIMO [77] and CIRCLE [3]
GT AvatarJLM AGRoL Ours
Figure 4. Qualitative results on the CIRCLE [ 3] dataset. We show the results of two motion sequences in different scenes and highlight the
implausible motions in the red box. It can be shown that our method generates more correlated leg motions and avoids scene penetration as
much as possible.
We show our quantitative results in Table 1 and Table 2.
Compared to other methods, our method achieves the best
MPJPE and FS among others in both datasets, demonstrating
the effectiveness of incorporating scene information to re-
solve the sparse-to-dense ambiguities and to generate much
more accurate motions. Moreover, our S2Fusion model im-
proves the smoothness in motion generation, as showcased
by the MPJVE ,Jitter andFSmetrics. It could be also ob-
served that the estimation quality of the lower body motion is
significantly better than other methods by a large margin. The
superiority of our method is also evident in the qualitative
results shown in Figure 4. By visualizing the pose trajectory
in different scenes, we demonstrate that our method gener-
ates accurate and smooth motions; being able to incorporate
scene information as inputs, the motion generated by our
method incurred fewer scene penetrations.
4.3. Ablation Study
To further investigate the effectiveness of design choices in
S2Fusion, we ablate the effectiveness of different compo-
nents. We first study the effectiveness of our model compo-
nents, including the benefit of introducing the scene as an
extra modality, and the use of pre-trained motion prior and
periodic autoencoder. Then we study the effectiveness of
our specially designed loss function during the loss-guided
sampling process.4.3.1 Effectiveness of S2Fusion Modules Design
To assess our method‚Äôs effectiveness, we incorporated a pre-
trained motion prior, a periodic autoencoder, and an addi-
tional scene modality. Comparing our approach against four
alternatives in Tables 3 and 4, the results show that adding
scene information substantially enhances motion estimation
quality with limited upper body tracking data. This confirms
our claim that scene data can significantly resolve ambigui-
ties in sparse-to-dense scenarios. Moreover, initializing with
a motion prior addresses data scarcity issues and improves
motion smoothness and accuracy, while the periodic autoen-
coder enhances estimation by aligning motion temporally
and spatially.
4.3.2 Effectiveness of Loss-guided Sampling
We ablate the importance of incorporating our designed loss
function in the loss-guided sampling process. We show the
results compared against three alternatives in Table 5 and Ta-
ble 6. We notice that although the scene-penetration loss and
phase-matching loss can improve the motion estimation qual-
ity, they may introduce jittering motions into the generated
motions. Compared to scene-penetration loss, incorporating
phase-matching loss is more effective in producing accurate
motions, coinciding with our observation that the phase fea-
21257
GIMO [77] CIRCLE [3]
Method Hand PE Upper PE Lower PE Hand PE Upper PE Lower PE
AvatarPoser [28] 36.8 40.1 198.6 11.4 15.3 83.3
AGRoL [17] 23.7 29.8 163.9 9.1 14.5 77.4
AvatarJLM [76] 25.0 33.4 132.6 9.3 15.8 69.1
Ours 23.1 28.7 107.9 8.8 12.2 57.3
Table 2. More metrics comparison with AvatarPoser [ 28], AGRoL [ 17], and AvatarJLM [ 76] on GIMO [ 77] and CIRCLE [ 3]. We show the
results of comparing the hand, upper body, and lower body reconstruction quality.
Components
MP Scene PAE MPJPE MPJVE Jit. FS
√ó √ó √ó 26.2 135.9 7.9 1.65
√ó ‚úì √ó 22.7 128.1 7.1 1.58
‚úì ‚úì √ó 20.9 120.5 6.0 1.51
√ó ‚úì ‚úì 21.8 125.3 6.6 1.52
‚úì ‚úì ‚úì 19.2 117.6 5.8 1.48
Table 3. Ablation on various components of our model on CIRCLE.
MPdenotes the pre-trained motion prior, Scene indicates whether
the model receives the scene information as an extra input, and
PAE denotes the periodic autoencoder.
Components
MP Scene PAE MPJPE MPJVE Jit. FS
√ó √ó √ó 76.6 264.9 11.7 1.68
√ó ‚úì √ó 68.1 257.4 11.2 1.67
‚úì ‚úì √ó 60.3 243.6 10.3 1.46
√ó ‚úì ‚úì 65.9 249.3 10.4 1.52
‚úì ‚úì ‚úì 57.8 235.7 10.1 1.39
Table 4. Ablation on various components of our model on GIMO.
MPdenotes the pre-trained motion prior, Scene indicates whether
the model receives the scene information as an extra input, and
PAE denotes the periodic autoencoder.
Loss fn.
‚Ñìpenetration ‚Ñìphase MPJPE MPJVE Jit. FS
√ó √ó 20.6 119.1 5.6 1.43
‚úì √ó 20.1 117.9 5.8 1.40
√ó ‚úì 19.8 118.5 6.1 1.50
‚úì ‚úì 19.2 117.6 5.8 1.48
Table 5. Ablation on the effect of our designed loss function during
loss-guided sampling on CIRCLE.
tures are composed of time-alignment features and feature
vector resembles momentum. The scene-penetration loss has
merit on its own as it can avoid falsely contacting the ground,
especially in the CIRCLE dataset.
5. Conclusion
Our work addresses the pivotal challenge of estimating full-
body human motion in 3D scenes from sparse tracking sig-Loss fn.
‚Ñìpenetration ‚Ñìphase MPJPE MPJVE Jit. FS
√ó √ó 59.9 240.5 10.5 1.52
‚úì √ó 58.3 239.1 10.4 1.58
√ó ‚úì 57.6 236.8 10.1 1.42
‚úì ‚úì 57.8 235.7 10.1 1.39
Table 6. Ablation on the effect of our designed loss function during
loss-guided sampling on GIMO.
nals, a critical aspect for advancing AR/VR applications. To
tackle this problem, we propose S2Fusion that integrates
Scene and sparse Signals through a conditional Dif Fusion
model. S2Fusion first captures spatial-temporal relations in
sparse signals using a periodic autoencoder, generating time-
alignment feature embeddings as additional inputs. Employ-
ing conditional diffusion and drawing initial motion from a
pre-trained prior, S2Fusion effectively fuses scene geometry
and sparse tracking signals to generate full-body scene-aware
motions. To improve plausibility and coherence, S2Fusion
incorporates a specially designed scene-penetration loss and
phase-matching loss, providing guidance for the sampling
procedure. These losses effectively regularize lower-body
motion, even in the absence of tracking signals. Extensive
experiments demonstrate that S2Fusion outperforms state-
of-the-art methods by a large margin. Extending S2Fusion
for human motion estimation in more complex scenarios by
systematically integrating comprehensive physically plausi-
ble constraints is under consideration for our future work.
Acknowledgement
This work was supported by Shanghai Local College Capac-
ity Building Program (23010503100), NSFC (No.62303319),
Shanghai Sailing Program (22YF1428800, 21YF1429400),
Shanghai Frontiers Science Center of Human-centered Ar-
tificial Intelligence (ShangHAI), MoE Key Laboratory of
Intelligent Perception and Human-Machine Collaboration
(ShanghaiTech University), and Shanghai Clinical Research
and Trial Center.
21258
References
[1]Sadegh Aliakbarian, Pashmina Cameron, Federica Bogo, An-
drew Fitzgibbon, and Thomas J. Cashman. Flag: Flow-based
3d avatar generation from sparse observations, 2022. 1, 2
[2]Sadegh Aliakbarian, Fatemeh Saleh, David Collier, Pashmina
Cameron, and Darren Cosker. Hmd-nemo: Online 3d avatar
motion generation from sparse observations. In Proceed-
ings of the IEEE/CVF International Conference on Computer
Vision , pages 9622‚Äì9631, 2023. 1, 2
[3]JoÀúao Pedro Ara ¬¥ujo, Jiaman Li, Karthik Vetrivel, Rishi Agar-
wal, Jiajun Wu, Deepak Gopinath, Alexander William Clegg,
and Karen Liu. Circle: Capture in rich contextual environ-
ments. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 21211‚Äì
21221, 2023. 6, 7, 8
[4]Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-
aming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli
Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion
models with an ensemble of expert denoisers. arXiv preprint
arXiv:2211.01324 , 2022. 3
[5]Philippe Beaudoin, Pierre Poulin, and Michiel van de Panne.
Adapting wavelet compression to human motion capture clips.
InProceedings of Graphics Interface 2007 , pages 313‚Äì318,
2007. 3
[6]Bharat Lal Bhatnagar, Xianghui Xie, Ilya A Petrov, Cristian
Sminchisescu, Christian Theobalt, and Gerard Pons-Moll.
Behave: Dataset and method for tracking human object inter-
actions. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 15935‚Äì15946,
2022. 3
[7]Armin Bruderlin and Lance Williams. Motion signal pro-
cessing. In Proceedings of the 22nd annual conference on
Computer graphics and interactive techniques , pages 97‚Äì104,
1995. 3
[8] Zhe Cao, Ilija Radosavovic, Angjoo Kanazawa, and Jitendra
Malik. Reconstructing hand-object interactions in the wild.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 12417‚Äì12426, 2021. 3
[9]Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, and Gang Yu. Executing your commands via motion
diffusion in latent space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 18000‚Äì18010, 2023. 2, 3
[10] Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul
Ye. Improving diffusion models for inverse problems using
manifold constraints. Advances in Neural Information Pro-
cessing Systems , 35:25683‚Äì25696, 2022. 3
[11] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann,
Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior
sampling for general noisy inverse problems. In The Eleventh
International Conference on Learning Representations , 2023.
2, 3
[12] Peishan Cong, Yiteng Xu, Yiming Ren, Juze Zhang, Lan Xu,
Jingya Wang, Jingyi Yu, and Yuexin Ma. Weakly supervised
3d multi-person pose estimation for large-scale scenes based
on monocular camera and single lidar. In Proceedings of theAAAI Conference on Artificial Intelligence , pages 461‚Äì469,
2023. 3
[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. Advances in neural information
processing systems , 34:8780‚Äì8794, 2021. 2, 3
[14] Xinhan Di, Xiaokun Dai, Xinkang Zhang, and Xinrong Chen.
Dual attention poser: Dual path body tracking based on at-
tention. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 2794‚Äì2803,
2023. 1
[15] Christian Diller and Angela Dai. Cg-hoi: Contact-guided
3d human-object interaction generation. arXiv preprint
arXiv:2311.16097 , 2023. 3
[16] Andrea Dittadi, Sebastian Dziadzio, Darren Cosker, Ben
Lundell, Thomas J. Cashman, and Jamie Shotton. Full-
body motion from a single head-mounted device: Generating
smpl poses from partial observations. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 11687‚Äì11697, 2021. 1, 2
[17] Yuming Du, Robin Kips, Albert Pumarola, Sebastian Starke,
Ali Thabet, and Artsiom Sanakoyeu. Avatars grow legs: Gen-
erating smooth human motion from sparse tracking inputs
with diffusion model. In CVPR , 2023. 1, 2, 5, 6, 7, 8
[18] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis. Structure
and content-guided video synthesis with diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 7346‚Äì7356, 2023. 2
[19] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tion2motion: Conditioned generation of 3d human motions.
InProceedings of the 28th ACM International Conference on
Multimedia , pages 2021‚Äì2029, 2020. 3
[20] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
Xingyu Li, and Li Cheng. Generating diverse and natural 3d
human motions from text. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 5152‚Äì5161, 2022. 3
[21] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and
Michael J Black. Resolving 3d human pose ambiguities
with 3d scene constraints. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 2282‚Äì
2292, 2019. 3
[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. Advances in neural information
processing systems , 33:6840‚Äì6851, 2020. 2
[23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,
Mohammad Norouzi, David J Fleet, et al. Imagen video:
High definition video generation with diffusion models. arXiv
preprint arXiv:2210.02303 , 2022. 2
[24] Chun-Hao P Huang, Hongwei Yi, Markus H ¬®oschle, Matvey
Safroshkin, Tsvetelina Alexiadis, Senya Polikovsky, Daniel
Scharstein, and Michael J Black. Capturing and inferring
dense full-body human-scene contact. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 13274‚Äì13285, 2022. 3
21259
[25] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu
Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-
based generation, optimization, and planning in 3d scenes.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2023. 3
[26] Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J
Black, Otmar Hilliges, and Gerard Pons-Moll. Deep iner-
tial poser: Learning to reconstruct human pose from sparse
inertial measurements in real time. ACM Transactions on
Graphics (TOG) , 37(6):1‚Äì15, 2018. 2
[27] Chaofan Huo, Ye Shi, Yuexin Ma, Lan Xu, Jingyi Yu, and
Jingya Wang. Stackflow: monocular human-object reconstruc-
tion by stacked normalizing flow with offset. In Proceedings
of the Thirty-Second International Joint Conference on Artifi-
cial Intelligence , pages 902‚Äì910, 2023. 3
[28] Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa
Laich, Patrick Snape, and Christian Holz. Avatarposer: Ar-
ticulated full-body pose tracking from sparse motion sensing.
InProceedings of European Conference on Computer Vision .
Springer, 2022. 1, 2, 3, 5, 6, 7, 8
[29] Yifeng Jiang, Yuting Ye, Deepak Gopinath, Jungdam Won,
Alexander W Winkler, and C Karen Liu. Transformer inertial
poser: Attention-based real-time human motion reconstruc-
tion from sparse imus. arXiv e-prints , pages arXiv‚Äì2203,
2022. 2
[30] Korrawe Karunratanakul, Jinlong Yang, Yan Zhang, Michael J
Black, Krikamol Muandet, and Siyu Tang. Grasping field:
Learning implicit representations for human grasps. In 2020
International Conference on 3D Vision (3DV) , pages 333‚Äì344.
IEEE, 2020. 3
[31] Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwa-
janakorn, and Siyu Tang. Guided motion diffusion for con-
trollable human motion synthesis. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2151‚Äì2162, 2023. 3
[32] Ben Kenwright. Quaternion fourier transform for character
motions. 2015. 3
[33] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan
Catanzaro. Diffwave: A versatile diffusion model for audio
synthesis. arXiv preprint arXiv:2009.09761 , 2020. 2
[34] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu,
Justin Johnson, David Fouhey, and Leonidas Guibas. Nifty:
Neural object interaction fields for guided human motion
synthesis. arXiv preprint arXiv:2307.07511 , 2023. 3
[35] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa.
Ai choreographer: Music conditioned 3d dance generation
with aist++. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 13401‚Äì13412, 2021.
3
[36] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan
Xu. Intergen: Diffusion-based multi-human motion gener-
ation under complex interactions. International Journal of
Computer Vision , pages 1‚Äì21, 2024. 3
[37] Zicheng Liu, Steven J Gortler, and Michael F Cohen. Hierar-
chical spacetime control. In Proceedings of the 21st annual
conference on Computer graphics and interactive techniques ,
pages 35‚Äì42, 1994. 3[38] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. In Seminal Graphics Papers: Pushing
the Boundaries, Volume 2 , pages 851‚Äì866. 2023. 3
[39] Zhaoyang Lyu, Xudong XU, Ceyuan Yang, Dahua Lin, and
Bo Dai. Accelerating diffusion models via early stop of the
diffusion process, 2022. 3
[40] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-
ard Pons-Moll, and Michael J Black. Amass: Archive
of motion capture as surface shapes. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 5442‚Äì5451, 2019. 1, 2, 3, 5
[41] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, De-
qing Sun, and Huaizu Jiang. Hoi-diff: Text-driven synthesis
of 3d human-object interactions using diffusion models. arXiv
preprint arXiv:2312.06553 , 2023. 3
[42] Mathis Petrovich, Michael J Black, and G ¬®ul Varol. Action-
conditioned 3d human motion synthesis with transformer vae.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 10985‚Äì10995, 2021. 3
[43] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas.
Pointnet++: Deep hierarchical feature learning on point sets
in a metric space. Advances in neural information processing
systems , 30, 2017. 4
[44] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano.
Human motion diffusion as a generative prior. arXiv preprint
arXiv:2303.01418 , 2023. 3
[45] Mingyi Shi, Sebastian Starke, Yuting Ye, Taku Komura, and
Jungdam Won. Phasemp: Robust 3d pose estimation via
phase-conditioned human motion prior. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 14725‚Äì14737, 2023. 3
[46] Jiaming Song, Qinsheng Zhang, Hongxu Yin, Morteza Mar-
dani, Ming-Yu Liu, Jan Kautz, Yongxin Chen, and Arash
Vahdat. Loss-guided diffusion models for plug-and-play con-
trollable generation. 2023. 2, 3
[47] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equations.
arXiv preprint arXiv:2011.13456 , 2020. 2
[48] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solv-
ing inverse problems in medical imaging with score-based
generative models. arXiv preprint arXiv:2111.08005 , 2021. 3
[49] Sebastian Starke, Ian Mason, and Taku Komura. Deepphase:
Periodic autoencoders for learning motion phase manifolds.
ACM Transactions on Graphics (TOG) , 41(4):1‚Äì13, 2022. 2,
3, 4
[50] Omid Taheri, Nima Ghorbani, Michael J Black, and Dimitrios
Tzionas. Grab: A dataset of whole-body human grasping of
objects. In Computer Vision‚ÄìECCV 2020: 16th European
Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings,
Part IV 16 , pages 581‚Äì600. Springer, 2020. 3
[51] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel
Cohen-Or, and Amit H Bermano. Human motion diffusion
model. arXiv preprint arXiv:2209.14916 , 2022. 2, 3, 5
[52] Jie Tian, Lingxiao Yang, Ran Ji, Yuexin Ma, Lan Xu, Jingyi
Yu, Ye Shi, and Jingya Wang. Gaze-guided hand-object in-
21260
teraction synthesis: Benchmark and method. arXiv preprint
arXiv:2403.16169 , 2024. 3
[53] Shashank Tripathi, Lea M ¬®uller, Chun-Hao P Huang, Omid
Taheri, Michael J Black, and Dimitrios Tzionas. 3d human
pose estimation via intuitive physics. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4713‚Äì4725, 2023. 3
[54] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:
Editable dance generation from music. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 448‚Äì458, 2023. 3
[55] Munetoshi Unuma, Ken Anjyo, and Ryozo Takeuchi. Fourier
principles for emotion-based human figure animation. In
Proceedings of the 22nd annual conference on Computer
graphics and interactive techniques , pages 91‚Äì96, 1995. 3
[56] Timo V on Marcard, Bodo Rosenhahn, Michael J Black, and
Gerard Pons-Moll. Sparse inertial poser: Automatic 3d human
pose estimation from sparse imus. In Computer graphics
forum , pages 349‚Äì360. Wiley Online Library, 2017. 2
[57] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiao-
long Wang. Synthesizing long-term 3d human motion and
interaction in 3d scenes. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 9401‚Äì9411, 2021. 3
[58] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin,
and Bo Dai. Towards diverse and natural scene-aware 3d
human motion synthesis. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 20460‚Äì20469, 2022.
[59] Zhenzhen Weng and Serena Yeung. Holistic 3d human and
scene mesh estimation from single view images. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 334‚Äì343, 2021. 3
[60] Alexander Winkler, Jungdam Won, and Yuting Ye. Questsim:
Human motion tracking from sparse sensors with simulated
avatars. In SIGGRAPH Asia 2022 Conference Papers , pages
1‚Äì8, 2022. 1
[61] Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan
Xu, and Jingya Wang. Thor: Text to human-object inter-
action diffusion via relation intervention. arXiv preprint
arXiv:2403.11208 , 2024. 3
[62] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan
Gui. Interdiff: Generating 3d human-object interactions with
physics-informed diffusion. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 14928‚Äì
14940, 2023. 3
[63] Dongseok Yang, Doyeon Kim, and Sung-Hee Lee. Lobstr:
Real-time lower-body pose prediction from sparse upper-body
tracking signals. In Computer Graphics Forum , pages 265‚Äì
275. Wiley Online Library, 2021. 1
[64] Yongjing Ye, Libin Liu, Lei Hu, and Shihong Xia. Neu-
ral3points: Learning to generate physically realistic full-body
motion for virtual reality users. In Computer Graphics Forum ,
pages 183‚Äì194. Wiley Online Library, 2022. 1
[65] Xinyu Yi, Yuxiao Zhou, and Feng Xu. Transpose: Real-time
3d human translation and pose estimation with six inertial
sensors. ACM Transactions on Graphics , 40(4), 2021. 2[66] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Soshi Shimada,
Vladislav Golyanik, Christian Theobalt, and Feng Xu. Physi-
cal inertial poser (pip): Physics-aware real-time human mo-
tion tracking from sparse inertial sensors. In IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition (CVPR) ,
2022. 2
[67] Xinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav
Golyanik, Shaohua Pan, Christian Theobalt, and Feng Xu.
Egolocate: Real-time motion capture, localization, and map-
ping with sparse body-mounted sensors. ACM Transactions
on Graphics (TOG) , 42(4), 2023. 2
[68] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan
Kautz. Physdiff: Physics-guided human motion diffusion
model. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision (ICCV) , 2023. 3
[69] Juze Zhang, Jingya Wang, Ye Shi, Fei Gao, Lan Xu, and
Jingyi Yu. Mutual adaptive reasoning for monocular 3d multi-
person pose estimation. In Proceedings of the 30th ACM
International Conference on Multimedia , pages 1788‚Äì1796,
2022. 2
[70] Juze Zhang, Haimin Luo, Hongdi Yang, Xinru Xu, Qianyang
Wu, Ye Shi, Jingyi Yu, Lan Xu, and Jingya Wang. Neu-
raldome: A neural modeling pipeline on multi-view human-
object interactions. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
8834‚Äì8845, 2023. 3
[71] Juze Zhang, Ye Shi, Yuexin Ma, Lan Xu, Jingyi Yu, and
Jingya Wang. Ikol: Inverse kinematics optimization layer
for 3d human pose and shape estimation via gauss-newton
differentiation. In Proceedings of the AAAI Conference on
Artificial Intelligence , pages 3454‚Äì3462, 2023. 2
[72] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong,
Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-
driven human motion generation with diffusion model. arXiv
preprint arXiv:2208.15001 , 2022. 3
[73] Siwei Zhang, Yan Zhang, Federica Bogo, Marc Pollefeys, and
Siyu Tang. Learning motion priors for 4d human body capture
in 3d scenes. In Proceedings of the IEEE/CVF International
Conference on Computer Vision , pages 11343‚Äì11353, 2021.
3
[74] Yan Zhang, Michael J Black, and Siyu Tang. Perpetual mo-
tion: Generating unbounded human motion. arXiv preprint
arXiv:2007.13886 , 2020. 3
[75] Rui Zhao, Hui Su, and Qiang Ji. Bayesian adversarial human
motion synthesis. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
6225‚Äì6234, 2020. 3
[76] Xiaozheng Zheng, Zhuo Su, Chao Wen, Zhou Xue, and Xiao-
jie Jin. Realistic full-body tracking from sparse observations
via joint-level modeling. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 14678‚Äì
14688, 2023. 1, 2, 5, 6, 7, 8
[77] Yang Zheng, Yanchao Yang, Kaichun Mo, Jiaman Li, Tao Yu,
Yebin Liu, C Karen Liu, and Leonidas J Guibas. Gimo: Gaze-
informed human motion prediction in context. In European
Conference on Computer Vision , pages 676‚Äì694. Springer,
2022. 6, 7, 8
21261
[78] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao
Li. On the continuity of rotation representations in neural
networks. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 5745‚Äì5753,
2019. 3
[79] Yuanzhi Zhu, Kai Zhang, Jingyun Liang, Jiezhang Cao, Bihan
Wen, Radu Timofte, and Luc Van Gool. Denoising diffusion
models for plug-and-play image restoration. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 1219‚Äì1229, 2023. 3
21262
