AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation
Qingping Sun*,1,2, Yanjun Wang‚àó,1, Ailing Zeng3, Wanqi Yin1, Chen Wei1,
Wenjia Wang5, Haiyi Mei1, Chi-Sing Leung2, Ziwei Liu4, Lei Yang1,5, Zhongang Cai‚Ä†,1,4,5
1SenseTime Research2City University of Hong Kong
3International Digital Economy Academy (IDEA)
4S-Lab, Nanyang Technological University5Shanghai AI Laboratory
‚àóEqual Contributions,‚Ä†Corresponding Author
https://ttxskk.github.io/AiOS/
(a) Top-Down, Multi-Stage(b) Top-Down, One-Stage(c) Our All-in-One-Stage Method
‚Ñ≥!"#$
Merge
‚Ñ≥%&'(
Encoder
‚Ñ≥)"*'
	‚Ñ≥Decoder
Figure 1. A comparison of existing methods in EHPS. (a) Top-down, multi-stage methods typically use detectors to detect humans, then use
different networks to regress body parts on cropped images. (b) Top-down, one-stage methods use only one network for regression but still
require detectors and rely on the cropped image. (c) Our all-in-one-stage pipeline, end-to-end human detection, and regression on full frame.
Abstract
Expressive human pose and shape estimation (a.k.a. 3D
whole-body mesh recovery) involves the human body, hand,
and expression estimation. Most existing methods have tack-
led this task in a two-stage manner, first detecting the human
body part with an off-the-shelf detection model and then in-
ferring the different human body parts individually. Despite
the impressive results achieved, these methods suffer from
1) loss of valuable contextual information via cropping, 2)
introducing distractions, and 3) lacking inter-association
among different persons and body parts, inevitably causing
performance degradation, especially for crowded scenes. To
address these issues, we introduce a novel all-in-one-stage
framework, AiOS, for multiple expressive human pose and
shape recovery without an additional human detection step.
Specifically, our method is built upon DETR, which treats
multi-person whole-body mesh recovery task as a progres-
sive set prediction problem with various sequential detection.
We devise the decoder tokens and extend them to our task.
Specifically, we first employ a human token to probe a hu-
man location in the image and encode global features for
each instance, which provides a coarse location for the later
transformer block. Then, we introduce a joint-related tokento probe the human joint in the image and encoder a fine-
grained local feature, which collaborates with the global
feature to regress the whole-body mesh. This straightfor-
ward but effective model outperforms previous state-of-the-
art methods by a 9% reduction in NMVE on AGORA, a
30% reduction in PVE on EHF , a 10% reduction in PVE on
ARCTIC, and a 3% reduction in PVE on EgoBody.
1. Introduction
Expressive human pose and shape estimation (EHPS)1is a
rapidly developing area. It plays an important role in human
understanding and has broad applications in the animation,
gaming, and streaming industries. Unlike human pose and
shape estimation (HPS), which focuses solely on the human
body, EHPS is designed to jointly estimate human body
poses, hand gestures, and facial expressions from the image.
In mainstream studies, the common approaches involve
utilizing parametric human models, such as SMPL-X [ 30],
to represent the articulated mesh model of a human and to
regress the parameters for each body part. Drawing from
research experience in single-part estimation, such as body
1EHPS is used interchangeably with 3D whole-body human mesh re-
covery in this work
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1834
pose and shape estimation [ 8,13‚Äì15,19,20,34,39,40,51],
existing methods [ 3,7,11,18,26,27,31] employ a multi-
stage paradigm. As shown in Fig. 1a), the process begins by
cropping the body parts using bounding boxes detected either
by off-the-shelf detection models or provided via ground
truth annotations. Following this, distinct models are utilized
for the separate reconstruction of each individual body part.
Obviously, this design compromises both complexity and
accuracy. The images are processed multiple times with each
model. The separate parts model blocks the inter-part, inter-
human connection and brings inconsistent poses and unnatu-
ral artifacts at the connected joints. Recently, OSX [ 21] and
SMPLer-X [ 3] discard part experts and regress the model
in a holistic manner, which alleviates the artifacts. Their
paradigm can be abstract to Fig. 1b), however, they still need
to be given a bounding box to crop the image. While their
benchmarks show promising results, the accurate ground
truth bounding boxes are not attainable in real-world sce-
narios. RoboSMPLX [ 27] has demonstrated that the per-
formance drops significantly under noisy boxes. Moreover,
CLIFF [ 20] points out that the cropping operation discards
the location information, which degrades the performance.
A direct solution to address the challenges posed by the
multi-stage paradigm is to utilize a one-stage framework
that directly recovers EHPS from the entire image without
requiring additional boxes for cropping. However, current
one-stage methods [ 35‚Äì37] are proposed for HPS. Both of
them use a body center heatmap and mesh parameter map
to represent the potential human location and corresponding
features. Relying solely on these human-centered global fea-
tures is insufficient for achieving accurate part-wise regres-
sion. Although numerous two-stage HPS methods [ 14,47]
that extract local features in various ways, it is non-trivial to
extend to a one-stage model, as most of the representations,
like part-attention maps, are designed for a single person.
In order to tackle the above challenges, we have proposed
the first All-in-One-Stage (AiOS) EHPS method. This novel
approach is capable of predicting every individual present
in an image solely based on a single image input without
any additional requirements. Inspired by the achievement of
DETR-based [ 5] methods in various vision tasks [ 41,42,46,
48,54], we designed our pipeline in a DETR [ 5] style with
image feature encoder and various location-aware decoders.
We tailored different queries and association strategies to
progressively guide the decoder to perceive global and local
human features from the entire image.
Three key design features distinguish the AiOS model.
First , it is built upon DETR structure, with a CNN back-
bone, transformer encoders, and decoders, and progressively
detects human and decodes person features in an end-to-end
manner. Second , we introduced the "Human-as-Tokens" de-
sign, where humans are conceptualized as a collection of box
tokens and joint tokens. With different supervision and loca-tion cues, these tokens aggregate both global and local fea-
ture representations with cross-attention for enhanced model
accuracy in diverse scenarios. Third , using self-attention
and cross-attention mechanisms in our model allows for an
in-depth analysis of inter-human and intra-human relation-
ships, enhancing performance in crowded and occlusion-
heavy environments.
Extensive experiments show that our proposed model
has overpass state-of-the-art (SOTA) methods that utilize
ground truth bounding boxes and also SOTA methods when
the bounding box is not given. Further, our bounding box is
accurate enough to improve the other two-stage methods on
the AGORA benchmark.
In summary, our contributions are i) The first one-stage
method for EHPS that eliminates the need for extra detec-
tion networks; ii) A unified framework to integrate local and
global features for whole-body regression; iii) SOTA per-
formance on mainstream benchmarks without ground truth
bounding boxes.
2. Related Work
2.1. Expressive Human Mesh Recovery Methods
EHPS focuses on reconstructing the mesh of the human
body, hands, and face from monocular images. Pioneering
research in this domain introduced whole-body parametric
models such as SMPL-X [ 30]. With advancements in regres-
sion techniques for the human body, hands, and face, early
studies adopted multi-stage solutions [ 11,26,29,31]. They
independently recover body pose, hand pose, and facial ex-
pressions from cropped images before integration. However,
these multi-stage methods often produce artifacts at joint
intersections and present complex network designs.
Given the recent surge in whole-body datasets [ 1,2,4,
28,43], many approaches have transitioned to a holistic
paradigm. OSX [ 21] presents a groundbreaking one-stage
method, eliminating part-specific experts and cropped im-
age regression. SMPLer-X [ 3] further amplifies one-stage
methods utilizing large vision models and extensive datasets.
However, they still rely on bounding boxes for image crop-
ping. Despite their precision with ground truth bounding
boxes, performance degrades under detected boxes [27].
2.2. One-Stage Human Mesh Recovery Methods
Most of the existing HPS methods [ 6,8,13,16,22,39,44,
45,47] are multi-staged. Although these methods preserve
relatively high-resolution images and generally have higher
accuracy, they neglect other information in the full frame, in-
cluding inter-person occlusions and individual positions [ 20].
To address these limitations, ROMP [ 35] first proposed to
recover humans from an entire frame. It locates the human
locations from a body center heatmap and indexes the corre-
sponding features from the feature map to regress all human
1835
meshes. Furthermore, BEV [ 37] extends the 2D heatmap to
3D by incorporating a bird-eye-view. It enables the model to
discern 3D relative positions within the frame. TRACE [ 36]
further achieved simultaneously tracking humans and pre-
dicting camera motions with added motion maps. However,
these center-map-based methods often distill the human into
a single vector on the feature map and recover the human
pose and shape based on this global feature. We reckon
that this representation is insufficient for the EHPS task, par-
ticularly given that hand pose and expression require more
fine-grind local features for accurate regression.
3. Method
3.1. Motivation
For EHPS, using cropped images presents significant prob-
lems. The cropping discards the location information [ 20],
and inaccurate bounding boxes may lead to missing body
parts, negatively impacting performance. In crowded scenes,
cropping struggles to distinguish individual humans, with
parts from others intruding into the frame, leading to errors
in human part detection and regression. Especially when
people overlap significantly, the model struggles to differ-
entiate them due to unclear bounding boxes. Furthermore,
the detectors used are typically trained on general object de-
tection datasets and are not specifically designed for human
detection, adding to these difficulties.
To tackle these problems, we introduced the AiOS, the
first fully end-to-end network for EHPS. Abandoning the
uncertain assumption of box-as-subject, our model lever-
ages feature tokens and position queries for more precise
human localization. We‚Äôve developed a cohesive approach
that combines global and local feature representations for ac-
curate regression. To handle crowded scenarios and enhance
the separation of human figures, our model employs atten-
tion mechanisms to establish intricate relationships between
different body parts and between multiple individuals.
3.2. Preliminaries
SMPL-X. We use 3D parametric model SMPL-X [ 30] to
study EHPS. It utilizes a set of parameters to model body,
face, and hands geometries. Specifically, our model esti-
mates pose parameters Œ∏‚ààR53√ó3, which include body
poses Œ∏body‚ààR22√ó3, left hand poses Œ∏lhand‚ààR15√ó3, right
hand poses Œ∏rhand‚ààR15√ó3, and jaw poses Œ∏jaw‚ààR1√ó3.
Additionally, it estimates shape parameters Œ≤‚ààR10, and
facial expression parameters œà‚ààR10. We use the joint
regressor Jto obtain the 3D joint from the parameters by
J(M(Œ≤, Œ∏, œà )), where Mis the SMPL-X function.
3.3. Overview
AiOS includes the backbone and transformer encoder-
decoder structures. It has three steps, 1) localize coarse bodylocation and extract global features of the body; 2) refine
body location, extract body local features, localize coarse
hands and face locations and extract global features of the
hands and face; 3) refine hand and face location, extract local
features for whole body.
Backbone. AiOS utilizes the ResNet-50 [ 12] to extract a
multi-scale feature maps Fimg, which provide features from
detailed to holistic.
Encoder. As our task needs more than local associations, we
utilize a standard transformer encoder [ 38] for long-distance
relations. To transform the CNN-based feature map into a
transformer-compatible feature vector, we flatten the multi-
layer feature maps along their spatial dimensions and con-
catenate them. The flattened feature is added with position
encodings PE‚ààRM√óDto derive the image feature token
Timg‚ààRM√óD, where Mrepresents the total length of the
image feature token. We fed Timga transformer encoder,
which produces the refined image feature tokens T‚Ä≤
img, serv-
ing as a reference for cross-attention in the decoder. Utilizing
T‚Ä≤
img, a feed-forward network (FFN) is applied to classify
each token as a human token. Following the approach in
DINO [ 49] and ED-Pose [ 42], we filter based on the classi-
fication score and retain the top Mh= 900 tokens. These
tokens serve as candidate human body localization tokens
Tbody‚ààRMh√óD, and they also function as the input for the
subsequent decoders.
Generic Decoder. Similar to PETR [ 33] and ED-Pose [ 42],
which extend the deformable decoder [ 53] to 2D human
body-only pose estimation, AiOS extends the deformable
decoder to 3D whole-body mesh recovery. It mainly has
three inputs, image content tokens T‚Ä≤
img‚ààRM√óD, object
content tokens T‚ààRMh√óDand object position queries Q‚àà
RMh√ó4. Utilizing this decoder, our model can automatically
probe the suitable global and local features around the body
parts for each human conditioned by various queries. We will
introduce our key decoder designs in the following sections.
3.4. Naive AiOS
Drawing inspiration from ROMP [ 35], we extend the DETR
structure [ 5] to EHPS and progressively regress SMPL-X pa-
rameters. Specifically, we follow DAB-DETR [ 24] and intro-
duce the location queries to probe the body, face, and hands-
related features, guided by bounding boxes (x, y, w, h ), that
considers both the location and size of each body part boxes.
The model first extracts features related to the body using
body box location queries and refines them through the body-
location decoder. Subsequently, they are expanded to include
hands and face queries and leverage the whole-body-location
decoder to extract whole-body features.
Body-location Decoder. The first two decoders are body-
centric, and the input object content tokens Tare the body
location tokens Tbl. We derive body location query Qbl
with FFN from the corresponding Tbl. The decoder first
1836
Encoder
CNN
‚Ä¶
Body LocationDecoder
‚Ä¶
Bod RefinementDecoderFilter & Expand
‚Ä¶
Whole-body RefinementDecoderExpandFilter
‚Ä¶
ùëá!"#$
ùëÉùê∏
‚Ä¶
ùëá!"#
ùëá!"#$ùëá%&
ùëá!"#$
Body Classùëá'(ùëá%(
ùêπ!"#
Body Box
ùëÑ%&
‚Ä¶
ùëá%&
ùëÑ%&
‚Ä¶ùëá%(
ùëÑ%&
ùëÑ%&
ùëÑ'(
‚Ä¶
Body ParamBody JointBody, Face,Hands Box
Body, Face,Hands ParamBody, Face,Hands Joint
ùëÑ'(
ùëá!"#$ùëá'(
ùëá%&
ùëá)&ùëá*&ùëá+Body LocalizationBody RefinementWhole-body Refinement
Body, Face,Hands Box
Body ClassFigure 2. Pipeline overview . AiOS performs human localization and SMPL-X estimation in a progressive manner. It is composed of (1) the
body localization stage that predicts coarse human location; (2) the Body refinement stage that refines body features and produces face and
hand locations; (3) the Whole-body Refinement stage that refines whole-body features and regress SMPL-X parameters.
associates the body location tokens and updates them by the
self-attention mechanism. Then, the decoder takes image
tokens T‚Ä≤
imgas the value and the updated body location
tokens as the query for cross-attention, and the Qblacts
as an indicator, which is used to aggregate the information
focusing on the corresponding body area. After that, the
body location tokens Tbland body location queries Qblare
refined with the decoder.
We estimate the body bounding box with an FFN from
Tbl, which is supervised by Lbox. This supervision makes
sure the tokens aggregate global information of the human.
Similar to the encoder, we classify the output Tblwith an
FFN on whether it is a token representing a human. The clas-
sification results from Tblare supervised with classification
lossLcls. At the end of the second decoder, we downsample
the body tokens again to Mb= 100 to further distill potential
human tokens and lower the computational complexity.
Whole-body-location Decoder. The latter four decoders of
naive AiOS jointly consider whole-body information and
their association. With the body location tokens from the
previous step, we expand them to hands and face location
tokens with learnable embedding. We first broadcast the
given embedding Ebl‚ààRDand add it to the body location
token Tbl‚ààRMb√óD. After that, we obtained hand loca-
tion tokens Tlhl,Trhl, and face location tokens Tfl, which
have the same shape as Tbl. Then we concat them into a
whole-body token Tfull= [Tbl, Tlhl, Trhl, Tfl]. Similarly,
the whole-body location queries Qfullare expanded from
Qblwith learnable embeddings.
The decoders use a self-attention module to explore inter-
part and inter-human relations and then extract each part‚Äôs
features around their bounding boxes with a conditionedcross-attention module. We utilize an attention mask to en-
sure that the bounding boxes for each person‚Äôs hands and
face are associated only with their own and others‚Äô body
bounding boxes. As our model is already capable of recog-
nizing each person‚Äôs body in the first two stages, this specific
attention mechanism allows for more accurate identification
of body parts in crowded scenes. We provide an illustration
of the attention mechanism in the Supplementary Material.
We regress body bounding boxes from Tbl, face boxes
from Tfland hand boxes from Trhl,Tlhl, and supervise
them with Lbox. We regress different part‚Äôs parameters from
the refined whole body Tfulltokens. The parameters are
supervised with SMPL-X loss Lsmplx , which includes pa-
rameter loss Lparam , 3d keypoints loss Lkp3d, and the 2d
keypoints reprojection loss Lkp2d.
3.5. AiOS
Previous methods [ 35,37] have shown that regressing multi-
person body meshes from global features alone can achieve
impressive results, but in EHPS, relying on global informa-
tion alone is insufficient. The model should also consider
local information to obtain a detailed context of the whole-
body regression. Therefore, to elevate the model‚Äôs ability,
we introduce joint-related tokens and their corresponding
queries to our model. Combined with location tokens, the
AiOS expresses human context in multilevel. We will further
regress the SMPL-X parameter on this well-rounded feature
group. Specifically, we adopt a progressive detection and
decoding strategy. The first two layers are body-location
decoders same as our naive design, which outputs coarse
human location. Further, two layers of body-refinement de-
coders utilize body joint tokens to enrich local body features
1837
and estimate rough hand and face location simultaneously
on the basis of human location. At last, two layers of whole-
body-refinement decoders extract whole-body local features
with extra hands and face joint tokens.
Body-refinement Decoder. This decoder is built on body-
location decoders in naive AiOS. In detail, we expand body
joints tokens, hands location tokens, and face location to-
kens. We adopt the learnable-embedding Ebj‚ààR17√óD
to expand body joint tokens Tbj‚ààRMb√ó17√óDfrom box
location tokens, and then we obtain detailed body token
setTbd= [Tbl, Tbj, Tlhl, Trhl, Tfl]. Note that we use an at-
tention mask to limit the joint attention within its subject
as inter-joint attention among different subjects brings no
incremental but much higher computation complexity.
TheTbdare refined with layers of decoders. Within each
layer, similar to naive AiOS, we regress bounding boxes of
body parts from their location tokens and supervise them
withLbox. Further, we regress body joint location from Tbj
and supervise them with Lj2d, helping these joint tokens
learn the local human features. Different from Naive AiOS,
in this stage, we regress SMPL-X body parameters based on
Tbl,Tbj. We use Lsmplx to supervise the body parameter,
helping to refine the body-related tokens representing more
accurate body features.
Whole-body-refinement Decoder. This decoder further
expands the face and hand joint tokens. Similarly, we use
embedding Elhj,Erhj, and Efjto expand Tlhl,Trhl, and
TfltoTlhj,Trhj, and Tfj, respectively. At this stage, the
model forms the complete tokens that represent a human
Twd= [Tbl, Tbj, Tlhl, Tlhj, Trhl, Trhj, Tfl, Tfj].
Based on Twd, we utilize FFN to regress box location
from Tbl, Tlhl, Trhl, Tfland supervised with Lbox. We also
regress whole-body joint location from Tbj,Tlhj,Trhj, and
Tfj, and supervise them with Lj2d. Finally, we estimate
SMPL-X body, hands, and face parameters from body, hand,
and face-related tokens, respectively, and supervise whole-
body parameters with Lsmplx .
Overall Loss Functions. The overall loss function is the
sum of all the losses at each stage. Please refer to the Sup-
plementary Material for the details.
4. Experiment
4.1. Experimental Setup
Due to the page limit, we put the detailed experiment setup,
implementation, and partial quantitative and qualitative com-
parison with SOTA methods in the Supplementary Material.
Datasets. AiOS is trained on the multi-person datasets
AGORA [ 28], BEDLAM [ 1], and COCO [ 23], and single-
person datasets UBody [ 21], ARCTIC [ 9], and EgoB-
ody [ 52]. We evaluate it on AGORA, UBody, EHF [ 30],
ARCTIC [9], Egobody [52], and BEDLAM [1].
Implementation. The training is conducted on 16 V100GPUs, with a total batch size of 32. We first train our model
for 60 epochs on AGORA, BEDLAM, and COCO. We fine-
tune it for 50 epochs on all train datasets.
Evaluation metrics. Following the previous EHPS meth-
ods [3, 21, 26], we report Procrustes Aligned per-vertex po-
sition error (PA-MPVPE) and the mean per-vertex position
error (MPVPE) across all benchmarks. In AGORA Leader-
board, we report mean vertex error (MVE), mean per-joint
position error (MPJPE) for pure reconstruction accuracy; F
Score, precision, recall for detection accuracy; Normalized
mean vertex error (NMVE) and normalized mean joint error
(NMJE) that considered regression accuracy with detection
accuracy. All metrics are reported in millimeters (mm).
4.2. Quantitative comparison with SOTA
In Table 1, we compare AiOS with the SOTA methods on
the AGORA test set. The results are provided by the leader-
board2with their bounding boxes on the upper part of the ta-
ble. We also feed our estimated bounding boxes to OSX [ 21]
and SMPLer-X [ 3] on the lower part, which helps to verify
our model‚Äôs localization quality.
For a fair comparison with the SOTA methods, we utilize
a threshold of 0.5 to filter the detected samples with lower
confidence, which generally have severe occlusions. As
shown on the upper part of Table 1, our model‚Äôs NMVE and
NMJE greatly surpass the current SOTA method SMPLer-X.
This observation proves that our one-stage pipeline achieves
the best overall quality, combining localization and recon-
struction. In terms of pure reconstruction quality, our model
also achieves SOTA performance with a relatively accurate
detection result on MVE and MPJPE. While BEDLAM [ 1]
excels in face and hand reconstruction, its recall performance
is comparatively low, omitting some instances for evaluation.
On the lower-part comparison, we lower the detection
threshold to 0.3, which has higher recall than any current
results, allowing more hard cases to be detected. We feed the
same bounding boxes to the OSX and SMPLer-X, and their
performance on whole-body MVE improves compared with
the results reported in the original paper (122.8 to 121.3 for
OSX, 99.7 to 98.3 for SMPLer-X) even with a higher recall.
This indicates that improvement is achieved not by filtering
out hard cases but by providing high-quality bounding boxes.
This finding proves that the current two-stage method is
sensitive to bounding box quality, and using the ground
truth box to crop images in other benchmarks is biased from
real use cases. Notably, under this bounding box setting,
AiOS is still much higher than the current SOTA OSX and
comparable with the foundation model SMPLer-X L20.
As the first one-stage method in EHPS, we cannot find
relevant one-stage methods for a fair comparison. Therefore,
similar to H4W [ 26], we compare the results of our body
part with existing body-only methods, which can be broadly
2https://agora-evaluation.is.tuebingen.mpg.de/
1838
Methods F Score ‚ÜëPrecision ‚ÜëRecall‚ÜëNMVE ‚Üì(mm) NMJE ‚Üì(mm) MVE ‚Üì(mm) MPJPE ‚Üì(mm)
All Body All Body All Body Face LHand RHand All Body Face LHand RHhand
BEDLAM [1] 0.73 0.98 0.59 179.5 132.2 177.5 131.4 131.0 96.5 25.8 38.8 39.0 129.6 95.9 27.8 36.6 36.7
H4W [26]‚Ä†0.94 0.96 0.92 144.1 96.0 141.1 92.7 135.5 90.2 41.6 46.3 48.1 132.6 87.1 46.1 44.3 46.2
BEDLAM [1]‚Ä†0.73 0.98 0.59 142.2 102.1 141.0 101.8 103.8 74.5 23.1 31.7 33.2 102.9 74.3 24.7 29.9 31.3
PyMaF-X [50]‚Ä†0.89 0.90 0.89 141.2 94.4 140.0 93.5 125.7 84.0 35.0 44.6 45.6 124.6 83.2 37.9 42.5 43.7
OSX [21]‚àó0.94 0.96 0.93 130.6 85.3 127.6 83.3 122.8 80.2 36.2 45.4 46.1 119.9 78.3 37.9 43.0 43.9
HybrIK-X [18] 0.93 0.95 0.92 120.5 73.7 115.7 72.3 112.1 68.5 37.0 46.7 47.0 107.6 67.2 38.5 41.2 41.4
SMPLer-X [3] 0.93 0.96 0.90 133.1 88.1 128.9 84.6 123.8 81.9 37.4 43.6 44.8 119.9 78.7 39.5 41.4 44.8
SMPLer-X [3]‚Ä†0.93 0.96 0.90 107.2 68.3 104.1 66.3 99.7 63.5 29.9 39.1 39.5 96.8 61.7 31.4 36.7 37.2
Native AiOS 0.93 0.98 0.89 105.7 66.5 103.9 65.8 98.3 61.8 27.2 40.7 41.7 96.6 61.2 28.4 38.4 39.4
AiOS 0.94 0.98 0.90 97.8 61.3 96.0 60.7 91.9 57.6 24.6 38.7 39.6 90.2 57.1 25.7 36.4 37.3
OSX [21]‚àó‚ãÑ0.96 0.97 0.95 126.4 81.8 123.4 80.0 121.3 78.5 36.1 45.9 46.3 118.5 76.8 37.6 43.5 44.0
SMPLer-X [3]‚Ä†‚ãÑ0.96 0.97 0.95 102.4 63.8 99.5 62.1 98.3 61.2 30.3 40.4 40.7 95.5 59.6 31.7 37.9 38.2
AiOS 0.96 0.97 0.95 103.0 63.5 100.8 62.6 98.9 61.0 27.7 42.5 43.4 96.8 60.1 29.2 40.1 40.9
Table 1. AGORA SMPL-X test set. ‚Ä†denotes the methods finetuned on the AGORA training set. ‚àódenotes the methods trained on the
AGORA training set only. ‚ãÑdenotes the methods that use the AiOS‚Äôs bounding box to crop the image. The best results are colored with red,
and the second-best results are colored with blue for the upper and lower parts of the table, respectively.
Methods F1-score ‚ÜëPrecision ‚ÜëRecall‚ÜëNMVE ‚ÜìNMJE‚ÜìMVE‚ÜìMPJPE ‚Üì
Top-down Methods
HMR [13] 0.80 0.93 0.70 217.0 226.0 173.6 180.5
PyMAF [51] 0.84 0.86 0.82 200.2 207.4 168.2 174.2
PARE [14] 0.84 0.96 0.75 167.7 174.0 140.9 146.2
H4W [26]‚Ä†0.94 0.96 0.93 90.2 95.5 84.8 89.8
CLIFF [20]‚Ä†0.91 0.96 0.87 83.5 89.0 76.0 81.0
HybrIK [19]‚Ä†0.91 0.92 0.90 81.2 84.6 73.9 77.0
ProPose [10]‚Ä†0.90 0.91 0.89 78.8 82.7 70.9 74.4
PLIKS [32]‚Ä†0.94 0.95 0.93 71.6 76.1 67.3 71.5
NIKI [17]‚Ä†0.91 0.92 0.90 70.2 74.0 63.9 67.3
One-stage Methods
ROMP [35]‚Ä†0.91 0.95 0.88 113.6 118.8 103.4 108.1
BEV [37]‚Ä†0.93 0.96 0.90 108.3 113.2 100.7 105.3
AiOS 0.5 0.94 0.98 0.90 61.2 68.0 57.5 63.9
AiOS 0.3 0.96 0.97 0.95 63.4 70.1 60.9 67.3
Table 2. AGORA SMPL test set .‚Ä†indicates that this method is
fine-tuned on the AGORA training set. AiOS 0.5andAiOS 0.3, rep-
resenting the use of a 0.5 score threshold and a 0.3 score threshold
to filter the data, respectively.
categorized into top-down methods [ 10,13,14,17,19,20,
32,51] and one-stage methods [ 35,37], on the AGORA
SMPL test set. Specifically, we downsample the SMPL-
X mesh estimated by AiOS to the SMPL [ 25] mesh using
official tools [ 30] and then measure MVE and NMVE. We
use the J-regressor to regress joints from the downsampled
SMPL mesh to measure NMJE and MPJPE.
As shown in Table 2, even though AiOS is designed
for EHPS, it still outperforms ROMP [ 35] and BEV [ 37],
with a notable improvement in NMVE of 43% (from 108.3
mm to 61.2 mm) and an NMJE enhancement of 40% (from
113.2 mm to 68.0 mm). It is worth noting that we do not
deliberately fine-tune our model exclusively on AGORA.
Single datasets. We compare UBody in Table 3, EHF in
Table 4. Note that the other methods utilize ground-truth
bounding boxes. Without any given bounding boxes, our
model achieves SOTA performance on real-life datasets.
4.3. Qualitative comparison with SOTA
We perform a qualitative comparison with current SOTA
methods on AGORA and EHF. To overlay the results onto
the image, we apply an affine transformation for the two-
stage methods that use images cropped by ground truth boxes.PA-PVE ‚Üì(mm) PVE ‚Üì(mm)
Method All Hands Face All Hands Face
PIXIE [11] 61.7 12.2 4.2 168.4 55.6 45.2
H4W [26] 44.8 8.9 2.8 104.1 45.7 27.0
OSX [21] 42.4 10.8 2.4 92.4 47.7 24.9
OSX [21]‚Ä†42.2 8.6 2.0 81.9 41.5 21.2
SMPLer-X [3] 33.2 10.6 2.8 61.5 43.3 23.1
SMPLer-X [3]‚Ä†31.9 10.3 2.8 57.4 40.2 21.6
Native AiOS 35.6 8.6 2.9 62.7 41.3 20.8
AiOS 32.5 7.3 2.8 58.6 39.0 19.6
Table 3. UBody. ‚Ä†indicates the model is finetuned with the UBody
training set.
PA-PVE ‚Üì(mm) PVE ‚Üì(mm)
Method All Hands Face All Hands Face
H4W [26] 50.3 10.8 5.8 76.8 39.8 26.1
OSX [21] 48.7 15.9 6.0 70.8 53.7 26.4
SMPLer-X [3] 37.8 15.0 5.1 65.4 49.4 17.4
Native AiOS 38.8 13.8 4.0 50.2 49.8 17.3
AiOS 34.0 12.8 3.8 45.4 44.1 16.9
Table 4. EHF . As EHF is absent from our training data, it serves as
a valuable tool to assess the generalization ability of our models.
In contrast, our method can be directly overlaid on the im-
age. Further, with accurate betas estimation, we are able
to recover the depth order, as shown in the Fig. 3. We
achieve comparable visual quality in both scenes, proving
our model‚Äôs accuracy.
We further perform a qualitative comparison with SOTA
one-stage methods [ 35,37]. As shown in Fig. 4, while
ROMP and BEV can achieve decent results for body recon-
struction in multi-person scenarios, they are limited by the
constraints of the SMPL [ 25] model, preventing them from
reconstructing detailed hand gestures and facial expressions.
4.4. Ablation Study
In this subsection, we analyze the effectiveness of the pro-
posed components in detail. All experiments are conducted
1839
InputOSXSMPLer-XOursOurs
InputHand4WholeOSXSMPLer-XOurs
Figure 3. Comparison of current SOTA methods [ 3,21,26] with our AiOS model. The upper part is visualization results on AGORA [ 28],
and the lower is EHF test [7].
InputROMPBEVOurs
Figure 4. Visual comparisons with SOTA one-stage HPS methods [35, 37] on the Internet data3.
on the AGORA validation set.
Analysis of the naive AiOS and full AiOS. Whole-body
mesh recovery requires attention to both small-scale ges-
tures, expression details, and large-scale pose details. To val-
idate the effectiveness of our joint-guided local feature query,
we compared naive AiOS and full AiOS models across the
benchmarks. The table shows that even the naive setting
3https://www.pexels.com/achieves comparable performance with SOTA methods, in-
dicating our one-stage pipeline, which treats EHPS as a
progressive set prediction problem with various sequential
detections following the DETR, is ideal for SMPL-X param-
eter regression. On this solid base, the full AiOS consistently
achieves higher accuracy on all parts of the human, and the
increment on the whole-body aspect is especially outstand-
ing. Since the body tends to have a relatively higher area on
1840
Body BoxRight Hand BoxLeft Hand BoxFace BoxBody Box CenterRight Knee JointLeft Hand BoxLeft Thumb Joint
Figure 5. Attention Visualization. The green dots represent the
location of the reference point, and the red dots are the sampling
points.
the image, adding joint queries to the body provides a large
number of local features for reference, while for smaller ar-
eas like face and gestures, the difference between global and
local features is not that obvious. However, adding the local
joints feature overall brings more comprehensive features.
The Scheme of the SMPL-X supervision. In this part,
we investigate how to supervise different tokens. For our
original AiOS, we don‚Äôt supervise the SMPL-X parameter
in the first stage, as we want the model to focus on body
localization. In the second stage, we don‚Äôt supervise hands
and face for the same reason, but supervise SMPL-X body
parameters as we have detailed body feature tokens. And
we supervise the whole body parameter at the third stage. In
the first ablation setting, we add body parameter supervision
in every stage and hand and face supervision in the second
stage, meaning every stage has SMPL-X supervision. In the
second setting, we remove the SMPL-X body supervision in
the second stage so that the model will be only supervised by
SMPL-X in the last stage. As shown in Table 5, a comparison
between AiOS and all stage settings shows adding SMPL-X
parameters when the location is not properly refined will
hinder the model‚Äôs performance. Comparing the AiOS and
3rd stage setting shows the design of gradually whole-body
estimation from body to whole-body increased performance.
The association between the human body, hands, and
face. We focus on the self-attention relations on this part.
Our stock design allows free attention among body, face,
and hand location tokens, but limits joint tokens to only
attend with tokens belonging to the same human. In the
full attention setting, we allow tokens to any other tokens.
The inter-person setting will further limit the hand and face
location tokens to attend with only its subject. As shown in
Table 5, the unlimited setting is the worst, as the complicated
attention mechanism is not properly learned. And the limited
setting is also not ideal compared to our original attention
mechanism. Furthermore, we visualize the cross-attention
of our model. As shown in Fig. 5, our model is able toAblation StudiesPA-PVE ‚Üì(mm) PVE ‚Üì(mm)
All Hands Face All Hands Face
Attention Format
Full 42.5 7.2 4.2 54.8 39.0 25.8
Inter-human Only 41.7 7.3 4.2 52.8 38.9 24.5
Ours 39.9 7.2 4.1 50.5 37.4 23.3
SMPL-X Supervision Manners
All stages 42.7 7.4 4.2 55.7 39.8 25.1
3rd stage only 40.3 7.2 4.2 51.8 38.0 23.8
Ours (2,3 stage) 39.9 7.2 4.1 50.5 37.4 23.3
Table 5. Ablation Studies . The upper part studies the attention for-
mat, and the bottom part studies the SMPL-X supervision manners.
localize global features with body location tokens and local
features with joint tokens. The lower part shows the attention
map under occlusion, and it shows that our model will take
reference from other body parts.
5. Conclusion
In this work, we propose the first all-in-one-stage model for
expressive human pose and shape estimation. We explored
the incorporation of body-, face-, and hand-related tokens,
as well as the aggregation of local and global features with
various supervision. Moreover, we carefully designed a self-
attention mechanism to establish the associations between
inter- and intra-human body and body parts, which helps us
to achieve its best performance. The SOTA results indicate
our one-stage pipeline, which treats EHPS as a progressive
set prediction problem with various sequential detections
following the DETR, is a crucial factor contributing to the
overall performance. This can be further proved by the per-
formance of our naive AiOS baseline. We hope this work can
contribute new insights to the EHPS research community.
Limitations. First, our model achieves SOTA, but there is
still a large room for improvement if we add more datasets
for training, particularly those containing multi-person real
data. Second, the versatile design can be further extended
with more dimensions of human perception tasks such as
tracking and 3D localization. Exploring the estimation of
hands under limited resolution is also worth investigating.
Acknowledgement. This project is supported by the
Hong Kong Innovation and Technology Commission (In-
noHK Project CIMDA). It is also supported by the Min-
istry of Education, Singapore, under its MOE AcRF Tier
2 (MOET2EP20221- 0012), NTU NAP, and under the
RIE2020 Industry Alignment Fund ‚Äì Industry Collabora-
tion Projects (IAF-ICP) Funding Initiative, as well as cash
and in-kind contribution from the industry partner(s).
1841
References
[1]Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong
Yang. Bedlam: A synthetic dataset of bodies exhibiting de-
tailed lifelike animated motion. In IEEE Conf. Comput. Vis.
Pattern Recog. , pages 8726‚Äì8737, 2023. 2, 5, 6
[2]Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao
Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang
Pan, et al. Humman: Multi-modal 4d human dataset for
versatile sensing and modeling. In Eur. Conf. Comput. Vis. ,
pages 557‚Äì577. Springer, 2022. 2
[3]Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qing-
ping Sun, Yanjun Wang, Hui En Pang, Haiyi Mei, Mingyuan
Zhang, Lei Zhang, et al. Smpler-x: Scaling up expres-
sive human pose and shape estimation. arXiv preprint
arXiv:2309.17448 , 2023. 2, 5, 6, 7
[4]Zhongang Cai, Mingyuan Zhang, Jiawei Ren, Chen Wei,
Daxuan Ren, Zhengyu Lin, Haiyu Zhao, Lei Yang, and Zi-
wei Liu. Playing for 3d human recovery. arXiv preprint
arXiv:2110.07588 , 2021. 2
[5]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-
end object detection with transformers. In Eur. Conf. Comput.
Vis., pages 213‚Äì229. Springer, 2020. 2, 3
[6]Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. Cross-
attention of disentangled modalities for 3d human mesh re-
covery with transformers. In Eur. Conf. Comput. Vis. , pages
342‚Äì359. Springer, 2022. 2
[7]Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-
itrios Tzionas, and Michael J Black. Monocular expressive
body regression through body-driven attention. In Eur. Conf.
Comput. Vis. , pages 20‚Äì40. Springer, 2020. 2, 7
[8]Zhiyang Dou, Qingxuan Wu, Cheng Lin, Zeyu Cao,
Qiangqiang Wu, Weilin Wan, Taku Komura, and Wenping
Wang. Tore: Token reduction for efficient human mesh recov-
ery with transformer. arXiv preprint arXiv:2211.10705 , 2022.
2
[9]Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed
Kocabas, Manuel Kaufmann, Michael J Black, and Otmar
Hilliges. Arctic: A dataset for dexterous bimanual hand-
object manipulation. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 12943‚Äì12954, 2023. 5
[10] Qi Fang, Kang Chen, Yinghui Fan, Qing Shuai, Jiefeng Li,
and Weidong Zhang. Learning analytical posterior probability
for human mesh recovery. In IEEE Conf. Comput. Vis. Pattern
Recog. , 2023. 6
[11] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios
Tzionas, and Michael Black. Collaborative regression of
expressive bodies using moderation. In International Con-
ference on 3D Vision (3DV) , pages 792‚Äì804, Dec. 2021. 2,
6
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 770‚Äì778, 2016. 3
[13] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and pose.
InIEEE Conf. Comput. Vis. Pattern Recog. , pages 7122‚Äì7131,
2018. 2, 6
[14] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,and Michael J Black. Pare: Part attention regressor for 3d
human body estimation. In Int. Conf. Comput. Vis. , pages
11127‚Äì11137, 2021. 2, 6
[15] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-fitting in the loop. In Int. Conf. Comput.
Vis., pages 2252‚Äì2261, 2019. 2
[16] Nikos Kolotouros, Georgios Pavlakos, and Kostas Daniilidis.
Convolutional mesh regression for single-image human shape
reconstruction. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 4501‚Äì4510, 2019. 2
[17] Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang,
and Cewu Lu. NIKI: Neural inverse kinematics with invertible
neural networks for 3d human pose and shape estimation. In
IEEE Conf. Comput. Vis. Pattern Recog. , June 2023. 6
[18] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang,
and Cewu Lu. Hybrik-x: Hybrid analytical-neural inverse
kinematics for whole-body mesh recovery. arXiv preprint
arXiv:2304.05690 , 2023. 2, 6
[19] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang,
and Cewu Lu. Hybrik: A hybrid analytical-neural inverse
kinematics solution for 3d human pose and shape estimation.
InIEEE Conf. Comput. Vis. Pattern Recog. , pages 3383‚Äì3393,
2021. 2, 6
[20] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,
and Youliang Yan. Cliff: Carrying location information in
full frames into human pose and shape estimation. In Eur.
Conf. Comput. Vis. , pages 590‚Äì606. Springer, 2022. 2, 3, 6
[21] Jing Lin, Ailing Zeng, Haoqian Wang, Lei Zhang, and Yu
Li. One-stage 3d whole-body mesh recovery with compo-
nent aware transformer. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 21159‚Äì21168, 2023. 2, 5, 6, 7
[22] Kevin Lin, Lijuan Wang, and Zicheng Liu. Mesh graphormer.
InInt. Conf. Comput. Vis. , pages 12939‚Äì12948, 2021. 2
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In Eur.
Conf. Comput. Vis. , pages 740‚Äì755. Springer, 2014. 5
[24] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi,
Hang Su, Jun Zhu, and Lei Zhang. DAB-DETR: Dynamic
anchor boxes are better queries for DETR. In Int. Conf. Learn.
Represent. , 2022. 3
[25] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. ACM Trans. Graph. , 34(6):1‚Äì16, 2015.
6
[26] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Ac-
curate 3d hand pose estimation for whole-body 3d human
mesh estimation. In IEEE Conf. Comput. Vis. Pattern Recog.
Worksh. , 2022. 2, 5, 6, 7
[27] Hui En Pang, Zhongang Cai, Lei Yang, Qingyi Tao, Zhonghua
Wu, Tianwei Zhang, and Ziwei Liu. Towards robust and
expressive whole-body human pose and shape estimation. In
Adv. Neural Inform. Process. Syst. , 2023. 2
[28] Priyanka Patel, Chun-Hao P Huang, Joachim Tesch, David T
Hoffmann, Shashank Tripathi, and Michael J Black. AGORA:
Avatars in geography optimized for regression analysis. In
IEEE Conf. Comput. Vis. Pattern Recog. , pages 13468‚Äì13478,
2021. 2, 5, 7
1842
[29] Georgios Pavlakos, Vasileios Choutas, Timo Bolkart, Dim-
itrios Tzionas, Michael J. Black, Vasileios Choutas, Georgios
Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael J.
Black. Monocular expressive body regression through body-
driven attention. Eur. Conf. Comput. Vis. , 2020. 2
[30] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo
Bolkart, Ahmed A. Osman, Dimitrios Tzionas, and Michael J.
Black. Expressive body capture: 3d hands, face, and body
from a single image. In IEEE Conf. Comput. Vis. Pattern
Recog. , 2019. 1, 2, 3, 5, 6
[31] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap:
A monocular 3d whole-body pose estimation system via re-
gression and integration. In Int. Conf. Comput. Vis. Worksh. ,
2021. 2
[32] Karthik Shetty, Annette Birkhold, Srikrishna Jaganathan, Nor-
bert Strobel, Markus Kowarschik, Andreas Maier, and Bern-
hard Egger. Pliks: A pseudo-linear inverse kinematic solver
for 3d human body estimation. In IEEE Conf. Comput. Vis.
Pattern Recog. , pages 574‚Äì584, 2023. 6
[33] Dahu Shi, Xing Wei, Liangqi Li, Ye Ren, and Wenming Tan.
End-to-end multi-person pose estimation with transformers.
InIEEE Conf. Comput. Vis. Pattern Recog. , pages 11059‚Äì
11068, 2022. 3
[34] Qingping Sun, Yi Xiao, Jie Zhang, Shizhe Zhou, Chi-Sing
Leung, and Xin Su. A local correspondence-aware hybrid
cnn-gcn model for single-image human body reconstruction.
IEEE Transactions on Multimedia , 25:4679‚Äì4690, 2023. 2
[35] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and Tao
Mei. Monocular, one-stage, regression of multiple 3d people.
InInt. Conf. Comput. Vis. , pages 11179‚Äì11188, 2021. 2, 3, 4,
6, 7
[36] Yu Sun, Qian Bao, Wu Liu, Tao Mei, and Michael J Black.
Trace: 5d temporal regression of avatars with dynamic cam-
eras in 3d environments. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 8856‚Äì8866, 2023. 3
[37] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J
Black. Putting people in their place: Monocular regression
of 3d people in depth. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 13243‚Äì13252, 2022. 2, 3, 4, 6, 7
[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Adv. Neural Inform.
Process. Syst. , 30, 2017. 3
[39] Wenjia Wang, Yongtao Ge, Haiyi Mei, Zhongang Cai, Qing-
ping Sun, Yanjun Wang, Chunhua Shen, Lei Yang, and
Taku Komura. Zolly: Zoom focal length correctly for
perspective-distorted human mesh reconstruction. arXiv
preprint arXiv:2303.13796 , 2023. 2
[40] Yanjun Wang, Qingping Sun, Wenjia Wang, Jun Ling,
Zhongang Cai, Rong Xie, and Li Song. Learning dense
uv completion for human mesh recovery. arXiv preprint
arXiv:2307.11074 , 2023. 2
[41] Jie Yang, Ailing Zeng, Feng Li, Shilong Liu, Ruimao Zhang,
and Lei Zhang. Neural interactive keypoint detection. In Int.
Conf. Comput. Vis. , pages 15122‚Äì15132, 2023. 2
[42] Jie Yang, Ailing Zeng, Shilong Liu, Feng Li, Ruimao
Zhang, and Lei Zhang. Explicit box detection unifies
end-to-end multi-person pose estimation. arXiv preprint
arXiv:2302.01593 , 2023. 2, 3[43] Zhitao Yang, Zhongang Cai, Haiyi Mei, Shuai Liu, Zhaoxi
Chen, Weiye Xiao, Yukun Wei, Zhongfei Qing, Chen Wei,
Bo Dai, et al. Synbody: Synthetic dataset with layered human
models for 3d human perception and modeling. arXiv preprint
arXiv:2303.17368 , 2023. 2
[44] Ailing Zeng, Xuan Ju, Lei Yang, Ruiyuan Gao, Xizhou Zhu,
Bo Dai, and Qiang Xu. Deciwatch: A simple baseline for 10 √ó
efficient 2d and 3d pose estimation. In Eur. Conf. Comput.
Vis., pages 607‚Äì624. Springer, 2022. 2
[45] Ailing Zeng, Lei Yang, Xuan Ju, Jiefeng Li, Jianyi Wang, and
Qiang Xu. Smoothnet: A plug-and-play network for refining
human poses in videos. In Eur. Conf. Comput. Vis. , pages
625‚Äì642. Springer, 2022. 2
[46] Fangao Zeng, Bin Dong, Yuang Zhang, Tiancai Wang, Xi-
angyu Zhang, and Yichen Wei. Motr: End-to-end multiple-
object tracking with transformer. In Eur. Conf. Comput. Vis. ,
2022. 2
[47] Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, and Xi-
aogang Wang. 3d human mesh regression with dense corre-
spondence. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
7054‚Äì7063, 2020. 2
[48] Aixi Zhang, Yue Liao, Si Liu, Miao Lu, Yongliang Wang,
Chen Gao, and Xiaobo Li. Mining the benefits of two-stage
and one-stage hoi detection. Adv. Neural Inform. Process.
Syst., 34:17209‚Äì17220, 2021. 2
[49] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun
Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr
with improved denoising anchor boxes for end-to-end object
detection. arXiv preprint arXiv:2203.03605 , 2022. 3
[50] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng
Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards
well-aligned full-body model regression from monocular im-
ages. IEEE Trans. Pattern Anal. Mach. Intell. , 2023. 6
[51] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,
Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human
pose and shape regression with pyramidal mesh alignment
feedback loop. In Int. Conf. Comput. Vis. , 2021. 2, 6
[52] Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein
Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Ego-
body: Human body shape and motion of interacting people
from head-mounted devices. In Eur. Conf. Comput. Vis. , pages
180‚Äì200. Springer, 2022. 5
[53] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang
Wang, and Jifeng Dai. Deformable detr: Deformable trans-
formers for end-to-end object detection. arXiv preprint
arXiv:2010.04159 , 2020. 3
[54] Cheng Zou, Bohan Wang, Yue Hu, Junqi Liu, Qian Wu, Yu
Zhao, Boxun Li, Chenguang Zhang, Chi Zhang, Yichen Wei,
et al. End-to-end human object interaction detection with
hoi transformer. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 11825‚Äì11834, 2021. 2
1843
