Pose-Transformed Equivariant Network for 3D Point Trajectory Prediction
Ruixuan Yu
Shandong University, Weihai
Weihai, 264209, China
yuruixuan@sdu.edu.cnJian Sun
Xiâ€™an Jiaotong University
Xiâ€™an, 710049, China
jiansun@xjtu.edu.cn
Abstract
Predicting 3D point trajectory is a fundamental learn-
ing task which commonly should be equivariant under Eu-
clidean transformation, e.g., SE(3). The existing equivari-
ant models are commonly based on the group equivariant
convolution, equivariant message passing, vector neuron,
frame averaging, etc. In this paper, we propose a novel
pose-transformed equivariant network, in which the points
are firstly uniquely normalized and then transformed by the
learned pose transformations, upon which the points after
motion are predicted and aggregated. Under each trans-
formed pose, we design the point position predictor consist-
ing of multiple Pose-Transformed Points Prediction blocks,
in which the global and local motions are estimated and
aggregated. This framework can be proven to be equiv-
ariant to SE(3) transformation over 3D points. We eval-
uate the pose-transformed equivariant network on exten-
sive datasets including human motion capture, molecular
dynamics modeling and dynamics simulation. Extensive
experimental comparisons demonstrated our SOTA perfor-
mance compared with the existing equivariant networks for
3D point trajectory prediction.
1. Introduction
3D Point trajectory prediction aims at forecasting the fu-
ture positions of the given 3D points, and the motion should
be equivariant under the transformation of the input points.
It is widely applied to the areas such as particle dynamics
modeling or simulation [14, 20, 30], human motion cap-
ture [14, 33], human robot interaction [22, 32], etc.
Various models have been proposed to achieve SE(3)
equivariance for 3D point analysis. The equivariant GNN-
based mothods [14, 15, 26, 33] established equivariant mes-
sage passing by learning linear combinations of the in-
put vectors, ensuring explicit equivariance for point rota-
tion and translation. The high-order-type methods [10, 11,
16, 29] achieve equivariance by building convolution ker-
nels based on equivariant high-order representation such asspherical harmonics or lie group. The vector-based meth-
ods [6, 17, 19, 28] generalize traditional network layers with
scalar neurons into vector neurons or update scalar-valued
feature using the norm of vector-valued features. The frame
averaging methods [8, 25] build equivariant models by av-
eraging over all the transformation group elements. These
equivariant models have achieved promising results over
point clouds. However, the equivariant network has intro-
duced additional computational cost for ensuring equivari-
ance, and sometimes limits the prediction accuracy.
Under an equivariant framework, how to design the spe-
cific network architecture is a challenging task. For point
trajectory prediction, the GNN-type models [14, 15, 26, 33]
are based on SE(3)-invariant relation reasoning to aggre-
gate geometric features such as input point coordinates. In
the high-order-type models [11, 29] and vector-based mod-
els [6, 17, 19, 28], they adaptively design fundamental lay-
ers such as convolution, non-linear and batch normalization
layer, etc., and build equivariant point trajectory network
based on these layers. For frame averaging methods [8, 25],
they directly utilize the architectures of classical deep learn-
ing models [13] to build their network architecture. As a
summary, how to design the equivariant learning framework
and the corresponding network architecture are still chal-
lenging and deserving to be investigated.
In this paper, for equivariant 3D point trajectory pre-
diction, we propose a novel pose-transformed equivariant
framework. It firstly normalizes the point cloud pose using
the unique pose-normalization. Then it learns to transform
the pose-normalized point clouds, and estimate/aggregate
the point predictions upon these transformed point clouds.
For the specific network design, we propose to learn the de-
composed global and local motion displacements using the
proposed Global-Local Motion Estimation Layer. Based on
these ideas, we design a deep network architecture, dubbed
Pose-Transformed Equivariant Network (PT-EvNet), for 3D
point trajectory prediction. We apply the proposed PT-
EvNet for 3D point trajectory prediction, including human
motion capture, molecular dynamics modeling and dynam-
ics simulation. Experiments show that PT-EvNet outper-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
5503
forms the state-of-the-art methods on these tasks. We also
conduct ablation studies to show the effectiveness of our
equivariant framework and the network designs.
2. Related Works
In this work, we focus on SE(3)equivariant model for 3D
point trajectory prediction. Given 3D point cloud P, the
equivariant model Î¨should predict point cloud Î¨(P)that
satisfy g(Î¨(P)) = Î¨( g(P)),âˆ€gâˆˆSE(3). We briefly
review the related works on equivariant models for 3D point
analysis including point trajectory prediction.
Group equivariant convolution [4] generalizes the spatial
convolution to group convolution under actions of discrete
symmetric group elements. It is further generalized to ten-
sor features in [24, 37]. Those approaches are theoretically
sound, however suffers from rapidly growing computational
cost with the increase of group cardinality.
The spherical harmonics or lie group representation [1,
10â€“12, 16, 29] is popular for building equivariant net-
work. The spherical harmonics are equivariant to contin-
uous group transformation and utilized to design basic net-
work layers for the equivariant models [5, 9, 11, 29]. The
method of LieConv [10, 16] maps point position into group
element on which one can conduct equivariant convolu-
tion with kernel parameterized as a neural network. These
networks have achieved remarkable success for equivariant
data analysis tasks, but rely on the high-order transforma-
tions with higher computational cost.
The equivariant graph neural networks of EGNN [26],
GMN [15] and EGHN [14] are designed based on equiv-
ariant message passing by weighted combination of input
vectors, and they keep equivariant under SE(3)transfor-
mation of the input vectors. GMN [15] relies on equivariant
geometrical constrains within the data during the message
passing progress, and EGHN [14] is based on equivariant
pooling and unpooling operations on the graph neural net-
work. These models have shown effectiveness for equiv-
ariant 3D data analysis. However, they learn features with
a scalar-based embedding to keep the equivariance of node
position and may be insufficient for expressing more com-
plex geometric quantities like torsion force [7].
Vector Neuron [6] extends neurons from 1D scalars to
3D vectors, enables SO(3)actions in the latent feature
space, and provides a general framework for building equiv-
ariant neural layers. The similar scalar-vector transforms
for equivariance can be found [18, 27]. These models
achieve equivariant by weighted combination on input vec-
tors, which maintain equivariance under SE(3)transforma-
tion on the input vectors. However, different dimensions
of the vectors are separated in their fully connected layer,
prohibiting information flows between dimensions [23]. In
addition, the vector-based layers may not well model the
geometric relationships of points such as angles.Frame Averaging [8, 25] achieves equivariance by ag-
gregating prediction over the elements of a symmetry group
computed by PCA. For N-dimensional data, the order of
the group is 2N, which is 23= 8 for 3D data. The frame
averaging method relies on the predictions and averaging
over these fixed transforms in the symmetry group.
Compared with frame averaging methods, we uniquely
normalize the pose of a 3D point cloud instead of construct-
ing a symmetry group of transformations. In network ar-
chitecture, we learn to transform point clouds for prediction
instead of using fixed transformations by symmetry group
action. Our framework is proven to be SE(3)equivariant,
and extensive experiments demonstrate its superiority com-
pared with the state-of-the-art methods, including the frame
averaging method that uses 8 fixed pose-transformations.
3. Pose-Transformed Equivariant Network
In this section, we first introduce setting of equivariant tra-
jectory prediction, and then present the equivariant frame-
work followed by the equivariant network architecture.
Problem Setting. Following the common setting in [14, 15,
26], we are given the 3D point cloud P={X, V, H, E },
with point position Xâˆˆ RNÃ—3, velocity Vâˆˆ RNÃ—3and
SE(3)-invariant point attribute Hâˆˆ RNÃ—d. The edge at-
tribute (node distance or indicator for connection types as
stick or hinge) E={eij|i, j= 1, ..., N}models point in-
teractions between points. The goal is to predict the future
point cloud Pâˆ—with point position Xâˆ—âˆˆ RNÃ—3equivari-
ant to SE(3)transformation on P.Ndenotes the number
of points, and dis the dimension of point attribute such as
particle charge or velocity norm. Note that SE(3)trans-
formation (including rotation and translation) is conducted
on the point coordinates of the point cloud, and the same
rotation is conducted on the velocity.
3.1. General Framework
Ï„ F Ï„1âˆ’
()Ï„ () FÏ„ï¯*ð’«ð’« ð’«ð’« ð’«ð’« ð’«ð’«
Figure 1. Illustration of the proposed equivariant framework Î¨.
For the given 3D point cloud P, we first design a general
equivariant framework as shown in Fig. 1. The 3D point
cloud is firstly normalized by the transformation Ï„, and then
we conduct trajectory prediction by the pose-transformed
point position predictor Fover the pose-normalized point
cloud Ï„(P), and the output is the predicted point cloud af-
ter motion. Finally, the predicted point cloud Fâ—¦Ï„(P)is
transformed back to the pose of PbyÏ„âˆ’1. This framework
is equivariant to SE(3)transformation of P, and the pre-
5504
Unique Pose -Normalization
Ï„
Pose -Transformed Points Prediction Block  Global- Local Motion Estimation Layerâ€¦
1Ï„âˆ’PCA
Point Position Predictor FfÎ“â€¦
Î¦
â€¦
Trans.â€¦2Ï„1Ï„
KÏ„
Pose -1
2Ï„âˆ’1
1Ï„âˆ’
1
KÏ„âˆ’âˆ‘â€¦Ã—SN
fÎ“fÎ“ð’«ð’«
ð’«ð’«âˆ—Figure 2. Pipeline of PT-EvNet. Point cloud Pis firstly pose-normalized with Ï„which is uniquely computed with PCA. The trajectory
is then predicted with Point Position Predictor F, which is a concatenation of Pose-Transformed Points Prediction Block fÎ“based on
learnable pose transformations and iterative global-local motion estimation.
dicted point cloud Pâˆ—is
Pâˆ—= Î¨(P) =Ï„âˆ’1â—¦Fâ—¦Ï„(P). (1)
We design the components Ï„andFof this framework, espe-
cially the dedicated design of Fbased on pose-transforms,
presented in the following paragraphs and Sect. 3.2- 3.4.
Unique Pose-Normalization Ï„by PCA. For point cloud
P, we define the pose-normalization based on PCA. Specif-
ically, we first centralize the point coordinates by subtract-
ing the points center Âµ, then compute the SVD over the co-
variance matrix, and the rotation matrix Uis the singular-
vector matrix from the SVD of (Xâˆ’Âµ)âŠ¤(Xâˆ’Âµ). The
pose-normalized point cloud is derived as
Ï„(P) ={X,V , H, E }, (2)
with the normalized position and velocity as
X= (Xâˆ’Âµ)U, V=V U. (3)
One challenge for PCA based pose-normalization is the un-
certainty on direction of singular vectors [8, 25, 31, 34, 35],
i.e., the columns of rotation matrix U. Inverting the singular
vector directions leads to different rotation matrix Uâ€²=US
where S=diag{Â±1,Â±1,Â±1}is diagonal matrix with ele-
ments as Â±1. We uniquely compute the rotation matrix U
by taking the strategy in [36]. Specifically, we determine
the direction of singular vectors by estimating the angles be-
tween each singular vector with a predefined anchor point,
i.e., the farthest point from centroid. The direction should
be flipped if the corresponding angle is larger than 90â—¦.
With this strategy, we determine Uby singular vectors with
unique directions and compute the pose-normalized point
cloud Ï„(P)by Eqns. (2-3). Ï„âˆ’1(Â·)is the inverse transform
ofÏ„, which conducts inverse rotation UâŠ¤for point velocity
and position, and further translation Âµon point position.
Point Position Predictor F.This operator takes the pose-
normalized point cloud Ï„(P)as input and predicts the futurepoint position after motion in the trajectory. This operator
is defined as multi-steps architecture for progressively re-
fining the estimated point cloud, and each step is denoted
as an operator fÎ“that relies on learnable SE(3)pose trans-
formations Î“. The operator Fis conceptually defined as
Fâ—¦Ï„(P)â‰œfÎ“â—¦ Â·Â·Â· â—¦ fÎ“â—¦Ï„(P). (4)
where fÎ“predicts future point cloud, by taking current nor-
malized point cloud as input. It estimates point motions
from multiple poses Î“with a global-local strategy. We will
present its details in Sect. 3.3.
The equivariance of Pâˆ—estimated by Eqn. (1) can be
proven in the following theorem, please refer to the sup-
plementary material for the proof.
Theorem 1. WithÏ„as Unique Pose-Normalization by PCA
andFas Point Position Predictor, Î¨(P)is equivariant un-
der SE(3) transformation of P.
3.2. Overview of Network Architecture
Based on above equivariant framework, we design the Pose-
Transformed Equivariant Network (PT-EvNet) by con-
cretizing the structure of FandfÎ“.
As shown in Figure 2, given point cloud P, PT-EvNet
first conducts Pose-Normalization byÏ„which is uniquely
computed by PCA as in Sect. 3.1. Then Point Position Pre-
dictor ( F)is utilized to predict the future point cloud over
pose-normalized point cloud Ï„(P). The predicted point
cloud are finally transformed back to the pose of Pwith
inverse transformation Ï„âˆ’1as final prediction of PT-EvNet.
The point position predictor Fis designed as the cascade
of several Pose-Transformed Points Prediction Block
(fÎ“). In every block fÎ“, it is fed with predicted point
cloud of its previous block (the first fÎ“takes Ï„(P)as in-
put). These blocks estimate point cloud after movement
5505
from multiple pose-transformed point clouds with Global-
Local Motion Estimation Layer ( Î¦), which conducts mo-
tion estimation with a combined global-local strategy. In
the following subsections, we will introduce them in detail.
3.3. Pose-Transformed Points Prediction Block ( fÎ“)
Taking the unique pose-normalized point cloud P=Ï„(P)
as input, Pose-Transformed Points Prediction Block fÎ“is
designed to estimate the corresponding point cloud after
movement. We accomplish this goal by predicting point-
wise motion and attribute on pose-transformed point clouds
fromP, which help us to explore the feature from multi-
ple poses and coordinates. We design fÎ“using two funda-
mental layers, i.e., the Pose-Transformation layer that maps
normalized point cloud PtoNKposes with learnable trans-
formations Î“ ={Ï„k;k= 1,Â·Â·Â·NK}and the Global-Local
Motion Estimation Layer Î¦that estimates the point-wise
motion and attribute for the pose-transformed point cloud.
Specifically, for the pose-normalized point cloud Pwith
point position, velocity, point attribute and edge attribute
as{X,V , H, E }, it is firstly transformed into NKposes
using learnable Euclidean transformations of Î“in the pose-
transformation layer, arriving at pose-transformed points
Ï„k(P) ={Xk, Vk, Hk, E}, k= 1, ..., N K, (5)
with its elements as
Xk=XUk+Âµk, Vk=V Uk, Hk=H, (6)
where Âµkâˆˆ R3Ã—1is the learnable translation vector,
Ukâˆˆ R3Ã—3is rotation matrix with the 6D rotation repre-
sentation [38] from 6D trainable parameters. These pose-
transformed point clouds are invariant under SE(3)trans-
formations of the network input P.
Then the point position and attribute are estimated over
all the pose-transformed point clouds with a shared Global-
Local Motion Estimation Layer Î¦(introduced in Sect. 3.4).
It takes Ï„k(P)as input and outputs estimated point cloud
Î¦â—¦Ï„k(P)with updated point position Xâ€²
kand attribute Hâ€²
k
Î¦â—¦Ï„k(P) ={Xâ€²
k, Vk, Hâ€²
k, E}. (7)
Finally, the predicted point cloud Î¦â—¦Ï„k(P)is trans-
formed back with inverse transformation Ï„âˆ’1
k. The pre-
dictions from all the pose-transformed point clouds are ag-
gregated as prediction of fÎ“, i.e., the output of this block
fÎ“â—¦P={Xâ€²,V , Hâ€², E}is obtained by
fÎ“â—¦P=1
NKNKX
k=1Ï„âˆ’1
kâ—¦Î¦â—¦Ï„k(P), (8)
where the point position and attribute are aggregated by
Xâ€²=1
NKNKX
k=1(Xâ€²
kâˆ’Âµk)UâŠ¤
k, Hâ€²=1
NKNKX
k=1Hâ€²
k.(9)In the Pose-Transformed Points Prediction Block fÎ“, the
point cloud is predicted from multiple transformed poses.
The pose transformation with Î“ ={Ï„k}Nk
k=1encourages Î¦
to explore rich positional information for point position pre-
diction and attribute learning. Î“is adaptively learned in
network driven by the training loss, providing flexibility for
specific tasks. Note that both the point position and attribute
are updated after this block, while the point velocity and
edge attribute keep unchanged.
Pose-Transformed Points Prediction Block fÎ“is de-
signed based on learnable pose transformations Î“in
Eqn. (8). This is different to frame averaging [25] which
conducts pose transformations using 8 elements of symme-
try group computed from PCA. Experimental comparison is
conducted in Sect. 4.4, demonstrating the superiority of our
learnable pose-transformed strategy.
3.4. Global-Local Motion Estimation Layer ( Î¦)
We now introduce the Global-Local Motion Estimation
layerÎ¦in Eqn. (8) for the point-wise position and attribute
estimation on the pose-transformed point cloud. Inspired
by global-local decomposition strategy of motion fields in
physics and engineering, the point-wise motion is estimated
with global and local displacements by three steps, i.e.,
Decomposition that separately represents global and local
components, Updating that iteratively learns global and lo-
cal displacement and attribute, Combination that combines
learned global and local components together as prediction
forÎ¦. An illustrative pipeline of the global-local motion
estimation layer Î¦is in Figure 2, which iteratively runs for
NStimes to estimate point motions with global and local
branches. The details of these three steps are as follows.
Decomposition. The input Ï„k(P) ={Xk, Vk, Hk, E}is
decomposed into global and local components in this step.
We remove index kfor brevity and denote point-wise posi-
tion, velocity, attribute and edge attribute as xi,vi,hi,eij
withi, jas point index. The point position and velocity are
decomposed into global and local components by
xg=1
NNX
i=1xi, vg=1
NNX
i=1vi,
xl
i=xiâˆ’xg, vl
i=viâˆ’vg,(10)
where superscripts g, lrepresent â€˜globalâ€™ and â€˜localâ€™ respec-
tively. xgandvgindicate the averaged position and veloc-
ity, while xl
iandvl
ireflect the relative geometric compo-
nents. The local and global attributes are computed by
hl
i=Ï([hi, xl
i, vl
i,||vl
i||,||vg||]), hg=MaxPool {hl
i},
(11)
where â€˜MaxPoolâ€™ is max-pooling operator by channel-wise
maximization across points. [Â·]denotes concatenation op-
eration, || Â· || denotes L2-norm, and Ïis for point attribute
learning that is designed as MLP.
5506
Updating. Based on above decomposition, the global and
local displacements âˆ†xgandâˆ†xl
iare iteratively updated in
the global and local branches, with âˆ†xg,âˆ†xl
iinitialized as
zero vectors. In each of the NSiterations, the global dis-
placement is updated in the global branch based on global
attribute hg, i.e.,
âˆ†xg= âˆ†xg+Îº(hg), (12)
where Îºis designed as MLP for global displacement learn-
ing. In the local branch , the local displacement is updated
based on local attribute and relationship by
âˆ†xl
i= âˆ†xl
i+Î²(hl
i) +X
jâˆˆN(i)Î¶(mij)Î·(xl
iâˆ’xl
j),
with mij=Î³([||xl
iâˆ’xl
j||, hl
i, hl
j, eij]),(13)
where Î², Î¶, Î·, Î³ are set as MLPs, N(i)denotes the neigh-
bors of point xiwithin two hops. The local and global point
attributes are then updated by
hl
i=Î¾([hl
i,X
jâˆˆN(i)mij]), hg=MaxPool {hl
i},(14)
withÎ¾set as MLP. In this Updating step, with NStimes
of iterations that sequentially conducts Eqns. (12-14), the
global and local displacements âˆ†xgandâˆ†xl
iare derived,
as well as the global and local point attribute hg, hl
i.
Combination. In this step, the global and local components
after Updating step are combined to form the estimation of
this layer. The estimated position Xâ€²={xi}is obtained by
combining global and local displacements âˆ†xg,âˆ†xl
iwith
xâ€²
i=xi+ âˆ†xg+ âˆ†xl
i. (15)
The corresponding point attributes are taken as Hâ€²={hâ€²
i}
withhâ€²
iâ‰œhl
i.
In this global-local motion estimation layer, taking pose-
transformed point cloud as input, we predict point position
Xâ€²and attribute Hâ€²with above three steps. This layer is
utilized as fundamental operation Î¦in the pose-transformed
points prediction block fÎ“as in Eqn. (8). The process of Î¦
is summarized as Algorithm 1 in supplementary material.
3.5. Details on Network Architecture and Training
PT-EvNet is designed based on the equivariant framework
proposed in Sect. 3.1 and the basic operations proposed in
Sect. 3.3-3.4. We design it using three pose-transformed
points prediction blocks fÎ“in the point position predic-
torF. In each block, we set NK= 4 in Eqn. (5), i.e.,
4 pose transformations, to transform the pose-normalized
point cloud, and we take shared transformations among the
three blocks. In the global-local motion estimation layer Î¦,we take NS= 4iterations to estimate global and local dis-
placements. Îº, Î², Î¶, Î· are set as two-layer MLPs with out-
put as three-dimensional vector, and Î¾, Ï, Î³ are set as two-
layer MLPs to learn 64-dimensional attribute. PT-EvNet
is trained by Adam optimizer with Mean Squared Error
(MSE) loss between predicted point position and ground
truth, and we set the batch-size to 12. Please refer to supple-
mentary material for more details. Our code will be avail-
able at https://github.com/yuruixuan/PT-
EvNet .
4. Experiments
We now evaluate our proposed PT-EvNet on multiple
benchmark datasets for 3D point trajectory prediction, and
conduct ablation studies to verify advantages of our designs.
4.1. Results on Motion Capture
Datasets. We conduct experiments on CMU Motion Cap-
ture Database [3] and evaluate different methods on Walk-
ing (Subject #35) and Running (Subject #9) subsets as in
EGHN [14]. For the Walking subset, we take 200/600/600
frame pairs for training/validation/testing, and for Running
subset, we take them as 200/240/240. For each pair of
frames, the 3D coordinates corresponding to the body joints
of the first frame are taken as input to estimate the coordi-
nates of these points in the paired frame. The frame interval
between each pair of frames is 30 in both scenarios. The
performance is evaluated in MSE.
Implementation details. For our PT-EvNet, the same ex-
periment setting is utilized as [14], and the input node at-
tribute His taken as L2-norm of velocity, represented by
the difference of positions in neighboring frames. The edge
attribute eijin Eqn. (13) is set to 1 for edges connecting
neighboring points, 2 for edges of points in two hops and
0 for others. All the results of the compared methods ex-
cept EqMotion [33] and FA-GNN [25] are reported from
[14], and we report the results of EqMotion and FA-GNN
by running their codes.
Results and comparisons. The comparative results are pre-
sented in Table 1. We also present the results of PT-EvNet-
B1 that uses only one pose-transformed points prediction
block fÎ“in the point position predictor F. Our PT-EvNet
achieves better performance than all the compared methods.
Notably, PT-EvNet reduces the MSE by 1.0%and1.6%on
both datasets compared with the second best method. Even
PT-EvNet-B1 with one pose-transformed points prediction
block performs better than all these compared methods on
the Walking subset.
In Figure 3, we show the trajectory estimation results by
comparing with EGNN [26], EGHN [14], EqMotion [33]
and FA-GNN [25]. Our PT-EvNet predicts the points coor-
dinates more accurately.
5507
EGHN
PT-EvNet
EGNN
EqMotion
FA-GNN
Walking    Running    Aspirin  Benzene (3,3) 
Figure 3. Visualization of trajectory estimation results on CMU (1-4 columns), MD17 (5-8 columns) and Simulation (9-10 columns)
datasets. The ground truth are in blue, and the predictions are in red.
Walking Running
RF [21] 188.0 521.3
MPNN [13] 36.1 66.4
TFN[29] 32.0 56.6
SE(3)-Tr. [11] 31.5 61.2
EGNN [26] 28.7 50.9
GMN [15] 21.6 44.1
EGHN [14] 8.5 25.9
EqMotionâˆ—[33] 5.8 20.3
FA-GNNâˆ—[25] 7.2 37.3
PT-EvNet-B1 5.6 21.3
PT-EvNet 4.8 18.7
Table 1. Prediction error (MSE, Ã—10âˆ’2) on CMU dataset.âˆ—indi-
cates that the results are reported by running their codes.
4.2. Results on Molecular Dynamics Modeling
Datasets. We conduct experiment for molecular trajectory
prediction on MD17 dataset [2], which has trajectories of
eight molecules. Taking current system state (particle po-
sitions) as input, we aim to predict the future particle posi-
tions. We take the same experiment setting as GMN [15],
i.e., randomly split the dataset into training/validation/test
set containing 500/2000/2000 frame pairs respectively, and
taking 5000 frames as interval between frames pairs.
Implementation details. The initial node attribute Hfor
our PT-EvNet is taken as concatenation of velocity L2-
norm and particle charge. The same edge attribute Eis
taken as GMN [15], i.e., the concatenation of point dis-
tance, charges of edge connected points, indication for stick
or hinge, and indication for edge connected points withinone or two hops. The results of EGHN [14], EqMotion [33]
and FA-GNN [25] are reported by running their codes, and
the results of the other compared methods are from [15].
Results and comparisons. Prediction errors are presented
in Table 2. Our PT-EvNet surpasses most of the compared
methods and obtains the best performance on six out of
eight molecules. In particular, PT-EvNet achieves a sub-
stantial reduction in MSE from 48.12Ã—10âˆ’2to26.25Ã—
10âˆ’2on the Benzene subset. Figure 3 presents the pre-
dicted trajectory by PT-EvNet and the SOTA methods on
Aspirin (5-6 columns) and Benzene (7-8 columns) subsets.
Our model generates more reasonable predictions for both
of the two molecules.
4.3. Results on Simulation Dataset
Datasets. We further conduct experiments on the Simula-
tion Dataset [14], which includes single and multiple sys-
tems of particles. Given initial particles positions and ve-
locities, the goal is to predict their future positions. All
the experiments are conducted with the same setting as
EGHN [14], i.e., the training/validation/test set contains
1000/2000/2000 frame pairs respectively, and we take 1500
frames as the interval between every frame pairs.
Implementation details. We follow EGHN [14] and as-
sign node attribute as the L2-norm of velocity. The edge
attribute eijconnecting point iandjis set as the concate-
nation of point charges product, point distance, and edge
indicator for stick. Edge indicator is 1 if a stick exists and 0
otherwise. All the results of compared methods are reported
from [14] except EqMotion [33] and FA-GNN [25], which
are reported by running their official codes.
Results and comparisons. As shown in Table 3, PT-EvNet
5508
Aspirin Benzene Ethanol Malonaldehyde Naphthalene Salicylic Toluene Uracil
RF [21] 10.94 103.72 4.64 13.93 0.50 1.23 10.93 0.64
TFN [29] 12.37 58.48 4.81 13.62 0.49 1.03 10.89 0.84
SE(3)-Tr. [11] 11.12 68.11 4.74 13.89 0.52 1.13 10.88 0.79
EGNN [26] 14.41 62.40 4.64 13.64 0.47 1.02 11.78 0.64
EGNNReg [26] 13.82 61.68 6.06 13.49 0.63 1.68 11.05 0.66
GMN [15] 10.14 48.12 4.83 13.11 0.40 0.91 10.22 0.59
GMN-L [15] 9.76 54.17 4.63 12.82 0.41 0.88 10.45 0.59
EGHNâˆ—[14] 11.68 59.34 5.60 13.74 0.43 0.90 21.52 0.62
EqMotionâˆ—[33] 13.14 66.70 5.82 13.75 0.63 0.93 20.31 0.67
FA-GNNâˆ—[25] 12.83 117.50 5.81 13.70 0.41 0.88 20.15 0.61
PT-EvNet-B1 7.64 26.87 4.02 13.73 0.42 0.92 9.67 0.63
PT-EvNet 6.63 26.25 4.34 13.22 0.40 0.87 8.63 0.62
Table 2. Prediction errors on MD17 dataset measured in MSE ( Ã—10âˆ’2). * denotes the results obtained by running the official codes.
Single System Multiple System
(3,3) (5,5) (5,10) (10,10) (3,3) (5,5) (5,10) (10,10)
Linear [14] 35.15 35.22 30.14 31.44 35.91 35.29 30.88 32.49
TFN [29] 25.11 29.35 26.01 OOM 27.33 29.01 25.57 OOM
SE(3)-Tr. [11] 27.12 28.87 24.48 OOM 28.14 28.66 25.00 OOM
MPNN [13] 16.00 17.55 16.15 15.91 16.76 17.58 16.55 16.05
RF [21] 14.20 18.37 17.08 18.57 15.17 18.55 17.24 19.34
EGNN [26] 12.69 15.37 15.12 14.64 13.33 15.48 15.29 15.02
EGHN [14] 11.58 14.42 14.29 13.09 12.80 14.85 14.50 13.11
EqMotionâˆ—[33] 13.52 16.28 16.61 14.57 14.91 16.88 15.81 13.38
FA-GNNâˆ—[25] 11.14 13.95 15.44 14.37 13.27 15.56 15.30 13.80
PT-EvNet-B1 11.08 13.38 14.76 14.62 12.78 14.67 14.98 13.59
PT-EvNet 10.06 12.34 13.64 12.99 11.14 14.44 14.45 14.25
Table 3. Prediction errors on Simulation dataset measured in MSE ( Ã—10âˆ’2). The â€˜Multiple Systemâ€™ contains 5 different systems, and
(A,B) indicates the system containing A complexes of average size B. * denotes the results obtained by running the official codes.
achieves the best performance compared with other meth-
ods on all the single systems and the challenging multi-
ple systems except the multiple system of (10,10). Even
PT-EvNet-B1 with one pose-transformed points prediction
block in the point position predictor generates competitive
results. In the last two columns of Figure 3, we show the
predictions by our PT-EvNet and compared methods on the
single simulation dataset of (3,3), and PT-EvNet predicts
the system trajectories more accurately.
4.4. Model Analysis
Effectiveness of the proposed equivariant framework.
In our framework in Sect. 3, we conduct pose normaliza-
tion using Unique Pose-Normalization by PCA, and then
conduct pose transformations on the uniquely normalized
pose with learnable pose transformations Î“to estimate
point motions. We conduct ablation studies on multiple
datasets to demonstrate the effectiveness of our framework
with learned pose transformations. In comparison, we set8 pose transformations in Î“of our PT-EvNet, denoted as
PT. We further replace these transformations by symmetry
group transformations computed by PCA [25] (denoted as
FA) or random rotations (denoted as RT) that are randomly
set for each instance in each training/validation/test step.
Our framework using learned 8 transformations achieves
the best performance in all datasets as shown in Table 4,
demonstrating the effectiveness of our pose-transformed
equivariant framework. Please refer to supplementary ma-
terial for visualizing these learned transformations.
Effectiveness of global-local motion estimation layer Î¦.
We replace Î¦in PT-EvNet with linear operation, MLP, and
other basic point position prediction layers in SOTA meth-
ods, including layers used in MPNN [13], EGNN [26],
GMN [15] and EGHN [14] (please refer to supplementary
material for details). The results of these modified PT-
EvNets are shown in Table 5. PT-EvNet with Î¦achieves
the best performance on both Walking and Aspirin datasets.
PT-EvNet-local that predicts displacement with only local
5509
Dataset Subdataset FA RT PT
CMUWalking 4.83 10.04 4.82
Running 19.51 55.18 18.67
MD17Aspirin 7.13 11.45 6.58
Benzene 72.88 48.47 26.33
Ethanol 5.20 5.51 4.28
Simulation(3,3,1) 11.57 10.60 10.11
(5,5,1) 13.42 13.84 12.25
(3,3,5) 12.93 12.16 11.32
Table 4. Comparison of our network using frame averaging (FA),
random pose transformation (RT) and proposed pose transforma-
tion (PT) for equivaraint tasks on various datasets. The lower MSE
results demonstrate effectiveness of our framework.
Walking Aspirin
*-linear 158.46 16.62
*-mlp 143.22 13.04
*-mpnn 34.42 7.56
*-egnn 7.26 9.04
*-gmn 9.42 10.87
*-eghn 11.09 8.36
*-local 6.89 7.27
PT-EvNet 4.83 6.63
Table 5. Prediction error (MSE, Ã—10âˆ’2) on Walking and Aspirin
datasets. * denotes networks that replace our global-local motion
estimation layer Î¦with other layers.
4.84
02    4   8   16  324.85.05.25.45.36
4.954.834.824.83 4.84
#KN6.77.6
7.07.37.61
6.63
6.587.73
6.576.57
(a)MSE              (Ã—10âˆ’2)
1    2    3    4    5    64.85.4MSE              (Ã—10âˆ’2)
5.05.2
#SN
(c)5.115.27
4.93
4.834.856.97.8
7.27.58.1
7.138.09
7.04
6.637.23
6.72
SN4.85.4
5.05.2
(b)6.67.5
6.97.2
5.55
4.944.834.79 4.76
1     2       3      4      5 #BN7.64
6.99
6.637.107.73 MSE              (Ã—10âˆ’2)
BNKN
Figure 4. Ablation study on hyper-parameters in PT-EvNet on the
Walking (blue lines) and Aspirin (yellow lines) datasets.
branch achieves competitive but higher MSE values, show-
ing the necessity of both the global and local branches.
Effect of the number of pose transformation. Figure 4 (a)
shows the results of PT-EvNet using different number ( NK)
of pose transformations on Walking and Aspirin datasets.
When NK= 0, we directly predict point motion on the
pose-normalized point cloud without transformations. PT-
EvNet with pose transformation ( NK>0) achieves better
performance than network without it ( NK= 0). But with
the increase of NK, the performance becomes stable.Memory FLOPs Parameters MSE
(MB) Ã—108(Ã—105) (Ã—10âˆ’2)
MPNN [13] 2479 35.85 2.05 36.1
RF [21] 983 0.02 0.02 188.0
TFN [29] 4395 47.10 4.40 32.0
SE(3)-Tr. [11] 7573 138.45 12.76 31.5
EGNN [26] 995 1.28 1.34 28.7
GMN [15] 997 1.39 2.02 21.6
EGHN [14] 1103 1.68 3.18 8.5
EqMotion [33] 1289 23.55 2.86 5.8
FA-GNN [25] 995 6.86 0.92 7.2
PT-EvNet 1243 8.18 4.88 4.8
Table 6. Computational cost of PT-EvNet and compared methods,
with MSE reported on Walking dataset.
Effect of the number of pose-transformed points pre-
diction block. We show the performance curves of PT-
EvNet with different pose-transformed points prediction
block number ( NB) in Figure 4 (b). As shown in the fig-
ure, setting three blocks, i.e., NB= 3is enough to produce
a stable result, and even with one block, our network can
achieve better results than state-of-the-art methods on Wal-
ing and Aspirin datasets.
Effect of the iteration number for displacement estima-
tion. In the global-local motion estimation layer Î¦, the dis-
placements are estimated in NStimes. Figure 4 (c) show
the performance with different NSon Walking and Aspirin
datasets. Results show that PT-EvNet with 4 times of dis-
placement estimation performs best.
Computational cost. Table 6 presents the GPU memory,
FLOPs and parameters of PT-EvNet and SOTA methods
(please refer to supplementary material for details), as well
as their MSE results on Walking dataset. Even with pose
transformations and multiple MLPs in the network, our
model exhibits tolerable/comparable computational costs,
but achieves the overall best performance.
5. Conclusion
In this paper, we propose a pose-transformed equivari-
ant network framework for 3D point trajectory prediction.
The equivariance is achieved based on the unique pose-
normalization and learnable pose-transformations, over
which the global-local motion estimation is conducted. Ex-
perimental results show that the PT-EvNet outperforms the
compared SOTA methods on benchmark datasets, and abla-
tion studies justify the effectiveness of the network design.
In the future work, we plan to further improve the network
architecture and apply it for sequential trajectory prediction.
Acknowledgment This work was supported by NSFC
(62306167, 12125104, U20B2075) and AI Super Fusion
Private Cloud Module of Shandong University, Weihai.
5510
References
[1] Alexander Bogatskiy, Brandon Anderson, Jan Offermann,
Marwah Roussi, David Miller, and Risi Kondor. Lorentz
group equivariant neural network for particle physics. In
ICML , pages 992â€“1002, 2020. 2
[2] Stefan Chmiela, Alexandre Tkatchenko, Huziel E. Sauceda,
Igor Poltavsky, Kristof T. Sch Â¨utt, and Klaus-Robert M Â¨uller.
Machine learning of accurate energy-conserving molecular
force fields. Science Advances , 3(5):e1603015, 2017. 6
[3] CMU. Carnegie-mellon motion capture database. 2003. 5
[4] Taco S. Cohen and Max Welling. Group equivariant convo-
lutional networks. In ICML , pages 2990â€“2999, 2016. 2
[5] Taco S. Cohen and Max Welling. Steerable cnns. arXiv
preprint arXiv:1612.08498 , 2016. 2
[6] Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard,
Andrea Tagliasacchi, and Leonidas J Guibas. Vector neu-
rons: A general framework for so(3)-equivariant networks.
InICCV , pages 12200â€“12209, 2021. 1, 2
[7] Weitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nan-
ning Zheng, Bin Shao, and Tie-Yan Liu. Se(3) equivariant
graph neural networks with complete local frames. In ICML ,
pages 5583â€“5608, 2022. 2
[8] Alexandre Agm Duval, Victor Schmidt, Alex Hern Â´andez-
GarcÄ±a, Santiago Miret, Fragkiskos D Malliaros, Yoshua
Bengio, and David Rolnick. Faenet: Frame averaging equiv-
ariant gnn for materials modeling. In ICML , pages 9013â€“
9033, 2023. 1, 2, 3
[9] Carlos Esteves, Christine Allen-Blanchette, Ameesh Maka-
dia, and Kostas Daniilidis. Learning so(3) equivariant repre-
sentations with spherical cnns. In ECCV , pages 52â€“68, 2018.
2
[10] Marc Finzi, Samuel Stanton, Pavel Izmailov, and An-
drew Gordon Wilson. Generalizing convolutional neural net-
works for equivariance to lie groups on arbitrary continuous
data. In ICML , pages 3165â€“3176, 2020. 1, 2
[11] Fabian Fuchs, Daniel Worrall, V olker Fischer, and Max
Welling. Se(3)-transformers: 3d roto-translation equivariant
attention networks. In NIPS , pages 1970â€“1981, 2020. 1, 2,
6, 7, 8
[12] Fabian B. Fuchs, Edward Wagstaff, Justas Dauparas, and In-
gmar Posner. Iterative se(3)-transformers. In Geometric Sci-
ence of Information , pages 585â€“595, 2021. 2
[13] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol
Vinyals, and George E. Dahl. Neural message passing for
quantum chemistry. In ICML , pages 1263â€“1272, 2017. 1, 6,
7, 8
[14] Jiaqi Han, Wenbing Huang, Tingyang Xu, and Yu Rong.
Equivariant graph hierarchy-based neural networks. In NIPS ,
2022. 1, 2, 5, 6, 7, 8
[15] Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun
Sun, and Junzhou Huang. Equivariant graph mechanics net-
works with constraints. In ICLR , 2022. 1, 2, 6, 7, 8
[16] Michael J Hutchinson, Charline Le Lan, Sheheryar Zaidi,
Emilien Dupont, Yee Whye Teh, and Hyunjik Kim. Lietrans-
former: Equivariant self-attention for lie groups. In ICML ,
pages 4533â€“4543, 2021. 1, 2[17] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL
Townshend, and Ron Dror. Learning from protein struc-
ture with geometric vector perceptrons. arXiv preprint
arXiv:2009.01411 , 2020. 1
[18] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL
Townshend, and Ron Dror. Learning from protein structure
with geometric vector perceptrons. In ICLR , 2020. 2
[19] S Â´ekou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang,
Yoshua Bengio, and Siamak Ravanbakhsh. Equivariance
with learned canonicalization functions. In ICML , pages
15546â€“15566, 2023. 1
[20] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max
Welling, and Richard Zemel. Neural relational inference for
interacting systems. In ICML , pages 2688â€“2697, 2018. 1
[21] Jonas K Â¨ohler, Leon Klein, and Frank No Â´e. Equivariant flows:
sampling configurations for multi-body systems with sym-
metric energies. arXiv preprint arXiv:1910.00753 , 2019. 6,
7, 8
[22] Maosen Li, Siheng Chen, Xu Chen, Ya Zhang, Yanfeng
Wang, and Qi Tian. Actional-structural graph convolutional
networks for skeleton-based action recognition. In CVPR ,
pages 3595â€“3603, 2019. 1
[23] Shitong Luo, Jiahan Li, Jiaqi Guan, Yufeng Su, Chaoran
Cheng, Jian Peng, and Jianzhu Ma. Equivariant point cloud
analysis via learning orientations for message passing. In
CVPR , pages 18932â€“18941, 2022. 2
[24] Diego Marcos, Michele V olpi, Nikos Komodakis, and Devis
Tuia. Rotation equivariant vector field networks. In ICCV ,
pages 5048â€“5057, 2017. 2
[25] Omri Puny, Matan Atzmon, Heli Ben-Hamu, Ishan Misra,
Aditya Grover, Edward J. Smith, and Yaron Lipman. Frame
averaging for invariant and equivariant network design. In
ICLR , 2022. 1, 2, 3, 4, 5, 6, 7, 8
[26] VÄ±ctor Garcia Satorras, Emiel Hoogeboom, and Max
Welling. E(n) equivariant graph neural networks. In ICML ,
pages 9323â€“9332, 2021. 1, 2, 5, 6, 7, 8
[27] Kristof Sch Â¨utt, Oliver Unke, and Michael Gastegger. Equiv-
ariant message passing for the prediction of tensorial prop-
erties and molecular spectra. In ICML , pages 9377â€“9388,
2021. 2
[28] Anthony Simeonov, Yilun Du, Andrea Tagliasacchi,
Joshua B. Tenenbaum, Alberto Rodriguez, Pulkit Agrawal,
and Vincent Sitzmann. Neural descriptor fields: Se(3)-
equivariant object representations for manipulation. In In-
ternational Conference on Robotics and Automation (ICRA) ,
pages 6394â€“6400, 2022. 1
[29] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann
Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field
networks: Rotation- and translation-equivariant neural net-
works for 3d point clouds. arXiv preprint arXiv:1802.08219 ,
2018. 1, 2, 6, 7, 8
[30] Fang Wu and Stan Z Li. Diffmd: a geometric diffusion model
for molecular dynamics simulations. In AAAI , pages 5321â€“
5329, 2023. 1
[31] Zelin Xiao, Hongxin Lin, Renjie Li, Lishuai Geng,
Hongyang Chao, and Shengyong Ding. Endowing deep 3d
5511
models with rotation invariance based on principal compo-
nent analysis. In IEEE International Conference on Multi-
media and Expo , pages 1â€“6, 2020. 3
[32] Chenxin Xu, Siheng Chen, Maosen Li, and Ya Zhang. In-
variant teacher and equivariant student for unsupervised 3d
human pose estimation. In AAAI , pages 3013â€“3021, 2021. 1
[33] Chenxin Xu, Robby T Tan, Yuhong Tan, Siheng Chen,
Yu Guang Wang, Xinchao Wang, and Yanfeng Wang. Eq-
motion: Equivariant multi-agent motion prediction with in-
variant interaction reasoning. In CVPR , pages 1410â€“1420,
2023. 1, 5, 6, 7, 8
[34] Ruixuan Yu, Xin Wei, Federico Tombari, and Jian Sun.
Deep positional and relational feature learning for rotation-
invariant point cloud analysis. In ECCV , pages 217â€“233,
2020. 3
[35] Zhiyuan Zhang, Binh-Son Hua, Wei Chen, Yibin Tian, and
Sai-Kit Yeung. Global context aware convolutions for 3d
point cloud understanding. In International Conference on
3D Vision (3DV) , pages 210â€“219, 2020. 3
[36] Chen Zhao, Jiaqi Yang, Xin Xiong, Angfan Zhu, Zhiguo
Cao, and Xin Li. Rotation invariant point cloud analysis:
Where local geometry meets global topology. Pattern Recog-
nition , 127:108626, 2022. 3
[37] Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao.
Oriented response networks. In CVPR , pages 519â€“528,
2017. 2
[38] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and Li
Hao. On the continuity of rotation representations in neural
networks. In CVPR , 2019. 4
5512
