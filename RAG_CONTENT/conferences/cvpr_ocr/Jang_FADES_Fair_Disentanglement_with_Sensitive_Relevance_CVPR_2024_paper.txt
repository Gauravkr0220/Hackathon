FADES: Fair Disentanglement with Sensitive Relevance
Taeuk Jang, Xiaoqian Wang*
Purdue University
465 Northwestern Ave, West Lafayette, IN 47907, USA
{jang141@,joywang }@purdue.edu
Abstract
Learning fair representation in deep learning is essential
to mitigate discriminatory outcomes and enhance trustwor-
thiness. However, previous research has been commonly es-
tablished on inappropriate assumptions prone to unrealis-
tic counterfactuals and performance degradation. Although
some proposed alternative approaches, such as employing
correlation-aware causal graphs or proxies for mutual in-
formation, these methods are less practical and not applica-
ble in general. In this work, we propose FAir DisEntangle-
ment with Sensitive relevance (FADES), a novel approach
that leverages conditional mutual information from the in-
formation theory perspective to address these challenges.
We employ sensitive relevant code to direct correlated in-
formation between target labels and sensitive attributes by
imposing conditional independence, allowing better sepa-
ration of the features of interest in the latent space. Utilizing
an intuitive disentangling approach, FADES consistently
achieves superior performance and fairness both quanti-
tatively and qualitatively with its straightforward struc-
ture. Specifically, the proposed method outperforms exist-
ing works in downstream classification and counterfactual
generations on various benchmarks.
1. Introduction
Deep generative models have made great accomplishments
in various applications including style transfer [54], video
generation [5], image translation [22], text-to-image gener-
ation [9], etc. Among them, variational autoencoder (V AE)
based models [21, 26, 38] significantly contributed to the
evolution of deep learning and generative models by offer-
ing a better understanding of the latent space and stable per-
formance. Although V AEs have been widely adopted due
to the compactness and robustness of the learned represen-
tation, there is plenty of room for improvement in various
*Corresponding author. This work was partially supported by NSF IIS
#1955890, IIS #2146091.aspects. One of the highlighted areas is fairness since pre-
venting discrimination is crucial to gaining trustworthiness
of deep learning models in practice.
While deep models achieve superior performance by ex-
ploiting high dimensional latent representation, the deep
representations are susceptible to absorbing spurious cor-
relations with sensitive information [29, 30, 41], which can
lead to potential fairness violations and biased outcomes.
To address the problem, a line of work was proposed to
learn fair representations. Some [29, 46] proposed to learn
latent representation that is invariant to sensitive informa-
tion, while others [6, 10] aimed to disentangle the latent
features into two subsets: target (non-sensitive) and sensi-
tive codes. This is often done by introducing regulariza-
tion terms for fair representation learning in addition to per-
formance objectives. Specifically, fair disentanglement ap-
proaches minimize mutual information between the subsets
[3] to separate the target label and sensitive information.
However, we argue that achieving such fair disentan-
glement is unattainable under data bias, which is one of
the prevalent causes of fairness violations. Many datasets
[11, 27] contain discriminatory labels that are influenced
by societal biases and stereotypes, which are unfavorable to
certain demographics [39]. This leads to an unwanted corre-
lation between the target and sensitive attributes in practice.
When this correlation is overlooked, the fairness and per-
formance goals of the disentanglement methods contradict
each other and cannot be achieved simultaneously. Specifi-
cally, the fairness goal aims at learning target and sensitive
codes to be independent, while the performance goal pur-
sues perfectly recovering the target and sensitive informa-
tion from the respective codes. However, when the target
label and sensitive attribute are inherently correlated, it is
impossible to achieve both goals at the same time.
Moreover, some features may contain both sensitive and
target information, making it difficult to protect sensitive
information while making accurate predictions in practice
[13]. This makes the objective of previous works, which
aimed to learn perfectly separable subsets, unattainable. For
instance, in the hair color estimation task from portraits
1
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
12067
[58], which is known to be gender-biased, the bald attribute
is related to both hair color andgender . Therefore, such
fair disentanglement can be achieved only under the naive
assumption: target and sensitive attributes are independent,
where no attribute contains information related to both tar-
get and sensitive information.
Some [25, 57] attempted to address this by introducing
complex causal models by considering more complicated
relations between features. However, these require domain
knowledge to understand the causality between the features
and build a comprehensive graph, which is demanding and
not always accessible. Without accurately knowing the
causal relationships among the attributes, the quality of the
generated counterfactuals is significantly impaired, e.g., a
girl with a mustache.
In this work, we propose a novel approach in disen-
tanglement learning for fairness to address the limitations
by introducing sensitive relevant code. Our method effec-
tively directs the information correlated to both target and
sensitive information to sensitive relevant code by ensuring
conditional independence without requiring domain knowl-
edge. To achieve this, we minimize conditional mutual in-
formation as it sets an upper bound for conditional inde-
pendence, which encourages learning genuinely fair disen-
tanglement, i.e.,independence between target and sensitive
codes. As a result, the proposed method provides fair rep-
resentation with improved control and interpretation while
outperforming existing works in both fairness and utility.
The contribution of the work can be summarized as fol-
lows:
1. We theoretically demonstrate that the fairness and per-
formance goals of previous works inherently contradict
under common data bias, resulting in unstable training
and degraded utility.
2. We propose a novel approach to address the contradic-
tion by introducing the sensitive relevant code, which
theoretically leads to optimal disentanglement of sensi-
tive and target codes.
3. We propose a framework to disentangle features in the
information theory perspective by leveraging conditional
mutual information. The proposed method has a sim-
ple structure but is effective and does not require domain
knowledge.
4. We empirically validate the effectiveness of the proposed
method on multiple benchmark datasets.
2. Related Work
2.1. Learning Fair Representation
The goal of learning fair representation is to make accu-
rate predictions on downstream tasks while filtering out the
influence of sensitive information. Simply removing sen-
sitive information, i.e., fairness through blindness, is notsufficient to achieve fairness as there exist related features
from which we can infer sensitive information. In clas-
sification, Madras et al. [36] proposed to constrain group
fairness metrics, e.g., equalized odds [18] and demographic
parity, as the adversarial objectives. Besides, the follow-up
method integrates the whole procedure of optimization in
a single neural network to improve stability of the adver-
sarial network [1]. Also, FairGAN [52] focuses on gener-
ating fair data aiming to fool a strong discriminator to rec-
ognize which sensitive group a generated image belongs to.
However, such discrimination-free representation requires
strong conditions and has degradation in performance in im-
age synthesis since sensitive information is also essential for
image generation.
2.2. Disentanglement Learning
Instead of learning sensitive-information-free representa-
tion, a line of works proposed to disentangle the represen-
tation into two sets of latent variables: target and sensi-
tive code. β-V AE [21] initiated disentangling semantic fea-
tures by encouraging the variational distribution to satisfy
stronger prior constraint. FactorV AE [24] proposed to min-
imize the total correlation for further decomposition and its
variant [32] further obfuscate sensitive information in the
latent space. FFV AE [6] applied the total correlation [3]
on disentangling sensitive code flexible to the dimension
of sensitive information. GV AE [10] also employed adver-
saries to ensure minimizing the leakage of unwanted infor-
mation in each latent code. ODV AE [47] proposed to learn
target and sensitive code to follow orthogonal priors while
FairDisCo [31] minimized distance covariance, serving as a
non-adversarial alternative to enforce independence.
However, we claim that strictly disentangling the orig-
inal space into two perfectly independent subsets of latent
codes is unachievable. The primary objective of represen-
tation learning is to recover sensitive and target informa-
tion from each latent code. However, existing fair meth-
ods mostly overlooked that the target and sensitive infor-
mation is often correlated in practice, which is the major
source of the data bias [39]. Therefore, if sensitive and tar-
get codes can perfectly recover sensitive and target infor-
mation, respectively, they inherently cannot be independent
in nature. For instance, facial attribute recognition task on
CelebA dataset [34] shows that attributes such as mustache
(sensitive relevant) is both related to gender (sensitive) and
attractiveness (target).
2.3. Counterfactual Fairness
The objective of counterfactual fairness [28] is to constrain
biased decisions when the protected attributes are perturbed
at the individual level, which is evaluated by the potential
outcomes of altering sensitive attributes of individuals while
keeping other features the same, i.e.,counterfactuals. The
2
12068
causal inference technique is utilized to generate the coun-
terfactual instances [4, 51, 57]. To comprehend the complex
connections between features, some studied the causal ef-
fect between features by building a graph-based model with
predefined intervention and individual variables [25]. How-
ever, without accurately knowing the causal relationships
among the attributes, i.e.,domain knowledge, the quality of
the generated counterfactuals can be significantly impaired
and negatively affect the utility. For instance, lacking do-
main knowledge can generate unrealistic data, such as a boy
attending a girl’s high school.
Unlike existing works, we introduce sensitive relevant
code to learn fair representation. We impose independence
between target and sensitive information conditioned on the
sensitive relevant code by minimizing conditional mutual
information (CMI), which results in the independence be-
tween sensitive and target codes. Moyer et al. [40] utilize
mutual information to learn sensitive-invariant representa-
tion. However, learning a representation that is independent
of sensitive information is too restrictive [47]. While a re-
cent work applied CMI in RL [12], to the best of our knowl-
edge, the proposed method is the first to learn a fair repre-
sentation that accounts for data bias by employing CMI.
3. Preliminaries
In this section, we discuss different approaches to achiev-
ing fair representation. Many works utilize variational in-
ference following V AE [26] that approximate inference by
maximizing evidence lower bound (ELBO):
LELBO(θ, ϕ) =Eqϕ(z|x)[logpθ(x|z)]−βD KL(qϕ(z|x)||p(z)),
where input xis fed to encoder f(x;ϕ)and decoder g(z;θ)
reconstructs the input xfrom the given latent posterior z.
The prior p(z)is usually set to isotropic Gaussian distri-
bution. With β > 1, the encoder is encouraged to have a
stronger agreement with the prior, which leads to factorized
disentanglement [21].
FactorV AE [24] imposes independence in latent dimen-
sions by minimizing total correlation for disentanglement:
LTC=KL 
q(z)||Y
jzj
≈Eqϕ(z)
logD(z)
1−D(z)
,(1)
where discriminator Dseeks whether the latent code is sam-
pled from aggregate posterior or a combination of in-batch
permutations of marginal distributions across each latent di-
mension jwhile the encoder tries to fool the discriminator.
Figure 1 depicts the graphical models of each approach for
fair representation learning.
3.1. Invariant Learning
Invariant learning aims to learn a unified latent variable z
that is invariant to sensitive attribute Aas in Figure 1a. The
(a) Invariant learning
 (b) Disentanglement learning
(c) Correlation-aware learning
 (d) Our proposed approach
Figure 1. Graphical models of fair representation learning meth-
ods. Dashed line indicates the connection may or may not exist,
depending on different model designs.
dataXand label Yare causal downstream of sensitive at-
tribute Aand unobserved variable z. The goal is to achieve
zindependent of Aso that the predicted Yis subsequently
invariant to sensitive information. Adversarial learning
[32, 35, 37] is often employed to fool a discriminator which
tries to predict sensitive information from z. While others
[15, 31, 44] employ a regularizer to minimize some diver-
gence D(e.g., KL divergence, MMD [16], distance covari-
ance) among the aggregate posteriors qϕ(z|A=ak).
3.2. Disentanglement Learning
This group of studies disentangles the latent variable into
the target and sensitive codes. Since the independence con-
dition between zandAis too restrictive, some works hy-
pothesize decomposing the latent codes into two sets would
promote no leakage of sensitive information from zXby
disentangling the sensitive information to zAas in Fig 1b.
Creager et al. [6] proposed to minimize the mutual in-
formation between zXandzAleveraging total correlation
[3, 24]. Following the previous works, a binary adversary
learns to distinguish aggregate posterior qϕ(zX,zA)from a
product of the marginals qϕ(zX)Q
jqϕ(z(j)
A)where super-
script jdenotes factorized subspace. This is empirically
approximated by randomly permuting sensitive latent code
zAwithin a batch.
Given the variables of interest T={Y, A}, GV AE [10]
incentivizes certain attribute t∈Tto be projected to spe-
3
12069
Figure 2. Illustration of the contradiction between the goals imposed by fairness loss Lfairand performance loss LCEof disentanglement
learning. Two goals conflict since target label Yand sensitive attribute Aare not independent by data bias (green-shaded region) in practice.
cific subspaces zt∈zand have no leakage of the informa-
tion in the remaining code zrst
t=z\zt. They employ two
sets of classifiers: {wt}and{ξt}, where {wt}conveys tar-
get features, while {ξt}is for preventing information leak-
age, respectively, for promoting disentanglement.
ODV AE [47] points out the potential convergence prob-
lem of the adversarial approaches [40, 42]. They relaxed
the independence condition by encouraging zXandzAto
follow orthogonal priors: p(zX) =N(µX, I)andp(zA) =
N(µA, I), where µ⊤
XµA= 0. However, it requires hard-
coded orthogonal priors and the orthogonality does not
guarantee independence.
3.3. Correlation-Aware Learning
It is suggested that employing a more sophisticated causal
graph may facilitate fair disentanglement, demonstrating
improved counterfactual generations. These models sep-
arate the latent variables based on their correlation with
sensitive information, as in Figure 1c. For instance, in
the case of gender asA, they define the descendant at-
tributes of AasXd={Makeup, Mustache ,···}, and at-
tributes irrelevant to AasXr={Age,···} where Xis
an image. To achieve the independence between zdand
zr, the total correlation is employed as in the previous
works [6, 25] and approximated with adversarial training as
LTC=DKL(q(a,zd,zr)||q(a,zd)q(zr)). However, build-
ing graphs with specific XdandXrrequires domain knowl-
edge, and evaluating the validity of the graph design can be
challenging and may not always be feasible. Moreover, not
considering the causal relationship between XdandXrcan
impede learning independent latent codes.
3.4. Contradicting Assumption in Existing Methods
We claim that performance and fairness objectives in the
previous works yield a contradiction, which can lead to
potentially unstable optimization and unsatisfactory perfor-
mance. We look into the disentanglement learning (Figure1b) as an example, but this can be easily generalized to Fig-
ure 1a and 1c as discussed in Appendix.
In Figure 2, we summarized the data flow of disentangle-
ment learning and the contradiction of two objectives. Fair
disentanglement learning mainly has two goals: 1) predic-
tiveness; 2) fairness. These methods project input Xinto
zXandzA, where the predictiveness objective is to recover
YandAfrom zXandzA, respectively. Commonly, cross-
entropy LCEis adopted, which is the lower bound of mutual
information. If we achieve sufficient predictiveness by min-
imizing cross-entropy loss, i.e.,ˆY≈Y(resp. ˆA≈A), it
naturally results in high mutual information between the la-
belY(resp. A) and latent code zX(resp. zA). On the other
hand, the fairness objective is to learn zXandzAthat are
independent by minimizing Lfair,i.e.,ZX⊥ZA.
However, we argue that it is not possible to attain both
objectives simultaneously due to one of the main causes
of fairness problems in algorithmic decision-making, which
is the undesired correlation between the target label Yand
sensitive information Aas highlighted in [39]. Under such
data bias, the predictiveness objective yields ˆY̸⊥ˆA. Ac-
cordingly, zXandzAcannot be independent, while the fair-
ness objective requires independence. As a result, the two
objectives contradict each other. We argue that overlooking
the inherent data bias and forcing two subspaces to be inde-
pendent may cause a poor fairness-accuracy trade-off [50]
and unstable optimization.
4. FADES: Fair Disentanglement with Sensi-
tive Relevance
Here, we propose a novel approach, FADES, to disentangle
the features to learn fair representation. Unlike the previous
works, we acknowledge that there exists an undesired cor-
relation between YandA, and the related information can-
not be explicitly partitioned. Instead, we propose to employ
sensitive relevant code zR, which directs the information
that overlaps between YandA. Here, zRis responsible for
4
12070
Figure 3. Illustration of the data flow of FADES. Since zRis re-
sponsible for information correlated with YandA, we can achieve
zY⊥zAalong with perfectly recovering YandA. The purple
lines connect the information in the observed space and their cor-
responding latent codes.
any correlated information between YandA. Thus, we can
learn independent zYandzAfree of unwanted correlation
while maintaining the predictiveness objective.
From the causal graph perspective as in Figure 1d, if Y
andAare independent conditioned on zR,i.e.,Y⊥A|zR,
then zRis the only common cause of YandA. As a result,
zYandzAhave zero mutual information, leading to inde-
pendence. We illustrate the data flow and corresponding
information projection in Figure 3. Then the fairness goal
to achieve independence between zYandzAby imposing
independence between ˆYandˆAconditioned on zR, as the
green arrows in Figure 3. zXis an optional feature that is
used to direct irrelevant features, such as the background of
an image, for more controlled counterfactual generations.
4.1. Conditional Independence
We can first think of satisfying the proposed disentangle-
ment by directly following the definition of conditional in-
dependence (CI):
pθ(ˆY ,ˆA|zR) =pθ(ˆY|zR)pθ(ˆA|zR).
Since ˆYis predicted with zRand zY, we need to find
the probability of prediction Yconditioned only on zR,
pθ(Y=y|zR)1. Empirically, we can compute the condi-
tional probability of k-th sample by marginalizing over all
zYwithin a batch similar to aggregate posterior as:
pθ(y|z(k)
R) =Ep(x)
Eqϕ(zY|x)
pθ(y|z(k)
R,zY)
≈1
|B|BX
i=1pθ(y|z(k)
R,z(i)
Y),(2)
1With some abuse of notation, we omit the random variable in proba-
bility by denoting pθ(Y=y|zR)aspθ(y|zR)from here.where z(i)
R,z(i)
Ydenotes the i-th sample in a mini-batch
B={z(1)
R,···} that are sampled uniformly from the dis-
tribution x(i)∼p(x) =1
|B|. To ensure independence,
p(zY|zR) =p(zY), we impose total correlation constraint
as in Eqn (1) by factorizing {zY,zR,zA}with in-batch per-
mutation. Similar to Eqn (2), we can compute
pθ(a|z(k)
R)≈1
|B||B|X
i=1pθ(a|z(k)
R,z(i)
A).
Then we may minimize CI by minimizing the divergence
D(pθ(ˆY ,ˆA|zR)||pθ(ˆY|zR)pθ(ˆA|zR)).However, this com-
putation is generally intractable.
4.2. Conditional Mutual Information
Instead, we propose to minimize conditional mutual infor-
mation (CMI), Iϕ(ˆA;ˆY|zR), to learn such representation.
In information theory, CMI for discrete random variables
X, Y, and continuous random variable Zis defined as:
I(X;Y|Z) =Z
ZDKL(P(X,Y )|Z||PX|Z⊗PY|Z)dPZ
=Z
ZX
y∈YX
x∈XPX,Y|Z(x, y|z) logPX,Y|Z(x, y|z)
PX|Z(x|z)PY|Z(y|z).
Proposition 4.1 below shows that minimizing CMI leads
to conditional independence. The proof is in Appendix.
Proposition 4.1. Conditional mutual information
I(X;Y|Z)is zero if and only if XandYare inde-
pendent conditioned on Z,i.e.,X⊥Y|Z.
Further, KL-divergence is lower bounded by total varia-
tion by Pinsker’s inequality [7] as
DKL(P(X,Y )|Z||PX|Z⊗PY|Z)
≥1
2X
X,YP(X,Y )|Z−PX|Z⊗PY|Z2
.
Therefore, reducing CMI, Iϕ(ˆA;ˆY|zR), is a valid objective
to learn a representation that satisfies conditional indepen-
dence ˆA⊥ˆY|zR. Empirically, CMI loss is computed by
utilizing the ground truth YandAfor training stability as:
LCMI=1
2 
Iϕ(ˆA;Y|ZR) +Iϕ(ˆY;A|ZR)
(3)
Then we can rewrite CMI with conditional entropy terms:
Iϕ(ˆY;A|ZR) =Hϕ(ˆY|zR)−Hϕ(ˆY|A,zR), (4)
where Hϕ(·|·)is conditional entropy. Then, we can empir-
ically compute the first conditional entropy term in Eqn (4)
by the following definition as
Hϕ(ˆY|zR) =1
|B||B|X
i=1X
y∈Y−pθ(y|z(i)
R) logpθ(y|z(i)
R),(5)
5
12071
where we can get pθ(y|zR)from Eqn (2).
To compute the second conditional entropy, we need to
compute pθ(ˆY|A,zR). This can be computed similarly for
z(k)
Rsampled from an instance x(k)∈Baas
pθ(y|z(k)
R, a) =Ep(x|A=a)
Eqϕ(zY|x)
pθ(y|z(k)
R,z(i)
Y)
≈1
|Ba||Ba|X
i=1pθ(y|z(k)
R,z(i)
Y),
where Badenotes a subset of the batch with A=a. Then
the conditional entropy can be computed as:
Hϕ(ˆY|A,zR) =−1
|B|X
a,ypθ(y|a,zR) logpθ(y|a,zR).(6)
Now we can compute LCMIin Eqn (3) by plugging Eqn (5)
and (6). Note that LCMIis not limited to binary YandA.
Note that when the whole information Y∪Ais cap-
tured in zR, we can also achieve conditional independence
or minimum CMI. To prevent this, we incorporate an infor-
mation bottleneck regularization term as
Lreg=− 
Hϕ(ˆY|zR) +Hϕ(ˆA|zR)
, (7)
which encourages zRto encapsulate only the correlated in-
formation Y∩Aby minimizing the confidence of prediction
only given zR.
4.3. Final Objective Function of FADES
For our purpose, the ELBO objective can be rewritten as
LELBO=Eqϕ(z|x,y,a)
logpθ(x|z) + log pθ(y|zY,zR)
+ log pθ(a|zA,zR)
−D(qϕ(z|x)||p(z)),
where z= [zX,zY,zR,zA]represents the full latent space
including the disentangled components. Then, the final ob-
jective function of FADES integrates LELBO with additional
terms to enforce fairness and latent space independence as:
LFADES =−L ELBO+λCMILCMI+λTCLTC+βLreg,(8)
where λandβare tunable hyperparameters.
5. Experiment
To provide a comprehensive comparison, we compare with
at least one method from the class of graphical models
presented in Figure 1, with the exception of correlation-
aware learning, as it necessitates additional annotated data
for constructing the causal graph, as discussed in Section
3.3. Accordingly, we mostly compare with recent fair dis-
entanglement learning methods. Specifically, FairFactor-
V AE [32] and FairDisCo [31] are invariant learning meth-
ods which encode latent representation independent of sen-
sitive attributes. FFV AE [6] is a disentanglement learningmethod minimizing the mutual information between the la-
tent subspaces. GV AE [10] is a disentanglement learning
method that aims at minimizing the leakage of unwanted
information to learn fair representation. ODV AE [47] is a
disentanglement learning method that learns fair represen-
tation without adversarial learning by enforcing the sub-
space to follow orthogonal priors. We set hyperparame-
ters of all methods with a grid search to achieve the best
EOD, especially for FADES, they are chosen in the range:
λCMI, λTC∈[0,100] andβ∈[0,1]. All methods adopt
ResNet-18 architecture [19] with 512 latent dimensions for
vision tasks and 3 layered MLP encoder-decoder with 32
latent dimensions for the tabular dataset.
5.1. Fair Downstream Classification
The goal of fair classification is to minimize fairness vio-
lations while preserving predictive performance. To com-
pare the methods, we conduct experiments on various fair-
ness datasets. For facial attribute classification, we adopt
CelebA [33] and UTK Face [56] datasets. Following pre-
vious works [49, 53, 55], the goal of CelebA is to predict
“Smiling” attribute of a portrait, while for UTKFace, it is to
classify whether a person in an image is over 35 years old,
with gender as the sensitive attribute. Dogs and Cats dataset
[8] is to distinguish a dog or cat given the color of its fur as
the sensitive attribute. In addition, we evaluate on Adult in-
come dataset [27], which is a popular fairness benchmark in
structured data. The goal is to predict if the income of an in-
stance exceeds $50K with gender as the sensitive attribute.
For evaluation, we employ a 3-layered and 2-layered
MLP classifier for vision and tabular datasets, respectively.
The input for the classifier is the target-related features of
pre-trained disentangled representation from each method.
For instance, we fed zYof FADES and zXof FFV AE [6],
which is one of the disentanglement learning methods dis-
cussed in Section 3.2. For invariant learning methods, the
whole latent space is utilized for downstream tasks. We
measure the fairness violations with metrics including de-
mographic parity (DP) [2] and equalized odds (EOD) [18].
For a fair comparison, we fixed the number of dimensions
in the latent codes for all methods if applicable. We con-
duct 5 runs of experiments for each method with different
random splits of the dataset if the split is not specified.
Table 1 summarizes the classification results on the
benchmarks. Throughout the experiments, FADES demon-
strated notable effectiveness in balancing classification ac-
curacy with fairness measures. In particular, FADES con-
sistently achieves the best accuracy while significantly im-
proving the fairness violation. While FFV AE and ODV AE
yield mixed results across the datasets, GV AE presents
comparable results with those of FADES. Nonetheless, the
proposed method consistently outperforms GV AE on all
datasets at the same accuracy level. This validates the ef-
6
12072
CelebA UTKFace Dogs and Cats Adult
Acc↑EOD↓ DP↓ Acc↑EOD↓ DP↓ Acc↑EOD↓ DP↓ Acc↑ EOD↓ DP↓
FADES 0.918 0.032 0.125 0.802 0.057 0.102 0.769 0.055 0.086 0.845 0.096 0.162
GV AE [10] 0.919 0.049 0.133 0.819 0.207 0.197 0.745 0.065 0.131 0.851 0.112 0.182
FFV AE [6] 0.891 0.075 0.071 0.767 0.271 0.206 0.727 0.067 0.111 0.802 0.063 0.092
ODV AE [47] 0.885 0.038 0.101 0.737 0.167 0.212 0.685 0.053 0.031 0.792 0.257 0.163
FairDisCo [31] 0.839 0.074 0.051 0.766 0.266 0.200 0.680 0.115 0.119 0.801 0.129 0.136
FairFactorV AE [32] 0.914 0.055 0.136 0.720 0.096 0.137 0.707 0.055 0.110 0.783 0.096 0.128
Table 1. Evaluation of downstream classification tasks on various benchmark datasets from learned representation.
Figure 4. Illustration of counterfactuals (2-5 rows) given source and reference images on CelebA dataset in the first row. From the second
row, we replace the latent subspaces zX,zY,zA, and [zA,zR]of the source images with those of the reference. Note that adding zRfor
counterfactuals (Row 5) naturally adapts sensitive relevant features without domain knowledge, e.g., mustache and makeup.
fectiveness of FADES in effectively disentangling corre-
lated features, which leads to superior latent representa-
tions. Specifically, it implies the proposed method effec-
tively isolated the features, preserving the quality informa-
tion related to the target label Ywhile simultaneously elim-
inating sensitive information A. Consequently, the learned
representations ensure target-specific performance with no
leakage of sensitive information with zY.
In addition, we evaluate the needs of information bot-
tleneck regularization Lregin the objective of FADES as
in Eqn (8). Figure 5 illustrates the results on 5 runs with
varying β∈[0,1]and assessing the accuracy and EOD
w.r.t. zY. Notably, we observe proportional improvement
in both accuracy and fairness violation as βdecreases until
β= 0.5. This suggests that zrmay initially attempt to pos-
sess the comprehensive information to minimize CMI in-
stead of distributing the information effectively. However,
adequately setting βmitigates the problem of information
bottleneck and enables zRto capture only correlated infor-
0.00.1 0.3 0.5 0.7 0.9 
 Coefficient
0.780.790.800.810.82Accuracy0.04
0.060.080.100.120.140.16Fairness ViolationFigure 5. Trend of fairness and accuracy by sweeping information
bottleneck weight βon UTKFace dataset.
mation. Conversely, the increase in fairness violation was
noted at β > 0.5. We hypothesize that this phenomenon
may be due to the overemphasis on maximizing the condi-
tional entropy, inadvertently forcing the encoder to project
7
12073
the correlated information to both zYandzA.
5.2. Fair Counterfactual Generation
We evaluate the counterfactual image generation task on
CelebA dataset [33]. We consider Smiling as the target la-
bel and Gender as the sensitive attribute. Figure 4 depicts
counterfactual generations by substituting portions of latent
subspace with the ones from the reference images. The
results showcase a smooth translation in the generations.
Notably, zRcaptures features semantically correlated with
bothYandAwithout domain knowledge. Specifically, re-
placing [zA,zR]shows a distinct difference with replacing
only zAby adding makeup (left half) and mustache (right
half) to the image in the last row.
In Figure 6, we compare the generated image by linear
interpolation of the sensitive code from a male with a mus-
tache to another female sample. Each row depicts generated
images when traversing sensitive code. The leftmost col-
umn is the reconstruction of a male with a mustache, and
the rightmost column is when the sensitive code is substi-
tuted by a female sample. We observe that FADES provides
better image quality than other methods when traversing the
sensitive code. Specifically, FFV AE and FairDisCo gener-
ate a female with a mustache, which is less probable and
unnatural. Additionally, GV AE and ODV AE had distortion
on irrelevant features such as background color or hairstyle,
which implies poor disentanglement. This is likely because
it has no consideration of the sensitive relevant feature. On
the other hand, FADES generates a much more natural fe-
male image while keeping other features unchanged. More
result on feature translation is in Appendix.
Figure 6. Reconstruction of interpolating sensitive code zAstart-
ing from a male with a mustache to a female.Acc (Digit) ↑Acc (Color) ↑
FADES (Ours) 93.46 95.31
GV AE [10] 85.98 88.20
FFV AE [6] 77.39 91.23
ODV AE [47] 77.45 83.26
FairDisCo [31] 87.21 99.31
FairFactorV AE [32] 87.33 90.18
Table 2. Digit and color recovery on unbiased C-MNIST.
5.3. Fair Image Reconstruction
We evaluate the fair image reconstruction on MNIST
dataset with color bias [23, 41]. In the training set, 10 dig-
its are correlated with 10 predefined colors with small ran-
dom perturbation for 70% of samples, and 30% has uniform
color assignment among the remaining colors. For the test
set, every digit has uniform color assignments. The dataset
is originally introduced to measure the color bias of a clas-
sifier predicting digits. In this experiment, we aim to inves-
tigate the ability of V AE-based models to disentangle the
digit information color bias in the latent space. We train
each method to reconstruct input images with color bias on
the training set, and evaluate the reconstruction on the test
set and expect to recover color and digit.
To quantitatively evaluate the reconstruction, we em-
ploy two pre-trained classifiers as oracles with accuracy
over 99.7% on color and digit prediction tasks, respectively,
trained on the unbiased dataset. We then measure the ac-
curacy of recovering the digit and color by comparing the
predicted digit and color of the reconstruction using the pre-
trained classifiers with ground truth. In Table 2, we sum-
marize the accuracy of digit and color recovery for com-
paring methods. We observe that most methods achieve
higher accuracy on color than digit. In contrast, FADES
renders both digit and color significantly better than others,
indicating that the color and digit information has been ef-
fectively disentangled and recovered. Note that FairDisCo
takes ground truth color label at decoder for reconstruction.
See Appendix for more experiments.
6. Conclusion
In this work, we propose a novel disentanglement approach
for addressing the limitations of existing fair methods under
the prevailing data bias scenario. We claim that the naive
assumption that the information is perfectly separable may
lead to unstable training and performance drop. Instead, we
direct the correlated information to a particular latent sub-
space by introducing sensitive relevant code. We theoreti-
cally demonstrate that we can effectively achieve fairness
while maintaining performance without adversarial train-
ing by minimizing proposed conditional mutual informa-
tion loss. Our experiments on fairness benchmarks validate
the effectiveness of our proposed method. We discuss some
limitations and future directions in the Appendix.
8
12074
References
[1] Tameem Adel, Isabel Valera, Zoubin Ghahramani, and
Adrian Weller. One-network adversarial fairness. In AAAI ,
pages 2412–2420, 2019. 2
[2] Solon Barocas and Andrew D Selbst. Big data’s disparate
impact. Calif. L. Rev. , 104:671, 2016. 6
[3] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K
Duvenaud. Isolating sources of disentanglement in varia-
tional autoencoders. Advances in neural information pro-
cessing systems , 31, 2018. 1, 2, 3
[4] Silvia Chiappa. Path-specific counterfactual fairness. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence ,
pages 7801–7808, 2019. 3
[5] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adver-
sarial video generation on complex datasets. arXiv preprint
arXiv:1907.06571 , 2019. 1
[6] Elliot Creager, David Madras, J ¨orn-Henrik Jacobsen,
Marissa Weis, Kevin Swersky, Toniann Pitassi, and Richard
Zemel. Flexibly fair representation learning by disentan-
glement. In International conference on machine learning ,
pages 1436–1445. PMLR, 2019. 1, 2, 3, 4, 6, 7, 8, 5
[7] Imre Csisz ´ar and J ´anos K ¨orner. Information theory: coding
theorems for discrete memoryless systems . Cambridge Uni-
versity Press, 2011. 5
[8] Will Cukierski. Dogs vs. cats, 2013. 6
[9] Ayushman Dash, John Cristian Borges Gamboa, Sheraz
Ahmed, Marcus Liwicki, and Muhammad Zeshan Afzal.
Tac-gan-text conditioned auxiliary classifier generative ad-
versarial network. arXiv preprint arXiv:1703.06412 , 2017.
1
[10] Zheng Ding, Yifan Xu, Weijian Xu, Gaurav Parmar, Yang
Yang, Max Welling, and Zhuowen Tu. Guided variational
autoencoder for disentanglement learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7920–7929, 2020. 1, 2, 3, 6, 7, 8, 4, 5
[11] Julia Dressel and Hany Farid. The accuracy, fairness, and
limits of predicting recidivism. Sci. Adv. , 4(1):eaao5580,
2018. 1
[12] Mhairi Dunion et al. Conditional mutual information for
disentangled representations in reinforcement learning. In
NeurIPS , 2023. 3
[13] Cynthia Dwork. Differential privacy: A survey of results. In
International conference on theory and applications of mod-
els of computation , pages 1–19. Springer, 2008. 1
[14] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Rein-
gold, and Richard Zemel. Fairness through awareness. In
Proceedings of the 3rd innovations in theoretical computer
science conference , pages 214–226, 2012. 4
[15] Vincent Grari, Sylvain Lamprier, and Marcin Detyniecki.
Adversarial learning for counterfactual fairness. Machine
Learning , pages 1–23, 2022. 3
[16] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bern-
hard Sch ¨olkopf, and Alexander Smola. A kernel two-sample
test. The Journal of Machine Learning Research , 13(1):723–
773, 2012. 3
[17] Laura Gustafson et al. Facet: Fairness in computer vision
evaluation benchmark. In ICCV , 2023. 5, 6[18] Moritz Hardt, Eric Price, and Nati Srebro. Equality of op-
portunity in supervised learning. In NIPS , pages 3315–3323,
2016. 2, 6
[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770–778, 2016. 6
[20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 4
[21] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,
Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and
Alexander Lerchner. beta-vae: Learning basic visual con-
cepts with a constrained variational framework. 2016. 1, 2,
3, 6
[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A
Efros. Image-to-image translation with conditional adver-
sarial networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1125–1134,
2017. 1
[23] Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim,
and Junmo Kim. Learning not to learn: Training deep neural
networks with biased data. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 9012–9020, 2019. 8
[24] Hyunjik Kim and Andriy Mnih. Disentangling by factoris-
ing. In International Conference on Machine Learning ,
pages 2649–2658. PMLR, 2018. 2, 3, 6
[25] Hyemi Kim, Seungjae Shin, JoonHo Jang, Kyungwoo Song,
Weonyoung Joo, Wanmo Kang, and Il-Chul Moon. Coun-
terfactual fairness with disentangled causal effect variational
autoencoder. In Proceedings of the AAAI Conference on Ar-
tificial Intelligence , pages 8128–8136, 2021. 2, 3, 4
[26] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 1, 3
[27] Ron Kohavi. Scaling up the accuracy of naive-bayes classi-
fiers: A decision-tree hybrid. In KDD , pages 202–207, 1996.
1, 6
[28] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo
Silva. Counterfactual fairness. Advances in neural informa-
tion processing systems , 30, 2017. 2
[29] Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien
Prost, Nithum Thain, Xuezhi Wang, and Ed Chi. Fair-
ness without demographics through adversarially reweighted
learning. Advances in neural information processing sys-
tems, 33:728–740, 2020. 1
[30] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghu-
nathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and
Chelsea Finn. Just train twice: Improving group robustness
without training group information. In International Confer-
ence on Machine Learning , pages 6781–6792. PMLR, 2021.
1
[31] Ji Liu, Zenan Li, Yuan Yao, Feng Xu, Xiaoxing Ma, Miao
Xu, and Hanghang Tong. Fair representation learning: An
alternative to mutual information. In SIGKDD , 2022. 2, 3,
6, 7, 8, 4, 5
9
12075
[32] Shaofan Liu, Shiliang Sun, and Jing Zhao. Fair transfer
learning with factor variational auto-encoder. Neural Pro-
cessing Letters , 55(3):2049–2061, 2023. 2, 3, 6, 7, 8, 4, 5
[33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
the IEEE international conference on computer vision , pages
3730–3738, 2015. 6, 8, 4
[34] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
Deep learning face attributes in the wild. In Proceedings of
International Conference on Computer Vision (ICCV) , 2015.
2
[35] Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag,
Richard Zemel, and Max Welling. Causal effect inference
with deep latent-variable models. Advances in neural infor-
mation processing systems , 30, 2017. 3
[36] David Madras, Elliot Creager, Toniann Pitassi, and Richard
Zemel. Learning adversarially fair and transferable represen-
tations. arXiv preprint arXiv:1802.06309 , 2018. 2
[37] David Madras, Elliot Creager, Toniann Pitassi, and Richard
Zemel. Fairness through causal awareness: Learning causal
latent-variable models for biased data. In Proceedings of
the conference on fairness, accountability, and transparency ,
pages 349–358, 2019. 3
[38] Emile Mathieu, Tom Rainforth, Nana Siddharth, and
Yee Whye Teh. Disentangling disentanglement in varia-
tional autoencoders. In International Conference on Ma-
chine Learning , pages 4402–4412. PMLR, 2019. 1
[39] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina
Lerman, and Aram Galstyan. A survey on bias and fairness
in machine learning. ACM Computing Surveys (CSUR) , 54
(6):1–35, 2021. 1, 2, 4
[40] Daniel Moyer, Shuyang Gao, Rob Brekelmans, Aram Gal-
styan, and Greg Ver Steeg. Invariant representations without
adversarial training. Advances in Neural Information Pro-
cessing Systems , 31, 2018. 3, 4
[41] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and
Jinwoo Shin. Learning from failure: De-biasing classifier
from biased classifier. Advances in Neural Information Pro-
cessing Systems , 33:20673–20684, 2020. 1, 8, 5
[42] Changdae Oh, Heeji Won, Junhyuk So, Taero Kim, Yewon
Kim, Hosik Choi, and Kyungwoo Song. Learning fair rep-
resentation via distributional contrastive disentanglement.
InProceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining , pages 1295–1305,
2022. 4
[43] Or Patashnik et al. Styleclip: Text-driven manipulation of
stylegan imagery. In ICCV , 2021. 5
[44] Stephen R Pfohl, Tony Duan, Daisy Yi Ding, and Nigam H
Shah. Counterfactual reasoning for fair clinical risk predic-
tion. In Machine Learning for Healthcare Conference , pages
325–358. PMLR, 2019. 3
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748–8763. PMLR, 2021. 5, 6[46] Proteek Chandan Roy and Vishnu Naresh Boddeti. Mitigat-
ing information leakage in image representations: A max-
imum entropy approach. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 2586–2594, 2019. 1
[47] Mhd Hasan Sarhan, Nassir Navab, Abouzar Eslami, and
Shadi Albarqouni. Fairness by learning orthogonal disentan-
gled representations. In European Conference on Computer
Vision , pages 746–761. Springer, 2020. 2, 3, 4, 6, 7, 8, 5
[48] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. Journal of machine learning research , 9
(11), 2008. 5
[49] Zhibo Wang, Xiaowei Dong, Henry Xue, Zhifei Zhang,
Weifeng Chiu, Tao Wei, and Kui Ren. Fairness-aware ad-
versarial perturbation towards bias mitigation for deployed
deep models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 10379–
10388, 2022. 6
[50] Michael Wick, Jean-Baptiste Tristan, et al. Unlocking fair-
ness: a trade-off revisited. Advances in neural information
processing systems , 32, 2019. 4
[51] Yongkai Wu, Lu Zhang, and Xintao Wu. Counterfactual fair-
ness: Unidentification, bound and algorithm. In Proceedings
of the Twenty-Eighth International Joint Conference on Arti-
ficial Intelligence , 2019. 3
[52] Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu.
Fairgan: Fairness-aware generative adversarial networks.
In2018 IEEE International Conference on Big Data (Big
Data) , pages 570–575. IEEE, 2018. 2
[53] Tian Xu, Jennifer White, Sinan Kalkan, and Hatice Gunes.
Investigating bias and fairness in facial expression recogni-
tion. In Computer Vision–ECCV 2020 Workshops: Glasgow,
UK, August 23–28, 2020, Proceedings, Part VI 16 , pages
506–523. Springer, 2020. 6
[54] Zheng Xu, Michael Wilber, Chen Fang, Aaron Hertzmann,
and Hailin Jin. Learning from multi-domain artistic images
for arbitrary style transfer. arXiv preprint arXiv:1805.09987 ,
2018. 1
[55] Huimin Zeng, Zhenrui Yue, Lanyu Shang, Yang Zhang, and
Dong Wang. Boosting demographic fairness of face attribute
classifiers via latent adversarial representations. In 2022
IEEE International Conference on Big Data (Big Data) ,
pages 1588–1593. IEEE, 2022. 6
[56] Zhifei Zhang, Yang Song, and Hairong Qi. Age progres-
sion/regression by conditional adversarial autoencoder. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5810–5818, 2017. 6
[57] Huaisheng Zhu and Suhang Wang. Learning fair models
without sensitive attributes: A generative approach. arXiv
preprint arXiv:2203.16413 , 2022. 2, 3
[58] Wei Zhu, Haitian Zheng, Haofu Liao, Weijian Li, and Jiebo
Luo. Learning bias-invariant representation by cross-sample
mutual information minimization. In ICCV , 2021. 2
10
12076
