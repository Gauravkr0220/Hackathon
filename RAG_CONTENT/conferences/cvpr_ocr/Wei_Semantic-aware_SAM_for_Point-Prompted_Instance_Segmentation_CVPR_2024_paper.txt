Semantic-aware SAM for Point-Prompted Instance Segmentation
Zhaoyang Wei1*, Pengfei Chen1‚àó, Xuehui Yu1‚àó, Guorong Li1,
Jianbin Jiao1, Zhenjun Han1‚Ä†
1University of Chinese Academy of Sciences(UCAS)
Abstract
Single-point annotation in visual tasks, with the goal of min-
imizing labelling costs, is becoming increasingly prominent
in research. Recently, visual foundation models, such as
Segment Anything (SAM), have gained widespread usage
due to their robust zero-shot capabilities and exceptional
annotation performance. However, SAM‚Äôs class-agnostic
output and high confidence in local segmentation intro-
duce semantic ambiguity , posing a challenge for precise
category-specific segmentation. In this paper, we introduce
a cost-effective category-specific segmenter using SAM. To
tackle this challenge, we have devised a Semantic-Aware In-
stance Segmentation Network (SAPNet) that integrates Mul-
tiple Instance Learning (MIL) with matching capability and
SAM with point prompts. SAPNet strategically selects the
most representative mask proposals generated by SAM to
supervise segmentation, with a specific focus on object cat-
egory information. Moreover, we introduce the Point Dis-
tance Guidance and Box Mining Strategy to mitigate inher-
ent challenges: group andlocal issues in weakly supervised
segmentation. These strategies serve to further enhance the
overall segmentation performance. The experimental re-
sults on Pascal VOC and COCO demonstrate the promis-
ing performance of our proposed SAPNet, emphasizing its
semantic matching capabilities and its potential to advance
point-prompted instance segmentation. The code is avail-
able at https://github.com/zhaoyangwei123/SAPNet .
1. Introduction
Instance segmentation seeks to discern pixel-level labels for
both instances of interest and their semantic content in im-
ages, a crucial function in domains like autonomous driv-
ing, image editing, and human-computer interaction. De-
spite impressive results demonstrated by various studies
[5, 11, 16, 40‚Äì42] , the majority of these high-performing
methods are trained in a fully supervised manner and heav-
ily dependent on detailed pixel-level mask annotations,
*Equal contribution.
‚Ä†Corresponding authors. (hanzhj@ucas.ac.cn)
Figure 1. Three Challenges Brought by SAM and single-MIL.
Orange dash box illustrates that semantic ambiguity in SAM-
generated masks, where it erroneously assigns higher scores to
non-object categories like clothes, despite the person being our de-
sired target. Green dash box depicts a comparison between mask
proposals using single-MIL and SAPNet. It illustrates two primary
challenges: ‚Äògroup‚Äô , where segmentation encounters difficulties
in isolating individual targets among adjacent objects of the same
category, and ‚Äòlocal‚Äô , where MIL favors foreground-dominant re-
gions, resulting in overlooked local details.
thereby incurring significant labeling costs. To address this
challenge, researchers are increasingly focusing on weakly
supervised instance segmentation, leveraging cost-effective
supervision methods, such as bounding boxes [23, 27, 39],
points [14, 28], and image-level labels [21, 45].
Recently, visual foundation models, such as Segment
Anything (SAM)[22], have been widely employed by re-
searchers for their exceptional generalization capabilities
and impressive annotation performance. Numerous stud-
ies based on SAM, such as [20, 44] have emerged, building
upon the foundations of SAM to further enhance its gener-
alization capabilities and efficiency. However, these efforts
have predominantly focused on improving the annotation
performance of SAM. One limitation arises from SAM‚Äôs
lack of classification ability, resulting in class-agnostic seg-
mentation results that fail to accurately segment specific cat-
egories as desired.
To tackle the inherent semantic ambiguity in SAM and
achieve specific-category segmentation, we propose inte-
grating weak annotations with SAM, employing point an-
notations as prompts to imbue semantic information into
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
3585
SAM‚Äôs outputs. A straightforward approach involves lever-
aging SAM‚Äôs intrinsic scoring mechanism, selecting the
top-scoring mask as the corresponding label for each cat-
egory. However, when annotating object points are fed into
the SAM, its category-agnostic characteristic tends to as-
sign higher scores to parts of the object, resulting in gener-
ated mask annotations that fail to encompass the object as
a whole. In Fig. 1 orange dashed box, we aim to obtain the
‚Äòperson‚Äô mask annotation, but SAM predicts the proposals
of ‚Äòclothes‚Äô, ‚Äòclothes+trousers‚Äô and ‚Äôperson‚Äô. Relying solely
on the score SAM provides is insufficient, as the highest
score corresponds to ‚Äòclothes‚Äô (col-2), which does not meet
our specific needs.
To address this challenge, we have proposed SAPNet,
a semantically-aware instance segmentation network de-
signed for high-quality, end-to-end segmentation. In this
study, we design a proposal selection module (PSM) using
the Multiple Instance Learning (MIL) paradigm to choose
proposals that align closely with the specified semantic la-
bel. However, the MIL-based method relies on the classi-
fication score, often leading to group and local predictions
[4, 21, 24]. In Fig. 1 green dashed box, the group issue is
evident, where two objects of the same category are often
both included when they are in close proximity. It also il-
lustrates the local issue, where the MIL classifier frequently
predicts the most discriminative region instead of the entire
object. To overcome these limitations, we have introduced
Point Distance Guidance (PDG) and Box Mining Strategies
(BMS). Specifically, we penalize the selection results by
calculating the Euclidean distances between the annotated
points of identical categories enclosed within the propos-
als. Additionally, for more localized proposals, we filter out
higher-quality proposals from their corresponding bags and
dynamically merge them in scale. By fully exploiting the
positional clues to prevent local and group prediction, we
aim to select the proposal that most effectively represents
the object category in refinement stage. The primary contri-
butions of this work can be outlined as follows:
1) We introduce SAPNet, an end-to-end semantic-aware
instance segmentation network based on point prompts.
SAPNet combines the visual foundation model SAM with
semantic information to address its inherent semantic ambi-
guity, facilitating the generation of semantically-aware pro-
posal masks.
2) We incorporate Point Distance Guidance (PDG) and
Box Mining Strategies (BMS) to prevent local and group
predictions induced by MIL-based classifiers in both the
proposal selection and refinement stages.
3) SAPNet achieves state-of-the-art performance in
Point-Prompted Instance Segmentation (PPIS), signifi-
cantly bridging the gap between point-prompted and
fully-supervised segmentation methods on two challenging
benchmarks (COCO and VOC2012).2. Related Work
Weakly-Supervised Instance Segmentation (WSIS) of-
fers a practical approach for accurate object masks us-
ing minimal supervision. It spans a range of annotations,
from image labels to bounding boxes. Research has fo-
cused on narrowing the performance gap between weakly
and fully-supervised methods, primarily through box-level
[18, 25, 39] and image-level annotations [1, 21]. Box-
based methods have explored structural constraints to guide
the segmentation, as seen in BBTP [18], BoxInst [39],
and Box2Mask [29], and applied structural constraints to
drive segmentation, treating it as a multiple-instance learn-
ing task or enforcing color consistency based on CondInst
[40]. These approaches, while innovative, can complicate
training and sometimes neglect the object‚Äôs overall shape
due to their focus on local features and proposal generation,
like MCG [2]. Conversely, the proposal-free methods, like
IRN [1], rely on class relationships for mask production but
can falter in accurately separating instances. To preserve
object integrity, recent methods such as Discobox [23] and
BESTIE [21] integrate advanced semantic insights into in-
stance segmentation using pairwise losses or saliency cues
[30, 39, 42]. However, semantic drift remains an issue, with
mislabeling or missed instances resulting in inferior pseudo
labels [3] compromising segmentation quality.
Pointly-Supervised Detection and Segmentation (PSDS)
cleverly balances minimal annotation costs with satisfac-
tory localization accuracy. By introducing point annota-
tions, WISE-Net [24] , P2BNet [9]and BESTIE [21] im-
prove upon weakly supervised methods that suffer from
vague localizations. That only slightly increases the costs
(by about 10%) and is almost as quick as the image-level an-
notation, but that is far speedier than more detailed bound-
ing box or mask annotations. Such precision allows for
tackling semantic bias, as seen in methods like PointRend
[12], which utilize multiple points for improved accuracy,
despite requiring additional bounding box supervision. Re-
cent advancements in point-supervised instance segmenta-
tion, employed by WISE-Net and Point2Mask [28], show
that even single-point annotations can yield precise mask
proposals. WISE-Net skillfully localizes objects and selects
masks, while BESTIE enhances accuracy using instance
cues and self-correction to reduce semantic drift. Attnshift
[31] advances this by extending single points to reconstruct
entire objects. Apart from their complexity, these methods
have yet to fully demonstrate their effectiveness, indicat-
ing ongoing challenges in harnessing single-point annota-
tions for image segmentation and presenting clear avenues
for further research.
Prompting and Foundation Models. Prompt-based learn-
ing enables pretrained foundation models to adapt to vari-
ous tasks using well-crafted prompts. SAM [22], a promi-
nent example in computer vision, exemplifies robust zero-
3586
shot generalization and interactive segmentation across
multiple applications. Additionally, SAM-based models
like Fast-SAM [44] increases speed, HQ-SAM [20] im-
proves segmentation quality, and Semantic-SAM [26] opti-
mizes performance by training on diverse data granularities.
Foundational models, pre-trained on large datasets, help
improve generalization in downstream tasks, especially in
data-scarce scenarios. Basing on SAM, Rsprompter [8]
utilizes SAM-derived pseudo labels for improved remote
sensing segmentation, meanwhile, adaptations for medical
imaging and video tracking are explored in A-SAM [17]
and Tracking Anything [43]. Further, [10] and [19] have in-
tegrated SAM with Weakly Supervised Semantic Segmen-
tation networks to refine pseudo labels. Our research builds
upon these innovations, transforming point annotations into
mask proposals in instance segmentation to significantly en-
hancing performance.
3. Methodology
3.1. Overview
The overview of our method is illustrated in Fig. 2, SAPNet
comprises of two branches: one dedicated to the selection
and refinement of mask proposals to generate pseudo-labels
and the other employing solov2 head [42] for instance seg-
mentation supervised by the generated pseudo labels. The
central focus of our approach is the pseudo-label genera-
tion branch, exclusively utilized during the training phase,
which includes the PSM, PNPG, and PRM modules. Fol-
lowing the initial proposal inputs, the PSM employs multi-
instance learning and a point-distance penalty to identify
semantically rich proposals. Subsequently, coupled with
selected proposals from the PSM stage, the PNPG gener-
ates quality positive-negative bags to mitigate background
and locality issues, emphasizing the primary regions of in-
terest. Then, the PRM processes these bags, which se-
lects refined proposals from positive bags to improve fi-
nal box quality. Ultimately, the mask mappings derived
from these box proposals are utilized to guide the segmenta-
tion branch. This guarantees the acquisition of high-quality
category-specified mask proposals to supervise the segmen-
tation branch.
3.2. Proposal Selection Module
SAM‚Äôs limited semantic discernment causes category-
agnostic labeling, leading to inconsistent proposal quality
for the same objects. Employing these proposals directly
for segmentation supervision could introduce noise and im-
pair performance. Our goal is to design a category-specific
segmenter, which needs to select the most semantically rep-
resentative proposals for robust supervision.
Motivated by the insights from WSDDN [4] and P2BNet
[9], our proposal selection module employs multi-instancelearning and leverages labeling information to prioritize
high-confidence proposals for segmentation. In the training
phase, we leverage SAM[22] solely to generate category-
agnostic proposals. To avoid excessive memory use and
slow training, we convert them into box proposals using the
minimum bounding rectangle, and combine with depth fea-
tures F‚ààRH√óW√óDfrom the image I‚ààRH√óW, serve
as input to the PSM. Utilizing our designed MIL loss, PSM
precisely predicts each proposal‚Äôs class and instance details.
It selects the highest-scoring proposal as the semantically
richest bounding box for each object, effectively choosing
higher quality mask proposals.
Given an image Iwith Npoint annotations Yn=
{(pi, ci)}N
i=1, where piis the coordinate of the anno-
tated point and ciis the class index. We transform each
class-informative point piintoMsemantic mask propos-
als, which is further converted to a semantic proposal bag
Bi‚ààRM√ó4. As illustrated in Fig. 2, after passing through
a 7x7 RoIAlign layer and two fully-connected layers, fea-
tures Fi‚ààRM√óH√óW√óDare extracted from proposal bag
Bi. Like in [4] and [37], the features Fserve as input for
the classification branch and instance branch, using fully-
connected layer fandf‚Ä≤to generate Wcls‚ààRM√óKand
Wins‚ààRM√óK. A softmax activation function over K
class and Minstance dimensions yields the classification
scores Scls‚ààRM√óKand instance scores Sins‚ààRM√óK.
Wcls=f(F); [Scls]mk=e[Wcls]mkXK
k=1e[Wcls]mk.
Wins=f‚Ä≤(F); [Sins]mk=e[Wins]mkXM
m=1e[Wins]mk.(1)
where [¬∑]mkis the value in row mand column kof matrix.
Point Distance Guidance. SAM and MIL struggle with
distinguishing adjacent objects of the same category, of-
ten merging two separate objects into one and giving high
score. To combat this, we incorporate instance-level an-
notated point information and introduce a spatially aware
selection with a point-distance penalty mechanism.
To address the challenge of overlapping objects and
thereby enhance model optimization, we propose a strategy
specifically aimed at penalizing instances of object overlap.
For each m-th proposal within the set Bi, we define tmj= 1
to denote an overlap with any proposal in another identical
class bag Bj; otherwise, tmj= 0. The penalty imposed in-
creases in proportion to the distance of the overlapping ob-
jects from the proposal in question. This penalty, Wdis, is
represented using the Euclidean distance between the anno-
tated points of the overlapping proposals. Subsequently, the
reciprocal of Wdisis then passed through a sigmoid func-
tion to compute the distance score Sdisfor the proposal.
[Wdis]im=NX
j=1,jÃ∏=i‚à•pi‚àípj‚à• ‚àótmj.
[Sdis]im= (1/e‚àí(1/[Wdis]im))d.(2)
3587
Figure 2. The framework of SAPNet comprises two components: one for generating mask proposals and another for their utilization in
instance segmentation. The process starts with generating category-agnostic mask proposals using point prompts within a visual foundation
model. That is followed by an initial proposal selection via MIL combined with PDG. Next, the PRM refines these proposals using positive
and negative samples from PNPG, capturing global object semantics. Finally, augmented with the multi-mask proposal supervision, the
segmentation branch aims to improve segmentation quality.
Figure 3. The mechanism of the proposal selection module.
where [¬∑]imis the value at the row iand column min the
matrix, and dis the exponential factor.
PSM Loss. The final score Sof each proposal is ob-
tained by computing the Hadamard product of the classi-
fication score, the instance score, and the distance score,
while the score bSfor each proposal bag Biis obtained by
summing the scores of the proposals in Bi. The MILloss
of the PSM is constructed using the form of binary cross-
entropy, and it is defined as follows:
S=Scls‚äôSins‚äôSdis‚ààRM√óK;bS=MX
m=1[S]m‚ààRK.
Lpsm=CE(bS,c) =‚àí1
NNX
n=1KX
k=1cklog(bSk) + (1 ‚àíck) log(1 ‚àíbSk)
(3)where c‚àà {0,1}Kis the one-hot category‚Äôs label.
Utilizing the MILloss, the PSM module skillfully iden-
tifies each proposal‚Äôs category and instance. The module
selects the proposal with the highest score, marked as S,
for a specific object and identifies a bounding box enriched
with semantic information.
3.3. Positive and Negative Proposals Generator
To further refine the selection of more accurate bounding
boxes, we employ PNPG based on boxpsm selected viaPSM. That consists of two components: PPG and NPG. The
PPG is designed to generate a richer set of positive samples,
enhancing bag‚Äôs quality. Concurrently, the NPG is respon-
sible for generating negative samples, which are crucial for
assisting model training. These negative samples, includ-
ing background samples for all objects and part samples for
each, are crucial in resolving part issues and ensuring high-
quality bounding box selection. The positive sample set B+
produced by PPG and the negative sample set Ugenerated
by NPG are utilized for training the subsequent PRM.
Positive Proposals Generator (PPG). Within this
phase, to implement adaptive sampling for the identified
bounding box, we capitalize on the boxpsm derived from
the PSM stage, coupled with the point distance penalty
scoreSdisattributed to each proposal. To further elaborate,
for each boxpsm (denoted as b‚àó
x,b‚àó
y,b‚àó
w,b‚àó
h) isolated dur-
ing the PSM phase, its dimensions are meticulously recali-
brated leveraging a scale factor vand its associated within-
category inclusion score Sdisto generate an augmented set
of positive proposals (bx, by, bw, bh). The formulation is
defined as follows:
bw= (1¬±v/Sdis)¬∑b‚àó
w, b h= (1¬±v/Sdis)¬∑b‚àó
h,
bx=b‚àó
x¬±(bw‚àíb‚àó
w)/2, b y=b‚àó
y¬±(bh‚àíb‚àó
h)/2.(4)
These newly cultivated positive proposals are carefully in-
tegrated into the existing set Bito enhance the positive in-
stances‚Äô pool. Such enhancements are pivotal in optimizing
the training of the forthcoming PRM.
Negative Proposals Generator(NPG). MIL-based se-
lection within a single positive bag may overemphasize the
background noise, leading to inadequate focus on the ob-
ject. To solve this, we create a negative bag from the back-
3588
Algorithm 1 Positive and Negative Proposals Generation
Input: Tneg1,Tneg2,boxpsmfrom PSM stage, image I, positive
bagsB+.
Output: Positive proposal bags B+,Negative proposal set U.
1:// Step1: positive proposals sampling
2:fori‚ààN,Nis the number of object in image Ido
3: B+
i‚ÜêBi,Bi‚ààB;
4: B+
i=B+
iSPPG (boxi
psm);
5:end for
6:// Step2: background negative proposals sampling
7:U ‚Üê {} ;
8:proposals ‚Üêrandom sampling (I)for each image I;
9:iou=IOU (proposals, B i)for each Bi‚ààB;
10:ifiou < T neg1then
11:U=USproposals ;
12:end if
13:// Step3: part negative proposals sampling
14:fori‚ààN,Nis the number of object in image Ido
15: proposals ‚Üêpart negsampling (boxi
psm);
16: iou=IOU (proposals, boxi
psm);
17: ifiou < T neg2then
18: U=USproposals ;
19: end if
20:end for
ground proposals post-positive bag training, which helps
MIL maximize the attention towards the object.
Considering the image dimensions, we randomly sample
proposals according to each image‚Äôs width and height, for
negative instance sampling. We assess the Intersection over
Union (IoU) between these negatives and the positive sets,
filtering out those below a threshold Tneg1.
Additionally, to rectify MIL localization errors, we en-
force the sampling of smaller proposals with an IoU under
a second threshold, Tneg2, from inside boxpsmbased on its
width and height, that is scored highest in PSM, as nega-
tive examples. These negative instances, partially capturing
the object, drive the model to select high-quality bounding
boxes that encompass the entire object. The PNPG is sys-
tematically elaborated upon in Algorithm1.
3.4. Proposals Refinement Module
In the PSM phase, we employ MIL to select high-quality
proposals from bag B+. However, as shown in Fig. 2, the
boxpsm outcomes derived solely from a single-stage MIL
are suboptimal and localized. Inspired by PCL [38], we
consider refining the proposals in a second phase. How-
ever, in contrast to most WSOD methods which choose to
continue refining using classification information in subse-
quent stages, we have established high-quality positive and
negative bags, and further combined both classification and
instance branches to introduce the PRM module to refine the
proposals, aiming to obtain a high-quality bounding box.
The PRM module, extending beyond the scope of PSM,focuses on both selection and refinement. It combines pos-
itive instances from the PPG with the initial set, forming
an enriched B+. Simultaneously, it incorporates the neg-
ative instance set Ufrom NPG, providing a comprehensive
foundation for PRM. This integration leads to a restructured
MIL loss in PRM, replacing the conventional CELoss with
Focal Loss for positive instances. The modified positive
loss function is as follows:
Lpos=1
NNX
i=1D
cT
i,bSiE
¬∑FL(bS‚àó
i,ci). (5)
where FLis the focal loss [32], bS‚àó
iandbSirepresent the bag
score predicted by PRM and PSM, respectively.D
cT
i,bSiE
represents the inner product of the two vectors, meaning the
predicted bag score of the ground-truth category.
Enhancing background suppression, we use negative
proposals and introduce a dedicated loss for these instances.
Notably, these negative instances pass only through the clas-
sification branch for instance score computation, with their
scores derived exclusively from classification. The specific
formulation of this loss function is detailed below:
Œ≤=1
NNX
i=1D
cT
i,bSiE
, (6)
Lneg=‚àí1
|U|X
UKX
k=1Œ≤¬∑([Scls
neg]k)2log(1‚àí[Scls
neg]k).
(7)
The PRM loss consists of the MIL loss Lposfor positive
bags and negative loss Lnegfor negative samples, i.e.,
Lprm=Œ±Lpos+ (1‚àíŒ±)Lneg, (8)
where Œ±= 0.25by default.
Box Mining Strategy. MIL‚Äôs preference for segments
with more foreground presence and SAM‚Äôs tendency to cap-
ture only parts of an object often bring to final bounding
boxes, boxprm, the ‚Äòlocal‚Äô issue of MIL inadequately cov-
ers the instances. To improve the bounding box quality,
we introduce a box mining strategy that adaptively expands
boxselect from proposal selection in PRM, by merging it
with the original proposals filter, aiming to address MIL‚Äôs
localization challenges.
The Box Mining Strategy (BMS) consists of two primary
components: (i) We select the top kproposals from the
positive proposal bag B+, to create a set G. We evaluate
the proposals in Gagainst boxselect based on IoU and size,
using a threshold Tmin1. Proposals larger than boxselect
and with an IoU above Tmin1undergo dynamic expansion
through IoU consideration, which allows for the adaptive
integration with boxselect . That mitigates the ‚Äôlocal‚Äô issue
and maintains the bounding box‚Äôs consistentcy to the ob-
ject‚Äôs true boundaries. (ii) Frequently, issues related to lo-
3589
cality can lead to an exceedingly low IoU between propos-
als and boxselect . Nonetheless, the ground truth box can
fully encompass the boxpart. Therefore, when component
(i) conditions are unmet, if a proposal can entirely encap-
sulate boxselect , we reset the threshold Tmin2. Proposals
surpassing this threshold adaptively merge with boxselect to
generate the final boxprm,used to yield Mask prm. These
two components collectively form our BMS strategy. A de-
tailed procedure of this approach will be delineated in Al-
gorithm2 of the supplementary materials.
Loss Function. After acquiring the final supervision
masks, Mask prmand the filtered Mask samin Multi-mask
Proposals Supervision(MPS) in Sec. 7 of supplementary,
we use them together to guide the dynamic segmentation
branch. To comprehensively train SAPNet, we integrate
the loss functions from the PSM and PRM, culminating in
the formulation of the total loss for our model, denoted as
Ltotal. The aggregate loss function, Ltotalcan be articulated
as:
Ltotal =Lmask +Lcls+Œª¬∑ Lpsm +Lprm (9)
where, LDice is the Dice Loss [35], Lclsis the Focal
Loss[32], and Œªis set as 0.25.
4. Experiment
4.1. Experimental Settings
Datasets. We use the publicly available MS COCO[33] and
VOC2012SBD [13] datasets for experiments. COCO17 has
118k training and 5k validation images with 80 common
object categories. VOC consists of 20 categories and con-
tains 10,582 images for model training and 1,449 validation
images for evaluation.
Evaluation Metric. We use mean average
precision mAP@[.5,.95] for the MS-COCO. The
{AP, AP 50, AP 75, AP Small , AP Middle , AP Large} is
reported for MS-COCO and for VOC12SBD segmentation,
and we report AP 25,50,75. The mIoU boxis the average
IoU between predicted pseudo-boxes and GT-boxes in the
training set. It measures SAPNet‚Äôs ability to select mask
proposals without using the segmentation branch.
Implementation Details. In our study, we employed
the Stochastic Gradient Descent (SGD) optimizer, as de-
tailed in [6]. Our experiments were conducted using the
mmdetection toolbox [7], following standard training pro-
tocols for each dataset. We used the ResNet architecture
[15], pretrained on ImageNet [36], as the backbone. For
COCO, batch size was set at four images per GPU across
eight GPUs, and for VOC2012, it was four GPUs. More de-
tails of the experiment are in Sec. 8 of the supplementary.
4.2. Experimental Comparisons
Tab. 1 shows the comparison results between our method
and previous SOTA approaches [11, 16, 34, 40, 42] onCOCO. In our experiments, we provide SAM with both the
labeled points and the annotations generated by the point
annotation enhancer [9]. SAM then utilizes these inputs to
generate subsequent mask proposals for selection and su-
pervision. For fair comparison, we design two baselines:
the top-1 scored mask from SAM and MIL-selected SAM
mask proposals are used as SOLOv2 supervision, respec-
tively. Tab. 1 shows our method substantially surpasses
these baselines in performance.
Comparison with point-annotated methods. Our ap-
proach achieves a 31.2 APperformance with a ResNet-50
backbone, surpassing all previous point-annotated methods,
including BESTIE on HRNet-48 and AttnShift on Vit-B.
Our model exhibits significant improvements under a 1x
training schedule, with a 13.5 APincrease when compared
to the previous SOTA method, BESTIE. Furthermore, un-
der a 3x training schedule, SAPNet outperforms AttnShift,
which relies on large model training, with 13.4 AP, im-
provements. Importantly, our method is trained end-to-end
without needing post-processing, achieving SOTA perfor-
mance in point-annotated instance segmentation.
Comparison with other annotation-based methods.
Our SAPNet has significantly elevated point annotation,
regardless of point annotation‚Äôs limitations in annotation
time and quality compared to box annotation. Utilizing a
ResNet-101 backbone and a 3x training schedule, SAPNet
surpasses most box-annotated instance segmentation meth-
ods, achieving a 1.4 APimprovement over BoxInst. More-
over, SAPNet‚Äôs segmentation performance nearly matches
the mask-annotated methods, effectively bridging the gap
between point-annotated and these techniques.
Segmentation performance on VOC2012SBD. Tab. 2
compares segmentation methods under different supervi-
sions on the VOC2012 dataset. SAPNet reports an enhance-
ment of 7.7 APover the AttnShift approach, evidencing a
notable advancement in performance. Thereby, it signifi-
cantly outstrips image-level supervised segmentation meth-
ods. Additionally, SAPNet surpasses box-annotated seg-
mentation methods, such as BoxInst by 3.4 AP50and Dis-
coBox by 32.6 AP50. Further, our point-prompted method
achieves 92.3%of the Mask-R-CNN.
4.3. Ablation Studies
More experiments have been conducted on COCO to further
analyze SAPNet‚Äôs effectiveness and robustness.
Training Stage in SAPNet. The ablation study of the
training stage is given in Tab. 3. We trained solov2 using
the top-1 scored mask provided by SAM and compared it to
the two training strategies of SAPNet. In the two-stage ap-
proach, the segmentation branch and multiple-mask super-
vision of SAPNet are removed. Instead, we use the selected
mask to train a standalone instance segmentation model, as
described by [42]. The end-to-end training method corre-
3590
Method Ann. Backbone sched. Arch. mAP mAP 50mAP 75 mAP smAP mmAP l
Fully-supervised instance segmentation models.
Mask R-CNN [16] M ResNet-50 1x Mask R-CNN 34.6 56.5 36.6 18.3 37.4 47.2
YOLACT-700 [5] M ResNet-101 4.5x YOLACT 31.2 54.0 32.8 12.1 33.3 47.
PolarMask [16] M ResNet-101 2x PolarMask 32.1 53.7 33.1 14.7 33.8 45.3
SOLOv2 [42] M ResNet-50 1x SOLOv2 34.8 54.9 36.9 13.4 37.8 53.7
CondInst [40] M ResNet-50 1x CondInst 35.3 56.4 37.4 18.0 39.4 50.4
SwinMR [34] M Swin-S 50e SwinMR 43.2 67.0 46.1 24.8 46.3 62.1
Mask2Former [11] M Swin-S 50e Mask2Former 46.1 69.4 52.8 25.4 49.7 68.5
Weakly-supervised instance segmentation models.
IRNet [45] I ResNet-50 1x Mask R-CNN 6.1 11.7 5.5 - - -
BESTIE [21] I HRNet-48 1x Mask R-CNN 14.3 28.0 13.2 - - -
BBTP [18] B ResNet-101 1x Mask R-CNN 21.1 45.5 17.2 11.2 22.0 29.8
BoxInst [39] B ResNet-101 3x CondInst 33.2 56.5 33.6 16.2 35.3 45.1
DiscoBox [23] B ResNet-50 3x SOLOv2 32.0 53.6 32.6 11.7 33.7 48.4
Boxlevelset [27] B ResNet-101 3x SOLOv2 33.4 56.8 34.1 15.2 36.8 46.8
WISE-Net [24] P ResNet-50 1x Mask R-CNN 7.8 18.2 8.8 - - -
BESTIE‚Ä†[21] P HRNet-48 1x Mask R-CNN 17.7 34.0 16.4 - - -
AttnShift [31] P Vit-B 50e Mask R-CNN 21.2 43.5 19.4 - - -
SAM-SOLOv2 P ResNet-50 1x SOLOv2 24.6 41.9 25.3 9.3 28.6 38.1
MIL-SOLOv2 P ResNet-50 1x SOLOv2 26.8 47.7 26.8 11.2 31.5 40.4
SAPNet( ours) P ResNet-50 1x SOLOv2 31.2 51.8 32.3 12.6 35.1 47.8
SAPNet( ours)‚àóP ResNet-101 3x SOLOv2 34.6 56.0 36.6 15.7 39.5 52.1
Table 1. Mask annotation( M), image annotation( I), box annotation( B) and point annotation( P) performance on COCO-17 val. ‚ÄòAnn.‚Äô is
the type of the annotation and ‚Äòsched.‚Äô means schedule.‚àóis the multi-scale augment training for re-training segmentation methods, and
other experiments are on single-scale training. SwinMR is Swin-Transformer-Mask R-CNN . SwinMR and Mask2Former use multi-scale
data augment strategies for SOTA.
Method Sup. Backbone AP25AP50AP75
Mask R-CNN [16] M R-50 78.0 68.8 43.3
Mask R-CNN [16] M R-101 79.6 70.2 45.3
BoxInst [39] B R-101 - 61.4 37.0
DiscoBox B R-101 72.8 62.2 37.5
BESTIE [21] I HRNet 53.5 41.7 24.2
IRNet [45] I R-50 - 46.7 23.5
BESTIE‚Ä†[21] I HRNet 61.2 51.0 26.6
WISE-Net [24] P R-50 53.5 43.0 25.9
BESTIE [21] P HRNet 58.6 46.7 26.3
BESTIE‚Ä†[21] P HRNet 66.4 56.1 30.2
Attnshift [31] P Vit-S 68.3 54.4 25.4
Attnshift‚Ä†[31] P Vit-S 70.3 57.1 30.4
SAPNet( ours) P R-101 76.5 64.8 58.7
Table 2. Instance segmentation performance on the VOC2012 test
set.‚Ä†indicates applying MRCNN refinement.
sponds to the architecture illustrated in Fig. 2. Our findings
indicate that our method is more competitive than directly
employing SAM (31.2 APvs24.6AP), and the visualiza-
tion of Fig. 4 shows us this enhancement. Moreover, the
end-to-end training strategy boasts a more elegant model
structure and outperforms the two-stage approach in overall
efficiency (31.2 APvs30.18 AP).
Effect of Each Component. Given the limited perfor-
mance of SAM-top1, we opted for the single-MIL as our
baseline. With a preliminary selection using MIL1, we
Figure 4. The comparative visualization between SAM-top1 and
SAPNet is presented, showcasing SAM‚Äôs segmentation outcomes
in green masks and our results in yellow. The orange and red
bounding boxes highlight the respective mask boundaries.
train stage on coco sched. AP AP 50AP75
SAM-top1 1x 24.6 41.9 25.3
Two stage 1x 30.2 49.8 31.5
End to end 1x 31.2 51.8 32.3
Table 3. The experimental comparisons of segmenters in COCO
dataset, SAM-top1 is the highest scoring mask generated by SAM.
have achieved a segmentation performance of 26.8 AP.i)
Point Distance Guidance. We updated the proposal scores
from the existing MIL by integrating the PDG module into
the foundational MIL selection. This approach successfully
segments adjacent objects of the same category, improving
the segmentation performance by 0.7 points (27.5 vs 26.8).
3591
mil1 PDG mil2 PNPG BMS MPS mAP
‚úì 26.8
‚úì ‚úì 27.5
‚úì ‚úì ‚úì 27.7
‚úì ‚úì ‚úì ‚úì 29.7
‚úì ‚úì ‚úì ‚úì ‚úì 30.8
‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 31.2
Table 4. The effect of each component in SAPNet: proposal
selection module(MIL1), point distance guidance(PDG), positive
and negative proposals generator(PNPG), proposal selection mod-
ule(MIL2), box mining strategy(BMS), and Multi-mask Proposals
Supervision(MPS) in Sec. 7 of supplementary.
ii) MIL2. Building on the previous step, we incorporate
a second MIL selection module to refine the initially se-
lected boxes, resulting in a performance increment of 0.2
points. iii) PNPG. For MIL2, we devised the positive-
negative sample sets, aiming to enhance the input quality for
the PRM module and use the negative samples to suppress
background. This adjustment leads to a segmentation per-
formance boost of 2 points (29.7 vs 27.7). iv) BMS. Within
the PRM, we refine the selected boxes using BMS, push-
ing the segmentation performance up by 1.1 points (30.8
vs 29.7). v) MPS. Utilizing MPS for segmentation branch
supervision yields a 0.4-point performance improvement.
Threshold of BMS. For point refinement, there are two
constraints (described in Sec. 3.4). Tmin1andTmin2are
thresholds of the Box Mining Strategy. In Tab. 5, it shows
that the two constraints together to obtain performance gain.
After multiple experiments, we have found that there is
a significant performance improvement when Tmin1and
Tmin2are set to 0.6 and 0.3, respectively.
Components of PNPG. Tab. 6 presents the results of a
dissected ablation study on the Positive and Negative Pro-
posals Generator(PNPG), illustrating the respective impacts
of the positive and negative examples on the model‚Äôs per-
formance. It is evident that the construction of negative ex-
amples plays a significant role in enhancing model efficacy.
Furthermore, the beneficial effects of both positive and neg-
ative examples are observed to be cumulative.
Performance Analysis. As presented in Tab. 7, we con-
ducted a statistical analysis to validate SAPNet‚Äôs capability
to address ‚Äôlocal‚Äô issue and compare the outcomes selected
by the single-MIL with those obtained by SAPNet in the
absence of segmentation branch integration. Specifically,
the part problem generated by the single-MIL, where MIL
is inclined to select proposals with a higher proportion of
foreground, is exemplified in Fig. 6 of supplementary. On
this premise, we initially establish an evaluative criterion
Rv=area mask
area box, which is the ratio of the mask area to the
bounding box area. Subsequently, we compute Rvifor each
proposal within the proposal bag corresponding to every in-
stance across the entire COCO dataset and select the max-
imum Rvmaxto compute the mean value over the dataset,Tmin1Tmin2AP AP 50AP75APsAPmAPl
0.5 0.3 30.9 51.3 32.0 12.2 34.7 47.4
0.5 0.4 30.7 51.2 31.8 11.9 34.7 47.1
0.6 0.3 31.2 51.8 32.3 12.6 35.1 47.8
0.6 0.4 30.8 51.1 32.0 12.1 34.7 47.3
0.7 0.3 31.0 51.5 32.2 12.6 34.9 47.3
0.7 0.4 30.7 51.1 31.9 12.0 34.6 47.2
Table 5. Constraints in box mining strategy.
PNPGAP AP 50AP75PPG NPG
29.3 49.7 30.0
‚úì 29.8 50.5 30.8
‚úì 30.7 51.2 31.7
‚úì ‚úì 31.2 51.8 32.3
Table 6. Meticulous ablation experiments in PNPG
Method Gap mIoU box
Single-MIL 0.199 63.8
SAPNet 0.131 69.1
Table 7. Experimental analysis with part problem.
which is then designated as the threshold Trv. Ultimately,
we identify the ground truth Rvgtand objects where Rvmax
exceeds Trvand calculates the discrepancy between Rvval-
ues selected by single-MIL and SAPNet. The description is
as follows:
Gapsingle =Rvsingle‚àíRvgt, Gap our=Rvour‚àíRvgt.
(10)
Tab. 7 shows that the proposed SAPNet mitigates the
locality issue faced by the single-MIL. Furthermore, the
boxes selected via SAPNet exhibit a substantially higher
IoU with GT than those selected by the single-MIL.
5. Conclusion
In this paper, we propose SAPNet, an innovative end-to-end
point-prompted instance segmentation framework. SAPNet
transforms point annotations into category-agnostic mask
proposals and employs dual selection branches to elect the
most semantic mask for each object, guiding the segmenta-
tion process. To address challenges such as indistinguish-
able adjacent objects of the same class and MIL‚Äôs locality
bias, we integrate PDG and PNPG, complemented by a Box
Mining Strategy for enhanced proposal refinement. SAP-
Net uniquely merges segmentation and selection branches
under multi-mask supervision, significantly enhancing its
segmentation performance. Extensive experimental com-
parisons on VOC and COCO datasets validate the SAPNet‚Äôs
effectiveness in point-prompted instance segmentation.
6. Acknowledgements
This work was supported in part by the Youth Innovation Promotion As-
sociation CAS, the National Natural Science Foundation of China (NSFC)
under Grant No. 61836012, 61771447 and 62272438, and the Strategic
Priority Research Program of the Chinese Academy of Sciences under
Grant No.XDA27000000.
3592
References
[1] Jiwoon Ahn, Sunghyun Cho, and Suha Kwak. Weakly su-
pervised learning of instance segmentation with inter-pixel
relations. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 2209‚Äì2218,
2019. 2
[2] Pablo Andr ¬¥es Arbel ¬¥aez, Jordi Pont-Tuset, and Jonathan
T. Barron et al . Multiscale combinatorial grouping. In
CVPR , 2014. 2
[3] Aditya Arun, CV Jawahar, and M Pawan Kumar. Weakly su-
pervised instance segmentation by learning annotation con-
sistent instances. In European Conference on Computer Vi-
sion, pages 254‚Äì270. Springer, 2020. 2
[4] Hakan Bilen and Andrea Vedaldi. Weakly supervised deep
detection networks. In CVPR , 2016. 2, 3
[5] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.
Yolact: Real-time instance segmentation. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 9157‚Äì9166, 2019. 1, 7
[6] L ¬¥eon Bottou. Stochastic gradient descent tricks. In Neural
Networks: Tricks of the Trade: Second Edition , pages 421‚Äì
436. Springer, 2012. 6
[7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Zi-
wei Liu, Jiarui Xu, et al. Mmdetection: Open mm-
lab detection toolbox and benchmark. arXiv preprint
arXiv:1906.07155 , 2019. https://github.com/
open-mmlab/mmdetection . 6
[8] Keyan Chen, Chenyang Liu, Hao Chen, Haotian Zhang,
Wenyuan Li, Zhengxia Zou, and Zhenwei Shi. Rsprompter:
Learning to prompt for remote sensing instance segmen-
tation based on visual foundation model. arXiv preprint
arXiv:2306.16269 , 2023. 3
[9] Pengfei Chen, Xuehui Yu, Xumeng Han, Najmul Hassan,
Kai Wang, Jiachen Li, Jian Zhao, Humphrey Shi, Zhenjun
Han, and Qixiang Ye. Point-to-box network for accurate ob-
ject detection via single point supervision. In European Con-
ference on Computer Vision , pages 51‚Äì67. Springer, 2022. 2,
3, 6
[10] Tianle Chen, Zheda Mai, Ruiwen Li, and Wei-lun Chao.
Segment anything model (sam) enhanced pseudo labels for
weakly supervised semantic segmentation. arXiv preprint
arXiv:2305.05803 , 2023. 3
[11] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In CVPR ,
2022. 1, 6, 7
[12] Bowen Cheng, Omkar Parkhi, and Alexander Kirillov.
Pointly-supervised instance segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2617‚Äì2626, 2022. 2
[13] Mark Everingham, Luc Van Gool, and Christopher K.
I. Williams et al . The pascal visual object classes (VOC)
challenge. IJCV , 2010. http://host.robots.ox.
ac.uk/pascal/VOC/ . 6
[14] Junsong Fan, Zhaoxiang Zhang, and Tieniu Tan. Pointly-supervised panoptic segmentation. In European Conference
on Computer Vision , pages 319‚Äì336. Springer, 2022. 1
[15] Kaiming He, Xiangyu Zhang, and Shaoqing Ren et al. Deep
residual learning for image recognition. In CVPR , 2016. 6
[16] Kaiming He, Georgia Gkioxari, and Piotr Doll ¬¥aret al. Mask
R-CNN. In ICCV , 2017. 1, 6, 7
[17] Sheng He, Rina Bao, Jingpeng Li, P Ellen Grant, and
Yangming Ou. Accuracy of segment-anything model (sam)
in medical image segmentation tasks. arXiv preprint
arXiv:2304.09324 , 2023. 3
[18] Cheng-Chun Hsu, Kuang-Jui Hsu, Chung-Chi Tsai, Yen-
Yu Lin, and Yung-Yu Chuang. Weakly supervised instance
segmentation using the bounding box tightness prior. In
NeurIPS , 2019. 2, 7
[19] Peng-Tao Jiang and Yuqi Yang. Segment anything is a good
pseudo-label generator for weakly supervised semantic seg-
mentation. arXiv preprint arXiv:2305.01275 , 2023. 3
[20] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing
Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in
high quality. arXiv preprint arXiv:2306.01567 , 2023. 1, 3
[21] Beomyoung Kim, Youngjoon Yoo, Chaeeun Rhee, and
Junmo Kim. Beyond semantic to instance segmenta-
tion: Weakly-supervised instance segmentation via semantic
knowledge transfer and self-refinement. In CVPR , 2022. 1,
2, 7
[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. https:
//segment-anything.com/ . 1, 2, 3
[23] Shiyi Lan, Zhiding Yu, Christopher Choy, Subhashree Rad-
hakrishnan, Guilin Liu, Yuke Zhu, Larry S Davis, and An-
ima Anandkumar. Discobox: Weakly supervised instance
segmentation and semantic correspondence from box super-
vision. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision , pages 3406‚Äì3416, 2021. 1, 2,
7
[24] Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro,
David V ¬¥azquez, and Mark Schmidt. Proposal-based instance
segmentation with point supervision. In ICIP , 2020. 2, 7
[25] Jungbeom Lee, Jihun Yi, Chaehun Shin, and Sungroh Yoon.
Bbam: Bounding box attribution map for weakly super-
vised semantic and instance segmentation. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 2643‚Äì2652, 2021. 2
[26] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu,
Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao.
Semantic-sam: Segment and recognize anything at any gran-
ularity. arXiv preprint arXiv:2307.04767 , 2023. 3
[27] Wentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui, Xian-
Sheng Hua, and Lei Zhang. Box-supervised instance seg-
mentation with level set evolution. In European conference
on computer vision , pages 1‚Äì18. Springer, 2022. 1, 7
[28] Wentong Li, Yuqian Yuan, Song Wang, Jianke Zhu, Jianshu
Li, Jian Liu, and Lei Zhang. Point2mask: Point-supervised
panoptic segmentation via optimal transport. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 572‚Äì581, 2023. 1, 2
3593
[29] Wentong Li, Wenyu Liu, Jianke Zhu, Miaomiao Cui,
Risheng Yu Xiansheng Hua, and Lei Zhang. Box2mask:
Box-supervised instance segmentation via level-set evolu-
tion. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 2024. 2
[30] Wentong Li, Yuqian Yuan, Song Wang, Wenyu Liu, Dongqi
Tang, Jianke Zhu, Lei Zhang, et al. Label-efficient segmen-
tation via affinity propagation. Advances in Neural Informa-
tion Processing Systems , 36, 2024. 2
[31] Mingxiang Liao, Zonghao Guo, , and Yuze Wang et al. At-
tentionshift: Iteratively estimated part-based attention map
for pointly supervised instance segmentation. In CVPR ,
2023. 2, 7
[32] Tsung-Yi Lin, Priya Goyal, and Ross B. Girshick et al. Focal
loss for dense object detection. In ICCV , 2017. 5, 6
[33] Tsung-Yi Lin, Michael Maire, and Serge et al. Belongie. Mi-
crosoft coco: Common objects in context. In ECCV , 2014.
https://cocodataset.org/ . 6
[34] Ze Liu, Yutong Lin, and Yue Cao et al. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , 2021. 6, 7
[35] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.
V-net: Fully convolutional neural networks for volumetric
medical image segmentation. In 2016 fourth international
conference on 3D vision (3DV) , pages 565‚Äì571. Ieee, 2016.
6
[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115:211‚Äì252, 2015. 6
[37] Peng Tang and Xinggang Wang et al. Multiple instance de-
tection network with online instance classifier refinement. In
CVPR , 2017. 3
[38] Peng Tang, Xinggang Wang, and Song Bai et al. PCL: pro-
posal cluster learning for weakly supervised object detection.
IEEE TPAMI , 2020. 5
[39] Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen.
Boxinst: High-performance instance segmentation with box
annotations. In CVPR , 2021. 1, 2, 7
[40] Zhi Tian, Bowen Zhang, Hao Chen, and Chunhua Shen. In-
stance and panoptic segmentation using conditional convo-
lutions. IEEE TPAMI , 2023. 1, 2, 6, 7
[41] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and
Lei Li. SOLO: segmenting objects by locations. In ECCV ,
2020.
[42] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-
hua Shen. Solov2: Dynamic and fast instance segmentation.
Proc. Advances in Neural Information Processing Systems
(NeurIPS) , 2020. https://github.com/WXinlong/
SOLO . 1, 2, 3, 6, 7
[43] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing
Wang, and Feng Zheng. Track anything: Segment anything
meets videos. arXiv preprint arXiv:2304.11968 , 2023. 3
[44] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu,
Min Li, Ming Tang, and Jinqiao Wang. Fast segment any-
thing. arXiv preprint arXiv:2306.12156 , 2023. 1, 3[45] Yanning Zhou, Hao Chen, Jiaqi Xu, Qi Dou, and Pheng-Ann
Heng. Irnet: Instance relation network for overlapping cer-
vical cell segmentation. In MICCAI , 2019. 1, 7
3594
