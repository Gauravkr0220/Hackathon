FreeU: Free Lunch in Diffusion U-Net
Chenyang Si Ziqi Huang Yuming Jiang Ziwei LiuB
S-Lab, Nanyang Technological University
{chenyang.si, ziqi002, yuming002, ziwei.liu }@ntu.edu.sg
https://github.com/ChenyangSi/FreeU
SD1.4 FreeU SDXL FreeUSD1.4 FreeU
Figure 1. FreeU substantially improves diffusion model sample quality at no costs: no training, no additional learnable parameter
introduced, and no increase in memory or sampling time.
Abstract
In this paper, we uncover the untapped potential of dif-
fusion U-Net, which serves as a â€œfree lunchâ€ that substan-
tially improves the generation quality on the fly. We initially
investigate the key contributions of the U-Net architecture
to the denoising process and identify that its main backbone
primarily contributes to denoising, whereas its skip connec-
tions mainly introduce high-frequency features into the de-
coder module, causing the potential neglect of crucial func-
tions intrinsic to the backbone network. Capitalizing on this
discovery, we propose a simple yet effective method, termed
â€œFreeU â€, which enhances generation quality without addi-
tional training or finetuning. Our key insight is to strategi-
cally re-weight the contributions sourced from the U-Netâ€™s
skip connections and backbone feature maps, to leverage
the strengths of both components of the U-Net architec-
ture. Promising results on image and video generation tasks
demonstrate that our FreeU can be readily integrated to ex-
isting diffusion models, e.g., Stable Diffusion, DreamBooth
and ControlNet, to improve the generation quality with only
a few lines of code. All you need is to adjust two scaling
factors during inference.1. Introduction
Diffusion probabilistic models, a cutting-edge category of
generative models, have garnered significant attention, par-
ticularly for tasks related to computer vision [7, 8, 11, 18,
33, 41, 45, 46, 49]. These diffusion models are composed
of two key processes: diffusion process and the denoising
process . In the diffusion process , Gaussian noise is gradu-
ally added to the input data and eventually corrupts it into
approximately pure Gaussian noise. During the denoising
process , the original input data is recovered from its noise
state through a learned sequence of inverse diffusion op-
erations. Usually, a U-Net is employed to iteratively pre-
dict the noise to be removed at each denoising step. Ex-
isting works [3, 47, 58, 65] primarily focus on utilizing
pre-trained diffusion U-Nets for downstream applications,
while the internal properties of the diffusion U-Net, remain
largely under-explored.
In this paper, we delve into the denoising process of the
diffusion U-Net. For a comprehensive analysis, our first ob-
jective is to explore the mechanics behind how images are
generated from noise during the denoising process . To
understand whatâ€™s going on, we conduct an investigation
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
4733
(a) UNetArchitecture(b) FreeUOperations
skip connections
âŠ•âŠ•!,#$backbone features (#)skip features (%)FFTIFFT
&Figure 2. FreeU Framework .(a) U-Net Skip Features and
Backbone Features . In U-Net, the skip features and backbone
features are concatenated together at each decoding stage. We ap-
ply the FreeU operations during concatenation. (b) FreeU Oper-
ations. Two modulation factors ( bands) are employed to balance
the feature contributions from the backbone and skip connections.
within the Fourier domain, focusing on the generative evo-
lution during the denoising process. Our meticulous analy-
sis reveals a subtle modulation of the low-frequency com-
ponents, which demonstrate a gentle rate of change. In con-
trast, the high-frequency components showcase more pro-
nounced dynamics throughout the denoising process. Fun-
damentally, low-frequency components bestow upon an im-
age its foundational structure and color attributes. Exces-
sive adjustments during iterative denoising risk undermin-
ing the imageâ€™s intrinsic semantic integrity. High-frequency
components, which represent details like edges and tex-
tures, are more affected by noise. Hence, the goal of the
denoising process is to reduce this noise while ensuring the
preservation of critical details.
Building on this foundational understanding, our analy-
sis scope is expanded to how diffusion U-Net implements
denoising process , thereby ascertaining the specific con-
tributions of the U-Net architecture within the diffusion
framework. Structurally, the U-Net architecture comprises a
primary backbone network, encompassing both an encoder
and a decoder, as well as the skip connections that bridge
information transfer between the encoder and decoder, as
shown in Fig. 2. Our investigation reveals that the main
backbone of the U-Net primarily contributes to denoising.
Conversely, the skip connections are observed to introduce
high-frequency features into the decoder module. These
connections propagate high-frequency information to make
U-Net easier to recover the input data during training. Yet,
an unintended consequence of this propagation is the poten-
tial weakening of the backboneâ€™s inherent denoising capa-
bilities during the inference. This can lead to a reduction in
the generation quality e.g. abnormal image details, as illus-
trated in Fig. 1.With these revelations as our backdrop, we propel for-
ward with the introduction of a novel strategy, denoted as
â€œFreeU â€, which holds the potential to improve sample qual-
ity without necessitating the computational overhead of ad-
ditional training or fine-tuning. Specifically, during infer-
ence, we instantiate two specialized modulation factors de-
signed to balance the feature contributions from the U-Net
architectureâ€™s primary backbone and skip connections. The
first, termed the backbone feature factors, aims to amplify
the feature maps of the main backbone, thereby bolstering
the denoising process. However, we find that while the in-
clusion of backbone feature scaling factors yields signif-
icant improvements, it can occasionally lead to an unde-
sirable oversmoothing of textures. To mitigate this issue,
we introduce the second factor, skip feature scaling factors,
aiming to alleviate the problem of texture oversmoothing.
Our FreeU method exhibits seamless adaptability when
integrated with existing diffusion models. We conduct a
comprehensive experimental evaluation of our approach,
employing Stable Diffusion [43, 46], ModelScope [37],
Dreambooth [47], ReVersion [23], Rerender [61], Scale-
Crafter [16], Animatediff [14] and ControlNet [65] as our
foundational models for benchmark comparisons. By em-
ploying FreeU during the inference phase, these models in-
dicate a discernible enhancement in the quality of generated
samples, as shown in Fig. 1. Our contributions are summa-
rized as follows:
â€¢ We investigate the denoising process in Fourier domain,
revealing that low-frequency components change gradu-
ally, while high-frequency components exhibit more sig-
nificant variations.
â€¢ We conduct a pioneering exploration of the potential of
diffusion U-Net, highlighting that its backbone primarily
contributes to denoising, whereas its skip connections in-
troduce high-frequency features into the decoder. This
novel perspective offers fresh research opportunities for
the community.
â€¢ We introduce a simple yet effective method, denoted as
â€œFreeU â€, which enhances U-Netâ€™s denoising capability
by leveraging the strengths of both components of the U-
Net architecture.
â€¢ We empirically evaluate our approach on various diffu-
sion models, demonstrating significant sample quality im-
provement and the effectiveness of FreeU at no extra cost.
2. Methodology
2.1. Preliminaries
Generating samples from a diffusion model is initi-
ated by sourcing from a Gaussian noise distribution
and subsequently following the inverse diffusion process
pÎ¸(xtâˆ’1|xt). This results in a trajectory sequence xT,
xTâˆ’1, ...,x0ending with the generated sample x0. Cru-
4734
Denoising
Generated image
Low frequency
High frequencyA squirrel eating a burger.Figure 3. Denoising process visualization: The top row shows the generated im-
ages of the denoising process. The next two rows display low-frequency and high-
frequency components after the inverse Fourier Transform. Low-frequency compo-
nents change slowly, whereas high-frequency components exhibit more significant
variations during the denoising process.
t=0LowHighFigure 4. Relative log amplitudes of Fourier
for denoising process. At each denoising step t,
we visualize the relative log amplitudes of Fourier
of recovered date xt. We observe that the high-
frequency components of xtdrops drastically dur-
ing the denoising process.
cially, the sampling process depends on the denoising
model ÏµÎ¸to eliminate noise. The optimization objective of
denoising model is as follows:
LDM =Ex,Ïµâˆ¼N (0,1),th
âˆ¥Ïµâˆ’ÏµÎ¸(xt, t)âˆ¥2
2i
(1)
In most implementations, the denoising model is realized
using a time-conditional U-Net architecture. Hence, its de-
noising ability plays a pivotal role in determining the quality
of the data generated.
2.2. How to Generate Images from Noise During
Denoising Process?
To better understand the denoising process, we conduct an
investigation within the Fourier domain to perspective the
generated process of diffusion models. As illustrated in
Fig. 3, the uppermost row provides the progressive denois-
ing process, showcasing the generated images across suc-
cessive iterations. The subsequent two rows exhibit the as-
sociated low-frequency and high-frequency spatial domain
information after the inverse Fourier Transform, aligning
with each respective step.
Evident from Fig. 3 is the gradual modulation of low-
frequency components, showing a soft rate of change,
while their high-frequency components show more obvi-
ous changes throughout the entire denoising process. These
findings are further corroborated in Fig.4. This can be intu-
itively explained: 1) Low-frequency components inherently
embody the global structure and characteristics of an im-
age, encompassing global layouts and smooth color. These
components encapsulate the foundational global elements
that constitute the imageâ€™s essence and representation. Its
rapid alterations are generally unreasonable in denoising
processes. Drastic changes to these components could fun-damentally reshape the imageâ€™s essence, an outcome typ-
ically incompatible with the objectives of denoising pro-
cesses. 2) Conversely, high-frequency components contain
rapid changes in the images, such as edges and textures.
These finer details are markedly sensitive to noise, often
manifesting as random high-frequency information when
noise is introduced to an image. Consequently, denoising
processes need to expunge noise while upholding indispens-
able intricate details.
2.3. How does Diffusion U-Net Perform Denoising?
Building on this foundational understanding throughout the
denoising process, we extend our investigation to delineate
the specific contributions of the U-Net architecture within
the denoising process, to explore the internal properties of
the denoising network. As illustrated in Fig. 2, the U-Net
architecture comprises a primary backbone network, as well
as the skip connections that facilitate information transfer
between the encoder and decoder.
To evaluate the role of the backbone and lateral skip con-
nections in the denoising process, we conduct a controlled
experiment wherein we introduce two multiplicative scal-
ing factorsâ€”denoted as bandsâ€”to modulate the feature
maps generated by the backbone and skip connections, re-
spectively, prior to their concatenation. As shown in Fig. 5,
it is evident that elevating the scale factor bof the backbone
distinctly enhances the quality of generated images. Con-
versely, variations in the scaling factor s, which modulates
the impact of the lateral skip connections, appear to exert a
limited influence on the quality of the generated images.
The backbone of U-Net. Building upon these observations,
we subsequently probed the underlying mechanisms for the
enhancement in image generation quality when the scaling
factor bassociated with the backbone feature map increases.
4735
b=0.6 ,s=1.0
 b=0.8 ,s=1.0 b=1.0 ,s=1.0 b=1.2 ,s=1.0 b=1.4 ,s=1.0
b=1.0, s=0.6 b=1.0, s=0.8 b=1.0, s=1.0 b=1.0, s=1.2 b=1.0, s=1.4Figure 5. Effect of backbone and skip connection scaling factors ( band s).
Increasing the backbone scaling factor bsignificantly enhances image quality, while
directly scaling sin the skip features has a limited influence on image synthesis
quality.
High-frequencydecreases as ð‘increasesFigure 6. Relative log amplitudes of Fourier with
variations of the backbone scaling factor b.In-
creasing in bcorrespondingly results in a suppres-
sion of high-frequency components in the images
generated by the diffusion model.
Our analysis reveals that this quality improvement is fun-
damentally linked to an amplified denoising capability im-
parted by the U-Net architectureâ€™s backbone. As delineated
in Fig. 6, a commensurate increase in bcorrespondingly
results in a suppression of high-frequency components in
the images generated by the diffusion model. Therefore, in
Fig. 5, when b= 0.6, the generated images exhibit a signifi-
cant amount of noise that adversely affects image quality. In
contrast, when b= 1.4, highly clear images are generated.
This indicates that the primary role of the U-Net backbone
network is to filter out high-frequency noise. Enhancing
the backbone features effectively boosts the denoising ca-
pability of the U-Net architecture, thereby contributing to
superior output in terms of fidelity and detail preservation.
The skip connections of U-Net. Conversely, the skip con-
nections serve to forward features from the earlier layers of
encoder blocks directly to the decoder. Intriguingly, as ev-
idenced in Fig. 7, these features primarily constitute high-
frequency information. Our conjecture, grounded in this ob-
servation, posits that during the training of the U-Net archi-
tecture, the presence of these high-frequency features may
inadvertently accelerate the convergence toward noise pre-
Figure 7. Fourier relative log amplitudes of backbone, skip,
and their fused feature maps. The skip features contain a large
amount of high-frequency information.diction with the optimization objective of Eqn. 1, making
it easier to reconstruct the input data. This phenomenon,
in turn, could result in an unintended attenuation of the
efficacy of the backboneâ€™s intrinsic denoising capabilities.
However, unlike the training process where the goal is to
reconstruct input data, the inference process aims to gener-
ate data from Gaussian noise. The generative capacity of
diffusion models manifests in their denoising capabilities.
Therefore, during inference, it is essential to enhance the
denoising capabilities of the U-Net to ensure high-quality
data generation.
2.4. Free Lunch in Diffusion U-Net
Capitalizing on the above discovery, we propel forward with
the introduction of a simple yet effective method, denoted
as â€œFreeU â€, which effectively bolsters the denoising capa-
bility of the U-Net architecture by leveraging the strengths
of both components of the U-Net architecture. It substan-
tially improves the generation quality without requiring ad-
ditional training or fine-tuning.
The backbone factors. To enhance the denoising capa-
bilities of the U-Net, we introduce a novel method known
as structure-aware scaling for the backbone features, which
dynamically adjusts the scaling of backbone features for
each sample. Unlike a fixed scaling factor applied uni-
formly to all samples or positions within the same channel,
our approach adjusts the scaling factor adaptively based on
the specific characteristics of the sample features. We first
compute the average feature map along the channel dimen-
sion:
Â¯xl=1
CCX
i=1xl,i (2)
where xl,irepresents the i-th channel of the backbone fea-
ture map xlin the l-th block of the U-Net decoder. Cde-
notes the total number of channels in xl. As illustrated in
4736
Generated image
Avg Feature mapGenerated image
Avg Feature map
Figure 8. Visualization of average feature maps: This visualiza-
tion displays the average feature maps along the channel dimen-
sion of backbone features.
Fig. 8, the average feature map Â¯xlinherently contains valu-
able structural information. Consequently, the backbone
factor map Î±lamplifies the backbone feature map xlin a
manner that aligns with its structural characteristics. Subse-
quently, the backbone factor map is determined as follows:
Î±l= (blâˆ’1)Â·Â¯xlâˆ’Min (Â¯xl)
Max (Â¯xl)âˆ’Min (Â¯xl)+ 1, (3)
where Î±lrepresents the backbone factor map. blis a scalar
constant and bl>1. Then, upon experimental investiga-
tion, we discern that indiscriminately amplifying all chan-
nels of xlthrough multiplication with Î±lengenders an
oversmoothed texture in the resulting synthesized images,
as shown in Fig. 9 (b). The reason is that U-Netâ€™s strong
denoising ability can damage the high-frequency details of
the image during denoising. Consequently, we confine the
scaling operation to the half channels of xlas follows:
xâ€²
l,i=(
xl,iâŠ™Î±lifi < C/ 2
xl,i otherwise(4)
Hence, the backbone factors can effectively enhance the de-
noising capabilities of the U-Net and generate better image
quality, as shown in Fig. 9 (c).
The skip factors. To further mitigate the issue of over-
smoothed texture due to enhancing denoising, we further
employ spectral modulation in the Fourier domain to selec-
tively diminish low-frequency components for the skip fea-
tures. Mathematically, this operation is performed as fol-
lows:
F(hl,i) =FFT (hl,i) (5)
Fâ€²(hl,i) =F(hl,i)âŠ™Î²l,i (6)
hâ€²
l,i=IFFT (Fâ€²(hl,i)) (7)
where hl,idenotes the i-th channel of the skip feature map
in the l-th block of the U-Net decoder. FFT (Â·)and IFFT (Â·)
are Fourier transform and inverse Fourier transform. âŠ™
denotes element-wise multiplication, and Î²l,iis a Fourier
mask, designed as a function of the magnitude of the Fourier
(b) (a) (c)Figure 9. Generated images with different backbone scaling
operations: (a) without backbone scaling, (b) scaling all channels,
(c) scaling half channels.
coefficients, serving to implement the frequency-dependent
scaling factor sl:
Î²l,i(r) =(
slifr < r thresh,
1otherwise .(8)
where ris the radius. rthresh is the threshold frequency, set
to 1 in our experiments. As shown in Fig. 10, reducing
low-frequency components of the skip features can generate
better details.
Remarkably, the proposed FreeU framework does not re-
quire any task-specific training or fine-tuning. Adding the
backbone and skip scaling factors can be easily done with
just a few lines of code, offering a more flexible and potent
denoising operation without adding any computational bur-
den. This makes FreeU a highly practical solution that can
be seamlessly integrated into existing diffusion models to
improve their generation quality.
3. Experiments
3.1. Implementation Details
To assess the effectiveness of the proposed FreeU, we sys-
tematically conduct a series of experiments, aligning our
benchmarks with state-of-the-art methods such as Stable
Diffusion [43, 46], ModelScope [37], Dreambooth [47], Re-
Version [23], Rerender [61], ScaleCrafter [16], Animate-
diff [14] and ControlNet [65]. Importantly, our approach
seamlessly integrates with these methods without impos-
ing any additional computational overhead associated with
training or fine-tuning. We strictly follow the prescribed set-
tings of these methods and exclusively introduce the back-
bone feature factors and skip feature factors during the in-
w/os w/s w/os w/s
Figure 10. Generated images of FreeU without skip scaling
(w/os), and with skip scaling (w/ s).
4737
SDXL SDXL +FreeU SDXL +FreeU SDXL
Figure 11. Text-to-image generation results of SD-XL [43] with or without FreeU. Images generated by SD-XL+FreeU show signifi-
cantly improved detail and quality compared to SD-XL.
ference. More ablation studies and quantitative results
can be found in supplementary material.
3.2. Text-to-Image Generation
Stable Diffusion [43, 46] is a latent text-to-image diffusion
model renowned for its capability to generate photorealistic
images based on textual input. It has consistently demon-
strated exceptional performance in various image synthe-
sis tasks. With the integration of our FreeU augmentation
into Stable Diffusion-XL [43], the results, as exemplified
in Fig. 11, exhibit a notable enhancement in the modelâ€™s
generative capacity. It becomes evident that our proposed
FreeU consistently excels in generating realistic images, es-
pecially in detail generation. More results of SD [46] and
SD-XL [43] are provided in the supplementary material.
These compelling results serve as a testament to the sub-
stantial qualitative enhancements engendered by the syn-
ergy of FreeU with the SD[46] or SDXL[43] frameworks.
Quantitative evaluation. We conduct a study with 120 par-
ticipants to assess image quality andimage-text alignment .
Each participant receives a text prompt and two correspond-
ing synthesized images, one from SD [46] and another from
SD+FreeU. To ensure fairness, we use the same randomly
sampled random seed for generating both images. The im-
age sequence is randomized to eliminate any bias. Par-
ticipants then select the image they consider superior for
image-text alignment andimage quality , respectively. We
tabulate the votes for SD [46] and SD+FreeU in each cat-
egory in Table 1. Our analysis reveals that the majority of
votes go to SD+FreeU, indicating that FreeU significantlyTable 1. Text-to-Image Quantitative Results. We count the
percentage of votes for the baseline and our method respectively.
Image-Text refers to Image-Text Alignment .
Method Image-Text Image Quality
SD [46] 15.42% 13.73%
SD+FreeU 84.58% 86.27%
Table 2. Text-to-Video Quantitative Results. We count the per-
centage of votes for the baseline and our method respectively.
Video-Text refers to Video-Text Alignment .
Method Video-Text Video Quality
ModelScope [37] 15.32% 14.25%
ModelScope+FreeU 84.68% 85.75%
enhances the Stable Diffusion text-to-image model in both
evaluated aspects.
3.3. Text-to-Video Generation
ModelScope [37], an avant-garde text-to-video diffusion
model, stands at the forefront of video generation from tex-
tual descriptions. The infusion of our FreeU augmentation
into ModelScope [37] serves to further hone its video syn-
thesis prowess, as substantiated by Fig. 12. For instance,
in response to the prompt â€œAn astronaut flying in spaceâ€ ,
ModelScope [37], with the assistance of FreeU, can gener-
ate a clear and vivid portrayal of an astronaut. These results
underscore the significant improvements achieved through
the synergistic application of FreeU with ModelScope [37],
resulting in high-quality generated content characterized by
clear motion, rich detail, and semantic alignment.
4738
A cinematic view of the ocean, from a cave.
An astronaut flying in space.ModelScope+FreeUModelScopeModelScope+FreeUModelScopeFigure 12. Text-to-video generation results of ModelScope [37] with or without FreeU. Videos generated by ModelScope+FreeU show
significantly improved appearance and motion compared to ModelScope.
Denoising
Figure 13. Fourier relative log amplitudes of SD [46] with or without FreeU within the denoising process. FreeU can significantly
reduce high-frequency information at each step of the denoising process, which indicates FreeUâ€™s capacity to effectively denoising.
Quantitative evaluation. We conduct the quantitative eval-
uation for FreeU on the text-to-video task in a similar way
as text-to-image. The results displayed in Table 2 indi-
cate that most participants prefer the video generated with
FreeU.
3.4. More Generative Models
We further incorporate FreeU into DreamBooth [47], Re-
Version [23], Rerender [61], ScaleCrafter [16], AnimateD-
iff [14] and ControlNet [65]. Their results are provided in
the supplementary material. These outcomes substantiate
that the incorporation of FreeU leads to enhanced synthesis
quality.
3.5. Ablation Study
Effects of FreeU. FreeU is introduced with the primary aim
of enhancing the denoising capabilities of the diffusion U-
Net. To assess the impact of FreeU, we conducted ana-
lytical experiments using SD [46] as the base framework.
In Fig. 13, we present visualizations of the Fourier rela-
tive log amplitudes of SD [46], comparing cases with and
without the incorporation of FreeU. These visualizations il-
lustrate that FreeU can significantly reduce high-frequency
information at each step of the denoising process, which
indicates FreeUâ€™s capacity to effectively denoising. Fur-
thermore, we extend our analysis by visualizing the feature
SD
SD+FreeU SD+FreeUSD
Figure 14. Visualization of feature maps for SD [46] with or
without FreeU.
maps of the U-Net. As shown in Fig. 14, we observe that the
feature maps generated by FreeU contain more pronounced
structural information. This observation aligns with the in-
tended effect of FreeU, as it preserves intricate details while
effectively removing noise, harmonizing with the denoising
objectives of the model.
Effects of components in FreeU. We evaluate the effects
of the proposed FreeU strategy, i.e. introducing backbone
feature scaling factors and skip feature scaling factors to
intricately balance the feature contributions from the back-
bone and skip connections. In Fig. 15, we present the re-
sults of our evaluations. In the case of SD+FreeU(b) , where
backbone scaling factors are integrated during inference,
we observe a noticeable improvement in the generation of
4739
A fat rabbit wearing a purple robe walking through a fantasy landscape
A synthwave style sunset above the reflecting water of the sea, digital art
SD SD+FreeU (b) SD+FreeU (b&s)Figure 15. Ablation study of backbone scaling factor band skip
scaling factor s.
vivid details compared to SD[46] alone. For instance,
SD+FreeU(b) generates a more realistic â€œrabbitâ€ with nor-
mal arms and ears, as opposed to SD[46]. However, it is
imperative to note that while the inclusion of feature scaling
factors yields significant improvements, it can occasionally
lead to an undesirable oversmoothing of textures. To mit-
igate this issue, we introduce skip feature scaling factors,
aiming to reduce low-frequency information and alleviate
the problem of texture oversmoothing. As demonstrated in
Fig. 15, the combination of both backbone and skip feature
scaling factors in SD+FreeU(b & s) leads to the genera-
tion of more realistic images. This highlights the efficacy of
FreeU strategy in balancing features and mitigating issues
related to texture smoothing, ultimately resulting in more
realistic image generation.
Effects of backbone structure-related factor. We evalu-
ate the effects of the proposed backbone scaling strategy,
structure-related scaling, on the delicate balance between
noise reduction and texture preservation. Illustrated in Fig-
ure 16, when compared to the results generated by SD[46],
we observe a substantial enhancement in the image qual-
ity generated by FreeU when utilizing a constant scaling
factor. However, it is pertinent to highlight that the utiliza-
tion of a constant factor can have undesirable consequences,
manifesting as pronounced oversmoothing of textures and
undesirable color oversaturation. Conversely, FreeU with
the structure-related scaling factor map employs an adaptive
scaling approach, leveraging structural information to guide
the assignment of the backbone factor map. Our observa-
tions indicate that FreeU with the structure-related factor
map effectively mitigates these issues and achieves signifi-
cant improvements in generating vivid and intricate details.
4. Related Work
Diffusion models have achieved great success in genera-
tion tasks [7, 8, 11, 13, 18, 24, 29, 33, 41, 45, 46, 49].
These models employ a fixed Markov chain to map the la-
tent space, facilitating intricate mappings that capture latent
structural complexities within a dataset. Recently, its im-
pressive generative capabilities have fueled groundbreak-
(a) (b) (c)
Figure 16. Comparing image generation with different back-
bone factors: (a) SD, (b) FreeU with a constant factor, and (c)
FreeU with a structure-related scaling factor map.
ing advancements in a variety of computer vision appli-
cations such as image synthesis [18, 46, 49], image edit-
ing [1, 6, 21, 38], and text-to-video generation [3, 15, 19,
37, 52, 57, 58, 64]. Though successful, these studies mainly
focus on utilizing pre-trained diffusion models for down-
stream applications, while the internal properties of the dif-
fusion models remain largely under-explored. In this paper,
we conduct a pioneering exploration of the potential of dif-
fusion models. More detailed discussion about related work
can be found in supplementary material.
5. Conclusion
In this study, we commence our investigation by analyzing
the process of image generation from noise. Subsequently,
we delve into a detailed analysis of how the U-Net architec-
ture implements the denoising process. Our investigation
reveals that the backbone primarily contributes to denois-
ing, while the skip connections predominantly introduce
high-frequency features into the decoder, potentially lead-
ing to a neglect of essential backbone semantics. To address
this, we introduce the elegantly simple yet highly effec-
tive approach, termed FreeU , which enhances U-Netâ€™s de-
noising capability by leveraging the strengths of both com-
ponents of the U-Net architecture. Extensive experiments
prove that FreeU can be seamlessly integrated into various
diffusion foundation models and their downstream tasks,
and substantially improve diffusion model sample quality
without additional training or fine-tuning.
6. Acknowledgement
This study is supported by the Ministry of Education, Sin-
gapore, under its MOE AcRF Tier 2 (MOET2EP20221-
0012), NTU NAP, and under the RIE2020 Industry Align-
ment Fund â€“ Industry Collaboration Projects (IAF-ICP)
Funding Initiative, as well as cash and in-kind contribution
from the industry partner(s).
4740
References
[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
diffusion for text-driven editing of natural images. In CVPR ,
2022. 8, 4
[2] Ronen Basri, Meirav Galun, Amnon Geifman, David Jacobs,
Yoni Kasten, and Shira Kritchman. Frequency bias in neural
networks for input of non-uniform density. In International
Conference on Machine Learning , pages 685â€“694. PMLR,
2020. 4
[3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In CVPR , 2023. 1, 8, 4
[4] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale GAN training for high fidelity natural image synthesis.
arXiv preprint arXiv:1809.11096 , 2018. 4
[5] Yuanqi Chen, Ge Li, Cece Jin, Shan Liu, and Thomas Li.
Ssd-gan: Measuring the realness in the spatial and spectral
domains. In Proceedings of the AAAI Conference on Artifi-
cial Intelligence , pages 1105â€“1112, 2021. 4
[6] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune
Gwon, and Sungroh Yoon. ILVR: Conditioning method for
denoising diffusion probabilistic models. In ICCV , 2021. 8,
4
[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat GANs on image synthesis. In NeurIPS , 2021. 1, 8, 4
[8] Patrick Esser, Robin Rombach, Andreas Blattmann, and
Bjorn Ommer. ImageBART: Bidirectional context with
multinomial diffusion for autoregressive image synthesis. In
NeurIPS , 2021. 1, 8, 4
[9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis. In CVPR ,
2021. 4
[10] Joel Frank, Thorsten Eisenhofer, Lea Sch Â¨onherr, Asja Fis-
cher, Dorothea Kolossa, and Thorsten Holz. Leveraging fre-
quency analysis for deep fake image recognition. In Inter-
national conference on machine learning , pages 3247â€“3258.
PMLR, 2020. 4
[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,
Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An
image is worth one word: Personalizing text-to-image gen-
eration using textual inversion. In ICLR , 2023. 1, 8, 4
[12] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron C Courville,
and Yoshua Bengio. Generative adversarial nets. In NeurIPS ,
2014. 4
[13] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo
Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-
tor quantized diffusion model for text-to-image synthesis. In
CVPR , 2022. 8, 4
[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725 , 2023. 2, 5, 7, 3, 9
[15] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelityvideo generation with arbitrary lengths. arXiv preprint
arXiv:2211.13221 , 2022. 8, 4
[16] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,
Menghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng
Chen, and Ying Shan. Scalecrafter: Tuning-free higher-
resolution visual generation with diffusion models. arXiv
preprint arXiv:2310.07702 , 2023. 2, 5, 7, 3, 8
[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems ,
30, 2017. 1, 3
[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In NeurIPS , 2020. 1, 8, 4
[19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,
and Jie Tang. CogVideo: Large-scale pretraining for
text-to-video generation via transformers. arXiv preprint
arXiv:2205.15868 , 2022. 8, 4
[20] Vlad Hosu, Hanhe Lin, Tamas Sziranyi, and Dietmar Saupe.
Koniq-10k: An ecologically valid database for deep learning
of blind image quality assessment. IEEE Transactions on
Image Processing , 29:4041â€“4056, 2020. 1
[21] Ziqi Huang, Kelvin C.K. Chan, Yuming Jiang, and Ziwei
Liu. Collaborative diffusion for multi-modal face generation
and editing. In CVPR , 2023. 8, 4
[22] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang
Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang
Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive
benchmark suite for video generative models. arXiv preprint
arXiv:2311.17982 , 2023. 1
[23] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan,
and Ziwei Liu. ReVersion: Diffusion-based relation inver-
sion from images. arXiv preprint arXiv:2303.13495 , 2023.
2, 5, 7, 3, 4, 12
[24] Yuming Jiang, Tianxing Wu, Shuai Yang, Chenyang Si,
Dahua Lin, Yu Qiao, Chen Change Loy, and Ziwei Liu.
Videobooth: Diffusion-based video generation with image
prompts. arXiv preprint arXiv:2312.00777 , 2023. 8, 4
[25] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.
Progressive growing of GANs for improved quality, stability,
and variation. In ICLR , 2018. 4
[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks. In
CVPR , 2019.
[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila. Analyzing and improving
the image quality of StyleGAN. In CVPR , 2020.
[28] Tero Karras, Miika Aittala, Samuli Laine, Erik H Â¨arkÂ¨onen,
Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free
generative adversarial networks. In NeurIPS , 2021. 4
[29] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen
Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:
Text-based real image editing with diffusion models. arXiv
preprint arXiv:2210.09276 , 2022. 8, 4
[30] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and
Feng Yang. Musiq: Multi-scale image quality transformer.
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 5148â€“5157, 2021. 1
4741
[31] Mahyar Khayatkhoei and Ahmed Elgammal. Spatial fre-
quency bias in convolutional generative adversarial net-
works. In Proceedings of the AAAI Conference on Artificial
Intelligence , pages 7152â€“7159, 2022. 4
[32] Diederik P Kingma and Max Welling. Auto-encoding varia-
tional bayes. arXiv preprint arXiv:1312.6114 , 2013. 4
[33] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli
Shechtman, and Jun-Yan Zhu. Multi-concept customization
of text-to-image diffusion. arXiv preprint arXiv:2212.04488 ,
2022. 1, 8, 4
[34] LAION-AI. aesthetic-predictor. https://https://
github.com/LAION-AI/aesthetic-predictor ,
2022. 1
[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll Â´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
Computer Visionâ€“ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings,
Part V 13 , pages 740â€“755. Springer, 2014. 3
[36] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang
Zhao. Latent consistency models: Synthesizing high-
resolution images with few-step inference. arXiv preprint
arXiv:2310.04378 , 2023. 1, 3, 4, 11
[37] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,
Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-
niu Tan. VideoFusion: Decomposed diffusion models for
high-quality video generation. In CVPR , 2023. 2, 5, 6, 7, 8,
1, 3, 4
[38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-
jun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided
image synthesis and editing with stochastic differential equa-
tions. In ICLR , 2022. 8, 4
[39] Mehdi Mirza and Simon Osindero. Conditional generative
adversarial nets. arXiv preprint arXiv:1411.1784 , 2014. 4
[40] Naila Murray, Luca Marchesotti, and Florent Perronnin.
Ava: A large-scale database for aesthetic visual analysis.
In2012 IEEE conference on computer vision and pattern
recognition , pages 2408â€“2415. IEEE, 2012. 1
[41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. GLIDE: Towards photorealistic image gener-
ation and editing with text-guided diffusion models. arXiv
preprint arXiv:2112.10741 , 2021. 1, 8, 4
[42] Namuk Park and Songkuk Kim. How do vision transformers
work? In International Conference on Learning Represen-
tations , 2021. 4
[43] Dustin Podell, Zion English, Kyle Lacey, Andreas
Blattmann, Tim Dockhorn, Jonas M Â¨uller, Joe Penna, and
Robin Rombach. Sdxl: Improving latent diffusion mod-
els for high-resolution image synthesis. arXiv preprint
arXiv:2307.01952 , 2023. 2, 5, 6, 1, 3, 7, 8
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748â€“8763. PMLR, 2021. 1, 3[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gener-
ation with CLIP latents. arXiv preprint arXiv:2204.06125 ,
2022. 1, 8, 4
[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj Â¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR , 2022. 1, 2, 5,
6, 7, 8, 3, 4
[47] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine
tuning text-to-image diffusion models for subject-driven
generation. In CVPR , 2023. 1, 2, 5, 7, 3, 4, 12
[48] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,
Jonathan Ho, Tim Salimans, David Fleet, and Mohammad
Norouzi. Palette: Image-to-image diffusion models. In ACM
SIGGRAPH , 2022. 4
[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,
Rapha Gontijo Lopes, et al. Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487 , 2022. 1, 8, 4
[50] Katja Schwarz, Yiyi Liao, and Andreas Geiger. On the fre-
quency bias of generative models. Advances in Neural Infor-
mation Processing Systems , 34:18126â€“18136, 2021. 4
[51] Chenyang Si, Weihao Yu, Pan Zhou, Yichen Zhou, Xinchao
Wang, and Shuicheng Yan. Inception transformer. Advances
in Neural Information Processing Systems , 35:23495â€“23509,
2022. 4
[52] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792 ,
2022. 8, 4
[53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete
representation learning. In NeurIPS , 2017. 4
[54] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing.
High-frequency component helps explain the generaliza-
tion of convolutional neural networks. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 8684â€“8694, 2020. 4
[55] Pei Wang, Yijun Li, and Nuno Vasconcelos. Rethinking and
improving the robustness of image style transfer. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 124â€“133, 2021. 4
[56] Tengfei Wang, Ting Zhang, Bo Zhang, Hao Ouyang, Dong
Chen, Qifeng Chen, and Fang Wen. Pretraining is all
you need for image-to-image translation. arXiv preprint
arXiv:2205.12952 , 2022. 4
[57] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,
Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo
Yu, Peiqing Yang, et al. Lavie: High-quality video gener-
ation with cascaded latent diffusion models. arXiv preprint
arXiv:2309.15103 , 2023. 8, 4
[58] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian
Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
Mike Zheng Shou. Tune-a-video: One-shot tuning of image
4742
diffusion models for text-to-video generation. arXiv preprint
arXiv:2212.11565 , 2022. 1, 8, 4
[59] Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao,
and Zheng Ma. Frequency principle: Fourier analy-
sis sheds light on deep neural networks. arXiv preprint
arXiv:1901.06523 , 2019. 4
[60] Zhi-Qin John Xu, Yaoyu Zhang, and Yanyang Xiao. Training
behavior of deep neural network in frequency domain. In
International Conference on Neural Information Processing ,
pages 264â€“274, 2019. 4
[61] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change
Loy. Rerender a video: Zero-shot text-guided video-to-video
translation. arXiv preprint arXiv:2306.07954 , 2023. 2, 5, 7,
3, 4, 13
[62] Xingyi Yang, Daquan Zhou, Jiashi Feng, and Xinchao Wang.
Diffusion probabilistic model made slim. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 22552â€“22562, 2023. 4
[63] Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus
Cubuk, and Justin Gilmer. A fourier perspective on model
robustness in computer vision. Advances in Neural Informa-
tion Processing Systems , 32, 2019. 4
[64] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui
Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng
Shou. Show-1: Marrying pixel and latent diffusion models
for text-to-video generation, 2023. 8, 4
[65] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 3836â€“3847, 2023. 1, 2, 5, 7, 3, 4, 10
4743
