Hierarchical Intra-modal Correlation Learning for
Label-free 3D Semantic Segmentation
Xin Kang1* Lei Chu2Jiahao Li2Xuejin Chen1Yan Lu2
1University of Science and Technology of China2Microsoft Research Asia
Abstract
Recent methods for label-free 3D semantic segmentation
aim to assist 3D model training by leveraging the open-
world recognition ability of pre-trained vision language
models. However, these methods usually suffer from in-
consistent and noisy pseudo-labels provided by the vision
language models. To address this issue, we present a hi-
erarchical intra-modal correlation learning framework that
captures visual and geometric correlations in 3D scenes at
three levels: intra-set, intra-scene, and inter-scene, to help
learn more compact 3D representations. We refine pseudo-
labels using intra-set correlations within each geometric
consistency set and align features of visually and geometri-
cally similar points using intra-scene and inter-scene corre-
lation learning. We also introduce a feedback mechanism to
distill the correlation learning capability into the 3D model.
Experiments on both indoor and outdoor datasets show the
superiority of our method. We achieve a state-of-the-art
36.6% mIoU on the ScanNet dataset, and a 23.0% mIoU on
the nuScenes dataset, with improvements of 7.8% mIoU and
2.2% mIoU compared with previous SOTA. We also provide
theoretical analysis and qualitative visualization results to
discuss the mechanism and conduct thorough ablation stud-
ies to support the effectiveness of our framework.
1. Introduction
Label-free 3D semantic segmentation, which aims to
achieve scene understanding without reliance on labeled
data, has recently emerged as a vital research topic. This
task holds significant value for practical applications, in-
cluding autonomous driving, robotic navigation, and aug-
mented reality, where the collection of 3D annotations is
expensive and novel objects may appear.
Existing methods [4,5,8,15,19,21,31,36,37,40] leverage
the open-world recognition capability of pre-trained vision
language models, such as CLIP [22] and MaskCLIP [39],
*Xin Kang did this work during his internship at Microsoft Research
Asia.
Intra -set label 
refinement
Intra -scene correlation Inter-scene correlationâ€¦
Noisy and inconsistent pseudo -labels from CLIP
Hierarchical intra -modal correlationsFigure 1. To address the noisy and inconsistent pseudo-label chal-
lenge, we design a hierarchical intra-modal correlation learning
framework, including intra-set label refinement, intra-scene corre-
lation learning, and inter-scene correlation learning. Intra-set la-
bel refinement reduces label inconsistency for points with similar
geometric attributes in each local set. For intra-scene correlation
learning, we model point correlations in each scene and constrain
visually and geometrically similar points to be closer in feature
space. For inter-scene correlation learning, we use cross-scene
point correlations to help constrain the consistency of feature dis-
tribution in different scenes.
to train 3D models through cross-modal transfer learning.
These vision language models generate semantic features
from texts and images to provide semantic guidance for
3D model training. However, pre-trained on image clas-
sification tasks, these vision language models struggle to
generate consistent dense semantic predictions [4, 39]. For
instance, as shown in Fig. 1, the predictions for pixels
belonging to the same chair may differ within each view
and across multiple views. The predictions for chairs in
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
28244
different scenes may also be inconsistent. Such ambigu-
ous guidance poses significant challenges for the 3D model
to learn stable visual representations. Recently, Segment
Anything (SAM) [16] has been proposed to pretrain mod-
els on dense prediction tasks and can obtain accurate object
masks. Leveraging SAM, concurrent work Chen et al. [4]
use a label refinement strategy to mitigate noisy supervision
in each mask, resulting in significant improvements. How-
ever, the use of SAM comes with a great increase in train-
ing costs. More importantly, the supervision inconsistency
across multiple views and different scenes still exists.
This inconsistency issue is caused by the unstable fea-
tures learned by the vision language model from images
with occlusions and a lack of geometric information. We
observe that point cloud data is free of occlusion and rich in
geometric clues, thus facilitating the learning of stable point
features. This allows us to establish reliable correlations
between different points, thus providing strong guidance to
help maintain feature compactness under inconsistent su-
pervision. As shown in Fig.1, intra-modal correlations in
3D scenes can be categorized into three aspects: Intra-
set correlation : Local point sets sharing similar geomet-
ric attributes usually can help generate cleaner and sharper
segmentation boundaries when compared to the boundaries
generated by the vision language model. This can be used
to reduce the noise and inconsistency of pseudo-labels and
thus improve local feature coherence. Intra-scene correla-
tion: Besides aligning 3D-2D and 3D-text features, align-
ing the features of objects with similar appearance and ge-
ometry during training can assist the 3D model in miti-
gating disruptions caused by ambiguous supervision from
the vision language model. This can help learn a more fo-
cused and concise feature space. Inter-scene correlation :
Aligning the features of objects with similar appearance and
geometry in different scenes can further address the inter-
scene contradictory semantic guidance, leading to consis-
tent feature distributions in various scenes.
In this paper, we present a hierarchical intra-modal cor-
relation learning framework to leverage the three aforemen-
tioned correlations. (1)We use an intra-set label refinement
scheme that statistically analyzes the pseudo-labels within
each geometric consistency set and refines the pseudo-
labels to encourage fewer label conflicts. (2)We propose
theintra-scene correlation learning module to capture point
feature correlations between different objects and thus con-
strain visually and geometrically similar points to be closer
in feature space. (3)We introduce the inter-scene correla-
tion learning module that leverages cross-scene attention to
model correlations among objects in different scenes, pro-
moting the 3D model to learn stable feature distributions.
Finally, we design a feedback mechanism that aligns the
output features of the 3D model with the final aggregated
point features, thereby distilling the correlation learning ca-pability into the 3D model. Experiments on both indoor and
outdoor datasets demonstrate the superiority of our method.
We achieve a state-of-the-art (SOTA) mIoU of 36.6% on
the ScanNet dataset, surpassing the previous SOTA method
CLIP2Scene [5] by 7.8% mIoU. On the nuScenes dataset,
we achieve 23.0% mIoU and surpass CLIP2Scene by 2.2%
mIoU. Theoretical analysis, qualitative visualization, and
extensive ablation studies further support the effectiveness
of our framework.
The key contributions can be summarized as follows.
â€¢ We propose a novel hierarchical intra-modal corre-
lation learning framework for label-free 3D semantic
segmentation that leverages intra-modal correlations
at three levels: intra-set, intra-scene, and inter-scene,
to capture visual and geometric correlations hierarchi-
cally and thus assist in learning compact 3D features.
â€¢ We present a comprehensive theoretical analysis, qual-
itative visualization, extensive ablation studies, and a
thorough discussion of our frameworkâ€™s mechanism.
â€¢ Our method achieves promising results on both indoor
and outdoor datasets, showing significant improve-
ment over previous SOTA methods.
2. Related Work
2.1. Label-free 3D Semantic Segmentation
Label-free 3D semantic segmentation, with the goal of
achieving scene understanding independent of labeled data,
has attracted significant attention in recent years. Ex-
isting works [4, 5, 8, 15, 18, 19, 21, 23, 31, 32, 36, 37, 40]
mainly utilize powerful pre-trained vision language mod-
els [16, 22, 39] to extract semantic knowledge from im-
age and text modalities and distill these semantic knowl-
edge into a 3D model through cross-modal transfer learning.
Specifically, these methods first construct dense correspon-
dences between 3D points, 2D images, and text descrip-
tions, and then optimize the similarities between point fea-
tures and text embeddings according to 2D pseudo-labels.
However, since vision language models are typically pre-
trained on image classification tasks and lack dense predic-
tion constraints, the pseudo-labels generated from dense vi-
sual features are usually noisy and inconsistent. This results
in ambiguous semantic guidance for the 3D model training
and hinders its performance. To address this issue, concur-
rent work by Chen et al. [4] use segmentation masks from
Segment Anything [16] to reduce the noise in 2D pseudo-
labels and improve the 3D modelâ€™s performance. Nonethe-
less, SAM brings considerable costs in the training process.
A comparison of training costs is provided in Section 5 of
the supplementary material. Besides, the inconsistency of
pseudo-labels still exists in different views in each scene
28245
and across different scenes and impedes the 3D model to
learn stable visual representations. In this work, we de-
sign a hierarchical intra-modal correlation learning frame-
work that models visual and geometric correlations between
points in each local neighborhood, in each scene, and across
different scenes. Based on the learned multi-scale correla-
tions, we draw the deep features of points with high corre-
lations closer, leading to a concise 3D feature space with
fewer conflicts.
2.2. Scene Context Learning
Scene context learning utilizes semantic and spatial cor-
relations among visual elements to facilitate various scene-
understanding tasks, such as semantic segmentation and ob-
ject detection. Existing methods can be mainly catego-
rized into three directions: convolutional-based, attention-
based, and graph-based methods. Convolutional-based
methods aim to design various sparse convolutional ker-
nels [6, 12â€“14, 27] to progressively integrate multi-scale
point features and help the 3D model capture both local de-
tails and global context. Another research line focuses on
attention mechanisms [9, 10, 17, 26, 29, 35, 38] to model se-
mantic and spatial correlations between different points, en-
abling the 3D model to learn robust contextual point repre-
sentations. Graph-based methods [1, 25, 28, 30, 33, 34] treat
scenes as graphs and utilize graph convolutional networks
to propagate information between different visual items. By
leveraging the inherent structure of the data, these methods
effectively capture the visual correlations within each scene.
In this work, we propose using flexible and adaptive atten-
tion mechanisms on point clouds to capture visual correla-
tions in 3D scenes for label-free 3D semantic segmentation.
3. Method
Following the cross-modal transfer learning framework
in CLIP2Scene [5], we optimize a 3D model using 2D
pseudo-labels generated by MaskCLIP [5, 39]. In Sec. 3.1,
we formulate the cross-modal transfer learning framework,
while in Sec. 3.2, we present our hierarchical scene correla-
tion learning framework that leverages visual and geomet-
ric correlations to build a compact feature space. We also
introduce a feedback distillation module in Sec. 3.3 to in-
corporate the correlation learning ability into the 3D model.
The overall training objective is explained in Sec. 3.4.
3.1. Cross-modal Transfer Learning
Given a scene S, its projected image Iunder certain
camera position, and class prompts {Tc}câˆˆ[1,C], the pre-
trained vision language model maps each image pixel Iij
to feature space fI
ijâˆˆRd, and each class prompt Tcto
text embedding Ë†TcâˆˆRd, where Cis the number of cat-
egories. Subsequently, with given similarity measurementÏˆ, the pixel-wise pseudo label can be defined as lij=
arg max cÏˆ(fI
ij,Ë†Tc). In the meanwhile, a point cloud P
can be sampled from the scene and be processed with a 3D
encoder mapping each point pmtofP
mâˆˆRd. Then we
project point pmback to the image plane of Ifiltered by
depth and transfer the pseudo label of Iijtopm, resulting
in(fP
m,Ë†Tlij), which we call a pair. For simplicity of ex-
pression, we denote fmas the m-th point feature, and lmas
them-th paired pseudo label. With the variation of Sunder
different projection camera positions, we can collect a set of
such pairs denoted as M. In the end, a cross-entropy loss is
used for cross-modal feature alignment as follows,
Lce=âˆ’X
Mlogexp(Ïˆ(fm,Ë†Tlm))P
cexp(Ïˆ(fm,Ë†Tc)). (1)
Denote hm,c=Ïˆ(fm,Ë†Tc)andym,lm=exp(hm,lm)P
cexp(hm,c).
The gradient of m-th item of loss function with respect to
hm,ccan be expressed as
âˆ‚Lm
âˆ‚hm,c=(
ym,câˆ’1, c=lm;
ym,c, c Ì¸=lm.(2)
During the optimization process, this gradient forces hm,lm
to increase and hm,c(cÌ¸=lm)to decrease, driving fmto-
wards Ë†Tlmand away from Ë†Tc(cÌ¸=lm). This results in an
alignment between the point features and text embeddings.
Following CLIP2Scene [5], we use MaskCLIP [39] as
the pre-trained vision language model and an inner product
function Ïˆ=âŸ¨Â·,Â·âŸ©as the similarity measurement. Dur-
ing training, the pre-trained text embeddings remain fixed,
serving as feature anchors for different categories. The loss
function Lceoptimizes the 3D model by pulling point fea-
tures towards corresponding text embeddings according to
the 2D pseudo-labels. However, due to the inherent noise
and inconsistency in pseudo-labels, point features within
the same category may be directed towards different text
embeddings, resulting in a confusing 3D feature space. To
address this, we propose a hierarchical intra-modal corre-
lation learning framework that captures visual and geomet-
ric correlations in 3D scenes hierarchically and helps learn
more consistent point representations in various environ-
ments.
3.2. Hierarchical Scene Correlation Learning
To leverage intra-modal correlations for learning consis-
tent visual features, we design a hierarchical scene corre-
lation learning framework, as shown in Fig. 2, containing
three parts, intra-set label refinement, intra-scene correla-
tion learning, and inter-scene correlation learning.
Intra-set label refinement. Pre-trained on image classifi-
cation tasks, CLIP potentially produces noisy dense predic-
tions and thus hinders the modelâ€™s performance. Concur-
rent work Chen et al. [4] address this inconsistency issue
28246
3D CLIP
ð‘ƒ{ð¼}
Pseudo labels Refined pseudo labels
Point featuresPoint predictions
ViT 1 ViT 1ViT 2Intra -set Intra -scene Inter -scene
Feedback alignmentSampled
predictionsSampled
labels
Projected 2D sets Paired supervision Sampled pointsPoint cloudMulti -view 
images
Sampled points in other scenesSampleFigure 2. Illustration of the hierarchical intra-modal correlation learning framework. First, we use a CLIP model and a 3D model to
generate pixel-wise pseudo-labels and point features. Then we perform intra-modal correlation learning hierarchically. For intra-set label
refinement, we cluster points into geometric consistency sets and adopt a voting mechanism to refine pseudo-labels in each set. We extract
intra-scene correlation by sampling points in each set and processing them via a vision transformer. For inter-scene correlation learning,
we use a cross-scene vision transformer that processes points from multiple scenes. Finally, we align the output features of the 3D model
with the final aggregated point features. Note that only the 3D model is retained for inference.
by employing the Segment Anything model [16]. However,
it introduces considerable additional training costs. In this
work, we choose a more efficient way for label refinement.
Our key insight is that geometric clues like normal smooth-
ness can help identify the same semantics for points in the
same object part. Specifically, following Chen et al. [3],
with a given equivalence relation âˆ¼between points in all the
scenes âˆªSi, we can obtain the quotient set {Gk}=âˆªSi/âˆ¼,
denoted as geometric consistency sets . Then we project
points within each set Gkto the image plane of Iand denote
the projected set as Ë†Gk. In each projected set Ë†Gk, we use a
voting mechanism formulated as Ë†lij= Mode( {lij}IijâˆˆË†Gk),
and replace the original pseudo-labels with this most fre-
quent label, resulting in refined pseudo-labels {Ë†lij}. In
this work, we generate geometric consistency sets using a
normal-based over-segmentation algorithm [11, 20]. Other
complex equivalences, such as topological, semantic, and
functional equivalences, can also be used to generate point
sets. We show the refined pseudo-labels for the point cloud
in Fig. 3. Through intra-set label refinement, we can ef-
ficiently reduce the noise in pseudo-labels and provide lo-
cally consistent supervision during training.
Intra-scene correlation learning. Previous cross-modal
transfer learning methods typically optimize the 3D model
by aligning point features with text embeddings according
to 2D pseudo-labels, as formulated in Eq. 1. Given that the
2D pseudo-labels tend to be noisy and inconsistent, the re-
sulting 3D feature space is often ambiguous. To this end, we
propose to investigate intra-scene correlations to help con-
strain features of points with strong correlations to be closerand construct a more focused feature space. Initially, we
sample a set of points {pi}kin each geometry consistency
setGkand concatenate all the sampled points to formulate
a subset of the input point cloud, noted as PS. The number
of sampled points Miis determined by the ratio of the num-
ber of points Niin each set to the total number of points N,
i.e.,Mi=MÃ—Ni/N. In our experiments, we limit the
total number of sampling points to M= 1024 to manage
GPU memory consumption.
The sampled point features in i-th scene, {fm}pmâˆˆPS
i,
are processed through a transformer block. The m-th output
feature of the transformer block can be formulated as
Ëœfm=fm+X
nwmnvn,
Ë†fm=Ëœfm+ Linear( Ëœfm)
= (W+I)(fm+X
nwmnvn) +b
=fâ€²
m+X
nwmnvâ€²
n+b,(3)
where wmndenotes the attention weight between the m-th
andn-th visual features, vnis the n-th value vector, which
is derived by passing fnthrough a linear projection layer
andWâˆˆRdÃ—dandbâˆˆRdare the weight matrix and
the bias vector of the linear layer. Since bis unrelated to
the gradient for other items, it can be omitted from the loss
function. The cross-entropy loss function is formulated as,
Ë†Lce=âˆ’X
Mlogexp(Ïˆ(fâ€²
m+P
nwmnvâ€²
n,Ë†TË†lm))
P
cexp(Ïˆ(fâ€²m+P
nwmnvâ€²n,Ë†Tc)),(4)
where Ë†lmrepresents the refined pseudo label for point pm.
28247
Denote hm,c=Ïˆ(fâ€²
m,Ë†Tc)anden,c=Ïˆ(vâ€²
n,Ë†Tc). We
getym,lm=exp(hm,lm+P
nwmnen,lm))P
cexp(hm,c+P
nwmnen,c)). The gradient of
m-th item of the loss function with respect to hm,cis the
same as Eq. 2. The gradient for en,ccan be formulated as
âˆ‚Ë†Lm
âˆ‚en,c=(
wmnÂ·(ym,câˆ’1), c=Ë†lm,
wmnÂ·ym,c, c Ì¸=Ë†lm.(5)
The gradient for wmncan be formulated as
âˆ‚Ë†Lm
âˆ‚wmn=(
en,cÂ·(ym,câˆ’1), c=Ë†lm,
en,cÂ·ym,c, c Ì¸=Ë†lm.(6)
During the optimization process, the gradient in Eq. 5 forces
en,Ë†lmto increase and en,c(cÌ¸=Ë†lm)to decrease if wmn>0,
denoting that points with higher correlations should move
toward the same text embedding. The degree of this con-
straint is determined by the value of wmn. The gradient in
Eq. 6 forces wmnto increase if en,Ë†lm>0and decrease if
en,c>0 (cÌ¸=Ë†lm). This ensures that point features are
positioned closer to each other if they surround the same
text embedding and farther from each other if they surround
different text embeddings.
The loss function in Eq. 1 pulls each point feature to-
wards its corresponding text embedding based on pseudo-
labels. This can be misleading when pseudo-labels are
inconsistent, resulting in a scattered feature distribution.
Through our intra-scene correlation learning, the loss func-
tion in Eq. 4 further constrains that, (a) points with stronger
correlations should be close to the same text embedding,
and (b) point features within the same category should be
closer together, assisting in the construction of a more fo-
cused and concise feature space.
Inter-scene correlation learning. To solve the issue of in-
consistent pseudo labels across different scenes , we propose
an inter-scene attention mechanism to learn feature correla-
tions among objects in different scenes. Specifically, we
first include point features from multiple scenes in a train-
ing batch as {fm}pmâˆˆâˆªPS
i. Then we process the batched
features through a transformer block as
{Ë†fm}pmâˆˆâˆªPS
i= Transformer( {fm}pmâˆˆâˆªPS
i),(7)
where the Transformer( Â·)function works the same as
Eq. 3. By integrating inter-scene attention weights {wmn}
between {fm}pmâˆˆPS
iand{fn}pnâˆˆPS
jinto{Ë†fm}pmâˆˆâˆªPS
i,
the gradient formulated in Eq. 5 and 6 promotes the com-
pactness of visual features in different scenes during train-
ing, leading to a stable and consistent feature distribution.
The visualization of intra- and inter-scene attention weights
can be found in Section 4 of the supplementary material.
Our theoretical analysis suggests that integrating correla-
tions within the optimization process can effectively impose
Pseudo labels Refined labelsFigure 3. Visualization of intra-set label refinement. By replacing
the original pseudo-labels with the most frequent labels in each
geometric consistency set, we can effectively reduce the noise.
further constraints on feature compactness among points of
high relevance, leading to a more compact feature space
with less ambiguity. This enables the 3D model to learn
robust features under noisy pseudo-labels.
3.3. Feedback Distillation
To distill the correlation learning capability of our hi-
erarchical framework to the 3D model, we align the output
features of the 3D model with the final aggregated point fea-
tures through a widely used Kullback-Leibler (KL) distance
loss, which can be formulated as
Lkl=MX
m=1KL(fm,Ë†fm), (8)
where Ë†fmism-th aggregated feature and KL(,)is the KL
distance that measures the distribution difference between
two input vectors. This feedback strategy enables the 3D
model to integrate the ability to capture visual correlations,
thereby generating more robust semantic predictions.
3.4. Overall Objective
Our training phase is segmented into three parts: the first
10 epochs involve training the 3D model with Lce. Starting
at epoch 11, we concurrently train the visual transformer by
introducing Ë†Lce, and post-epoch 20, we incorporate Lklfor
feedback distillation. We use the average of the losses at
each stage of the training and the entire training process is
seamless. During the inference stage, we only retain the 3D
model for semantic segmentation.
4. Experiment
4.1. Experiment Setup
Datasets and metrics. We conduct experiments on both
an indoor dataset ScanNet [7] and an outdoor dataset
nuScenes [2]. ScanNet includes 1,603 scanned indoor
scenes, where 1,201 scans are for training, 312 scans are
for validation, and 100 scans are for testing. The nuScenes
dataset contains 1,000 scenes, where 700 scenes are for
training, 150 scenes are for evaluation, and 150 scenes are
28248
Methods YearScanNet mIoU nuScenes mIoU Time Parameter
(%) (%) (GPUÂ·h) (M)
MaskCLIP [39] 2022 14.2 12.8 - -
MaskCLIP+ [39] 2022 21.6 15.3 - -
OpenScene [21] 2023 16.8 14.6 24 15.6
CLIP2Scene [5] 2023 25.6 20.8 4 8.2
Chen et al. [4] â€  2023 33.5 26.8 20 31.7
Ours 2023 36.6 23.0 14 8.2
Table 1. Comparison with previous SOTA label-free 3D semantic segmentation methods on ScanNet and nuScenes datasets. The training
time is measured on the ScanNet dataset. â€ indicates concurrent work.
for testing. We use the mean Intersection of Union (mIoU)
metric to evaluate the semantic segmentation performance,
the training GPU hour to measure the training cost, and the
number of model parameters to evaluate the model size.
Implementation details. We follow CLIP2Scene [5] to use
MinkowskiNet14 [6] and SPVCNN [24] as the 3D models
for ScanNet and nuScenes, respectively. The image features
and text embeddings are generated with MaskCLIP [39],
which is fixed during training. To extract intra-scene and
inter-scene correlations, we utilize two three-layer vision
transformers [9] in our experiments: one for intra-scene
correlation learning and the other for inter-scene correlation
learning. For the 3D model, we use SGD as the optimizer,
while AdamW is employed for the vision transformer. We
adopt the cosine learning rate decay strategy, setting the ini-
tial learning rate at 0.2 for the 3D model and 2e-4 for the vi-
sion transformer. Our framework is developed on PyTorch
and trained on four NVIDIA Tesla V100 GPUs. For Scan-
Net, we train our model for 120 epochs over 3.5 hours. The
batch size used is 8. For nuScenes, we train our model for
30 epochs over 12.5 hours, with a batch size of 8.
4.2. Comparison Methods
We compare our method with state-of-the-art meth-
ods, including MaskCLIP [39], MaskCLIP+ [39], Open-
Scene [21], CLIP2Scene [5], and Chen et al. [4], on the
ScanNet and nuScenes datasets.
MaskCLIP & MaskCLIP+ [39] generate segmentation re-
sults through projecting 2D pixel-wise predictions to 3D
points according to camera parameters and depth maps. The
2D predictions are derived by evaluating the similarities be-
tween the CLIP visual features and text embeddings.
OpenScene [21] combines multi-view image features with
point features and evaluates their similarity with CLIP text
embeddings to make predictions. For a fair comparison,
the reported results are produced by using the same image
encoder as used in MaskCLIP.
CLIP2Scene [5] achieves 3D segmentation by utilizing se-
mantic consistency regularization and spatial-temporal con-
sistency regularization to align point features with corre-
sponding text embeddings and concentrating point features
in each spatial-temporal interval.Chen et al. [4] adopt SAMâ€™s [16] segmentation masks to re-
fine noisy pseudo-labels from MaskCLIP, thereby improv-
ing the 3D modelâ€™s performance.
Our approach hierarchically utilizes intra-modal corre-
lations to promote the compactness between point features
and thereby reduces prediction ambiguity. Our principal in-
novation capitalizes on intra-modal feature alignment and
does not conflict with the concurrent work of Chen et al. [4],
which leverages cross-modal feature alignment to learn ro-
bust point features from refined pseudo-labels. Finally,
we achieve state-of-the-art results without additional priors,
like the feature fusion strategy in OpenScene or the SAM
model used by Chen et al.
4.3. Label-free 3D Semantic Segmentation
We demonstrate the effectiveness of our hierarchical
intra-modal correlation learning framework on both indoor
dataset ScanNet and outdoor dataset nuScenes. In Table 1,
we report the semantic segmentation metric â€˜mIoUâ€™ on
ScanNet and nuScenes, as well as training time and model
size for models trained on ScanNet. On the ScanNet dataset,
our method outperforms all the state-of-the-art methods by
achieving 36.6% mIoU, with a significant gain of 11.0%
mIoU compared with CLIP2Scene [5] and 3.1% mIoU gain
compared with Chen et al. [4]. Furthermore, the training ex-
penses and model size of our methods are notably minimal.
On the nuScenes dataset, we achieve 23.0% mIoU with a
gain of 2.2% mIoU compared with CLIP2Scene. All these
results show the superiority of our hierarchical intra-modal
correlation learning framework. In Fig. 4, we present the
qualitative results of our method and CLIP2Scene on the
validation set of ScanNet. Note that Chen et al. [4] have
not released code yet, so their visualization results are not
presented. Compared with CLIP2Scene, our method pro-
duces more consistent predictions, such as the wall in the
first row, the table in the second row, and the sofa in the last
row. Besides, our method can make more accurate recog-
nition for categories with limited training samples, like the
sink in the first row and the cabinet in the second row. By
employing our hierarchical intra-model correlation learning
framework, the 3D model can generate more coherent and
accurate semantic segmentation results.
28249
RGB Input CLIP2Scene Ours Ground Truth
Floor Bed Sofa Door Window
Counter Desk Curtain Refrig Toilet Sink TableWall Cabinet Chair Bookslf
Picture OtherFigure 4. Qualitative comparison for semantic segmentation of our method and CLIP2Scene [5] on the ScanNet dataset. More visualization
results are reported in the supplementary material.
Pseudo labels Refined labels
 Ground truth
Figure 5. Visualization of pseudo-labels on nuScenes dataset.
We also observe that the improvement of our method on
the nuScenes dataset is not as much as that on the ScanNet
dataset. This is because the pseudo-labels generated from
CLIP [22, 39] in the nuScenes dataset are notably deficient
in quality. As shown in Fig. 5, despite the fact that intra-set
refinement can improve the local consistency, the overall la-
bel quality from CLIP is so poor that erroneous labels con-
stitute the majority. Consequently, the refined labels may
not accurately depict the true categories of the points. In the
meanwhile, Chen et al. [4] implement an additional step of
training a 2D semantic segmentation model. Although this
step is costly, it effectively corrects these erroneous labels,
contributing to a more substantial improvement.
4.4. Ablation Study
To evaluate the effectiveness of different components
of our hierarchical intra-modal correlation learning frame-
work, we conduct a series of ablation experiments on the
ScanNet dataset and results are shown in Table 2. All exper-EXP IntraSet IntraScene InterScene FB mIoU
I - - - - 28.8
II âœ“ - - - 31.1 (+2.3)
III âœ“ âœ“ - - 32.1 (+3.3)
IV âœ“ âœ“ âœ“ -36.1 (+7.3)
V âœ“ âœ“ âœ“ âœ“ 36.6 (+7.8)
Table 2. Ablation study of our hierarchical intra-modal correlation
learning framework on the ScanNet validation set. â€˜FBâ€™ denotes
the feedback distillation mechanism.
iments are implemented on four NVIDIA V100 GPUs with
a batch size of 4 over 120 training epochs. In EXP I, we re-
produce CLIP2Scene [5] as our baseline model, achieving
a mIoU of 28.8% on the ScanNet validation set.
Ablation of intra-set label refinement. In EXP II, we clus-
ter the input point cloud into geometric consistent sets and
perform label refinement in each set, leading to a mIoU of
31.1% and a gain of 2.3% mIoU compared to EXP I. In
Fig. 3, we demonstrate that intra-set label refinement can
produce cleaner pseudo-labels, thus improving the coher-
ence of the 3D modelâ€™s predictions.
Ablation of intra-scene correlation learning. In EXP III,
we incorporate the intra-scene correlation learning module
based on EXP II, resulting in a higher gain of 3.3% mIoU.
As shown in the second column of Fig. 7, the intra-scene
correlation learning enables the model to learn a more con-
28250
26.42 28.42 28.42 28.75 30.06 34.34 35.16 36.60 
25.0027.0029.0031.0033.0035.0037.0039.00
30 80 100 120mIoU(%)
EpochOurs
CLIP2SceneFigure 6. Semantic segmentation results of our method and
CLIP2Scene [5] on ScanNet across different training epochs.
centrated and concise feature space, thereby reducing con-
fusion between different categories.
Ablation of inter-scene correlation learning. Building on
EXP III, we additionally employ the inter-scene correlation
learning module in EXP IV , yielding a gain of 7.3% mIoU.
As shown in the third row of Fig. 7, this strategy further con-
strains that the feature spaces of multiple scenes should be
consistent, assisting the model in generating stable feature
distributions in different scenes.
Ablation of feedback distillation. In EXP V , we imple-
ment the feedback distillation mechanism and achieve a
mIoU of 36.6%, with a total gain of 7.8% mIoU compared
with CLIP2Scene. By aligning the feature space, we can
transfer the correlation learning capacity into the 3D model,
further enhancing its performance.
Ablation of training epochs. We compare the segmenta-
tion performance of our method with CLIP2Scene [5] as
the number of training epochs increases from 30 to 120.
The results are presented in Fig. 6. Our method con-
sistently outperforms CLIP2Scene across various training
epochs, achieving a maximum mIoU gain of 7.8% at the
120th epoch. Additionally, our method shows a better up-
ward trend with a 1.4% mIoU increase from 100 to 120
epochs, compared to only a 0.3% mIoU increase exhibited
by CLIP2Scene across the same epoch range.
Additional ablation studies of geometric consistency sets
are detailed in Section 3 of the supplementary material.
4.5. Feature Space Visualization
Our hierarchical intra-modal correlation learning frame-
work aims to create a concise feature space. To validate
this, we compare the t-SNE visualizations of the feature
space from our method and CLIP2Scene [5] in Fig. 7. The
first two rows show the t-SNE map of two unique scenes.
We present the feature spaces separately for CLIP2Scene,
our method that incorporates intra-scene correlation learn-
ing, and our method that integrates inter-scene correlation
learning. Compared to CLIP2Scene, both the intra- and
inter-scene strategies assist the 3D model in learning a more
focused feature space with fewer conflicts, as indicated by
the dotted box. This verifies the effectiveness of introduc-
ing point correlations to help concentrate point features. In
the last row of Fig. 7, we concatenate point features from
CLIP2Scene Ours Intra -Scene Ours Inter -SceneCabinet
Bed
Door
CounterDesk
Refrig
Toilet
Table
Window
S curtain
Bathtub
Chair
Sofa
Sink
OtherFigure 7. Feature space visualization. We visualize the learned
point features of our method and CLIP2Scene [5] using t-SNE
maps. The first two rows are t-SNE maps of two distinct scenes
and the last row shows the t-SNE map of the concatenated features
from these two scenes. Refrig andS curtain are the abbreviations
forrefrigerator andshower curtain , respectively.
the two aforementioned scenes to verify the consistency of
feature distributions across different scenes. Compared to
CLIP2Scene and the intra-scene correlation learning strat-
egy, our inter-scene setting results in a more stable feature
space across diverse scenes. This indicates that incorporat-
ing global correlations across multiple scenes can enhance
the alignment of feature distributions in different scenes and
help the 3D model produce more robust predictions.
5. Limitations and Future Work
Our work demonstrates that exploring intra-modal cor-
relations can help the 3D model training for label-free 3D
semantic segmentation. However, we have not yet explored
intra-modal correlations within images and texts. By us-
ing correlations in images, we can create a more consistent
2D feature space, leading to more accurate and coherent
pseudo-labels. Similarly, investigating correlations within
texts enables us to use more precise and detailed descrip-
tions, offering rich guidance for cross-modal alignment. In
the future, we will extend our hierarchical intra-modal cor-
relation learning framework to images and texts to achieve
better label-free 3D semantic segmentation performance.
6. Conclusion
In this paper, we introduce a hierarchical intra-modal
correlation learning framework for label-free 3D seman-
tic segmentation. Our method hierarchically utilizes visual
and geometric correlations at three scales including intra-
set, intra-scene, and inter-scene, to help mitigate noise in
pseudo-labels and concentrate point features, resulting in a
focused and concise 3D feature space with fewer conflicts.
Experiments on both indoor and outdoor datasets demon-
strate the superiority of our method. Comprehensive theo-
retical analysis and extensive ablation studies further sup-
port the effectiveness of our framework.
28251
References
[1] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir,
Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene
graph: A structure for unified semantics, 3d space, and cam-
era. In Proceedings of the IEEE/CVF international confer-
ence on computer vision , pages 5664â€“5673, 2019. 3
[2] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In CVPR , 2020. 5
[3] Nenglun Chen, Lei Chu, Hao Pan, Yan Lu, and Wenping
Wang. Self-supervised image representation learning with
geometric set consistency. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 19292â€“19302, 2022. 4
[4] Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun
Chen, ZHU Xinge, Yuexin Ma, Tongliang Liu, and Wen-
ping Wang. Towards label-free scene understanding by vi-
sion foundation models. In Thirty-seventh Conference on
Neural Information Processing Systems , 2023. 1, 2, 3, 6, 7
[5] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu,
Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wen-
ping Wang. Clip2scene: Towards label-efficient 3d scene
understanding by clip. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
7020â€“7030, 2023. 1, 2, 3, 6, 7, 8
[6] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d
spatio-temporal convnets: Minkowski convolutional neural
networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 3075â€“3084,
2019. 3, 6
[7] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias NieÃŸner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proc. Computer Vision and Pattern Recognition (CVPR),
IEEE , 2017. 5
[8] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,
Song Bai, and Xiaojuan Qi. Pla: Language-driven open-
vocabulary 3d scene understanding. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7010â€“7019, 2023. 1, 2
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 3, 6
[10] Nico Engel, Vasileios Belagiannis, and Klaus Dietmayer.
Point transformer. IEEE access , 9:134826â€“134840, 2021.
3
[11] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient
graph-based image segmentation. International journal of
computer vision , 59:167â€“181, 2004. 4
[12] Ben Graham. Sparse 3d convolutional neural networks.
arXiv preprint arXiv:1505.02890 , 2015. 3
[13] Benjamin Graham, Martin Engelcke, and Laurens Van
Der Maaten. 3d semantic segmentation with submani-fold sparse convolutional networks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 9224â€“9232, 2018. 3
[14] Benjamin Graham and Laurens van der Maaten. Sub-
manifold sparse convolutional networks. arXiv preprint
arXiv:1706.01307 , 2017. 3
[15] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and Yin-
jie Lei. Context-aware alignment and mutual masking for 3d-
language pre-training. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
10984â€“10994, 2023. 1, 2
[16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C Berg, Wan-Yen Lo, et al. Segment any-
thing. arXiv preprint arXiv:2304.02643 , 2023. 2, 4, 6
[17] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang
Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-
former for 3d point cloud segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8500â€“8509, 2022. 3
[18] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-
Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, and
Winston H Hsu. Learning from 2d: Contrastive pixel-to-
point knowledge transfer for 3d pretraining. arXiv preprint
arXiv:2104.04687 , 2021. 2
[19] Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie,
Masayoshi Tomizuka, Kurt Keutzer, and Shanghang Zhang.
Open-vocabulary point-cloud object detection without 3d
annotation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 1190â€“
1199, 2023. 1, 2
[20] Jeremie Papon, Alexey Abramov, Markus Schoeler, and Flo-
rentin Worgotter. V oxel cloud connectivity segmentation-
supervoxels for point clouds. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pages 2027â€“2034, 2013. 4
[21] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea
Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al.
Openscene: 3d scene understanding with open vocabularies.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 815â€“824, 2023. 1, 2,
6
[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748â€“8763. PMLR, 2021. 1, 2, 7
[23] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre
Boulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidar
self-supervised distillation for autonomous driving data. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 9891â€“9901, 2022. 2
[24] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji Lin,
Hanrui Wang, and Song Han. Searching efficient 3d architec-
tures with sparse point-voxel convolution. In European con-
ference on computer vision , pages 685â€“702. Springer, 2020.
6
28252
[25] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico
Tombari. Learning 3d semantic scene graphs from 3d in-
door reconstructions. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
3961â€“3970, 2020. 3
[26] Peng-Shuai Wang. Octformer: Octree-based transformers
for 3d point clouds. arXiv preprint arXiv:2305.03045 , 2023.
3
[27] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,
and Xin Tong. O-cnn: Octree-based convolutional neu-
ral networks for 3d shape analysis. ACM Transactions On
Graphics (TOG) , 36(4):1â€“11, 2017. 3
[28] Ziqin Wang, Bowen Cheng, Lichen Zhao, Dong Xu, Yang
Tang, and Lu Sheng. Vl-sat: Visual-linguistic semantics
assisted training for 3d semantic scene graph prediction in
point cloud. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 21560â€“
21569, 2023. 3
[29] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-
shuang Zhao. Point transformer v2: Grouped vector atten-
tion and partition-based pooling. Advances in Neural Infor-
mation Processing Systems , 35:33330â€“33342, 2022. 3
[30] Danfei Xu, Yuke Zhu, Christopher B Choy, and Li Fei-Fei.
Scene graph generation by iterative message passing. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition , pages 5410â€“5419, 2017. 3
[31] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart Â´Ä±n-Mart Â´Ä±n,
Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles,
and Silvio Savarese. Ulip: Learning a unified representation
of language, images, and point clouds for 3d understanding.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 1179â€“1189, 2023. 1,
2
[32] Xu Yan, Jiantao Gao, Chaoda Zheng, Chao Zheng, Ruimao
Zhang, Shuguang Cui, and Zhen Li. 2dpass: 2d priors
assisted semantic segmentation on lidar point clouds. In
European Conference on Computer Vision , pages 677â€“695.
Springer, 2022. 2
[33] Jingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou,
Wayne Zhang, and Ziwei Liu. Panoptic scene graph gen-
eration. In European Conference on Computer Vision , pages
178â€“196. Springer, 2022. 3
[34] Jianwei Yang, Jiasen Lu, Stefan Lee, Dhruv Batra, and Devi
Parikh. Graph r-cnn for scene graph generation. In Proceed-
ings of the European conference on computer vision (ECCV) ,
pages 670â€“685, 2018. 3
[35] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu,
Hao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo.
Swin3d: A pretrained transformer backbone for 3d indoor
scene understanding. arXiv preprint arXiv:2304.06906 ,
2023. 3
[36] Junbo Yin, Dingfu Zhou, Liangjun Zhang, Jin Fang, Cheng-
Zhong Xu, Jianbing Shen, and Wenguan Wang. Proposal-
contrast: Unsupervised pre-training for lidar-based 3d ob-
ject detection. In European Conference on Computer Vision ,
pages 17â€“33. Springer, 2022. 1, 2
[37] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
peng Miao, Bin Cui, Yu Qiao, Peng Gao, and HongshengLi. Pointclip: Point cloud understanding by clip. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 8552â€“8562, 2022. 1, 2
[38] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and
Vladlen Koltun. Point transformer. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 16259â€“16268, 2021. 3
[39] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free
dense labels from clip. In European Conference on Com-
puter Vision , pages 696â€“712. Springer, 2022. 1, 2, 3, 6, 7
[40] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyao Zeng,
Shanghang Zhang, and Peng Gao. Pointclip v2: Adapting
clip for powerful 3d open-world learning. arXiv preprint
arXiv:2211.11682 , 2022. 1, 2
28253
