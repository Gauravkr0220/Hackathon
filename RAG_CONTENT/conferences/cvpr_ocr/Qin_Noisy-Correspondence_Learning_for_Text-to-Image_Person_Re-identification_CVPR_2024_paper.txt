Noisy-Correspondence Learning for Text-to-Image Person Re-identification
Yang Qin1Yingke Chen2Dezhong Peng1,4,5Xi Peng1Joey Tianyi Zhou3Peng Hu1*
1College of Computer Science, Sichuan University, Chengdu, 610095, China.
2Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne NE1 8ST, UK.
3Centre for Frontier AI Research (CFAR) and Institute of High Performance Computing (IHPC), A*STAR, Singapore.
4Sichuan Newstrong UHD Video Technology Co., Ltd., Chengdu 610095, China.
5Chengdu Ruibei Yingte Information Technology Company Ltd., Chengdu 610065, China.
Abstract
Text-to-image person re-identification (TIReID) is a
compelling topic in the cross-modal community, which aims
to retrieve the target person based on a textual query. Al-
though numerous TIReID methods have been proposed and
achieved promising performance, they implicitly assume the
training image-text pairs are correctly aligned, which is not
always the case in real-world scenarios. In practice, the
image-text pairs inevitably exist under-correlated or even
false-correlated, a.k.a noisy correspondence (NC), due to
the low quality of the images and annotation errors. To ad-
dress this problem, we propose a novel Robust Dual Embed-
ding method (RDE) that can learn robust visual-semantic
associations even with NC. Specifically, RDE consists of
two main components: 1) A Confident Consensus Division
(CCD) module that leverages the dual-grained decisions of
dual embedding modules to obtain a consensus set of clean
training data, which enables the model to learn correct and
reliable visual-semantic associations. 2) A Triplet Align-
ment Loss (TAL) relaxes the conventional Triplet Ranking
loss with the hardest negative samples to a log-exponential
upper bound over all negative ones, thus preventing the
model collapse under NC and can also focus on hard-
negative samples for promising performance. We conduct
extensive experiments on three public benchmarks, namely
CUHK-PEDES, ICFG-PEDES, and RSTPReID, to evaluate
the performance and robustness of our RDE. Our method
achieves state-of-the-art results both with and without syn-
thetic noisy correspondences on all three datasets. Code is
available at https://github.com/QinYang79/RDE.
1. Introduction
Text-to-image person re-identification (TIReID) [21, 24,
38] aims to understand the natural language descriptions
*Corresponding author: Peng Hu (penghu.ml@gmail.com).
A man with black 
hair, dressed in a 
grey and black c-
oat, black trouse-
rs and  white  ca-
nvas  shoes,  was 
walking  happily 
with his hands in 
his pockets 
The man is w-
alking  with  a 
black bag. He 
wears  a  brig-
ht  jacket  and 
a pair of  blac-
k trousers. His 
shoes  is  dark 
tanned.
(a) Clean correspondence (b) Noisy correspondence
A man 
wearing a 
blue and 
white stripe 
tank top, a 
pair of green 
pants and a 
pair of pink 
shoes.
A man 
wearing a 
blue and 
white stripe 
tank top, a 
pair of green 
pants and a 
pair of pink 
shoes.
Figure 1. The illustration of noisy correspondence. The fig-
ure shows an example of the NC problem, which occurs when
the image-text pairs are wrongly aligned, i.e., false positive pairs
(FPPs). Since the model does not know which pairs are noisy in
practice, they will unavoidably degrade the performance by incor-
rect supervision information. As seen in the figure, (a) the clean
image-text pair is semantically matched, while (b) the noisy pair is
not, which would cause the cross-modal model to learn erroneous
visual-textual associations. Note that both examples in (a) and (b)
are from and actually exist in the RSTPReid dataset [50].
to retrieve the matched person image from a large gallery
set. This task has received increasing attention from
both academic and industrial communities recently, e.g.,
finding/tracking suspect/lost persons in a surveillance sys-
tem [10, 40]. However, TIReID remains a challenging
task due to the inherent heterogeneity gap across different
modalities and appearance attribute redundancy.
To tackle these challenges, most of the existing methods
explore global- and local-matching alignment to learn ac-
curate similarity measurements for person re-identification.
To be specific, some global-matching methods [38, 41, 48]
leverage vision/language backbones to extract modality-
specific features and employ contrastive learning to achieve
global visual-semantic alignments. To capture fine-grained
information, some local-matching methods [22, 29, 35,
39] explicitly align local body regions to textually de-
scribed entities/objectives to improve the discriminability
of pedestrian features. Recently, some works [16, 21, 42]
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
27197
propose to exploit visual/semantic knowledge learned by
the pre-trained models, such as BERT [5], ViT [9], and
CLIP [34], and achieve explicit global alignments or dis-
cover more fine-grained local correspondence, thus boost-
ing the re-identification performance. Although these meth-
ods achieve remarkable progress, they implicitly assume
that all training image-text pairs are aligned correctly.
In reality, this assumption is hard or even impossible to
hold due to the person‚Äôs pose, camera angle, illumination,
and other inevitable factors in images, which may result
in some inaccurate/mismatched textual descriptions of im-
ages (see Figure 1), e.g., the RSTPReid dataset [50]. More-
over, we observe that excessive such imperfect/mismatched
image-text pairs would cause an overfitting problem and de-
grade the performance of existing TIReID methods shown
in Figure 5. Based on the observation, in this paper, we
reveal and study a new problem in TIReID, i.e., noisy cor-
respondence (NC). Different from noisy labels, NC refers
to the false correspondences of image-text pairs in TIReID,
i.e., False Positive Pairs (FPPs): some negative image-text
pairs are used as positive ones for cross-modal learning. In-
evitably, FPPs will misguide models to overfit noisy super-
vision and collapse to suboptimal solutions due to the mem-
orization effect [1] of Deep Neural Networks (DNNs).
To address the NC problem, we propose a Robust Dual
Embedding method (RDE) for TIReID in this paper, which
benefits from an effective Confident Consensus Division
mechanism (CCD) and a novel Triplet Alignment Loss
(TAL). Specifically, CCD fuses the dual-grained decisions
to consensually divide the training data into clean and noisy
sets, thus providing more reliable correspondences for ro-
bust learning. To diversify the model grain, the basic
global embeddings (BGE) and token selection embeddings
(TSE) are presented for coarse-grained and fine-grained
cross-modal interactions respectively, thus capturing visual-
semantic associations comprehensively. Different from the
widely-used Triplet Ranking loss with the hardest negatives,
our TAL relaxes the similarity learning from the hardest
negative samples to all negative ones by applying an up-
per bound, which brings a stable solution for the collapse
of training under NC while also benefiting from the hardest
negatives mining to achieve promising performance. As a
result, our RDE can achieve robustness against NC thanks
to the proposed reliable supervision and stable triplet loss.
The contributions and innovations of this paper are summa-
rized as follows:
‚Ä¢ We reveal and study a new and ubiquitous problem in
TIReID, termed noisy correspondence (NC). Different
from class-level noisy labels, NC refers to erroneous cor-
respondences in the person-description pairs that can mis-
lead the model to learn incorrect visual-semantic associ-
ations. To the best of our knowledge, this paper could be
the first work to explore this problem in TIReID.‚Ä¢ We propose a robust method, termed RDE, to mitigate
the adverse impact of NC through the proposed Confi-
dent Consensus Division (CCD) and novel Triplet Align-
ment Loss (TAL). By using CCD and TAL, RDE can ob-
tain convincing consensus pairs and reduce the mislead-
ing risks in training, thus embracing robustness against
NC.
‚Ä¢ Extensive experiments on three public image-text person
benchmarks demonstrate the robustness and superiority
of our method. Our method achieves the best perfor-
mance both with and without synthetic noisy correspon-
dence on all three datasets.
2. Related Work
2.1. Text-to-Image Person Re-identification
Text-to-image person re-identification (TIReID) is a novel
and challenging task that aims to match a person image with
a given natural language description [2‚Äì4, 24, 25, 28, 36,
37, 43, 48]. Existing TIReID methods could be roughly
classified into two groups according to their alignment lev-
els,i.e.,global-matching methods [38, 49, 50] and local-
matching methods [13, 35, 39]. The former try to learn
cross-modal embeddings in a common latent space by em-
ploying textual and visual backbones with a matching loss
(e.g., CMPM/C loss [48] and Triplet Ranking loss [11]) for
TIReID. However, these methods mainly focus on global
features while ignoring the fine-grained interactions be-
tween local features, which limits their performance im-
provement. To achieve fine-grained interactions, some of
the latter methods explore explicit local alignments between
body regions and textual entities for more refined align-
ments. However, these methods require more computa-
tional resources due to the complex local-level associations.
Recently, inspired and benefited from vision-language pre-
training models [34], some methods [16, 21, 42] expect
to use the learned rich alignment knowledge of pre-trained
models for local- or global-alignments. Although these
methods achieve promising performance, almost all of them
implicitly assume that all input training pairs are correctly
aligned, which is hard to meet in practice due to the ubiq-
uitous noise. In this paper, we address the inevitable and
challenging noisy correspondence problem in TIReID.
2.2. Learning with Noisy Correspondence
As a special learning paradigm with noisy labels [12,
23, 27] in multi-modal/view community [18, 31, 31, 32,
45], the studies for noisy correspondence (NC) have re-
cently attracted more and more attention in various tasks,
e.g., video-text retrieval [47], visible-infrared person re-
identification [26, 44, 46], and image-text matching [20,
30], which means that the negative pairs are wrongly treated
as positive ones, i.e., false positive pairs (FPPs). To handle
27198
‚ãÆImage (Batch)[SOS] This person stands with her weight on her left leg. She wears all dark clothing and carries a brown sack slung over her right shoulder. [ESO]Text (Batch)
ùëì+Token 1Token 2Token 3Token ùëÅ!‚ãÆ[ CLS ][ SOS ]Token 1Token 2Token 3Token ùëÅ‚ãÆ[ EOS ]Token  SelectionToken  Selection
ùíó"#$ùíï"#$
TSEBGE
(a) Cross-modal Embedding ModelConfident Consensus Division
ùêºùëá&ùëá'ùëá'ùëá'ùëá'ùëá'ùëá'TRLTALTriplet Alignment LossùêºData with NCsClean pairUncertain pairNoisy pairGMM with BGEGMM with TSE
(b) Robust Similarity Learningùëá'ùëá'ùëá'ùëá'ùëá'ùëá'ùëá&ùëöùëö
Token EmbeddingTransformer12xLiner ProjectionTransformer12xùëì%Consensus processFigure 2. The overview of our RDE. (a) is the illustration of the cross-modal embedding model used in RDE, which consists of basical
global embedding (BGE) and token selection embedding (TSE) modules with different granularity. By integrating them, RDE can capture
coarse-grained cross-modal interactions while selecting informative local token features to encode more fine-grained representations for
a more accurate similarity. (b) shows the core of RDE to achieve robust similarity learning, which consists of Confident Consensus
Division (CCD) and Triplet Alignment Loss (TAL). CCD performs consensus division to obtain confident clean training data, thus avoiding
misleading from noisy pairs. Unlike traditional Triplet Ranking Loss (TRL) [11], TAL exploits an upper bound to consider all negative
pairs, thus embracing more stable learning.
this problem, numerous methods are proposed to learn with
NC, which can be broadly categorized into sample selec-
tion [15, 20, 47] and robust loss functions [19, 30, 33, 44].
The former commonly leverage the memorization effect of
DNNs [1] to gradually distinguish the noisy data, thus pay-
ing more attention to clean data while less attention to noisy
data. Differently, the latter methods aim to develop noise-
tolerance loss functions to improve the robustness of model
training against NC. Although the aforementioned methods
achieve promising performance in various tasks, they are
not specifically designed for TIReID and may be inefficient
or ineffective in person re-identification. In this paper, we
propose a well-designed method to tackle the NC problem
in TIReID, which not only performs superior in noisy sce-
narios but also achieves promising performance in ordinary
scenarios.
3. Methodology
3.1. Problem Statement
The purpose of TIReID is to retrieve a pedestrian im-
age from the gallery set that matches the given textual
description. For clarity, we represent the gallery set as
V={Ii, yp
i, yv
i}Nv
i=1and the corresponding text set as
T={Ti, yv
i}Nt
i=1, where Nvis the number of images,
Ntis the number of texts, yp
i‚àà Y p={1,¬∑¬∑¬∑, C}is
the class label (person identify), Cis the number of iden-
tifies, and yv
i‚àà Y v={1,¬∑¬∑¬∑, Nv}is the image label.
The image-text pair set used in TIPeID can be defined asP={(Ii, Ti), yv
i, yp
i}N
i=1, where the cross-modal samples
of each pair have the same image label yv
iand class label yp
i.
We define a binary correspondence label lij‚àà {0,1}to in-
dicate the matched degree of any image-text pair. If lij= 1,
the pair (Ii, Tj)is matched (positive pair), otherwise it is
not (negative pair). In practice, due to ubiquitous annota-
tion noise, some unmatched pairs (lij= 0) are wrongly
labeled as matched (lij= 1) , resulting in noisy correspon-
dences (NCs) and performance degradation. To handle NC
for robust TIReID, we present an RDE that leverages the
Confident Consensus Division (CCD) and Triplet Align-
ment Loss (TAL) to mitigate the negative impact of label
noise.
3.2. Cross-modal Embedding Model
In this section, we describe the cross-modal model used
in our RDE. Following previous work [21], we utilize
the visual encoder fvand textual encoder ftof the pre-
trained model CLIP as modality-specific encoders to obtain
token representations and implement cross-modal interac-
tions through two embedding modules.
3.2.1 Token Representations
Give an input image Ii‚àà V, we use the visual encoder fv
of CLIP to tokenize the image into a discrete token rep-
resentation sequence with a length of N‚ó¶+ 1,i.e.,Vi=
fv(Ii) ={vi
g,vi
1,vi
2,¬∑¬∑¬∑,vi
N‚ó¶}‚ä§‚ààR(N‚ó¶+1)√ód, where d
is the dimensionality of the shared latent space. These fea-
27199
tures include an encoded feature vi
gof the [ CLS] token and
patch-level local features {vi
j}N‚ó¶
j=1ofN‚ó¶fixed-sized non-
overlapping patches of Ii, wherein vi
gcan represent the
global representation. For an input text Ti‚àà T , we ap-
ply the textual encoder ftof CLIP to obtain global and lo-
cal representations. Specifically, following IRRA [21], we
first tokenize the input text Tiusing lower-cased byte pair
encoding (BPE) with a 49,152 vocab size into a token se-
quence. The token sequence is bracketed with [ SOS] and
[EOS] tokens to represent the beginning and end of the se-
quence. Then, we feed the token sequence into ftto obtain
the features Ti={ti
s,ti
1,¬∑¬∑¬∑,ti
N‚ãÑ,ti
e}‚ä§‚ààR(N‚ãÑ+2)√ód,
where ti
sandti
eare the features of [ SOS] and [ EOS] tokens
and{vi
j}N‚ãÑ
j=1are the word-level local features of N‚ãÑword
tokens of text Ti. Generally, the ti
e‚ààRd√ó1can be regarded
as the sentence-level global feature of Ti.
3.2.2 Dual Embedding Modules
To measure the similarity between any image-text pair
(Ii, Tj), we can directly use the global features of [ CLS]
and [ EOS] tokens to compute the Basic Global Embed-
ding (BGE) similarity by the cosine similarity, i.e.,Sb
ij=
vi
g‚ä§tj
e
‚à•vi
g‚à•‚à•tj
e‚à•, where the global features represent the
global embedding representations of two modalities. How-
ever, optimizing the BGE similarities alone may not capture
the fine-grained interactions between two modalities, which
will limit performance improvement. To address this issue,
we exploit the local features of informative tokens to learn
more discriminative embedding representations, thus min-
ing the fine-grained correspondences. In CLIP, the global
features of the tokens ([ CLS] and [ EOS]) are obtained by
a weighted aggregation of all local token features. These
weights reflect the correlation between the global token and
each local token. Following previous methods [42, 51], we
could select the informative tokens based on these correla-
tion weights to aggregate local features for a more represen-
tative global embedding.
In practice, these correlation weights can be obtained
directly in the self-attention map of the last Transformer
blocks of fvandft, which reflects the relevance among
the input 1 +N‚ó¶(or2 +N‚ãÑ) tokens. Given the output
self-attention map Av
i‚ààR(1+N‚ó¶)√ó(1+N‚ó¶)of image Ii, the
correlation weights between global token and local tokens
are{av
i,j}N‚ó¶
j=1=av
i=Av
i[0,1 :N‚ó¶+ 1]‚ààRN‚ó¶. Similarly,
for text Ti, the correlation weights are {at
i,j}N‚ãÑ
j=1=at
i=
At
i[0,1 :N‚ãÑ+ 1]‚ààRN‚ãÑ, where At
i‚ààR(2+N‚ãÑ)√ó(2+N‚ãÑ)
is the output self-attention map for text Ii. Then, we select
a proportion ( TopK ) of the corresponding token features
with higher scores for embedding. Specifically, for Ii, the
selected token sequences and correlation weights are reor-
ganized as Vs
i={vi
j}j‚ààKv
iandÀÜav
i={av
i,j}j‚ààKv
i, whereKv
i‚ààR‚åäRN‚ó¶‚åãis the set of indices for the selected local
tokens of IiandRis the selection ratio. For text Ti, the
selected token sequences and correlation weights are also
reorganized as Ts
i={ti
j}j‚ààKt
iandÀÜat
i={at
i,j}j‚ààKt
i,
where Kt
i‚ààRmin(‚åäRN‚Ä≤
‚ãÑ‚åã,N‚ãÑ)is the set of indices for the
selected local tokens of Ti.N‚Ä≤
‚ãÑis the maximum input se-
quence length of ft. For IiandTi, we perform an em-
bedding transformation on these selected token features to
obtain subtle representations, instead of using complex fine-
grained correspondence discovery used in CFine [42]. The
transformation is performed by an embedding module like
the residual block [17], as follows:
vi
tse=MaxPool (MLP (ÀÜVs
i) +FC(ÀÜVs
i)),
ti
tse=MaxPool (MLP (ÀÜTs
i) +FC(ÀÜTs
i)),(1)
where MaxPool (¬∑)is the max-pooling function, MLP (¬∑)
is a multi-layer perceptron (MLP) layer, FC(¬∑)is a linear
layer, ÀÜVs
i=L2Norm (Vs
i), and ÀÜTs
i=L2Norm (Ts
i).
L2Norm (¬∑)is the ‚Ñì2-normalization function to normalize
features. Finally, for any pair (Ii, Tj), we compute the co-
sine similarity St
ijbetween vi
tseandtj
tseas the Token Se-
lection Embedding (TSE) similarity to measure the cross-
modal matching degree for auxiliary training and inference.
3.3. Robust Similarity Learning
In this section, we detail how we use the image-text sim-
ilarities computed by the dual embedding modules for ro-
bust TIReID, which involves Confident Consensus Division
(CCD) and Triplet Alignment Loss (TAL).
3.3.1 Confident Consensus Division
To alleviate the negative impact of NC, the key is to filter
the possible noisy pairs in the training data, which directly
avoids false supervision information. Some previous work
in learning with noisy labels [14, 20, 23] are inspired by the
memorization effect [1] of DNNs to perform filtrations, i.e.,
the clean (easy) data tend to have a smaller loss value than
that of noisy (hard) data in early training. Based on this,
we can exploit the two-component Gaussian Mixture Model
(GMM) to fit the per-sample loss distributions computed by
the predictions of BGE and TSE to further identify the noisy
pairs in the training data. Specifically, given a cross-modal
model M, we first define the per-sample loss as:
‚Ñì(M,P) ={‚Ñìi}N
i=1=
L(Ii, Ti)	N
i=1, (2)
where Lis the loss function for pair (Ii, Ti)‚àà P to bring
them closer in the shared latent space. In our method, L
is the proposed Ltaldefined in Equation (5). Then, the per-
sample loss is fed into the GMM to separate clean and noisy
data, i.e., assigning the Gaussian component with a lower
27200
mean value as a clean set and the other as a noisy one,
respectively. Following [20, 23], we use the Expectation-
Maximization algorithm to optimize the GMM and com-
pute the posterior probability p(k|‚Ñìi) =p(k)p(‚Ñìi|k)/p(‚Ñìi)
for the i-th pair as the probability of being clean/noisy pair,
where k‚àà {0,1}is used to indicate whether it is a clean
or a noisy component. Then, we set a threshold Œ¥= 0.5
to{p(k= 0|li)}N
i=1to divide the data into clean and noisy
sets, i.e.,
Pc={(Ii, Ti)|p(k= 0|‚Ñìi)> Œ¥,‚àÄ(Ii, Ti)‚àà P} ,
Pn={(Ii, Ti)|p(k= 0|‚Ñìi)‚â§Œ¥,‚àÄ(Ii, Ti)‚àà P} ,(3)
where PcandPnare the divided clean and noisy sets,
respectively. For BGE and TSE, the divisions conducted
with Equation (3) are P=Pc
bge‚à™Pn
bgeandP=Pc
tse‚à™Pn
tse,
separately.
To obtain the final reliable divisions, we propose to ex-
ploit the consistency between the two divisions to find the
consensus part as the final confident clean set, i.e.,ÀÜPc=
Pc
bge‚à© Pc
tse. The rest of the data can be divided into
noisy and uncertain subsets, i.e.,ÀÜPn=Pn
bge‚à© Pn
tseand
ÀÜPu=P ‚àí(ÀÜPc‚à™ÀÜPn). Finally, we exploit the divisions to
further recalibrate the correspondence labels, e.g., for i-th
pair, the process can be expressed as:
ÀÜlii=Ô£±
Ô£≤
Ô£≥1, if(Ii, Ti)‚ààÀÜPc,
0, if(Ii, Ti)‚ààÀÜPn,
Rand ({0,1}),if(Ii, Ti)‚ààÀÜPu,(4)
where Rand (X)is the function to randomly select an ele-
ment from the collection X.
3.3.2 Triplet Alignment Loss
The Triplet Ranking Loss (TRL) is a common matching loss
that is widely used in cross-modal learning, and achieves
promising performance by employing the hardest negatives,
e.g., image-text matching [6], video-text retrieval [8], etc.
However, we find that this strategy may lead to bad local
minima or even model collapse for TIReID under NC in the
early stages of training. In contrast, the summation version
of TRL that considers all negative samples, namely TRL-
S, can maintain better stability and avoid model collapse,
but suffers from insufficient performance due to the lack of
attention to hard negatives (see Section 3.3.3 for more dis-
cussion). Therefore, we propose a novel Triplet Alignment
Loss (TAL) to guide TIReID, which differs from TRL in
that it relaxes the optimization of the hardest negatives to
all negatives with an upper bound (see Lemma 1). Thanks
to the relaxation, TAL reduces the risk of the optimization
being dominated by the hardest negatives, thereby making
the training more stable and comprehensive by consideringall pairs. For an input pair (Ii, Ti)in a mini-batch x, TAL
is defined as
Ltal(Ii, Ti) =
m‚àíS+
i2t(Ii) +œÑlog(KX
j=1qijexp(S(Ii, Tj)/œÑ))
+
+
m‚àíS+
t2i(Ti) +œÑlog(KX
j=1qjiexp(S(Ij, Ti)/œÑ))
+,
(5)
where mis a positive margin coefficient, œÑis a tempera-
ture coefficient to control hardness, S(Ii, Tj)‚àà {Sb
ij, St
ij},
[x]+‚â°max( x,0),exp(x)‚â°ex,qij= 1‚àílij, and K
is the size of x. From Lemma 1, as œÑ‚Üí0, TAL ap-
proaches TRL and focuses more on hard negatives. Since
multiple positive pairs from the same identity may appear
in the mini-batch, S+
i2t(Ii) =PK
j=1Œ±ijS(Ii, Tj)is the
weighted average similarity of positive pairs for image Ii,
where Œ±ij=lijexp (S(Ii,Tj)/œÑ)PN
k=1likexp (S(Ii,Tk)/œÑ). Similarly, S+
i2t(Ti)is
the weighted average similarity of positive pairs for text Ti.
Lemma 1 TAL is the upper bound of TRL, i.e.,
Ltrl(Ii, Ti) =
m‚àíS+
i2t(Ii) +S(Ii,ÀÜTi)
+
+
m‚àíS+
t2i(Ti) +S(ÀÜIi, Ti)
+‚â§ Ltal(Ii, Ti),(6)
where ÀÜTi‚àà {Tj|lij= 0,‚àÄj‚àà {1,¬∑¬∑¬∑, K}}is the hardest
negative text for IiandÀÜIi‚àà {Ij|lji= 0,‚àÄj‚àà {1,¬∑¬∑¬∑, K}}
is the hardest negative image for Ii, respectively.
3.3.3 Revisit Triplet Raking Loss
To explore the behaviors of the triplet losses in the noisy
case, we record the similarity distributions versus iterations
of TRL, TRL-S, and the proposed TAL under 50% noise.
From Figure 3a, one can see that the similarities of all pairs
are gradually gathered to 1 during training with TRL, i.e.,
all samples collapses to a narrow neighborhood space on a
hypersphere, resulting in a trivial solution and a bad perfor-
mance (3.64%). To delve deeper into the underlying rea-
Iteration1e30
1
2
3
4
5
6Similarity
0.8000.8250.8500.8750.9000.9250.9500.9751.000Density1e5
0123456
(a) TRL (3.64%)
Iteration1e30
1
2
3
4
5
6Similarity
0.00.20.40.60.81.0Density1e4
0.51.01.52.02.53.03.54.0 (b) TRL-S (44.93%)
Iteration1e30
1
2
3
4
5
6Similarity
0.20.30.40.50.60.70.80.91.0Density1e4
01234567 (c) TAL (63.35%)
Figure 3. The difference between TRL, TRL-S, and proposed TAL
on the similarity distribution versus iterations. The y-zplane rep-
resents the similarity density. The corresponding Rank-1 scores of
testing are placed in brackets for convenience.
son, we performed a gradient analysis. For ease of repre-
sentation and analysis, we only consider one direction since
27201
image-to-text retrieval and text-to-image retrieval are sym-
metrical. And, we suppose that there is only one paired text
for each image in the mini-batch. Due to the truncation op-
eration [x]+, we only discuss the case of L>0that could
generate gradients. Taking the image-to-text direction as an
example, the gradients generated by TRL, TRL-S, and TAL
are
‚àÇLtrl
‚àÇvi=ÀÜti‚àíti,‚àÇLtrl
‚àÇti=‚àívi,‚àÇLtrl
‚àÇÀÜti=vi,(7)
‚àÇLtrls
‚àÇvi=P
j‚ààZ(tj‚àíti),‚àÇLtrls
‚àÇti=‚àí|Z|vi,‚àÇLtrls
‚àÇtj=vi,
(8)
‚àÇLtal
‚àÇvi=PK
jÃ∏=iŒ≤j(tj‚àíti),‚àÇLtal
‚àÇti=‚àívi,‚àÇLtal
‚àÇtj=Œ≤jvi,
(9)
where Z={z|
m‚àíS(Ii, Ti) +S(Ii, Tz)
+>0, zÃ∏=
i, z‚àà {0,¬∑¬∑¬∑, K}},Œ≤j=exp(v‚ä§
itj/œÑ)PK
kÃ∏=iexp(v‚ä§
itk/œÑ),ÀÜti,tjandti
are the hardest negative sample, negative sample, and pos-
itive sample of the anchor sample vi, respectively. Since
the hardest sample is most similar to the positive one,‚àÇtrl
‚àÇviwould easily approach 0and the gradients for other neg-
ative samples except for the hardest negative one are all
0, which may lead to bad local minima early on in train-
ing and even cause the worst-case scenario, i.e., model col-
lapse (see Figure 3a). Unlike TRL, TRL-S aims to push
all negative samples away from the anchor by a constant
margin and produces stronger gradients for the anchor, i.e.,
‚à•‚àÇtrls
‚àÇvi‚à•2‚â• ‚à•‚àÇtrl
‚àÇvi‚à•2, thus avoiding model collapse (see Fig-
ure 3b). However, the drawback is that TRL-S treats every
negative sample equally while ignoring challenging ones,
which limits performance improvement. Different from
TRL and TRL-S, from Equation (9), our TAL can com-
prehensively consider all negative samples and exploits the
anchor-negative semantic relationships to adaptively adjust
the gradients for each negative, thus paying more attention
to hard negatives. As a result, TAL would avoid model
collapse under NC while achieving superior performance
(63.35% vs. 44.93% vs. 3.64%). More details for the
derivations of gradients are provided in the supplementary
material.
3.3.4 Training and Inference
To train the model robustly, we use the corrected label ÀÜlii
instead of the original correspondence label liito compute
the final matching loss, i.e.,
Lm=KX
i=1ÀÜlii(Lb(Ii, Ti) +Lt(Ii, Ti)), (10)
where Lb(Ii, Ti)andLt(Ii, Ti)are the TAL losses com-
puted by Equation (5) with BGE and TSE similarities, re-
spectively. The training process of RDE is shown in Al-
gorithm 1. For the joint inference, we compute the finalAlgorithm 1 The training process of our RDE
Input: The training data PwithNimage-text pairs, maxi-
mal epoch Ne, the cross-modal model M(Œò), and the
hyper-parameters R, m, œÑ ;
1:Initialize the backbones with the weights of the pre-
trained CLIP except for the TSE module, which is ran-
domly initialized;
2:fore = 1, 2, ¬∑¬∑¬∑,Nedo
3: Calculate the per-sample loss ‚Ñì(M,P);
4: Divide the training data with the predictions of BGE
and TSE using Equation (3), respectively;
5: Obtain the consensus divisions to recalibrate the cor-
respondence labels {ÀÜlii}N
i=1with Equation (4);
6: forxin mini-batches {xm}M
m=1do
7: Extract the BGE and TSE features of x;
8: Compute the similarities between Kimage-text
pairs in xwith above features;
9: Calculate the final matching loss Lmwith Equa-
tion (10);
10: Œò = Optimizer (Œò,Lm);
11: end for
12:end for
Output: The optimized parameters ÀÜŒò.
similarity of the image-text pair as the average of the sim-
ilarities computed by both embedding modules, i.e.,S=
(Sb+St)/2.
4. Experiments
In this section, we conduct extensive experiments to verify
the effectiveness and superiority of the proposed RDE on
three widely-used benchmark datasets.
4.1. Datasets and Settings
4.1.1 Datasets
In the experiments, we use the CHUK-PEDES [24], ICFG-
PEDES [7], and RSTPReid [50] datasets to evaluate our
RDE. We follow the data partitions used in IRRA [21]
to split the datasets into training, validation, and test sets,
wherein the ICFG-PEDES dataset only has training and val-
idation sets. More details are provided in the supplementary
material.
4.1.2 Evaluation Protocols
For all experiments, we mainly employ the popular Rank-K
metrics (K=1,5,10) to measure the retrieval performance. In
addition to Rank-K, we also adopt the mean Average Preci-
sion (mAP) and mean Inverse Negative Penalty (mINP) as
auxiliary retrieval metrics to further evaluate performance
following [21].
27202
CUHK-PEDES ICFG-PEDES RSTPReid
Noise Methods R-1 R-5 R-10 mAP mINP R-1 R-5 R-10 mAP mINP R-1 R-5 R-10 mAP mINP
0%SSAN Best 61.37 80.15 86.73 - - 54.23 72.63 79.53 - - 43.50 67.80 77.15 - -
IVT Best 65.59 83.11 89.21 - - 56.04 73.60 80.22 - - 46.70 70.00 78.80 - -
CFine Best 69.57 85.93 91.15 - - 60.83 76.55 82.42 - - 50.55 72.50 81.60 - -
IRRA Best 73.38 89.93 93.71 66.13 50.24 63.46 80.25 85.82 38.06 7.93 60.20 81.30 88.20 47.17 25.28
RDE Best 75.94 90.14 94.12 67.56 51.44 67.68 82.47 87.36 40.06 7.87 65.35 83.95 89.90 50.88 28.08
20%SSANBest 46.52 68.36 77.42 42.49 28.13 40.57 62.58 71.53 20.93 2.22 35.10 60.00 71.45 28.90 12.08
Last 45.76 67.98 76.28 40.05 24.12 40.28 62.68 71.53 20.98 2.25 33.45 58.15 69.60 26.46 10.08
IVTBest 58.59 78.51 85.61 57.19 45.78 50.21 69.14 76.18 34.72 8.77 43.65 66.50 75.70 37.22 20.47
Last 57.67 78.04 85.02 56.17 44.42 48.70 67.42 75.06 34.44 9.25 37.95 63.35 73.75 34.24 19.67
IRRABest 69.74 87.09 92.20 62.28 45.84 60.76 78.26 84.01 35.87 6.80 58.75 81.90 88.25 46.38 24.78
Last 69.44 87.09 92.04 62.16 45.70 60.58 78.14 84.20 35.92 6.91 54.00 77.15 85.55 43.20 22.53
CLIP-CBest 66.41 85.15 90.89 59.36 43.02 55.25 74.76 81.32 31.09 4.94 54.45 77.80 86.70 42.58 21.38
Last 66.10 86.01 91.02 59.77 43.57 55.17 74.58 81.46 31.12 4.97 53.20 76.25 85.40 41.95 21.95
DECLBest 70.29 87.04 91.93 62.84 46.54 61.95 78.36 83.88 36.08 6.25 61.75 80.70 86.90 47.70 26.07
Last 70.08 87.20 92.14 62.86 46.63 61.95 78.36 83.88 36.08 6.25 60.85 80.45 86.65 47.34 25.86
RDEBest 74.46 89.42 93.63 66.13 49.66 66.54 81.70 86.70 39.08 7.55 64.45 83.50 90.00 49.78 27.43
Last 74.53 89.23 93.55 66.13 49.63 66.51 81.70 86.71 39.09 7.56 63.85 83.85 89.45 50.27 27.75
50%SSANBest 13.43 31.74 41.89 14.12 6.91 18.83 37.70 47.43 9.83 1.01 19.40 39.25 50.95 15.95 6.13
Last 11.31 28.07 37.90 10.57 3.46 17.06 37.18 47.85 6.58 0.39 14.10 33.95 46.55 11.88 4.04
IVTBest 50.49 71.82 79.81 48.85 36.60 43.03 61.48 69.56 28.86 6.11 39.70 63.80 73.95 34.35 18.56
Last 42.02 65.04 73.72 40.49 27.89 36.57 54.83 62.91 24.30 5.08 28.55 52.05 62.70 26.82 13.97
IRRABest 62.41 82.23 88.40 55.52 38.48 52.53 71.99 79.41 29.05 4.43 56.65 78.40 86.55 42.41 21.05
Last 42.79 64.31 72.58 36.76 21.11 39.22 60.52 69.26 19.44 1.98 31.15 55.40 65.45 23.96 9.67
CLIP-CBest 64.02 83.66 89.38 57.33 40.90 51.60 71.89 79.31 28.76 4.33 53.45 76.80 85.50 41.43 21.17
Last 63.97 83.74 89.54 57.35 40.88 51.49 71.99 79.32 28.77 4.37 52.35 76.35 85.25 40.64 20.45
DECLBest 65.22 83.72 89.28 57.94 41.39 57.50 75.09 81.24 32.64 5.27 56.75 80.55 87.65 44.53 23.61
Last 65.09 83.58 89.26 57.89 41.35 57.49 75.10 81.23 32.63 5.26 55.00 80.50 86.50 43.81 23.31
RDEBest 71.33 87.41 91.81 63.50 47.36 63.76 79.53 84.91 37.38 6.80 62.85 83.20 89.15 47.67 23.97
Last 71.25 87.39 91.76 63.59 47.50 63.76 79.53 84.91 37.38 6.80 62.85 83.20 89.15 47.67 23.96
Table 1. Performance comparison under different noise rates on three benchmarks. ‚ÄúBest‚Äù means choosing the best checkpoint on the
validation set to test, and ‚ÄúLast‚Äù stands for choosing the checkpoint after the last training epoch to conduct inference. R-1,5,10 is an
abbreviation for Rank-1,5,10 (%) accuracy. The best and second-best results are in bold and underline , respectively.
4.1.3 Implementation Details
As mentioned earlier, we adopt the pre-trained model
CLIP [34] as our modality-specific encoders. In fairness,
we use the same version of CLIP-ViTB/16 as IRRA [21]
to conduct experiments. During training, we introduce
data augmentations to increase the diversity of the training
data. Specifically, we utilize random horizontal flipping,
random crop with padding, and random erasing to augment
the training images. For training texts, we employ random
masking, replacement, and removal for the word tokens as
the data augmentation. Moreover, the input size of images
is384√ó128and the maximum length of input word to-
kens is set to 77. We employ the Adam optimizer to train
our model for 60 epochs with a cosine learning rate decay
strategy. The initial learning rate is 1e‚àí5for the original
model parameters of CLIP and the initial one for the net-
work parameters of TSE is initialized to 1e‚àí3. The batch
size is 64. Following IRRA [21], we adopt an early train-
ing process with a gradually increasing learning rate. For
hyperparameter settings, the margin value mof TAL is set
to 0.1, the temperature parameter œÑis set to 0.015, and the
selection ratio Ris 0.3.
4.2. Comparison with State-of-the-Art Methods
In this section, we evaluate the performance of our RDE on
three benchmarks under different scenarios. For a compre-
hensive comparison, we compare our method with severalstate-of-the-art methods, including both ordinary methods
and robust methods. Moreover, we use two synthetic noise
levels ( i.e., noise rates), 20%, and 50%, to simulate the
real-world scenario where the image-text pairs are not well-
aligned. We randomly shuffle the text descriptions to inject
NCs into the training data. We compare our RDE with five
state-of-the-art baselines: SSAN [7], IVT [38], IRRA [21],
DECL [30], and CLIP-C. SSAN, IVT, and IRRA are re-
cent ordinary methods that are not designed for NC. DECL
is a general framework that can enhance the robustness
of image-text matching methods against NC. We use the
model of IRRA as the base model of DECL for TIReID.
CLIP-C is a strong baseline that fine-tunes the CLIP(ViT-
B/16) model with only clean image-text pairs. We report
the results of both the best checkpoint on the validation set
and the last checkpoint to show the overfitting degree. Fur-
thermore, we also evaluate our RDE on the original datasets
without synthetic NC to demonstrate its superiority in Ta-
ble 1. We compare our RDE with two local-matching meth-
ods: SSAN [7] and CFine [42]); and two global-matching
methods: IVT [38] and IRRA [21]. More comparisons with
other methods are provided in the supplementary material.
From Table 1, one can see that our RDE achieves state-
of-the-art performance on three datasets and we can draw
three observations: (1) On the datasets with synthetic NC,
the ordinary methods suffer from remarkable performance
degradation or poor performance as the noise rate increases.
In contrast, our RDE achieves the best results on all met-
27203
rics. Moreover, by comparing the ‚ÄòBest‚Äô performance with
the ‚ÄòLast‚Äô ones in Table 1, we can see that our RDE can
effectively prevent the performance deterioration caused by
overfitting against NC. (2) Compared with the robust frame-
work DECL and the strong baseline CLIP-C, our RDE also
shows obvious advantages, which indicates that our solu-
tion against NC is effective and superior in TIReID. For
instance, on CUHK-PEDES under 50% noise, our RDE
achieves 71.33%, 87.41%, and 91.81% in terms of Rank-
1,5,10 on the ‚ÄòBest‚Äô rows, respectively, which surpasses
the best baseline DECL by a large margin, i.e., +6.11%,
+3.69%, and +2.53%, respectively. (3) On the datasets
without synthetic NC, our RDE outperforms all baselines
by a large margin. Specifically, RDE achieves performance
gains of +2.56%, +4.22%, and +5.15% in terms of Rank-1
compared with the best baseline IRRA on three datasets, re-
spectively, demonstrating the effectiveness and advantages
of our method.
4.3. Ablation Study
In this section, we conduct ablation studies on the CUHK-
PEDES dataset with 50% noise to investigate the effects
and contributions of each proposed component in RDE. We
compare different combinations of our components in Ta-
ble 2. From the experimental results, we could draw the
following observation: (1) RDE achieves the best perfor-
mance by using both BGE and TSE for joint inference,
which demonstrates that these two modules are complemen-
tary and effective. (2) RDE benefits from CCD, which can
enhance the robustness and alleviate the overfitting effect
caused by NC. (3) Our TAL outperforms the widely-used
Triplet Ranking Loss (TRL) and SDM loss [21], which
demonstrates the superior stability and robustness of our
TAL against NC.
No. SbSeCCD Loss R-1 R-5 R-10 mAP mINP
#1‚úì ‚úì ‚úì TAL 71.33 87.41 91.81 63.50 47.36
#2‚úì ‚úì ‚úì TRL 6.40 16.08 22.14 6.53 2.51
#3‚úì ‚úì ‚úì TRL-S 67.38 85.35 90.64 60.04 43.60
#4‚úì ‚úì ‚úì SDM 69.33 86.99 91.68 61.99 45.34
#5 ‚úì ‚úì TAL 70.70 86.60 91.16 62.67 46.19
#6‚úì ‚úì TAL 69.07 86.09 91.13 61.69 45.40
#7‚úì ‚úì TAL 63.11 81.04 87.22 55.42 38.68
Table 2. Ablation studies on the CHUK-PEDES dataset.
4.4. Parametric Analysis
To study the impact of different hyperparameter settings
on performance, we perform sensitivity analyses for two
key hyperparameters ( i.e.,mandœÑ) on the CHUK-PEDES
dataset with 50% noise. From Figure 4, we can see that:
(1) Too large or too small mwill lead to suboptimal per-
formance. We choose m= 0.1in all our experiments. (2)
Too small œÑwill cause training failure, while the increas-
ingœÑwill gradually decrease the separability (hardness) ofpositive and negative pairs for suboptimal performance. We
choose œÑ= 0.015in all our experiments.
0.0 0.1 0.2 0.3 0.4
m586062646668707274Rank-1 / mAP
RanK-1
mAP
Fail
Figure 4. Variation of performance with different mandœÑ.
4.5. Robustness Study
In this section, we provide some visualization results during
cross-modal training to verify the robustness and effective-
ness of our method. As shown in Figure 5, one can clearly
see that our RDE not only achieves excellent performance
under noise but also effectively alleviates noise overfitting.
(a) CUHK-PEDES
 (b) ICFG-PEDES
Figure 5. Test performance (Rank-1) versus epochs on the CHUK-
PEDES and ICFG-PEDES datasets with 50% noise.
5. Conclusion
In this paper, we reveal and study a novel challenging prob-
lem of noisy correspondence (NC) in TIReID, which vi-
olates the common assumption of existing methods that
image-text data is perfectly aligned. To this end, we pro-
pose a robust method, i.e., RDE, to effectively handle the re-
vealed NC problem and achieve superior performance. Ex-
tensive experiments are conducted on three datasets to com-
prehensively demonstrate the superiority and robustness of
RDE both with and without synthetic NCs.
Acknowledgments
This work was supported in part by NSFC under Grant
U21B2040, 62176171, 62372315, and 62102274, in part
by Sichuan Science and Technology Program under Grant
2022YFH0021 and 2023ZYD0143; in part by Chengdu
Science and Technology Project under Grant 2023-XT00-
00004-GX; in part by the SCU-LuZhou Sciences and Tech-
nology Coorperation Program under Grant 2023CDLZ-16;
in part by the Fundamental Research Funds for the Central
Universities under Grant CJ202303 and YJ202140.
27204
References
[1] Devansh Arpit, Stanis≈Çaw Jastrze ¬∏bski, Nicolas Ballas, David
Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan
Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and
Simon Lacoste-Julien. A closer look at memorization in deep
networks. In Proceedings of the 34th International Confer-
ence on Machine Learning , pages 233‚Äì242. PMLR, 2017. 2,
3, 4
[2] Yang Bai, Min Cao, Daming Gao, Ziqiang Cao, Chen Chen,
Zhenfeng Fan, Liqiang Nie, and Min Zhang. Rasa: relation
and sensitivity aware representation learning for text-based
person search. In Proceedings of the Thirty-Second Interna-
tional Joint Conference on Artificial Intelligence , pages 555‚Äì
563, 2023. 2
[3] Yang Bai, Jingyao Wang, Min Cao, Chen Chen, Ziqiang
Cao, Liqiang Nie, and Min Zhang. Text-based person search
without parallel image-text data. In Proceedings of the 31st
ACM International Conference on Multimedia , pages 757‚Äì
767, 2023.
[4] Min Cao, Yang Bai, Ziyin Zeng, Mang Ye, and Min Zhang.
An empirical study of clip for text-based person search.
arXiv preprint arXiv:2308.10045 , 2023. 2
[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805 , 2018. 2
[6] Haiwen Diao, Ying Zhang, Lin Ma, and Huchuan Lu. Sim-
ilarity reasoning and filtration for image-text matching. In
Proceedings of the AAAI Conference on Artificial Intelli-
gence , pages 1218‚Äì1226, 2021. 5
[7] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao. Semantically self-aligned network for text-to-
image part-aware person re-identification. arXiv preprint
arXiv:2107.12666 , 2021. 6, 7
[8] Jianfeng Dong, Xirong Li, Chaoxi Xu, Xun Yang, Gang
Yang, Xun Wang, and Meng Wang. Dual encoding for video
retrieval by text. IEEE Transactions on Pattern Analysis and
Machine Intelligence , 44(8):4065‚Äì4080, 2021. 5
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2
[10] Chanho Eom and Bumsub Ham. Learning disentangled rep-
resentation for robust person re-identification. Advances in
neural information processing systems , 32, 2019. 1
[11] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja
Fidler. Vse++: Improving visual-semantic embeddings with
hard negatives. arXiv preprint arXiv:1707.05612 , 2017. 2, 3
[12] Yanglin Feng, Hongyuan Zhu, Dezhong Peng, Xi Peng, and
Peng Hu. Rono: Robust discriminative learning with noisy
labels for 2d-3d cross-modal retrieval. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 11610‚Äì11619, 2023. 2
[13] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and XingSun. Contextual non-local alignment over full-scale rep-
resentation for text-based person search. arXiv preprint
arXiv:2101.03036 , 2021. 2
[14] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao
Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-
teaching: Robust training of deep neural networks with ex-
tremely noisy labels. Advances in neural information pro-
cessing systems , 31, 2018. 4
[15] Haochen Han, Kaiyao Miao, Qinghua Zheng, and Minnan
Luo. Noisy correspondence learning with meta similarity
correction. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 7517‚Äì
7526, 2023. 3
[16] Xiao Han, Sen He, Li Zhang, and Tao Xiang. Text-
based person search with limited data. arXiv preprint
arXiv:2110.10807 , 2021. 1, 2
[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition , pages 770‚Äì778, 2016. 4
[18] Peng Hu, Xi Peng, Hongyuan Zhu, Liangli Zhen, and Jie
Lin. Learning cross-modal retrieval with noisy labels. In
Proceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 5403‚Äì5413, 2021. 2
[19] Peng Hu, Zhenyu Huang, Dezhong Peng, Xu Wang, and Xi
Peng. Cross-modal retrieval with partially mismatched pairs.
IEEE Transactions on Pattern Analysis and Machine Intelli-
gence , pages 1‚Äì15, 2023. 3
[20] Zhenyu Huang, Guocheng Niu, Xiao Liu, Wenbiao Ding,
Xinyan Xiao, Hua Wu, and Xi Peng. Learning with noisy
correspondence for cross-modal matching. Advances in Neu-
ral Information Processing Systems , 34:29406‚Äì29419, 2021.
2, 3, 4, 5
[21] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 2787‚Äì2797, 2023. 1, 2,
3, 4, 6, 7, 8
[22] Ya Jing, Chenyang Si, Junbo Wang, Wei Wang, Liang Wang,
and Tieniu Tan. Pose-guided multi-granularity attention net-
work for text-based person search. In Proceedings of the
AAAI Conference on Artificial Intelligence , pages 11189‚Äì
11196, 2020. 1
[23] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix:
Learning with noisy labels as semi-supervised learning.
arXiv preprint arXiv:2002.07394 , 2020. 2, 4, 5
[24] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In Proceedings of the IEEE conference on
computer vision and pattern recognition , pages 1970‚Äì1979,
2017. 1, 2, 6
[25] Shenshen Li, Xing Xu, Yang Yang, Fumin Shen, Yijun Mo,
Yujie Li, and Heng Tao Shen. Dcel: Deep cross-modal evi-
dential learning for text-based person retrieval. In Proceed-
ings of the 31st ACM International Conference on Multime-
dia, pages 6292‚Äì6300, 2023. 2
[26] Yijie Lin, Jie Zhang, Zhenyu Huang, Jia Liu, Zujie Wen, and
Xi Peng. Multi-granularity correspondence learning from
27205
long-term noisy videos. arXiv preprint arXiv:2401.16702 ,
2024. 2
[27] Yangdi Lu, Yang Bo, and Wenbo He. An ensemble model for
combating label noise. In Proceedings of the Fifteenth ACM
International Conference on Web Search and Data Mining ,
pages 608‚Äì617, 2022. 2
[28] Yiwei Ma, Xiaoshuai Sun, Jiayi Ji, Guannan Jiang, Weilin
Zhuang, and Rongrong Ji. Beat: Bi-directional one-to-many
embedding alignment for text-based person retrieval. In Pro-
ceedings of the 31st ACM International Conference on Mul-
timedia , pages 4157‚Äì4168, 2023. 2
[29] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments. IEEE Transactions on
Image Processing , 29:5542‚Äì5556, 2020. 1
[30] Yang Qin, Dezhong Peng, Xi Peng, Xu Wang, and Peng
Hu. Deep evidential learning with noisy correspondence
for cross-modal retrieval. In Proceedings of the 30th ACM
International Conference on Multimedia , pages 4948‚Äì4956,
2022. 2, 3, 7
[31] Yalan Qin, Nan Pu, and Hanzhou Wu. Edmc: Efficient multi-
view clustering via cluster and instance space learning. IEEE
Transactions on Multimedia , 2023. 2
[32] Yalan Qin, Nan Pu, and Hanzhou Wu. Elastic multi-view
subspace clustering with pairwise and high-order correla-
tions. IEEE Transactions on Knowledge and Data Engineer-
ing, 2023. 2
[33] Yang Qin, Yuan Sun, Dezhong Peng, Joey Tianyi Zhou,
Xi Peng, and Peng Hu. Cross-modal active complemen-
tary learning with self-refining correspondence. In Thirty-
seventh Conference on Neural Information Processing Sys-
tems, 2023. 3
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748‚Äì8763. PMLR, 2021. 2, 7
[35] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
Proceedings of the 30th ACM International Conference on
Multimedia , 2022. 1, 2
[36] Zhiyin Shao, Xinyu Zhang, Changxing Ding, Jian Wang, and
Jingdong Wang. Unified pre-training with pseudo texts for
text-to-image person re-identification. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 11174‚Äì11184, 2023. 2
[37] Fei Shen, Xiangbo Shu, Xiaoyu Du, and Jinhui Tang.
Pedestrian-specific bipartite-aware similarity learning for
text-based person retrieval. In Proceedings of the 31st ACM
International Conference on Multimedia , pages 8922‚Äì8931,
2023. 2
[38] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval.
InEuropean Conference on Computer Vision , pages 624‚Äì
641. Springer, 2022. 1, 2, 7[39] Chengji Wang, Zhiming Luo, Yaojin Lin, and Shaozi Li.
Text-based person search via multi-granularity embedding
learning. In IJCAI , pages 1068‚Äì1074, 2021. 1, 2
[40] Zheng Wang, Ruimin Hu, Yi Yu, Chao Liang, and Wenxin
Huang. Multi-level fusion for person re-identification with
incomplete marks. In Proceedings of the 23rd ACM interna-
tional conference on Multimedia , pages 1267‚Äì1270, 2015.
1
[41] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 1624‚Äì1633, 2021. 1
[42] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
arXiv preprint arXiv:2210.10276 , 2022. 1, 2, 4, 7
[43] Shuanglin Yan, Neng Dong, Jun Liu, Liyan Zhang, and
Jinhui Tang. Learning comprehensive representations with
richer self for text-to-image person re-identification. In Pro-
ceedings of the 31st ACM international conference on multi-
media , pages 6202‚Äì6211, 2023. 2
[44] Mouxing Yang, Zhenyu Huang, Peng Hu, Taihao Li,
Jiancheng Lv, and Xi Peng. Learning with twin noisy labels
for visible-infrared person re-identification. In Proceedings
of the IEEE/CVF conference on computer vision and pattern
recognition , pages 14308‚Äì14317, 2022. 2, 3
[45] Mouxing Yang, Yunfan Li, Peng Hu, Jinfeng Bai, Jiancheng
Lv, and Xi Peng. Robust multi-view clustering with incom-
plete information. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 45(1):1055‚Äì1069, 2022. 2
[46] Mouxing Yang, Zhenyu Huang, and Xi Peng. Robust ob-
ject re-identification with coupled noisy labels. International
Journal of Computer Vision , pages 1‚Äì19, 2024. 2
[47] Huaiwen Zhang, Yang Yang, Fan Qi, Shengsheng Qian, and
Changsheng Xu. Robust video-text retrieval via noisy pair
calibration. IEEE Transactions on Multimedia , 2023. 2, 3
[48] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In Proceedings of the Eu-
ropean conference on computer vision (ECCV) , pages 686‚Äì
701, 2018. 1, 2
[49] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM) , 16(2):1‚Äì23, 2020. 2
[50] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua. Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In Proceedings of the 29th ACM International
Conference on Multimedia , pages 209‚Äì217, 2021. 1, 2, 6
[51] Haowei Zhu, Wenjing Ke, Dong Li, Ji Liu, Lu Tian, and Yi
Shan. Dual cross-attention learning for fine-grained visual
categorization and object re-identification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 4692‚Äì4702, 2022. 4
27206
