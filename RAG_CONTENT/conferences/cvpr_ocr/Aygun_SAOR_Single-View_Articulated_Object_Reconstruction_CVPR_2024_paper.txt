SAOR: Single-View Articulated Object Reconstruction
Mehmet Ayg ¨un Oisin Mac Aodha
University of Edinburgh
mehmetaygun.github.io/saor
Input Pose Reconstruction Parts Input Pose Reconstruction Parts
Figure 1. SAOR capable of predicting the 3D shape of an articulated object category from a single image. Our model is trained on
multiple categories simultaneously using self-supervision on single-view image collections. It can efficiently predict object pose, 3D shape
reconstruction, and unsupervised part-level assignment using only a single forward pass per image at test time in a category-agnostic way.
Abstract
We introduce SAOR, a novel approach for estimating the
3D shape, texture, and viewpoint of an articulated object
from a single image captured in the wild. Unlike prior
approaches that rely on pre-defined category-specific 3D
templates or tailored 3D skeletons, SAOR learns to ar-
ticulate shapes from single-view image collections with a
skeleton-free part-based model without requiring any 3D
object shape priors. To prevent ill-posed solutions, we pro-
pose a cross-instance consistency loss that exploits disen-
tangled object shape deformation and articulation. This is
helped by a new silhouette-based sampling mechanism to
enhance viewpoint diversity during training. Our method
only requires estimated object silhouettes and relative depth
maps from off-the-shelf pre-trained networks during train-
ing. At inference time, given a single-view image, it effi-
ciently outputs an explicit mesh representation. We obtain
improved qualitative and quantitative results on challeng-
ing quadruped animals compared to relevant existing work.1. Introduction
Considered as one of the first PhD theses in computer vi-
sion, Roberts [49] aimed to reconstruct 3D objects from
single-view images. Despite significant progress in the pre-
ceding sixty years [5, 6, 19, 20], the problem remains very
challenging, especially for highly deformable categories
photographed in the wild, e.g., animals. In contrast, humans
can infer the 3D shape of an object from a single image by
making use of priors about the natural world and familiarity
with the object category present. Some of these natural-
world low-level priors can be explicitly defined (e.g., sym-
metry or smoothness), but manually encoding and utilizing
high-level priors (e.g., 3D category shape templates) for all
categories of interest is not a straightforward task.
Recently, multiple methods have attempted to learn 3D
shape by making use of advances in deep learning and
progress in differentiable rendering [21, 36, 38]. This has
resulted in impressive results for synthetic man-made cate-
gories [7, 21, 59] and humans [12, 37], where full or par-
tial 3D supervision is readily available. However, when
3D supervision is not available, the reconstruction of ar-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
10382
ticulated object classes remains challenging. This is due
to factors such as: (i) methods not modeling articula-
tion [10, 19, 28, 41], (ii) the reliance on category-specific
3D template [25, 27, 71] or manually defined 3D skeleton
supervision [60, 61], or (iii) requiring multi-view training
data such as video [25, 60, 63].
In this paper, we introduce SAOR , a novel self-
supervised Single-view Articulated Object Reconstruction
method that can estimate the 3D shape of articulating ob-
ject categories, e.g., animals. We forgo the need for explicit
3D object shape or skeleton supervision at training time by
making use of the following assumption: objects are made
of parts, and these parts move together . Given a single input
image, our proposed method predicts the 3D shape of the
object and partitions it into parts. It also predicts the trans-
formation for each part and deforms the initially estimated
shape, in a skeleton-free manner, using a linear skinning ap-
proach. We only require easy to obtain information derived
from single-view images during training, e.g., estimated ob-
ject silhouettes and predicted relative depth maps. SAOR is
trained end-to-end, and outputs articulated 3D object shape,
texture, 3D part assignment, and camera viewpoint. Exam-
ple qualitative results can be seen in Fig. 1.
We make the following contributions: (i) We demon-
strate that articulation can be learned using image-based
self-supervision alone via our new part-based SAOR ap-
proach which is trained on multiple categories simultane-
ously without requiring any 3D template or skeleton prior.
(ii) As estimating the 3D shape of an articulated object from
a single image is an under-constrained problem, we intro-
duce a cross-instance swap consistency loss that leverages
our disentanglement of shape deformation and articulation,
in addition to a new silhouette-based sampling mechanism,
that enhances the diversity of object viewpoints sampled
during training. (iii) We illustrate the effectiveness of our
approach on a diverse set of over 100 challenging categories
covering quadrupeds and bipeds, and present quantitative
results where we outperform existing methods that do not
use explicit 3D supervision. Code will be made available.
2. Related Work
Here we discuss works that attempt to estimate the 3D shape
of an object in a single image using image-based 2D su-
pervision during training. We do not focus on works that
require explicit 3D supervision [7, 21, 39, 59] or multi-
view images for training [18, 35, 57, 68]. We also do not
cover methods that only reconstruct single object instances
[40, 44, 45] or models for multi-object scenes [43]. For a
recent overview of related topics, we refer readers to [55].
Deformable 3D Models. The pioneering work of Blanz
and Vetter [5] marked the introduction of deformable mod-
els to represent the 3D shape of an object category using
vector spaces. By using 3D scans of human faces, theycreated a deformable model which captured inter-subject
shape variation and demonstrated the ability to reconstruct
3D faces from unseen single-view images. This concept
was later expanded to more complex shapes such as the hu-
man body [1, 37], hands [22, 53], and animals [72].
Recent work has combined deep learning with 3D de-
formable models [4, 37, 50, 71] to predict the shape of
articulated objects from single-view input images. Given
an input image, these methods estimate the parameters of
a known deformable 3D model and render the object us-
ing the predicted camera viewpoint. Although this line of
work has led to impressive results for the human body [37],
the results for deformable animal categories are lacking
[4, 50, 71]. This is because popular human deformable
models, e.g., SMPL [37], are constructed using thousands
of high-quality real human 3D scans. In contrast, animal
focused 3D models, e.g., SMAL [71], are generated using
3D scans from a small number of toy animals.
The above models are parameter-efficient due to their
low dimensional shape parameterization, which facilitates
easier optimization. However, beyond common categories,
such as dogs [50], it can be prohibitively difficult to find 3D
scans for each new object category of interest. In this work,
we eliminate the need for prior 3D scans of objects by com-
bining linear vertex deformation with a skeleton-free [34]
linear blend skinning [30] approach to model the 3D shape
of articulated objects using only images at training time.
Unsupervised Learning of 3D Shape. To overcome the
need for large collections of aligned 3D scans from an ob-
ject category of interest, there has been a growing body of
work that attempts to learn 3D shape using images from
only minimal, if any, 3D supervision. The common theme
of these methods is that they treat shape estimation as an
image synthesis task during training while enforcing geo-
metric constraints on the rendering process.
One of the first object-centric deep learning-based meth-
ods to not use dense 3D shape supervision for single-
view reconstruction was CMR [19]. CMR utilizes cam-
era pose supervision estimated from structure from motion,
along with human-provided 2D semantic keypoint supervi-
sion during training and a coarse template mesh initialized
from the keypoints. Subsequently, U-CMR [10] remove
the keypoint supervision by using a multi-camera hypothe-
sis approach which assigns and optimizes multiple cameras
for each instance during training. IMR [56] starts from a
category-level 3D template and learns to estimate shape and
camera viewpoint from images and segmentation masks.
UMR [32] enforces consistency between per-instance un-
supervised 2D part segmentations and 3D shape. They do
not assume access to a 3D shape template (or keypoints) but
instead learn one via iterative training. SMR [15] also uses
object part segmentation from a self-supervised network as
weak supervision. Shelf-SS [67] uses a semi-implicit volu-
10383
metric representation and obtains consistent multi-view re-
constructions using generative models similar to [14]. Like
us, all of these methods use object silhouettes (i.e., fore-
ground masks) as supervision.
Recently, Unicorn [41] combined curriculum learning
with a cross-instance swap loss to help encourage approx-
imate multi-view consistency across object instances when
training a reconstruction network without silhouettes. Their
swap loss makes use of an online memory bank to select
pairs of images that contain similar shape or texture. The
pairs are restricted to be observed from different estimated
viewpoints. Then a consistency loss is applied which ex-
plicitly forces pairs to share the same shape or texture. In
essence, this is a form of weak multi-view supervision un-
der the assumption that the shape of the object pair are the
same. However, this assumption breaks down for articulat-
ing objects. Inspired by this, we propose a more efficient
and effective swap loss designed for articulating objects.
There are also approaches that predict a mapping from
image pixels to the surface of a 3D object template as in [12,
42]. CSM [28] eliminates the need for large-scale 2D to
3D surface annotations via an unsupervised 2D to 3D cycle
consistency loss. The goal of their loss is to minimize the
discrepancy between a pixel location and a corresponding
3D surface point that is reprojection based on the estimated
camera viewpoint. In contrast, we do not require any 3D
templates or manually defined 2D annotations.
Learning Articulated 3D Shape. Most natural object cat-
egories are non-rigid and can thus exhibit some form of ar-
ticulation. This natural shape variation between individual
object instances violates the simplifying assumptions made
by approaches that do not attempt to model articulation.
A-CSM [27] extends CSM [28] by making the learned
mapping articulation aware. Given a 3D template of the ob-
ject category, they first manually define the parts of the ob-
ject category and a hierarchy between the parts. Then, given
an input image, they predict transformation parameters for
each part so they can articulate the initial 3D template be-
fore calculating the mapping between the 3D template and
the input pixels. Recently [52] show that A-CSM can be
trained with noisy keypoint labels. Instead of manually
defining parts, [25] initialize sparse handling points, pre-
dict displacements for these points, and articulate the shape
using differentiable Laplacian deformation. However, each
of these methods requires a pre-defined 3D template of the
object category.
DOVE [60], LASSIE [65], and MagicPony [61] are re-
cent methods that are capable of predicting the 3D geometry
of articulated objects without requiring a 3D category tem-
plate shape. However, they require a predefined category-
level 3D skeleton prior in order to model articulating ob-
ject parts such as legs. While 3D skeletons are easier to
define compared to full 3D shapes, they still need to beprovided for each object category of interest and have to
be tailored to the specifics of each category, e.g., the trunk
of the elephant is not present in other quadrupeds. In the
case of MagicPony [61], in addition to the skeleton and its
connectivity, per-bone articulation constraints are also pro-
vided, which necessitates more manual labor. Additionally,
a single skeleton may be insufficient if there are large shape
changes exhibited across instances of the category.
MagicPony [61] builds on DOVE [60], by removing
the need for explicit video data during training. Inspired
by UMR [32], MagicPony makes use of weak correspon-
dence supervision from a pre-trained self-supervised net-
work to enforce pixel-level consistency between 2D im-
ages and learned 3D shape. Concurrent to our work 3D-
Fauna [33] extends MagicPony for quadrupeds in a multi-
category setting. LASSIE [65] is another skeleton-based
approach that uses correspondence information from self-
supervised features and manually pre-defined part primi-
tives. Like us, they model object parts, but their goal is not
to learn a model that can directly predict shape from a single
image. Instead, their approach learns instance shape from
a set of images via test-time optimization. In recent work,
[66] automatically extracts the skeleton from a user-defined
canonical image, but still requires test-time optimization.
We train with single-view image collections, but there
are also several works that use video as a data source for
modeling articulating objects [31, 60, 63, 64] and other
methods that perform expensive test-time optimization for
fitting or refinement [26, 31, 61, 65, 71]. In contrast, we
only require self-supervision derived from single-view im-
ages and our inference step is performed efficiently via a
single forward pass through a deep network.
3. Method
Our objective is to estimate the shape S, texture T, and
camera pose (i.e., viewpoint) Pof an object from an in-
put image I. To accomplish this, we employ a self-
supervised analysis-by-synthesis framework [11, 29] which
reconstructs images using a differentiable rendering opera-
tion, denoted as ˆI= Π( S, T, P ). The model is optimized
by minimizing the discrepancy between a real image Iand
the corresponding rendered one ˆI. In this section, we de-
scribe how the above quantities are estimated to ensure that
the predicted 3D shape is plausible. An overview of the
generation phase of our method can be seen in Fig. 2
3.1. SAOR Model
Taking inspiration from previous works [19, 41, 67], we ini-
tialize a sphere-shaped mesh with initial vertices S◦with
fixed connectivity. We then extract a global image repre-
sentation ϕim=fenc(I)∈RDusing a neural network
encoding function. From this, we utilize several modules,
described below, to predict the shape deformation, articu-
lation, camera viewpoint, and object texture necessary to
10384
Figure 2. Overview of the generation phase of our SAOR method. Given a single image Ias input, we extract a global feature vector ϕim
which is decoded by four separate networks ( fd,fa,ft, andfp) to generate a final output image ˆI. We start by deforming an initial sphere,
articulate it using a part-based linear blend skinning (LBS) operation ξ, texture the mesh, and render it using a differential render Πso that
it is depicted from the same viewpoint as the input image. The parameters for each of the networks presented are trained in an end-to-end
manner using image reconstruction-based self-supervision from multiple different categories using the same model.
generate the final target shape.
Shape. We predict the object shape by deforming and artic-
ulating an initial sphere mesh S◦={s◦
n}N
n. Here, each of
theNelements of S◦are 3D coordinates. We estimate the
vertices of the deformed shape using a deformation func-
tions′
i=s◦
i+fd(s◦
i, ϕim), which outputs the displace-
ment vector for the initial points. The deformation func-
tionfdis modeled as a functional field, which is a 3-layer
MLP similar to [41, 43]. As most natural objects exhibit
bilateral symmetry, similar to [19], we only deform the ver-
tices of the zero-centered initial shape that are located on
the positive side of the xy-plane and reflect the deformation
for the vertices on the negative side. We then articulate the
deformed shape using linear skinning [30] in a skeleton-
free manner [34] to obtain the final shape S=ξ(S′, A),
where Ais the output of our articulation prediction func-
tion, which we describe in more detail later in Sec. 3.2.
Texture. To predict the texture of the object, we gener-
ate a UV image by transforming the global image feature,
T=ft(ϕim). The function ftis implemented as a convo-
lutional decoder, which maps a one-dimensional input rep-
resentation to a texture map, ft:RD7→RH×W×3. This
approach is similar to previous works [41, 43]. However,
unlike existing work [19, 32] that copy the pixel colors of
the input image directly to create a texture image using a
predicted flow field, we predict texture directly. In initial ex-
periments, we found that estimating texture flow only gave
minimal improvements, for an increase in complexity.
Camera Pose. We use Euler angles (azimuth, elevation,
and roll) along with camera translation to predict the camera
pose, similar to previous works [10, 41]. Instead of using
multiple camera hypotheses for each input instance [41], for
each forward pass, or optimizing them for each training in-
stance [10], we use several camera pose predictors, but only
select the one with the highest confidence score for each for-
ward pass, as described in [61]. Specifically, we predict the
camera pose as P∈R6=fp(ϕim). Here, P=rp,tprep-
resents the predicted camera rotation and translation. This
approach accelerates the training process and reduces mem-ory requirements since we only need to compute the loss
for one camera in each forward pass. We only incorporate
priors about the ranges of elevation and roll predictions, in-
stead of a strong uniformity constraint on the distribution of
the camera poses as in [41] or fixed elevation as in [61].
3.2. Skeleton-Free Articulation
Many natural world object categories exhibit some form of
articulation, e.g., the legs of an animal. Existing work has
attempted to model this via deformable 3D template mod-
els [50] or by using manually defined category-level skele-
ton priors [60, 61]. However, this assumes one has access to
category-level 3D supervision during training. This would
be difficult to obtain in our setting as we train on over 100
categories simultaneously. We instead propose a skeleton-
free approach by modeling articulation using a part-based
model. Our approach is inspired by [34], who proposed
a related skeleton-free representation for the task of pose
transfer between 3D meshes. However, in our case, we
train a model that can predict parts in an image from self-
supervision alone.
Our core idea is to partition the 3D shape into parts and
deform each part based on predicted transformations. To
achieve this, we predict a part assignment matrix W∈
RN×K, that represents how likely it is that a vertex belongs
to a particular part, wherePK
kWi,k= 1. Here, Kis a hy-
perparameter that represents the number of parts and Nis
the number of vertices in the mesh. We also predict trans-
formation parameters π={(zk,rk,tk)}K
kfor each part
which consists of scale zk∈R3, rotation rk∈R3×3, and
translation tk∈R3. Each of these parameters are predicted
using different MLPs that take the global image feature ϕim
as input and output fa(S◦, ϕim) =A={W,π}.
Articulation can be applied to a shape using a set of de-
formations using the linear blend skinning equation [17].
Here, each vertex needs to be associated with deformations
by the skinning weights. In previous work [60, 61, 65],
skinning weights are calculated using a skeleton prior (e.g.,
a set of bones and their connectivity). We instead estimate
10385
Figure 3. Illustration of our articulated swap loss. To calculate
the loss, a swap image ˆIsw
iis rendered using a randomly chosen
paired image’s shape S′
j, combined with estimated texture, view-
point, and articulation ( Ti, Pi, Ai) from the input image Ii. It en-
sures that 3D predictions are not degenerate and helps disentangle
deformation and articulation.
skinning weights using a part-based model that does not re-
quire a prior skeleton or any ground truth part segmenta-
tions. We first calculate the centers for each part from the
vertices of the deformed shape s′
i∈S′,
ck=PN
is′
i∗Wi,kPN
iWi,k. (1)
The final position of a vertex sifor the final shape Sis
then calculated using the skinning weight of the vertex and
estimated part transformations as
si=KX
kWi,kzk⊙(rk(s′
i−ck) +tk), (2)
where zk,rk, andtkare the predicted scale, rotation, and
translation parameters corresponding to part kand⊙is an
element-wise multiplication. In addition to the reconstruc-
tion losses, we apply regularization on the part assignment
matrix Wthat encourages the size of each part segment
to be similar for each instance. As each of the above op-
erations are differentiable, articulation is learned via self-
supervised without requiring any 3D template shapes [27],
predefined skeletons [61], or part segmentations [32].
3.3. Swap Loss and Balanced Sampling
One of the hardest challenges in single-view 3D reconstruc-
tion is the tendency to predict degenerate solutions as a re-
sult of the ill-posed nature of the task (i.e., an infinite num-
ber of 3D shapes can explain the same 2D input). Exam-
ples of such failure cases include models predicting flat 2D
textured planes which are visually consistent when viewed
from the same pose as the input image but lack full 3D
shape [41]. To mitigate these issues, and to ensure multi-
view consistency of our 3D reconstructions, we build on
the swap loss idea recently introduced in [41].
To estimate their swap loss, [41] take a pair of images
(Ii, Ij) that depict two different instances of the same ob-
ject category, and estimate their respective shape, texture,and camera pose, ({Si, Ti, Pi},{Sj, Tj, Pj}). They then
generate an image ˆIsw
i= Π( Sj, Ti, Pi)by swapping the
shape encodings SiandSj, where Πis a differentiable ren-
derer. Finally, they estimate the appearance loss between Ii
andˆIsw
iwhich aims to enforce cross-instance consistency.
The intuition here is that the shape from Ijand texture from
Iishould be sufficient to describe the appearance of Ii, even
though Ijis potentially captured from a different viewpoint.
In [41], the shapes SiandSjshould be similar, while
the predicted viewpoints PiandPjshould be different to
get a useful ‘multi-view’ training signal. To obtain simi-
lar shapes, they store latent shape codes in a memory bank
which is queried online via a nearest neighbor lookup. This
memory bank is updated at each iteration for the selected
shape codes using the current state of the network. More-
over, they limit the search neighborhood based on the pre-
dicted viewpoints to ensure that they obtain some viewpoint
variation, i.e., in [41] the viewpoints PiandPjshould not
be too similar, or too different. While this results in plausi-
ble predictions for mostly rigid categories such as birds and
cars, for highly articulated animal categories it can led to
degenerate solutions due to more variety in terms of shape
appearance, as can be seen in Fig. 7.
Swap Loss. To address this issue, we introduced a straight-
forward but more effective swap loss that generalizes to
articulated object classes. Our hypothesis is that given a
set of images that contain a variety of viewpoints exhibit-
ing disentangled deformation and articulation, we can use
randomly chosen image pairs to calculate the swap loss.
Since we model the articulation along with the deforma-
tion to obtain the final shape, articulation can be used to
explain the difference between shapes. In our proposed
loss, we swap random deformed shapes S′
iandS′
jfrom
instances of the same object category, but use the original
estimated articulation Ssw=ξ(S′
j, Ai)and reconstruct the
swap image ˆIsw
i= Π( Ssw, Ti, Pi)to calculate the swap
lossLswap(Ii,ˆIsw
i). Our loss is illustrated in Fig. 3.
Balanced Sampling. For our swap loss to be successful it
requires the selected image pairs to ideally be from differ-
ent viewpoints. To obtain informative image pairs, we pro-
pose an image sampling mechanism which makes use of the
segmentation masks of the input images. Before training,
we cluster predicted segmentation masks of the training im-
ages and then during training we sample images from each
cluster uniformly to form batches. This ensures that each
batch includes the object of interest depicted from different
viewpoints. In Fig. 4 we can see that cluster centers mostly
capture the rough distribution of viewpoints and thus help
stabilize training. As our image pairs (Ii, Ij)are sampled
from within the same batch during training, this results in
varied images from different viewpoints for the swap loss.
Combined, our swap and balanced sampling steps drasti-
cally simplifies the swap loss from [41] and improves recon-
10386
Figure 4. (Top) Subset of the resulting cluster centers that arise
from clustering the object segmentation masks. (Bottom) Repre-
sentative images from each of the clusters above. We can see that
our simple clustering operation captures the main viewpoint varia-
tions present in the data, e.g., left facing, frontal, right facing, etc.
struction quality and training stability on articulated classes.
3.4. Optimization
Given an input image, I, we reconstruct it as ˆIusing esti-
mated shape, texture, and viewpoint. In addition, we use the
swapped shape to predict another image ˆIswand calculate
the swap loss, as discussed in Sec. 3.3. We also use differ-
entiable rendering to obtain a predicted object segmentation
mask and depth derived from the predicted 3D shape, ˆM
andˆDrespectively. Our model is trained using a combina-
tion of the following losses,
L=Lappr+Lmask +Ldepth +Lswap+Lreg.(3)
The appearance loss, Lappr(I,ˆI), is an RGB and per-
ceptual loss [70], Ldepth(D,ˆD)is the translation and shift-
invariant depth loss introduced in [46], and Lmask(M,ˆM)
estimates silhouette discrepancy. To avoid degenerate so-
lutions, we use Lswap(I,ˆIsw)and regularize predictions
usingLreg, which encourages smoothness [8] and normal
consistency on the predicted 3D shape along with a uniform
distribution on the part assignment. While we use predicted
segmentation masks and relative depth during training, at
test time, our model only requires a single image.
3.5. Implementation Details
We employ a ResNet [13] as our global encoder, fenc,
and perform end-to-end training using Adam [23]. Object
masks Mand depths Dare obtained for training by uti-
lizing off-the-shelf pre-trained networks. To implement all
3D operations in our model we use the Pytorch3D frame-
work [48] using their default mesh rasterization [36] which
is differentiable and enables end-to-end training. Prior to
being passed to the model, images are resized to 128x128
pixels. We disable articulation for the first 100 epochs when
training a model from scratch, and continue training mod-
els for another 100 epochs by enabling deformation and ar-
ticulation jointly. The lightweight design of our proposed
method enables the estimation of the final shape, articu-
lation, texture, and viewpoint in approximately 15 ms perSource λ= 1.0λ= 0.7λ= 0.3λ= 0.0TargetArticulation
 Deformation
Figure 5. Disentanglement of articulation and deformation. On
top, we interpolate articulation latent features between a source
and target image, and on the bottom do the same for shape defor-
mation features. λ= 1 indicates that original features are used
for reconstruction, while λ= 0 indicates the target ones. We can
see that the difference between the reconstructions is explained by
articulation changes between the source and target image pairs.
image. We provide more details regarding losses, hyperpa-
rameters, and optimization in the supplementary material.
4. Experiments
Here we present results on multiple quadruped and biped
animal categories, providing both quantitative and qualita-
tive comparisons to previous work.
4.1. Data and Pre-Processing
For our experiments, we trained two models: SAOR-Bird
andSAOR-101 . The bird model is trained from scratch us-
ing the CUB [58] dataset following the original train/test
split. SAOR-101, the general animal model, is trained on
101 animal categories that contain birds, quadrupeds, and
bipeds. This model is first trained using only horse images
from the LSUN [69] dataset with an additional 500 front-
facing horse images from iNaturalist [16], as LSUN mostly
contains side-view images of horses. Then, as in [61], we
finetune the horse model on a new dataset that we collected
from iNaturalist [16] which contains 90k images from 101
different animal classes. In the supplementary material, we
provide more details about the dataset.
For pre-processing, we run a general-purpose animal ob-
ject detector [2] to detect all the animals present in the in-
put images and then filter the detections based on the con-
fidence, size, and location of the bounding box. We then
extract segmentation masks using SAM [24] and estimate
the relative monocular depth using MiDaS [46, 47].
4.2. Quantitative Results
To compare to existing work, we quantitatively evaluate us-
ing the 2D keypoint transfer task, which reflects the qual-
ity of the estimated shape and viewpoint and 3D evalua-
tion which reflects how predicted and ground truth depth is
aligned We report results using the PCK metric with a 0.1
threshold for the keypoint transfer task and normalized L1
Chamfer distance for 3D evaluation.
Birds. Keypoint transfer results on CUB [58] are presented
in Table 1, both for all bird classes and the non-aquatic sub-
10387
Supervision Method all w/o aqua
∗,/ct,, CMR [19] 54.6 59.1
∗,/ct U-CMR [10] 35.9 41.2
♂,/ct∗,,/frard∗DOVE [60] 44.7 51.0
♂,/ct∗,,† MagicPony [61] 55.5 63.5
/ct CMR [19] 25.5 27.7
/ct, SCOPS∗UMR [32] 51.2 55.5
None Unicorn [41] 49.0 53.5
/ct∗,∗SAOR-Bird 51.9 57.8
Table 1. Keypoint transfer results on CUB [58] using the PCK
metric with 0.1 threshold (higher is better). 3D template shape,
♂3D skeleton,camera viewpoint, 2D keypoints, /ctsegmen-
tation mask,/frardoptical flow,video,DINO features, SCOPS
part segmentation, and monocular depth. †also uses additional
video frames from [60]. The initial 3D template in [10, 19] is
derived from 2D keypoints.∗indicates that the supervision is pre-
dicted, hence it is weak supervision. We obtain the best results for
methods that do not use 3D templates ( ), skeletons (♂), or extra
data during training in addition to CUB (e.g., [60, 61]).
Supervision Method Horse Cow Sheep
/ct Dense-Equi [54] 23.3 20.9 19.6
/ct, CSM [28] 31.2 26.3 24.7
/ct, A-CSM [27] 32.9 26.3 28.6
None Unicorn [41] 14.9 12.1 11.0
♂,/ct∗,,†MagicPony [61] 42.9 42.5 26.2
/ct∗,∗SAOR-101 44.9 33.6 29.1
Table 2. Keypoint transfer results for quadruped animals.
set as in [61]. Our method obtains the best results out of
methods that do not use keypoint supervision, 3D object pri-
ors (e.g., 3D templates or skeletons [60, 61]), or additional
data (e.g., [60, 61]).
Quadrupeds. Keypoint transfer results for quadruped an-
imals from the Pascal dataset [9] are presented in Table 2.
As noted earlier, we trained the horse model from scratch,
while the other models were finetuned using data from
iNaturalist [16]. For the Unicorn [41] baseline, we used
their pre-trained model which was also trained on LSUN
horses. For the remaining categories, we also finetuned
their model in a similar fashion to ours. Our method out-
performs CSM [28] and its articulated version A-CSM [27],
which use a 3D template of the object category and 3D part
segmentation for the horse and cow category. Moreover,
our method achieved significantly better scores than Uni-
corn [41], which produces degenerate (i.e., flat) shape pre-
dictions for these classes (see Fig. 7). We visualize some
keypoint transfer results in Fig 6.
We also present 3D evaluation using results using An-
imal3D dataset [62] on a few quadruped categories in Ta-
ble 3. We calculate the normalized L1 Chamfer distance be-
tween ground truth and predictions after running ICP [3] be-Source Src. Rec. Trg. Rec. KP Transfer
Figure 6. Keypoint transfer results. Our model captures articula-
tion and viewpoint differences between images.
Supervision Method Horse Cow Sheep
None Unicorn [41] 0.091 0.118 0.134
♂,/ct∗,,†MagicPony [61] 0.046 0.040 -
/ct∗,∗SAOR-101 0.046 0.043 0.045
Table 3. 3D evaluation on the Animal3D dataset [62] using nor-
malized L1 Chamfer error, where lower is better.
Method Horse Cow Sheep Bird
Ours 44.9 33.6 29.1 51.9
Ours w/o depth 42.4 30.1 26.8 49.9
Ours w/o swap 30.8 17.7 18.4 44.5
Ours w/o sampling 27.5 20.1 18.3 38.8
Ours w/o articulation 26.3 19.4 17.9 41.7
Table 4. Keypoint transfer ablation results for SAOR where we
disable individual components to measure their impact.
tween the ground truth and predictions as there is no canon-
ical alignment between methods and ground truth data.
SAOR obtains better results than Unicorn [41] and similar
results to MagicPony [61], while being category agnostic.
4.3. Ablation Experiments
To provide insight into the impact of our proposed model
components, we provide ablation experiments on Pascal for
quadrupeds and on CUB for birds in Table 4. While depth
information helps to improve results, we can see that our ar-
ticulation and swap modules are significantly more impor-
tant. Our model trained without the swap loss obtains rea-
sonable keypoint matching performance for birds but pro-
duces degenerate flat plane-like solutions and fails miser-
ably for quadrupeds. The performance also drops if artic-
ulation is not utilized. This is because we choose random
pairs for the swap loss (unlike [41]’s more expensive pair
selection), and thus only viewpoint changes can be used to
explain the difference between images.
4.4. Qualitative Results
Comparison with Previous Work. We compare SAOR
with methods that do not use any 3D shape priors (i.e., Uni-
corn [41] and UMR [32]) and methods that use a 3D skele-
10388
Input SAOR-101 Unicorn [41]
SAOR-101 UMR [32]
Figure 7. Comparison of our model to Unicorn [41] and UMR [32]
on horses. Compared to UMR which predicts thin shapes with two
legs, we can reconstruct multi-view consistent results with four
legs. Unicorn fails to produce 3D consistent shapes.
Input SAOR-101 MagicPony [61]
Figure 8. Comparison of our model to MagicPony [61] (without
texture refinement) which uses a category specific skeleton prior
during training. We obtain on-par reconstructions compared to
MagicPony without using any 3D prior on the articulation of the
object class and with a simpler and more efficient architecture.
ton prior (i.e., MagicPony [61]). A comparison of shape
predictions for horses can be seen in Figs. 7 and 8. While
Unicorn produces reasonable reconstructions from the in-
put viewpoint, their predictions are flat from the side. UMR
also predicts thin 3D shapes and does not generate four legs.
Our method reconstructs multi-view consistent 3D shapes,
with prominent four legs. In general, our method produces
similar results to MagicPony. However, MagicPony’s hy-
brid volumetric-mesh representation requires an extra trans-
formation from implicit to explicit representation using [51]
and requires multiple rendering operations to estimate the
final shape. Moreover, the texture predictions of our meth-
ods do not require test-time optimization.
Deformation and Articulation Disentanglement. In
Fig. 5 we illustrate the disentanglement of articulation and
deformation learned by our model. Given two images de-
picting differently articulating instances, we interpolate the
deformation and articulation features between them to vi-
sualize reconstructions. While interpolating the articulation
feature changes the result, changing the deformation feature
does not as the shape difference between both images can
Figure 9. Our model, trained on real-world images, plausibly es-
timates 3D shape and viewpoint from different domains, e.g., car-
toons, line drawings, and paintings.
be explained via articulation changes.
Part Consistency. After finetuning the pre-trained horse
model on different quadruped categories, we observe that
the predicted part assignments stay consistent across cate-
gories, as can be seen in Fig. 1. For instance, although the
shapes of giraffes and elephants are significantly different,
our method is able to assign similar parts to similarly ar-
ticulated areas. Here, each color represents the part that is
predicted with the highest probability from the part assign-
ment matrix Wby the articulation network fa.
Out-of-Distribution Images. We illustrate the generaliza-
tion capabilities of our model by predicting 3D shapes from
non-photoreal images, e.g., drawings. Fig. 9 shows that we
can reconstruct plausible shapes and poses from input im-
ages that are very different from the training domain.
4.5. Discussion and Limitations
Although our proposed approach is able to estimate plausi-
ble 3D shapes, the texture predictions are still not fully re-
alistic. This could be improved using test-time refinement
similar to [61] or alternative texture representations. During
training, our method uses estimated silhouettes and relative
depth maps as supervision. Both depth maps and silhouettes
come from a generic pre-trained models [24, 47], hence are
free to acquire. Finally, our method fails to predict accurate
shape if the input images contains unusual viewpoints that
differ significantly from the training images or the object is
not full visible. We present some examples of these failure
cases in the supplementary material.
5. Conclusion
We presented SAOR, a new approach for single-view ar-
ticulated object reconstruction. SAOR is capable of pre-
dicting the 3D shape of articulated object categories with-
out requiring any explicit object-specific 3D information,
e.g., 3D templates or skeletons, at training time. To achieve
this, we learn to segment objects into parts which move to-
gether and propose a new swap-based regularization loss
that improves 3D shape consistency in addition to simplify-
ing training compared to competing methods. These con-
tributions enable us to simultaneously represent over 100
different categories, with diverse shapes, in one model.
Acknowledgments: OMA was in part supported by univer-
sity partner contributions to the Alan Turing Institute.
10389
References
[1] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. Scape: shape
completion and animation of people. In SIGGRAPH , 2005.
2
[2] Sara Beery, Dan Morris, and Siyu Yang. Efficient pipeline
for camera trap image review. In Data Mining and AI for
Conservation Workshop at KDD , 2019. 6
[3] Paul J Besl and Neil D McKay. Method for registration of
3-d shapes. In PAMI , 1992. 7
[4] Benjamin Biggs, Oliver Boyne, James Charles, Andrew
Fitzgibbon, and Roberto Cipolla. Who left the dogs out?
3d animal reconstruction with expectation maximization in
the loop. In ECCV , 2020. 2
[5] V olker Blanz and Thomas Vetter. A morphable model for the
synthesis of 3d faces. In SIGGRAPH , 1999. 1, 2
[6] Thomas J Cashman and Andrew Fitzgibbon. What shape are
dolphins? building 3d morphable models from 2d images.
PAMI , 2012. 1
[7] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese. 3d-r2n2: A unified approach for
single and multi-view 3d object reconstruction. In ECCV ,
2016. 1, 2
[8] Mathieu Desbrun, Mark Meyer, Peter Schr ¨oder, and Alan H
Barr. Implicit fairing of irregular meshes using diffusion and
curvature flow. In SIGGRAPH , 1999. 6
[9] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christo-
pher KI Williams, John Winn, and Andrew Zisserman. The
pascal visual object classes challenge: A retrospective. In
IJCV , 2015. 7
[10] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik.
Shape and viewpoint without keypoints. In ECCV , 2020. 2,
4, 7
[11] U. Grenander. Lectures in Pattern Theory: Volume 2 Pattern
Analysis . 1978. 3
[12] Rıza Alp G ¨uler, Natalia Neverova, and Iasonas Kokkinos.
Densepose: Dense human pose estimation in the wild. In
CVPR , 2018. 1, 3
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 6
[14] Philipp Henzler, Niloy J Mitra, and Tobias Ritschel. Escap-
ing plato’s cave: 3d shape from adversarial rendering. In
ICCV , 2019. 3
[15] Tao Hu, Liwei Wang, Xiaogang Xu, Shu Liu, and Jiaya Jia.
Self-supervised 3d mesh reconstruction from single images.
InCVPR , 2021. 2
[16] iNaturalist. iNaturalist. www.inaturalist.org , ac-
cessed 8 November 2023. 6, 7
[17] Alec Jacobson, Zhigang Deng, Ladislav Kavan, and John P
Lewis. Skinning: Real-time shape deformation. In SIG-
GRAPH Courses . 2014. 4
[18] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf
on a diet: Semantically consistent few-shot view synthesis.
InICCV , 2021. 2
[19] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and
Jitendra Malik. Learning category-specific mesh reconstruc-
tion from image collections. In ECCV , 2018. 1, 2, 3, 4, 7
[20] Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jiten-dra Malik. Category-specific object reconstruction from a
single image. In CVPR , 2015. 1
[21] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neu-
ral 3d mesh renderer. In CVPR , 2018. 1, 2
[22] Sameh Khamis, Jonathan Taylor, Jamie Shotton, Cem Ke-
skin, Shahram Izadi, and Andrew Fitzgibbon. Learning an
efficient model of hand shape variation from depth images.
InCVPR , 2015. 2
[23] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. In ICLR , 2015. 6
[24] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Doll ´ar, and
Ross Girshick. Segment anything. In ICCV , 2023. 6, 8
[25] Filippos Kokkinos and Iasonas Kokkinos. Learning monoc-
ular 3d reconstruction of articulated categories from motion.
InCVPR , 2021. 2, 3
[26] Filippos Kokkinos and Iasonas Kokkinos. To the point:
Correspondence-driven monocular 3d category reconstruc-
tion. In NeurIPS , 2021. 3
[27] Nilesh Kulkarni, Abhinav Gupta, David Fouhey, and Shub-
ham Tulsiani. Articulation-aware canonical surface map-
ping. In CVPR , 2020. 2, 3, 5, 7
[28] Nilesh Kulkarni, Abhinav Gupta, and Shubham Tulsiani.
Canonical surface mapping via geometric cycle consistency.
InICCV , 2019. 2, 3, 7
[29] Tejas D Kulkarni, William F Whitney, Pushmeet Kohli, and
Josh Tenenbaum. Deep convolutional inverse graphics net-
work. In NeurIPS , 2015. 3
[30] John P Lewis, Matt Cordner, and Nickson Fong. Pose space
deformation: a unified approach to shape interpolation and
skeleton-driven deformation. In SIGGRAPH , 2000. 2, 4
[31] Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xi-
aolong Wang, Ming-Hsuan Yang, and Jan Kautz. Online
adaptation for consistent mesh reconstruction in the wild. In
NeurIPS , 2020. 3
[32] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun
Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised
single-view 3d reconstruction via semantic consistency. In
ECCV , 2020. 2, 3, 4, 5, 7, 8
[33] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas
Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi,
and Jiajun Wu. Learning the 3d fauna of the web. 2024. 3
[34] Zhouyingcheng Liao, Jimei Yang, Jun Saito, Gerard Pons-
Moll, and Yang Zhou. Skeleton-free pose transfer for styl-
ized 3d characters. In ECCV , 2022. 2, 4
[35] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3:
Zero-shot one image to 3d object. In ICCV , 2023. 2
[36] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft ras-
terizer: A differentiable renderer for image-based 3d reason-
ing. In ICCV , 2019. 1, 6
[37] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. TOG , 2015. 1, 2
[38] Matthew M Loper and Michael J Black. Opendr: An approx-
imate differentiable renderer. In ECCV , 2014. 1
[39] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In CVPR ,
10390
2019. 2
[40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. Communications of the ACM , 2021. 2
[41] Tom Monnier, Matthew Fisher, Alexei A Efros, and Mathieu
Aubry. Share with thy neighbors: Single-view reconstruction
by cross-instance consistency. In ECCV , 2022. 2, 3, 4, 5, 7,
8
[42] Natalia Neverova, David Novotn ´y, and Andrea Vedaldi.
Continuous surface embeddings. In NeurIPS , 2020. 3
[43] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-
ing scenes as compositional generative neural feature fields.
InCVPR , 2021. 2, 4
[44] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
Martin-Brualla. Nerfies: Deformable neural radiance fields.
InICCV , 2021. 2
[45] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR ,
2023. 2
[46] Ren ´e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-
sion transformers for dense prediction. ICCV , 2021. 6
[47] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. PAMI , 2022. 6, 8
[48] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-
lor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia
Gkioxari. Accelerating 3d deep learning with pytorch3d.
arXiv:2007.08501 , 2020. 6
[49] Lawrence G Roberts. Machine perception of three-
dimensional solids . PhD thesis, MIT, 1963. 1
[50] Nadine Rueegg, Silvia Zuffi, Konrad Schindler, and
Michael J Black. Barc: Learning to regress 3d dog shape
from images by exploiting breed information. In CVPR ,
2022. 2, 4
[51] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and
Sanja Fidler. Deep marching tetrahedra: a hybrid represen-
tation for high-resolution 3d shape synthesis. In NeurIPS ,
2021. 8
[52] Anastasis Stathopoulos, Georgios Pavlakos, Ligong Han,
and Dimitris N Metaxas. Learning articulated shape with
keypoint pseudo-labels from web images. In CVPR , 2023. 3
[53] Jonathan Taylor, Richard Stebbing, Varun Ramakrishna,
Cem Keskin, Jamie Shotton, Shahram Izadi, Aaron Hertz-
mann, and Andrew Fitzgibbon. User-specific hand modeling
from monocular depth sequences. In CVPR , 2014. 2
[54] James Thewlis, Hakan Bilen, and Andrea Vedaldi. Unsuper-
vised learning of object frames by dense equivariant image
labelling. In NeurIPS , 2017. 7
[55] Edith Tretschk, Navami Kairanda, Mallikarjun B R, Rishabh
Dabral, Adam Kortylewski, Bernhard Egger, Marc Haber-
mann, Pascal Fua, Christian Theobalt, and Vladislav
Golyanik. State of the art in dense monocular non-rigid 3d
reconstruction. In Computer Graphics Forum , 2022. 2
[56] Shubham Tulsiani, Nilesh Kulkarni, and Abhinav Gupta. Im-
plicit mesh reconstruction from unannotated image collec-
tions. arXiv:2007.08504 , 2020. 2
[57] Kalyan Alwala Vasudev, Abhinav Gupta, and Shubham Tul-siani. Pre-train, self-train, distill: A simple recipe for super-
sizing 3d reconstruction. In CVPR , 2022. 2
[58] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 6, 7
[59] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei
Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh
models from single rgb images. In ECCV , 2018. 1, 2
[60] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and An-
drea Vedaldi. Dove: Learning deformable 3d objects by
watching videos. IJCV , 2023. 2, 3, 4, 7
[61] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rup-
precht, and Andrea Vedaldi. Magicpony: Learning articu-
lated 3d animals in the wild. In CVPR , 2023. 2, 3, 4, 5, 6, 7,
8
[62] Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen,
Pengliang Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao
Wang, et al. Animal3d: A comprehensive dataset of 3d ani-
mal pose and shape. In ICCV , 2023. 7
[63] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,
Forrester Cole, Huiwen Chang, Deva Ramanan, William T
Freeman, and Ce Liu. Lasr: Learning articulated shape re-
construction from a monocular video. In CVPR , 2021. 2,
3
[64] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic,
Forrester Cole, Ce Liu, and Deva Ramanan. Viser: Video-
specific surface embeddings for articulated 3d shape recon-
struction. In NeurIPS , 2021. 3
[65] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Ru-
binstein, Ming-Hsuan Yang, and Varun Jampani. Lassie:
Learning articulated shape from sparse image ensemble via
3d part discovery. In NeurIPS , 2022. 3, 4
[66] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Ru-
binstein, Ming-Hsuan Yang, and Varun Jampani. Hi-lassie:
High-fidelity articulated shape and skeleton discovery from
sparse image ensemble. In CVPR , 2023. 3
[67] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-
supervised mesh prediction in the wild. In CVPR , 2021. 2,
3
[68] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance fields from one or few images. In
CVPR , 2021. 2
[69] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser, and Jianxiong Xiao. LSUN: Construction of
a Large-scale Image Dataset using Deep Learning with Hu-
mans in the Loop. arXiv:1506.03365 , 2015. 6
[70] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR , 2018. 6
[71] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and
Michael J Black. Three-d safari: Learning to estimate ze-
bra pose, shape, and texture from images “in the wild”. In
ICCV , 2019. 2, 3
[72] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and
Michael J Black. 3d menagerie: Modeling the 3d shape and
pose of animals. In CVPR , 2017. 2
10391
