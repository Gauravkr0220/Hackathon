Mean-Shift Feature Transformer
Takumi Kobayashiyz
yNational Institute of Advanced Industrial Science and Technology, Japan
zUniversity of Tsukuba, Japan
takumi.kobayashi@aist.go.jp
Abstract
Transformer models developed in NLP make a great im-
pact on computer vision Ô¨Åelds, producing promising per-
formance on various tasks. While multi-head attention, a
characteristic mechanism of the transformer, attracts keen
research interest such as for reducing computation cost, we
analyze the transformer model from a viewpoint of feature
transformation based on a distribution of input feature to-
kens. The analysis inspires us to derive a novel transfor-
mation method from mean-shift update which is an effective
gradient ascent to seek a local mode of distinctive repre-
sentation on the token distribution. We also present an efÔ¨Å-
cient projection approach to reduce parameter size of lin-
ear projections constituting the proposed multi-head fea-
ture transformation. In the experiments on ImageNet-1K
dataset, the proposed methods embedded into various net-
work models exhibit favorable performance improvement in
place of the transformer module. Codes are available at
https://github.com/tk1980/MSFtransformer .
1. Introduction
After the renaissance of neural networks, deep models
have been attracting keen attention in pattern recognition
Ô¨Åelds. While convolutional neural networks (CNNs) pro-
vide promising performance especially on computer vision
tasks, transformer [44] revolutionizes the network archi-
tecture. The transformer originated from NLP efÔ¨Åciently
encodes (causal) dependency in a sequence by means of
attention mechanism replacing complicated recurrent net-
works [19]. It is now distributed across various recognition
Ô¨Åelds [16] such as for images [14] and videos [3].
Much research effort has been made to improve trans-
former networks. In computer vision, a vision transformer
(ViT) [14] successfully encodes images by utilizing image-
patch tokens. Transformer modules on the patch tokens
are capable of extracting non-local visual features [47] be-
yond the local characteristics exploited by convolutions in
CNN. On the other hand, convolutions are incorporated toexhibit synergy with the transformers [32, 36, 48, 49] such
as for fusing global and local information as well as uti-
lizing convolutional inductive bias that the patch-token ap-
proach misses [12]. The original transformer module is di-
rectly embedded even to those sophisticated networks due
to its versatile applicability.
The transformer module itself is improved mainly in
terms of attention which manifests a distinctive mechanism
in the module. Attention is an important process for recog-
nition as studied in neuroscience [28] and the transformer
utilizes attention weights (map) among input tokens, the
size of which is quadratic in the number of tokens; specif-
ically, multiple attention weights are extracted by means of
multi-head attention (MHA) in the transformer. Thus, there
are plenty of works to provide efÔ¨Åcient computation of at-
tention such as by hashing [23], clustering [34], low-rank
approximation [2, 8, 22, 46, 51] and IO-aware implemen-
tation [11]. The attention maps are also enhanced, e.g., by
aggregating multiple maps [18, 62, 63] and exploring ef-
fective similarity (kernel) function [43, 58]. A transformer
model leverages the attention weights to transforming input
feature tokens toward discriminative feature representation.
In this paper, we focus on the process to transform fea-
ture vectors in the transformer. In contrast to the approaches
which focus only on attention weights, we consider the
whole transformation process that aggregates feature tokens
in MHA to modify the feature representation via a resid-
ual connection. Thereby, rather apart from semantic con-
cept of attention, we regard the transformer mechanism as
a feature transformation to explore characteristic points on
a probabilistic distribution composed of input feature to-
kens. Through the lens of the probabilistic density function,
we can rewrite the transformer model into a gradient-ascent
updating to seek the local maxima, modes , of the distribu-
tion. This analysis inspires us to derive a simple yet effec-
tive transformation from mean-shift [40] update of efÔ¨Åcient
mode seeking on the distribution. In the mean-shift frame-
work utilizing a multi-head approach, linear projection from
an input feature space into subspace representation is a
key process. We also present an efÔ¨Åcient grouped projec-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
6047
tion to reduce parameter sizes of the projection matrices
while retaining performance. It is related to grouped con-
volution [26, 50, 56], and we further analyze the grouping
for improving back-propagation in a multi-head framework.
The proposed feature transformation works in place of the
transformer module, thereby being applicable to the ex-
tensive methods that improve transformer-based networks
mentioned in Section 1.1.
Our contributions are summarized as follows.
‚Ä¢ We provide a novel viewpoint to analyze transformer
mechanism by utilizing a probabilistic distribution of in-
put feature tokens.
‚Ä¢ Based on the analysis, we propose a mean-shift based fea-
ture transformation, MSF-transformer , as an extension of
the transformer. An efÔ¨Åcient grouped projection is also
presented to enhance efÔ¨Åciency of the MSF-transformer.
‚Ä¢ In the experiments on ImageNet dataset, the method is
thoroughly evaluated from various aspects regarding fea-
ture transformation and exhibits favorable performance
on various networks in place of a transformer module.
1.1. Related works
Vision Transformer. Transformer [44] Ô¨Årst proposed in
NLP makes a great impact on computer vision such as
through vision transformer (ViT) [14] which is versatilely
applied to diverse vision tasks [3, 14, 29, 60]; comprehen-
sive survey is shown in [16]. In contrast to words , ingredi-
ents of NLP, images are not intrinsically tokenized, which
encourages improving tokenization of simple patch parti-
tioning [14] by Swin-Transformer [30] and T2T [52]. The
transformer can be combined with CNNs to further improve
the efÔ¨Åcacy on vision tasks, such as by means of convo-
lution stems [49], light-weight CNN model [32] and con-
volutional local dependency extraction [48]. ViTs are also
thoroughly evaluated in comparison to CNNs under huge
computation budget [35]. In [10], positional self-attention
in transformers is shown to be connected to convolution,
which induces a sophisticated approach to enhance posi-
tional encoding in the self-attention model [12]. Since ViTs
are regarded as data-hunger models demanding large com-
putation resources [14, 54], effective training approaches
are explored in [37, 41, 42] and an optimizer is sophisti-
cated to efÔ¨Åciently train ViTs [6, 15]. While ViTs are ad-
vanced from those various aspects, the original transformer
module is directly applied in most models. In this paper, we
improve the feature transformation mechanism of the trans-
former module through analysis about a token distribution.
Attention. Transformers are characterized by their atten-
tion mechanism which is embedded in multi-head atten-
tion (MHA) [44]. There are some works to improve the
attention weights which are built on pair-wise token simi-
larities. Multiple attention maps are fused to enhance the
weights in the MHA framework [62, 63] and a residualconnection [18]. Similarity functions to compute atten-
tion are explored by means of MLP [58] and kernel func-
tions [43]. On the other hand, as the attention weights
have time and memory complexity quadratic by the num-
ber of tokens, the computation issue is addressed in lots of
works. To efÔ¨Åciently search similar pairs of tokens (query
and key), clustering [34] and hashing [23] are incorpo-
rated in the process of attention computation. The atten-
tion weight (matrix) is efÔ¨Åciently described by low-rank ap-
proximation [2, 8, 22, 46, 51]. In [11], IO-aware approach
is proposed to compute attention weights in a computation-
ally efÔ¨Åcient manner. While those approaches mainly focus
on attention weights and their computation, we analyze the
transformer from a viewpoint of feature transformation , not
limited to the attention; the proposed method could incor-
porate those efÔ¨Åcient attention computation techniques.
Mean Shift. In computer vision, mean-shift algorithm [7,
9] is mostly applied to clustering. Recently, it is lever-
aged to loss function to measure afÔ¨Ånity between feature
representation for self-supervised learning [24, 25], and is
also applied to key-point detection on an attention map for
instance segmentation [27]. In contrast, we focus on the
updating form of mean shift and analyze the transformer
mechanism through the lens of the mean-shift updating to
show resemblance between those approaches.
Grouped Convolution. To reduce parameter size of con-
volution kernels, grouped convolution is applied [26, 50] in
CNN. We also employ a grouping approach to enhance pa-
rameter efÔ¨Åciency in linear projection of token features. It
is similar to interleaved group convolution [56, 57] which
stacks couples of group convolution and channel shufÔ¨Çing.
We further analyze the effect of interleaved grouping on
back-propagation in the multi-head framework.
2. Method
We begin with brieÔ¨Çy reviewing a transformer module [14,
44] (Section 2.1), and then formulate the feature transfor-
mation framework using a token distribution (Section 2.2)
to derive the proposed method, dubbed MSF-transformer
(Section 2.2.2). Then, an efÔ¨Åcient approach to linearly
project features into the distribution is presented in Sec-
tion 2.3.
2.1. Transformer
Suppose we have mtokens ofd-dimensional feature vec-
tors to form a matrix of X= [x1;;xm]2Rdm, and
transform them by means of multi-head attention (MHA)
withHheads [44]. As shown in Figure 1a, the h-th head
is equipped with four types of projection matrices Rd^d,
fQh;Kh;Vh;Whg.Transformer converts a feature vector
6048
Sum
LinearMatMulScaleSoftMaxMatMul
Linear LinearLinear
h-th head
(a) Original transformer [44]
LinearDistCompScaleSoftMaxMatMul
Linear LinearLinear
LinearSum
h-th head
 (b) MSF-transformer
Figure 1. Transformer models.
xintoxby applying those projection matrices in MHA as
^x=HX
h=1Whh
V>
hX(X>KhQ>
hx)i
; (1)
x=x+^x; (2)
whereis a softmax function processing mcomponents
with a temperature of ^d1
2[44]. MHA (1) contains three in-
gredients of QUERY Q>
hx,KEYK>
hxand VALUE V>
hx.
Attention weights (X>KhQ>
hx)are constructed by mea-
suring similarity between QUERY and KEY via the softmax
functionin order to effectively aggregate VALUE . The at-
tention weights are distributed over non-local features [47]
in contrast to convolution, for improving feature represen-
tation. It should be noted that this attention process is per-
formed in ^d-dimensional subspaces through projection by
Qh;KhandVhfrom the input d-dimensional features X.
While most works focus on the attention mechanism
composed of QUERY ,KEY and VALUE , we explicitly in-
corporate both the subsequent linear projection by WEIGHT
Whin (1) and the residual connection (2) which accom-
pany with MHA in the transformer model [44] (Figure 1a).
Namely, we focus on the feature transformation process that
an input xis projected into the ^d-dimensional subspace via
MHA and then back-projected into the d-dimensional input
space by Wh, followed by residual connection (2) to Ô¨Ånally
modify the feature representation. So transformed feature x
is fed into MLP and subsequent transformer layers [14, 44].
2.2. Distribution of input token features
We view the feature tokens from a probabilistic viewpoint.
Supposemsamples (tokens) of ^d-dimensional features,
K= [k1;;km]2R^dm, form a distribution. The dis-
tribution can be characterized by their modes , local maxima
of the probability density function. For detecting the modes,
a probe point q2R^dexplores the distribution by means ofa gradient ascent of the probability p(q)as
p(q) =1
ZmX
i=1g(ki;q); (3)
^q,d
dqlogp(q) =Pm
i=1d
dqg(ki;q)
Pm
i=1g(ki;q);q q+^q;(4)
where gis a kernel function to build a kernel density esti-
mation (KDE) [45] in (3) and is a step size of gradient
ascent.
We relate the probe point q2R^dwith a higher-
dimensional representation x2Rdvia projection q=
Q>x, thereby formulating the gradient w.r.t xin
^x,dq
dxd
dqlogp(q) =Q^q: (5)
By incorporating (4) into (5), the probe xis updated to x
by a gradient ascent of
x=x+^x=x+Q"Pm
i=1d
dqg(ki;q)
Pm
i=1g(ki;q)#
:(6)
2.2.1 Connection to Transformer
A kernel function gcan be Ô¨Çexibly designed in the KDE (3).
One practical possibility is to set g(k;q) = exp( ^d 1
2k>q),
though it produces ill-deÔ¨Åned probability1; the gradient as-
cent (6) is written as
x=x+^d 1
2QP
iexp( ^d 1
2k>
iq)kiP
iexp( ^d 1
2k>
iq)(7)
=x+^d 1
2Q
K(K>Q>x)
: (8)
As is the case with q, we assume that samples Kare
also projections from the higher-dimensional features X2
RdmviaK=K>X, to further rewrite (8) into
x=x+^d 1
2Q
K>X(X>KQ>x)
; (9)
which is a gradient ascent toward the local mode around x
in the pseudo probability density function of
p(x)/mX
i=1exp( ^d 1
2x>
iKQ>x): (10)
It should be noted that the update form (9) bears resem-
blance to the transformer (1, 2); only difference lies in the
back-projection from ^d-dimensional to d-dimensional space
where (9) applies ^d 1
2QK>instead of WV>in (1). So,
it can be said that the transformer (1) Ô¨Çexibly describes the
projection matrices by breaking the ties among the matrices
1The normalization Zin (3) is not Ô¨Ånite.
6049
in (9) for effective feature representation learning. Thus,
the transformer is naively interpreted as feature update that
moves xtoward its local mode of a representative and dis-
tinctive point on the distribution of input feature tokens.
This analysis also reveals an issue that the transformer
(1) might contain a bias toward increasing feature norm
kxk2through end-to-end learning since the pseudo prob-
ability (10) could be easily increased by enlarging the norm
of a probe x. Such a bias to increasing feature norm
is derived from the ill-posed kernel function g(k;q) =
exp( ^d 1
2k>q), though the bias can be practically mitigated
by normalization techniques, such as LayerNorm [1], em-
bedded in the networks [14, 44].
2.2.2 Mean-shift update as Transformer
Following a standard KDE [9], we apply a Gaussian kernel
g(k;q) = exp( 1
2^d 1
2kk qk2
2)to formulate the gradient
ascent (6) in
x=x+ (11)
^d 1
2Qh
K>X 
 1
2kK>xi Q>xk2
2	m
i=1
 Q>xi
;
where the softmax function produces an m-dimensional
vector. This is a mean-shift update [7, 9]. As KDE us-
ing the Gaussian kernel provides well deÔ¨Åned probabil-
ity density, the mean-shift update (11) effectively moves
xtoward its local mode as shown in [9], in contrast to
the case of ill-posed kernel function exp( ^d 1
2q>k)in the
original transformer (Section 2.2.1); the probability density
p(x) =Pm
i=1exp( 1
2^d 1
2kK>xi Q>xk2
2)not neces-
sarily favors a probe point xof larger norm. Other than the
kernel function, (11) is different from (9) in that differen-
tialis computed by subtracting the probe vector Q>x. This
effectively orients the update toward local maxima without
bias for feature norm.
Following the relaxation from the gradient ascent (9) to
the transformer (1), we break ties among projection matri-
ces in (11) to propose mean-shift feature (MSF) transformer
of
x=x+ (12)
HX
h=1Whh
V>
hX 
 1
2kK>
hxi Q>
hxk2
2	m
i=1
 P>
hxi
;
where Ph2Rd^dis additionally introduced to represent
PROBE . The architecture of the MSF-transformer is de-
picted in Figure 1b which modiÔ¨Åes the original transformer
(Figure 1a) in simple yet effective ways regarding PROBE
and Gaussian kernel function2based on the above analysis.
2The softmax(f 1
2kK>
hxi Q>
hxk2
2gm
i=1)is efÔ¨Åciently computed
with a negligible overhead compared to the standard one (X>KhQ>
hx)
as shown in supplementary material.
head1
head2
head3
head4
back
prop(a) Block-wise grouping
head1
head2
head3
head4back
prop (b) Interleaved grouping
Figure 2. Grouped projection with multiple heads. These are
schematic diagrams of 2-grouped projection by matrix-vector mul-
tiplication of, e.g., [q>
1; ::: ;q>
H]>=[Q1; ::: ;QH]>xwithH=4.
2.3. Grouped projection
It is known that KDE and mean-shift update degenerate
in a high-dimensional feature space [33]. In transformer
models (Figure 1), the issue of high dimensionality is mit-
igated by projecting high-dimensional feature xinto lower
^d-dimensional space so that the updating in (1, 12) works
effectively. The MHA approach in (1, 12) constructs multi-
ple^d-dimensional subspaces rendering diverse feature dis-
tributions to encode various (visual) characteristics at lo-
cal modes for enhancing feature representation. Those sub-
space representations are back-projected into the original
space via WEIGHT Whand then merged as shown in Fig-
ure 1. While the multi-head projection plays an important
role in the transformer models, it consumes considerable
amount of memory (parameters); note that, for computation
efÔ¨Åciency, projection matrices are concatenated to larger
one, e.g., Q= [Q1;;QH]2RdH^d, to enjoy efÔ¨Åcient
matrix multiplication.
Visual characteristics embedded in each projection are
considered to be not distributed across whole dfeature com-
ponents in an input xbut limited to a subset of features.
To effectively exploit such localized features in MHA, we
apply grouped linear projection in a similar way to the
grouped convolution [26, 50]; as shown in Figure 2, G-
grouped projection reduces parameter size of linear projec-
tion into1
Gand the number of groups is arbitrarily deter-
mined, independently of the number of heads H.
We analyze the grouped projection from a viewpoint of
back-propagation. The ordinary grouping in Figure 2a pro-
duces a block-wise output in which one head is Ô¨Ålled with
one of groups. Thereby, each head is connected only to
small portion of feature components, passing gradient in-
formation only to that part of features via back-propagation.
This impedes end-to-end learning as each head in MHA
contributes only to learning one subset (group) of fea-
tures without inter-connection among groups in the back-
propagation. Therefore, in a similar way to [56, 57], we
apply interleaved grouped projection to alleviate the above-
mentioned drawback of grouping in the back-propagation.
As shown in Figure 2b, grouped weights are not block-
sparse but interleaved so that each head is Ô¨Ålled with the
6050
metric
probe
feature
representation
0.43
0.31
0.350.33
0.560.27
0.43 0.33 0.27 0.22
0.31 0.25 0.21
0.35 0.29
0.56
distribution
sample
(a) MSF (Figure 1b)
0.43 0.28 0.23
0.31 0.21
0.32
metric
probe
0.43
0.330.31distribution
sample
 (b) Original (Figure 1a)
Figure 3. Relationships among projection matrices in the trans-
former models. The upper-triangle matrix shows subspace simi-
larities (13) computed between the projection weights of the trans-
formers embedded in ImageNet pretrained ViT-S. The numbers in
diagrams show high similarity scores.
output projections from all groups. It facilitates learning by
delivering gradients from one head to whole feature compo-
nents in back-propagation. This interleaved grouped projec-
tion improves multi-head approach, which is thus different
from [56, 57] stacking interleaved group convolutions to ef-
Ô¨Åciently approximate dense convolution.
3. Results
We apply the proposed method to transformer networks
on ImageNet-1K dataset [13]; the MSF-transformer (Fig-
ure 1b) replaces all the transformer modules (Figure 1a) in
the networks. The method is Ô¨Årst analyzed through ablation
studies using ViT-S [4] (Table 5) in Section 3.1, and then
evaluated on various network models in Section 3.2.
3.1. Ablation study
Training protocol. ViT-S [4] is optimized by AdamW [31]
with 0.05 weight decay and 0.001 initial learning rate which
is decayed in a cosine schedule. For data augmentation,
we apply random resized cropping [38] with three types of
appearance Ô¨Çuctuation [42] as well as mixup [55] and cut-
mix [53]. We train ViT-S over 100 epochs with 1024 batch
size on four GPUs of NVIDIA RTX3090; detailed training
settings are shown in supplementary material. For evalua-
tion, top-1 classiÔ¨Åcation accuracies are measured on an Im-
ageNet [13] validation set.
Types of projection. As shown in Figure 1b, the proposed
formulation (12) contains ^d-dimensional subspace features
ofPROBE P>
hxas well as QUERY ,KEY and VALUE , which
are back-projected to d-dimensional space by WEIGHT Wh;
thus there are totally Ô¨Åveprojection matrices in the method.
First, we qualitatively analyze relationships among them
from a viewpoint of mean-shift updating (Section 2.2.2),
as summarized in Figure 3a. By comparing the mean-
shift update (11) with MSF-transformer (12), we can real-
ize that KEY and VALUE are coupled as distribution samplesModel QUERY KEY VALUE PROBE WEIGHT Acc. Param (M)
Simplest
aMSF Q Q Q Q Q 61.13 16.60
2-matrices
bMSF Q K K Q Q 77.16 18.37
cMSF Q Q V V V 69.97 18.37
dMSF Q Q V V Q 72.54 18.37
eMSF Q Q Q P P 75.50 18.37
fMSF Q K K K Q 77.84 18.37
gMSF Q K K Q K 74.61 18.37
3-matrices
hMSF Q K K K W 78.82 20.14
iMSF Q K K Q W 78.51 20.14
jMSF Q K K P P 78.74 20.14
kMSF Q K K P Q 77.87 20.14
4-matrices
lMSF Q Q V P W 77.23 21.91
mMSF Q K K P W 79.19 21.91
nMSF Q K V V W 79.49 21.91
oMSF Q K V P P 79.52 21.91
pMSF Q K V Q W 79.19 21.91
qMSF Q K V P Q 78.97 21.91
Full
rMSF Q K V P W 79.79 23.68
Table 1. Various types of conÔ¨Ågurations for projections in MSF-
transformer (12) on 100-epoch trained ViT-S. We report top-1 ac-
curacy (%) on an ImageNet validation set with the number of pa-
rameters in each model. Colored letters indicate shared matrices.
Model QUERY KEY VALUE WEIGHT Acc. Param (M)
Simplest
aOrig. Q Q Q Q 71.34 16.60
2-matrices
bOrig. Q K K Q 76.25 18.37
cOrig. Q Q Q W 76.04 18.37
dOrig. Q Q V Q 77.25 18.37
eOrig. Q Q V V 74.64 18.37
3-matrices
fOrig. Q Q V W 78.04 20.14
gOrig. Q K K W 78.38 20.14
hOrig. Q K V V 76.47 20.14
Full
iOrig. Q K V W 78.98 21.91
Table 2. Various types of conÔ¨Ågurations for projections in the orig-
inal transformer (1) on ViT-S in the same way as Table 1.
ki, while the other threes of QUERY ,PROBE and WEIGHT
are connected via a probe point q. In (12), VALUE is
comparable with PROBE to form a differential vector in ^d-
dimensional feature representation. Besides, the attention
weights are computed based on a metric constructed by
QUERY and KEY. We can similarly analyze the original
transformer (Figure 1a) for clarifying qualitative relation-
6051
ships among QUERY ,KEY,VALUE and WEIGHT as shown
in Figure 3b.
Next, we further analyze their similarities by using the
learnt projection matrices ofQ;K;V;PandWin a quan-
titative way. Since a subspace is a essential representation to
characterize projection, we compute subspace similarity [5]
among projection matrices; let singular value decomposi-
tion of a matrix X2Rd^dbeX=UXXV>
X, and the
similarity between subspaces of XandYis given by
sim(X;Y) =1
^dtrace (UXU>
XUYU>
Y) =1
^d^dX
r=1cos2r;
(13)
whereris ther-th canonical angle between two subspaces,
X;Y2fQh;Kh;Vh;Ph;Whg, and 0sim1. The
similarity scores among projection matrices are shown in
Figure 3a together with a diagram to describe the relation-
ships. This quantitative measurement is roughly coincident
with the qualitative analysis discussed above. A high simi-
larity can be found in the relationship between PROBE and
WEIGHT which are used for forward-backward projection in
(12). The connections of QUERY -KEY and VALUE -PROBE
also exhibit relatively high similarities in accordance with
the above qualitative analysis. Then, the same analyses are
applied to the original transformer as shown in Figure 3b,
demonstrating that the relationships among four projection
matrices resemble those of MSF-transformer (Figure 3a).
However, as it lacks PROBE , there are less strong relation-
ships like VALUE -PROBE and WEIGHT -PROBE in Figure 3a.
ConÔ¨Åguration of projections. While MSF-transformer
(12) individually assigns matrices to those projections, there
is a possibility to tie some of them by sharing projection
matrices according to the discussion in Figure 3a and Sec-
tion 2.2.2. From that viewpoint, we analyze conÔ¨Ågurations
of projections in Table 1. Since increasing parameters gen-
erally improves performance, we compare the conÔ¨Ågura-
tions under the same budget of parameter size as follows.
The simplest model is given by sharing a single matrix
across all the projections (Table 1 a). While it minimizes
memory consumption, such a hard constraint to tie all the
projections degrades performance in comparison to the full
model (Table 1 n), and thus we require more parameter bud-
get to attain favorable performance.
By using two projection matrices, the mean-shift up-
date (11) is constructed to interestingly produce favorable
performance (Table 1 b); the smaller model of 18.37M pa-
rameters even outperforms ResNet-50 (75.97% in Table 6)
of 25.50M-parameterized model. It validates our approach
that relates mean-shift update with feature transformation.
Table 1 c-fshow the performance results by tying projec-
tion matrices in different semantic groups, demonstrating
superiority of the mean-shift update ( b). Particularly, the re-
sults of c-efail to validate the connection between QUERYModel Test Acc. Training loss Training Acc.
MSF (12) 79.79 2.50 69.90
Residual (14) 79.28 2.43 71.20
Gauss-Orig. 79.31 2.51 69.34
Table 3. Performance comparison in terms of mean-shift for-
mulation in ViT-S. Gauss-Orig indicates the original transformer
equipped with Gaussian kernel, i.e., the Ô¨Årst term in (14).
and KEY. Due to Gaussian kernel in MSF-transformer
(12), assimilation of QUERY and KEY leading tokK>
hxi 
Q>
hxik= 08iinevitably maximizes self-attention weights
(diagonal components of attention weights). It may impede
metric learning on KEY/QUERY representation, deteriorat-
ing performance. On the other hand, the mean-shift update
(Table 1 b) is further improved by slightly touching PROBE
to relate with VALUE in Table 1 f; the remaining connection
between QUERY and WEIGHT is qualitatively validated as
discussed above. In contrast, by tying unrelated ones, per-
formance is degraded, e.g., in Table 1 gwhere a projection
matrix is shared among KEY,VALUE and WEIGHT which
are not connected semantically nor quantitatively; they ex-
hibit low similarity scores in Figure 3a.
Then, we disentangle the successful conÔ¨Ågurations of
Table 1 bfto have three projection matrices. As shown in
Table 1 h-k, performance is effectively boosted by increas-
ing parameter sizes.
Under the budge of four matrices, we let a pair of pro-
jections share a matrix as shown in Table 1 l-q. While the
assimilation of QUERY and KEY is inferior due to the self-
attention issue discussed above, the other conÔ¨Ågurations en-
joy performance improvement by augmenting representa-
tions to outperform the original transformer (Table 2 i). In
particular, coupling PROBE toWEIGHT in Table 1 opro-
duces the best performance; their relationship is strongly
suggested in Figure 3a by exhibiting high similarity score.
Finally, the full model in Table 1 routperforms those con-
Ô¨Ågurations.
The similar analysis and discussion are applicable to the
original transformer as shown in Table 2. While the sim-
plest form works less effectively, the mean-shift update (9)
produces favorable performance (Table 2 b) under the bud-
get of two projection matrices (Table 2 b-e). As shown in Ta-
ble 2 e, performance is deteriorated by tying unrelated ones
as is the case with Table 1 g. Since the original transformer
(1) applies inner-product between QUERY and KEY in com-
puting attention, it is free from the self-attention issue in
Table 1 discussed above, as shown in Table 2 cdf. It should
be noted that the models of the original transformer in Ta-
ble 2 are inferior to those of MSF-transformer (Table 1) un-
der the same budge of parameter size; even the full model in
Table 2 iis defeated by the MSF models equipped with four
6052
Model QUERY KEY VALUE PROBE WEIGHT Group (mode) Acc. Param (M)
MSF Q K V P W 1 79.79 23.68
MSF Q K V P W 2(interleave) 79.55 20.14
MSF Q K V P W 2(block) 78.87 20.14
MSF Q K K K W 1 78.82 20.14
MSF Q K K K W 2(interleave) 78.23 18.37
Orig. Q K V -W 1 78.98 21.91
Orig. Q K V -W 2(interleave) 78.57 19.26
Orig. Q K V -W 2(block) 78.05 19.26
Table 4. Performance comparison by applying the grouped projec-
tion (Section 2.3) to both MSF and original transformers as well
as to the parameter-sharing MSF model (Table 1 h).
projection matrices (Table 1 o). Thus, these results validate
the efÔ¨Åcacy of the proposed MSF formulation.
Mean-shift formulation. As discussed in Section 2.2.2, the
MSF-transformer derived from mean-shift updating (11) on
KDE is distinctive in that it additionally introduces PROBE
P>
hxto produce a differential against attention-aggregated
VALUE features. Focusing on PROBE , the updating vector ^x
can be rewritten into
^x=HX
h=1Whh
V>
hX 
 1
2kK>
hxi Q>
hxk2
2	m
i=1
 P>
hxi
;
=HX
h=1Whh
V>
hX 
 1
2kK>
hxi Q>
hxk2
2	m
i=1i
 ~Px;
(14)
where ~P=P
hWhP>
h2Rddis a re-parameterized pro-
jection matrix, consuming the same number of parameters
as that of P= [P1;;PH]2Rdd; usually,H^d=d.
The differential vector is also produced in (14) by means
of a linear projection using ^P, which may be regarded as a
way to add a residual connection [17] to the original trans-
former model (Figure 1a) equipped with Gaussian kernel,
i.e., the Ô¨Årst term in (14). The approach (14), however, is
less related to the mean-shift model (Section 2.2.2) since
WEIGHT Whdoes not have any effect on the probe repre-
sentation of ~Px; in MSF-transformer (12), PROBE is back-
projected to d-dimensional space via WEIGHT according to
the mean-shift update. We compare the MSF-transformer
(12) to the residual-based model (14) in Table 3 which also
shows performance of the original transformer using Gaus-
sian kernel for reference. The residual model (14) is inferior
to the MSF-transformer while producing better scores on a
training set due to the lack of mean-shift constraint. We
conjecture that degenerating mean-shift model curbs gener-
alization performance by inducing overÔ¨Åtting. These results
validate effectiveness of the mean-shift model (Section 2) in
feature transformation.
LayerAveraged Feature NormOrig.
MSF
patch
embedFigure 4. Averaged L2norm of features xat the patch-embedding
and 12 layers in ViT-S.
Model Layer Width d HeadH MLP
ViT-Ti [4] 12 192 3 4d
ViT-SS [4] 6 384 6 4d
ViT-S [4] 12 384 6 4d
ViT-B [4] 12 768 12 4d
Swin-T [30] [2,2,6,2] [96,192,384,768] [3,6,12,24] 4d
Swin-S [30] [2,2,18,2] [96,192,384,768] [3,6,12,24] 4d
Table 5. Architectures of transformer-based networks. We apply
simple ViT models [4] slightly modiÔ¨Åed from the original ViT [14]
such as by applying global average pooling to aggregate features.
Feature norm. We then analyze feature representations
embedded in the transformer models. As discussed in Sec-
tion 2.2.1, the ill-posed kernel of exp( ^d 1
2k>q)would in-
duce bias toward enlarging feature magnitude ( L2norm)
in gradient-ascent updating, i.e., the original transformer.
To empirically analyze it, Figure 4 evaluates averaged fea-
ture norm1
mPm
i=1kxik2at each layer. While the pre-
processing layer of patch embedding produces almost the
same feature norms, the two transformer models seem to
produce different feature representation in terms of feature
norms along layers. The original transformer increases the
feature norms at deeper layers, which inevitably demands
normalization layers to stabilize training. This implies the
above-mentioned bias induced by the ill-posed kernel func-
tion. On the other hand, the MSF-transformer leverages
well-deÔ¨Åned mean shift based on a Gaussian kernel to pro-
vide features of rather stable norms across layers.
Grouped projection. Next, we analyze the grouped pro-
jection (Section 2.3) for reducing parameter size. In the
multi-head framework, the grouped projection is applied in
two ways of block-wise and interleaved grouping which
are different speciÔ¨Åcally in terms of back-propagation as
shown in Figure 2. They are evaluated on the models of
MSF-transformer and the original one; the performance re-
sults are shown in Table 4 where the grouped projection
operates on four projections of QUERY ,KEY,VALUE and
6053
Acc. (%)
Model Param (M)FLOPS (G)100-ep 300-ep
CNN EfÔ¨ÅcientNet-B0 5.24 0.77 73.47 76.04
ResNet-50 25.50 8.24 75.97 78.78
ResNet-101 44.44 15.71 78.36 80.72
ResNeXt-50 24.96 8.54 77.67 80.11
ResNeXt-101 88.59 31.1 81.28 81.71
ViT-Ti Orig. 5.65 2.50 72.25 75.71
MSF 6.09 2.67 73.50 76.81
MSF w/ 2- group 5.20 2.32 72.74 76.20
ViT-SS Orig. 11.30 4.64 74.65 77.22
MSF 12.18 4.98 75.50 78.20
MSF w/ 2- group 10.41 4.29 75.09 77.31
ViT-S Orig. 21.91 9.16 78.98 80.98
MSF 23.68 9.85 79.79 81.49
MSF w/ 2- group 20.14 8.46 79.55 81.41
ViT-B Orig. 86.29 34.96 81.47 82.67
MSF 93.37 37.73 81.79 82.86
MSF w/ 2- group 79.21 32.18 81.70 82.65
Swin-T Orig. (1, 2) 28.20 8.99 78.92 81.69
MSF (12) 30.36 9.68 79.61 82.34
MSF (12) w/ 2- group 26.04 8.29 79.25 81.95
Swin-S Orig. (1, 2) 49.43 17.49 81.25 83.43
MSF (12) 53.36 18.88 81.33 83.70
MSF (12) w/ 2- group 45.51 16.10 81.18 83.52
Table 6. Performance results by embedding two types of MSF-
transformer modules into ViT and Swin networks (Table 5) trained
from scratch on ImageNet. Top-1 accuracies are reported.
PROBE . The interleaved approach effectively works to re-
duce parameter size while keeping performance on both
the models; especially, MSF-transformer with 2-group in-
terleaved projection outperforms the original transformer
while consuming smaller number of parameters (79.55%
with 20.14M param vs 78.98% with 21.91M param). On the
other hand, the block-wise method interferes with perfor-
mance as it impedes back-propagation in end-to-end learn-
ing. The interleaved grouping also contributes to reducing
parameter size of projection-sharing model (Table 1 h).
3.2. Performance comparison
We embed the MSF-transformer into various networks
based on ViT [4, 14] and Swin-Transformer [30] (Table 5)
with comparison to representative CNN models [17, 39, 50]
of various parameter sizes. For fair comparison, all the net-
works are trained from scratch on an ImageNet [13] train-
ing set by the same training protocol as in Section 3.1 using
four GPUs; only the intensity of mixing augmentation and
batch size are tuned according to the model size, the details
of which are shown in supplementary material. It is note-
worthy that both the network models of transformers andBackbone model iNat2018 (Acc) iNat2019 (Acc) ADE (mIoU)
ViT-S Orig. 66.84 73.45 41.81
MSF 68.19 74.60 44.01
MSF w/ 2- group 67.94 74.74 43.50
Table 7. Performance results on Ô¨Åne-grained iNaturalist classiÔ¨Åca-
tion (acc, %) and ADE semantic segmentation (mIoU).
CNNs are effectively trained to produce competitive perfor-
mance with the reported ones, such as ViT-S (76.5%) [4]
and ResNeXt-50 (77.8%) [50]. We evaluate the networks
trained in 100 and 300 epochs by reporting top-1 classiÔ¨Åca-
tion accuracies on an ImageNet validation set in Table 6.
We apply two types of MSF-transformer (Figure 1b)
equipped with full projection and the grouped projection
(Figure 2b) using interleaved 2 groups. The MSF models
improve performance on various models of diverse parame-
ter sizes. The grouped projection effectively reduces param-
eter size, especially working on large-scaled models while
retaining classiÔ¨Åcation performance. Even on a small-
/middle-scaled models, the MSF-transformer produces fa-
vorable performance improvement; e.g., ViT-Ti with MSF
outperforms EfÔ¨ÅcientNet-B0 [39] and ViT-S with MSF is
competitive to ResNet-family [17, 50] under the same or
smaller budget of parameter size. These experimental re-
sults show versatile applicability of the proposed method in
place of the original transformer module.
3.3. Transfer learning to downstream tasks
To show transferability, we apply the proposed method to
Ô¨Åne-grained visual classiÔ¨Åcation on iNaturalist datasets [20,
21] and semantic segmentation on ADE dataset [61] in a
framework of SETR [59]. In those two types of downstream
tasks, we leverage a backbone of ImageNet-pretrained ViT-
S (Table 6). The performance results in Table 7 demon-
strate effectiveness of the proposed method. Particularly,
on semantic segmentation, the SETR [59] exploits multiple
feature maps, each of which can be improved by our MSF
model to produce favorable improvement.
4. Conclusion
We have proposed a feature transformation method to ex-
tend the transformer. By analyzing the transformer model
from a viewpoint of feature updating on a token distribution,
the method is formulated based on the mean-shift update
which is a gradient ascent of KDE equipped with Gaussian
kernel. We also present an efÔ¨Åcient approach to reduce pa-
rameter size of feature projections which are essential ingre-
dients in our multi-head transformation method. In the ex-
periments on an ImageNet classiÔ¨Åcation task, the methods
are thoroughly evaluated from various aspects and improve
performance in place of the original transformer module.
6054
References
[1] Jimmy Le Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.
Layer normalization. arXiv, 1607.06450, 2016. 4
[2] Irwan Bello. Lambdanetworks: Modeling long-range inter-
actions without attention. In ICLR , 2021. 1, 2
[3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
space-time attention all you need for video understanding?
InICML , pages 813‚Äì824, 2021. 1, 2
[4] Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Bet-
ter plain vit baselines for imagenet-1k. arXiv, 2205.01580,
2022. 5, 7, 8
[5] ÀöAke Bj ¬®orck and Gene H. Golub. Numerical methods for
computing angles between linear subspaces. Mathematics
of Computation , 27(123):579‚Äì594, 1973. 6
[6] Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When
vision transformers outperform resnets without pre-training
or strong data augmentations. In ICLR , 2022. 2
[7] Yizong Cheng. Mean shift, mode seeking, and clustering.
IEEE TPAMI , 17(8):790‚Äì799, 1995. 2, 4
[8] Krzysztof Choromanski, Valerii Likhosherstov, David Do-
han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter
Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,
David Belanger, Lucy Colwell, and Adrian Weller. Rethink-
ing attention with performers. In ICLR , 2021. 1, 2
[9] Dorin Comaniciu and Peter Meer. Mean shift: A robust ap-
proach toward feature space analysis. IEEE TPAMI , 24(5):
603‚Äì619, 2002. 2, 4
[10] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin
Jaggi. On the relationship between self-attention and con-
volutional layers. In ICLR , 2020. 2
[11] Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and
Christopher R ¬¥e. Flashattention: Fast and memory-efÔ¨Åcient
exact attention with io-awareness. arXiv, 2205.14135, 2022.
1, 2
[12] St ¬¥ephane d‚ÄôAscoli, Hugo Touvron, Matthew Leavitt, Ari
Morcos, Giulio Biroli, and Levent Sagun. Convit: Improving
vision transformers with soft convolutional inductive biases.
InICML , 2021. 1, 2
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR , pages 248‚Äì255, 2009. 5, 8
[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is
worth 16x16 words: Trans- formers for image recognition at
scale. In ICLR , 2021. 1, 2, 3, 4, 7, 8
[15] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam
Neyshabur. Sharpness-aware minimization for efÔ¨Åciently
improving generalization. In ICLR , 2021. 2
[16] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,
Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing
Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng
Tao. A survey on vision transformer. IEEE TPAMI , 45(1):
87‚Äì110, 2023. 1, 2[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
pages 770‚Äì778, 2016. 7, 8
[18] Ruining He, Anirudh Ravula, Bhargav Kanagal, and Joshua
Ainslie. Realformer: Transformer likes residual attention.
arXiv, 2012.11747, 2021. 1, 2
[19] Sepp Hochreiter and J ¬®urgen Schmidhuber. Long short-term
memory. Neural Computation , 9(8):1735‚Äì1780, 1997. 1
[20] iNatrualist. The inaturalist competition dataset. https:
//github.com/visipedia/inat_comp/tree/master/
2018 , 2018. 8
[21] iNatrualist. The inaturalist 2019 competition dataset.
https://www.kaggle.com/c/inaturalist- 2019-
fgvc6 , 2019. 8
[22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and
Franc ¬∏ois Fleuret. Transformers are rnns: Fast autoregressive
transformers with linear attention. In ICML , pages 5156‚Äì
5165, 2020. 1, 2
[23] Nikita Kitaev, ≈Åukasz Kaiser, and Anselm Levskaya. Re-
former: The efÔ¨Åcient transformer. In ICLR , 2021. 1, 2
[24] Takumi Kobayashi. Mutual conditional probability for self-
supervised learning. In BMVC , 2022. 2
[25] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and
Hamed Pirsiavash. Mean shift for self-supervised learning.
InICCV , pages 10326‚Äì10335, 2021. 2
[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.
Imagenet classiÔ¨Åcation with deep convolutional neural net-
works. In NIPS , pages 1097‚Äì1105, 2012. 2, 4
[27] Mingxiang Liao, Zonghao Guo, Yuze Wang, Peng Yuan, and
Bailan Feng. Attentionshift: Iteratively estimated part-based
attention map for pointly supervised instance segmentation.
InCVPR , pages 19519‚Äì19528, 2023. 2
[28] Grace W. Lindsay. Attention in psychology, neuroscience,
and machine learning. Frontiers in Computational Neuro-
science , 14(29), 2020. 1
[29] Yaoyao Liu, Bernt Schiele, Andrea Vedaldi, and Christian
Rupprecht. Continual detection transformer for incremental
object detection. In CVPR , pages 23799‚Äì23808, 2023. 2
[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
ICCV , pages 10012‚Äì10022, 2021. 2, 7, 8
[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. In ICLR , 2019. 5
[32] Sachin Mehta and Mohammad Rastegari. Mobilevit: Light-
weight, general-purpose, and mobile-friendly vision trans-
former. In ICLR , 2022. 1, 2
[33] Miguel ¬¥A. Carreira-Perpi n ¬¥an. A review of mean-shift algo-
rithms for clustering. arXiv, 1503.00687, 2015. 4
[34] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David
Grangier. EfÔ¨Åcient content-based sparse attention with rout-
ing transformers. arXiv, 2003.05997, 2020. 1, 2
[35] Samuel L Smith, Andrew Brock, Leonard Berrada, and So-
ham De. Convnets match vision transformers at scale. arXiv,
2310.16764, 2023. 2
[36] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon
Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck
6055
transformers for visual recognition. In CVPR , pages 16519‚Äì
16529, 2021. 1
[37] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross
Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train
your vit? data, augmentation, and regularization in vision
transformers. arXiv, 2106.10270, 2021. 2
[38] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In CVPR , 2015. 5
[39] Mingxing Tan and Quoc V . Le. EfÔ¨Åcientnet: Rethinking
model scaling for convolutional neural networks. In ICML ,
2019. 8
[40] Antti Tarvainen and Harri Valpola. Mean teachers are better
role models: Weight-averaged consistency targets improve
semi-supervised deep learning results. In Advances in Neu-
ral Information Processing Systems (NeurIPS) , pages 1195‚Äì
1204, 2017. 1
[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ¬¥e J¬¥egou. Training
data-efÔ¨Åcient image transformers & distillation through at-
tention. In ICML , 2021. 2
[42] Hugo Touvron, Matthieu Cord, and Herv ¬¥e J¬¥egou. Deit iii:
Revenge of the vit. In ECCV , 2022. 2, 5
[43] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-
Philippe Morency, and Ruslan Salakhutdinov. Transformer
dissection: A uniÔ¨Åed understanding of transformer‚Äôs atten-
tion via the lens of kernel. In EMNLP , pages 4344‚Äì4353,
2019. 1, 2
[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , pages
5998‚Äì6008, 2017. 1, 2, 3, 4
[45] M.P. Wand and M.C. Jones. Kernel Smoothing . Chapman
and Hall, 1995. 3
[46] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and
Hao Ma. Linformer: Self-attention with linear complexity.
arXiv, 2006.04768, 2020. 1, 2
[47] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-
ing He. Non-local neural networks. In CVPR , pages 7794‚Äì
7803, 2018. 1, 3
[48] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han.
Lite transformer with long-short range attention. In ICLR ,
2020. 1, 2
[49] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr
Doll¬¥ar, and Ross Girshick. Early convolutions help trans-
formers see better. In NeurIPS , 2021. 1, 2
[50] Saining Xie, Ross Girshick, Piotr Doll ¬¥ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In CVPR , pages 5987‚Äì5995, 2017. 2, 4, 8
[51] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty,
Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.
Nystr ¬®omformer: A nystr ¬®om-based algorithm for approximat-
ing self-attention. In AAAI , pages 14138‚Äì14148, 2021. 1, 2
[52] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,
Zihang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng
Yan. Tokens-to-token vit: Training vision transformers from
scratch on imagenet. In ICCV , pages 558‚Äì567, 2019. 2[53] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
larization strategy to train strong classiÔ¨Åers with localizable
features. In ICCV , pages 6023‚Äì6032, 2019. 5
[54] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lu-
cas Beyer. Scaling vision transformers. In CVPR , pages
12104‚Äì12113, 2022. 2
[55] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. In ICLR , 2018. 5
[56] Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. In-
terleaved group convolutions. In ICCV , pages 4383‚Äì4392,
2017. 2, 4, 5
[57] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
ShufÔ¨Çenet: An extremely efÔ¨Åcient convolutional neural net-
work for mobile devices. In CVPR , pages 6848‚Äì6856, 2018.
2, 4, 5
[58] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring
self-attention for image recognition. In CVPR , pages 10076‚Äì
10085, 2020. 1, 2
[59] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic
segmentation from a sequence-to-sequence perspective with
transformers. In CVPR , 2021. 8
[60] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,
Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao
Xiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic
segmentation from a sequence-to-sequence perspective with
transformers. In CVPR , pages 6881‚Äì6890, 2021. 2
[61] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela
Barriuso, and Antonio Torralba. Scene parsing through
ade20k dataset. In CVPR , 2017. 8
[62] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xi-
aochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng.
Deepvit: Towards deeper vision transformer. arXiv,
2103.11886, 2021. 1, 2
[63] Daquan Zhou, Yujun Shi, Bingyi Kang, Weihao Yu, Zihang
Jiang, Yuan Li, Xiaojie Jin, Qibin Hou, and Jiashi Feng. Re-
Ô¨Åner: ReÔ¨Åning self-attention for vision transformers. arXiv,
2106.03714, 2021. 1, 2
6056
