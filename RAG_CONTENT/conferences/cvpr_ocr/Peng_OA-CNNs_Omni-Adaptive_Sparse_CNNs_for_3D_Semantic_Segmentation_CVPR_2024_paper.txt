OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation
Bohao Peng1Xiaoyang Wu2Li Jiang3*Yukang Chen1
Hengshuang Zhao2Zhuotao Tian4*Jiaya Jia1
1CUHK2HKU3CUHK, Shenzhen4HIT, Shenzhen
Abstract
The booming of 3D recognition in the 2020s began
with the introduction of point cloud transformers. They
quickly overwhelmed sparse CNNs and became state-of-
the-art models, especially in 3D semantic segmentation.
However, sparse CNNs are still valuable networks, due to
their efficiency treasure, and ease of application. In this
work, we reexamine the design distinctions and test the
limits of what a sparse CNN can achieve. We discover
that the key credit to the performance difference is adap-
tivity . Specifically, we propose two key components, i.e.,
adaptive receptive fields (spatially) and adaptive relation,
to bridge the gap. This exploration led to the creation
of Omni-Adaptive 3D CNNs (OA-CNNs), a family of net-
works that integrates a lightweight module to greatly en-
hance the adaptivity of sparse CNNs at minimal computa-
tional cost. Without any self-attention modules, OA-CNNs
favorably surpass point transformers in terms of accuracy
in both indoor and outdoor scenes, with much less latency
and memory cost. Notably, it achieves 76.1%, 78.9%, and
70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI
validation benchmarks respectively, while maintaining at
most 5 √óbetter speed than transformer counterparts. This
revelation highlights the potential of pure sparse CNNs to
outperform transformer-related networks. Our code is built
upon Pointcept [9], which is available at here1.
1. Introduction
3D scene understanding is critical in various practical appli-
cations, including robotics, autonomous driving, and aug-
mented reality [13, 16, 19, 20, 30, 55, 70, 71, 74]. In con-
trast to images, which typically exhibit densely and uni-
formly arranged pixels [10, 23, 35, 49, 51, 52], 3D point
clouds often manifest irregular and scattered distributions.
It leads to various feature extractors in 3D scene under-
standing.
*Corresponding authors.
1https://github.com/Pointcept/Pointcept
SmallLargeRaw PointsReceptive Fields
TableChairWallJunctionSpecific Objects in Red Boxes‚ë†‚ë°‚ë¢‚ë£
‚ë†‚ë°‚ë¢‚ë£Figure 1. Visualization of 3D scene receptive fields controlled by
our proposed adaptive aggregator. Objects‚Äô edges and junctions re-
quire smaller receptive fields due to their sophisticated structures,
while flat planes and unitary structures require broader fields.
There are two mainstream 3D networks. The first is
point-based networks [41, 42], which advocate directly ma-
nipulating the unstructured points. Thanks to the flexibility
of point-wise operations, point-based methods, particularly
those with transformer architectures [12, 32, 36, 39, 50, 53,
54, 73], have gradually become dominant. The second is
sparse CNNs [7, 13], where irregular point clouds are con-
verted into voxels during data preprocessing. This allows us
to leverage the locally structured benefits and facilitate high
efficiency. Due to this practical value, sparse CNNs have
been widely exploited in existing literature [37, 45, 65, 75].
However, its accuracy is usually inferior to its transformer
counterparts [21, 36, 60, 73], especially in 3D scene seman-
tic segmentation.
Given the high potential of sparse CNNs, we carefully
examine the inner reasons for the performance gap in this
paper. We find that the key distinction between sparse
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
21305
Stratified TransformerPTv2
MinkUNetPTv2
Point Transformer
Sparse UNet
(b) Memory and mIoU (a) Speed and mIoU76
75
74
73
72
71
70
100 1730ms 200300400 500mIoU
76
75
74
73
72
71
70mIoU
1 G 3 5 7 9Ours
SparseUNetOurs
OctFormerFigure 2. Comparison between various transformer-based [21, 60,
73] and CNN-based [7, 13] within RTX 3090. For OctFormer, we
reproduce the official repository and include the cost of building
the octree. If a method has multiple versions, they are indicated
by different dots.
CNNs and point transformers behind is adaptivity ‚Äì the
latter can flexibly adapt to individual contexts while it may
not be feasible for the former with static perception. With-
out degrading efficiency, we bridge this gap via two key
components: (1) spatially adaptive receptive fields, and (2)
adaptive relations.
Adaptively adjusting receptive fields via attention
mechanisms is one of key designs in transformer-based
frameworks [54, 73] to achieve top performance. Intu-
itively, different parts of the 3D scene with various geomet-
ric structures and appearances should be catered with differ-
ent receptive sizes, as visualized in Fig. 1. Flat and sparse
regions like the wall and floor need large receptive fields
to yield consistent predictions with broader cues, while so-
phisticated parts like the plane junctions and small objects
need smaller ones to screen unnecessary context that may
overwhelm the local details. To enable our CNN-based
framework to adaptively perceive the contextual informa-
tion, we partition the 3D scene into non-overlapping pyra-
mid grids. We then utilize the proposed Adaptive Rela-
tion Convolution (ARConv) in multiple scales and design
a selective aggregator to adaptively aggregate the multi-
scale outputs based on the local characteristics . Instead
of pursuing consistent large receptive fields (like LargeK-
ernel3D [6]), we find that this adaptive manner is sufficient
and more efficient.
Adaptive relationships , achieved via self-attention
maps, is another key strength over CNNs. To facilitate the
establishment of relationships among local contexts, we in-
troduce a multi-one-multi paradigm in ARConv, as depicted
in Fig. 6. Specifically, we dynamically generate kernel
weights for non-empty voxels based on their correlations
with the grid centroid. By adopting this approach, we can
maintain a lightweight design[58] with a linear complexity
proportional to the voxel quantity, which effectively expands
the receptive fields and achieves optimal efficiency .
xyzRaw PointsMLPVoxelizationSparse ConvReference PointsReceptive Field ComparisonConvNetPointNetFigure 3. Comparisons between the 3D point-based [41, 73] and
convolutional networks [7, 13]. PointNets directly process the raw
points and provide more flexible and broader receptive fields. Con-
vNets handle structural data after additional voxelization pretreat-
ment with higher efficiency and lower consumption.
Extensive experiments validate our approach‚Äôs effective-
ness, and our designs enable sparse CNNs to outperform
state-of-the-art point-based methods with transformer ar-
chitectures, with little efficiency compromise, as shown in
Fig. 2. We conduct the comparisons under the same ex-
perimental settings, without any additional pretraining or
auxiliary methods. Remarkably, it achieves mIoU scores
of 76.1%, 78.9%, and 70.6% on the ScanNet v2 [11],
nuScenes [4], and SemanticKITTI [2] validation bench-
marks, respectively. It highlights the potential of sparse
CNNs over transformer-related models in both performance
and efficiency, regardless of indoor or outdoor scenes.
In conclusion, our contributions are listed as follows:
‚Ä¢ We analyze and find that adaptivity is the key to bridging
the gap between sparse CNNs and point transformers.
‚Ä¢ We propose OA-CNNs as solutions, consisting of dy-
namic receptive fields and adaptive relation mapping.
‚Ä¢ Our method outperforms state-of-the-art methods with
promising efficiency on popular benchmarks including
ScanNet v2, ScanNet200, nuScenes and SemanticKITTI
semantic segmentation.
2. Related Work
Point-based learning. Point-based methods advocate di-
rectly processing the unstructured raw points without any
additional regulation pretreatment [14, 18, 31, 63, 67].
PointNet [41] is pioneering work in this trend which lever-
ages point-wise MLP and permutation invariance operation
to obtain the global feature of input points. More details
and comparisons are shown in Fig. 3. Several follow-up
works [15, 18, 42] continue to strengthen their capabilities
through hieratical multi-scale perception and local-global
feature aggregation. Especially with the development of
the attention mechanism [54, 68, 69], point-wise perception
with the transformer architecture [21, 60, 62, 73] provides
long-range dependences and bridges global contexts rela-
21306
tionships. These frameworks have shown outperforming su-
periority and gradually become dominant. However, atten-
tion calculation and point-wise operation suffer from more
expensive computation and memory consumption, and the
complex architecture also makes them more challenging to
deploy.
CNN-based learning. Compared with dense images ar-
ranging pixels into a rasterized grid, point cloud directly
records the points‚Äô spatial coordinates, which are typically
irregular and lack unified metrics. Projection-based [5, 24‚Äì
26, 29, 46] methods intuitively project the raw 3D points
into flat images from various views, and the subsequence
operations are logically the same as the 2D pipeline. How-
ever, the projection seriously destroyed the point cloud‚Äôs
geometrical information, especially for the in-door scenes
with more stereoscopic structures. An alternative technique
is to quantize the 3D scene and transform irregular point
clouds into regular voxel representation [3, 37, 38, 45]. 3D
convolutions are commonly applied to handle these voxel
collections while consuming high computation and mem-
ory. Sparse and submanifold convolutions [13] are in-
troduced to alleviate these issues and improve efficiency.
Sparse convolution introduces the hash table for the voxels‚Äô
indices retrieval, which is convenient and efficient. More-
over, 3D submanifold convolution has made a further re-
striction only processing the non-empty elements sacrific-
ing some flexibility in change for more efficiency and less
consumption. However since the complexity of the kernel
size is O(K3), the receptive fields of sparse convolutions
are still limited by the parameter quantity, which seriously
restricts the global perception ability. In this work, we ex-
plore a lightweight design [58] to expand 3D convolution
with an adaptive receptive range [27].
Dynamic convolutions. Regular convolutions optimize
the learnable kernel weights during training and fix ker-
nel weights in the inference process. Dynamic convolu-
tion [17, 66] proposes to generate the convolution kernel
adaptively depending on the specific conditions. Previous
works [48, 59, 64] have widely explored introducing dy-
namic convolution into sparse data processing. However,
these works are also based on point-wise methods and typi-
cally generate kernel weights depending on the relative po-
sition information, which requires expensive computation
and memory consumption. In this work, we inherit condi-
tional convolution to propose a lightweight grid convolution
with a regular structure. Moreover, we introduce the adap-
tive aggregator for the multi-scale pyramid aggregation to
bridge extended-range contexts efficiently.
3. Omni-Adaptive 3D Sparse CNNs
In this section, we provide a detailed introduction to our
designed lightweight modules and their application in con-
ARConv
ARConv
ARConv
Linear SoftMax‚àë
√ó
√ó
√ó
Linearùë§‡¨µùë§‡¨∂ùë§‡¨∑SumPyramid Grid Partition
ùëî‡¨µùëî‡¨∂
ùëî‡¨∑ùíê‡Øú,‡¨µùíê‡Øú,‡¨∂ùíê‡Øú,‡¨∑
ùíá‡Øú
Concat
LinearFigure 4. Illustration for the adaptive aggregator, which learns to
aggregate various grid contexts under multi-pyramid scales from
the voxel‚Äôs instinct characteristics.
structing a series of omni-adaptive 3D sparse CNNs (OA-
CNNs). It surpasses point transformers in 3D recognition
with limited latency/memory overhead. OA-CNNs consist
of three design contents, i.e., spatially adaptive receptive
fields in Sec. 3.1, Adaptive Relation Convolution (ARConv)
in Sec. 3.2, and the overall architecture in Sec. 3.3.
3.1. Spatially adaptive receptive fields
Motivation. Various receptive field sizes are required in
distinct positions and objects in one 3D scene. For exam-
ple, as shown in Fig. 1, regions belonging to the wall and
floor are relatively flat and elementary, which require larger
receptive fields to yield consistent predictions. However
the geometric structures of the plane junction or sophisti-
cated objects are more complex and need smaller recep-
tive fields to retain the local characteristics. Transformer
frameworks [21, 54, 73] adjust the perception range by the
attention mechanism retrieving the relevance with the sur-
rounding contexts but significantly increasing memory and
computing consumption. However, sparse CNNs lack the
ability to handle this issue. In OA-CNNs, we overcome this
by directly determining the perception size with the aid of
the intrinsic voxel features, as illustrated in Fig. 4.
Voxel grid. Expanding the receptive field is necessary for
pursuing adaptive perception since the typical 3D convo-
lution kernel size is generally set as 3√ó3√ó3limited
by the parameter quantity. To achieve this, we utilize the
voxel grid in our approach. Formally, define V= (P,F)
as a sparse voxelized 3D scene representation containing
a set of voxels vi= (pi,fi), where pi‚ààR3represents
the positional integer indice and fi‚ààRdis the corre-
sponding feature with dchannels. The global voxel set
Vis then partitioned into Nnon-overlapping voxel grids
[V1,V2, . . . ,VN],Vi={vj|pj‚àà‚Ñ¶(i)}, where Viindi-
cates i-th voxel grid and ‚Ñ¶(i)obtains i-th voxel grid‚Äôs in-
dices range. The voxel grid size can be considerably larger
than that of the typical 3D convolution kernel, such that the
21307
receptive field is effectively expanded.
Pyramid grid partition. Although a sufficiently large
grid size can provide a global view, it may not be able to
capture intricate details for sophisticated objects. In an ef-
fort to prepare the alternative grid sizes for adaptively ac-
commodating different areas, we rasterize the entire 3D
scene into pyramid voxel grids. Specifically, let‚Äôs define
G={gk}Kas the set of Kgrid sizes partitioning the 3D
space, where Kis set as 3in our experiments. The output
oi‚ààRk√ódof the i-th voxel grid under k-th scale is obtained
as:
oi,k,:=Conv({fj|pj‚àà‚Ñ¶(i, gk)}), (1)
where ‚Ñ¶(i, gk)represents the range of voxel indices in the
i-th voxel grid in the size gk, and Conv (¬∑)indicates the con-
volution for aggregating voxel features in the voxel grids to
get the voxel grid feature. Observing the intolerably heavy
parameters associated with the standard sparse 3D convolu-
tion Conv (¬∑)using a large kernel, we introduce the ARConv
in Sec. 3.2 as a solution to this issue. The ARConv im-
proves results without sacrificing efficiency and establishes
relationships among the voxel grid.
Adaptive aggregator. To achieve a customizable recep-
tive field, we propose an adaptive aggregator that au-
tonomously adjusts the receptive fields based on the intrin-
sic characteristics and spatial structure of individual voxels,
which is illustrated in Fig. 4. Given Kmulti-scale grid par-
titions with sizes G={gk}K, our proposed adaptive aggre-
gator weights and fuses the multi-scale outputs. We use a
learnable function Œ¥adpto predict the preference weights wi
ofKgrid sizes as:
wi=SoftMax (Œ¥adp(fi)), (2)
where w‚ààRNi√óK,Nidenotes the number of voxels in-
side the i-th voxel grid, and Œ¥adp:Rd7‚ÜíRKis a learnable
linear layer and SoftMax (¬∑)denotes the softmax operation
overKgrid sizes. We subsequently employ the predicted
weights to aggregate the convolution outputs, which contain
global information, with the original features to enhance
them,
f‚Ä≤
i=Œ¥out(Œ¥proj(fi)‚äïKX
k=1wi,k¬∑oœï‚Ä≤(i,k),k), (3)
where Œ¥out:R2d7‚ÜíRdandŒ¥proj:Rd7‚ÜíRdare two linear
layers with normalization and activation, ‚äïdenotes vector
concatenation and œï‚Ä≤(i, k)reversely returns the voxel grid
index containing the i-th voxel under gkgrid size partition.
So far, we have presented a method for constructing the
spatially adaptive receptive fields based on individual con-
text, but it is not yet capable of establishing adaptive rela-
tionships as the point-based transformer counterparts.
Voxel Grid
Linearùëì!(#,%)ùëì!(#,')ùëì!(#,(!)‚Ä¶CentroidVoxel
FeaturesWeightsùëä!,:,$ùëä!,:,%ùëä!,:,&!Weights GenerationSoftMax
‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶‚Ä¶
‚Ñù)√ó+‚Ñù)√ó+
HadamardProductSumOutput‚Ñù+Figure 5. Illustration of the Adaptive Relation Convolution (AR-
Conv). It dynamically generates grid convolution‚Äôs kernel weights
only for the non-empty voxels with their relationships to the cen-
troid voxel.
3.2. Adaptive relation convolution
Observations. Transformer frameworks [39, 60] have
achieved remarkable success and become one of the domi-
nant architectures in 3D semantic segmentation. Their per-
formance superiority is largely related to the ability of re-
lation learning among various local point features. It is
achieved by self-attention mechanisms and essentially in-
creases the representation capacity. However, plain sparse
CNNs miss this design.
On the other hand, CNNs have verified, via extensive re-
search [6, 33, 57], the importance of large receptive fields to
enable a global perception. Unfortunately, 3D convolution
struggles to improve perception range by directly expand-
ing the convolution kernel since its complexity is O(K3),
where Kis the kernel size, indicating that the consump-
tion of the large kernel may be unacceptable in practice,
especially for the edge devices. To this end, we explore the
large-kernel design to be lightweight and propose the Adap-
tive Relation Convolution (ARConv), which incorporates
the aforementioned adaptive relation reasoning into sparse
CNNs. More details are illustrated in Fig. 5.
Depthwise convolution. To assemble the framework in a
lightweight manner, we could start by considering depth-
wise convolution for parsing the voxel grid features. In
practical applications, it is also found that the depthwise
convolution generalizes better [58] and converges faster as
shown in our experiments. Compared with regular convo-
lutions performed over multiple input channels, depthwise
convolutions independently apply a single convolutional fil-
ter for each input channel and keep each channel separate.
The output for i-th voxel grid feature oi‚ààRdandc-th di-
21308
Reference SetQuery SetCentroidVoxel BlockK-Nearest Neighbors
Multi-to-MultiMulti-One-MultiGridPartitionFigure 6. Comparison of the multi-to-multi paradigm in point
transformers with the multi-one-multi paradigm in OA-CNNs.
mension can be precisely described as,
oi,c=XNi
j=1Wi,c,j¬∑fœï(i,j),c, (4)
where Niis the number of non-empty voxels in the i-th
voxel grid Vi,Wi‚ààRd√óNiindicates the learnable kernel
weight and œï(i, j)returns the j-th non-empty voxel index
ini-th voxel grid.
Adaptive relation kernel. For accomplishing the adap-
tive relation reasoning, the attention mechanisms [54, 73]
adopt the multi-to-multi paradigm, which incorporates a
‚Äúreference set‚Äù [42, 60] for capturing long-range depen-
dencies through multiple queries and keys. However, this
approach results in significant inference time and memory
demands on GPUs. In contrast, we propose a more effi-
cient multi-one-multi pipeline, generating a single centroid
voxel of the grid, which serves as the agent for capturing
long-range relationships. This strategy facilitates efficient
computation and lowers memory consumption, while still
enabling the extraction of complex relationships among the
non-empty voxels in the grid. The idea is illustrated in
Fig. 6.
Specifically, for the sub voxel grid Vi, its corresponding
centroid voxel feature fctr
i‚ààRd, where dindicates the
number of channels, is formed as:
fctr
i=AvgPool ({Œ¥proj(fj)|pj‚àà‚Ñ¶(i)}), (5)
where AvgPool (¬∑)applies 3D average pooling over the in-
put,‚Ñ¶(¬∑)indicates the subset‚Äôs indices range, and Œ¥proj:
Rd7‚ÜíRdis a linear projection layer with normalization
and activation.
Then the dynamic kernel weight Wi‚ààRd√óNiof the
depthwise convolution for the i-th voxel grid is generated
by considering voxels‚Äô feature correlations with the centroid
voxel:
Wi,:,j=Œ¥weight (fœï(i,j)‚àífctr
i), (6)
EmbeddingDownSampleConvBlockDownSampleConvBlockDownSampleConvBlockDownSampleConvBlockUpSampleUpSampleUpSampleUpSample√ó(%InputOutput
√ó(&√ó()√ó(*LinearUpSample$#$$#%&$ARConv&AggregatorSubmanifold ConvSubmanifold ConvKernel(3, 3, 3)  Stride1Sparse ConvPointsVoxelizationStem LayerKernel(2, 2, 2)  Stride2
√ó(%EmbeddingsEmbeddingsFigure 7. Illustration for the whole architecture and more imple-
mentation details.
where Œ¥weight :Rd7‚ÜíRdis a linear projection layer, and
œï(i, j)returns the j-th non-empty voxel index in i-th voxel
grid.
We normalize the dynamically generated weights Wi,:,j
using softmax operation along each channel separately
across the whole voxel grid. The normalization enhances
the stability of the neural network outputs during training
and assigns feature weights based on internal relevance be-
tween the specific voxel and the centroid voxel. Mathemat-
ically, for the c-th channel,
W‚Ä≤
i,c,j=exp(Wi,c,j‚àíMax(Wi,:,:))PNi
k=1exp(Wi,c,k‚àíMax(Wi,:,:)),(7)
where Max (¬∑)returns the maximum value. We empirically
find that the dynamically generated weights were volatile
at the early training phase, yielding large values that may
cause the exponential function explosion and lead to ‚Äúinf‚Äù
outputs. Thus, we adopt an additional operation in Eq. (7)
that subtracts the maximum values from the numerator and
denominator respectively to prevent the explosion without
affecting the output ‚Äì it is numerically equal to the case
without this operation.
In essence, we have introduced an efficient approach
named Adaptive Relation Convolution (ARConv) that gen-
erates kernel weights only for the non-empty voxels dynam-
ically by considering their correlations to the geometric cen-
troid representatives, thus achieving effectiveness without
sacrificing efficiency.
3.3. Architecture
In this section, we provide the architectural details of the
OA-CNNs. Fig. 7 depicts the overall structure.
Concretely, the sparse and submanifold voxel mod-
ules [13, 36] both process spatially sparse data effectively.
The primary difference between them is that submanifold
convolution only handles the non-empty voxels in the 3D
21309
scene and strictly preserves the original geometric struc-
ture. Differently, sparse convolution can extract features at
empty locations and is more flexible. We construct our basic
blocks with an ARConv module followed by two subman-
ifold convolutions with necessary normalization and acti-
vation layers. Following [40, 42], we adopt the hieratical
structure to the encoder and use a sparse convolution with
kernel size and stride that are both set to (2,2,2), down-
sampling the spacial size to 1/8at each time. As for the
upsampling process, the up-block only consists of a skip
connection and a single linear layer that aligns the feature
channel numbers without other components.
4. Experiments
4.1. Implementation details.
Datasets. We conducted experiments using our proposed
OA-CNNs on the standard benchmark, ScanNet v2 [11],
as well as its recent extension, ScanNet200 [44], and the
S3DIS dataset [1] for indoor scenes. ScanNet v2 con-
tains 1,201 training scenes and 312 validation scans recon-
structed from RGB-D frames. The model utilizes recon-
structed meshes to sample point clouds as input, where each
point cloud is attributed a semantic label from a set of 20
categories. ScanNet200 benchmark extends the class cate-
gories to 200, an order of magnitude more than the previ-
ous. The S3DIS dataset consists of 271 rooms in six areas
from three different buildings with 13 categories. Following
a standard protocol, area 5 is withheld during training and
used for S3DIS testing. As for the outdoor semantic seg-
mentation, we select two popular benchmarks, nuScenes [4]
and SemanticKITTI [2]. The nuScenes dataset contains
approximately 1000 scenes, with each scene consisting of
multiple sensor sweeps captured from a moving vehicle. In
contrast, the SemanticKITTI dataset consists of sequences
from the raw KITTI dataset, which contains 22 sequences
in total. Each sequence includes around 1,000 lidar scans,
corresponding to approximately 20,000 individual frames.
Training details. We train our models on 4RTX 3090
GPUs with the batch size and the number of epochs set
to16and100, respectively. With the considerations re-
garding computational efficiency and memory constraints,
the training process leverages a subset of up to 100,000
randomly sampled points from the point cloud. In con-
trast, the full point cloud is used during validation to en-
sure an unbiased and rigorous evaluation of the model‚Äôs per-
formance. Moreover, we attribute parts of the point-based
frameworks‚Äô performance superiority to the modern train-
ing strategy with advanced data enhancement [43, 60]. We
refer to these strategies to train our models. Specifically, we
use the AdamW optimizer [34] for parameter optimization,
which is widely used in transformer architectures. The ini-Method Input Val mIoU Test mIoU
PointNet++ [42] point 53.5 55.7
PointNeXt-XL [43] point 71.5 71.2
PointCNN [28] point - 45.8
KPConv [48] point 69.2 68.6
PointConv [59] point 61.0 66.6
PointTransformer [73] point 70.6 -
FastPointTransformer [39] point 72.1 -
Stratified Transformer [21] point 74.3 73.7
OctFormer [56] point 75.7 76.6
PTv2 [60] point 75.4 75.2
SparseUNet [13] voxel 69.3 72.5
MinkowskiNet [7] voxel 72.2 73.6
LargeKernel3D [6] voxel 73.2 73.9
OA-CNNs(ours) voxel 76.1 75.6
Table 1. We compared semantic segmentation results on ScanNet
v2. All the selected methods are under the same experimental set-
tings without the use of additional pretraining or auxiliary meth-
ods.
Outdoor Sem. Seg. Benchmarks
Method nuScenes [4] SemanticKITTI [2]
SparseUNet [13] 73.3 63.8
SPVNAS [47] 77.4 64.7
Cylender3D [76] 76.1 64.3
SphereFormer [22] 78.4 67.8
OA-CNNs(ours) 78.9 70.6
Table 2. Results on outdoor semantic segmentation benchmarks.
MethodVal Test
Head Comm. Tail All All
MinkowskiNet [7] 48.3 19.1 7.9 25.1 25.3
LGround [44] 51.5 22.7 12.5 28.9 27.2
SparseUNet [61] - - - 28.8 -
OctFormer [56] - - - - 32.6
PTv2 [61] - - - 29.3 -
OA-CNNs(Ours) 51.3 28.0 17.7 32.3 33.3
Table 3. Results on ScanNet200 for semantic segmentation.
tial learning rate lris0.001, and the weight decay is set to
0.02with the cosine annealing strategy. Following [60] for
data preprocessing, we estimate normal vectors for points
and add coordinates as additional feature input. As for the
data augmentation, we apply random drop, random defor-
mation, and color jitter following [60, 73].
4.2. Comparisons
Performance. We conduct a comprehensive comparison
of our proposed OA-CNNs with alternative backbone mod-
els on multiple benchmarks, including ScanNet v2, Scan-
Net200, S3DIS, nuScenes, and SemanticKITTI [1, 2, 4, 11,
44]. All the methods compared in our experiments are eval-
uated under the same experimental settings, without any ad-
21310
Method Input OA mIoU
PointNet [41] point - 41.1
PointTransformer [73] point 90.8 70.4
PTv2 [60] point 91.1 71.6
MinkowskiNet [7] voxel - 65.4
OA-CNNs(ours) voxel 90.7 71.1
Table 4. Results on S3DIS area 5 for semantic segmen-
tation.ID Aggregation Stage NumsmIoU‚àÜ(%)
I w/o 1 75.0 + 0.0
II Concatenation 3 75.2 + 0.2
III Adaptive (ours) 2 75.2 + 0.2
IV Adaptive (ours) 3 76.1 + 1.1
Table 5. Effectiveness of adaptive aggregator and naive concatenation
through ablation studies with varying stage numbers.
ID Enlarge MethodsmIoU
(%)
I Baseline 73.0
II Multi-head Self-attention 73.5
III Grouped Vector Attention 74.3
IV Pyramid Pooling 75.0
V ARConv 76.1
Table 6. Ablation studies on the different methods com-
monly used for enlarging receptive fields.ID Conv GroupsmIoU
(%)
I Grouped [2,2,4,8] 75.0
II Grouped [4,4,8,16] 75.4
III Depthwise - 76.1
Table 7. Performance Comparison of
Depthwise Convolution and Regular
Grouped Convolution.ID TypemIoU
(%)
I Pos 75.3
II Pos+Ctr 75.9
III Ctr 76.1
Table 8. Comparison of
various weight generation
methods.
EpochEpochVal / LossVal / mIoU
OursMulti-head Self-attentionGroup Vector AttentionPyramid Pooling
Figure 8. Compared with other classical modules to expand the
receptive fields, our proposed method is more stable, has faster
convergence during training, and acquires better performance.
ditional pretraining or auxiliary methods. The results are
shown in Tabs. 1, 2, 3, 4. Our proposed model exhibits su-
perior performance over prior state-of-the-art point-based
frameworks and transformer architectures in both indoor
and outdoor scenes. Indeed, these results highlight the su-
perior generalization capability of OA-CNNs, demonstrat-
ing their potential to outperform point-based and trans-
former models in various benchmarks even without any
self-attention modules.
4.3. Ablation Study
Efficiency. We also compare our models with various
CNN-Based and transformer-based methods [7, 13, 21, 60,
73] regarding accuracy, inference speed, and GPU mem-
ory consumption, as shown in Fig. 2. We can observe that,
while transformer-based methods have demonstrated im-
pressive performance, they come with a drawback ‚Äì they
require extensive time and memory for frequently querying
nearest neighbors, attention computation, and other point-
based operations. Differently, thanks to the CNN architec-Type BlocksTime Mem. mIoU
(ms) (G) (%)
OA-CNN (S) [ 2 ,2,2,2]117 2.1 73.6
OA-CNN (B) [ 3 ,3,9,3]190 3.3 75.3
OA-CNN (L) [ 3 ,3,9,8]213 3.6 76.1
Table 9. Comparison between various versions of our proposed
models. The channels for each stage are set to [64 ,64,128 ,256]
and kept the same.
ture that exploits the structural data arrangement and hash
acceleration to attain notable efficiency and low memory
consumption, our method takes the performance lead but
still preserves a superior balance between effectiveness and
efficiency.
Receptive field expansion. We verify the effectiveness
of our proposed Adaptive Relation Convolution (ARConv)
by the comparison with three alternative modules com-
monly used for receptive field expansion: 1) multi-head
self-attention [54]; 2) grouped vector attention [60]; and 3)
pyramid pooling [72].
For attention-based modules, we operate the voxels like
nearest neighbor finding and grouping following the point
transformer [73]. The test results are shown in Tab. 6, where
our ARConv outperforms other competitors. Moreover,
Fig. 8 presents the comparison of the validation loss/mIoU
during the training process, and ARConv exhibits a supe-
rior capacity for mitigating overfitting than the others, as
evidenced by the lack of considerable deterioration in vali-
dation loss during the later period of training.
Aggregation methods. We validate the effectiveness and
superiority of the pyramid grid partition and proposed adap-
tive aggregator, and the experimental results are shown in
21311
Input Ground Truth Prediction
Figure 9. Visualization of segmentation results on ScanNet v2.
Tab. 5. The first row shows the result of the model with the
single-scale partition, where the additional aggregation is
not necessary. The second experiment, which adopts direct
concatenation for aggregation, leads to a marginal improve-
ment in performance. Then, by introducing our proposed
adaptive aggregator that adjusts the receptive field of each
voxel based on its intrinsic properties, we observed a sig-
nificant improvement in performance as compared to using
concatenation. Also, we investigate the effects of the num-
ber of pyramid stages and find that the three stages acquire
the best result, and all experiments follow this configuration
without otherwise specified.
Depthwise convolution. In contrast to regular convolu-
tion that applies one filter W‚ààRc√ól√óc, where crepre-
sents the channels and lrepresents the inputs‚Äô length, across
all input channels, depthwise convolution applies a single
filter for each input channel independently. Initially, we
attempted to implement regular convolution with the pro-
posed dynamic kernel weights but found it to be unstable
and non-convergent, particularly during the early training
stages. Consequently, we replaced it with both grouped
convolution [8] and depthwise convolution. The outcomes
are presented in Tab. 7. Our adoption of depthwise con-
volution with dynamically generated weights W‚ààRl√óc
yields linear complexity to input channels, showcasing the
dual benefits of efficiency and performance.
Dynamic kernel weights. Previous point-based methods,
such as [48, 64], have also explored dynamically generated
kernel weights. However, their approaches aim to incor-
porate geometric information from points rather than local
semantic relationships, mimicking convolutional operations
while still following the PointNet paradigm. Differently,
our work is based on sparse convolution networks. We
assess the effectiveness of our design for dynamic kernel
weight generation by comparing it with other alternatives
in Tab. 8, where ctrrepresents our adaptive relation kernelandposdenotes kernel weight generation using relative po-
sitions. Our adaptive relation kernel demonstrates superior
performance to other methods.
Multiple versions. We present multiple versions of OA-
CNNs, achieved by adjusting the number of blocks in each
stage while keeping other configurations consistent. In
all models, the number of channels per block is set to
[64,64,128,256]. The impact on performance and effi-
ciency is demonstrated in Tab. 9, where all models are eval-
uated on a single RTX 3090 to ensure a fair comparison.
4.4. Visual Analysis
Predictions. The qualitative results of point cloud seman-
tic segmentation are presented in Fig. 9. Our model ex-
hibits exceptional predictive accuracy on the ScanNet v2
dataset, with results that demonstrate high consistency with
the ground truth.
Receptive fields. Fig. 1 visualizes the varying receptive
field sizes of different objects and parts with distinct geo-
metric structures and appearances within a 3D indoor scene.
We calculate the sizes of the receptive fields as follows:
ri=XK
k=1wi,kgk, (8)
where gkrepresents the k-th grid size and w‚ààRn√óKin-
dicates preference weights predicted by the learnable adap-
tive aggregator in Eq. (2). We then map different sizes rito
the corresponding colors. Fig. 1 confirms our intuition that
3D scenes‚Äô flatter areas with simplistic structures, such as
walls and floors, require larger receptive fields. Conversely,
smaller objects and more intricate areas, such as edges and
junctions, need smaller ones. Additionally, we observe that
the floor generally requires a smaller receptive field than the
wall and ceiling, since it is necessary to exploit more local
contexts to distinguish itself from the objects placed on the
floor. More visual comparisons of the receptive field are put
in the supplementary materials.
5. Conclusion
This study highlights the potential of sparse convolution
networks to surpass transformer architectures in both ef-
ficiency and performance. To achieve this, we introduce
omni-adaptive 3D CNNs (OA-CNNs), which consist of two
key components: spatially dynamic receptive fields and
adaptive relation convolution. As for limitations, the cur-
rent pyramid grid sizes are set empirically, highlighting the
need for future research to develop more scientifically and
logically grounded search algorithms.
Acknowledgements . This work receives partial support
from the Shenzhen Science and Technology Program under
No. KQTD20210811090149095.
21312
References
[1] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioan-
nis Brilakis, Martin Fischer, and Silvio Savarese. 3d seman-
tic parsing of large-scale indoor spaces. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition , pages 1534‚Äì1543, 2016. 6
[2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-
mantickitti: A dataset for semantic scene understanding of
lidar sequences. In Proceedings of the IEEE/CVF inter-
national conference on computer vision , pages 9297‚Äì9307,
2019. 2, 6
[3] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fis-
cher. 3dmfv: Three-dimensional point cloud classification
in real-time using convolutional neural networks. IEEE
Robotics and Automation Letters , 3(4):3145‚Äì3152, 2018. 3
[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-
ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-
modal dataset for autonomous driving. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11621‚Äì11631, 2020. 2, 6
[5] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.
Multi-view 3d object detection network for autonomous
driving. In Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition , pages 1907‚Äì1915, 2017. 3
[6] Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang,
Jian Sun, and Jiaya Jia. Scaling up kernels in 3d cnns. CoRR ,
abs/2206.10555, 2022. 2, 4, 6
[7] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d
spatio-temporal convnets: Minkowski convolutional neural
networks. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pages 3075‚Äì3084,
2019. 1, 2, 6, 7
[8] Taco Cohen and Max Welling. Group equivariant convo-
lutional networks. In International conference on machine
learning , pages 2990‚Äì2999. PMLR, 2016. 8
[9] Pointcept Contributors. Pointcept: A codebase for point
cloud perception research. https://github.com/
Pointcept/Pointcept , 2023. 1
[10] Jiequan Cui, Shu Liu, Zhuotao Tian, Zhisheng Zhong, and
Jiaya Jia. Reslt: Residual learning for long-tailed recogni-
tion. TPAMI , 2023. 1
[11] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
ber, Thomas Funkhouser, and Matthias Nie√üner. Scannet:
Richly-annotated 3d reconstructions of indoor scenes. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 5828‚Äì5839, 2017. 2, 6
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1
[13] Benjamin Graham, Martin Engelcke, and Laurens Van
Der Maaten. 3d semantic segmentation with submani-
fold sparse convolutional networks. In Proceedings of theIEEE conference on computer vision and pattern recogni-
tion, pages 9224‚Äì9232, 2018. 1, 2, 3, 5, 6, 7
[14] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan
Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham.
Randla-net: Efficient semantic segmentation of large-scale
point clouds. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 11108‚Äì
11117, 2020. 2
[15] Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Recur-
rent slice networks for 3d segmentation of point clouds. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 2626‚Äì2635, 2018. 2
[16] Zhening Huang, Xiaoyang Wu, Xi Chen, Hengshuang Zhao,
Lei Zhu, and Joan Lasenby. Openins3d: Snap and lookup for
3d open-vocabulary instance segmentation. arXiv preprint
arXiv:2309.00616 , 2023. 1
[17] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V
Gool. Dynamic filter networks. Advances in neural informa-
tion processing systems , 29, 2016. 3
[18] Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-
Wing Fu, and Jiaya Jia. Hierarchical point-edge interaction
network for point cloud semantic segmentation. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 10433‚Äì10441, 2019. 2
[19] Li Jiang, Shaoshuai Shi, Zhuotao Tian, Xin Lai, Shu Liu,
Chi-Wing Fu, and Jiaya Jia. Guided point contrastive learn-
ing for semi-supervised point cloud semantic segmentation.
InICCV , 2021. 1
[20] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao,
Liwei Wang, and Jiaya Jia. Semi-supervised semantic seg-
mentation with directional context-aware consistency. In
CVPR , 2021. 1
[21] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang
Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-
former for 3d point cloud segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 8500‚Äì8509, 2022. 1, 2, 3, 6, 7
[22] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya
Jia. Spherical transformer for lidar-based 3d recognition. In
CVPR , 2023. 6
[23] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. LISA: reasoning segmentation
via large language model. arXiv preprint arXiv:2308.00692 ,
2023. 1
[24] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,
Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders
for object detection from point clouds. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 12697‚Äì12705, 2019. 3
[25] Felix J ¬®aremo Lawin, Martin Danelljan, Patrik Tosteberg,
Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg.
Deep projective 3d semantic segmentation. In Computer
Analysis of Images and Patterns: 17th International Confer-
ence, CAIP 2017, Ystad, Sweden, August 22-24, 2017, Pro-
ceedings, Part I 17 , pages 95‚Äì107. Springer, 2017.
[26] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from
3d lidar using fully convolutional network. arXiv preprint
arXiv:1608.07916 , 2016. 3
21313
[27] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec-
tive kernel networks. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition , pages
510‚Äì519, 2019. 3
[28] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di,
and Baoquan Chen. Pointcnn: Convolution on x-transformed
points. Advances in neural information processing systems ,
31, 2018. 6
[29] Yiqun Lin, Zizheng Yan, Haibin Huang, Dong Du, Ligang
Liu, Shuguang Cui, and Xiaoguang Han. Fpconv: Learn-
ing local flattening for point convolution. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 4293‚Äì4302, 2020. 3
[30] Jianhui Liu, Yukang Chen, Xiaoqing Ye, Zhuotao Tian, Xiao
Tan, and Xiaojuan Qi. Spatial pruned sparse convolution for
efficient 3d object detection. In NeurIPS , 2022. 1
[31] Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming
Xiang, and Chunhong Pan. Densepoint: Learning densely
contextual representation for efficient point cloud process-
ing. In Proceedings of the IEEE/CVF international confer-
ence on computer vision , pages 5239‚Äì5248, 2019. 2
[32] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012‚Äì10022, 2021. 1
[33] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-
enhofer, Trevor Darrell, and Saining Xie. A convnet for the
2020s. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 11976‚Äì11986,
2022. 4
[34] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 , 2017. 6
[35] Xiaoliu Luo, Zhuotao Tian, Taiping Zhang, Bei Yu,
Yuan Yan Tang, and Jiaya Jia. Pfenet++: Boosting few-shot
semantic segmentation with the noise-filtered context-aware
prior mask. TPAMI , 2024. 1
[36] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi
Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. V oxel
transformer for 3d object detection. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 3164‚Äì3173, 2021. 1, 5
[37] Daniel Maturana and Sebastian Scherer. V oxnet: A 3d con-
volutional neural network for real-time object recognition.
In2015 IEEE/RSJ international conference on intelligent
robots and systems (IROS) , pages 922‚Äì928. IEEE, 2015. 1,
3
[38] Hsien-Yu Meng, Lin Gao, Yu-Kun Lai, and Dinesh
Manocha. Vv-net: V oxel vae net with group convolutions for
point cloud segmentation. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 8500‚Äì
8508, 2019. 3
[39] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jae-
sik Park. Fast point transformer. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 16949‚Äì16958, 2022. 1, 4, 6[40] Bohao Peng, Zhuotao Tian, Xiaoyang Wu, Chengyao Wang,
Shu Liu, Jingyong Su, and Jiaya Jia. Hierarchical dense cor-
relation distillation for few-shot segmentation. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 23641‚Äì23651, 2023. 6
[41] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 652‚Äì660,
2017. 1, 2, 7
[42] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 1, 2, 5, 6
[43] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,
Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, and
Bernard Ghanem. Pointnext: Revisiting pointnet++ with
improved training and scaling strategies. arXiv preprint
arXiv:2206.04670 , 2022. 6
[44] David Rozenberszki, Or Litany, and Angela Dai. Language-
grounded indoor 3d semantic segmentation in the wild. In
Proceedings of the European Conference on Computer Vi-
sion (ECCV) , 2022. 6
[45] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-
lis Savva, and Thomas Funkhouser. Semantic scene com-
pletion from a single depth image. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 1746‚Äì1754, 2017. 1, 3
[46] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik
Learned-Miller. Multi-view convolutional neural networks
for 3d shape recognition. In Proceedings of the IEEE in-
ternational conference on computer vision , pages 945‚Äì953,
2015. 3
[47] Haotian* Tang, Zhijian* Liu, Shengyu Zhao, Yujun Lin, Ji
Lin, Hanrui Wang, and Song Han. Searching efficient 3d ar-
chitectures with sparse point-voxel convolution. In European
Conference on Computer Vision , 2020. 6
[48] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,
Beatriz Marcotegui, Franc ¬∏ois Goulette, and Leonidas J
Guibas. Kpconv: Flexible and deformable convolution for
point clouds. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 6411‚Äì6420, 2019. 3,
6, 8
[49] Zhuotao Tian, Michelle Shu, Pengyuan Lyu, Ruiyu Li, Chao
Zhou, Xiaoyong Shen, and Jiaya Jia. Learning shape-aware
embedding for scene text detection. In CVPR , 2019. 1
[50] Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu,
Hengshuang Zhao, and Jiaya Jia. Generalized few-shot se-
mantic segmentation. In CVPR , 2022. 1
[51] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng
Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrich-
ment network for few-shot segmentation. TPAMI , 2022. 1
[52] Zhuotao Tian, Pengguang Chen, Xin Lai, Li Jiang, Shu Liu,
Hengshuang Zhao, Bei Yu, Ming-Chang Yang, and Jiaya Jia.
Adaptive perspective distillation for semantic segmentation.
TPAMI , 2023. 1
21314
[53] Zhuotao Tian, Jiequan Cui, Li Jiang, Xiaojuan Qi, Xin Lai,
Yixin Chen, Shu Liu, and Jiaya Jia. Learning context-aware
classifier for semantic segmentation. In AAAI , 2023. 1
[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 1, 2, 3, 5, 7
[55] Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bo-
hao Peng, Hengshuang Zhao, and Jiaya Jia. Groupcontrast:
Semantic-aware self-supervised representation learning for
3d understanding. arXiv preprint arXiv:2403.09639 , 2024.
1
[56] Peng-Shuai Wang. Octformer: Octree-based transformers
for 3d point clouds. arXiv preprint arXiv:2305.03045 , 2023.
6
[57] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei
Chen, Zhuang Liu, In So Kweon, and Saining Xie. Con-
vnext v2: Co-designing and scaling convnets with masked
autoencoders. arXiv preprint arXiv:2301.00808 , 2023. 4
[58] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin,
and Michael Auli. Pay less attention with lightweight and dy-
namic convolutions. arXiv preprint arXiv:1901.10430 , 2019.
2, 3, 4
[59] Wenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep
convolutional networks on 3d point clouds. In Proceedings
of the IEEE/CVF Conference on computer vision and pattern
recognition , pages 9621‚Äì9630, 2019. 3, 6
[60] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and
Hengshuang Zhao. Point transformer v2: Grouped vec-
tor attention and partition-based pooling. arXiv preprint
arXiv:2210.05666 , 2022. 1, 2, 4, 5, 6, 7
[61] Xiaoyang Wu, Xin Wen, Xihui Liu, and Hengshuang Zhao.
Masked scene contrast: A scalable framework for unsu-
pervised 3d representation learning. In Proceedings of
the IEEE/CVF Conference on computer vision and pattern
recognition , 2023. 6
[62] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xi-
hui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang
Zhao. Point transformer v3: Simpler, faster, stronger. In
CVPR , 2024. 2
[63] Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui
Liu, Kaicheng Yu, and Hengshuang Zhao. Towards large-
scale 3d representation learning with multi-dataset point
prompt training. In CVPR , 2024. 2
[64] Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiao-
juan Qi. Paconv: Position adaptive convolution with dy-
namic kernel assembling on point clouds. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 3173‚Äì3182, 2021. 3, 8
[65] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-
ded convolutional detection. Sensors , 18(10):3337, 2018. 1
[66] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan
Ngiam. Condconv: Conditionally parameterized convolu-
tions for efficient inference. Advances in Neural Information
Processing Systems , 32, 2019. 3
[67] Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi
Zhu, Tong He, Shixiang Tang, Hengshuang Zhao, Qibo Qiu,Binbin Lin, Xiaofei He, and Wanli Ouyang. Unipad: A
universal pre-training paradigm for autonomous driving. In
CVPR , 2024. 2
[68] Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao
Peng, Shu Liu, and Jiaya Jia. An improved baseline for
reasoning segmentation with large language model. arXiv
preprint arXiv:2312.17240 , 2023. 2
[69] Senqiao Yang, Jiarui Wu, Jiaming Liu, Xiaoqi Li, Qizhe
Zhang, Mingjie Pan, Yulu Gan, Zehui Chen, and Shanghang
Zhang. Exploring sparse visual prompt for domain adaptive
dense prediction. In Proceedings of the AAAI Conference on
Artificial Intelligence , pages 16334‚Äì16342, 2024. 2
[70] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao,
and Xihui Liu. Sam3d: Segment anything in 3d scenes. arXiv
preprint arXiv:2306.03908 , 2023. 1
[71] Dong Zhang, Yi Lin, Hao Chen, Zhuotao Tian, Xin Yang,
Jinhui Tang, and Kwang-Ting Cheng. Deep learning for
medical image segmentation: Tricks, challenges and future
directions. arXiv preprint arXiv:2209.10307 , 2022. 1
[72] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 2881‚Äì2890, 2017. 7
[73] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and
Vladlen Koltun. Point transformer. In Proceedings of
the IEEE/CVF international conference on computer vision ,
pages 16259‚Äì16268, 2021. 1, 2, 3, 5, 6, 7
[74] Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xi-
aojuan Qi, Xiangyu Zhang, and Jiaya Jia. Understanding
imbalanced semantic segmentation through neural collapse.
2023. 1
[75] Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha
Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chun-
hua Shen, Yu Qiao, and Wanli Ouyang. Ponderv2: Pave the
way for 3d foundation model with a universal pre-training
paradigm. arXiv preprint arXiv:2310.08586 , 2023. 1
[76] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin
Ma, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and
asymmetrical 3d convolution networks for lidar segmenta-
tion. arXiv preprint arXiv:2011.10033 , 2020. 6
21315
