Bring Event into RGB and LiDAR:
Hierarchical Visual-Motion Fusion for Scene Flow
Hanyu Zhou1, Yi Chang1*, Zhiwei Shi1
1National Key Lab of Multispectral Information Intelligent Processing Technology,
School of ArtiÔ¨Åcial Intelligence and Automation, Huazhong University of Science and Technology
{hyzhou, yichang, shizhiwei}@hust.edu.cn
Abstract
Single RGB or LiDAR is the mainstream sensor for the
challenging scene Ô¨Çow, which relies heavily on visual fea-
tures to match motion features. Compared with single modal-
ity, existing methods adopt a fusion strategy to directly fuse
the cross-modal complementary knowledge in motion space.
However, these direct fusion methods may suffer the modality
gap due to the visual intrinsic heterogeneous nature between
RGB and LiDAR, thus deteriorating motion features. We dis-
cover that event has the homogeneous nature with RGB and
LiDAR in both visual and motion spaces. In this work, we
bring the event as a bridge between RGB and LiDAR, and
propose a novel hierarchical visual-motion fusion frame-
work for scene Ô¨Çow, which explores a homogeneous space to
fuse the cross-modal complementary knowledge for physical
interpretation. In visual fusion, we discover that event has a
complementarity (relative v.s. absolute) in luminance space
with RGB for high dynamic imaging, and has a complemen-
tarity (local boundary v.s. global shape) in scene structure
space with LiDAR for structure integrity. In motion fusion,
we Ô¨Ågure out that RGB, event and LiDAR are complementary
(spatial-dense, temporal-dense v.s. spatiotemporal-sparse)
to each other in correlation space, which motivates us to
fuse their motion correlations for motion continuity. The pro-
posed hierarchical fusion can explicitly fuse the multimodal
knowledge to progressively improve scene Ô¨Çow from visual
space to motion space. Extensive experiments have been
performed to verify the superiority of the proposed method.
1. Introduction
Scene Ô¨Çow aims to model the correspondence between
adjacent visual RGB or LiDAR features to estimate 3D mo-
tion features, which has been applied in various vision tasks,
e.g., 3D object detection [ 1] and motion segmentation [ 2].
The key to scene Ô¨Çow estimation is the highly discriminative
*Corresponding author.
Visual space 
Visual space Motion spaceFuse
FuseFuseRGB LiDAR 
Scene flowEvent RGB feature
Event feature
LiDAR featureVisual feature 
Motion feature
HMotion homogeneous feature Visual homogeneous feature HHeterogeneous Gap Visual space Visual space Visual space HH
Motion spaceMotion spaceHHH
H
HBridge Bridge 
Figure 1. Illustration of the main idea. There exists a large modality
gap due to the visual intrinsic heterogeneous nature between RGB
and LiDAR, thus deteriorating the motion features. We discover
that the event has the homogeneous nature with RGB and LiDAR in
both visual and motion spaces. In this work, we bring the event as a
bridge between RGB and LiDAR, and propose a novel hierarchical
visual-motion fusion framework for scene Ô¨Çow, which explores
a homogeneous feature space to explicitly fuse the cross-modal
complementary knowledge for physical interpretation.
visual feature for the valid motion feature matching.
Existing scene Ô¨Çow methods can be divided into two
categories: unimodal direct learning methods [ 3‚Äì7] and mul-
timodal motion fusion methods [ 8,9]. Unimodal direct
learning methods mainly resort to the ideal visual feature to
directly learn the motion feature from single RGB or LiDAR
modality. Multimodal motion fusion methods further exploit
the complementary knowledge between RGB and LiDAR to
fuse their features in the motion space. For example, Liu et
al.[9] fused dense RGB motion features and sparse LiDAR
motion features for scene Ô¨Çow. However, these methods
neglect the visual intrinsic heterogeneous nature, e.g., de-
graded visual RGB features with weakened texture under low
light due to low dynamic range and limited visual LiDAR
features with incomplete contour due to non-uniform laser
beam. These can enlarge the modality gap and deteriorate the
motion features. Therefore, we suggest that introducing an
auxiliary modality as the bridge to assistantly enhance the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26477
RGB and LiDAR features in both visual and motion spaces.
To our investigation, event camera [ 10] is a neuromorphic
sensor, which can sense the dynamic scene boundary struc-
ture with high dynamic range and high temporal resolution.
In visual space, RGB records the absolute value of luminance
while the event triggers the relative change of luminance,
and LiDAR reÔ¨Çects the global shape of the scene while the
event senses the local boundary. In motion space, we trans-
form the visual features of these modalities into correlation
space, and we Ô¨Ågure out that RGB provides the spatial-dense
2D features, event provides the temporal-dense 2D features,
and LiDAR provides the spatiotemporal-sparse 3D features.
Therefore, the event has the complementarity with RGB and
LiDAR in visual homogeneous ( i.e., luminance and struc-
ture) and motion homogeneous ( i.e., correlation) spaces.
In this work, we bring the auxiliary event as a bridge
between RGB and LiDAR in Fig. 1, and propose a novel
hierarchical visual-motion fusion framework for scene Ô¨Çow
(VisMoFlow), which focuses on fusing the cross-modal com-
plementary knowledge in the homogeneous space. In visual
luminance fusion, we fuse the relative luminance of event
and the absolute luminance of RGB for high dynamic imag-
ing with the theoretically derived spatiotemporal gradient
consistency. In visual structure fusion, we fuse the local
boundary of event into the global shape of LiDAR for phys-
ical structure integrity using the self-similarity clustering
strategy. In motion correlation fusion, we map event, RGB
and LiDAR into the same motion manifold, and fuse the
x, y-axis spatial-dense correlation features of RGB, the x,
y-axis temporal-dense correlation features of event and the x,
y, z-axis sparse correlation features of LiDAR for 3D motion
spatiotemporal continuity by aligning the motion distribu-
tions between various modalities. The proposed hierarchical
visual-motion fusion can explicitly learn the multimodal
complementary knowledge from visual to motion space for
scene Ô¨Çow. Overall, the main contributions are as follows:
‚Ä¢We bring the auxiliary event as a bridge between RGB
and LiDAR modalities, and propose a novel hierarchical
visual-motion fusion framework for scene Ô¨Çow, which can
explicitly fuse the multimodal knowledge to progressively
improve scene Ô¨Çow from visual space to motion space.
‚Ä¢We reveal that RGB, event and LiDAR modalities have the
complementary knowledge in both visual homogeneous
(i.e., luminance and structure) and motion homogeneous
(i.e., correlation) spaces, which can make the entire multi-
modal fusion process more physically interpretable.
‚Ä¢ We conduct extensive experiments on daytime and night-
time scenes, which verify that the VisMoFlow achieves
state-of-the-art performance for all-day scene Ô¨Çow.
2. Related Work
Unimodal Scene Flow Estimation. Since scene Ô¨Çow is the
3D version of optical Ô¨Çow, the previous scene Ô¨Çow methods[3‚Äì7,11‚Äì13] mainly directly follow the optical Ô¨Çow frame-
work [ 14,15]. They take monocular [ 5,16,17] or stereo
sequences [ 6,12] as input, and calculate cost volume with
warp operation through feature pyramid [ 18] or recurrent net-
work [ 14] for 3D motion. However, since 2D RGB images
lack the intrinsic 3D geometry property, LiDAR becomes
a trend in scene Ô¨Çow estimation due to its 3D scene per-
ception capability. Most LiDAR-based scene Ô¨Çow methods
[7,13,19,20] also follow the architecture of the optical Ô¨Çow
framework, while the difference is that they use MLP or
voxel to represent the point cloud features. However, due to
the potential incomplete physical contour of LiDAR, these
single LiDAR-based scene Ô¨Çow methods are still limited.
Instead, there is a strong complementarity between the two
modalities, which inspires us to fully exploit the cross-modal
complementary knowledge for scene Ô¨Çow.
Multimodal Scene Flow Estimation. As unimodal scene
Ô¨Çow methods provide the partial information, multimodal
fusion strategies [ 8,9,21,22] have gradually received atten-
tion. Liu et al. [9] proposed a bidirectional 2D-3D motion
feature fusion framework, which can exploit the comple-
mentary features between RGB and LiDAR. Wan et al. [22]
introduced the event camera to enhance the RGB and LiDAR
features in motion space via a cross-attention mechanism.
However, these methods may suffer the potential degraded
visual RGB features with weakened texture and limited vi-
sual LiDAR features with incomplete contour, thus leading
to a large modality gap and deteriorating the valid motion
features. In this work, we will introduce the auxiliary event
as a bridge to hierarchically enhance the RGB and LiDAR
features in both visual and motion spaces.
Multimodal Fusion. Multimodal fusion focuses on exploit-
ing the inter-modality complementarity for the target task.
To our knowledge, most existing multimodal fusion methods
learn the mutual knowledge between different modalities via
cross-attention mechanism [ 23‚Äì26] or similarity represen-
tation learning [ 27‚Äì30]. For example, Cai et al. [26] pro-
posed to utilize transformer [ 31] to directly fuse the proposal
features of RGB images and LiDAR for 3D object detec-
tion. However, these methods neglect the modality gap due
to the intrinsic heterogeneous nature of feature representa-
tion between various modalities, limiting the complementary
knowledge fusion. To alleviate this issue, we introduce an
auxiliary modality as the bridge to mitigate the modality gap,
and explore the homogeneous space to explicitly fuse the
cross-modal complementary knowledge for scene Ô¨Çow.
3. Hierarchical Visual-Motion Fusion
3.1. Overall Framework
Multimodal scene Ô¨Çow framework aims to fuse the cross-
modal complementary knowledge for enhancing the discrim-
inative visual and motion features that scene Ô¨Çow relies
26478
tt+ Œît Images I‚àÜ - ‚àëpippCrE
Time Events 
t+Œît tU
Temporal
Project
Spatial
Project
KNNconsis LRGB‚ÜíYUV
adv L
2D‚Üí3D 
Project
Time slice Voxelization Sampling 
Saptiotemporal Gradient Consistency Luminance
Fusion
Self-Similarity Clustering ev E2D 
lE3D    Cost 
Volume Motion Distribution Alignment 
CV Temporal
dense featureSpatial
dense feature
Main 
sparse feature¬∑¬∑¬∑CV 
CV 
CV Œît 
U2D Feature
Encoder
Feature
Encoder
Feature
Encoder2D Visual Luminance Fusion 
Motion Correlation Fusion Visual Structure Fusion 
Position
project3D 2D Temporal
Sample2D Spatial
Sample
2D position 
3D position 3D Spatial
Sample zxyxyx
y
xy yxCorrelation features xx
yy
GRU Fusion 
pp
pp
p
pxxx
xx
x
xy
yyy
yy
y
zppp
kl Lx
Scene flow pho LEnhanced point cloud 
Neighbors 
Depth map Intrinsic K 
Point cloud Event frames 
kl Ly
2D position 
Positionp pp
RGB xyp
Eventxyp
Lidarz xy pIntrinsic K Correlation features 
Correlation features Paired 
Paired 
TransformerCBoundary map Cluster
Cluster
2D‚Üí3D 
Project
pse L
Fused depth
Gradient Gradient 
HDR images 
TimeEvents
YUV
tt+ Œît Figure 2. The architecture of the VisMoFlow mainly contains visual luminance fusion, visual structure fusion and motion correlation fusion.
In visual luminance fusion, we fuse the relative luminance of event and the absolute luminance of RGB for high dynamic imaging. In visual
structure fusion, we fuse the local boundary structure of event and the global shape structure of LiDAR for structure integrity. In motion
correlation fusion, we fuse the spatiotemporal complementary correlation knowledge of the three modalities for 3D motion continuity.
on. However, there exists a modality gap due to the visual
intrinsic heterogeneous nature between RGB and LiDAR,
deteriorating motion features. To this end, we introduce the
auxiliary event as a bridge to mitigate the gap between RGB
and LiDAR, and propose a novel multimodal hierarchical
visual-motion fusion framework for scene Ô¨Çow in Fig. 2. The
whole architecture looks complicated but is simply regarded
as a task that builds a homogeneous space to fuse the cross-
modal complementary knowledge for physical interpretation.
In visual luminance fusion, we transform the event and RGB
into the luminance space, and fuse the complementary (rel-
ative v.s. absolute) knowledge for high dynamic imaging
under the constraint of the similar spatiotemporal gradient.
In visual structure fusion, we represent the event and LiDAR
into the spatial structure space, and fuse the complementary
(local boundary v.s. global shape) knowledge for physical
structure integrity using the self-similarity clustering strat-
egy. In motion correlation fusion, we map the visual features
of RGB, event and LiDAR to the same correlation space,
and fuse the complementary (x, y-axis spatial-dense corre-
lation, x, y-axis temporal-dense correlation and x, y, z-axis
sparse correlation) knowledge for 3D motion spatiotempo-
ral continuity via motion distribution alignment. Under the
uniÔ¨Åed framework, the proposed method can explicitly fuse
the cross-modal complementary knowledge to progressively
improve scene Ô¨Çow from visual space to motion space. Next,
we will describe the hierarchical visual-motion fusion frame-
work in the form of ‚Äúwhat to fuse‚Äù and ‚Äúhow to fuse‚Äù.
3.2. Visual Luminance Fusion
Under oversaturated and low-light conditions, the RGB
image may loss the texture, thus mismatching the motionfeatures. The main reason is that RGB imaging mechanism
limits the dynamic range. Therefore, we introduce event
camera with the advantage of high dynamic range to assis-
tantly enhance the visual RGB features.
Relative v.s.Absolute Luminance. Event camera can asyn-
chronously capture the events triggered by the brightness
change, recording the relative luminance, while RGB camera
records the absolute luminance via global scan. Motivated
by this, we argue that event and RGB share the homoge-
neous luminance space, where the relative value of event and
the absolute value of RGB are complementary to each other.
SpeciÔ¨Åcally, given the RGB image Iand the corresponding
event stream, we transform the RGB image into YUV color
space [IY;IU;IV], whereIYdenotes the luminance, IU,
IVis the chrominance information. We also accumulate the
event stream to generate the intensity frame:
IX=X
piC; (1)
wherepidenotes the event polarity. Cis the event trigger
threshold. Therefore, the absolute value IYfrom the RGB
image and the relative value IXfrom the event are comple-
mentary in the homogeneous luminance space. This makes
us naturally transform RGB and event into a uniÔ¨Åed lumi-
nance space to achieve complementary knowledge fusion.
Spatiotemporal Gradient Consistency. We set various
weights to fuse the absolute luminance of the RGB and the
relative luminance of the event as follows:
HY=W(IY;IX) = (!yIY+!xIX)=(!y+!x);(2)
where!x,!ydenote the weights of event and RGB, respec-
tively. The fused luminance image HYhas a high dynamic
range, but without color information. The color can be com-
pensated from U, V channels IU,IVof the input RGB
26479
Cluster 
LiDAR (x, y, z)Spatial
Project
Temporal 
ProjectCluster 
-20 -10 0 10 20 -30 30 20 
10 
0
-10
-2030 
-30Shape feature of LiDAR
Boundary feature of Eventt-SNE of Structure Clustering Features
Event (x, y, t)2D Shape
2D BoundaryStructure manifoldShared
v.s. 
v.s. 
Figure 3. Clustering feature distribution of event and LiDAR. Event
and LiDAR share the same structure manifold, where the boundary
distribution of event is continuous while the shape distribution of
LiDAR is truncated. This motivates us to take the structure as a
homogeneous space to fuse the boundary and shape knowledges.
images, namely HY UV=C(HY;IU;IV). We then merge
this equal and Eq. 2 as follows:
H=C(W(IY;IX);IU;IV): (3)
The fused YUV image Hshould be further inversely
transformed to the RGB color space. We follow [ 32] to
utilize the luminance fusion network to model the Eq. 3. We
take the adjacent RGB images [ It;It+t] at the timestamps
tandt+ t, and the corresponding event stream as input,
and output the fused RGB images [ ~It;~It+t]. Note that, the
fused images have two limitations: slightly unnatural color
appearance and potential motion discontinuity. To recover
the realistic color appearance, we force the adversarial loss
[33] to make the fused images look natural:
Ladv=E[log(1 D(~It))] +E[log(1 D(~It+t))];(4)
whereDis the discriminator. To prevent the motion dis-
continuity, we explore a motion-related prior knowledge to
constrain the fusion process. We start ideally to yield the
optical Ô¨Çow basic model as follows via Taylor expansion:
I(x+dx;t +dt) =I(x;t) + (@I
@xdx+@I
@tdt) +O(dx;dt ):(5)
Then, we remove the high-order error term O(dx;dt )to
approximate Eq. 5 as follows:
It= rIU; (6)
whereUdenotes optical Ô¨Çow,  rIUis the spatiotempo-
ral gradient of current frame. Itdenotes temporal bright-
ness change, which can be approximated as the accumulated
events warped by optical Ô¨Çow in a certain time, namely
It=UP
ei2tpiC. Therefore, we take the spatiotempo-
ral gradient consistency to constrain the fusion:
Lconsis =X
jjUX
ei2tpiC+rIUjj1V=X
V; (7)
whereVis the valid mask generated by optical Ô¨Çow U. The
visual luminance fusion that obeys the basic motion model
can guarantee the valid motion feature matching.
3.3. Visual Structure Fusion
LiDAR point cloud has the incomplete contour of the
scene due to a non-uniform laser beam, limiting the scene
Ô¨Çow performance. We aim to introduce the event to assist in
completing the physical structure of the LiDAR point cloud.
Local Boundary v.s.Global Shape. LiDAR emits uniformlaser beams in the horizontal direction, and the reÔ¨Çected laser
can show the global shape of 360scene. However, LiDAR
just emits the non-uniform laser beams in the vertical direc-
tion due to the vertical angle. The larger the vertical angle,
the fewer the number of the reÔ¨Çected point cloud, resulting
in the incomplete contour of LiDAR. In contrast, event can
sense the rich motion boundaries of the scene. Therefore,
the local boundary of event and the global shape of LiDAR
can jointly complete the intrinsic structure of the scene. To
illustrate the complementary nature between LiDAR and
event in the structure space, as shown in Fig. 3, we cluster
the spatially-projected 2D LiDAR frame and the temporally-
projected 2D event frame into the same structure manifold,
and visualize the clustering feature distributions of LiDAR
and event. First, the distribution trend of LiDAR is similar
to the event. Second, the distribution of event boundary is
continuous while the distribution of LiDAR shape is trun-
cated, indicating that the contours of the 2D LiDAR frame
are incomplete but those of the 2D event frame are relatively
complete. This motivates us to take the scene structure as a
homogeneous space to fuse the local boundary of event into
the global shape of LiDAR for physical structure integrity.
Self-Similarity Clustering. Structure fusion mainly de-
pends on a self-similarity clustering strategy, which is di-
vided into two steps: intra-modal structure neighbor and
inter-modal data fusion. The former is to properly cluster
LiDAR shape points and event boundary points into the same
neighbor. The latter is to map the local boundary coordinates
of event to the global shape coordinates of LiDAR in the
neighbor, thus completing the physical contour. Given the
aligned LiDAR point cloud pc=fxi;yi;ziji2Mg, the cor-
responding event stream ev=fxj;yj;tj;pjjj2Ng, intrin-
sic parameter K=ff;cx;cyg, we Ô¨Årst normalize the event
stream along the timestamps to obtain the 2D coordinates of
eventPe=fuj=xj;vj=yj;pjjfxj;yj;pjg2evg, and
use the intrinsic parameter to project the LiDAR point cloud
into the event camera coordinate system Pl=fui;vi;dig:
ui=fxi=zi+cx;vi=fyi=zi+cy;di=zi;
s:t:fxi;yi;zig2pc;0<ui<w; 0<ui<h;(8)
wherew,his the size of event frame, fis focal length, cx,cy
are x, y-axis offsets. In the intra-modal structure neighbor
stage, we construct the distance function as the self-similarity
measurement metric for 2D coordinates of the event:
dp=q
(p1 p2)2;ds=q
(u1 u2)2+ (v1 v2)2;
De=q
d2p+ (ds=Ns)2;s:t:fu1;2;v1;2;p1;2g2Pe;(9)
whereNsis the max distance of 2D coordinates. We also
deÔ¨Åne the same distance function Dlfor the 2D coordinates
of LiDAR. We iteratively optimize the distance functions De,
Dlto obtain the neighbors U, which represent the region-
level structure information like the image superpixel. In the
inter-modal data fusion stage, we propose a similarity-based
bi-direction fusion strategy: event !LiDAR structure fusion
26480
x
yt
x
yx
yzRGB images (x, y) Event stream (t, x, y) LiDAR point cloud (x, y, z)
Spatial
dense Temporal 
dense 3D 
sparse Correlation 
Sample
RGB 
Event
LiDAR Warp Warp Warp 
Time 
x y
Sample Sample
t2t1
x-axis y-axis z-axis Time 
Density Sample
space 
t1t2Figure 4. Correlation distributions of RGB, event and LiDAR.
The three modalities have similar distributions in x, y-axis corre-
lation, with z-axis correlation unique to LiDAR. RGB correlation
is spatially dense, event is temporally dense, while LiDAR is spa-
tiotemporally sparse. This inspires us to build the homogeneous
correlation space to fuse the complementary motion knowledge.
and LiDAR!event depth fusion. As for the event !LiDAR
structure fusion, we take the various 2D LiDAR coordinates
of each neighbor as the center, and use the K-nearest neigh-
bor to select the top k2D event coordinates with the highest
similarity and Ô¨Åll them into the 2D LiDAR coordinates in
the same neighbor. As for the LiDAR !event depth fu-
sion, we Ô¨Ålter the top k2D LiDAR coordinates in the same
way, and weightedly fuse their depths to the selected event
coordinates to obtain the Ô¨Ånal depth map. To ensure the
back-propagation of the whole framework, we introduce a
cross-attention transformer [ 31] to model the visual struc-
ture fusion process between event and LiDAR. We use the
weighted-fused depth maps as the pseudo labels dpse
t,dpse
t+t
to train the cross-attention transformer:
Lpse=X
jj~dt dpse
tjj1+jj~dt+t dpse
t+tjj1; (10)
where ~dt,~dt+tare the fused adjacent depth maps estimated
from the transformer network, and then are inversely trans-
formed to the 3D point clouds ~pct,~pct+t.
3.4. Motion Correlation Fusion
Visual luminance and structure fusions do ensure the
visualization of RGB and LiDAR, while how to physically
and complementarily fuse the mapped motion features of
various modalities to improve scene Ô¨Çow is a challenge.
Spatiotemporal Dense v.s.Sparse Correlation. Motion
space that scene Ô¨Çow relies on consists of two types: tempo-
ral correlation subspace [ 14] and decoded motion subspace[15]. The former is to model the feature similarity between
adjacent frames, and the latter contains the features decoded
by recurrent network ( e.g., GRU [ 34]). Since the correlation
subspace has more physical meaning, it has the potential to
be a homogeneous space. As far as we know, RGB provides
texture information with high spatial resolution, event pro-
vides boundary information with high temporal resolution,
and LiDAR provides 3D (x, y, z) scene structure information.
This inspires us to consider that, in the correlation space,
RGB may promise the x, y-axis spatial-dense correlation,
event may ensure the x, y-axis temporal-dense correlation
and LiDAR may guarantee the x, y, z-axis accurate but sparse
correlation. To verify our insight, as shown in Fig. 4, we
transform the visual RGB, event and LiDAR features into the
correlation subspace of motion space via warp operator [ 18],
and calculate their distributions in x, y, z three dimensions,
respectively. Note that, the correlation distribution of event
is analyzed along the timestamps. On one hand, the corre-
lation distribution trends of these three modalities are quite
similar along the x and y axes, with z-axis correlation unique
to LiDAR. On the other hand, in terms of the complementar-
ity in the correlation space, RGB is spatial-dense, event is
temporal-dense, and LiDAR is spatiotemporal-sparse. There-
fore, we treat the correlation space as a homogeneous space
to fuse the cross-modal knowledge for motion continuity.
Motion Distribution Alignment. Correlation fusion is Ô¨Årst
to fuse the dense-sparse complementary knowledge between
various modalities in the single-axis correlation, and then
merge the fused correlation distributions across all three axes.
First, we voxelize the event stream into multiple-slice vox-
els. We encode the fused RGB images, the voxelized events
and the fused LiDAR point clouds to the RGB correlation
cvr, the event correlation cveand the LiDAR correlation cvl
using the warp operator. Then, we randomly sample N3D
points from LiDAR. From these points, we obtain 2D coor-
dinates for RGB and event with intrinsic parameters. Next,
we use the sampled coordinates as the center, and perform
2D spatially-sampling for the RGB correlation to obtain the
corresponding spatial-dense x, y-axis correlations cvx
r,cvy
r.
As for the event, we temporally sample the correlation fea-
tures to get the temporal-dense x, y-axis correlations cvx
e,
cvy
e. For the LiDAR, we spatially sample the correlations
into the relatively sparse x, y, z-axis correlations cvx
l,cvy
l,
cvz
l. Before the correlation fusion, we apply K-L divergence
to align the multimodal motion correlation distributions:
Lkl
corr=X
(cvx;y
l)log(cvx;y
l)
(cvx;y
r)+ (cvx;y
l)log(cvx;y
l)
(cvx;y
e);(11)
where is the softmax function. We Ô¨Årst fuse the aligned
x, y-axis correlations between various modalities and then
concatenate the z-axis correlations of LiDAR:
corr =Concatf1
TXT
i=0(cvx
r+cvx;i
e+cvx
l)=3;
1
TXT
i=0(cvy
r+cvy;i
e+cvy
l)=3;cvz
lg;(12)
whereTdenotes the number of the temporal slices of events.
26481
MethodScene Ô¨Çow methods Optical Ô¨Çow methods
RAFT-3D RAFT-3D w/ e PV-RAFT CamLiFlow RPEFlow VisMoFlow SMURF FlowFormer RPEFlow VisMoFlow
Input RGB RGB PC RGB+PC RGB+PC+EV RGB+PC+EV RGB RGB RGB+PC+EV RGB+PC+EV
DayEPE 0.095 ‚Äì 0.055 0.033 0.060 0.012 2.01 0.607 0.556 0.198
ACC 73.48% ‚Äì 79.96% 91.40% 81.73% 98.55% 83.34% 89.08% 88.98% 97.11%
NightEPE 0.112 0.104 0.055 0.047 0.056 0.027 11.360 2.085 0.716 0.353
ACC 65.65% 72.65% 79.97% 85.62% 81.47% 95.62% 55.12% 77.05% 78.85% 96.28%
Table 1. Quantitative results on synthetic Event-KITTI dataset. ‚ÄúPC‚Äù is LiDAR point cloud, ‚ÄúEV‚Äù is event. ‚Äúw/ e‚Äù is image enhancement.
Figure 5. Visual comparison of scene Ô¨Çows on synthetic Event-KITTI dataset.
Method RAFT-3D PV-RAFT CamLiFlow RPEFlow VisMoFlow
Input RGB PC RGB+PC RGB+PC+EV RGB+PC+EV
DayEPE 0.167 0.183 0.113 0.103 0.084
ACC 13.16% 37.28% 55.69% 60.81% 70.34%
NightEPE 0.359 0.190 0.125 0.094 0.090
ACC 5.04% 40.98% 53.10% 66.49% 68.31%
Table 2. Quantitative results on real DSEC dataset.
Furthermore, we introduce GRU to recursively decode the
fused correlation features to estimate the Ô¨Ånal scene Ô¨Çow.
In addition, we learn the scene Ô¨Çow and the corresponding
optical Ô¨Çow with photometric loss [35]:
Lpho=X
 (~It w2D(~It+t))V2D=X
V2D
+X
 ( ~pct w3D( ~pct+t))V3D=X
V3D;(13)
where is aLpnorm (p= 0:4).V2D,V3Dis the 2D and 3D
occlusion masks by checking forward-backward consistency
[36]. We also apply the photometric loss to train the event
motion feature encoder. The proposed motion correlation
fusion can ensure 3D motion spatiotemporal continuity.
3.5. Optimization and Implementation Details
Consequently, the total objective for the proposed hierar-
chical fusion framework is written as follows:
L=Lpho+1Ladv+2Lconsis +3Lpse+4Lkl
corr;(14)
where [1;:::; 4] are the weights that control the impor-
tance of the related losses. The Ô¨Årst term is to maintain
the capability of motion feature encoders of RGB, event
and LiDAR. The second and third terms are to constrain
the Event-RGB visual luminance fusion, the fourth term is
to promise that the cross-attention transformer can learn to
fuse the Event-LiDAR structure, and the intention of the
last term aims to promote the RGB-Event-LiDAR motion
correlation fusion. We set the KNN selection number kas 5,
the event temporal slices Tas 10, and the sample number N
as 1000. During the training phase, we only need three steps.First, we train Event-RGB visual luminance fusion for high
dynamic imaging. Second, we train Event-LiDAR visual
structure fusion for physical structure integrity. Finally, we
train RGB-Event-LiDAR motion correlation fusion for 3D
motion spatiotemporal continuity. Note that, the proposed
method is Ô¨Årst pre-trained to initialize the motion estimators
in a supervised manner. During the testing phase, the whole
framework consists of the luminance fusion network, the
cross-attention transformer and motion estimation networks,
thus achieving the end-to-end estimation of scene Ô¨Çow.
4. Experiments
4.1. Experiment Setup
Dataset. We conduct extensive experiments on one syn-
thetic ( e.g., Event-KITTI) and one real ( e.g., DSEC) datasets,
including daytime and nighttime scenes.
Event-KITTI. The origin KITTI [ 37] is set in daytime
scenes. We take the V2E model [ 38] to generate the event
stream from daytime images, and use the noise model [ 39]
to synthesize nighttime images corresponding to daytime.
DSEC. DSEC is a multimodal dataset [ 40], which cov-
ers the multimodal data of RGB camera, LiDAR and event
camera in various outdoor scenes and various times.
Comparison Methods. We choose optical Ô¨Çow and scene
Ô¨Çow methods for comparison in both daytime and night-
time scenes. As for optical Ô¨Çow comparison, we choose
unsupervised (SMURF [ 41]), supervised (FlowFormer [ 42]),
and multimodal fusion (RPEFlow [ 22]) methods. As for
scene Ô¨Çow comparison, we choose RGB-based unimodal
(RAFT-3D [ 6]), LiDAR-based unimodal (PV-RAFT [ 7]),
RGB-LiDAR multimodal fusion (CamLiFlow [ 9]) and RGB-
Event-LiDAR multimodal fusion (RPEFlow) methods. Note
that, in nighttime scenes, we add an additional comparison:
Ô¨Årst performing image enhancement ( e.g., KinD++ [ 43]) and
then estimate motion on the enhanced results (named as ‚Äúw/
26482
Figure 6. Visual comparison of scene Ô¨Çows on real DSEC dataset.
Fusion strategy EPE ACC
w/o any fusion 0.151 49.20%
w/ visual fusion 0.118 54.35%
w/ motion fusion 0.098 62.87%
w/ visual-motion fusion 0.084 70.34%
Table 3. Effectiveness of hierarchical visual-motion fusion.
e‚Äù). We choose the average end-point error (EPE [ 37]) and
the percentage of Ô¨Çow accuracy within 1px for optical Ô¨Çow
or 5cm for scene Ô¨Çow (ACC [9]) as the evaluation metrics.
4.2. Comparison Experiment
Comparison on Synthetic Dataset. In Table 1 and Fig. 5,
we compare different motion methods on the synthetic Event-
KITTI dataset. First, the proposed method signiÔ¨Åcantly
outperforms the competing methods in optical Ô¨Çow and scene
Ô¨Çow. Second, in nighttime scenes, the performance of those
methods that use RGB images decreases slightly. Note that,
image enhancement can indeed promote the RGB-based
motion methods in nighttime scenes, but the improvement is
limited. Third, multimodal methods are more effective than
unimodal methods in improving motion estimation due to
the complementary knowledge between various modalities.
Notably, the proposed visual-motion fusion surpasses the
RPEFlow that relies solely on motion fusion.
Comparison on Real Scenes. In Table 2, we compare the
scene Ô¨Çow methods in real daytime and nighttime scenes on
the DSEC dataset. We have two conclusions. First, multi-
modal methods are far superior to unimodal methods. This
is because the complementary knowledge of various modali-
ties in real scenes can make up for their own disadvantage,
conducive to deeply exploiting the characteristic of scene
Ô¨Çow. Second, event camera can further effectively improve
nighttime scene Ô¨Çow. The main reason is that, event camera
have the advantage of high dynamic range to sense scene
motion at night. In Fig. 6, our scene Ô¨Çow visualization re-
sults show more complete structures and perform better than
other competing methods. The main reason is that, the visual
structure fusion does fuse the local boundary of event into
the global shape of LiDAR for point cloud physical integrity,
thus improving the matching capability of motion features.
w/ FC20 40 60 80 100
0
w/ FC+L w/ FC+L+S w/ FC+L+S+CEvent-KITTI
DSEC (e) Accuracy of scene flowAccuracy (%, 5cm) 
(b) w/ FC+L
(c) w/ FC+L+S
(d) w/ FC+L+S+C
(a) Multimodal data
Figure 7. Effectiveness of homogeneous spaces in fusion. (a) Input
data. (b-d) Scene Ô¨Çow. ‚ÄúFC‚Äù is implicit feature concatenate. ‚ÄúL‚Äù is
luminance space. ‚ÄúS‚Äù is structure space. ‚ÄúC‚Äù is correlation space.
4.3. Ablation Study
How does Hierarchical Visual-Motion Fusion Work? In
Table 3, we demonstrate the effectiveness of the proposed hi-
erarchical visual-motion fusion framework. With only visual
fusion, there is an improvement in scene Ô¨Çow. With only
motion fusion, scene Ô¨Çow is signiÔ¨Åcantly improved. When
we apply visual fusion and motion fusion, the hierarchical
fusion is superior to the motion fusion alone. Therefore,
motion fusion is the key to promoting scene Ô¨Çow, and visual
fusion can update the upper limit of scene Ô¨Çow performance.
The Role of homogeneous Space in Fusion. We illustrate
the impact of different homogeneous spaces on scene Ô¨Çow
in Fig. 7. When only implicit feature fusion ( i.e., feature
concatenate (FC)), more errors of the scene Ô¨Çow are caused
by feature mismatching. With visual luminance space (w/
FC+L), scene Ô¨Çow is improved due to the enhanced visual
RGB features. Visual structure space (w/ FC+L+S) further
promotes the integrity of the scene Ô¨Çow structure. With mo-
tion correlation space (w/ FC+L+S+C), the motion metric is
greater, and the scene Ô¨Çow visualization is spatiotemporally
smoother. Therefore, the homogeneous space can serve as a
guide to physically facilitate the fusion process.
Effectiveness of Fusion Losses. In Table 4, we conduct
ablation studies on the main fusion losses. Without any fu-
sion losses, the whole framework still has the visual-motions
fusion process. Each loss improves scene Ô¨Çow to a certain
extent, while the motion distribution alignment loss Lkl
corr
plays a key role in scene Ô¨Çow. The three losses can jointly
promote the visual!motion progressive fusion process.
26483
LconsisLpseLkl
corr EPE ACC
   0.122 53.17%p  0.112 56.41%
p 0.107 58.25%
 p0.092 65.43%p p p0.084 70.34%
Table 4. Ablation study on fusion losses.
LiDAR (a) Visual features distribution EventRGB 
Fuse
FuseFuse
(b) Motion features distributionRGB
Event
LiDAR 
kl=0.0414
kl=0.0107
Figure 8. t-SNE visualization of visual and motion features. In (a)
visual space, there exists a gap between RGB and LiDAR, while
event serves as a bridge across the two modalities. In (b) motion
space, the three modalities are further close to each other.
4.4. Discussion
How does Event Play the Bridge. In Fig. 8, we illustrate the
importance of event as a bridge between RGB and LiDAR
in visual and motion feature spaces via t-SNE. In the visual
space, there exists a large gap between RGB and LiDAR
features, while event features can just serve as a bridge
across the two modalities. When the fused visual features are
mapped to the motion space, the motion features of RGB and
LiDAR are close to each other and have a certain intersection,
which contains most of the event motion features. Therefore,
the event can reasonably bridge the RGB and LiDAR.
Effectiveness of Spatiotemporal Gradient Consistency.
We analyze the impact of spatiotemporal gradient consis-
tency on the Event-RGB fused image result and the optical
Ô¨Çow result from the training process in Fig. 9. Without the
consistency loss, the training curve Ô¨Çuctuates violently, the
fused RGB image looks clear while there are some artifacts
in the optical Ô¨Çow. With the consistency loss, the Ô¨Çuctuation
of the training curve is smoother, the fused RGB image is
also clear and the optical Ô¨Çow boundaries become sharp.
This shows that spatiotemporal gradient consistency does
constrain the mapping relationship between the Event-RGB
fused visual features and the motion features.
Why Structure Neighbor is Necessary? Structure neighbor
is to extract the region-level structure from the pixel-level
structure, which has the local self-similarity characteristics.
In Table 5, without any neighbor, the scene Ô¨Çow is slightly
limited. This is because the full pixel association between
event and LiDAR increases the probability of local-global
structure feature mismatching. With grid-based neighbor,
the scene Ô¨Çow performance is improved but is lower than
the clustering-based neighbor. The main reason is that, theStructure neighbor strategy EPE ACC
w/o any neighbor 0.095 64.70%
w/ grid-based neighbor 0.088 69.05%
w/ clustering-based neighbor 0.084 70.34%
Table 5. Choice of different structure neighbor strategies.
(a1) Nighttime image
(b1) Fused image
(c1) Fused image(b2) Optical flow
(c2) Optical flow(a2) Optical flow
100 200 0246810 12 
(d) Iteration process
300 400 500 600 700 800 900 1000w/o Gradient consis. 
w/ Gradient consis. 
EpochLoss 
Figure 9. Effect of spatiotemporal gradient consistency on the fused
image and optical Ô¨Çow. (a) Without fusion (b) Fusion without gradi-
ent consistency. (c) Fusion with gradient consistency. The gradient
consistency can smooth training curve, conducive to constraining
the mapping relationship between the fused image and optical Ô¨Çow.
grid-based neighbor may contain partial information of dif-
ferent objects, leading to the structure fusion bias, while the
clustering-based neighbor can reduce this error.
Limitation. The proposed method can indeed cope with the
all-day ( i.e., daytime and nighttime) scene Ô¨Çow estimation,
but usually fails for adverse weather, such as rain and fog.
Rain will lead to varying degrees of dynamic streak noise to
RGB, event and LiDAR, and fog will attenuate atmospheric
light, limiting the imaging of these sensors. In the future,
we will further introduce the near-infrared sensor into these
modalities to handle adverse weather scene Ô¨Çow.
5. Conclusion
In this work, we bring the event as a bridge between RGB
and LiDAR, and propose a novel hierarchical visual-motion
fusion framework to fuse the multimodal complementary
knowledge in the homogeneous space for scene Ô¨Çow. To
the best of our knowledge, we are the Ô¨Årst to investigate
the inter-modal homogeneous space to explicitly learn the
complementary knowledge for physical interpretation. We
construct the Event-RGB luminance space, Event-LiDAR
structure space and RGB-Event-LiDAR correlation space.
The three homogeneous spaces progressively improve scene
Ô¨Çow from visual space to motion space. We also conduct
extensive experiments on daytime and nighttime scenes, and
verify the superiority of the proposed method. We believe
that the proposed method could not only facilitate the devel-
opment of scene Ô¨Çow but also enlighten the researchers of
the broader Ô¨Åeld, e.g., adverse scene multimodal perception.
Acknowledgments. This work was supported by the Na-
tional Natural Science Foundation of China under Grant
62371203. The computation is completed in the HPC Plat-
form of Huazhong University of Science and Technology.
26484
References
[1]Fabian Duffhauss and Stefan A Baur. PillarÔ¨Çownet: A real-
time deep multitask network for lidar-based 3d object detec-
tion and scene Ô¨Çow estimation. In 2020 IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS) ,
pages 10734‚Äì10741. IEEE, 2020. 1
[2]Stefan Andreas Baur, David Josef Emmerichs, Frank Moos-
mann, Peter Pinggera, Bj√∂rn Ommer, and Andreas Geiger.
Slim: Self-supervised lidar scene Ô¨Çow and motion segmenta-
tion. In Int. Conf. Comput. Vis. , pages 13126‚Äì13136, 2021.
1
[3]Aseem Behl, Omid Hosseini Jafari, Siva Karthik Mustikovela,
Hassan Abu Alhaija, Carsten Rother, and Andreas Geiger.
Bounding boxes, segmentations and object coordinates: How
important is recognition for 3d scene Ô¨Çow estimation in au-
tonomous driving scenarios? In Int. Conf. Comput. Vis. , pages
2574‚Äì2583, 2017. 1, 2
[4]Wei-Chiu Ma, Shenlong Wang, Rui Hu, Yuwen Xiong, and
Raquel Urtasun. Deep rigid instance scene Ô¨Çow. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 3614‚Äì3622, 2019.
[5]Junhwa Hur and Stefan Roth. Self-supervised multi-frame
monocular scene Ô¨Çow. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 2684‚Äì2694, 2021. 2
[6]Zachary Teed and Jia Deng. Raft-3d: Scene Ô¨Çow using rigid-
motion embeddings. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 8375‚Äì8384, 2021. 2, 6
[7]Yi Wei, Ziyi Wang, Yongming Rao, Jiwen Lu, and Jie Zhou.
Pv-raft: Point-voxel correlation Ô¨Åelds for scene Ô¨Çow estima-
tion of point clouds. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 6954‚Äì6963, 2021. 1, 2, 6
[8]Rishav Rishav, Ramy Battrawy, Ren√© Schuster, Oliver Wasen-
m√ºller, and Didier Stricker. DeeplidarÔ¨Çow: A deep learn-
ing architecture for scene Ô¨Çow estimation using monocular
camera and sparse lidar. In 2020 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS) , pages
10460‚Äì10467. IEEE, 2020. 1, 2
[9]Haisong Liu, Tao Lu, Yihui Xu, Jia Liu, Wenjie Li, and Lijun
Chen. CamliÔ¨Çow: Bidirectional camera-lidar fusion for joint
optical Ô¨Çow and scene Ô¨Çow estimation. In IEEE Conf. Comput.
Vis. Pattern Recog. , pages 5791‚Äì5801, 2022. 1, 2, 6, 7
[10] Guillermo Gallego, Tobi Delbr√ºck, G. Orchard, Chiara Bar-
tolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, An-
drew J. Davison, J√∂rg Conradt, Kostas Daniilidis, and Davide
Scaramuzza. Event-based vision: A survey. IEEE Trans.
Pattern Anal. Mach. Intell. , 44:154‚Äì180, 2019. 2
[11] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,
Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-
tion of optical Ô¨Çow estimation with deep networks. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 2462‚Äì2470, 2017.
2
[12] Anurag Ranjan, Varun Jampani, Lukas Balles, Kihwan Kim,
Deqing Sun, Jonas Wulff, and Michael J Black. Competitive
collaboration: Joint unsupervised learning of depth, camera
motion, optical Ô¨Çow and motion segmentation. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 12240‚Äì12249, 2019. 2
[13] Xingyu Liu, Charles R Qi, and Leonidas J Guibas. Flownet3d:Learning scene Ô¨Çow in 3d point clouds. In IEEE Conf. Com-
put. Vis. Pattern Recog. , pages 529‚Äì537, 2019. 2
[14] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs Ô¨Åeld
transforms for optical Ô¨Çow. In Eur. Conf. Comput. Vis. , pages
402‚Äì419, 2020. 2, 5
[15] Shihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, and
Richard Hartley. Learning to estimate hidden motions with
global motion aggregation. In Int. Conf. Comput. Vis. , pages
9772‚Äì9781, 2021. 2, 5
[16] Fabian Brickwedde, Steffen Abraham, and Rudolf Mester.
Mono-sf: Multi-view geometry meets single-view depth for
monocular scene Ô¨Çow estimation of dynamic trafÔ¨Åc scenes.
InInt. Conf. Comput. Vis. , pages 2780‚Äì2790, 2019. 2
[17] Vitor Guizilini, Kuan-Hui Lee, Rare¬∏ s Ambru¬∏ s, and Adrien
Gaidon. Learning optical Ô¨Çow, depth, and scene Ô¨Çow without
real-world labels. IEEE Robotics and Automation Letters ,
7(2):3491‚Äì3498, 2022. 2
[18] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.
Pwc-net: Cnns for optical Ô¨Çow using pyramid, warping, and
cost volume. In IEEE Conf. Comput. Vis. Pattern Recog. ,
pages 8934‚Äì8943, 2018. 2, 5
[19] Yaqi Shen, Le Hui, Jin Xie, and Jian Yang. Self-supervised
3d scene Ô¨Çow estimation guided by superpoints. In IEEE
Conf. Comput. Vis. Pattern Recog. , pages 5271‚Äì5280, 2023.
2
[20] Itai Lang, Dror Aiger, Forrester Cole, Shai Avidan, and
Michael Rubinstein. Scoop: Self-supervised correspondence
and optimization-based scene Ô¨Çow. In IEEE Conf. Comput.
Vis. Pattern Recog. , pages 5281‚Äì5290, 2023. 2
[21] Fangqiang Ding, Andras Palffy, Dariu M Gavrila, and
Chris Xiaoxuan Lu. Hidden gems: 4d radar scene Ô¨Çow learn-
ing using cross-modal supervision. In IEEE Conf. Comput.
Vis. Pattern Recog. , pages 9340‚Äì9349, 2023. 2
[22] Zhexiong Wan, Yuxin Mao, Jing Zhang, and Yuchao Dai.
RpeÔ¨Çow: Multimodal fusion of rgb-pointcloud-event for joint
optical Ô¨Çow and scene Ô¨Çow estimation. In Int. Conf. Comput.
Vis., pages 10030‚Äì10040, 2023. 2, 6
[23] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang, Bret
Harsham, John R Hershey, Tim K Marks, and Kazuhiko Sumi.
Attention-based multimodal fusion for video description. In
Int. Conf. Comput. Vis. , pages 4193‚Äì4202, 2017. 2
[24] Xi Wei, Tianzhu Zhang, Yan Li, Yongdong Zhang, and Feng
Wu. Multi-modality cross attention network for image and
sentence matching. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 10941‚Äì10950, 2020.
[25] Lei Sun, Christos Sakaridis, Jingyun Liang, Qi Jiang, Kailun
Yang, Peng Sun, Yaozu Ye, Kaiwei Wang, and Luc Van Gool.
Event-based fusion for motion deblurring with cross-modal at-
tention. In Eur. Conf. Comput. Vis. , pages 412‚Äì428. Springer,
2022.
[26] Qi Cai, Yingwei Pan, Ting Yao, Chong-Wah Ngo, and Tao
Mei. Objectfusion: Multi-modal 3d object detection with
object-centric fusion. In Int. Conf. Comput. Vis. , pages 18067‚Äì
18076, 2023. 2
[27] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep-
resentation learning: A review and new perspectives. IEEE
Trans. Pattern Anal. Mach. Intell. , 35(8):1798‚Äì1828, 2013. 2
26485
[28] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto,
and Masashi Sugiyama. Learning discrete representations via
information maximizing self-augmented training. In Int. Conf.
on Machine Learning , pages 1558‚Äì1567. PMLR, 2017.
[29] Jing Zhang, Deng-Ping Fan, Yuchao Dai, Xin Yu, Yiran
Zhong, Nick Barnes, and Ling Shao. Rgb-d saliency de-
tection via cascaded mutual information minimization. In Int.
Conf. Comput. Vis. , pages 4338‚Äì4347, 2021.
[30] Yuhang Lu, Qi Jiang, Runnan Chen, Yuenan Hou, Xinge Zhu,
and Yuexin Ma. See more and know more: Zero-shot point
cloud segmentation via multi-modal visual data. In Int. Conf.
Comput. Vis. , pages 21674‚Äì21684, 2023. 2
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Adv. Neural Inform.
Process. Syst. , 30, 2017. 2, 5
[32] Jin Han, Yixin Yang, Peiqi Duan, Chu Zhou, Lei Ma, Chao
Xu, Tiejun Huang, Imari Sato, and Boxin Shi. Hybrid high dy-
namic range imaging fusing neuromorphic and conventional
images. 45(7):8553‚Äì8565, 2023. 4
[33] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
Unpaired image-to-image translation using cycle-consistent
adversarial networks. In Int. Conf. Comput. Vis. , pages 2223‚Äì
2232, 2017. 4
[34] Kyunghyun Cho, Bart Van Merri√´nboer, Caglar Gulcehre,
Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and
Yoshua Bengio. Learning phrase representations using rnn
encoder-decoder for statistical machine translation. arXiv
preprint arXiv:1406.1078 , 2014. 5
[35] Jason J Yu, Adam W Harley, and Konstantinos G Derpanis.
Back to basics: Unsupervised learning of optical Ô¨Çow via
brightness constancy and motion smoothness. In Eur. Conf.
Comput. Vis. , pages 3‚Äì10, 2016. 6
[36] Yuliang Zou, Zelun Luo, and Jia-Bin Huang. Df-net: Un-
supervised joint learning of depth and Ô¨Çow using cross-task
consistency. In Eur. Conf. Comput. Vis. , pages 1‚Äì18. Springer,
2018. 6
[37] Moritz Menze and Andreas Geiger. Object scene Ô¨Çow for
autonomous vehicles. In IEEE Conf. Comput. Vis. Pattern
Recog. , pages 3061‚Äì3070, 2015. 6, 7
[38] Yuhuang Hu, Shih-Chii Liu, and Tobi Delbruck. v2e: From
video frames to realistic dvs events. In IEEE Conf. Comput.
Vis. Pattern Recog. Worksh. , pages 1312‚Äì1321, 2021. 6
[39] Yinqiang Zheng, Mingfang Zhang, and Feng Lu. Optical Ô¨Çow
in the dark. In IEEE Conf. Comput. Vis. Pattern Recog. , pages
6749‚Äì6757, 2020. 6
[40] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide
Scaramuzza. DSEC: A stereo event camera dataset for driving
scenarios. IEEE Robotics and Automation Letters , 2021. 6
[41] Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova,
and Rico Jonschkowski. Smurf: Self-teaching multi-frame
unsupervised raft with full-image warping. In IEEE Conf.
Comput. Vis. Pattern Recog. , pages 3887‚Äì3896, 2021. 6
[42] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang,
Ka Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng
Li. Flowformer: A transformer architecture for optical Ô¨Çow.
InEur. Conf. Comput. Vis. , pages 668‚Äì685. Springer, 2022. 6[43] Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling
the darkness: A practical low-light image enhancer. In ACM
Int. Conf. Multimedia , pages 1632‚Äì1640, 2019. 6
26486
