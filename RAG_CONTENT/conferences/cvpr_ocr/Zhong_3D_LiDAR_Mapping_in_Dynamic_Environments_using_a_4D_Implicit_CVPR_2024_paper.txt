3D LiDAR Mapping in Dynamic Environments
Using a 4D Implicit Neural Representation
Xingguang Zhong1Yue Pan1Cyrill Stachniss1;2Jens Behley1
1Center for Robotics, University of Bonn,2Lamarr Institute for Machine Learning and ArtiÔ¨Åcial Intelligence
fzhong, yue.pan, cyrill.stachniss, jens.behley g@igg.uni-bonn.de
Abstract
Building accurate maps is a key building block to en-
able reliable localization, planning, and navigation of au-
tonomous vehicles. We propose a novel approach for build-
ing accurate maps of dynamic environments utilizing a se-
quence of LiDAR scans. To this end, we propose encoding
the 4D scene into a novel spatio-temporal implicit neural
map representation by Ô¨Åtting a time-dependent truncated
signed distance function to each point. Using our repre-
sentation, we extract the static map by Ô¨Åltering the dynamic
parts. Our neural representation is based on sparse feature
grids, a globally shared decoder, and time-dependent ba-
sis functions, which we jointly optimize in an unsupervised
fashion. To learn this representation from a sequence of Li-
DAR scans, we design a simple yet efÔ¨Åcient loss function
to supervise the map optimization in a piecewise way. We
evaluate our approach1on various scenes containing mov-
ing objects in terms of the reconstruction quality of static
maps and the segmentation of dynamic point clouds. The
experimental results demonstrate that our method is capa-
ble of removing the dynamic part of the input point clouds
while reconstructing accurate and complete 3D maps, out-
performing several state-of-the-art methods.
1. Introduction
Mapping using range sensors, like LiDAR or RGB-D cam-
eras, is a fundamental task in computer vision and robotics.
Often, we want to obtain accurate maps to support down-
stream tasks such as localization, planning, or navigation.
For achieving an accurate reconstruction of an outdoor en-
vironment, we have to account for dynamics caused by
moving objects, such as vehicles or pedestrians. Further-
more, dynamic object removal plays an important role in au-
tonomous driving and robotics applications for creating dig-
ital twins for realistic simulation and high-deÔ¨Ånition map-
ping, where a static map is augmented with semantic and
task-relevant information.
1Code: https://github.com/PRBonn/4dNDF
(a)
 (b)
(c)
 (d)
Figure 1. Given a sequence of point clouds, as shown in (a), we
optimize our 4D neural representation that can be queried at ar-
bitrary positions for a speciÔ¨Åc time. (b) Based on the estimated
time-dependent TSDF values, we can extract a mesh at a speciÔ¨Åc
point in time. Additionally, our 4D neural representation can be
also used for static mapping (c) and dynamic object removal (c).
Mapping and state estimation in dynamic environments
is a classical problem in robotics [5, 56, 57]. Approaches for
simultaneous localization and mapping (SLAM) can apply
different strategies to deal with dynamics. Common ways
are: (1) Ô¨Åltering dynamics from the input [1, 30, 47, 48, 51]
as a pre-processing step, which requires a semantic inter-
pretation of the scene; (2) modeling the occupancy in the
map representation [17, 34, 37, 49, 50, 64], where dynam-
ics can be implicitly removed by retrospectively removing
measurements in free space; (3) including it in the state es-
timation [4, 16, 55, 61, 67] to model which measurements
originated from the dynamic and static parts of the environ-
ment. Our proposed method falls into the last category and
allows us to model dynamics directly in the map represen-
tation leading to a spatio-temporal map representation.
Recently, implicit neural representations gained increas-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
15417
ing interest in computer vision for novel view synthesis [35,
36] and 3D shape reconstruction [33, 40]. Due to their
compactness and continuity, several approaches [65, 70, 73]
investigate the use of neural representations in large-scale
3D LiDAR mapping leading to accurate maps while sig-
niÔ¨Åcantly reducing memory consumption. However, these
approaches often do not address the problem of handling
dynamics during mapping. The recent progress on dynamic
NeRF [7, 13, 44, 52] and neural deformable object recon-
struction [6, 10] indicates that neural representations can be
also used to represent dynamic scenes, which inspires us
to tackle the problem of mapping in dynamic environments
from the perspective of 4D reconstruction.
In this paper, we propose a novel method to reconstruct
large 4D dynamic scenes by encoding every point‚Äôs time-
dependent truncated signed distance function (TSDF) into
an implicit neural scene representation. As illustrated in
Fig. 1, we take sequentially recorded LiDAR point clouds
collected in dynamic environments as input and generate a
TSDF for each time frame, which can be used to extract a
mesh using marching cubes [29]. The background TSDF,
which is unchanged during the whole sequence, can be ex-
tracted from the 4D signal easily. We regard it as a static
map that can be used to segment dynamic objects from the
original point cloud. Compared to the traditional voxel-
based mapping method, the continuous neural representa-
tion allows for the removal of dynamic objects while pre-
serving rich map details. In summary, the main contribu-
tions of this paper are:
‚Ä¢ We propose a novel implicit neural representation to
jointly reconstruct a dynamic 3D environment and main-
tain a static map using sequential LiDAR scans as input.
‚Ä¢ We employ a piecewise training data sampling strat-
egy and design a simple, yet effective loss function that
maintains the consistency of the static point supervision
through gradient constraints.
‚Ä¢ We evaluate the mapping results by the accuracy of the
dynamic object segmentation as well as the quality of the
reconstructed static map showing superior performance
compared to several baselines. We provide our code and
the data used for experiments.
2. Related Work
Mapping and SLAM in dynamic environments is a classi-
cal topic in robotics [5, 56, 57] with a large body of work,
which tackles the problem by pre-processing the sensor
data [1, 30, 47, 48, 51], occupancy estimation to Ô¨Ålter dy-
namics by removing measurements in free space [17, 34,
37, 39, 49, 50, 64], or state estimation techniques [4, 16,
55, 61, 67]. Below, we focus on closely related approaches
using neural representations but also static map building ap-
proaches for scenes containing dynamics.
Dynamic NeRF. Dynamic NeRFs aim to solve the prob-lem of novel view synthesis in dynamic environments.
Some approaches [41‚Äì43, 58, 63] address this challenge by
modeling the deformation of each point with respect to a
canonical frame. However, these methods cannot represent
newly appearing objects. This can render them unsuited for
complicated real-life scenarios. In contrast, NSFF [24] and
DynIBaR [26] get rid of the canonical frame by computing
the motion Ô¨Åeld of the whole scene. While these methods
can deliver satisfactory results, the training time is usually
in the order of hours or even days.
Another type of method leverages the compactness of
the neural representation to model the 4D spatio-temporal
information directly. Several works [7, 13, 52] project the
4D input into multiple voxelized lower-dimensional fea-
ture spaces to avoid large memory consumption, which im-
proves the efÔ¨Åciency of the optimization. Song et al. [54]
propose a time-dependent sliding window strategy for accu-
mulating the voxel features. Instead of only targeting novel
view synthesis, several approaches [26, 68, 71] decompose
the scene into dynamic objects and static background in
a self-supervised way, which inspired our work. Other
approaches [22, 23, 53] accomplish neural representation-
based reconstruction for larger scenes by adding additional
supervision such as object masks or optical Ô¨Çow.
Neural representations for LiDAR scans. Recently,
many approaches aim to enhance scene reconstruction us-
ing LiDAR data through neural representations. The early
work URF [46] leverages LiDAR data as depth supervision
to improve the optimization of a neural radiance Ô¨Åeld. With
only LiDAR data as input, Huang et al. [20] achieve novel
view synthesis for LiDAR scans with differentiable render-
ing. Similar to our work, Shine-mapping [73] and EIN-
RUL [70] utilize sparse hierarchical feature voxel structures
to achieve large-scale 3D mapping. Additionally, the data-
driven approach NKSR [18] based on learned kernel re-
gression demonstrates accurate surface reconstruction with
noisy LiDAR point cloud as input. Although these ap-
proaches perform well in improving reconstruction accu-
racy and reducing memory consumption, none of them con-
sider the problem of dynamic object interference in real-
world environments.
Static map building and motion detection. In addi-
tion to removing moving objects from the voxel map with
ray tracing, numerous works [8, 19, 31, 32] try to segment
dynamic points from raw LiDAR point clouds. However,
these methods require a signiÔ¨Åcant amount of labeled data,
which makes it challenging to generalize them to various
scenarios or sensors with different scan patterns. In con-
trast, geometry-based, more heuristic approaches have also
produced promising results. Kim et al. [21] solve this prob-
lem using the visibility of range images, but their results
are still highly affected by the resolution. Lim et al. pro-
posed Erasor [27], which leverages ground Ô¨Åtting as prior
15418
to achieve better segmentation for dynamic points. More
recent approaches [9, 28] extend it to instance level to im-
prove results. However, these methods rely on an accurate
ground Ô¨Åtting method, which is mainly designed for au-
tonomous driving scenarios, which cannot be guaranteed in
complex unstructured real environments.
In contrast to the approaches discussed above, we fol-
low recent developments in neural reconstruction and pro-
pose a novel scene representation that allows us to capture
the spatio-temporal progression of a scene. We represent
the time-varying SDF of a scene in an unsupervised fash-
ion, which we exploit to remove dynamic objects and re-
construct accurate meshes of the static scene.
3. Our Approach
The input of our approach is given by a sequence of point
clouds,S1:N= (S1;:::;SN), and their corresponding
global poses Tt2R44,t2[1;N], estimated via scan
matching, LiDAR odometry, or SLAM methods [2, 11, 12,
60]. Each scan‚Äôs point cloud St=fs1
t;:::;sMt
tgis a set of
points,si
t2R3, collected at time t. Given such a sequence
of scansS1:N, our approach aims to reconstruct a 4D TSDF
of the traversed scene and maintain a static 3D map at the
same time.
In the next sections, we Ô¨Årst introduce our spatio-
temporal representation and then explain how to optimize
it to represent the dynamic and static parts of a point cloud
sequenceS1:N.
3.1. Map Representation
The key component of our approach is an implicit neural
scene representation that allows us to represent a 4D TSDF
of the scene, as well as facilitates the extraction of a static
map representation. Our proposed spatio-temporal scene
representation is optimized for the given point cloud se-
quenceS1:Nsuch that we can retrieve for an arbitrary point
p2R3and timet2[1;N]the corresponding time-varying
signed distances value at that location.
Temporal representation. We utilize an TSDF to rep-
resent the scene, i.e., a function that provides the signed
distance to the nearest surface for any given point p2R3.
The sign of the distance is positive when the point is in free
space or in front of the measured surface and is negative
when the point is inside the occupied space or behind the
measured surface.
In a dynamic 3D scene, measuring the signed distance of
any coordinate at each moment produces a time-dependent
function that captures the signed distance changes over
time, see Fig. 2 for an illustration. Additionally, if a co-
ordinate is static throughout the period, the signed dis-
tance should remain constant. The key idea of our spatio-
temporal scene representation is to Ô¨Åt the time-varying SDF
at each point with several basis functions. Inspired by Li
Figure 2. Principle of our 4D TSDF representation: The left Ô¨Ågure
shows a moving object and a query point p. The one on the right
depicts the corresponding signed distance at pover time. At t0,
p‚Äôs signed distance is a positive truncated value. When the moving
object reaches pat timet1,pis inside the object and its signed
distance is negative accordingly. At t2, the moving object moved
pastp, the signed distance of pgets positive again.
et al. [26]‚Äôs representation of moving point trajectories, we
exploitKglobally shared basis functions k:R7!R. Us-
ing these basis functions k(t), we model the time-varying
TSDFF(p;t)that maps a location p2R3at timetto a
signed distance as follows:
F(p;t) =KX
k=1wk
pk(t); (1)
wherewkp2Rare optimizable location-dependent coefÔ¨Å-
cients. In line with previous works [26, 62], we initialize the
basis functions with discrete cosine transform (DCT) basis
functions:
k(t) = cos
2N(2t+ 1)(k 1)
: (2)
The Ô¨Årst basis function for k= 1 is time-independent
as1(t) = 1 . During the training process, we Ô¨Åx 1(t)and
determine the other basis functions by backpropagation. We
consider1(t)‚Äôs corresponding weight w1
pas the static SDF
value of the point p. Hence,F(p;t)consists of its static
background value, i.e.,w1p1(t) =w1p, and the weighted
sum of dynamic basis functions 2(t);:::; K(t).
As the basis functions 1(t);:::; K(t)are shared be-
tween all points in the scene, we need to optimize the
location-dependent weights that are implicitly represented
in our spatial representation.
Spatial representation. To achieve accurate scene re-
construction while maintaining memory efÔ¨Åciency, we em-
ploy a multi-resolution sparse voxel grid to store spatial ge-
ometric information.
First, we accumulate the input point clouds, S1;:::;SN
based on their poses T1;:::;TNcomputed from LiDAR
odometry and generate a hierarchy of voxel grids around
points to ensure complete coverage in 3D. We use a spatial
hash table for fast retrieval of the resulting voxels that are
only initialized if points fall into a voxel.
15419
Figure 3. Overview of querying a TSDF value in our 4D map representation. For querying a point pattiandti+1, we Ô¨Årst retrieve each
corner‚Äôs feature inFlof the voxel that pis located in and obtain the fused feature fpby trilinear interpolation. Then, we feed fpinto the
decoderDmlpand take the output as the weights of different basis functions 1(t);:::; K(t). Finally, we calculate the weighted sum of
basis functions‚Äô values at tiandti+1to get their respective SDF results. For simplicity, we only illustrate one level of hashed feature grids.
Similar to Instant-NGP [36], we save a feature vector
f2RDat each corner vertex of the voxel grid in each res-
olution level, where we denote as Flthe level-wise corner
features. We compute the feature vector fp2RDfor given
query pointp2R3inside the hierarchical grid as follows:
fp=LX
l=1interpolate (p;Fl); (3)
where interpolate is the trilinear interpolation for a given
pointpusing the corner features Flat levell.
Then, we decode the interpolated feature vector fpinto
the desired weights wp= 
w1
p;:::;wK
p
2RKby a glob-
ally shared multi-layer perceptron (MLP) Dmlp:
wp=Dmlp(fp): (4)
As every step is differentiable, we can optimize the
multi-resolution feature grids Fl, the MLP decoder Dmlp,
and the values of the basis functions jointly by gradient de-
scent once we have training data and corresponding target
values. The SDF querying process is illustrated in Fig. 3.
3.2. Objective Function
We take samples along the rays from the input scans St
to collect training data. Each scan frame Stcorresponds
to a moment tin time, so we gather four-dimensional data
points (q;t)via sampling along the ray from the scan origin
ot2R3to a pointsi
t2St. We can represent the sampled
pointsqi
salong the ray as qi
s=ot+(si
t ot). By setting
a truncation threshold , we split the ray into two regions,
at the surface and in the free-space:
Ti
surf=
qi
sj2(1 ;1 + )	
(5)
Ti
free=
qi
sj2(0;1 )	
; (6)
where =(ksi
t otk) 1. Thus,Ti
surfrepresents the re-
gion close to the endpoint si
t2St, andTi
freeis the regionin the free space. We uniformly sample MsandMfpoints
fromTi
surfandTi
freeseparately. We obtain two sets Dsurfand
Dfreeof samples by sampling over all scans. Unlike prior
work [20, 46] that use differentiable rendering to calculate
the depth by integration along the ray, we design different
losses forDsurfandDfreeto supervise the 4D TSDF directly.
Near Surface Loss. Since the output of our 4D map is
the signed distance value ^d=F(p;t)at an arbitrary posi-
tionp2R3in timet2[1;N], we expect that the predicted
value ^ddoes not change over time for static points. How-
ever, this cannot be guaranteed if we use the projective dis-
tancedsurfto the surface along the ray direction directly as
the target value, since the projective distance would change
over time due to the change of view direction by the moving
sensor, even in a static scene. Thus, for the sampled data
inDsurf,i.e., the sampled points near the surface, we can
only obtain reliable information about the sign of the TSDF
value of these points, which should be positive if the point
is before the endpoint and negative if the point is behind.
In addition, for a sampled point in front of the endpoint, its
projective signed distance dsurfshould be the upper bound
of its actual signed distance value. And for sampled points
behind the endpoint, dsurfshould be the lower bound.
We design a piecewise loss Lsurfto supervise the sampled
points near the surface:
Lsurf(^d;d surf) =8
<
:j^dj if^ddsurf<0
j^d dsurfjif^ddsurf>d2
surf
0 otherwise;(7)
where ^d=F(q;t)is the predicted value from our map
for a sample point q2D surfanddsurfis its corresponding
projective signed distance for that sampled point in the cor-
responding scanSt. This loss punishes only a prediction
when the sign is wrong or its absolute value is larger than
the absolute value of dsurf. For a query point exactly on the
surface, i.e.,dsurf= 0,Lsurfis simply the L1 loss.
15420
To calculate an accurate signed distance value and main-
tain the consistency of constraints for static points from dif-
ferent observations, we use the natural property of signed
distance function to constraint the length of the gradient
vector for samples inside Dsurf, which is called Eikonal reg-
ularization [15, 38]:
Leikonal (p;t) =@F(p;t)
@p 12
; (8)
Inspired by Neuralangelo [25], we manually add pertur-
bations to compute more robust gradient vectors instead of
using automatic differentiation, which means we compute
numerical gradients:
rxF(p;t) =F(p+x;t) F(p x;t)
2; (9)
whererxF(p;t)is the component of the gradient@F(p;t)
@p
on thexaxis, andx= (;0;0)>is the added perturbation.
We apply the same operation on yandzaxes to calculate
the numerical gradient. Furthermore, in order to get faster
convergence at the beginning and ultimately recover the rich
geometric details, we Ô¨Årst set a large and gradually reduce
it during the training process.
Free Space Loss. As we tackle the problem of map-
ping in dynamic environments, we cannot simply accumu-
late point clouds and then calculate accurate supervision of
signed distance value via nearest neighbor search. There-
fore, we use a L1 loss Lfreeto constrain the signed distance
prediction ^dof the free space points, i.e.,p2D free:
Lfree(^d) =j^d j; (10)
whereis the truncation threshold we used in Sec. 3.2.
Thanks to our spatio-temporal representation, a single
query point can get both, static and dynamic TSDF values.
Thus, for some regions that are determined to be free space,
we can directly add constraints to their static TSDF values.
We divide the free space points Dfreeinto dense and
sparse subsetDdense andDsparse based on a threshold rdense
for the distance from the free space point sampled at time
tto the scan origin ot. For each point p2D dense, we Ô¨Ånd
the nearest neighbor npin the corresponding scan St,i.e.,
np= arg minq2Stkp qk2. LetDcertain =fp2D densej
jjp npjj> gbe the points that we consider in the cer-
tain free space. Then, we supervise p2D certain by its static
signed distance value directly:
Lcertain(p) =jw1p j; (11)
wherew1pis the Ô¨Årst weight of the decoder‚Äôs output.
(a)
 (b)
(c)
 (d)
Figure 4. Reconstructed TSDF for KITTI dataset [14]: SubÔ¨Ågures
(a) and (b) are the input neighboring frames. Correspondingly, (c)
and (d) are horizontal TSDF slices queried from our 4D map. Note
that we only display the TSDF values that are less than 0.3m.
In summary, the Ô¨Ånal loss Ltotalis given by:
Ltotal=1
jDsurfjX
(p;t)2D surfLsurf(^d;d surf) +eLeikonal (p;t)
+f
jDfreejX
(p;t)2D freeLfree(^d)
+c
jDcertainjX
(p;t)2D certainLcertain(p); (12)
where ^d=F(p;t)is the predicted signed distance at the
sample position pat timetanddsurfis the projective signed
distance of sample p. With the above loss function and data
sampling strategy, we train our map ofÔ¨Çine until conver-
gence. In Fig. 4, we show TSDF slices obtained using our
optimized 4D map at different times.
One application of our 4D map representation is dy-
namic object segmentation. For a point pin the input scans
S1:N, its static signed distance value w1pcan be obtained
by a simple query. If pbelongs to the static background, it
should have w1p= 0. Therefore, we simply set a threshold
dstatic and regard a point as dynamic if w1p>d static.
3.3. Implementation Details
As hyperparameters of our approach, we use the values
listed in Tab. 1 in all LiDAR experiments. Additional pa-
rameters are determined by the characteristics of the sensor
and the dimensions of the scene. For instance, in the recon-
struction of autonomous driving scenes, like KITTI, we set
the highest resolution for the feature voxels to 0:3m. The
truncation distance is set to = 0:5m, and the dense area
split threshold rdense = 15 m. Regarding training time, it
takes 12minutes to train 140frames from the KITTI dataset
using a single Nvidia Quadro RTX 5000.
15421
4. Experiments
In this section, we show the effectiveness of our proposed
approach with respect to two aspects: (1) Static mapping
quality: The static TSDF built by our method allows us
to extract a surface mesh using marching cubes [29]. We
compare this extracted mesh with the ground truth mesh to
evaluate the reconstruction. (2) Dynamic object segmenta-
tion: As mentioned above, our method can segment out the
dynamic objects in the input scans. We use point-wise dy-
namic object segmentation accuracy to evaluate the results.
4.1. Static Mapping Quality
Datasets. We select two datasets collected in dynamic en-
vironments for quantitative evaluation. One is the synthetic
dataset ToyCar3 from Co-Fusion [47], which provides ac-
curate depth images and accurate masks of dynamic objects
rendered using Blender, but also depth images with added
noise. For this experiment, we select 150 frames from the
whole sequence, mask out all dynamic objects in the accu-
rate depth images, and accumulate background static points
as the ground-truth static map. The original noisy depth im-
ages are used as the input for all methods.
Furthermore, we use the Newer College [45] dataset as
the real-world dataset, which is collected using a 64-beam
LiDAR. Compared with synthetic datasets, it contains more
uncertainty from measurements and pose estimates. We se-
lect 1,300 frames from the courtyard part for testing and this
data includes a few pedestrians as dynamic objects. This
dataset offers point clouds obtained by a high-precision ter-
restrial laser scanner that can be directly utilized as ground
truth to evaluate the mapping quality.
Metric and Baselines. We report the reconstruction ac-
curacy, completeness, the Chamfer distance, and the F1-
score. Further details on the computation of the metrics can
be found in the supplement.
We compare our method with several different types of
state-of-the-art methods: (i) the traditional TSDF-fusion
method, VDBfusion [59], which uses space carving to
eliminate the effects of dynamic objects, (ii) the data-
driven-based method, neural kernel surface reconstruc-
tion (NKSR) [18], and (iii) the neural representation based
3D mapping approach, SHINE-mapping [73].
For NKSR [18], we use the default parameters provided
by Huang et al. with their ofÔ¨Åcial implementation. To en-
sure a fair comparison with SHINE-mapping, we adopt an
equal number of free space samples (15 samples), aligning
with our method for consistency.
For the ToyCar3 dataset, we set VDB-Fusion‚Äôs resolu-
tion to 1cm. To have all methods with a similar memory
consumption, we set the resolution of SHINE-mapping‚Äôs
leaf feature voxel to 2cm, and our method‚Äôs highest resolu-
tion accordingly to 2cm. For the Newer College dataset, we
set the resolution to 10cm,30cm, and 30cm respectively.Table 1. Hyperparameters of our approach.
Parameter Value Description
L 2 number of feature voxels level
D 8 The length of feature vectors
K 32 The number of basis functions
Dmlp 264 layer and size of the MLP decoder
Ms 5 The number of surface area samples
Mf 15 The number of free space samples
e 0.02 weight for Eikonal loss
f 0.25 weight for free space loss
c 0.2 weight for certain free loss
Table 2. Quantitative results of the reconstruction quality on Toy-
Car3 . We report the distance error metrics, namely completion,
accuracy and Chamfer-L1 in cm. Additionally, we show the F-
score in %with a 1cm error threshold.
Method Comp. #Acc.#C-L1#F-score"
VDB-fusion [59] 0.574 0.481 0.528 97.95
NKSR [18] 0.526 2.809 1.667 89.54
SHINE-mapping [73] 0.583 0.626 0.605 98.01
Ours 0.438 0.468 0.452 98.35
Results. The quantitative results for synthetic dataset
ToyCar3 and real-world dataset Newer College are pre-
sented in Tab. 2 and Tab. 3, respectively. We also show
the extracted meshes from all methods in Fig. 5 and Fig. 6.
Our method outperforms the baselines in terms of Com-
pleteness and Chamfer distance for both datasets ( cf. Fig. 5
and Fig. 6). Regarding the accuracy, SHINE-mapping and
VDB-Fusion can Ô¨Ålter part of high-frequency noise by fu-
sion of multiple frames, resulting in better performance on
noisy Newer College dataset. In comparison, our method
considers every scan as accurate to store 4D information,
which makes it more sensitive to measurement noise. On
theToyCar3 dataset, both our method and VDB-Fusion suc-
cessfully eliminate all moving objects. However, on the
Newer College dataset, VDB-Fusion incorrectly eliminates
the static tree and parts of the ground, resulting in poor com-
pleteness shown in Tab. 3. SHINE-mapping eliminates dy-
namic pedestrians on the Newer College dataset but retains a
portion of the dynamic point cloud on the ToyCar3 dataset,
which has a larger proportion of dynamic objects, leading
to poorer accuracy in Tab. 2. NKSR performs the worst ac-
curacy because it is unable to eliminate dynamic objects,
which means it‚Äôs not suitable to apply NKSR in dynamic
real-world scenes directly.
4.2. Dynamic Object Segmentation
Datasets. For dynamic object segmentation, we use the
KTH-Dynamic-Benchmark [72] for evaluation, which in-
15422
(a) Merged input scans
 (b) Ours
 (c) VDB-Fusion [59]
 (d) NKSR [18]
 (e) SHINE-mapping [73]
Figure 5. A comparison of the static mapping results of different methods on the ToyCar3 dataset. There are two dynamic toy cars moving
through the scene. Our method can reconstruct the static scene with Ô¨Åne details and eliminate the dynamic car.
(a) Merged input scans
 (b) Ours
 (c) VDB-Fusion [59]
 (d) NKSR [18]
 (e) SHINE-mapping [73]
Figure 6. A comparison of the static mapping results of different methods on the Newer College dataset. Several pedestrians are moving
through the scene during the data collection. Our method can reconstruct the static scene completely and eliminate the moving pedestrians.
Although VDB-Fusion manages to eliminate the pedestrians, it incorrectly removes the tree highlighted in the orange box.
Table 3. Quantitative results of the reconstruction quality on
Newer College . We report the distance error metrics, namely com-
pletion, accuracy and Chamfer-L1 in cm. Additionally, we show
the F-score in %with a 20 cm error threshold.
Method Comp. #Acc.#C-L1#F-score"
VDB-fusion [59] 7.32 5.99 6.65 96.68
NKSR [18] 6.87 9.28 8.08 95.65
SHINE-mapping [73] 6.80 5.86 6.33 97.67
Ours 5.85 6.49 6.17 97.50
cludes four sequences in total: sequence 00 (frame 4,390
‚Äì 4,530 ) and sequence 05 (frame 2,350 ‚Äì 2,670) from the
KITTI dataset [3, 14], which are captured by a 64-beam Li-
DAR, one sequence from the Argoverse2 dataset [66] con-
sisting of 575 frames captured by two 32-beam LiDARs,
and a semi-indoor sequence captured by a sparser 16-beam
LiDAR. All sequences come with corresponding pose Ô¨Åles
and point-wise dynamic or static labels as the ground truth.
It is worth noting that the poses for KITTI 00 and 05 were
obtained from SuMa [2] and the pose Ô¨Åles for the Semi-
indoor sequence come from NDT-SLAM [50].
Metric and Baselines. The KTH-Dynamic-Benchmark
evaluates the performance of the method by measuring the
classiÔ¨Åcation accuracy of dynamic points (DA%), static
points (SA%) and also their associated accuracy (AA%)
whereAA=p
DASA. The benchmark provides variousbaselines such as the state-of-the-art LiDAR dynamic object
removal methods ‚Äì Erasor [27] and Removert [21], as well
as the traditional 3D mapping method, Octomap [17, 69],
and its modiÔ¨Åed versions, Octomap with ground Ô¨Åtting and
outlier Ô¨Åltering. As SHINE-mapping demonstrates the abil-
ity to remove dynamic objects in our static mapping exper-
iments, we also report its result in this benchmark. Addi-
tionally, we report the performance of the state-of-the-art
online moving object segmentation methods, 4DMOS [31]
and its extension MapMOS [32]. As these two methods uti-
lize KITTI sequences 00 and 05 for training, we only show
the results of the remaining two sequences. For the parame-
ter setting, we set our method‚Äôs leaf resolution to 0:3m, and
the threshold for segmentation as dstatic= 0:16m. We set
the leaf resolution for Octomap to 0:1m.
Results. The quantitative results of the dynamic object
segmentation are shown in Tab. 4. And we depict the ac-
cumulated static points generated by different methods in
Fig. 7. We can see that our method achieves the best associ-
ated accuracy (AA) in three autonomous driving sequences
(KITTI 00, KITTI 05, Argoverse2) and vastly outperforms
baselines. The supervised learning-based methods 4DMOS
and MapMOS do not obtain good dynamic accuracy (DA)
due to limited generalizability. Erasor and Octomap tend
to over-segment dynamic objects, resulting in poor static
accuracy (SA). Removert and SHINE-mapping are too con-
servative and cannot detect all dynamic objects. BeneÔ¨Åting
15423
Table 4. Quantitative results of the dynamic object removal quality on the KTH-Dynamic-Benchmark. We report the static accuracy SA,
dynamic static DA and the associated accuracy AA. Octomap* refers to the modiÔ¨Åed Octomap implementation by Zhang et al. [72].
KITTI Seq. 00 KITTI Seq. 05 Argoverse2 Semi-Indoor
Method SA DA AA SA DA AA SA DA AA SA DA AA
Octomap [17] 68.05 99.69 82.37 66.28 99.24 81.10 65.91 96.70 79.84 88.97 82.18 85.51
Octomap* [72] 93.06 98.67 95.83 93.54 92.48 93.01 82.66 82.44 82.55 96.79 73.50 84.34
Removert [21] 99.44 41.53 64.26 99.42 22.28 47.06 98.97 31.16 55.53 99.96 12.15 34.85
Erasor [27] 66.70 98.54 81.07 69.40 99.06 82.92 77.51 99.18 87.68 94.90 66.26 79.30
SHINE [73] 98.99 92.37 95.63 98.91 53.27 72.58 97.66 72.62 84.21 98.88 59.19 76.51
4DMOS [31] - - - - - - 99.94 69.33 83.24 99.99 10.60 32.55
MapMOS [32] - - - - - - 99.96 85.88 92.65 99.99 4.75 21.80
Ours 99.46 98.47 98.97 99.54 98.36 98.95 99.17 95.91 97.53 94.17 72.79 82.79
(a) Ground truth
 (b) Ours
 (c) Erasor [27]
 (d) Removert [21]
 (e) Octomap* [72]
(f) Ground truth
 (g) Ours
 (h) Erasor [27]
 (i) Removert [21]
 (j) Octomap* [72]
Figure 7. Comparison of dynamic object removal results produced by our proposed method and three baseline methods on the Argoverse2
data sequence of the KTH-benchmark. We show the bird‚Äôs eye view on the Ô¨Årst row and the zoomed view from the blue frustum shown in
(a) on the second row. For the ground truth results in (a), the dynamic objects are shown in red. We only show the static points of ground
truth for clearer comparison in zoomed view (f). We highlight the over-segmented parking car and sign by Erasor and the undetected
moving vehicle by Removert.
from the continuity and large capacity of the 4D neural rep-
resentation, we strike a better balance between preserving
static background points and removing dynamic objects.
It is worth mentioning again that our method does not
rely on any pre-processing or post-processing algorithm
such as ground Ô¨Åtting, outlier Ô¨Åltering, and clustering, but
also does not require labels for training.
5. Conclusion
In this paper, we propose a 4D implicit neural map repre-
sentation for dynamic scenes that allows us to represent the
TSDF of static and dynamic parts of a scene. For this pur-
pose, we use a hierarchical voxel-based feature representa-
tion that is then decoded into weights for basis functions to
represent a time-varying TSDF that can be queried at arbi-
trary locations. For learning the representation from a se-quence of LiDAR scans, we design an effective data sam-
pling strategy and loss functions. Equipped with our pro-
posed representation, we experimentally show that we are
able to tackle the challenging problems of static mapping
and dynamic object segmentation. More speciÔ¨Åcally, our
experiments show that our method has the ability to accu-
rately reconstruct 3D maps of the static parts of a scene and
can completely remove moving objects at the same time.
Limitations. While our method achieves compelling re-
sults, we have to acknowledge that we currently rely on esti-
mated poses by a separate SLAM approach, but also cannot
apply our approach in an online fashion. However, we see
this as an avenue for future research into joint incremental
mapping and pose estimation.
Acknowledgements. We thank Benedikt Mersch for the
fruitful discussion and for providing experiment baselines.
15424
References
[1] Ioan A. Barsan, Peidong Liu, Marc Pollefeys, and Andreas
Geiger. Robust Dense Mapping for Large-Scale Dynamic
Environments. In Proc. of the IEEE Intl. Conf. on Robotics
& Automation (ICRA) , 2018. 1, 2
[2] Jens Behley and Cyrill Stachniss. EfÔ¨Åcient Surfel-Based
SLAM using 3D Laser Range Data in Urban Environments.
InProc. of Robotics: Science and Systems (RSS) , 2018. 3, 7
[3] Jens Behley, Martin Garbade, Aandres Milioto, Jan Quen-
zel, Sven Behnke, Cyrill Stachniss, and Juergen Gall. Se-
manticKITTI: A Dataset for Semantic Scene Understand-
ing of LiDAR Sequences. In Proc. of the IEEE/CVF
Intl. Conf. on Computer Vision (ICCV) , 2019. 7
[4] Peter Biber and Tom Duckett. Dynamic Maps for Long-Term
Operation of Mobile Service Robots. In Proc. of Robotics:
Science and Systems (RSS) , 2005. 1, 2
[5] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif,
Davide Scaramuzza, Jose Neira, Ian Reid, and John J.
Leonard. Past, Present, and Future of Simultaneous Local-
ization And Mapping: Towards the Robust-Perception Age.
IEEE Trans. on Robotics (TRO) , 32(6):1309‚Äì1332, 2016. 1,
2
[6] Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang, and
Juyong Zhang. Neural surface reconstruction of dynamic
scenes with monocular rgb-d camera. In Proc. of the Conf.
on Neural Information Processing Systems (NeurIPS) , 2022.
2
[7] Ang Cao and Justin Johnson. HexPlane: A Fast Representa-
tion for Dynamic Scenes. In Proc. of the IEEE/CVF Conf. on
Computer Vision and Pattern Recognition (CVPR) , 2023. 2
[8] Xieyuanli Chen, Shijie Li, Benedikt Mersch, Louis Wies-
mann, Juergen Gall, Jens Behley, and Cyrill Stachniss. Mov-
ing Object Segmentation in 3D LiDAR Data: A Learning-
based Approach Exploiting Sequential Data. IEEE Robotics
and Automation Letters (RA-L) , 6(4):6529‚Äì6536, 2021. 2
[9] Xieyuanli Chen, Benedikt Mersch, Lucas Nunes, Rodrigo
Marcuzzi, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss.
Automatic Labeling to Generate Training Data for Online
LiDAR-Based Moving Object Segmentation. IEEE Robotics
and Automation Letters (RA-L) , 7(3):6107‚Äì6114, 2022. 3
[10] Xu Chen, Tianjian Jiang, Jie Song, Max Rietmann, Andreas
Geiger, Michael J. Black, and Otmar Hilliges. Fast-snarf: A
fast deformer for articulated neural Ô¨Åelds. IEEE Trans. on
Pattern Analysis and Machine Intelligence (TPAMI) , 45(10):
11796‚Äì11809, 2023. 2
[11] Pierre Dellenbach, Jean-Emmanuel Deschaud, Bastien
Jacquet, and Francois Goulette. CT-ICP Real-Time Elastic
LiDAR Odometry with Loop Closure. In Proc. of the IEEE
Intl. Conf. on Robotics & Automation (ICRA) , 2022. 3
[12] Jean-Emmanuel Deschaud. IMLS-SLAM: scan-to-model
matching based on 3D data. In Proc. of the IEEE
Intl. Conf. on Robotics & Automation (ICRA) , 2018. 3
[13] Sara Fridovich-Keil, Giacomo Meanti, Frederik R. Warburg,
Benjamin Recht, and Angjoo Kanazawa. K-Planes: Explicit
Radiance Fields in Space, Time, and Appearance. In Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recog-
nition (CVPR) , 2023. 2[14] Andreas Geiger, Peter Lenz, and Raquel Urtasun. Are we
ready for Autonomous Driving? The KITTI Vision Bench-
mark Suite. In Proc. of the IEEE Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2012. 5, 7
[15] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit Geometric Regularization for Learn-
ing Shapes. In Proc. of the Intl. Conf. on Machine Learning
(ICML) , 2020. 5
[16] Dirk H ¬®ahnel, Dirk Schulz, and Wolfram Burgard. Mo-
bile robot mapping in populated environments. In Proc. of
the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems
(IROS) , 2002. 1, 2
[17] Armin Hornung, Kai M. Wurm, Maren Bennewitz, Cyrill
Stachniss, and Wolfram Burgard. OctoMap: An EfÔ¨Åcient
Probabilistic 3D Mapping Framework Based on Octrees. Au-
tonomous Robots , 34(3):189‚Äì206, 2013. 1, 2, 7, 8
[18] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja
Fidler, and Francis Williams. Neural Kernel Surface Recon-
struction. In Proc. of the IEEE/CVF Conf. on Computer Vi-
sion and Pattern Recognition (CVPR) , 2023. 2, 6, 7
[19] Shengyu Huang, Zan Gojcic, Jiahui Huang, Andreas Wieser,
and Konrad Schindler. Dynamic 3D Scene Analysis by Point
Cloud Accumulation. In Proc. of the Europ. Conf. on Com-
puter Vision (ECCV) , 2022. 2
[20] Shengyu Huang, Zan Gojcic, Zian Wang, Francis Williams,
Yoni Kasten, Sanja Fidler, Konrad Schindler, and Or Litany.
Neural LiDAR Fields for Novel View Synthesis. In Proc. of
the IEEE/CVF Intl. Conf. on Computer Vision (ICCV) , 2023.
2, 4
[21] Giseop Kim and Ayoung Kim. Remove, Then Revert: Static
Point Cloud Map Construction Using Multiresolution Range
Images. In Proc. of the IEEE/RSJ Intl. Conf. on Intelligent
Robots and Systems (IROS) , 2020. 2, 7, 8
[22] Xin Kong, Shikun Liu, Marwan Taher, and Andrew J. Davi-
son. vMAP: Vectorised Object Mapping for Neural Field
SLAM. In Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2023. 2
[23] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi,
Caroline Pantofaru, Leonidas Guibas, Andrea Tagliasacchi,
Frank Dellaert, and Thomas Funkhouser. Panoptic neural
Ô¨Åelds: A semantic object-aware neural scene representation.
InProc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2022. 2
[24] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.
Neural scene Ô¨Çow Ô¨Åelds for space-time view synthesis of dy-
namic scenes. In Proc. of the IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR) , 2021. 2
[25] Zhaoshuo Li, Thomas M ¬®uller, Alex Evans, Russell H Tay-
lor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.
Neuralangelo: High-Ô¨Ådelity neural surface reconstruction. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2023. 5
[26] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,
and Noah Snavely. DynIBaR: Neural Dynamic Image-Based
Rendering. In Proc. of the IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR) , 2023. 2, 3
15425
[27] Hyungtae Lim, Sungwon Hwang, and Hyun Myung. ERA-
SOR: Egocentric Ratio of Pseudo Occupancy-Based Dy-
namic Object Removal for Static 3D Point Cloud Map Build-
ing. IEEE Robotics and Automation Letters (RA-L) , 6(2):
2272‚Äì2279, 2021. 2, 7, 8
[28] Hyungtae Lim, Lucas Nunes, Benedikt Mersch, Xieyuanli
Chen, Jens Behley, and Cyrill Stachniss. ERASOR2:
Instance-Aware Robust 3D Mapping of the Static World in
Dynamic Scenes. In Proc. of Robotics: Science and Systems
(RSS) , 2023. 3
[29] William E. Lorensen and Harvey E. Cline. Marching Cubes:
a High Resolution 3D Surface Construction Algorithm. In
Proc. of the Intl. Conf. on Computer Graphics and Interac-
tive Techniques (SIGGRAPH) , 1987. 2, 6
[30] John McCormac, Ankur Handa, Aandrew J. Davison, and
Stefan Leutenegger. SemanticFusion: Dense 3D Semantic
Mapping with Convolutional Neural Networks. In Proc. of
the IEEE Intl. Conf. on Robotics & Automation (ICRA) ,
2017. 1, 2
[31] Benedikt Mersch, Xieyuanli Chen, Ignacio Vizzo, Lucas
Nunes, Jens Behley, and Cyrill Stachniss. Receding Mov-
ing Object Segmentation in 3D LiDAR Data Using Sparse
4D Convolutions. IEEE Robotics and Automation Letters
(RA-L) , 7(3):7503‚Äì7510, 2022. 2, 7, 8
[32] Benedikt Mersch, Tiziano Guadagnino, Xieyuanli Chen,
Tiziano, Ignacio Vizzo, Jens Behley, and Cyrill Stachniss.
Building V olumetric Beliefs for Dynamic Environments Ex-
ploiting Map-Based Moving Object Segmentation. IEEE
Robotics and Automation Letters (RA-L) , 8(8):5180‚Äì5187,
2023. 2, 7, 8
[33] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
bastian Nowozin, and Andreas Geiger. Occupancy networks:
Learning 3d reconstruction in function space. In Proc. of the
IEEE/CVF Conf. on Computer Vision and Pattern Recogni-
tion (CVPR) , 2019. 2
[34] Daniel Meyer-Delius, Maximilitan Beinhofer, and Wolfram
Burgard. Occupancy Grid Models for Robot Mapping in
Changing Environments. In Proc. of the Conf. on Advance-
ments of ArtiÔ¨Åcial Intelligence (AAAI) , 2012. 1, 2
[35] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing Scenes as Neural Radiance Fields for View
Synthesis. In Proc. of the Europ. Conf. on Computer Vision
(ECCV) , 2020. 2
[36] Thomas M ¬®uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a mul-
tiresolution hash encoding. ACM Trans. on Graphics , 41(4):
102:1‚Äì102:15, 2022. 2, 4
[37] Richard A. Newcombe, Shahram Izadi, Otmar Hilliges,
David Molyneaux, David Kim, Andrew J. Davison, Push-
meet Kohli, Jamie Shotton, Steve Hodges, and Andrew
Fitzgibbon. KinectFusion: Real-Time Dense Surface Map-
ping and Tracking. In Proc. of the Intl. Symposium on Mixed
and Augmented Reality (ISMAR) , 2011. 1, 2
[38] Joseph Ortiz, Alexander Clegg, Jing Dong, Edgar Sucar,
David Novotny, Michael Zollhoefer, and Mustafa Mukadam.
isdf: Real-time neural signed distance Ô¨Åelds for robot per-ception. In Proc. of Robotics: Science and Systems (RSS) ,
2022. 5
[39] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe
Giguere, and Cyrill Stachniss. ReFusion: 3D Reconstruction
in Dynamic Environments for RGB-D Cameras Exploiting
Residuals. In Proc. of the IEEE/RSJ Intl. Conf. on Intelligent
Robots and Systems (IROS) , 2019. 2
[40] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. DeepSDF: Learning
Continuous Signed Distance Functions for Shape Represen-
tation. In Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2019. 2
[41] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, SoÔ¨Åen
Bouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo
Martin-Brualla. NerÔ¨Åes: Deformable Neural Radiance
Fields. In Proc. of the IEEE/CVF Intl. Conf. on Computer
Vision (ICCV) , 2021. 2
[42] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.
Barron, SoÔ¨Åen Bouaziz, Dan B Goldman, Ricardo Martin-
Brualla, and Steven M. Seitz. Hypernerf: A higher-
dimensional representation for topologically varying neural
radiance Ô¨Åelds. ACM Trans. on Graphics (TOG) , 40(6),
2021.
[43] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and
Francesc Moreno-Noguer. D-nerf: Neural radiance Ô¨Åelds for
dynamic scenes. In Proc. of the IEEE/CVF Conf. on Com-
puter Vision and Pattern Recognition (CVPR) , 2021. 2
[44] Sameera Ramasinghe, Violetta Shevchenko, Gil Avraham,
and Anton Van Den Hengel. Blirf: Band limited radi-
ance Ô¨Åelds for dynamic scene modeling. arXiv preprint
arXiv:2302.13543 , 2023. 2
[45] Milad Ramezani, Yiduo Wang, Marco Camurri, David
Wisth, Matias Mattamala, and Maurice Fallon. The Newer
College Dataset: Handheld LiDAR, Inertial and Vision with
Ground Truth. In Proc. of the IEEE/RSJ Intl. Conf. on Intel-
ligent Robots and Systems (IROS) , 2020. 6
[46] Konstantinos Rematas, Andrew Liu, Pratul P. Srini-
vasan, Jonathan T. Barron, Andrea Tagliasacchi, Thomas
Funkhouser, and Vittorio Ferrari. Urban radiance Ô¨Åelds. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2022. 2, 4
[47] Martin R ¬®unz and Lourdes Agapito. Co-Fusion: Real-Time
Segmentation, Tracking and Fusion of Multiple Objects. In
Proc. of the IEEE Intl. Conf. on Robotics & Automation
(ICRA) , 2017. 1, 2, 6
[48] Martin R ¬®unz, Maud BufÔ¨Åer, and Lourdes Agapito. MaskFu-
sion: Real-Time Recognition, Tracking and Reconstruction
of Multiple Moving Objects. In Proc. of the Intl. Sympo-
sium on Mixed and Augmented Reality (ISMAR) , 2018. 1,
2
[49] Jari Saarinen, Henrik Andreasson, and Achim Lilienthal. In-
dependent Markov Chain Occupancy Grid Maps for Rep-
resentation of Dynamic Environments. In Proc. of the
IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems
(IROS) , 2012. 1, 2
[50] Jari P. Saarinen, Todor Stoyanov, Henrik Andreasson, and
Achim J. Lilienthal. Fast 3D Mapping in Highly Dynamic
15426
Environments Using Normal Distributions Transform Occu-
pancy Maps. In Proc. of the IEEE/RSJ Intl. Conf. on Intelli-
gent Robots and Systems (IROS) , 2013. 1, 2, 7
[51] Renato F. Salas-Moreno, Richard A. Newcombe, Hauke
Strasdat, Paul H. Kelly, and Andrew J. Davison. SLAM++:
Simultaneous Localisation and Mapping at the Level of Ob-
jects. In Proc. of the IEEE/CVF Conf. on Computer Vision
and Pattern Recognition (CVPR) , 2013. 1, 2
[52] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,
Hongwen Zhang, and Yebin Liu. Tensor4d : EfÔ¨Åcient neural
4d decomposition for high-Ô¨Ådelity dynamic reconstruction
and rendering. In Proc. of the IEEE/CVF Conf. on Computer
Vision and Pattern Recognition (CVPR) , 2023. 2
[53] Chonghyuk Song, Gengshan Yang, Kangle Deng, Jun-Yan
Zhu, and Deva Ramanan. Total-recon: Deformable scene
reconstruction for embodied view synthesis. In Proc. of the
IEEE/CVF Intl. Conf. on Computer Vision (ICCV) , 2023. 2
[54] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele
Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. NeRF-
Player: A Streamable Dynamic Scene Representation with
Decomposed Neural Radiance Fields. IEEE Transactions
on Visualization and Computer Graphics , 29(5):2732‚Äì2742,
2023. 2
[55] Cyrill Stachniss and Wolfram Burgard. Mobile Robot Map-
ping and Localization in Non-Static Environments. In
Proc. of the National Conf. on ArtiÔ¨Åcial Intelligence (AAAI) ,
2005. 1, 2
[56] Cyrill Stachniss, John J. Leonard, and Sebastian Thrun.
Springer Handbook of Robotics, 2nd edition , chapter
Chapt. 46: Simultaneous Localization and Mapping.
Springer Verlag, 2016. 1, 2
[57] Sebstian Thrun, Wolfram Burgard, and Dieter Fox. Proba-
bilistic Robotics . MIT Press, 2005. 1, 2
[58] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael
Zollh ¬®ofer, Christoph Lassner, and Christian Theobalt. Non-
rigid neural radiance Ô¨Åelds: Reconstruction and novel view
synthesis of a dynamic scene from monocular video. In
Proc. of the IEEE/CVF Intl. Conf. on Computer Vision
(ICCV) , 2021. 2
[59] Ignacio Vizzo, Tiziano Guadagnino, Jens Behley, and Cyrill
Stachniss. VDBFusion: Flexible and EfÔ¨Åcient TSDF Inte-
gration of Range Sensor Data. Sensors , 22(3):1296, 2022. 6,
7
[60] Ignacio Vizzo, Tiziano Guadagnino, Benedikt Mersch, Louis
Wiesmann, Jens Behley, and Cyrill Stachniss. KISS-ICP:
In Defense of Point-to-Point ICP ‚Äì Simple, Accurate, and
Robust Registration If Done the Right Way. IEEE Robotics
and Automation Letters (RA-L) , 8(2):1029‚Äì1036, 2023. 3
[61] Aishan Walcott-Bryant, Michael Kaess, Hordur Johannsson,
and John J. Leonard. Dynamic Pose Graph SLAM: Long-
Term Mapping in Low Dynamic Environments. In Proc. of
the IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems
(IROS) , 2012. 1, 2
[62] Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio
Gallo. Neural trajectory Ô¨Åelds for dynamic novel view syn-
thesis. arXiv preprint arXiv:2105.05994 , 2021. 3[63] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan,
Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-
manNeRF: Free-Viewpoint Rendering of Moving People
From Monocular Video. In Proc. of the IEEE/CVF Conf. on
Computer Vision and Pattern Recognition (CVPR) , 2022. 2
[64] Thomas Whelan, Stefan Leutenegger, Renato F. Salas-
Moreno, Ben Glocker, and Andrew J. Davison. ElasticFu-
sion: Dense SLAM Without A Pose Graph. In Proc. of
Robotics: Science and Systems (RSS) , 2015. 1, 2
[65] Louis Wiesmann, Tiziano Guadagnino, Ignacio Vizzo, Nicky
Zimmerman, Yue Pan, Haofei Kuang, Jens Behley, and
Cyrill Stachniss. LocNDF: Neural Distance Field Mapping
for Robot Localization. IEEE Robotics and Automation Let-
ters (RA-L) , 8(8):4999‚Äì5006, 2023. 2
[66] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lam-
bert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Rat-
nesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes,
Deva Ramanan, Peter Carr, and James Hays. Argoverse
2: Next Generation Datasets for Self-driving Perception and
Forecasting. In Proc. of the Conf. on Neural Information
Processing Systems (NeurIPS) , 2021. 7
[67] Denis F. Wolf and Guarav S. Sukhatme. Mobile Robot Si-
multaneous Localization and Mapping in Dynamic Environ-
ments. Autonomous Robots , 19, 2005. 1, 2
[68] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-
rester Cole, and Cengiz Oztireli. DÀÜ2NeRF: Self-Supervised
Decoupling of Dynamic and Static Objects from a Monocu-
lar Video. In Proc. of the Conf. on Neural Information Pro-
cessing Systems (NeurIPS) , 2022. 2
[69] Kai M. Wurm, Armin Hornung, Maren Bennewitz, Cyrill
Stachniss, and Wolfram Burgard. OctoMap: A Probabilistic,
Flexible, and Compact 3D Map Representation for Robotic
Systems. In Workshop on Best Practice in 3D Perception
and Modeling for Mobile Manipulation, IEEE Int. Conf. on
Robotics & Automation (ICRA) , 2010. 7
[70] Dongyu Yan, Xiaoyang Lyu, Jieqi Shi, and Yi Lin. EfÔ¨Åcient
Implicit Neural Reconstruction Using LiDAR. In Proc. of the
IEEE Intl. Conf. on Robotics & Automation (ICRA) , 2023. 2
[71] Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, and Steven
Lovegrove. Star: Self-supervised tracking and reconstruc-
tion of rigid objects in motion with neural rendering. In
Proc. of the IEEE/CVF Conf. on Computer Vision and Pat-
tern Recognition (CVPR) , 2021. 2
[72] Qingwen Zhang, Daniel Duberg, Ruoyu Geng, Mingkai Jia,
Lujia Wang, and Patric Jensfelt. A dynamic points removal
benchmark in point cloud maps. In IEEE 26th International
Conference on Intelligent Transportation Systems (ITSC) ,
pages 608‚Äì614, 2023. 6, 8
[73] Xingguang Zhong, Yue Pan, Jens Behley, and Cyrill Stach-
niss. SHINE-Mapping: Large-Scale 3D Mapping Using
Sparse Hierarchical Implicit Neural Representations. In
Proc. of the IEEE Intl. Conf. on Robotics & Automation
(ICRA) , 2023. 2, 6, 7, 8
15427
