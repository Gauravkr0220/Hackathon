GROUNDHOG
 : Grounding Large Language Models to Holistic Segmentation
Yichi Zhang1‚Ä†, Ziqiao Ma1‚Ä†, Xiaofeng Gao2, Suhaila Shakiah2, Qiaozi Gao2, Joyce Chai1
1University of Michigan,2Amazon AGI
zhangyic@umich.edu
https://groundhog-mllm.github.io/
Describe the image briefly.
A man and a little girl are 
sitting in a shopping cart.
üë§
 Generate a short caption.
Two dogs are playing with 
a stick in a field.
üë§
Describe the given 
picture in very detail. 
In this image, we can see a 
boat on the water. There 
are few people inside the 
boat. There are also few 
people on the rocks. In the 
background, there are 
trees, hills, and sky.
üë§
(a) Grounded Image Captioning (GIC).
Provide a distinct description for 
that <PTR> .
Blue container with apples.
üë§
<PTR>
Could you please segment out "laptop 
barely onscreen not apple" in the image? 
Laptop barely onscreen not apple.
 Feb 2.
üë§
What date is shown on the 
calendar?
 üë§
(d) Referential Dialogue (RD). (b) Referential Expression Segmentation (RES). (c) Grounded Visual Question Answering (GVQA).
Figure 1. We propose G ROUNDHOG , a multimodal large language model that enhances its text output with pixel-level phrase grounding
across diverse semantic granularities. The figure demonstrates outputs from our model on the four task types we considered in this work.
Abstract
Most multimodal large language models (MLLMs)
learn language-to-object grounding through causal lan-
guage modeling where grounded objects are captured by
bounding boxes as sequences of location tokens. This
paradigm lacks pixel-level representations that are impor-
tant for fine-grained visual understanding and diagnosis.
In this work, we introduce GROUNDHOG , an MLLM de-
veloped by ground ing Large Language Models to ho listic
segmentation. GROUNDHOG incorporates a masked fea-
ture extractor and converts extracted features into visual
entity tokens for the MLLM backbone, which then con-
‚Ä†Work done during internship at Amazon AGI.nects groundable phrases to unified grounding masks by re-
trieving and merging the entity masks. To train GROUND -
HOG, we carefully curated M3G2, a grounded visual in-
struction tuning dataset with M ulti-M odal M ulti-G rained
Grounding, by harvesting a collection of segmentation-
grounded datasets with rich annotations. Our experimental
results show that GROUNDHOG achieves superior perfor-
mance on various language grounding tasks without task-
specific fine-tuning, and significantly reduces object hallu-
cination. GROUNDHOG also demonstrates better ground-
ing towards complex forms of visual input and provides
easy-to-understand diagnosis in failure cases.
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14227
1. Introduction
Multimodal large language models (MLLMs) have re-
ceived an increasing amount of attention to address tasks
that necessitate non-linguistic knowledge, e.g., perception
and reasoning about the visual world [39, 84]. For fine-
grained visual understanding, grounded MLLMs often learn
language-to-object grounding by causal language modeling,
where grounded objects are captured by bounding boxes as
sequences of location tokens. However, bounding boxes
are insufficient in indicating amorphous stuff [5], seman-
tic parts of objects [23], finer-grained regions with irregular
shapes [26], or groups of instances at the same time. As a
result, a single bounding box can often include other irrel-
evant semantics in order to engulf the target entities, lead-
ing to ambiguity in detection. In addition, the generated
box coordinate lacks interpretability. When the model hal-
lucinates, such as incorrectly predicting the association be-
tween objects and language, it is hard to diagnose whether
the problem is due to the model‚Äôs failure to detect the object,
or its incorrect alignment of the object with language.
To address these issues, in this work, we introduce
GROUNDHOG , an MLLM developed by ground ing Large
Language Models to ho listic seg mentation. Our goal of lan-
guage grounding is to connect text spans that refer to or
can be deduced from visual information, termed as ground-
able phrases [50], to their corresponding regions of visual
entities (Figure 1). G ROUNDHOG incorporates a masked
feature extractor that takes an input image and a set of
class-agnostic entity mask proposals, and converts each
mask‚Äôs features into visual entity tokens for an MLLM
backbone. This MLLM then connects groundable phrases
to unified grounding masks by retrieving and merging the
entity masks. Compared to previous grounded MLLMs,
GROUNDHOG unlocks unprecedented pixel-level vision-
language alignment. It naturally supports visual point-
ers as input, and can plug-in-and-play with any choice of
mask proposal networks, e.g., Segment Anything Model
(SAM) [35], domain-specific semantic segmentation mod-
els, or user-provided mask candidates. We introduce an en-
hanced Mask2Former [10] as our default mask proposal net-
work, which detects regions at multiple granularities, e.g.,
instances (things and stuff), semantic parts, and visual text,
leading to a holistic coverage of visual semantics.
To train G ROUNDHOG , we curated a M ulti-M odal M ulti-
Grained G rounding (M3G2) dataset consisting of 2.5M
text-image pairs for visually grounded instruction tuning,
consisting of 36 sub-problems derived and augmented from
27 existing datasets. We present extensive experiments
on vision-language tasks that require grounding, including
grounded language generation with minimal object halluci-
nation, language-guided segmentation, visual question an-
swering with answer grounding, and referential dialog with
spatial pointer inputs (Figure 1). Our empirical results showthat G ROUNDHOG , without task-specific fine-tuning, can
achieve superior or comparable performance with previous
models that either require fine-tuning or are specialized only
for that dataset. In addition, G ROUNDHOG has supports
easy-to-understand diagnosis when grounding fails.
2. Our Method: G ROUNDHOG
The language grounding task can be succinctly delineated
into two fundamental components: localization andrecog-
nition, as established in the literature [50, 68,92]. Such
categorization not only aids in the identification of ob-
ject presence (objectness) without reliance on specific ob-
ject classes, but also sets the stage for models to be ro-
bust in open-vocabulary settings. Building upon this frame-
work, we formulate the grounding process as an entity seg-
ment selection problem, which involves (1) proposing en-
tity segmentation masks where the masks encapsulate re-
gions with discernible semantic content, and (2) recogniz-
ing the retrieved entities through the understanding of both
visual and language context. Concurrently performing both
tasks is where MLLMs bring a distinct advantage. This
decoupled design of entity mask proposal and language-
guided grounding brings several advantages. First, it al-
lows independent improvement of the mask proposal model
and MLLM, where specialized data, training, and infer-
ence setups can be applied. Second, by decoupling lan-
guage grounding, it becomes straightforward to determine
if a failure is due to the model‚Äôs inability to propose the
entity segment, or its misalignment of the object with the
language, thus improving the interpretability of the whole
framework. Third, as shown later, when connecting the two
parts to work in tandem in a model-independent manner, the
MLLM can benefit from multiple different vision specialist
models in a plug-and-play fashion. In the remainder of this
section, we give details of our model design.
2.1. Building Entity Features from Masks
Our approach assumes the availability of a mask proposal
model, which is capable of generating a set of class-
agnostic entity masks from an image with high coverage.
In contrast to prior studies that relied on low-level fea-
tures [8, 13,45,56], G ROUNDHOG interprets the image as a
collection of entities. The primary challenge then becomes
the derivation of effective visual features to accurately rep-
resent these entities. To achieve a complete decoupling of
the MLLM from the mask proposal model responsible for
providing the masks, we propose to condition the entity fea-
tures solely on the binary masks without using any embed-
dings from the mask proposal model. Specifically, the mask
corresponding to each entity is employed to extract patch
features from pretrained vision foundation models, such as
CLIP [62] and DINOv2 [54], through a convolutional mask
pooling layer [12]. Given that the feature map dimensions
14228
beach
Multi-ModalLanguage ModelI      see  <GRD>two  dogs</GRD>on  <GRD>theGrounding MaskLM Head
Next Token
Visual Entity Tokens
Class-Agnostic MasksMergeInput Image
Masked Feature Extractor
CLIP
MLPMLPDownSamplingDINO v2
Mask PoolingMask Pooling
Mask Retrieval HeadMask Proposal ModelFigure 2. The model architecture of G ROUNDHOG model. Given a set of class-agnostic entity mask proposals, the masked feature extractor
first extracts the feature of each entity as the visual input of the multi-modal large language model (left). The output hidden states of the
grounding tokens are averaged and used to retrieve the entities to ground, which will be merged into a single grounding mask for the phrase.
Modules are colored by their trainability: parameter-free operators (grey), frozen (blue), trainable (orange), and partially trainable (mix).
SAM
Multi-Modal Language ModelWhat    is   that <PTR>?A black dog ‚Ä¶
Masked Feature Extractor
Figure 3. G ROUNDHOG can take arbitrary spatial prompts that can
be resolved by an interactive segmentation model, such as SAM.
The placeholder pointer token <PTR> will be replaced by the ex-
tracted entity features and fed as input to the model.
are usually smaller than those of the mask proposals, we re-
size the masks to match the size of the feature maps prior to
pooling. The pooled features are then fed into a Multi-Layer
Perceptron (MLP) network to align with the input embed-
dings of the MLLM. We empirically find the combination
of CLIP and DINOv2 features yields the best result, and
these features are added to obtain the final input visual en-
tity tokens to the MLLM.
Spatial Prompts Furthermore, for grounded MLLMs to
be more broadly applicable, they must be capable of inter-
preting multi-modal user inputs, including spatial prompts.
Thanks to the mask model agnostic design, G ROUND -
HOGcan seamlessly support such inputs. As demonstrated
in Figure 3, by applying an interactive segmentation model
such as Segment-Anything (SAM) [35], arbitrary spatial
prompts can be translated into binary masks and processed
by the same masked feature extractor we just introduced.
This extracted feature for the pointed entity will replace the
pointer token <PTR> placeholder in the textual input.2.2. Language Grounding to Entity Segmentation
Existing box-grounded MLLMs typically append location
tokens after the groundable phrases [8, 9, 56, 85]. However,
this method is not readily interpretable. To alleviate this dis-
connect, we introduce a pair of grounding tokens <GRD>
and</GRD> to indicate the start and end of groundable
phrases, with the assumption that grounding these phrases
requires mapping to certain representations of visual enti-
ties irrespective of the visual modality. In Figure 2, a sen-
tence can be represented as I see <GRD> two dogs
</GRD> on <GRD> the beach </GRD> , with two
distinct visual entities grounded. The representation of each
groundable phrase, termed as the grounding query , is ob-
tained by adding <GRD> and</GRD> ‚Äôs output embedding
from the last transformer layer of the MLLM. The repre-
sentation is then used to retrieve the entities that the phrase
should be grounded to. In particular, we concatenate the
grounding query with the last layer output of each visual
entity token, and use an MLP to predict a scalar score for
each entity. Finally, we merge all the mask proposals into
one single mask with pixel-wise maximization:
Mh,w= max
q
Sq¬∑cMq,h,w
where Sqis the normalized score of the q-th mask ranging
from 0 to 1, and cMq,h,w denotes the pixel probability at
position (h, w)for the q-th mask. Note that a phrase may
ground to multiple entities, thus multiple mask proposals
may get a high score simultaneously and be selected in con-
junction. One of the primary benefits of this decoupled de-
sign is its transparency in the selection of entities. Users can
easily visualize both the mask proposals and their respective
14229
scores, providing a clear understanding of how a grounding
mask is predicted. This level of clarity and interpretability
is a significant advantage, offering users a tangible insight
into the model‚Äôs grounding process.
2.3. Towards Holistic Entity Mask Proposals
In order to support holistic language grounding to arbitrary
segmentations, the entity proposal should have two essen-
tial properties. First, the proposals should strike a delicate
balance in terms of semantic atomicity. While it is possi-
ble to merge multiple proposals later to form multi-entity
segmentations, the reverse, i.e., dividing a single proposal
into smaller segments, is not feasible. Therefore, instance
segmentation is generally preferred over semantic segmen-
tation. However, the segmentation should not be exces-
sively fine-grained to the extent that it compromises basic
semantic integrity. Over-segmentation can lead to a loss of
the coherent concept of an entity, which is detrimental to
the grounding process. Second, the entity proposals should
have a high coverage of entities, encompassing a diverse
range of granularities. This includes not only tangible ob-
jects (things) and amorphous concepts (stuff) but also ex-
tends to sub-components of objects (parts of things) and
structured regions such as areas containing visual text. The
ability to propose entities across this spectrum of granular-
ity is pivotal, as it directly determines the upper bound of
the grounding capability of MLLM.
We initiated our study with a Mask2Former model pre-
trained on the COCO panoptic segmentation dataset. How-
ever, preliminary experiments revealed its limitations in
semantic coverage and adaptability to open-world scenar-
ios. To enhance this, we developed Mask2Former+, an
upgraded version designed for multi-grained segmentation.
This upgrade involved creating a diverse dataset by merg-
ing annotations from various sources, including COCO [5],
LVIS [25], Entity-v2 [60], Pascal [16], PACO [63] (Fig-
ure7); MHP-v2 [40] for human part parsing; and Tex-
tOCR [67] for text segmentation. We expanded the model‚Äôs
capabilities by adding 50 expert queries each for seman-
tic parts and visual text regions, alongside the original 200
entity queries. We assessed Mask2Former+‚Äôs performance
on 1000 images from validation splits from 4 grounding
benchmarks, RefCOCO+ [86], PhraseCut [76], Reason-
Seg [37], and TextVQA-X [66]. We use the Any-IoU [30]
metric for evaluation, i.e., for each ground truth mask, we
extract the most overlapped mask proposals and compute
the IoU, then take the average. As Table 1demonstrates,
Mask2Former+ shows consistent improvements across all
domains, particularly in those significantly divergent from
COCO. This highlights its enhanced adaptability and pre-
cision in a broader range of segmentation challenges, pro-
viding a good mask proposal model for G ROUNDHOG . We
refer to Appendix Afor more details of the model and data.Model
RefCOCO+ PhraseCut ReasonSeg TextVQA-X
Mask2F
ormer 0.867 0.563 0.602 0.137
Mask2Former+ 0.873 0.624 0.745 0.446
Table 1. The average Any-IoU of the proposals on each dataset.
The vanilla Mask2Former is trained on the COCO-Panoptic
dataset and our Mask2Former+ is trained on our combined dataset.
Mask2Former+ obtains a consistent improvement in all scenarios,
especially in non-COCO domains.
T
ask Dataset Gr
. Ann. Sem.
Gran. #
Pairs
M B Po STh P
aG Tx T
rain
GCAPPNG ‚úì
‚úì ‚úì ‚úì ‚úì 132k
Flickr30K-Entity ‚úì ‚úì ‚úì ‚úì ‚úì 149k
RESRefCOCO ‚úì
‚úì ‚úì 113k
RefCOCO+ ‚úì ‚úì ‚úì 112k
RefCOCOg ‚úì ‚úì ‚úì 80k
RefCLEF ‚úì ‚úì ‚úì 105k
gRefCOCO ‚úì ‚úì ‚úì 194k
PhraseCut ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 85k
D-Cube ‚úì ‚úì ‚úì ‚úì 10k
ReasonSeg ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 1k
RIO ‚úì ‚úì ‚úì ‚úì 28k
SK-VG ‚úì ‚úì 23k
GVQAV
izWiz-G ‚úì ‚úì ‚úì ‚úì ‚úì 6k
TextVQA-X ‚úì ‚úì ‚úì 15k
GQA ‚úì ‚úì ‚úì ‚úì ‚úì 302k
VQS ‚úì ‚úì 20k
Shikra-BinaryQA ‚úì ‚úì ‚úì ‚úì ‚úì 4k
EntityCount ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 11k
FoodSeg-QA ‚úì ‚úì ‚úì ‚úì 7k
LVIS-QA ‚úì ‚úì ‚úì ‚úì ‚úì 95k
RDRefCOCO-REG ‚úì
‚úì ‚úì ‚úì 17k
RefCOCO+-REG ‚úì ‚úì ‚úì ‚úì 17k
RefCOCOg-REG ‚úì ‚úì ‚úì ‚úì 22k
gRefCOCO-REG ‚úì ‚úì ‚úì ‚úì 20k
VG-SpotCap ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 247k
V7W ‚úì ‚úì ‚úì 23k
PointQA ‚úì ‚úì 64k
VCR ‚úì ‚úì ‚úì 156k
ShikraRD ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 2k
SVIT-RD ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 33k
Guesswhat ‚úì ‚úì ‚úì ‚úì 193k
VG-RefMatch ‚úì ‚úì ‚úì ‚úì ‚úì ‚úì 247k
HierText ‚úì ‚úì ‚úì ‚úì 6k
M3G2
(Total) 2.5M
Table 2. Summary of datasets included in M3G2. We show the
availability of Gr ounding Ann otations (B ox, M ask, and Po inter
inputs), the Sem antic Gran ularity (S tuff, Th ings, Pa rts, G roups,
and T ext), and the number of text-image pairs for training.
3. Our Dataset: M3G2
In this section, we introduce M3G2, a Multi-Modal Multi-
Grained Grounding dataset consisting of 2.5M text-image
pairs for visually grounded instruction tuning, consisting of
36 sub-problems derived and augmented from 27 existing
datasets. We re-organize and augment public datasets of
language grounding, visual question answering, referring
expression segmentation, and referring expression genera-
tion into various forms of visually grounded dialogue for
grounded instruction tuning, outlined briefly in Table 2. The
dataset is categorized into four main types: (1) Grounded
Image Captioning (GIC), (2) Referential Expression Seg-
mentation (RES), (3) Grounded Visual Question Answer-
14230
ing (GVQA), and (4) Referential Dialog (RD). We provide
illustrated descriptions of our prompt design, accompanied
by examples of each task type as depicted in Figure 4. We
detail the task schema in the following sections and provide
the complete sets of templates in Appendix B.
3.1. Grounded Image Captioning (GIC)
The task of grounded image captioning requires the model
to produce a narrative for the visual scene, and accurately
identify and associate the groundable phrases with their re-
spective binary segmentation masks. The objective of this
task is to empower the model to articulate the scene while
acknowledging various visual elements and their spatial in-
terrelations. We incorporate the Panoptic Narrative Ground-
ing (PNG) dataset [34] for dense and detailed scene descrip-
tions, as well as the Flickr30K-Entity dataset [58] for con-
cise descriptions of the salient contents in the image. We
create a collection of task prompt templates that instruct the
model to describe the image either in detail or briefly.
3.2. Referring Expression Segmentation (RES)
In contrast to previous tasks, the referring expression seg-
mentation task requires that the model generates a segmen-
tation mask based on a given referring expression. Besides
the RefCOCO series [43, 52,86], we have further lever-
aged existing RES benchmarks [37, 65,76,78,79] for this
purpose. As shown in Figure 4, we instruct the model to
localize the referred objects, with the expected output be-
ing a repetition of the referring expression whose grounding
mask is the target object.
3.3. Grounded Visual Question Answering (GVQA)
The task of grounded visual question answering requires
the model to comprehend a question (with optional point-
ers) and to produce an answer that is grounded to a binary
segmentation mask that justifies the answer. The goal of this
task is to enable natural QA-based interaction with users
and reasoning in the model with grounded explanations.
Specifically, we harvest and adapt a collection of public
VQA datasets with grounding annotations [6, 8,19,29], QA
benchmarks on visual text [64], and create templated QA
pairs from segmentation datasets [25, 60]. Our prompt tem-
plates instruct the model to respond either as open-ended
answers or by selecting from multiple choices, with the re-
sponse anchored to a segmentation mask (Figure 4).
3.4. Referential Dialogue (RD)
The task of referential dialogue requires the model to con-
duct dialogue communication with users, especially when
conditioned on user-provided spatial prompts. This in-
cludes existing RD datasets [8, 51,88,90,94], multi-turn
augmentations from segmentation datasets [17, 36,47] as
well as the referring expression generation (REG) task the
[USER]  Provide a concise and informative caption for 
the image, focusing on the primary subjects.
[ASSISTANT]  <GRD>  A burger </GRD>  and 
<GRD>  a pickle </GRD>  sit on <GRD>  a plate </GRD> .
[USER]  Is this vegetarian? Highlight the region in the 
picture that supports your answer.
[ASSISTANT]  <GRD>  No </GRD> .
[USER]  Please describe <PTR> in the image in a 
way that it can be uniquely identified.
[ASSISTANT]  <GRD>  the sandwich lying at the  
farthest side of my plate </GRD> .[USER]  Localize the sandwich in the plate at the far end.
[ASSISTANT]  Here it is: <GRD>  the sandwich in the plate  
at the far end </GRD> .
Visual Input Segm. Output Instruction Tuning with Grounded Dialogue
Grounded Image Captioning
Grounded Visual Question Answering
Referential Dialogue
Referring Expression SegmentationFigure 4. The M3G2 dataset for grounded visual instruction tun-
ing. M3G2 is a diverse dataset of multiple granularities, unifying
4 different task types with visually grounded dialogue.
RefCOCO series [43, 52,86]. The REG task differs from
the region captioning task in that it demands the description
to be a referring expression that distinctly identifies the tar-
geted object. Effective REG calls for the model to engage in
dialogue interactions cooperatively, adhering to the Gricean
Maxims [24] which dictate that communication should be
as informative, truthful, relevant, and clear as necessary.
4. Experiment and Analysis
We adopt the LLaMA2-7B model [70] as our base LLM,
and initialized the weight from LLaV A-1.5 [44]. For the
vision encoders, we use the OpenAI CLIP@336 [62] model
and DINOv2-L/14-reg [15] pretrained checkpoints. Please
refer to Appendix Cfor more implementation details.
4.1. Generalist in Grounded Vision-Language Tasks
We first demonstrate G ROUNDHOG ‚Äôs capabilities as a gen-
eralist model for three different types of grounded vision-
language tasks. It‚Äôs worth noting that, unlike previous work
that needs dataset-specific fine-tuning on each of the tasks,
GROUNDHOG can achieve comparable performance on all
the tasks directly after training on M3G2, i.e., all the re-
ported results from our model are from a single set of
weights without any dataset-specific fine-tuning.
Language Grounding To Segmentation. We start by
evaluating the model on language grounding tasks, which
takes text as input and generates segmentation masks as out-
put. We assess G ROUNDHOG on Referential Expression
Segmentation (RES) [32] and Caption Phrase Grounding
(CPG) tasks. While traditional RES benchmarks [32, 53]
focus on single-instance referents requiring primarily visual
understanding, we expanded our evaluation to include com-
plex scenarios involving multi-instance or negative queries
14231
Single
Instance Multi-/No Instance Reasoning
Model
RefCOCO RefCOCO+ RefCOCOg gRefCOCO PhraseCut ReasonSeg RIO
v
al test-A test-B val test-A test-B val-u test-u val test val test-c test-u
Specialist
MDETR
[30] - - - - - - - - - 53.7 - 44.1 22.0
CRIS [75] 70.5 73.2 66.1 62.3 68.1 53.7 59.9 60.4 55.3 - - - -
LA VT [82] 72.7 75.8 68.8 62.1 68.4 55.1 61.2 62.1 58.4 - - - -
ReLA [43] 73.8 76.5 70.2 66.0 71.0 57.7 65.0 66.0 63.6 - 22.4 - -
PolyFormer [46] 76.0 78.3 73.3 69.3 74.6 61.9 69.2 70.2 - - - 48.8 26.8
UNINEXT-H [80] 82.2 83.4 81.3 72.5 76.4 66.2 74.7 76.4 - - - - -
Gener
alist
LISA 7B[
37] 74.1 76.5 71.1 62.4 67.4 56.5 66.4 68.5 - - 44.0 - -
LISA 7B(FT) [37] 74.9 79.1 72.3 65.1 70.8 58.1 67.9 70.6 - - 52.9 - -
7B 78.5
79.9 75.7 70.5 75.0 64.9 74.1 74.6 66.7 54.5 56.2 57.9 33.9
Table 3. Results on 7 Referring Expression Segmentation (RES) benchmarks with single instance queries [32, 53], multi-/null instance
queries [43, 76] and reasoning-based queries [37, 61]. We report cIoU for RefCOCO/+/g and mIoU for other benchmarks, respectively.
Flickr30K-E
Model
R@1 val R@1 test
Shikra 13B 77.4
78.4
Ferret 13B 81.1 84.8
Shikra 7B 75.8
76.5
Ferret 7B 80.4 82.2
7B 79.2
79.8
Table 4. Top-1 box recall results on
Flickr30K-Entity [58].PNG
Model
AR AR thAR stAR sAR p
PiGLET
65.9 64.0 68.6 67.2 54.5
7B 66.8
65.0 69.4 70.4 57.7
Table 5. Phrase grounding results on PNG [21].
Model
TextVQA-X [mIoU]
SAB
29.0
7B 39.8
Table 6. Visual text QA results on the TextVQA-
X [66] validation set.P
ointQA Twice V7W
Model Acc Acc
Shikra 13B 70.3
85.3
GPT4RoI 13B - 84.8
Shikra 7B -
-
GPT4RoI 7B - 81.8
7B 72.4
85.5
Table 7. Results on PointQA Twice [51]
and V7W [94] test sets.
[43, 76], and those necessitating common sense reason-
ing [37, 61]. The results in Table 3show G ROUNDHOG out-
performing the generalist model LISA across all bench-
marks and achieving significant improvements over special-
ist models in multi-instance, null, and reasoning-based RES
tasks. It also performs comparably on the competitive Re-
fCOCO series. For CPG tasks, which involve grounding
all phrases in a caption and demand a deep understanding
of the context for coreference resolution, we first evaluated
GROUNDHOG on the Flickr30K-Entity dataset [58]. Since
this dataset only has box annotations, we convert the mask
predictions of our model to box and compute the top-1 box
recall following the merged-box protocol [30]. Despite not
specializing in predicting boxes, G ROUNDHOG still outper-
forms Shikra 7B/13B [8] and is on par with Ferret-7B [85]
in a concurrent work (Table 4). Additionally, on the PNG
dataset [34] which tests phrase grounding in longer narra-
tives, G ROUNDHOG surpasses the previous state-of-the-art
model, PiGLET [22], in all metrics including average recall
of grounding masks and detailed scores for things, stuffs,
and singular and plural entities (Table 5).
Grounded Language Generation. Our model excels in
generating language that accurately grounds to segmen-
tation masks during user conversations. Quantitatively,
we assess grounded captioning on the Flickr30K-Entity
dataset [58], employing standard text generation metricsModel
Bleu-4 METEOR CIDEr SPICE F1 all
Shikra 13B -
- 73.9 - -
Ferret 13B 37.0 25.5 76.1 18.3 15.1
Ferret 7B 35.1
24.6 74.8 18.0 15.0
7B 36.7
26.5 91.3 20.4 32.1
Table 8. Grounded Captioning on Flickr30K-Entity [58].
such as Bleu-4 [55], METEOR [4], CIDEr [72], and
SPICE [2] for language quality; and the F1 allscore for
grounding accuracy following You et al. [85]. As shown
in Table 8, G ROUNDHOG significantly surpasses existing
box-based grounded MLLMs, even their 13B versions, in
both language quality and grounding accuracy. This im-
provement is hypothesized to stem from the diverse task
distribution in our M3G2 dataset. For groundable ques-
tion answering, we evaluate on the TextVQA-X bench-
mark [64]. G ROUNDHOG outperforms the state-of-the-art
specialist model SAB [33] by a significant margin, as mea-
sured by the mean IoU of the predicted mask (Table 6).
Spatial Prompt Understanding. For grounded MLLMs,
accurately interpreting multimodal instructions is essen-
tial, particularly in interactive tasks. We evaluated
its performance on two pointer-based QA benchmarks,
PointerQA Twice [51] and V7W [94], which require the
model to answer questions guided by spatial prompts, such
as bounding boxes. The model is tasked to generate free-
14232
Input Prompt M2F+ Best Match SAM
‚Äúa red 
brick tower‚Äù‚Äúspire in red 
brick‚ÄùFigure 5. Region caption using the best match proposal from
Mask2Former+ versus from SAM. Mask2Former+ fails to propose
the exact mask of the spire, leading to a less precise caption.
‚ÄúKWIK E MART‚Äù
Merge
0.352 0.002 0.713 1.00 0
Figure 6. Illustration of a partially correct grounding. The ground-
ing phrase and the ground truth mask are shown on the left. The
top-4 mask proposals are presented, with highly-scored masks
(green) selected for the merged mask, and low-scored masks
(red) excluded. This illustrates the failure to recognize the word
‚ÄúKWIK‚Äù by the MLLM, despite its successful proposal.
form textual answers in PointerQA Twice, and selects from
multiple-choice options in V7W. G ROUNDHOG demon-
strates superior performance in these benchmarks, outper-
forming previous models as shown in Table 7. This high-
lights its effectiveness in spatial understanding and response
accuracy. To further demonstrate the effectiveness of using
SAM for the pointer-to-mask conversion, we show the best-
matched mask proposal from our Mask2Former+ model in
comparison to the mask from SAM in Figure 5. While
the best match proposal from the Mask2Former+ model in-
cludes a broader area, the SAM-generated mask offers a
more precise representation of the specified region, poten-
tially leading to a more accurate caption.
4.2. Trustworthiness and Transparency
Beyond its superior performance as a grounding general-
ist, we highlight two key improvements for creating a more
trustworthy and transparent agent.
Reduced Object Hallucination. Thanks to the varied
task distribution and the inclusion of negative question-
answering samples in M3G2 dataset, G ROUNDHOG signifi-
cantly reduces object hallucination. We assessed this using
the POPE [42] benchmark, which includes binary questions
about object existence across three splits, each with a dif-
ferent object distribution (with an order of difficulty Ran-
dom<Popular <Adversarial ). Remarkably, G ROUND -Model
Accuracy Precision Recall F1 Score Yes (%)
Random
mPLUG-Owl
53.30 51.71 99.53 68.06 96.23
LLaV A 54.43 52.32 99.80 68.65 95.37
MultiModal-GPT 50.03 50.02 100.00 66.68 99.97
MiniGPT-4 77.83 75.38 82.67 78.86 54.83
InstructBLIP 88.73 85.08 93.93 89.29 55.20
Shikra-13B 86.90 94.40 79.26 86.19 43.26
Ferret-13B 90.24 97.72 83.00 89.76 43.26
7B 91.03
85.80 96.40 90.79 45.88
P
opular
mPLUG-Owl 50.63 50.32 99.27 66.79 98.63
LLaV A 52.43 51.25 99.80 67.72 97.37
MultiModal-GPT 50.00 50.00 100.00 66.67 100.00
MiniGPT-4 68.30 64.27 82.40 72.21 64.10
InstructBLIP 81.37 75.07 93.93 83.45 62.57
Shikra-13B 83.97 87.55 79.20 83.16 45.23
Ferret-13B 84.90 88.24 80.53 84.21 45.63
7B 90.13
85.93 93.81 89.70 45.80
Adver
sarial
mPLUG-Owl 50.67 50.34 99.33 66.82 98.67
LLaV A 50.77 50.39 99.87 66.98 99.10
MultiModal-GPT 50.00 50.00 100.00 66.67 100.00
MiniGPT-4 66.60 62.45 83.27 71.37 66.67
InstructBLIP 74.37 67.67 93.33 78.45 68.97
Shikra-13B 83.10 85.60 79.60 82.49 46.50
Ferret-13B 82.36 83.60 80.53 82.00 48.18
7B 86.33
85.93 86.63 86.28 49.60
Table 9. Object hallucination results on the POPE [42] benchmark.
HOG consistently outperforms other models in both accu-
racy and F1 score across all splits, particularly on the more
challenging ones. It shows an absolute improvement of
5.2% in accuracy for Popular and 4.0% for Adversarial over
the previously best-performing model. This suggests that
our model‚Äôs enhanced grounding capability plays a signifi-
cant role in mitigating the object hallucination problem.
Explainability and Diagnosability. Another important
highlight of G ROUNDHOG is its enhancement of explain-
ability through the decoupled design of entity proposal and
selection, as outlined earlier in section 2.2. This is exem-
plified in the case study illustrated in Figure 6, which il-
lustrates the mask proposal scoring and selective merging
process of our model. We show the top-4 masks, where the
higher-score masks are labeled in green while the lower-
score masks are labeled in red. Users can easily interpret
that the failure is due to the incapability of MLLM to rec-
ognize the word ‚ÄúKWIK‚Äù, despite it being successfully lo-
calized and proposed as an entity candidate.
4.3. Ablation Studies
We performed ablation studies to validate our design deci-
sions, training, and evaluating a subset of the M3G2 dataset
that includes RefCOCO+, Flickr30K, and TextVQA. These
cover a range of visual entities from various image
sources and granularities. We start by comparing our
Mask2Former+ with the original Mask2Former for mask
proposal effectiveness. As indicated in Table 10, the orig-
14233
Setups
RefCOCO+ Flickr30K TextVQA-X
Mask
Proposal Models
Mask2F
ormer 67.1 69.0 9.8
Mask2Former+ 66.6 77.2 34.0
Entity
Features
CLIP
59.8 75.0 32.0
DINOv2 62.3 76.3 28.4
CLIP+DINOv2 66.6 77.2 34.0
Gr
ounding Query
<GRD> only
64.4 67.5 34.2
</GRD> only 64.4 77.2 33.5
Sum 66.6 77.2 34.0
Eval
Input Resolution
224‚Äì480
54.7 67.2 27.6
480‚Äì640 65.5 76.7 27.6
800‚Äì1024 66.6 77.2 34.0
Table 10. Ablation study on model design choices and evaluation
setups. Models are trained on RefCOCO+, Flickr30K, TextVQA-
X and tested on corresponding validation sets.
inal Mask2Former performs slightly better on RefCOCO,
as it is developed specificity on COCO object categories.
However, Mask2Former+ significantly surpasses the orig-
inal in domains with non-COCO entities. Our second set
of experiments examined the choice of visual entity fea-
tures. Although using either CLIP or DINOv2 features
alone shows advantages in specific datasets, their combina-
tion consistently yields the best results across all datasets.
To obtain a robust grounding query representation, we ex-
perimented with using the output embedding of the <GRD>
token, the </GRD> token, and their sum. We found that
the latter approach achieves the best overall results. Finally,
we demonstrate that our decoupling design of the mask pro-
posal model and MLLM allows for training at a lower reso-
lution (320px) to expedite grounding training, while scaling
up the resolution during evaluation enhances performance.
5. Related Work
Multimodal Large Language Models. Building on the
recent advance of large language models (LLMs), there is
an increasing effort in adapting pretrained large language
models for multimodal tasks, such as understanding and
interpreting visual information [1, 71]. More recently, vi-
sual instruction tuning has gained much interest due to its
surprising performance with a modest amount of data and
computing resources. Various models have been developed,
noticeably MiniGPT4[93] , LLaV A [44, 45] and concurrent
models [13, 20,38,74,83]. Despite their promising per-
formances, MLLMs often produce objects that are not pre-
sented in the given images, a phenomenon referred to as the
object hallucination problem [14, 31,42].
MLLM with Language Grounding The ability to con-
nect language to their corresponding visual elements in
the physical world, known as grounding [27], is crucial
in everyday human communication about our shared sur-roundings. Grounding datasets have been shown to ben-
efit vision-language pre-training, both in terms of object-
level recognition [41] and language learning [50]. Re-
cent works unify text and grounding regions into token se-
quences [49, 73,81] in casual language modeling. Based
on such paradigm, researchers have developed a family
of grounded MLLM, including GPT4ROI [89], Kosmos-
2 [56], Shikra [8], PVIT [7], BuboGPT [91], Qwen-VL [3],
and Ferret [85]. Despite their promising performance, these
models focus on object grounding to bounding boxes, which
cannot handle pixel-level grounding across various seman-
tic granularities. Furthermore, it lacks the diagnosability
and explainability in failure cases. Our model fills this gap.
Language-Guided Semantic Localization The field of
language-guided semantic localization has a long history
in the vision-language research community, requiring that
the model localize a given referring expression with bound-
ing boxes or segmentation masks. This task has evolved
from early attempts to understand simple referring expres-
sions within images, such as the well-known RefCOCO se-
ries [52, 86] and their generalized variant [43] that takes
no-target and multi-target into account. The integration of
advanced language reasoning from LLMs has enabled re-
search to tackle even more nuanced reasoning tasks that
involve complex language contexts [37, 57,87]. Notably,
LISA [37] formulates a reasoning segmentation task to
bring language-informed reasoning into semantic segmen-
tation, and contributes a powerful baseline. Our model
builds on these developments, but is designed to be more
universally applicable as a grounded MLLM.
6. Conclusion
In this study, we introduce G ROUNDHOG , a novel frame-
work designed to enable pixel-level explainable grounding
in large language models, leveraging holistic segmentation.
The system builds upon a pre-trained mask proposal net-
work to provide pixel-level visual features for the large lan-
guage models, allowing them to retrieve segmentation mask
proposals that can be used for grounding. We also present
M3G2, a dataset of 1.9M training text-image pairs with 36
sub-problems derived from 27 existing datasets for visu-
ally grounded instruction tuning, facilitating precise vision-
language alignment at the pixel level. We show that af-
ter training on M3G2, G ROUNDHOG achieves superior per-
formance on various grounding tasks. Through extensive
case studies, we further show that G ROUNDHOG unlocks
explainability and diagnosability, and demonstrates better
grounding towards occluded objects, groups of multiple in-
stances, amorphous background regions, semantic parts of
objects, and objects with irregular shapes.
Acknowledgements This work was supported by Ama-
zon and NSF IIS-1949634. We thank the anonymous re-
viewers for their valuable comments and suggestions.
14234
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. Advances in
Neural Information Processing Systems, 35:23716‚Äì23736,
2022. 8
[2] Peter Anderson, Basura Fernando, Mark Johnson, and
Stephen Gould. Spice: Semantic propositional image cap-
tion evaluation. In Proceedings of the 14th European Con-
ference on Computer Vision, pages 382‚Äì398, 2016. 6
[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-vl: A frontier large vision-language model with
versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 8
[4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
metric for mt evaluation with improved correlation with hu-
man judgments. In Proceedings of the acl workshop on in-
trinsic and extrinsic evaluation measures for machine trans-
lation and/or summarization, pages 65‚Äì72, 2005. 6
[5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-
stuff: Thing and stuff classes in context. In Proceedings of
the IEEE conference on computer vision and pattern recog-
nition, pages 1209‚Äì1218, 2018. 2,4,1
[6] Chongyan Chen, Samreen Anjum, and Danna Gurari.
Grounding answers for visual questions asked by visually
impaired people. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
19098‚Äì19107, 2022. 5,2
[7] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li,
Maosong Sun, and Yang Liu. Position-enhanced visual
instruction tuning for multimodal large language models.
arXiv preprint arXiv:2308.13437, 2023. 8
[8] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi-
modal llm‚Äôs referential dialogue magic. arXiv preprint
arXiv:2306.15195, 2023. 2,3,5,6,8
[9] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Ge-
offrey Hinton. Pix2seq: A language modeling framework
for object detection. In International Conference on Learn-
ing Representations, 2021. 3
[10] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-
der Kirillov, and Rohit Girdhar. Masked-attention mask
transformer for universal image segmentation. In Proceed-
ings of the IEEE/CVF conference on computer vision and
pattern recognition, pages 1290‚Äì1299, 2022. 2,1,4
[11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt quality.
https://vicuna.lmsys.org , 2023. 4
[12] Jifeng Dai, Kaiming He, and Jian Sun. Convolutional feature
masking for joint object and stuff segmentation. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 3992‚Äì4000, 2015. 2
[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, PascaleFung, and Steven Hoi. Instructblip: Towards general-
purpose vision-language models with instruction tuning.
arXiv preprint arXiv:2305.06500, 2023. 2,8
[14] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale
Fung. Plausible may not be faithful: Probing object hallu-
cination in vision-language pre-training. In Proceedings of
the 17th Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 2128‚Äì2140, 2023.
8
[15] Timoth ¬¥ee Darcet, Maxime Oquab, Julien Mairal, and Pi-
otr Bojanowski. Vision transformers need registers. arXiv
preprint arXiv:2309.16588, 2023. 5,4
[16] Daan de Geus, Panagiotis Meletis, Chenyang Lu, Xiaox-
iao Wen, and Gijs Dubbelman. Part-aware panoptic seg-
mentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 5485‚Äì
5494, 2021. 4,1
[17] Harm De Vries, Florian Strub, Sarath Chandar, Olivier
Pietquin, Hugo Larochelle, and Aaron Courville. Guess-
what?! visual object discovery through multi-modal dia-
logue. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 5503‚Äì5512, 2017. 5,
3
[18] Mark Everingham, Luc Van Gool, Christopher KI Williams,
John Winn, and Andrew Zisserman. The pascal visual object
classes (voc) challenge. International journal of computer
vision, 88:303‚Äì338, 2010. 1
[19] Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Bo-
qing Gong. Vqs: Linking segmentations to questions and
answers for supervised attention in vqa and question-focused
semantic segmentation. In Proceedings of the IEEE inter-
national conference on computer vision, pages 1811‚Äì1820,
2017. 5,2
[20] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping
Luo, and Kai Chen. Multimodal-gpt: A vision and lan-
guage model for dialogue with humans. arXiv preprint
arXiv:2305.04790, 2023. 8
[21] Cristina Gonz ¬¥alez, Nicol ¬¥as Ayobi, Isabela Hern ¬¥andez, Jos ¬¥e
Hern ¬¥andez, Jordi Pont-Tuset, and Pablo Arbel ¬¥aez. Panoptic
narrative grounding. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 1364‚Äì1373,
2021. 6
[22] Cristina Gonz ¬¥alez, Nicol ¬¥as Ayobi, Isabela Hern ¬¥andez, Jordi
Pont-Tuset, and Pablo Arbel ¬¥aez. Piglet: Pixel-level ground-
ing of language expressions with transformers. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 2023.
6
[23] Abel Gonzalez-Garcia, Davide Modolo, and Vittorio Ferrari.
Objects as context for detecting their semantic parts. In Pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6907‚Äì6916, 2018. 2
[24] Herbert P Grice. Logic and conversation. In Speech acts,
pages 41‚Äì58. Brill, 1975. 5
[25] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A
dataset for large vocabulary instance segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 5356‚Äì5364, 2019. 4,5,1,3
14235
[26] Kai Han, Yunhe Wang, Jianyuan Guo, Yehui Tang, and En-
hua Wu. Vision gnn: An image is worth graph of nodes. Ad-
vances in Neural Information Processing Systems, 35:8291‚Äì
8303, 2022. 2
[27] Stevan Harnad. The symbol grounding problem. Physica D:
Nonlinear Phenomena, 42(1-3):335‚Äì346, 1990. 8
[28] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-
rank adaptation of large language models. In International
Conference on Learning Representations, 2021. 4
[29] Drew A Hudson and Christopher D Manning. Gqa: A new
dataset for real-world visual reasoning and compositional
question answering. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition, pages
6700‚Äì6709, 2019. 5,2
[30] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-
modulated detection for end-to-end multi-modal understand-
ing. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision, pages 1780‚Äì1790, 2021. 4,6
[31] Osman Semih Kayhan, Bart Vredebregt, and Jan C
Van Gemert. Hallucination in object detection‚Äîa study in
visual part verification. In 2021 IEEE International Confer-
ence on Image Processing (ICIP), pages 2234‚Äì2238. IEEE,
2021. 8
[32] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and
Tamara Berg. Referitgame: Referring to objects in pho-
tographs of natural scenes. In Proceedings of the 2014 con-
ference on empirical methods in natural language processing
(EMNLP), pages 787‚Äì798, 2014. 5,6,2
[33] Seyedalireza Khoshsirat and Chandra Kambhamettu. Sen-
tence attention blocks for answer grounding. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 6080‚Äì6090, 2023. 6
[34] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten
Rother, and Piotr Doll ¬¥ar. Panoptic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition, pages 9404‚Äì9413, 2019. 5,6,1
[35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,
Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and
Ross Girshick. Segment anything. In Proceedings of the
IEEE/CVF International Conference on Computer Vision
(ICCV), pages 4015‚Äì4026, 2023. 2,3
[36] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-
tidis, Li-Jia Li, David A Shamma, et al. Visual genome:
Connecting language and vision using crowdsourced dense
image annotations. International journal of computer vision,
123:32‚Äì73, 2017. 5,3
[37] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui
Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation
via large language model. arXiv preprint arXiv:2308.00692,
2023. 4,5,6,8,2
[38] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint
arXiv:2305.03726, 2023. 8[39] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang,
Linjie Li, Lijuan Wang, and Jianfeng Gao. Multimodal foun-
dation models: From specialists to general-purpose assis-
tants. arXiv preprint arXiv:2309.10020, 2023. 2
[40] Jianshu Li, Jian Zhao, Yunchao Wei, Congyan Lang, Yidong
Li, Terence Sim, Shuicheng Yan, and Jiashi Feng. Multiple-
human parsing in the wild. arXiv preprint arXiv:1705.07206,
2017. 4,1
[41] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
language-image pre-training. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10965‚Äì10975, 2022. 8
[42] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucination in
large vision-language models. In Proceedings of the 2023
Conference on Empirical Methods in Natural Language Pro-
cessing, 2023. 7,8
[43] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gener-
alized referring expression segmentation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 23592‚Äì23601, 2023. 5,6,8,2,3
[44] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744, 2023. 5,8,4
[45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485,
2023. 2,8
[46] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Ku-
mar Satzoda, Vijay Mahadevan, and R Manmatha. Poly-
former: Referring image segmentation as sequential poly-
gon generation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 18653‚Äì
18663, 2023. 6
[47] Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessan-
dro Bissacco, Yasuhisa Fujii, and Michalis Raptis. Towards
end-to-end unified scene text detection and layout analysis.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 1049‚Äì1059, 2022. 5,
3
[48] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017. 4
[49] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mot-
taghi, and Aniruddha Kembhavi. Unified-io: A unified
model for vision, language, and multi-modal tasks. In The
Eleventh International Conference on Learning Representa-
tions, 2022. 8
[50] Ziqiao Ma, Jiayi Pan, and Joyce Chai. World-to-words:
Grounded open vocabulary acquisition through fast mapping
in vision-language models. In Proceedings of the 61st An-
nual Meeting of the Association for Computational Linguis-
tics (Volume 1: Long Papers), pages 524‚Äì544, 2023. 2,8
[51] Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Rus-
sakovsky. Point and ask: Incorporating pointing into visual
question answering. arXiv preprint arXiv:2011.13681, 2020.
5,6,3
14236
[52] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L Yuille, and Kevin Murphy. Generation
and comprehension of unambiguous object descriptions. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 11‚Äì20, 2016. 5,8,3
[53] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Camburu, Alan L Yuille, and Kevin Murphy. Generation
and comprehension of unambiguous object descriptions. In
Proceedings of the IEEE conference on computer vision and
pattern recognition, pages 11‚Äì20, 2016. 5,6,2
[54] Maxime Oquab, Timoth ¬¥ee Darcet, Th ¬¥eo Moutakanni, Huy
V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.
Dinov2: Learning robust visual features without supervision.
arXiv preprint arXiv:2304.07193, 2023. 2
[55] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting of the
Association for Computational Linguistics, pages 311‚Äì318,
2002. 6
[56] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan
Huang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-
ing multimodal large language models to the world. arXiv
preprint arXiv:2306.14824, 2023. 2,3,8
[57] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong,
Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Ling-
peng Kong Tong Zhang. Detgpt: Detect what you need via
reasoning. arXiv preprint arXiv:2305.14167, 2023. 8
[58] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
nik. Flickr30k entities: Collecting region-to-phrase corre-
spondences for richer image-to-sentence models. In Pro-
ceedings of the IEEE international conference on computer
vision, pages 2641‚Äì2649, 2015. 5,6,1
[59] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu
Soricut, and Vittorio Ferrari. Connecting vision and lan-
guage with localized narratives. In Proceedings of the 16th
European Conference on Computer Vision, pages 647‚Äì664,
2020. 1
[60] Lu Qi, Jason Kuen, Tiancheng Shen, Jiuxiang Gu, Wenbo
Li, Weidong Guo, Jiaya Jia, Zhe Lin, and Ming-Hsuan Yang.
High quality entity segmentation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 4047‚Äì4056, 2023. 4,5,1,3
[61] Mengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan
Song, Yao Zhao, and Yunchao Wei. RIO: A benchmark for
reasoning intention-oriented objects in open environments.
InThirty-seventh Conference on Neural Information Pro-
cessing Systems Datasets and Benchmarks Track, 2023. 6,
2
[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning, pages
8748‚Äì8763. PMLR, 2021. 2,5,4
[63] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi
Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Mar-quez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts
and attributes of common objects. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7141‚Äì7151, 2023. 4,1
[64] Varun Nagaraj Rao, Xingjian Zhen, Karen Hovsepian, and
Mingwei Shen. A first look: Towards explainable textvqa
models via visual and textual explanations. arXiv preprint
arXiv:2105.02626, 2021. 5,6,2
[65] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor
Darrell, and Bernt Schiele. Grounding of textual phrases
in images by reconstruction. In Proceedings of the 14th
European Conference on Computer Vision, pages 817‚Äì834.
Springer, 2016. 5,2
[66] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang,
Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards
vqa models that can read. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
8317‚Äì8326, 2019. 4,6
[67] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wo-
jciech Galuba, and Tal Hassner. TextOCR: Towards large-
scale end-to-end reasoning for arbitrary-shaped scene text.
2021. 4,1
[68] Bharat Singh, Hengduo Li, Abhishek Sharma, and Larry S
Davis. R-fcn-3000 at 30fps: Decoupling detection and
classification. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1081‚Äì1090,
2018. 2
[69] Zhi Tian, Chunhua Shen, Xinlong Wang, and Hao Chen.
Boxinst: High-performance instance segmentation with box
annotations. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 5443‚Äì
5452, 2021. 4
[70] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 5,4
[71] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-
lami, Oriol Vinyals, and Felix Hill. Multimodal few-shot
learning with frozen language models. Advances in Neural
Information Processing Systems, 34:200‚Äì212, 2021. 8
[72] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
Parikh. Cider: Consensus-based image description evalua-
tion. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4566‚Äì4575, 2015. 6
[73] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,
Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and
Hongxia Yang. Ofa: Unifying architectures, tasks, and
modalities through a simple sequence-to-sequence learning
framework. In International Conference on Machine Learn-
ing, pages 23318‚Äì23340. PMLR, 2022. 8
[74] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,
Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu
Qiao, et al. Visionllm: Large language model is also an
open-ended decoder for vision-centric tasks. arXiv preprint
arXiv:2305.11175, 2023. 8
[75] Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong
Guo, Mingming Gong, and Tongliang Liu. Cris: Clip-
14237
driven referring image segmentation. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 11686‚Äì11695, 2022. 6
[76] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and
Subhransu Maji. Phrasecut: Language-based image segmen-
tation in the wild. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
10216‚Äì10225, 2020. 4,5,6,2
[77] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen
Lo, and Ross Girshick. Detectron2. https://github.
com/facebookresearch/detectron2, 2019. 1
[78] Yixuan Wu, Zhao Zhang, Chi Xie, Feng Zhu, and Rui Zhao.
Advancing referring expression segmentation beyond single
image. In Proceedings of the IEEE/CVF International Con-
ference on Computer Vision, pages 2628‚Äì2638, 2023. 5,2
[79] Chi Xie, Zhao Zhang, Yixuan Wu, Feng Zhu, Rui Zhao, and
Shuang Liang. Described object detection: Liberating object
detection with flexible expressions. In Thirty-seventh Con-
ference on Neural Information Processing Systems , 2023. 5,
2
[80] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Ze-
huan Yuan, and Huchuan Lu. Universal instance percep-
tion as object discovery and retrieval. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pages 15325‚Äì15336, 2023. 6
[81] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,
Faisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.
Unitab: Unifying text and box outputs for grounded vision-
language modeling. 2022. 8
[82] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Heng-
shuang Zhao, and Philip HS Torr. Lavt: Language-aware
vision transformer for referring image segmentation. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 18155‚Äì18165, 2022. 6
[83] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178, 2023. 8
[84] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,
Tong Xu, and Enhong Chen. A survey on multimodal large
language models. arXiv preprint arXiv:2306.13549, 2023. 2
[85] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen
Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and
Yinfei Yang. Ferret: Refer and ground anything anywhere
at any granularity. arXiv preprint arXiv:2310.07704, 2023.
3,6,8
[86] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,
and Tamara L Berg. Modeling context in referring expres-
sions. In Computer Vision‚ÄìECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14,
2016, Proceedings, Part II 14, pages 69‚Äì85. Springer, 2016.
4,5,8,3
[87] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and
Chen Change Loy. Contextual object detection with
multimodal large language models. arXiv preprint
arXiv:2305.18279, 2023. 8[88] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
From recognition to cognition: Visual commonsense rea-
soning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 6720‚Äì6731,
2019. 5,3
[89] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi
Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-
struction tuning large language model on region-of-interest.
arXiv preprint arXiv:2307.03601, 2023. 8
[90] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up
visual instruction tuning. arXiv preprint arXiv:2307.04087 ,
2023. 5,3
[91] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi
Feng, and Bingyi Kang. Bubogpt: Enabling visual ground-
ing in multi-modal llms. arXiv preprint arXiv:2307.08581,
2023. 8
[92] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-
yuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,
Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-
based language-image pretraining. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16793‚Äì16803, 2022. 2
[93] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592, 2023. 8
[94] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.
Visual7w: Grounded question answering in images. In Pro-
ceedings of the IEEE conference on computer vision and pat-
tern recognition, pages 4995‚Äì5004, 2016. 5,6,3
14238
