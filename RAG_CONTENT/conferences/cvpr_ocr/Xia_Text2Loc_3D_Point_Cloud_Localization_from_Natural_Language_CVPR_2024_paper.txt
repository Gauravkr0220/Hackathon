Text2Loc: 3D Point Cloud Localization from Natural Language
Yan Xiaâˆ—1,2â€ 
Letian Shiâˆ—1Zifeng Ding3JoËœao F. Henriques4Daniel Cremers1,2
1Technical University of Munich2Munich Center for Machine Learning (MCML)
3LMU Munich4Visual Geometry Group, University of Oxford
{yan.xia, letian.shi, cremers }@tum.de, zifeng.ding@campus.lmu.de, joao@robots.ox.ac.uk
Hi, I am standing on the west of a green building,east of a green road,west of a black garage...Got it! Coming soon! 
Localization Recall (%)Number of top retrievals
Figure 1. (Left) We introduce Text2Loc, a solution designed for city-scale position localization using textual descriptions. When provided
with a point cloud representing the surroundings and a textual query describing a position, Text2Loc determines the most probable location
of that described position within the map. (Right) Localization performance on the KITTI360Pose test set. The proposed Text2Loc achieves
consistently better performance across all top retrieval numbers. Notably, it outperforms the best baseline by up to 2 times, localizing text
queries below 5 m.
Abstract
We tackle the problem of 3D point cloud localization
based on a few natural linguistic descriptions and intro-
duce a novel neural network, Text2Loc, that fully interprets
the semantic relationship between points and text. Text2Loc
follows a coarse-to-fine localization pipeline: text-submap
global place recognition, followed by fine localization. In
global place recognition, relational dynamics among each
textual hint are captured in a hierarchical transformer with
max-pooling (HTM), whereas a balance between positive
and negative pairs is maintained using text-submap con-
trastive learning. Moreover, we propose a novel matching-
free fine localization method to further refine the location
predictions, which completely removes the need for compli-
cated text-instance matching and is lighter, faster, and more
accurate than previous methods. Extensive experiments
show that Text2Loc improves the localization accuracy by
up to 2Ã—over the state-of-the-art on the KITTI360Pose
dataset. Our project page is publicly available at https:
//yan-xia.github.io/projects/text2loc/ .
â€ Corresponding author. * Equal contribution.1. Introduction
3D localization [18, 28] using natural language descrip-
tions in a city-scale map is crucial for enabling autonomous
agents to cooperate with humans to plan their trajecto-
ries [11] in applications such as goods delivery or vehicle
pickup [36, 38]. When delivering a takeaway, couriers of-
ten encounter the â€œlast mile problemâ€. Pinpointing the ex-
act delivery spot in residential neighborhoods or large office
buildings is challenging since GPS signals are bound to fail
among tall buildings and vegetation [34, 37]. Couriers of-
ten rely on voice instructions over the phone from the recip-
ient to determine this spot. More generally, the â€œlast mile
problemâ€ occurs whenever a user attempts to navigate to an
unfamiliar place. It is therefore essential to develop the ca-
pability to perform localization from the natural language,
as shown in Fig. 1.
As a possible remedy, we can match linguistic descrip-
tions to a pre-built point cloud map using calibrated depth
sensors like LiDAR. Point cloud localization, which focuses
on the sceneâ€™s geometry, offers several advantages over im-
ages. It remains consistent despite lighting, weather, and
season changes, whereas the same geometric structure in
images might appear vastly different.
The main challenge of 3D localization from natural
language descriptions lies in accurately interpreting the
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
14958
language and semantically understanding large-scale point
clouds. To date, only a few networks have been pro-
posed for language-based localization in a 3D large-scale
city map. Text2Pose [12] is a pioneering work that aligns
objects described in text with their respective instances in
a point cloud, through a coarse-to-fine approach. In the
coarse stage, Text2Pose first adopts a text-to-cell cross-
model retrieval method to identify the possible regions that
contain the target position. In particular, Text2Pose matches
the text and the corresponding submaps by the global de-
scriptors from 3D point clouds using PointNet++ [20] and
the global text descriptors using a bidirectional LSTM cell
[10, 25]. This method describes a submap with its contained
instances of objects, which ignores the instance relation-
ship for both points and sentences. Recently, the authors
of RET [33] noted this shortcoming and designed Relation-
Enhanced Transformer networks. While this results in bet-
ter global descriptors, both approaches match global de-
scriptors using the pairwise ranking loss without consider-
ing the imbalance in positive and negative samples.
Inspired by RET [33], we also notice the importance of
effectively leveraging relational dynamics among instances
within submaps for geometric representation extraction.
Furthermore, there is a natural hierarchy in the descriptions,
composed of sentences, each with word tokens. We thus
recognize the need to analyze relationships within (intra-
text) and between (inter-text) descriptions. To address these
challenges, we adopt a frozen pre-trained large language
model T5 [23] and design a hierarchical transformer with
max-pooling (HTM) that acts as an intra- and inter-text en-
coder, capturing the contextual details within and across
sentences. Additionally, we enhance the instance encoder
in Text2Pose [12] by adding a number encoder and adopt-
ing contrastive learning to maintain a balance between pos-
itive and negative pairs. Another observation is that, when
refining the location prediction in the fine localization stage,
the widely used text-instance matching module in previous
methods should be reduced since the noisy matching or in-
accurate offset predictions are a fatal interference in pre-
dicting the exact position of the target. To address this is-
sue, we propose a novel matching-free fine localization net-
work. Specifically, we first design a prototype-based map
cloning (PMC) module to increase the diversity of retrieved
submaps. Then, we introduce a cascaded cross-attention
transformer (CCAT) to enrich the text embedding by fusing
the semantic information from point clouds. These opera-
tions enable one-stage training to directly predict the target
position without any text-instance matcher.
To summarize, the main contributions of this work are:
â€¢ We focus on the relatively-understudied problem of point
cloud localization from textual descriptions, to address
the â€œlast mile problemâ€.
â€¢ We propose a novel attention-based method that is hierar-chical and represents contextual details within and across
sentence descriptions of places.
â€¢ We study the importance of positive-negative pairs bal-
ance in this setting, and show how contrastive learning is
an effective tool that significantly improves performance.
â€¢ We are the first to completely remove the usage of text-
instance matcher in the final localization stage. We pro-
pose a lighter and faster localization model while still
achieving state-of-the-art performance via our designed
prototype-based map cloning (PMC) module in training
and cascaded cross-attention transformer (CCAT).
â€¢ We conduct extensive experiments on the KITTI360Pose
benchmark [12] and show that the proposed Text2Loc
greatly improves over the state-of-the-art methods.
2. Related work
To date, only a few networks have been proposed for the
natural language based 3D localization in a large-scale out-
door scene. Other tasks that are related to ours include 2D
visual localization, 3D point cloud based localization, and
3D understanding with language.
2D visual localization. Visual localization in 2D images
has wide-ranging applications from robotics to augmented
reality. Given a query image or image sequence, the aim
is to predict an accurate pose. One of the early works,
Scale-Invariant Feature Transform (SIFT) [15], proposes
the use of distinctive invariant features to match objects
across different viewpoints, forming a basis for 2D local-
ization. Oriented FAST and Rotated BRIEF (ORB) [24]
has been pivotal in achieving robustness against scale, ro-
tation, and illumination changes in 2D localization tasks.
Recent learning-based methods [26, 29] commonly adopt a
coarse-to-fine pipeline. In the coarse stage, given a query
image, place recognition is performed as nearest neighbor
search in high-dimensional spaces. Subsequent to this, a
pixel-wise correspondence is ascertained between the query
and the retrieved image, facilitating precise pose prediction.
However, the performance of image-based methods often
degrades when facing drastic variations in illumination and
appearance caused by weather and seasonal changes. Com-
pared to feature matching in 2D visual localization, in this
work, we aim to solve cross-model localization between
text and 3D point clouds.
3D point cloud based localization. With breakthroughs in
learning-based image localization methods, deep learning
of 3D localization has become the focus of intense research.
Similar to image-based methods, a two-step pipeline is
commonly used in 3D localization: (1) place recognition,
followed by (2) pose estimation. PointNetVlad [2] is a pio-
neering network that tackles 3D place recognition with end-
to-end learning. Subsequently, SOE-Net [35] introduces the
PointOE module, incorporating orientation encoding into
14959
Text descriptionsð‘‡!: The pose is on-top of a gray road. ð‘‡!!"#$%"&'(%")("*%(+"',"-"./-0"&-/1)2.3"ð‘‡"!"#$%"&'(%")("%-(+"',"-"4-/15./%%2"6%.%+-+)'2ð‘‡#!"#$%"&'(%")("('7+$"',"-"./-05./%%2"&'8%3"ð‘‡$!"#$%"&'(%")("2'/+$"',"-"98-:1"&'8%3"ð‘‡%!"#$%"&'(%")("*%(+"',"-"98-:1"6%.%+-+)'23
Global place recognitionTop-k submaps
Instances in retrieval submaps
Predicted PositionText-to-submapretrievalPositionestimationFine localizationFigure 2. The proposed Text2Loc architecture. It consists of two tandem modules: Global place recognition and Fine localization. Global
place recognition. Given a text-based position description, we first identify a set of coarse candidate locations, â€submaps,â€ potentially
containing the target position. This is achieved by retrieving the top-k nearest submaps from a previously constructed database of submaps
using our novel text-to-submap retrieval model. Fine localization. We then refine the center coordinates of the retrieved submaps via our
designed matching-free position estimation module, which adjusts the target location to increase accuracy.
PointNet to generate point-wise local descriptors. Further-
more, various methods [3, 6, 8, 16, 17, 40, 41] have ex-
plored the integration of different transformer networks,
specifically stacked self-attention blocks, to learn long-
range contextual features. In contrast, Minkloc3D [13] em-
ploys a voxel-based strategy to generate a compact global
descriptor using a Feature Pyramid Network [14] (FPN)
with generalized-mean (GeM) pooling [21]. However, the
voxelization methods inevitably suffer from lost points due
to the quantization step. CASSPR [37] thus introduces a
dual-branch hierarchical cross attention transformer, com-
bining both the advantages of voxel-based approaches with
the point-based approaches. After getting the coarse loca-
tion of the query scan, the pose estimation can be computed
with the point cloud registration algorithms, like the itera-
tive closest point (ICP) [30] or autoencoder-based registra-
tion [7]. By contrast to point cloud based localization, we
use natural language queries to specify any target location.
3D vision and language. Recent work has explored the
cross-modal understanding of 3D vision and language. [19]
bridges language implicitly to 3D visual feature represen-
tations and predicts 3D bounding boxes for target objects.
Methods [1, 4, 9, 39] locate the most relevant 3D target
objects in a raw point cloud scene given by the query text
descriptions. However, these methods focus on real-world
indoor scene localization. Text2Pos [12] is the first attempt
to tackle the large city-scale outdoor scene localization task,
which identifies a set of coarse locations and then refines the
pose estimation. Following this, Wang et al. [33] propose
a Transformer-based method to enhance representation dis-
criminability for both point clouds and textual queries.
3. Problem statement
We begin by defining the large-scale 3D map
Mref={mi:i= 1, ..., M }to be a collection of cu-
bic submaps mi. Each submap mi={Pi,j:j= 1, ..., p}
includes a set of 3D object instances Pi,j. LetTbe a query
text description consisting of a set of hints {âƒ—hk}h
k=1, each
describing the spatial relation between the target location
and an object instance. Following [12], we approach thistask in a coarse-to-fine manner. The text-submap global
place recognition involves the retrieval of submaps based
onT. This stage aims to train a function F, which encodes
bothTand a submap minto a unified embedding space.
In this space, matched query-submap pairs are brought
closer together, while unmatched pairs are repelled. In
fine-grained localization, we employ a matching-free
network to directly regress the final position of the target
based on Tand the retrieved submaps. Thus, the task of
training a 3D localization network from natural language is
defined as identifying the ground truth position (x, y)(2D
planar coordinates w.r.t. the scene coordinate system) from
Mref:
min
Ï•,FE
(x,y,T )âˆ¼D(x, y)âˆ’Ï•
T,argmin
mâˆˆMrefd(F(T), F(m))2
(1)
where d(Â·,Â·)is a distance metric (e.g. the Euclidean dis-
tance), Dis the dataset, and Ï•is a neural network that is
trained to output fine-grained coordinates from a text em-
bedding Tand a submap m.
4. Methodology
Fig. 2 shows our Text2Loc architecture. Given a text-based
query position description, we aim to find a set of coarse
candidate submaps that potentially contain the target posi-
tion by using a frozen pre-trained T5 language model [23]
and an intra- and inter-text encoder with contrastive learn-
ing, described in Section 4.1. Next, we refine the location
based on the retrieved submaps via a designed fine localiza-
tion module, which will be explained in Section 4.2. Sec-
tion 4.3 describes the training with the loss function.
4.1. Global place recognition
3D point cloud-based place recognition is usually expressed
as a 3D retrieval task. Given a query LiDAR scan, the aim is
to retrieve the closest match and its corresponding location
from the database by matching its global descriptor against
the global descriptors extracted from a database of reference
scans based on their descriptor distances. Following this
general approach, we adopt the text-submap cross-modal
14960
Textural position descriptionsPretrained T5modelIntra- and inter-text encoder
Max Poolingð‘†!Â·ð‘‡!ð‘†!Â·ð‘‡"ð‘†!Â·ð‘‡#â‹¯ð‘†!Â·ð‘‡$ð‘†"Â·ð‘‡!ð‘†"Â·ð‘‡"ð‘†"Â·ð‘‡#â‹¯ð‘†"Â·ð‘‡$ð‘†#Â·ð‘‡!ð‘†#Â·ð‘‡"ð‘†#Â·ð‘‡#â‹¯ð‘†#Â·ð‘‡$â‹®â‹®â‹®â‹±â‹®ð‘†$Â·ð‘‡!ð‘†%Â·ð‘‡"ð‘†%Â·ð‘‡#â‹¯ð‘†$Â·ð‘‡$ð‘‡!ð‘‡"â€¦ð‘‡#ð‘‡$â‹®S!S"S#S$â€¦
Instances in submapsâ€¦
Multi-head Transformer 
Max PoolingInstanceencoderAttention+Max-pooling
Instance point (XYZ, RGB)Instance CenterInstance ColorPointNet++Color EncoderInstance NumberCenter EncoderNum.Encoder+ProjectionFeature ð¹%!ð‘ƒ!ð‘ƒ"ð‘ƒ&ð‘†!,ð‘†",â‹¯ð‘†$ð‘‡!,ð‘‡",â‹¯ð‘‡$
Intra- and inter-text encoderInstance encoder
Multi-head Transformer â†Figure 3. (top) The architecture of global place recognition, (bot-
tom) instance encoder architecture for point clouds, and the archi-
tecture of intra- and inter-text encoder. Note that the pre-trained
T5 model is frozen.
global place recognition for coarse localization. With this
stage, we aim to retrieve the nearest submap in response to
a textual query. The main challenge lies in how to find si-
multaneously robust and distinctive global descriptors for
3D submaps Sand textual queries T. Similar to [12, 33],
we employ a dual branch to encode SandTinto a shared
embedding space, as shown in Fig. 3 (top).
Text branch. We initially use a frozen pre-trained large
language model, T5 [23], to extract nuanced features from
textual descriptions, enhancing the embedding quality. We
then design a hierarchical transformer with max-pooling
layers to capture the contextual details within sentences
(via self-attention) and across them (via the semantics that
are shared by all sentences), as depicted in Fig. 3 (Bot-
tom right). Each transformer is a residual module compris-
ing Multi-Head Self-Attention (MHSA) and FeedForward
Network (FFN) sublayers. The feed-forward network com-
prises two linear layers with the ReLU activation function.
More details are in the Supplementary Materials.
3D submap branch. Each instance Piin the submap
SNis represented as a point cloud, containing both spa-
tial and color (RGB) coordinates, resulting in 6D features
(Fig. 3 (bottom left)). We utilize PointNet++ [20] (which
can be replaced with a more powerful encoder) to extract
semantic features from the points. Additionally, we obtain
a color embedding by encoding RGB coordinates with our
color encoder and a positional embedding by encoding the
instance center Â¯Pi(i.e., the mean coordinates) with our po-
sitional encoder. We find that object categories consistently
differ in point counts; for example, roads typically ( >1000
points) have a higher point count than poles ( <500points).
We thus design a number encoder, providing potential class-
specific prior information by explicitly encoding the point
numbers. All the color, positional, and number encoders are
3-layer multi-layer perceptrons (MLPs) with output dimen-
sions matching the semantic point embedding dimension.
Text descriptions
Instances in a submap Pretrained T5modelInstanceencoder
Cascaded Cross AttentionMLPPredicted PositionPMC
â‹¯
SubmapMap Variant GenerationMap PickingRandom selectPrototype-based map cloning (PMC)Cascaded Cross Attentionâ†Attention+Max-poolingFigure 4. The proposed matching-free fine localization architec-
ture. It consists of two parallel branches: one is extracting fea-
tures from query text descriptions (top) and another is using the
instance encoder to extract point cloud features (bottom) . Cas-
caded cross-attention transformers (CCAT) use queries from one
branch to look up information in the other branch, aiming to fuse
the semantic information from point clouds into the text embed-
ding. The result is then processed with a simple MLP to directly
estimate the target position.
We merge the semantic, color, positional, and quantity em-
beddings through concatenation and process them with a
projection layer, another 3-layer MLP. This projection layer
produces the final instance embedding Fpi. Finally, we
aggregate in-submap instance descriptors {Fpi}Np
i=1into a
global submap descriptor FSusing an attention layer [35]
followed by a max pooling operation.
Text-submap Contrastive learning. We introduce a
cross-modal contrastive learning objective to address the
limitations of the widely used pairwise ranking loss in
[12, 33]. This objective aims to jointly drive closer the fea-
ture centroids of 3D submaps and the corresponding text
prompt. In our overall architecture, illustrated in Figure 3,
we incorporate both a text encoder and a point cloud en-
coder. These encoders serve the purpose of embedding the
text-submap pairs into text features denoted as FTâˆˆ R1Ã—C
and 3D submap features represented as FSâˆˆ R1Ã—C, re-
spectively. Here, Csignifies the embedding dimension. In-
spired by CLIP [22], we computer the feature distance be-
tween language descriptions and 3D submaps with a con-
trastive learning loss (See Sec. 4.3 for details).
4.2. Fine localization
Following the text-submap global place recognition, we aim
to refine the target location prediction within the retrieved
submaps in fine localization. Although the final localiza-
tion network in previous methods [12, 33] achieved notable
success using a text-submap matching strategy, the inherent
ambiguity in the text descriptions significantly impeded ac-
curate offset predictions for individual object instances. To
address this issue, we propose a novel matching-free fine
localization network, as shown in Fig. 4. The text branch
(top) captures the fine-grained features by using a frozen
pre-trained language model T5 [23] and an attention unit
followed by a max pooling layer. The submap branch (bot-
14961
tom) performs a prototype-based map cloning module to in-
crease more map variants and then extracts the point cloud
features using an instance encoder, the same as in the global
place recognition. We then fuse the text-submap feature
with a Cascaded Cross-Attention Transformer and finally
regress the target position via a simple MLP.
Cascaded Cross-attention Transformer (CCAT). To
efficiently exploit the relationship between the text branch
and the 3D submap branch, we propose a CCAT to fuse the
features from the two branches. The CCAT consists of two
Cross Attention Transformers (CAT), each is the same as
in [37]. The CAT1 takes the point cloud features as Query
and the text features as Key and Value. It extracts text fea-
tures with reference to the point features and outputs point
feature maps that are informed by the text features. Con-
versely, CAT2 produces enhanced text features by taking
the text features as the Query and the enhanced point cloud
features from CAT1 as the Key and Value. Notably, the
CAT1 and the CAT2 are a cascading structure, which is the
main difference from the HCAT in [37]. In this work, two
cascaded CCATs are used. More ablation studies and anal-
yses are in the Supplementary Materials.
Prototype-based Map Cloning (PMC). To produce
more effective submap variants for training, we propose a
novel prototype-based map cloning module. For each pair
{Ti, Si}, we hope to generate a collection Giof surrounding
map variants centered on the current map Si, which can be
formulated as follows:
Gi={Sj|Â¯sjâˆ’Â¯si
âˆž< Î±,Â¯sjâˆ’ci
âˆž< Î²}, (2)
where Â¯si,Â¯sjare the center coordinates of the submaps Si
andSjrespectively. cirepresents the ground-truth target
position described by Ti,Î±andÎ²are the pre-defined thresh-
olds. In this work, we set Î±= 15 andÎ²= 12 .
In practice, we find that certain submaps in Gihave an
insufficient number of object instances corresponding to the
textual descriptions Ti. To address this, we introduce a fil-
tering process by setting a minimum threshold Nm= 1.
This threshold implies that at most one instance mismatch
is permissible. After applying this filter, we randomly se-
lected a single submap from the refined Gifor training.
4.3. Loss function
Global place recognition. Different from the pairwise
ranking loss widely used in previous methods [12, 33], we
train the proposed method for text-submap retrieval with a
cross-model contrastive learning objective. Given an input
batch of 3D submap descriptors {FS
i}N
i=1and matching text
descriptors {FT
i}N
i=1where Nis the batch size, the con-trastive loss among each pair is computed as follows,
l(i, T, S ) =âˆ’logexp(FT
iÂ·FS
i/Ï„)P
jâˆˆNexp(FT
iÂ·FS
j/Ï„)âˆ’logexp(FS
iÂ·FT
i/Ï„)P
jâˆˆNexp(FS
iÂ·FT
j/Ï„),
(3)
where Ï„is the temperature coefficient, similar to CLIP [22].
Within a training mini-batch, the text-submap alignment ob-
jective L(T, S)can be described as:
L(T, S) =1
N"X
iâˆˆNl(i, T, S )#
. (4)
Fine localization. Unlike previous method [12, 33], our
fine localization network does not include a text-instance
matching module, making our training more straightfor-
ward and faster. Note that this model is trained separately
from the global place recognition. Here, our goal is to min-
imize the distance between the predicted location of the tar-
get and the ground truth. In this paper, we use only the mean
squared error loss Lrto train the translation regressor.
L(Cgt, Cpred) =Cgtâˆ’Cpred
2, (5)
where Cpred= (x, y)(see Eq. (1)) is the predicted target
coordinates, and Cgtis the ground-truth coordinates.
5. Experiments
5.1. Benchmark Dataset
We train and evaluate the proposed Text2Loc on the
KITTI360Pose benchmark presented in [12]. It includes
point clouds of 9 districts, covering 43,381 position-query
pairs with a total area of 15.51 km2. Following [12], we
choose five scenes (11.59 km2) for training, one for valida-
tion, and the remaining three (2.14 km2) for testing. The
3D submap is a cube that is 30m long with a stride of 10m.
This creates a database with 11,259/1,434/4,308 submaps
for training/validation/testing scenes and a total of 17,001
submaps for the entire dataset. For more details, please re-
fer to the supplementary material in [12].
5.2. Evaluation criteria
Following [12], we use Retrieve Recall at Top k(kâˆˆ
{1,3,5}) to evaluate text-submap global place recognition.
For assessing localization performance, we evaluate with
respect to the top kretrieved candidates ( kâˆˆ {1,5,10})
and report localization recall. Localization recall measures
the proportion of successfully localized queries if their er-
ror falls below specific error thresholds, specifically Ïµ <
5/10/15mby default.
5.3. Results
5.3.1 Global place recognition
We compare our Text2Loc with the state-of-the-art meth-
ods: Text2Pos [12] and RET [33]. We evaluate global place
14962
Localization Recall ( Ïµ <5/10/15m)â†‘
Methods Validation Set Test Set
k= 1 k= 5 k= 10 k= 1 k= 5 k= 10
Text2Pos [12] 0.14/0.25/0.31 0.36/0.55/0.61 0.48/0.68/0.74 0.13/0.21/0.25 0.33/0.48/0.52 0.43/0.61/0.65
RET [33] 0.19/0.30/0.37 0.44/0.62/0.67 0.52/0.72/0.78 0.16/0.25/0.29 0.35/0.51/0.56 0.46/0.65/0.71
Text2Loc (Ours) 0.37/0.57/0.63 0.68 /0.85/0.87 0.77 /0.91/0.93 0.33 /0.48/0.52 0.61 /0.75/0.78 0.71 /0.84/0.86
Table 1. Performance comparison on the KITTI360Pose benchmark [12].
Submap Retrieval Recall â†‘
Methods Validation Set Test Set
k= 1 k= 3 k= 5 k= 1 k= 3 k= 5
Text2Pos [12] 0.14 0.28 0.37 0.12 0.25 0.33
RET [33] 0.18 0.34 0.44 - - -
Text2Loc (Ours) 0.32 0.56 0.67 0.28 0.49 0.58
Table 2. Performance comparison for gloabl place recognition on
the KITTI360Pose benchmark [12]. Note that only values that are
available in RET [33] are reported.
recognition performance on the KITTI360Pose validation
and test set for a fair comparison. Table 2 shows the top-
1/3/5 recall of each method. The best performance on the
validation set reaches the recall of 0.32 at top-1. Notably,
this outperforms the recall achieved by the current state-of-
the-art method RET by a wide margin of 78% . Further-
more, Text2Loc achieves recall rates of 0.56 and 0.67 at
top-3 and top-5, respectively, representing substantial im-
provements of 65% and 52% relative to the performance
of RET. These improvements are also observed in the test
set, indicating the superiority of the method over baseline
approaches. Note that we report only the values available
in the original publication of RET. These improvements
demonstrate the efficacy of our proposed Text2Loc in cap-
turing cross-model local information and generating more
discriminative global descriptors. More qualitative results
are given in Section 6.2.
5.3.2 Fine localization
To improve the localization accuracy of the network, [12,
33] further introduce fine localization. To make the com-
parisons fair, we follow the same setting in [12, 33] to train
our fine localization network. As illustrated in Table 1, we
report the top- k(k= 1/5/10)recall rate of different er-
ror thresholds Ïµ < 5/10/15mfor comparison. Text2Loc
achieves the top-1 recall rate of 0.37 on the validation set
and 0.33 on the test set under error bound Ïµ <5m, which
are95%and2Ã—higher than the previous state-of-the-art
RET, respectively. Furthermore, our Text2Loc performs
consistently better when relaxing the localization error con-
straints or increasing k. This demonstrates that Text2Loc
can accurately interpret the text descriptions and semanti-
cally understand point clouds better than the previous state-of-the-art methods. We also show some qualitative results
in Section 6.2.
6. Performance analysis
6.1. Ablation study
The following ablation studies evaluate the effectiveness of
different components of Text2Loc, including both the text-
submap global place recognition and fine localization.
Global place recognition. To assess the relative contri-
bution of each module, we remove the frozen pre-trained
large language model T5, hierarchical transformer with
max-pooling (HTM) module in the text branch, and number
encoder in the 3D submap branch from our network one by
one. We also analyze the performance of the proposed text-
submap contrastive learning. All networks are trained on
the KITTI360Pose dataset, with results shown in Table. 3.
Utilizing the frozen pre-trained LLM T5, we observed an
approximate 8% increase in retrieval accuracy at top 1 on
the test set. While the HTM notably enhances performance
on the validation set, it shows marginal improvements on
the test set. Additionally, integrating the number encoder
has led to a significant 6% improvement in the recall metric
at top 1 on the validation set. Notably, the performance on
the validation/test set reaches 0.32/0.28 recall at top 1, ex-
ceeding the same model trained with the pairwise ranking
loss by 52% and 40%, respectively, highlighting the superi-
ority of the proposed contrastive learning approach.
Fine localization. To analyze the effectiveness of
each proposed module in our matching-free fine-grained
localization, we separately evaluate the Cascaded Cross-
Attention Transformer (CCAT) and Prototype-based Map
Cloning (PMC) module, denoted as Text2Loc CCAT and
Text2Loc PMC. For a fair comparison, all methods utilize
the same submaps retrieved from our global place recogni-
tion. The results are shown in Table. 4. Text2Pos* signifi-
cantly outperforms the origin results of Text2Pos [12], indi-
cating the superiority of our proposed global place recogni-
tion. Notably, replacing the matcher in Text2Pos [12] with
our CCAT results in about 7% improvements at top 1 on
the test set. We also observe the inferior performance of
Text2Loc PMC to the proposed method when interpreting
only the proposed PMC module into the Text2Pos [12] fine
14963
MethodsSubmap Retrieval Recall â†‘
Validation Set Test Set
k= 1 k= 3 k= 5 k= 1 k= 3 k= 5
w/o T5 0.29 0.53 0.65 0.26 0.45 0.54
w/o HTM 0.30 0.54 0.65 0.28 0.48 0.57
w/o CL 0.21 0.42 0.53 0.20 0.36 0.45
w/o NE 0.30 0.52 0.63 0.27 0.47 0.56
Full (Ours) 0.32 0.56 0.67 0.28 0.49 0.58
Table 3. Ablation study of the global place recognition on
KITTI360Pose benchmark. â€w/o T5â€ indicates replacing the
frozen pre-trained T5 model with the LSTM in [12]. â€w/o HTMâ€
indicates removing the proposed hierarchical transformer with
max-pooling (HTM). â€w/o CLâ€ indicates replacing the proposed
contrastive learning with the widely used pairwise ranking loss.
â€w/o NEâ€ indicates reducing the number encoder in the instance
encoder of 3D submap branch.
localization network. The results are consistent with our
expectations since PMC can lead to the loss of object in-
stances in certain submaps (See Supp.). Combining both
modules achieves the best performance, improving the per-
formance by 10% at top 1 on the test set. This demonstrates
adding more training submaps by PMC is beneficial for our
matching-free strategy without any text-instance matches.
6.2. Qualitative analysis
In addition to quantitative results, we show some qualita-
tive results of two correctly point cloud localization from
text descriptions and one failure case in Fig. 5. Given a
query text description, we visualize the ground truth, top-3
retrieved submaps, and our fine localization results. In text-
submap global place recognition, a retrieved submap is de-
fined as positive if it contains the target location. Text2Loc
excels in retrieving the ground truth submap or those near
in most cases. However, there are instances where nega-
tive submaps are retrieved, as observed in (b) with the top
3. Text2Loc showcases its ability to predict more accurate
locations based on positively retrieved submaps in fine lo-
calization. We also present one failure case in (c), where all
retrieved submaps are negative. In these scenarios, our fine
localization struggles to predict accurate locations, high-
lighting its reliance on the coarse localization stage. An
additional observation is that despite their distance from the
target location, all these negative submaps contain instances
similar to the ground truth. These observations show the
challenge posed by the low diversity of outdoor scenes, em-
phasizing the need for highly discriminative representations
to effectively disambiguate between submaps.
6.3. Computational cost analysis
In this section, we analyze the required computational re-
sources of our coarse and matching-free fine localization
network regarding the number of parameters and time ef-Localization Recall ( Ïµ <5m)â†‘
Methods Validation Set Test Set
k= 1 k= 5 k= 10 k= 1 k= 5 k= 10
Text2Pos [12] 0.14 0.36 0.48 0.13 0.33 0.43
Text2Pos* 0.33 0.65 0.75 0.30 0.58 0.67
Text2Loc CCAT 0.32 0.64 0.74 0.32 0.60 0.70
Text2Loc PMC 0.32 0.64 0.74 0.29 0.56 0.66
Text2Loc (Ours) 0.37 0.68 0.77 0.33 0.61 0.71
Table 4. Ablation study of the fine localization on the
KITTI360Pose benchmark. * indicates the fine localization net-
work from Text2Pose [12], and the submaps retrieved through our
global place recognition. Text2Loc CCAT indicates the removal
of only the PMC while retaining the CCAT in our network. Con-
versely, Text2Loc PMC keeps the PMC but replaces the CCAT
with the text-instance matcher in Text2Pos.
Methods Parameters (M) Runtime (ms) Localization Recall
Text2Loc Matcher 2.08 43.11 0.30
Text2Loc (Ours) 1.06 2.27 0.33
Table 5. Computational cost requirement analysis of our fine lo-
calization network on the KITTI360Pose test dataset.
ficiency. For a fair comparison, all methods are tested on
the KITTI360Pose test set with a single NVIDIA TITAN X
(12G) GPU. Text2Loc takes 22.75 ms and12.37 ms to ob-
tain a global descriptor for a textual query and a submap
respectively, while Text2Pos [12] achieves it in 2.31 ms and
11.87 ms . Text2Loc has more running time for the text
query due to the extra frozen T5 ( 21.18 ms ) and HTM mod-
ule (1.57 ms ). Our text and 3D networks have 13.65 M
(without T5) and 1.84 M parameters respectively. For fine
localization, we replace the proposed matching-free CCAT
module with the text-instance matcher in [12, 33], denoted
as Text2Loc Matcher. From Table. 5, we observe that
Text2Loc is nearly two times more parameter-efficient than
the baselines [12, 33] and only uses their 5 %inference time.
The main reason is that the previous methods adopt Super-
glue [27] as a matcher, which resulted in a heavy and time-
consuming process. Besides, our matching-free architec-
ture prevents us from running the Sinkhorn algorithm [5].
These improvements significantly enhance the networkâ€™s ef-
ficiency without compromising its performance.
6.4. Robustness analysis
In this section, we analyze the effect of text changes on
localization accuracy. For a clear demonstration, we only
change one sentence in the query text descriptions, de-
noted as Text2Loc modified. All networks are evaluated on
the KITTI360Pose test set, with results shown in Table. 6.
Text2Loc modified only achieves the recall of 0.15 at top-1
retrieval, indicating our text-submap place recognition net-
work is very sensitive to the text embedding. We also ob-
serve the inferior performance of Text2Loc modified in the
14964
The pose is on topof a gray road. The pose is east of a beige sidewalk. The pose is south of a beige wall. The pose is west of a black fence. The pose is west of black vegetation. The pose is north of a black terrain.
The pose is on topof a gray-green road.The pose is north of a gray sidewalk. The pose is on topof a gray parking.The pose is east of gray vegetation.The pose is north of a gray-green terrain. The pose is east of a dark-greenpole.
The pose is east of a gray road. The pose is on topof a gray sidewalk.The pose is on topof black vegetation. The pose is south of a dark-green terrain. The pose is south of a dark-green pole. The pose is south of a dark-green traffic sign.
Text descriptionsGround truthTop 1Top 2Top 3Fine localization(a)(b)(c)Global place recognitionFigure 5. Qualitative localization results on the KITTI360Pose dataset: In global place recognition, the numbers in top3 retrieval submaps
represent center distances between retrieved submaps and the ground truth. Green boxes indicate positive submaps containing the target
location, while red boxes signify negative submaps. For fine localization, red and black dots represent the ground truth and predicted target
locations, with the red number indicating the distance between them.
MethodsTest set
Submap Retrieval Recall Localization Recall
k= 1 k= 3 k= 5 k= 5(Ïµ <5/10/15m)
Text2Loc modified 0.15 0.30 0.38 0.39 0.54 0.58
Text2Loc (Ours) 0.28 0.49 0.58 0.53 0.68 0.71
Table 6. Performance comparisons of changing one sentence in
the queries on the KITTI360Pose test set.
fine localization. More qualitative results are in the Supple-
mentary Materials.
6.5. Embedding space analysis
We employ T-SNE [31] to visually represent the learned
embedding space, as illustrated in Figure 6. The baseline
method Text2Pos [12] yields a less discriminative space,
with positive submaps often distant from the query text de-
scriptions and even scattered across the embedding space.
In contrast, our method brings positive submaps and query
text representations significantly closer together within the
embedding distance. It shows that the proposed network in-
deed results in a more discriminative cross-model space for
recognizing places.
7. Conclusion
We proposed Text2Loc for 3D point cloud localization
based on a few natural language descriptions. In global
Positive SubmapNegative SubmapsText descriptionsText2PosOursFigure 6. T-SNE visualization for the global place recognition.
place recognition, we capture the contextual details within
and across text sentences with a novel attention-based
method and introduce contrastive learning for the text-
submap retrieval task. In addition, we are the first to pro-
pose a matching-free fine localization network for this task,
which is lighter, faster, and more accurate. Extensive exper-
iments demonstrate that Text2Loc improves the localization
performance over the state-of-the-art by a large margin. Fu-
ture work will explore trajectory planning in real robots.
Acknowledgements. This work was supported by the ERC
Advanced Grant SIMULACRON, by the Munich Center for
Machine Learning, and by the Royal Academy of Engineer-
ing (RF \201819 \18\163).
14965
References
[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed
Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners
for fine-grained 3d object identification in real-world scenes.
InComputer Visionâ€“ECCV 2020: 16th European Confer-
ence, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part
I 16, pages 422â€“440. Springer, 2020. 3
[2] Mikaela Angelina Uy and Gim Hee Lee. Pointnetvlad: Deep
point cloud based retrieval for large-scale place recognition.
InProceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pages 4470â€“4479, 2018. 2
[3] Tiago Barros, Lu Â´Ä±s Garrote, Ricardo Pereira, Cristiano Pre-
mebida, and Urbano J Nunes. Attdlnet: Attention-based
deep network for 3d lidar place recognition. In Iberian
Robotics conference , pages 309â€“320. Springer, 2022. 3
[4] Dave Zhenyu Chen, Angel X Chang, and Matthias NieÃŸner.
Scanrefer: 3d object localization in rgb-d scans using natural
language. In European conference on computer vision , pages
202â€“221. Springer, 2020. 3
[5] Marco Cuturi. Sinkhorn distances: Lightspeed computation
of optimal transport. In Advances in Neural Information Pro-
cessing Systems . Curran Associates, Inc., 2013. 7
[6] Haowen Deng, Tolga Birdal, and Slobodan Ilic. Ppfnet:
Global context aware local features for robust 3d point
matching. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition , pages 195â€“205, 2018.
3
[7] Gil Elbaz, Tamar Avraham, and Anath Fischer. 3d point
cloud registration for localization using a deep neural net-
work auto-encoder. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pages 4631â€“
4640, 2017. 3
[8] Zhaoxin Fan, Zhenbo Song, Hongyan Liu, Zhiwu Lu, Jun
He, and Xiaoyong Du. Svt-net: Super light-weight sparse
voxel transformer for large scale place recognition. AAAI,
2022. 3
[9] Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong
Zhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Aj-
mal Mian. Free-form description guided 3d visual graph net-
work for object grounding in point cloud. In Proceedings
of the IEEE/CVF International Conference on Computer Vi-
sion, pages 3722â€“3731, 2021. 3
[10] Sepp Hochreiter and J Â¨urgen Schmidhuber. Long short-term
memory. Neural computation , 9(8):1735â€“1780, 1997. 2
[11] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,
Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai
Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu
Qiao, and Hongyang Li. Planning-oriented autonomous driv-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , 2023. 1
[12] Manuel Kolmet, Qunjie Zhou, Aljo Ë‡sa OË‡sep, and Laura Leal-
TaixÂ´e. Text2pos: Text-to-point-cloud cross-modal localiza-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 6687â€“6696,
2022. 2, 3, 4, 5, 6, 7, 8, 1
[13] Jacek Komorowski. Minkloc3d: Point cloud based large-
scale place recognition. In Proceedings of the IEEE/CVFWinter Conference on Applications of Computer Vision ,
pages 1790â€“1799, 2021. 3
[14] Tsung-Yi Lin, Piotr Doll Â´ar, Ross Girshick, Kaiming He,
Bharath Hariharan, and Serge Belongie. Feature pyramid
networks for object detection. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition ,
pages 2117â€“2125, 2017. 3
[15] David G Lowe. Distinctive image features from scale-
invariant keypoints. International journal of computer vi-
sion, 60:91â€“110, 2004. 2
[16] Junyi Ma, Jun Zhang, Jintao Xu, Rui Ai, Weihao Gu, and
Xieyuanli Chen. Overlaptransformer: An efficient and yaw-
angle-invariant transformer network for lidar-based place
recognition. IEEE Robotics and Automation Letters , 7(3):
6958â€“6965, 2022. 3
[17] Junyi Ma, Guangming Xiong, Jingyi Xu, and Xieyuanli
Chen. Cvtnet: A cross-view transformer network
for place recognition using lidar data. arXiv preprint
arXiv:2302.01665 , 2023. 3
[18] Zhixiang Min, Bingbing Zhuang, Samuel Schulter, Buyu
Liu, Enrique Dunn, and Manmohan Chandraker. Neurocs:
Neural nocs supervision for monocular 3d object localiza-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 21404â€“
21414, 2023. 1
[19] Mihir Prabhudesai, Hsiao-Yu Fish Tung, Syed Ashar Javed,
Maximilian Sieb, Adam W Harley, and Katerina Fragki-
adaki. Embodied language grounding with implicit 3d visual
feature representations. 2019. 3
[20] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J
Guibas. Pointnet++: Deep hierarchical feature learning on
point sets in a metric space. Advances in neural information
processing systems , 30, 2017. 2, 4
[21] Filip Radenovi Â´c, Giorgos Tolias, and Ond Ë‡rej Chum. Fine-
tuning cnn image retrieval with no human annotation. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
41(7):1655â€“1668, 2018. 3
[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning
transferable visual models from natural language supervi-
sion. In International conference on machine learning , pages
8748â€“8763. PMLR, 2021. 4, 5
[23] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J. Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. Journal of Machine Learn-
ing Research , 21(140):1â€“67, 2020. 2, 3, 4
[24] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary
Bradski. Orb: An efficient alternative to sift or surf. In 2011
International conference on computer vision , pages 2564â€“
2571. Ieee, 2011. 2
[25] Hasim Sak, Andrew W. Senior, and Franc Â¸oise Beaufays.
Long short-term memory based recurrent neural network ar-
chitectures for large vocabulary speech recognition. CoRR ,
abs/1402.1128, 2014. 2
[26] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and
Marcin Dymczyk. From coarse to fine: Robust hierarchical
14966
localization at large scale. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 12716â€“12725, 2019. 2
[27] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,
and Andrew Rabinovich. SuperGlue: Learning feature
matching with graph neural networks. In CVPR , 2020. 7
[28] Paul-Edouard Sarlin, Daniel DeTone, Tsun-Yi Yang, Armen
Avetisyan, Julian Straub, Tomasz Malisiewicz, Samuel Rota
Bul`o, Richard Newcombe, Peter Kontschieder, and Vasileios
Balntas. Orienternet: Visual localization in 2d public maps
with neural matching. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 21632â€“21642, 2023. 1
[29] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Efficient
& effective prioritized matching for large-scale image-based
localization. IEEE transactions on pattern analysis and ma-
chine intelligence , 39(9):1744â€“1756, 2016. 2
[30] Aleksandr Segal, Dirk Haehnel, and Sebastian Thrun.
Generalized-icp. In Robotics: science and systems , page
435. Seattle, WA, 2009. 3
[31] Laurens Van der Maaten and Geoffrey Hinton. Visualizing
data using t-sne. JMLR , 2008. 8
[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 3
[33] Guangzhi Wang, Hehe Fan, and Mohan Kankanhalli. Text to
point cloud localization with relation-enhanced transformer.
arXiv preprint arXiv:2301.05372 , 2023. 2, 3, 4, 5, 6, 7
[34] Yan Xia. Perception of vehicles and place recognition in
urban environment based on MLS point clouds . PhD thesis,
Technische Universit Â¨at M Â¨unchen, 2023. 1
[35] Yan Xia, Yusheng Xu, Shuang Li, Rui Wang, Juan Du,
Daniel Cremers, and Uwe Stilla. Soe-net: A self-attention
and orientation encoding network for point cloud based place
recognition. In Proceedings of the IEEE/CVF Conference
on computer vision and pattern recognition , pages 11348â€“
11357, 2021. 2, 4
[36] Yan Xia, Yusheng Xu, Cheng Wang, and Uwe Stilla. Vpc-
net: Completion of 3d vehicles from mls point clouds. ISPRS
Journal of Photogrammetry and Remote Sensing , 174:166â€“
181, 2021. 1
[37] Yan Xia, Mariia Gladkova, Rui Wang, Qianyun Li, Uwe
Stilla, Jo Ëœao F Henriques, and Daniel Cremers. Casspr: Cross
attention single scan place recognition. In Proceedings of
the IEEE/CVF International Conference on Computer Vision
(ICCV) , pages 8461â€“8472, 2023. 1, 3, 5
[38] Yan Xia, Qiangqiang Wu, Wei Li, Antoni B Chan, and
Uwe Stilla. A lightweight and detector-free 3d single object
tracker on point clouds. IEEE Transactions on Intelligent
Transportation Systems , 2023. 1
[39] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang,
Sheng Wang, Zhen Li, and Shuguang Cui. Instancerefer:
Cooperative holistic understanding for visual grounding on
point clouds through instance multi-level contextual refer-
ring. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision , pages 1791â€“1800, 2021. 3[40] Wenxiao Zhang, Huajian Zhou, Zhen Dong, Qingan Yan,
and Chunxia Xiao. Rank-pointretrieval: Reranking point
cloud retrieval via a visually consistent registration evalu-
ation. IEEE Transactions on Visualization and Computer
Graphics , 2022. 3
[41] Zhicheng Zhou, Cheng Zhao, Daniel Adolfsson, Songzhi
Su, Yang Gao, Tom Duckett, and Li Sun. Ndt-transformer:
Large-scale 3d point cloud localisation using the normal
distribution transform representation. In 2021 IEEE Inter-
national Conference on Robotics and Automation (ICRA) ,
pages 5654â€“5660. IEEE, 2021. 3
14967
