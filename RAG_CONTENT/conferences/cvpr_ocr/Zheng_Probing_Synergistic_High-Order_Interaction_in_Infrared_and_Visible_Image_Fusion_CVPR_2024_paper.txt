Probing Synergistic High-Order Interaction in Infrared and Visible Image
Fusion
Naishan Zheng1, Man Zhou1, Jie Huang1, Junming Hou2, Haoying Li3, Yuan Xu4, Feng Zhao1*
1University of Science and Technology of China
2Southeast University,3Zhejiang University,4Nanyang Technology University
{nszheng,manman,hj0117 }@mail.ustc.edu.cn,
junming hou@seu.edu.cn, lhaoying@zju.edu.cn, xu.yuan@ntu.edu.sg, fzhao956@ustc.edu.cn
Abstract
Infrared and visible image fusion aims to generate a
fused image by integrating and distinguishing complemen-
tary information from multiple sources. While the cross-
attention mechanism with global spatial interactions ap-
pears promising, it only capture second-order spatial inter-
actions, neglecting higher-order interactions in both spatial
and channel dimensions. This limitation hampers the ex-
ploitation of synergies between multi-modalities. To bridge
this gap, we introduce a Synergistic High-order Interaction
Paradigm (SHIP), designed to systematically investigate the
spatial fine-grained and global statistics collaborations be-
tween infrared and visible images across two fundamental
dimensions: 1) Spatial dimension : we construct spatial
fine-grained interactions through element-wise multiplica-
tion, mathematically equivalent to global interactions, and
then foster high-order formats by iteratively aggregating
and evolving complementary information, enhancing both
efficiency and flexibility; 2) Channel dimension : expanding
on channel interactions with first-order statistics (mean),
we devise high-order channel interactions to facilitate the
discernment of inter-dependencies between source images
based on global statistics. Harnessing high-order interac-
tions significantly enhances our modelâ€™s ability to exploit
multi-modal synergies, leading to superior performance
over state-of-the-art alternatives, as shown through com-
prehensive experiments across various benchmarks. Code
is available at https://github.com/zheng980629/SHIP .
1. Introduction
Infrared and visible image fusion strives to aggregate and
discern complementary information from source images
into fused images, enhancing their applicability in subse-
quent tasks [8, 10, 18, 32, 48, 50, 75]. Specifically, visible
*Corresponding author.
(c) Our Synergistic High -Order Interaction and its Features
(a) No Explicit  Interaction
Softmax
(b) ğŸğ’ğ’…Order Spatial InteractionC
 CNN
Vis
InfFusedVis
Inf
Inf
Cross -Attentionâ€¦
Vis
InfğŸğ’ğ’… Order ğŸ‘ğ’“ğ’… Order
ğŸ“ğ’•ğ’‰ Order ğŸ’ğ’•ğ’‰ OrderHigh -Order
Spatial Interactio n
High -Order
Channel Interactio n
â€¦
Synergistic High -
Order Interaction
Figure 1. Comparison between previous fusion rules and our pro-
posed paradigm. Previous works either (a) lack explicit interaction
or (b) achieve only 2ndorder spatial interactions; and (c) our SHIP
incorporates high-order spatial and channel interactions to explore
synergistic correlations between modalities in spatial fine-grained
details and global statistics, progressively integrating and distin-
guishing the complementary information.
images are distinguished by their intricate texture details
and alignment with human visual perception. In contrast,
infrared images excel in capturing essential thermal radi-
ation information, enabling the highlighting of significant
targets like vehicles and pedestrians, especially in low-light
environments. Consequently, there has been considerable
attention on investigating synergistic correlations between
various modalities to integrate complementary information.
Recently, the remarkable advancements in deep learn-
ing [19, 21, 31, 35, 47, 54, 79, 81, 86, 87] have sparked a
revolution within this community. Some approaches typi-
cally begin by pre-training an encoder-decoder for feature
extraction and image reconstruction. Subsequently, spe-
cific fusion rules such as concatenation [45], addition [51],
weight summation [25], and maximum [88] are applied to
capture correlations between source images for information
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
26384
fusion. Additionally, the design of loss functions based on
image fusion properties, like saliency-based [39, 45, 62]
and illumination-aware [56], provides crucial guidance for
models to explore complementary information within input
sources. In parallel, GAN-based methods [27, 30, 39, 42]
compels the generator to investigate reciprocal information
between modalities by conceptualizing image fusion as a
game between the generator and discriminator. However, as
shown in Fig. 1(a), these approaches do not explicitly estab-
lish synergistic correlations and inter-dependencies learning
between infrared and visible images.
The emergence of Transformer [6, 20, 38], characterized
by second-order spatial interactions [53] deviating from
the dot-products among key, query, and value, challenges
the dominance of CNNs. YDTR [59] introduces a dy-
namic Transformer module for acquiring local features and
context information from different modalities. SwinFu-
sion [46] develops self-attention and cross-attention mech-
anisms, modeling and integrating dependencies within both
intra-domain and inter-domain features. PanFormer [84]
designs a customized Transformer, incorporating two value
terms from disparate modalities to effectively capture their
collaborations. However, these transformer architectures
limit their interactions to the second order in spatial di-
mensions, neglecting the untapped potential of high-order
interactions in both spatial and channel dimensions. This
limitation results in a restricted exploration of synergistic
modality correlations, as illustrated in Fig. 1 (b).
Motivated by the above analysis, our objective is to
model high-order interactions in spatial and channel di-
mensions to comprehensively explore synergies between
infrared and visible modalities. Regrettably, employing a
straightforward approach of cascading self-attention oper-
ations in two dimensions only captures multiple second-
order interactions, simultaneously imposing an intolerable
computational burden due to matrix multiplications. There-
fore, how to extend second-order interactions to arbitrary
orders without introducing substantial computational over-
head is the key ingredient.
In this paper, we introduce a Synergistic High-order In-
teraction Paradigm (SHIP), offering an innovative approach
to efficiently capture synergies in spatial fine-grained and
global statistics among multiple modalities through high-
order interactions. Specifically, it involves two dimensions:
1)Spatial dimension: we employ the frequency domain to
establish spatial fine-grained correlations between infrared
and visible representations through element-wise multipli-
cation, a mathematically equivalent yet computationally ef-
ficient alternative to the costly matrix multiplication. Sub-
sequently, we iteratively aggregate complementary infor-
mation and evolve the synergistic correlations, thus fos-
tering high-order spatial interactions and effectively exca-
vating collaborations between modalities. 2) Channel di-mension: building on the SE blockâ€™s adaptive recalibration
of feature responses using first-order statistics (mean) for
first-order channel interaction, we elevate this concept into
a high-order format. This extension enables the exploration
of synergistic correlations grounded in global statistics from
source images, providing a deeper insight into distinguish-
ing intricate inter-dependencies among different modalities.
Our contributions are summarized as follows:
â€¢ The novel Synergistic High-order Interaction Paradigm
(SHIP) in this study explores intricate high-order inter-
actions in infrared and visible image fusion. By incorpo-
rating high-order interactions in both spatial and channel
dimensions, SHIP stands as a pioneering approach, inves-
tigating synergistic correlations between modalities.
â€¢ This paradigm investigates high-order interactions in-
volving spatial fine-grained and global statistics, collabo-
ratively aggregating complementary information and dis-
tinguishing inter-dependencies from source modalities.
â€¢ Our experiments on multiple infrared-visible benchmarks
show that the proposed framework outperforms state-of-
the-art methods. Furthermore, we also demonstrate its
effectiveness in the pan-sharpening task.
2. Related Work
2.1. Infrared and Visible Image Fusion
Infrared and visible image fusion aims to obtain a synthetic
image effectively highlights salient objects from the source
images while preserving visual quality. To align the synthe-
sized fusion results more closely with human visual percep-
tion, various image processing techniques were introduced,
including discrete wavelet [36], Laplacian pyramid [49],
contourlet transform [74], sparse representation [37], low-
rank representation [26], principal component analysis [9],
and total variation [41]. However, these methods involved
the formulation of fusion rules and intricate activity levels,
constraining their applicability in complex scenarios.
Recently, explosive deep learning-based methods [17,
28, 29, 33, 34, 58, 62, 70, 78, 80, 83] have revolution-
ized image fusion, falling into three main categories: auto-
encoder (AE)-based methods, CNN-based methods, and
generative adversarial network (GAN)-based methods. AE-
based methods [25, 57, 65, 66, 76] typically employ pre-
trained auto-encoders for feature extraction and image re-
construction, emphasizing the design of network architec-
tures and fusion strategies. CNN-based methods [40, 45,
45, 55, 56, 61] integrate these components within an end-
to-end framework. Zhang et al. [72] developed intensity
and gradient branches to preserve these essential properties
of source images. In addition, researchers have developed
various loss functions [40, 43, 45, 56] grounded in image fu-
sion properties, providing substantial guidance during net-
work training. For example, Ma et al. [45] designed a fusion
26385
Conv
â€¦
â€¦
â€¦ğ‘­ğ“¥
ğ‘­ğ“¡ğ–ğ‘¸ğŸğŸ
ğ–ğŠğŸğŸğ–ğ•ğŸğŸFFT FFTIFFTNormNorm
â€¦
â€¦
ğ–ğğŸğŸ ğ–ğ•ğŸğŸğ–ğ‘½ğ‘µğŸ Norm
ğ–ğğğŸâ€¦Spatial High -Order Interaction
âˆ™âˆ™âˆ™âˆ™ğ‘­ğ“¥ğ’”ğŸ
GAPâˆ™Channel High -Order Interaction
ğ–ğ…ğŸğŸğ–ğ…ğğŸ
â€¦â€¦
â€¦
ğ‘­ğŸ
ğ‘­ğ“¡ğ’”ğŸğ‘­ğ‘ªğŸğŸğ‘­ğ‘ªğŸğŸğ‘­ğ‘ªğ‘µğŸConv
âˆ™âˆ™ğ‘­ğ“¥ğ’„ğŸ
ğ‘­ğ“¥ğ’”ğ‘µğŸ
ğ‘­ğ“¥ğ’”ğŸğŸ
ğ‘­ğ“¥ğ’”ğŸğŸğŸğ¬ğ­ SHIP
ğ–ğğŸğ‹
ğ–ğŠğŸğ‘³ğ–ğ•ğŸğ‹FFT FFTIFFTNormNorm
â€¦
â€¦
ğ–ğğŸğ‹ ğ–ğ•ğŸğ‘³ğ–ğ•ğğ‹ Norm
ğ–ğğğ‹â€¦Spatial High -Order Interaction
âˆ™âˆ™âˆ™âˆ™ğ‘­ğ“¥ğ’”ğ‘³
GAPâˆ™Channel High -Order Interaction
ğ–ğ…ğŸğ‹ğ–ğ™ğŸğ‹ â€¦â€¦
â€¦
ğ‘­ğ‘³
ğ‘­ğ“¡ğ’”ğ‘³ğ‘­ğ‘ªğŸğ‘³ğ‘­ğ‘ªğŸğ‘³ğ‘­ğ‘ªğ‘µğ‘³Conv
âˆ™âˆ™ğ‘­ğ“¥ğ’„ğ‘³
ğ‘­ğ“¥ğ’”ğ‘µğ‘³
ğ‘­ğ“¥ğ’”ğŸğ‘³
ğ‘­ğ“¥ğ’”ğŸğ‘³ğ‹ğ­ğ¡ SHIP
ğ‘­ğ“¥ğ’”ğ‘³âˆ’ğŸ
ğ‘­ğ“¡ğ’”ğ‘³âˆ’ğŸ
Element -wise addition Â·Element -wise multiplication GAP Global Average Poolingğ‘°ğ“¥
ğ‘°ğ“¡ğ–ğ™ğŸğŸğ–ğ™ğğŸğ–ğ…ğğ‹ğ–ğ™ğğ‹
Sigmoi dFigure 2. The detailed framework of the proposed Synergistic High-order Interaction Paradigm (SHIP) comprises alternating spatial and
channel high-order interactions, executed over L iterations. Specifically, the spatial high-order interaction sufficiently excavates collabora-
tions between two modalities and integrates spatial fine-grained complementary information through high-order modelling. Subsequently,
the channel high-order interaction, rooted in the global first-order statistic (mean), further investigates global statistics, distinguishing inter-
dependencies between visible and infrared modalities.
loss that utilizes a specialized target mask, allowing for se-
lective fusion of target and background regions. However,
due to the absence of authentic fused images for reference,
researchers [27, 30, 42, 44, 82] attempted to introduce GAN
into the learning paradigm. They utilized the discrimina-
tor to compel the generator to preserve more texture details
highlight salient objects from source images.
2.2. High-Order Interaction Modeling
The vanilla convolution operation does not inherently cap-
ture the spatial interactions between a specific location and
its neighboring region. A refined approach, known as dy-
namic convolution [2, 13, 22], introduces a first-order spa-
tial interaction by generating dynamic weights adapted to
the input. In Transformers [6], the self-attention mech-
anism facilitates a second-order spatial interaction via its
key ingredient: the intrinsic matrix multiplication involving
queries, keys, and values. Shifting to the channel dimen-
sion, the Squeeze-and-Excitation block [15, 63] utilizes the
first-order statistic (mean) to recalibrate channel responses.
Generally, these improvements only focus on capturing in-
teractions in either the spatial or channel dimensions, rather
than achieving high-order interactions in both dimensions.
3. Methods
3.1. Overview Framework
The proposed paradigm, illustrated in Fig. 2, operates as
follows: given an infrared image, IRâˆˆRHÃ—WÃ—1, and a
visible image, IVâˆˆRHÃ—WÃ—3, we extract correspondingshallow features using separate convolution layers for each
modality, yielding FRâˆˆRHÃ—WÃ—CandFVâˆˆRHÃ—WÃ—C.
Then, these modality-aware features undergo a series of
core Synergistic High-order Interactions Paradigm (SHIP),
incorporating both spatial and channel dimensions. This
process explores synergies between the two modalities in
spatial fine-grained details and global statistics. Finally,
these features are projected back into the image space to
generate the fused result, IFâˆˆRHÃ—WÃ—1. The fusion pro-
cess specifically targets the Ychannel in the YCbCrcolor
space, following the approach of prior works [24, 57]. In
summary, the paradigm can be formulated as follows:
IF= SHIP L(Ïˆ(IR), Ï•(IV)), (1)
where Ïˆ(Â·)andÏ•(Â·)denote feature extractors and Lindi-
cates the iteration number of our SHIP .
3.2. High-Order Spatial Interactions
Revisiting the Self-attention. The self-attention mecha-
nism, the key ingredient of Transformer [6], fosters second-
order spatial interactions through matrix multiplications
among key, query, and value components. This process em-
powers the model to dynamically distinguish and aggregate
complementary information, grounded in the query modal-
ity. For the infrared and visible image fusion, the query Q,
keyK, and value Vare derived by:
Q=FVWQ,K=FRWK,V=FRWV,(2)
where WQ,WK, andWVindicate linear transformations
applied to project modality-aware feature representations.
26386
01
(b) ğ‘­ğ“¥ğ’”ğŸğŸ(a) ğ‘­ğ“¥ğ’”ğŸğŸ(c) ğ‘­ğ“¥ğ’”ğŸ‘ğŸ(f) ğ‘­ğ“¥ğ’”ğŸğŸ‘(e) ğ‘­ğ“¥ğ’”ğŸğŸ‘(g) ğ‘­ğ“¥ğ’”ğŸ‘ğŸ‘
(d) ğ‘­ğ“¥ğ’”ğŸ’ğŸ(h) ğ‘­ğ“¥ğ’”ğŸ’ğŸ‘ğŸğ’ğ’… Spatial High -Order Interaction ğŸ‘ğ’“ğ’… Spatial High -Order InteractionFigure 3. Feature visualization after each interaction at different spatial high-order interaction step. For instance, F2
V3sindicates the feature
after the third-order interaction in the 2ndspatial high-order interaction. These visualizations illustrate the efficacy of high-order spatial
interactions in two perspectives: (1) within each high-order interaction, feature responses escalate as the order increases, highlighting the
salient objects; (2) distinct high-order interactions yield unique responses, showcasing the diversity in feature representations.
The self-attention mechanism, which captures second-order
spatial interactions centered on the input FV, is realized
through dot-product operations among these components:
OS((FV)2) =F1
VS= softmaxQâŠ—KT
âˆšdk
âŠ—V=AâŠ—V,
(3)
where dkrepresents the dimension of the key, âŠ—indicates
the dot-product operation, AâˆˆRHWÃ—HWis the corre-
lation matrix, and OS((FV)2)signifies the output of the
self-attention module, capturing second-order spatial inter-
actions about the input feature FV.
However, the dot product, despite its effectiveness,
comes with significant computational costs, rendering it im-
practical for achieving high-order operations through cas-
cading self-attention mechanisms.
Equivalently efficient form. Each element of Acan be
redefined by the inner product: Aij=âŸ¨qi,kjâŸ©,qiâˆˆQ,
kjâˆˆK, andâŸ¨Â·âŸ©indicates the inner product. The convolu-
tion theorem establishes that the correlation or convolution
of two signals in the spatial domain equals them Hadamard
product between them in the frequency domain. To lever-
age this property, we incorporate the frequency domain into
the self-attention mechanism, simplifying matrix multipli-
cation to a lightweight element-wise operation. Initially, we
transform the modality-aware features FRandFVinto the
frequency domain using fast Fourier transform (FFT). The
correlation is computed as follows:
A=Fâˆ’1
F 
FVWQ
âŠ™F(FRWK)
, (4)
where F(Â·)andFâˆ’1(Â·)denote FFT and inverse FFT, âŠ™in-
dicates the Hadamard product, and F(Â·)represents the con-
jugate transpose operation. Furthermore, the integrated fea-
tures with second-order spatial interactions are obtained:
OS((FV)2) =F1
VS= Norm( A)âŠ™(FRWV),(5)
where Norm represents layer normalization applied to A.
Delving into the High-Order Format. Recent method-
ologies, such as [3, 4, 73], have shown a strong preferencefor employing self-attention mechanisms. However, these
approaches, often seen in cascading self-attention blocks,
tend to generate multiple second-order interactions centered
around the query feature, rather than achieving higher-order
modeling. Formally, the recursive format of Lcascaded
self-attention can be expressed as:
OS((Fiâˆ’1
VS)2) =Fi
VS= Attention ( Qi,Ki,Vi),
Qi=Fiâˆ’1
VSWQi,Ki=FRWKi,Qi=FRWVi,(6)
where 1â‰¤iâ‰¤L. It is apparent that this process only
captures second-order interactions about the input feature
FViâˆ’1while inducing huge computational costs.
In contrast, standing on the equivalently efficient form,
we progress beyond second-order interactions and extend
our reach to arbitrary-order interactions ( N-order) while
maintaining efficiency. Concretely, for each ithiteration,
we extend Eq. 5 into the following high-order formulation:
OS((Fiâˆ’1
V)j) =Fi
Vj
s= Norm( Fi
Vjâˆ’1
SWQi
j)âŠ™(Fi
Rjâˆ’1
SWVi
j),
Fi
Vjâˆ’1
S= Norm( Fi
Vjâˆ’2
SWQi
jâˆ’1), Fi
Rjâˆ’1
S=Fi
Rjâˆ’2
SWVi
jâˆ’1,
(7)
where 2â‰¤jâ‰¤N. This formulation enables us to capture
interactions up to the N-th order efficiently.
In general, for the traditional transformer chain with L,
the sequence unfolds as follows:
FVâ†’ O S((FV)2)â†’F1
VSâ†’ O S((F1
VS)2)â†’F2
VS. . .
â†’ O S((Fi
VS)2)â†’Fi+1
VS. . .OS((FLâˆ’1
VS)2)â†’FL
VS.
(8)
In contrast, our high-order modeling replaces this with:
FVâ†’ O S((FV)N)â†’F1
VSâ†’ O S((F1
VS)N)â†’F2
VS. . .
â†’ O S((Fi
VS)N)â†’Fi+1
VS. . .OS((FLâˆ’1
VS)N)â†’FL
VS.
(9)
Indeed, this modification empowers us to capture interac-
tions up to the N-th order within each iteration. As depicted
in Fig. 3, diverse orders within each spatial high-order in-
teraction integrate complementary information of varying
26387
granularity. Moreover, interactions at different iterations
exhibit discriminative responses, enriching the feature di-
versity throughout the iterative process.
3.3. High-Order Channel Interaction
Revisiting Squeeze and Excitation Block. The Squeeze-
and-Excitation (SE) block [15] leverages the first-order
global statistic, mean, to model channel interactions. This
approach enables the SE block to explicitly capture inter-
dependencies between input feature channels. For the in-
frared and visible image fusion, the SE block formulates
the dependency between infrared and visible features from
theithhigh-order spatial interaction as follows:
Zi=1
HÃ—WHX
x=1WX
y=1Fi(x, y),
OC((Fi)1) =Fi
C=Ïƒ
WZi
1Zi
Â·Fi,(10)
where Fi= concat[ Fi
VS, Fi
RS],Zcrepresents the first-
order statistic, and Ïƒdenotes the Sigmoid function. WZ
includes two linear transformations and a ReLU function.
Delving into the High-Order Format. Similar to high-
order spatial interactions, we extend the SE block to achieve
high-order channel interactions:
OC((Fi)j) =Fi
Cj=Ïƒ
WZi
jZi
jâˆ’1
Â·(WFi
jFi
jâˆ’1),
Zi
jâˆ’1=Ïƒ(WFi
jâˆ’1Zi
jâˆ’2), Fi
jâˆ’1=WFi
jâˆ’1Fi
jâˆ’2.
(11)
Finally, a convolution layer integrates Fi
Cinto the fused
modality, yielding the integrated feature, Fi
VC.
Through N-order spatial and channel interactions con-
ducted over Literations, the interaction chain can be math-
ematically expressed as follows:
FVâ†’ O S((FV)N)â†’OC((F1
VS)N)â†’ O S((F1
VC)N)
â†’ O C((F2
VS)N)â†’. . .OS((FLâˆ’1
VC)N)â†’ O C((FL
VS)N).
(12)
We analyze channel responses at 2ndchannel high-order
interaction along the channel dimension. Contrary to con-
sistent responses across different orders, our high-order
modeling adaptively distinguishes inter-dependencies be-
tween source modalities, as illustrated in Fig. 4.
3.4. Loss Functions
The loss function comprises the intensity and gradient
terms: L=Lint+Î»Lgra. Î»represents the trade-off pa-
rameter. To emphasize salient objects from visible and in-
frared images [30, 39], we introduce a saliency-based inten-
sity loss, defined as follows:
Lint=âˆ¥(Ï‰Vâ—¦IV+Ï‰Râ—¦IR)âˆ’IFâˆ¥1, (13)
Figure 4. Channel interactions of different orders across channel
indices. This observation serves as compelling evidence, suggest-
ing that the interactions of varying orders explore diverse inter-
dependencies between infrared and visible modalities.
where âˆ¥ Â· âˆ¥ 1denotes â„“1norm. The weighted maps Ï‰V
andÏ‰Rare derived from the visible and infrared images
asÏ‰V=SV/(SVâˆ’SR)andSR= 1âˆ’SV, where Sis the
saliency matrix computed using the algorithm in [11].
To preserve crucial texture details from the source im-
ages in the fused results, we introduce a gradient loss:
Lgra=1
HWâˆ¥âˆ‡IFâˆ’max (âˆ‡IR,âˆ‡IV)âˆ¥1, (14)
where âˆ‡indicates the gradient operator used for texture in-
formation measurement within an image, and max(Â·)de-
notes the element-wise maximum operation.
4. Experiments
4.1. Experimental Settings
Datasets and metrics. To assess the effectiveness of our
SHIP, we conduct comprehensive experiments on three pub-
licly available datasets: M3FD [30], RoadScene [64], and
TNO [60]. The M3FD dataset comprises 4200 paired in-
frared and visible images, with 3900 allocated for training
and 300 for official testing. To further evaluate the gener-
alization capability of our approach, we test our algorithm
(trained on M3FD) on the RoadScene and TNO datasets.
Since the latter two datasets lack a predefined split, we fol-
low the configuration described in [72] and randomly select
25 pairs from each dataset for comparison.
A high-quality fused image should capture both salient
objects and visual quality from the multi-modal images. To
comprehensively measure the fusion results, we employ six
metrics, including spatial frequency (SF) [7], mutual infor-
mation (MI) [52], visual information fidelity (VIF) [14], av-
erage gradient (AG) [5], Qabf[68], and feature mutual in-
formation (FMI) [12]. Moreover, higher values for these
metrics indicate superior fusion performance.
Implementations. We implement our SHIP with Py-
Torch on a single NVIDIA GTX 3090 GPU. We use the
Adam optimizer with Î²1= 0.9,Î²2= 0.99to update our
model for 30 K, each with a batch size of 8. The initial
26388
Figure 5. Qualitative results of different fusion methods on M3FD, RoadScene, and TNO datasets, respectively.
learning rate is set to 1Ã—10âˆ’4and decreases by a factor of
0.5 every 5 Kiterations. The patch size is set to 128Ã—128.
4.2. Comparison with State-of-the-Arts
We compare our proposed SHIP with 9 state-of-the-art ap-
proaches: DDcGAN [44], DenseFuse [25], AUIF [77],
DIDFuse [76], ReCoNet [16], SDNet [71], TarDAL [30],
U2Fusion [64], and UMFusion [61] on three datasets.
Qualitative Comparisons. The qualitative results for
three typical image pairs from various datasets are illus-
trated in Fig. 5. Compared to other existing methods, our
proposed SHIP boasts two notable advantages. Firstly, our
algorithm excels in preserving prominent objects from both
infrared and visible images. As demonstrated in Fig. 5,
pedestrians and tree branches in our method showcases high
contrast, and distinctive contours, enhancing its suitabil-
ity for visual observation (see red tangles of the first and
third examples). Additionally, our results generate fusion
outputs with intricate textures, aligning well with human
visual perception. In contrast, visualization results show
that DenseFuse, SDNet, and U2Fusion fail to effectivelyhighlight discriminative targets, whereas DDcGAN and Re-
CoNet lack the ability to capture intricate textural details.
Quantitative Comparisons. Table 1 reveals the excep-
tional performance of our method across multiple metrics
on the three datasets. The superior MI and FMI scores in-
dicate our modelâ€™s ability to effectively leverage informa-
tion from both source images, showcasing its competence
in transferring abundant information into the fused results.
Furthermore, our approachâ€™s leading performance in SF,
AG, and Qabfsignifies its remarkable capacity to integrate
multi-modal complementary information and preserve in-
tricate texture details. These achievements contribute to the
preservation of fine-grained textures, ultimately resulting in
visually appealing and detailed fused images. Moreover,
the highest VIF also demonstrates that our fusion results
have high-quality visual effects and small distortion, satis-
fying the human visual perception. These results collec-
tively emphasize the robustness and generalization of our
SHIP across various evaluation metrics, confirming its ef-
fectiveness in diverse scenarios and datasets.
26389
Table 1. Quantitative comparison of our SHIP with 9 state-of-the-
art methods on M3FD, RoadScne, and TNO datasets. The best and
the second results are marked in bold and underlined.
MethodsM3FD dataset [30]
SFâ†‘MIâ†‘VIFâ†‘AGâ†‘Qabfâ†‘FMIâ†‘
DDcGAN [44] 0.059 2.540 0.768 5.275 0.480 0.836
DenseFuse [25] 0.037 2.930 0.762 3.303 0.503 0.863
AUIF [77] 0.046 3.049 0.819 3.922 0.503 0.845
DIDFuse [76] 0.055 3.048 0.877 4.871 0.494 0.831
ReCoNet [16] 0.041 3.049 0.818 3.929 0.486 0.845
SDNet [71] 0.053 3.231 0.678 4.730 0.528 0.846
TarDAL [30] 0.049 3.162 0.810 4.136 0.405 0.825
U2Fusion [64] 0.042 2.759 0.709 3.967 0.538 0.850
UMFusion [61] 0.034 3.087 0.709 2.928 0.397 0.855
Ours 0.060 4.813 0.922 5.177 0.549 0.892
MethodsRoadScene dataset [64]
SFâ†‘MIâ†‘VIFâ†‘AGâ†‘Qabfâ†‘FMIâ†‘
DDcGAN [44] 0.039 2.618 0.595 3.771 0.309 0.859
DenseFuse [25] 0.040 3.128 0.803 4.012 0.513 0.868
AUIF [77] 0.055 3.111 0.847 5.318 0.517 0.856
DIDFuse [76] 0.052 3.184 0.827 5.038 0.487 0.853
ReCoNet [16] 0.033 3.159 0.796 3.484 0.394 0.858
SDNet [71] 0.054 3.423 0.821 5.531 0.533 0.863
TarDAL [30] 0.050 3.464 0.787 4.389 0.448 0.852
U2Fusion [64] 0.047 2.811 0.740 4.885 0.526 0.861
UMFusion [61] 0.038 3.202 0.791 3.715 0.505 0.866
Ours 0.058 3.914 0.905 5.589 0.550 0.873
MethodsTNO dataset [60]
SFâ†‘MIâ†‘VIFâ†‘AGâ†‘Qabfâ†‘FMIâ†‘
DDcGAN [44] 0.048 1.847 0.674 4.831 0.349 0.858
DenseFuse [51] 0.037 2.402 0.800 3.634 0.423 0.890
AUIF [77] 0.051 2.271 0.815 4.712 0.426 0.879
DIDFuse [76] 0.049 2.442 0.829 4.636 0.405 0.863
ReCoNet [16] 0.029 2.426 0.827 3.323 0.357 0.878
SDNet [71] 0.050 2.186 0.762 4.844 0.421 0.883
TarDAL [30] 0.051 2.648 0.860 4.887 0.467 0.881
U2Fusion [64] 0.035 1.922 0.688 3.733 0.418 0.879
UMFusion [61] 0.036 2.247 0.717 3.166 0.398 0.888
Ours 0.052 3.849 0.933 5.012 0.518 0.890
4.3. Ablation Studies
We conduct ablation studies on the M3FD dataset to further
investigate the effectiveness of our proposed SHIP under
different the number of orders Nand iterations L. For ex-
ample, L4N5denotes SHIP with 4iterations and 5-order
spatial and channel interactions.
Effect of the number of orders N:To investigate the
impact of different orders of spatial and channel interac-
tions, we conduct experiments on proposed SHIP with vary-
SF MI VIF
AG Qabf FMI
Figure 6. Ablation studies of the proposed SHIP with different
numbers of order Non the M3FD dataset.
SF MI VIF
AG Qabf FMI
Figure 7. Ablation studies of the proposed SHIP with different
numbers of block Lon the M3FD dataset.
ing orders denoted as N. As illustrated in Fig. 6, the per-
formance significantly increases as the number of stages in-
creases until reaching 5. After this threshold, the perfor-
mance stabilizes with slight improvements as Nincreases
further. To strike a balance between performance and com-
putational cost, we set N= 5 as the default order number.
Furthermore, we present visualization results of our SHIPâ€™s
feature responses with varied orders in Fig. 8. These visu-
alizations demonstrate that the effectiveness of high-order
interactions in unveiling synergistic correlations between
modalities, thus highlighting salient objects.
Effect of the number of iterations L:We conduct 5
experiment (L1N5, L2N5, L3N5, L4N5, and L5N5) to in-
vestigate the effect of the number of blocks on the results.
From the observations in Fig. 7, it is evident that the modelâ€™s
performance improves considerably with an increase in the
number of blocks. However, further increments in Lresult
in a decreasing trend in SF and AG, possibly due to chal-
lenges in gradient propagation. Consequently, we employ
L= 3as the default block number in all experiments.
4.4. Extension on Pan-sharpening
To further demonstrate the effectiveness of our SHIP in
the multi-modality image fusion task, we extend it on the
panchromatic and multi-spectral image fusion task, named
26390
01
(d) ğ‘µ=ğŸ“
 (c) ğ‘µ=ğŸ’(b) ğ‘µ=3
 (a) ğ‘µ=ğŸ
(e) ğ‘µ=ğŸ”(f) InfraredFigure 8. Visualization results of the proposed SHIP with different numbers of orders Non the M3FD dataset.
Table 2. Quantitative comparison of our SHIP with 6 state-of-the-art methods on WorldView II, GaoFen2, and WorldView III datasets.
MethodsWordView II GaoFen2 WordView III
PSNRâ†‘SSIMâ†‘SAMâ†“ERGAS â†“PSNRâ†‘SSIMâ†‘SAMâ†“ERGAS â†“PSNRâ†‘SSIMâ†‘SAMâ†“ERGAS â†“
GS [23] 35.638 0.918 0.042 1.877 37.226 0.903 0.031 1.674 22.561 0.547 0.122 8.243
PANNet [69] 40.818 0.963 0.026 1.056 43.066 0.969 0.018 0.858 29.684 0.907 0.085 3.426
SRPPNN [1] 41.454 0.968 0.023 0.990 47.200 0.988 0.011 0.559 30.435 0.920 0.077 3.155
GPPNN [67] 41.162 0.968 0.024 1.032 44.215 0.982 0.014 0.736 30.179 0.918 0.078 3.260
MutNet [85] 41.677 0.971 0.022 0.952 47.304 0.989 0.010 0.547 30.491 0.922 0.075 3.113
INNformer [84] 41.690 0.970 0.023 0.951 47.353 0.989 0.010 0.548 30.537 0.921 0.074 3.099
Ours 41.736 0.973 0.022 0.948 47.458 0.990 0.010 0.537 30.615 0.925 0.074 3.056
LRMS PAN PANNET SRPPNN GPPNN MutNet INNformer Ours
PANNET SRPPNN GPPNN MutNet INNformer Ours
GS
GS
Figure 9. Visual comparison on WorldView-II dataset.
d Pan-sharpening. Following [84], we conduct extensive
experiments on three widely used datasets: WorldView II,
GaoFen2, and WorldView III datasets [84].
Quantitative Comparisons. Table 2 presents the eval-
uation metrics across three datasets, with the best and
second-best values highlighted in bold and underline. Our
proposed method consistently outperforms other competi-
tive techniques across all satellite datasets. Specifically, our
method surpasses 0.105 dB over the second-best INNformer
on the GaoFen2 dataset. These consistent performances un-
derscore the lower spectral distortion and superior preserva-
tion of spatial textures achieved by our SHIP.
Qualitative Comparisons. Fig. 9 visually demonstrates
the comparison results, providing further confirmation of
the effectiveness of our method. The last row displays the
Mean Squared Error (MSE) residuals between the output
pan-sharpened results and the ground truth. In comparison,
our model exhibits minimal spatial and spectral distortions.
The outstanding performance of our method underscores
the effectiveness of the proposed synergistic high-order in-
teraction mechanism, which integrates complementary in-
formation and enhance the visual quality of results.5. Conclusion
In this paper, we pioneer the exploration of the Syner-
gistic High-order Interaction paradigm (SHIP) to inves-
tigate collaborations between infrared and visible image
modalities for image fusion. Our SHIP comprises both
spatial and channel dimensions. The spatial high-order
interaction progressively captures synergistic correlations
between infrared and visible modalities, effectively in-
tegrating spatial fine-grained complementary information
through high-order modeling. The channel high-order in-
teraction, grounded global statistic, investigates and distin-
guishes the inter-dependencies between source modalities.
Extensive experiments on multiple infrared and visible im-
age fusion benchmarks have shown the superiority of our
proposed synergistic high-order interaction paradigm.
Acknowledgments: This work was supported by the
JKW Research Funds under Grant 20-163-14-LZ-001-004-
01, and the Anhui Provincial Natural Science Foundation
under Grant 2108085UD12. We acknowledge the support
of GPU cluster built by MCC Lab of Information Science
and Technology Institution, USTC.
26391
References
[1] Jiajun Cai and Bo Huang. Super-resolution-guided progres-
sive pansharpening based on a deep convolutional neural net-
work. IEEE Transactions on Geoscience and Remote Sens-
ing, 59(6):5206â€“5220, 2020. 8
[2] Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong
Chen, Lu Yuan, and Zicheng Liu. Dynamic convolu-
tion: Attention over convolution kernels. In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition , pages 11030â€“11039, 2020. 3
[3] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, Xin
Yuan, et al. Cross aggregation transformer for image restora-
tion. Advances in Neural Information Processing Systems ,
35:25478â€“25490, 2022. 4
[4] Zheng Chen, Yulun Zhang, Jinjin Gu, Linghe Kong, and
Xiaokang Yang. Recursive generalization transformer for
image super-resolution. arXiv preprint arXiv:2303.06373 ,
2023. 4
[5] Guangmang Cui, Huajun Feng, Zhihai Xu, Qi Li, and Yuet-
ing Chen. Detail preserved fusion of visible and infrared im-
ages using regional saliency extraction and multi-scale im-
age decomposition. Optics Communications , 341:199â€“209,
2015. 5
[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 2, 3
[7] Ahmet M Eskicioglu and Paul S Fisher. Image quality mea-
sures and their performance. IEEE Transactions on commu-
nications , 43(12):2959â€“2965, 1995. 5
[8] Jiawei Feng, Ancong Wu, and Wei-Shi Zheng. Shape-erased
feature learning for visible-infrared person re-identification.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) , pages 22752â€“
22761, 2023. 1
[9] Zhizhong Fu, Xue Wang, Jin Xu, Ning Zhou, and Yufei
Zhao. Infrared and visible images fusion based on rpca and
nsct. Infrared Physics & Technology , 77:114â€“123, 2016. 2
[10] Hongbo Gao, Bo Cheng, Jianqiang Wang, Keqiang Li, Jian-
hui Zhao, and Deyi Li. Object classification using cnn-based
fusion of vision and lidar in autonomous vehicle environ-
ment. IEEE Transactions on Industrial Informatics , 14(9):
4224â€“4231, 2018. 1
[11] Sanjay Ghosh, Ruturaj G Gavaskar, and Kunal N Chaud-
hury. Saliency guided image detail enhancement. In 2019
National Conference on Communications (NCC) , pages 1â€“6.
IEEE, 2019. 5
[12] Mohammad Bagher Akbari Haghighat, Ali Aghagolzadeh,
and Hadi Seyedarabi. A non-reference image fusion metric
based on mutual information of image features. Computers
& Electrical Engineering , 37(5):744â€“756, 2011. 5
[13] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Ji-
aying Liu, and Jingdong Wang. Demystifying local vision
transformer: Sparse connectivity, weight sharing, and dy-namic weight. arXiv preprint arXiv:2106.04263 , 2(3), 2021.
3
[14] Yu Han, Yunze Cai, Yin Cao, and Xiaoming Xu. A new im-
age fusion performance metric based on visual information
fidelity. Information fusion , 14(2):127â€“135, 2013. 5
[15] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation
networks. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 7132â€“
7141, 2018. 3, 5
[16] Zhanbo Huang, Jinyuan Liu, Xin Fan, Risheng Liu, Wei
Zhong, and Zhongxuan Luo. Reconet: Recurrent correc-
tion network for fast and efficient multi-modality image fu-
sion. In European Conference on Computer Vision , pages
539â€“555. Springer, 2022. 6, 7
[17] Zhanbo Huang, Jinyuan Liu, Xin Fan, Risheng Liu, Wei
Zhong, and Zhongxuan Luo. Reconet: Recurrent correc-
tion network for fast and efficient multi-modality image fu-
sion. In European Conference on Computer Vision , pages
539â€“555. Springer, 2022. 2
[18] Deyi Ji, Haoran Wang, Hanzhe Hu, Weihao Gan, Wei Wu,
and Junjie Yan. Context-aware graph convolution network
for target re-identification. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , pages 1646â€“1654, 2021. 1
[19] Deyi Ji, Haoran Wang, Mingyuan Tao, Jianqiang Huang,
Xian-Sheng Hua, and Hongtao Lu. Structural and statistical
texture knowledge distillation for semantic segmentation. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition , pages 16876â€“16885, 2022. 1
[20] Deyi Ji, Feng Zhao, and Hongtao Lu. Guided patch-grouping
wavelet transformer with spatial congruence for ultra-high
resolution segmentation. International Joint Conference on
Artificial Intelligence , 2023. 2
[21] Deyi Ji, Feng Zhao, Hongtao Lu, Mingyuan Tao, and Jieping
Ye. Ultra-high resolution segmentation with ultra-rich con-
text: A novel benchmark. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition ,
pages 23621â€“23630, 2023. 1
[22] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V
Gool. Dynamic filter networks. Advances in neural informa-
tion processing systems , 29, 2016. 3
[23] Craig A Laben and Bernard V Brower. Process for enhanc-
ing the spatial resolution of multispectral imagery using pan-
sharpening, 2000. US Patent 6,011,875. 8
[24] Zhuliang Le, Jun Huang, Han Xu, Fan Fan, Yong Ma, Xi-
aoguang Mei, and Jiayi Ma. Uifgan: An unsupervised
continual-learning generative adversarial network for unified
image fusion. Information Fusion , 88:305â€“318, 2022. 3
[25] Hui Li and Xiao-Jun Wu. Densefuse: A fusion approach to
infrared and visible images. IEEE Transactions on Image
Processing , 28(5):2614â€“2623, 2018. 1, 2, 6, 7
[26] Hui Li, Xiao-Jun Wu, and Josef Kittler. Mdlatlrr: A novel
decomposition method for infrared and visible image fu-
sion. IEEE Transactions on Image Processing , 29:4733â€“
4746, 2020. 2
[27] Jing Li, Hongtao Huo, Chang Li, Renhua Wang, and Qi
Feng. Attentionfgan: Infrared and visible image fusion us-
ing attention-based generative adversarial networks. IEEE
Transactions on Multimedia , 23:1383â€“1396, 2020. 2, 3
26392
[28] Jinyuan Liu, Xin Fan, Ji Jiang, Risheng Liu, and Zhongx-
uan Luo. Learning a deep multi-scale feature ensemble and
an edge-attention guidance for image fusion. IEEE Transac-
tions on Circuits and Systems for Video Technology , 32(1):
105â€“119, 2021. 2
[29] Jinyuan Liu, Yuhui Wu, Zhanbo Huang, Risheng Liu, and
Xin Fan. Smoa: Searching a modality-oriented architecture
for infrared and visible image fusion. IEEE Signal Process-
ing Letters , 28:1818â€“1822, 2021. 2
[30] Jinyuan Liu, Xin Fan, Zhanbo Huang, Guanyao Wu, Risheng
Liu, Wei Zhong, and Zhongxuan Luo. Target-aware dual
adversarial learning and a multi-scenario multi-modality
benchmark to fuse infrared and visible for object detection.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 5802â€“5811, 2022. 2,
3, 5, 6, 7
[31] Jinyuan Liu, Runjia Lin, Guanyao Wu, Risheng Liu,
Zhongxuan Luo, and Xin Fan. Coconet: Coupled con-
trastive learning network with multi-level feature ensemble
for multi-modality image fusion. International Journal of
Computer Vision , pages 1â€“28, 2023. 1
[32] Jinyuan Liu, Zhu Liu, Guanyao Wu, Long Ma, Risheng
Liu, Wei Zhong, Zhongxuan Luo, and Xin Fan. Multi-
interactive feature learning and a full-time multi-modality
benchmark for image fusion and segmentation. In Proceed-
ings of the IEEE/CVF international conference on computer
vision , pages 8115â€“8124, 2023. 1
[33] Risheng Liu, Jinyuan Liu, Zhiying Jiang, Xin Fan, and
Zhongxuan Luo. A bilevel integrated model with data-driven
layer ensemble for multi-modality image fusion. IEEE
Transactions on Image Processing , 30:1261â€“1274, 2020. 2
[34] Risheng Liu, Zhu Liu, Jinyuan Liu, and Xin Fan. Searching
a hierarchically aggregated fusion architecture for fast multi-
modality image fusion. In Proceedings of the 29th ACM
International Conference on Multimedia , pages 1600â€“1608,
2021. 2
[35] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongx-
uan Luo. Retinex-inspired unrolling with cooperative prior
architecture search for low-light image enhancement. In Pro-
ceedings of the IEEE/CVF conference on computer vision
and pattern recognition , pages 10561â€“10570, 2021. 1
[36] Yipeng Liu, Jing Jin, Qiang Wang, Yi Shen, and Xiaoqiu
Dong. Region level based multi-focus image fusion using
quaternion wavelet and normalized cut. Signal Processing ,
97:9â€“30, 2014. 2
[37] Yu Liu, Xun Chen, Rabab K Ward, and Z Jane Wang. Image
fusion with convolutional sparse representation. IEEE signal
processing letters , 23(12):1882â€“1886, 2016. 2
[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng
Zhang, Stephen Lin, and Baining Guo. Swin transformer:
Hierarchical vision transformer using shifted windows. In
Proceedings of the IEEE/CVF international conference on
computer vision , pages 10012â€“10022, 2021. 2
[39] Zhu Liu, Jinyuan Liu, Guanyao Wu, Long Ma, Xin Fan,
and Risheng Liu. Bi-level dynamic learning for jointly
multi-modality image fusion and beyond. arXiv preprint
arXiv:2305.06720 , 2023. 2, 5[40] Yongzhi Long, Haitao Jia, Yida Zhong, Yadong Jiang, and
Yuming Jia. Rxdnfuse: A aggregated residual dense network
for infrared and visible image fusion. Information Fusion ,
69:128â€“141, 2021. 2
[41] Jiayi Ma, Chen Chen, Chang Li, and Jun Huang. Infrared and
visible image fusion via gradient transfer and total variation
minimization. Information Fusion , 31:100â€“109, 2016. 2
[42] Jiayi Ma, Wei Yu, Pengwei Liang, Chang Li, and Junjun
Jiang. Fusiongan: A generative adversarial network for in-
frared and visible image fusion. Information fusion , 48:11â€“
26, 2019. 2, 3
[43] Jiayi Ma, Pengwei Liang, Wei Yu, Chen Chen, Xiaojie Guo,
Jia Wu, and Junjun Jiang. Infrared and visible image fusion
via detail preserving adversarial learning. Information Fu-
sion, 54:85â€“98, 2020. 2
[44] Jiayi Ma, Han Xu, Junjun Jiang, Xiaoguang Mei, and Xiao-
Ping Zhang. Ddcgan: A dual-discriminator conditional gen-
erative adversarial network for multi-resolution image fu-
sion. IEEE Transactions on Image Processing , 29:4980â€“
4995, 2020. 3, 6, 7
[45] Jiayi Ma, Linfeng Tang, Meilong Xu, Hao Zhang, and
Guobao Xiao. Stdfusionnet: An infrared and visible im-
age fusion network based on salient target detection. IEEE
Transactions on Instrumentation and Measurement , 70:1â€“
13, 2021. 1, 2
[46] Jiayi Ma, Linfeng Tang, Fan Fan, Jun Huang, Xiaoguang
Mei, and Yong Ma. Swinfusion: Cross-domain long-range
learning for general image fusion via swin transformer.
IEEE/CAA Journal of Automatica Sinica , 9(7):1200â€“1217,
2022. 2
[47] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongx-
uan Luo. Toward fast, flexible, and robust low-light image
enhancement. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 5637â€“
5646, 2022. 1
[48] Long Ma, Dian Jin, Nan An, Jinyuan Liu, Xin Fan, Zhongx-
uan Luo, and Risheng Liu. Bilevel fast scene adaptation
for low-light image enhancement. International Journal of
Computer Vision , pages 1â€“19, 2023. 1
[49] Run Mao, Xian Song Fu, Ping-juan Niu, Hui Quan Wang,
Jie Pan, Shu Shu Li, and Lei Liu. Multi-directional lapla-
cian pyramid image fusion algorithm. In 2018 3rd Inter-
national Conference on Mechanical, Control and Computer
Engineering (ICMCCE) , pages 568â€“572. IEEE, 2018. 2
[50] Nirmala Paramanandham and Kishore Rajendiran. Infrared
and visible image fusion using discrete cosine transform and
swarm intelligence for surveillance applications. Infrared
Physics & Technology , 88:13â€“22, 2018. 1
[51] K. R. Prabhakar, V . S. Srikar, and R. V . Babu. Deepfuse:
A deep unsupervised approach for exposure fusion with ex-
treme exposure image pairs. In IEEE International Confer-
ence on Computer Vision , pages 4724â€“4732, 2017. 1, 7
[52] Guihong Qu, Dali Zhang, and Pingfan Yan. Information
measure for performance of image fusion. Electronics let-
ters, 38(7):1, 2002. 5
[53] Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou,
Ser Nam Lim, and Jiwen Lu. Hornet: Efficient high-
order spatial interactions with recursive gated convolutions.
26393
Advances in Neural Information Processing Systems , 35:
10353â€“10366, 2022. 2
[54] Linfeng Tang, Yuxin Deng, Yong Ma, Jun Huang, and Jiayi
Ma. Superfusion: A versatile image registration and fusion
network with semantic awareness. IEEE/CAA Journal of Au-
tomatica Sinica , 9(12):2121â€“2137, 2022. 1
[55] Linfeng Tang, Jiteng Yuan, and Jiayi Ma. Image fusion in
the loop of high-level vision tasks: A semantic-aware real-
time infrared and visible image fusion network. Information
Fusion , 82:28â€“42, 2022. 2
[56] Linfeng Tang, Jiteng Yuan, Hao Zhang, Xingyu Jiang, and
Jiayi Ma. Piafusion: A progressive infrared and visible im-
age fusion network based on illumination aware. Information
Fusion , 83:79â€“92, 2022. 2
[57] Linfeng Tang, Xinyu Xiang, Hao Zhang, Meiqi Gong, and
Jiayi Ma. Divfusion: Darkness-free infrared and visible im-
age fusion. Information Fusion , 91:477â€“493, 2023. 2, 3
[58] Linfeng Tang, Hao Zhang, Han Xu, and Jiayi Ma. Rethink-
ing the necessity of image fusion in high-level vision tasks:
A practical infrared and visible image fusion network based
on progressive semantic injection and scene fidelity. Infor-
mation Fusion , page 101870, 2023. 2
[59] Wei Tang, Fazhi He, and Yu Liu. Ydtr: Infrared and visible
image fusion via y-shape dynamic transformer. IEEE Trans-
actions on Multimedia , 2022. 2
[60] Alexander Toet. The tno multiband image data collection.
Data in brief , 15:249â€“251, 2017. 5, 7
[61] Di Wang, Jinyuan Liu, Xin Fan, and Risheng Liu. Unsuper-
vised misaligned infrared and visible image fusion via cross-
modality image generation and registration. arXiv preprint
arXiv:2205.11876 , 2022. 2, 6, 7
[62] Di Wang, Jinyuan Liu, Risheng Liu, and Xin Fan. An inter-
actively reinforced paradigm for joint infrared-visible image
fusion and saliency object detection. Information Fusion , 98:
101828, 2023. 2
[63] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So
Kweon. Cbam: Convolutional block attention module. In
Proceedings of the European conference on computer vision
(ECCV) , pages 3â€“19, 2018. 3
[64] Han Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin
Ling. U2fusion: A unified unsupervised image fusion net-
work. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 44(1):502â€“518, 2020. 5, 6, 7
[65] Han Xu, Hao Zhang, and Jiayi Ma. Classification saliency-
based rule for visible and infrared image fusion. IEEE Trans-
actions on Computational Imaging , 7:824â€“836, 2021. 2
[66] Meilong Xu, Linfeng Tang, Hao Zhang, and Jiayi Ma. In-
frared and visible image fusion via parallel scene and texture
learning. Pattern Recognition , 132:108929, 2022. 2
[67] Shuang Xu, Jiangshe Zhang, Zixiang Zhao, Kai Sun, Junmin
Liu, and Chunxia Zhang. Deep gradient projection networks
for pan-sharpening. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
1366â€“1375, 2021. 8
[68] Costas S Xydeas, Vladimir Petrovic, et al. Objective image
fusion performance measure. Electronics letters , 36(4):308â€“
309, 2000. 5[69] Junfeng Yang, Xueyang Fu, Yuwen Hu, Yue Huang, Xinghao
Ding, and John Paisley. Pannet: A deep network architecture
for pan-sharpening. In Proceedings of the IEEE international
conference on computer vision , pages 5449â€“5457, 2017. 8
[70] Wei Yu, Qi Zhu, Naishan Zheng, Jie Huang, Man Zhou, and
Feng Zhao. Learning non-uniform-sampling for ultra-high-
definition image enhancement. In Proceedings of the 31st
ACM International Conference on Multimedia , pages 1412â€“
1421, 2023. 2
[71] Hao Zhang and Jiayi Ma. Sdnet: A versatile squeeze-and-
decomposition network for real-time image fusion. Interna-
tional Journal of Computer Vision , 129:2761â€“2785, 2021. 6,
7
[72] Hao Zhang, Han Xu, Yang Xiao, Xiaojie Guo, and Jiayi Ma.
Rethinking the image fusion: A fast unified image fusion net-
work based on proportional maintenance of gradient and in-
tensity. In AAAI Conference on Artificial Intelligence , pages
12797â€“12804, 2020. 2, 5
[73] Jiale Zhang, Yulun Zhang, Jinjin Gu, Yongbing Zhang,
Linghe Kong, and Xin Yuan. Accurate image restora-
tion with attention retractable transformer. arXiv preprint
arXiv:2210.01427 , 2022. 4
[74] Qiong Zhang and Xavier Maldague. An adaptive fusion ap-
proach for infrared and visible images based on nsct and
compressed sensing. Infrared Physics & Technology , 74:11â€“
20, 2016. 2
[75] Yukang Zhang and Hanzi Wang. Diverse embedding expan-
sion network and low-light cross-modality benchmark for
visible-infrared person re-identification. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , pages 2153â€“2162, 2023. 1
[76] Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu,
Pengfei Li, and Jiangshe Zhang. Didfuse: Deep image de-
composition for infrared and visible image fusion. arXiv
preprint arXiv:2003.09210 , 2020. 2, 6, 7
[77] Zixiang Zhao, Shuang Xu, Jiangshe Zhang, Chengyang
Liang, Chunxia Zhang, and Junmin Liu. Efficient and model-
based infrared and visible image fusion via algorithm un-
rolling. IEEE Transactions on Circuits and Systems for Video
Technology , 32(3):1186â€“1196, 2021. 6, 7
[78] Naishan Zheng, Jie Huang, Feng Zhao, Xueyang Fu, and
Feng Wu. Unsupervised underexposed image enhancement
via self-illuminated and perceptual guidance. IEEE Transac-
tions on Multimedia , 2022. 2
[79] Naishan Zheng, Jie Huang, Qi Zhu, Man Zhou, Feng Zhao,
and Zheng-Jun Zha. Enhancement by your aesthetic: An in-
telligible unsupervised personalized enhancer for low-light
images. In Proceedings of the 30th ACM International Con-
ference on Multimedia , pages 6521â€“6529, 2022. 1
[80] Naishan Zheng, Jie Huang, Man Zhou, Zizheng Yang, Qi
Zhu, and Feng Zhao. Learning semantic degradation-aware
guidance for recognition-driven unsupervised low-light im-
age enhancement. In Proceedings of the AAAI Conference
on Artificial Intelligence , pages 3678â€“3686, 2023. 2
[81] Naishan Zheng, Man Zhou, Yanmeng Dong, Xiangyu Rui,
Jie Huang, Chongyi Li, and Feng Zhao. Empowering low-
light image enhancer through customized learnable priors.
26394
InProceedings of the IEEE/CVF International Conference
on Computer Vision , pages 12559â€“12569, 2023. 1
[82] Huabing Zhou, Wei Wu, Yanduo Zhang, Jiayi Ma, and
Haibin Ling. Semantic-supervised infrared and visible image
fusion via a dual-discriminator generative adversarial net-
work. IEEE Transactions on Multimedia , 2021. 3
[83] Huabing Zhou, Jilei Hou, Yanduo Zhang, Jiayi Ma, and
Haibin Ling. Unified gradient-and intensity-discriminator
generative adversarial network for image fusion. Informa-
tion Fusion , 88:184â€“201, 2022. 2
[84] Man Zhou, Jie Huang, Yanchi Fang, Xueyang Fu, and Aip-
ing Liu. Pan-sharpening with customized transformer and
invertible neural network. In Proceedings of the AAAI Con-
ference on Artificial Intelligence , pages 3553â€“3561, 2022. 2,
8
[85] Man Zhou, Keyu Yan, Jie Huang, Zihe Yang, Xueyang Fu,
and Feng Zhao. Mutual information-driven pan-sharpening.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 1798â€“1808, 2022. 8
[86] Qi Zhu, Man Zhou, Naishan Zheng, Chongyi Li, Jie Huang,
and Feng Zhao. Exploring temporal frequency spectrum in
deep video deblurring. In Proceedings of the IEEE/CVF In-
ternational Conference on Computer Vision , pages 12428â€“
12437, 2023. 1
[87] Qi Zhu, Jie Huang, Naishan Zheng, Hongzhi Gao, Chongyi
Li, Yuan Xu, Feng Zhao, et al. Fouridown: Factoring down-
sampling into shuffling and superposing. Advances in Neural
Information Processing Systems , 36, 2024. 1
[88] Zhiqin Zhu, Hongpeng Yin, Yi Chai, Yanxia Li, and Guanqiu
Qi. A novel multi-modality image fusion method based on
image decomposition and sparse representation. Information
Sciences , 432:516â€“529, 2018. 1
26395
