VINECS: Video-based Neural Character Skinning
Zhouyingcheng Liao1,2Vladislav Golyanik1Marc Habermann1,3Christian Theobalt1,3
1Max Planck Institute for Informatics, Saarland Informatics Campus2The University of Hong Kong
3Saarbr Â¨ucken Research Center for Visual Computing, Interaction and AI
Abstract
Rigging and skinning clothed human avatars is a chal-
lenging task and traditionally requires a lot of manual work
and expertise. Recent methods addressing it either general-
ize across different characters or focus on capturing the dy-
namics of a single character observed under different pose
configurations. However, the former methods typically pre-
dict solely static skinning weights, which perform poorly
for highly articulated poses, and the latter ones either re-
quire dense 3D character scans in different poses or can-
not generate an explicit mesh with vertex correspondence
over time. To address these challenges, we propose a fully
automated approach for creating a fully rigged character
with pose-dependent skinning weights, which can be solely
learned from multi-view video. Therefore, we first acquire
a rigged template, which is then statically skinned. Next, a
coordinate-based MLP learns a skinning weights field pa-
rameterized over the position in a canonical pose space and
the respective pose. Moreover, we introduce our pose- and
view-dependent appearance field allowing us to differen-
tiably render and supervise the posed mesh using multi-view
imagery. We show that our approach outperforms state-of-
the-art while not relying on dense 4D scans. More details
can be found on our project page1.
1. Introduction
Animating 3D characters is a long-standing and challeng-
ing problem in computer graphics and vision. Traditionally,
rigging, the process of positioning a sparse skeletal structure
inside a dense 3D mesh, and skinning, the process of asso-
ciating vertices with the skeleton, require large amounts of
manual effort by experienced artists. This is not only expen-
sive and time-consuming but also inherently difficult since
one set of skinning weights is typically not ideal for all sorts
of articulated character poses.
There are also methods for automated rigging and skin-
ning. Some recent works [33, 34, 41, 59] propose a
learning-based solution for obtaining skinning and rigging
1https : / / people . mpi - inf . mpg . de / Ëœmhaberma /
projects/2023-Vinecs
Multi-view 
imagesâ€¦
Input posePose-dependent 
skinningAnimated 
characterVINECS
Animation
Rigged 
characterFigure 1. We propose the first end-to-end trainable method for
generating a dense and rigged 3D character mesh including
learned pose-dependent skinning weights solely from multi-
view videos . The results demonstrate that our model can generate
visually-plausible geometry without requiring manual user input.
given a single 3D character mesh. However, they do not
consider pose-dependent skinning correctives and lead to
the typical appearance artifacts. Therefore, other techniques
focus on pose-dependent skinning formulations accounting
for pose-dependent variations and deformations [3, 30, 40,
52, 57]. Noteworthy, none of these methods tried to learn
pose-dependent skinning weight correctives purely from 2D
image data . Instead, they typically assume a 3D mesh is
given, and most of them solely work on a fixed mesh reso-
lution, which cannot be easily adjusted.
To overcome these limitations, we propose VINECS,
i.e.,VIdeo-based NEuralCharacter Skinning. Given solely
a multi-view video of a human actor, our method creates a
fully rigged, skinned, and textured 3D template (including
hands) along with pose-dependent skinning weights while
no manual editing is required. Moreover, VINECS enables
multi-resolution character skinning since skinning weights
can be sampled over a continuous 3D canonical space,
which is not limited to any specific mesh resolution.
For each character, we select one canonical pose frame,
for which we extract a dense and textured 3D template
mesh using implicit surface reconstruction techniques [55].
Given the template and the respective pose obtained from
an off-the-shelf multi-view markerless capture system [53],
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1377
we leverage an auto-rigging and skinning method, Pinoc-
chio [7], for obtaining an initial set of skinning weights.
Note that Pinocchio produces only a static set of skinning
weights, which leads to the typical geometry artifacts when
poses deviate too much from the canonical pose. There-
fore, we propose a coordinate-based multi-layer perceptron
(MLP)â€”which takes as input a 3D point on the surface
of the template in canonical pose and the skeletal poseâ€”
and predicts the pose-dependent skinning weights at that
point. Since we want to learn the skinning weights solely
from multi-view imagery, we design dedicated losses con-
sisting of a silhouette loss and a rendering loss. Specifically,
we find that obtaining a well-behaved convergence is chal-
lenging only using a static texture for the rendering loss.
Thus, we adopt the rendering formulation with an albedo
network and a shadow network, which predict pose- and
view-dependent appearance of our 3D character resulting in
an improved convergence and skinning weights prediction.
In summary, our technical contributions are as follows:
â€¢ An end-to-end trainable method for animation-ready ex-
plicit character creation, involving static template gener-
ation, rigging, and pose-dependent skinning solely using
2D multi-view videos (as few as 10 views).
â€¢ A coordinate-based and pose-dependent skinning formu-
lation, which enables multi-resolution skinning.
â€¢ A dedicated character appearance formulation and differ-
entiable rendering enabling weakly supervised learning
based on multi-view videos.
We compare VINECS to the state-of-the-art and our results
confirm that our method is a clear step toward accurate and
automated creation of animatable 3D characters.
Potential Impact. The focus of this work is notto model
deformable geometry [13, 14, 20, 32] or photorealistic ap-
pearance [15, 16, 25, 35, 44, 47, 48] of humans but on learn-
ing 3D characters with pose-dependent skinning from 2D
observations only. We believe our method can greatly ben-
efit many downstream tasks as many existing works lever-
age naked body models [36] for space canonicalization and
feature assignment, fundamentally limiting them to cloth-
ing types of the same topology as the human body. As our
rigging and skinning are accurate, easy-to-use, and robust to
any type of clothing, our approach can stimulate research on
the modeling of geometry and appearance of loose apparel.
2. Related Work
Static Skinning. Linear Blend Skinning (LBS) [28, 38]
linearly blends rigid transformations of each bone to obtain
a posed geometry. However, this formulation can lead to
a collapse of geometry near the skeleton joints, also called
candy-wrappers effect . To overcome this, several follow-
up works have been proposed using spherical representa-
tions [22], sweep surfaces [17], curve-based skeleton pa-
rameterizations [61], Dual Quaternions [23], or by optimiz-ing the centers of rotation [27]. Other works focused more
on computing the skinning weights itself by modeling the
weight distribution as a heat diffusion process [7], as illu-
mination [58], or by voxelizing the mesh to increase ro-
bustness [9, 10]. Later on, deformation editing was also
modeled using a Laplacian energy formulation [18], and
user-defined splines on the mesh enable intuitive editing of
skinning-based deformation behaviors [6]. They all only
assume a single geometry and skeleton are given.
In contrast, data-driven methods assume multiple in-
stances of the same character under different poses or a
large collection of rigged and skinned characters. Earlier
works [19, 26] requires meshes under different poses and
then automatically detect skeleton joints and compute the
skinning weights. Recently, NeuroSkinning [34] was pro-
posed as the first learning-based method predicting a static
set of skinning weights given the mesh and a corresponding
skeleton. RigNet [59] instead jointly predicts the skeleton
as well as a set of skinning weights solely given a 3D mesh
as input. Therefore, they collected a large dataset of 3D
characters including artist-designed skeletons and skinning
weights. SkinningNet [41] proposed a two-stream graph
convolutional network architecture for improved skinning
weight prediction given a mesh and the skeleton. Yang et
al. [60] proposed to predict the 3D shape together with the
rigging from a single image. Liao et al. [33] introduced a
learning-based method which jointly learns to predict skin-
ning weights and transfer poses.
What all the above methods have in common is that they
do not explicitly account for dynamic or pose-dependent ef-
fects, such as muscle bulging or coarse cloth deformations.
In contrast, our proposed method learns pose-dependent
skinning weights, which alleviate typical artifacts that arise
when using a static set of skinning weights. Moreover,
it can also represent coarse deformations induced by the
skeletal pose, such as coarse cloth deformations.
Dynamic Methods. More closely related to our method
are approaches that account for dynamic or pose-dependent
effects, which are discussed next. There are works [11]
that allow an artist to add pose-dependent effects by paint-
ing onto specialized textures or by adding so-called helper
bones [42]. Alternatively, Kavan et al. [21] proposed to find
the optimal skinning weights by minimizing an elastic en-
ergy and adding joint-based deformers. Interestingly, those
methods do not assume any dynamic observations of the
character while still modeling dynamic effects. However,
they either require manual editing [11, 42] or can mostly
model elasticity-based dynamics [21].
Similar to the static methods, several dynamic methods
assume a (large) dataset of character meshes is given. Ear-
lier works [3, 40, 46, 57] account for dynamic effects by
capturing a set of range scans depicting the subject in differ-
ent poses. Given a new pose, a set of pose-dependent blend
1378
weights is estimated, which linearly combines the set of ex-
ample poses in the dataset or alternatively uses linear and ra-
dial basis functions [52]. However, since the complexity of
computation typically linearly scales with the number of ex-
ample poses in the dataset, those methods cannot scale up to
a very large number of examples (e.g., 20000 pose frames),
which is in stark contrast to our approach that can deal with
such an amount of data. There are also parametric naked hu-
man body models [4, 5, 36] learned by applying PCA over
a large set of human scans, which can account for different
shape- and pose-dependent deformations. In contrast, we
are building a highly-detailed rigged and skinned character
that can wear (loose) clothing in a completely automated
fashion. Li et al. [30] predict static skinning weights and
pose-dependent neural blend shapes. They assume a dataset
of dense meshes is provided, whereas we purely learn pose-
dependent skinning weights from videos. SCANimate [49]
also requires dense point clouds, and learns a static set
of skinning weights, while pose-dependent shapes are pre-
dicted by a pose-aware implicit field. SNARF [8] repre-
sents the character as a pose-dependent neural implicit sur-
face in the canonical pose space while the learned skin-
ning weights are pose-agnostic. Again, they require point
cloud data while our method can be supervised directly on
the video. In summary, none of the above methods that
consider pose-dependent effects investigated learning solely
from 2D imagery. Thus, we are the first method demon-
strating that dynamic effects in the form of pose-dependent
skinning weights can be learned only from videos.
Recently, some works were proposed to skin and rig im-
plicit surface representations learn an implicit human avatar.
ARAH [56] learns an implicit human model using neural
rendering. However, as it relies on a SMPL template and an
inverse root finding to convert from view space to canonical
space, it easily fails when the garment deviates much from
the body. Importantly, it also does not have the vertex cor-
respondence over time. TA V A [31] avoids these problems
by getting rid of a template, which, however, leads to poor
geometry recovery mainly due to their density-based geom-
etry representation. While these methods can reconstruct
an implicit avatar, they do not learn a high-quality skinning
weight, which is important to traditional graphics pipeline.
3. Method
Given synchronized and calibrated multi-view RGB videos
of a human character in motion, our goal is to learn an an-
imatable 3D human model with dynamic skinning, without
any manual process, such as meshing, rigging, and skin-
ning. To this end, we first extract the static geometry of
the actor from one canonical pose frame (typically depict-
ing the actor in a T-pose) of the recording and automati-
cally compute initial skinning weights (Sec. 3.1). However,
the initial skinning weights are calculated only based onthe static geometry of the human character, and, as stud-
ied in earlier works, one set of weights might be ideal for
one pose while it can lead to strong artifacts in other poses.
Thus, we propose a skinning network, called SkinNet, to
predict the dynamic skinning weights as a function of the
skeletal pose and 3D coordinates in the canonical space
(Sec. 3.2). Given the skeletal pose, the static geometry,
and the pose-dependent skinning weights, we leverage Lin-
ear Blend Skinning (LBS) to obtain the posed geometry.
We then use a differentiable renderer to weakly supervise
the skinning weights solely based on multi-view imagery.
Thereafter, we propose a novel appearance field (Sec. 3.3),
which is composed of an albedo component and a pose-
and view-dependent shadow/shading component. In addi-
tion, we propose several regularization losses ensuring that
the skinning weights are also robust to out-of-distribution
poses and that the final animated geometry looks visually
plausible (Sec. 3.4). An overview is shown in Fig. 2.
Data Preparation. For a human subject, we record a
multi-view video of the performance using Ccalibrated and
synchronized cameras with a total length of Fframes. For
all frames Ic,f, we calculate the foreground segmentation
using color keying or background matting [51] and com-
pute respective distance transform images Dc,f. Here, cand
frefer to the respective camera and frame index. We cap-
ture the size of the human skeleton and the motion using
markerless motion capture [53]. The skeleton motion is pa-
rameterized by a root rotation Î±fâˆˆR3, a root translation
tfâˆˆR3, and the joint angles ÏfâˆˆRD. The full pose vector
is defined as Î¸f= [Î±f,tf,Ïf]TâˆˆR3+3+D.
Notation. To represent a variable in the canonical space,
we use a bar over the letter, i.e., Â¯x. For any point in canoni-
cal and global space, we use Â¯xandx, respectively, and mesh
vertices are denoted as Â¯viandviwhere iis the ith vertex.
Without loss of generality, we assume the frame fis fixed
in the following (if not explicitly mentioned otherwise) and
omit the subscript for better readability.
3.1. Canonical Character Model
As the first step, we acquire a canonical model of the actor.
Recently, implicit surface and coordinate-based representa-
tions [45] have gained considerable attention due to their
flexibility and comparably simple integration into learning
frameworks, and they were also leveraged in the context of
pose-dependent human deformation modeling [8]. More-
over, we compared the reconstruction accuracy on the first
frame f=0 between classical multi-view stereo and im-
plicit reconstruction method called NeuS [55] in the sup-
plemental material. Interestingly, NeuS has a much higher
level of detail and comparably less noise on the surface.
Thus, we use NeuS [55] to reconstruct the mesh in the first
frame, also called the canonical frame/space.
Implicit vs. Explicit Canonical Model. In stark con-
1379
Multi-view 
images of the 
first frameSkinNet
Current skinning Articulated meshCanonical Character Model Neural Character Skinning
Differentiable 
rendererRendering 
loss
Silhouette 
lossAlbedo 
NetShadow
NetAlbedoShadow 
MapAuxiliary
AppearanceCurrent pose ðœƒ W
Initial 
skinningð–à­§à­¬à­§à­²ð‘“à° Template 
mesh Và´¥
ð‘Žà° á‡±Poseðœƒ
ViewdPinocchioNeuSLBS
ðœƒ
V
ð‘Ÿà° á‡±á‡±d
â€¦
Point 
queryInference
ac ð‘ Figure 2. Method overview. Our method, VINECS, is a fully automated solution to template generation, rigging, and pose-dependent
skinning solely using multi-view videos. We first recover the skeletal poses using markerless motion capture [53] and a static template
mesh using implicit surface reconstruction [55]. An initial skinning is obtained by a heat diffusion process [7]. Since one fixed set of
skinning weights can lead to artifacts when significantly changing the character pose, we learn pose-dependent skinning weights using
a coordinate-based MLP. An auxiliary appearance field is applied to better supervise the learning guide the training of pose-dependent
skinning. Note that we do not intend to learn a detailed appearance and only the components in the blue box are used during inference.
trast to Chen et al. [8], we run Marching Cubes [37] to con-
vert the implicit surface into an explicit mesh and obtain
per-vertex colors from NeuS by setting the viewing direc-
tion to the normal direction of the mesh. The reason for
choosing an explicit mesh in canonical space is three-fold:
1.) It avoids running the rather slow Marching Cubes algo-
rithm for every potentially posed mesh. 2.) We avoid the
backward skinning [8], which boils down to a correspon-
dence search that has to be solved using iterative optimiza-
tion. 3.) We can maintain correspondence on the surface
across poses, which is especially useful when texturing and
other editing have to be applied. Thus, we aim for a hybrid
strategy for template acquisition, i.e., using the state-of-the-
art method for implicit surface reconstruction and then con-
verting it to an explicit mesh.
Meshing and Initial Skinning. During training, we
downsample the template mesh to Nvertices where Â¯viâˆˆR3
denotes the ith vertex. Given the template mesh and the cor-
responding posed skeleton, i.e. Î¸f, we utilize Pinocchio [7]
to obtain an initial set of skinning weights winit,iâˆˆRJfor
each vertex i(see Fig. 4). Here, Jdenotes the number of
skinning joints. Given these weights and a skeletal pose Î¸,
we can transform the vertex Â¯vifrom the canonical space to
the posed space using LBS [28]:
vi=LBS(Â¯vi,winit,i,Î¸):R3Ã—RJÃ—RDâ†’R3. (1)
Mesh Parsing. Since different body parts and materials
(skin vs. clothing) have different deformation behaviors, we
compute per-vertex human parsing labels. We render the 3D
model into all camera views and apply a 2D human parsing
approach [29] on each view. We then perform max-voting
to obtain the per-vertex human parsing labels. Note that so
far, we obtained a segmented, rigged, and statically skinned
character model without any manual user input.3.2. Pose-dependent Skinning Field
So far, the initial skinning weights obtained in Sec. 3.1 have
two main limitations: 1.) They are solely computed on a
single static pose leading to artifacts such as candy wrap-
pers and mesh distortions if the new pose significantly dif-
fers from the canonical one. 2.) The set of weights is com-
puted per-vertex. Thus, the resolution of the mesh has to
be fixed beforehand and any change in the mesh requires a
new computation of skinning weights or some complicated
weight transfer from one mesh to the other.
To resolve this, we propose SkinNet fÏ‰, which is a
coordinate-based MLP where Ï‰denotes the trainable net-
work weights. Given any 3D point Â¯xin the canonical space,
fÏ‰predicts its skinning weight wconditioned on the nor-
malized human pose ËœÎ¸, which can be formulated as:
wËœÎ¸=fÏ‰(Â¯x,ËœÎ¸):R3Ã—RDâ†’RJ, (2)
where Â¯xand ËœÎ¸are concatenated before being fed into the
MLP. Here, the normalized pose ËœÎ¸is obtained by neglecting
the global translation as well as the yaw angle, i.e., the rota-
tion around the spine. The intuition behind is that the skin-
ning weights do not depend on where in global 3D space
the person performs a respective pose, and, similarly, the
skinning weight prediction should be agnostic to which di-
rection the person is facing. Thus, we mask those degrees
of freedom before feeding the pose vector into the network.
To obtain the transformed point xin posed space, one can
insert Eq. (2) into Eq. (1) resulting in:
x=LBS(Â¯x,fÏ‰(Â¯x,ËœÎ¸),Î¸). (3)
Revisiting the drawbacks of initial skinning discussed
above, the SkinNet now supports pose-dependent skinning
1380
weights such that the artifacts of static skinning can be re-
duced. Moreover, since SkinNet is a coordinate-based MLP,
it supports arbitrary mesh resolution since each vertex is
queried independently, which is in stark contrast to an archi-
tecture that outputs all skinning weights at once and where
the final tensor shape is fixed to a specific mesh resolution.
3.3. Auxiliary Appearance Field
We can now pose the geometry with our pose-dependent
skinning weight formulation. However, since we want to
supervise SkinNet solely on multi-view images, we also
have to model the appearance of the actor. A naive choice
would be to just leverage the static vertex colors acquired
from NeuS. However, we found that this texture contains
baked-in shadows and wrinkles, and cannot account for the
pose- and view-dependent appearance changes harming the
training of the SkinNet. We also refer to our ablation studies
in Sec. 4.3. To address this issue, we propose an auxiliary
appearance field, which consists of an albedo field predict-
ing a pose- and view- agnostic color for a given point Â¯xin
the canonical space and a shadow field predicting the pose-
and view- dependent shadow/shading properties of Â¯x. Im-
portant to note here is that we are notinterested in creating
a highly photorealistic appearance model of the human, but
we mainly use it as an auxiliary tool to better supervise the
learning of the skinning weight network.
More specifically, the albedo field, referred to as
AlbedoNet , is an MLP that predicts a static albedo value
aÏ‰â€²(Â¯x) =aâˆˆR3for a given 3D sample point in the canon-
ical space. Here, Ï‰â€²are the trainable weights of the albedo
network. Moreover, the shadow field is also parameter-
ized by an MLP rÏ‰â€²â€²(Â¯x,Î¸,n,d) =sâˆˆR+, called Shad-
owNet , which predicts a scalar multiplier accounting for
color changes due to shadows and shading. Note that we
assume that such effects happen uniformly across all color
channels and, thus, we are only predicting a single scalar
instead of a scaling factor per color channel. In addition
to taking the point in canonical space, the shadow field
also takes the skeletal pose Î¸as input as well as the sur-
face normal nofxin global space and the viewing direc-
tion d, which allows to potentially model pose- and view-
dependent lighting effects such as shadows and shading. Ï‰â€²â€²
denotes the respective network weights. Details about the
architectures are provided in the supplemental document.
The final color câˆˆR3for point xis computed as
c=aÏ‰â€²(Â¯x)rÏ‰â€²â€²(Â¯x,Î¸,n,d). (4)
3.4. Multi-view Video-based Supervision
Next, we describe our dedicated supervision strategy that
solely requires multi-view imagery. We first introduce the
individual loss terms and then describe our training strat-
egy. Importantly, we can obtain a posed template meshVâˆˆRNÃ—3and the pose- and view-dependent per-vertex col-
orsCcâˆˆRNÃ—3by evaluating Eq. (3) and Eq. (4) for all ver-
tices Â¯viand views c. Moreover, the following losses that are
applied to the deformed geometry and its appearance can di-
rectly backpropagate into the respective networks since our
formulation is fully differentiable.
Silhouette Loss. This loss [15] ensures that the pro-
jected posed geometry projects onto the foreground masks
for all views (see supp. material). Since our pose-dependent
skinning field Eq. (3) is differentiable with respect to the
SkinNet weights Ï‰, the silhouette loss can provide super-
vision and learn pose-dependent skinning by matching the
posed model silhouette and the image silhouettes.
Rendering Loss. Although the silhouette loss ensures
that the posed geometry matches the multi-view silhouettes,
it cannot supervise drifts on the visual hull. Supervision
directly from the RGB images can help with resolving this
issue since appearance features can constrain drifts in the
image plane. Thus, we introduce the rendering loss
Lrend=C
âˆ‘
c=1âˆ¥Î c(V,Cc)âˆ’Icâˆ¥1, (5)
which leverages a differentiable renderer Î cbased on the
one of Habermann et al. [15] to render the posed and col-
ored model into all camera views cthat is then compared to
the ground truth image Ic. Importantly, this loss can super-
vise both the pose-dependent skinning field as well as the
pose- and view-dependent appearance field.
Laplacian Loss. Since the data terms alone can still lead
to degenerate results, we also introduce some regularization
on the geometry and the skinning weights. First, we regu-
larize the posed geometry with a Laplacian loss
Llap=N
âˆ‘
i=1âˆ¥viâˆ’1
|Ni|âˆ‘
kâˆˆNivkâˆ¥2
2, (6)
whereNiis the indices of one-ring neighborhood of vertex
i. This regularizer ensures locally smooth geometry.
Skinning Regularization. We found that SkinNet can
overfit the training poses and respective deformations by
predicting non-local skinning weights, e.g., the foot joint
has a non-zero skinning weight for vertices on the shoulder.
This, however, harms generalization to novel poses at test
time. To prevent this, we regularize the skinning weights
by constraining their values based on the geodesic distance
dgeobetween the vertex and the initial body part it belongs
to. This can be formalized with the following loss
Lskin=N
âˆ‘
i=1J
âˆ‘
j=1wi,jmin kâˆˆAjdgeo(Â¯vi,Â¯vk)
dgeomaxr
, (7)
Aj={k|j=argmax
twinit,k,t}, (8)
1381
whereAjis the set of vertices, which have the highest
skinning weight with respect to joint j.r=3 is a hyper-
parameter which controls how penalization increases as
the geodesic distance increases. dgeomax is the maximum
geodesic distance between any of the vertex pairs.
Part-wise Regularization. In contrast to the clothed
areas, the skin part has rather static skinning. Thus, we
use the initial skinning to regularize the predicted skinning
weights of the skin part, i.e., the set Gof vertices, which
have the parsing label skin. However, even for skin vertices,
we found that the initial skinning weights around the joints
are not accurate while the rigid parts (i.e., the vertices away
from joints) usually have correct initial skinning. We iden-
tify the set of rigid vertices as R={k|max(winit,k)>u}
where u=0.95. Thus, our part loss
Lpart=âˆ‘
iâˆˆRâˆ©Gâˆ¥fÏ‰(Â¯vi,ËœÎ¸)âˆ’winit,iâˆ¥2
2, (9)
enforces that the predicted skinning weights are close to the
initial ones iffthe human parsing label of a vertex is the skin
(G) and the vertex belongs to the rigid part ( R).
Training Strategy. Our training proceeds in four stages.
First, we train SkinNet without using the rendering loss
Lrend. This step is required to ensure that the posed mesh
roughly overlaps with the foreground masks. Next, the
AlbedoNet is trained using only the rendering loss Lrend
while the SkinNet weights Ï‰are fixed. Importantly, the
AlbedoNet is notpose-dependent, but we optimize the
weights Ï‰â€²across all frames. Afterward, the AlbedoNet and
the SkinNet are fixed and we train the ShadowNet using the
rendering loss Lrend. Again we train on all training frames.
Lastly, since we now have a pose- and view-dependent ap-
pearance model available, we refine the SkinNet weights
including the rendering loss. We found this gives the best
performance, as validated by our results.
4. Results
Dataset. We evaluate our method on 3 subjects wearing
tight and loose clothing, i.e., pants or skirts. Two of them
are from the DynaCap dataset [15] and they are referred as
D2andD5.D2is wearing short pants and a T-shirt while
D5is in a skirt. The total numbers of cameras are 94 and
101 for those two sequences and our method can be trained
using as few as 10 cameras. There are around 19000 train-
ing frames per subject depicting the actor in various articu-
lated poses. For evaluation, there is a separate testing set of
around 7000 frames per subject. For convenience, we keep
1 out of 10 frames for training and evaluation. Addition-
ally, we captured a new subject, V6, wearing a T-shirt and
trousers using 116 cameras. We captured a separate training
(17730 frames) and testing sequence (5000 frames). Impor-
tantly, for this sequence, we recover hand gestures and a
respective 3D model including hands.Metrics. For evaluation, we reconstruct the dense 3D
geometry for the testing frames using multi-view stereo re-
construction [2]. We do not use NeuS here as it is very
time-consuming. Those dense meshes serve as ground truth
scans and we compare the posed meshes recovered by our
method as well as competing methods in terms of the Cham-
fer distance and the Hausdorff distance going from the re-
covered mesh to the ground truth scan (M2S) and vice versa
(S2M). We report the average results over all testing frames.
All metrics are reported in centimeter scale.
Competing Methods. We compare our work to our re-
implementation of Pinocchio [7], which proposed a heat
diffusion process to obtain a static set of skinning weights.
Moreover, we compare to SCANimate [49] and SNARF [8],
which, in stark contrast to our work, require dense point
clouds for training. They learn static skinning weights and
a pose-aware implicit shape. Since our approach mostly
focuses on skinning, we compare to SCANimate with and
without pose-aware shape, referred to as SCANimate and
SCANimate* in the following. Finally, we compare to
RigNet [59], which can jointly predict a rigging skeleton
and static skinning weights given a template. Unfortunately,
SkinningNet [41] and NeuroSkinning [34] do not provide
their code and, thus, we cannot compare to them. For
more details and comparisons to other (slightly less) related
works [31, 56], we refer to the supplemental document.
4.1. Qualitative Results
In Fig. 3, we qualitatively evaluate our approach. In the
first column, we show the reconstructed static geometry for
different subjects. In the following columns, we show the
characters in novel poses from the test set overlayed onto
the respective images. Note that our posed character nicely
matches the reference image, which confirms that our pose-
dependent skinning indeed generalizes to novel poses and
results in accurately posed meshes. In Fig. 4, we further
visualize the effect of the pose-dependent skinning weights
obtained by our approach. The initial skinning weights
are fixed across poses leading to the typical skinning arti-
facts and wrong deformation behavior. In contrast, our pro-
posed pose-dependent skinning weights adapt according to
the pose and as a result the deformations look more natural.
4.2. Comparison
In Tab. 1, we compare our results to previous works. Note
that we achieve the most accurate results when compared
to works only focusing on character skinning. This val-
idates the necessity of pose-dependent skinning weights
since other works rely on static ones. In this table, SCAN-
imate and SNARF predict pose-aware shapes while assum-
ing dense point clouds are given. Since it is not the scope of
this work to predict pose-dependent non-rigid deformations
and our method solely learns from video data, we placed
1382
Ref.
imageTemplate 
meshPredicted 
meshPredicted 
appearanceRef.
imagePredicted 
meshPredicted 
appearanceRef.
imagePredicted 
meshPredicted 
appearance
D2 D5 V6Figure 3. Qualitative results. From left to right: Recovered static character template. Reference image showing a test pose. Our posed
character overlayed onto the reference image. The precise overlay of the posed geometry confirms the effectiveness of our approach.
Quantitative Geometry Comparison
Subject D2 D5 V6
Method Chamfer â†“M2Sâ†“S2Mâ†“Chamfer â†“M2Sâ†“S2Mâ†“Chamfer â†“M2Sâ†“S2Mâ†“
Pinnochio [7] 3.760 2.162 1.599 5.077 2.811 2.267 3.358 1.871 1.487
SCANimateâˆ—[49] 3.750 2.099 1.650 5.453 2.965 2.488 3.502 1.890 1.612
RigNet [59] 3.599 2.078 1.521 4.989 2.836 2.153 3.369 1.894 1.475
Ours 3.034 1.746 1.288 4.512 2.442 2.070 2.993 1.719 1.274
SCANimate [49] 2.842 1.646 1.196 4.982 2.284 2.698 3.154 1.714 1.440
SNARF [8] 2.591 1.452 1.139 4.320 2.288 2.032 2.760 1.496 1.263
Table 1. We compare our method to previous works on 3 subjects. Our method obtains better results than other skinning-based approaches.
It is even close to SCANimate and SNARF, which require dense pointclouds while we solely use multi-view video. Importantly, they focus
on learning pose-aware shapes while our goal is to obtain a fully rigged and skinned character in a completely automated fashion.
Pose 1 Pose 2
Initial 
skinningOur 
prediction
Figure 4. Qualitative results showing the pose-dependent skinning
weights. As a reference, we show the result using the initial static
skinning weights. Notably, using static skinning weights leads to
artifacts and worse deformation. When using our pose-dependent
skinning, one can see a clear improvement in the final result and
we also show how the weights change over different poses.
them separately. Nonetheless, it is worth noting that our
approach is close to SCANimate and SNARF in terms of
accuracy and better than SCANimate for subject D5andV6
even though our training does not require point clouds.
We also visually compare to those works in Fig. 5. We
can see qualitatively our method achieves a lower error es-
pecially in joint regions where most of the skinning artifactsAblation Study on D2
Method Chamfer â†“M2Sâ†“S2Mâ†“
Initial weights 3.761 2.162 1.599
Static weights 3.354 1.935 1.418
w/oLrend 3.116 1.783 1.333
w/ albedo-only 3.047 1.750 1.297
w/ NeuS appearance 3.152 1.786 1.365
w/ NeuS albedo 3.137 1.795 1.342
w/ displacements 3.191 1.805 1.386
Ours (10 views) 3.129 1.791 1.337
Ours 3.034 1.746 1.288
Table 2. We compare our method against baselines. All designs
improve the accuracy confirming the effectiveness of VINECS.
become visible. This can be nicely seen in the error maps,
which visualize the per-vertex error. Although SCAnimate
and SNARF perform well on naked body and tight clothes
regions, they are worse than ours around loose clothing, as
they rely on SMPL while our method does not have such a
restriction. Further, the posed meshes clearly show fewer
artifacts for our approach compared to others. Note that
the entire character including the geometry, rigged skele-
ton, and the pose-dependent skinning can be obtained solely
from multi-view video without any manual user input.
1383
GT
 Pinocchio
 RigNet
 SCANimate
 SCANimate*
Ours
0 cm10 cm
SNARF
Figure 5. Qualitative comparison. For each method, we visualize the recovered posed geometry as well as the per-vertex error map when
comparing the ground truth in terms of M2S. Note that our method consistently shows the lowest error and also has the least visual artifacts.
GTInitial 
weightsStatic 
weights
without 
â„’à­°à­£à­¬à­¢Ours
Figure 6. We compare the initial skinning, the static skinning op-
timized using our supervision scheme, ablating the rendering loss,
and our pose-dependent skinning. We outperform all baselines.
4.3. Ablation
Next, we evaluate our design choices in Tab. 2. First,
we evaluate the effect of pose-dependent skinning weights.
Therefore, we compare our final result to the initial skin-
ning, which we obtain by leveraging Pinocchio [7] and
a version of our method where we use our supervision
scheme, but optimize a static set of skinning weights, i.e.,
the weights are fixed across all poses. From the results, we
can see that pose-dependent weights significantly improve
the accuracy over the two baselines proofing the effective-
ness of our formulation. This is also visualized in Fig. 6.
Another key aspect when learning those weights solely
from video data is our proposed supervision scheme. When
we do not employ the dense rendering loss or soley use the
albedo without shadow estimation, the performance drops.
Moreover, we also evaluate the accuracy when using a
dense rendering loss with the static appearance from NeuS
and using NeuS appearance as albedo while the shadow
network is learned. We found that our proposed rendering
scheme performs best. To validate the effectiveness of train-
ing pose-dependent skinning, we replace the SkinNet by a
network predicting per-vertex displacement, which gener-
alizes worse than SkinNet. We train our model with sparse
cameras (10 views), and the performance is still satisfying.5. Discussion and Conclusion
Limitations. Although we propose the first method for
learning pose-dependent skinning solely from multi-view
video, our method still has some limitations, which opens
up interesting directions for future work. Currently, we
query the SkinNet for every vertex in canonical space,
which may be inefficient when many points are sampled.
Thus, future work could involve exploring more efficient
architectures like hashgrids [43]. While hand and body can
be faithfully animated, there is no control over the facial
expression. Thus, extending our automated character cre-
ation pipeline with some parametric model for facial ex-
pressions might be a promising direction. Also, a joint con-
sideration of rigging, skinning, and pose tracking may fur-
ther improve the quality of the 3D animation-ready charac-
ters. Lastly, some artifacts still remain since skinning alone
cannot account for high-frequency deformations. Combin-
ing VINECS with downstream tasks such as human surface
tracking/modeling could alleviate such artifacts.
Conclusion. We proposed the first method for creating
a fully rigged and skinned character solely from multi-view
video without any manual processing. To this end, we first
acquire a canonical template using an implicit surface re-
construction, automated initial skinning and mesh parsing.
Most importantly, we learn a pose-dependent skinning field
from a multi-view video, which greatly reduces artifacts.
We show how such a skinning field can be learned in a
weakly supervised manner by an appearance model and dif-
ferentiable rendering. Our work demonstrates that animat-
able character creation can be fully automated while main-
taining high-quality geometry, rigging, and skinning.
Acknowledgements. We thank Heming Zhu and Diogo
Luvizon for their help. This project was also supported by
Saarbr Â¨ucken Research Center for Visual Computing, Inter-
action, and AI. Christian Theobalt was supported by ERC
Consolidator Grant 4DReply (770784).
1384
References
[1] Mart Â´Ä±n Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy
Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat,
Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Is-
ard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Man-
junath Kudlur, Josh Levenberg, Dan Man Â´e, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal-
war, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-
nanda Vi Â´egas, Oriol Vinyals, Pete Warden, Martin Watten-
berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor-
Flow: Large-scale machine learning on heterogeneous sys-
tems, 2015. Software available from tensorflow.org. 2
[2] Agisoft. PhotoScan. http://www.agisoft.com ,
2016. 6, 1, 3
[3] Brett Allen, Brian Curless, and Zoran Popovi Â´c. Articulated
body deformation from range scan data. ACM Trans. Graph. ,
21(3):612â€“619, 2002. 1, 2
[4] Brett Allen, Brian Curless, Zoran Popovic, and Aaron Hertz-
mann. Learning a Correlated Model of Identity and Pose-
Dependent Body Shape Variation for Real-Time Synthesis.
InACM SIGGRAPH / Eurographics Symposium on Com-
puter Animation . The Eurographics Association, 2006. 3
[5] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. SCAPE:
Shape Completion and Animation of People. ACM Trans-
actions on Graphics , 24(3):408â€“416, 2005. 3
[6] Seungbae Bang and Sung-Hee Lee. Spline interface for in-
tuitive skinning weight editing. ACM Trans. Graph. , 37(5),
2018. 2
[7] Ilya Baran and Jovan Popovi Â´c. Automatic rigging and ani-
mation of 3d characters. ACM Trans. Graph. , 26(3), 2007.
2, 4, 6, 7, 8, 3
[8] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,
and Andreas Geiger. Snarf: Differentiable forward skinning
for animating non-rigid neural implicit shapes. In Interna-
tional Conference on Computer Vision (ICCV) , 2021. 3, 4,
6, 7
[9] Olivier Dionne and Martin de Lasa. Geodesic voxel bind-
ing for production character meshes. In Proceedings of the
12th ACM SIGGRAPH/Eurographics Symposium on Com-
puter Animation , page 173â€“180, New York, NY , USA, 2013.
Association for Computing Machinery. 2
[10] Olivier Dionne and Martin de Lasa. Geodesic binding for de-
generate character geometry using sparse voxelization. IEEE
Transactions on Visualization and Computer Graphics , 20
(10):1367â€“1378, 2014. 2
[11] Sven Forstmann, Jun Ohya, Artus Krohn-Grimberghe, and
Ryan McDougall. Deformation styles for spline-based skele-
tal animation. In Proceedings of the 2007 ACM SIG-
GRAPH/Eurographics Symposium on Computer Animation ,
page 141â€“150, Goslar, DEU, 2007. Eurographics Associa-
tion. 2
[12] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and
Yaron Lipman. Implicit geometric regularization for learningshapes. In Proceedings of Machine Learning and Systems
2020 , pages 3569â€“3579. 2020. 1
[13] Marc Habermann, Weipeng Xu, Michael Zollh Â¨ofer, Gerard
Pons-Moll, and Christian Theobalt. Livecap: Real-time
human performance capture from monocular video. ACM
Transactions on Graphics (TOG) , 38(2):14:1â€“14:17, 2019.
2
[14] Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard
Pons-Moll, and Christian Theobalt. Deepcap: Monocular
human performance capture using weak supervision. Pro-
ceedings of the Conference on Computer Vision and Pattern
Recognition (CVPR) , 1:1, 2020. 2
[15] Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zoll-
hoefer, Gerard Pons-Moll, and Christian Theobalt. Real-time
deep dynamic characters. ACM Trans. Graph. , 40(4), 2021.
2, 5, 6, 1
[16] Marc Habermann, Lingjie Liu, Weipeng Xu, Gerard Pons-
Moll, Michael Zollhoefer, and Christian Theobalt. Hdhu-
mans: A hybrid approach for high-fidelity digital humans.
Proc. ACM Comput. Graph. Interact. Tech. , 6(3), 2023. 2
[17] Dae-Eun Hyun, Seung-Hyun Yoon, Jung-Woo Chang, Joon-
Kyung Seong, Myung-Soo Kim, and Bert J Â¨uttler. Sweep-
based human deformation. The Visual Computer , 21:542â€“
550, 2005. 2
[18] Alec Jacobson, Ilya Baran, Jovan Popovi Â´c, and Olga
Sorkine-Hornung. Bounded biharmonic weights for real-
time deformation. Commun. ACM , 57(4):99â€“106, 2014. 2
[19] Doug L. James and Christopher D. Twigg. Skinning mesh
animations. ACM Trans. Graph. , 24(3):399â€“407, 2005. 2
[20] Yue Jiang, Marc Habermann, Vladislav Golyanik, and Chris-
tian Theobalt. Hifecap: Monocular high-fidelity and expres-
sive capture of human performances. In BMVC , 2022. 2
[21] Ladislav Kavan and Olga Sorkine. Elasticity-inspired de-
formers for character articulation. ACM Trans. Graph. , 31
(6), 2012. 2
[22] Ladislav Kavan and Ji Ë‡rÂ´Ä±Ë‡ZÂ´ara. Spherical blend skinning: A
real-time deformation of articulated models. In Proceed-
ings of the 2005 Symposium on Interactive 3D Graphics and
Games , page 9â€“16, New York, NY , USA, 2005. Association
for Computing Machinery. 2
[23] Ladislav Kavan, Steven Collins, Ji Ë‡rÂ´Ä±Ë‡ZÂ´ara, and Carol
Oâ€™Sullivan. Skinning with dual quaternions. In Proceed-
ings of the 2007 symposium on Interactive 3D graphics and
games , pages 39â€“46. ACM, 2007. 2
[24] Diederik Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. International Conference on Learn-
ing Representations , 2014. 3
[25] Youngjoong Kwon, Lingjie Liu, Henry Fuchs, Marc Haber-
mann, and Christian Theobalt. Deliffas: Deformable light
fields for fast avatar synthesis. Advances in Neural Informa-
tion Processing Systems , 2023. 2
[26] Binh Huy Le and Zhigang Deng. Robust and accurate skele-
tal rigging from mesh sequences. ACM Trans. Graph. , 33(4),
2014. 2
[27] Binh Huy Le and Jessica K. Hodgins. Real-time skeletal
skinning with optimized centers of rotation. ACM Trans.
Graph. , 35(4), 2016. 2
1385
[28] J. P. Lewis, Matt Cordner, and Nickson Fong. Pose
space deformation: A unified approach to shape interpo-
lation and skeleton-driven deformation. In Proceedings of
the 27th Annual Conference on Computer Graphics and
Interactive Techniques , page 165â€“172, USA, 2000. ACM
Press/Addison-Wesley Publishing Co. 2, 4
[29] Peike Li, Yunqiu Xu, Yunchao Wei, and Yi Yang. Self-
correction for human parsing. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 2020. 4, 1
[30] Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga
Sorkine-Hornung, and Baoquan Chen. Learning skeletal ar-
ticulations with neural blend shapes. ACM Transactions on
Graphics (TOG) , 40(4):1, 2021. 1, 3
[31] Ruilong Li, Julian Tanke, Minh V o, Michael Zollhofer, Jur-
gen Gall, Angjoo Kanazawa, and Christoph Lassner. Tava:
Template-free animatable volumetric actors. 2022. 3, 6, 1, 2
[32] Yue Li, Marc Habermann, Bernhard Thomaszewski, Stelian
Coros, Thabo Beeler, and Christian Theobalt. Deep physics-
aware inference of cloth deformation for monocular human
performance capture. 2020. 2
[33] Zhouyingcheng Liao, Jimei Yang, Jun Saito, Gerard Pons-
Moll, and Yang Zhou. Skeleton-free pose transfer for styl-
ized 3d characters. In European Conference on Computer
Vision (ECCV) . Springer, 2022. 1, 2
[34] Lijuan Liu, Youyi Zheng, Di Tang, Yi Yuan, Changjie Fan,
and Kun Zhou. Neuroskinning: Automatic skin binding
for production characters with deep graph networks. ACM
Trans. Graph. , 38(4), 2019. 1, 2, 6
[35] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu
Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor:
Neural free-view synthesis of human actors with pose con-
trol. ACM Trans. Graph. , 40(6), 2021. 2
[36] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-
ard Pons-Moll, and Michael J. Black. SMPL: A skinned
multi-person linear model. ACM Trans. Graphics (Proc.
SIGGRAPH Asia) , 34(6):248:1â€“248:16, 2015. 2, 3
[37] William Lorensen and Harvey Cline. Marching cubes: A
high resolution 3d surface construction algorithm. ACM SIG-
GRAPH Computer Graphics , 21:163â€“, 1987. 4, 3
[38] N. Magnenat-Thalmann, A. Laperriâ€˜ere, and D. Thalmann.
Joint-dependent local deformations for hand animation and
object grasping. In Proceedings of Graphics Interface
â€™88, pages 26â€“33. Canadian Man-Computer Communica-
tions Society, 1988. 2
[39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance fields for view syn-
thesis. In Computer Vision â€“ ECCV 2020 , pages 405â€“421,
Cham, 2020. Springer International Publishing. 1
[40] Alex Mohr and Michael Gleicher. Building efficient, accu-
rate character skins from examples. ACM Trans. Graph. , 22
(3):562â€“568, 2003. 1, 2
[41] A. Mosella-Montoro and J. Ruiz-Hidalgo. Skinningnet:
Two-stream graph convolutional neural network for skin-
ning prediction of synthetic characters. In 2022 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR) , pages 18572â€“18581, Los Alamitos, CA, USA,
2022. IEEE Computer Society. 1, 2, 6[42] Tomohiko Mukai and Shigeru Kuriyama. Efficient dynamic
skinning with low-rank helper bone controllers. ACM Trans.
Graph. , 35(4), 2016. 2
[43] Thomas M Â¨uller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM Trans. Graph. , 41(4):102:1â€“
102:15, 2022. 8
[44] Haokai Pang, Heming Zhu, Adam Kortylewski, Christian
Theobalt, and Marc Habermann. Ash: Animatable gaussian
splats for efficient and photoreal human rendering. 2023. 2
[45] Jeong Joon Park, Peter Florence, Julian Straub, Richard
Newcombe, and Steven Lovegrove. Deepsdf: Learning con-
tinuous signed distance functions for shape representation.
InProceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2019. 3
[46] Sang Il Park and Jessica K. Hodgins. Capturing and animat-
ing skin deformation in human motion. ACM Trans. Graph. ,
25(3):881â€“889, 2006. 2
[47] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan
Zhang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Animat-
able neural radiance fields for human body modeling. ICCV ,
2021. 2
[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,
Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:
Implicit neural representations with structured latent codes
for novel view synthesis of dynamic humans. CVPR , 1(1):
9054â€“9063, 2021. 2
[49] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J.
Black. SCANimate: Weakly supervised learning of skinned
clothed avatar networks. In Proceedings IEEE/CVF Conf. on
Computer Vision and Pattern Recognition (CVPR) , 2021. 3,
6, 7
[50] Tim Salimans and Durk P Kingma. Weight normalization:
A simple reparameterization to accelerate training of deep
neural networks. Advances in neural information processing
systems , 29, 2016. 1
[51] Soumyadip Sengupta, Vivek Jayaram, Brian Curless, Steve
Seitz, and Ira Kemelmacher-Shlizerman. Background mat-
ting: The world is your green screen. In Computer Vision
and Pattern Regognition (CVPR) , 2020. 3
[52] Peter-Pike J. Sloan, Charles F. Rose, and Michael F. Cohen.
Shape by example. In Proceedings of the 2001 Symposium
on Interactive 3D Graphics , pages 135â€“143, New York, NY ,
USA, 2001. Association for Computing Machinery. 1, 3
[53] TheCaptury. The Captury. http://www.thecaptury.
com/ , 2020. 1, 3, 4
[54] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt
Haberland, Tyler Reddy, David Cournapeau, Evgeni
Burovski, Pearu Peterson, Warren Weckesser, Jonathan
Bright, St Â´efan J. van der Walt, Matthew Brett, Joshua Wil-
son, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J.
Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey,
Ë™Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, De-
nis Laxalde, Josef Perktold, Robert Cimrman, Ian Henrik-
sen, E. A. Quintero, Charles R. Harris, Anne M. Archibald,
AntË†onio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt,
and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algo-
1386
rithms for Scientific Computing in Python. Nature Methods ,
17:261â€“272, 2020. 3
[55] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. Neus: Learning neural implicit
surfaces by volume rendering for multi-view reconstruction.
NeurIPS , 2021. 1, 3, 4
[56] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu
Tang. Arah: Animatable volume rendering of articulated
human sdfs. In Computer Vision â€“ ECCV 2022: 17th Euro-
pean Conference, Tel Aviv, Israel, October 23â€“27, 2022, Pro-
ceedings, Part XXXII , page 1â€“19, Berlin, Heidelberg, 2022.
Springer-Verlag. 3, 6, 1, 2
[57] Xiaohuan Corina Wang and Cary Phillips. Multi-weight
enveloping: Least-squares approximation techniques for
skin animation. In Proceedings of the 2002 ACM SIG-
GRAPH/Eurographics Symposium on Computer Animation ,
page 129â€“138, New York, NY , USA, 2002. Association for
Computing Machinery. 1, 2
[58] Rich Wareham and Joan Lasenby. Bone glow: An improved
method for the assignment of weights for mesh deformation.
InArticulated Motion and Deformable Objects , pages 63â€“
71, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. 2
[59] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Lan-
dreth, and Karan Singh. Rignet: Neural rigging for articu-
lated characters. ACM Trans. on Graphics , 39, 2020. 1, 2, 6,
7, 3
[60] Ji Yang, Xinxin Zuo, Sen Wang, Zhenbo Yu, Xingyu Li,
Bingbing Ni, Minglun Gong, and Li Cheng. Object wake-
up: 3d object rigging from a single image. In Computer
Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Is-
rael, October 23â€“27, 2022, Proceedings, Part II , pages 311â€“
327. Springer, 2022. 2
[61] Xiaosong Yang, Arun Somasekharan, and Jian Jun Zhang.
Curve skeleton skinning for human and creature characters.
Computer Animation and Virtual Worlds , 17, 2006. 2
1387
