Under review as submission to TMLR
Are EEG Sequences Time Series?
EEG Classification with Time Series Models and Joint Sub-
ject Training
Anonymous authors
Paper under double-blind review
Abstract
As with most other data domains, EEG data analysis relies on rich domain-specific prepro-
cessing. Beyond such preprocessing, machine learners would hope to deal with such data as
with any other time series data. For EEG classification many models have been developed
with layer types and architectures we typically do not see in time series classification. Fur-
thermore, typically separate models for each individual subject are learned, not one model
for all of them. In this paper, we systematically study the differences between EEG clas-
sification models and generic time series classification models. We describe three different
model setups to deal with EEG data from different subjects, namely subject-specific models
(most EEG literature), subject-agnostic models and subject-conditional models. In experi-
ments on three datasets, we demonstrate that off-the-shelf time series classification models
trained per subject perform close to EEG classification models, but that do not quite reach
the performance of domain-specific modeling. Additionally, we combine time-series models
with subject embeddings to train one joint subject-conditional classifier on all subjects. The
resulting models are competitive with dedicated EEG models in 2 out of 3 datasets, even
outperforming all EEG methods on one of them.
1 Introduction
An electroencephalogram (EEG) measures electrical activity in the brain for diagnostic purposes. EEG
classification deals with mapping measured EEG signals to a downstream classification task, usually in the
medical domain. This includes for example the identification of sleep stages to identify disorders (Teplan
et al., 2002; Subha et al., 2010). While the usage of EEGs has potential for empowering disabled individuals
(Al-Qaysi et al., 2018; Wang et al., 2016; Chen et al., 2015) and facilitating stroke rehabilitation (Ang et al.,
2015; Alonso-Valerdi et al., 2015), the technological advancement is hindered by substantial obstacles in
the consistency and clarity of EEG signals. EEG classification is particularly challenging because EEG
signals inherently have a low signal-to-noise ratio (SNR) (Johnson, 2006) and are highly non-Gaussian, non-
stationary, and nonlinear (Subha et al., 2010). One promising approach to analyzing EEG sequences is the
use of machine learning techniques, such as deep learning, which can adeptly navigate the complexities of
this data modality.
TheongoingpursuitforEEGclassificationhasturnedtowardsleveragingsophisticatedcomputationalstrate-
gies, embodying progressions in deep learning (DL) (Roy et al., 2019) and geometric learning (GL) (Lotte
et al., 2007). This includes dedicated manifold attention networks that employ non-Euclidean geometries
for incorporating spatial and temporal domains. The research landscape for deep learning-based EEG clas-
sification however is isolated, with a lot of methods being developed for this special task. The question of
whether EEG classification is closely connected to other areas of deep learning such as time-series classifica-
tion is rarely investigated. Thus, there is only a small amount of literature that employs established methods
and evaluation protocols from other learning domains to EEG classification. While some recent time series
literature has done evaluation on EEG data in (Dong et al., 2024; Zhang et al., 2022) the inverse for the
EEG domain rarely is the case and only building blocks are adapted. Additionally, the prevalent practice
1Under review as submission to TMLR
of training individual models for each subject imposes significant limitations on the scalability, adaptability,
and efficacy of EEG classification. These issues arise because the trained model cannot be generalize to other
subjects and is unable to reliable predict for unseen patients.
This paper proposes a paradigm shift towards using out-of-the-box time-series classification (TSC) models
for EEG classification, challenging existing specialized frameworks, and promoting training a unified model
instead of one model per subject. The differences in each of the subjectsâ€™ brain activity, traditionally a
challenge for aggregate training methodologies, are addressed through the use of distinct subject embeddings.
To utilize this meta-information about the subject, we present three approaches to incorporate a dedicated
subject embedding into a time series classification pipeline for EEG classification. This strategy focuses on
the variability between subjects to the model. Our results indicate that established time-series classification
approaches with subject embeddings can outperform dedicated state-of-the-art EEG classification models
for the task of learning one classification model for all subjects.
This paper presents a step towards embedding EEG classification into well-researched deep learning areas
such as time-series classification. Thus it opens the door towards more efficient and better-understood
learning on EEG data. Our study presents three primary contributions to the EEG and TSC research
landscape:
1. We argue that EEG-series sequences are a special form of time-series, namely time-series with one
categorical attribute. This indicates that they should be tackled with time-series models which can
incorporate such additional attributes. (Section 3)
2. We provide a theoretical framework to classify EEG models into three categories: (Section 3.4)
â€¢subject-specific , where a separate model is learned for each subject.
â€¢subject-agnostic , where one joint model for all subjects is learned without any subject informa-
tion.
â€¢subject-conditional , where one joint model for all subjects is learned while utilizing subject
information.
3. We propose three novel methodologies for subject-conditional EEG classification. Our procedure is
model-agnostic and can be integrated into any differentiable classification model. (Section 4)
4. We show that some simple time-series classification baselines such as ResNet and Inception can be
competitive or even outperform dedicated EEG classification models in 2 out of 3 cases, while other
TSC models are unable to capture the EEG inherent noise pattern. (Section 6)
Our code can be found in the supplementary materials and will be released upon acceptance.
2 Related Work
2.1 EEG Classification
In the EEG literature, most models consist of a combination of convolutions (spatial, temporal, and hybrid),
normalization, pooling, and a final linear layer. One of the pioneering models is EEGNet (Lawhern et al.,
2018), which employs a temporal convolution, two convolutional blocks, and a linear layer. The convolu-
tional blocks comprise depthwise/separable convolutions, batch normalization, ELU activation, and average
pooling, followed by dropout.
MAtt (Manifold Attention) (Pan et al., 2022) introduces a novel approach by utilizing a manifold attention
layer in the Riemann Space instead of the standard Euclidean Space. It combines a feature extractor,
a manifold attention module, and a feature classifier. The feature extractor consists of a 2-layered CNN
that convolves over the spatial and spatio-temporal dimensions sequentially. The manifold attention layer
transforms the data from Euclidean Space to Riemann Space, applies an attention mechanism using the
Log-Euclidean metric, and then converts the data back to Euclidean Space. The feature classifier is a linear
layer responsible for class predictions.
2Under review as submission to TMLR
Table 1: Comparison of the EEG Classification literature.
Data Usage
Model Single All Transfer Meta Specialised Layer Architecture
MAtt (Pan et al., 2022) âœ“ âˆ’ âˆ’ âˆ’Riemann space
transformationAttention
MBEEGSE (Altuwaijri et al., 2022) âœ“ âˆ’ âˆ’ âˆ’ EEGNetBlock CNN
EEG-TCNet (Ingolfsson et al., 2020) âœ“ âˆ’ âˆ’ âˆ’ TCN-Blocks CNN
TCNet-Fusion (Musallam et al., 2021) âœ“ âˆ’ âˆ’ âˆ’ notch filter CNN
FBCNet (Mane et al., 2021) âœ“ âˆ’ âˆ’ âˆ’ spectral filtering CNN
SCCNet (Wei et al., 2019) âœ“ âˆ’ âœ“ âˆ’spatio-temporal
filteringCNN
EEGNet (Lawhern et al., 2018) âœ“ âœ“ âˆ’ âˆ’ bandpass filtering CNN
Shallow
ConvNet(Schirrmeister et al., 2017) âœ“ âˆ’ âˆ’ âˆ’ spatial filtering CNN
Inception (ours) (Ismail Fawaz et al., 2020) âœ“ âœ“ âˆ’ âœ“ InceptionBlock CNN
ResNet (ours) (Wang et al., 2017) âœ“ âœ“ âˆ’ âœ“ ResNetBlock CNN
MBEEGSE (Altuwaijri et al., 2022) employs three independent sequences of convolutional blocks, each
utilizing a combination of EEGblocks (Riyad et al., 2020) and SE attention blocks (Altuwaijri & Muhammad,
2022). The outputs of these sequences are concatenated and passed through a fully connected layer. This
design allows for the exploration of different kernel sizes, dropout rates, number of temporal filters, and
reduction ratios in each sequence, resembling a mixture of experts.
EEG-TCNet (Ingolfsson et al., 2020) comprises three main parts: processing the input through temporal,
depth-wise, and separable convolutions; extracting additional temporal features using causal convolutions
with batch normalization, dropout, ELU activation, and skip connections; and utilizing a fully connected
layer for classification. TCNet-Fusion (Musallam et al., 2021) retains this architecture but concatenates the
outputs of the first and second layers before the final classification step.
FBCNet (Mane et al., 2021) follows a similar approach to EEG-TCNet but incorporates spectral filtering in
the initial stage. Multiple bandpass filters with varying cut-off frequencies are applied for spectral filtering.
SCCNet (Wei et al., 2019) is a straightforward architecture comprising spatial and spatiotemporal con-
volutions, a dropout layer, average pooling, and a linear layer for logit generation. The spatiotemporal
convolution aims to learn spectral filtering.
ShallowConvNet (Schirrmeister et al., 2017) performs temporal and spatial convolutions, followed by average
poolingandalinearlayer. TheactivationfunctionusedafterconvolutionsistypicallyReLU(RectifiedLinear
Unit).
In Table 1, we present the architectures used in EEG classification literature along with the corresponding
training protocols. Here, Singledenotes the case where the model solely utilizes data from each subject
individually, Allsignifies the usage of data from all subjects without any metadata (for example using the
subject Id has an extra feature), Transfer indicates the utilization of data from all subjects in a transfer
learning setting, and Metaimplies the utilization of data from all subjects along with the metadata. To the
best of our knowledge, we are the first to evaluate the Metasetting by incorporating subject information.
Furthermore, we show the specialized layers used in each of the respective models. While earlier architectures
like Schirrmeister et al. (2017); Lawhern et al. (2018); Wei et al. (2019); Mane et al. (2021); Musallam et al.
(2021) focus on filtering approaches, later works such as Ingolfsson et al. (2020); Altuwaijri et al. (2022)
developed specialized building blocks for EEG encoding. MAtt Pan et al. (2022) is the first to successfully
incorporate a Transformer architecture by transferring the data from the Euclidean space to a Riemannian
manifold.
3Under review as submission to TMLR
2.2 Time Series Classification
In the time series domain, strategies from other domains, particularly architectures from computer vision,
have been successfully applied (Wei et al., 2019; Kachuee et al., 2018a). CNN-based models such as ResNet
(He et al., 2016; Wang et al., 2017) or Inception (Szegedy et al., 2015; Ismail Fawaz et al., 2020) have been
adapted by applying convolutions over time. Additionally, transformers or attention-based models (Vaswani,
2017) have been tailored to the time series domain by effectively tokenizing inputs before processing them
(Kachuee et al., 2018b). Similar to the computer vision domain (Dosovitskiy et al., 2020), attention-based
models are considered more effective for high data regimes compared to CNNs, as they provide a global view
of the data instead of a local focus imposed by convolutional kernels.
Inadditiontotheseadaptations, therearealsoarchitecturesspecificallydesignedfortimeseriestasks, suchas
ROCKET (Dempster et al., 2020). ROCKET simplifies time series models while maintaining performance
by employing a convolutional layer with multiple randomly initialized kernels of different sizes, dilations,
and paddings. The output is then passed to a logistic or ridge regression model. Further notable models
include Wu et al. (2023) which is achieving state-of-the-art performance for many TSC problems by stacking
TimesBlocks in a residual manner. In Zerveas et al. (2021) a transformer architecture in combination with
self-supervised pertaining is used in both time series classification and regression.
There are also initial efforts to integrate EEG datasets into the time series literature. In Dong et al. (2024),
the TSLD dataset, which consists of multiple time series datasets from different domains including EEG
data, is used for evaluation. Additionally, Zhang et al. (2022) studies the impact of pretraining on the
SleepEEG dataset Kemp et al. (2000) for other downstream EEG classification tasks. However, both works
focus on the pretraining aspect, comparing only against other methods in this domain, and do not include
any dedicated EEG classification models.
2.3 Static and time-independent features
In integrating static, time-independent features such as subject IDs into time series data, approaches similar
to those in the literature have been identified Leontjeva & Kuzovkin (2016); Tayal et al. (2022). These
approaches involve either copying static features for each time step to construct a new time series with
additional channels or concatenating static features later in the model with features inferred solely from the
time series, typically leading to a model with two separate encoders (one for the static features and one
for the time series). This integration of static and time-independent information, such as subjects in EEG
datasets, shares similarities with techniques used in the recommender systems literature. In recommender
systems, item IDs are utilized to explicitly capture differences between items while employing joint training
protocols. Neural networks have been extensively used to embed item information Song & Chai (2018); He
et al. (2017), leading to state-of-the-art performance in the field Rashed et al. (2022).
3 Understanding EEG data as Time Series with Static Attributes
Notation: By1:N:={1,...,N}we denote the set of the first Nintegers. By Râˆ—Ã—C:= (RC)âˆ—:=/uniontext
TâˆˆNRTÃ—Cwe denote the finite sequences of vectors with Cdimensions.
3.1 Problem Setting
We represent a regularly sampled time series with static attributes by elements x= (xsta,xdyn)âˆˆX :=
RMÃ—Râˆ—Ã—C, wherexdyn
t,cdenotes the observed value of channel cat timetandxsta
mdenotes the m-th static
attribute (not changing over time; tâˆˆ1:len(x),câˆˆ1:C,mâˆˆ1:Mand len (x)denotes the length of the time
series).
3.2 Time Series Classification
Given a sequence of labeled time series data (xn,yn)nâˆˆ1:Nsampled from an unknown distribution pon
XÃ— 1:Y, withXâˆˆRTÃ—CandYâˆˆNthe number of classes, and a loss â„“: 1:YÃ—1:Yâ†’R, the task of time
4Under review as submission to TMLR
series classification then is to find a function Ë†y:Xâ†’ 1:Y(called model) with minimal expected loss
E(x,y)âˆ¼pâ„“(y,Ë†y(x)).
3.3 EEG Classification
EEG classification aims to classify EEG recordings that consist of Csensor readings regularly sampled, e.g.,
at a set frequency in Hz, usually for a short time range of a few seconds. For each recording additional
information, often called meta data, is available, e.g., the ID of a human subject, the sequence number of a
session or the sequence number of a task within a session. While the EEG recording is represented by the
dynamic part xdynof a time series, the additional information fits nicely into static attributes xsta. Thus,
from the perspective of the problem setting, EEG classification is just time series classification with static
attributes. Furthermore, if we only are interested in the question, to which class a specific EEG recording
belongs, we arrive in a special and simple case of time series classification with static attributes. Namely,
the case where we only have one static attribute, i.e., M= 1. Furthermore, for an EEG dataset DâŠ†X, we
assume that we only have a finite set of layers (the subject IDs), i.e.,
{xsta|xâˆˆD} ={1,...,S}.
This leads us to the special case under which EEG classification falls, which we call time series classification
with a static categorical attribute , where our time series has the form
x= (xsta,xdyn)âˆˆ1:SÃ—Râˆ—Ã—C.
3.4 Dealing with Static Attributes in Time Series Classification
Intimeseriesclassification, staticattributesoftenarerepresentedasconstantchannels: oneaddsonechannel
per static attribute and sets its value to the value of the attribute for all times, i.e.,
xdyn
t,C+m:=xsta
mâˆ€t,m;Cnew:=C+M;Mnew:= 0. (1)
This way, any time series model such as a vanilla convolutional neural network can handle static attributes,
even if its input is just a dynamic time series.
However, this approach assumes the static attributes to be numerical and is therefore not dedicated to our
task of time series classification with a static categorical attribute. Additionally, in EEG classification signals
between subjects often are considered to be so different, that a model is built for each subject individually.
To capture the underlying assumption of such an approach, we distinguish three different cases:
1. The data generating distribution decomposes into subject specific distributions, that share
no commonalities: This means
p(y|xdyn,xsta) =pxsta(y|xdyn)
andp1,...,pShave nothing in common for the Sdifferent subjects. This is the usual EEG classification
setting. If this assumption is true, a model per subject can be learned. We call them subject-specific models.
2. The data generating distribution does not depend on the subject ID / static attributes :
Here, we have
p(y|xdyn,xsta) =p(y|xdyn).
If this assumption is true, one model for all subjects, that does not have access to the subject ID as input,
can be learned. We call such models subject-agnostic.
3. The data generating distribution does depend on the subject ID / static attributes, but
does not decompose into completely isolated distributions either :p(y|xdyn,xsta)depends on both,
xdynandxstain a way that needs to be learned. In this case, a model needs to be learned, that has access
to both, the EEG signal and the subject ID. We will propose such subject-conditional models in Section 4.
5Under review as submission to TMLR
EEG Signal Subject 
(a) Constant Indicator Channels CNN Layers { ğ›¼, 1, ğ›¼, â€¦, ğ›¼}Id = 2 
âŠ•
ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼ 
1,  1, 1, 1, 1, 1, 1, 1 
ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼
â€¦... 
ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼, ğ›¼
Pooling 
FC-Layer 
Classification 
EEG Signal Subject 
(b) Constant Embedding Channels CNN Layers Id = 2 
âŠ•
z â‚, z â‚, z â‚, z â‚, z â‚, z â‚
z â‚‚, z â‚‚, z â‚‚, z â‚‚, z â‚‚, z â‚‚
z â‚ƒ, z â‚ƒ, z â‚ƒ, z â‚ƒ, z â‚ƒ, z â‚ƒ
â€¦... 
zd, zd, zd, zd, zd, zd
Pooling 
FC-Layer 
Classification Embedding Layer 
EEG Signal Subject 
(c) Separate Embedding CNN Layers { ğ›¼, 1, ğ›¼, â€¦, ğ›¼}Id = 2 
Pooling Linear 
Tanh 
Linear 
âŠ•
Linear 
ELU 
Linear 
Classification 
Figure 1: In the figure above we demonstrate the three proposed methods of using the subject information
where is (a) Constant Indicator Channels, (b) the Constant Embedding Channels, and (c) Separate Embed-
ding. HereâŠ•denotes the concatenation of two tensors across the channel dimension and for (b) drepresents
the embedding size. We have set the subject Id to 2 as an example.
4 Joint Training for Time Series with Static Attributes
If the data-generating distribution decomposes into subject-specific distributions that have no commonalities
(i.e., subject-specific), no joint model is needed as we train one model for each occurring value of the static
attributexsta. If the data generating distribution does not depend on the static attribute (i.e., subject-
agnostic), it is sufficient to train one time series model that ignores the static attributes.
However, ifweassumethatthedata-generatingdistributiondoesdependonthestaticattributesbutdoesnot
decompose into completely isolated distributions either, we need to modify existing time-series classification
models such that they can incorporate this information properly. Let DâŠ† 1:Sâˆ’Râˆ—âˆ’Cbe a time series
dataset with a categorical attribute. We propose three ways to incorporate the categorical attribute:
Constant Indicator Channels The first approach, the Constant Indicator Channels (CIC), introduces
a hyperparameter Î±â‰¥0and transforms a time series (i,Xdyn)âˆˆXwith a categorical attribute to a time
series with (numerical) static attributes by encoding the categorical attribute via
fÎ±: 1:Sâ†’RS
iâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’(Î±,...,Î±, 1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
positioni,Î±,...,Î± ) (2)
then we can transform that to a time series without static attributes via Equation (1). This transformation
can be interpreted as a generalization of one-hot encoding, where higher values of Î±correspond to higher
similarities between different subjects.
Constant Embedding Channels For the second approach, Constant Embedding Channels (CEC), we
train an embedding layer of dimension Efor the subject encoding, represented via a parameterized function
fÎ¸: 1:Sâ†’RE.
6Under review as submission to TMLR
Wethenapplyatimeseriesmodel Ëœm:RTÃ—(C+E)â†’1:YonthetimeserieswhichisderivedfromEquation(1).
Thus, ifÏ•:REÃ—Râˆ—Ã—Câ†’Râˆ—Ã—(C+E)is the transformation coming from Equation (1), we arrive on the to-
trained end-to-end model
m: 1:SÃ—RTÃ—Câ†’1:Y
(i,xdyn)âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’Ëœm(Ï•(fÎ¸(i),xdyn)),
where both the parameters of Ëœmand the parameters Î¸of the subject encoder are trained.
Separate Embedding For the last method we build a Separate Embedding (SE). Let Ëœm:RTÃ—Câ†’
1:Ybe a time-series classification model. Let us furthermore assume, that we can decompose Ëœminto a
feature extractor fFE:RTÃ—Câ†’Rd0and aclassification head CLF: Rd0â†’1:Ywhich is given via a multi-
layer perception. We again transform subject encodings iâˆˆ1:Swith the transformation fÎ±proposed
in Equation (2). We then feed this embedding through a multi-layer perceptron MLP: RSâ†’Rd1. Following
the MLP we concatenate the output of this MLP with the output of the feature extractor and apply a
classification head CLF : Rd0+d1â†’1:Y. The overall model is thus given via:
m: 1:SÃ—RTÃ—Câ†’1:Y
(i,xdyn)âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’CLF/parenleftbig
fFE(xdyn)âŠ•MLP(fÎ±(i))/parenrightbig
,
whereâŠ•denotes the concatenation.
5 EEG Classification Models
5.1 EEG Classification
For comparison with existing EEG literature, we have opted to expand upon the state-of-the-art MAtt
framework (Pan et al., 2022). MAtt represents an advancement in deep learning and geometric learning
methodologies, particularly leveraging Riemannian geometry. The model integrates a manifold attention
mechanism, aimed at capturing spatio-temporal representations of EEG data on a Riemannian symmetric
positive definite (SPD) manifold. By operating within this framework, MAtt capitalizes on the robustness
afforded by geometric learning when dealing with EEG data represented in a manifold space. The model
is evaluated on three EEG datasets and is found to outperform several leading deep learning methods for
general EEG decoding. Additionally, the paper claims that MAtt is capable of identifying informative EEG
features and managing the non-stationarity inherent in brain dynamics.
MAtt initiates feature extraction with two convolutional layers, followed by an embedding step transitioning
from the Euclidean space to the SPD manifold. The latent representation obtained from the Manifold At-
tention Module is subsequently projected back into the Euclidean space and channeled into the classification
head via a fully connected layer which produces the final prediction.
5.2 Time Series Classification
InadditiontoEEG-specificapproaches, wealsoadvocatefortheutilizationoftimeseriesclassificationmodels
to highlight similarities in EEG datasets with foundational time series data. This approach aims to under-
score the generalizability of these models across diverse datasets, establishing them as robust benchmarks
and mitigating insular comparisons within the realm of EEG data analysis.
Firstly, we propose the adoption of a standard ResNet architecture, renowned for its significant impact in
both image processing and time series analysis domains. Our ResNet model comprises three ResNetBlocks
followed by the classification head.
Secondly, we selected Inception architecture for its multiscale feature extraction capabilities, hierarchical
representation learning, and computational efficiency, making it adept at capturing intricate temporal pat-
terns in time series data. For the implementation, we use two different sizes of the model with three and
four InceptionBlocks respectively.
7Under review as submission to TMLR
Table 2: Summary for the three datasets with the number of subjects, instances, channels, timesteps and
classes.
Dataset Subjects Instances Channels Timesteps Classes Task
Mi 9 7.452 22 438 4 Motor Imagery
SSVEP 11 5.500 8 128 5 Visual Stimulus
ERN 16 5.440 56 160 2 Error recognition
6 Experiments
WecomparethetimeseriesmodelsResNetHeetal.(2016)andInceptionSzegedyetal.(2015)withtheprevi-
ous and current state-of-the-art models for EEG Classification including MAtt Pan et al. (2022), MBEEGSE
Altuwaijri et al. (2022), TCNet-Fusion Musallam et al. (2021), EEG-TCNet Ingolfsson et al. (2020), FBC-
Net Mane et al. (2021), SCCNet Wei et al. (2019), EEGNet Lawhern et al. (2018), and ShallowConvNet
Schirrmeister et al. (2017). For the proposed joint training protocol we evaluate the three proposed methods
on ResNet He et al. (2016), Inception Szegedy et al. (2015) and MAtt Pan et al. (2022) as the representative
for the EEG models. The results for the other EEG models were taken from Pan et al. (2022), as they have
already reproduced all previous work.
6.1 Datasets
We conduct our experiments on three common EEG datasets, BCIC-IV-2a (MI)(Brunner et al., 2008),
MAMEM EEG SSVEP Dataset II (SSVEP) (Nikolopoulos, 2021) and the BCI challenge error-related
negativity (ERN)dataset(Margauxetal.,2012). Allpreprocessingsteps,aswellasthetrain/validation/test
splits respectively, follow the same protocol as MAtt (Pan et al., 2022).
Dataset I - MI The BCIC-IV-2a dataset is one of the most commonly used public EEG datasets released for
the BCI Competition IV in 2008 (Brunner et al., 2008), containing EEG measurements for a motor-imagery
task. The EEG signals were recorded by 22 Ag/AgCl EEG electrodes at the central and surrounding regions
at a sampling rate of 250 Hz. We applied standard preprocessing procedures to the 22-channel EEG signals.
First, we down-sampled the data from 256 Hz to 128 Hz. Next, we applied a band-pass filter to retain
frequencies between 4 Hz and 38 Hz. Finally, we segmented the EEG signals into portions starting 0.5
seconds after the onset of the cue and lasting up to 4 seconds, resulting in 438 time points per segment.
The motor-imagery task has 4 classes for the imagination of one of four types of movement (right hand, left
hand, feet, andtongue). Forourexperiments, thedataissplitinto2268/324/2592instancesfortrain/val/test
respectively.
Dataset II - SSVEP The MAMEM-SSVEP-II (Nikolopoulos, 2021) contains EEG measurements of an
EGI 300 Geodesic EEG System (GES 300). The subjects gazed at one of the five visual stimuli flickering at
different frequencies (6.66, 7.50, 8.57, 10.00, and 12.00 Hz) for five seconds. The preprocessing procedures
for this dataset included band-pass filtering between 1 Hz and 50 Hz. We then selected eight channels (PO7,
PO3, PO, PO4, PO8, O1, Oz, and O2) in the occipital area, where the visual cortex is located. Each trial was
segmented into four 1-second segments, starting from 1 second to 5 seconds after the onset of the cue. This
resulted in a total of 500 trials of 1-second, 8-channel SSVEP signals for each subject, with each segment
consisting of 125 time points. Of the 5 sessions in this dataset, we assigned sessions 1, 2, and 3 as the
training set, 4 as the validation set, and 5 as the test set. This split results in 3300/1100/1100 train/val/test
instances, respectively.
Dataset III - ERN The third dataset BCI Challenge ERN dataset (BCI-ERN) (Margaux et al., 2012) was
used for the BCI Challenge on Kaggle1. This dataset captures a P300-based BCI spelling task measured
by 56 Ag/AgCl EEG electrodes. The spelling task is a binary classification task with an unbalance due to
more correct inputs. The preprocessing steps included downsampling from 600 Hz to 128 Hz and applying
1https://www.kaggle.com/c/inria-bci-challenge
8Under review as submission to TMLR
a band-pass filter between 1 Hz and 40 Hz. After preprocessing, each trial consisted of 56 channels and 160
time points. We have split the data into 2880/960/1600 train/val/test instances respectively.
6.2 Experimental Setup
We evaluate the time series models ResNet and Inception, as well as the EEG classification architecture
MAtt on these three datasets. For the datasets MI and SSVEP, we use Accuracy as the evaluation metric.
For ERN we report the Area Under the Curve (AUC) (Rosset, 2004), following the original MAtt paper. We
train the models for 500 epochs with early stopping based on validation loss, with a patience of 20 epochs
without improvement. For each model, we perform hyperparameter optimization of the batch size {32,64},
learning rate{1eâˆ’4,1eâˆ’5,1eâˆ’6}, and weight decay {0.0,0.01,0.1,0.5,1.0}. For the subject-conditional task
we search for Î±in{0.1,0.25,0.5,0.75}. Each set of hyperparameters is evaluated, and the best configuration
is selected based on the minimum validation loss averaged across 3 repeats. For the Inception model, we train
both variants with three and four InceptionBlocks respectively, and choose the one with the lower validation
loss. We then repeat the training 5 times for the best hyperparameter configuration for each subject and
in the joint protocol. We report the standard deviation over the runs for each dataset. To evaluate this
deviation we use the following equation:
â„“(r):=1
SS/summationdisplay
i=1â„“(yn,Ë†y(r)(xi))
meanruns:=1
RR/summationdisplay
r=1â„“(r)
stddevruns:= (1
Râˆ’1R/summationdisplay
r=1(â„“(r)Ã—meanruns)2)1
2
Here,Ris the total number of runs. For our main baseline model MAtt, we have taken the best hyper-
parameters reported by the authors. However, we were unable to reproduce the results. In Table 3, we,
therefore, report our reproduction of the MAtt as well as the reported results denoted as MAttâ€ as well as
common baselines from the EEG literature.
6.3 Performance Comparison
In this section, we evaluate the subject-specific, subject-agnostic, and subject-conditional models on the
three datasets. Our results are shown in Table 3.
For thesubject-specific approach, we compare our proposed TSC baseline Inception and ResNet with the
leading EEG literature, where MAtt represents our reproduction of the original paper and MAttâ€ denotes
the reported results by the authors. The results for the other models were extracted from (Pan et al., 2022).
For this approach, the TSC models achieve competitive results for the SSVEP and ERN datasets, where the
Inception model in particular produces results close to the state-of-the-art, whereas, ResNet underperforms,
especially for the SSVEP dataset which we show in Table 5 and Table 6 respectively. Here we can observe
the standard behavior for EEG datasets, where the model performance varies depending on the subject. For
the MI dataset, the designated EEG models emerge as clear winners over the time series models as shown
in Table 4. Here the patching for the input sequence employed by MAtt proves to be beneficial for this
particular dataset with its longer sequence length, while the vanilla Inception and ResNet models have no
such functionality.
Secondly, for the subject-conditional approach, we evaluate our proposed subject-conditional methods CIC,
CEC, and SE for the Inception, ResNet, and MAtt architectures (Table 7), and report the best-performing
subject-conditional models, which is selected on validation loss, under MAttJoint, ResNetJoint, and Incep-
tionJoint in Table 3, respectively. With the additional subject information and the ability to learn patterns
across the subjects in a joint manner, we beat all other baselines for the SSVEP dataset. Additionally, we
produce the second-best results on the ERN dataset. Generally, the Id embedding method in CEC yields
9Under review as submission to TMLR
Table 3: Performance comparison for the datasets BCIC-IV-2a (MI), MAMEM EEG SSVEP Dataset II
(SSVEP) and the BCI challenge error-related negativity (ERN). We report the average accuracy for MI and
SSVEP and the AUC for ERN over 5 runs respectively. The first block contains the common baselines for the
EEGliterature, whereMAttâ€ aretheresultsreportedintheoriginalpaperandMAttisourreproduction. The
secondblockiscomposedoftheproposedTSCmodelsResNetandInceptionforthesubject-specificapproach.
Thelastblockconsistsofthesubject-conditionalapproachwhereallsubjectsaretrainedjointlyandweutilize
the subject information. The best result is highlighted in bold and the second best is underlined.
Models MI SSVEP ERN
ShallowConvNet 61.84Â±6.39 56.93Â±6.97 71.86Â±2.64
EEGNet 57.43Â±6.25 53.72Â±7.23 74.28Â±2.47
SCCNet 71.95Â±5.05 62.11Â±7.70 70.93Â±2.31
EEG-TCNet 67.09Â±4.66 55.45Â±7.6677.05Â±2.46
TCNet-Fusion 56.52Â±3.07 45.00Â±6.45 70.46Â±2.94
FBCNet 71.45Â±4.45 53.09Â±5.67 60.47Â±3.06
MBEEGSE 64.58Â±6.07 56.45Â±7.27 75.46Â±2.34
MAttâ€ 74.71Â±5.01 65.50Â±8.20 76.01Â±2.28
MAtt 74.37Â±3.39 63.90Â±1.95 69.28Â±5.31
ResNet (ours) 58.05Â±3.68 43.49Â±3.41 69.16Â±5.16
Inception (ours) 62.85Â±3.21 62.71Â±2.95 73.55Â±5.08
MAttJoint (ours) 61.13Â±0.56 60.71Â±0.29 75.78Â±1.23
ResNetJoint (ours) 55.54Â±2.72 54.15Â±1.19 73.09Â±0.72
InceptionJoint (ours) 61.38Â±1.5766.00Â±0.36 76.13Â±0.95
Table 4: Performance Comparison on the MI dataset for the subject-specific case. We report the average
accuracy over 5 runs respectively.
Subject Inception ResNet MAtt
1 78.96Â±1.82 72 .83Â±3.2286.94Â±1.36
2 41.25Â±2.63 38 .12Â±3.2756.94Â±2.42
3 82.09Â±3.05 70 .76Â±3.1088.33Â±1.17
4 52.43Â±2.40 49 .72Â±3.4167.85Â±3.42
5 38.75Â±3.74 37 .50Â±4.5161.32Â±1.07
6 48.61Â±1.78 44 .58Â±4.3952.50Â±2.52
7 75.99Â±4.51 62 .36Â±3.3991.18Â±0.89
8 74.31Â±3.28 73 .68Â±2.5483.06Â±2.01
9 73.26Â±2.14 72 .92Â±2.2481.18Â±1.06
Summary 62.85Â±3.21 58.05 Â±3.68 74.37Â±3.39
the best outcome and is able to learn better embeddings compared to the weighted one-hot encoding in the
CIC and SE approaches. For MI, we observe that the joint training procedure provides no benefits, and the
subject-specific models have a clear advantage.
It is important to note that the Inception architecture demonstrates significantly faster computational per-
formance, with a speed-up factor of 11.2 and 9.8 for the three and four-layer variants respectively, while
ResNet exhibits a speed-up factor of 10.4 in wall-clock time compared to the MAtt architecture. These
measurements were obtained while running the models on the same NVIDIA GeForce RTX 4070 Ti GPU.
The main computational cost of MAtt arises from its full Attention mechanism (Vaswani, 2017) compared
to the lightweight CNN counterparts.
These findings suggest that time series models are well-suited for EEG classification tasks and can be
readily applied for both subject-specific and subject-conditional cases, offering promising performance and
computational efficiency.
10Under review as submission to TMLR
Table 5: Performance Comparison on the SSVEP dataset for the subject-specific case. We report the average
accuracy over 5 runs respectively.
Subject Inception ResNet MAtt
1 80.40Â±2.06 56.40Â±3.9881.60Â±2.87
2 86.60Â±1.62 75.20Â±3.8289.40Â±1.36
3 61.60Â±3.07 38.20Â±2.64 58.20Â±5.64
4 25.00Â±4.00 20.40Â±4.41 20.60Â±3.88
5 25.00Â±6.72 22.60Â±1.7426.40Â±4.80
6 79.20Â±1.72 38.20Â±3.92 79.00Â±2.68
7 69.20Â±1.72 42.80Â±2.14 66.00Â±2.19
8 23.60Â±1.74 23.40Â±1.8523.80Â±2.71
9 79.40Â±2.58 62.40Â±3.2088.20Â±2.04
10 68.60Â±3.72 35.00Â±3.0370.60Â±4.54
11 91.20Â±2.48 63.80Â±6.79 90.20Â±1.47
Summary 62.71 Â±2.95 43.49Â±3.4163.90Â±1.95
Table 6: Performance Comparison on the ERN dataset for the subject-specific case. We report the average
AUC over 5 runs respectively.
Subject Inception ResNet MAtt
2 81.14Â±4.60 74 .03Â±3.37 81.86Â±2.56
6 90.89Â±2.02 88 .40Â±2.67 68 .34Â±1.76
7 83.45Â±6.80 85.45Â±7.05 69 .28Â±10.90
11 59.50Â±10.52 55 .27Â±9.19 70.50Â±2.66
12 74.49Â±3.67 68 .62Â±9.92 60 .62Â±4.70
13 56.34Â±2.26 51 .15Â±4.16 62.92Â±3.65
14 78.96Â±2.47 75 .22Â±3.17 70 .42Â±4.80
16 56.09Â±5.53 50 .53Â±3.54 55 .71Â±2.56
17 79.93Â±3.78 75 .93Â±4.20 80.60Â±5.52
18 77.40Â±1.73 72 .30Â±1.39 75 .11Â±1.42
20 65.90Â±4.96 59 .97Â±2.56 57 .60Â±8.12
21 74.53Â±6.72 67 .95Â±7.76 62 .43Â±9.56
22 95.87Â±2.03 95 .09Â±0.89 89 .33Â±4.28
23 69.85Â±7.00 61 .66Â±2.38 70.03Â±5.40
24 77.66Â±1.36 70 .88Â±4.60 73 .71Â±2.99
26 54.90Â±5.80 54 .03Â±4.66 60.08Â±1.92
Summary 73.55Â±5.08 69.16 Â±5.16 69.28 Â±5.31
6.4 Ablation Study
As an ablation study for the subject-agnostic case, we also evaluate joint training without any subject
information added. We refer to this method as SA in Table 7, where in 7 out of 9 cases, the proposed
methods of utilizing subject information outperform the model variant without it. Hence, incorporating IDs
into the learning process via static features generally enhances performance. Note, that most models only
witness a small performance drop when being trained in a subject-agnostic manner. It especially turns out,
that a subject-agnostic vanilla Inception beats the current state-of-the-art on SSVEP. This is another strong
argument for considering time-series classification models as baselines in EEG classification. In addition to
the better performance, the standard deviation of all subject-conditional models is much lower compared to
itssubject-specific counterparts demonstrating better generalization.
11Under review as submission to TMLR
Table 7: Performance comparison with subject information for the joint training protocol for the Constant
Indicator Channels (CIC), Constant Embedding Channels (CEC), and Separate Embedding (SE). (SA) is
the subject-agnostic approach with no additional subject information. We report the average accuracy for
MI and SSVEP and the AUC for ERN over 5 runs respectively. The best result per dataset is marked in
bold and the best method per model underlined.
Method MI SSVEP ERN
ResNet
SA 55.42 Â±2.72 50.48Â±0.48 69.76Â±0.72
CIC 55.54 Â±1.72 50.39Â±0.24 73.09Â±0.66
CEC 55.21 Â±2.52 54.15Â±1.19 73.06Â±2.21
SE 49.08 Â±2.07 49.88Â±0.75 68.77Â±1.43
Inception
SA 59.21 Â±1.39 59.73Â±1.35 75.15Â±1.16
CIC 58.55 Â±2.0066.00Â±0.36 75.28Â±0.53
CEC 61.38Â±1.57 65.79Â±0.8776.13Â±0.95
SE 54.80 Â±3.01 64.00Â±0.36 74.29Â±1.38
MAtt
SA 61.13 Â±0.56 60.71Â±0.2975.77Â±0.72
CIC 60.56 Â±0.20 59.80Â±0.86 75.02Â±1.14
CEC 60.14 Â±0.94 60.20Â±0.83 75.78Â±1.23
SE 60.31 Â±1.47 60.23Â±0.74 73.39Â±0.54
7 Conclusion and Future Work
In this paper, we bridged the gap between time series analysis and EEG processing. We argued that EEG
data can be seen as time-series data with static attributes. Furthermore, we showed that established models
for time-series classification can be competitive with methods specifically dedicated to EEG classification.
While EEG classification is usually evaluated by training an individual model per subject, we also train joint
time-series classification models for all subjects. By incorporating subject embeddings into the classification
process, we showed that these subject-conditional time series models can be competitive or even outperform
dedicated EEG approaches which are trained for all subjects individually.
We see our contribution as a first step towards integrating recent advancements in deep learning methodology
into the field of EEG classification. While this paper demonstrates that well-established time-series baselines
should be considered for EEG data, not all TSC models can adapt to the EEG-specific noise patterns. By
openingupisolatedresearchinEEGclassificationtomoregenerallearningdomains,anaturalfuturedirection
is to explore whether recent trends in machine learning should be transferred to EEG analysis. This could
include, for example, the development of foundation models for EEG data. Several open issues need to be
addressed, such as possible pre-training objectives, evaluation scenarios, and tasks specific to EEG analysis.
Additionally, an in-depth analysis of EEG-specific noise patterns and how to encode this data modality is
required in future work.
References
ZT Al-Qaysi, BB Zaidan, AA Zaidan, and MS Suzani. A review of disability eeg based wheelchair control
system: Coherent taxonomy, open challenges and recommendations. Computer methods and programs in
biomedicine , 164:221â€“237, 2018.
Luz Maria Alonso-Valerdi, Ricardo Antonio Salido-Ruiz, and Ricardo A Ramirez-Mendoza. Motor imagery
basedbrainâ€“computerinterfaces: Anemergingtechnologytorehabilitatemotordeficits. Neuropsychologia ,
79:354â€“363, 2015.
12Under review as submission to TMLR
Ghadir Ali Altuwaijri and Ghulam Muhammad. A multibranch of convolutional neural network models for
electroencephalogram-based motor imagery classification. Biosensors , 12(1):22, 2022.
Ghadir Ali Altuwaijri, Ghulam Muhammad, Hamdi Altaheri, and Mansour Alsulaiman. A multi-branch
convolutional neural network with squeeze-and-excitation attention blocks for eeg-based motor imagery
signals classification. Diagnostics , 12(4):995, 2022.
Kai Keng Ang, Karen Sui Geok Chua, Kok Soon Phua, Chuanchu Wang, Zheng Yang Chin, Christopher
Wee Keong Kuah, Wilson Low, and Cuntai Guan. A randomized controlled trial of eeg-based motor
imagery brain-computer interface robotic rehabilitation for stroke. Clinical EEG and neuroscience , 46(4):
310â€“320, 2015.
Clemens Brunner, Robert Leeb, Gernot MÃ¼ller-Putz, Alois SchlÃ¶gl, and Gert Pfurtscheller. Bci competition
2008â€“graz data set a. Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces), Graz
University of Technology , 16:1â€“6, 2008.
Xiaogang Chen, Yijun Wang, Masaki Nakanishi, Xiaorong Gao, Tzyy-Ping Jung, and Shangkai Gao. High-
speed spelling with a noninvasive brainâ€“computer interface. Proceedings of the national academy of sci-
ences, 112(44):E6058â€“E6067, 2015.
Angus Dempster, FranÃ§ois Petitjean, and Geoffrey I Webb. Rocket: exceptionally fast and accurate time
series classification using random convolutional kernels. Data Mining and Knowledge Discovery , 34(5):
1454â€“1495, 2020.
Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin Wang, and Mingsheng Long.
Timesiam: A pre-training framework for siamese time-series modeling. CoRR, abs/2402.02475, 2024. doi:
10.48550/ARXIV.2402.02475. URL https://doi.org/10.48550/arXiv.2402.02475 .
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770â€“778, 2016.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural collaborative
filtering. In Proceedings of the 26th international conference on world wide web , pp. 173â€“182, 2017.
Thorir Mar Ingolfsson, Michael Hersche, Xiaying Wang, Nobuaki Kobayashi, Lukas Cavigelli, and Luca
Benini. Eeg-tcnet: An accurate temporal convolutional network for embedded motor-imagery brainâ€“
machine interfaces. In 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC) ,
pp. 2958â€“2965. IEEE, 2020.
Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier, Daniel F Schmidt, Jonathan
Weber, GeoffreyIWebb, LhassaneIdoumghar, Pierre-AlainMuller, andFranÃ§oisPetitjean. Inceptiontime:
Finding alexnet for time series classification. Data Mining and Knowledge Discovery , 34(6):1936â€“1962,
2020.
Don H Johnson. Signal-to-noise ratio. Scholarpedia , 1(12):2088, 2006.
Mohammad Kachuee, Shayan Fazeli, and Majid Sarrafzadeh. Ecg heartbeat classification: A deep transfer-
able representation. In 2018 IEEE international conference on healthcare informatics (ICHI) , pp. 443â€“444.
IEEE, 2018a.
Mohammad Kachuee, Shayan Fazeli, and Majid Sarrafzadeh. Ecg heartbeat classification: A deep transfer-
able representation. In 2018 IEEE international conference on healthcare informatics (ICHI) , pp. 443â€“444.
IEEE, 2018b.
13Under review as submission to TMLR
Bob Kemp, Aeilko H Zwinderman, Bert Tuk, Hilbert AC Kamphuisen, and Josefien JL Oberye. Analysis of
a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the eeg. IEEE Transactions
on Biomedical Engineering , 47(9):1185â€“1194, 2000.
Vernon J Lawhern, Amelia J Solon, Nicholas R Waytowich, Stephen M Gordon, Chou P Hung, and Brent J
Lance. Eegnet: a compact convolutional neural network for eeg-based brainâ€“computer interfaces. Journal
of neural engineering , 15(5):056013, 2018.
Anna Leontjeva and Ilya Kuzovkin. Combining static and dynamic features for multivariate sequence clas-
sification. In 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA) , pp.
21â€“30. IEEE, 2016.
Fabien Lotte, Marco Congedo, Anatole LÃ©cuyer, Fabrice Lamarche, and Bruno Arnaldi. A review of clas-
sification algorithms for eeg-based brainâ€“computer interfaces. Journal of neural engineering , 4(2):R1,
2007.
Ravikiran Mane, Effie Chew, Karen Chua, Kai Keng Ang, Neethu Robinson, A Prasad Vinod, Seong-Whan
Lee, and Cuntai Guan. Fbcnet: A multi-view convolutional neural network for brain-computer interface.
arXiv preprint arXiv:2104.01233 , 2021.
Perrin Margaux, Maby Emmanuel, Daligault SÃ©bastien, Bertrand Olivier, and Mattout JÃ©rÃ©mie. Objective
and subjective evaluation of online error correction during p300-based spelling. Advances in Human-
Computer Interaction , 2012:4â€“4, 2012.
Yazeed K Musallam, Nasser I AlFassam, Ghulam Muhammad, Syed Umar Amin, Mansour Al-
sulaiman, Wadood Abdul, Hamdi Altaheri, Mohamed A Bencherif, and Mohammed Algabri.
Electroencephalography-based motor imagery classification using temporal convolutional network fusion.
Biomedical Signal Processing and Control , 69:102826, 2021.
Spiros Nikolopoulos. Mamem eeg ssvep dataset ii (256 channels, 11 subjects, 5 frequencies presented simul-
taneously). 2021.
Yue-Ting Pan, Jing-Lun Chou, and Chun-Shu Wei. Matt: A manifold attention network for eeg decoding.
Advances in Neural Information Processing Systems , 35:31116â€“31129, 2022.
Ahmed Rashed, Shereen Elsayed, and Lars Schmidt-Thieme. Context and attribute-aware sequential rec-
ommendation via cross-attention. In Proceedings of the 16th ACM Conference on Recommender Systems ,
pp. 71â€“80, 2022.
Mouad Riyad, Mohammed Khalil, and Abdellah Adib. Incep-eegnet: a convnet for motor imagery decoding.
InImage and Signal Processing: 9th International Conference, ICISP 2020, Marrakesh, Morocco, June
4â€“6, 2020, Proceedings 9 , pp. 103â€“111. Springer, 2020.
Saharon Rosset. Model selection via the auc. In Proceedings of the twenty-first international conference on
Machine learning , pp. 89, 2004.
Yannick Roy, Hubert Banville, Isabela Albuquerque, Alexandre Gramfort, Tiago H Falk, and Jocelyn
Faubert. Deep learning-based electroencephalography analysis: a systematic review. Journal of neural
engineering , 16(5):051001, 2019.
Robin Tibor Schirrmeister, Jost Tobias Springenberg, Lukas Dominique Josef Fiederer, Martin Glasstetter,
Katharina Eggensperger, Michael Tangermann, Frank Hutter, Wolfram Burgard, and Tonio Ball. Deep
learning with convolutional neural networks for eeg decoding and visualization. Human brain mapping , 38
(11):5391â€“5420, 2017.
Guocong Song and Wei Chai. Collaborative learning for deep neural networks. Advances in neural informa-
tion processing systems , 31, 2018.
D Puthankattil Subha, Paul K Joseph, Rajendra Acharya U, and Choo Min Lim. Eeg signal analysis: a
survey.Journal of medical systems , 34:195â€“212, 2010.
14Under review as submission to TMLR
ChristianSzegedy, WeiLiu, YangqingJia, PierreSermanet, ScottReed, DragomirAnguelov, DumitruErhan,
Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE
conference on computer vision and pattern recognition , pp. 1â€“9, 2015.
Kshitij Tayal, Xiaowei Jia, Rahul Ghosh, Jared Willard, Jordan Read, and Vipin Kumar. Invertibility aware
integration of static and time-series data: An application to lake temperature modeling. In Proceedings of
the 2022 SIAM international conference on data mining (SDM) , pp. 702â€“710. SIAM, 2022.
Michal Teplan et al. Fundamentals of eeg measurement. Measurement science review , 2(2):1â€“11, 2002.
Ashish Vaswani. Attention is all you need. Advances in neural information processing systems , 30:I, 2017.
Yu-Te Wang, Masaki Nakanishi, Yijun Wang, Chun-Shu Wei, Chung-Kuan Cheng, and Tzyy-Ping Jung. An
onlinebrain-computerinterfacebasedonssvepsmeasuredfromnon-hair-bearingareas. IEEE Transactions
on Neural Systems and Rehabilitation Engineering , 25(1):14â€“21, 2016.
Zhiguang Wang, Weizhong Yan, and Tim Oates. Time series classification from scratch with deep neural
networks: A strong baseline. In 2017 International joint conference on neural networks (IJCNN) , pp.
1578â€“1585. IEEE, 2017.
Chun-Shu Wei, Toshiaki Koike-Akino, and Ye Wang. Spatial component-wise convolutional network (scc-
net) for motor-imagery eeg classification. In 2019 9th International IEEE/EMBS Conference on Neural
Engineering (NER) , pp. 328â€“331. IEEE, 2019.
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Timesnet: Temporal 2d-
variation modeling for general time series analysis. In The Eleventh International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023.
George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and Carsten Eickhoff. A
transformer-based framework for multivariate time series representation learning. In Feida Zhu, Beng Chin
Ooi, and Chunyan Miao (eds.), KDD â€™21: The 27th ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, Virtual Event, Singapore, August 14-18, 2021 , pp. 2114â€“2124. ACM, 2021. doi: 10.
1145/3447548.3467401.
Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-
training for time series via time-frequency consistency. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35:
Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,
USA, November 28 - December 9, 2022 , 2022.
15