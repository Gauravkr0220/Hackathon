Published in Transactions on Machine Learning Research (09/2024)
Learning Hierarchical Relational Representations through
Relational Convolutions
Awni Altabaa awni.altabaa@yale.edu
Department of Statistics & Data Science
Yale University
John Lafferty john.lafferty@yale.edu
Department of Statistics & Data Science, Wu Tsai Institute
Yale University
Reviewed on OpenReview: https: // openreview. net/ forum? id= vNZlnznmV2
Abstract
An evolving area of research in deep learning is the study of architectures and inductive biases
that support the learning of relational feature representations. In this paper, we address the
challenge of learning representations of hierarchical relations‚Äîthat is, higher-order relational
patterns among groups of objects. We introduce ‚Äúrelational convolutional networks‚Äù, a neural
architecture equipped with computational mechanisms that capture progressively more
complex relational features through the composition of simple modules. A key component of
this framework is a novel operation that captures relational patterns in groups of objects
by convolving graphlet filters‚Äîlearnable templates of relational patterns‚Äîagainst subsets
of the input. Composing relational convolutions gives rise to a deep architecture that
learns representations of higher-order, hierarchical relations. We present the motivation and
details of the architecture, together with a set of experiments to demonstrate how relational
convolutional networks can provide an effective framework for modeling relational tasks that
have hierarchical structure.
1 Introduction
Objects in the real world rarely exist in isolation; modeling the relationships between them is essential to
accurately capturing complex systems. As increasingly powerful machine learning models advance towards
building internal ‚Äúworld models,‚Äù it becomes crucial to explore natural inductive biases to enable efficient
learning of relational representations. The computational challenge lies in developing the components required
to construct robust, flexible, and progressively more complex relational representations.
1
2
3
Figure 1: A variant of a
relational match-to-sample
task.An important component of relational cognitive processing in humans is an
ability to reason about higher-order relational patterns between groups of
objects. To illustrate this, it is instructive to consider experimental tasks from
the cognitive psychology literature that probe abstract relational reasoning
ability. Consider, for example, the task depicted to the right in Figure 1 which
is a variant of a relational ‚Äúmatch-to-sample‚Äù task (Ferster, 1960; Webb et al.,
2021). The subject is presented with a sourcetriplet of objects and several
targettriplets, with each triplet having a particular relational pattern. The task
is to match the source to a target triplet with the same relational pattern (in
this case, the source has an ‚ÄúABA‚Äù pattern that matches the second target).
This task requires going beyond reasoning about pairwise relations; the subject
must reason about each triplet of objects as a group , determine its relational
pattern, then compare it to those of the target triplets, inferring the abstract
rule in the process. The ability to infer generalizable abstract rules in such tasks
is believed to be unique to humans (Fagot et al., 2001).
1Published in Transactions on Machine Learning Research (09/2024)
Compositionality‚Äîused here to mean an ability to compose modules together to build iteratively more complex
feature representations‚Äîis essential to the success of deep representation learning. For example, in the
domain of visual processing, CNNs are able to extract higher-level features (e.g., textures and object-specific
features) by composing simpler feature maps (Zeiler and Fergus, 2014), resulting in a flexible architecture
for computing ‚Äúfeatures of features‚Äù. In contrast, existing work on relational representation learning has
primarily been limited to ‚Äúshallow‚Äù first-order architectures (e.g., only explicitly capturing pairwise relations).
In this work, we propose relational convolutional networks as a compositional framework for learning
hierarchical relational representations. The key to the framework involves formalizing the concept of convolving
learnable templates of a relational pattern against a larger relation tensor. This operation produces a sequence
of vectors representing the relational pattern within each group of objects. Crucially, composing relational
convolutions captures higher-order relational features‚Äîi.e., relations between relations. Specifically, our
proposed architecture introduces the following concepts and computational mechanisms.
‚Ä¢Graphlet filters. A ‚Äúgraphlet filter‚Äù is a template for the pattern of relations between a (small)
collection of objects. Since pairwise relations can be viewed as edges on a graph, the term ‚Äúgraphlet‚Äù
is used to refer to a subgraph, and the term ‚Äúfilter‚Äù is used to refer to a learnable template or pattern.
‚Ä¢Relational convolutions. We formalize a notion of relational convolution, analogous to spatial
convolutions in CNNs, where a graphlet filter is matched against the relations within groupsof
objects to obtain a representation of the relational pattern in different groupings of the input.
‚Ä¢Grouping mechanisms. For large problem instances, considering relational convolutions across all
object combinations would be intractable. To achieve scalability, we introduce a learnable grouping
mechanism based on attention which identifies the relevant groups that should be considered for the
downstream task.
‚Ä¢Compositional relational modules. The proposed architecture supports composable modules,
where each module has learnable graphlet filters and groups. This enables learning higher-order
relationships between objects‚Äîrelations between relations.
The components of the architecture are presented in detail in Sections 2 and 3, and a schematic of the proposed
architecture is shown in Figure 2. In a series of experiments, we show how relational convolutional networks
provide a powerful framework for relational learning. We first carry out experiments on the ‚Äúrelational games‚Äù
benchmark for relational reasoning proposed by Shanahan et al. (2020), which consists of a suite of binary
classification tasks for identifying abstract relational rules between a set of geometric objects represented
as images. We next carry out experiments on a version of the Setcard game, which requires processing of
higher-order relations across multiple attributes. We find that relational convolutional networks outperform
Transformers, graph neural networks, and existing relational architectures. These results demonstrate that
both compositionality and relational inductive biases are essential for efficiently learning representations of
complex higher-order relations.
1.1 Related Work
To place our framework in the context of previous work, we briefly discuss related forms of relational learning
below, pointing first to the review of relational learning inductive biases by Battaglia et al. (2018).
Graph neural networks (GNNs) are a class of neural network architectures which operate on graphs and
process ‚Äúrelational‚Äù data (e.g., Niepert et al., 2016; Kipf and Welling, 2017; Schlichtkrull et al., 2018; Veliƒçkoviƒá
et al., 2018; Kipf et al., 2018; Xu et al., 2019). A defining feature of GNN models is their use of a form of
neural message-passing, wherein the hidden representation of a node is updated as a function of the hidden
representations of its neighbors on a graph (Gilmer et al., 2017). Typical examples of tasks that GNNs are
applied to include node classification, graph classification, and link prediction (Hamilton, 2020).
In GNNs, the ‚Äòrelations‚Äô are given to the model as input via edges in a graph. In contrast, our architecture,
as well as the relational architectures described below, operate on collections of objects without any relations
given as input. Instead, such relational architectures must infer the relevant relations from the objects
themselves. Still, graph neural networks can be applied to these relational tasks by passing in the collection
of objects along with a complete graph.
2Published in Transactions on Machine Learning Research (09/2024)
Initial Embedder
ùë•1,‚Ä¶,ùë•ùëõ‚ààùí≥ùëõInner Product RelationRelational ConvolutionInner Product RelationRelational Convolution
ùëß11,‚Ä¶,ùëßùëõ1‚àà‚Ñùùëõ√óùëëùëÖ(1)‚àà‚Ñùùëõ√óùëõ√óùëü1ùëß12,‚Ä¶,ùëßùëõ22‚àà‚Ñùùëõ2√óùëë2ùëÖ(ùëô‚àí1)‚àà‚Ñùùëõùëô‚àí1√óùëõùëô‚àí1√óùëüùëô‚àí1
‚ãÆùëß1ùëô,‚Ä¶,ùëßùëõùëôùëô‚àà‚Ñùùëõùëô√óùëëùëô
Figure 2: Proposed architecture for relational convo-
lutional networks. Hierarchical relations are modeled
by iteratively computing pairwise relations between
objects and convolving the resultant relation tensor
with graphlet filters representing templates of relations
between groups of objects.Several works have proposed architectures with the
ability to model relations by incorporating an atten-
tion mechanism (e.g., Vaswani et al., 2017; Veliƒçkoviƒá
et al., 2018; Santoro et al., 2018; Zambaldi et al.,
2018; Locatello et al., 2020). Attention mechanisms,
such as self-attention in Transformers (Vaswani et
al., 2017), model relations between objects implicitly
as an intermediate step in an information-retrieval
operation to update the representation of each object
as a function of its context.
There also exists a growing literature on neural ar-
chitectures that aim to explicitly model relational
informationbetweenobjects. Anearlyexampleisthe
relation network proposed by Santoro et al. (2017),
which produces an embedding representation for a
set of objects based on aggregated pairwise rela-
tions. Shanahan et al. (2020) proposes the PrediNet
architecture, which aims to learn relational repre-
sentations that are compatible with predicate logic.
Kerg et al. (2022) proposes CoRelNet, a simple ar-
chitecture based on ‚Äòsimilarity scores‚Äô that aims to
distill the relational inductive biases discovered in
previous work into a minimal architecture. Altabaa
et al. (2024) and Altabaa and Lafferty (2024b) ex-
plore relational inductive biases in the context of
Transformers, and propose a view of relational in-
ductive biases as a type of selective ‚Äúinformation
bottleneck‚Äù which disentangles relational informa-
tion from object-level features. Webb et al. (2024) provides a cognitive science perspective on this idea,
arguing that a relational information bottleneck may be a mechanism for abstraction in the human mind.
2 Multi-Dimensional Inner Product Relation Module
A relation function maps a pair of objects x,y PXto a vector that represents the relationship between them.
For example, a relation may represent comparisons along different attributes of the two objects, such as
‚Äúxhas the same color as y,xis larger than y, andxis to the left of y‚Äù. In principle, this can be modeled
by an arbitrary learnable function on the concatenation of the two objects‚Äô feature representations. For
example, Santoro et al. (2017) use multilayer perceptrons (MLPs) to model relations by processing the
concatenated feature vectors of object pairs. However, this approach lacks crucial inductive biases. While it
is theoretically capable of modeling relations, it imposes no constraints to ensure that the learned pairwise
function reflects meaningful relational patterns. In particular, it entangles the feature representations of the
two objects without explicitly comparing their features.
Following previous work (e.g., Vaswani et al., 2017; Webb et al., 2021; Kerg et al., 2022; Altabaa et al.,
2024), we propose modeling pairwise relations between objects via inner products of feature maps. This
introduces added structure to the pairwise function that explicitly incorporates a comparison operation
(the inner product). The advantage of this approach is that it provides added pressure to learn explicitly
relational representations, disentangling relational information from attributes of individual objects, and
inducing a geometry on the object space X. For example, in the symmetric case, the inner product relation
rpx,y q  xœï px q,œï py qysatisfies symmetry, positive definiteness, and induces a pseudometric on X. The triangle
inequality of the pseudometric expresses a transitivity property‚Äîif xis related to yandyis related to z,
thenxmust be related to z.
More generally, we can allow for multi-dimensional relations by having multiple encoding functions, each
extracting a feature to compute a relation on. Furthermore, we can allow for asymmetric relations by having
3Published in Transactions on Machine Learning Research (09/2024)
different encoding functions for each object. Hence, we model relations by
rpx,y q  pxœï1px q,œà1py qy, ..., xœïdrpx q,œàdrpy qyq, (1)
whereœï1,œà1,...,œïdr,œàdrare learnable functions. The intuition is that, for each dimension, the encoders
extract, or ‚Äòfilter‚Äô out, particular attributes of the objects and the inner products compute similarity across
each attribute. A relation, in this sense, is similarity across a particular attribute. In the asymmetric case,
the attributes extracted from the two objects are different, resulting in an asymmetric relation where one
attribute of the first object is compared with a different attribute of the second object. For example, this can
model relations of the form ‚Äú xis brighter than y‚Äù (an antisymmetric relation).
Altabaa and Lafferty (2024a) analyzes the function approximation properties of neural relation functions of
the form of Equation (1). In particular, the function class of inner products of neural networks is characterized
in both the symmetric case and the asymmetric case. In the symmetric case (i.e., œï œà), it is shown
that inner products of MLPs are universal approximators for symmetric positive definite kernels. In the
asymmetric case, inner products of MLPs are universal approximators for continuous bivariate functions.
The efficiency of approximation is characterized in terms of a bound on the number of neurons needed to
achieve a particular approximation error.
To promote weight sharing, we can have one common non-linear map œïshared across all dimensions together
with different linear projections for each dimension of the relation. That is, r:X X √ëRdris given by
rpx,y q  @
Wk
1œï px q,Wk
2œï py qD
k Prdr s, (2)
where the learnable parameters are œïandWk
1,Wk
2,k P rdrs. The non-linear map œï:X √ëRdœïmay be an
MLP, for example, and Wk
1,Wk
2aredproj dœïmatrices. The class of functions realizable by Equation (2) is
the same as Equation (1) but enables greater weight sharing.
The ‚ÄúMulti-dimensional Inner Product Relation‚Äù (MD-IPR) module receives a sequence of objects px1,...,xn q
as input and models the pairwise relations between them by Equation (2), returning an n n drrelation
tensor,R ri,j s rpxi,xjq, describing the relations between each pair of objects.
3 Relational Convolutions with Graphlet Filters
3.1 Relational Convolutions with Discrete Groups
In this section, we formalize a relational convolution operation which processes pairwise relations between
objects to produce representations of the relational patterns within groups of objects. Suppose that we
have a sequence of objects px1,...,xn qand a relation tensor Rdescribing the pairwise relations between
them, obtained by an MD-IPR layer via R ri,j s rpxi,xjq. The key idea is to learn a template of relations
between a small set of objects, and to ‚Äúconvolve‚Äù the template with the relation tensor, matching it against
the relational patterns in different groups of objects. This transforms the relation tensor into a sequence
of vectors, each summarizing the relational pattern in some group of objects. Crucially, this can now be
composed with another relational layer to compute higher-order relations‚Äîi.e., relations on relations.
Fix some filter size s ¬†n, wheresis a hyperparameter of the relational convolution layer. One ‚Äòfilter‚Äô of size
sis given by the graphlet filter f1 PRs s dr. This is a template for the pairwise relations between a group of
sobjects. Since pairwise relations can be viewed as edges on a graph, we use the term ‚Äúgraphlet filter‚Äù to
refer to a template of pairwise relations between a small set of objects. Let g ¬Ä rn sbe a group of sobjects
among px1,...,xn q. Then, denote the relation sub-tensor associated with this group by R rg s: rR ri,j ssi,j Pg.
We define the ‚Äòrelational inner product‚Äô between this relation subtensor and the filter f1by
xR rg s,f1yrel:¬∏
i,j Pg¬∏
k Prdr sR ri,j,k sf1ri,j,k s. (3)
This is simply the standard inner product in the corresponding Euclidean space Rs2dr. This quantity
represents how much the relational pattern in gmatches the template f1.
In a relational convolution layer, we learn nfdifferent filters. Denote the collection of filters by f  
f1,...,fnf
PRs s dr nf, which we call a graphlet filter . We define the relational inner product of a relation
4Published in Transactions on Machine Learning Research (09/2024)
‚ãÖ,‚ãÖ!"#‚ãÖ,‚ãÖ!"#‚ãÖ,‚ãÖ!"#‚ãØùëß!ùëß"!‚ãØgraphlet	filtersùíá‚àà‚Ñù"√ó"√ó$!√ó%"relation	subtensors	ùëÖùëî,ùëî‚ààùí¢ùëÖ‚àóùíáùëß#
Figure 3: A depiction of the relational convolution operation. A graphlet filter fis compared to the relation
subtensor in each group of objects, producing a sequence of vectors summarizing the relational pattern within
each group. The groups can be differentiably learned through an attention mechanism.
subtensorR rg swith the graphlet filters fas thenf-dimensional vector consisting of the relational inner
products with each individual filter,
xR rg s,f yrel 
xR rg s,f1yrel...@
R rg s,fnfD
rel
 PRnf. (4)
This vector summarizes various aspects of the relational pattern within a group, captured by several different
filters1. Each filter corresponds to one dimension. This is reminiscent of convolutional neural networks, where
each filter gives us one channel in the output tensor.
For a given group g ¬Ä rn s, the relational inner product with a graphlet filter, xR rg s,f yrel, gives us a vector
summarizing the relational patterns inside that group. Let Gbe a set of groupings of the nobjects, each of
sizes. The relational convolution between a relation tensor Rand a relational graphlet filter fis defined as
the sequence of relational inner products with each group in G
R f  pxR rg s,f yrel qg PG 
z1,...,z |G|
PR|G| nf(5)
In this section, we assumed that Gwas given. If some prior information is known about what groupings
are relevant, this can be encoded in G. Otherwise, if nis small, Gcan be all possible combinations of size
s. However, when nis large, considering all combinations will be intractable. In the next subsection, we
consider the problem of differentiably learning the relevant groups.
3.2 Relational Convolutions with Group Attention
In the above formulation, the groups are ‚Äòdiscrete‚Äô. Having discrete groups can be desirable for interpretability
if the relevant groupings are known a priori or if considering every possible grouping is computationally
and statistically feasible. However, if the relevant groupings are not known, then considering all possible
combinations results in a rapid growth of the number of objects at each layer.
In order to address these issues, we can explicitly model and learnthe relevant groups. This allows us to
control the number of objects in the output sequence of a relational convolution such that only relevant
groups are considered. We propose modeling groups via an attention operation.
1We have overloaded the notation x, yrel, but will use the convention that a collection of filters is denoted by a bold symbol
(e.g., fvsfi) to distinguish between the two forms of the relational inner product.
5Published in Transactions on Machine Learning Research (09/2024)
Consider the input px1,...,xn q, xi PRd. Letngbe the number of groups to be learned and sbe the size
of the graphlet filter (and hence the size of each group). These are hyperparameters of the model that we
control. For each group g P rngs, we learnsdifferent queries, tqg
i ug Prng s,i Prs s, that will be used to retrieve a
group of size svia attention. The i-th object in the k-th group is retrieved as follows,
¬Øxg
i n¬∏
j 1Œ±g
ijxj, g P rngs,i P rss,
Œ±g
ij exp pŒ≤ xqg
i,key pxjqyq¬∞n
k 1exp pŒ≤ xqg
i,key pxk qyq, g P rngs,i P rss,j P rn s(6)
where ¬Øxg
iis thei-th object retrieved in the g-th group,qg
iis the query for retrieving the i-th object in the
g-th group, key pxjqis the key associated with the object xj, andŒ≤is a temperature scaling parameter.
The keyfor each object is computed as a function of its position, features, and/or context. For example,
to group objects based on their position, the key can be a positional embedding, key pxiq PEi. To group
based on features, the keycan be a linear projection of the object‚Äôs feature vector, key pxiq Wkxi. To group
based on both position and features, the keycan be a sum or concatenation of the above. Finally, computing
keys after a self-attention operation allows objects to be grouped based on the context in which they occur.
The relation subtensor ¬ØR rg s PRs s drfor each group g P rngsis computed using a shared MD-IPR layer
rp,q,
¬ØR rg s 
rp¬Øxg
i,¬Øxg
jq
i,j Prs s. (7)
The relational convolution is computed as before via,
¬ØR f  @¬ØR rg s,fD
rel
g Prng s. (8)
Overall, relational convolution with group attention can be summarized as follows: 1) learn nggroupings
of objects, retrieving sobjects per group; 2) compute the relation tensor of each group using an MD-IPR
module; 3) compute a relational convolution with a learned set of graphlet filters f, producing a sequence of
ngvectors each describing the relational pattern within a (learned) group of objects.
Computing input-dependent queries. In the simplest case, the query vectors are simply learned
parameters of the model, representing a fixed criterion for selecting the nggroups. The queries can also
be produced in an input-dependent manner. There are many ways to do this. For example, the input
px1,...,xn qcan be processed with some sequence or set embedder (e.g., through a self-attention operation)
producing a vector embedding that can be mapped to different queries tqg
i ui,gusing learned linear maps.
Entropy regularization. Intuitively, we would ideally like the group attention scores in Equation (6) to be
close to discrete assignments. To encourage the model to learn more structured group assignments, we add an
entropy regularization to the loss function, Lentr  png sq1¬∞
g,iH pŒ±g
i, q, whereH pŒ±g
i, q  ¬∞
jŒ±g
ijlog pŒ±g
ij q
is the Shannon entropy. As a heuristic, this regularization can be scaled by a factor proportional to
log pn_classes q{log pn qso that it doesn‚Äôt dominate the underlying task‚Äôs loss. Sparsity regularization in
neural attention has been explored in several previous works, including through entropy regularization (e.g.,
Niculae and Blondel, 2017; Martins et al., 2020; Attanasio et al., 2022).
Symmetric relational inner products. So far, we considered orderedgroups. That is, the relational
pattern computed by the relational inner product xR rg s,f yrelfor the group p1,2,3 qis different from the group
p2,3,1 q. In some scenarios, symmetry in the representation of the relational pattern is a useful inductive
bias. To capture this, we define a symmetric variant of the relational inner product that is invariant to the
ordering of the elements in the group. This can be done by pooling over all permutations in the group. In
particular, we suggest max-pooling or average-pooling, although any set-aggregator would be valid. We define
the permutation-invariant relational inner product as
xR rg s,f yrel,sym Pool  @
R rg1s,fD
rel:g1Pg!(
, (9)
whereg!denotes the set of permutations of the group g, and pooling is done independently across dimensions.
6Published in Transactions on Machine Learning Research (09/2024)
Computational efficiency. Equation (6) can be computed in parallel with O pn ng s d qoperations. When
the hyperparameters of the model are fixed, this is linear in the sequence length n. Equation (7) can be
computedinparallelviaefficientmatrixmultiplicationwith O png s2drdproj qoperations. Finally,Equation(8)
can be computed in parallel with O png s2dr nf qoperations. The latter two computations do not scale
with the number of objects in the input, and are only a function of the hyperparameters of the model.
3.3 Deep Relational Architectures by Composing Relational Convolutions
A relational convolution block (including a MD-IPR module) is a simple neural module that can be composed
to build a deep architecture for learning iteratively more complex relational feature representations.
Following the notation in Figure 2, let n‚Ñìdenote the number of objects and d‚Ñìthe object dimension at layer ‚Ñì.
A relational convolution block receives as input a sequence of objects of shape n‚Ñì d‚Ñìand returns a sequence
of objects of shape n‚Ñì  1 d‚Ñì  1representing the relational patterns among groupings of objects. The output
dimensiond‚Ñì  1corresponds to the number of graphlet filters nf, and is a hyperparameter. The sequence
lengthn‚Ñì  1corresponds to the number of groups, and is |G|in the case of given discrete groups (Section 3.1)
or a hyperparameter ngin the case of learned groups via group attention (Section 3.2). Each composition
of a relational convolution block computes relational features of one degree higher (i.e., relations between
relations).
A common recipe for building modern deep learning architectures is by using residual connections (He et al.,
2016) and normalization (Ba et al., 2016). This can be achieved for relational convolutional networks by fixing
the number of groups ngand number of filters nfhyperparameters to be the same across all layers, such
that the input shape and output shape remain the same. Then, letting H‚Ñìdenote the hidden representation
at layer‚Ñì, the overall architecture becomes H‚Ñì  1 Norm pH‚Ñì  W‚Ñì  1RelConvBlock pH‚Ñìqq, whereW‚Ñì  1is a
linear transformation that controls where information is written to in the residual stream. This ResNet-style
architecture allows for the hidden representation to encode relational information at multiple layers of
hierarchy, retaining the information at shallower layers. Additionally, we can insert MLP layers to process the
relational representations before the next relational convolution layer. In this paper, we limit our exploration
to relatively shallow networks of the form H‚Ñì  1 RelConvBlock pH‚Ñìq.
4 Experiments
In this section, we empirically evaluate the proposed relational convolutional network architecture (abbreviated
RelConvNet) to assess its effectiveness at learning relational tasks. We compare this architecture to several
existing relational architectures as well as general-purpose sequence models. The common input to all models
is a sequence of objects X  px1,...,xn q PRn d. We evaluate against the following baselines.
‚Ä¢Transformer (Vaswani et al., 2017). The Transformer is a powerful general-purpose sequence model.
It consists of alternating self-attention and multi-layer perceptron blocks. Self-attention performs
an information retrieval operation, which updates the internal representation of each object as a
function of its context. Dot product attention is computed via X1√êSoftmax ppXWqqpXWk q‚ä∫qWvX,
and the MLP is applied independently on each object‚Äôs internal representation. The attention scores
computed as an intermediate step in dot-product attention can perhaps be thought of as relations
that determine what information to retrieve.
‚Ä¢PrediNet (Shanahan et al., 2020). The PrediNet architecture is an explicitly relational architecture
inspired by predicate logic. At a high-level, the PrediNet architecture computes jrelations between
kpairs of objects. The kpairs of objects are selected via a learned attention operation. The ‚Äú j
relations‚Äù refer to a difference between j-dimensional embeddings of the selected objects. More
precisely, for each head h P rk s, a pair of objects Eh
1,Eh
2 PRdis retrieved via an attention operation,
and the final output of PrediNet is a set of difference relations given by DhEh
1Ws Eh
2Ws.
‚Ä¢CoRelNet (Kerg et al., 2022). The CoRelNet architecture is proposed as a minimal relational
architecture distilling the core inductive biases that the authors argue are important for relational
tasks. The CoRelNet module simply computes inner products between object representations
and applies Softmax normalization, returning an n n‚Äúsimilarity matrix‚Äù. That is, the objects
X  px1,...,xn qare processed independently to produce embeddings Z  pz1,...,zn q, and the
7Published in Transactions on Machine Learning Research (09/2024)
similarity matrix is computed as R Softmax pZZ‚ä∫q. The similarity matrix Ris then flattened and
passed through an MLP to produce the final output.
‚Ä¢Graph Neural Networks . Graph neural networks are a class of neural network architectures
which operate on graphs-structured data. A graph neural network typically receives two inputs:
a graph described by a set of edges, and feature vectors for each node in the graph. GNNs can
be described through the unifying framework of neural message-passing. Under this framework,
graph-structured data is processed through an iterative message-passing operation given by hpl  1 q
i √ê
Update phpl q
i,thpl q
j uj PN pi qq, wherehp0 q
i √êxi. That is, each node‚Äôs internal representation is iteratively
updated as a function of its neighborhood. Here, Updateis parameterized by a neural network, and
thevariationbetweendifferentGNNarchitecturesliesinthearchitecturaldesignofthisupdateprocess.
We use Graph Convolution Networks (Kipf and Welling, 2017), Graph Attention Networks (Veliƒçkoviƒá
et al., 2018), and Graph Isomorphism Networks (Xu et al., 2019) as representative GNN baselines.
‚Ä¢CNN. As a non-relational baseline, we test a regular convolutional neural network which processes the
raw image input. The central modules in the baselines above receive an object-centric representation
as input. That is, a sequence of vector embeddings produced by a small CNN each corresponding to
one of thenobjects in the input. Here, instead, a deeper CNN model processes the raw image input
representing the entire ‚Äúscene‚Äù in an end-to-end manner.
4.1 Relational Games
Therelational games dataset was contributed as a benchmark for relational reasoning by Shanahan et al.
(2020). It consists of a family of binary classification tasks for identifying abstract relational rules between
a set of objects represented as images. The object images depict simple geometric shapes and consist of
three different splits with different visual styles for evaluating out-of-distribution generalization, referred to
as ‚Äúpentominoes‚Äù, ‚Äúhexominoes‚Äù, and ‚Äústripes‚Äù. The input is a sequence of objects arranged in a 3 3grid.
Each task corresponds to some relationship between the objects, and the target is to classify whether the
relationship holds among the objects in the input or not (see Figure 4).
In our experiments, we evaluate out-of-distribution generalization by training on the pentominoes objects
and evaluating on the hexominoes and stripes objects. The input to the models is presented as a sequence
of9objects, with each object represented as a 12 12 3RGB image. All models share the common
architecture px1,...,xn q √ë CNN √ë tu √ë MLP √ëÀÜy, where tuindicates the central module being tested.
In all models, the objects are first processed independently by a CNN with a shared architecture. The
processed objects are then passed to the central module of the model. The final prediction is produced by
an MLP with a shared architecture. In this section, we focus our comparison on four models: RelConvNet
(ours), CoRelNet (Kerg et al., 2022), PrediNet (Shanahan et al., 2020), and a Transformer (Vaswani et al.,
2017)2. The pentominoes split is used for training, and the hexominoes and stripes splits are used to test
out-of-distribution generalization after training. We train for 50 epochs using the categorical cross-entropy
loss and the Adam optimizer with learning rate 0.001,Œ≤1 0.9,Œ≤2 0.999,œµ 107, and batch size 512. For
each model and task, we run 5 trials with different random seeds. Appendix A describes further experimental
details about the architectures and training setup.
Out-of-distribution generalization. Figure 5 reports model performance on the two hold-out object
sets after training. On the hexominoes objects, which are similar-looking to the pentominoes objects used
for training, RelConvNet and CoRelNet do nearly perfectly. PrediNet and the Transformer do well on the
simpler tasks, but struggle with the more difficult ‚Äòmatch pattern‚Äô task. The ‚Äòstripes‚Äô objects are visually
more distinct from the objects in the training split, making generalization more difficult. We observe an
overall drop in performance for all models. The drop is particularly dramatic for CoRelNet3. The separation
between RelConvNet and the other models is largest on the ‚Äòmatch pattern‚Äô task of the stripes split (the
most difficult task and the most difficult generalization split). Here, RelConvNet maintains a mean accuracy
2The GNN baselines failed to learn the relational games tasks in a way that generalizes, often severely overfitting. For clarity
of presentation, we defer results on the GNN baselines to Appendix A.
3TheexperimentsinKergetal.(2022)ontherelationalgamesbenchmarkuseatechniquecalled‚Äúcontextnormalization‚Äù(Webb
et al., 2020) as a preprocessing step. We choose not to use this technique since it is an added confounder. We discuss this choice
in Appendix C.
8Published in Transactions on Machine Learning Research (09/2024)
Pentominoes
 Hexominoes
 Stripes
same
 occurs
 xoccurs
 between
 match patt
Figure 4: Relational games dataset. LeftExamples of objects from each split. RightExamples of problem
instances for each task. The first row is an example where the relation holds and the second row is an example
where the relation does not hold.
same between occurs xoccurs match pattern
Task0.00.20.40.60.81.0AccuracySplit = Hexos
same between occurs xoccurs match pattern
TaskSplit = Stripes
Model
RelConvNet
CoRelNet
PrediNet
Transformer
CNN
Figure 5: Out-of-distribution generalization on hold-out object sets. Bar heights indicate the mean over 5
trials and the error bars indicate a bootstrap 95% confidence interval.
of 87% while the other models drop below 65%. We attribute this to RelConvNet‚Äôs ability to naturally
represent higher-order relations and model groupings of objects. The CNN baseline learns the easier ‚Äòsame‚Äô,
‚Äòbetween‚Äô, and ‚Äòoccurs‚Äô tasks nearly perfectly, but completely fails to learn the more difficult ‚Äòxoccurs‚Äô and
‚Äòmatch pattern‚Äô tasks. This hard boundary suggests that explicit relational architectural inductive biases are
necessary for learning more difficult relational tasks.
Data efficiency. We observe that the relational inductive biases of RelConvNet, and relational models
more generally, grant a significant advantage in sample-efficiency. Figure 6 shows the training accuracy
over the first 2,000 batches for each model. RelConvNet, CoRelNet, and PrediNet are explicitly relational
architectures, whereas the Transformer is not. The Transformer is able to process relational information
through its attention mechanism, but this information is entangled with the features of individual objects
(which, for these relational tasks, is extraneous information). The Transformer consistently requires the
largest amount of data to learn the relational games tasks. PrediNet tends to be more sample-efficient.
RelConvNet and CoRelNet are the most sample-efficient, with RelConvNet only slightly more sample-efficient
on most tasks.
On the ‚Äòmatch pattern‚Äô task, which is the most difficult, RelConvNet is significantly more sample-efficient.
We attribute this to the fact that RelConvNet is able to model higher-order relations through its relational
convolution module. The ‚Äòmatch pattern‚Äô task can be thought of as a second-order relational task‚Äîit involves
computing the relational pattern in each of two groups, and comparing the two relational patterns. The
relational convolution module naturally models this kind of situation since it learns representations of the
relational patterns within groups of objects.
9Published in Transactions on Machine Learning Research (09/2024)
0 1000 2000
batch step0.60.81.0training accuracysame
0 1000 2000
batch step0.60.81.0training accuracybetween
0 1000 2000
batch step0.60.81.0training accuracyoccurs
0 1000 2000
batch step0.60.81.0training accuracyxoccurs
0 1000 2000
batch step0.60.81.0training accuracymatch pattern
Model
RelConvNet
CoRelNet
PrediNet
Transformer
CNN
Figure 6: Training curves, up to 2,000 batch steps, for each relational games task. Solid lines indicate the
mean over 5 trials and the shaded regions indicate a bootstrap 95% confidence interval. Note that this is
in-distribution (i.e., on the ‚Äúpentominoes‚Äù split).
Group 1
 Group 2
 Group 3
 Group 4
Group 5
 Group 6
 Group 7
 Group 8
Figure 7: Learned groups in the ‚Äòmatch pattern‚Äô tasks
by a 2-layer RelConvNet with group attention.Learning groups via group attention. Next, we
analyze RelConvNet‚Äôs ability to learn useful group-
ings through group attention in an end-to-end man-
ner. We train a 2-layer relational convolutional net-
work with 8learned groups and a graphlet size of
3. We group based on position by using positional
embeddings for key pxiq. In Figure 7, we visualize
the group attention scores Œ±g
ij(see Equation (6))
learned from one of the training runs. For each
groupg P rngs, the figure depicts a 3 3grid repre-
senting the objects attended to in that group. Since
each group contains 3objects, we represent the value
pŒ±g
ij qi Pr3 sin the 3-channel HSV color representation.
We observe that 1) group attention learns to ignore the middle row, which contains no relevant information;
and 2) the selection of objects in the top row and the bottom row is structured. In particular, group 2
considers the relational pattern within the bottom row and group 8considers the relational pattern in the
top row, which is exactly how a human would tackle this problem. We refer to Figure 11 for an exploration
of the effect of entropy regularization on group attention. We find that entropy regularization is necessary for
the model to learn and causes the group attention scores to converge to interpretable discrete assignments.
4.2 Set: Grouping and Compositionality in Relational Reasoning
Setis a card game that forms a simple-to-describe but challenging relational task. The ‚Äòobjects‚Äô are a set of
cards with four attributes, each of which can take one of three possible values. ‚ÄòColor‚Äô can be red, green, or
purple; ‚Äònumber‚Äô can be one, two, or three; ‚Äòshape‚Äô can be diamond, squiggle, or oval; and ‚Äòfill‚Äô can be solid,
striped, or empty. A ‚Äòset‚Äô is a triplet of cards such that each attribute is either a) the same on all three cards,
or b) different on all three cards.
InSet, the task is: given a hand of n ¬°3cards, find a ‚Äòset‚Äô among them. Figure 8a depicts a positive and
negative example for n 5, with indicating the ‚Äòset‚Äô in the positive example. This task is deceptively
challenging, and is representative of the type of relational reasoning that humans excel at but machine
learning systems still struggle with. To solve the task, one must process the sensory information of individual
cards to identify the values of each attribute, and then reason about the relational pattern in each triplet of
10Published in Transactions on Machine Learning Research (09/2024)
label = 0
 label = 1
*
 *
 *
(a) Example of ‚Äúcontains set‚Äù task.
RelConvNetCoRelNet PrediNetTransformerGCN GIN GATLSTMCNN
Model0.00.20.40.60.81.0Accuracy (b)Hold-out test accuracy.
0 20 40 60 80 100
epoch0.50.60.70.80.91.0AccuracySplit = Train
0 20 40 60 80 100
epochSplit = Validation
Model
RelConvNet
CoRelNet
PrediNet
Transformer
GCN
GIN
GAT
LSTM
CNN
(c) Training accuracy and validation accuracy over the course of training.
Figure 8: Results of ‚Äúcontains set‚Äù experiments. Bar height/solid lines indicate the mean over 10 trials and
error bars/shaded regions indicate 95% bootstrap confidence intervals.
cards. The construct of relational convolutions proposed in this paper is a step towards developing machine
learning systems that can perform this kind of relational reasoning.
In this section, we evaluate RelConvNet on a task based on Setand compare it to several baselines. The task
is: given a collection of n 5images of Setcards, determine whether or not they contain a ‚Äòset‚Äô. All models
share the common architecture px1,...,xn q √ëCNN √ë tu √ë MLP √ëÀÜy, where tuindicates the central module
being tested. The CNN embedder is pre-trained on the task of classifying the four attributes of the cards
and an intermediate layer is used to generate embeddings. The output MLP architecture is shared across all
models. Further architectural details can be found in Appendix A.
InSet, there exists 81
3
85 320triplets of cards, of which 1 080are a ‚Äòset‚Äô. We partition the ‚Äòsets‚Äô into
training (70%), validation (15%), and test (15%) sets. The training, validation, and test datasets are generated
by sampling n-tuples of cards such that with probability 1 {2then-tuple does not contain a set, and with
probability 1 {2it contains a set among the corresponding partition of sets. Partitioning the data in this way
allows us to measure the models‚Äô ability to ‚Äúlearn the rule‚Äù and identify new unseen ‚Äòsets‚Äô. We train for 100
epochs with the same loss, optimizer, and batch size as the experiments in the previous section. For each
model, we run 10 trials with different random seeds.
When using the default optimizer hyperparameters as in the previous experiment without hyperparameter
tuning, we find that RelConvNet is the only model able to meaningfully learn the task in a manner that
generalizes to unseen ‚Äòsets‚Äô. In particular, we observe that many baselines severely overfit to the training
data, failing to learn the rule and generalize (see Appendix B.1). Although RelConvNet did not require
hyperparameter tuning, we carry out an extensive hyperparameter sweep for all other baselines individually
in order to validate our conclusions against the best-achievable performance for each baseline. We ran a total
11Published in Transactions on Machine Learning Research (09/2024)
of 1600 experimental runs searching over combinations of architectural hyperparameters (number of layers)
and optimization hyperparameters (weight decay, learning rate schedule) individually for each baseline, with
the goal of finding a hyperparameter configuration that is representative of the best achievable performance
for each model class on this task. The results of the hyperparameter sweep are summarized in Appendix B.
Figure 8b shows the hold-out test accuracy for each model. Figure 8c shows the training and validation
accuracy over the course of training. Here, RelConvNet uses the Adam optimizer with the default Tensorflow
hyperparameters (constant learning rate of 0.001,Œ≤1 0.9,Œ≤2 0.999) while each baseline has its own
individually-optimized hyperparameters, described in Appendix B.
We observe a sharp separation between RelConvNet and all other baselines. While RelConvNet is able
to learn the task and generalize to new ‚Äòsets‚Äô with near-perfect accuracy (avg: 97.9%), no other model is
able to reach a comparable generalization accuracy even after hyperparameter tuning. The next best is the
GAT model (avg: 67.5%). Several models are able to fit the training data, reaching near-perfect training
accuracy, but they are unable to ‚Äúlearn the rule‚Äù in a way that generalizes to the validation or test sets. This
suggests that while these models are powerful function approximators, they lack the inductive biases to learn
hierarchical relations.
40
 20
 0 20 40 60 80
R[g],frel [PC1]
20
02040R[g],frel [PC2]
not set
set
Figure 9: The relational convolution layer produces
representations that separates ‚Äòsets‚Äô from ‚Äònon-sets‚Äô.In Figure 9 we analyze the geometry of the represen-
tations learned by the relational convolution layer.
We consider all triplets of cards, compute the rela-
tion subtensor using the learned MD-IPR layer, and
plot the relational inner product with the learned
graphlet filter f PRs s dr nf. The result is a nf-
dimensional vector for each triplet of cards. We
perform PCA to plot this in two dimensions, and
color-code each triplet of cards according to whether
or not it forms a ‚Äòset‚Äô. We find that the relational
convolution layer learns a representation of the re-
lational pattern in groups of objects that separates
‚Äòsets‚Äô and ‚Äònon-sets‚Äô. In particular, the two classes
form clusters that are linearly separable even when
projected down to two dimensions by PCA. This
explains why RelConvNet is able to learn the task
in a way that generalizes while the other models are
not. In Appendix E we expand on this discussion,
and further analyze the representations learned by the MD-IPR layer, showing that the learned relations
map to the color, number, shape, and fill attributes.
It is perhaps surprising that models like GNNs and Transformers perform poorly on these relational tasks,
given their apparent ability to process relations through neural message-passing and attention, respectively.
We remark that GNNs operate in a different domain compared to relational models like RelConvNe, PrediNet,
and CoRelNet. In GNNs, the relations are an input to the model, received in the form of a graph, and are
used to dictate the flow of information in a neural message-passing operation. By contrast, in relational
convolutional networks, the input is simply a set of objects without relations‚Äîthe relations need to be inferred
as part of the feature representation process. Thus, GNNs operate in domains where relational information is
already present (e.g., analysis of social networks, biological networks, etc.), whereas our framework aims to
solve tasks that rely on relations but those relations need to be inferred end-to-end. This offers a partial
explanation for the inability of GNNs to learn this task‚ÄîGNNs are good at processing network-style relations
when they are given as input, but may not be able to infer and hierarchically process relations when they
are not given. In the case of Transformers, relations are modeled implicitly to direct information retrieval
in attention, but are not encoded explicitly in the final representations. By contrast, RelConvNet operates
on collections of objects and possesses inductive biases for learning iteratively more complex relational
representations, guided only by the supervisory signal of the downstream task.
12Published in Transactions on Machine Learning Research (09/2024)
Models like CoRelNet and PrediNet have relational inductive biases, but lack compositionality. On the other
hand, deep models like Transformers and GNNs are compositional, but lack relational inductive biases. This
experiment suggests that compositionality and relational inductive biases are both necessary ingredients to
efficiently learn representations of higher-order relations . RelConvNet is a compositional architecture imbued
with relational inductive biases and a demonstrated ability to tackle hierarchical relational tasks.
5 Discussion
Summary
In this paper, we proposed a compositional architecture and framework for learning hierarchical relational
representations via a novel relational convolution operation. The relational convolution operation we propose
here is a ‚Äòconvolution‚Äô in the sense that it considers a patch of the relation tensor, given by a subset of
objects, and compares the relations within it to a template graphlet filter via an appropriately-defined inner
product. This is analogous to convolutional neural networks, where an image filter is compared against
different patches of the input image. Moreover, we propose an attention-based mechanism for modeling
useful groupings of objects in order to maintain scalability. By alternating inner product relation layers and
relational convolution layers, we obtain an architecture that naturally models hierarchical relations.
Discussion on relational inductive biases
In our experiments, we observed that general-purpose sequence models like the Transformer struggle to
learn tasks that involve relational reasoning in a data-efficient manner. The relational inductive biases of
RelConvNet, CoRelNet, and PrediNet result in significantly improved performance on the relational games
tasks. These models each implement different kinds of relational inductive biases, and are each designed with
different motivations in mind. For example, PrediNet‚Äôs architecture is loosely inspired by the structure of
predicate logic, but can be understood as ultimately producing representations of pairwise difference-relations,
with pairs of objects selected by an attention operation. CoRelNet is a minimal relational architecture
that consists of computing an n ninner product similarity matrix followed by a softmax normalization.
RelConvNet, our proposed architecture, provides further flexibility across several dimensions. Like CoRelNet,
it models relations as inner products of feature maps, but it achieves greater representational capacity by
learning multi-dimensional relations through multiple learned feature maps or filters. More importantly, the
relational convolutions operation enables learning higher-order relations between groups of objects. This is in
contrast to both PrediNet and CoRelNet, which are limited to pairwise relations. Our experiments show
that the inductive biases of RelConvNet result in improved performance in relational reasoning tasks. In
particular, the Settask, where RelConvNet was the only model able to generalize non-trivially, demonstrates
the necessity for explicit inductive biases that support learning hierarchical relations.
Limitations and future work
The tasks considered here are solvable by modeling only second-order relations. In the case of the relational
games benchmark of Shanahan et al. (2020), we observe that the tasks are saturated by the relational
convolutional networks architecture. While the ‚Äúcontains set‚Äù task demonstrates a sharp separation between
relational convolutional networks and existing baselines, this task too only involves second-order relations. A
more thorough evaluation of this architecture, and future architectures for modeling hierarchical relations,
would require the development of new benchmark tasks and datasets that involve a larger number of objects
and higher-order relations. This is a subtle and non-trivial task that we leave for future work.
The modules proposed in this paper assume object-centric representations as input. In particular, the tasks
considered in our experiments have an explicit delineation between different objects. In more general settings,
object information may need to be extracted from raw stimulus explicitly by the system (e.g., a natural image
containing multiple objects in apriori unknown positions). Learning object-centric representations is an active
area of research (Sabour et al., 2017; Greff et al., 2019; Locatello et al., 2020; Kipf et al., 2022), and is related
but separate from learning relational representations. These methods produce a set of embedding vectors,
each describing a different object in the scene, which can then be passed to the central processing module
(e.g., a relational processing module such as RelConvNet). In future work, it will be important to explore how
well RelConvNet integrates with methods for learning object-centric representations in an end-to-end system.
13Published in Transactions on Machine Learning Research (09/2024)
The experiments considered here are synthetic relational tasks designed for a controlled evaluation. In
more realistic settings, we envision relational convolutional networks as modules embedded in a broader
architecture. For example, a relational convolutional network can be embedded into an RL agent to enable
performing tasks involving relational reasoning. Similarly, relational convolutions can perhaps be integrated
into general-purpose sequence models, such as Transformers, to enable improved relational reasoning while
retaining the generality of the architecture.
Code and Reproducibility
The project repository can be found here: https://github.com/Awni00/Relational-Convolutions . It
includes an implementation of the relational convolutional networks architecture, code and instructions for
reproducing our experimental results, and links to experimental logs.
Acknowledgment
This work is supported by the funds provided by the National Science Foundation and by DoD OUSD (R&E)
under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence).
References
Altabaa, Awni and John Lafferty (2024a). ‚ÄúApproximation of relation functions and attention mechanisms‚Äù.
arXiv: 2402.08856 [cs.LG] (cited on page 4).
Altabaa, Awni and John Lafferty (2024b). ‚ÄúDisentangling and Integrating Relational and Sensory Information
in Transformer Architectures‚Äù. arXiv: 2405.16727 [cs.LG] (cited on page 3).
Altabaa, Awni, Taylor Whittington Webb, Jonathan D. Cohen, and John Lafferty (2024). ‚ÄúAbstractors and
relational cross-attention: An inductive bias for explicit relational reasoning in Transformers‚Äù. In: The
Twelfth International Conference on Learning Representations (cited on page 3).
Attanasio, Giuseppe, Debora Nozza, Dirk Hovy, and Elena Baralis (2022). ‚ÄúEntropy-based Attention Regu-
larization Frees Unintended Bias Mitigation from Lists‚Äù. In: Findings of the Association for Computational
Linguistics: ACL 2022 (cited on page 6).
Ba, Jimmy Lei, Jamie Ryan Kiros, and Geoffrey E. Hinton (2016). ‚ÄúLayer Normalization‚Äù. arXiv: 1607.06450
[stat.ML] (cited on page 7).
Battaglia, Peter W. et al. (2018). ‚ÄúRelational Inductive Biases, Deep Learning, and Graph Networks‚Äù. arXiv:
1806.01261 [cs, stat] (cited on page 2).
Carpenter, Patricia A, Marcel A Just, and Peter Shell (1990). ‚ÄúWhat one intelligence test measures: a
theoretical account of the processing in the Raven Progressive Matrices Test.‚Äù In: Psychological review .
Englund, CE, DL Reeves, CA Shingledecker, DR Thorne, KP Wilson, and FW Hegge (1987). ‚ÄúUnified tri-
service cognitive performance assessment battery (UTC-PAB) I. Design and Specification of the Battery‚Äù.
In:Naval Health Research Center Report. San Diego, California .
Fagot, Jo√´l, Edward A Wasserman, and Michael E Young (2001). ‚ÄúDiscriminating the relation between
relations: the role of entropy in abstract conceptualization by baboons (Papio papio) and humans (Homo
sapiens).‚Äù In: Journal of Experimental Psychology: Animal Behavior Processes (cited on page 1).
Ferster, Charles Bohris (1960). ‚ÄúIntermittent reinforcement of matching to sample in the pigeon‚Äù. In: Journal
of the Experimental Analysis of Behavior (cited on page 1).
Frank, Stefan L, Rens Bod, and Morten H Christiansen (2012). ‚ÄúHow hierarchical is language use?‚Äù In:
Proceedings of the Royal Society B: Biological Sciences (cited on page 28).
Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl (2017). ‚ÄúNeural
Message Passing for Quantum Chemistry‚Äù. In: International Conference on Machine Learning . PMLR
(cited on page 2).
Greff, Klaus, Rapha√´l Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, Loic
Matthey, Matthew Botvinick, and Alexander Lerchner (2019). ‚ÄúMulti-Object Representation Learning with
Iterative Variational Inference‚Äù. In: Proceedings of the 36th International Conference on Machine Learning .
Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov. Proceedings of Machine Learning Research. PMLR
(cited on page 13).
Hamilton, William L (2020). ‚ÄúGraph Representation Learning‚Äù. Synthesis Lectures on Artificial Intelligence
and Machine Learning. San Rafael, CA: Morgan & Claypool (cited on page 2).
14Published in Transactions on Machine Learning Research (09/2024)
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2016). ‚ÄúDeep residual learning for image
recognition‚Äù. In: Proceedings of the IEEE conference on computer vision and pattern recognition (cited on
page 7).
Hochmann, Jean-R√©my, Arin S. Tuerk, Sophia Sanborn, Rebecca Zhu, Robert Long, Megan Dempster, and
Susan Carey (2017). ‚ÄúChildren‚Äôs representation of abstract relations in relational/array match-to-sample
tasks‚Äù. In: Cognitive Psychology .
Johnson, Justin, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross
Girshick (2017). ‚ÄúCLEVR: Adiagnosticdataset forcompositionallanguageandelementaryvisualreasoning‚Äù.
In:Proceedings of the IEEE conference on computer vision and pattern recognition (cited on page 28).
Kerg, Giancarlo, Sarthak Mittal, David Rolnick, Yoshua Bengio, Blake Richards, and Guillaume Lajoie (2022).
‚ÄúOn Neural Architecture Inductive Biases for Relational Tasks‚Äù. arXiv: 2206.05056 [cs] (cited on pages 3,
7, 8, 26).
Kipf, Thomas, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold,
Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff (2022). ‚ÄúConditional Object-Centric Learning
from Video‚Äù. In: International Conference on Learning Representations (cited on page 13).
Kipf, Thomas, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel (2018). ‚ÄúNeural relational
inference for interacting systems‚Äù. In: International conference on machine learning (cited on page 2).
Kipf, Thomas N. and Max Welling (2017). ‚ÄúSemi-Supervised Classification with Graph Convolutional
Networks‚Äù. arXiv: 1609.02907 [cs, stat] (cited on pages 2, 8).
Locatello, Francesco, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf (2020). ‚ÄúObject-centric learning with slot attention‚Äù. In:
Advances in Neural Information Processing Systems (cited on pages 3, 13).
Marcus, Gary F, Sugumaran Vijayan, Shoba Bandi Rao, and Peter M Vishton (1999). ‚ÄúRule learning by
seven-month-old infants‚Äù. In: Science.
Martins, Andr√©, Ant√≥nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo (2020).
‚ÄúSparse and continuous attention mechanisms‚Äù. In: Advances in Neural Information Processing Systems
(cited on page 6).
Niculae, Vlad and Mathieu Blondel (2017). ‚ÄúA regularized framework for sparse and structured neural
attention‚Äù. In: Advances in neural information processing systems (cited on page 6).
Niepert, Mathias, Mohamed Ahmed, and Konstantin Kutzkov (2016). ‚ÄúLearning convolutional neural networks
for graphs‚Äù. In: International conference on machine learning . PMLR (cited on page 2).
Rosario, Barbara, Marti A Hearst, and Charles J Fillmore (2002). ‚ÄúThe descent of hierarchy, and selection in
relational semantics‚Äù. In: Proceedings of the 40th Annual Meeting of the Association for Computational
Linguistics (cited on page 28).
Sabour, Sara, Nicholas Frosst, and Geoffrey E Hinton (2017). ‚ÄúDynamic routing between capsules‚Äù. In:
Advances in neural information processing systems (cited on page 13).
Santoro, Adam, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan
Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap (2018). ‚ÄúRelational Recurrent Neural
Networks‚Äù. In: Advances in Neural Information Processing Systems . Curran Associates, Inc. (cited on
page 3).
Santoro, Adam, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia,
and Timothy Lillicrap (2017). ‚ÄúA simple neural network module for relational reasoning‚Äù. In: Advances in
neural information processing systems (cited on pages 3, 28).
Schlichtkrull, Michael, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling
(2018). ‚ÄúModeling relational data with graph convolutional networks‚Äù. In: The Semantic Web: 15th
International Conference, ESWC 2018, Heraklion, Crete, Greece, June 3‚Äì7, 2018, Proceedings 15 . Springer
(cited on page 2).
Shanahan, Murray, Kyriacos Nikiforou, Antonia Creswell, Christos Kaplanis, David Barrett, and Marta
Garnelo (2020). ‚ÄúAn Explicitly Relational Neural Network Architecture‚Äù. In: Proceedings of the 37th
International Conference on Machine Learning . Ed. by Hal Daum√© III and Aarti Singh. Proceedings of
Machine Learning Research. PMLR (cited on pages 2, 3, 7, 8, 13).
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser,
and Illia Polosukhin (2017). ‚ÄúAttention is all you need‚Äù. In: Advances in neural information processing
systems(cited on pages 3, 7, 8).
15Published in Transactions on Machine Learning Research (09/2024)
Veliƒçkoviƒá, Petar, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li√≤, and Yoshua Bengio
(2018). ‚ÄúGraph Attention Networks‚Äù. In: International Conference on Learning Representations (cited on
pages 2, 3, 8).
Webb, Taylor, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall O‚ÄôReilly, and Jonathan Cohen
(2020). ‚ÄúLearning Representations that Support Extrapolation‚Äù. In: Proceedings of the 37th International
Conference on Machine Learning . Ed. by Hal Daum√© III and Aarti Singh. Proceedings of Machine Learning
Research. PMLR (cited on pages 8, 26).
Webb, Taylor W., Steven M. Frankland, Awni Altabaa, Kamesh Krishnamurthy, Declan Campbell, Jacob
Russin, Randall O‚ÄôReilly, John Lafferty, and Jonathan D. Cohen (2024). ‚ÄúThe Relational Bottleneck as an
Inductive Bias for Efficient Abstraction‚Äù. arXiv: 2309.06629 [cs] (cited on page 3).
Webb, Taylor Whittington, Ishan Sinha, and Jonathan Cohen (2021). ‚ÄúEmergent Symbols through Binding
in External Memory‚Äù. In: International Conference on Learning Representations (cited on pages 1, 3, 26).
Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka (2019). ‚ÄúHow Powerful are Graph Neural
Networks?‚Äù In: International Conference on Learning Representations (cited on pages 2, 8).
Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander
J Smola (2017). ‚ÄúDeep sets‚Äù. In: Advances in neural information processing systems (cited on page 28).
Zambaldi, Vinicius, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls,
David Reichert, Timothy Lillicrap, Edward Lockhart, et al. (2018). ‚ÄúDeep reinforcement learning with
relational inductive biases‚Äù. In: International conference on learning representations (cited on page 3).
Zeiler, Matthew D and Rob Fergus (2014). ‚ÄúVisualizing and understanding convolutional networks‚Äù. In:
Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,
Proceedings, Part I 13 . Springer (cited on page 2).
16Published in Transactions on Machine Learning Research (09/2024)
A Experiments Supplement
A.1 Relational Games (Section 4.1)
The pentominoes split is used for training, and the hexominoes and stripes splits are used to test out-of-
distribution generalization after training. We hold out 1000 samples for validation (during training) and 5000
samples for testing (after training), and use the rest as the training set. We train for 50 epochs using the
categorical cross-entropy loss and the Adam optimizer with learning rate 0.001,Œ≤1 0.9,Œ≤2 0.999,œµ 107.
We use a batch size of 512. For each model and task, we run 5 trials with different random seeds.Table 1
contains text descriptions of each task in the relational games dataset in the experiments of Section 4.1. Table 2
contains a description of the architectures of each model (or shared component) in the experiments. Table 3
reports the accuracy on the hold-out object sets (i.e., the numbers depicted in Figure 5 of the main
text). Figures 10 and 11 explore the effect of entropy regularization in group attention on learning using the
‚Äúmatch pattern‚Äù task as an example.
Task Description
same Two random cells out of nine are occupied by an object. They are the ‚Äúsame‚Äù
if they have the same color, shape, and orientation (i.e., identical image)
occurs The top row contains one object and the bottom row contains three objects.
The ‚Äúoccurs‚Äù relationship holds if at least one of the objects in the bottom
row is the same as the object in the top row.
xoccurs Same as occurs, but the relationship holds if exactly one of the objects in
the bottom row is the same as the object in the top row.
between The grid is occupied by three objects in a line (horizontal or vertical). The
‚Äúbetween‚Äù relationship holds if the outer objects are the same.
row match pattern The first and third rows of the grid are occupied by three objects each. The
‚Äúmatch pattern‚Äù relationship holds if the relation pattern in each row is the
same (e.g., AAA, AAB, ABC, etc.)
Table 1: Relational games tasks.
17Published in Transactions on Machine Learning Research (09/2024)
Model / Component Architecture
Common CNN
EmbedderConv2D √ëMaxPool2D √ëConv2D √ëMaxPool2D √ëFlatten.
Conv2D: num filters = 16, filter size = 3 3, activation = relu.
MaxPool2D : stride = 2.
Common output MLP Dense(64, ‚Äòrelu‚Äô) √ëDense(2) .
RelConvNet CNN Embedder √ëMD-IPR √ëRelConv √ëFlatten √ëMLP.
MD-IPR: relation dim = 16, projection dim = 4, symmetric.
RelConv: num filters = 16, filter size = 3, discrete groups = combinations.
CoRelNet CNN Embedder √ëCoRelNet √ëFlatten √ëMLP.
Standard CoRelNet has no hyperparameters.
PrediNet CNN Embedder √ëPrediNet √ëFlatten √ëMLP.
PrediNet : key dim = 4, number of heads = 4, num relations = 16.
Transformer CNN Embedder √ëTransformerEncoder √ëAveragePooling √ëMLP.
TransformerEncoder : num layers = 1, num heads = 8, feedforward intermediate
size = 32, activation = relu.
GCN CNN Embedder √ëAddPosEmb √ë(GCNConv √ëDense) 2 √ëAveragePooling √ë
MLP.
GCConv: channels = 32, Dense: num neurons = 32, activation = relu
GAT CNN Embedder √ëAddPosEmb √ë(GATConv √ëDense) 2 √ëAveragePooling √ë
MLP.
GATonv: channels = 32, Dense: num neurons = 32, activation = relu
GCN CNN Embedder √ëAddPosEmb √ë(GINConv √ëDense) 2 √ëAveragePooling √ë
MLP.
GINConv: channels = 32, Dense: num neurons = 32, activation = relu
CNN ( Conv2D √ëMaxPool2D ) 8 √ëFlatten √ëDense(128, ‚Äòrelu‚Äô) √ëDense(2)
Conv2D: num filters = [16, 16, 32, 32, 64, 64, 128, 128], filter size = 3
MaxPool2D : stride = 2, apply every other layer.
Table 2: Model architectures for relational games experiments.
18Published in Transactions on Machine Learning Research (09/2024)
Hexos Accuracy Stripes Accuracy
Task Model
same RelConvNet 0.989 0.002 0.974 0.003
CoRelNet 0.988 0.006 0.724 0.112
PrediNet 0.990 0.004 0.983 0.007
Transformer 0.997 0.001 0.993 0.004
CNN 0.990 0.001 0.976 0.002
between RelConvNet 0.991 0.001 0.988 0.002
CoRelNet 0.995 0.001 0.582 0.063
PrediNet 0.978 0.006 0.950 0.019
Transformer 0.986 0.003 0.961 0.010
CNN 0.990 0.001 0.971 0.003
occurs RelConvNet 0.980 0.001 0.880 0.015
CoRelNet 0.992 0.004 0.518 0.012
PrediNet 0.907 0.020 0.775 0.046
Transformer 0.881 0.015 0.724 0.021
CNN 0.984 0.008 0.953 0.012
xoccurs RelConvNet 0.967 0.001 0.946 0.006
CoRelNet 0.980 0.007 0.606 0.035
PrediNet 0.872 0.036 0.810 0.028
Transformer 0.867 0.017 0.753 0.031
CNN 0.597 0.097 0.590 0.084
match pattern RelConvNet 0.961 0.015 0.870 0.041
CoRelNet 0.942 0.011 0.581 0.026
PrediNet 0.710 0.040 0.658 0.053
Transformer 0.627 0.005 0.591 0.006
CNN 0.499 0.000 0.489 0.000
Table 3: Out-of-distribution generalization results on relational games. We report means standard error of
mean over 5 trials. These are the numbers associated with Figure 5.
A.2 Set(Section 4.2)
We train for 100 epochs using the cross-entropy loss. RelConvNet uses the Adam optimizer with learning
rate0.001,Œ≤1 0.9,Œ≤2 0.999,œµ 107. The baselines each use their own individually-tuned optimization
hyperparameters, described in Appendix B. We use a batch size of 512. For each model and task, we run
5 trials with different random seeds.Table 4 contains a description of the architecture of each model in
the ‚Äúcontains set‚Äù experiments of Section 4.2. Table 5 reports the generalization accuracies on the hold-out
‚Äòsets‚Äô (i.e., the numbers depicted in Figure 8b of the main text). Figure 12 explores the effect of different
RelConvNet hyperparameters on the model‚Äôs ability to learn the the Settask.
19Published in Transactions on Machine Learning Research (09/2024)
Model / Component Architecture
Common CNN
EmbedderConv2D √ëMaxPool2D √ëConv2D √ëMaxPool2D √ëFlatten √ëDense(64,
‚Äôrelu‚Äô) √ëDense(64, ‚Äôtanh‚Äô) .
Conv2D: num filters = 32, filter size = 5 5, activation = relu.
MaxPool2D : stride = 4.
Common output MLP Dense(64, ‚Äòrelu‚Äô) √ëDense(32, ‚Äòrelu‚Äô) √ëDense(2) .
RelConvNet CNN Embedder √ëMD-IPR √ëRelConv √ëFlatten √ëMLP.
MD-IPR: relation dim = 16, projection dim = 16, symmetric.
RelConv: num filters = 16, filter size = 3, discrete groups = combinations,
symmetric relational inner product with ‚Äòmax‚Äô aggregator.
CoRelNet CNN Embedder √ëCoRelNet √ëFlatten √ëMLP.
Standard CoRelNet has no hyperparameters.
PrediNet CNN Embedder √ëPrediNet √ëFlatten √ëMLP.
PrediNet : key dim = 4, number of heads = 4, num relations = 16.
Transformer CNN Embedder √ëTransformerEncoder √ëAveragePooling √ëMLP.
TransformerEncoder : num layers = 2, num heads = 8, feedforward intermediate
size = 128, activation = relu.
GCN CNN Embedder √ë(GCNConv √ëDense) 2 √ëAveragePooling √ëMLP.
GCConv: channels = 128, Dense: num neurons = 128, activation = relu
GAT CNN Embedder √ë(GATConv √ëDense) 1 √ëAveragePooling √ëMLP.
GATonv: channels = 128, Dense: num neurons = 128, activation = relu
GCN CNN Embedder √ë(GINConv √ëDense) 2 √ëAveragePooling √ëMLP.
GINConv: channels = 128, Dense: num neurons = 128, activation = relu
CNN ( Conv2D √ëMaxPool2D ) 10 √ëFlatten √ëDense(128, ‚Äòrelu‚Äô) √ëDense(2)
Conv2D: num filters = [16, 16, 32, 32, 64, 64, 128, 128, 128, 128], filter size = 3
MaxPool2D : stride = [(2,2), NA, (2,2), NA, (2,2), NA, (2,2), (1,2), (1,2), (2, 2)]
Table 4: Model architectures for ‚Äúcontains set‚Äù experiments.
Accuracy
Model
RelConvNet 0.979 0.006
CoRelNet 0.563 0.001
PrediNet 0.513 0.003
Transformer 0.563 0.002
GCN 0.635 0.003
GIN 0.593 0.001
GAT 0.675 0.002
LSTM 0.609 0.002
CNN 0.506 0.006
Table 5: Hold-out test accuracy on ‚Äúcontains set‚Äù task. We report means standard error of mean over 10
trials. These are the numbers associated with Figure 8b.
20Published in Transactions on Machine Learning Research (09/2024)
0.0 0.05 0.1 0.2 0.4 0.8 1.6
Entropy Regularization Scale ( Œª)0.00.10.20.30.40.50.60.7Cross-entropy Loss
0.00.20.40.60.81.0
Group Attention Entropy
Cross-entropy Loss Group Attention Entropy
Figure 10: Trade-off between task loss and group attention entropy. RelConvNet models are trained on the
‚Äúmatch pattern‚Äù task in the Relational Games benchmark varying the entropy regularization level. The overall
model loss is Lloss  ŒªLentr, where Lloss CrossEntropy py,ÀÜy qis the task loss (blue line), Lentris the entropy
regularization term for the group attention scores (orange line) as defined in Section 3.2, and Œªis a scaling
factor. Different lines correspond to different values of Œª. WhenŒª 0(no entropy regularization) the model
fails to learn the task. A small amount of regularization is enough to guide the model to a good solution.
IncreasingŒªcauses smaller group attention entropy at convergence, approaching discrete assignments.
0 20 40 60 80 100
Epoch0.51.01.52.0Group Attention Entropy
0 20 40 60 80 100
Epoch0.00.20.40.6Cross-entropy Loss
Entropy Regularization Scale
0.0 0.05 0.1 0.2 0.4EÔ¨Äect of Entropy Regularization on Group Attention
Figure 11: Effect of group attention entropy regularization. Group attention entropy (left) and baseline
cross-entropy loss (right) of a relational convolutional network model trained on the ‚Äúmatch pattern‚Äù task
with different levels of entropy regularization. The overall model loss is Lloss  ŒªLentr, where Llossis the
task loss, Lentris the entropy regularization term, and Œªis a scaling factor. Different lines correspond to
different values of Œª. Without entropy regularization, the model fails to learn the task. With sufficient entropy
regularization, the model is able to learn the task and group attention converges towards discrete assignments.
The group attention entropy starts at log9 2.2(the entropy of a uniform distribution) and decreases over
the course of training. Expectedly, larger Œªvalues cause the entropy to decrease faster, converging towards a
smaller value. When Œªis too large, the entropy regularization overwhelms the base cross-entropy loss and
results in converging to a worse cross-entropy loss. Intuitively, one needs to strike a balance such that both
the entropy regularization and the cross-entropy loss guide the evolution of the group attention map.
21Published in Transactions on Machine Learning Research (09/2024)
0 20 40 60 80 100
epoch0.50.60.70.80.91.0AccuracySplit = Train
0 20 40 60 80 100
epochSplit = Validation
Model
RelConvNet RelConvNet ( dr= 1) RelConvNet (Symmetry Relations = False)
Figure 12: Exploring the effect of multi-dimensional relations and symmetric relations in RelConvNet.
RelConvNet models matching the architecture described in Table 4 are trained on the Settask. We test
two variants: 1) set the relation dimension to be dr 1(instead of dr 16), and 2) remove the symmetry
inductive bias (i.e., W1 W2in Equation (2)). We find that with dr 1(which is analogous to CoRelNet‚Äôs
single-dimensional similarity matrix), the model struggles to find good solutions. In 10 different runs with
random seeds, one run was able to find a good solution reaching an accuracy of 98.5%, whereas the other runs
were stuck below 65%. This suggests that having multi-dimensional relations yields a more robust model with
multiple different avenues for finding good solutions during the optimization process. In the case of the model
with the asymmetric relations (i.e., lacking a symmetry inductive bias), the model is able to fit the training
data, but fails to generalize. This suggests the symmetry is an important inductive bias for certain tasks.
B Hyperparameter sweep for baseline models
In order to ensure that we compare RelConvNet against the best-achievable performance by each baseline
architecture, we carry out an extensive hyperparameter sweep over combinations of architectural hyperparam-
eters and optimization hyperparameters. In particular, as seen in Appendix B.1, the baseline models severely
overfit on the Settask, fitting the training data but failing to generalize to unseen ‚Äòsets‚Äô. Hence, we explore
whether it is possible to avoid or alleviate overfitting through an appropriate choice of hyperparameters.
In Figure 13, we vary the number of layers in the baseline models to select an optimal configuration of each
architecture. We find that increased depth beyond 2 layers is generally detrimental on this task. Based on
these results, we choose the optimal number of layers as 2 for the Transformer, GCN, GIN baselines and 1 for
the GAT baseline.
In Figure 14, we vary the level of weight decay. Expectedly, larger weight decay results in decreased training
accuracy. Generally, weight decay has a small effect on validation performance (e.g., no discernable effect in
CoRelNet or CNN). For some models, some choices of weight decay result in improved validation performance.
Based on these results, we use a weight decay of 0 for CoRelNet/CNN, 0.032 for Transformer/GAT/GIN,
and 1.024 for PrediNet/GCN/LSTM.
In Figure 15, we explore the effect of the learning rate schedule, comparing a cosine decay schedule against
our default constant learning rate. For most models, there is no significant difference, with a constant learning
rate sometimes slightly better. On the GAT model, however, the cosine learning rate schedule results in
significantly improved performance. Based on these results, we use a cosine learning rate schedule for GAT
and a constant learning rate for all other models.
22Published in Transactions on Machine Learning Research (09/2024)
2 4 6 8
Number of Layers0.50.60.70.80.91.0Training Accuracy
2 4 6 8
Number of Layers0.500.550.600.65Validation Accuracy
Model
Transformer
GCN
GAT
GIN
Figure 13: Hyperparameter sweep over number of layers in baseline architectures. Transformers and GNNs
(e.g., GCN, GAT, GIN) are ‚Äúcompositional‚Äù deep learning architectures. Here, we explore the effect of depth
(i.e., number of layers) on task performance for these baselines. The plots show the maximum training
and validation accuracy reached throughout training for each depth (5 trials with different random seeds).
Generally, we find that generalization performance drops with increasing depth. The optimal depth for the
Transformer, GCN, and GIN models is 2 layers, and the optimal depth for the GAT model is 1-layer. The
performance drop with depth can perhaps be attributed to increased difficulty of training and overfitting
due to limited data and a lack of relational inductive biases. The AdamW optimizer is used with a constant
learning rate of 103.
10‚àí210‚àí1100101
Weight Decay0.50.60.70.80.91.0Training Accuracy
10‚àí210‚àí1100101
Weight Decay0.500.550.600.650.70Validation Accuracy
Model
CoRelNet
PrediNet
Transformer
GCN
GAT
GIN
LSTM
CNN
Figure 14: Hyperparameter sweep of weight decay in baseline architectures. To alleviate possible overfitting,
we optimally tune a weight decay parameter for each model independently. We test weight decay values
including 0and0.004 2i,i P t0,...,10 u. We use the AdamW optimizer. The default weight decay in Tensorflow
is0.004.
23Published in Transactions on Machine Learning Research (09/2024)
0 20 40 60 80 100
epoch0.50.60.70.80.91.0Training Accuracy
0 20 40 60 80 100
epoch0.500.550.600.65Validation AccuracyModel
CoRelNet
PrediNet
Transformer
GCN
GAT
GIN
LSTM
CNN
LR Sched
Constant
Cosine
Figure 15: Hyperparameter sweep over learning rate schedule (constant vs cosine decay). We explore the
effect of the learning rate schedule on model performance, comparing a constant learning rate against a cosine
decay schedule. For most models, there is no significant difference, with a constant learning rate sometimes
slightly better. On the GAT model, however, the cosine learning rate schedule results in significantly improved
performance.
B.1 Results without hyperparameter tuning
Figure 16 and Table 6 show the results of the Setexperiment with a common default optimizer, without
individual hyperparameter tuning.
Accuracy
Model
RelConvNet 0.979 0.006
CoRelNet 0.563 0.001
PrediNet 0.508 0.002
Transformer 0.584 0.004
GCN 0.595 0.003
GAT 0.517 0.015
GIN 0.590 0.003
LSTM 0.602 0.003
GRU 0.593 0.004
CNN 0.506 0.006
Table 6: Hold-out test accuracy on ‚Äúcontains set‚Äù task, with default optimizer hyperparameters. We report
means standard error of mean over 10 trials. These are the numbers associated with Figure 16a.
24Published in Transactions on Machine Learning Research (09/2024)
RelConvNetCoRelNet PrediNetTransformerGCN GATLSTMCNN
Model0.00.20.40.60.81.0Accuracy
(a)Hold-out test accuracy.
0 20 40 60 80 100
epoch0.50.60.70.80.91.0AccuracySplit = Train
0 20 40 60 80 100
epochSplit = Validation
Model
RelConvNet
CoRelNet
PrediNet
Transformer
GCN
GAT
LSTM
CNN
(b) Training accuracy and validation accuracy over the course of training, without hyperparameter tuning (i.e.,
Adam optimizer, no weight decay, constant learning rate of 0.001.
Figure 16: Results of ‚Äúcontains set‚Äù experiments with default optimization hyperparameters. Bar height/solid
lines indicate the mean over 10 trials and error bars/shaded regions indicate 95% bootstrap confidence
intervals.
25Published in Transactions on Machine Learning Research (09/2024)
C Discussion on use of TCN in evaluating relational architectures
In Section 4.1 the CoRelNet model of Kerg et al. (2022) was among the baselines we compared to. In that
work, the authors also evaluate their model on the relational games benchmark. A difference between their
experimental set up and ours is that they use a method called ‚Äúcontext normalization‚Äù as a preprocessing
step on the sequence of objects.
‚ÄúContext normalization‚Äù was proposed by Webb et al. (2020). The proposal is simple: Given a sequence of
objects, px1,...,xm q, and a set of context windows W1,...,WW ¬Ä t1,...,m uwhich partition the objects,
each object is normalized along each dimension with respect to the other objects in its context. That is,
pz1,...,zm q CN px1,...,xm qis computed as,
¬µpk q
j 1
|Wk|¬∏
t PWkpxtqj
œÉpk q
j d
1
|Wk|¬∏
t PWk
pxtqj ¬µpk q
j	2
 Œµ
pztqj Œ≥j
pxtqj ¬µpk q
j
œÉpk q
j
 Œ≤j,fort PWk
whereŒ≥  pŒ≥1,...,Œ≥dq,Œ≤  pŒ≤1,...,Œ≤dqare learnable gain and shift parameters for each dimension (initialized
at 1 and 0, respectively, as with batch normalization). The context windows represent logical groupings
of objects that are assumed to be known. For instance, (Webb et al., 2021; Kerg et al., 2022) consider a
‚Äúrelational match-to-sample‚Äù task where 3 pairs of objects are presented in sequence, and the task is to identify
whether the relation in the first pair is the same as the relation in the second pair or the third pair. Here,
the context windows would be the pairs of objects. In the relational games ‚Äúmatch rows pattern‚Äù task, the
context windows would be each row.
It is reported in (Webb et al., 2021; Kerg et al., 2022) that context normalization significantly accelerates
learning and improves out-of-distribution generalization. Since (Webb et al., 2021; Kerg et al., 2022) use
context normalization in their experiments, in this section we aim to explain our choice to exclude it. We
argue that context normalization is a confounder and that an evaluation of relational architectures without
such preprocessing is more informative.
To understand how context normalization works, consider first a context window of size 2, and let Œ≤ 0,Œ≥ 1.
Then, along each dimension, we have
CN px,x q  p 0,0 q,
CN px,y q  p sign px y q,sign py x qq.
In particular, what context normalization does when there are two objects is, along each dimension, output 0
if the value is the same, and 1if it is different (encoding whether it is larger or smaller). Hence, it makes the
context-normalized output independent of the original feature representation. For tasks like relational games,
where the key relation to model is same/different, this preprocessing is directly encoding this information
in a ‚Äúsymbolic‚Äù way. In particular, for two objects x1,x2, context normalized to produce z1,z2, we have
thatx1 x2if and only if xz1,z2y 0. This makes out-of-distribution generalization trivial, and does not
properly test a relational architecture‚Äôs ability to model the same/different relation.
Similarly, consider a context window of size 3. Then, along each dimension, we have,
CN px,x,x q  p 0,0,0 q,
CN px,x,y q 1?
2sign px y q,1?
2sign px y q,1?
2sign py x q
.
Again, context normalization symbolically encodes the relational pattern. For any triplet of objects, regardless
of the values they take, context normalization produces identical output in the cases above. With context
windows larger than 3, the behavior becomes more complex.
26Published in Transactions on Machine Learning Research (09/2024)
These properties of context normalization make it a confounder in the evaluation of relational architectures.
In particular, for small context windows especially, context normalization symbolically encodes the relevant
information. Experiments on relational architectures should evaluate the architectures‚Äô ability to learnthose
relations from data. Hence, we do not use context normalization in our experiments.
27Published in Transactions on Machine Learning Research (09/2024)
D Higher-order relational tasks
As noted in the discussion, the tasks considered in this paper are solvable by modeling second-order relations
at most. One of the main innovations of the relational convolutions architecture over existing relational
architectures is its compositionality and ability to model higher-order relations. An important direction
of future research is to test the architecture‚Äôs ability to model hierarchical relations of increasingly higher
order. Constructing such benchmarks is a non-trivial task which requires careful thought and consideration.
This was outside the scope of this paper, but we provide an initial discussion here which may be useful for
constructing such benchmarks in future work.
Propositional logic. Consider evaluating boolean logic formula such as,
x1 ^ ppx2 _x3q ^ pp x3 ^x4q _ px5 ^x6 ^x7qqq.
Evaluating this logical expression (in this form) requires iteratively grouping objects and computing the
relations between them. For instance, we begin by computing the relation within g1  px3,x4qand the
relation within g2  px5,x6,x7q, then we compute the relation between the groups g1andg2, etc. For a
task which involves logical reasoning of this hierarchical form, one might imagine the group attention in
RelConvNet learning the relevant groups and the relational convolution operation computing the relations
within each group. Taking inspiration from logical reasoning with such hierarchical structure may lead to
interesting benchmarks of higher-order relational representation.
Sequence modeling. In sequence modeling (e.g., language modeling), modeling the relations between
objects is usually essential. For example, syntactic and semantic relations between words are crucial to
parsing language. Higher-order relations are also important, capturing syntactic and semantic relational
features across different locations in the text and across multiple length-scales and layers of hierarchy (see
for example some relevant work in linguistics Frank et al., 2012; Rosario et al., 2002). The attention
matrix in Transformers can be thought of as implicitly representing relations between tokens. It is possible
that composing Transformer layers also learns hierarchical relations. However, as shown in this work and
previous work on relational representation, Transformers have limited efficiency in representing relations.
Thus, incorporating relational convolutions into Transformer-based sequence models may yield meaningful
improvements in the relational aspects of sequence modeling. One way to do this is by cross-attending to a
the sequence of relational objects produced by relational convolutions, each of which summarizes the relations
within a group of objects at some level of hierarchy.
Set embedding. The objective of set embedding is to map a collection of objects to a euclidean vector
which represents the important features of the objects in the set (Zaheer et al., 2017). Depending on what the
set embedding will be used for, it may need to represent a combination of object-level features and relational
information, including perhaps relations of higher order. A set embedder which incorporates relational
convolutions may be able to generate representations which summarize relations between objects at multiple
layers of hierarchy.
Visual scene understanding. In a visual scene, there are typically several objects with spatial, visual,
and semantic relations between them which are crucial for parsing the scene. The CLEVR benchmark on
visual scene understanding (Johnson et al., 2017) was used in early work on relational representation (Santoro
et al., 2017). In more complex situations, the objects in the scene may fall into natural groupings, and the
spatial, visual, and semantic relations between those groupsmay be important for parsing a scene (e.g.,
objects forming larger components with functional dependence determined by the relations between them).
Integrating relational convolutions into a visual scene understanding system may enable reasoning about
such higher-order relations.
28Published in Transactions on Machine Learning Research (09/2024)
E Geometry of representations learned by MD-IPR and Relational Convolutions
In this section, we explore and visualize the representations learned by MD-IPR and RelConv layers. In
particular, we will visualize the representations produced by the RelConvNet model trained on the Set
task described in Section 4.2. Recall that the MD-IPR layer learns encoders œï1,œà1,...,œïdr,œàdr. In this
modeldr 16,œïi œài(so that learned relations are symmetric), and each œïiis a linear transformation to
dproj 4-dimensional space. The representations learned by a selection of 6 encoders is visualized in Figure 17.
For each of the 81 possible Setcards, we apply each encoder in the MD-IPR layer, reduce to 2-dimensions
via PCA, and visualize how each encoder separates the 4 attributes: number, color, fill, and shape. Observe,
for example, that ‚ÄúEncoder 0‚Äù disentangles color and shape, ‚ÄúEncoder 2‚Äù disentangles fill, and ‚ÄúEncoder 3‚Äù
disentangles number.
Next, we visualize, we explore the geometry of learned representations of relation vectors. That is, the inner
products producing the 16-dimensional relation vector for each pair of objects. For each 81
2
pairs ofSet
cards, we compute the 16-dimensional relation vector learned by the MD-IPR layer, reduce to 2 dimensions
via PCA, and visualize how the learned relation disentangles the latent same/different relations among the
four attributes. This is shown in Figure 18. We see some separation of the underlying same/different relations
among the four attributes, even with only two dimensions out of 16.
Finally, we visualize the representations learned by the relational convolution layer. Recall that this layer
learns a set of graphlet filters f PRs s dr nfwhich form templates of relational patterns against which
groups of objects are compared. In our experiments, the filter size is s 3and the number of filters is
nf 16. Hence, for each group gof 3Setcards, the relational convolution layer produces a 16-dimensional
vector, xR rg s,f yrel PRnf, summarizing the relational structure of the group. Of the 81
3
possible triplets of
Setcards, we create a balanced sample of ‚Äúsets‚Äù and ‚Äúnon-sets‚Äù. We then compute xR rg s,f yreland reduce to 2
dimensions via PCA. Figure 9 strikingly shows that the representations learned by the relational convolution
layer very clearly separate triplets of cards which form a set from those that don‚Äôt form a set.
29Published in Transactions on Machine Learning Research (09/2024)
Encoder 0
Number
 Color
 Fill
 ShapeEncoder 1
 Encoder 2
 Encoder 3
 Encoder 4
 Encoder 5
one
two
three
red
green
purple
empty
striped
solid
diamond
oval
squiggle
Figure 17: The encoders learned in the MD-IPR layer represent the latent attributes in the Setcards, with
different encoders seemingly specializing to encode one or two attributes.
30Published in Transactions on Machine Learning Research (09/2024)
Figure 18: The relations learned by the MD-IPR layer encodes the latent relations underlying the Settask.
40
 20
 0 20 40 60 80
R[g],frel [PC1]
20
02040R[g],frel [PC2]
not set
set
Figure 19: The relational convolution layer produces representations which separates ‚Äòsets‚Äô from ‚Äònon-sets‚Äô.
31