Under review as submission to TMLR
Thompson Sampling for Non-Stationary Bandit Problems
Anonymous authors
Paper under double-blind review
Abstract
Non-stationary multi-armed bandit (MAB) problems have recently attracted extensive
attention. We focus on the abruptly changing scenario where reward distributions remain
constant for a certain period and change at unknown time steps. Although Thompson
Sampling (TS) has shown empirical success in non-stationary settings, there is currently no
regretboundanalysisforTSwithGaussianpriors. Toaddressthis, weproposetwoalgorithms,
discounted TS and sliding-window TS, designed for sub-Gaussian reward distributions. For
these algorithms, we establish an upper bound for the expected regret by bounding the
expected number of times a suboptimal arm is played. We show that the regret order of both
algorithms is ËœO(âˆšTBT), whereTis the time horizon, BTis the number of breakpoints. This
upper bound matches the lower bound for abruptly changing problems up to a logarithmic
factor. Empirical comparisons with other non-stationary bandit algorithms highlight the
competitive performance of our proposed methods.
1 Introduction
MAB is a classic sequential decision problem. At each time step, the learner selects an arm from a finite
set of arms (also known as actions) based on its past observations, and she only observes the reward of the
chosen action. The learnerâ€™s goal is to maximize its expected cumulative reward or minimize the regret
incurred during the learning process. The regret is defined as the difference between the expected reward of
the optimal arm and the expected reward achieved by the MAB algorithm.
MAB has found practical use in various scenarios, with one of the earliest applications being the diagnosis
and treatment experiments proposed by Robbins (1952). In this experiment, each patientâ€™s treatment plan
corresponds to an arm in the MAB problem, and the goal is to minimize the patientâ€™s health loss by making
optimal treatment decisions. Recently, MAB has gained wide-ranging applicability. For example, MAB
algorithms have been used in online recommendation systems to improve user experiences and increase
engagement (Li et al., 2011; Bouneffouf et al., 2012; Li et al., 2016). Similarly, MAB has been employed
in online advertising campaigns to optimize the allocation of resources and maximize the effectiveness of
ad placements (Schwartz et al., 2017). While the standard MAB model assumes fixed reward distributions,
real-world scenarios often involve changing distributions over time. For instance, in online recommendation
systems, the collected data gradually becomes outdated, and user preferences are likely to evolve (Wu et al.,
2018). This dynamic nature necessitates the development of algorithms that can adapt to these changes,
leading to the exploration of non-stationary MAB problems.
In recent years, there has been much research on non-stationary multi-armed bandit problems. These methods
can be roughly divided into two categories: they either detect changes in the reward distribution using
change-point detection algorithms (Liu et al., 2018; Cao et al., 2019; Auer et al., 2019; Chen et al., 2019;
Besson et al., 2022), or they passively reduce the effect of past observations (Garivier & Moulines, 2011;
Raj & Kalyani, 2017; Trovo et al., 2020; Baudry et al., 2021). The former method needs to make some
assumptions about the change of arms distribution to ensure the effectiveness of the change-point detection
algorithm. For instance, (Liu et al., 2018; Cao et al., 2019) require a lower bound on the amplitude of change
of each armâ€™s expected rewards. The latter requires fewer assumptions about the characteristics of the change.
They often use a sliding window or discount factor to forget past information to adapt to the change of arms
distribution.
1Under review as submission to TMLR
These methods all provide the theoretical guarantees for regret upper bounds. However, the known Thompson
sampling, have received little theoretical analysis of regret in non-stationary MAB problems, despite the fact
TS algorithms often have superior or comparable performance to frequentist algorithms in most non-stationary
scenarios. Raj & Kalyani (2017) have studied the discounted Thompson sampling with Beta priors. However,
they only derive the probability of picking a suboptimal arm for the simple case of a two-armed bandit. To
the best of our knowledge, only sliding-window Thompson sampling with Beta priors (Trovo et al., 2020)
provides the regret upper bounds. However, their proof is incorrect with a wrong application of a well-known
result (Lemma A.3). We analyze their mistakes in detail and provide a counterexample in Appendix C.
There are two main challenges in analyzing Thompson sampling algorithm in non-stationary setting. The
first challenge is that the DS-TS algorithm cannot fully forget previous information and the second is
theunder-estimation of the optimal arm . In non-stationary setting, solving these problems is highly
challenging due to the changing reward distribution. We define a UCB-like function serving as the upper
confidence bound to tackle the first challenge, as detailed in Lemma 5.1 and Lemma 5.2. Along with the
defined function, we employ a new regret decomposition to bound the regret comes from the under-estimation
of the optimal arm, as presented in the proof of Lemma 5.3. We provide details about these challenges and
their solutions in the theoretical analysis section (Section 5).
Our contributions are as follows: we propose discounted TS (DS-TS) and sliding-window TS (SW-TS) with
Gaussian priors for abruptly changing settings. We adopt a unified method to analyze the regret upper
bound for both algorithms. The theoretical analysis results show that their regret upper bounds are of
order ËœO(âˆšTBT), whereTis the number of time steps, BTis the number of breakpoints. This regret bound
matches the â„¦(âˆš
T)lower bound proven by Garivier & Moulines (2011) in an order sense. We also verify
the algorithms in various environmental settings with Gaussian and Bernoulli rewards, and both DS-TS and
SW-TS achieve competitive performance.
2 Related Works
Many works are based on the idea of forgetting past observations. Discounted UCB (DS-UCB) (Kocsis &
SzepesvÃ¡ri, 2006; Garivier & Moulines, 2011) uses a discounted factor to average the past rewards. In order to
achieve the purpose of forgetting information, the weight of the early reward is smaller. Garivier & Moulines
(2011) also propose the sliding-window UCB (SW-UCB) by only using a few recent rewards to compute the
UCB index. They calculate the regret upper bound for DS-UCB and SW-UCB as ËœO(âˆšTBT). EXP3.S, as
proposed in (Auer et al., 2002), has been shown to achieve the regret upper bound by ËœO(âˆšTBT). Under the
assumption that the total variation of the expected rewards over the time horizon is bounded by a budget
VT, Besbes et al. (2014) introduce REXP3 with regret ËœO(T2/3). Combes & Proutiere (2014) propose the
SW-OSUB algorithm, specifically for the case of smoothly changing with an upper bound of ËœO(Ïƒ1/4T), where
Ïƒis the Lipschitz constant of the evolve process. Raj & Kalyani (2017) propose the discounted Thompson
sampling for Bernoulli priors without providing the regret upper bound. They only calculate the probability
of picking a sub-optimal arm for the simple case of a two-armed bandit. Trovo et al. (2020) propose the
sliding-window Thompson sampling algorithm with regret ËœO(T1+Î±
2)for abruptly changing settings and ËœO(TÎ²)
for smoothly changing settngs. Baudry et al. (2021) propose a novel algorithm named Sliding Window Last
Block Subsampling Duelling Algorithm (SW-LB-SDA) with regret ËœO(âˆšTBT). They only assume that the
reward distributions belong to the same one-parameter exponential family for all arms during each stationary
phase.
There are also many works that exploit techniques from the field of change detection to deal with reward
distributions varying over time. Mellor & Shapiro (2013) combine a Bayesian change point mechanism
and Thompson sampling strategy to tackle the non-stationary problem. Their algorithm can detect global
switching and per-arm switching. Liu et al. (2018) propose a change-detection framework that combines UCB
and a change-detection algorithm named CUSUM. They obtain an upper bound for the average detection
delay and a lower bound for the average time between false alarms. Cao et al. (2019) propose M-UCB, which
is similar to CUSUM but uses another simpler change-detection algorithm. M-UCB and CUSUM are nearly
optimal, their regret bounds are ËœO(âˆšTBT).
2Under review as submission to TMLR
Recently, there are also some works deriving regret bounds without knowing the number of changes. For
example, Auer et al. (2019) propose an algorithm called ADSWITCH with optimal regret bound ËœO(âˆšBTT).
Suk & Kpotufe (2022) improve the work (Auer et al., 2019) so that the obtained regret bound is smaller than
ËœO(âˆš
ST), whereSonly counts the best arms switches.
3 Problem Formulation
Assume that the non-stationary MAB problem has KarmsA:={1,2,...,K}with finite time horizon T. At
each round t, the learner must select an arm itâˆˆAand obtain the corresponding reward Xt(it). The rewards
are generated from Ïƒ-subGaussian distributions. The expectation of Xt(i)is denoted as Âµt(i) =E[Xt(i)].
A policyÏ€is a function that selects arm itto play at round t. LetÂµt(âˆ—) :=maxiâˆˆ{1,...,K}Âµt(i)denote the
expected reward of the optimal arm iâˆ—
tat roundt. Unlike the stationary MAB settings, where an arm is
optimal all of the time (i.e. âˆ€tâˆˆ{1,...,T},iâˆ—
t=iâˆ—), while in the non-stationary settings, the optimal arms
might change over time. The performance of a policy Ï€is measured in terms of cumulative expected regret:
RÏ€
T=E/bracketleftï£¬iggT/summationdisplay
t=1(Âµt(âˆ—)âˆ’Âµt(it))/bracketrightï£¬igg
, (1)
where E[Â·]is the expectation with respect to randomness of Ï€. Let âˆ†t(i) =Âµt(âˆ—)âˆ’Âµt(i)and let
kT(i) =T/summationdisplay
t=11{it=i,iÌ¸=iâˆ—
t}
denote the number of plays of arm iwhen it is not the best arm until time T. When we analyze the upper
bound ofRÏ€
T, we can directly analyze E[kT(i)]to get the upper bound of each arm.
Abruptly Changing Setting The abruptly changing setting is introduced by Garivier & Moulines (2011)
for the first time. The number of breakpoints is denoted as BT=/summationtextTâˆ’1
t=11{âˆƒiâˆˆA:Âµt(i)Ì¸=Âµt+1(i)}. Suppose
the set of breakpoints isB={b1,...,bBT}(we defineb1= 1). At each breakpoint, the reward distribution
changes for at least one arm. The rounds between two adjacent breakpoints are called stationary phase .
Abruptly changing bandits pose a more challenging problem as the learner needs to balance exploration
and exploitation within each stationary phase and during the changes between different phases. Trovo et al.
(2020) makes assumption about the number of breakpoints to facilitate more generalized analysis, while we
explicitly use BTto represent the number of breakpoints for analysis.
4 Algorithms
In this section, we propose the DS-TS and SW-TS with Gaussian priors for the non-stationary stochastic
MAB problems. Different from Agrawal & Goyal (2013), we assume that the reward distribution follows a
Ïƒ-subGaussian distribution rather than a bounded distribution. Assume that X1,...,Xnare independently
and identically distributed, following a Ïƒ-subGaussian distribution with mean Âµ. Assume further that the
prior distribution is a Gaussian distribution Âµâˆ¼N (0,Ïƒ2
0). The posterior distribution is also Gaussian
distributionN(Âµ1,Ïƒ2
1)where
Âµ1=Ïƒ2
1(0
Ïƒ2
0+/summationtextn
i=1Xi
Ïƒ2),Ïƒ2
1=1
1
Ïƒ2
0+n
Ïƒ2.
LetÏƒ0= +âˆž, we get the posterior distribution as N(1
n/summationtextn
i=1Xi,Ïƒ2
n).
4.1 DS-TS
DS-TS uses a discount factor Î³(0<Î³ < 1) to dynamically adjust the estimate of each armâ€™s distribution. The
key to our algorithm is to decrease the sampling variance of the selected arm while increasing the sampling
variance of the unselected arms.
3Under review as submission to TMLR
Algorithm 1: DS-TS
Input:discounted factor Î³,Ë†Âµ1(i) = 0,ËœÂµ1(i) = 0,Nt(Î³,i) = 0
1fort= 1,...,T do
2fori= 1,..,K do
3 sampleÎ¸t(i)âˆ¼N(Ë†Âµt(Î³,i),4Ïƒ2
Nt(Î³,i))
4end
5Pull armit= arg max iÎ¸t(i), observe reward Xt(it);
6fori= 1,...,K do
7 ËœÂµt+1(Î³,i) =Î³ËœÂµt(Î³,i) + 1{it=i}Xt(i)
8Nt+1(Î³,i) =Î³Nt(Î³,i) + 1{it=i}
9 Ë†Âµt+1(Î³,i) =ËœÂµt+1(Î³,i)
Nt+1(Î³,i)
10 end
11end
Specifically, let
Nt(Î³,i) =t/summationdisplay
j=1Î³tâˆ’j1{ij=i}
denote the discounted number of plays of arm iuntil timet. We use
Ë†Âµt(Î³,i) =1
Nt(Î³,i)t/summationdisplay
j=1Î³tâˆ’jXj(i) 1{ij=i}
called discounted empirical average to estimate the expected rewards of arm i. In non-stationary settings, we
use the discounted average and discounted number of plays instead of the true average and number of plays
respectively. Therefore, the posterior distribution is N(Ë†Âµt(Î³,i),Ïƒ2
Nt(Î³,i)).
Algorithm 2: SW-TS
Input: sliding window Ï„,Ë†Âµ1(i) = 0,ËœÂµ1(i) = 0,Nt(Ï„,i) = 0
1fort= 1,...,T do
2fori= 1,..,K do
3 sampleÎ¸t(i)âˆ¼N(Ë†Âµt(Ï„,i),4Ïƒ2
Nt(Ï„,i))
4end
5Pull armit= arg max iÎ¸t(i), observe reward Xt(it)
6fori= 1,...,K do
7Nt+1(Ï„,i) =Nt(Ï„,i) + 1{it=i}âˆ’ 1{itâˆ’Ï„=i}
8 ËœÂµt+1(Ï„,i) = ËœÂµt(Ï„,i) + 1{it=i}Xt(i)âˆ’ 1{itâˆ’Ï„=i}Xtâˆ’Ï„(i)
9 Ë†Âµt+1(Ï„,i) =ËœÂµt+1(Ï„,i)
Nt+1(Ï„,i)
10 end
11end
Algorithm 1 shows the pseudocode of DS-TS. Step 3 is the Thompson sampling. For each arm, we draw a
random sample Î¸t(i)fromN(Ë†Âµt(Î³,i),4Ïƒ2
Nt(Î³,i)). We use4Ïƒ2
Nt(Î³,i)as the posterior variance instead ofÏƒ2
Nt(Î³,i), which
helps the subsequent analysis. Then we select arm itwith the maximum sample value and obtain the reward
Xt(it)(Step 5). To avoid the time complexity going to O(T2), we introduce ËœÂµt(Î³,i) =/summationtextt
j=1Î³tâˆ’jXj(i) 1{ij=
i}to calculate Ë†Âµt(Î³,i)using an iterative method(Step 7-9).
If armiis selected at round t, the posterior distribution is updated as follows:
Ë†Âµt+1(Î³,i) =Î³Ë†Âµt(Î³,i)Nt(Î³,i) +Xt(i)
Î³Nt(Î³,i) + 1=ËœÂµt+1(Î³,i)
Nt+1(Î³,i)
4Under review as submission to TMLR
If armiisnâ€™t selected at round t, the posterior distribution is updated as
Ë†Âµt+1(Î³,i) =ËœÂµt+1(Î³,i)
Nt+1(Î³,i)=Î³ËœÂµt(Î³,i)
Î³Nt(Î³,i)= Ë†Âµt(Î³,i)
i.e. the expectation of posterior distribution remains unchanged.
4.2 SW-TS
SW-TS uses a sliding window Ï„to adapt to changes in the reward distribution. Let
Nt(Ï„,i) =t/summationdisplay
j=tâˆ’Ï„+11{ij=i},Ë†Âµt(Ï„,i) =1
Nt(Ï„,i)t/summationdisplay
j=tâˆ’Ï„+1Xj(i) 1{ij=i}.
Ift < Ï„, the range of summation is from 1tot. Similar to DS-TS, the posterior distribution is
N(Ë†Âµt(Ï„,i),4Ïƒ2
Nt(Ï„,i)). Algorithm 2 shows the pseudocode of SW-TS. To avoid the time complexity going
toO(T2), we introduce ËœÂµt(Ï„,i) =/summationtextt
j=tâˆ’Ï„+1Xj(i) 1{ij=i}to update Ë†Âµt(Ï„,i).
4.3 Results
In this section, we give the regret upper bounds of DS-TS and SW-TS. Then we discuss how to take the
values of the parameters so that these algorithms reach the optimal upper bound.
Recall that âˆ†t(i) =Âµt(âˆ—)âˆ’Âµt(i). Let âˆ†T(i) =min{âˆ†t(i) :tâ‰¤T,iÌ¸=iâˆ—
t}, be the minimum difference between
the expected reward of the best arm iâˆ—
tand the expected reward of arm iin all timeTwhen the arm iis not
the best arm. Let âˆ†T
max=max{Âµt1(i)âˆ’Âµt2(i) :t1Ì¸=t2,iâˆˆ[K]}denote the maximum expected variation of
arms.
Theorem 4.1 (DS-TS) .LetÎ³âˆˆ(0,1)satisfyingÏƒ2
âˆ†Tmax(1âˆ’Î³)2log1
1âˆ’Î³<1. For any suboptimal arm i,
E[kT(i)]â‰¤BTD(Î³) +C1(Î³)L1(Î³)Î³âˆ’1
1âˆ’Î³T(1âˆ’Î³),
where
D(Î³) =log((Ïƒ
âˆ†Tmax)2(1âˆ’Î³)2log1
1âˆ’Î³)
logÎ³,C1(Î³) =e17+ 12 + 3 log1
1âˆ’Î³,L1(Î³) =1152 log(1
1âˆ’Î³+e17)Ïƒ2
Î³1/(1âˆ’Î³)(âˆ†T(i))2.
Corollary 4.2. WhenÎ³is close to 1,Î³âˆ’1
1âˆ’Î³is arounde. If the time horizon Tand number of breakpoints
BTare known in advance, the discounted factor can be chosen as Î³= 1âˆ’1
Ïƒ/radicalï£¬ig
BT
TlogT. IfBTâ‰ªT,
Ïƒ2
âˆ†Tmax(1âˆ’Î³)2log1
1âˆ’Î³<Ïƒ/e
âˆ†Tmax/radicalï£¬igg
BT
TlogT<1.
We have
E[kT(i)] =O(/radicalbig
TBT(logT)3
2).
Theorem 4.3 (SW-TS) .LetÏ„ >0, for any suboptimal arm i,
E[kT(i)]â‰¤BTÏ„+C2(Ï„)L2(Ï„)T
Ï„,
where
C2(Ï„) =e11+ 12 + 3 log Ï„,L2(Ï„) =1152 log(Ï„+e11)Ïƒ2
(âˆ†T(i))2.
Corollary 4.4. If the time horizon Tand number of breakpoints BTare known in advance, the sliding
window can be chosen as Ï„=Ïƒ/radicalbig
T/BTlogT, then
E[kT(i)] =O(/radicalbig
TBTlogT).
5Under review as submission to TMLR
5 Proofs of Upper Bounds
Before giving the detailed proof, we discuss the main challenges in regret analysis of Thompson sampling in
non-stationary setting. These challenges are addressed by Lemmas 5.1 to 5.3.
5.1 Challenges in Regret Analysis
Existing analyses of regret bounds for Thompson sampling (Agrawal & Goyal, 2013; Jin et al., 2021; 2022)
decompose the regret into two parts. The first part of regret comes from the over-estimation of suboptimal
arm, which can be dealt with by the concentration properties of the sampling distribution and rewards
distribution. The second part is the under-estimation of the optimal arm, which mainly relies on bounding
the following equation.
T/summationdisplay
t=1E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(âˆ—)â‰¤Âµt(âˆ—)âˆ’Ïµi}], (2)
wherepi,t=P(Î¸t(âˆ—)>Âµt(âˆ—)âˆ’Ïµi)is the probability that the best arm will not be under-estimated from the
mean reward by a margin Ïµi.
The first challenge is specific to the DS-TS algorithm. Unlike SW-TS, which completely forgets previous
information after Ï„rounds following a breakpoint, DS-TS cannot fully forget past information . This
makes it challenging to utilize the concentration properties of the reward distribution to bound regret comes
from the over-estimate of the suboptimal arm. And this will further affect the analysis of Equation (2).
The second challenge is the under-estimation of the optimal arm. In stationary settings, pi,tchanges
only when the optimal arm is selected, Equation (2) can be bounded by the method proposed by Agrawal
& Goyal (2013). However, the distribution of Î¸t(âˆ—)may vary over time in non-stationary settings. It is
challenging and nontrivial to obtain a tight bound of Equation (2).
To overcome the first challenges, we adjust the posterior variance to be4Ïƒ2
Nt(Î³,i). This slightly larger variance
is specifically designed for the Ïƒ2-subGaussian distribution, which helps to bound E[1
pi,t]. Then, we define
Ut(Î³,i), which serves a role similar to the upper confidence bound in the UCB algorithm. We solve this
problem through Lemma 5.1 and Lemma 5.2.
For the second challenge, we use the new defined Ut(Î³,i)and employ a new regret decomposition for
Equation (2) based on whether the event {Nt(Î³,âˆ—)> L 1(Î³)}occurs. Intuitively, if Nt(Î³,âˆ—)> L 1(Î³),pi,t
is close to 1, which will lead to a sharp bound. If Nt(Î³,âˆ—)â‰¤L1(Î³), using Lemma A.3 we can also get the
upper bound of Equation (2). We derive the upper bound of E[1
pi,t]for non-stationary settings, with an
extra logarithmic term compared with the stationary settings. The proof of Lemma 5.3 in Appendix B.3
demonstrates these details.
5.2 Proofs of Theorem 4.1
For armiÌ¸=iâˆ—
t, we choose two threshold xt(i),yt(i)such thatxt(i) =Âµt(i) +âˆ†t(i)
3,yt(i) =Âµt(âˆ—)âˆ’âˆ†t(i)
3. Then
Âµt(i)<xt(i)<yt(i)<Âµt(âˆ—)andyt(i)âˆ’xt(i) =âˆ†t(i)
3. The historyFtis defined as the plays and rewards of
the previous tplays. Ë†Âµt(Î³,i),itand the distribution of Î¸t(i)are determined by the history Ftâˆ’1.
The abruptly changing setting is in fact piecewise-stationary. The rounds between two adjacent breakpoints
is stable stationary. Based on this observation, we define the pseudo-stationary phase as
T(Î³) ={tâ‰¤T:âˆ€sâˆˆ(tâˆ’D(Î³),t],Âµs(Â·) =Âµt(Â·)}.
LetS(Î³) ={tâ‰¤T:t /âˆˆT(Î³)}. Note that, on the right side of any breakpoint, there will be at most D(Î³)
rounds belonging to S(Î³). Therefore, the number of elements in the set S(Î³)has an upper bound BTD(Î³),
i.e.
|S(Î³)|â‰¤BTD(Î³) (3)
Figure 1 showsT(Î³)andS(Î³)in two different situations.
6Under review as submission to TMLR
ð‘ð‘–ð‘ð‘–+ð·(ð›¾) ð‘ð‘–+1ð’®(ð›¾) ð’¯(ð›¾)
ð‘ð‘–ð‘ð‘–+ð·(ð›¾) ð‘ð‘–+2ð’®(ð›¾) ð’¯(ð›¾)
ð‘ð‘–+1 ð‘ð‘–+1+ð·(ð›¾)ð·ð›¾<ð‘ð‘–+1âˆ’ð‘ð‘–
ð·ð›¾>ð‘ð‘–+1âˆ’ð‘ð‘–
Figure 1: Illustration of T(Î³)andS(Î³)in two different situations. bi,bi+1,bi+2are the breakpoints. The
situation that bi+1âˆ’bi>D(Î³)is shown in the top figure, and bi+1âˆ’biâ‰¤D(Î³)is in the bottom.
To facilitate the analysis, we define the following quantities
n= 6âˆš
2 + 3/radicalbig
1âˆ’Î³,A(Î³) =n2log(1
1âˆ’Î³)Ïƒ2
(âˆ†T(i))2,Ut(Î³,i) =Ïƒ/radicalï£¬igg
(1âˆ’Î³) log1
1âˆ’Î³
Nt(Î³,i). (4)
Now we list some useful lemmas. The detailed proofs are provided in the appendix. The following lemma
depicts that after finite rounds at the breakpoint, i.e., in the pseudo-stationary phase, the distance between
Âµt(i)and discounted average of expectation for arm ican be bounded by Ut(Î³,i).Ut(Î³,i)is analogous to
the upper confidence bound in the UCB algorithm.
Lemma 5.1. LetÂ¨Âµt(Î³,i) =1
Nt(Î³,i)/summationtextt
j=1Î³tâˆ’j1{ij=i}Âµj(i)denote the discounted average of expectation for
armiat time step t.âˆ€tâˆˆT(Î³), the distance between Âµt(i)andÂ¨Âµt(Î³,i)is less than Ut(Î³,i).
|Âµt(i)âˆ’Â¨Âµt(Î³,i)|â‰¤Ut(Î³,i), (5)
UsingLemma5.1andtheself-normalizedHoeffding-typeinequalityforsubGaussiandistributions(LemmaA.1),
we have the following lemma, which helps to bound regret comes from the over-estimation of suboptimal arm.
Lemma 5.2.âˆ€tâˆˆT(Î³),iÌ¸=iâˆ—
t,
P(Ë†Âµt(Î³,i)>xt(i),Nt(Î³,i)>A(Î³))â‰¤(1âˆ’Î³)2
The following key lemma helps bound the regret comes from the under-estimation of the optimal arm. This
is the most tricky part of analyzing TS. Note that, the proof in Trovo et al. (2020) does not prove the result
of the following lemma.
Lemma 5.3. Letpi,t=P(Î¸t(âˆ—)>yt(i)|Ftâˆ’1). For anytâˆˆT(Î³)andiÌ¸=iâˆ—
t,
/summationdisplay
tâˆˆT(Î³)E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(i)<yt(i)}]â‰¤(e17+ 9 + 3 log1
1âˆ’Î³)T(1âˆ’Î³)L1(Î³)Î³âˆ’1/(1âˆ’Î³).
Now we can give the detailed proof. The proof is in 5steps:
Step 1We can divide the rounds tâˆˆ{1,...,T}into two parts:{tâˆˆT(Î³)}and{t /âˆˆT(Î³)}. Equation (3)
shows that the number of elements in the second part is smaller than BTD(Î³), we have
E[kT(i)]â‰¤BTD(Î³) +/summationdisplay
tâˆˆT(Î³)P(it=i). (6)
7Under review as submission to TMLR
Step 2Then we consider the event {Nt(Î³,i)>A(Î³)}.
/summationdisplay
tâˆˆT(Î³)P(it=i) =/summationdisplay
tâˆˆT(Î³)P(it=i,Nt(Î³,i)<A(Î³)) +/summationdisplay
tâˆˆT(Î³)P(it=i,Nt(Î³,i)>A(Î³)).
We first bound/summationtext
tâˆˆT(Î³)P(it=i,Nt(Î³,i)<A(Î³)).
/summationdisplay
tâˆˆT(Î³)P(it=i,Nt(Î³,i)<A(Î³)) =/summationdisplay
tâˆˆT(Î³)E/bracketleftbig
P(it=i,Nt(Î³,i)<A(Î³)|Ftâˆ’1)/bracketrightbig
=/summationdisplay
tâˆˆT(Î³)E/bracketleftbig
E/bracketleftbig
1(it=i,Nt(Î³,i)<A(Î³)|Ftâˆ’1)/bracketrightbig/bracketrightbig
=/summationdisplay
tâˆˆT(Î³)E/bracketleftbig
1(it=i,Nt(Î³,i)<A(Î³))/bracketrightbig
,(7)
where the last equation uses the tower rule of expectation.
Using Lemma A.3, we have
/summationdisplay
tâˆˆT(Î³)P(it=i,Nt(Î³,i)<A(Î³))â‰¤T(1âˆ’Î³)A(Î³)Î³âˆ’1/(1âˆ’Î³)(8)
Therefore,
E[kT(i)]â‰¤T(1âˆ’Î³)A(Î³)Î³âˆ’1/(1âˆ’Î³)+BTD(Î³) +/summationdisplay
tâˆˆT(Î³)P(it=i,Nt(Î³,i)>A(Î³))(9)
Step 3 DefineEt(Î³,i)as the event{it=i,Nt(Î³,i)> A(Î³)}. DefineEÎ¸
t(i)as the event Î¸t(i)< yt(i).
Equation (9) may be decomposed as follows:
/summationdisplay
tâˆˆT(Î³)P(Et(Î³,i)) =/summationdisplay
tâˆˆT(Î³)P(Et(Î³,i),Ë†Âµt(Î³,i)>xt(i)) +/summationdisplay
tâˆˆT(Î³)P(Et(Î³,i),Ë†Âµt(Î³,i)<xt(i),EÎ¸
t(i))
+/summationdisplay
tâˆˆT(Î³)P(Et(Î³,i),Ë†Âµt(Î³,i)<xt(i),EÎ¸
t(i))(10)
Using Lemma 5.2, the first part in Equation (10) can be bounded by T(1âˆ’Î³)2.
Step 4Then we bound the second part in Equation (10). Use the fact that Nt(Î³,i)andË†Âµt(i)are determined
by the historyFtâˆ’1, we have
/summationdisplay
tâˆˆT(Î³)P(Et(Î³,i),Ë†Âµt(Î³,i)<xt(i),EÎ¸
t(i))
=E/bracketleftbigg/summationdisplay
tâˆˆT(Î³)E/bracketleftbig
1{it=i,Nt(Î³,i)>A(Î³),Ë†Âµt(Î³,i)<xt(i),EÎ¸
t(i)}|Ftâˆ’1/bracketrightbig/bracketrightbigg
=E/bracketleftbigg/summationdisplay
tâˆˆT(Î³)1{Nt(Î³,i)>A(Î³),Ë†Âµt(Î³,i)<xt(i)}P(it=i,EÎ¸
t(i)|Ftâˆ’1)/bracketrightbigg
â‰¤E/bracketleftbigg/summationdisplay
tâˆˆT(Î³)1{Nt(Î³,i)>A(Î³),Ë†Âµt(Î³,i)<xt(i)}P(Î¸t(i)>yt(i)|Ftâˆ’1)/bracketrightbigg
.(11)
Given the history Ftâˆ’1such thatNt(Î³,i)>A(Î³)andË†Âµt(Î³,i)<xt(i), we have
yt(i)âˆ’Ë†Âµt(Î³,i)>yt(i)âˆ’xt(i) =âˆ†t(i)
3â‰¥âˆ†T(i)
3.
8Under review as submission to TMLR
Therefore,
P(Î¸t(i)>yt(i)|Ftâˆ’1))â‰¤P(Î¸t(i)âˆ’Ë†Âµt(Î³,i)>âˆ†T(i)
3|Ftâˆ’1)â‰¤1
2exp(âˆ’(âˆ†T(i))2A(Î³)
72Ïƒ2)â‰¤1
2(1âˆ’Î³),(12)
where the second inequality follows Î¸t(i)âˆ¼N/parenleftbig
Ë†Âµt(Î³,i),4Ïƒ2
Nt(Î³,i)/parenrightbig
and Fact 1.
For otherFtâˆ’1, the indicator term 1{Nt(Î³,i)>A(Î³),Ë†Âµt(Î³,i)<xt(i)}will be 0. Hence, we can bound the
second part byT
2(1âˆ’Î³)
Step 5Finally, we focus the third term in Equation (10). Using Lemma A.2 and the fact that pi,tis fixed
givenFtâˆ’1,
/summationdisplay
tâˆˆT(Î³)P(Et(Î³,i),Ë†Âµt(Î³,i)<xt(i),EÎ¸
t(i))â‰¤/summationdisplay
tâˆˆT(Î³)E/bracketleftbigg1âˆ’pi,t
pi,tP(it=iâˆ—
t,EÎ¸
t(i)|Ftâˆ’1)/bracketrightbigg
=/summationdisplay
tâˆˆT(Î³)E/bracketleftbigg
E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,EÎ¸
t(i)|Ftâˆ’1}]/bracketrightbigg
=/summationdisplay
tâˆˆT(Î³)E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,EÎ¸
t(i)}]
Then by Lemma 5.3, we have
/summationdisplay
tâˆˆT(Î³)P(Et(Î³,i),Ë†Âµt(Î³,i)<xt(i),EÎ¸
t(i))â‰¤(e17+ 9 + 3 log1
1âˆ’Î³)T(1âˆ’Î³)L1(Î³)Î³âˆ’1/(1âˆ’Î³).(13)
Substituting the results in Step 3-5 to Equation (10) and Equation (9),
E[kT(i)]â‰¤T(1âˆ’Î³)A(Î³)Î³âˆ’1
1âˆ’Î³+BTD(Î³) + 2T(1âˆ’Î³) + (e17+ 9 + 3 log1
1âˆ’Î³)T(1âˆ’Î³)L1(Î³)Î³âˆ’1
1âˆ’Î³
â‰¤BTD(Î³) + (e17+ 12 + 3 log1
1âˆ’Î³)L1(Î³)Î³âˆ’1
1âˆ’Î³T(1âˆ’Î³).
5.3 Proofs of Theorem 4.3
The proof of Theorem 4.3 is similar to Theorem 4.1. The main difference is that the pseudo-stationary phase
is now defined as T(Ï„) ={tâ‰¤T:âˆ€sâˆˆ(tâˆ’Ï„,t],Âµs(Â·) =Âµt(Â·)}. Let
Â¨Âµt(Ï„,i) =1
Nt(Ï„,i)t/summationdisplay
j=tâˆ’Ï„+11{ij=i}Âµj(i).
IftâˆˆT(Ï„),
Â¨Âµt(Ï„,i) =1
Nt(Ï„,i)t/summationdisplay
j=tâˆ’Ï„+11{ij=i}Âµt(i) =Âµt(i).
This means the bias ( Ut(Î³,i)) vanishes. We no longer need an nrelated toÏ„to deal with the bias issue. We
only need to define A(Ï„)as
A(Ï„) =72 log(Ï„)Ïƒ2
(âˆ†T(i))2.
We directly list the following two lemmas, corresponding to Lemma 5.2 and Lemma 5.3, respectively. The
detailed proofs can be found in Appendix B.4 and Appendix B.5.
9Under review as submission to TMLR
Lemma 5.4.âˆ€tâˆˆT(Ï„),tÌ¸=iâˆ—
t,
P(Ë†Âµt(Ï„,i)>xt(i),Nt(Ï„,i)>A(Ï„))â‰¤1
Ï„2.
Lemma 5.5. Letpi,t=P(Î¸t(âˆ—)>yt(i)|Ftâˆ’1). For anytâˆˆT(Ï„)andiÌ¸=iâˆ—
t,
/summationdisplay
tâˆˆT(Î³)E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(i)<yt(i)}]â‰¤(e11+ 9 + 3 logÏ„)T
Ï„L2(Ï„).
LetS(Ï„) ={tâ‰¤T:t /âˆˆT(Ï„)}. Similar to Equation (3), we have |S(Ï„)|â‰¤BTÏ„. Then the proof of step 1 is
E[kT(i)]â‰¤BTÏ„+/summationdisplay
tâˆˆT(Ï„)P(it=i).
The rest of the proof is nearly identical to the proof of Theorem 4.1.
6 Experiments
In this section, we empirically compare the performance of our method with state-of-the-art algorithms on
Bernoulli and a Gaussian reward distributions. Specifically, we compare DS-TS and SW-TS with Thompson
Sampling to evaluate the improvement obtained thanks to the employment of the discounted factor Î³and
sliding window Ï„. We also compare our method with the UCB method, DS-UCB and SW-UCB (Garivier
& Moulines, 2011) to evaluate the effect of Thompson Sampling and UCB. Furthermore, we compare our
method with some novel and efficient algorithms such as CUSUM (Liu et al., 2018), M-UCB (Cao et al., 2019)
and SW-LB-SDA (Baudry et al., 2021). We measure the performance of each algorithm with the cumulative
expected regret defined in Equation Equation (1). The expected regret is averaged over 100independently
runs. The 95% confidence interval is obtained by performing 100independent runs and is depicted as a
semi-transparent region in the figure.
0 20000 40000 60000 80000 100000
Round tâˆ’10âˆ’50510Âµt(i)Arm 1
Arm 2
Arm 3
Arm 4
Arm 5
(a)
0 20000 40000 60000 80000 100000
Round t0.00.20.40.60.81.0Âµt(i)Arm 1
Arm 2
Arm 3
Arm 4
Arm 5 (b)
Figure 2:K= 5,BT= 5. Gauss arms (a), Bernoulli arms (b).
6.1 Gaussian Arms
Experimental setting for Gaussian arms We fix the time horizon as T= 100000 . The mean and variance
are drawn from distributions N(0,52)andU(1,5). For Gaussian rewards, we conduct two experiments. In
the first experiment, we split the time horizon into 5phases and use a number of arms K= 5. While in the
second experiment, we split the time horizon into 10phases and use a number of arms K= 10.
10Under review as submission to TMLR
The analysis of SW-UCB and DS-UCB is conducted under the bounded reward assumption, but the algorithms
can adapt to Gaussian scenarios. To achieve reasonable performance, it is necessary to adjust the discounted
factor and the sliding-window appropriately. We use the settings recommended in (Baudry et al., 2021),
whereÏ„= 2(1 + 2Ïƒ)/radicalbig
Tlog(T)/BTfor SW-UCB and Î³= 1âˆ’1/(4(1 + 2Ïƒ))/radicalbig
BT/Tfor DS-UCB.
Results Figure 3 illustrates the performance of these algorithms for Gaussian rewards under two different
settings. Notably, CUSUM and M-UCB are not applicable to Gaussian rewards: CUSUM is designed for
Bernoulli distributions, while M-UCB assumes bounded distributions. The discounted methods tend to
perform better than sliding-windows methods in Gaussian rewards. Among these algorithms, only our
algorithms and SW-LB-SDA provide regret analysis for unbounded rewards. Our algorithm (DS-TS) and
SW-LB-SDA have demonstrated highly competitive experimental performance.
0 20000 40000 60000 80000 100000
Round t050000100000150000200000250000300000350000Regret
SW-LB-SDA
DS-UCB
SW-UCB
TS
SW-TS
DS-TS
(a)
0 20000 40000 60000 80000 100000
Round t0100000200000300000400000500000Regret
SW-LB-SDA
DS-UCB
SW-UCB
TS
SW-TS
DS-TS (b)
Figure 3: Gaussian arms. (a) K= 5,BT= 5. (b)K= 10,BT= 10
6.2 Bernoulli Arms
Experimental setting for Bernoulli arms The time horizon is set as T= 100000 . We split the time
horizon into 5,10phases of equal length and use a number of arms K={5,10},respectively.
For Bernoulli rewards, the expected value Âµt(i)of each arm iis drawn from a uniform distribution over [0,1].
In the stationary phase, the rewards distributions remain unchanged. The Bernoulli arms for each phase are
generated as Âµt(i)âˆ¼U(0,1).Figure 2 depicts the expected rewards for Gaussian arms and Bernoulli arms
withK= 5andBT= 5.
For Bernoulli distribution, we modify the Thompson sampling (step 3) in our algorithm as Î¸t(i)âˆ¼
N(Ë†Âµt(Î³,i),1
Nt(Î³,i))andÎ¸t(i)âˆ¼ N (Ë†Âµt(Ï„,i),1
Nt(Ï„,i)). Based on Corollary 4.2 and Corollary 4.4, we set
Î³= 1âˆ’/radicalï£¬ig
BT
TlogTandÏ„=/radicalbig
T/BTlogT. To allow for fair comparison, DS-UCB uses the discount fac-
torÎ³= 1âˆ’/radicalbig
BT/T/4, SW-UCB uses the sliding window Ï„= 2/radicalbig
TlogT/BTsuggested by (Garivier &
Moulines, 2011). Based on (Baudry et al., 2021), we set Ï„= 2/radicalbig
Tlog(T)/BTfor LB-SDA. For changepoint
detection algorithm M-UCB, we set w= 800,b=/radicalbig
w/2 log(2KT2)suggested by (Cao et al., 2019). But
set the amount of exploration Î³=/radicalbig
KBTlog(T)/T. In practice, it has been found that using this value
instead of the one guaranteed in (Cao et al., 2019) will improve empirical performance (Baudry et al., 2021).
For CUSUM, following from (Liu et al., 2018), we set Î±=/radicalbig
BT/Tlog(T/BT)andh=log(T/BT). For our
experiment settings, we choose M= 50,Ïµ= 0.05.
Results Figure 4 presents the results for Bernoulli arms in abruptly changing settings. It can be observed
that our method (SW-TS) and SW-LB-SDA exhibit almost identical performance. Thompson Sampling,
designed for stationary MAB problems, shows significant oscillations at the breakpoints. The changepoint
11Under review as submission to TMLR
0 20000 40000 60000 80000 100000
Round t02500500075001000012500150001750020000Regret
SW-LB-SDA
CUSUM
DS-UCB
SW-UCB
M-UCB
SW-TS
DS-TS
TS
(a)
0 20000 40000 60000 80000 100000
Round t05000100001500020000Regret
SW-LB-SDA
CUSUM
DS-UCB
SW-UCB
M-UCB
SW-TS
DS-TS
TS (b)
Figure 4: Bernoulli arms. Settings with K= 5,BT= 5(a),K= 10,BT= 10(b)
detection algorithm CUSUM (Liu et al., 2018) also shows competitive performance. Note that, our experiment
does not satisfy the detectability assumption of CUSUM. As the number of arms and breakpoints increase,
the performance of UCB-class algorithms (DS-UCB, SW-UCB) declines, while two TS-based algorithms
(DS-TS, SW-TS) still work well.
Storage and Compute Cost These algorithms can be divided into three class: UCB, TS and SW-LB-SDA.
At each round, UCB-class and TS-class algorithms require O(K)storage and spend O(K)time complexity for
computational cost. However, for round T, SW-LB-SDA require O(K(logT)2)storage and spend O(KlogT)
time cost. Although the experimental performance of SW-LB-SDA is similar to our algorithms, our algorithm
has less storage space and lower computational complexity.
7 Conclusion
In this paper, we analyze the regret upper bound of the TS algorithm with Gaussian prior in non-stationary
settings, filling a research gap in this field. Our approach builds upon previous works while tackling two key
challenges specific to non-stationary environments: under-estimation of the optimal arm and the inability of
DS-TS algorithm to fully forget previous information. Finally, we conduct some experiments to verify theory
results. Below we discuss the results and propose directions for future research.
(1) The standard posterior update rule for Thompson Sampling has a sampling variance asÏƒ2
N. We use4Ïƒ2
N
only for ease of analysis. While this discrepancy is significant only for relatively small values of N. It would
be valuable to develop proof techniques that leverage the variance of standard Bayesian updates.
(2) Our regret upper bound includes an additional logarithmic term compared to DS-UCB and SW-UCB,
along with coefficients of e17ande11. It would be interesting to explore whether the additional logarithm
and large coefficients are intrinsic to DS-TS and SW-TS algorithms or is a limitation of our analysis.
References
Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas, graphs, and
mathematical tables , volume 55. US Government printing office, 1964.
Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Artificial
intelligence and statistics , pp. 99â€“107. PMLR, 2013.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit
problem. SIAM journal on computing , 32(1):48â€“77, 2002.
12Under review as submission to TMLR
Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an unknown
number of distribution changes. In Conference on Learning Theory , pp. 138â€“158. PMLR, 2019.
Dorian Baudry, Yoan Russac, and Olivier CappÃ©. On limited-memory subsampling strategies for bandits. In
International Conference on Machine Learning , pp. 727â€“737. PMLR, 2021.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary
rewards. Advances in neural information processing systems , 27, 2014.
Lilian Besson, Emilie Kaufmann, Odalric-Ambrym Maillard, and Julien Seznec. Efficient change-point
detection for tackling piecewise-stationary bandits. Journal of Machine Learning Research , 23(77):1â€“40,
2022.
Djallel Bouneffouf, Amel Bouzeghoub, and Alda Lopes Ganarski. A contextual-bandit algorithm for mobile
context-aware recommender system. In International conference on neural information processing , pp.
324â€“331. Springer, 2012.
Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure with change
detection for piecewise-stationary bandit. In The 22nd International Conference on Artificial Intelligence
and Statistics , pp. 418â€“427. PMLR, 2019.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextual
bandits: Efficient, optimal and parameter-free. In Conference on Learning Theory , pp. 696â€“726. PMLR,
2019.
Richard Combes and Alexandre Proutiere. Unimodal bandits: Regret lower bounds and optimal algorithms.
InInternational Conference on Machine Learning , pp. 521â€“529. PMLR, 2014.
AurÃ©lien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. In
International Conference on Algorithmic Learning Theory , pp. 174â€“188. Springer, 2011.
Tianyuan Jin, Pan Xu, Jieming Shi, Xiaokui Xiao, and Quanquan Gu. Mots: Minimax optimal thompson
sampling. In International Conference on Machine Learning , pp. 5074â€“5083. PMLR, 2021.
Tianyuan Jin, Pan Xu, Xiaokui Xiao, and Anima Anandkumar. Finite-time regret of thompson sampling
algorithms for exponential family multi-armed bandits. Advances in Neural Information Processing Systems ,
35:38475â€“38487, 2022.
Levente Kocsis and Csaba SzepesvÃ¡ri. Discounted ucb. In 2nd PASCAL Challenges Workshop , volume 2, pp.
51â€“134, 2006.
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-bandit-
based news article recommendation algorithms. In Proceedings of the fourth ACM international conference
on Web search and data mining , pp. 297â€“306, 2011.
Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. Collaborative filtering bandits. In Proceedings of the
39th International ACM SIGIR conference on Research and Development in Information Retrieval , pp.
539â€“548, 2016.
Fang Liu, Joohyun Lee, and Ness Shroff. A change-detection based framework for piecewise-stationary
multi-armed bandit problem. In Proceedings of the AAAI Conference on Artificial Intelligence , 2018.
Joseph Mellor and Jonathan Shapiro. Thompson sampling in switching environments with bayesian online
change detection. In Artificial intelligence and statistics , pp. 442â€“450. PMLR, 2013.
Vishnu Raj and Sheetal Kalyani. Taming non-stationary bandits: A bayesian approach. arXiv preprint
arXiv:1707.09727 , 2017.
Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical
Society, 58(5):527â€“535, 1952.
13Under review as submission to TMLR
Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising using
multi-armed bandit experiments. Marketing Science , 36(4):500â€“522, 2017.
Joe Suk and Samory Kpotufe. Tracking most significant arm switches in bandits. In Conference on Learning
Theory, pp. 2160â€“2182. PMLR, 2022.
Francesco Trovo, Stefano Paladino, Marcello Restelli, and Nicola Gatti. Sliding-window thompson sampling
for non-stationary settings. Journal of Artificial Intelligence Research , 68:311â€“364, 2020.
Qingyun Wu, Naveen Iyer, and Hongning Wang. Learning contextual bandits in a non-stationary environment.
InThe 41st International ACM SIGIR Conference on Research & Development in Information Retrieval ,
pp. 495â€“504, 2018.
A Facts and Lemmas
In this section, we list some well-known lemmas.
Garivier & Moulines (2011) has derived a Hoeffding-type inequality for self-normalized means with a random
number of summands. Their bound is for bounded distribution. Leveraging the properties of Ïƒ-subGaussian
distributions, we have the following bound for Ïƒ-subGaussian. Recall that
Nt(Î³,i) =t/summationdisplay
j=1Î³tâˆ’j1{ij=i},Ë†Âµt(Î³,i) =1
Nt(Î³,i)t/summationdisplay
j=1Î³tâˆ’jXj(i) 1{ij=i}
Â¨Âµt(Î³,i) =1
Nt(Î³,i)t/summationdisplay
j=1Î³tâˆ’j1{ij=i}Âµj(i)
Lemma A.1. LettâˆˆT(Î³),Î´> 0,
P(Nt(Î³,i)(Ë†Âµ(Î³,i)âˆ’Â¨Âµ(Î³,i))/radicalbig
Nt(Î³2,i)>Î´)â‰¤log(1
1âˆ’Î³) exp (âˆ’3Î´2
8Ïƒ2).
LettâˆˆT(Ï„),Î´> 0,
P(/radicalbig
Nt(Ï„,i)(Ë†Âµ(Ï„,i)âˆ’Âµt(i))>Î´)â‰¤logÏ„exp (âˆ’3Î´2
8Ïƒ2),
The following inequality is the anti-concentration and concentration bound for Gaussian distributed random
variables.
Fact 1(Abramowitz & Stegun (1964)) .For a Gaussian distributed random variable Xwith mean Âµand
varianceÏƒ2, for anya>0
1âˆš
2Ï€a
1 +a2eâˆ’a2/2â‰¤P(Xâˆ’Âµ>aÏƒ )â‰¤1
a+âˆš
a2+ 4eâˆ’a2/2
Since1
a+âˆš
a2+4â‰¤1
2, we also have the following well-known result:
P(Xâˆ’Âµ>aÏƒ )â‰¤1
2eâˆ’a2/2
The following lemma is adapted from Agrawal & Goyal (2013) and is often used in the analysis of Thompson
Sampling, which can transform the probability of selecting the ith arm into the probability of selecting the
optimal arm iâˆ—
t.
Lemma A.2. Letpi,t=P(Î¸t(âˆ—)>yt(i)|Ftâˆ’1). For anyA>0,iÌ¸=iâˆ—
t,
P(it=i,Î¸t(i)<yt(i)|Ftâˆ’1)â‰¤(1âˆ’pi,t)
pi,tP(it=iâˆ—
t,Î¸t(i)<yt(i)|Ftâˆ’1)
14Under review as submission to TMLR
Lemma A.3 (Garivier & Moulines (2011)) .For anyiâˆˆ{1,...,K},Î³âˆˆ(0,1)andA>0,
T/summationdisplay
t=11{it=i,Nt(Î³,i)<A}â‰¤âŒˆT(1âˆ’Î³)âŒ‰AÎ³âˆ’1/(1âˆ’Î³),
T/summationdisplay
t=11{it=i,Nt(Ï„,i)<A}â‰¤/ceilingleftbiggT
Ï„/ceilingrightbigg
A.
B Detailed Proofs of Lemmas and Theorems
In this section, we provide the detailed proofs of Lemma 5.1,Lemma 5.2,Lemma 5.3, Lemma 5.4 and Lemma 5.5.
The proof of Theorem 4.3 is almost identical to that of Theorem 4.1, so we have omitted the details of the
proof.
B.1 Proof of Lemma 5.1
Recall that Â¨Âµt(Î³,i) =1
Nt(Î³,i)/summationtextt
j=1Î³tâˆ’j1{ij=i}Âµj(i). Since Â¨Âµt(Î³,i)is a convex combination of elements
Âµj(i),j= 1,...,t, we have
|Âµt(i)âˆ’Â¨Âµt(Î³,i)|â‰¤âˆ†T
max (14)
We can write Âµt(i)asÂµt(i) =1
Nt(Î³,i)/summationtextt
j=1Î³tâˆ’j1{ij=i}Âµt(i). Thus, we have
|Âµt(i)âˆ’Â¨Âµt(Î³,i)|=1
Nt(Î³,i)|t/summationdisplay
j=1Î³tâˆ’j(Âµj(i)âˆ’Âµt(i)) 1{ij=i}|.
Recall thatT(Î³) ={tâ‰¤T:âˆ€sâˆˆ(tâˆ’D(Î³),t],Âµs(Â·) =Âµt(Â·)}. IftâˆˆT(Î³), we haveÂµj(i) =Âµt(i),âˆ€jâˆˆ
(tâˆ’D(Î³),t).
Therefore,âˆ€tâˆˆT(Î³), we have
|Âµt(i)âˆ’Â¨Âµt(Î³,i)|=1
Nt(Î³,i)|tâˆ’D(Î³)/summationdisplay
j=1Î³tâˆ’j(Âµj(i)âˆ’Âµt(i)) 1{ij=i}|
â‰¤âˆ†T
max
Nt(Î³,i)tâˆ’D(Î³)/summationdisplay
j=1Î³tâˆ’j1{ij=i}
=âˆ†T
max
Nt(Î³,i)Î³D(Î³)Ntâˆ’D(Î³)(Î³,i)
â‰¤âˆ†T
maxÎ³D(Î³)
Nt(Î³,i)(1âˆ’Î³)
where the last inequality follows from Ntâˆ’D(Î³)(Î³,i)â‰¤1
1âˆ’Î³.
IfÎ³D(Î³)
Nt(Î³,i)(1âˆ’Î³)<1,Î³D(Î³)
Nt(Î³,i)(1âˆ’Î³)</radicalï£¬ig
Î³D(Î³)
Nt(Î³,i)(1âˆ’Î³), we have
|Âµt(i)âˆ’Â¨Âµt(Î³,i)|â‰¤âˆ†T
max/radicalï£¬igg
Î³D(Î³)
Nt(Î³,i)(1âˆ’Î³).
IfÎ³D(Î³)
Nt(Î³,i)(1âˆ’Î³)â‰¥1, from Equation (14), we also have
|Âµt(i)âˆ’Â¨Âµt(Î³,i)|â‰¤âˆ†T
maxâ‰¤âˆ†T
max/radicalï£¬igg
Î³D(Î³)
Nt(Î³,i)(1âˆ’Î³).
15Under review as submission to TMLR
By the definition of D(Î³) =log((Ïƒ
âˆ†Tmax)2(1âˆ’Î³)2log1
1âˆ’Î³)
logÎ³,
|Âµt(i)âˆ’Â¨Âµt(Î³,i)|â‰¤Ïƒ/radicalï£¬igg
(1âˆ’Î³) log1
1âˆ’Î³
Nt(Î³,i)
B.2 Proof of Lemma 5.2
From the definition of n,A(Î³),Ut(Î³,i)in Equation (4), we can get
Ut(Î³,i) =âˆš1âˆ’Î³âˆ†T(i)
n/radicalï£¬igg
A(Î³)
Nt(Î³,i). (15)
IfNt(Î³,i)>A(Î³),Ut(Î³,i)<âˆš1âˆ’Î³
nâˆ†T(i). Thus, we have
âˆ†t(i)
3âˆ’Ut(Î³,i)>âˆ†T(i)
3âˆ’âˆš1âˆ’Î³
nâˆ†T(i) =2âˆš
2
nâˆ†T(i). (16)
Therefore,
P(Ë†Âµt(Î³,i)>Âµt(i) +âˆ†t(i)
3,Nt(Î³,i)>A(Î³))
(a)
â‰¤P(Ë†Âµt(Î³,i)âˆ’Â¨Âµt(Î³,i)>âˆ†t(i)
3âˆ’Ut(Î³,i),Nt(Î³,i)>A(Î³))
(b)
â‰¤P(Ë†Âµt(Î³,i)âˆ’Â¨Âµt(Î³,i)>2âˆš
2
nâˆ†T(i),Nt(Î³,i)>A(Î³))
(c)
â‰¤P(Nt(Î³,i)(Ë†Âµt(Î³,i)âˆ’Â¨Âµt(Î³,i))/radicalbig
Nt(Î³2,i)>2âˆš
2
nâˆ†T(i)/radicalbig
A(Î³))
(d)
â‰¤log1
1âˆ’Î³exp(âˆ’3(âˆ†T(i))2
n2Ïƒ2A(Î³))
â‰¤(1âˆ’Î³)3log1
1âˆ’Î³(17)
where (a) uses Lemma 5.1, (b) uses Equation (16), (c) follows from Nt(Î³,i)>Nt(Î³2,i), (d) uses Lemma A.1.
Since (1âˆ’Î³) log1
1âˆ’Î³â‰¤1
e<1, this ends the proof.
B.3 Proof of Lemma 5.3
This proof is adapted from Agrawal & Goyal (2013) for the stationary settings. However, there are some
technical problems that are difficult to overcome in non-stationary settings. The tricky problem is to lower
bound the probability of the meanâ€™s estimation of optimal arm Equation (21). By designing the function
Ut(Î³,i)and decomposing the regret to use Lemma A.3 again, we solve this challenge. We use blue font to
emphasize the techniques used in the proof.
The proof is in 3steps.
Step 1We first prove that E[1
pi,t]has an upper bound independent of t.
Define a Bernoulli experiment as sampling from N(Ë†Âµt(âˆ—),4Ïƒ2
Nt(Î³,âˆ—)), where success implies that Î¸t(âˆ—)>yt(i).
LetGtdenote the number of experiments performed when the event {Î¸t(âˆ—)>yt(i)}first occurs. Then
E[1
pi,t] =E[E[Gt|Ftâˆ’1]] =E[Gt]
16Under review as submission to TMLR
Letz=âˆšlogr+1
2(râ‰¥1is an integer ) and let MAXrdenote the maximum of rindependent Bernoulli
experiment. Then
P(Gtâ‰¤r)â‰¥P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)â‰¥yt(i))
=E[E[ 1{MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)â‰¥yt(i)}|Ftâˆ’1]]
=E[ 1{Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)â‰¥yt(i)}P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)|Ftâˆ’1)](18)
Using Fact 1,
P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)|Ftâˆ’1)â‰¥1âˆ’(1âˆ’1âˆš
2Ï€z
z2+ 1eâˆ’z2/2)r
= 1âˆ’(1âˆ’1âˆš
2Ï€âˆšlogr+1
2
(âˆšlogr+1
2)2+ 1eâˆ’1/4âˆ’âˆš
logr/2
âˆšr)r
â‰¥1âˆ’eâˆ’âˆšreâˆ’âˆš
logr/2
e0.25âˆš
2Ï€(âˆš
logr+1)(19)
For anyrâ‰¥e17,eâˆ’âˆšreâˆ’âˆš
logr/2
e0.25âˆš
2Ï€(âˆš
logr+1)â‰¤1
r2. Hence, for any râ‰¥e17,
P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)|Ftâˆ’1)â‰¥1âˆ’1
r2.
Therefore, for any râ‰¥e17,
P(Gtâ‰¤r)â‰¥(1âˆ’1
r2)P(Ë†Âµt(âˆ—) +z/radicalbig
Nt(Î³,âˆ—)â‰¥yt(i))
Next, we apply Lemma A.1 to lower bound P(Ë†Âµt(âˆ—) +zÂ·2Ïƒâˆš
Nt(Î³,âˆ—)â‰¥yt(i)).
P(Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)â‰¥yt(i))â‰¥1âˆ’P(Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)â‰¤Âµt(âˆ—))
â‰¥1âˆ’P(Ë†Âµt(âˆ—)âˆ’Â¨Âµt(âˆ—)â‰¤Ut(Î³,âˆ—)âˆ’zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—))(20)
SinceUt(Î³,âˆ—) =Ïƒ/radicalbig
(1âˆ’Î³) log1
1âˆ’Î³âˆš
Nt(Î³,âˆ—),z= logr+1
2,
Ut(Î³,âˆ—)âˆ’zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)=Ïƒ/radicalï£¬ig
(1âˆ’Î³) log1
1âˆ’Î³âˆ’Ïƒâˆ’2Ïƒâˆšlogr
/radicalbig
Nt(Î³,âˆ—)<âˆ’2Ïƒâˆšlogr/radicalbig
Nt(Î³,âˆ—).
Then we have
P(Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)â‰¥yt(i))â‰¥1âˆ’P(Ë†Âµt(âˆ—)âˆ’Â¨Âµt(âˆ—)<âˆ’2Ïƒâˆšlogr/radicalbig
Nt(Î³,âˆ—))
â‰¥1âˆ’log(1
1âˆ’Î³)eâˆ’3
2logr
â‰¥1âˆ’log1
1âˆ’Î³1
r1.5.(21)
Substituting, for any r>e17,
P(Gtâ‰¤r)â‰¥1âˆ’log1
1âˆ’Î³1
r1.5âˆ’1
r2(22)
17Under review as submission to TMLR
Therefore,
E[Gt] =âˆž/summationdisplay
r=0P(Gtâ‰¥r)
â‰¤1 +e17+/summationdisplay
r>e17(log1
1âˆ’Î³1
r1.5+1
r2)
â‰¤e17+ 3 + 3 log1
1âˆ’Î³
This proves a bound of E[1
pi,t]â‰¤e17+ 3 + 3 log1
1âˆ’Î³independent of t.
Step 2. DefineL(Î³) =1152 log(1
1âˆ’Î³+e17)Ïƒ2
(âˆ†T(i))2. We consider the upper bound of E[1
pi,t]whenNt(Î³,âˆ—)>L(Î³).
P(Gtâ‰¤r)â‰¥P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)âˆ’âˆ†t(i)
6â‰¥yt(i))
=E[ 1{Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)âˆ’âˆ†t(i)
6â‰¥yt(i)}P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)âˆ’âˆ†t(i)
6|Ftâˆ’1)](23)
Now, since Nt(Î³,âˆ—)>L(Î³),1âˆš
Nt(Î³,âˆ—)<âˆ†t(i)
48/radicalbig
log(1
1âˆ’Î³+e17)Ïƒ. Therefore, for any râ‰¤(1
1âˆ’Î³+e17)2,
zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)âˆ’âˆ†t(i)
6=2Ïƒâˆšlogr+Ïƒ/radicalbig
Nt(Î³,âˆ—)âˆ’âˆ†t(i)
6â‰¤âˆ’âˆ†t(i)
12.
Using Fact 1,
P(Î¸t(i)>Ë†Âµt(i)âˆ’âˆ†t(i)
12|Ftâˆ’1)â‰¤1âˆ’1
2eâˆ’Nt(Î³,âˆ—)
4Ïƒ2âˆ†t(i)2
288â‰¥1âˆ’1
2(1/(1âˆ’Î³) +e17).
This implies
P(MAXr>Ë†Âµt(âˆ—) +z/radicalbig
Nt(Î³,âˆ—)âˆ’âˆ†t(i)
6|Ftâˆ’1)â‰¥1âˆ’1
2r(1/(1âˆ’Î³) +e17)r.
Also, apply the self-normalized Hoeffding-type inequality,
P(Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Î³,âˆ—)âˆ’âˆ†t(i)
6â‰¥yt(i))â‰¥1âˆ’P(Ë†Âµt(âˆ—)â‰¤Âµt(âˆ—)âˆ’âˆ†t(i)
6)
â‰¥1âˆ’P(Ë†Âµt(âˆ—)âˆ’Â¨Âµt(âˆ—)â‰¥âˆ’Ut(Î³,âˆ—) +âˆ†t(i)
6)
>1âˆ’P(Ë†Âµt(âˆ—)âˆ’Â¨Âµt(âˆ—)â‰¥âˆ†T(i)
8/radicalï£¬igg
L(Î³)
Nt(Î³,âˆ—))
â‰¥1âˆ’log(1
1âˆ’Î³+e17)1
(1/(1âˆ’Î³) +e17)3.
LetÎ³â€²= (1
1âˆ’Î³+e17)2. Therefore,for any 1â‰¤râ‰¤Î³â€²,
P(Gtâ‰¤r)â‰¥1âˆ’1
2rÎ³â€²r/2âˆ’log(1
1âˆ’Î³+e17)1
Î³â€²1.5.
Whenrâ‰¥Î³â€²>e17, we can use Equation (22) to obtain,
P(Gtâ‰¤r)â‰¥1âˆ’log1
1âˆ’Î³1
r1.5âˆ’1
r2
18Under review as submission to TMLR
Combining these results,
E[Gt]â‰¤âˆž/summationdisplay
r=0P(Gtâ‰¥r)
â‰¤1 +Î³â€²/summationdisplay
r=1P(Gtâ‰¥r) +âˆž/summationdisplay
r=Î³â€²P(Gtâ‰¥r)
â‰¤1 +Î³â€²/summationdisplay
r=1(1
2rÎ³â€²r/2+ log(1
1âˆ’Î³+e17)1
Î³â€²1.5) +âˆž/summationdisplay
r=Î³â€²(log1
1âˆ’Î³1
r1.5+1
r2)
â‰¤1 +1
2âˆšÎ³â€²+ log(1
1âˆ’Î³+e17)1âˆšÎ³â€²+2
Î³â€²+ log(1
1âˆ’Î³)3âˆšÎ³â€²
â‰¤1 + 6(1âˆ’Î³) log(1
1âˆ’Î³+e17).
Therefore, when Nt(Î³,âˆ—)>L(Î³), it holds that
E[1
pi,t]âˆ’1 =E[Gt]âˆ’1â‰¤6(1âˆ’Î³) log(1
1âˆ’Î³+e17).
Step 3 LetA(Î³,âˆ—) ={tâˆˆ{1,...,T}:Nt(Î³,âˆ—)â‰¤L(Î³)}.
/summationdisplay
tâˆˆT(Î³)E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(i)<yt(i)}]
â‰¤/parenleftbigg/summationdisplay
tâˆˆT(Î³)âˆ©A(Î³,âˆ—)+/summationdisplay
tâˆˆT(Î³)\A(Î³,âˆ—)/parenrightbigg
E/bracketleftbigg1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(i)<yt(i)}/bracketrightbigg
â‰¤/vextendsingle/vextendsingle{t:it=iâˆ—
t,Nt(Î³,âˆ—)â‰¤L(Î³)}/vextendsingle/vextendsingle(e17+ 3 + 3 log1
1âˆ’Î³) +/summationdisplay
tâˆˆT(Î³)\A(Î³,âˆ—)E/bracketleftbigg1âˆ’pi,t
pi,t/bracketrightbigg
â‰¤T(1âˆ’Î³)L(Î³)Î³âˆ’1/(1âˆ’Î³)(e17+ 3 + 3 log1
1âˆ’Î³) + 6T(1âˆ’Î³) log(1
1âˆ’Î³+e17)
â‰¤(e17+ 9 + 3 log1
1âˆ’Î³)T(1âˆ’Î³)L(Î³)Î³âˆ’1/(1âˆ’Î³).(24)
Lemma A.1 has a stricter upper bound aslog1
1âˆ’Î³
log(1+Î·)exp(âˆ’1
2Ïƒ2(1âˆ’Î·2
16)). Suppose the variance of Thompson
sampling isÎ¾Ïƒ2
Nt(Î³,i). The lower bound of Equation (21) becomes
log1
1âˆ’Î³
log(1 +Î·)exp(âˆ’Î¾logr
2(1âˆ’Î·2
16)).
To ensure that E[Gt]has a finite upper bound, our analysis method requires Î¾>2, i.e. the sampling variance
needs to be strictly greater than2Ïƒ2
Nt(Î³,i).
19Under review as submission to TMLR
B.4 Proof of Lemma 5.4
Recall that A(Ï„) =72 log(Ï„)Ïƒ2
(âˆ†T(i))2. Using Lemma A.1, we have
P(Ë†Âµt(Ï„,i)>xt(i),Nt(Ï„,i)>A(Ï„)) =P(Ë†Âµt(Ï„,i)âˆ’Âµt(i)>âˆ†t(i)
3,Nt(Ï„,i)>A(Î³))
â‰¤P(/radicalbig
Nt(Ï„,i)(Ë†Âµt(Ï„,i)âˆ’Âµt(i))>âˆ†T(i)
3/radicalbig
A(Î³))
â‰¤logÏ„exp(âˆ’3(âˆ†T(i))2
72Ïƒ2A(Î³))
â‰¤1
Ï„2(25)
B.5 Proof of Lemma 5.5
The proof is similar to the proof of Lemma 5.3.
Step 1We first prove that E[1
pi,t]has an upper bound independent of t.
Letz=âˆšlogr(râ‰¥1is an integer ). Then
P(Gtâ‰¤r)â‰¥P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)â‰¥yt(i))
=E[ 1{Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)â‰¥yt(i)}P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)|Ftâˆ’1)](26)
Using Fact 1,
P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)|Ftâˆ’1)â‰¥1âˆ’(1âˆ’1âˆš
2Ï€z
z2+ 1eâˆ’z2/2)r
= 1âˆ’(1âˆ’1âˆš
2Ï€âˆšlogr
(âˆšlogr)2+ 11âˆšr)r
â‰¥1âˆ’eâˆ’âˆšrâˆš
2Ï€(âˆš
logr+1)(27)
For anyrâ‰¥e11,eâˆ’âˆšrâˆš
2Ï€(âˆš
logr+1)â‰¤1
r2. Hence, for any râ‰¥e11,
P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)|Ftâˆ’1)â‰¥1âˆ’1
r2.
Therefore, for any râ‰¥e11,
P(Gtâ‰¤r)â‰¥(1âˆ’1
r2)P(Ë†Âµt(âˆ—) +z/radicalbig
Nt(Ï„,âˆ—)â‰¥yt(i))
Next, we apply Lemma A.1 to lower bound P(Ë†Âµt(âˆ—) +zÂ·2Ïƒâˆš
Nt(Ï„,âˆ—)â‰¥yt(i)).
P(Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)â‰¥yt(i))â‰¥1âˆ’P(Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)â‰¤Âµt(âˆ—))
â‰¥1âˆ’P(Ë†Âµt(âˆ—)âˆ’Âµt(âˆ—)<âˆ’2Ïƒâˆšlogr/radicalbig
Nt(Ï„,âˆ—))
â‰¥1âˆ’logÏ„eâˆ’3
2logr
= 1âˆ’logÏ„1
r1.5.
20Under review as submission to TMLR
Substituting, for any r>e11,
P(Gtâ‰¤r)â‰¥1âˆ’logÏ„1
r1.5âˆ’1
r2(28)
Therefore,
E[Gt] =âˆž/summationdisplay
r=0P(Gtâ‰¥r)
â‰¤1 +e11+/summationdisplay
r>e11(logÏ„1
r1.5+1
r2)
â‰¤e11+ 3 + 3 logÏ„
This proves a bound of E[1
pi,t]â‰¤e11+ 3 + 3 logÏ„independent of t.
Step 2. DefineL(Ï„) =1152 log(Ï„+e11)Ïƒ2
(âˆ†T(i))2. We consider the upper bound of E[1
pi,t]whenNt(Ï„,âˆ—)>L(Ï„).
P(Gtâ‰¤r)â‰¥P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)âˆ’âˆ†t(i)
6â‰¥yt(i))
=E[ 1{Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)âˆ’âˆ†t(i)
6â‰¥yt(i)}P(MAXr>Ë†Âµt(âˆ—) +zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)âˆ’âˆ†t(i)
6|Ftâˆ’1)](29)
Now, since Nt(Ï„,âˆ—)>L(Ï„),1âˆš
Nt(Ï„,âˆ—)<âˆ†t(i)
48âˆš
log(Ï„+e11)Ïƒ. Therefore, for any râ‰¤(Ï„+e11)2,
zÂ·2Ïƒ/radicalbig
Nt(Ï„,âˆ—)âˆ’âˆ†t(i)
6=2Ïƒâˆšlogr+Ïƒ/radicalbig
Nt(Ï„,âˆ—)âˆ’âˆ†t(i)
6â‰¤âˆ’âˆ†t(i)
12.
Using Fact 1,
P(Î¸t(i)>Ë†Âµt(i)âˆ’âˆ†t(i)
12|Ftâˆ’1)â‰¤1âˆ’1
2eâˆ’Nt(Ï„,âˆ—)
4Ïƒ2âˆ†t(i)2
288â‰¥1âˆ’1
2(Ï„+e11).
This implies
P(MAXr>Ë†Âµt(âˆ—) +z/radicalbig
Nt(Ï„,âˆ—)âˆ’âˆ†t(i)
6|Ftâˆ’1)â‰¥1âˆ’1
2r(Ï„+e11)r.
Also, apply Lemma A.1,
P(Ë†Âµt(âˆ—) +z/radicalbig
Nt(Ï„,âˆ—)âˆ’âˆ†t(i)
6â‰¥yt(i))â‰¥1âˆ’P(Ë†Âµt(âˆ—)âˆ’Âµt(âˆ—)â‰¥âˆ†t(i)
6)
â‰¥1âˆ’log(Ï„+e11)1
(Ï„+e11)3.
LetÏ„â€²= (Ï„+e11)2. Therefore, for any 1â‰¤râ‰¤Ï„â€²,
P(Gtâ‰¤r)â‰¥1âˆ’1
2rÏ„â€²r/2âˆ’log(Ï„+e11)1
Ï„â€²1.5.
Whenrâ‰¥Ï„â€²>e11, we can use Equation (22) to obtain,
P(Gtâ‰¤r)â‰¥1âˆ’logÏ„1
r1.5âˆ’1
r2
Combining these results,
E[Gt]â‰¤âˆž/summationdisplay
r=0P(Gtâ‰¥r)
â‰¤1 +Ï„â€²/summationdisplay
r=1P(Gtâ‰¥r) +âˆž/summationdisplay
r=Ï„â€²P(Gtâ‰¥r)
â‰¤1 +6
Ï„log(Ï„+e11).
21Under review as submission to TMLR
Therefore, when Nt(Ï„,âˆ—)>L(Ï„), it holds that
E[1
pi,t]âˆ’1 =E[Gt]âˆ’1â‰¤6
Ï„log(Ï„+e11).
Step 3 LetA(Ï„,âˆ—) ={tâˆˆ{1,...,T}:Nt(Ï„,âˆ—)â‰¤L(Ï„)}andC=e11+ 9.
/summationdisplay
tâˆˆT(Ï„)E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(i)<yt(i)}]
â‰¤/summationdisplay
tâˆˆT(Ï„)âˆ©A(Ï„,âˆ—)E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(i)<yt(i)}] +/summationdisplay
tâˆˆT(Ï„)\A(Ï„,âˆ—)E[1âˆ’pi,t
pi,t1{it=iâˆ—
t,Î¸t(i)<yt(i)}]
â‰¤|{tâˆˆ{1,...,T}:it=iâˆ—
t,Nt(Ï„,âˆ—)â‰¤L(Ï„)}|(e11+ 3 + 3 logÏ„) +/summationdisplay
tâˆˆT(Ï„)\A(Ï„,âˆ—)E[1âˆ’pi,t
pi,t]
â‰¤T
Ï„L(Ï„)(e11+ 3 + 3 logÏ„) + 6T
Ï„log(Ï„+e11)
â‰¤(e11+ 9 + 3 logÏ„)T
Ï„L(Ï„).(30)
C Incorrect Proof of SW-TS with Beta Priors
Here, we discuss the mistakes in proof of Trovo et al. (2020). It is precisely because of these errors that they
bypassed the analysis of under-estimation of the optimal arm ( i.e. Lemma 5.3).
We first define the same notions in Trovo et al. (2020).
LetFâ€²
Ï•:={t:bÏ•âˆ’1+Ï„â‰¤t<bÏ•},bÏ•is theÏ•-th breakpoints. Ti(Fâ€²
Ï•) :=/summationtext
tâˆˆFâ€²
Ï•1{it=i,iÌ¸=iâˆ—
Ï•}denote the
number of times a suboptimal arm is played during phase Fâ€²
Ï•.
Ti,t,r:=/summationtextt
s=max{tâˆ’Ï„+1,1}1{is=i}.Ï‘i,tis the result of Thompson sampling from the Beta distribution.
Then we cite the same equations in Trovo et al. (2020). Use Lemma A.3, they also have the following result:
/summationdisplay
tâˆˆFâ€²
Ï•E/bracketleftbig
1{it=i,Ti,t,Ï„â‰¤Â¯nA}/bracketrightbig
â‰¤Â¯nANÏ•
Ï„, (31)
whereFâ€²
Ï•â‰¤NÏ•. Thus by choosing Â¯nA=/ceilingleftï£¬ig
19
logÏ„/ceilingrightï£¬ig
, we have:
RA=/summationdisplay
tâˆˆFâ€²
Ï•P/parenleftbigg
Ï‘iâˆ—
Ï•,tâ‰¤Âµiâˆ—
Ï•,tâˆ’/radicalï£¬igg
5 logÏ„
Tiâˆ—
Ï•,t,Ï„/parenrightbigg
(32)
â‰¤/summationdisplay
tâˆˆFâ€²
Ï•P/parenleftbigg
Ï‘iâˆ—
Ï•,tâ‰¤Âµiâˆ—
Ï•,tâˆ’/radicalï£¬igg
5 logÏ„
Tiâˆ—
Ï•,t,Ï„,Tiâˆ—
Ï•,t,Ï„>Â¯nA/parenrightbigg
+/summationdisplay
tâˆˆFâ€²
Ï•P/parenleftbig
Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nA/parenrightbig
(33)
â‰¤/summationdisplay
tâˆˆFâ€²
Ï•P/parenleftbigg
Ï‘iâˆ—
Ï•,tâ‰¤Âµiâˆ—
Ï•,tâˆ’/radicalï£¬igg
5 logÏ„
Tiâˆ—
Ï•,t,Ï„,Tiâˆ—
Ï•,t,Ï„>Â¯nA/parenrightbigg
+/summationdisplay
tâˆˆFâ€²
Ï•E/bracketleftbig
1{Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nA}/bracketrightbig
(34)
â‰¤/summationdisplay
tâˆˆFâ€²
Ï•P/parenleftbigg
Ï‘iâˆ—
Ï•,tâ‰¤Âµiâˆ—
Ï•,tâˆ’/radicalï£¬igg
5 logÏ„
Tiâˆ—
Ï•,t,Ï„,Tiâˆ—
Ï•,t,Ï„>Â¯nA/parenrightbigg
+ Â¯nANÏ•
Ï„(35)
The first mistake appears in the blue part. They claim that
/summationdisplay
tâˆˆFâ€²
Ï•E/bracketleftbig
1{Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nA}/bracketrightbig
â‰¤Â¯nANÏ•
Ï„. (36)
22Under review as submission to TMLR
This inequality is not true, as it lacks one condition it=iâˆ—
Ï•. And from the context in their proof, we know
that onlyit=iholds, notit=iâˆ—
Ï•.
To see why Equation (36) is wrong, consider the simple example: the algorithm always select the suboptimal
arm in phaseFâ€²
Ï•. Thus, 1{Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nA}= 1,âˆ€tâˆˆFâ€²
Ï•. We have
/summationdisplay
tâˆˆFâ€²
Ï•E/bracketleftbig
1{Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nA}/bracketrightbig
=|Fâ€²
Ï•|.
This implies Equation (36) is not true.
In their proof, there exists other three mistakes related to Equation (36)(the numbering of the equations
below is the same as in their proof):
Eq 43â†’Eq 44 :/summationdisplay
tâˆˆFâ€²
Ï•P/parenleftbig
Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nBâˆ—/parenrightbig
â‰¤Â¯nBâˆ—NÏ•
Ï„(37)
Eq 71â†’Eq 72 :/summationdisplay
tâˆˆFâˆ†C,NP/parenleftbig
Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nA/parenrightbig
â‰¤Â¯nB/ceilingleftbiggN
Ï„/ceilingrightbigg
(38)
Eq 95â†’Eq 96 :/summationdisplay
tâˆˆFâˆ†C,NP/parenleftbig
Tiâˆ—
Ï•,t,Ï„â‰¤Â¯nBâˆ—/parenrightbig
â‰¤Â¯nBâˆ—/ceilingleftbiggNÏ•
Ï„/ceilingrightbigg
(39)
The last two inequalities are for smoothly changes. We speculate that fixing these errors would also require
proving conclusions similar to Lemma 5.3 and Lemma 5.5. However, since the Beta distribution does not
have concentration properties like the Gaussian distribution (Fact 1), fixing these errors is challenging.
23