Published in Transactions on Machine Learning Research (04/2023)
Uncovering the Representation of Spiking Neural Networks
Trained with Surrogate Gradient
Yuhang Li yuhang.li@yale.edu
Yale University
Youngeun Kim youngeun.kim@yale.edu
Yale University
Hyoungseob Park hyoungseob.park@yale.edu
Yale University
Priyadarshini Panda priya.panda@yale.edu
Yale University
Reviewed on OpenReview: https: // openreview. net/ forum? id= s9efQF3QW1
Abstract
Spiking Neural Networks (SNNs) are recognized as the candidate for the next-generation
neuralnetworksduetotheirbio-plausibilityandenergyefficiency. Recently, researchershave
demonstrated that SNNs are able to achieve nearly state-of-the-art performance in image
recognition tasks using surrogate gradient training. However, some essential questions exist
pertaining to SNNs that are little studied: Do SNNs trained with surrogate gradient learn
different representations from traditional Artificial Neural Networks (ANNs)? Does the time
dimension in SNNs provide unique representation power? In this paper, we aim to answer
these questions by conducting a representation similarity analysis between SNNs and ANNs
using Centered Kernel Alignment (CKA). We start by analyzing the spatial dimension of
the networks, including both the width and the depth. Furthermore, our analysis of residual
connections shows that SNNs learn a periodic pattern, which rectifies the representations
in SNNs to be ANN-like. We additionally investigate the effect of the time dimension on
SNN representation, finding that deeper layers encourage more dynamics along the time
dimension. We also investigate the impact of input data such as event-stream data and
adversarial attacks. Our work uncovers a host of new findings of representations in SNNs.
We hope this work will inspire future research to fully comprehend the representation power
of SNNs. Code is released at https://github.com/Intelligent-Computing-Lab-Yale/
SNNCKA.
1 Introduction
Lately, Spiking Neural Networks (SNNs) (Tavanaei et al., 2019; Roy et al., 2019; Deng et al., 2020; Panda
et al., 2020; Christensen et al., 2022) have received increasing attention thanks to their biology-inspired
neuron activation and efficient neuromorphic computation. SNNs process information with binary spike
representation and therefore avoid the need for multiplication operations during inference. Neuromorphic
hardware such as TrueNorth (Akopyan et al., 2015) and Loihi (Davies et al., 2018) demonstrate that SNNs
can save energy by orders of magnitude compared to Artificial Neural Networks (ANNs).
Although SNNs can bring enormous energy efficiency in inference, training SNNs is notoriously hard because
of their spiking activation function. This function returns a zero-but-all gradient ( i.e.,Dirac delta function)
and thus makes gradient-based optimization infeasible. To circumvent this problem, various training tech-
niques have been proposed. For example, spike-timing-dependent plasticity (STDP) (Rao & Sejnowski, 2001)
1Published in Transactions on Machine Learning Research (04/2023)
PreactFeature1ReLUPreactFeature1LIFPreactFeature2PreactFeature2
TestImagesBatchsize:mSpikingNeuralNetworkArtificialNeuralNetwork
TTReLU
LIFâ€¦â€¦PreactFeature3PreactFeature3
TCorrelationGrammatrixmmmð¶!"=#ð‘‹!#âˆ—ð‘‹$%	#
ð¶!"=##ð‘‹!#(')âˆ—ð‘‹$%(')	#'CKASimilarity
Figure 1: The representation similarity analysis workflow. The test images are fed into both ANN and SNN,
then we record the intermediate feature for computing the correlation matrix, which is used for inferring the CKA
similarity (Kornblith et al., 2019).
eitherstrengthensorweakensthesynapticweightbasedonthefiringtime; time-to-first-spike(Mostafa,2017)
encodes the information into the time of spike arrival to get a closed-form solution of the gradients. However,
these two methods are restricted to small-scale tasks and datasets. Surrogate gradient technique (Bengio
et al., 2013; Bender et al., 2018; Wu et al., 2018; Bellec et al., 2018; Kim et al., 2022b; Li et al., 2022), on
the other hand, can achieve the best task performance by applying an alternate function during backprop-
agation. Combined with surrogate gradient, SNNs can be optimized by Backpropagation Through Time
(BPTT) (Neftci et al., 2019) algorithm, outperforming other learning rules in SNNs.
Despite increasing interest in pursuing high-performance SNNs with surrogate gradient training, there is
limited understanding of how surrogate gradient training affects the representation of SNNs. Investigating
this fundamental question is critical since surrogate gradient-based BPTT algorithm mimics the way how
ANN learns and is less biological-plausible compared to other learning rules like STDP. Therefore, it would
be intriguing to study whether surrogate gradient-based SNNs learn different representations than ANNs.
Understanding the representation learned in SNN can also promote further research developments, e.g.,
designing spiking-friendly architectures (Kim et al., 2022a; Na et al., 2022) and exploring other ways to
optimize SNNs (Bellec et al., 2020; Zhang & Li, 2020; Kim & Panda, 2021a).
More concretely, we ask, do SNNs optimized by surrogate gradient BPTT learn distinct representations from
ANNs? How do the width and depth of the neural network affect the representation learned in SNNs and
ANNs? Does the extra temporal dimension in SNNs yield unique intermediate features? On neuromorphic
datasets, how does the SNN process event-based data? In this paper, we aim to answer these core questions
through a detailed analysis of ResNets (He et al., 2016a) and VGG-series (Simonyan & Zisserman, 2015)
models using a representation similarity analysis tool. Specifically, we utilize the popular Centered Kernel
Alignment (CKA) (Kornblith et al., 2019) to measure the similarity between SNNs and ANNs. Fig. 1
demonstrates the overall workflow of our representation similarity analysis framework. Our analysis spans
both the spatial and temporal dimensions of SNNs, as well as the impact of network architecture and input
data.
Our contributions and findings include:
â€¢We analyze the representation similarity between SNNs and ANNs using the centered kernel alignment
to determine whether SNNs produce different feature representations from ANNs. We examine various
aspects of representation similarity between SNNs and ANNs, including spatial and temporal dimensions,
input data type, and network architecture.
â€¢Surprisingly, our findings show that SNNs trained with surrogate gradient have a rather similar represen-
tation to ANNs. We also find that residual connections greatly affect the representations in SNNs.
2Published in Transactions on Machine Learning Research (04/2023)
â€¢Meanwhile, we find that the time dimension in SNNs does not provide much unique representation. We
also find that shallow layers are insensitive to the time dimension, where the representation in each time
step converges together.
2 Related Work
Spiking Neural Networks (SNNs). SNNs have gained increasing attention for building low-power intel-
ligence. Generally, the SNN algorithms to obtain high performance can be divided into two categories: (1)
ANN-SNN conversion (Rueckauer et al., 2016; 2017; Han et al., 2020; Sengupta et al., 2019; Han & Roy,
2020) and (2) direct training SNN from scratch (Wu et al., 2018; 2019). Conversion-based methods utilize
the knowledge from ANN and convert the ReLU activation to a spike activation mechanism. This type of
method can produce an SNN in a short time. For example, in Rueckauer et al. (2017), one can find the
percentile number and set it as the threshold for spiking neurons. Authors in Deng & Gu (2021) and Li et al.
(2021a) decompose the conversion error to each layer and then propose to reduce the error by calibrating
the parameters. However, achieving near-lossless conversion requires a considerable amount of time steps
to accumulate the spikes. Direct training from scratch allows SNNs to operate in extremely low time steps,
even less than 5 (Zheng et al., 2020). To enable gradient-based learning, direct training leverages surrogate
gradient to compute the derivative of the discrete spiking function. This also benefits the choice of hyper-
parameters in spiking neurons. Recent works (Fang et al., 2021; Rathi & Roy, 2020; Kim & Panda, 2021b;
Deng et al., 2022) co-optimize parameters, firing threshold, and leaky factor together via gradient descent.
Our analysis is mostly based on directly trained SNNs, as converted SNNs only contain ANN features and
may be misleading for representation comparison.
Representation Similarity Analysis (RSA). RSA (Kriegeskorte et al., 2008) was not originally de-
signed for analyzing neural networks specifically. Rather, it is used for representation comparison between
any two computational models. Prior works such as Khaligh-Razavi & Kriegeskorte (2014); Yamins et al.
(2014) have used RSA to find the correlation between visual cortex features and convolutional neural network
features. Authors of Seminar (2016); Raghu et al. (2017); Morcos et al. (2018); Wang et al. (2018) have
studied RSA between different neural networks. However, recent work (Kornblith et al., 2019) argues that
none of the above methods for studying RSA can yield high similarity even between two different initializa-
tions of the same architecture. They further propose CKA, which has become a powerful evaluation tool
for RSA and has been successfully applied in several studies. For example, Nguyen et al. (2020) analyzes
the representation pattern in extremely deep and wide neural networks, and Raghu et al. (2021) studies the
representation difference between convolutional neural networks and vision transformers with CKA. In this
work, we leverage this tool to compare ANNs and SNNs.
3 Preliminary
3.1 ANN and SNN Neurons
In this paper, vectors/matrices are denoted with bold italic/capital letters ( e.g.xandWdenote the input
vector and weight matrix, respectively). Constants are denoted by small upright letters. For non-linear
activation function in artificial neurons, we use the rectified linear unit (ReLU) (Krizhevsky et al., 2012),
given by y= max(0,Wx). As for the non-linear activation function in spiking neurons, we adopt the well-
known Leaky Integrate-and-Fire (LIF) model. Formally, given a membrane potential u(t)at time step tand
a pre-synaptic input i(t+1)=Wx(t+1), the LIF neuron will update as
u(t+1),pre=Ï„u(t)+i(t+1),y(t+1)=/braceleftï£¬igg
1ifu(t+1),pre>vth
0otherwise,u(t+1)=u(t+1),preÂ·(1âˆ’y(t+1)).(1)
Here, u(t+1),preis the pre-synaptic membrane potential, Ï„is a constant leak factor within (0,1). Letvthbe
the firing threshold, the LIF neuron will fire a spike ( y(t+1)= 1) when the membrane potential exceeds the
threshold; otherwise, it will stay inactive ( y(t+1)= 0). After firing, the spike output y(t+1)will propagate to
3Published in Transactions on Machine Learning Research (04/2023)
the next layer and become the input x(t+1)of the next layer. Note that here the layer index is omitted for
simplicity. The membrane potential will be reset to 0 if a spike fires (refer to Eq. (1) the third sub-equation).
3.2 Optimize SNN with Surrogate Gradient
To enable gradient descent for SNN, we adopt the BPTT algorithm (Werbos, 1990). Formally, denote the
loss function value as L, the gradient of the loss value with respect to weights can be formulated by
âˆ‚L
âˆ‚W=T/summationdisplay
t=1âˆ‚L
âˆ‚y(t)âˆ‚y(t)
âˆ‚u(t),preK(t),where K(t)=/parenleftbiggâˆ‚u(t),pre
âˆ‚i(t)âˆ‚i(t)
âˆ‚W+âˆ‚u(t),pre
âˆ‚u(tâˆ’1)âˆ‚u(tâˆ’1)
âˆ‚u(tâˆ’1),preK(tâˆ’1)/parenrightbigg
.(2)
Here, the gradient is computed based on the output spikes from all time steps. In each time step, we denote
Kas the gradient of pre-synaptic membrane potential with respect to weightsâˆ‚upre
âˆ‚W, which consists of the
gradient of pre-synaptic input and the gradient of membrane potential in the last time steps.
As a matter of fact, all terms in Eq. (2) can be easily differentiated exceptâˆ‚y(t)
âˆ‚u(t),prewhich returns a zero-
but-all gradient (Dirac delta function). Therefore, the gradient descent ends up either freezing the weights
or updating weights to infinity. To address this problem, the surrogate gradient is proposed (Bender et al.,
2018; Wu et al., 2018; Bellec et al., 2018; Neftci et al., 2019; Li et al., 2021b) to replace the Dirac delta
function with another function:
âˆ‚y(t)
âˆ‚u(t),pre=1
Î±1|u(t),preâˆ’vth|<Î±, (3)
whereÎ±is a hyper-parameter for controlling the sharpness and Î±= 1the surrogate gradient becomes the
Straight-Through Estimator (Bengio et al., 2013).
Compared to other methods such as ANN-SNN conversion (Deng & Gu, 2021) or spike-timing-dependent
plasticity (Caporale et al., 2008), BPTT using surrogate gradient learning yields the best performance in
image recognition tasks. However, from a biological perspective, BPTT is implausible: for each weight
update, BPTT requires the use of the transpose of the weights to transmit errors backward in time and
assign credit for how past activity affected present performance. Running the network with transposed
weights requires the network to either have two-way synapses or use a symmetric copy of the feedforward
weights to backpropagate the error (Marschall et al., 2020). Therefore, the question remains whether the
representation in SNNs learned with surrogate gradient-based BPTT actually differs from the representation
in ANNs.
3.3 Centered Kernel Alignment
LetXsâˆˆRmÃ—Tp1andXaâˆˆRmÃ—p2contain the representation in an arbitrary layer of SNN with p1hidden
neurons across Ttime steps and the representation in an arbitrary layer of ANN with p2hidden neurons,
respectively. Here mis the batch size and we concatenate features from all time steps in the SNN altogether.
We intend to use a similarity index s(Xs,Xa)to describe how similar they are. We use the Centered Kernel
Alignment (CKA) (Kornblith et al., 2019) to measure this:
CKA (K,L) =HSIC (K,L)/radicalbig
HSIC (K,K)HSIC (L,L),HSIC (K,L) =1
(mâˆ’1)2tr(KHLH ). (4)
Here,K=XsXâŠ¤
s,L=XaXâŠ¤
aare the Gram matrices as shown in Fig. 1. Each Gram matrix has the shape
ofmÃ—m, reflecting the similarities between a pair of examples. For example, Ki,jindicates the similarity
between the ithandjthexample in the SNN feature Xs. Further measuring the similarity between KandL,
one can measure whether SNN has a similar inter-example similarity matrix with ANN. Let H=Iâˆ’1
m11âŠ¤
be the centering matrix, the Hilbert-Schmidt Independence Criterion (HSIC) proposed by Gretton et al.
(2005) can conduct a test statistic for determining whether two sets of variables are independent. HSIC
= 0implies independence. The CKA further normalizes HSIC to produce a similarity index between 0 and
1 (the higher the CKA, the more similar the input pair) which is invariant to isotropic scaling. In our
implementation, we use the unbiased estimator of HSIC (Song et al., 2012; Nguyen et al., 2020) to calculate
it across mini-batches.
4Published in Transactions on Machine Learning Research (04/2023)
0 20 40 60
Spiking ResNet-200204060Artificial ResNet-20
0 50 100
Spiking ResNet-380255075100125Artificial ResNet-38
0 50 100 150
Spiking ResNet-56050100150Artificial ResNet-56
0 100 200 300
Spiking ResNet-1100100200300Artificial ResNet-110
0 200 400
Spiking ResNet-1640100200300400500Artificial ResNet-164
0 20 40 60
Spiking ResNet-200204060Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 x20204060Artificial ResNet-20 x2
0 20 40 60
Spiking ResNet-20 x40204060Artificial ResNet-20 x4
0 20 40 60
Spiking ResNet-20 x80204060Artificial ResNet-20 x8
0 20 40 60
Spiking ResNet-20 x160204060Artificial ResNet-20 x16
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 10 20 30 40 50 60 70
0.00.51.0
ResNet20
0 20 40 60 80 100 120 140
0.00.51.0
ResNet38
0 25 50 75 100 125 150 175 200
0.00.51.0
ResNet56
0 50 100 150 200 250 300 350 400
0.00.51.0
ResNet110
0 100 200 300 400 500 600
Layers0.00.51.0
ResNet164
0 10 20 30 40 50 60 70
Layers0.20.30.40.50.60.70.80.91.0
ResNet20
ResNet20 x2ResNet20 x4
ResNet20 x8ResNet20 x16Similarity
Figure 2: CKA heatmap between SNNs and ANNs with different depth and width on the CIFAR-10
dataset. Top : the CKA cross-layer heatmap across different depths from 20 layers to 164 layers. Middle: the
CKA cross-layer heatmap across different widths from the original channel number to 16 times. Bottom : visualizing
only the corresponding layer, which is the diagonal of the CKA heatmap. We find generally SNNs and ANNs have
relatively high similarity, and deeper/wider networks have positive/negative effects on the representation similarity.
4 Do SNNs Learn Different Representation from ANNs?
In this section, we comprehensively compare the representation learned in SNNs and ANNs. Our primary
study case is ResNet with identity mapping block (He et al., 2016b) on the CIFAR10 dataset, which is the
standard architecture and dataset in modern deep learning for image recognition.1There are two differences
between our SNNs and ANNs. First, ANNs adopt the Batch Normalization layer (Ioffe & Szegedy, 2015),
and SNNs use the time-dependent Batch Normalization layer (Zheng et al., 2020), which normalizes the
feature across all time steps ( i.e.Xs). Second, the ANNs use ReLU activation, and SNNs leverage the LIF
spiking neurons. For default SNN training, we use direct encoding, Ï„= 0.5for the leaky factor, vth= 1.0
for the firing threshold, T= 4for the number of time steps, and Î±= 1.0for surrogate gradient, which are
tuned for the best training performance on SNNs. Detailed training setup and codes can be found in the
supplementary material.
4.1 Scaling up Width or Depth
We begin our study by studying how the spatial dimension of a model architecture affects internal represen-
tation structure in ANNs and SNNs. We first investigate a simple model: ResNet-20, and then we either
increase its number of layers or increase its channel number to observe the effect of depth and width, re-
spectively. In the most extreme cases, we scale the depth to 164 and the width to 16 Ã—(see detailed network
configuration in Table D.1). For each network, we compute CKA between all possible pairs of layers, includ-
1We also provide RSA on VGG-series networks in Sec. A.1 and RSA on CIFAR100 dataset in Sec. A.2.
5Published in Transactions on Machine Learning Research (04/2023)
Table 1: The top-1 accuracy of SNNs and ANNs on CIFAR-10 dataset, as well as the gap âˆ†between ANN and SNN.
Network Depth Width
20 38 56 110 164 Ã—1Ã—2Ã—4Ã—8Ã—16
ANN 91.06 92.34 92.98 92.37 93.00 91.06 93.36 94.52 94.78 94.78
SNN 89.67 91.14 91.94 91.83 92.05 89.67 92.00 93.96 94.48 94.73
âˆ†A-S 1.39 1.20 1.04 0.53 0.95 1.39 1.36 0.56 0.30 0.05
ing convolutional layers, normalization layers, ReLU/LIF layers, and residual block output layers. Therefore,
the total number of layers is much greater than the stated depth of the ResNet, as the latter only accounts
for the convolutional layers in the network. Then, we visualize the result as a heatmap, with the xandy
axes representing the layers of the network, going from the input layer to the output layer. Following Nguyen
et al. (2020), our CKA heatmap is computed on 4096 images from the test dataset.
As shown in Fig. 2, the CKA heatmap emerges as a checkerboard-like grid structure, especially for the
deeper neural network. In ResNet-20, we observe a bright block in the middle and deep layers, indicating
that ANNs and SNNs learn overlapped representation. As the network goes deep, we find the CKA heatmap
becomes darker, meaning that representations in ANNs and those in SNNs are diverging. Notably, we find
a large portion of layers in artificial ResNet-164 exhibit significantly different representations than spiking
ResNet-164 ( <0.2 CKA value) which demonstrates that deeper layers tend to learn disparate features.
In Fig. 2 middle part, we progressively enlarge the channel number of ResNet-20. In contrast to depth,
the heatmap of wider neural networks becomes brighter, which indicates the representations in SNNs and
ANNs are converging. Interestingly, although the majority of layers are learning more similar representations
between ANN and SNN in wide networks, the last several layers still learn different representations.
We further select only the diagonal elements in the heatmap and plot them in Fig. 2 bottom part. Because
SNNs and ANNs have the same network topology, this visualization is more specific and may accurately
reveal the similarity between SNNs and ANNs at each corresponding layer. First, we can find that in Fig. 2
the CKA curve of ResNet-20 shows relatively high values. Most layers go above 0.5 and some of them can
even reach nearly 1.0. Interestingly, we observe that deeper networks tend to derive a curve with a jagged
shape. This means some layers in SNN indeed learn different representations when compared to ANN,
however, the difference is intermittently mitigated . In later sections, we will show that the mitigation of
dissimilarity is performed by residual connection. As for width, we generally notice that CKA curves mostly
become higher for wider networks, especially when comparing ResNet-20 and ResNet-20 Ã—8, where most
layers have above 0.8 CKA value.
Evaluating the Task Performance. WelayouttheaccuracyofANNsandSNNsinTable1. Additionally,
we also calculate their accuracy difference across different width and depth configurations. We can find that
the accuracy gap decreases if we scale up the width, ranging from 1.39% to 0.05%. However, as the depth
increases, the accuracy gap does not consistently reduce as it does in wider networks. This explains that
wider neural networks which bring more similar representations encourage less accuracy gap. However,
SNNs do not achieve the same accuracy level as ANNs with different representations. This finding means
that current state-of-the-art SNNs relying on wider neural networks (e.g., ResNet-19 (Zheng et al., 2020)
which is similar to our ResNet-20 8 Ã—) do not truly develop unique feature representations than ANNs.
4.2 The Effect of Residual Connections
In Fig. 2, the CKA curves appear with a periodic jagged shape. To investigate what causes this similarity
oscillation, we investigate each layer in a residual block. In Fig. 3 left, we plot the CKA curve of the ResNet-
110 and additionally sample two residual blocks, the 10- thand the 34- thblock, whose architecture details
are depicted in Fig. 3 right. Surprisingly, we find that every time when the residual connection meets the
main branch, the CKA similarity restores nearly to 1. Moreover, every time when the activation passes a
convolutional layer or an LIF layer, the similarity decreases. The BN layers, in contrast, do not affect the
6Published in Transactions on Machine Learning Research (04/2023)
LIF/ReLUConv(td)BN
Figure 3: Emergence of periodic jagged CKA curve. Left : CKA curve of ResNet-110. We subplot the 10- th
and the 34- thresidual blocks in ResNet-110, which forms a periodic jagged curve. Top right : The averaged CKA
value in all blocks, Bottom right : The architecture specification of the residual block we used.
0 50 100 150
Layers050100150LayersSpiking Res56 Full Residual
0 50 100 150
Layers050100150Spiking Res56 Partial Residual (1)
0 50 100 150
Layers050100150Spiking Res56 Partial Residual (2)
0 50 100 150
Layers050100150Spiking Res56 Partial Residual (3)
0.00.20.40.60.81.0
Figure 4: The effect of residual connections in SNNs. We remove residual connections in one of three stages
in the ResNet-56 and show the CKA heatmaps. The non-residual stage is annotated with green square â–¡.
similarity since it is a linear transformation. These results substantiate that the convolutional layers and LIF
layers in SNNs are able to learn different representations than ANNs. However, the representation in the
residual branch still dominates the representation in post-residual layers and leads to the junction of ANNâ€™s
and SNNâ€™s representation.
To further explore why residual connections can restore the representations in SNNs to ANN-like, we conduct
an ablation study. We select one of the three stages in the spiking ResNet-56 where the residual connections
are disabled selectively. In Fig. 4, we visualize the CKA heatmaps of SNN itself, which means both xand
yaxes are the same layers in SNN. The first heatmap demonstrates the full residual network, while the
remaining three heatmaps show the partial residual networks, with the 1st, 2nd, and 3rd stages disabled,
respectively. Our observations can be summarized as follows: (1) In terms of inter-stage similarity, residual
connections can preserve the input information from the previous stage. In the 1st and 2nd heatmaps in
Fig. 4, we find residual blocks can have high similarity with their former stage. The non-residual block,
however, does not have this property. In the 3rd and 4th heatmaps, we can see that blocks without residual
connections exhibit significantly different representations when compared to their former stage. Therefore,
we can find that residual connections preserve the representation in early layers. As such, if ANN and SNN
learn similar representations in the first layer, the similarity can propagate to very deep layers due to residual
connections. (2) In terms of intra-stage similarity, the non-residual stageâ€™s heatmap appears with a uniform
representation across all layers, meaning that layers in this stage are similar. In contrast, residual stages
share a grid structure.
Next, we verify the accuracy of SNNs and ANNs when both are equipped with residual connections or not,
under different network depths. As shown in Table 2, both the SNNs and ANNs can successfully train very
7Published in Transactions on Machine Learning Research (04/2023)
Table 2:The impact of residual connections on accuracy.
Model TypeDepth
20 38 56 74
ANNw/. residual connection 91.06 92.34 92.98 92.85
w/o residual connection 91.32 91.17 89.62 21.07
SNNw/. residual connection 89.63 91.14 91.94 91.83
w/o residual connection 86.50 82.64 33.61 10.00
deep networks if the residual connections are enabled. In this case, though SNNs do not surpass the accuracy
of ANNs, the gap is relatively small, with 1 âˆ¼2% accuracy degradation. However, if the residual connections
are removed from SNNs, the gap between the accuracies of ANNs and SNNs significantly enlarges, ranging
from 5âˆ¼56%. Therefore, we can conclude that the residual connections help the gradient descent optimization
in SNNs and regularize the representations in SNNs to be similar to those in ANNs so that SNNs can have
similar task performances with ANNs.
4.3 Scaling up Time Steps
The results of the previous sections help characterize the effects of spatial structure on internal represen-
tation differences between SNNs and ANNs. Next, we ask whether the time dimension helps SNN learn
some unique information. To verify this, we train several spiking ResNet-20 with 4/8/16/32/64/128 time
steps and calculate the ANN-SNN CKA similarity. In Fig. 5, we envision the CKA heatmaps and curves
respectively between artificial ResNet-20 and spiking ResNet-20 with various time steps. Notably, we cannot
find significant differences among these heatmaps. Looking at the CKA curves, we also discover that many
layers are overlapped, especially when we focus on the residual block output (the local maximums). We find
similarities between different time steps reaching the same point, meaning that the time step variable does
not provide much unique representation in SNNs.
TofurtheranalyzetherepresentationalongthetimedimensioninSNNs, wecomparetheCKAamongvarious
timesteps. Concretely, foranylayerinsideanSNN,wereshapethefeature Xsto[X(1),X(2),...,X(T)]where
X(i)is thei-thtime stepâ€™s output. By computing the CKA similarity between arbitrary two time steps,
i.e.,CKA (X(i),X(j)), we are able to construct a CKA heatmap with x,yaxes being the time dimension,
which demonstrates whether the features are similar across different time steps. Fig. 6 illustrates such CKA
heatmaps of outputs from all residual blocks in the spiking ResNet-20, with time steps varying from 4 to
32. In general, deeper residual block output exhibits darker CKA heatmaps and the shallower layers tend to
become yellowish. In particular, all the residual blocks from the first stage have an all-yellow CKA heatmap,
0 20 40 60
Spiking ResNet-20 T4010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T8010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T16010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T32010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T64010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T128010203040506070Artificial ResNet-20
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 10 20 30 40 50 60 70
Layers0.20.30.40.50.60.70.80.91.0Similarity
ResNet-20 T4
ResNet-20 T8
ResNet-20 T16ResNet-20 T32
ResNet-20 T64
ResNet-20 T128
Figure 5: The effect of time steps in SNNs. Left : CKA heatmaps between ANNs and SNNs with the different
number of time steps. Right: The CKA curve of corresponding layers (diagonal values as in left).
8Published in Transactions on Machine Learning Research (04/2023)
0 20123s1.b0
0 20123s1.b1
0 20123s1.b2
0 20123s2.b0
0 20123s2.b1
0 20123s2.b2
0 20123s3.b0
0 20123s3.b1
0 20123s3.b2
0 50246s1.b0
0 50246s1.b1
0 50246s1.b2
0 50246s2.b0
0 50246s2.b1
0 50246s2.b2
0 50246s3.b0
0 50246s3.b1
0 50246s3.b2
0 200102030s1.b0
0 200102030s1.b1
0 200102030s1.b2
0 200102030s2.b0
0 200102030s2.b1
0 200102030s2.b2
0 200102030s3.b0
0 200102030s3.b1
0 200102030s3.b2
0.00.20.40.60.81.0
Figure 6: The similarity across times in SNN. Each heatmap shows the CKA among different time steps in
the output of the residual block. â€œsâ€ means stage, and â€œbâ€ means block. The top/middle/bottom rows are spiking
ResNet-20 with 4/8/32 time steps, respectively.
Table 3:The sensitivity of time steps in SNNs.
Model Type# Time Steps
4 8 16 32
SNNFull time steps 89.67 90.44 90.98 90.99
Reduce the time steps in the first stage 88.81 89.70 89.91 90.47
Reduce the time steps in the last stage 87.41 87.78 87.38 87.73
indicating extremely high similarity in these blocks. The second stage starts to produce differences across
time steps, but they still share >0.8 similarities between any pair of time steps. The last stage, especially
the last block, demonstrates around 0.5 similarities between different time steps. To summarize, the impact
of time in SNN is gradually increased as the feature propagates through the network. In Appendix A.4, we
provide the heatmap of convolutional/LIF layers and find a similar trend.
We further conduct an empirical verification to verify the findings in Fig. 6. More specifically, we define
a sensitivity metric and measure it by reducing the number of time steps to 1 in certain layers of an SNN
and recording the corresponding accuracy degradation. In Fig. 6 we find the first stage (s1) has the same
representation in time dimension while the last stage (s3) exhibits a more diverse representation. Therefore,
we choose to reduce the number of time steps to 1 either in s1 or in s3. To achieve this â€œmixed-time-
step SNN", we repeat/average the activation in time dimension after s1/before s3 to match the dimension.
Table 3 summarizes the sensitivity results. We observe that the first stage can have much lower accuracy
degradation (<1%), while the last stage drop 2 âˆ¼4% accuracy. Moreover, if the last stage only uses 1 time
step, then increasing the time steps for the other two stages does not benefit the accuracy at all. This
indicates that LIF neurons are more effective in deep layers than in shallow layers.
4.4 Representation under Event Data
In this section, we evaluate the CKA similarity on the event-based dataset. We choose CIFAR10-DVS (Li
et al., 2017), N-Caltech 101 (Orchard et al., 2015) and train spiking/artificial ResNet-20. Since the ANN
cannot process 4-dimensional spatial-temporal event data easily, we integrate all events into one frame for
ANN training and 10 frames for SNN training. Fig. 7 provides the CKA heatmaps/curves on the event
dataset, showing a different similarity distribution than the previous CIFAR-10 dataset. The heatmaps
have a different pattern and the curves also do not appear with a periodic jagged shape. In addition, the
similarity distribution differs at the dataset level, i.e., the CIFAR10-DVS and N-Caltech 101 share different
9Published in Transactions on Machine Learning Research (04/2023)
0 20 40 60
Spiking ResNet-20 T10010203040506070Artificial ResNet-20CIFAR10-DVS
0 20 40 60
Spiking ResNet-20 T10010203040506070Artificial ResNet-20N-Caltech 101
0.00.20.40.60.81.0
0 10 20 30 40 50 60 70
Layers0.40.50.60.70.80.91.0Similarity
CIFAR10-DVS
N-Caltech 101
Figure 7: CKA Similar-
ity on Event Dataset. We
train spiking and artificial
ResNet-20 on CIFAR10-DVS
and N-Caltech 101, respec-
tively. Left: the CKA
heatmaps. Right: The CKA
curves of corresponding lay-
ers between ANN and SNN.
0.0 0.001 0.005 0.01 0.02 0.05
020406080Accuracy91.0689.67
87.9987.77
72.8180.72
48.368.04
14.9240.08
0.111.88Robustness of ANN and SNN
ANN SNN
010 20 30 40 50 60 70
Layers0.00.20.40.60.81.0SimilaritySimilarity of ANN
=0.001
=0.005
=0.01
=0.02
=0.05
010 20 30 40 50 60 70
Layers0.00.20.40.60.81.0SimilaritySimilarity of SNN
=0.001
=0.005
=0.01
=0.02
=0.05
Figure 8: The robust-
ness against adversarial
attack. Left : The accu-
racy of SNN and ANN af-
ter attack under different Ïµ.
Right: The CKA curve be-
tween clean images and ad-
versarial images of ANN and
SNN, respectively.
CKA curves and heatmaps. On N-Caltech 101, the SNN learns different feature representations compared
with ANN in shallow and deep layers, but similar representation in intermediate layers. For CIFAR10-DVS,
the similarity continues to decrease from 0.9 to 0.5 as the layers deepen. In summary, with the event-based
dataset, SNNs and ANNs share a different CKA pattern in comparison to the natural image dataset, which
implies that SNNs may have further optimization space in this type of dataset. We put more CKA results
on various models for the CIFAR10-DVS dataset in Appendix A.3.
4.5 Representation under Adversarial Attack Data
We next study the adversarial robustness of SNN and ANN using CKA. Inspired by quantized ANNs that are
robust to adversarial attack Lin et al. (2019), the SNNs could also inherit this property since their activations
are also discrete. Previous works have explored understanding the inherent robustness of SNNs (Sharmin
et al., 2020; Kundu et al., 2021; Liang et al., 2021; Kim & Panda, 2021c). However, they either evalu-
ate on converted SNN or using rate-encoded images. Here, we test Projected Gradient Descent (PGD)
attack (Madry et al., 2017) on the directly trained SNN and ANN using direct encoding. Formally, we
generate the adversarial images by restricting the L-infinity norm of the perturbation, given by
xk+1
adv= Î PÏµ(x)(xk
adv+Î±sign(âˆ‡xk
advL(xk
adv,w,y))), (5)
where xk
advis the generated adversarial sample at the k-thiteration. Î PÏµ(x)(Â·)projects the generated sample
onto the projection space, the Ïµâˆ’Lâˆžneighborhood of the clean sample. Î±is the attack optimization step
size. With higher Ïµ, the adversarial image is allowed to be perturbed in a larger space, thus degrading task
performance.
We evaluate the performance of spiking and artificial ResNet-20 on the CIFAR-10 dataset with Ïµvalues
ranging from 0.001,0.005,0.01,0.02,0.05to generate adversarial images. We then compute the CKA value
between the features of the clean images and the adversarially corrupted images. The results are summarized
in Fig. 8 (left). We find that although the clean accuracy of ANN is higher than that of SNN, SNN has higher
robustness against adversarial attacks. For example, the PGD attack with 0.01 L-infinity norm perturbation
reduces the accuracy of ANN by 43%, while only reducing the accuracy of SNN by 22%. We also investigate
the CKA similarity between clean and adversarial images, as shown in the second and third subplots of
Fig. 8. We observe that the higher the robustness against adversarial attacks, the higher the similarity
between clean and corrupted images. This intuition is confirmed by the CKA curves, which show that SNN
has a higher similarity than ANN. We also observe several interesting phenomena. For example, the ANN
10Published in Transactions on Machine Learning Research (04/2023)
suffers a large decrease in similarity in the first block, even with a small Ïµvalue of 0.001. Additionally, when
we focus on the purple line ( Ïµ= 0.02), we notice that ANN and SNN have similar perseverance in earlier
layers, but ANN drops much more similarity than SNN in the last block. These results provide insight into
model robustness and suggest that SNNs are more robust than ANN, especially in their shallow and deep
layers.
5 Discussion and Conclusion
Given that SNNs are drawing increasing research attention due to their bio-plausibility and recent progress
in task performance, it is necessary to verify if SNNs, especially those trained with the surrogate gradient
algorithms, can or have the potential to truly develop desired features different from ANNs. In this work,
we conduct a pilot study to examine the internal representation of SNNs and compare it with ANNs using
the popular CKA metric. This metric measures how two models respond to two different examples. Our
findings can be briefly summarized as follows:
1. Generally, the layer-wise similarity between SNNs and ANNs is high, suggesting SNNs trained with
surrogate gradient learn similar representation with ANNs. Moreover, wider networks like ResNet-20 8 Ã—
can even have >0.8similarities for almost all layers.
2. For extremely deep ANNs and SNNs, the CKA value would become lower, however, the residual con-
nections play an important role in regularizing the representations. By conducting ablation studies, we
demonstrate that the residual connections make SNNs learn similar representations with ANNs and help
SNNs achieve high accuracy.
3. ThetimedimensiondoesnotprovidemuchadditionalrepresentationpowerinSNNs. Wealsodemonstrate
that the shallow layers learn completely static representation along the time dimension. Even reducing
the number of time steps to 1 in shallow layers does not significantly affect the performance of SNNs.
4. On other types of datasets, SNNs may develop less similar representations with ANNs, e.g.,event-data.
Our results show that SNNs optimized by surrogate gradient algorithm do not learn distinct spatial-temporal
representation compared to the spatial representation in ANNs. Current SNN learning relies on the residual
connection and wider neural networks (for example, Zheng et al. (2020) use ResNet-19 which is similar to our
ResNet-20 8Ã—) to obtain decent task performance. However, our study suggests that this task performance
is highly credited to the similar representation with ANN. Furthermore, the time dimension brings limited
effect to the SNN representation on static datasets like CIFAR10 and CIFAR100. In particular, the first
stage of ResNets results in quite similar representation across time steps.
Nonetheless, our study is not a negation against SNNs. Our results are based on the surrogate-gradient
BPTT optimization, which, as aforementioned, is inherently bio-implausible and resembles the optimization
method for ANNs. Therefore, it may not be surprising to see SNNs and ANNs have similar representations
under a similar optimization regime. Additionally, we find that input data is also important in developing
the representations. Indeed, the direct encoding used in SNNs inputs the same static images for each time
step, again leading to less gap between the representations of ANNs and SNNs.
Here, we provide several directions worth studying in the future: a) Bio-plausible learning rule for SNNs:
surrogate gradient training tends to learn an ANN-like representation in SNN, thus it is necessary to develop
an optimization method that suits SNN better. b) Spiking architecture design: a specialized SNN network
architecture may avoid learning similar representation, e.g.,Kim et al. (2022a). c) Understanding the
robustness of SNN: adversarial attack is inconsequential for human visual systems, which may be reflected
in SNN as well. We believe the SNN robustness can be significantly improved. To conclude, our work tries
to understand the representation of SNNs trained with surrogate gradient and reveals some counter-intuitive
observations. We hope our work can inspire more research in pushing the limit of SNNs.
Acknowledgements. This work was supported in part by CoCoSys, a JUMP2.0 center sponsored by
DARPA and SRC, Google Research Scholar Award, the National Science Foundation CAREER Award,
TII (Abu Dhabi), the DARPA AI Exploration (AIE) program, and the DoE MMICC center SEA-CROGS
(Award #DE-SC0023198).
11Published in Transactions on Machine Learning Research (04/2023)
References
Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John Arthur, Paul Merolla, Nabil
Imam, Yutaka Nakamura, Pallab Datta, Gi-Joon Nam, et al. Truenorth: Design and tool flow of a 65
mw 1 million neuron programmable neurosynaptic chip. IEEE transactions on computer-aided design of
integrated circuits and systems , 34(10):1537â€“1557, 2015.
Guillaume Bellec, Darjan Salaj, Anand Subramoney, Robert Legenstein, and Wolfgang Maass. Long short-
term memory and learning-to-learn in networks of spiking neurons. arXiv preprint arXiv:1803.09574 ,
2018.
Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and
Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature
communications , 11(1):1â€“15, 2020.
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. Understanding and
simplifying one-shot architecture search. In ICML, 2018.
Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
Natalia Caporale, Yang Dan, et al. Spike timing-dependent plasticity: a hebbian learning rule. Annual
review of neuroscience , 31(1):25â€“46, 2008.
DennisValbjÃ¸rnChristensen, ReginaDittmann, BernabÃ©Linares-Barranco, AbuSebastian, ManuelLeGallo,
Andrea Redaelli, Stefan Slesazeck, Thomas Mikolajick, Sabina Spiga, StephanMenzel, et al. 2022 roadmap
on neuromorphic computing and engineering. Neuromorphic Computing and Engineering , 2022.
Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday,
GeorgiosDimou, PrasadJoshi, NabilImam, ShwetaJain, etal. Loihi: Aneuromorphicmanycoreprocessor
with on-chip learning. Ieee Micro , 38(1):82â€“99, 2018.
Lei Deng, Yujie Wu, Xing Hu, Ling Liang, Yufei Ding, Guoqi Li, Guangshe Zhao, Peng Li, and Yuan Xie.
Rethinking the performance comparison between snns and anns. Neural Networks , 121:294 â€“ 307, 2020.
Shikuang Deng and Shi Gu. Optimal conversion of conventional artificial neural networks to spiking neural
networks. arXiv preprint arXiv:2103.00476 , 2021.
Shikuang Deng, Yuhang Li, Shanghang Zhang, and Shi Gu. Temporal efficient training of spiking neural
network via gradient re-weighting. arXiv preprint arXiv:2202.11946 , 2022.
Wei Fang, Zhaofei Yu, Yanqi Chen, TimothÃ©e Masquelier, Tiejun Huang, and Yonghong Tian. Incorporating
learnable membrane time constant to enhance learning of spiking neural networks. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pp. 2661â€“2671, 2021.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard SchÃ¶lkopf. Measuring statistical depen-
dence with hilbert-schmidt norms. In International conference on algorithmic learning theory , pp. 63â€“77.
Springer, 2005.
Bing Han and Kaushik Roy. Deep spiking neural network: Energy efficiency through time based coding. In
Proc. IEEE Eur. Conf. Comput. Vis.(ECCV) , pp. 388â€“404, 2020.
Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy. Rmp-snn: Residual membrane potential neuron for
enabling deeper high-accuracy and low-latency spiking neural network. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 13558â€“13567, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
CVPR, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In
European conference on computer vision , pp. 630â€“645. Springer, 2016b.
12Published in Transactions on Machine Learning Research (04/2023)
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In ICML, 2015.
Seyed-Mahdi Khaligh-Razavi and Nikolaus Kriegeskorte. Deep supervised, but not unsupervised, models
may explain it cortical representation. PLoS computational biology , 10(11):e1003915, 2014.
Youngeun Kim and Priyadarshini Panda. Optimizing deeper spiking neural networks for dynamic vision
sensing. Neural Networks , 144:686â€“698, 2021a.
YoungeunKimandPriyadarshiniPanda. Revisitingbatchnormalizationfortraininglow-latencydeepspiking
neural networks from scratch. Frontiers in neuroscience , pp. 1638, 2021b.
Youngeun Kim and Priyadarshini Panda. Visual explanations from spiking neural networks using inter-spike
intervals. Scientific reports , 11(1):19037, 2021c.
Youngeun Kim, Yuhang Li, Hyoungseob Park, Yeshwanth Venkatesha, and Priyadarshini Panda. Neural
architecture search for spiking neural networks. arXiv preprint arXiv:2201.10355 , 2022a.
YoungeunKim, YuhangLi, HyoungseobPark, YeshwanthVenkatesha, RuokaiYin, andPriyadarshiniPanda.
Exploring lottery ticket hypothesis in spiking neural networks. In Computer Visionâ€“ECCV 2022: 17th
European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XII , pp. 102â€“120. Springer,
2022b.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network
representations revisited. In International Conference on Machine Learning , pp. 3519â€“3529. PMLR, 2019.
NikolausKriegeskorte, MariekeMur, andPeterABandettini. Representationalsimilarityanalysis-connecting
the branches of systems neuroscience. Frontiers in systems neuroscience , 2:4, 2008.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. Advances in neural information processing systems , 25, 2012.
Souvik Kundu, Massoud Pedram, and Peter A Beerel. Hire-snn: Harnessing the inherent robustness of
energy-efficient deep spiking neural networks by training with crafted input noise. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pp. 5209â€“5218, 2021.
Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi. Cifar10-dvs: an event-stream dataset
for object classification. Frontiers in neuroscience , 11:309, 2017.
Yuhang Li, Shikuang Deng, Xin Dong, Ruihao Gong, and Shi Gu. A free lunch from ann: Towards efficient,
accurate spiking neural networks calibration. In International Conference on Machine Learning , pp. 6316â€“
6325. PMLR, 2021a.
Yuhang Li, Yufei Guo, Shanghang Zhang, Shikuang Deng, Yongqing Hai, and Shi Gu. Differentiable spike:
Rethinking gradient-descent for training spiking neural networks. Advances in Neural Information Pro-
cessing Systems , 34:23426â€“23439, 2021b.
Yuhang Li, Youngeun Kim, Hyoungseob Park, Tamar Geller, and Priyadarshini Panda. Neuromorphic data
augmentation for training spiking neural networks. In Computer Visionâ€“ECCV 2022: 17th European
Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part VII , pp. 631â€“649. Springer, 2022.
LingLiang,XingHu,LeiDeng,YujieWu,GuoqiLi,YufeiDing,PengLi,andYuanXie. Exploringadversarial
attack in spiking neural networks with spike-compatible gradient. IEEE transactions on neural networks
and learning systems , 2021.
Ji Lin, Chuang Gan, and Song Han. Defensive quantization: When efficiency meets robustness. arXiv
preprint arXiv:1904.08444 , 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards
deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 , 2017.
13Published in Transactions on Machine Learning Research (04/2023)
Owen Marschall, Kyunghyun Cho, and Cristina Savin. A unified framework of online learning algorithms
for training recurrent neural networks. Journal of machine learning research , 2020.
Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural networks
with canonical correlation. Advances in Neural Information Processing Systems , 31, 2018.
Hesham Mostafa. Supervised learning based on temporal coding in spiking neural networks. IEEE transac-
tions on neural networks and learning systems , 29(7):3227â€“3235, 2017.
Byunggook Na, Jisoo Mok, Seongsik Park, Dongjin Lee, Hyeokjun Choe, and Sungroh Yoon. Autosnn:
Towards energy-efficient spiking neural networks. arXiv preprint arXiv:2201.12738 , 2022.
Emre O Neftci, Hesham Mostafa, and Friedemann Zenke. Surrogate gradient learning in spiking neural
networks: Bringing the power of gradient-based optimization to spiking neural networks. IEEE Signal
Processing Magazine , 36(6):51â€“63, 2019.
Thao Nguyen, Maithra Raghu, and Simon Kornblith. Do wide and deep networks learn the same things? un-
covering how neural network representations vary with width and depth. arXiv preprint arXiv:2010.15327 ,
2020.
Garrick Orchard, Ajinkya Jayawant, Gregory K Cohen, and Nitish Thakor. Converting static image datasets
to spiking neuromorphic datasets using saccades. Frontiers in neuroscience , 9:437, 2015.
Priyadarshini Panda, Sai Aparna Aketi, and Kaushik Roy. Toward scalable, efficient, and accurate deep spik-
ing neural networks with backward residual connections, stochastic softmax, and hybridization. Frontiers
in Neuroscience , 14:653, 2020.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical
correlation analysis for deep learning dynamics and interpretability. Advances in neural information
processing systems , 30, 2017.
Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision
transformers see like convolutional neural networks? Advances in Neural Information Processing Systems ,
34, 2021.
Rajesh PN Rao and Terrence J Sejnowski. Spike-timing-dependent hebbian plasticity as temporal difference
learning. Neural computation , 13(10):2221â€“2237, 2001.
Nitin Rathi and Kaushik Roy. Diet-snn: Direct input encoding with leakage and threshold optimization in
deep spiking neural networks. arXiv preprint arXiv:2008.03658 , 2020.
Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelligence with
neuromorphic computing. Nature, 575(7784):607â€“617, 2019.
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, and Michael Pfeiffer. Theory and tools for the
conversion of analog to spiking convolutional neural networks. arXiv preprint arXiv:1612.04052 , 2016.
Bodo Rueckauer, Iulia-Alexandra Lungu, Yuhuang Hu, Michael Pfeiffer, and Shih-Chii Liu. Conversion of
continuous-valued deep networks to efficient event-driven networks for image classification. Frontiers in
neuroscience , 11:682, 2017.
Statistics Student Seminar. Convergent learning: Do different neural networks learn the same representa-
tions? 2016.
Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going deeper in spiking neural
networks: Vgg and residual architectures. Frontiers in neuroscience , 13:95, 2019.
Saima Sharmin, Nitin Rathi, Priyadarshini Panda, and Kaushik Roy. Inherent adversarial robustness of
deep spiking neural networks: Effects of discrete input encoding and non-linear activations. In European
Conference on Computer Vision , pp. 399â€“414. Springer, 2020.
14Published in Transactions on Machine Learning Research (04/2023)
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
InICLR, 2015.
LeSong, AlexSmola, ArthurGretton, JustinBedo, andKarstenBorgwardt. Featureselectionviadependence
maximization. Journal of Machine Learning Research , 13(5), 2012.
Amirhossein Tavanaei, Masoud Ghodrati, Saeed Reza Kheradpisheh, TimothÃ©e Masquelier, and Anthony
Maida. Deep learning in spiking neural networks. Neural networks , 111:47â€“63, 2019.
Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John Hopcroft. Towards under-
standing learning representations: To what extent do different neural networks learn the same represen-
tation.Advances in neural information processing systems , 31, 2018.
Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE , 78
(10):1550â€“1560, 1990.
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training
high-performance spiking neural networks. Frontiers in neuroscience , 12:331, 2018.
Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, Yuan Xie, and Luping Shi. Direct training for spiking neural net-
works: Faster, larger, better. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33,
pp. 1311â€“1318, 2019.
Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo.
Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings
of the national academy of sciences , 111(23):8619â€“8624, 2014.
Wenrui Zhang and Peng Li. Temporal spike sequence learning via backpropagation for deep spiking neural
networks. arXiv preprint arXiv:2002.10085 , 2020.
Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going deeper with directly-trained larger spiking
neural networks. arXiv preprint arXiv:2011.05280 , 2020.
15Published in Transactions on Machine Learning Research (04/2023)
A Additional CKA Results
A.1 Results on VGG Networks
0 10 20 30
Spiking VGG-130102030Artificial VGG-13
0 20 40
Spiking VGG-1901020304050Artificial VGG-19
0 20 40 60
Spiking VGG-250204060Artificial VGG-25
0 25 50 75
Spiking VGG-31020406080Artificial VGG-31
0 50 100
Spiking VGG-430255075100Artificial VGG-43
0 10 20 30
Spiking VGG-130102030Artificial VGG-13
0 10 20 30
Spiking VGG-13 x20102030Artificial VGG-13 x2
0 10 20 30
Spiking VGG-13 x40102030Artificial VGG-13 x4
0 10 20 30
Spiking VGG-13 x80102030Artificial VGG-13 x8
0 10 20 30
Spiking VGG-13 x100102030Artificial VGG-13 x10
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 5 10 15 20 25 30
0.00.51.0
VGG-13
0 10 20 30 40 50
0.00.51.0
VGG-19
0 10 20 30 40 50 60 70
0.00.51.0
VGG-25
0 20 40 60 80
0.00.51.0
VGG-31
0 20 40 60 80 100 120
Layers0.00.51.0
VGG-43
0 5 10 15 20 25 30
Layers0.20.30.40.50.60.70.80.91.0
VGG-13
VGG-13 x2
VGG-13 x4VGG-13 x8
VGG-13 x10Similarity
Figure A.1: CKA heatmap between spiking VGGs and artificial VGGs with different depth and width
on CIFAR-10 dataset. Top : the CKA cross-layer heatmap across different depth from 13 layers to 43 layers.
Middle: the CKA cross-layer heatmap across different width from original channel number to 8 times. Bottom :
visualizing only the corresponding layer, which is the diagonal of the CKA heatmap.
A.1.1 Scaling up Width or Depth
In this section, we study the representation similarity between ANNs and SNNs based on VGG-series net-
works Simonyan & Zisserman (2015). Since VGG-Networks do not employ residual connections, they may
bring different representation heatmaps when compared to ResNets. We first evaluate if the spatial scaling
observation in ResNets can also be found in VGG Networks. Starting from a VGG-13, we either increase its
channel size to 10 times as before or its number of layers to 43, (detailed network configuration is provided
in Table D.2). Since VGG networks do not contain residual connections, we could scale less depth in VGG
networks than ResNets.
The results are illustrated in Fig. A.1. We can find that for deeper networks, the heatmaps tend to exhibit
a hierarchical structure, which means the shallow layers and deeper layers have different representations.
Increasing the number of layers in VGG networks to 19 or 25, the shallower and deeper layers only have 0.3
CKA similarity (purple). More seriously, when the network depth reaches 31 or 43, the similarity becomes
0.4 even across each other in deeper layers, indicating the representations are diverging. Another important
discovery for deep VGG networks is the disappearance of periodical CKA curve, which is likely due to the
lack of residual connections.
16Published in Transactions on Machine Learning Research (04/2023)
As for the wider networks, the observations are consistent with ResNet families. The wider networks have
bothbrighterheatmapsandhigherCKAcurves. Theseresultsconfirmthesimilarrepresentationinextremely
wide networks.
0 10 20 30
Spiking VGG-13 T4051015202530Artificial VGG-13
0 10 20 30
Spiking VGG-13 T8051015202530Artificial VGG-13
0 10 20 30
Spiking VGG-13 T16051015202530Artificial VGG-13
0 10 20 30
Spiking VGG-13 T32051015202530Artificial VGG-13
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 5 10 15 20 25 30
Layers0.20.30.40.50.60.70.80.91.0Similarity
VGG-13 T4
VGG-13 T8VGG-13 T16
VGG-13 T32
Figure A.2: The effect of time steps in spiking VGG networks. Left : CKA heatmaps between ANNs and
SNNs with the different number of time steps (from 4 to 32). Right: The CKA curve of corresponding layers
(diagonal values as in left).
0 20123features.0
0 20123features.3
0 20123features.7
0 20123features.10
0 20123features.14
0 20123features.17
0 20123features.21
0 20123features.24
0 20123features.27
0 50246features.0
0 50246features.3
0 50246features.7
0 50246features.10
0 50246features.14
0 50246features.17
0 50246features.21
0 50246features.24
0 50246features.27
0 200102030features.0
0 200102030features.3
0 200102030features.7
0 200102030features.10
0 200102030features.14
0 200102030features.17
0 200102030features.21
0 200102030features.24
0 200102030features.27
0.00.20.40.60.81.0
Figure A.3: The similarity across times in Spiking VGG-13. Each heatmap shows the CKA among different
time steps in the output of convolutional layers. The top/middle/bottom rows stand for spiking ResNet-20 with
4/8/32 time steps.
A.1.2 Scaling up Time Steps
We next study whether the time dimension in spiking VGG networks has a similar effect to that in spiking
ResNets. As can be found in Fig. A.2, the spiking VGG-13s with 4/8/16/32 time steps do not show a
significant difference in CKA heatmaps as well as CKA curves, which is the same with Fig. 5. Fig. A.3 shows
the CKA across different time steps in the spiking VGGs. In the first layer, the convolution does not have
dynamic representation through time. As the layer goes deeper, the dissimilarity across time steps continues
to increase, similar to Fig. 6.
17Published in Transactions on Machine Learning Research (04/2023)
A.2 Results on CIFAR-100
The results we reported in the main context are majorly based on the CIFAR-10 dataset. Here, we provide
the visualizations on the CIFAR-100 dataset to further strengthen our findings in the main context.
We first report the spatial dimension results, i.e., scaling up the width and depth of the network. Starting
from the ResNet-20, we either increase its width to 164 layers or increase its width to 8 times as before. The
visualizations are shown below. We find extremely deep networks, e.g., ResNet-164, has a very dark heatmap
compared to the ResNet-20 heatmap. The wider network shows somewhat irregular results. The extremely
wide network â€” ResNet-20 Ã—8â€” yet has even lowest similarity in the last several blocks. However, it has
the highest similarity in the output layer.
0 20 40 60
Spiking ResNet-200204060Artificial ResNet-20
0 50 100 150
Spiking ResNet-56050100150Artificial ResNet-56
0 100 200 300
Spiking ResNet-1100100200300Artificial ResNet-110
0 200 400
Spiking ResNet-1640100200300400500Artificial ResNet-164
0 20 40 60
Spiking ResNet-200204060Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 x20204060Artificial ResNet-20 x2
0 20 40 60
Spiking ResNet-20 x40204060Artificial ResNet-20 x4
0 20 40 60
Spiking ResNet-20 x80204060Artificial ResNet-20 x8
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 10 20 30 40 50 60 70
0.00.51.0
ResNet20
0 25 50 75 100 125 150 175 200
0.00.51.0
ResNet56
0 50 100 150 200 250 300 350 400
0.00.51.0
ResNet110
0 100 200 300 400 500 600
Layers0.00.51.0
ResNet164
0 10 20 30 40 50 60 70
Layers0.20.30.40.50.60.70.80.91.0
ResNet20
ResNet20 x2ResNet20 x4
ResNet20 x8Similarity
Figure A.4: CKA heatmap between SNNs and ANNs with different depth and width on CIFAR-100.
Top: the CKA cross-layer heatmap across different depth from 20 layers to 164 layers. Middle: the CKA cross-layer
heatmapacrossdifferentwidthfromoriginalchannelnumberto16times. Bottom : visualizingonlythecorresponding
layer, which is the diagonal of the CKA heatmap.
Next, we visualize the details inside a residual block. In Fig. A.5, we sub-sample the 10- thand the 34- th
residualblockinaResNet-110, whichshowsthesamephenomenon. TheLIFandconvolutionallayerscausea
decrease in similarity, while the residual addition operation restores the similarity. We also provide the CKA
heatmap of the partial residual network. As done in Fig. 4, we train 3 spiking ResNet-56 on the CIFAR-100
dataset with several blocks disabling the residual connections. Moreover, we train a linear probing layer â€”
the fully-connected classifier on top of each block to see if it contributes to the overall performance of the
whole network. The visualization is shown in Fig. A.6, where we find similar observations.
We also run experiments to test the time dimension of SNNs on CIFAR-100. In Fig. A.7, we train 4 spiking
ResNet-20 with 4/8/16/32 time steps and compute their representations with artificial ResNet-20. Both
18Published in Transactions on Machine Learning Research (04/2023)
0 50 100 150 200 250 300 350
Layers0.00.51.01.52.02.53.0Similarity64 66 68 700.000.250.500.751.0010-th Residual Block
242 244 246 2480.000.250.500.751.0034-th Residual Block
Res. Block
LIF/ReLU
Conv
(td)BN
Figure A.5: Emergence
of periodic jagged CKA
curve on CIFAR-100. We
subplot the 10- thand the 34-
thresidual blocks in ResNet-
110, which forms a periodic
jagged curve.
0 50 100 150050100150Spiking Res56 Full Residual
0 50 100 150050100150Spiking Res56 Partial Residual (1)
0 50 100 150050100150Spiking Res56 Partial Residual (2)
0 50 100 150050100150Spiking Res56 Partial Residual (3)
0 10 20
Blocks0204060Accuracy
0 10 20
Blocks0204060
0 10 20
Blocks0204060
0 10 20
Blocks0204060
0.00.20.40.60.81.0
Figure A.6: The effect of
residual connections in
theSNN. Weselectivelydis-
able residual connections in
one of three stages in the
ResNet-56. Top: the CKA
heatmap of SNN itself , con-
taining networks with dif-
ferent types of non-residual
blocks.Bottom : The lin-
ear probing accuracy of each
block.
the CKA heatmap and the CKA curve show little variations by changing the number of time steps. This
confirms the results on CIFAR-10. In addition, we visualize the CKA similarities across time steps in each
residual block. Fig. A.8 demonstrates that the first stage still produces temporal static features while the
last stage has lower similarity across time steps.
0 20 40 60
Spiking ResNet-20 T4010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T8010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T16010203040506070Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 T32010203040506070Artificial ResNet-20
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 10 20 30 40 50 60 70
Layers0.20.30.40.50.60.70.80.91.0Similarity
ResNet-20 T4
ResNet-20 T8ResNet-20 T16
ResNet-20 T32
Figure A.7: The effect of time steps in SNNs. Left : CKA heatmaps between ANNs and SNNs with the different
number of time steps. Right: The CKA curve of corresponding layers (diagonal values as in left).
19Published in Transactions on Machine Learning Research (04/2023)
0 20123s1.b0
0 20123s1.b1
0 20123s1.b2
0 20123s2.b0
0 20123s2.b1
0 20123s2.b2
0 20123s3.b0
0 20123s3.b1
0 20123s3.b2
0 50246s1.b0
0 50246s1.b1
0 50246s1.b2
0 50246s2.b0
0 50246s2.b1
0 50246s2.b2
0 50246s3.b0
0 50246s3.b1
0 50246s3.b2
0 200102030s1.b0
0 200102030s1.b1
0 200102030s1.b2
0 200102030s2.b0
0 200102030s2.b1
0 200102030s2.b2
0 200102030s3.b0
0 200102030s3.b1
0 200102030s3.b2
0.00.20.40.60.81.0
Figure A.8: The similarity across times in SNN on CIFAR-100. Each heatmap shows the CKA among
different time steps in the output of residual block. â€œsâ€ means stage, and â€œbâ€ means block. The top/middle/bottom
rows stand for spiking ResNet-20 with 4/8/32 time steps.
Finally, we rerun the adversarial robustness experiments on CIFAR-100. Here, we train two ResNet-20 with
4 times more channels and use PGD attack to measure the robustness against adversarial attack. Also, we
plot the CKA curve between the feature of clean images and the feature of corrupted images. The results
are shown in Fig. A.9.
0.0 0.001 0.005 0.01 0.02 0.05
010203040506070Accuracy74.1272.39
68.2869.25
45.1556.98
22.2340.78
5.4915.49
0.390.48Robustness of ANN and SNN
ANN SNN
010 20 30 40 50 60 70
Layers0.00.20.40.60.81.0SimilaritySimilarity of ANN
=0.001
=0.005
=0.01
=0.02
=0.05
010 20 30 40 50 60 70
Layers0.00.20.40.60.81.0SimilaritySimilarity of SNN
=0.001
=0.005
=0.01
=0.02
=0.05
Figure A.9: The robustness against adversarial attack on CIFAR-100. Left : The accuracy of SNN and ANN
after attack under different Ïµ.Right: The CKA curve between clean images and adversarial images of ANN and
SNN, respectively.
A.3 Results on CIFAR10-DVS
In this section, we conduct a representation similarity analysis on an event-based datasetâ€”CIFAR10-DVS Li
et al. (2017). As aforementioned in Fig. 7, the event dataset may generate different CKA heatmaps and
curves when compared to the static dataset. Here, we first scale up the spatial dimensions in SNNs and
ANNs for the CIFAR10-DVS dataset. As shown in Fig. A.10, we gradually increase either the depth to 110
layers or the width to 8 times as before.
Increasing the depth of ResNets on CIFAR10-DVS demonstrates a similar effect when compared to static
CIFAR-10. Interestingly, the deep ResNet, for example, ResNet-110, emerges a similar periodical pattern to
that on static CIFAR-10. However, we can find the peak CKA value is only around 0.75, (recall that the
20Published in Transactions on Machine Learning Research (04/2023)
peak CKA value on the CIFAR10 dataset is nearly 0.95, cf. Fig. 2), indicating the CIFAR10-DVS creates
higher ANN-SNN difference in representations.
0 20 40 60
Spiking ResNet-200204060Artificial ResNet-20
0 50 100
Spiking ResNet-380255075100125Artificial ResNet-38
0 50 100 150
Spiking ResNet-56050100150Artificial ResNet-56
0 100 200 300
Spiking ResNet-1100100200300Artificial ResNet-110
0 20 40 60
Spiking ResNet-200204060Artificial ResNet-20
0 20 40 60
Spiking ResNet-20 x20204060Artificial ResNet-20 x2
0 20 40 60
Spiking ResNet-20 x40204060Artificial ResNet-20 x4
0 20 40 60
Spiking ResNet-20 x80204060Artificial ResNet-20 x8
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 10 20 30 40 50 60 70
0.00.51.0
ResNet20
0 20 40 60 80 100 120 140
0.00.51.0
ResNet38
0 25 50 75 100 125 150 175 200
0.00.51.0
ResNet56
0 50 100 150 200 250 300 350 400
Layers0.00.51.0
ResNet110
0 10 20 30 40 50 60 70
Layers0.20.30.40.50.60.70.80.91.0
ResNet20
ResNet20 x2ResNet20 x4
ResNet20 x8Similarity
Figure A.10: CKA heatmap between spiking and artificial ResNets with different depth and width on
CIFAR10-DVS dataset. Top : the CKA cross-layer heatmap across different depths from 20 layers to 110 layers.
Middle: the CKA cross-layer heatmap across different widths from the original channel number to 8 times. Bottom :
visualizing only the corresponding layer, which is the diagonal of the CKA heatmap.
0 3 6 90369conv1
0 3 6 90369s1.b0.conv1
0 3 6 90369s1.b0.conv2
0 3 6 90369s1.b1.conv1
0 3 6 90369s1.b1.conv2
0 3 6 90369s1.b2.conv1
0 3 6 90369s1.b2.conv2
0 3 6 90369s2.b0.conv1
0 3 6 90369s2.b0.conv2
0 3 6 90369s2.b0.ds.1
0 3 6 90369s2.b1.conv1
0 3 6 90369s2.b1.conv2
0 3 6 90369s2.b2.conv1
0 3 6 90369s2.b2.conv2
0 3 6 90369s3.b0.conv1
0 3 6 90369s3.b0.conv2
0 3 6 90369s3.b0.ds.1
0 3 6 90369s3.b1.conv1
0 3 6 90369s3.b1.conv2
0 3 6 90369s3.b2.conv1
0 3 6 90369s3.b2.conv2
0 3 6 90369fc
0.00.20.40.60.81.0
Figure A.11: The similarity across times in SNN on CIFAR10-DVS. Each heatmap shows the CKA among
different time steps in the output of residual block. â€œsâ€ means stage, and â€œbâ€ means block. The model is trained
with 10 time steps, i.e., 10 frames integrated for each CIFAR10-DVS data input.
For the extremely wide neural network, the trend holds for ResNet-20, ResNet-20 ( Ã—2), and ResNet-20
(Ã—4). However, we find the ResNet-20 (Ã—8)exhibits a significantly low CKA curve than other models. The
difference may come from the artificial ResNet-20 (Ã—8). According to the CKA heatmap, the 35th âˆ’60th
layers in artificial ResNet-20 (Ã—8)become much darker than other heatmaps. We hypothesize that, with a
21Published in Transactions on Machine Learning Research (04/2023)
larger capacity of the neural network on the layer dynamics in the input data, the representation may change
significantly.
We next study the internal CKA of each convolution in ResNet-20, which reveals the temporal dynamics
of each convolution. As illustrated in Fig. A.11, the first convolution demonstrates dynamic features across
10 time steps, which shows different characteristics with static dataset (cf. Fig. A.12). Another notable
difference is that the first and the second time step show very low similarity with other time steps. In
summary, the rich temporal information dataset may increase the dynamics in SNNs across time steps and
bring more differences when compared to ANNs.
A.4 Similarity Across Time
In addition to the residual block, we also visualize the CKA heatmaps of convolutional layers and ReLU/LIF
layers by comparing the similarity among different time steps. As can be seen in Fig. A.12, different from
residual blocks, the similarity in convolutional and activation layers is more dynamic. Even in the first stage,
the convolutional layers show different outputs across different time steps. This result further confirms our
observations in residual blocks, where we found the convolutional and activation layers always decrease the
ANN-SNN similarity while the residual block restores the similarity. Therefore, it suggests that a more
temporal dynamic CKA heatmap may produce distinct features.
B Numerical Results
Here, we provide the clean accuracy of our trained models, both on CIFAR-10 and CIFAR-100. All models
are trained with 300 epochs of stochastic gradient descent. The learning rate is set to 0.1 followed by a cosine
annealing decay. The weight decay is set to 0.0001 for all models. The original ResNet-20 is a 3-stage model,
each stage contains 2 residual blocks. The first stage contains 16 channels and the channels are doubled
every time when entering the next stage. ResNet-38/56/110/164 contains 6/9/18/27 residual blocks in each
stage. The wider networks just simply multiply all the channels by a fixed factor. We provide their top-1
accuracy in Table B.1.
Table B.1: The top-1 accuracy of SNNs and ANNs on CIFAR-10 (aka C10) and CIFAR-100 (aka C100) datasets.
Layers Width Factor Time Steps ANN (C10) SNN (C10) ANN (C100) SNN (C100)
20 1 4 91.06 89.67 64.23 61.51
38 1 4 92.34 91.14 N/A N/A
56 1 4 92.98 91.94 68.39 66.63
110 1 4 92.37 91.83 69.20 66.95
164 1 4 93.00 92.05 70.27 67.09
20 2 4 93.36 92.00 70.14 68.65
20 4 4 94.52 93.96 74.12 72.39
20 8 4 94.78 94.48 76.57 75.81
20 16 4 94.78 94.73 N/A N/A
20 1 8
91.0690.44
64.2363.03
20 1 16 90.98 64.34
20 1 32 90.99 64.33
20 1 64 91.08 N/A
C The Effect of Network Initialization
In this section, we study the effect of network initialization and verify if our findings are invariant to different
initialization. Specifically, we train spiking and artificial ResNet-20s with two random seeds and measure the
CKAsimilarityonthese4models. Fig.C.1showsthecaseofCKAheatmapsonthesamemodel: thefirsttwo
columns demonstrate the CKA heatmaps of the same initialization and the third column demonstrates the
22Published in Transactions on Machine Learning Research (04/2023)
CKAheatmapsofdifferentinitialization. WecanfindthatSNNsorANNstrainedwithdifferentinitialization
share a high CKA similarity. Moreover, in Fig. C.2, we compare the CKA between SNNs and ANNs by
permuting two different initialization. It can be seen that all four permutations have a similar distribution
of CKA values. By comparing Fig. C.2 and Fig. C.1, we also find that the CKA heatmaps between ANNs
and SNNs are also similar to the CKA heatmaps on the same model with two initializations. This indicates
that the similarity between ANN and SNN is quite high.
D Network Architecture Details
Table D.1: The architecture details of ResNets.
20 layers 38 layers 56 layers 110 layers 164 layers
conv1 3Ã—3,16,s1 3Ã—3,16,s1 3Ã—3,16,s1 3Ã—3,16,s1 3Ã—3,16,s1
block1/parenleftbigg3Ã—3,16
3Ã—3,16/parenrightbigg
Ã—3/parenleftbigg3Ã—3,16
3Ã—3,16/parenrightbigg
Ã—6/parenleftbigg3Ã—3,16
3Ã—3,16/parenrightbigg
Ã—9/parenleftbigg3Ã—3,16
3Ã—3,16/parenrightbigg
Ã—18/parenleftbigg3Ã—3,16
3Ã—3,16/parenrightbigg
Ã—27
block2/parenleftbigg3Ã—3,32
3Ã—3,32/parenrightbigg
Ã—3/parenleftbigg3Ã—3,32
3Ã—3,32/parenrightbigg
Ã—6/parenleftbigg3Ã—3,32
3Ã—3,32/parenrightbigg
Ã—9/parenleftbigg3Ã—3,32
3Ã—3,32/parenrightbigg
Ã—18/parenleftbigg3Ã—3,32
3Ã—3,32/parenrightbigg
Ã—27
block3/parenleftbigg3Ã—3,64
3Ã—3,64/parenrightbigg
Ã—3/parenleftbigg3Ã—3,64
3Ã—3,64/parenrightbigg
Ã—6/parenleftbigg3Ã—3,64
3Ã—3,64/parenrightbigg
Ã—9/parenleftbigg3Ã—3,64
3Ã—3,64/parenrightbigg
Ã—18/parenleftbigg3Ã—3,64
3Ã—3,64/parenrightbigg
Ã—27
pooling Global average pooling
classifier 10-d fully connected layer, softmax
Table D.2: The architecture details of VGG networks.
13 layers 19 layers 25 layers 31 layers 43 layers
block1 (3Ã—3,64)Ã—1 (3Ã—3,64)Ã—2 (3Ã—3,64)Ã—2 (3Ã—3,64)Ã—2 (3Ã—3,64)Ã—2
pooling1 Average pooling, s2
block2 (3Ã—3,128)Ã—1(3Ã—3,128)Ã—2(3Ã—3,128)Ã—3 (3Ã—3,128)Ã—4 (3Ã—3,128)Ã—5
pooling2 Average pooling, s2
block3 (3Ã—3,256)Ã—2(3Ã—3,256)Ã—3(3Ã—3,256)Ã—4 (3Ã—3,256)Ã—5 (3Ã—3,256)Ã—6
pooling3 Average pooling, s2
block4 (3Ã—3,512)Ã—4(3Ã—3,512)Ã—8(3Ã—3,512)Ã—12 (3Ã—3,512)Ã—16 (3Ã—3,512)Ã—24
pooling Global average pooling
classifier 10-d fully connected layer, softmax
23Published in Transactions on Machine Learning Research (04/2023)
0 20123conv1
0 20123s1.b0.conv1
0 20123s1.b0.conv2
0 20123s1.b1.conv1
0 20123s1.b1.conv2
0 20123s1.b2.conv1
0 20123s1.b2.conv2
0 20123s2.b0.conv1
0 20123s2.b0.conv2
0 20123s2.b0.ds.1
0 20123s2.b1.conv1
0 20123s2.b1.conv2
0 20123s2.b2.conv1
0 20123s2.b2.conv2
0 20123s3.b0.conv1
0 20123s3.b0.conv2
0 20123s3.b0.ds.1
0 20123s3.b1.conv1
0 20123s3.b1.conv2
0 20123s3.b2.conv1
0 20123s3.b2.conv2
0 20123fc
0 50246conv1
0 50246s1.b0.conv1
0 50246s1.b0.conv2
0 50246s1.b1.conv1
0 50246s1.b1.conv2
0 50246s1.b2.conv1
0 50246s1.b2.conv2
0 50246s2.b0.conv1
0 50246s2.b0.conv2
0 50246s2.b0.ds.1
0 50246s2.b1.conv1
0 50246s2.b1.conv2
0 50246s2.b2.conv1
0 50246s2.b2.conv2
0 50246s3.b0.conv1
0 50246s3.b0.conv2
0 50246s3.b0.ds.1
0 50246s3.b1.conv1
0 50246s3.b1.conv2
0 50246s3.b2.conv1
0 50246s3.b2.conv2
0 50246fc
0 200102030conv1
0 200102030s1.b0.conv1
0 200102030s1.b0.conv2
0 200102030s1.b1.conv1
0 200102030s1.b1.conv2
0 200102030s1.b2.conv1
0 200102030s1.b2.conv2
0 200102030s2.b0.conv1
0 200102030s2.b0.conv2
0 200102030s2.b0.ds.1
0 200102030s2.b1.conv1
0 200102030s2.b1.conv2
0 200102030s2.b2.conv1
0 200102030s2.b2.conv2
0 200102030s3.b0.conv1
0 200102030s3.b0.conv2
0 200102030s3.b0.ds.1
0 200102030s3.b1.conv1
0 200102030s3.b1.conv2
0 200102030s3.b2.conv1
0 200102030s3.b2.conv2
0 200102030fc
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0 20123s1.b0.relu1
0 20123s1.b0.relu2
0 20123s1.b1.relu1
0 20123s1.b1.relu2
0 20123s1.b2.relu1
0 20123s1.b2.relu2
0 20123s2.b0.relu1
0 20123s2.b0.relu2
0 20123s2.b1.relu1
0 20123s2.b1.relu2
0 20123s2.b2.relu1
0 20123s2.b2.relu2
0 20123s3.b0.relu1
0 20123s3.b0.relu2
0 20123s3.b1.relu1
0 20123s3.b1.relu2
0 20123s3.b2.relu1
0 20123s3.b2.relu2
0 50246s1.b0.relu1
0 50246s1.b0.relu2
0 50246s1.b1.relu1
0 50246s1.b1.relu2
0 50246s1.b2.relu1
0 50246s1.b2.relu2
0 50246s2.b0.relu1
0 50246s2.b0.relu2
0 50246s2.b1.relu1
0 50246s2.b1.relu2
0 50246s2.b2.relu1
0 50246s2.b2.relu2
0 50246s3.b0.relu1
0 50246s3.b0.relu2
0 50246s3.b1.relu1
0 50246s3.b1.relu2
0 50246s3.b2.relu1
0 50246s3.b2.relu2
0 200102030s1.b0.relu1
0 200102030s1.b0.relu2
0 200102030s1.b1.relu1
0 200102030s1.b1.relu2
0 200102030s1.b2.relu1
0 200102030s1.b2.relu2
0 200102030s2.b0.relu1
0 200102030s2.b0.relu2
0 200102030s2.b1.relu1
0 200102030s2.b1.relu2
0 200102030s2.b2.relu1
0 200102030s2.b2.relu2
0 200102030s3.b0.relu1
0 200102030s3.b0.relu2
0 200102030s3.b1.relu1
0 200102030s3.b1.relu2
0 200102030s3.b2.relu1
0 200102030s3.b2.relu2
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.00.20.40.60.81.0
Figure A.12: The effect of time steps in convolutional and activation layers of SNNs.
24Published in Transactions on Machine Learning Research (04/2023)
0 20 40 60
Artificial ResNet20 init00204060Artificial ResNet20 init0
0 20 40 60
Artificial ResNet20 init10204060Artificial ResNet20 init1
0 20 40 60
Artificial ResNet20 init00204060Artificial ResNet20 init1
0.00.20.40.60.81.0
0 20 40 60
Spiking ResNet20 init00204060Spiking ResNet20 init0
0 20 40 60
Spiking ResNet20 init10204060Spiking ResNet20 init1
0 20 40 60
Spiking ResNet20 init00204060Spiking ResNet20 init1
0.00.20.40.60.81.0
Figure C.1: The similarity across different initialization on SNNs and ANNs. We compare the CKA
heatmap on the same model (optionally initialized with a different random seed).
0 20 40 60
Spiking ResNet20 init00204060Artificial ResNet20 init0
0 20 40 60
Spiking ResNet20 init10204060Artificial ResNet20 init1
0 20 40 60
Spiking ResNet20 init00204060Artificial ResNet20 init1
0 20 40 60
Spiking ResNet20 init10204060Artificial ResNet20 init0
0.00.20.40.60.81.0
Figure C.2: The similarity between ANN and SNNs across different initialization.
25