Published in Transactions on Machine Learning Research (08/2024)
MixedNUTS: Training-Free Accuracy-Robustness Balance
via Nonlinearly Mixed Classifiers
Yatong Bai yatong_bai@berkeley.edu
University of California, Berkeley
Mo Zhou mzhou32@jhu.edu
John Hopkins University
Vishal M. Patel vpatel36@jhu.edu
John Hopkins University
Somayeh Sojoudi sojoudi@berkeley.edu
University of California, Berkeley
Reviewed on OpenReview: https: // openreview. net/ forum? id= pyD6cujUmL
Abstract
Adversarial robustness often comes at the cost of degraded accuracy, impeding real-life
applications of robust classification models. Training-based solutions for better trade-offs are
limited by incompatibilities with already-trained high-performance large models, necessitating
the exploration of training-free ensemble approaches. Observing that robust models are more
confident in correct predictions than in incorrect ones on clean and adversarial data alike, we
speculate amplifying this â€œbenign confidence propertyâ€ can reconcile accuracy and robustness
in an ensemble setting. To achieve so, we propose â€œMixedNUTSâ€, a training-free method
where the output logits of a robust classifier and a standard non-robust classifier are processed
by nonlinear transformations with only three parameters, which are optimized through an
efficient algorithm. MixedNUTS then converts the transformed logits into probabilities
and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet datasets,
experimental results with custom strong adaptive attacks demonstrate MixedNUTSâ€™s vastly
improved accuracy and near-SOTA robustness â€“ it boosts CIFAR-100 clean accuracy by 7.86
points, sacrificing merely 0.87points in robust accuracy.
1 Introduction
Neural classifiers are vulnerable to adversarial attacks, producing unexpected predictions when subject to
purposefully constructed human-imperceptible input perturbations and hence manifesting severe safety risks
(Goodfellow et al., 2015; Madry et al., 2018). Existing methods for robust deep neural networks (Madry
et al., 2018; Zhang et al., 2019) often suffer from significant accuracy penalties on clean (unattacked) data
(Tsipras et al., 2019; Zhang et al., 2019; Pang et al., 2022). As deep learning continues to form the core
of numerous products, trading clean accuracy for robustness is understandably unattractive for real-life
users and profit-driven service providers. As a result, despite the continuous development in adversarial
robustness research, robust models are rarely deployed and practical services powered by neural networks
remain non-robust (Ilyas et al., 2018; Borkar & Chen, 2021).
Tobridgethegapbetweenrobustnessresearchandapplications, researchershavestrivedtoreconcilerobustness
and accuracy (Balaji et al., 2019; Chen et al., 2021; Raghunathan et al., 2020; Rade & Moosavi-Dezfooli,
2021; Liu & Zhao, 2022; Pang et al., 2022; Cheng et al., 2022), mostly focusing on improving robust training
This work was supported by grants from ONR and NSF.
1Published in Transactions on Machine Learning Research (08/2024)
MixedProbabilityInputProcessedProbabilitiesMixedLogitsAccurate Base Classifier (ğ‘”std)Robust Base Classifier (â„rob)Argmax (One-hot)Nonlinear Transform Eq.(4)SoftmaxNaturalLogRawLogitsConvexCombination
Figure 1: Overview of the proposed MixedNUTS classifier. The nonlinear logit transformation, to be
introduced in Section 4, significantly improves the accuracy-robustness balance while only introducing three
parameters efficiently optimized with Algorithm 1.
methods. Despite some empirical success, the training-based approach faces inherent challenges. Training
robust neural networks from scratch is highly expensive. More importantly, training-based methods suffer
from performance bottlenecks. This is because the compatibility between different training schemes is unclear,
making it hard to combine multiple advancements. Additionally, it is hard to integrate robust training
techniques into rapidly improving large models, often trained or pre-trained with non-classification tasks.
To this end, an alternative training-free direction has emerged, relieving the accuracy-robustness trade-off
through an ensemble of a standard (often non-robust) model and a robust model (Bai et al., 2024b;a). This
ensemble is referred to as the mixed classifier , whereas base classifiers refers to its standard and robust
components. The mixing approach is mutually compatible with the training-based methods, and hence
should be regarded as an add-on. Unlike conventional homogeneous ensembling, where all base classifiers
share the same goal, the mixed classifier considers heterogeneous mixing, with one base classifier specializing
in the benign attack-free scenario and the other focusing on adversarial robustness. Thus, the number of base
classifiers is naturally fixed as two, in turn maintaining a high inference efficiency.
We observe that many robust base models share a benign confidence property: their correct predictions are
much more confident than incorrect ones. Verifying such a property for numerous existing models trained via
different methods (Peng et al., 2023; Pang et al., 2022; Wang et al., 2023; Debenedetti et al., 2023; Na, 2020;
Gowal et al., 2020; Liu et al., 2023; Singh et al., 2023), we speculate that strengthening this property can
improve the mixed classifiersâ€™ trade-off even without changing the base classifiersâ€™ predicted classes.
Based on this intuition, we propose MixedNUTS (Mixed neUral classifiers with Nonlinear TranSformation),
a training-free method that enlarges the robust base classifier confidence difference between correct and
incorrect predictions and thereby optimizes the mixed classifierâ€™s accuracy-robustness trade-off. MixedNUTS
applies nonlinear transformations to the accurate and robust base classifiersâ€™ logits before converting them
into probabilities used for mixing. We parameterize the transformation with only three coefficients and
design an efficient algorithm to optimize them for the best trade-off. Unlike (Bai et al., 2024b), MixedNUTS
does not modify base neural network weights or introduce additional components and is for the first time
efficiently extendable to larger datasets such as ImageNet. MixedNUTS is compatible with various pre-trained
standard and robust models and is agnostic to the base model details such as training method, defense norm
(â„“âˆ,â„“2, etc.), training data, and model architecture. Therefore, MixedNUTS can take advantage of recent
developments in accurate or robust classifiers while being general, lightweight, and convenient.
Our experiments leverage AutoAttack (Croce & Hein, 2020) and strengthened adaptive attacks (details
in Appendix B) to confirm the security of the mixed classifier and demonstrate the balanced accuracy
and robustness on datasets including CIFAR-10, CIFAR-100, and ImageNet. On CIFAR-100, MixedNUTS
improves the clean accuracy by 7.86percentage points over the state-of-the-art non-mixing robust model
while reducing robust accuracy by merely 0.87points. On ImageNet, MixedNUTS is the first robust model
to leverage even larger pre-training datasets such as ImageNet-21k. Furthermore, MixedNUTS allows for
inference-time adjustments between clean and adversarial accuracy.
2 Background and Related Work
2.1 Definitions and Notations
This paper uses Ïƒ:Rcâ†’(0,1)cto denote the standard Softmax function: for an arbitrary zâˆˆRc, theith
entry ofÏƒ(z)is defined as Ïƒ(z)i:=exp(zi)/summationtextc
j=1exp(zj), wherezidenotes the ithentry ofz. Consider the special case
2Published in Transactions on Machine Learning Research (08/2024)
90 92 94 96 98
Clean Accuracy (%)030606570AutoAttack Accuracy (%)
CIFAR-10 (/lscriptâˆ,/epsilon1=8/255)
65 70 75 80 85 90
Clean Accuracy (%)0303540AutoAttack Accuracy (%)
CIFAR-100 (/lscriptâˆ,/epsilon1=8/255)
73 76 79 82 85
Clean Accuracy (%)04545505560AutoAttack Accuracy (%)
 AS (Bai et al, 2024) does not report ImageNet performanceImageNet (/lscriptâˆ,/epsilon1=4/255)
Standalone Robust Models
Standard Model
AS (Bai et al, 2024)
MixedNUTS (ours)
AS refers to adaptive smoothing (Bai et al., 2024a), another ensemble framework. Unlike AS, MixedNUTS requires no additional training.
Figure 2: MixedNUTSâ€™s accuracy-robustness balance compared to state-of-the-art models on RobustBench.
MixedNUTS is more accurate on clean data than all standalone robust models. At the same time, MixedNUTS
achieves the second-highest robustness among all models for CIFAR-100 and ImageNet, and is the third most
robust for CIFAR-10.
ofzi= +âˆfor somei, with all other entries of zbeing less than +âˆ. We define Ïƒ(z)for such azvector to be
the basis (one-hot) vector ei. For a classifier h:Rdâ†’Rc, we use the composite function Ïƒâ—¦h:Rdâ†’[0,1]c
to denote its output probabilities and use Ïƒâ—¦hito denote the ithentry of it.
We define the notion of confidence margin of a classifier as the prediction probability gap between the top
two predicted classes:
Definition 2.1. Consider a model h:Rdâ†’Rc, an arbitrary input xâˆˆRd, and its associated predicted label
/hatwideyâˆˆ[c]. Theconfidence margin is defined as
mh(x):=Ïƒâ—¦h/hatwidey(x)âˆ’max
iÌ¸=/hatwideyÏƒâ—¦hi(x).
We consider a classifier to be ( â„“p-norm) robust at some input xâˆˆRdif it assigns the same label to all
perturbed inputs x+Î´such thatâˆ¥Î´âˆ¥pâ‰¤Ïµ, whereÏµâ‰¥0is the attack radius. We additionally introduce the
notion of the worst-case adversarial perturbation in the sense of minimizing the margin:
Definition 2.2. Consider an adversarial attack against the confidence margin mh(x), defined as
min
âˆ¥Î´âˆ¥â‰¤Ïµmh(x+Î´).
We define the optimizer of this problem, Î´â‹†
h(x), as theminimum-margin perturbation ofh(Â·)aroundx. We
further define the optimal objective value, denoted as mâ‹†
h(x), as theminimum margin ofh(Â·)aroundx.
The attack formulation considered in Definition 2.2 is highly general. Intuitively, when the minimum margin
is negative, the adversarial perturbation successfully changes the model prediction. When it is positive, the
model is robust at x, as perturbations within radius Ïµcannot change the prediction.
2.2 Related Adversarial Attacks and Defenses
While the fast gradient sign method (FGSM) and projected gradient descent (PGD) attacks could attack
and evaluate certain models (Madry et al., 2018; Goodfellow et al., 2015), they are insufficient and can fail
to attack non-robust models (Carlini & Wagner, 2017; Athalye et al., 2018b; Papernot et al., 2017). To
this end, stronger adversaries leveraging novel attack objectives, black-box attacks, and expectation over
transformation have been proposed (Gowal et al., 2019; Croce & Hein, 2020; TramÃ¨r et al., 2020). Benchmarks
based on these strong attacks, such as RobustBench (Croce et al., 2021), ARES-Bench (Liu et al., 2023), and
OODRobustBench (Li et al., 2023), aim to unify defense evaluation.
AutoAttack (Croce & Hein, 2020) is a combination of white-box and black-box attacks (Andriushchenko
et al., 2020). It is the attack algorithm of RobustBench (Croce et al., 2021), where AutoAttack-evaluated
robust models are often agreed to be trustworthy. We select AutoAttack as the main evaluator, with further
strengthening tailored to our defense.
3Published in Transactions on Machine Learning Research (08/2024)
Models aiming to be robust against adversarial attacks often incorporate adversarial training (Madry et al.,
2018;Baietal.,2022;2023), TRADES(Zhangetal.,2019), ortheirvariations. Laterworkfurtherenhancedthe
adversarial robustness by synthetic training data (Wang et al., 2023; Sehwag et al., 2022), data augmentation
(Gowal et al., 2020; Rebuffi et al., 2021; Gowal et al., 2021), improved training loss functions (Cui et al., 2023),
purposeful architectures (Peng et al., 2023), or efficient optimization (Shafahi et al., 2019). Nevertheless,
these methods still suffer from the trade-off between clean and robust accuracy.
To this end, there has been continuous interest from researchers to alleviate this trade-off (Zhang et al.,
2019; Lamb et al., 2019; Balaji et al., 2019; Chen et al., 2021; Cheng et al., 2022; Pfrommer et al., 2023;
2024). Most methods are training-based. They are therefore cumbersome to construct and cannot leverage
already-trained state-of-the-art robust or non-robust models.
2.3 Ensemble and Calibration
Model ensembles, where the outputs of multiple models are combined to produce the overall prediction, have
been explored to improve model performance (Ganaie et al., 2022) or estimate model uncertainty (Liu et al.,
2019). Ensembling has also been considered to strengthen adversarial robustness (Pang et al., 2019; Adam
& Speciel, 2020; Alam et al., 2022; Co et al., 2022). Theoretical robustness analyses of ensemble models
indicate that the robust margins, gradient diversity, and runner-up class diversity all contribute to ensemble
robustness (Petrov et al., 2023; Yang et al., 2022).
These existing works usually consider homogeneous ensemble, meaning that all base classifiers share the
same goal (better robustness). They often combine multiple robust models for incremental improvements.
In contrast, this work focuses on heterogeneous mixing, a fundamentally different paradigm. Here, the base
classifiers specialize in different data and the mixed classifier combines their advantages. Hence, we focus on
thetwo-model setting, where the role of each model is clearly defined. Specifically, the accurate base classifier
specializes in clean data and is usually non-robust, and the robust base classifier excels in adversarial data.
A parallel line of work considers mixing neural model weights (Ilharco et al., 2022; Cai et al., 2023). They
generally require all base classifiers to have the same architecture and initialization, which is more restrictive.
Model calibration often involves adjusting confidence, which aims to align a modelâ€™s confidence with its
mispredicting probability, usually via temperature scaling (Guo et al., 2017; Yu et al., 2022; Hinton et al.,
2015). While adjusting the confidence of a single model generally does not change its prediction, this is not
the case in the ensemble setting. Unlike most calibration research focusing on uncertainty, this paper adjusts
the confidence for performance.
2.4 Mixing Classifiers for Accuracy-Robust Trade-Off
Consider a classifier gstd:Rdâ†’Rc, whose predicted logits are gstd,1,...,g std,c, wheredis the input dimension
andcis the number of classes. We assume gstd(Â·)to be a standard classifier trained for high clean accuracy
(and hence may not manifest adversarial robustness). Similarly, we consider another classifier hrob:Rdâ†’Rc
and assume it to be robust against adversarial attacks. We use accurate base classifier (ABC) androbust
base classifier (RBC) to refer togstd(Â·)andhrob(Â·).
Mixing the outputs of a standard classifier and a robust classifier improves the accuracy-robustness trade-
off, and it has been shown that mixing the probabilities is more desirable than mixing the logits from
theoretical and empirical perspectives (Bai et al., 2024b;a). Here, we denote the proposed mixed model with
fmix:Rdâ†’Rc. Specifically, the ithoutput logit of the mixed model follows the formulation
fmix,i(x):= log/parenleftbig
(1âˆ’Î±)Â·Ïƒâ—¦gstd,i(x) +Î±Â·Ïƒâ—¦hrob,i(x)/parenrightbig
(1)
for alliâˆˆ[c], whereÎ±âˆˆ[1/2,1]adjusts the mixing weight1. The mixing operation is performed in the
probability space, and the natural logarithm maps the mixed probability back to the logit space without
changing the predicted class for interchangeability with existing models. If the desired output is the probability
Ïƒâ—¦fmix(Â·), the logarithm can be omitted.
1Bai et al. (2024b;a) have shown that Î±should be no smaller than 1/2forfmix (Â·)to have non-trivial robustness.
4Published in Transactions on Machine Learning Research (08/2024)
3 Base Classifier Confidence Modification
We observe that the robust base classifier hrob(Â·)often enjoys a benign confidence property: it is much more
confident in correct predictions than in mispredictions. I.e., hrob(Â·)â€™s confidence margin is much higher when
it makes correct predictions. Even if some input is subject to attack (which vastly decreases the confidence
margin of correct predictions), if it is correctly predicted, its margin is still expected to be larger than
incorrectly predicted natural examples. Section 5.2 verifies this property with multiple model examples, and
Appendix C.3 visualizes the confidence margin distributions.
As a result, when mixing the output probabilities Ïƒâ—¦hrob(Â·)andÏƒâ—¦gstd(Â·)on clean data, where gstd(Â·)is
expected to be more accurate than hrob(Â·),gstd(Â·)can correct hrob(Â·)â€™s mistake because hrob(Â·)is unconfident.
Meanwhile, when the mixed classifier is under attack and hrob(Â·)becomes much more reliable than gstd(Â·),
hrob(Â·)â€™s high confidence in correct predictions can overcome gstd(Â·)â€™s misguided outputs. Hence, even when
gstd(Â·)â€™s robust accuracy is near zero, the mixed classifier still inherits most of hrob(Â·)â€™s robustness. Combining
the above two cases, we can see that the â€œbenign confidence propertyâ€ of hrob(Â·)allows the mixed classifier
to simultaneously take advantage of gstd(Â·)â€™s high clean accuracy and hrob(Â·)â€™s adversarial robustness. As a
result, modifying and enhancing the base classifiersâ€™ confidence has vast potential to further improve the
mixed classifier.
Note that this benign confidence property is only observed on robust classifiers. Neural classifiers trained
without any robustness considerations often make highly confident mispredictions when subject to adversarial
attack. These mispredictions can be even more confident than correctly predicted unperturbed examples,
often seeing confidence margins very close to 1. As a result, gstd(Â·)does not enjoy the benign confidence
property, and its confidence property is in general detrimental to the mixture.
3.1 Accurate Base Classifier Temperature Scaling
We start with analyzing the accurate base classifier gstd(Â·), with the goal of mitigating its detrimental
confidence property. One approach to achieve this is to scale up gstd(Â·)â€™s logits before the Softmax operation.
To this end, we consider temperature scaling (Hinton et al., 2015). Specifically, we construct the temperature
scaled model gTS(T)
std (Â·), whoseithentry is
gTS(T)
std,i(x):=gstd,i(x)/T
for alli, whereTâ‰¥0is the temperature constant. To scale up the confidence, Tshould be less than 1.
To understand this operation, observe that temperature scaling increases gstd(Â·)â€™s confidence in correct clean
examples and incorrect adversarial examples simultaneously. However, because gstd(Â·)â€™s confidence under
attack is already close to 1before scaling, the increase in attacked misprediction confidence is negligible due
to the saturation of the Softmax function. Since gstd(Â·)becomes more confident on correct examples with the
mispredicting confidence almost unchanged, its detrimental confidence property is mitigated.
The extreme selection for the temperature Tis0, in which case the predicted probabilities Ïƒâ—¦gTS(0)
std(Â·)becomes
a one-hot vector corresponding to gstd(Â·)â€™s predicted class. By scaling with T= 0, the detrimental confidence
property of gstd(Â·)is completely eliminated, as a constant margin of precisely 1is enforced everywhere. Note
that we still hope to preserve the ranking of class-wise outputs gstd,i(Â·), so that we can preserve the high
accuracy of gstd(Â·). Given this requirement, eliminating gstd(Â·)â€™s detrimental confidence property by enforcing
a consistent margin is the best one can expect. Appendix D.4 verifies that T= 0produces the best empirical
effectiveness among several temperature values. Appendix B.2 discusses how our attacks circumvent the
non-differentiability resulting from using T= 0.
In addition to eliminating the detrimental confidence property of gstd(Â·), selectingT= 0also simplifies the
analysis on the robust base model hrob(Â·)by establishing a direct correlation between hrob(Â·)â€™s confidence and
the mixed classifierâ€™s correctness, thereby allowing for tractable and efficient optimization. Hence, we select
T= 0and usegTS(0)
std (Â·)as the accurate base classifier for the remaining analyses.
5Published in Transactions on Machine Learning Research (08/2024)
4 MixedNUTS â€“ Nonlinearly Mixed Classifier
In contrast to the accurate base classifier, the robust base classifier hrob(Â·)â€™s confidence property is benign.
To achieve the best accuracy-robustness trade-off with the mixed classifier, we need to augment this benign
property as much as possible. While a similar temperature scaling operation can achieve some of the desired
effects, its potential is limited by applying the same operation to confident and unconfident predictions, and
is therefore suboptimal. To this end, we extend confidence modification beyond temperature scaling (which is
linear) to allow nonlinear logit transformations. By introducing nonlinearities, we can treat low-confidence
and high-confidence examples differently, significantly amplifying hrob(Â·)â€™s benign property and thereby
considerably enhancing the mixed classifierâ€™s accuracy-robustness balance.2
4.1 Nonlinear Robust Base Classifier Transformation
We aim to build a nonlinearly mapped classifier hM
rob(Â·):=M(hrob(Â·)), whereMâˆˆM :Rcâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’Rcis a nonlinear
transformation applied to the classifier hrob(Â·)â€™s logits, andMis the set of all possible transformations. The
prediction probabilities from this transformed robust base model are then mixed with those from gTS(0)
std(Â·)to
form the mixed classifier fM
mix(Â·)following (1). For the optimal accuracy-robustness trade-off, we select an M
that maximizes the clean accuracy of fM
mix(Â·)while maintaining the desired robust accuracy. Formally, this
goal is described as the optimization problem
max
MâˆˆM, Î±âˆˆ[1/2,1]P(X,Y)âˆ¼D/bracketleftbig
arg max
ifM
mix,i(X) =Y/bracketrightbig
(2)
subject to P(X,Y)âˆ¼D/bracketleftbig
arg max
ifM
mix,i(X+Î´â‹†
fM
mix(X)) =Y/bracketrightbig
â‰¥rfM
mix,
whereDis the distribution of data-label pairs, rfM
mixis the desired robust accuracy of fM
mix(Â·), andÎ´â‹†
fM
mix(x)is
the minimum-margin perturbation of fM
mix(Â·)atx. Note that fM
mix,i(Â·)implicitly depends on MandÎ±.
The problem (2) depends on the robustness behavior of the mixed classifier, which is expensive to probe.
Ideally, the optimization should only need the base classifier properties, which can be evaluated beforehand.
To allow such a simplification, we make the following two assumptions.
Assumption 4.1. On unattacked clean data, if hM
rob(Â·)makes a correct prediction, then gstd(Â·)is also correct.
Assumption 4.1 allows us to focus on examples correctly classified by the accurate base classifier gTS(0)
std (Â·)
but not by the robust base model hM
rob(Â·)when optimizing the transformation M(Â·)to maximize the clean
accuracy of the mixed classifier. Under Assumption 4.1, we can safely discard the opposite case of gTS(0)
std (Â·)
being incorrect while hM
rob(Â·)being correct on clean data. Assumption 4.1 makes sense because gTS(0)
std (Â·)â€™s
clean accuracy should be considerably higher than hM
rob(Â·)â€™s to justify mixing them together, and training
standard classifiers that noticeably outperforms robust models on clean data is usually possible in practice.
Assumption 4.2. The transformation M(Â·)does not change the predicted class due to, e.g., monotonicity.
Namely, it holds that arg maxiM(hrob(x))i= arg maxihrob,i(x)for allx.
We make this assumption because we want the logit transformation to preserve the accuracy of hrob(Â·). While
it is mathematically possible to obtain an M(Â·)that further increases the accuracy of hrob(Â·), finding it could
be as hard as training a new improved robust model. Hence, the best one would expect from a relatively
simple nonlinear transformation is to enhance hrob(Â·)â€™s benign confidence margin property while not changing
the predicted class. Later in this paper, we will propose Algorithm 1 to find such a transformation.
These two assumptions allow us to decouple the optimization of M(Â·)from the accurate base classifier gstd(Â·).
This is because as proven in (Bai et al., 2024b, Lemma 1), the mixed classifier is guaranteed to be robust
whenhM
robis robust with margin no smaller than1âˆ’Î±
Î±(with the implicit assumption Î±â‰¥0.5. Hence, we can
solve the following problem as a surrogate for our goal formulation (2):
min
MâˆˆM, Î±âˆˆ[1/2,1]PXâˆ¼Xâœ—
clean/bracketleftbig
mhM
rob(X)â‰¥1âˆ’Î±
Î±/bracketrightbig
subject to PZâˆ¼Xâœ“
adv/bracketleftbig
mâ‹†
hM
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightbig
â‰¥Î²,(3)
2The same nonlinear logit transformation is not applied to the accurate base classifier because its confidence property is not
benign. As explained in Section 3.1, eliminating gstd(Â·)â€™s detrimental confidence property by enforcing a constant margin with
one-hot encoding is the best one can expect.
6Published in Transactions on Machine Learning Research (08/2024)
whereXâœ—
cleanis the distribution formed by clean examples incorrectly classified by hM
rob(Â·),Xâœ“
advis the
distribution formed by attacked examples correctly classified by hM
rob(Â·),X,Zare the random variables drawn
from these distributions, and Î²âˆˆ[0,1]controls the mixed classifierâ€™s desired level of robust accuracy with
respect to the robust accuracy of hrob(Â·).
Note that (3) no longer depends on gstd(Â·), allowing for replacing the standard base classifier without
re-solving for a new transformation M(Â·). The following two theorems justify approximating (2) with (3) by
characterizing the optimizers of (3):
Theorem 4.3. Suppose that Assumption 4.2holds. LetrfM
mixandrhrobdenote the robust accuracy of fM
mix(Â·)
andhrob(Â·)respectively. If Î²â‰¥rfM
mix/rhrob, then a solution to (3)is feasible for (2).
Theorem 4.4. Suppose that Assumption 4.1holds. Furthermore, consider an input random variable X
and suppose that the margin of hM
rob(X)is independent of whether gstd(X)is correct. Then, minimizing the
objective of (3)is equivalent to maximizing the objective of (2).
The proofs of Theorem 4.3 and Theorem 4.4 are provided in Appendices A.1 and A.2, respectively. In
Appendix D.8.1 and Appendix D.8.2, we discuss the minor effects of slight violations to Assumption 4.1 and
Assumption 4.2, respectively. Moreover, the independence assumption in Theorem 4.4 can be relaxed with
minor changes to our method, which we discuss in Appendix D.8.3. Also note that Theorems 4.3 and 4.4 rely
on usingT= 0forgstd(Â·)â€™s temperature scaling, justifying this temperature setting selected in Section 3.1.
4.2 Parameterizing the Transformation M
Optimizing the nonlinear transformation M(Â·)requires representing it with parameters. To avoid introducing
additional training requirements or vulnerable backdoors, the parameterization should be simple ( i.e., not
introducing yet another neural network). Thus, we introduce a manually designed transformation with only
three parameters, along with an algorithm to efficiently optimize the three parameters.
Unlike linear scaling and the Softmax operation, which are shift-agnostic ( i.e., adding a constant to all
logits does not change the predicted probabilities), the desired nonlinear transformationsâ€™ behavior heavily
depends on the numerical range of the logits. Thus, to make the nonlinear transformation controllable and
interpretable, we pre-process the logits by applying layer normalization (LN): for each input x, we standardize
the logitshrob(x)to have zero mean and identity variance. We observe that LN itself also slightly increases
the margin difference between correct and incorrect examples, favoring our overall formulation as shown in
Figure 6. This phenomenon is further explained in Appendix D.7.
Among the post-LN logits, only those associated with confidently predicted classes can be large positive
values. To take advantage of this property, we use a clamping function Clamp (Â·), such as ReLU, GELU, ELU,
or SoftPlus, to bring the logits smaller than a threshold toward zero. This clamping operation can further
suppress the confidence of small-margin predictions while preserving large-margin predictions. Since correct
examples often enjoy larger margins, the clamping function enlarges the margin gap between correct and
incorrect examples. We provide an ablation study over candidate clamping functions in Appendix D.2 and
empirically select GELU for our experiments.
Finally, since the power functions with greater-than-one exponents diminish smaller inputs while amplifying
larger ones, we exponentiate the clamping function outputs to a constant power and preserve the sign. Putting
everything together, with the introduction of three scalars s,p, andcto parameterize M(Â·), the combined
nonlinearly transformed robust base classifier hM(s,p,c)
rob (Â·)becomes
hM(s,p,c)
rob (x) =sÂ·/vextendsingle/vextendsinglehClamp(c)
rob (x)/vextendsingle/vextendsinglepÂ·sgn/parenleftbig
hClamp(c)
rob (x)/parenrightbig
wherehClamp(c)
rob (x) = Clamp/parenleftbig
LN(hrob(x)) +c/parenrightbig
.(4)
Here,sâˆˆ(0,+âˆ)is a scaling constant, pâˆˆ(0,+âˆ)is an exponent constant, and câˆˆRis a bias constant that
adjusts the cutoff location of the clamping function. With a slight abuse of notation, M(s,p,c )(Â·)denotes the
transformation parameterized with s,p, andc. In (4), we apply the absolute value before the exponentiation
to maintain compatibility with non-integer pvalues and use the sign function to preserve the sign. Note that
when the clamping function is linear and p= 1, (4) degenerates to temperature scaling with LN. Hence, an
optimal combination of s,p, andcis guaranteed to be no worse than temperature scaling.
7Published in Transactions on Machine Learning Research (08/2024)
Note that the nonlinear transformation M(s,p,c )(Â·)generally adheres to Assumption 4.2. While Assump-
tion 4.2 may be slightly violated if GELU is chosen as the clamping function due to its portion around zero
being not monotonic, its effect is empirically very small according to our observation, partly because the
negative slope is very shallow. We additionally note that the certified robustness results presented in (Bai
et al., 2024b) also apply to the nonlinearly mixed classifiers in this work.
With the accurate base classifierâ€™s temperature scaling and the robust base classifierâ€™s nonlinear logit
transformation in place, the overall formulation of MixedNUTS becomes
fM(s,p,c)
mix (x):= log/parenleftbig
(1âˆ’Î±)Â·gTS(0)
std (x) +Î±Â·hM(s,p,c)
rob (x)/parenrightbig
, (5)
as illustrated in Figure 1.
4.3 Efficient Algorithm for Optimizing s,p,c, andÎ±
With the nonlinear transformation parameterization in place, the functional-space optimization problem (3)
reduces to the following algebraic optimization formulation:
min
s,p,c,Î±âˆˆRPXâˆ¼Xâœ—
clean/bracketleftbig
mhM(s,p,c )
rob(X)â‰¥1âˆ’Î±
Î±/bracketrightbig
subject to PZâˆ¼Xâœ“
adv/bracketleftbig
mâ‹†
hM(s,p,c )
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightbig
â‰¥Î², sâ‰¥0, pâ‰¥0,1/2â‰¤Î±â‰¤1.(6)
Exactly solving (6) involves evaluating mâ‹†
hM(s,p,c )
rob(x)for everyxin the support of the distribution of correctly
predicted adversarial examples Xâœ“
adv. This is intractable because the support is a continuous set and the
distributionsXâœ—
cleanandXâœ“
advimplicitly depend on the optimization variables s,p, andc. To this end, we
approximateXâœ—
cleanandXâœ“
advwith a small set of data. Consider the subset of clean examples incorrectly
classified by hLN
rob(Â·), denoted as /tildewideXâœ—
clean, and the subset of attacked examples correctly classified by hLN
rob(Â·),
denoted as/tildewideXâœ“
adv. Because we use hLN
rob(Â·)instead ofhM(s,p,c)
rob (Â·)to obtain/tildewideXâœ—
cleanand/tildewideXâœ“
adv, using them as
surrogates toXâœ—
cleanandXâœ“
advdecouples the probability measures from the optimization variables.
Despite optimizing s,p,c, andÎ±on a small set of data, overfitting is unlikely since there are only four
parameters, and thus a small number of images should suffice. Appendix D.3 analyzes the effect of the data
subset size on optimization quality and confirms the absence of overfitting.
The minimum margin mâ‹†
hM(s,p,c )
rob(x)also depends on the optimization variables s,p,c, andÎ±, as its calculation
requires the minimum-margin perturbation for hM(s,p,c)
rob (Â·)aroundx. Since finding mâ‹†
hM(s,p,c )
rob(x)for alls,p,
andccombinations is intractable, we seek to use an approximation that does not depend on s,p, andc.
Specifically, the approximation is /tildewidemhM(s,p,c )
rob(x), defined as
/tildewidemhM(s,p,c )
rob(x):=mhM(s,p,c )
rob/parenleftbig
x+/tildewideÎ´hLN
rob(x)/parenrightbig
â‰ˆmhM(s,p,c )
rob/parenleftbig
x+Î´â‹†
hM(s,p,c )
rob(x)/parenrightbig
=mâ‹†
hM(s,p,c )
rob(x),
where/tildewideÎ´hLN
rob(x)is an empirical minimum-margin perturbation of hLN
rob(Â·)aroundxobtained from a strong
adversarial attack. Note that calculating /tildewidemhM(s,p,c )
rob(x)does not require attacking hM(s,p,c)
rob (Â·)and instead
attackshLN
rob(Â·), which is independent of the optimization variables, ensuring optimization efficiency. To obtain
/tildewidemhM(s,p,c )
rob(x), in Appendix B.1, we propose minimum-margin AutoAttack (MMAA) , an AutoAttack variant
that keeps track of the minimum margin while generating perturbations. While some components of MMAA
requirehLN
rob(Â·)â€™s gradient information, Algorithm 1 can still apply after some modifications even if the base
classifiers are black boxes with unavailable gradients, with the details discussed in Appendix D.5.
Since the probability measures and the perturbations are now both decoupled from s,p,c,Î±, we only need
to run MMAA once to estimate the worst-case perturbation, making this hyper-parameter search problem
efficiently solvable. While using hLN
rob(Â·)as a surrogate to hM(s,p,c)
rob (Â·)introduces a distribution mismatch, we
expect this mismatch to be benign. To understand this, observe that the nonlinear logit transformation (4)
generally preserves the predicted class due to the (partially) monotonic characteristics of GELU and the
sign-preserving power function. Consequently, we expect the accuracy and minimum-margin perturbations of
hM(s,p,c)
rob(Â·)to be very similar to those of hLN
rob(Â·). Appendix D.9 empirically verifies this speculated proximity.
8Published in Transactions on Machine Learning Research (08/2024)
Algorithm 1 Algorithm for optimizing s,p,c, andÎ±.
1:Given an image set, save the predicted logits associated with mispredicted clean images/braceleftbig
hLN
rob(x) :xâˆˆ/tildewideXâœ—
clean/bracerightbig
.
2:Run MMAA on hLN
rob(Â·)and save the logits of correctly classified perturbed inputs/braceleftbig
hLN
rob(x) :xâˆˆ/tildewideAâœ“
adv/bracerightbig
.
3:Initialize candidate values s1,...,sl,p1,...,pm,c1,...,cn.
4:forsifori= 1,...,l do
5:forpjforj= 1,...,m do
6: forckfork= 1,...,n do
7: Obtain mapped logits/braceleftbig
hM(si,pj,ck)
rob (x) :xâˆˆ/tildewideAâœ“
adv/bracerightbig
.
8: Calculate the margins from the mapped logits/braceleftbig
m
hM(si,pj,ck)
rob(x) :xâˆˆ/tildewideAâœ“
adv/bracerightbig
.
9: Store the bottom 1âˆ’Î²-quantile of the margins as qijk
1âˆ’Î²(corresponds to1âˆ’Î±
Î±in (7)).
10: Record the current objective oijkâ†PXâˆˆ/tildewideXâœ—
clean/bracketleftbig
m
hM(si,pj,ck)
rob(X)â‰¥qijk
1âˆ’Î²/bracketrightbig
.
11: end for
12: end for
13:end for
14:Find optimal indices (iâ‹†,jâ‹†,kâ‹†) = arg mini,j,koijk.
15:Recover optimal mixing weight Î±â‹†:=1//parenleftbig
1+qiâ‹†jâ‹†kâ‹†
1âˆ’Î²/parenrightbig.
16:returnsâ‹†:=siâ‹†,pâ‹†:=pjâ‹†,câ‹†:=ckâ‹†,Î±â‹†.
To simplify notations, let /tildewideAâœ“
adv:=/braceleftbig
x+/tildewideÎ´hLN
rob(x) :xâˆˆ/tildewideXâœ“
adv/bracerightbig
denote all correctly predicted minimum-margin
perturbed images for hLN
rob(Â·). Inherently, it holds that
PZâˆˆ/tildewideAâœ“
adv/bracketleftbig
mhM(s,p,c )
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightbig
=PZâˆˆ/tildewideXâœ“
adv/bracketleftbig
/tildewidemhM(s,p,c )
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightbig
â‰ˆPZâˆ¼Xâœ“
adv/bracketleftbig
mâ‹†
hM(s,p,c )
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightbig
.
The approximate hyper-parameter selection problem, which can be solved in surrogate to (6), is then
min
s,p,c,Î±âˆˆRPXâˆˆ/tildewideXâœ—
clean/bracketleftbig
mhM(s,p,c )
rob(X)â‰¥1âˆ’Î±
Î±/bracketrightbig
subject to PZâˆˆ/tildewideAâœ“
adv/bracketleftbig
mhM(s,p,c )
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightbig
â‰¥Î², sâ‰¥0, pâ‰¥0,1/2â‰¤Î±â‰¤1.(7)
Since (7) only has four optimization variables, it can be solved via a grid search algorithm. Furthermore, the
constraint PZâˆˆ/tildewideAâœ“
adv/bracketleftbig
mhM(s,p,c )
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightbig
â‰¥Î²should always be active at optimality.3Hence, we can treat
this constraint as equality, reducing the searching grid dimension to three. Specifically, we sweep over a
range ofs,p, andcto form the grid, and calculate the Î±value that binds the chance constraint for each
combination. Among the grid, we then select an s,p,ccombination that minimizes (7)â€™s objective.
The resulting algorithm is Algorithm 1. As discussed above, this algorithm only needs to query MMAAâ€™s
APGD components once on a small set of validation data, and all other steps are simple mathematical
operations requiring minimal computation. Additionally, note that the optimization precision of Algorithm 1
is governed by the discrete nature of the evaluation dataset. I.e., with a dataset consisting of 10,000 examples
(such as the CIFAR-10 and CIFAR-100 evaluation sets), the finest optimization accuracy one can expect
is0.01%in terms of objective value (accuracy). Hence, it is not necessary to solve (7) to a high accuracy.
Moreover, as shown in Figure 9 in Appendix D.1, which analyzes the sensitivity of the formulation (7)â€™s
objective value with respect to s,p, andc, the optimization landscape is relatively smooth. Therefore, a
relatively coarse grid (512 combinations in our case) can find a satisfactory solution, and hence Algorithm 1 is
highly efficient despite the triply nested loop structure. Furthermore, the base classifier raw logits associated
withhLN
rob(Â·)â€™s minimum-margin perturbations do not depend on s,p,cand can be cached. Hence, the number
of forward loops is agnostic to the search space size.
All of the above makes Algorithm 1 efficiently solvable. In practice, the triply-nested grid search loop can
be completed within ten seconds on a laptop computer, and performing MMAA on 1000 images requires
3752/10172 seconds for CIFAR-100/ImageNet with a single Nvidia RTX-8000 GPU.
3To understand this, suppose that for some combination of s,p,c, andÎ±, this inequality is satisfied strictly. Then, it will be
possible to decrease Î±(i.e., increase1âˆ’Î±
Î±) without violating this constraint, and thereby further reduce the objective value.
9Published in Transactions on Machine Learning Research (08/2024)
Class A         B CCase 1
Large margin1.0
-1.1-1.0Raw
Logits
Class A         B C0.8
0.097 0.11Margin:
0.698Raw
Probabilities
Class A         B C1.0
0.023 0.023Margin:
0.93 (â†‘)Transformed
Probabilities
Class A         B CCase 2
Small margin-1.01.11.0
Class A         B C0.060.490.45Margin: 0.047
Class A         B C0.33 0.33 0.33Margin: 0.0013 (â†“)
Figure 3: The raw logits, the corresponding prediction
probabilities, and the probabilities computed with the
transformed logits. Our transformation augments the
confidence margin difference between the two scenarios.
Raw
ProbabilitiesA
BCClassClassClassT=3
A
BCT=0.06
A
BCT=0.003Transformed
ProbabilitiesA
BCA
BCA
BCCase 1
Case 2
Figure 4: Probability trajectories on the probabil-
ity simplex formed by temperature scaling, with
or without the logit transformation. The transfor-
mation reduces confidence when classes compete.
4.4 Visualization of the Nonlinear Logit Transformation M(s,p,c )
To better understand the effects of the proposed nonlinear logit transformation M(s,p,c )(Â·), we visualize how
it affects the base classifier prediction probabilities when coupled with the Softmax operation. Consider a
three-class (A, B, and C) classification problem, with two example logit vectors. The first example simulates
the case where class A is clearly preferred over the rest (large margin), while the second example illustrates
a competition between classes B and C (small margin). The raw logits, the corresponding prediction
probabilities, and the probabilities computed with the transformed logits are visualized in Figure 3. Clearly,
the margin is further increased for the large margin case and shrunk for the small margin case, which aligns
with the goal of enlarging the benign confidence property of the base classifiers.
For further demonstration, we adjust the overall confidence level for the above two cases and compare how
their prediction probabilities change with the confidence level. Specifically, by applying temperature scaling
and varying the temperature Ï„, the prediction probability vectors form trajectories on the probability simplex,
whose vertices represent the classes.4For example, a small temperature Ï„increases the overall prediction
confidence, moving the vector toward a vertex. Conversely, a large temperature Ï„attracts the prediction
probability to the simplexâ€™s centroid. By continuously adjusting the temperature, we obtain trajectories that
connect the centroid to the vertices. By comparing the trajectories formed with or without the nonlinear logit
transformation ( Ïƒ(hrob(Â·)/Ï„)andÏƒ(hM(s,p,c )
rob(Â·)/Ï„)), we can better understand the transformationâ€™s properties.
Figure4showsthepredictionprobabilityvectorsatthreeexampletemperaturevalues, aswellasthetrajectories
formed by continuously varying the temperature. We observe that the nonlinear logit transformation
significantly slows down the movement of the small margin case from the centroid to the vertex. Moreover, the
trajectory with the transformation is straighter and further from the edge BC, implying that the competition
between classes B and C has been reduced. In the context of mixed classifiers, the nonlinear transformation
reduces the robust base classifierâ€™s relative authority in the mixture when it encounters competing classes,
thereby improving the mixed classifierâ€™s accuracy-robustness trade-off.
5 Experiments
We use extensive experiments to demonstrate the accuracy-robustness balance of the MixedNUTS classifier
fM(sâ‹†,pâ‹†,câ‹†)
mix (Â·), focusing on the effectiveness of the nonlinear logit transformation. Our evaluation uses CIFAR-
10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), and ImageNet (Deng et al., 2009) datasets. For
4Here, the purpose of temperature scaling is different from Section 3.1. In Section 3.1, temperature scaling mitigates gstd(Â·)â€™s
detrimental confidence property. Here, scaling with variable temperatures generates probability trajectories for visualization.
10Published in Transactions on Machine Learning Research (08/2024)
Clean AccuracyRobust Accuracy (AutoAttack)ABC
Clean: 98.50%
Robust: 0.00%RBC
Clean: 93.27%
Robust: 71.07%
Mixed
Clean: 94.51%
Robust: 68.64%MixedNUTS
(ours)
Clean: 95.19%
Robust: 69.71%CIFAR-10 (/lscriptâˆ,/epsilon1=8/255)
Clean AccuracyABC
Clean: 91.38%
Robust: 0.00%RBC
Clean: 75.22%
Robust: 42.67%
Mixed
Clean: 78.93%
Robust: 40.13%MixedNUTS
(ours)
Clean: 83.08%
Robust: 41.80%CIFAR-100 (/lscriptâˆ,/epsilon1=8/255)
Clean AccuracyABC
Clean: 86.18%
Robust: 0.00%RBC
Clean: 78.92%
Robust: 59.56%
Mixed
Clean: 80.82%
Robust: 55.66%MixedNUTS
(ours)
Clean: 81.48%
Robust: 58.50%
Uses 5000 validation images
as speciï¬ed in RobustBenchImageNet (/lscriptâˆ,/epsilon1=4/255)
Robust Base
Classiï¬er (RBC)
Accurate Base
Classiï¬er (ABC)
Mixed
(Bai et al, 2023b)
MixedNUTS
(ours)
Figure 5: MixedNUTS balances the robustness from its robust base classifier and the accuracy from its
standard base classifier. The nonlinear logit transformation helps MixedNUTS achieve a much better
accuracy-robust trade-off than a baseline mixed model without transformation. Appendix C.1 reports the
base model details and the optimal s,p,c,Î±values.
(Â·) gstd(Â·) hrob(Â·) hLN
robhM(sâ‹†,pâ‹†,câ‹†)
rob(Â·)0.00.20.40.60.81.0Conï¬dence Margin.534
.386.986CIFAR-10 (/lscriptâˆ,/epsilon1=8/255)
(Â·) gstd(Â·) hrob(Â·) hLN
robhM(sâ‹†,pâ‹†,câ‹†)
rob(Â·)0.00.20.40.60.81.0
.376.481.973CIFAR-100 (/lscriptâˆ,/epsilon1=8/255)
(Â·) gstd(Â·) hrob(Â·) hLN
robhM(sâ‹†,pâ‹†,câ‹†)
rob(Â·)0.00.20.40.60.81.0
.525.837.980ImageNet (/lscriptâˆ,/epsilon1=4/255)Clean
Correct
Clean
Incorrect
AutoAttacked
Correct
AutoAttacked
Incorrect
Margin Gap
(higher is better).xxx
Figure 6: The median confidence margin of the accurate/robust base classifier gstd(Â·)/hrob(Â·), the layer-normed
logitshLN
rob(Â·), and the nonlinearly transformed model hM(sâ‹†,pâ‹†,câ‹†)
rob(Â·)on clean and AutoAttacked data, grouped
by prediction correctness. The number above each bar group is the â€œmargin gapâ€, defined as the difference
between the medians on clean incorrect inputs and AutoAttacked correct ones. A higher margin gap signals
more benign confidence property, and thus better accuracy-robustness trade-off for the mixed classifier.
Table 1: MixedNUTSâ€™s error rate changes relative to
the robust base classifier (more negative is better).
Clean (â†“)Robust (AutoAttack) (â†“)
CIFAR-10âˆ’28.53% +4.70%
CIFAR-100âˆ’31.72% +1.52%
ImageNetâˆ’12.14% +2.62%each dataset, we select the model with the highest ro-
bust accuracy verified on RobustBench (Croce et al.,
2021) as the robust base classifier hrob(Â·), and se-
lect a state-of-the-art standard (non-robust) model
enhanced with extra training data as the accurate
base classifier gstd(Â·). Detailed model information is
reported in Appendix C.1.
As an ensemble method, in addition to being training-
free, MixedNUTS is also highly efficient during infer-
ence time. Compared with a state-of-the-art robust classifier, MixedNUTSâ€™s increase in inference FLOPs is as
low as 24.79%. A detailed comparison and discussion on inference efficiency can be found in Appendix C.2.
All mixed classifiers are evaluated with strengthened adaptive AutoAttack algorithms specialized in attacking
MixedNUTS and do not manifest gradient obfuscation issues, with the details explained in Appendix B.2.
5.1 Main Experiment Results
Figure 5 compares MixedNUTS with its robust base classifier, its accurate base classifier, and the baseline
method Mixed (Bai et al., 2024b) on three datasets. Specifically, Mixed is a mixed classifier without the
nonlinear logit transformations. Figure 5 shows that MixedNUTS consistently achieves higher clean accuracy
and better robustness than this baseline, confirming that the proposed logit transformations mitigate the
overall accuracy-robustness trade-off.
Table 1 compares MixedNUTSâ€™s relative error rate change over its robust base classifier, showing that
MixedNUTS vastly reduces the clean error rate with only a slight robust error rate increase. Specifically, the
11Published in Transactions on Machine Learning Research (08/2024)
Peng et al. (2023)
RaWRN-70-1693.5% / 71.4%Pang et al. (2022)
WRN-70-1689.1% / 62.4%Debenedetti et al. (2022)
XCiT-L12 (Extra data)92.8% / 58.0%Na et al. (2020)
ResNet-1885.4% / 40.1%0.00.20.40.60.81.0Conï¬dence Margin.395
.229.192.179CIFAR-10 (/lscriptâˆ)
Wang et al. (2023)
WRN-70-1695.8% / 85.0%.486CIFAR-10 (/lscript2)
Wang et al. (2023)
WRN-70-1673.6% / 42.9%Gowal et al. (2020)
WRN-70-16 (Extra data)69.3% / 35.0%.480.575CIFAR-100 ( /lscriptâˆ)
Liu et al. (2023)
Swin-L77.8% / 57.5%Singh et al. (2023)
ConvNeXt-L74.5% / 57.6%.710.760ImageNet (/lscriptâˆ)Clean
Correct
Clean
Incorrect
AutoAttacked
Correct
AutoAttacked
Incorrect
Margin Gap
(higher is better).xxx
Figure 7: The median confidence margin of a diverse set of robust models with the logits standardized via
layer normalization. All models enjoy higher margins on correct predictions than on incorrect ones for clean
and adversarial inputs alike. The percentages below each model name are the clean/AutoAttack accuracy.
relative clean error rate improvement is 6 to 21 times more prominent than the relative robust error rate
increase. Clearly, MixedNUTS balances accuracy and robustness without additional training.
Figure 6 compares the robust base classifierâ€™s confidence margins on clean and attacked data with or without
our nonlinear logit transformation (4). For each dataset, the transformation enlarges the margin gap
between correct and incorrect predictions, especially in terms of the median which represents the margin
majority. Using hM(sâ‹†,pâ‹†,câ‹†)
rob (Â·)instead ofhrob(Â·)makes correct predictions more confident while keeping the
mispredictions less confident, making the mixed classifier more accurate without losing robustness.
Figure 2 compares MixedNUTS with existing methods with the highest AutoAttack-validated adversarial
robustnesses, confirming that MixedNUTS noticeably improves clean accuracy while maintaining competitive
robustness. Moreover, since MixedNUTS can use existing or future improved accurate or even robust models
as base classifiers, the entries of Figure 2 should not be regarded as pure competitors.
Existing models suffer from the most pronounced accuracy-robustness trade-off on CIFAR-100, where
MixedNUTS offers the most prominent improvement. MixedNUTS boosts the clean accuracy by 7.86
percentage points over the state-of-the-art non-mixing robust model while reducing merely 0.87points in
robust accuracy. In comparison, the previous mixing method (Bai et al., 2024a) sacrifices 3.95points of
robustness (4.5x MixedNUTSâ€™s degradation) for a 9.99-point clean accuracy bump using the same base
models. Moreover, (Bai et al., 2024a) requires training an additional mixing network component, whereas
MixedNUTS is training-free (MixedNUTS is also compatible with the mixing network for even better results).
Clearly, MixedNUTS utilizes the robustness of hrob(Â·)more effectively and efficiently.
On CIFAR-10 and ImageNet, achieving robustness against common attack budgets penalizes the clean
accuracy less severely than on CIFAR-100. Nonetheless, MixedNUTS is still effective in these less suitable
cases, reducing the clean error rate by 28.53%/12.14%(relative) while only sacrificing 1.91%/0.98%(relative)
robust accuracy on CIFAR-10/ImageNet compared to non-mixing methods. On CIFAR-10, MixedNUTS
matches (Bai et al., 2024a)â€™s clean accuracy while reducing the robust error rate by 5.17%(relative).
With the nonlinear transformation in place, it is still possible to adjust the emphasis between clean and
robust accuracy at inference time. This can be achieved by simply re-running Algorithm 1 with a different Î²
value. Note that the MMAA step in Algorithm 1 does not depend on Î², and hence can be cached to speed
up re-runs. Meanwhile, the computational cost of the rest of Algorithm 1 is marginal. Our experiments use
Î²= 98.5%for CIFAR-10 and -100, and use Î²= 99.0%for ImageNet. The optimal s,p,cvalues and the
searching grid used in Algorithm 1 are discussed in Appendix C.1.
5.2 Confidence Properties of Various Robust Models
MixedNUTS is built upon the observation that robust models are more confident in correct predictions than
incorrect ones. Figure 7 confirms this property across existing models with diverse structures trained with
different loss functions across various datasets, and hence MixedNUTS is applicable for a wide range of base
12Published in Transactions on Machine Learning Research (08/2024)
84 86 88 90 92 94 96
Clean Accuracy (%)020404550Robust Accuracy (%)
MixedNUTS (ours)
Mixed
TRADES
TRADES (larger)
Figure 8: Accuracy-robustness trade-off compari-
son between MixedNUTS, mixed classifier without
nonlinear transformation, and TRADES on 1000
CIFAR-10 images. TRADES (larger) denotes a
larger TRADES model trained from scratch that
has the same size as MixedNUTS.classifier combinations. For a fair confidence margin
comparison, all logits are standardized to zero mean
and identity variance (corresponding to hLN
rob(Â·)) before
converted into probabilities. Appendix C.3 presents
histograms to offer more margin distribution details.
Figure 7 illustrates that existing CIFAR-10 and -100
models have tiny confidence margins for mispredictions
and moderate margins for correct predictions, implying
that most mispredictions have a close runner-up class.
ImageNet classifiers also have higher confidence in cor-
rect predictions than incorrect ones. However, while
the logits are always standardized before Softmax, the
ImageNet models have higher overall margins than the
CIFAR ones. This observation indicates that ImageNet
models often do not have a strong confounding class
despite having more classes, and their non-predicted
classesâ€™ probabilities spread more evenly.
5.3 Accuracy-Robustness Trade-Off Curves
In Figure 8, we show MixedNUTSâ€™s robust accuracy as a function of its clean accuracy. We then compare this
accuracy-robustness trade-off curve with that of the mixed classifier without nonlinear logit transformation
(Bai et al., 2024b) and that of TRADES (Zhang et al., 2019), a popular adjustable method that aims to
improve the trade-off. Note that adjusting between accuracy and robustness with TRADES requires tuning
its training loss hyper-parameter Î²TRand training a new model, whereas the mixed classifiers are training-free
and can be adjusted at inference time.
Specifically, we select CIFAR-10 WideResNet-34-10 models trained with Î²TR= 0,0.1,0.3, and 6as the
baselines, where 0corresponds to standard (non-robust) training and 6is the default which optimizes
robustness. For a fair comparison, we use the TRADES models with Î²TR= 0and 6to assemble the
mixed classifiers. For MixedNUTS, we adjust the level of robustness by tuning Î², the level-of-robustness
hyperparameter of Algorithm 1, specifically considering Î²values of 1,0.96,0.93,0.8, and 0.
Figure 8 confirms that training-free mixed classifiers, MixedNUTS and Mixed (Bai et al., 2024b), achieve much
more benign accuracy-robustness trade-offs than TRADES, with MixedNUTS attaining the best balance.
Since MixedNUTS is an ensemble, it inevitably results in a larger overall model than the TRADES baseline. To
clarify that MixedNUTSâ€™s performance gain is not due to the increased size, we train a larger TRADES model
(other training settings are unchanged) to match the parameter count, inference FLOPS, and parallelizability.
As shown in Figure 8, this larger TRADES modelâ€™s clean and robust accuracy does not improve over the
original, likely because the original training schedule is suboptimal for the increased size. This is unsurprising,
as it has been shown that no effective one-size-fits-all adversarial training parameter settings exist (Duesterwald
et al., 2019). Hence, an increased inference computation does not guarantee better performance on its own.
To make a model benefit from a larger size via training, neural architecture and training setting searches are
likely required, which is highly cumbersome and unpredictable. In contrast, MixedNUTS is a training-free
plug-and-play add-on, enjoying significantly superior practicality.
6 Conclusions
This work proposes MixedNUTS, a versatile training-free method that combines the output probabilities
of a robust classifier and an accurate classifier. By introducing nonlinear base model logit transformations,
MixedNUTS more effectively exploits the benign confidence property of the robust base classifier, thereby
achieving a balance between clean data classification accuracy and adversarial robustness. For performance-
driven practitioners, this balance implies less to lose in using robust models, incentivizing the real-world
deployment of safe deep learning systems. For researchers, as improving the accuracy-robustness trade-off with
13Published in Transactions on Machine Learning Research (08/2024)
a single model becomes harder, MixedNUTS identifies building base models with better margin properties as
a novel alternative direction to improve the trade-off in an ensemble setting.
References
George Adam and Romain Speciel. Evaluating ensemble robustness against adversarial attacks. arXiv preprint
arXiv:2005.05750 , 2020.
Manaar Alam, Shubhajit Datta, Debdeep Mukhopadhyay, Arijit Mondal, and Partha Pratim Chakrabarti.
Resisting adversarial attacks in deep neural networks using diverse decision boundaries. arXiv preprint
arXiv:2208.08697 , 2022.
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: A
query-efficient black-box adversarial attack via random search. In European Conference on Computer
Vision, 2020.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In International Conference on Machine Learning , 2018a.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples.
InInternational Conference on Machine Learning , 2018b.
Yatong Bai, Tanmay Gautam, Yu Gai, and Somayeh Sojoudi. Practical convex formulation of robust
one-hidden-layer neural network training. In American Control Conference , 2022.
Yatong Bai, Tanmay Gautam, and Somayeh Sojoudi. Efficient global optimization of two-layer relu networks:
Quadratic-time algorithms and adversarial training. SIAM Journal on Mathematics of Data Science , 5(2):
446â€“474, 2023.
Yatong Bai, Brendon G Anderson, Aerin Kim, and Somayeh Sojoudi. Improving the accuracy-robustness
trade-off of classifiers via adaptive smoothing. SIAM Journal on Mathematics of Data Science , 6(3):
788â€“814, 2024a.
Yatong Bai, Brendon G Anderson, and Somayeh Sojoudi. Mixing classifiers to alleviate the accuracy-robustness
trade-off. In Annual Learning for Dynamics and Control Conference , 2024b.
Yogesh Balaji, Tom Goldstein, and Judy Hoffman. Instance adaptive adversarial training: Improved accuracy
tradeoffs in neural nets. arXiv preprint arXiv:1910.08051 , 2019.
Yoshua Bengio, Nicholas LÃ©onard, and Aaron Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
Jaydeep Borkar and Pin-Yu Chen. Simple transparent adversarial examples. arXiv preprint arXiv:2105.09685 ,
2021.
Ruisi Cai, Zhenyu Zhang, and Zhangyang Wang. Robust weight signatures: gaining robustness as easy as
patching weights? In International Conference on Machine Learning , 2023.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In IEEE
Symposium on Security and Privacy , 2017.
Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting may be
mitigated by properly learned smoothening. In International Conference on Learning Representations ,
2021.
Minhao Cheng, Qi Lei, Pin Yu Chen, Inderjit Dhillon, and Cho Jui Hsieh. CAT: Customized adversarial
training for improved robustness. In International Joint Conference on Artificial Intelligence , 2022.
Kenneth T Co, David Martinez-Rego, Zhongyuan Hau, and Emil C Lupu. Jacobian ensembles improve
robustness trade-offs to adversarial attacks. In Artificial Neural Networks and Machine Learning , 2022.
14Published in Transactions on Machine Learning Research (08/2024)
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In International Conference on Machine Learning , 2020.
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion,
Mung Chiang, Prateek Mittal, and Matthias Hein. RobustBench: a standardized adversarial robustness
benchmark. In Conference on Neural Information Processing Systems Datasets and Benchmarks Track ,
2021.
Jiequan Cui, Zhuotao Tian, Zhisheng Zhong, Xiaojuan Qi, Bei Yu, and Hanwang Zhang. Decoupled
kullback-leibler divergence loss. arXiv preprint arXiv:2305.13948 , 2023.
Edoardo Debenedetti, Vikash Sehwag, and Prateek Mittal. A light recipe to train robust vision transformers.
InConference on Secure and Trustworthy Machine Learning , 2023.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical
image database. In IEEE Conference on Computer Vision and Pattern Recognition , 2009.
Evelyn Duesterwald, Anupama Murthi, Ganesh Venkataraman, Mathieu Sinn, and Deepak Vijaykeerthy.
Exploring the hyperparameter landscape of adversarial robustness. arXiv preprint arXiv:1905.03837 , 2019.
Mudasir A Ganaie, Minghui Hu, AK Malik, M Tanveer, and PN Suganthan. Ensemble deep learning: A
review.Engineering Applications of Artificial Intelligence , 115:105151, 2022.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.
InInternational Conference on Learning Representations , 2015.
Sven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy Mann, and Pushmeet Kohli. An
alternative surrogate loss for PGD-based adversarial testing. arXiv preprint arXiv:1910.09338 , 2019.
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits of
adversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593 , 2020.
Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A
Mann. Improving robustness using generated data. In Annual Conference on Neural Information Processing
Systems, 2021.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In
International Conference on Machine Learning , 2017.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh
Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089 , 2022.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited
queries and information. In International Conference on Machine Learning , 2018.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil
Houlsby. Big transfer (BiT): General visual representation learning. In European Conference on Computer
Vision, 2020.
Alex Krizhevsky. Learning multiple layers of features from tiny images. https://www.cs.toronto.edu/
~kriz/learning-features-2009-TR.pdf , 2009.
Alex Lamb, Vikas Verma, Juho Kannala, and Yoshua Bengio. Interpolated adversarial training: Achieving
robust neural networks without sacrificing too much accuracy. In ACM Workshop on Artificial Intelligence
and Security , 2019.
Lin Li, Yifei Wang, Chawin Sitawarin, and Michael Spratling. OODRobustBench: benchmarking and
analyzing adversarial robustness under distribution shift. arXiv preprint arXiv:2310.12793 , 2023.
15Published in Transactions on Machine Learning Research (08/2024)
Chang Liu, Yinpeng Dong, Wenzhao Xiang, Xiao Yang, Hang Su, Jun Zhu, Yuefeng Chen, Yuan He, Hui Xue,
and Shibao Zheng. A comprehensive study on robustness of image classification models: Benchmarking
and rethinking. arXiv preprint arXiv:2302.14301 , 2023.
Faqiang Liu and Rong Zhao. Towards both accurate and robust neural networks without extra data. In
International Conference on Artificial Neural Networks , 2022.
Jeremiah Liu, John Paisley, Marianthi-Anna Kioumourtzoglou, and Brent Coull. Accurate uncertainty
estimation and decomposition in ensemble learning. In Annual Conference on Neural Information Processing
Systems, 2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks. In International Conference on Learning Representations ,
2018.
Dongbin Na. Pytorch adversarial training on cifar-10. https://github.com/ndb796/
Pytorch-Adversarial-Training-CIFAR , 2020.
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via promoting
ensemble diversity. In International Conference on Machine Learning , 2019.
Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan. Robustness and accuracy could be
reconcilable by (proper) definition. In International Conference on Machine Learning , 2022.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In ACM Asia Conference on Computer and Commu-
nications Security , 2017.
ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, Jason
Martin, and Duen Horng Chau. Robust principles: Architectural design principles for adversarially robust
CNNs.arXiv preprint arXiv:2308.16258 , 2023.
Aleksandar Petrov, Francisco Eiras, Amartya Sanyal, Philip HS Torr, and Adel Bibi. Certifying ensembles: A
general certification theory with S-Lipschitzness. arXiv preprint arXiv:2304.13019 , 2023.
Samuel Pfrommer, Brendon G Anderson, and Somayeh Sojoudi. Projected randomized smoothing for certified
adversarial robustness. Transactions on Machine Learning Research , 2023.
Samuel Pfrommer, Brendon Anderson, Julien Piet, and Somayeh Sojoudi. Asymmetric certified robustness
via feature-convex neural networks. Advances in Neural Information Processing Systems , 2024.
Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Helper-based adversarial training: Reducing excessive
margin to achieve a better accuracy vs. robustness trade-off. In ICML Workshop on Adversarial Machine
Learning , 2021.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Understanding and
mitigating the tradeoff between robustness and accuracy. In International Conference on Machine Learning ,
2020.
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann.
Fixing data augmentation to improve adversarial robustness. arXiv preprint arXiv:2103.01946 , 2021.
Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, and Prateek
Mittal. Robust learning meets generative models: Can proxy distributions improve adversarial robustness?
InInternational Conference on Learning Representations , 2022.
Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S
Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Annual Conference on Neural
Information Processing Systems , 2019.
16Published in Transactions on Machine Learning Research (08/2024)
Naman D Singh, Francesco Croce, and Matthias Hein. Revisiting adversarial training for imagenet: Architec-
tures, training and generalization across threat models. arXiv preprint arXiv:2303.01870 , 2023.
Florian TramÃ¨r, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial
example defenses. In Annual Conference on Neural Information Processing Systems , 2020.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness
may be at odds with accuracy. In International Conference on Learning Representations , 2019.
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Annual Conference on
Neural Information Processing Systems , 2017.
Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models
further improve adversarial training. In International Conference on Machine Learning , 2023.
Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie.
ConvNeXt V2: Co-designing and scaling convnets with masked autoencoders. In IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2023.
Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, and Bo Li. On the certified robustness for
ensemble models and beyond. In International Conference on Learning Representations , 2022.
Yaodong Yu, Stephen Bates, Yi Ma, and Michael Jordan. Robust calibration with multi-domain temperature
scaling. In Annual Conference on Neural Information Processing Systems , 2022.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference on
Machine Learning , 2019.
17Published in Transactions on Machine Learning Research (08/2024)
Appendix
A Proofs
A.1 Proof to Theorem 4.3
Theorem 4.3 (restated). Suppose that Assumption 4.2holds. LetrfM
mixandrhrobdenote the robust accuracy
offM
mix(Â·)andhrob(Â·)respectively. If Î²â‰¥rfM
mix/rhrob, then a solution to (3)is feasible for (2).
Proof.Suppose that M(Â·)is a solution from (3). Since the mixed classifier fM
mix(Â·)is by construction
guaranteed to be correct and robust at some xifhM
rob(Â·)is correct and robust with a margin no smaller than
1âˆ’Î±
Î±atx, it holds that
P(X,Y)âˆ¼D/bracketleftbig
arg max
ifM
mix,i(X+Î´â‹†
fM
mix(X)) =Y/bracketrightbig
â‰¥P(X,Y)âˆ¼D/bracketleftbig
mhM
rob,i(X+Î´â‹†
fM
mix(X))â‰¥1âˆ’Î±
Î±, HM
cor(X)/bracketrightbig
=P(X,Y)âˆ¼D/bracketleftbig
mhM
rob,i(X+Î´â‹†
fM
mix(X))â‰¥1âˆ’Î±
Î±/vextendsingle/vextendsingleHM
cor(X)/bracketrightbig
Â·P(X,Y)âˆ¼D[HM
cor(X)],
whereHcor(X)denotes the event of hrob(Â·)being correct at X, i.e., arg maxihrob,i(X+Î´â‹†
fM
mix(X)) =Y.
Similarly,HM
cor(X)denotes arg maxihM
rob,i(X+Î´â‹†
fM
mix(X)) =Y. Under Assumption 4.2, Hcor(X)is equivalent
toHM
cor(X). Therefore,
P(X,Y)âˆ¼D/bracketleftbig
arg max
ifM
mix,i(X+Î´â‹†
fM
mix(X)) =Y/bracketrightbig
=P(X,Y)âˆ¼D/bracketleftbig
mhM
rob,i(X+Î´â‹†
fM
mix(X))â‰¥1âˆ’Î±
Î±/vextendsingle/vextendsingleHcor(X)/bracketrightbig
Â·P(X,Y)âˆ¼D[Hcor(X)]
=rhrobÂ·PXâˆ¼Xâœ“
adv/bracketleftbig
mhM
rob,i(X+Î´â‹†
fM
mix(X))â‰¥1âˆ’Î±
Î±/bracketrightbig
â‰¥rhrobÂ·PZâˆ¼Xâœ“
adv/bracketleftï£¬ig
mâ‹†
hM
rob(Z)â‰¥1âˆ’Î±
Î±/bracketrightï£¬ig
â‰¥rhrobÂ·Î²â‰¥rfM
mix,
which proves the statement.
A.2 Proof to Theorem 4.4
Theorem 4.4 (restated). Suppose that Assumption 4.1holds. Furthermore, consider an input random
variableXand suppose that the margin of hM
rob(X)is independent of whether gstd(X)is correct. Then,
minimizing the objective of (3)is equivalent to maximizing the objective of (2).
Proof.By the construction of the mixed classifier, for a clean input xincorrectly classified by hM
rob(Â·)(i.e.,x
is in the support of Xâœ—
clean), the mixed classifier prediction fmix(x)is correct if and only if gTS(0)
std(x)is correct
andhM
rob(x)â€™s margin is no greater than1âˆ’Î±
Î±.
LetGcor(X)denote the event of gstd(X)being correct, i.e., arg maxigTS(0)
std (X) =Y. Furthermore, let Dic
denote the data-label distribution formed by clean examples incorrectly predicted by hM
rob(Â·). Then,
P(X,Y)âˆ¼Dic/bracketleftbig
arg max
ifM
mix,i(X) =Y/bracketrightbig
=PXâˆ¼Xâœ—
clean/bracketleftï£¬ig
mhM
rob(X)<1âˆ’Î±
Î±,Gcor(X)/bracketrightï£¬ig
=PXâˆ¼Xâœ—
clean/bracketleftï£¬ig
mhM
rob(X)<1âˆ’Î±
Î±|Gcor(X)/bracketrightï£¬ig
Â·PXâˆ¼Xâœ—
clean[Gcor(X)]
for all transformations Mand mixing weight Î±(recall that the mixed classifier fM
mix(Â·)depends on Î±).
Suppose that the margin of hM
rob(Â·)is independent of the accuracy of gstd(Â·), then the above probability
further equals to
/parenleftï£¬ig
1âˆ’PXâˆ¼Xâœ—
clean/bracketleftï£¬ig
mhM
rob(X)â‰¥1âˆ’Î±
Î±/bracketrightï£¬ig/parenrightï£¬ig
Â·PXâˆ¼Xâœ—
clean[Gcor(X)]
18Published in Transactions on Machine Learning Research (08/2024)
SincePXâˆ¼Xâœ—
clean[Gcor(X)]does not depend on MorÎ±, it holds that
arg min
MâˆˆM, Î±âˆˆ[1/2,1]PXâˆ¼Xâœ—
clean/bracketleftï£¬ig
mhM
rob(X)â‰¥1âˆ’Î±
Î±/bracketrightï£¬ig
= arg max
MâˆˆM, Î±âˆˆ[1/2,1]P(X,Y)âˆ¼Dic/bracketleftbig
arg max
ifM
mix,i(X) =Y/bracketrightbig
= arg max
MâˆˆM, Î±âˆˆ[1/2,1]P(X,Y)âˆ¼D/bracketleftbig
arg max
ifM
mix,i(X) =Y/bracketrightbig
,
where the last equality holds because under Assumption 4.1, hM
rob(x)being correct guarantees gTS(0)
std (x)â€™s
correctness. Since the mixed classifier fM
mix(Â·)must be correct given that hM
rob(x)andgTS(0)
std (x)are both
correct,fM
mix(Â·)must be correct at clean examples correctly classified by hM
rob(Â·). Hence, maximizing fM
mix(Â·)â€™s
clean accuracy on hM
rob(Â·)â€™s mispredictions is equivalent to maximizing fM
mix(Â·)â€™s overall clean accuracy.
B Custom Attack Algorithms
B.1 Minimum-Margin AutoAttack for Margin Estimation
Our margin-based hyper-parameter selection procedure (Algorithm 1) and confidence margin estimation
experiments (Figures 6 and 7) require approximating the minimum-margin perturbations associated with the
robust base classifier in order to analyze the mixed classifier behavior. Approximating these perturbations
requires a strong attack algorithm. While AutoAttack is often regarded as a strong adversary that reliably
evaluates model robustness, its original implementation released with (Croce & Hein, 2020) does not return
all perturbed examples. Specifically, traditional AutoAttack does not record the perturbation around some
input if it deems the model to be robust at this input (i.e., model prediction does not change). While this
is acceptable for estimating robust accuracy, it forbids the calculation of correctly predicted AutoAttacked
examplesâ€™ confidence margins, which are required by Algorithm 1, Figure 6, and Figure 7.
To construct a strong attack algorithm compatible with margin estimation, we propose minimum-margin
AutoAttack (MMAA) . Specifically, we modify the two APGD components of AutoAttack (untargeted APGD-
CE and targeted APGD-DLR) to keep track of the margin at each attack step (the margin history is shared
across the two components) and always return the perturbation achieving the smallest margin. The FAB
and Square components of AutoAttack are much slower than the two APGD components, and for our base
classifiers, FAB and Square rarely succeed in altering the model predictions for images that APGD attacks
fail to attack. Therefore, we exclude them for the purpose of margin estimation (but include them for the
robustness evaluation of MixedNUTS).
B.2 Adaptive Attacks for Nonlinearly Mixed Classifier Robustness Evaluation
When proposing a novel adversarially robust model, reliably measuring its robustness with strong adversaries
is always a top priority. Hence, in addition to designing MMAA for the goal of margin estimation, we devise
two adaptive attack algorithms to evaluate the robustness of the MixedNUTS and its nonlinearly mixed
model defense mechanism. Both algorithms are strengthened adaptive versions of AutoAttack. As is the
original AutoAttack, both algorithms are ensembles of four attack methods, including a black-box component.
Hence, the reported accuracy numbers in this paper are lower bounds to the attacked accuracy associated
with each of the components.
B.2.1 Transfer-Based Adaptive AutoAttack with Auxiliary Mixed Classifier
Following the guidelines for constructing adaptive attacks (TramÃ¨r et al., 2020), our adversary maintains full
access to the end-to-end gradient information of the mixed classifier fmix(Â·). Nonetheless, when temperature
scaling with T= 0is applied to the accurate base classifier gstd(Â·)as discussed in Section 3.1, gTS(T)
std(Â·)is no
longer differentiable. While this is an advantage in practice since the mixed classifier becomes harder to attack,
we need to circumvent this obfuscated gradient issue in our evaluations to properly demonstrate white-box
robustness. To this end, transfer attack comes to the rescue. We construct an auxiliary differentiable mixed
classifier/tildewidefmix(Â·)by mixinggstd(Â·)â€™s unmapped logits with hM
rob(Â·). We allow our attacks to query the gradient
19Published in Transactions on Machine Learning Research (08/2024)
of/tildewidefmix(Â·)to guide the gradient-based attack on fmix(Â·). Sincegstd(Â·)andgTS(T)
std (Â·)always produce the same
predictions, the transferability between /tildewidefmix(Â·)andfmix(Â·)should be high.
On the other hand, while hM(s,p,c)
rob (Â·)â€™s nonlinear logit transformation (4) is differentiable, it may also hinder
gradient flow in certain cases, especially when the logits fall into the relatively flat near-zero portion of the
clamping function Clamp (Â·). Hence, we also provide the raw logits of hrob(Â·)to our evaluation adversary for
better gradient flow. To keep the adversary aware of the transformation M(s,p,c )(Â·), we still include it in the
gradient (i.e., M(s,p,c )(Â·)is only partially bypassed). The overall construction of the auxiliary differentiable
mixed classifier /tildewidefmix(Â·)is then
/tildewidefmix(x) = log/parenleftbig
(1âˆ’Î±d)Â·Ïƒâ—¦gstd(Â·) +Î±drdÂ·Ïƒâ—¦hrob(Â·) +Î±d(1âˆ’rd)Â·Ïƒâ—¦hM(sâ‹†,pâ‹†,câ‹†)
rob (Â·)/parenrightbig
,(8)
whereÎ±dis the mixing weight and rdadjusts the level of contribution of M(s,p,c )(Â·)to the gradient. Our
experiments fix rdto0.9and calculate Î±dusing Algorithm 1 with no clamping function, sandpfixed to
1, andcfixed to 0. The gradient-based components (APGD and FAB) of our adaptive AutoAttack use
âˆ‡L(/tildewidefmix(x))as a surrogate for âˆ‡L(fM(sâ‹†,pâ‹†,câ‹†)
mix (x))whereLis the adversarial loss function. The gradient-free
Square attack component remains unchanged. Please refer to our source code for details on implementation.
With the transfer-based gradient query in place, our adaptive AutoAttack does not suffer from gradient
obfuscation, a phenomenon that leads to overestimated robustness. Specifically, we observe that the black-
box Square component of our adaptive AutoAttack does not change the prediction of any images that
white-box components fail to attack, confirming the effectiveness of querying the transfer-based auxiliary
differentiable mixed classifier for the gradient. If we set rdto 0 (i.e., do not bypass M(s,p,c )(Â·)for gradient),
the AutoAttacked accuracy of the CIFAR-100 model reported in Figure 5 becomes 42.97%instead of 41.80%,
and the black-box Square attack finds 12 vulnerable images. This comparison confirms that the proposed
modifications on AutoAttack strengthen its effectiveness against MixedNUTS and eliminate the gradient flow
issue, making it a reliable robustness evaluator.
B.2.2 Direct Gradient Bypass
An alternative method for circumventing the non-differentiability challenge introduced by our logit transfor-
mations is to allow the gradient to bypass the corresponding non-differentiable operations. To achieve so, we
again leverage the auxiliary differentiable mixed classifier defined in (8), and construct the overall output as
/tildewidefmix(x) +StopGrad/parenleftbig
fM(sâ‹†,pâ‹†,câ‹†)
mix (x)âˆ’/tildewidefmix(x)/parenrightbig
,
whereStopGrad denotes the straight-through operation that passes the forward activation but stops the
gradient (Bengio et al., 2013), for which a PyTorch realization is Tensor.detach() . The resulting mixed
classifier retains the output values of the MixedNUTS classifier fM(sâ‹†,pâ‹†,câ‹†)
mix (x)while using the gradient
computation graph of the differentiable auxiliary classifier /tildewidefmix(x). In the literature, a similar technique is
often used to train neural networks with non-differentiable components, such as VQ-VAEs (Van Den Oord
et al., 2017).
This direct gradient bypass method is closely related to the transfer-based adaptive attack described in
Appendix B.2.1, but has the following crucial differences:
â€¢Compatibility with existing attack codebases. The transfer-based attack relies on the outputs
from bothfM(sâ‹†,pâ‹†,câ‹†)
mix (Â·)and/tildewidefmix(Â·). Since most existing attack codebases, such as AutoAttack, are
implemented assuming that the neural network produces a single output, they need to be modified to
accept two predictions. In contrast, direct gradient bypass does not introduce or require multiple network
outputs, and is therefore compatible with existing attack frameworks without modifications. Hence, our
submission to RobustBench uses the direct gradient bypass method.
â€¢Calculation of attack loss functions. From a mathematical perspective, the transfer-based attack
uses the auxiliary differentiable mixture to evaluate the attack objective function. In contrast, the direct
gradient bypass method uses the original MixedNUTSâ€™s output for attack objective calculation, and
then uses the gradient computation graph of /tildewidefmix(Â·)to perform back-propagation. Hence, the resulting
gradient is slightly different between the two methods.
20Published in Transactions on Machine Learning Research (08/2024)
Table 2: Details of the base classifiers used in our main experiments.
Dataset Robust Base Classifier gstd(Â·) Accurate Base Classifier hrob(Â·)
CIFAR-10 ResNet-152 (Kolesnikov et al., 2020) RaWideResNet-70-16 (Peng et al., 2023)
CIFAR-100 ResNet-152 (Kolesnikov et al., 2020) WideResNet-70-16 (Wang et al., 2023)
ImageNet ConvNeXt V2-L (Woo et al., 2023) Swin-L (Liu et al., 2023)
Table 3: The optimal s,p,c,Î±values returned by Algorithm 1 used in our main experiments, presented
along with the minimum and maximum candidate values in Algorithm 1â€™s searching grid.
sâ‹†câ‹†pâ‹†Î±â‹†sminsmaxcmincmaxpminpmax
CIFAR-10 5.00âˆ’1.10 4.00.999 0.05 5âˆ’1.1 0 1 4
CIFAR-100 .612âˆ’2.14 3.57.986 0.05 4âˆ’2.5âˆ’0.4 1 4
ImageNet .0235âˆ’.286 2.71.997 0.01 0.2âˆ’2 0 2 3
Table 4: The proposed nonlinear logit transformation M(sâ‹†,pâ‹†,câ‹†)(Â·)has minimal effect on base classifier
accuracy.
DatasetClean (full dataset) AutoAttack (1000 images)
hrob(Â·)hLN
rob(Â·)hM(sâ‹†,pâ‹†,câ‹†)
rob (Â·)hrob(Â·)hLN
rob(Â·)hM(sâ‹†,pâ‹†,câ‹†)
rob (Â·)
CIFAR-10 93.27% 93.27% 93.25% 71.4% 71.4% 71.4%
CIFAR-100 75.22% 75.22% 75.22% 43.0% 42.9% 43.3%
ImageNet 78.75% 78.75% 78.75% 57.5% 57.5% 57.5%
Our experiments show that when using direct gradient bypass, the original AutoAttack algorithm returns
70.08%,41.91%, and 58.62%for CIFAR-10, CIFAR-100, and ImageNet respectively with the MixedNUTS
model used in Figure 5. Compared with the transfer-based adaptive AutoAttack, which achieves 69.71%,
41.80%, and 58.50%, AutoAttack with direct gradient bypass consistently achieves a lower success rate, but
the difference is very small. Hence, we use the transfer-based AutoAttack for Figure 5, but note that both
methods can evaluate MixedNUTS reliably.
C MixedNUTS Model Details
C.1 Base Classifier and Mixing Details
Table 2 presents the sources and architectures of the base classifiers selected for our main experiments
(Figure 5, Figure 2, Figure 6, and Table 1). The robust base classifiers are the state-of-the-art models listed
on RobustBench as of submission, and the accurate base classifiers are popular high-performance models
pre-trained on large datasets. Note that since MixedNUTS only queries the predicted classes from gstd(Â·)and
is agnostic of its other details, gstd(Â·)may be any classifier, including large-scale vision-language models that
currently see rapid development.
Table 3 presents the optimal sâ‹†,pâ‹†,câ‹†, andÎ±â‹†values used in MixedNUTSâ€™s nonlinear logit transformation
returned by Algorithm 1. When optimizing s,p, andc, Algorithm 1 performs a grid search, selecting from a
provided set of candidate values. In our experiments, we generate uniform linear intervals as the candidate
values for the power coefficient pand the bias coefficient c, and use a log-scale interval for the scale coefficient
s. Each interval has eight numbers, with the minimum and maximum values for the intervals listed in Table 3.
Table 4 shows that MixedNUTSâ€™s nonlinear logit transformation M(sâ‹†,pâ‹†,câ‹†)(Â·)has negligible effects on base
classifier accuracy, confirming that the improved accuracy-robustness balance is rooted in the improved base
classifier confidence properties.
21Published in Transactions on Machine Learning Research (08/2024)
Table 5: MixedNUTSâ€™s accuracy and inference efficiency compared with state-of-the-art classifiers.
Model Architecture Parameters GFLOPs Clean (â†‘)AutoAttack (â†‘)
CIFAR-10
This work Mixed (see Table 2) 499.5M 151.02 95.19% 69.71%
Peng et al. (2023) RaWideResNet-70-16 267.2M 121.02 93.27% 71.07%
Bai et al. (2024a) Mixed with Mixing Net 566.9M 117.31 95.23% 68.06%
Rebuffi et al. (2021) WideResNet-70-16 266.8M 77.55 92.23% 66.58%
Kolesnikov et al. (2020) ResNet-152 232.3M 30.00 98.50% 0.00%
CIFAR-100
This work Mixed (see Table 2) 499.5M 107.56 83.08% 41.80%
Wang et al. (2023) WideResNet-70-16 266.9M 77.56 75.22% 42.67%
Bai et al. (2024a) Mixed with Mixing Net 567.4M 117.31 85.21% 38.72%
Gowal et al. (2020) WideResNet-70-16 266.9M 77.55 69.15% 36.88%
Kolesnikov et al. (2020) ResNet-152 232.6M 30.00 91.38% 0.00%
ImageNet
This work Mixed (see Table 2) 394.5M 136.91 81.48% 58.50%
Liu et al. (2023) Swin-L 198.0M 68.12 78.92% 59.56%
Singh et al. (2023) ConvNeXt-L + ConvStem 198.1M 71.16 77.00% 57.70%
Peng et al. (2023) RaWideResNet-101-2 104.1M 51.14 73.44% 48.94%
Woo et al. (2023) ConvNeXt V2-L 196.5M 68.79 86.18% 0.00%
C.2 Model Inference Efficiency
In this section, we compare the performance and inference efficiency of MixedNUTS with existing methods.
As a training-free ensemble method, MixedNUTS naturally trades inference efficiency for training efficiency.
Nonetheless, since MixedNUTS only requires two base models and does not add new neural network
components, it is among the most efficient ensemble methods. Specifically, the computational cost of
MixedNUTS is the sum of the computation of its two base classifiers, as the mixing operation itself is trivial
from a computational standpoint.
In Table 5, the efficiency of MixedNUTS, evaluated in terms of parameter count and floating-point operations
(FLOPs), is compared with some other state-of-the-art methods. Compared with the fellow mixed classifier
method adaptive smoothing (Bai et al., 2024a), MixedNUTS is more efficient when the base classifiers are
the same, as is the case for CIFAR-100. This is because adaptive smoothing introduces an additional mixing
network, whereas MixedNUTS only introduces four additional parameters. On CIFAR-10, MixedNUTS
uses a denser robust base classifier than adaptive smoothing, with similar number of parameters but higher
GFLOPs (121.02 vs 77.56). MixedNUTSâ€™s FLOPs count is thus also higher than (Bai et al., 2024a).
C.3 Base Classifier Confidence Margin Distribution
Table 6 displays the histograms of the confidence margins of the base classifiers used in the CIFAR-100
experiment in Figure 5. We can observe the following conclusions, which support the design of MixedNUTS:
â€¢gstd(Â·)is more confident when making mistakes under attack than when correctly predicting clean images.
â€¢hrob(Â·)is more confident in correct predictions than in incorrect ones, as required by MixedNUTS. Note
that even when subject to strong AutoAttack, correct predictions are still more confident than clean
unperturbed incorrect predictions.
â€¢Layer normalization increases hrob(Â·)â€™s correct prediction margins while maintaining the incorrect margins.
â€¢MixedNUTSâ€™s nonlinear logit transformation significantly increases the correct predictionâ€™s confidence
margins while keeping most of the incorrect margins small.
22Published in Transactions on Machine Learning Research (08/2024)
Table6: Predictionconfidencemarginof hrob(Â·),hLN
rob(Â·), andhM(sâ‹†,pâ‹†,câ‹†)
rob(Â·)usedintheCIFAR-100experiments
in Figure 6. The nonlinear logit transformation (4) amplifies the margin advantage of correct predictions over
incorrect ones. As in Figure 6, 10000 clean examples and 1000 AutoAttack examples are used.
Standard base model gstd(Â·)Robust base model hrob(Â·)With LNhLN
rob(Â·)With transform hM(sâ‹†,pâ‹†,câ‹†)
rob (Â·)Clean
0.0 0.2 0.4 0.6 0.8 1.0010002000300040005000 Correct
Incorrect
0.0 0.2 0.4 0.6 0.8 1.0020040060080010001200 Correct
Incorrect
0.0 0.2 0.4 0.6 0.8 1.002004006008001000Correct
Incorrect
0.0 0.2 0.4 0.6 0.8 1.00100020003000400050006000
Correct
Incorrect AutoAttack (APGD)
0.0 0.2 0.4 0.6 0.8 1.00200400600800Correct
Incorrect
0.0 0.2 0.4 0.6 0.8 1.0050100150200250300 Correct
Incorrect
0.0 0.2 0.4 0.6 0.8 1.0050100150200250300
Correct
Incorrect
0.0 0.2 0.4 0.6 0.8 1.0050100150200250300350Correct
Incorrect
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=1.000
49.454.660.3466.6973.781.4590.0299.48
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=1.429
49.454.660.3466.6973.781.4590.0299.48
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=1.857
49.454.660.3466.6973.781.4590.0299.48
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=2.286
49.454.660.3466.6973.781.4590.0299.48
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=2.714
49.454.660.3466.6973.781.4590.0299.48
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=3.143
49.454.660.3466.6973.781.4590.0299.48
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=3.571
49.454.660.3466.6973.781.4590.0299.48
âˆ’2.5 âˆ’2.2 âˆ’1.9 âˆ’1.6 âˆ’1.3 âˆ’1.0 âˆ’0.7 âˆ’0.4
c0.050.090.170.330.611.142.144.0sSlice at p=4.000
49.454.660.3466.6973.781.4590.0299.48
Figure 9: Sensitivity analysis of the nonlinear logit transformation. Lower objective (darker color) is better.
D Ablation Studies and Additional Discussions
D.1 Sensitivity to s,p,cValues
In this section, we visualize the optimization landscape of the hyper-parameter optimization problem (7) in
terms of the sensitivity with respect to s,p, andc. To achieve so, we store the objective of (7) corresponding
to each combination of s,p,cvalues in our grid search as a three-dimensional tensor (recall that the value of
Î±can be determined via the constraint). We then visualize the tensor by displaying each slice of it as a color
map. We use our CIFAR-100 model as an example, and present the result in Figure 9.
23Published in Transactions on Machine Learning Research (08/2024)
Table 7: Ablation study on clamping functions.
Clamp (Â·)sâ‹†câ‹†pâ‹†Î±â‹†Î²Obj (â†“)
CIFAR-10
Linear .050 - 9.14.963.985.671
ReLU 10.4âˆ’.750 4.00.934.985.671
GELU 2.59âˆ’.750 4.00.997.985.671
CIFAR-100
Linear .000005 - 10.5.880.985.504
ReLU .612âˆ’2.294.00.972.985.500
GELU .612âˆ’2.143.57.986.985.500Table 8: The accuracy on images used for calculating
sâ‹†,pâ‹†, andcâ‹†(marked as âœ“in the â€œSeenâ€ column)
is similar to that on images unseen by Algorithm 1
(marked as âœ—), confirming the absence of overfitting.
Dataset SeenClean AutoAttack
CIFAR-10âœ“ 95.20% 69.20%
âœ— 95.18% 69.77%
CIFAR-100âœ“ 82.80% 41.60%
âœ— 83.11% 41.82%
ImageNetâœ“ 82.60% 60.80%
âœ— 81.20% 57.93%
Table 9: MixedNUTSâ€™s clean
and AutoAttack accuracy when
s,p, andcare optimized us-
ing different numbers of images.
Evaluated with the CIFAR-100
base models from Figure 5 on a
1000-example subset.
# Images forCleanAuto
Optimization Attack
1000 (Default) 82.8% 41.6%
300 83.0% 41.5%
100 85.1% 39.5%Table 10: MixedNUTSâ€™s clean and
AutoAttack accuracy on a 1000-
exampleCIFAR-100subsetwithvar-
ious temperature scales for the stan-
dard base model gstd(Â·). The robust
base classifier is hM(sâ‹†,pâ‹†,câ‹†)
rob(Â·)with
thes,p,cvalues reported in Ta-
ble 3.
AccurateCleanAuto
Base Model Attack
gTS(0)
std (Â·)(Default) 82.8% 41.6%
gTS(0.5)
std (Â·) 82.8% 41.4%
gTS(1)
std (Â·) 82.8% 41.3%Table 11: The optimal hyper-
parameters are similar across similar
models, and hence can transfer be-
tween them. CIFAR-10 models are
used, with (Peng et al., 2023) being
the model in the main experiments in
Figure 5.
Robustsâ‹†câ‹†pâ‹†
Base Model
(Peng et al., 2023) 5.0âˆ’1.14.0
(Pang et al., 2022) 5.0âˆ’1.14.0
(Wang et al., 2023) 5.0âˆ’1.12.71
As shown in Figure 9, while the optimization landscape is non-convex, it is relatively smooth and benign,
with multiple combinations achieving similar, relatively low objective values. When the exponent parameter
pis small, the other two parameters, sandc, have to be within a smaller range for optimal performance.
Whenpis larger, a wide range of values for sandccan work. Nonetheless, an excessively large pmay
potentially cause numerical instabilities and should be avoided if possible. For the same consideration, we do
not recommend using the exponentiation function in the nonlinear logit transformation.
For further illustration, we construct a mixed CIFAR-100 classifier with a simple GELU as the nonlinear
logit transformation (still using gTS(0)
std(Â·)as the standard base classifier). The resulting clean/robust accuracy
is77.9%/40.4%on a 1000-example data subset. While this result is slightly better than the 77.6%/39.9%
accuracy of the baseline mixed classifier without nonlinearity, it is noticeably worse than MixedNUTSâ€™s
82.8%/41.6%. We can thus conclude that selecting a good combination of s,p, andcis crucial for achieving
optimal performance.
D.2 Selecting the Clamping Function
This section performs an ablation study on the clamping function in the nonlinear logit transformation
defined in (4). Specifically, we compare using GELU or ReLU as Clamp (Â·)to bypassing Clamp (Â·)(i.e., use a
linear function). Here, we select a CIFAR-10 ResNet-18 model (Na, 2020) and a CIFAR-100 WideResNet-
70-16 model (Wang et al., 2023) as two examples of hrob(Â·)and compare the optimal objective returned by
Algorithm 1 using each of the clamping function settings. As shown in Table 7, while the optimal objective
is similar for all three options, the returned hyper-parameters sâ‹†,pâ‹†,câ‹†, andÎ±â‹†is the most â€œmodestâ€ for
GELU, which translates to the best numerical stability. In comparison, using a linear clamping function
requires applying a power of 9.14 to the logits, whereas using the ReLU clamping function requires scaling
24Published in Transactions on Machine Learning Research (08/2024)
the logits up by a factor of 10.4 for CIFAR-10, potentially resulting in significant numerical instabilities.
Therefore, we select GELU as the default clamping function and use it for all other experiments.
D.3 Effect of Optimization Dataset Size and Absence of Overfitting
Since the MMAA step has the dominant computational time, reducing the number of images used in
Algorithm 1 can greatly accelerate it. Analyzing the effect of this data size also helps understand whether
optimizings,p, andcon validation images introduces overfitting. Table 9 shows that on CIFAR-100, reducing
the number of images used in the optimization from 1000 to 300 (3 images per class) has minimal effect on
the resulting mixed classifier performance. Further reducing the optimized subset size to 100 still allows for
an accuracy-robustness balance, but shifts the balance towards clean accuracy.
To further demonstrate the absence of overfitting, Table 8 reports that under the default setting of optimizing
s,p,con 1000 images, the accuracy on these 1000 images is similar to that on the rest of the validation
images unseen during optimization. The CIFAR-10 and -100 models, in fact, perform slightly better on
unseen images. The ImageNet modelâ€™s accuracy on unseen images is marginally lower than seen ones, likely
due to the scarcity of validation images per class (only 5 per class in total since ImageNet has 1000 classes)
and the resulting performance variance across the validation set.
D.4 Temperature Scaling for gstd(Â·)
This section verifies that scaling up the logits of gstd(Â·)improves the accuracy-robustness trade-off of the
mixed classifier. We select the pair of CIFAR-100 base classifiers used in Figure 5. By jointly adjusting
the temperature of gstd(Â·)and the mixing weight Î±, we can keep the clean accuracy of the mixed model
to approximately 84percent and compare the APGD accuracy. In Table 10, we consider two temperature
constants: 0.5and0. Note that as defined in Section 2.1, when the temperature is zero, the resulting prediction
probabilities Ïƒâ—¦gstd(Â·)is the one-hot vector associated with the predicted class. As demonstrated by the
CIFAR-100 example in Table 10, when we fix the clean accuracy to 82.8%, usingT= 0.5andT= 0produces
higher AutoAttacked accuracy than T= 1(no scaling), with T= 0producing the best accuracy-robustness
balance.
D.5 Algorithm 1 for Black-Box hrob(Â·)without Gradient Access
When optimizing the hyper-parameters s,p, andc, Step 2 of Algorithm 1 requires running MMAA on
the robust base classifier. While MMAA does not explicitly require access to base model parameters, its
gradient-based components query the robust base classifier gradient (the standard base classifier gstd(Â·)can
be a black box).
However, even if the gradient of hrob(Â·)is also unavailable, then s,p,c, andÎ±can be selected with one of the
following options:
â€¢Black-box minimum-margin attack. Existing gradient-free black-box attacks, such as Square
(Andriushchenko et al., 2020) and BPDA (Athalye et al., 2018a), can be modified into minimum-margin
attack algorithms. As are gradient-based methods, these gradient-free algorithms are iterative, and the
only required modification is to record the margin at each iteration to keep track of the minimum margin.
â€¢Transfer from another model. Since the robust base classifiers share the property of being more
confident in correct examples than in incorrect ones (as shown in Figure 5), an optimal set of s,p,c
values for one model likely also suits another model. So, one may opt to run MMAA on a robust classifier
whose gradient is available, and transfer the s,p,cvalues back to the black-box model.
â€¢Educated guess. Since each component of our parameterization of the nonlinear logit transformation
is intuitively motivated, a generic selection of s,p,cvalues should also perform better than mixing
linearly. In fact, when we devised this project, we used hand-selected s,p,cvalues for prototyping and
idea verification, and later designed Algorithm 1 for a more principled selection.
To empirically verify the feasibility of transferring hyper-parameters across robust base classifiers, we show
that the optimal hyper-parameters are similar across similar models. Consider the CIFAR-10 robust base
25Published in Transactions on Machine Learning Research (08/2024)
classifier used in our main results, which is from (Peng et al., 2023). Suppose that this model is a black box,
and the gradient-based components of MMAA cannot be performed. Then, we can seek some similar robust
models whose gradients are visible. We use two models, one from (Pang et al., 2022), and the other from
(Wang et al., 2023), as examples. As shown in Table 11, the optimal s,p,cvalues calculated via Algorithm 1
are highly similar for these three models. Hence, if we have access to the gradients of one of (Peng et al.,
2023; Pang et al., 2022; Wang et al., 2023), then we can use Algorithm 1 to select the hyper-parameter
combinations for all three models.
Since other parts of MixedNUTS do not require access to base model weights or gradients, MixedNUTS can
be applied to a model zoo even when all base classifiers are black boxes.
D.6 Selecting the Base Classifiers
This section provides guidelines on how to select the accurate and robust base classifiers for the best mixed
classifier performance. For the accurate classifier, since MixedNUTS only considers its predicted class and
does not depend on its confidence (recall that MixedNUTS uses gTS(0)
std(Â·)), the classifier with the best-known
clean accuracy should be selected. Meanwhile, for the robust base classifier, since MixedNUTS relies on
its margin properties, one should select a model that has high robust accuracy as well as benign margin
characteristics (i.e., is significantly more confident in correct predictions than incorrect ones). As shown
in Figure 7, most high-performance robust models share this benign property, and the correlation between
robust accuracy and margins is insignificant. Hence, state-of-the-art robust models are usually safe to use.
That being said, consider the hypothetical scenario that between a pair of robust base classifiers, one has
higher robust accuracy and the other has more benign margin properties. Here, one should compare the
percentages of data for which the two models are robust with a certain non-zero margin. The model with
higher â€œrobust accuracy with marginâ€ should be used.
D.7 Behavior of Logit Normalization
The LN operation on the model logits makes the margin agnostic to the overall scale of the logits. Consider
two example logit vectors in R3, namely (0.9,1,1.1)and(âˆ’2,1,1.1). The first vector corresponds to the
case where the classifier prefers the third class but is relatively unconfident. The second vector reflects the
scenario where the classifier is generally more confident, but the second and third classes compete with
each other. The LN operation will scale up the first vector and scale down the second. It is likely that the
competing scenario is more common when the prediction is incorrect, and therefore the LN operation, which
comparatively decreases the margin under the competing scenario, makes incorrect examples less confident
compared with correct ones. As a result, the LN operation itself can slightly enlarge the margin difference
between incorrect and correct examples.
For ImageNet, instead of performing LN on the logits based on the mean and variance of all 1000 classes,
we normalize using the statistics of the top 250 classes. The intuition of doing so is that the predicted
probabilities of bottom classes are extremely small and likely have negligible influence on model prediction
and robustness. However, they considerably influence the mean and variance statistics of logits. Excluding
these least-related classes makes the LN operation less noisy.
D.8 Further Discussions on Assumptions
D.8.1 When Assumption 4.1 is Slightly Violated
If Assumption 4.1 is slightly violated, then there is a slight mismatch between the objective functions of (3)
and (2) due to discarding the case of gTS(0)
std (Â·)being incorrect while hM(s,p,c)
rob (Â·)being correct on clean data.
As a result, the reformulations in this section become slightly suboptimal. However, note that the constraint
in (2), which enforces the level of robustness of the mixed classifier, is not compromised. Furthermore, as
mentioned above, the amount of clean examples correctly classified by hM(s,p,c)
rob (Â·)but not by gTS(0)
std (Â·)is
usually exceedingly rare, and hence the degree of suboptimality is extremely small.
26Published in Transactions on Machine Learning Research (08/2024)
Also note that with a slight violation of Assumption 4.1, while our algorithm may become slightly suboptimal,
the mixed classifier outperforms our expectation, because it can now correctly classify additional clean
examples than suggested by Theorem 4.4, the only theoretical result dependent on Assumption 4.1.
D.8.2 When Assumption 4.2 is Slightly Violated
Assumption 4.2 assumes that the nonlinear logit transformation applied to hrob(Â·)does not affect its predicted
class and hence inherits hrob(Â·)â€™s accuracy. When Assumption 4.2 is violated, consider the following two
cases: 1) the logit transformation M(s,p,c )(Â·)corrects mispredictions; 2) M(s,p,c )(Â·)contaminates correct
predictions.
Consider the first scenario, i.e., hM(s,p,c)
rob (Â·)is correct whereas hrob(Â·)is not. In this case, Theorem 4.3 (the
only theoretical result dependent on Assumption 4.2) still holds, and the mixed classifier can correctly classify
even more clean examples than Theorem 4.3 suggests.
Conversely, consider the second case, where hM(s,p,c)
rob (Â·)is incorrect whereas hrob(Â·)is correct. In this case,
Theorem 4.3 may not hold. However, this is the best one can expect. In this worst-case scenario, although
the nonlinear logit transformation improves hrob(Â·)â€™s confidence property, it also harms hrob(Â·)â€™s standalone
accuracy, which in turn negatively affects the MixedNUTS model. Fortunately, this worst case is easily
avoidable in practice by checking hM(s,p,c)
rob (Â·)â€™s standalone clean accuracy. If hM(s,p,c)
rob (Â·)â€™s clean accuracy
deteriorates, the search space for s,p, andccan be adjusted accordingly before re-running Algorithm 1.
D.8.3 Relaxing the Independence Assumption in Theorem 4.4
Theorem 4.4 assumes that the margin of hM
rob(X)and the correctness of gstd(X)are independent. Suppose that
such an assumption does not hold for a pair of base classifiers. Then, PXâˆ¼Xâœ—
clean/bracketleftbig
mhM
rob(X)â‰¥1âˆ’Î±
Î±/bracketrightbig
may not be
equal to PXâˆ¼Xâœ—
clean/bracketleftbig
mhM
rob(X)â‰¥1âˆ’Î±
Î±/vextendsingle/vextendsingleGcor(X)/bracketrightbig
. In this case, we need to minimize the latter quantity in order
to effectively optimize (2). Hence, we need to modify the objective functions of (3) and (6) accordingly, and
change the objective value assignment step in Line 10 of Algorithm 1 to oijkâ†PXâˆˆ/tildewideXâœ—
clean/bracketleftbig
mhM(si,pj,ck)
rob(X)â‰¥
qijk
1âˆ’Î²/vextendsingle/vextendsingleGcor(X)/bracketrightbig
. With such a modification, the optimization of s,p,cis no longer decoupled from gstd(Â·), but
the resulting algorithm is still efficiently solvable and Theorem 4.4 still holds.
D.9 Approximation Quality of (7)
Algorithm 1 solves (7) as a surrogate of (6) for efficiency. One of the approximations of (7) is to use the
minimum-margin perturbation against hLN
rob(Â·)instead of that associated with hM
rob(Â·). WhilehM
rob(Â·)andhLN
rob(Â·)
are expected to have similar standalone accuracy and robustness, their confidence properties are different,
and therefore the minimum-margin perturbation associated with hM
rob(Â·)can be different from that associated
withhLN
rob(Â·), inducing a distribution mismatch. To analyze the influence of this mismatch on the effectiveness
of Algorithm 1, we record the values of sâ‹†,pâ‹†, andcâ‹†, compute the minimum-margin-AutoAttacked examples
ofhM(sâ‹†,pâ‹†,câ‹†)
rob (Â·)and re-run Algorithm 1 with the new examples. If the objective value calculated via the
new examples and sâ‹†,pâ‹†,câ‹†is close to the optimal objective returned from the original Algorithm 1, then
the mismatch is small and benign and Algorithm 1 is capable of indirectly optimizing (6).
We use the CIFAR-100 model from Figure 5 as an example to perform this analysis. The original optimal
objective returned by Algorithm 1 is 50.0%. The re-computed objective based on hM(sâ‹†,pâ‹†,câ‹†)
rob (Â·)â€™s minimum-
margin perturbations, where sâ‹†=.612,pâ‹†= 3.57,câ‹†=âˆ’2.14, is66.7%. While there is a gap between the
two objective values and therefore the approximation-induced distribution mismatch exists, Algorithm 1 can
still effectively decrease the objective value of (6).
27