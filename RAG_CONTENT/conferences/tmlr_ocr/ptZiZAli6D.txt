Under review as submission to TMLR
MANDERA: Malicious Node Detection in Federated Learn-
ing via Ranking
Anonymous authors
Paper under double-blind review
Abstract
Byzantine attacks aim to hinder the deployment of federated learning algorithms by sending
malicious gradients to degrade the model. Although the benign gradients and Byzantine
gradients are distributed diï¬€erently, identifying the malicious gradients is challenging due to
(1) the gradient is high-dimensional and each dimension has its unique distribution, and (2)
the benign gradients and the malicious gradients are mixed (two-sample test methods cannot
apply directly). To address these issues, we propose MANDERA which is theoretically
guaranteed to eï¬ƒciently detect all malicious gradients under Byzantine attacks with no
prior knowledge or history about the number of attacked nodes. More speciï¬cally, we
proposed to transfer the original updating gradient space into a ranking matrix. By such an
operation, the scales of diï¬€erent dimensions of the gradients in the ranking space become
identical. Then the high-dimensional benign gradients and the malicious gradients can be
easily separated in the ranking space. The eï¬€ectiveness of MANDERA is further conï¬rmed
by experimentation on fourByzantine attack implementations (Gaussian, Zero Gradient,
Sign Flipping, Shifted Mean), compared with state-of-the-art defences. The experiments
cover both IID and Non-IID datasets.
1 Introduction
Federated Learning (FL) is a decentralized learning framework that allows multiple participating nodes to
learn on a local collection of training data. The updating gradient values of each respective node are sent
to a global coordinator for aggregation. The global model collectively learns from each of these individual
nodes by aggregating the gradient updates before relaying the updated global model back to the participating
nodes. The aggregation of multiple nodes allows the model to learn from a larger dataset which will result in
a model having greater performance than the ones only learning on their local subset of data. FL presents
two key advantages: (1) the increase of privacy for the contributing node as local data is not communicating
with the global coordinator, and (2) a reduction in computation by the global node as the computation is
oï¬„oaded to contributing nodes.
However, FL is vulnerable to various attacks, including data poisoning attacks (Tolpegin et al., 2020) and
Byzantine attacks (Lamport et al., 2019). The presence of malicious actors in the collaborative process may
seek to poison the performance of the global model, to reduce the output performance of the model (Chen
et al., 2017; Baruch et al., 2019; Fang et al., 2020; Tolpegin et al., 2020), or to embed hidden back-doors
within the model (Bagdasaryan et al., 2020). Byzantine attack aims to devastate the performance of the
global model by manipulating the gradient values. These gradient values that have been manipulated are
sent from malicious nodes which are unknown to the global node. The Byzantine attacks can result in a
global model which produces an undesirable outcome (Lamport et al., 2019).
Researchers seek to defend FL from the negative impacts of these attacks. This can be done by either
identifying the malicious nodes or making the global model more robust to these types of attacks. In our
paper, we focus on identifying the malicious nodes to exclude the nodes which are deemed to be malicious in
the aggregation step to mitigate the impact of malicious nodes. Most of the existing methods rely on the
gradient values to determine whether a node is malicious or not, for example, Blanchard et al. (2017); Yin
1Under review as submission to TMLR
et al. (2018); Guerraoui et al. (2018); Li et al. (2020); Fang et al. (2020); Cao et al. (2020); Wu et al. (2020b);
Xie et al. (2019; 2020); Cao et al. (2021) and So et al. (2021). All the above methods are eï¬€ective in certain
scenarios.
10 20 30Gradient Ranking
0 20 40 600 20 40 600 20 40 600.498400.498450.49850
102030
MeanSDNode.type
Benign
Malicious
Figure 1: Patterns of nodes in gradient space and ranking space respectively under mean shift attacks. The
columns of the ï¬gure represent the number of malicious nodes among 100 nodes: 10, 20 and 30.
There is a lack of theoretical guarantee to detect all the malicious nodes in the literature. Although the extreme
malicious gradients can be excluded by the above approaches, some malicious nodes could be mis-classiï¬ed
as benign nodes and vice versa. The challenging issues in the community are caused by the following two
phenomena: [F1] the gradient values of benign nodes and malicious nodes are often non-distinguishable; [F2]
the gradient matrix is always high-dimensional (large column numbers) and each dimension follows its unique
distribution. The phenomenon [F1] indicates that it is not reliable to detect malicious nodes only using a
single column from the gradient matrix. And the phenomenon [F2] hinders us from using all the columns of
the gradient matrix, because it requires a scientiï¬c way to accommodate a large number of columns which
are distributed considerably diï¬€erently.
In this paper, we propose to resolve these critical challenges from a novel perspective. Instead of working on the
node updates directly, we propose to extract information about malicious nodes indirectly by transforming the
node updates from numeric gradient values to the ranking space. Compared to the original numeric gradient
values, whose distribution is diï¬ƒcult to model, the rankings are much easier to handle both theoretically
and practically. Moreover, as rankings are scale-free, we no longer need to worry about the scale diï¬€erence
across diï¬€erent dimensions. We proved under mild conditions that the ï¬rst two moments of the transformed
ranking vectors carry key information to detect the malicious nodes under Byzantine attacks. Based on these
theoretical results, a highly eï¬ƒcient method called MANDERA is proposed to separate the malicious nodes
from the benign ones by clustering all local nodes into two groups based on the ranking vectors. Figure 1
shows an illustrative motivation for our method. It demonstrates the behaviors of malicious and benign nodes
under mean shift attacks. Obviously, the malicious and benign nodes are not distinguishable in the gradient
space due to the challenges we mentioned above, while they are well separated in the ranking space.
The contributions of this work are as follows: (1)we propose the ï¬rst algorithm leveraging the ranking
space of model updates to detect malicious nodes (Figure 2); (2)we provide a theoretical guarantee for the
detection of malicious nodes based on the ranking space under Byzantine attacks; (3)our method does not
assume knowledge of the number of malicious nodes, which is required in the learning process of most of
the prior methods; (4)we experimentally demonstrate the eï¬€ectiveness and robustness of our defense on
Byzantine attacks, including Gaussian attack (GA), Sign Flipping attack (SF), Zero Gradient attack (ZG)
and Mean Shift attack (MF); (5)an experimental comparison between MANDERA and a collection of robust
aggregation techniques is provided.
Related works. In the literature, there have been a collection of eï¬€orts along the research on defensing
Byzantine attacks. Blanchard et al. (2017) propose a defense referred to as Krum that treats local nodes
whose update vector is too far away from the aggregated barycenter as malicious nodes and precludes
them from the downstream aggregation. Guerraoui et al. (2018) propose Bulyan, a process that performs
aggregation on subsets of node updates (by iteratively leaving each node out) to ï¬nd a set of nodes with the
most aligned updates given an aggregation rule. Cao et al. (2020) maintains a trusted model and dataset on
2Under review as submission to TMLR
Global modelMANDERARejectMalicious Update0.03, 0.12, 0.06, 0.20, 0.90,  Rank ğ‘´column-wiseğ‘´:,ğŸ			ğ‘´:,ğŸğ‘´:,ğŸ‘			ğ‘´:,ğŸ’		â€¦			ğ‘´:,ğ’‘0.72, 0.90, 0.69, 0.70, 0.12,  0.48, 0.42, 0.43, 0.50, 0.81,  0.18, 0.20, 0.16, 0.30, 0.00,  MeanSDFind and reject malicious cluster
Aggregate and updateglobal modelwith benign clusterCompute MeanandSDofğ‘¹row-wise0.90 0.21 0.22 0.31 0.10  â€¦5, 3, 4, 2, 1,  2, 1, 4, 3, 5,  3, 5, 4, 2, 1,  2, 4, 3, 1, 5,  1 4 3 2 5  â€¦ğ‘¹:,ğŸ					ğ‘¹:,ğŸ				ğ‘¹:,ğŸ‘				ğ‘¹:,ğŸ’		â€¦				ğ‘¹:,ğ’‘
Figure 2: An overview of MANDERA.
which submitted node updates may be bootstrapped by weighting each nodeâ€™s update in the aggregation step
based on itâ€™s cosine similarity to the trusted update. Xie et al. (2019) compute a Stochastic Descendant Score
(SDS) based on the estimated descendant of the loss function and the magnitude of the update submitted to
the global node, and only include a predeï¬ned number of nodes with the highest SDS in the aggregation.
On the other hand, Chen et al. (2021) propose a zero-knowledge approach to detect and remove malicious
nodes by solving a weighted clustering problem. The resulting clusters update the model individually and
accuracy against a validation set is checked. All nodes in a cluster with signiï¬cant negative accuracy impact
are rejected and removed from the aggregation step.
2 Defense against Byzantine attacks via Ranking
In this section, notations are ï¬rst introduced and an algorithm to detect malicious nodes is proposed.
2.1 Notations
Suppose there are nlocal nodes in the federated learning framework, where n1nodes are benign nodes whose
indices are denoted by Iband the other n0=nâˆ’n1nodes are malicious nodes whose indices are denoted
byIm. The training model is denoted by f(Î¸,D), whereÎ¸âˆˆRpÃ—1is ap-dimensional parameter vector
andDis a data matrix. Denote the message matrix received by the central server from all local nodes as
MâˆˆRnÃ—p, whereMi,:denotes the message received from node i. For a benign node i, letDibe the data
matrix on it with Nias the sample size, we have Mi,:=âˆ‚f(Î¸,Di)
âˆ‚Î¸|Î¸=Î¸âˆ—, whereÎ¸âˆ—is the parameter value from
the global model. In the rest of the paper, we suppressâˆ‚f(Î¸,Di)
âˆ‚Î¸|Î¸=Î¸âˆ—toâˆ‚f(Î¸,Di)
âˆ‚Î¸to denote the gradient
value for simplicity purpose. A malicious node jâˆˆIm, however, tends to attack the learning system by
manipulating Mj,:in some way. Hereinafter, we denote Nâˆ—=min({Ni}iâˆˆIb)to be the minimal sample size
of the benign nodes.
Given a vector of real numbers aâˆˆRnÃ—1, deï¬ne its ranking vector as b=Rank (a)âˆˆperm{1,Â·Â·Â·,n}, where
the ranking operator Rankmaps the vector ato an element in permutation space perm{1,Â·Â·Â·,n}which is
the set of all the permutations of {1,Â·Â·Â·,n}. For example, Rank (1.1,âˆ’2,3.2) = (2,3,1), it ranks the values
from largest to smallest. We adopt average ranking, when there are ties. With the Rankoperator, we can
transfer the message matrix Mto a ranking matrix Rby replacing its column M:,jby the corresponding
ranking vector R:,j=Rank (M:,j). Further, deï¬ne
ei,1
pp/summationdisplay
j=1Ri,jandvi,1
pp/summationdisplay
j=1(Ri,jâˆ’ei)2
to be the mean and variance of Ri,:, respectively. As it is shown in later subsections, we can judge whether
nodeiis a malicious node based on (ei,vi)under various attack types. In the following, we will highlight the
behavior of the benign nodes ï¬rst, and then discuss the behavior of malicious nodes and their diï¬€erence with
the benign nodes under Byzantine attacks.
2.2 Behaviors of nodes under Byzantine attacks
Byzantine attacks aim to devastate the global model by manipulating the gradient values of some local
nodes. For a general Byzantine attack, we assume that the gradient vectors of benign nodes and malicious
3Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
40455055 40455055 40455055 40455055 40455055 40455055203040
2530354045
25303540
102030
eisiNode.type
Benign
Malicious
Figure 3: The scatter plots of (ei,si)for the 100 nodes under four types of attack as illustrative examples
demonstrating ranking mean and standard deviation from the 1st epoch of training for the FASHION-MNIST
dataset. Four attacks are Gaussian Attack (GA), Zero Gradient attack (ZG), Sign Flipping attack (SF) and
Mean shift attack (MS).
nodes follow two diï¬€erent distributions GandF. We would expect systematical diï¬€erences in their behavior
patterns in the ranking matrix R, based on which malicious node detection can be achieved. Theorem 2.1
demonstrates the concrete behaviors of benign nodes and malicious nodes under general Byzantine attacks.
Theorem 2.1 (Behavior under Byzantine attacks) .For a general Byzantine attack, assume that the gradient
values from benign nodes and malicious nodes follow two distributions G(Â·)andF(Â·)respectively (both Gand
Farep-dimensional). We have
lim
Nâˆ—â†’âˆlim
pâ†’âˆei= Â¯ÂµbÂ·I(iâˆˆIb) + Â¯ÂµmÂ·I(iâˆˆIm)a.s.,
lim
Nâˆ—â†’âˆlim
pâ†’âˆvi= Â¯s2
bÂ·I(iâˆˆIb) + Â¯s2
mÂ·I(iâˆˆIm)a.s.,
where (Â¯Âµb,Â¯s2
b)and(Â¯Âµm,Â¯s2
m)are highly non-linearly functions of G(Â·)andF(Â·)whose concrete form is detailed
in the Appendix A, and â€œa.s.â€ is the abbreviation of â€œalmost surelyâ€.
The proof can be found in the Appendix A. If the attackers can access the exact distribution G, which is very
rare, an obvious strategy to evade defense is to let F=G. In this case, the attack will have no impact on the
global model. More often, the attackers have little information about distribution G. In this case, it is a rare
event for the attackers to design a distribution Fsatisfying (Â¯Âµb,Â¯s2
b) = ( Â¯Âµm,Â¯s2
m)for the malicious nodes to
follow. In fact, most popular Byzantine attacks never try to make such an eï¬€ort at all. Thus, the malicious
nodes and the benign nodes are distinguishable with respect to their feature vectors {(ei,vi)}1â‰¤iâ‰¤n, because
(ei,vi)reaches to diï¬€erent limits for begin and malicious nodes. Considering that the standard deviation
si=âˆšviis typically of the similar scale of ei, hereinafter we employ (ei,si), instead of (ei,vi), as the feature
vector of node ifor malicious node detection.
Figure 3 illustrates the typical scatter plots of (ei,si)for benign and malicious nodes under four typical
Byzantine attacks, i.e., GA, SN, ZG and MS. It can be observed that malicious nodes and benign nodes are
all well separated in these scatter plots, indicating a proper clustering algorithm will distinguish these two
groups. We note that both siandeiare informative for malicious node detection, since in some cases (e.g.,
under Gaussian attacks) it is diï¬ƒcult to distinguish malicious nodes from benign ones based on eionly.
2.3 Algorithm for Malicious node detection under Byzantine attacks
Theorem 2.1 implies that, under general Byzantine attacks, the feature vector (ei,si)of nodeiconverges to
two diï¬€erent limits for benign and malicious nodes, respectively. Thus, for a real dataset where Niâ€™s andp
are all ï¬nite but reasonably large numbers, the scatter plot of {(ei,si)}1â‰¤iâ‰¤nwould demonstrate a clustering
structure: one cluster for the benign nodes and the other cluster for the malicious nodes.
4Under review as submission to TMLR
Algorithm 1 MANDERA
Input:The message matrix M.
1:Convert the message matrix Mto the ranking matrix Rby applying Rankoperator.
2:Compute mean and standard deviation of rows in R, i.e.,{(ei,si)}1â‰¤iâ‰¤n.
3:Run the clustering algorithm K-means to{(ei,si)}1â‰¤iâ‰¤nwithK= 2, and predict the set of benign nodes
with the larger cluster denoted by Ë†Ib.
Output: The predicted benign node set Ë†Ib.
Based on this intuition, we propose MAlicious Node DEtection via RAnking (MANDERA) to detect the
malicious nodes, whose workï¬‚ow is detailed in Algorithm 1. MANDERA can be applied to either a single
epoch or multiple epochs. For a single-epoch mode, the input data Mis the message matrix received from a
single epoch. For multiple-epoch mode, the data Mis the column-concatenation of the message matrices
from multiple epochs. By default, the experiments below all use a single epoch to detect the malicious nodes.
The predicted benign nodes Ë†Ibobtained by MANDERA naturally leads to an aggregated message Ë†mb,:=
1
#(Ë†Ib)/summationtext
iâˆˆË†IbMi,:. Theorem 2.2 shows that Ë†Iband Ë†mblead to consistent estimations of Ibandmb=
1
n1/summationtext
iâˆˆIbMi,:respectively, indicating that MANDERA enjoys robustness guarantee Steinhardt (2018) for
Byzantine attacks.
Theorem 2.2 (Robustness guarantee) .Under Byzantine attacks, we have:
lim
Nâˆ—,pâ†’âˆP(Ë†Ib=Ib) = 1,lim
Nâˆ—,pâ†’âˆE||Ë†mb,:âˆ’mb,:||2= 0.
The proof of Theorem 2.2 can be found in Appendix B. As E(Ë†mb,:) =mb,:, MANDERA obviously satisï¬es
the(Î±,f)-Byzantine Resilience condition, which is used in Blanchard et al. (2017) and Guerraoui et al. (2018)
to measure the robustness of their estimators.
3 Theoretical analysis for speciï¬c Byzantine attacks
Theorem 2.1 provides us general guidance about the behavior of nodes under Byzantine attacks. In this
section, we examine the behavior for speciï¬c attacks, including Gaussian attacks, zero gradient attacks, sign
ï¬‚ipping attacks and mean shift attacks.
As the behavior of benign nodes does not depend on the type of Byzantine attack, we can study the statistical
properties of (ei,vi)for a benign node iâˆˆIbbefore the speciï¬cation of a concrete attack type. For any
benign node i, the message generated for jthparameter isMi,j=1
Ni/summationtextNi
l=1âˆ‚f(Î¸,Di,l)
âˆ‚Î¸j, whereDi,ldenotes the
lthsample on it. Throughout this paper, we assume that Di,lâ€™s are independent and identically distributed
(IID) samples drawn from a data distribution D.
Lemma 3.1. Under the IID data assumption, further denote Âµj=E/parenleftBig
âˆ‚f(Î¸,Di,l)
âˆ‚Î¸j/parenrightBig
andÏƒ2
j=Var/parenleftBig
âˆ‚f(Î¸,Di,l)
âˆ‚Î¸j/parenrightBig
<
âˆ, withNigoing to inï¬nity, for âˆ€jâˆˆ{1,Â·Â·Â·,p}, we haveMi,jâ†’Âµjalmost surely (a.s.) and Mi,jâ†’d
N/parenleftbig
Âµj,Ïƒ2
j/Ni/parenrightbig
.
Lemma 3.1 can be proved by using the Kolmogorovâ€™s Strong Law of Large Numbers (KSLLN) and Central
Limit Theorem. For the rest of this section, we will derive the detailed forms of Â¯Âµb,Â¯Âµm,Â¯s2
bandÂ¯s2
m, as deï¬ned
in Theorem 2.1, under four speciï¬c Byzantine attacks.
3.1 Gaussian attack
Deï¬nition 3.2 (Gaussian attack) .In a Gaussian attack, the attacker generates malicious gradient values
as follows:{Mi,:}iâˆˆImâˆ¼MVN (mb,:,Î£), wheremb,:=1
n1/summationtext
iâˆˆIbMi,:is the mean vector of Gaussian
distribution and Î£is the covariance matrix determined by the attacker.
5Under review as submission to TMLR
0200040006000
0.00 0.25 0.50 0.75 1.00
p.valuecount
Figure 4: Independence test for 100,000 column pairs randomly chosen from message matrix Mgenerated
from FASHION-MNIST data.
Considering that Mi,jâ†’Âµja.s. withNigoing to inï¬nity for all iâˆˆIbbased on Deï¬nition 3.2, it is
straightforward to see that limNâˆ—â†’âˆmb,j=Âµja.s.,and the distribution of Mi,jfor eachiâˆˆImconverges
to the Gaussian distribution centered at Âµj. Based on this fact, the limiting behavior of the feature vector
(ei,vi)can be established for both benign and malicious nodes. Theorem 3.3 summarizes the results, with
the proof detailed in Appendix C.
Theorem 3.3 (Behavior under Gaussian attacks) .Assuming{R:,j}1â‰¤jâ‰¤pare independent of each other,
under the Gaussian attack, the behaviors of benign and malicious nodes are as follows:
Â¯Âµb= Â¯Âµm=n+ 1
2,Â¯s2
b=1
pp/summationdisplay
j=1s2
b,j,Â¯s2
m=1
pp/summationdisplay
j=1s2
m,j,
wheres2
b,jands2
m,jare both complex functions of n0,n1,Ïƒ2
j,Î£j,jandNâˆ—whose concrete form is detailed in
the Appendix C.
Considering that Â¯s2
b=Â¯s2
mif and only if Î£j,jâ€™s fall into a lower dimensional manifold whose measurement is
zero under the Lebesgue measure, we have P(Â¯s2
b=Â¯s2
m) = 0if the attacker speciï¬es the Gaussian variance
Î£j,jâ€™s arbitrarily in the Gaussian attack. Thus, Theorem 3.3 in fact suggests that the benign nodes and the
malicious nodes are diï¬€erent on the value of vi, and therefore provides a guideline to detect the malicious
nodes. Although the we do need Nâˆ—andpto go to inï¬nity for getting the theoretical results in Theorem 3.3,
in practice the malicious node detection algorithm based on the theorem typically works very well when Nâˆ—
andpare reasonably large and Niâ€™s are not dramatically far away from each other.
The independent ranking assumption in Theorem 3.3, which assumes that {R:,j}1â‰¤jâ‰¤pare independent of
each other, may look restrictive. However, in fact it is a mild condition that can be easily satisï¬ed in practice
due to the following reasons. First, for a benign node iâˆˆIb,Mi,jandMi,kare often nearly independent, as
the correlation between two model parameters Î¸jandÎ¸kis often very weak in a large deep neural network
with a huge number of parameters. To verify the statement, we implemented independence tests for 100,000
column pairs randomly chosen from the message matrix Mgenerated from the FASHION-MNIST data.
Distribution of the p-values of these tests are demonstrated in Figure 4 via a histogram, which is very close
to a uniform distribution, indicating that Mi,jandMi,kare indeed nearly independent in practice. Second,
even someM:,jandM:,kshow a strong correlation, the magnitude of the correlation would be reduced
greatly during the transformation from MtoR, as the ï¬nal ranking Ri,jalso depends on many other factors.
Actually, the independent ranking assumption could be relaxed to be an uncorrelated ranking assumption
which assumes the rankings are uncorrelated with each other. Adopting the weaker assumption will result in
a change in the convergence type of our theorems from the â€œalmost surely convergenceâ€ to â€œconvergence in
probabilityâ€.
3.2 Sign ï¬‚ipping attack
Deï¬nition 3.4 (Sign ï¬‚ipping attack) .Sign ï¬‚ipping attack aims to generate the gradient values of malicious
nodes by ï¬‚ipping the sign of the average of all the benign nodesâ€™ gradient at each epoch, i.e., specifying
Mi,:=âˆ’rmb,:for anyiâˆˆIm, wherer>0,mb=1
n1/summationtext
kâˆˆIbMk,:.
6Under review as submission to TMLR
Based on the above deï¬nition, the update message of a malicious node iunder the sign ï¬‚ipping attack is
Mi,:=âˆ’rmb,:=âˆ’r
n1/summationtext
kâˆˆIbMk,:. The theorem 3.5 summarizes the behavior of malicious nodes and benign
nodes respectively, with the detailed proof provided in Appendix D.
Theorem 3.5 (Behavior under sign ï¬‚ipping attacks) .With the same assumption as posed in Theorem 3.3,
under the sign ï¬‚ipping attack, the behaviors of benign and malicious nodes are as follows:
Â¯Âµb=n+n0+1
2âˆ’n0Ï,Â¯Âµm=n1Ï+n0+1
2,
Â¯s2
b=ÏS2
[1,n1]+ (1âˆ’Ï)S2
[n0+1,n]âˆ’(Â¯Âµb)2,
Â¯s2
m=ÏS2
[n1+1,n]+ (1âˆ’Ï)S2
[1,n0]âˆ’(Â¯Âµm)2,
whereÏ=limpâ†’âˆ/summationtextp
j=1I(Âµj>0)
pwhich depends on n0andn1,S2
[a,b]=1
bâˆ’a+1/summationtextb
k=ak2. And Â¯s2
mandÂ¯s2
bare
both quadratic functions of Ï.
Considering that Â¯Âµb=Â¯Âµmif and only if Ï=1
2, and Â¯s2
b=Â¯s2
mif and only if Ïis the solution of a quadratic
function, the probability of (Â¯Âµb,Â¯s2
b) = ( Â¯Âµm,Â¯s2
m)is zero aspâ†’âˆ. Such a phenomenon suggests that we
can detect the malicious nodes based on the moments (ei,vi)to defense the sign ï¬‚ipping attack as well.
Noticeably, we note that the limit behavior of eiandvidoes not dependent on the speciï¬cation of r, which
deï¬nes the sign ï¬‚ipping attack. Although such a fact looks a bit abnormal at the ï¬rst glance, it is totally
understandable once we realize that with the variance of Mi,jshrinks to zero with Nigoes to inï¬nity for
each benign node i, any diï¬€erent between ÂµjandÂµj(r)would result in the same ranking vector R:,jin the
ranking space.
3.3 Zero gradient attack
Deï¬nition 3.6 (Zero gradient attack) .Zero gradient attack aims to make the aggregated message to be
zero, i.e.,/summationtextn
i=1Mi,:= 0, at each epoch, by specifying Mi,:=âˆ’n1
n0mb,:for alliâˆˆIm.
Apparently, the zero gradient attack deï¬ned above is a special case of sign ï¬‚ipping attack by specifying
r=n1
n0. The conclusions of Theorem 3.5 keep unchanged for diï¬€erent speciï¬cations of r. Therefore, the
behavior follows the same limiting behaviors as described in Theorem 3.5.
3.4 Mean shift attack
Deï¬nition 3.7 (Mean shift attack) .Mean shift attack (Baruch et al., 2019) manipulates the updates
of the malicious nodes in the following fashion, mi,j=Âµjâˆ’zÂ·ÏƒjforiâˆˆImand1â‰¤jâ‰¤p, where
Âµj=1
n1/summationtext
iâˆˆIbMi,j,Ïƒj=/radicalBig
1
n1/summationtext
iâˆˆIb(Mi,jâˆ’Âµj)2andz= arg maxtÏ†(t)<nâˆ’2
2(nâˆ’n0).
Mean shift attacks aim to generate malicious gradients which are not well separated, but of diï¬€erent
distributions, from the benign nodes. Theorem 3.8 details the behavior of malicious nodes and benign nodes
under mean shift attacks. The proof can be found in Appendix E
Theorem 3.8. With the same assumption as posed in Theorem 3.3 and additionally nis relatively large,
under the mean shift attack, the behaviors of benign and malicious nodes are as follows:
Â¯Âµb=n+1
2+n0
n1(n1âˆ’Î±),Â¯Âµm=Î±+n0+1
2,
Â¯s2
b=1
n1(Ï„(n) +Ï„(Î±)âˆ’Ï„(Î±+ 1 +n0))âˆ’Â¯Âµ2
b,Â¯s2
m= 0,
whereâŒŠÂ·âŒ‹denotes the ï¬‚oor function, Î±=âŒŠn1Î¦(z)âŒ‹,Î¦(z)is the cumulative density function of the standard
normal distribution and Ï„(Â·)is the function of â€˜sum of squaresâ€™, i.e., Ï„(n) =/summationtextn
k=1k2.
4 Experiments
In these experiments we extend the data poisoning experimental framework of Tolpegin et al. (2020); Wu
et al. (2020a), integrating Byzantine attack implementations released by Wu et al. (2020b) and the mean
7Under review as submission to TMLR
shift attack Baruch et al. (2019). The mean shift attack was designed to poison gradients by adding â€˜a
littleâ€™ amount of noise, and shown to be eï¬€ective in defeating Krum (Blanchard et al., 2017) and Bulyan
(Guerraoui et al., 2018) defenses. The mean shift attack is deï¬ned in Deï¬nition 3.7. In our experiments, we
setÎ£ = 30Ifor the Gaussian attack and r= 3for the sign ï¬‚ipping attack, where Iis the identity matrix.
For all experiments we ï¬x n= 100participating nodes, of which a variable number of nodes are poisoned
|n0|âˆˆ{5,10,15,20,25,30}. The training process is run until 25 epochs have elapsed. We have described the
structure of these networks in Appendix F.
4.1 Defense by MANDERA for IID Settings
We evaluate the eï¬ƒcacy in detecting malicious nodes within the federated learning framework with the use
of three IID datasets. The ï¬rst is the FASHION-MNIST dataset Xiao et al. (2017), a dataset of 60,000
and 10,000 training and testing samples respectively divided into 10 classes of apparel. The second is
CIFAR-10 Krizhevsky et al. (2009), a dataset of 60,000 small object images also containing 10 object classes.
The third is the MNIST Deng (2012) dataset. The MNIST dataset is a dataset of 60,000 and 10,000 training
and testing samples respectively divided into 10 classes of handwritten digits from multiple authors.
We test the performance of MANDERA on the update gradients of a model under attacks. In this section,
MANDERA acts as an observer without intervening in the learning process to identify malicious nodes with
a set of gradients from a single epoch. Each conï¬guration of 25 training epochs, with a given number of
malicious nodes was repeated 20 times. Figure 5 demonstrates the classiï¬cation performance (Metrics deï¬ned
in Appendix G) of MANDERA with diï¬€erent settings of participating malicious nodes and the four poisoning
attacks, i.e., GA, ZG, SF and MS.
While we have formally demonstrated the eï¬ƒcacy of MANDERA in accurately detecting potentially malicious
nodes participating in the federated learning process. In practice, to leverage an unsupervised K-means
clustering algorithm, we must also identify the correct group of nodes as the malicious group. Our strategy
is to identify the group with the most exact gradients, or otherwise the smaller group (we regard a system
with over 50% of their nodes compromised as having larger issues than just poisoning attacks).1We also test
other clustering algorithms, such as hierarchical clustering and Gaussian mixture models Fraley & Raftery
(2002). It turns out that the performance of MANDERA is quite robust with diï¬€erent choices of clustering
methods. Detailed results can be found in Appendix I. From Figure 5, it is immediately evident that the
recall of the malicious nodes for the Byzantine attacks is exceptional. However, occasionally benign nodes
have also been misclassiï¬ed as malicious under SF attacks. On all attacks, in the presence of more malicious
nodes, the recall of malicious nodes trends down.
We encapsulate MANDERA into a module prior to the aggregation step, MANDERA has the sole objective of
identifying malicious nodes, and excluding their updates from the global aggregation step. Each conï¬guration
of 25 training epochs, a given poisoning attack, defense method, and a given number of malicious nodes
was repeated 10 times. We compare MANDERA against 5 other robust aggregation defense methods,
Krum Blanchard et al. (2017), Bulyan Guerraoui et al. (2018), Trimmed Mean Yin et al. (2018), Median Yin
et al. (2018) and FLTrust Cao et al. (2020). Of which the ï¬rst 2 require an assumed number of malicious
nodes, and the latter 3 only aggregate robustly.
Table 1 demonstrates the accuracy of the global model at the 25th epoch under four Byzantine attacks and
six defense strategies, using the MNIST-Digits data set. It shows MANDERA universally outperforms all the
other competing defence strategies for the MNIST-Digits data set. Note that MANDERA is approaching
(sometimes even better than) the performance of a model which is not attacked. Interestingly, FLTrust as a
standalone defense is weak in protecting against the most extreme Byzantine attacks. However, we highlight
that FLtrust is a robust aggregation method against speciï¬c attacks that may thwart defences like Krum,
Trimmed mean. We see FLTrust as a complementary defence that relies on a base method of defence against
Byzantine attacks, but expands the protection coverage of the FL system against adaptive attacks.
1More informed approaches to selecting the malicious cluster can be tested in future work. E.g. Figure 3 displays less
variation of ranking variance in malicious cluster compared to benign nodes. This could robust selection of the malicious group,
and enabling selection of malicious groups larger than 50%.
8Under review as submission to TMLR
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(a) CIFAR-10
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1 (b) FASHION-MNIST
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(c) MNIST-Digits
Figure 5: Classiï¬cation performance of our proposed approach MANDERA under four types of attack for
three IID settings.
The performance of all the epochs for MNIST-Digits can be found in Figure 6. It consistently shows
MANDERA outperforms the other competing strategies at each epoch. For the performance of the other
two data sets, see Appendix H, where MANDERA also performs better than other defence strategies. The
corresponding model losses can be found in Appendix J.
4.2 Defense by MANDERA for non-IID Settings
In this section, we evaluate the applicability of MANDERA when applied in a non-IID setting in Federated
learning to validate its eï¬€ectiveness. The batch size present through the existing evaluations of Section 4.1 is
10. This low setting practically yields gradient values at each local worker node as if they were derived from
non-IID samples. This is a strong indicator that MANDERA could be eï¬€ective for non-IID settings. We
reinforce MANDERAâ€™s applicability in the non-IID setting by repeating the experiment on QMNIST Yadav
& Bottou (2019), a dataset that is per-sample equivalent to MNIST Deng (2012). QMNIST, however,
additionally provides us with writer identiï¬cation information. This identity is leveraged to ensure that each
local node only trains on digits written by a set of unique users not seen by other workers. Such a setting is
widely recognized as non-IID setting in the community (Kairouz et al., 2021). For 100 nodes, this works
9Under review as submission to TMLR
Table 1: MNIST-Digits model accuracy at 25th epoch. The boldhighlights the best defense strategy under
attack. â€œNO-attackâ€ is the baseline, where no attack is conducted. And n0denotes the number of malicious
nodes among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 96.77 96.63 96.78 96.89 96.90 96.90
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 98.46 98.43 98.40 98.36 98.35 98.29
Median 98.33 98.31 98.32 98.31 98.31 98.34
Trim-mean 98.45 98.43 98.41 98.38 98.38 98.35
MANDERA 98.48 98.46 98.44 98.43 98.44 98.42
FLTrust 95.33 65.22 61.02 37.45 11.37 12.17
ZGKrum 96.95 96.35 96.93 96.96 97.07 96.50
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 97.97 98.19 98.25 98.24 98.17 98.13
Median 98.17 98.00 97.74 97.36 96.77 96.10
Trim-mean 98.12 97.89 97.54 97.06 96.55 95.69
MANDERA 98.47 98.35 98.44 98.46 98.44 98.41
FLTrust 97.78 95.42 94.09 89.74 87.33 93.08
SFKrum 96.82 96.73 96.79 96.77 96.78 96.69
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 98.38 98.35 98.30 98.25 98.19 98.13
Median 98.16 98.00 97.75 97.33 96.78 96.14
Trim-mean 98.24 98.03 97.69 97.17 96.58 95.56
MANDERA 98.51 98.47 98.44 98.43 98.41 98.40
FLTrust 98.28 98.02 97.55 97.02 90.58 84.53
MSKrum 98.45 98.40 98.34 98.33 98.29 98.24
NO-attack 98.45 98.45 98.45 98.45 98.45 98.45
Bulyan 98.42 98.38 98.38 98.33 98.27 98.23
Median 98.41 98.39 98.33 98.28 98.25 98.23
Trim-mean 98.46 98.41 98.38 98.34 98.29 98.26
MANDERA 98.48 98.45 98.46 98.43 98.44 98.44
FLTrust 98.46 98.44 98.45 98.42 98.42 98.38
out to be approximately 5 writers in each node. All other experimental conï¬gurations remain the same as
Section 4.1.
Figure 7 demonstrates the eï¬€ectiveness of MANDERA in malicious node detection for the non-IID setting.
These results are very similar to the results where data is IID settings. Except for sign-ï¬‚ipping attacks,
MANDERA can perfectly distinguish malicious nodes from benign nodes. when the number of malicious
nodes is less than 25, MANDERA mis-classiï¬es some benign nodes as malicious under sign-ï¬‚ipping attacks.
It is noticeable that even though MANDERA does not perform perfectly for SF attacks, the recall is always
equal to 1. This indicates that all the malicious nodes are correctly identiï¬ed, but a few of benign nodes
are misclassiï¬ed as malicious nodes. This is important to understand why MANDERA outperforms the
completing defence strategies, as shown in Table 2.
Table 2 shows the global model training accuracy with diï¬€erent defense strategies for a non-IID setting. It
indicates that MANDERA almost universally outperforms the other defensing strategies and achieves the
best performance. Considering the performance of malicious detection under GA, ZG and MS, shown in
Figure 7, it is natural to expect a good performance of MANDERA in terms of the accuracy of the global
model. At the ï¬rst glance, it is puzzling to observe MANDERA outperforms the others under SF attacks,
considering the â€˜badâ€™ performance of malicious node detection under SF attacks. To explain this phenomenon,
we should pay special attention to the recall in Figure 7. A recall of 1 indicates all the malicious nodes are
identiï¬ed. Low values of accuracy and precision mean that some â€˜extremeâ€™ benign nodes are identiï¬ed as
malicious nodes. Therefore, the aggregated gradient values using MANDERA are close to the true gradient
values, resulting in high accuracy. The results for all the epochs can be found in Figure 8. The corresponding
model losses can be found in Appendix K.
10Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025255075100
60708090100
60708090100
909396
Number of EpochAccuracyDefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
Figure 6: Model Accuracy at each epoch of training, each line of the curve represents a diï¬€erent defense against
the Byzantine attacks. Shown above is the result for MNIST-Digits, ï¬gures for CIFAR and FASHION-MNIST
can be found in the appendix.
4.3 Computational speed
MANDERA enjoys super-fast computation. We have previously been able to observe that MANDERA can
perform at par with the current highest-performing poisoning attack defenses. Another beneï¬t arises with
the simpliï¬cation of the mitigation strategy with the introduction of ranking at the core of the algorithm.
Sorting and Ranking algorithms are fast. Additionally, we only apply clustering on the two dimensions (mean
and standard deviation of the ranking), in contrast to other works that seek to cluster on the entire node
update Chen et al. (2021). The times in Table 3 for MANDERA, Krum and Bulyan do not include the
parameter/gradient aggregation step. These times were computed on 1 core of a Dual Xeon 14-core E5-2690,
with 8 Gb of system RAM and a single Nvidia Tesla P100. Table 3 demonstrates that MANDERA is able to
achieve a faster speed than that of single Krum2(by more than half) and Bulyan (by an order of magnitude).
We have listed the computational times of state-of-art methods in Table 3.
2The use of multi-krum would have yielded better protection (c.f. Section 4) at the behest of speed.
11Under review as submission to TMLR
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
Figure 7: Malicious node detection by MANDERA for a Non-IID data set: QMNIST under four diï¬€erent
Byzantine attacks.
5 Discussion and Conclusion
Theorem 2.1 indicates that Byzantine attacks can only evade MANDERA when the attackers know the
distribution of benign nodes and at the same time huge computational resources are required. This makes
MANDERA a strategy which is challenging for attackers to evade.
We acknowledge FL framework may learn the global model only using subset of nodes at each round. In
these settings MANDERA would still function, as we would rank and cluster on the parameters of the
participating nodes, without assuming any number of poisoned nodes. In Algorithm 1, performance could
be improved by incorporating higher order moments. MANDERA is unable to function when gradients are
securely aggregated in its current form. However, malicious nodes can be identiï¬ed and excluded from the
secure aggregation step, while still protecting the privacy of participating nodes by performing MANDERA
through secure ranking Zhang et al. (2013); Lin & Tzeng (2005) (recall that MANDERA only requires the
ranking matrix to detect poisoned nodes).
In conclusion, we proposed a novel way to tackle the challenges for malicious node detection when using the
gradient values. Our method transfers the gradient values to a ranking space. We have provided theoretical
guarantees and experimentally shown eï¬ƒcacy in MANDERA for the detection of malicious nodes performing
poisoning attacks against federated learning. Our proposed method MANDERA, is able to achieve excellent
detection accuracy and maintain a higher model accuracy than other seminal.
12Under review as submission to TMLR
Table 2: QMNIST model accuracy at 25th epoch. The boldhighlights the best defense strategy under attack.
â€œNO-attackâ€ is the baseline, where no attack is conducted. And n0denotes the number of malicious nodes
among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 94.16 93.87 93.95 94.10 94.27 93.89
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 98.09 98.07 98.06 98.02 97.99 97.88
Median 97.76 97.76 97.77 97.78 97.75 97.77
Trim-mean 98.08 98.04 98.00 97.96 97.91 97.85
MANDERA 98.11 98.11 98.12 98.10 98.10 98.08
FLTrust 83.48 57.32 25.75 18.80 15.43 9.75
ZGKrum 94.21 93.90 93.92 94.11 93.84 93.95
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 97.58 97.83 97.90 97.87 97.79 97.71
Median 97.59 97.27 96.84 96.33 95.54 94.45
Trim-mean 97.66 97.20 96.67 96.02 95.04 93.97
MANDERA 97.85 97.78 97.64 98.21 98.13 98.09
FLTrust 91.60 95.65 92.15 85.53 88.85 89.58
SFKrum 94.22 93.92 94.01 94.20 93.89 93.84
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 98.01 97.96 97.98 97.93 97.81 97.66
Median 97.61 97.29 96.84 96.33 95.58 94.55
Trim-mean 97.82 97.52 96.97 96.21 94.98 93.75
MANDERA 98.20 98.23 98.22 98.19 98.15 98.14
FLTrust 97.75 97.21 96.65 88.25 89.99 88.29
MSKrum 95.97 94.09 94.17 94.28 95.23 95.80
NO-attack 98.12 98.12 98.12 98.12 98.12 98.12
Bulyan 98.07 98.01 97.97 97.92 97.84 97.82
Median 97.88 97.96 97.96 97.90 97.79 97.70
Trim-mean 98.05 97.98 97.94 97.92 97.88 97.81
MANDERA 98.11 98.12 98.10 98.08 98.08 98.06
FLTrust 98.13 98.11 98.12 98.10 98.09 98.06
Table 3: Mean and standard deviation of computational times for defense function given the same set of
gradients from 100 nodes, of which 30 were malicious. Each function was repeated 100 times.
Defense (Detection) Mean Â±SD (ms) Defense (Aggregation) Mean Â±SD (ms)
MANDERA 643 Â±8.646 Trimmed Mean 3.96 Â±0.41
Krum (Single) 1352 Â±10.09 Median 9.81 Â±3.88
Bulyan 27209 Â±233.4 FLTrust 361 Â±4.07
13Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025255075100
406080100
406080100
5060708090100
Number of EpochAccuracyDefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
Figure 8: Model accuracy under diï¬€erent defences strategies for Non-IID data set: QMNIST
14Under review as submission to TMLR
A Proof of Theorem 2.1
Proof.LetFj(x)andGj(x)be the cumulative distribution functions of Fj(Â·)andGj(Â·),fj(x)andgj(x)be
the corresponding density functions, and rj(x) =n1âˆ’n1Gj(x) +n0âˆ’n0Fj(x) + 1be the expected ranking
of valuexamong all entries in the jthcolumn of the gradient value matrix.
Further deï¬ne
Ebj=/integraldisplayâˆ
âˆ’âˆrj(x)gj(x)dx, Vbj=/integraldisplayâˆ
âˆ’âˆ(rj(x)âˆ’Ebj)2gj(x)dx,
Emj=/integraldisplayâˆ
âˆ’âˆrj(x)fj(x)dx, Vmj=/integraldisplayâˆ
âˆ’âˆ(rj(x)âˆ’Emj)2fj(x)dx.
It can be shown for any 1â‰¤jâ‰¤pthat
Eij=E(Ri,j) =EbjÂ·I(iâˆˆIb) +EmjÂ·I(iâˆˆIm),
Vij=V(Ri,j) =VbjÂ·I(iâˆˆIb) +VmjÂ·I(iâˆˆIm).
Thus, we would have according to Kolmogorovâ€™s strong law of large numbers (KSLLN) that
lim
Nâˆ—â†’âˆlim
pâ†’âˆei= Â¯ÂµbÂ·I(iâˆˆIb) + Â¯ÂµmÂ·I(iâˆˆIm)a.s.,
lim
Nâˆ—â†’âˆlim
pâ†’âˆvi= Â¯s2
bÂ·I(iâˆˆIb) + Â¯s2
mÂ·I(iâˆˆIm)a.s.,
where the moments (Â¯Âµb,Â¯s2
b)and(Â¯Âµm,Â¯s2
m)are deterministic functions of (Ebj,Vbj)and(Emj,Vmj)of the
following form:
Â¯Âµb= lim
pâ†’âˆ1
pp/summationdisplay
j=1Ebj, Â¯Âµm= lim
pâ†’âˆ1
pp/summationdisplay
j=1Emj,
Â¯s2
b= lim
pâ†’âˆ1
pp/summationdisplay
j=1Vbj, Â¯s2
m= lim
pâ†’âˆ1
pp/summationdisplay
j=1Vmj.
It completes the proof.
B Proof of Theorem 2.2
Proof.According to Theorem 2.1, when both Nâˆ—andpare large enough, with probability 1 there exist
(eb,vb),(em,vm)andÎ´>0such that||(eb,vb)âˆ’(em,vm)||2>Î´, and
||(ei,vi)âˆ’(eb,vb)||2â‰¤Î´
2forâˆ€iâˆˆIband||(ei,vi)âˆ’(em,vm)||2â‰¤Î´
2forâˆ€iâˆˆIm.
Therefore, with a reasonable clustering algorithm such as K-mean with K= 2, we would expect Ë†Ib=Ibwith
probability 1.
Because we can always ï¬nd a âˆ†>0such that||Mi,:âˆ’Mj,:||2â‰¤âˆ†for any node pair (i,j)in a ï¬xed dataset
with a ï¬nite number of nodes, and Ë†mb,:=mb,:when Ë†Ib=Ib, we have
E||Ë†mb,:âˆ’mb,:||2â‰¤âˆ†Â·P(Ë†Ib/negationslash=Ib),
and thus
lim
Nâˆ—â†’âˆlim
pâ†’âˆE||Ë†mb,:âˆ’mb,:||2= 0.
It completes the proof.
15Under review as submission to TMLR
C Proof of Theorem 3.3
Proof.According to Theorem 2.1, we only need to compute Â¯Âµb,Â¯Âµm,Â¯s2
bandÂ¯s2
munder the Gaussian attacks.
BecauseMi,jâ†’dN/parenleftbig
Âµj,Î£j,j/parenrightbig
forâˆ€iâˆˆImandMi,jâ†’dN/parenleftbig
Âµj,Ïƒ2
j/Ni/parenrightbig
forâˆ€iâˆˆIbwhenNâˆ—â†’âˆ, it is
straightforward to see due to the symmetry of Gaussian distribution that
lim
Nâˆ—â†’âˆEbj= lim
Nâˆ—â†’âˆEmj= lim
Nâˆ—â†’âˆE(Ri,j) =n+ 1
2,1â‰¤iâ‰¤n,1â‰¤jâ‰¤p. (1)
Therefore, we have
Â¯Âµb= lim
Nâˆ—â†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1Ebj=n+ 1
2,
Â¯Âµm= lim
Nâˆ—â†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1Emj=n+ 1
2.
Moreover, assuming that the sample sizes of diï¬€erent benign nodes approach to each other with Nâˆ—going to
inï¬nity, i.e.,
lim
Nâˆ—â†’âˆ1
Nâˆ—max
i,kâˆˆIb|Niâˆ’Nk|= 0, (2)
for each parameter dimension j,{Mi,j}iâˆˆIbwould converge to the same Gaussian distribution N(Âµj,Ïƒ2
j/Nâˆ—)
with the increase of Nâˆ—. Thus, due to the exchangeability of {Mi,j}iâˆˆIband{Mi,j}iâˆˆIm, it is easy to see
that that
lim
Nâˆ—â†’âˆVbj=s2
b,j, lim
Nâˆ—â†’âˆVmj=s2
m,j, (3)
wheres2
b,jands2
m,jare both complex functions of n0,n1,Ïƒ2
j,Î£j,jandNâˆ—, ands2
b,j=s2
m,jif and only
ifÏƒ2
j/Nâˆ—= Î£j,j. According to Theorem 2.1, Â¯s2
b=limpâ†’âˆ1
p/summationtextp
j=1Vbj=limNâˆ—â†’âˆ1
p/summationtextp
j=1s2
b,jandÂ¯s2
m=
limpâ†’âˆ1
p/summationtextp
j=1Vmj= limpâ†’âˆ1
p/summationtextp
j=1s2
m,j. The proof is complete.
D Proof of Theorem 3.5
Proof.According to Theorem 2.1, we only need to compute Â¯Âµb,Â¯Âµm,Â¯s2
bandÂ¯s2
munder the sign ï¬‚ipping attacks.
Lemma D.1. Under the sign ï¬‚ipping attack, for each malicious node iâˆˆImand any parameter dimension
j, we haveMi,j=âˆ’r
n1/summationtext
kâˆˆIbMk,jis a deterministic function of {Mk,j}kâˆˆIb, whose limiting distribution
whenNâˆ—goes to inï¬nity is
Mi,jâ†’dN/parenleftbig
Âµj(r),Ïƒ2
j(r)/parenrightbig
,1â‰¤jâ‰¤p, (4)
whereÂµj(r) =âˆ’rÂµj,Ïƒ2
j(r) =r2Â·Ïƒ2
j
n1Â·Â¯Nb, and Â¯Nb=n1/summationtext
kâˆˆIb1
Nkis the harmonic mean of {Nk}kâˆˆIb.
Lemma 3.1 and Lemma D.1 tell us that for each parameter dimension j, the distribution of {Mi,j}n
i=1
is a mixture of Gaussian components {N/parenleftbig
Âµj,Ïƒ2
j/Ni/parenrightbig
}iâˆˆIbcentered at Âµjplus a point mass located at
Âµj(r) =âˆ’rÂµj. IfNiâ€™s are reasonably large, variances Ïƒ2
j/Niâ€™s would be very close to zero, and the probability
mass of the mixture distribution would concentrate to two local centers ÂµjandÂµj(r) =âˆ’rÂµj, one for the
benign nodes and the other one for the malicious nodes.
Under the sign ï¬‚ipping attack, because Mi,jâ†’dN/parenleftbig
Âµj(r),Ïƒ2
j(r)/parenrightbig
forâˆ€iâˆˆImandMi,jâ†’dN/parenleftbig
Âµj,Ïƒ2
j/Ni/parenrightbig
for
âˆ€iâˆˆIbwhenNâˆ—â†’âˆ, and
lim
Nâˆ—â†’âˆ(Ïƒ2
j/Ni) = lim
Nâˆ—â†’âˆÏƒ2
j(r) = 0.
It is straightforward to see that
lim
Nâˆ—â†’âˆP(Mi,j>Mk,j) =I(Âµj>0),âˆ€iâˆˆIb,âˆ€kâˆˆIm,
16Under review as submission to TMLR
which further indicates that
lim
Nâˆ—â†’âˆEbj= lim
Nâˆ—â†’âˆE(Ri,j) =n1+ 1
2, if Âµj>0,
lim
Nâˆ—â†’âˆEmj= lim
Nâˆ—â†’âˆE(Ri,j) =n+n1+ 1
2, if Âµj>0,
lim
Nâˆ—â†’âˆEbj= lim
Nâˆ—â†’âˆE(Ri,j) =n+n0+ 1
2if Âµj<0
lim
Nâˆ—â†’âˆEmj= lim
Nâˆ—â†’âˆE(Ri,j) =n0+ 1
2if Âµj<0,(5)
lim
Nâˆ—â†’âˆE(R2
i,j) =S2
[1,n1]Â·I(iâˆˆIb) +S2
[n1+1,n]Â·I(iâˆˆIm)if Âµj>0,
lim
Nâˆ—â†’âˆE(R2
i,j) =S2
[1,n0]Â·I(iâˆˆIm) +S2
[n0+1,n]Â·I(iâˆˆIb)if Âµj<0,(6)
whereS2
[a,b]=1
bâˆ’a+1/summationtextb
k=ak2.
Therefore, we have
/braceleftBigg
Â¯Âµm= limNâˆ—â†’âˆlimpâ†’âˆ1
p/summationtextp
j=1Ebj=ÏÂ·n+n1+1
2+ (1âˆ’Ï)Â·n0+1
2,
Â¯Âµb= limNâˆ—â†’âˆlimpâ†’âˆ1
p/summationtextp
j=1EmjÏÂ·n1+1
2+ (1âˆ’Ï)Â·n+n0+1
2,
whereÏ= limpâ†’âˆ/summationtextp
j=1I(Âµj>0)
p.
Deï¬ne Â¯Âµi= Â¯ÂµmÂ·I(iâˆˆIm) + Â¯ÂµbÂ·I(iâˆˆIb). Considering that
lim
Nâˆ—â†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1Vij
= lim
Nâˆ—â†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1E(Ri,jâˆ’Â¯Âµi)2
= lim
pâ†’âˆlim
Nâˆ—â†’âˆ1
pp/summationdisplay
j=1/parenleftBig
E(R2
i,j)âˆ’2Â¯ÂµiE(Ri,j) + (Â¯Âµi)2/parenrightBig
=/bracketleftbig
Â¯Ï„mâˆ’(Â¯Âµm)2/bracketrightbig
Â·I(iâˆˆIm) +/bracketleftbig
Â¯Ï„bâˆ’(Â¯Âµb)2/bracketrightbig
Â·I(iâˆˆIb),
where
Â¯Ï„b=ÏÂ·S2
[1,n1]+ (1âˆ’Ï)Â·S2
[n0+1,n],
Â¯Ï„m=ÏÂ·S2
[n1+1,n]+ (1âˆ’Ï)Â·S2
[1,n0].
According to Theorem 2.1,
Â¯s2
b= lim
pâ†’âˆlim
Nâˆ—â†’âˆ1
pp/summationdisplay
j=1Vbj= Â¯Ï„bâˆ’(Â¯Âµb)2,
Â¯s2
m= lim
pâ†’âˆlim
Nâˆ—â†’âˆ1
pp/summationdisplay
j=1Vmj= Â¯Ï„mâˆ’(Â¯Âµm)2.
It completes the proof.
E Proof of Theorem 3.8
Proof.According to Theorem 2.1, we only need to compute Â¯Âµb,Â¯Âµm,Â¯s2
bandÂ¯s2
munder the mean shift attacks.
Under the mean shift attack, all the malicious gradient will be inserted at a position which is dependent on z.
More speciï¬cally, for a relatively large n, the samples from benign nodes are normally distributed. Therefore,
17Under review as submission to TMLR
on average, with proportion Î¦(z)of the benign nodes having higher values of gradient than the malicious
nodes.
First of all, we derive the property in term of the ï¬rst moment. Denote Î±=âŒŠn1Î¦(z)âŒ‹. For a benign node, we
have
lim
Nâˆ—â†’âˆlim
nâ†’âˆEbj= lim
Nâˆ—â†’âˆlim
nâ†’âˆE(Ri,j) =1
n1/parenleftBiggÎ±/summationdisplay
k=1k+n/summationdisplay
s=n0+1+Î±s/parenrightBigg
=n+ 1
2+n0
n1(n1âˆ’Î±).
For a malicious node, we have
lim
Nâˆ—â†’âˆlim
nâ†’âˆEmj= lim
Nâˆ—â†’âˆlim
nâ†’âˆE(Ri,j) =Î±+ 1 +Î±+n0
2=Î±+1 +n0
2.
Therefore, according to Theorem 2.1,
Â¯Âµb= lim
Nâˆ—â†’âˆlim
nâ†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1Ebj=n+ 1
2+n0
n1(n1âˆ’Î±),
Â¯Âµm= lim
Nâˆ—â†’âˆlim
nâ†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1Emj=Î±+1 +n0
2.
Now, we derive the property in term of the second moment. For a benign node, we have
lim
Nâˆ—â†’âˆlim
nâ†’âˆE(R2
i,j) =1
n1/parenleftBiggÎ±/summationdisplay
k=1k2+n/summationdisplay
s=n0+1+Î±s2/parenrightBigg
=1
n1(Ï„(n) +Ï„(Î±)âˆ’Ï„(Î±+ 1 +n0)),
whereÏ„(Â·)is the function of â€˜sum of squaresâ€™, i.e., Ï„(n) =/summationtextn
k=1k2.
For a malicious node, we have
lim
Nâˆ—â†’âˆlim
nâ†’âˆE(R2
i,j) =/parenleftbigg
Î±+1 +n0
2/parenrightbigg2
,
Therefore, according to Theorem 2.1,
Â¯s2
b= lim
Nâˆ—â†’âˆlim
nâ†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1Vbj=1
n1(Ï„(n) +Ï„(Î±)âˆ’Ï„(Î±+ 1 +n0))âˆ’Â¯Âµ2
b,
Â¯s2
m= lim
Nâˆ—â†’âˆlim
nâ†’âˆlim
pâ†’âˆ1
pp/summationdisplay
j=1Vmj= 0.
It completes the proof.
F Neural Network conï¬gurations
We train these models with a batch size of 10, an SGD optimizer operates with a learning rate of 0.01, and
0.5 momentum for 25 epochs. The accuracy of the model is evaluated on a holdout set of 1000 samples.
F.1 FASHION-MNIST, MNIST and QMNIST
â€¢Layer 1: 1âˆ—16âˆ—5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
â€¢Layer 2: 16âˆ—32âˆ—5, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
â€¢Output: 10Classes, Linear.
18Under review as submission to TMLR
F.2 CIFAR-10
â€¢Layer 1: 1âˆ—32âˆ—3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
â€¢Layer 2: 32âˆ—32âˆ—3, 2D Convolution, Batch Normalization, ReLU Activation, Max pooling.
â€¢Output: 10Classes, Linear.
G Metrics
The metrics observed in Section 4 to evaluate the performance of the defense mechanisms are deï¬ned as
follows:
Precision =TP
TP+FP,
Accuracy =TP+TN
TP+FP+FN+TN,
Recall =TP
TP+FN,
F1= 2Ã—PrecisionÃ—Recall
Precision+Recall.
H Accuracy of the global model under diï¬€erent attacks
In Table 4 and 5 the numeric accuracies of each experimental conï¬guration at the 25th epoch are presented.
I MANDERA performance with diï¬€erent clustering algorithms
In this section, Figure 10 demonstrate that the discriminating performance of MANDERA when hierarchical
clustering and Gaussian mixture models are used in-place of K-means for FASHION-MNIST data set remain
robust.
J Model Losses on CIFAR-10, FASHION-MNIST and MNIST data
Figure 11 - 13 present the model loss to accompany the model prediction performance for CIFAR-10,
FASHION-MNIST and MNIST-Digits respectively, which are previously seen in Section 4.
K Model Losses on QMNIST data
Figure 14 presents the model loss to accompany the model prediction performance of QMNIST previously
seen in Section 4.
19Under review as submission to TMLR
Table 4: FASHION-MNIST model accuracy at 25th epoch. The boldhighlights the best defense strategy
under attack. Note â€œNO-attackâ€ is the baseline, where no attack is conducted. And n0denotes the number
of malicious nodes among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 83.66 84.13 84.09 83.30 84.22 82.32
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 87.80 87.80 87.79 87.73 87.67 87.69
Median 87.73 87.76 87.73 87.70 87.72 87.70
Trim-mean 87.85 87.78 87.75 87.74 87.72 87.73
MANDERA 87.81 87.83 87.82 87.77 87.80 87.76
FLTrust 66.13 36.35 50.20 17.85 16.00 9.66
ZGKrum 83.56 83.57 84.11 84.33 84.10 84.30
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 86.88 87.38 87.49 87.45 87.48 87.38
Median 87.36 86.91 86.20 85.33 84.07 82.45
Trim-mean 87.13 86.57 85.67 84.61 83.06 81.48
MANDERA 87.79 87.81 87.84 87.72 87.76 87.78
FLTrust 81.59 83.58 79.41 80.62 79.00 74.01
SFKrum 84.49 84.71 84.43 83.58 83.61 83.72
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 87.60 87.64 87.62 87.50 87.47 87.35
Median 87.40 86.91 86.21 85.36 84.11 82.31
Trim-mean 87.48 86.97 86.20 84.92 83.08 81.20
MANDERA 87.85 87.79 87.82 87.79 87.77 87.74
FLTrust 86.96 85.97 84.55 76.92 75.72 76.90
MSKrum 87.82 87.77 87.66 87.50 87.36 86.89
NO-attack 87.83 87.83 87.83 87.83 87.83 87.83
Bulyan 87.81 87.78 87.75 87.75 87.60 87.21
Median 87.75 87.78 87.69 87.52 87.26 86.99
Trim-mean 87.81 87.79 87.76 87.73 87.61 87.33
MANDERA 87.81 87.78 87.78 87.79 87.71 87.79
FLTrust 87.77 87.75 87.78 87.77 87.73 87.73
20Under review as submission to TMLR
Table 5: CIFAR-10 model accuracy at 25 th epoch. The boldhighlights the best defense strategy under
attack. Note â€œNO-attackâ€ is the baseline, where no attack is conducted. And n0denotes the number of
malicious nodes among 100 nodes.
Attack Defence n0= 5n0= 10n0= 15n0= 20n0= 25n0= 30
GAKrum 47.66 47.16 47.18 47.26 47.25 46.77
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 55.69 55.85 55.67 55.63 55.46 55.22
Median 55.47 55.53 55.47 55.40 55.29 55.22
Trim-mean 55.77 55.72 55.56 55.50 55.43 55.31
MANDERA 55.74 55.69 55.63 55.65 55.76 55.69
FLTrust 19.66 27.54 11.99 9.21 9.73 9.96
ZGKrum 46.85 46.84 47.96 47.13 47.12 47.53
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 52.30 53.87 54.28 54.36 54.35 54.10
Median 54.06 52.18 50.18 48.01 44.89 38.08
Trim-mean 53.34 51.22 49.14 46.45 42.02 34.36
MANDERA 55.77 55.69 55.78 55.65 55.72 55.56
FLTrust 48.05 39.21 39.44 44.25 40.27 39.49
SFKrum 48.11 47.79 46.93 47.89 47.59 47.13
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 55.30 54.99 54.86 54.68 54.43 54.05
Median 53.96 52.29 50.49 47.89 44.93 37.22
Trim-mean 54.37 52.40 49.97 47.30 42.32 33.76
MANDERA 55.78 55.69 55.62 55.55 55.67 55.56
FLTrust 54.18 50.21 46.39 44.45 36.19 34.39
MSKrum 55.60 55.23 54.51 53.79 52.31 50.54
NO-attack 55.78 55.78 55.78 55.78 55.78 55.78
Bulyan 55.68 55.62 55.37 54.98 54.26 52.10
Median 55.47 55.20 54.55 53.72 52.17 50.55
Trim-mean 55.64 55.59 55.38 55.09 54.29 52.32
MANDERA 55.65 55.77 55.72 55.62 55.66 55.63
FLTrust 55.81 55.64 55.62 55.42 55.09 54.65
21Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520251020304050
20304050
20304050
304050
Number of EpochAccuracyDefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
(a) CIFAR-10 accuracy
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025255075
405060708090
5060708090
76808488
Number of EpochAccuracyDefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
(b) FASHION-MNIST accuracy
Figure 9: Model Accuracy at each epoch of training, each line of the curve represents a diï¬€erent defense
against the Byzantine attacks.
22Under review as submission to TMLR
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(a) Gaussian mixture model.
GA ZG SF MSAccuracy Recall Precision F1
510152025305101520253051015202530510152025300.250.500.751.00
0.250.500.751.00
0.250.500.751.00
0.250.500.751.00
Number of malicious nodesScoreMetric Accuracy Recall Precision F1
(b) Hierarchical clustering.
Figure 10: Classiï¬cation performance of our proposed approach MANDERA (Algorithm 1) with other
clustering algorithms under four types of attack for FASHION-MNIST data. GA: Gaussian attack; ZG:
Zero-gradient attack; SF: Sign-ï¬‚ipping; and MS: mean shift attack. The boxplot bounds the 25th (Q1) and
75th (Q3) percentile, with the central line representing the 50th quantile (median). The end points of the
whisker represent the Q1-1.5(Q3-Q1) and Q3+1.5(Q3-Q1) respectively.
23Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
051015202505101520250510152025051015202505101520250510152025468
2.53.03.54.04.5
2.62.83.03.2
2.62.72.82.9
Number of Epochlog(Loss)DefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
Figure 11: Model Loss for CIFAR-10 data at each epoch of training, each line of the curve represents a
diï¬€erent defense against the Byzantine attacks.
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520252.55.07.5
123456
1.52.02.5
1.21.41.61.82.0
Number of Epochlog(Loss)DefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
Figure 12: Model Loss for FASHION-MNIST data at each epoch of training, each line of the curve represents
a diï¬€erent defense against Byzantine attacks.
24Under review as submission to TMLR
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520250.02.55.07.5
01234
0123
01
Number of Epochlog(Loss)DefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
Figure 13: Model Loss for MNIST-Digits data at each epoch of training, each line of the curve represents a
diï¬€erent defense against the Byzantine attacks.
5 10 15 20 25 30GA ZG SF MS
0510152025051015202505101520250510152025051015202505101520250.02.55.07.5
0123
024
012
Number of Epochlog(Loss)DefenceKrum
NOâˆ’attackBulyan
MedianTrimâˆ’mean
MANDERAFLTrust
Figure 14: QMNIST model loss.
Figure 15: Model Loss at each epoch of training, each line of the curve represents a diï¬€erent defense against
the Byzantine attacks.
25Under review as submission to TMLR
References
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor
federated learning. In International Conference on Artiï¬cial Intelligence and Statistics , pp. 2938â€“2948.
PMLR, 2020.
Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for distributed
learning. Advances in Neural Information Processing Systems , 32, 2019.
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning with
adversaries: Byzantine tolerant gradient descent. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf .
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust federated learning
via trust bootstrapping. arXiv preprint arXiv:2012.13995 , 2020.
Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong. Provably secure federated learning against malicious
clients. In Proceedings of the AAAI Conference on Artiï¬cial Intelligence , 2021.
Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:
Byzantine gradient descent. Proc. ACM Meas. Anal. Comput. Syst. , 1(2), December 2017. doi: 10.1145/
3154503. URL https://doi.org/10.1145/3154503 .
Zheyi Chen, Pu Tian, Weixian Liao, and Wei Yu. Zero knowledge clustering based adversarial mitigation in
heterogeneous federated learning. IEEE Transactions on Network Science and Engineering , 8(2):1070â€“1083,
2021. doi: 10.1109/TNSE.2020.3002796.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
Processing Magazine , 29(6):141â€“142, 2012.
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to byzantine-robust
federated learning. In 29th{USENIX}Security Symposium ( {USENIX}Security 20) , pp. 1605â€“1622, 2020.
Chris Fraley and Adrian E Raftery. Model-based clustering, discriminant analysis, and density estimation.
Journal of the American statistical Association , 97(458):611â€“631, 2002.
Rachid Guerraoui, SÃ©bastien Rouault, et al. The hidden vulnerability of distributed learning in byzantium.
InInternational Conference on Machine Learning , pp. 3521â€“3530. PMLR, 2018.
Peter Kairouz, H Brendan McMahan, Brendan Avent, AurÃ©lien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open
problems in federated learning. Foundations and Trends Â®in Machine Learning , 14(1â€“2):1â€“210, 2021.
Alex Krizhevsky, Geoï¬€rey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. In Concurrency: the
works of leslie lamport , pp. 203â€“226. ACM, 2019.
Suyi Li, Yong Cheng, Wei Wang, Yang Liu, and Tianjian Chen. Learning to detect malicious clients for
robust federated learning. arXiv preprint arXiv:2002.00211 , 2020.
Hsiao-Ying Lin and Wen-Guey Tzeng. An eï¬ƒcient solution to the millionairesâ€™ problem based on homomorphic
encryption. In International Conference on Applied Cryptography and Network Security , pp. 456â€“466.
Springer, 2005.
Jinhyun So, BaÅŸak GÃ¼ler, and A. Salman Avestimehr. Byzantine-resilient secure federated learning. IEEE
Journal on Selected Areas in Communications , 39(7):2168â€“2181, 2021. doi: 10.1109/JSAC.2020.3041404.
Jacob Steinhardt. Robust learning: Information theory and algorithms . PhD thesis, Stanford University, 2018.
26Under review as submission to TMLR
Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against federated
learning systems. In European Symposium on Research in Computer Security , pp. 480â€“501. Springer, 2020.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Byrd-saga - github.
https://github.com/MrFive5555/Byrd-SAGA, 2020a.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variance-reduced stochastic
gradient descent with robustness to byzantine attacks. IEEE Transactions on Signal Processing , 68:
4583â€“4596, 2020b.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno: Distributed stochastic gradient descent with suspicion-
based fault-tolerance. In International Conference on Machine Learning , pp. 6893â€“6901. PMLR, 2019.
Cong Xie, Sanmi Koyejo, and Indranil Gupta. Zeno++: Robust fully asynchronous sgd. In International
Conference on Machine Learning , pp. 10495â€“10503. PMLR, 2020.
Chhavi Yadav and LÃ©on Bottou. Cold case: The lost mnist digits. In Advances in Neural Information
Processing Systems 32 . Curran Associates, Inc., 2019.
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning:
Towards optimal statistical rates. In International Conference on Machine Learning , pp. 5650â€“5659. PMLR,
2018.
Lan Zhang, Xiang-Yang Li, Yunhao Liu, and Taeho Jung. Veriï¬able private multi-party computation: ranging
and ranking. In 2013 Proceedings IEEE INFOCOM , pp. 605â€“609. IEEE, 2013.
27