Published in Transactions on Machine Learning Research (01/2024)
Out-of-Distribution Optimality of Invariant Risk Minimiza-
tion
Shoji Toyota shoji@ism.ac.jp
The Institute of Statistical Mathematics
Kenji Fukumizu fukumizu@ism.ac.jp
The Institute of Statistical Mathematics
Reviewed on OpenReview: https: // openreview. net/ forum? id= pWsfWDnJDa
Abstract
Deep Neural Networks often inherit spurious correlations embedded in training data and
hence may fail to generalize to unseen domains, which have different distributions from the
domain to provide training data. Arjovsky et al. (2019) introduced the concept out-of-
distribution (o.o.d.) risk , which is the maximum risk among all domains, and formulated
the issue caused by spurious correlations as a minimization problem of the o.o.d. risk. In-
variant Risk Minimization (IRM) is considered to be a promising approach to minimize the
o.o.d. risk: IRM estimates a minimum of the o.o.d. risk by solving a bi-level optimization
problem. While IRM has attracted considerable attention with empirical success, it comes
with few theoretical guarantees. Especially, a solid theoretical guarantee that the bi-level
optimization problem gives the minimum of the o.o.d. risk has not yet been established.
Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a
solution to the bi-level optimization problem minimizes the o.o.d. risk under certain condi-
tions. The result also provides sufficient conditions on distributions providing training data
and on a dimension of a feature space for the bi-leveled optimization problem to minimize
the o.o.d. risk.
1 Introduction
Training data used in supervised learning may contain features that are spuriously correlated to the response
variables of data. Deep Neural Networks (DNNs) often learn such spurious correlations embedded in the
data and hence may fail to predict desirable response variables of test data generated by a distribution that
is different from the one to provide training data. To list a few examples, in a classification of animal images,
models obtained by conventional procedures tend to misclassify cows on sandy beaches because most training
pictures are captured in green pastures and DNNs inherit context information in training (Beery et al., 2018;
Shane, 2018). Another example is learning from medical data. Systems trained with data collected in
one hospital do not generalize well to other hospitals; DNNs unintentionally extract environmental factors
specific to a particular hospital in training (AlBadawy et al., 2018; Perone et al., 2019; Heaven, 2020).
Arjovskyetal.(2019)introducedtheconcept out-of-distribution (o.o.d.) risk toformulatetheissuecausedby
spurious correlations. Let XandYbe measurable spaces of explanatory and response variables respectively.
LetEbe a set with each element eâˆˆEcalled the domain(or environment) e. Assume that for a given
domaineâˆˆE, there corresponds a corresponding random variable (Xe,Ye)that takes values in XÃ—Ywith
its probability law PXe,Ye. Assume we are given training datasets De:={(xe
i,ye
i)}ne
i=1âˆ¼PXe,Yei.i.d. from
multiple domains EtrâŠ‚E. For a given predictor f:Xâ†’Y,
Re(f) :=/integraldisplay
l(f(x),y)dPXe,Ye
1Published in Transactions on Machine Learning Research (01/2024)
denotes the risk of fon domain e. The o.o.d. risk of the predictor fis as follows:
Ro.o.d.(f) := max
eâˆˆERe(f), (1)
which is the worst-case risk over Eincluding unseen domain Eâˆ’Etr. Arjovsky et al. (2019) formulated the
problem caused by spurious correlations as a minimization problem of the o.o.d. risk (1):
min
fâˆˆFRo.o.d.(f), (2)
whereFis the set of all measurable functions f:Xâ†’Y.
It is difficult to directly solve the o.o.d. risk minimization (2) since we can not evaluate the maximum of
risks among all domains E, including unseen domains Eâˆ’Etr, only by data from training domains EtrâŠ‚E.
Invariant Risk Minimization (IRM) is a rapidly developing approach to the challenging o.o.d. risk minimiza-
tion (Arjovsky et al., 2019). Its proposed predictor f:=wâ—¦Î¦is composed of two maps: a feature map
Î¦ :Xâ†’H, which is called an invariance , and a predictor w:Hâ†’Ywhich estimates the response variable
of feature Î¦(x). Here, for a given feature space H, we call a measurable function Î¦ :Xâ†’Han invariance
when it holds that PYe1|Î¦(Xe1)=PYe2|Î¦(Xe2)for anye1,e2âˆˆE1.Arjovsky et al. (2019) estimated the two
maps by solving the bi-leveled optimization problem
minÎ¦âˆˆItr,wâˆˆW/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦), (3)
whereWis a model of predictors w:Hâ†’YandItris the set of invariances captured by training domains
Etr:
Itr:=/braceleftbig
Î¦ :Xâ†’H|PYe1|Î¦(Xe1)=PYe2|Î¦(Xe2)for anye1,e2âˆˆEtr/bracerightbig
. (4)
Influenced by the seminal study, several alternative bi-leveled optimization problems have been proposed
(Ahuja et al., 2020; Chang et al., 2020; Ahuja et al., 2021a;b; Lin et al., 2022a; Zhou et al., 2022; Liu et al.,
2021a;b; Lu et al., 2022; Koyama & Yamaguchi, 2021; Parascandolo et al., 2022; Krueger et al., 2021; Toyota
&Fukumizu,2022;Linetal.,2022b;Huh&Baidya,2022;Rameetal.,2022;Pogodinetal.,2023;Chenetal.,
2023; Tan et al., 2023). For example, Ahuja et al. (2020) proposed a novel bi-leveled optimization problem
leveraging the principles of game theory. The recently proposed Maximal Invariant Predictor (Koyama &
Yamaguchi, 2021) employed a new bi-leveled problem grounded in the concept of information theory.
While IRM is widely recognized as a promising approach for the o.o.d. risk minimization (2), it comes with
few theoretical guarantees; especially, a mathematical guarantee that the bi-level optimization problem (3)
gives the minimum of the o.o.d. risk (1) has not yet been established.2The original IRM paper did not
mention any theoretical properties for the minimum of (3). Rosenfeld et al. (2021) proved that, assuming
that data follow a simple linear Gaussian structural equation model (SEM), a predictor obtained by (3)
makes a prediction relying only on a feature of XeâˆˆXwhose distribution does not depend on domains
(Rosenfeld et al., 2021, Section 5). However, their analysis did not focus on relations between the bi-level
optimization problem (3) and the o.o.d. risk (2). More recently, Kamath et al. (2021) provided an example
of distributions on which a minimum of (3) does not minimize the o.o.d risk (Kamath et al., 2021, Section
4). However, their analysis assumed that data follow particular SEMs constructed to derive the case where
(3) does not provide a minimum of the o.o.d. risk; for verifying the o.o.d. performance of the bi-leveled
optimization problem (3), it should be analyzed under more general assumptions on distributions.
Aiming at providing a theoretical justification for IRM, this paper rigorously proves that a solution to the
bi-leveled optimization problem (3) also minimizes the o.o.d. risk (1); formally speaking, we prove that the
1The definition is based on conditional independence (Peters et al., 2016; Koyama & Yamaguchi, 2021; Rojas-Carulla et al.,
2018), while Arjovsky et al. (2019); Ahuja et al. (2020) used a different type of invariances based on arg minwRe(wâ—¦Î¦)instead
ofPYe|Î¦(Xe). Throughout the paper, we argue by adopting the definition based on conditional independence.
2Since it is difficult to solve the bi-leveled optimization problem (3), several papers have proposed optimization methods for
(3) such as IRMv1 (Arjovsky et al., 2019) or Invariant Rationalization (Chang et al., 2020). While their optimization ability
for solving (3) should also be discussed theoretically, this paper does not address it and only focuses on the problem of whether,
assuming that (3) can be solved completely, the resulting predictor minimizes the o.o.d. risk.
2Published in Transactions on Machine Learning Research (01/2024)
inclusion
arg minÎ¦âˆˆItr,wâˆˆW/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦)âŠ‚arg min
fâˆˆFRo.o.d.(f) (5)
is attained under certain conditions. The result also provides sufficient conditions on the training domains
Etrand the feature space Hto minimize the o.o.d risk. In our analysis, we set distributions on domains
Eby the ones proposed in Rojas-Carulla et al. (2018). The distributions do not rely on any specific SEM
structures, unlike existing theoretical analysis of IRM (Rosenfeld et al., 2021; Kamath et al., 2021), and they
are used for the analysis of methods related to invariances (Rojas-Carulla et al., 2018; Toyota & Fukumizu,
2022).
The rest of the paper is organized as follows. Section 2 illustrates two main theorems. Section 2.1 provides
the first main theorem, which states that the inclusion (5) is achieved in the regression case. In Section 2.2,
we extend the first theorem to the classification case. The novelty and significance of these two theorems
are discussed in Section 2.3. We provide a review of the prior works concerning the relationship between the
bi-leveled optimization problem (3) and the o.o.d. risk (1) in Section 3. The two main theorems stated in
Section 2 are proved in Section 4. Section 5 is devoted to brief concluding remarks.
2 Main Results
We explain the settings and assumptions persisting throughout our analysis.
We set domains{(Xe,Ye)}eâˆˆEby the ones proposed in Rojas-Carulla et al. (2018). Let X:=X1Ã—X2where
X1:=Rd1andX2:=Rd2withd1,d2âˆˆN>0, and (XI
1,YI)be a fixed random variable on X1Ã—Y. Rojas-
Carulla et al. (2018) defined the domain set Eby all the probability distributions with the fixed conditional
distribution PYI|XI
1; namely, denoting Î¦X1:Xâ†’X 1a projection onto X1,{(Xe,Ye)}eâˆˆEis defined by
{(Xe,Ye)}eâˆˆE:=/braceleftï£¬ig
(X,Y ) :a random variable on XÃ—Y/vextendsingle/vextendsingle/vextendsinglePY|Î¦X1(X)=PYI|XI
1/bracerightï£¬ig
. (6)
Notethat, underthesetting(6), theprojection Î¦X1:Xâ†’X 1isaninvarianceamong E, becausePY|Î¦X1(X)=
PYI|XI
1forany (X,Y )âˆˆ{(Xe,Ye)}eâˆˆE. Forsimplicityoftheoreticalanalysis, weassumethattheconditional
distribution PYI|XI
1has a probability density function pI(y|x1).
We explain assumptions about the feature space and models. The feature space Hfor an invariance Î¦âˆˆItr
is assumed to be the multi-dimensional Euclidean space RdH. Moreover, we assume that Î¦âˆˆItrandwâˆˆW
in the minimization problem (3) run only continuous functions; namely, we investigate the property of a
solution for
minÎ¦âˆˆIC0
tr,wâˆˆWC0/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦), (7)
whereWC0is the set of all continuous functionsw:Hâ†’Y, andIC0
tris the set of continuous invariances
captured by a training domain Etr:
IC0
tr:=/braceleftbig
Î¦ :Xâ†’H|PYe1|Î¦(Xe1)=PYe2|Î¦(Xe2)for anye1,e2âˆˆEtr,Î¦ :continuous/bracerightbig
.
2.1 Case I: Least Square Loss
First, we consider the case where Y=RdY(dYâˆˆN>0) andlis the least square loss; that is, for a given
predictorf:Xâ†’Y, its riskRe(f)on(Xe,Ye)âˆˆ{(Xe,Ye)}eâˆˆEis given by
Re(f) :=/integraldisplay
âˆ¥yâˆ’f(x)âˆ¥2dPXe,Ye.
The following theorem ensures that the optimization problem (7) provides a solution for the o.o.d. risk
minimization problem (2) under four conditions:
Theorem 1 (o.o.d. optimality of the bi-leveled optimization problem (7) under least square loss setting) .
Domains{(Xe,Ye)}eâˆˆEare assumed to be (6). We also assume that the following four conditions hold:
3Published in Transactions on Machine Learning Research (01/2024)
(i)IC0
tr=IC0, whereIC0is the set of continuous invariances captured by all domains E, not training
domainsEtr:
IC0:=/braceleftbig
Î¦ :Xâ†’H|PYe1|Î¦(Xe1)=PYe2|Î¦(Xe2)for anye1,e2âˆˆE,Î¦ :continuous/bracerightbig
.
(ii)/uniontext
eâˆˆE trsupp(PÎ¦X1(Xe)) =X1. Here, for probability measure ÂµonX1, supp (Âµ)is defined by
supp(Âµ) :={x1âˆˆX1|Nx1:open neighborhood around x1â‡’Âµ(Nx1)>0}.
(iii) The dimensions d1anddHon the subspaceX1âŠ‚Xof the input space Xand the feature space
H=RdHsatisfyd1â‰¤dH.
(iv)PYI|XI
1has a continuous probability density function pI(y|x1). Here, we call pI(y|x1)continuous
when correspondence X1Ã—Yâˆˆ (x1,y)âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€âˆ’â†’pI(y|x1)is continuous.
Then, we have
arg minÎ¦âˆˆIC0
tr,wâˆˆWC0/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦)âŠ‚arg min
fâˆˆFRo.o.d.(f). (8)
Here,Fis the set of all measurable functions f:Xâ†’Y.
We explain the feasibilities and interpretations of the above four conditions.
Condition (i): Condition (i) implies that invariances captured by training domains Etrcorrespond to the
ones by all domains E. Arjovsky et al. (2019) also discussed the relationship between the equation Itr=I
ando.o.d.generalization, brieflyillustratingthattheequation Itr=Ifacilitatestheestimationofapredictor
with high o.o.d. performance solely based on data from training domains Etr(Arjovsky et al., 2019, Section
4.1). If it holds that Itr=I, we can capture an invariance Î¦âˆˆIamong all domains Eonly using the training
domainsEtr. Arjovsky et al. (2019) pointed that, once an invariance Î¦âˆˆIamong all domains is obtained,
a predictor wâˆ—that minimizes risks only on training domains Etr, namelywâˆ—âˆˆarg minW/summationtext
eâˆˆEtrRe(wâ—¦Î¦),
satisfiesRe(wâˆ—â—¦Î¦) = min wRe(wâ—¦Î¦)for all domains eâˆˆE, including unseen domains Eâˆ’Etr, under certain
settings. Developing the discussion by Arjovsky et al. (2019), Theorem 1 clarifies a more rigorous relation
among the equation Itr=I, the o.o.d. risk (1), and the bi-leveled optimization problem (7): the equation
I=Itris one of the sufficient conditions for the bi-leveled optimization problem (7) to minimize the o.o.d.
risk (1).
The conditionItr=Iis not generally satisfied and Peters et al. (2016); Arjovsky et al. (2019) presented
sufficient conditions on the training domains Etrfor the equationItr=Iwhen data follow simple SEMs.
Peters et al. (2016) proved the equation Itr=Iholds when distributions on domains follow a linear
Gaussian SEM and training data are obtained by certain types of interventions (Peters et al., 2016, Section
4.3). Arjovsky et al. (2019) generalized the result by Peters et al. (2016). Assuming that data follow a linear
SEM, which is not restricted to a Gaussian distribution and a certain type of interventions, Arjovsky et al.
(2019) deduced a sufficient condition for the equality Itr=Ion training domains Etr, which is called lying
in the general position (Arjovsky et al., 2019, Assumption 8). On the other hand, sufficient conditions for the
equalityItr=Iunder the setting (6) have not yet been revealed. Providing them would be an important
area for future research.
Conditions (ii) and (iii): As shown in Lemma 3, the conditional expectation/integraltext
yÂ·pI(y|x1)dy=E[YI=
y|XI
1=x1]achieves the minimization of the o.o.d. risk, signifying that the information embedded in X1is
important for predicting response variables on unseen domains. Condition (ii) implies that the support of
training domains EtrcoversX1that contains such important information for o.o.d. prediction. Condition
(iii) implies that His such a large feature space that a feature Î¦ :Xâ†’Hcan preserve information on the
X1-component of xâˆˆXby selecting Î¦appropriately. Condition (iii) also provides a practical perspective
on how to construct the feature space H=RdH: the dimension dHon the feature space Hshould be fixed
high. The dimension dHof the feature space is fixed by hand, and hence, Condition (iii) is expected to hold
unless we fix the dimension of the feature space too small.
4Published in Transactions on Machine Learning Research (01/2024)
Condition (iv): Condition (iv) presents continuity of the p.d.f. of PYI|XI
1. By Condition (iv), we also
have continuity of the conditional expectation/integraltext
yÂ·pI(y|x1)dy=E[YI=y|XI
1=x1]. In our analysis, we
assume that the model WC0consists of all continuous functions, and hence, Condition (iv) ensures that the
model includes the conditional expectation E[YI|XI
1], which minimizes the o.o.d. risk (Lemma 3).
2.2 Case II: Cross Entropy Loss
Theorem 1 can be easily extended to the classification case where wâˆˆWhas a probabilistic output and
evaluate risks by the cross entropy loss. Let Ybe a finite set{1,...,m}(mâˆˆN>0), and we model w:Hâ†’Y
bypÎ¸:Hâ†’PY, wherePYdenotes the set of probabilities on Y; namely
PY:=/braceleftï£¬igg
pâˆˆRm
+/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglem/summationdisplay
i=1pi= 1/bracerightï£¬igg
.
Here, R+:={xâˆˆR|xâ‰¥0}andpidenotes the i-th component of p. We callpÎ¸:Hâ†’PYcontinuous, that
ispÎ¸âˆˆWC0, when correspondence Hâˆ‹hâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€âˆ’â†’pÎ¸(h)âˆˆR|Y|is continuous, seeing pÎ¸(h)âˆˆPYas a vector on
R|Y|. For a given pÎ¸:Hâ†’PYandiâˆˆY,/parenleftbig
pÎ¸(h)/parenrightbig
iis often abbreviated by pÎ¸(i|h). The risk evaluated by
the cross-entropy loss is then written as
Re(pÎ¸â—¦Î¦) =/integraldisplay
âˆ’logpÎ¸(Ye|Î¦(Xe))dPXe,Ye.
We expand Theorem 1 to the above classification case:
Theorem 2 (o.o.d. optimality of the bi-leveled optimization problem (7) under cross-entropy loss setting) .
Domains{(Xe,Ye)}eâˆˆEare assumed to be (6). Assume that, in addition to (i) âˆ¼(iii) in Theorem 1, the
following condition (v) holds:
(v) For any xâˆ—
1âˆˆX1,#/braceleftbig
yâˆˆY/vextendsingle/vextendsinglepI(y|xâˆ—
1)>0/bracerightbig
>1.
Then, we have the inclusion
arg minÎ¦âˆˆIC0
tr,pÎ¸âˆˆWC0/summationdisplay
eâˆˆEtrRe(pÎ¸â—¦Î¦)âŠ‚arg min
pÎ¸âˆˆFRo.o.d.(f), (9)
whereFis the set of all measurable functions f:Xâ†’PY.3
Condition (v) indicates that domains {(Xe,Ye)}eâˆˆEhave high uncertainty in labels yâˆˆYgivenx1âˆˆX 1.
Theconditionisexpectedtobefeasiblewhenclasses Yaresubdividedanddifficulttobeuniquelydetermined
fromx1âˆˆX1.
2.3 Novelty and Significance of Theorems 1 and 2
Theorems 1 and 2 and their proofs have the following four novel and significant points:
Setting of Domains The first point is the setting of domains. The setting by Rojas-Carulla et al.
(2018), which is used throughout our analysis, does not impose any specific SEM structures, linearity,
and Gaussianity on domains while existing works on theoretical analysis of IRM assumed that data follow
simple SEMs. Theorems 1 and 2 indicate that, under such a general setting, IRM presents the minimum
of the o.o.d. risk. This implies that our results provide a solid foundation to use IRM for a broad range of
o.o.d. generalization problem.
3The same as the definition of continuous , we call fâˆˆFmeasurable when correspondence X âˆ‹ xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€âˆ’â†’f(x)âˆˆR|Y|is
measurable, seeing f(x)âˆˆPYas a vector on R|Y|.
5Published in Transactions on Machine Learning Research (01/2024)
Assumption on the Underlying Distribution PYI|XI
1As well as the assumption on domains, prior
theoretical results of IRM assume that the underlying true distribution PYI|XI
1is represented by a simple
SEM (Arjovsky et al., 2019; Rosenfeld et al., 2021; Kamath et al., 2021). On the other hand, Condition (iv)
only imposes PYI|XI
1on the continuity; hence, Condition (iv) is a significantly mild condition in comparison
with the assumptions on PYI|XI
1by prior works.
Characterization of Invariance Second, a theoretical characterization of invariances Î¦âˆ—âˆˆIC0is given
in Lemmas 4 and 6: it is proved that Î¦âˆ—âˆˆIC0can be represented as Î¦âˆ—= Î¨âˆ—â—¦Î¦X1for some continuous
map Î¨âˆ—. Any theoretical characterizations have not yet been presented, and hence, the results in Lemmas 4
and 6 are novel. To present the non-trivial characterization, we develop a novel theoretical technique based
on the proof by contradiction. Lemmas 4 and 6 play an important role in our desirable assertion (5), and
hence, the derivation of these lemmas is a significant technical contribution of our analysis.
Range of Invariance The fourth point is a range of invariances Î¦: we assume that Î¦run all continuous
functions, while most of the existing works on theoretical analysis of IRM assume that Î¦run more simplified
functions, such as linear functions (Rosenfeld et al., 2021) or variable selections (Toyota & Fukumizu, 2022).
It is common to construct a learning model of invariances with deep neural networks in the context of IRM,
and hence, the variable selection and linear function settings by Toyota & Fukumizu (2022); Rosenfeld et al.
(2021) are significantly simplified to analyze IRM. On the other hand, our large class of continuous functions
is relatively realistic compared to existing ones, since it is widely recognized that neural networks of sufficient
size can represent a wide range of functions (Cybenko, 1989; Hornik et al., 1989; Barron, 1993; Mhaskar,
1996; Sonoda & Murata, 2017).
3 Previous Works
As explained in Section 1, Rosenfeld et al. (2021); Kamath et al. (2021) derived the theoretical results
concerning the minimum of the bi-leveled optimization problem (3) and its connection to the o.o.d. risk (1).
Rosenfeld et al. (2021) proved that a predictor obtained by minimizing (3) predicts YeâˆˆYrelying only on
a feature of XeâˆˆXwhose distribution does not depend on domains (Rosenfeld et al., 2021, Section 5).
However, they did not provide any connections between the minimum of (3) and the o.o.d risk. Moreover,
they assume that data follow a linear Gaussian SEM, and that invariances Î¦in the bi-leveled optimization
problem run linear functions for simplicity. Unlike their analysis, this paper derives the direct relations
between (3) and the o.o.d. risk (1). Additionally, we assume that data follow the distributions proposed
by Rojas-Carulla et al. (2018) that do not rely on any specific SEM structures and that invariances run all
continuous functions including neural networks. Kamath et al. (2021) provided an example of distributions
on which a minimum of (3) does not minimize the o.o.d risk. However, the distributions are particular
SEMs constructed to derive the case where (5) is violated, and analysis in more general settings is required
(Kamath et al., 2021, Section 4). In construct, the distributions by Rojas-Carulla et al. (2018) used in this
paper do not rely on any specific SEM structures, and they are used to analyze estimation methods related
to invariances (Rojas-Carulla et al., 2018; Toyota & Fukumizu, 2022).
Arjovsky et al. (2019); Koyama & Yamaguchi (2021); Rojas-Carulla et al. (2018) discussed theoretical re-
lations between invariances and the o.o.d. risk (1). As explained in the last section, Arjovsky et al. (2019)
intuitively explained that the condition Itr=Ifacilitates an estimation of a predictor which can predict Ye
on unseen domains only by data from training domains Etr(Arjovsky et al., 2019, Section 4.1). They also
derived sufficient conditions on training domains for the equation Itr=I, assuming that data follow a simple
linear SEM (Arjovsky et al., 2019, Theorem 9). Koyama & Yamaguchi (2021); Rojas-Carulla et al. (2018)
presented sufficient conditions for an invariance Î¦to achieve the minimum of (1). Koyama & Yamaguchi
(2021) proved that the invariance that maximizes the mutual information with labels also maximizes the
o.o.d. risk. Rojas-Carulla et al. (2018) proved that, under the domain setting (6), the conditional expectation
E[Ye|Î¦X1(Xe) =x1]also minimizes the o.o.d. risk, even when E[Ye|Î¦X1(Xe)]is nonlinear. However, all the
results by Koyama & Yamaguchi (2021); Rojas-Carulla et al. (2018); Arjovsky et al. (2019) did not deal with
any theoretical connections between invariances obtained by minimizing the bi-leveled optimization problem
(3) and the o.o.d. risk (1). It does not follow obviously that the minimum of (3) satisfies these sufficient
6Published in Transactions on Machine Learning Research (01/2024)
conditions by Koyama & Yamaguchi (2021); Rojas-Carulla et al. (2018), and hence our main theorems can
not be deduced as a trivial corollary of the results by Koyama & Yamaguchi (2021); Rojas-Carulla et al.
(2018). To discuss the non-trivial relation between the bi-leveled optimization problem (3) and the o.o.d. risk
(1), we establish a novel characterization of invariances (Lemmas 4 and 6), and derive the main theorems
based on it.
To reduce the annotation cost required for the original IRM approach, Toyota & Fukumizu (2022) introduced
a new bi-level optimization problem similar to (3). They considered a situation in which the training data for
targetclassificationareprovidedinonlyonedomain, whilethetaskofahigherlabelhierarchy, whichrequires
lowerannotationcost, hasdatafrommultipledomains. Undertheavailabilityofdata, theydeducedabi-level
optimization problem, in which invariances were given by additional data in a higher label hierarchy. For
further details, we refer the reader to the original paper Toyota & Fukumizu (2022). Their study provided
a detailed theoretical analysis concerning their method and its connection to the o.o.d. risk; however, they
did not analyze relationships between their bi-leveled optimization problem and the o.o.d. risk, which is
the focus of this paper. Instead, they investigated relationships between an optimization method for their
bi-level optimization problem and the o.o.d. risk. Moreover, they assume that invariances Î¦run all variable
selections for simplicity of theoretical analysis. On the other hand, this paper derives the direct relations
between the minimum of the bi-leveled optimization problem and the o.o.d. risk (1). Moreover, we consider
the more realistic setting for the analysis of IRM where invariances Î¦run all continuous functions.
4 Proofs
In this section, we prove Theorem 1 and 2. Through the section, for XeâˆˆXandxâˆˆX, itsXi-components
(i= 1,2) are denoted by Xe
iandxirespectively.
4.1 Proof Sketch of Main Theorems
Before giving rigorous proof, we briefly describe the rough proof sketch of the main theorems. The following
two lemmas (A) and (B) play an important role in our proof:
(A) The conditional expectation E[Ye|Xe
1] =E[YI|XI
1]and conditional probability PYe|Xe
1=PYI|XI
1
minimize the o.o.d. risk under the least-square and cross-entropy losses respectively (Lemmas 3 and
5).
(B) Any invariance among all domains can be represented by the composition of the projection onto X1;
that is, Î¦âˆˆIC0can be represented as
Î¦ = Î¨â—¦Î¦X1
for some continuous map Î¨(Lemmas 4 and 6).
The two lemmas intuitively conclude the proof of the main theorem as follows. Firstly, since IC0
tr=IC0holds
(Condition (i)), observe that a predictor in (3) runs composition maps wâ—¦Î¦withwâˆˆWC0andÎ¦âˆˆIC0.
Moreover, since the above second lemma (B) ensures that Î¦âˆˆIC0can be represented by the composition
of the projection onto X1, we can see that a predictor in (3) runs wâ—¦Î¦X1for some function class wâˆˆWâˆ—,
and hence, the bi-leveled optimization problem is expressed as
minwâˆˆWâˆ—/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦X1). (10)
It is well-known that, assuming that wruns all measurable functions,
Ë†wâˆˆarg min
wRe(wâ—¦Î¦X1)â‡â‡’ Ë†w(x1) =E[Ye|Xe
1=x1] =E[YI|XI
1=x1]PXe
1âˆ’almost everywhere .
or
Ë†wâˆˆarg min
wRe(wâ—¦Î¦X1)â‡â‡’ Ë†w(x1) =PYe|Xe
1=x1=PYI|XI
1=x1PXe
1âˆ’almost everywhere .
7Published in Transactions on Machine Learning Research (01/2024)
hold for any eâˆˆEunder the least-square and cross-entropy losses respectively (Christmann & Steinwart,
2008, Example 2.6). Hence, ignoring the capability of Wâˆ—and the discussion of almost everywhere , we can
see that
E[YI|XI
1=x1]â‰ˆarg minwâˆˆWâˆ—/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦X1) (11)
or
PYI|XI
1=x1â‰ˆarg minwâˆˆWâˆ—/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦X1) (12)
hold. Combining eq.s (11), (12) and the first lemma (A), it concludes the main theorems intuitively. In the
following section, we give the rigorous justification of the above rough proof sketch.
4.2 Proof of Theorem 1
To prove the main theorem, we prepare two lemmas.
Lemma 3. LetwI:X1â†’Ybe the conditional expectation obtained by pI(y|x1); namely,
wI(x1) =E[YI|XI
1=x1] :=/integraldisplay
yÂ·pI(y|x1)dy.
Then,
wIâ—¦Î¦X1âˆˆarg min
f:Xâ†’YRo.o.d.(f).
Lemma 4. Any Î¦âˆˆIC0
tris represented as
Î¦ = Î¨â—¦Î¦X1
for some continuous map Î¨ :X1â†’H.
Proof of Lemma 34It suffices to prove the following statement:
For anyfâˆˆFand(Xa,Ya)âˆˆ{(Xe,Ye)}eâˆˆE, there exists (Xb,Yb)âˆˆ{(Xe,Ye)}eâˆˆEsuch that
/integraldisplay
âˆ¥wIâ—¦Î¦X1(x)âˆ’yâˆ¥2dPXa,Ya(x,y)â‰¤/integraldisplay
âˆ¥f(x)âˆ’yâˆ¥2dPXb,Yb(x,y). (13)
Take arbitrary fâˆˆ Fand (Xa,Ya)âˆˆ {(Xe,Ye)}eâˆˆE. Define (Xb,Yb)âˆˆ {(Xe,Ye)}eâˆˆEsuch that its
distribution is the direct product PXa
1,YaâŠ—PX2, wherePXa
1,Yais the marginal distribution of PXa,Yaon
X1Ã—YandPX2is an arbitrary distribution on X2.
Then, the right-hand side of the inequality (13) is given by
/integraldisplay
âˆ¥f(x)âˆ’yâˆ¥2dPXb,Yb(x,y) =/integraldisplay
âˆ¥f(x)âˆ’yâˆ¥2d(PXa
1,YaâŠ—PX2)(x,y)
=/integraldisplay
PX2(x2)/integraldisplay
âˆ¥f(x1,x2)âˆ’yâˆ¥2dPXa
1,Ya(x1,y).
Clearly, for any xâˆ—
2âˆˆX2, the inequality
/integraldisplay
âˆ¥f(x1,xâˆ—
2)âˆ’yâˆ¥2dPXa
1,Ya(x1,y)â‰¥/integraldisplay
âˆ¥E[Ye1|Xa
1=x1]âˆ’yâˆ¥2dPXa
1,Ya(x1,y)
4The proof is essentially the same as the one for Theorem 1 in Rojas-Carulla et al. (2018) and Theorem 6 in Toyota &
Fukumizu (2022).
8Published in Transactions on Machine Learning Research (01/2024)
holds, because the minimum of a risk on the least square loss is attained at the conditional expectation
E[Ya|Xa
1]. Hence, we obtain
/integraldisplay
âˆ¥f(x)âˆ’yâˆ¥2dPXb,Yb(x,y) =/integraldisplay
PX2(x2)/integraldisplay
âˆ¥f(x1,x2)âˆ’yâˆ¥2dPXa
1,Ya(x1,y)
â‰¥/integraldisplay
PX2(x2)/integraldisplay
âˆ¥E[Ye1|Xa
1=x1]âˆ’yâˆ¥2dPXa
1,Ya(x1,y)
=/integraldisplay
âˆ¥E[Ya|Xa
1=x1]âˆ’yâˆ¥2dPXa
1,Ya(x1,y)
=/integraldisplay
PXa
2|Xa
1,Ya(x2)/integraldisplay
âˆ¥E[Ya|Xa
1=x1]âˆ’yâˆ¥2dPXa
1,Ya(x1,y)
=/integraldisplay
âˆ¥E[Ya|Xa
1= Î¦X1(x)]âˆ’yâˆ¥2dPXa,Ya(x,y)
=/integraldisplay
âˆ¥wIâ—¦Î¦X1(x)âˆ’yâˆ¥2dPXa,Ya(x,y),
which concludes the proof. Here, the last equality is derived from the fact that the conditional expectation
E[Ye|Xe
1= Î¦X1(x)]does not depend on eâˆˆEand corresponds to wIâ—¦Î¦X1.
Proof sketch of Lemma 4 Before providing a complete proof, we show a proof sketch of Lemma 4 to
make the flow of our proof easier to understand. First, we prove that Î¦âˆˆIC0
trcan be represented as
Î¦ = Î¨â—¦Î¦X1(14)
by some map Î¨ :X1â†’H, which is not restricted to a continuous map. Take arbitrary Î¦âˆˆIC0
tr. Then,
since Î¦âˆˆIC0=IC0
tr(Condition (i)), for any (Xa,Ya),(Xb,Yb)âˆˆ{(Xe,Ye)}eâˆˆE,
PYa|Î¦(Xa)=PYb|Î¦(Xb),
and therefore, we have
PYa|Î¦(Xa)(N|Î¦(x)) =PYb|Î¦(Xb)(N|Î¦(x)) (15)
for any set NâŠ‚Yandâˆ€xâˆˆX. We prove the statement (14) by contradiction. Assume that there exist no
maps Î¨that satisfy (14). Then, there exist xâˆ—
1âˆˆX1,xâˆ—
2,xâˆ—âˆ—
2âˆˆX2such that
Î¦(xâˆ—
1,xâˆ—
2)Ì¸= Î¦(xâˆ—
1,xâˆ—âˆ—
2).5
By utilizing xâˆ—
1âˆˆX1,xâˆ—
2,xâˆ—âˆ—
2âˆˆX2, we can construct (Xa,Ya),(Xb,Yb)âˆˆ{(Xe,Ye)}eâˆˆEandNâŠ‚Ywhich
satisfy
PYa|Î¦(Xa)(N|Î¦(xâˆ—
1,xâˆ—
2))Ì¸=PYb|Î¦(Xb)(N|Î¦(xâˆ—
1,xâˆ—
2)).
This contradicts the assumption (15), and we can conclude Î¦âˆˆ IC0
trcan be represented as (14). The
continuity of Î¨is easily derived from the continuity of Î¦, and we can conclude the proof.
Proof of Lemma 4 First, we prove that Î¦âˆˆIC0
trcan be represented as
Î¦ = Î¨â—¦Î¦X1(16)
by some map Î¨ :X1â†’H, which is not restricted to a continuous map. We prove this statement by
contradiction. Take Î¦âˆˆIC0
trand assume that there exist no maps Î¨which satisfy (16). Then, there exist
xâˆ—
1âˆˆX1,xâˆ—
2,xâˆ—âˆ—
2âˆˆX2such that
Î¦(xâˆ—
1,xâˆ—
2)Ì¸= Î¦(xâˆ—
1,xâˆ—âˆ—
2). (17)
5IfÎ¦(xâˆ—
1, xâˆ—
2) = Î¦( xâˆ—
1, xâˆ—âˆ—
2)for any xâˆ—
1âˆˆX1,xâˆ—
2, xâˆ—âˆ—
2âˆˆX2,Î¦depend only on the first component X1; hence, we can see that
Î¦âˆˆIC0
trcan be represented as Î¦ = Î¨â—¦Î¦X1by some map Î¨, which contradicts to the assumption.
9Published in Transactions on Machine Learning Research (01/2024)
ð‘¥2âˆ—ð‘¥2âˆ—âˆ—
ð‘¥1âˆ—
ð‘¥1ð‘¥2ð‘¦
ð‘ð‘¦âˆ—supp(ð‘ƒð‘‹ð‘Ž,ð‘Œð‘Ž)
supp(ð‘ƒð‘‹ð‘,ð‘Œð‘)
Figure 1: Supports of probability distributions PXa,YaandPXb,Yb. The figure implies that PXa,Ya(Nyâˆ—Ã—
Î¦âˆ’1(Î¦âˆ—))Ì¸= 0andPXb,Yb(Nyâˆ—Ã—Î¦âˆ’1(Î¦âˆ—)) = 0(âˆµ(xâˆ—
1,xâˆ—âˆ—
2)/âˆˆÎ¦âˆ’1(Î¦âˆ—)(17)), and that PXa,Ya(Î¦âˆ’1(Î¦âˆ—))Ì¸= 0
andPXb,Yb(Î¦âˆ’1(Î¦âˆ—))Ì¸= 0. These Eqs. lead us PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—)Ì¸= 0 =PYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—).
Fixyâˆ—âˆˆYwithpI(yâˆ—|xâˆ—
1)>0and take an open neighborhood Nyâˆ—âŠ‚Ycentered at yâˆ—which satisfies
0</integraldisplay
Nyâˆ—pI(y|xâˆ—
1)dy< 1.
Here, the existence of Nyâˆ—is derived from the continuity of pI(Â·|xâˆ—
1)(Condition (iv)).
Define two maps gi:Yâ†’X 2(i= 1,2) by
g1(y) =/braceleftbigg
xâˆ—
2(yâˆˆNyâˆ—)
xâˆ—âˆ—
2(else)aaaaaag2(y) =/braceleftbiggxâˆ—âˆ—
2(yâˆˆNyâˆ—)
xâˆ—
2(else).
Take two distributions (Xa,Ya),(Xb,Yb)âˆˆ{(Xe,Ye)}eâˆˆEsuch that their distributions PXa,YaandPXb,Yb
coincide with
PXa,Ya=PXa
2|YaâŠ—PYI|XI
1âŠ—PX1, PXb,Yb=PXb
2|YbâŠ—PYI|XI
1âŠ—PX1.
Here,
â€¢PX1is a distribution on X1where its p.d.f. coincides with a delta function Î´xâˆ—
1(x1)onxâˆ—
1,
â€¢the conditional p.d.f.s of PXa
2|Ya(Â·|y)andPXb
2|Yb(Â·|y)coincide with Î´g1(y)(x2)andÎ´g2(y)(x2)respec-
tively.
The supports of PXa,YaandPXb,Ybare visualized in Fig. 1. As Î¦âˆˆ IC0=IC0
tr(Condition (i)) and
(Xa,Ya),(Xb,Yb)âˆˆ{(Xe,Ye)}eâˆˆE,
PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—) =PYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—), (18)
where Î¦âˆ—:= Î¦(xâˆ—
1,xâˆ—
2). Let us compute PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—)andPYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—)to derive
PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—)Ì¸=PYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—), which contradicts to the equality (18)6. We evaluate
PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—) =PÎ¦(Xa),Ya({Î¦âˆ—}Ã—Nyâˆ—)
PÎ¦(Xa)({Î¦âˆ—})=PXa,Ya(Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—)
PXa(Î¦âˆ’1(Î¦âˆ—))
6Fig. 1 illustrates the intuitive reason why PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—)Ì¸=PYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—)is derived, and hence, will help us
understand the following rigorous proof.
10Published in Transactions on Machine Learning Research (01/2024)
by computing its numerator and denominator separately. First, the numerator is evaluated as
PXa,Ya(Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—) =/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—Î´g1(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dxdy
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g1(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx.
Noting that g1(y) =xâˆ—
2forâˆ€yâˆˆNyâˆ—andÎ´xâˆ—
1(x1)Ã—Î´xâˆ—
2(x2) =Î´(xâˆ—
1,xâˆ—
2)(x1,x2), we obtain
/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g1(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´xâˆ—
2(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—
2)(x1,x2)Â·pI(y|x1)dx.
Since (xâˆ—
1,xâˆ—
2)âˆˆÎ¦âˆ’1(Î¦âˆ—), we have
/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—
2)(x1,x2)Â·pI(y|x1)dx=/integraldisplay
Nyâˆ—pI(y|xâˆ—
1)dy,
which leads us to the equality
PXa,Ya(Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—) =/integraldisplay
Nyâˆ—pI(y|xâˆ—
1)dy.
Next, let us evaluate the denominator PXa(Î¦âˆ’1(Î¦âˆ—)).
PXa(Î¦âˆ’1(Î¦âˆ—)) =PXa,Ya(Î¦âˆ’1(Î¦âˆ—)Ã—Y) =/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Ã—YÎ´g1(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dxdy
=/integraldisplay
Ydy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g1(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g1(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
+/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g1(y)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´xâˆ—
2(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
+/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´xâˆ—âˆ—
2(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—
2)(x1,x2)Â·pI(y|x1)dx
+/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—âˆ—
2)(x1,x2)Â·pI(y|x1)dx.
Here, the fourth equality is derived from the facts that g1(y) =xâˆ—
2forâˆ€yâˆˆNyâˆ—andg1(y) =xâˆ—âˆ—
2for
âˆ€yâˆˆYâˆ’Nyâˆ—. Noting that (xâˆ—
1,xâˆ—
2)âˆˆÎ¦âˆ’1(Î¦âˆ—)and(xâˆ—
1,xâˆ—âˆ—
2)/âˆˆÎ¦âˆ’1(Î¦âˆ—), we obtain
PXa(Î¦âˆ’1(Î¦âˆ—)) =/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—
2)(x1,x2)Â·pI(y|x1)dx
+/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—âˆ—
2)(x1,x2)Â·pI(y|x1)dx
=/integraldisplay
Nyâˆ—dyÂ·pI(y|xâˆ—
1) +/integraldisplay
Yâˆ’Nyâˆ—dyÂ·0
=/integraldisplay
Nyâˆ—pI(y|xâˆ—
1)dy.
11Published in Transactions on Machine Learning Research (01/2024)
Hence, we obtain
PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—) =PXa,Ya(Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—)
PXa(Î¦âˆ’1(Î¦âˆ—))
=/integraltext
Nyâˆ—pI(y|xâˆ—
1)dy
/integraltext
Nyâˆ—pI(y|xâˆ—
1)dy= 1.
Next, let us evaluate
PYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—) =PÎ¦(Xb),Yb({Î¦âˆ—}Ã—Nyâˆ—)
PÎ¦(Xb)({Î¦âˆ—})=PXb,Yb(Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—)
PXb(Î¦âˆ’1(Î¦âˆ—)).
The numerator is evaluated as
PXb,Yb(Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—) =/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—Î´g2(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dxdy
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g2(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx.
Noting that g2(y) =xâˆ—âˆ—
2forâˆ€yâˆˆNyâˆ—, we obtain
/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g2(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´xâˆ—âˆ—
2(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—âˆ—
2)(x1,x2)Â·pI(y|x1)dx
=/integraldisplay
Nyâˆ—dyÂ·0 = 0.
Here, the third equality is derived from (xâˆ—
1,xâˆ—âˆ—
2)/âˆˆÎ¦âˆ’1(Î¦âˆ—). Next, the denominator PXb(Î¦âˆ’1(Î¦âˆ—))is
evaluated as
PXb(Î¦âˆ’1(Î¦âˆ—)) =PXb,Yb(Î¦âˆ’1(Î¦âˆ—)Ã—Y) =/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Ã—YÎ´g2(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dxdy
=/integraldisplay
Ydy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g2(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g2(y)(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx+
/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´g2(y)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´xâˆ—âˆ—
2(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
+/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´xâˆ—
2(x2)Â·pI(y|x1)Â·Î´xâˆ—
1(x1)dx
=/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—âˆ—
2)(x1,x2)Â·pI(y|x1)dx
+/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—
2)(x1,x2)Â·pI(y|x1)dx.
12Published in Transactions on Machine Learning Research (01/2024)
Noting that (xâˆ—
1,xâˆ—
2)âˆˆÎ¦âˆ’1(Î¦âˆ—)and(xâˆ—
1,xâˆ—âˆ—
2)/âˆˆÎ¦âˆ’1(Î¦âˆ—), we obtain
PXb(Î¦âˆ’1(Î¦âˆ—)) =/integraldisplay
Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—âˆ—
2)(x1,x2)Â·pI(y|x1)dx
+/integraldisplay
Yâˆ’Nyâˆ—dy/integraldisplay
Î¦âˆ’1(Î¦âˆ—)Î´(xâˆ—
1,xâˆ—
2)(x1,x2)Â·pI(y|x1)dx
=/integraldisplay
Nyâˆ—dyÂ·0 +/integraldisplay
Yâˆ’Nyâˆ—dyÂ·pI(y|xâˆ—
1)
=/integraldisplay
Yâˆ’Nyâˆ—pI(y|xâˆ—
1)dyÌ¸= 0.
Hence, we obtain
PYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—) =PXb,Yb(Î¦âˆ’1(Î¦âˆ—)Ã—Nyâˆ—)
PXb(Î¦âˆ’1(Î¦âˆ—))
=0/integraltext
Yâˆ’Nyâˆ—pI(y|xâˆ—
1)= 0.
Combing these results, we obtain
PYa|Î¦(Xa)(Nyâˆ—|Î¦âˆ—) = 1Ì¸= 0 =PYb|Î¦(Xb)(Nyâˆ—|Î¦âˆ—),
which contradicts the assumption
PYa|Î¦(Xa)=PYb|Î¦(Xb).
Because the continuity of Î¨is trivial, we can conclude the proof.
Finally, we prove Theorem 1.
Proof of Theorem 1 Take
fâˆ—âˆˆarg minÎ¦âˆˆIC0
tr,wâˆˆWC0/summationdisplay
eâˆˆEtrRe(wâ—¦Î¦). (19)
Then, by Lemma 4, we can represent fâˆ—as
fâˆ—=wâˆ—â—¦Î¦X1
for some continuous map wâˆ—:X1â†’ Y7. Let us prove that wâˆ—â—¦Î¦X1âˆˆarg minf:Xâ†’YRo.o.d.(f)
by contradiction; assuming that wâˆ—â—¦Î¦X1/âˆˆarg minfRo.o.d.(f), we will derive wâˆ—â—¦Î¦X1/âˆˆ
arg minÎ¦âˆˆIC0
tr,wâˆˆWC0/summationtext
eâˆˆEtrRe(wâ—¦Î¦), which contradicts to (19). We prove it by the following three steps.
Step 1 First, we prove that there exist a training domain eâˆ—âˆ—âˆˆEtrand an open set N1âŠ‚X 1which satisfy
wâˆ—(x1)Ì¸=wI(x1)forâˆ€x1âˆˆN1 (20)
withPXeâˆ—âˆ—
1(N1)>0. AswIâ—¦Î¦X1minimizes the o.o.d. risk (Lemma 3), we have
Ro.o.d.(wâˆ—â—¦Î¦X1)>min
f:Xâ†’YRo.o.d.(f) =Ro.o.d.(wIâ—¦Î¦X1).
7By Lemma 4, fâˆ—can be represented by fâˆ—=wâˆ—â—¦Î¨âˆ—â—¦Î¦X1forÎ¨âˆ—:X1â†’Handwâˆ—:Hâ†’Y. Replacing wâˆ—â—¦Î¨âˆ—bywâˆ—,
we can obtain the desirable statement.
13Published in Transactions on Machine Learning Research (01/2024)
Here, the first inequality is derived from the assumption of a proof by contradiction. Noting that Ro.o.d.is
maximum of risk among {(Xe,Ye)}, there exists (Xeâˆ—,Yeâˆ—)âˆˆ{(Xe,Ye)}eâˆˆEsuch that
Reâˆ—(wâˆ—â—¦Î¦X1)/parenleftï£¬ig
=/integraldisplay
âˆ¥yâˆ’wâˆ—(x1)âˆ¥2dPXeâˆ—
1,Yeâˆ—/parenrightï£¬ig
>Reâˆ—(wIâ—¦Î¦X1)/parenleftï£¬ig
=/integraldisplay
âˆ¥yâˆ’wI(x1)âˆ¥2dPXeâˆ—
1,Yeâˆ—/parenrightï£¬ig
(21)
holds8. Since (21) is rewritten as
/integraldisplay/braceleftbig
âˆ¥yâˆ’wâˆ—(x1)âˆ¥2âˆ’âˆ¥yâˆ’wI(x1)âˆ¥2/bracerightbig
dPXeâˆ—
1,Yeâˆ—>0,
we can see that
âˆ¥yâˆ—âˆ’wâˆ—(xâˆ—
1)âˆ¥2âˆ’âˆ¥yâˆ—âˆ’wI(xâˆ—
1)âˆ¥2>0
for some (xâˆ—
1,yâˆ—)âˆˆX1Ã—Y. Sincewâˆ—andwIare continuous, taking sufficiently small Îµ>0, we have
âˆ¥yâˆ—âˆ’wâˆ—(x1)âˆ¥2âˆ’âˆ¥yâˆ—âˆ’wI(x1)âˆ¥2>0forâˆ€x1âˆˆNÎµ
xâˆ—
1, (22)
whereNÎµ
xâˆ—
1is theÎµ-ball centered at xâˆ—
1. Here, the continuity of wIis derived from Condition (iv) in Theorem
1. Moreover, (22) leads us to the statement
wâˆ—(x1)Ì¸=wI(x1)forâˆ€x1âˆˆNÎµ
xâˆ—
1.
By the condition (ii), NÎµ
xâˆ—
1/intersectiontextsupp(PXeâˆ—âˆ—
1)Ì¸=âˆ…for someeâˆ—âˆ—âˆˆEtr. Take
xâˆ—âˆ—
1âˆˆNÎµ
xâˆ—
1/intersectiondisplay
supp(PXeâˆ—âˆ—
1)def of supp= = =
NÎµ
xâˆ—
1/intersectiondisplay/braceleftï£¬ig
x1âˆˆX1/vextendsingle/vextendsingle/vextendsingleNx1:open neighborhood around x1â‡’(PXeâˆ—âˆ—
1)(Nx1)>0/bracerightï£¬ig
Ì¸=âˆ….
Replacingxâˆ—âˆ—
1, if necessary, we may assume that
xâˆ—âˆ—
1âˆˆNÎµ
xâˆ—
1/intersectiondisplay/braceleftï£¬ig
x1âˆˆX1/vextendsingle/vextendsingle/vextendsingleNx1:open neighborhood around x1â‡’(PXeâˆ—âˆ—
1)(Nx1)>0/bracerightï£¬ig
.(23)
Take an open set N1âŠ‚NÎµ
xâˆ—
1which includes xâˆ—âˆ—
1. Then, we have
wâˆ—(x1)Ì¸=wI(x1)forâˆ€x1âˆˆN1. (24)
Observing that
xâˆ—âˆ—
1âˆˆ{x1âˆˆX1|Nx1:open neighborhood with x1âˆˆNx1â‡’(PXeâˆ—âˆ—)(Nx1)>0},
we havePXeâˆ—âˆ—
1(N1)>0. It concludes the proof of Step 1.
Step 2 Next, we prove the inequality
/summationdisplay
eâˆˆEtrRe(wâˆ—â—¦Î¦X1)>/summationdisplay
eâˆˆEtrRe(wIâ—¦Î¦X1). (25)
To derive the inequality, note that
Ë†wâˆˆarg min
wRe(wâ—¦Î¦X1)â‡â‡’ Ë†w(x1) =wI(x1)PXe
1âˆ’a.e.,
8Note that eâˆ—is not necessarily included in training domains Etr. The inequality (21) for some training domain eâˆ—âˆ—âˆˆEtr
are proved in Step 2 (eq. (29)).
14Published in Transactions on Machine Learning Research (01/2024)
or equivalently,
Ë†wâˆˆarg min
wRe(wâ—¦Î¦X1)â‡â‡’PXe
1(/braceleftbig
x1âˆˆX1/vextendsingle/vextendsingleË†w(x1)Ì¸=wI(x1)/bracerightbig
) = 0 (26)
holds for any eâˆˆ E(Christmann & Steinwart, 2008, Example 2.6). Taking the contraposition of the
implication from the left to right propositions in (26), we have
Ë†wsatisfiesPXe
1(/braceleftbig
x1âˆˆX1/vextendsingle/vextendsingleË†w(x1)Ì¸=wI(x1)/bracerightbig
)>0â‡’Ë†w /âˆˆarg min
wRe(wâ—¦Î¦X1). (27)
From (20), we have the inequality
PXeâˆ—âˆ—
1(/braceleftbig
x1âˆˆX1/vextendsingle/vextendsinglewâˆ—(x1)Ì¸=wI(x1)/bracerightbig
)>PXeâˆ—âˆ—
1(N1)>0 (28)
for someeâˆ—âˆ—âˆˆEtrand an open set N1âŠ‚X 1. (27) and (28) lead us to statement wâˆ—/âˆˆarg minwReâˆ—âˆ—(wâ—¦Î¦X1),
and hence, we have the inequality
Reâˆ—âˆ—(wâˆ—â—¦Î¦X1)>min
wReâˆ—âˆ—(wâ—¦Î¦X1) =Reâˆ—âˆ—(wIâ—¦Î¦X1). (29)
Moreover, since the conditional expectation wIminimizes the risk, we have
Re(wâˆ—â—¦Î¦X1)â‰¥Re(wIâ—¦Î¦X1) (30)
for anyeâˆˆE. (29) and (30) lead us to the inequality
/summationdisplay
eâˆˆEtrRe(wâˆ—â—¦Î¦X1) =Reâˆ—âˆ—(wâˆ—â—¦Î¦X1) +/summationdisplay
eâˆˆEtrâˆ’{eâˆ—âˆ—}Re(wâˆ—â—¦Î¦X1)
(29)
>Reâˆ—âˆ—(wIâ—¦Î¦X1) +/summationdisplay
eâˆˆEtrâˆ’{eâˆ—âˆ—}Re(wâˆ—â—¦Î¦X1)
(30)
â‰¥ Reâˆ—âˆ—(wIâ—¦Î¦X1) +/summationdisplay
eâˆˆEtrâˆ’{eâˆ—âˆ—}Re(wIâ—¦Î¦X1) =/summationdisplay
eâˆˆEtrRe(wIâ—¦Î¦X1).
Step 3 Finally, we prove wâˆ—â—¦Î¦X1/âˆˆarg minÎ¦âˆˆIC0
tr,wâˆˆWC0/summationtext
eâˆˆEtrRe(wâ—¦Î¦), which contradicts to (19). By
the inequality (25) proved in Step 2, it suffices to prove that there exist Î¦â€ âˆˆIC0
trandwâ€ âˆˆWC0such that
wIâ—¦Î¦X1=wâ€ â—¦Î¦â€ . Define Î¦â€ = Î¨â€ â—¦Î¦X1where the embedding Î¨â€ :X1(=Rd1)â†’H (=RdH)is defined by
Rd1âˆ‹ï£«
ï£¬ï£¬ï£¬ï£­x1
x2
...
xdï£¶
ï£·ï£·ï£·ï£¸Î¨â€ 
âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€âˆ’â†’ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­x1
x2
...
xd
0
...
0ï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸âˆˆRdH.
Here, we can define the embedding Î¨â€ sinced1â‰¤dH(Condition (iii)). Noting that PYe|Î¦â€ (Xe)=
PYe|Î¦X1(Xe)=PYI|XI
1for anyeâˆˆE, we can see that Î¦â€ âˆˆIC0
tr. Defining
RdHâˆ‹ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£¬ï£­x1
x2
...
xd
xd+1
...
xhï£¶
ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£·ï£¸wâ€ 
âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€âˆ’â†’E[YI|XI
1=ï£«
ï£¬ï£¬ï£¬ï£­x1
x2
...
xdï£¶
ï£·ï£·ï£·ï£¸]âˆˆY,
we can see that wIâ—¦Î¦X1=wâ€ â—¦Î¦â€ . Observing wâ€ âˆˆWC0by Condition (iv), we can concludes wâˆ—â—¦Î¦X1/âˆˆ
arg minÎ¦âˆˆIC0
tr,wâˆˆWC0/summationtext
eâˆˆEtrRe(wâ—¦Î¦), which contradicts to (19).
15Published in Transactions on Machine Learning Research (01/2024)
4.3 Proof of Theorem 2
We prepare two lemmas.
Lemma 5. LetpI:X1â†’PYbe the conditional p.d.f. of PYI|XI
1; namely,
/parenleftbig
pI(x)/parenrightbig
i:=pI(i|x).
Then,
pIâ—¦Î¦X1âˆˆarg min
f:Xâ†’PYRo.o.d.(f).
Lemma 6. Any Î¦âˆˆIC0
tris represented as
Î¦ = Î¨â—¦Î¦X1
for some continuous map Î¨ :X1â†’H.
Proof of Lemma 5 The proof is essentially the same as the ones for Lemma 3; hence, we omit the
proof.
Proof of Lemma 6 First, we prove that Î¦âˆˆIC0
trcan be represented as
Î¦ = Î¨â—¦Î¦X1(31)
by some map Î¨ :X1â†’X 1, which is not restricted to a continuous map. We prove this statement by
contradiction in the same manner as the proof in Lemma 4. Take Î¦âˆˆIC0
tr. Then, there exist xâˆ—
1âˆˆX 1,
xâˆ—
2,xâˆ—âˆ—
2âˆˆX2such that
Î¦(xâˆ—
1,xâˆ—
2)Ì¸= Î¦(xâˆ—
1,xâˆ—âˆ—
2).
Fixyâˆ—âˆˆYwithpI(yâˆ—|xâˆ—
1)>0. Define two maps gi:Yâ†’X 2(i= 1,2) by
g1(y) =/braceleftbigg
xâˆ—
2(y=yâˆ—)
xâˆ—âˆ—
2(else)aaaaaag2(y) =/braceleftbigg
xâˆ—âˆ—
2(y=yâˆ—)
xâˆ—
2(else)
Take two distributions (Xa,Ya),(Xb,Yb)âˆˆ{(Xe,Ye)}eâˆˆEsuch that their distributions PXa,YaandPXb,Yb
coincide with
PXa,Ya=PXa
2|YaâŠ—PYI|XI
1âŠ—PX1, PXb,Yb=PXb
2|YbâŠ—PYI|XI
1âŠ—PX1.
Here
â€¢PX1is a distribution on X1where its p.d.f. coincides with a delta function Î´xâˆ—
1(x1)onxâˆ—
1,
â€¢the conditional p.d.f.s of PXa
2|Ya(Â·|y)andPXb
2|Yb(Â·|y)coincide with Î´g1(y)(x2)andÎ´g2(y)(x2)respec-
tively.
Since Î¦âˆˆIC0=IC0
tr(Condition (i)) and (Xa,Ya),(Xb,Yb)âˆˆ{(Xe,Ye)}eâˆˆE,
PYa|Î¦(Xa)({yâˆ—}|Î¦âˆ—) =PYb|Î¦(Xb)({yâˆ—}|Î¦âˆ—), (32)
where Î¦âˆ—:= Î¦(xâˆ—
1,xâˆ—
2). Let us compute PYa|Î¦(Xa)({yâˆ—}|Î¦âˆ—)andPYb|Î¦(Xb)({yâˆ—}|Î¦âˆ—), respectively. Same as
the proof in Lemma 4, we have the two equalities
PÎ¦(Xa),Ya({Î¦âˆ—}Ã—{yâˆ—}) =pI(yâˆ—|xâˆ—
1)andPÎ¦(Xa)({Î¦âˆ—}) =pI(yâˆ—|xâˆ—
1),
which lead us to the equality
PYa|Î¦(Xa)({yâˆ—}|Î¦âˆ—) =PÎ¦(Xa),Ya({Î¦âˆ—}Ã—{yâˆ—})
PÎ¦(Xa)({Î¦âˆ—})
=pI(yâˆ—|xâˆ—
1)
pI(yâˆ—|xâˆ—
1)= 1.
16Published in Transactions on Machine Learning Research (01/2024)
Similarly, we have
PYb|Î¦(Xb)({yâˆ—}|Î¦âˆ—) = 0andPÎ¦(Xb)({Î¦âˆ—}) =/summationdisplay
yâˆˆYâˆ’{yâˆ—}pI(y|xâˆ—
1)Ì¸= 0,
which lead us to the equality
PYb|Î¦(Xb)({yâˆ—}|Î¦âˆ—) =PÎ¦(Xb),Yb({Î¦âˆ—}Ã—{yâˆ—})
PÎ¦(Xb)({Î¦âˆ—})
=0/summationtext
yâˆˆYâˆ’{yâˆ—}pI(y|xâˆ—
1)= 0.
Here,/summationtext
yâˆˆYâˆ’{yâˆ—}pI(y|xâˆ—
1)Ì¸= 0is derived by Condition (v). Combing these results, we obtain
PYa|Î¦(Xa)({yâˆ—}|Î¦âˆ—) = 1Ì¸= 0 =PYb|Î¦(Xb)({yâˆ—}|Î¦âˆ—),
which contradicts the assumption
PYa|Î¦(Xa)=PYb|Î¦(Xb).
Because the continuity of Î¨is trivial, we can conclude the proof.
Proof of Theorem 2 This is essentially the same as the one for Theorem 1, and hence, we omit the
proof.
5 Conclusions
In this paper, we have proved that a solution for the bi-leveled optimization problem (3) also minimizes
o.o.d. risk (2) under four conditions in regression and classification cases, assuming that distributions on
domains are the ones proposed in Rojas-Carulla et al. (2018) and that models run all continuous functions.
Particularly, we have provided a sufficient condition on the training domains Etrand the dimension of the
feature spaceHfor the optimization problem (3) to minimize the o.o.d. risk.
Several challenges still exist. The first problem is the theoretical analysis of the optimization method for
(3). To solve the challenging optimization problem (3), various optimization techniques have been proposed
(Arjovsky et al., 2019; Lin et al., 2022a; Zhou et al., 2022), and there has been little discussion about their
effectiveness. For example, while Arjovsky et al. (2019) optimized (3) by minimizing
/summationdisplay
eâˆˆEtrRe(Î¦) +Î»Â·âˆ¥âˆ‡w|w=1.0Re(wÂ·Î¦)âˆ¥2,
their effectiveness was evaluated only under specific SEMs (Rosenfeld et al., 2021). Thus, it is important to
investigate this analysis under a more general case.
Second, we should evaluate the o.o.d. performance of the bi-leveled optimization problem (3) under the case
where the conditions in Theorems 1 and 2 are violated. Particularly, as noted in Section 2, condition Itr=I
does not generally hold. In such cases, for (Î¦âˆ—,wâˆ—)âˆˆarg minÎ¦âˆˆIC0
tr,wâˆˆWC0/summationtext
eâˆˆEtrRe(wâ—¦Î¦),
Ro.o.d(wâˆ—â—¦Î¦âˆ—)âˆ’minRo.o.d(f)
is not necessarily 0. The quantitative evaluation of the difference is crucial for future work.
Thirdly, we should investigate the feasibility of the condition Itr=I, which is known to be an important
and unsolved problem shared by all invariance-based methods (Arjovsky et al., 2019; Peters et al., 2016;
Toyota & Fukumizu, 2022). As Condition (i) in our main results, all methods based on invariances implicitly
or explicitly assume that invariances among training domains correspond to ones among all domains. As
discussed in Section 2, some sufficient conditions under a simple linear SEM setting have been found (Peters
17Published in Transactions on Machine Learning Research (01/2024)
et al., 2016; Arjovsky et al., 2019), but general theoretical results have not yet been established. This is also
among our unsolved problems, and should be provided in further work.
Finally, extending our results to general domain sets beyond the case by Rojas-Carulla et al. (2018) is an
important topic for future work. Invariant Risk Minimization (IRM) estimates the feature map Î¦that
has the same conditional distribution PYe|Î¦(Xe)among all domains eâˆˆE; in other words, IRM framework
assumes that a domain set Ehas a feature map Î¦such thatPYe|Î¦(Xe)are equal among all domains. Among
domain sets that satisfy the property, the domain set by Rojas-Carulla et al. (2018) is the simplest one; the
projection Î¦X1induces the same conditional independence PYe|Î¦X1(Xe). In some cases, a map that induces
the sameconditional distribution is amore complex function than theprojection Î¦X1, so the relationbetween
(3) and the o.o.d. risk on such general domains beyond the case by Rojas-Carulla et al. (2018) is should be
investigated.
Acknowledgements
WethankDr.YanointheInstituteofStatisticalMathematicsforvaluablediscussions. Theresearchwassup-
ported by Grant-in-Aid for JSPS Fellows 20J21396, Grant-in-Aid for Research Activity Start-up 23K19966,
JST CREST JPMJCR2015, and JSPS Grant-in-Aid for Transformative Research Areas (A) 22H05106.
References
Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant Risk Minimiza-
tion Games. In Proceedings of the 37th International Conference on Machine Learning , pp. 145â€“155,
2020.
Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis
Mitliagkas, and Irina Rish. Invariance Principle Meets Information Bottleneck for Out-of-Distribution
Generalization. In Advances in Neural Information Processing Systems , 2021a.
Kartik Ahuja, Karthikeyan Shanmugam, and Amit Dhurandhar. Linear Regression Games: Convergence
Guarantees to Approximate Out-of-Distribution Solutions. In Proceedings of The 24th International Con-
ference on Artificial Intelligence and Statistics , pp. 1270â€“1278, 2021b.
Ehab A. AlBadawy, Ashirbani Saha, and Maciej A. Mazurowski. Deep learning for segmentation of brain
tumors: Impact of cross-institutional training and testing. Medical Physics , 45(3):1150â€“1158, 2018.
Martin Arjovsky, LÃ©on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization.
arXiv:1907.02893 , 2019.
Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory , 39(3):930â€“945, 1993.
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in Terra Incognita. In Computer Vision â€“
ECCV 2018 , volume 11220, pp. 472â€“489. 2018.
Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant Rationalization. In Proceedings of the
37th International Conference on Machine Learning , pp. 1448â€“1458, 2020.
Yongqiang Chen, Kaiwen Zhou, Yatao Bian, Binghui Xie, Bingzhe Wu, Yonggang Zhang, Ma Kaili, Han
Yang,PeilinZhao,BoHan,andJamesCheng. ParetoInvariantRiskMinimization: TowardsMitigatingthe
Optimization Dilemma in Out-of-Distribution Generalization. In The Eleventh International Conference
on Learning Representations , 2023.
Andreas Christmann and Ingo Steinwart. Support Vector Machines . Information Science and Statistics.
Springer, 2008.
George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals
and Systems , 2(4):303â€“314, 1989.
18Published in Transactions on Machine Learning Research (01/2024)
Will Douglas Heaven. Googleâ€™s medical AI was super accurate in a lab. Real life was a different
story. https://www.technologyreview.com/2020/04/27/1000658/google-medical-ai-accurate-lab-real-life-
clinic-covid-diabetes-retina-disease/, 2020.
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal
approximators. Neural Networks , 2(5):359â€“366, 1989.
Dongsung Huh and Avinash Baidya. The Missing Invariance Principle found â€“ the Reciprocal Twin of
Invariant Risk Minimization. In Advances in Neural Information Processing Systems , 2022.
PritishKamath, AkileshTangella, DanicaSutherland, andNathanSrebro. DoesInvariantRiskMinimization
Capture Invariance? In Proceedings of The 24th International Conference on Artificial Intelligence and
Statistics , pp. 4069â€“4077, 2021.
Masanori Koyama and Shoichiro Yamaguchi. When is invariance useful in an Out-of-Distribution General-
ization problem ? arXiv:2008.01883 , 2021.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang,
Remi Le Priol, and Aaron Courville. Out-of-Distribution Generalization via Risk Extrapolation (REx).
InProceedings of the 38th International Conference on Machine Learning , pp. 5815â€“5826, 2021.
Yong Lin, Hanze Dong, Hao Wang, and Tong Zhang. Bayesian Invariant Risk Minimization. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pp. 16000â€“16009, 2022a.
Yong Lin, Shengyu Zhu, Lu Tan, and Peng Cui. Zin: When and how to learn invariance without environment
partition? In Advances in Neural Information Processing Systems , 2022b.
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Heterogeneous Risk Minimization. In
Proceedings of the 38th International Conference on Machine Learning , pp. 6804â€“6814, 2021a.
Jiashuo Liu, Zheyuan Hu, Peng Cui, Bo Li, and Zheyan Shen. Kernelized Heterogeneous Risk Minimization,
2021b.
Chaochao Lu, Yuhuai Wu, JosÃ© Miguel HernÃ¡ndez-Lobato, and Bernhard SchÃ¶lkopf. Invariant Causal Rep-
resentation Learning for Out-of-Distribution Generalization. In International Conference on Learning
Representations , 2022.
Hrushikesh Mhaskar. Neural Networks for Optimal Approximation of Smooth and Analytic Functions.
Neural Computation , 8(1):164â€“177, 1996.
Giambattista Parascandolo, Alexander Neitz, Antonio Orvieto, Luigi Gresele, and Bernhard SchÃ¶lkopf.
Learning explanations that are hard to vary. In International Conference on Learning Representations ,
2022.
Christian S. Perone, Pedro Ballester, Rodrigo C. Barros, and Julien Cohen-Adad. Unsupervised domain
adaptation for medical imaging segmentation with self-ensembling. NeuroImage , 194:1â€“11, 2019.
Jonas Peters, Peter BÃ¼hlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction:
Identification and confidence intervals. Journal of the Royal Statistical Society: Series B (Statistical
Methodology) , 78(5):947â€“1012, 2016.
Roman Pogodin, Namrata Deka, Yazhe Li, Danica J. Sutherland, Victor Veitch, and Arthur Gretton. Ef-
ficient Conditionally Invariant Representation Learning. In The Eleventh International Conference on
Learning Representations , 2023.
Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant Gradient Variances for Out-of-
Distribution Generalization. In Proceedings of the 39th International Conference on Machine Learning ,
pp. 18347â€“18377, 2022.
19Published in Transactions on Machine Learning Research (01/2024)
Mateo Rojas-Carulla, Bernhard SchÃ¶lkopf, Richard Turner, and Jonas Peters. Invariant Models for Causal
Transfer Learning. Journal of Machine Learning Research , 19(36):1â€“34, 2018.
Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The Risks of Invariant Risk Minimization.
InInternational Conference on Learning Representations , 2021.
Janelle Shane. Do neural nets dream of electric sheep? - AI WeirdnessCommentShareCommentShare.
https://www.aiweirdness.com/do-neural-nets-dream-of-electric-18-03-02/, 2018.
Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approx-
imator.Applied and Computational Harmonic Analysis , 43(2):233â€“268, 2017.
Xiaoyu Tan, LIN Yong, Shengyu Zhu, Chao Qu, Xihe Qiu, Xu Yinghui, Peng Cui, and Yuan Qi. Provably
invariant learning without domain information. In Proceedings of the 40th International Conference on
Machine Learning , 2023.
Shoji Toyota and Kenji Fukumizu. Invariance Learning based on Label Hierarchy. In Advances in Neural
Information Processing Systems , 2022.
Xiao Zhou, Yong Lin, Weizhong Zhang, and Tong Zhang. Sparse Invariant Risk Minimization. In Proceedings
of the 39th International Conference on Machine Learning , pp. 27222â€“27244, 2022.
20