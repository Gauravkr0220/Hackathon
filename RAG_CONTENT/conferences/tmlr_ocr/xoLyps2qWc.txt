Published in Transactions on Machine Learning Research (07/2023)
Linearized Relative Positional Encoding
2Zhen Qin2,3Weixuan Sun2Kaiyue Lu4Hui Deng3Dongxu Li2Xiaodong Han
4Yuchao Dai5Lingpeng Kong1,2Yiran Zhongâˆ—
1Shanghai AI Laboratory2OpenNLPLab3Australian National University
4Northwestern Polytechnical University5The University of Hong Kong
Reviewed on OpenReview: https: // openreview. net/ forum? id= xoLyps2qWc
Abstract
Relative positional encoding is widely used in vanilla and linear transformers to repre-
sent positional information. However, existing encoding methods of a vanilla transformer
are not always directly applicable to a linear transformer, because the latter requires a
decomposition of the query and key representations into separate kernel functions. Never-
theless, principles for designing encoding methods suitable for linear transformers remain
understudied. In this work, we put together a variety of existing linear relative positional
encoding approaches under a canonical form and further propose a family of linear rela-
tive positional encoding algorithms via unitary transformation . Our formulation leads to a
principled framework that can be used to develop new relative positional encoding methods
that preserve linear space-time complexity. Equipped with different models, the proposed
linearized relative positional encoding ( LRPE) family derives effective encoding for vari-
ous applications. Experiments show that compared with existing methods, LRPEachieves
state-of-the-art performance in language modeling, text classification, and image classifi-
cation. Meanwhile, it emphasizes a general paradigm for designing broadly more relative
positional encoding methods that are applicable to linear transformers.
1 Introduction
Transformers have achieved remarkable progress in natural language processing (Devlin et al., 2019; Radford
et al., 2019; Brown et al., 2020), computer vision (Dosovitskiy et al., 2020; Liu et al., 2021; Arnab et al., 2021)
and audio processing (Karita et al., 2019; Zhang et al., 2020; Gulati et al., 2020; Sun et al., 2022). As an
important ingredient in transformers, positional encoding assigns a unique representation for each position of
a token in a sequence so that the transformers can break the permutation invariance property. Among these
encoding methods, absolute positional encoding (Vaswani et al., 2017; Sukhbaatar et al., 2015; Devlin et al.,
2019; Liu et al., 2020) maps each individual position index into a continuous encoding. Whereas relative
positional encoding (Shaw et al., 2018; Su et al., 2021; Horn et al., 2021; Liutkus et al., 2021; Huang et al.,
2020; Raffel et al., 2019) generates encoding for each query-key pair, representing their relative positional
offset. We focus on relative positional encoding as they are not constrained by input lengths (Chen, 2021)
while showing superior performance (Shaw et al., 2018).
Linear transformers (Chen, 2021; Qin et al., 2022b;a; Liu et al., 2022; Lu et al., 2022) attract more attention
recently as they can achieve linear space-time complexity with respect to input sequence length, while main-
taining comparable performance with vanilla transformers. Most existing linear transformers use absolute
positional encoding methods to encode positional information, since most existing relative positional encod-
ing methods are designed for vanilla transformers and are not directly applicable to linear transformers. The
main cause behind this limitation is that linear transformers decompose key and value representations in
the self-attention modules into separate kernel functions to achieve linear space-time complexity. Such an
additional requirement on the decomposibility is not always satisfied by existing relative positional encoding
âˆ—Indicates the corresponding author. Email: zhongyiran@gmail.com
1Published in Transactions on Machine Learning Research (07/2023)
Softmax
d
nÃ—
Q V KTğ‘¤ğ‘¡âˆ’ğ‘ d
n Ã— Ã—
Qğ‘¤ğ‘¡Ã— Ã—
ğ‘¤ğ‘ VÃ— Ã—d * d d * dd* d
O(n2d + n2d)â‰ˆO(n2)                                                       O(nd2+ nd2)â‰ˆO(n)KT
Figure 1: Illustration of existing relative positional encoding (left) and the proposed LRPE(right). Q,K,
andVare all in the shape of nbyd, wherenis input length and dis feature dimension. Tensors in the
same dashed line box are associated for computation. In the vanilla relative positional encoding, query key
attention has to be calculated first, leading to a quadratic complexity. Wtâˆ’srefers to relative positional
encoding, where t,sare two positional indices on the query and key, respectively. Our LRPEachieves a
decomposable encoding, i.e.,WtandWsare only dependent on positions of the query and key, making
it fully compatible with linear transformers. When dealing with long sequences, dâ‰ªn, the computation
complexity is dominated by n, rendering dnegligible.
methods. On the other hand, despite some individual works (Qin et al., 2022b; Chen, 2021), general princi-
ples for designing relative positional encoding for linear transformers remain largely understudied. A recent
work, RoPE Su et al. (2021) proposes a new set of multiplicative encoding solutions based on rotational
positional encoding and can be applied to linear transformers. In Appendix D.1, we show that RoPE can
be seen as a special form of LRPE.
In this work, we aim to bridge this gap and study the principal framework to develop relative positional
encoding applicable for linear transformers. To this end, we start by presenting a canonical form of relative
positional encoding, which reveals that differences in existing encoding methods boil down to choices of a set
of query, key and relative positional matrix primitives . By properly selecting and composing these primitives,
we could derive various existing encoding methods for transformers.
Taking advantage of the canonical form, we introduce the main contribution of our work, i.e., a special family
of relative positional encoding methods called linearized relative positional encoding (LRPE). Specifically, we
supply a sufficient condition for designing compatible encoding methods, especially for linear transformers,
and prove that the linearized relative positional encoding is a unitary transformation. The benefits of using
unitary transformation are twofold. On one side, since it is derived from the decomposable positional matrix,
it can maintain the linear space-time complexity as shown in Fig. 1. Second, the unitary transformation
property allows us to effectively derive the family of closed-form solutions. In particular, we show that a
number of encoding methods pertain to the LRPEfamily, including those used in RoPE (Su et al., 2021)
and PermuteFormer (Chen, 2021).
Furthermore, LRPEsheds light on a simple yet flexible theoretical paradigm to develop new effective rela-
tive positional encoding. To demonstrate this, we derive non-exhaustively three additional LRPEencoding
methods by parameterizing the generic solution differently, including solutions living in either real or com-
plex domains. Since unitary transformations are special cases of a relative positional matrix, LRPEis
applicable to linear transformers and exclusively suitable within encoder and/or decoder layers. We exper-
imentally demonstrate the effectiveness of the LRPEfamily on autoregressive and bidirectional language
modeling, text classification, and image classification. Results show that LRPEachieves superior capability
in representing relative positional information, commonly resulting in unrivalled performance than previous
encoding methods.
In summary, our main contributions are as follow:
â€¢We present a canonical form of relative positional encoding, which derives most existing relative
positional encoding methods as its special case, including those used in linear transformers.
2Published in Transactions on Machine Learning Research (07/2023)
â€¢Based on the canonical form, we propose linearized relative position encoding ( LRPE), a simple yet
principal formulation to derive an encoding familythat respects the linear space-time complexity
in linear transformers. We show several existing relative positional encoding methods in linear
transformers are in LRPEfamily. We also provide additional solutions from this generic form.
â€¢Experiments on various downstream tasks, such as language modeling, text classification, and image
classification show that the LRPEfamily is more robustand consistently produces better results
across tasks than previous relative encoding methods, are flexiblein being plugged into encoder
and/or decoder layers in linear models. In addition, it is genericto derive existing and potentially
new encoding methods.
2 Background and Preliminary
In this section, we provide preliminary knowledge and describe related work to facilitate the rest discussions.
In the following, we denote the k-th row of matrix MasmT
k, thed-dimensional identity matrix as Id. We
omit the subscript dwhen it is unambiguous from the context. The complete list of notations can be found
in Appendix A.
2.1 Transformer and its linearization
We first briefly review vanilla transformer (Vaswani et al., 2017) and its linearization (Katharopoulos et al.,
2020). The key component of transformer models is the self-attention block, which involves three matrices
Q(Query),K(Key) and V(Value); each of them is a linear projection taking XâˆˆRnÃ—das input:
Q=XWQ,K=XWK,V=XWVâˆˆRnÃ—d. (1)
The output OâˆˆRnÃ—dis computed using the Softmax weighted sum:
O= Softmax( QKT/âˆš
d)V. (2)
The computation overhead of the vanilla transformer grows quadratically with respect to the sequence length
n, which becomes the bottleneck for transformers to handle long input sequences. Linearization of self-
attention aims to reduce the computation complexity to linear (Katharopoulos et al., 2020; Ke et al., 2021;
Qin et al., 2022b; Vyas et al., 2020; Peng et al., 2021; Xiong et al., 2021; Sun et al., 5555), typically achieved
via a decomposable kernel function Ï•:Rdâ†’RÂ¯d. Specifically, the output of linear attention is computed as:
O=âˆ†âˆ’1Ï•(Q)[Ï•(K)TV],
âˆ†= diag(Ï•(Q)[Ï•(K)T1n]).(3)
The key property of linear attention is the decomposability of the kernel function. This enables to compute
Ï•(K)TVâˆˆRdÃ—dfirst, which leads to the O(nd2)complexity, further reducing to O(n)with longer inputs
(dâ‰ªn). See Appendix B for a detailed discussion.
2.2 Positional encoding
Self-attention is capable of parallel sequence processing but cannot capture positional information of each
token. To address this issue, positional encoding methods are proposed, which can be generally categorized
into two groups: absolute positional encoding and relative positional encoding.
Absolute positional encoding employs handcraft functions (Vaswani et al., 2017; Sukhbaatar et al., 2015)
or learnable encoding lookup tables PâˆˆRnÃ—d(Devlin et al., 2019; Liu et al., 2020) to represent position
indices as encodings. These encodings are then combined with the context vector additively:
qs=WQ(xs+ps),ks=WK(xs+ps),
vs=WV(xs+ps),(4)
where the encoding formulation only depends on the absolute position index s, and the positional encoding
size is restricted by the input sequence length.
3Published in Transactions on Machine Learning Research (07/2023)
Relative positional encoding considers relative position offsets between two input tokens (Shaw et al.,
2018; Qin et al., 2023), i.e.,
est=xT
sWT
QWKxt+f(xs,xt,tâˆ’s), (5)
wheres,tare the two positional indeices, estdenotes the attention score before softmax. Compared to
absolute positional encoding, relative positional encoding generally achieves better performance as it can
handle variable input length (Chen, 2021). However, extra cost on computation and memory makes it not
so efficient than absolute positional encoding (Likhomanenko et al., 2021).
Most existing relative positional encoding methods (Raffel et al., 2019; Shaw et al., 2018; Huang et al.,
2020; Chi et al., 2022) require computing query-key attention QKTand combine with relative positional
information, whichincursquadraticcomplexity. Incontrast, linearattentionavoidssuchaquery-keyproduct
to achieve the linear complexity. Therefore, common relative positional encoding methods are usually not
applicable in linear transformers.
3 Our Method
In this section, we present our main technical contribution on linearized relative positional encoding, which
is an encoding family that preserves linear space-time complexity. Specifically, we start by presenting a
canonical form of relative positional encoding and show that existing encoding methods can be derived by
instantiating the canonical form with different choices of so-called primitive queries, keys, and positional
matrices in Section 3.1. When imposing the decomposability constraint on this canonical form, we obtain
a sufficient condition for linearized relative positional encoding (LRPE) and derive a family of concrete
solutions in real and complex domains in Section 3.2. We provide an implementation sketch in Section 3.3.
3.1 Canonical form of relative positional encoding
Inordertobetterestablishconnectionsbetweenexistingrelativepositionalencodingmethodsandunderstand
their design principles, we first present a canonical form of relative positional encoding in this section. In
particular, given a query qsand key kspair, their relative positional encoding frel:CdÃ—Cdâ†’Ccan be
represented as:
frel(qs,kt) =m/summationdisplay
l=1(Ë† q(l)
s)HW(l)
tâˆ’sË†k(l)
t, (6)
where Hrepresents conjugate transposition andmrepresents number of primitives. We refer Ë† q(l)
sâˆˆ
Cd(l)
1,Ë†k(l)
tâˆˆCd(l)
2,W(l)
tâˆ’sâˆˆCd(l)
1Ã—d(l)
2as query, key and relative positional matrix primitives , respectively,
used as constituent components to construct the relative positional encoding. Note that query primitives do
not always indicate a reliance on query embeddings, similarly for other primitives. For example, an identify
matrix can also serve as a primitive, as we will show shortly in Section 3.1.1.
To demonstrate Eq. 6 is a generic formulation, we show that it flexibly induces a wide range of existing
relative encoding methods (Shaw et al., 2018; Su et al., 2021; Horn et al., 2021; Liutkus et al., 2021; Huang
et al., 2020; Raffel et al., 2019) by selecting and compositing different choices of primitives. Among them,
we highlight four examples in the following section and leave the complete discussions in the Appendix C.1.
3.1.1 Typical encoding examples
Additive. In (Huang et al., 2020), the relative positional encoding is formulated as an extra additive term
to the query-key inner-product:
frel(qs,kt) =qH
skt+wtâˆ’s, (7)
which can be derived by including an extra identity term as a primitive, formally denoted as:
m= 2,
Ë† q(1)
s=qs,Ë†k(1)
t=kt,W(1)
tâˆ’s=Id,
Ë† q(2)
s=1d,Ë†k(2)
t=1d,W(2)
tâˆ’s= (wtâˆ’s/d)Id.(8)
4Published in Transactions on Machine Learning Research (07/2023)
Multiplicative. In RoPE (Su et al., 2021), the relative positional encoding works in the form of the
weighted inner product:
frel(qs,kt) =qH
sWtâˆ’skt, (9)
which can be denoted as:
m= 1,
Ë† q(1)
s=qs,Ë†k(1)
t=kt,W(1)
tâˆ’s=Wtâˆ’s.(10)
3.1.2 Simplification
For the ease of the remaining discussion, we introduce the necessary notations and simplify Eq. 6.
Ë†d1=m/summationdisplay
l=1d(l)
1,Ë†d2=m/summationdisplay
l=1d(l)
2,
Ë†qs=/bracketleftï£¬ig
(Ë†q(1)
s)T,..., (Ë†q(m)
s)T/bracketrightï£¬igT
âˆˆCË†d1,Ë†kt=/bracketleftï£¬ig
(Ë†k(1)
t)T,..., (Ë†k(m)
t)T/bracketrightï£¬igT
âˆˆCË†d2,
Ë†Wtâˆ’s=block-diag{W(1)
tâˆ’s...,W(m)
tâˆ’s}âˆˆCË†d1Ã—Ë†d2.(11)
With these notations, we can rewrite Eq. 6 into the matrix form: frel(qs,kt) =Ë†qH
sË†Wtâˆ’sË†kt. Since every
component of Ë†qsand Ë†ktare handled with no difference, without losing generality, we only discuss cases
wherem= 1:
frel(qs,kt) =qH
sWtâˆ’skt. (12)
3.2 Linearized relative position encoding
Eq. 6 is a canonical form of relative positional encoding, meaning that its variants are applicable to vanilla
transformers but not necessarily for linear ones. To design relative encoding compatible with linear trans-
formers, the attention computation has to respect the decomposibilty condition. This additional condition
leads to the linearized relative position encoding ( LRPE) family, defined as follows.
Definition 3.1. A relative position encoding is called linearized relative position encoding ( LRPE), when
the following holds:
âˆ€qs,ktâˆˆCd,frel(qs,kt) =qH
sWtâˆ’skt
=(Msqs)H(Mtkt) =qH
sMH
sMtkt,(13)
where qs,ktâˆˆCd,Ws,MsâˆˆCdÃ—d,W0=Id.
The assumption of W0=Idimplies that the interaction between tokens from the same position only
depends on the content, which is reasonable enough that most encoding methods respect. In its essence,
Eq. 13 ensures the positional matrix is decomposable. In this way, the query-key inner-product can be
avoided in the attention computation. Consequently, complexity of computing LRPEisO(nd2), wherenis
sequence length, dis embedding dimension as Appendix C.2 shows in detail.
We prove that Eq. 13 can be simplified based on the following proposition:
Proposition 3.2. Eq. 13 is equivalent to Eq. 14 and Wtis Unitary matrix,
Wtâˆ’s=WH
sWt. (14)
Proof of Proposition 3.2. According to the arbitrariness of qs,kt, Eq. 13 is equivalent to
Wtâˆ’s=MH
sMt. (15)
Takes=tin Eq 13, we get (since we assume that W0=Id):
MH
sMs=W0=Id. (16)
5Published in Transactions on Machine Learning Research (07/2023)
Thus, Msis a unitary matrix. On the other hand, note that for any unitary matrix P, we always have
Wtâˆ’s=MH
sMt=MH
sIdMt
=MH
sPHPMt= (PMs)H(PMt).(17)
This means that left multiplying Mtby a unitary matrix Pdoes not change Eq. 13. Since MsandMH
0are
also unitary matrices, we can perform the following transformation:
Ms=MH
0Ms. (18)
With Ms, Eq. 15 becomes
Wtâˆ’s=MH
sMt. (19)
Takes= 0, we have
Wt=MH
0Mt=MH
0M0Mt=IdMt=Mt. (20)
Thus Eq. 19 becomes
Wtâˆ’s=WH
sWt. (21)
Since Msis a unitary matrix, Wsis also a unitary matrix, i.e.,
WH
sWs=Id.
In the following section, we derive some particular solutions of Eq. 14.
3.2.1 Particular solutions
In this section, we discuss Eq. 14 and give a family of solutions. It is worth noting that the solutions we
provide are all in the form of Ws=PHÎ›(s)P, where P,Î›(s)are unitary matrices. The complete derivation
can be found in Appendix C.4, C.5, C.6.
Unitary(Solution1) Thefirstcaseisdiscussedinthecomplexdomain,whichisnotcommonintransformer
models yet exhibiting an elegant solution.
Proposition 3.3. The following form of WsâˆˆCdÃ—dsatisfies Eq. 14:
Ws=PHÎ›(s)P,
Î›(s)= diag{exp(isÎ±1),..., exp(isÎ±d)},(22)
where PâˆˆCdÃ—disunitary matrix,Î±k,k= 1,...,dare parameters.
Orthogonal (Solution 2) Now we consider the real domain, a more general case in transformers.
Proposition 3.4. The following form of WsâˆˆRdÃ—dsatisfies Eq. 14:
Ws=PTÎ›(s)P,Î›(s)=block-diag{A(s),B(s)},
A(s)=block-diag{A(s)
1,...,A(s)
n}âˆˆR2pÃ—2p,B(s)=IqâˆˆRqÃ—q,
A(s)
k=/bracketleftbiggcos(sÎ±k)âˆ’sin(sÎ±k)
sin(sÎ±k) cos(sÎ±k)/bracketrightbigg
,(23)
where PâˆˆRdÃ—disorthogonal matrix,Î±k,k= 1,...,dare parameters.
Permutation (Solution 3) The last case is inspired by PermuteFormer (Chen, 2021), which is associated
with the permutation matrix:
Proposition 3.5. The following form of WkâˆˆRdÃ—dsatisfies Eq. 14:
Wk=PTÎ›(k)P,
Ï€:{1,2,Â·Â·Â·,d}â†’{ 1,2,Â·Â·Â·,d}is permutation ,
Î›(k)= (I)Ï€k,(24)
where PâˆˆRdÃ—dis the orthogonal matrix.
6Published in Transactions on Machine Learning Research (07/2023)
Table 1: Quantitative results of the Roberta model fine-tuned on the GLUE dataset. MNLI is reported by
the match/mismatch splits. CoLA is reported by Matthews correlation coefficient. All the other tasks are
measured by accuracy. The best result is highlighted with bold.â†“meanssmaller is better .The experimental
results are the average of five trials, and Â±âˆ†represents the standard deviation.
Method Loss â†“MNLIâ†‘ QNLIâ†‘ QQPâ†‘ RTEâ†‘ SST-2â†‘ MRPCâ†‘ CoLAâ†‘ STS-Bâ†‘
Base 5.35 76.39Â±0.64/76.41Â±0.85 85 .25Â±0.95 88.25Â±0.94 52.99Â±0.85 89.91Â±1.09 70.16Â±0.66âˆ’ 47.94Â±0.47
RoPE 5.17 76.97Â±0.64/76.66Â±1.07 83 .07Â±0.91 83.38Â±1.28 55.98Â±0.95 90.65Â±1.28 70.10Â±0.66 39.23Â±0.47 49.58Â±0.45
SPE 6.07 68.03Â±0.38/69.08Â±0.73 73 .75Â±0.73 87.82Â±1.32 53.32Â±0.37 84.67Â±0.74 70.24Â±0.47âˆ’ 17.89Â±0.14
PER 5.32 77.26Â±0.88/76.95Â±1.02 83 .31Â±0.97 88.14Â±0.91 55.81Â±0.66 90.12Â±0.65 71.45Â±0.74 28.83Â±0.42 68.09Â±0.88
Type1 5.18 79.16Â±0.90/78.34Â±0.86 87.91Â±0.79 89.45Â±1.12 55.85Â±0.55 90.56Â±1.25 72.90Â±0.87 48.36Â±0.71 81.92Â±1.15
Type2 5.12 80.30Â±0.99/80.88Â±1.16 87.37Â±0.87 89.67Â±0.87 59.33Â±0.51 91.90Â±1.07 73.49Â±0.77 49.61Â±0.25 79.01Â±0.89
Type3 5.28 76.70Â±0.81/77.52Â±1.11 85 .87Â±0.96 89.00Â±0.58 58.39Â±0.99 90.49Â±0.98 71.27Â±0.96 36.71Â±0.37 75.44Â±0.83
3.3 The LRPE family
LRPE(Ws=PHÎ›(s)P) contains two components, i.e., a fixed unitary matrix Pand a unitary matrix
family Î›(s)as mentioned in Proposition 3.3, 3.4, and 3.5. The Pcan be seen as a rotation matrix that
rotates the token feature to a particular coordinate system and the Î›(s)derives the positional information
from the rotated feature.
To meet all the requirements in Proposition 3.3, 3.4, and 3.5, Pneeds to be an orthogonal matrix. We
empirically find that when Pis a householder matrix (Golub & Van Loan, 2013), the overall performance is
better than other options such as permutation matrix and Identity matrix. We provide a detailed ablation
in Table 5. For ease of expression, we use Type 1for the unitary solution, Type 2for the orthogonal solution,
andType 3for the permutation solution. Details can be found in Appendix D.1.
4 Experiments
In this section, we validate the effectiveness of the proposed LRPEon natural language processing tasks
and computer vision tasks that resort to different Transformer architectures. Specifically, we first study the
autoregressive language model (Radford et al., 2018). This is followed by the bidirectional language model,
which adopts the Roberta architecture (Liu et al., 2020) and is pretrained and then fine-tuned on several
downstream tasks from the GLUE benchmark (Wang et al., 2018). We also extend our evaluation on image
classification task to verify the generalization ability of LRPE.
4.1 Experimental settings
Dataset We use Wikitext-103 Merity et al. (2016), Books Zhu et al. (2015), and WikiBook Wettig et al.
(2022) datasets for NLP task evaluation and ImageNet-1k Deng et al. (2009) for image classification evalu-
ation. Wikitext-103 is a small dataset containing a preprocessed version of the Wikipedia dataset. Books
consists of a large number of novels, making it suitable for long sequence modeling evaluation. WikiBook is
a large corpus (22 GB) of Wikipedia articles and books collected by Wettig et al. (2022). ImageNet-1k is the
most popular large image classification dataset. It contains 1000 object classes and over 1 million training
images and is often used to verify the performance of models in image modeling.
Configurations Our experiments are implemented in the Fairseqframework (Ott et al., 2019) and trained
with V100 GPUs. All the methods share the same configurations such as learning rate, batch size, and
optimizer. The detailed configurations are listed in Appendix E.
Competing methods Our baseline (marked as Base) is a Linear Transformer with 1+elu(Â·)(Katharopou-
los et al., 2020) as the kernel function with sinusoidal positional encoding (Vaswani et al., 2017). For com-
parison, we choose several state-of-the-art methods, i.e., RoPE (Su et al., 2021), SPE (Liutkus et al., 2021),
PermuteFormer (abbreviated as â€œPERâ€) (Chen, 2021). We also choose the method without positional
encoding as a competitor (abbreviated as â€™NoPEâ€™).
7Published in Transactions on Machine Learning Research (07/2023)
30000 35000 40000 45000 50000
Step31323334353637PPL
Base
RoPE
PER
LRPE
5000 7500 10000 12500 15000 17500 20000 22500
Step12141618202224262830PPL
Base
RoPE
PER
LRPE
Figure 2: The validation result of Autoregressive language model on the Wikitext-103 dataset(left) and Roberta
on the Wikibook dataset(right). In both cases, the best result of the proposed LRPE has a better PPL and faster
convergence speed than competing methods.
Table 2: Quantitative results of the autoregressive language model on the WikiText-103 and Books dataset.
The best result is highlighted with bold.â†“meanssmaller is better . The experimental results are the average
of five trials, and Â±âˆ†represents the standard deviation.
MethodWikitext-103 Books
Valâ†“ Testâ†“ Valâ†“ Testâ†“
Base 33.97Â±0.19 33 .67Â±0.16 9 .13Â±0.05 8 .79Â±0.02
NoPE 35.96Â±0.24 35 .38Â±0.18 9 .61Â±0.08 9 .18Â±0.04
RoPE 33.36Â±0.20 33 .15Â±0.22 9 .00Â±0.06 8 .65Â±0.04
SPE 43.36Â±0.19 41 .76Â±0.28 11 .85Â±0.09 10 .85Â±0.06
PER 32.88Â±0.09 32 .51Â±0.25 8 .53Â±0.04 8 .21Â±0.07
Type1 31.83Â±0.08 31 .60Â±0.19 8.50Â±0.03 8 .18Â±0.06
Type2 32.02Â±0.15 31 .74Â±0.24 8.48Â±0.03 8 .14Â±0.05
Type3 33.81Â±0.21 33 .85Â±0.18 8 .66Â±0.05 8 .43Â±0.05
4.2 Results
Autoregressive language model. The autoregressive language model has 6 decoder layers and is trained
on the WikiText-103 dataset (Merity et al., 2017). In order to test the performance of the method on long se-
quence modeling, we tested the performance of the model on the Books (Zhu et al., 2015) dataset. We use the
Perplexity (PPL) as the evaluation metric and report the results in Table 2. We observe that all variants of
LRPEpresentaperformancegainoverthebaseline. Notably,Type1andType2modelsachievethebestper-
formance on Wikitext-103 and Books, respectively, demonstrating superior capability in language modeling.
Table 3: Quantitative results of image classification
on the ImageNet-1k dataset. The best result is high-
lighted with bold.â†‘meanslarger is better . The ex-
perimental results are the average of five trials, and
Â±âˆ†represents the standard deviation.
Method Acc â†‘ Params
Base 78.04Â±0.15 22.04
RoPE 78.64Â±0.28 22.04
PER 77.81Â±0.23 22.04
Type1 78.60Â±0.28 22.05
Type2 78.77Â±0.21 22.05
Type3 77.72Â±0.18 22.05Bidirectional language model. The bidirec-
tional model follows an encoder-only structure, i.e.,
Roberta(Liuetal.,2020),with12layers. Inorderto
verify the performance of the model on a large data
set, we adopt the Wikibook dataset used by Wettig
et al. (2022) for pre-training and used their config-
urations to update 23k times. The results are in
Table 1 and Figure 4.1. In the pre-training phase,
LRPEoutperforms all competitors. Next, we fine-
tune the model for the GLUE task. As shown in
Table 1, our method outperforms competing meth-
ods on all tasks with a clear margin.
Image classification model. To verify the ro-
bustness and effectiveness of LRPEunder different modal tasks, we test our method on the computer vision
domain. Specifically, we conduct experiments on Imagenet-1k Deng et al. (2009) dataset using the Deit-small
architecture Touvron et al. (2021) on the image classification task. In particular, we replace the Attention
8Published in Transactions on Machine Learning Research (07/2023)
with Linear Attention Katharopoulos et al. (2020) and then adopt various relative positional encoding. As
shown in Table 3, LRPE beats all the competing methods.
Long-Range Arena. In order to validate the effectiveness of LRPE on long-sequence modeling tasks, we
conducted experiments on Long-Range Arena benchmark (Tay et al., 2020). As shown in Table 4, LRPE
has positive effects on almost all tasks.
Table 4: Quantitative results of classification tasks on the Long-Range Arena benchmark. The best result is
highlighted with bold.â†‘meanslarger is better . The experimental results are the average of five trials, and
Â±âˆ†represents the standard deviation.
Model Text â†‘ ListOpsâ†‘ Retrievalâ†‘ Pathfinderâ†‘ Imageâ†‘ AVGâ†‘
Base 65.10Â±0.37 39 .22Â±0.24 84.96Â±0.58 71.54Â±0.32 41 .15Â±0.17 60 .39
RoPE 66.08Â±0.28 40.74Â±0.28 78.64Â±0.52 72 .70Â±0.46 62 .60Â±0.43 64 .15
PER 65.52Â±0.43 39 .14Â±0.11 83 .33Â±0.45 69 .50Â±0.40 42 .05Â±0.27 59 .91
Type1 66.75Â±0.35 39.98Â±0.12 82 .31Â±0.67 74.20Â±0.40 63 .53Â±0.46 65 .35
Type2 66.43Â±0.62 39 .63Â±0.29 81 .83Â±0.34 73 .48Â±0.33 62 .39Â±0.34 64 .75
Type3 65.01Â±0.23 39 .54Â±0.27 83 .91Â±0.36 70 .85Â±0.32 45 .19Â±0.22 60 .90
4.3 Discussion
Table 5: Ablation results with different rotation matrix P
for language modeling on the WikiText-103 dataset.
PÎ›(s)
Unitary Orthogonal Permutation Avg.
Householder 31.90 31.95 33.90 32.58
Identity 32.04 31.86 34.53 32.80
Permutation 32.09 31.59 34.16 32.61An explanation of LRPE. According to
the discussion in Section. 3.3, The proposed
LRPErotates the token feature through P,
and encodes the positional information through
Î›(s). In Table 5, we ablate the effectiveness
of the Pmatrix on the autoregressive language
modeling task. Our approach with the House-
holder matrix achieves better results than the
one equipped with other metrics. It indicates that we can get better performance by carefully selecting the
projection of the positional encoding.
Table6: Trainingspeedofdifferentmethodsonthebidirec-
tional language model. The value standards for the speed
relative to the base method. â†‘meanslarger is faster .
Method Relative speed â†‘
Base 1.00
Rope 0.86
SPE 0.61
PER 0.94
Type1 0.82
Type2 0.82
Type3 0.89Complexity and efficiency. The imple-
mentation of the proposed LRPEdoes not af-
fect the computational complexity of the linear
transformer, i.e., preserving the linear complex-
ity asO(n). We also measure the training speed
of the bidirectional language model on the same
local machine and observe that the speed after
usingLRPEis only a bit slower than the base-
line on average. The detailed comparison of the
efficiency can be found in Table 6. In general,
LRPEdoes not incur significant computational
burden to the transformer, and can fulfill the practical needs by maintaining comparable efficiency.
5 Conclusion
In this paper, we standardize the form of relative positional encoding for linear attention. The unitary
transformation is employed as a special solution to the linearized relative positional encoding, and the
solutions as per various constraints constitute the unitary relative positional encoding ( LRPE) family. We
validate the effectiveness of LRPEthrough extensive experiments on both natural language processing and
computer vision tasks with different transformer architectures. It outperforms competing methods in all
tasks. In addition, it highlights a broad paradigm for formulating linear transformer-applicable positional
encoding techniques that are more generically relative.
9Published in Transactions on Machine Learning Research (07/2023)
References
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario LuÄiÄ‡, and Cordelia Schmid. Vivit: A
video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,
pp. 6836â€“6846, 2021.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:1877â€“1901, 2020.
Peng Chen. Permuteformer: Efficient relative position encoding for long sequences. arXiv preprint
arXiv:2109.02377 , 2021.
Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and Alexander Rudnicky. KERPLE: Kernelized rela-
tive positional embedding for length extrapolation. In Alice H. Oh, Alekh Agarwal, Danielle Bel-
grave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL
https://openreview.net/forum?id=hXzOqPlXDwm .
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248â€“255. Ieee,
2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidi-
rectional transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers) , pp. 4171â€“4186, Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423 .
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
Gene H Golub and Charles F Van Loan. Matrix computations . JHU press, 2013.
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,
Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-augmented Transformer for
Speech Recognition. In Proc. Interspeech 2020 , pp. 5036â€“5040, 2020. doi: 10.21437/Interspeech.2020-3015.
Max Horn, Kumar Shridhar, Elrich Groenewald, and Philipp FM Baumann. Translational equivariance in
kernelizable attention. arXiv preprint arXiv:2102.07680 , 2021.
Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint
arXiv:2202.10447 , 2022.
Zhiheng Huang, Davis Liang, Peng Xu, and Bing Xiang. Improve transformer models with better relative
position embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp.
3327â€“3335, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.
findings-emnlp.298. URL https://aclanthology.org/2020.findings-emnlp.298 .
Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe, Marc Delcroix, Atsunori Ogawa, and Tomo-
hiro Nakatani. Improving Transformer-Based End-to-End Speech Recognition with Connectionist Tempo-
ral Classification and Language Model Integration. In Proc. Interspeech 2019 , pp. 1408â€“1412, 2019. doi:
10.21437/Interspeech.2019-1938.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. Transformers are rnns: Fast
autoregressive transformers with linear attention. In International Conference on Machine Learning , pp.
5156â€“5165. PMLR, 2020.
GuolinKe, DiHe, andTie-YanLiu. Rethinkingpositionalencodinginlanguagepre-training. In International
Conference on Learning Representations , 2021. URL https://openreview.net/forum?id=09-528y2Fgf .
10Published in Transactions on Machine Learning Research (07/2023)
Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. Cape:
Encoding relative positions with continuous augmented positional embeddings. Advances in Neural Infor-
mation Processing Systems , 34:16079â€“16092, 2021.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke
Zettlemoyer, and Veselin Stoyanov. Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020.
URL https://openreview.net/forum?id=SyxS0T4tvS .
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pp. 10012â€“10022, 2021.
Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neural archi-
tecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955 , 2022.
Antoine Liutkus, OndÅ™ej CÃ­fka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan Yang, and Gael Richard. Relative
positional encoding for transformers with linear complexity. In International Conference on Machine
Learning , pp. 7067â€“7079. PMLR, 2021.
Kaiyue Lu, Zexiang Liu, Jianyuan Wang, Weixuan Sun, Zhen Qin, Dong Li, Xuyang Shen, Hui Deng,
Xiaodong Han, Yuchao Dai, et al. Linear video transformer with feature fixation. arXiv preprint
arXiv:2210.08164 , 2022.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models,
2016.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. 5th
International Conference on Learning Representations, ICLR, Toulon, France , 2017.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael
Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 , 2019.
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Ran-
dom feature attention. In International Conference on Learning Representations , 2021. URL https:
//openreview.net/forum?id=QtTKTdVrFBB .
Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The
devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing , pp. 7025â€“7041, Abu Dhabi, United Arab Emirates, December 2022a. Association for
Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.473 .
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong,
and Yiran Zhong. cosformer: Rethinking softmax in attention. In International Conference on Learning
Representations , 2022b. URL https://openreview.net/forum?id=Bl8CQrx2Up4 .
Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and
Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference
on Learning Representations , 2023. URL https://openreview.net/forum?id=IxmWsm4xrua .
AlecRadford, KarthikNarasimhan, TimSalimans, andIlyaSutskever. Improvinglanguageunderstandingby
generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf , 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
arXiv preprint arXiv:1910.10683 , 2019.
11Published in Transactions on Machine Learning Research (07/2023)
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.
InProceedings of the 2018 Conference of the North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, Volume 2 (Short Papers) , pp. 464â€“468, New Orleans,
Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. URL
https://aclanthology.org/N18-2074 .
Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary
position embedding. arXiv preprint arXiv:2104.09864 , 2021.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural
information processing systems , 28, 2015.
Jingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang Li, and Yiran Zhong. Locality matters: A locality-
biased linear attention for automatic speech recognition. arXiv preprint arXiv:2203.15609 , 2022.
W. Sun, Z. Qin, H. Deng, J. Wang, Y. Zhang, K. Zhang, N. Barnes, S. Birchfield, L. Kong, and Y. Zhong.
Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence , (01):1â€“14,
jun 5555. ISSN 1939-3539. doi: 10.1109/TPAMI.2023.3285569.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,
Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. In
International Conference on Learning Representations , 2020.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Je-
gou. Training data-efficient image transformers &amp; distillation through attention. In International
Conference on Machine Learning , volume 139, pp. 10347â€“10357, July 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Apoorv Vyas, Angelos Katharopoulos, and FranÃ§ois Fleuret. Fast transformers with clustered attention.
Advances in Neural Information Processing Systems , 33:21665â€“21674, 2020.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-
task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018
EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP , pp. 353â€“355,
2018.
Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in masked language
modeling?, 2022.
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas
Singh. NystrÃ¶mformer: A nystrÃ¶m-based algorithm for approximating self-attention. In AAAI, 2021.
Musheng Yao and Advanced Algebra. Advanced Algebra . Fudan University Press, 2015.
Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo, and Shankar Kumar.
Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t
loss. InICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 7829â€“7833. IEEE, 2020.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading
books. In The IEEE International Conference on Computer Vision (ICCV) , December 2015.
12Published in Transactions on Machine Learning Research (07/2023)
Appendix
A Mathematical Notations
Notation Meaning
X Hidden state.
Q,K,V Query, key, value.
WQ,WK,WV Weight matrices for Q,K,V.
O Attention output.
mT
s s-th row of matrix M(real domain).
mH
s s-th row of matrix M(complex domain).
Ï• Kernel function for linear attention.
1d All-ones vector with dimention d.
Id Identity matrix with dimention d.
block-diag Combining matrices into larger
block diagonal matrices as in Eq. 25
Table 7: Mathematical notations used in the paper.
block-diag{W1,W2,...,Wn}=ï£®
ï£¯ï£¯ï£¯ï£°W1
W2
...
Wnï£¹
ï£ºï£ºï£ºï£». (25)
B Computation of Vanilla/Linear Attention
B.1 Basic Notations
Both vanilla and linear attention blocks involve three matrices, i.e.,Q(Query),K(Key) and V(Value).
All of them are linear projections of input XâˆˆCnÃ—d,i.e.,
X=ï£®
ï£¯ï£°xT
1
...
xT
nï£¹
ï£ºï£»âˆˆRnÃ—d,
Q=ï£®
ï£¯ï£°qT
1
...
qT
nï£¹
ï£ºï£»=XWQ=ï£®
ï£¯ï£°xT
1WQ
...
xT
nWQï£¹
ï£ºï£»âˆˆRnÃ—d,
K=ï£®
ï£¯ï£°kT
1
...
kT
nï£¹
ï£ºï£»=XWK=ï£®
ï£¯ï£°xT
1WK
...
xT
nWKï£¹
ï£ºï£»âˆˆRnÃ—d,
V=ï£®
ï£¯ï£°vT
1
...
vT
nï£¹
ï£ºï£»=XWV=ï£®
ï£¯ï£°xT
1WV
...
xT
nWVï£¹
ï£ºï£»âˆˆRnÃ—d,(26)
where WQ,WK,WVâˆˆRdÃ—d.
The vector form is organized as
qs=WT
Qxs,ks=WT
Kxs,vs=WT
Vxs. (27)
13Published in Transactions on Machine Learning Research (07/2023)
The attention output is
O=ï£®
ï£¯ï£°oT
1
...
oT
nï£¹
ï£ºï£»âˆˆRnÃ—d. (28)
B.2 Vanilla Attention
In vanilla attention, the output is computed using the Softmax weighted sum, i.e.,
os=Attention (qs,K,V)
=n/summationdisplay
t=1astvt
=n/summationdisplay
t=1exp/parenleftï£¬ig
qT
skt/âˆš
d/parenrightï£¬ig
vt
/summationtextn
r=1exp/parenleftï£¬ig
qTskr/âˆš
d/parenrightï£¬ig,
O=Softmax (QKT/âˆš
d)V.(29)
B.3 Linear Attention
The linear attention is formulated as follows,
os=LinearAttention (qs,K,V)
=n/summationdisplay
t=1astvt
=n/summationdisplay
t=1Ï•(qs)TÏ•(kt)/summationtextn
t=1Ï•(qs)TÏ•(kt)vt
=/summationtextn
t=1Ï•(qs)TÏ•(kt)vt/summationtextn
t=1Ï•(qs)TÏ•(kt)
=Ï•(qs)T/summationtextn
t=1Ï•(kt)vt
Ï•(qs)T/summationtextn
t=1Ï•(kt),
O=âˆ†âˆ’1Ï•(Q)Ï•(K)TV
=âˆ†âˆ’1Ï•(Q)[Ï•(K)TV],
âˆ†= diag(Ï•(Q)[Ï•(K)T1n]).(30)
C Proof of Theorem
C.1 More Examples
In the following, we provide two additional examples of relative positional encoding with the canonical form.
DeBERTa (Huang et al., 2020):
frel(qs,kt) =qH
skt+qH
sÂ¯kg(sâˆ’t)+Â¯ qH
g(tâˆ’s)kt,
g(x) =ï£±
ï£´ï£²
ï£´ï£³0xâ‰¤âˆ’c
2câˆ’1xâ‰¥c
x+cothers.(31)
14Published in Transactions on Machine Learning Research (07/2023)
The canonical form is
m= 3,
Ë† q(1)
s=qs,Ë†k(1)
t=kt,W(1)
tâˆ’s=Id,
Ë† q(2)
s=qs,Ë†k(2)
t=Id,W(2)
tâˆ’s=1
d/bracketleftbigÂ¯kg(sâˆ’t)...Â¯kg(sâˆ’t)/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
dcolumns,
Ë† q(3)
s=Id,Ë†k(3)
t=kt,W(3)
tâˆ’s=1
d/bracketleftbigÂ¯ qg(tâˆ’s)...Â¯ qg(tâˆ’s)/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
dcolumns.(32)
RPR(Shaw et al., 2018):
frel(qs,kt) =qH
skt+qH
sctâˆ’s,
ctâˆ’s=wclip(tâˆ’s,k),
clip(x,k) = max(âˆ’k,min(k,x)),
wsâˆˆCd,âˆ’kâ‰¤sâ‰¤k.(33)
The canonical form is
m= 2,
Ë† q(1)
s=qs,Ë†k(1)
t=kt,W(1)
tâˆ’s=Id,
Ë† q(2)
s=qs,Ë†k(2)
t=Id,W(2)
tâˆ’s=1
d/bracketleftbigctâˆ’s...ctâˆ’s/bracketrightbig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
dcolumns.(34)
cosFormer (Qin et al., 2022b):
frel(qs,kt) =qH
sktcos(Î±(tâˆ’s)), (35)
which indicates that the relative positional encoding is effectively a coefficient term in the attention matrix,
as such, it can be derived via a positional matrix primitive with the coefficients.
m= 1,
Ë† q(1)
s=qs,Ë†k(1)
t=kt,W(1)
tâˆ’s= cos(Î±(tâˆ’s))Id.(36)
C.2 Speed analysis
Proof of Lrpe speed. For this, we only need to prove that the time complexity is linear with respect to n. To
this end, we first give basic notations as follows,
Q=ï£®
ï£¯ï£°qH
1
...
qH
nï£¹
ï£ºï£»âˆˆCnÃ—d,K=ï£®
ï£¯ï£°kH
1
...
kH
nï£¹
ï£ºï£»âˆˆCnÃ—d,V=ï£®
ï£¯ï£°vH
1
...
vH
nï£¹
ï£ºï£»âˆˆCnÃ—d,
ËœQ=ï£®
ï£¯ï£°(M1q1)H
...
(Mnqn)Hï£¹
ï£ºï£»âˆˆCnÃ—d,ËœK=ï£®
ï£¯ï£°(M1k1)H
...
(Mnkn)Hï£¹
ï£ºï£»âˆˆCnÃ—d.(37)
The time complexity of transforming Q,KtoËœQ,ËœKisO(nd2). The next step is to calculate the output, i.e.,
O=Q(KHV)âˆˆCnÃ—d,
O=âˆ†âˆ’1ËœQËœKHV
=âˆ†âˆ’1ËœQ[ËœKHV],
âˆ†= diag( ËœQ)[ËœKH1n].(38)
Clearly, Eq. 38 is a standard formulation for the linear attention with the time complexity as O(nd2).
Combing it with the first step, we have the total time complexity as O(nd2), which is unchanged.
15Published in Transactions on Machine Learning Research (07/2023)
C.3 Linearized Relative Positional Encoding
Before the proof, we first give the following theorems (Yao & Algebra, 2015):
Theorem C.1. If matrix WâˆˆCdÃ—dis a unitary matrix, there exists another unitary matrix PâˆˆCdÃ—d,
such that
W=PHÎ›P,
Î›= diag{exp(iÎ¸1),..., exp(iÎ¸d)},
i2=âˆ’1.(39)
Theorem C.2. If matrix WâˆˆRdÃ—dis an orthogonal matrix, there exists another orthogonal matrix
PâˆˆRdÃ—d, such that
W=PTÎ›P,
Î›= diag{Î›1,...,Î›r; 1,..., 1;âˆ’1,...,âˆ’1},
Î›k=/bracketleftbigg
cosÎ¸kâˆ’sinÎ¸k
sinÎ¸kcosÎ¸k/bracketrightbigg
,k= 1,...r.(40)
C.4 Unitary (Solution 1)
Proof of Proposition 3.3. According to Theorem C.1, we can assume that Wshas the following form ( Pâˆˆ
CdÃ—dis aunitary matrix),
Ws=PHÎ›(s)P,
Î›(s)= diag{exp(iÎ¸(s)
1),..., exp(iÎ¸(s)
d)}.(41)
Hence, Eq. 14 is equivalent to
WH
sWt=Wtâˆ’s,
PHÎ›(s)HPPHÎ›(t)P=PHÎ›(tâˆ’s)P,
PHÎ›(s)HÎ›(t)P=PHÎ›(tâˆ’s)P,
Î›(s)HÎ›(t)=Î›(tâˆ’s),
diag/braceleftï£¬ig
j(Î¸(t)
1âˆ’Î¸(s)
1),j(Î¸(t)
2âˆ’Î¸(s)
2),Â·Â·Â·,j(Î¸(t)
dâˆ’Î¸(s)
d)/bracerightï£¬ig
= diag/braceleftï£¬ig
jÎ¸(tâˆ’s)
1,jÎ¸(tâˆ’s)
2,Â·Â·Â·,jÎ¸(tâˆ’s)
d/bracerightï£¬ig
.(42)
In this case,âˆ€k= 1,...,d, we have
Î¸(t)
kâˆ’Î¸(s)
k=Î¸(tâˆ’s)
k+ 2lÏ€,k,lâˆˆZ. (43)
Note that 2lÏ€does not affect the result, so we can assume l= 0,i.e.,
Î¸(t)
kâˆ’Î¸(s)
k=Î¸(tâˆ’s)
k. (44)
Takingt=s+ 1, we get
Î¸(s+1)
kâˆ’Î¸(s)
k=Î¸(1)
k,
Î¸(s)
k=sÎ¸(1)
kâ‰œsÎ±k.(45)
16Published in Transactions on Machine Learning Research (07/2023)
C.5 Orthogonal (Solution 2)
Proof of Proposition 3.4. According to Theorem C.2, we can assume that Wshas the following form ( Pâˆˆ
RdÃ—dis anorthogonal matrix),
Ws=PTÎ›(s)P,
Î›(s)=ï£®
ï£°A(s)
B(s)
C(s)ï£¹
ï£»,
A(s)=ï£®
ï£¯ï£¯ï£°A(s)
1
...
A(s)
nï£¹
ï£ºï£ºï£»âˆˆR2pÃ—2p,
B(s)=IqâˆˆRqÃ—q,
C(s)=âˆ’IrâˆˆRrÃ—r,
A(s)
k=/bracketleftï£¬igg
cosÎ¸(s)
kâˆ’sinÎ¸(s)
k
sinÎ¸(s)
kcosÎ¸(s)
k/bracketrightï£¬igg
.(46)
Hence, Eq. 14 is equivalent to
WT
sWt=Wtâˆ’s,
PTÎ›(s)TPPTÎ›(t)P=PTÎ›(tâˆ’s)P,
PTÎ›(s)TÎ›(t)P=PTÎ›(tâˆ’s)P,
Î›(s)TÎ›(t)=Î›(tâˆ’s),
ï£®
ï£¯ï£°A(s)T
B(s)T
C(s)Tï£¹
ï£ºï£»ï£®
ï£°A(t)
B(t)
C(t)ï£¹
ï£»=ï£®
ï£°A(tâˆ’s)
B(tâˆ’s)
C(tâˆ’s)ï£¹
ï£»,(47)
where
A(s)TA(t)=A(tâˆ’s),
B(s)TB(t)=B(tâˆ’s),
C(s)TC(t)=C(tâˆ’s).(48)
ForA(s), considering the k-th component, we get
A(s)
kTA(t)
k=A(tâˆ’s)
k
=/bracketleftï£¬igg
cosÎ¸(s)
ksinÎ¸(s)
k
âˆ’sinÎ¸(s)
kcosÎ¸(s)
k/bracketrightï£¬igg/bracketleftï£¬igg
cosÎ¸(t)
kâˆ’sinÎ¸(t)
k
sinÎ¸(t)
kcosÎ¸(t)
k/bracketrightï£¬igg
=/bracketleftï£¬igg
cosÎ¸(s)
kcosÎ¸(t)
k+ sinÎ¸(s)
kcosÎ¸(t)
ksinÎ¸(s)
kcosÎ¸(t)
kâˆ’cosÎ¸(s)
ksinÎ¸(t)
k
âˆ’sinÎ¸(s)
kcosÎ¸(t)
k+ cosÎ¸(s)
ksinÎ¸(t)
kcosÎ¸(s)
kcosÎ¸(t)
k+ sinÎ¸(s)
ksinÎ¸(t)
k/bracketrightï£¬igg
=ï£®
ï£°cos/parenleftï£¬ig
Î¸(t)
kâˆ’Î¸(s)
k/parenrightï£¬ig
âˆ’sin/parenleftï£¬ig
Î¸(t)
kâˆ’Î¸(s)
k/parenrightï£¬ig
sin/parenleftï£¬ig
Î¸(t)
kâˆ’Î¸(s)
k/parenrightï£¬ig
cos/parenleftï£¬ig
Î¸(t)
kâˆ’Î¸(s)
k/parenrightï£¬igï£¹
ï£»
=A(tâˆ’s)
k
=/bracketleftï£¬igg
cosÎ¸(tâˆ’s)
kâˆ’sinÎ¸(tâˆ’s)
k
sinÎ¸(tâˆ’s)
kcosÎ¸(tâˆ’s)
k/bracketrightï£¬igg
.(49)
17Published in Transactions on Machine Learning Research (07/2023)
Hence,âˆ€k= 1,...,d, we have
Î¸(t)
kâˆ’Î¸(s)
k=Î¸(tâˆ’s)
k+ 2kÏ€,kâˆˆZ. (50)
Note that 2tÏ€does not affect the result, so we can assume t= 0,i.e.,
Î¸(t)
kâˆ’Î¸(s)
k=Î¸(tâˆ’s)
k. (51)
Takingt=s+ 1, we have
Î¸(s+1)
kâˆ’Î¸(s)
k=Î¸(1)
k,
Î¸(s)
k=sÎ¸(1)
kâ‰œsÎ±k.(52)
Next, for B(s), the conclusion is more obvious, i.e.,
B(s)TB(t)=IT
qIq
=Iq
=B(tâˆ’s).(53)
Finally, for C(s), we have
C(s)TC(t)= (âˆ’IT
r)(âˆ’Ir)
=Ir
Ì¸=C(tâˆ’s).(54)
In that case, we must have r= 0.
C.6 Permutation
Prior to the proof, we first provide some relevant definitions and propositions.
Definition C.3. Permutation Ï€is abijection defined on the integer set:
Ï€:{1,2,Â·Â·Â·,d}â†’{ 1,2,Â·Â·Â·,d},dâˆˆZ+. (55)
Definition C.4. For matrix
M=ï£®
ï£¯ï£¯ï£¯ï£°mT
1
mT
2
...
mT
dï£¹
ï£ºï£ºï£ºï£»âˆˆRdÃ—d,mkâˆˆRd,k= 1,...,d, (56)
MÏ€is defined as
MÏ€=ï£®
ï£¯ï£¯ï£¯ï£¯ï£°mT
Ï€(1)
mT
Ï€(2)
...
mT
Ï€(d)ï£¹
ï£ºï£ºï£ºï£ºï£». (57)
Definition C.5. For identity matrix IdâˆˆRdÃ—dand permutation Ï€, we define
Î›k= (Id)Ï€k. (58)
ForÎ›k, we have the following important properties:
Lemma C.6. For permutation Ï€, matrix MâˆˆRdÃ—dandÎ›kâˆˆRdÃ—ddefined in C.5, we have
MÏ€=Î›1M. (59)
18Published in Transactions on Machine Learning Research (07/2023)
Proof.We first organize IdâˆˆRdÃ—din the following form, where ekâˆˆRd,k= 1,...,drepresents the one-hot
vector with the k-th element as one, i.e.,
Id=ï£®
ï£¯ï£¯ï£¯ï£°eT
1
eT
2
...
eT
dï£¹
ï£ºï£ºï£ºï£». (60)
Notice that
eT
kM=mT
k, (61)
so we get
Î›1M=ï£®
ï£¯ï£¯ï£¯ï£¯ï£°eT
Ï€(1)
eT
Ï€(2)
...
eT
Ï€(d)ï£¹
ï£ºï£ºï£ºï£ºï£»M
=ï£®
ï£¯ï£¯ï£¯ï£¯ï£°eT
Ï€(1)M
eT
Ï€(2)M
...
eT
Ï€(d)Mï£¹
ï£ºï£ºï£ºï£ºï£»
=ï£®
ï£¯ï£¯ï£¯ï£¯ï£°mT
Ï€(1)
mT
Ï€(2)
...
mT
Ï€(d)ï£¹
ï£ºï£ºï£ºï£ºï£»
=MÏ€.(62)
Theorem C.7. ForÎ›kdefined in C.5, we have:
Î›k=Î›k
1. (63)
Proof.We use induction for the proof.
Fork= 1, the conclusion is obvious. Now assuming that the conclusion holds for k=sâˆ’1, whenk=s, we
have
Î›s= (Id)Ï€s
= ((Id)Ï€sâˆ’1)Ï€
= (Î›sâˆ’1)Ï€
= (Î›sâˆ’1
1)Ï€.(64)
The next step is to prove
(Î›sâˆ’1
1)Ï€=Î›s
1=Î›1Î›sâˆ’1
1. (65)
The above conclusion follows from C.6.
Theorem C.8. Î›kâˆˆRdÃ—ddefined in C.5 are orthogonal matrices, i.e.,
Î›kÎ›T
k=Î›T
kÎ›k=Id. (66)
19Published in Transactions on Machine Learning Research (07/2023)
Proof.We first prove that the conclusion holds for k= 1:
Î›1Î›T
1=ï£®
ï£¯ï£¯ï£¯ï£¯ï£°eT
Ï€(1)
eT
Ï€(2)
...
eT
Ï€(d)ï£¹
ï£ºï£ºï£ºï£ºï£»/bracketleftbig
eÏ€(1)eÏ€(2)...eÏ€(d)/bracketrightbig
,
/bracketleftbig
Î›1Î›T
1/bracketrightbig
st=eT
Ï€(s)eÏ€(t)
=Î´st,
Î›1Î›T
1=Id.(67)
Since Î›1is a square matrix, we also have
Î›T
1Î›1=Id. (68)
In general cases, we only use C.7, i.e.,
Î›kÎ›T
k=Î›k
1(Î›k
1)T
=Î›k
1(Î›T
1)k
=Î›kâˆ’1
1Î›1Î›T
1(Î›T
1)kâˆ’1
=Î›kâˆ’1
1(Î›T
1)kâˆ’1
=...
=Id.(69)
With the same proof, we get
Î›T
kÎ›k=Id. (70)
Based on the above conclusions, we can prove Proposition 3.5 below.
Proof of Proposition 3.5. According to Theorem C.8 and the product of the orthogonal matrix is an or-
thogonal matrix, we can assume that Wkhas the following form ( PâˆˆRdÃ—dis anorthogonal matrix),
i.e.,
Wk=PTÎ›(k)P. (71)
The next step is to verify that it satisfies Eq. 14, which follows Theorem C.7 and C.8:
WT
sWt=PTÎ›(s)TPPTÎ›(t)P
=PTÎ›(s)TÎ›(t)P
=PTÎ›(s)T(Î›(1))tP
=PTÎ›(s)T(Î›(1))s(Î›(1))tâˆ’sP
=PTÎ›(s)TÎ›(s)(Î›(1))tâˆ’sP
=PTÎ›(tâˆ’s)P
=Wtâˆ’s.(72)
20Published in Transactions on Machine Learning Research (07/2023)
D Implementation
D.1 Theory
LRPE(Ws=PHÎ›(s)P) contains two components, i.e., the fixed unitary matrix Pand the unitary matrix
family Î›(s)mentioned in proposition 3.3, 3.4, and 3.5. We first introduce the choice of matrices P/Î›(s),
and then illustrate some implementation tricks.
Choice of matrices
For matrix P, We list the species mentioned in the paper below:
â€¢Householder matrix: denoted as a vector vâˆˆRd,i.e.,
W=Idâˆ’2vvT/(vTv). (73)
In our implementation, we sample vfrom a standard normal distribution, and make it determin-
istic.
â€¢Permutation matrix: formulated as per the following permutation (inspired by Flash (Hua et al.,
2022)),i.e.,
Ï€(2k) =k,Ï€(2k+ 1) =âŒŠd/2âŒ‹+ 1,1â‰¤2k,2k+ 1â‰¤d. (74)
â€¢Identity matrix.
Formatrix family Î›(s), we use the following settings:
â€¢For unitary (Solution 1) (3.3), we use the same method in (Su et al., 2021) with initialized Î±t=
10000âˆ’2t/d, and make it learnable.
â€¢For orthogonal (Solution 2) (3.4), we choose the dimension of identity submatrix q= 0with initial-
izedÎ±t= 10000âˆ’2t/das in (Su et al., 2021) and make it learnable .
â€“Another notable version to choose the dimension of the identity submatrix q= 0with initialized
Î±t= 10000âˆ’2t/das in (Su et al., 2021), and make it deterministic . When using this version
along with the identity matrix, we can get RoPE(Su et al., 2021).
â€¢Forpermutation(Solution3)(3.5), werandomlychoosethepermutationandmakeit deterministic .
â€“Notice that when combing this method with identity matrix, we can get a version of Permute-
Former (Chen, 2021).
Implementation tricks
According to the following facts, we can simplify the computation, i.e.,
qH
sWH
sWtkt=qH
sPH(Î›(s))HPPHÎ›(t)Pkt
=qH
sPH(Î›(s))HÎ›(t)Pkt
= (Î›(s)Pqs)H(Î›(t)Pkt).(75)
Hence, in practice, we can use Ws=PHÎ›(s)instead of Ws=PHÎ›(s)Pto reduce the computational costs.
21Published in Transactions on Machine Learning Research (07/2023)
D.2 Pseudocode
In this section, we provide pseudocodes for LRPEin Python:
import torch
import torch.nn as nn
import numpy as np
class Lrpe(nn.Module):
def __init__(self, core_matrix, p_matrix, max_positions=512, embedding_dim=768,
theta_type="a", theta_learned=False, householder_learned=False):
super().__init__()
self.core_matrix = core_matrix
self.p_matrix = p_matrix
self.theta_type = theta_type
self.theta_learned = theta_learned
self.householder_learned = householder_learned
# Lambda matrix
if self.core_matrix == 1:
if self.theta_learned:
print("Learn theta!")
self.theta = nn.Parameter(10000 ** (-2 / embedding_dim * torch.arange(embedding_dim
// 2)).reshape(1, 1, -1))
else:
print(f"Theta_type {self.theta_type}")
elif self.core_matrix == 2:
print("Mixed")
elif self.core_matrix == 3:
print("Permutation")
permutation = self.get_permutation(max_positions, embedding_dim)
self.register_buffer("permutation", permutation)
elif self.core_matrix == 4:
print("Complex exp")
if self.theta_learned:
print("Learn theta!")
self.theta = nn.Parameter(10000 ** (-2 / embedding_dim *
torch.arange(embedding_dim)).reshape(1, 1, -1))
else:
print(f"Theta_type {self.theta_type}")
# P matrix
if self.p_matrix == 1:
print("Identity")
elif self.p_matrix == 2:
print("Householder")
if self.householder_learned:
print("learn householder!")
self.v = nn.Parameter(torch.randn(1, embedding_dim, 1))
else:
v = torch.randn(1, embedding_dim, 1)
v = v / torch.norm(v)
print(f"Householder norm is {torch.norm(v)}")
self.v = nn.Parameter(v, requires_grad=False)
elif self.p_matrix == 3:
print("Fourier")
elif self.p_matrix == 4:
print("Odd_even")
22Published in Transactions on Machine Learning Research (07/2023)
self.p = self.get_p()
self.core_transform = self.get_core_transform()
def forward(self, x):
â€™â€™â€™
input shape: (b, l, e), b stands for batch size, l stands for sequence length, e stands for
embedding dimension.
â€™â€™â€™
x = self.p(x)
x = self.core_transform(x)
return x
def get_p(self):
if self.p_matrix == 1:
def f(x):
return x
return f
elif self.p_matrix == 2:
return self.householder
elif self.p_matrix == 3:
def f(x):
return torch.fft.fft(x, norm="ortho")
return f
elif self.p_matrix == 4:
return self.odd_even_permutation
def get_core_transform(self):
if self.core_matrix == 1:
return self.reflect
elif self.core_matrix == 2:
return self.mix_reflect
elif self.core_matrix == 3:
return self.do_permutation
elif self.core_matrix == 4:
return self.complex_exp
def get_permutation(self, max_positions, embedding_dim):
permutation = torch.randperm(embedding_dim).reshape(1, -1)
expanded = [torch.arange(embedding_dim).unsqueeze(0)]
for _ in range(max_positions - 1):
previous = expanded[-1]
current = previous.gather(-1, permutation)
expanded.append(current)
expanded = torch.stack(expanded, dim=1)
return expanded
def odd_even_permutation(self, x):
# 2k->k, 2k+1->d+k
e = x.shape[-1]
d = e - e // 2
permutation = torch.arange(e)
index = torch.arange(e)
permutation[::2] = index[::2] // 2
permutation[1::2] = (index[1::2] - 1) // 2 + d
permutation = permutation.to(x.device)
x = x.gather(-1, permutation.expand_as(x))
return x
23Published in Transactions on Machine Learning Research (07/2023)
def do_permutation(self, x):
b, l, e = x.shape
x = x.gather(-1, self.permutation[:, :l, :].expand_as(x))
return x
def reflect(self, x):
b, l, d = x.shape
e = d - 1 if d % 2 == 1 else d
return self.transform(x, e)
def mix_reflect(self, x):
b, l, d = x.shape
assert d >= 3
# split
e = d // 2
# to even
if e % 2:
e += 1
return self.transform(x, e)
def transform(self, x, e):
assert e % 2 == 0
b, l, d = x.shape
# do identity transformation
x1 = x[:, :, e:]
# do reflection
x = x[:, :, :e]
if self.theta_learned:
theta = self.theta
else:
if self.theta_type == "a":
theta = 10000 ** (-2 / e * torch.arange(e // 2))
elif self.theta_type == "b":
theta = np.pi / 2 / l / (e // 2) * torch.arange(1, e // 2 + 1)
elif self.theta_type == "c":
theta = np.pi / 2 / l / torch.arange(1, e // 2 + 1)
theta = theta.reshape(1, 1, -1).to(x)
theta = torch.stack([theta, theta], dim=-1).reshape(1, 1, e)
theta = theta * torch.arange(l).reshape(1, -1, 1).to(x)
# (-q1, -q3), (q0, q2) -> (-q1, q0, -q3, q2)
x_half = torch.stack([-x[..., 1::2], x[..., ::2]], dim=-1).reshape_as(x)
x_transform = x * torch.cos(theta) + x_half * torch.sin(theta)
# merge
if e != d:
x_transform = torch.cat([x_transform, x1], dim=-1)
return x_transform
def complex_exp(self, x):
b, l, e = x.shape
if self.theta_learned:
theta = self.theta
else:
if self.theta_type == "a":
theta = 10000 ** (-2 / e * torch.arange(e))
theta = theta.reshape(1, 1, -1).to(x.device)
matrix = theta * torch.arange(l).reshape(1, -1, 1).to(x.device)
24Published in Transactions on Machine Learning Research (07/2023)
sin_cos = torch.complex(torch.cos(matrix),torch.sin(matrix)).to(x.device)
x = self.element_wise_complex(x, sin_cos)
return x
def element_wise_complex(self, t1, t2):
return torch.complex(t1.real * t2.real - t1.imag * t2.imag, t1.real * t2.imag + t1.imag *
t2.real)
def householder(self, x, eps=1e-6):
if self.householder_learned:
v = self.v / (torch.norm(self.v) + eps)
else:
v = self.v
# (b, n, e), (1, e, 1) -> (1, n, 1)
y = torch.matmul(x, v)
# (1, n, 1), (1, 1, e) -> (1, n, e)
y = torch.matmul(y, v.transpose(1, 2))
return x - 2 * y
E Configuration
Table 8: Detailed configurations used in our experiments. â€œTotal batch sizeâ€ means batch_per_gpuÃ—
update_freqÃ—num_gpus. â€œAttention dropoutâ€ is only used for vanilla attention. â€œALMâ€: autoregressive
Language Model. â€œBLMâ€: bidirectional Language Model. â€œIMâ€: Image Modeling.
ALM BLM IM
Data WikiText-103/Books Wikibook ImageNet-1k
Tokenizer method BPE BPE -
Vocab size 267744/50265 50265 -
Encoder layers 0 12 12
Decoder layers 6 0 0
Hidden dimensions 512 768 384
Number of heads 8 12 6
FFN dimensions 2048 3072 1536
FFN activation function Relu Gelu Gelu
Seqence length 512 51 -
Total batch size 128 512 1600
Number of updates 50k updates 23k updates 300 epochs
Warmup steps 4k steps 3k steps 20 epochs
Peak learning rate 5e-4 5e-4 5e-4
Learning rate scheduler Inverse sqrt Polynomial decay Cosine
Optimizer Adam Adam Adamw
AdamÏµ 1e-8 1e-6 1e-8
Adam (Î²1,Î²2) (0.9, 0.98) (0.9, 0.98) (0.9, 0.98)
Weight decay 0.01 0.01 0.05
25Published in Transactions on Machine Learning Research (07/2023)
Table9: DetailedconfigurationsusedinLRAexperiments. â€˜BNâ€˜standsforbatchnormalization. Allmethods
use the same configuration, except for relative positional encodings.
Task Feature dim LayerNormBatch size Epoch Lr
Text 128 4BN256 32 0.001
ListOps 128 4BN256 40 0.0001
Retrieval 64 4BN64 20 0.001
Pathfinder 32 4BN128 200 0.0005
Image 100 12BN100 200 0.001
26