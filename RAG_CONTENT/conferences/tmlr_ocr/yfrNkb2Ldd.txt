Published in Transactions on Machine Learning Research (09/2024)
Noise Stability Optimization for Finding Flat Minima:
A Hessian-based Regularization Approach
Hongyang R. Zhang ho.zhang@northeastern.edu
Northeastern University, Boston
Dongyue Li li.dongyu@northeastern.edu
Northeastern University, Boston
Haotian Ju ju.h@northeastern.edu
Northeastern University, Boston
Reviewed on OpenReview: https://openreview.net/forum?id=yfrNkb2Ldd
Abstract
The training of over-parameterized neural networks has received much study in recent lit-
erature. An important consideration is the regularization of over-parameterized networks
due to their highly nonconvex and nonlinear geometry. In this paper, we study noise injec-
tion algorithms, which can regularize the Hessian of the loss, leading to regions with flat
loss surfaces. Specifically, by injecting isotropic Gaussian noise into the weight matrices of
a neural network, we can obtain an approximately unbiased estimate of the trace of the
Hessian. However, naively implementing the noise injection via adding noise to the weight
matrices before backpropagation presents limited empirical improvements. To address this
limitation, we design a two-point estimate of the Hessian penalty, which injects noise into
the weight matrices along both positive and negative directions of the random noise. In
particular, this two-point estimate eliminates the variance of the first-order Taylorâ€™s expan-
sion term on the Hessian. We show a PAC-Bayes generalization bound that depends on the
trace of the Hessian (and the radius of the weight space), which can be measured from data.
We conduct a detailed experimental study to validate our approach and show that it can
effectively regularize the Hessian and improve generalization. First, our algorithm can out-
perform prior approaches on sharpness-reduced training, delivering up to a 2.4% test ac-
curacy increase for fine-tuning ResNets on six image classification datasets. Moreover, the
trace of the Hessian reduces by 15.8%, and the largest eigenvalue is reduced by 9.7% with
our approach. We also find that the regularization of the Hessian can be combined with
alternative regularization methods, such as weight decay and data augmentation, leading
to stronger regularization. Second, our approach remains highly effective for improving
generalization in pretraining multimodal CLIP models and chain-of-thought fine-tuning.
1 Introduction
The loss landscape and its geometry properties are a recurring theme in the study of neural networks
(Keskar et al., 2017; Hochreiter & Schmidhuber, 1997). Recently, the design of training methods such as
sharpness-aware minimization and stochastic weight averaging has led to empirical advances in a wide variety
of settings (Izmailov et al., 2018; Foret et al., 2021; Wortsman et al., 2022). The theoretical study of these
training methods has also been explored (Andriushchenko & Flammarion, 2022). For instance, recent work
shows that sharpness-aware minimization (Foret et al., 2021) has an implicit bias to flat surface regions by
penalizing the largest eigenvalue of the loss Hessian matrix (Wen et al., 2023; Bartlett et al., 2023). In this
paper, we study methods that can provide explicit regularization of the trace of the Hessian, and we will
1Published in Transactions on Machine Learning Research (09/2024)
ğ‘¾ğ’Š	ğ‘¼ğ’Šâˆ’ğ‘¼ğ’Šâˆ‡ğ‘“(ğ‘Š"+ğ‘ˆ")âˆ‡ğ‘“(ğ‘Š"âˆ’ğ‘ˆ")ğ‘¾ğ’Š#ğŸğğ’ğ	
Figure 1: An illustration of one update step in our algorithm. At each iteration i, we sample a random
variableUifrom a zero-mean distribution P(e.g., an isotropic Gaussian with variance Ïƒ2), whereÏƒis a
hyper-parameter that controls the strength of the noise injection (hence the regularization). We query the
gradient of f, atf(Wi+Ui), andf(Wiâˆ’Ui), and take their average. This results in a two-point noise
injection scheme, whose computation cost is the same as sharpness-aware minimization (Foret et al., 2021),
and twice the cost of running SGD. Notice that in practice, we can also implement an extension of this
algorithm, which samples multiple Us. For details, see Algorithm 1.
show provable generalization guarantees of our methods. More formally, given an input function f:Rdâ†’R
that represents the empirical risk of a neural network and a d-dimensional distribution Pwith mean zero,
we consider minimizing the noise-perturbed function
F(W) :=E
Uâˆ¼P[f(W+U)]. (1)
Minimizingthisperturbedfunctioncanimprovetheresilienceoftheneuralnetworktonoiseinjection, leading
to flatter loss surfaces and improved regularization (Nagarajan & Kolter, 2020). By analyzing the perturbed
loss of a fine-tuned model, one can identify a measure of the sharpness of loss surfaces based on the trace
of the Hessian (Ju et al., 2022; 2023). We remark that the minimization problem of the form (1) traces
back to earlier works on randomized smoothing (Duchi et al., 2012), which have provided a detailed study of
convergence rates for nonsmooth stochastic optimization. Our work differs from this line of literature in that
we focus on evaluating the regularization effect of penalizing the Hessian trace upon neural network training.
Although noise injection algorithms can be theoretically motivated as improving generalization (and sta-
bility), its practical implication is not evident (Hinton & Van Camp, 1993; An, 1996; Graves, 2011). To
motivate our study, we begin by running several empirical studies to compare the performance of (standard)
SGD and weight-perturbed SGD (WP-SGD), which first injects random noise into the weight matrices of a
neural network before computing its gradient in SGD. As mentioned above, this would provide a randomized
smoothing effect to the loss surface (Duchi et al., 2012). We will fine-tune (pretrained) ResNets on three
image classification tasks for this empirical study. To ensure the robustness of the analysis, we also vary the
distribution ofPand the variance of U. Our overall finding is that WP-SGD (or randomized smoothing)
does not offer clear benefits over SGD, which is also consistent with recent studies of weight noise injection
(Orvieto et al., 2023; Dauphin et al., 2024) (see Section 2.2, Table 2 for the complete results). However,
we hypothesize that these results may be due to the randomness of the noise injection (upon the Hessian
penalty term) rather than the ineffectiveness of regularizing the Hessian trace.
Our approach to mitigate the randomness of the noise injection on the Hessian penalty involves two parts.
First, we retrieve the gradient at Wâˆ’Uto cancel out the first-order expansion term of W+U(recall that
Uis a random sample from P). Meanwhile, the second-order expansion term remains the same after this
cancellation. We term this modification a two-point noise injection scheme, which is reminiscent of two-point
gradient estimates in zeroth-order optimization (Duchi et al., 2015). The difference in our setting is that this
two-point averaging cancels out the first-order gradient term, thereby eliminating its variance on the Hessian
2Published in Transactions on Machine Learning Research (09/2024)
Table 1: Comparison between our approach (NSO) and SAM (Foret et al., 2021), based on inductive bias,
generalization guarantee, and convergence rate. In particular, the inductive bias of SAM is based on the
results of Wen et al. (2023). The list of notations used in the table is explained as follows. âˆ‡2â„“refers to
the Hessian matrix of the loss function â„“.Î»1[Â·]and Tr [Â·]refer to the largest eigenvalue and the trace of an
input matrix. Î±refers to the trace norm, taken over the maximum of the entire hypothesis space and data
distribution (including unseen test data). ris the radius of the fine-tuning region measured in â„“2distance.
nis the number of samples in the training dataset. Tis the total number of iterations run by our algorithm.
Approach Inductive Bias Generalization Guarantee Convergence Rate
Sharpness-Aware Minimization (SAM) Î»1[âˆ‡2â„“] - -
Noise Stability Optimization (NSO) Tr[âˆ‡2â„“]/radicalï£¬ig
Î±r2
n(Theorem 2.1) Î˜/parenleftï£¬ig
1âˆš
T/parenrightï£¬ig
(Section 4)
penalty. Second, we sample multiple perturbations U1,U2,...,Ukat each epoch and take their averaged
two-point (noise-injected) gradients. See Figure 1 for an illustration of one step.
A primary advantage of our approach compared to prior sharpness minimization algorithms is that our
approach can provide an approximately unbiased estimate of the Hessian trace. We empirically validate
this claim across three real-world settings (see Figure 2, Section 2.2 for an illustration). By utilizing this
property, we show a PAC-Bayes bound that depends on the trace of the Hessian and the radius of the weight
hypothesis space. We briefly describe this result, leaving a formal statement to Theorem 2.1. Let Î±be an
upper bound on the trace of the Hessian measured within the hypothesis space and the data distribution (in
practice, one may take this as the union of training and testing data). Let rbe the radius of the hypothesis
space, measured in â„“2distance. Suppose there are nempirical samples from an unknown distribution. We
show a generalization bound that scales as O/parenleftï£¬ig/radicalï£¬ig
Î±r2
n/parenrightï£¬ig
. Our proof utilizes a linear PAC-Bayes bound (Catoni,
2007; McAllester, 2013; Alquier, 2021), and we optimize the variance of the prior and posterior distributions
to derive the result. A detailed proof sketch is presented in Section 2.3.
Next, we validate our approach with a detailed empirical study. First, we compare our approach with four
prior approaches for the setting of fine-tuning pretrained ResNets, including sharpness-aware minimization
(Foret et al., 2021), tested on six image classification datasets. We show that our algorithm can reduce the
trace and the largest eigenvalue of the loss Hessian matrix by 15.8% and 9.7%, respectively. Our approach
also improves test accuracy by 2.4%. Second, we show that by combining our approach with regularization
methods such as data augmentation and distance-based regularization (Gouk et al., 2022), we can further
regularize the Hessien, leading to 13.6% lower trace values and 16.3% lower test loss values (averaged over
six tasks). Third, we extend our approach to pretraining and chain-of-thought fine-tuning. The details can
be found in Section 3.2. Overall, our algorithm can consistently provide better regularization of Hessian and
improved test accuracy across these different settings and datasets. Some of these empirical results are not
completely explained by our theory, and we discuss the limitations in Section 7.
Finally, we analyze the convergence of our algorithm using techniques from the stochastic optimization
literature (Ghadimi & Lan, 2013; Lan, 2020; Carmon et al., 2020; Drori & Shamir, 2020; Zhang, 2023),
leading to matching upper and lower bounds. We also present a case study of Hessian regularization in over-
parametrized matrix sensing and show that it is equivalent to nuclear norm regularization for this setting.
Ourworkraisesseveralnewquestionsthatmaybeworthrevisiting: canacceleratedgradientdescentmethods
be applied to design flat-minima optimizers? Can recent advances in zeroth-order optimization be leveraged
to better regularize the training of transformer neural networks?
In summary, the contributions of this paper are three-fold. First, we present an algorithm that can explicitly
regularize the Hessian trace and show a PAC-Bayes generalization bound that could be measured from data.
Second, we conduct experiments on multiple settings to validate our approach by comparing downstream
performance and Hessian statistics with prior sharpness minimization algorithms and alternative regulariza-
tion methods. Third, we analyze the convergence of our algorithm using stochastic optimization techniques.
In Table 1, we highlight the key aspects of our approach compared to prior approaches.
3Published in Transactions on Machine Learning Research (09/2024)
Organization: The rest of this paper is organized as follows. In Section 2, we will present our approach.
We will start by presenting the motivating experiments. Then, we describe our algorithm and a PAC-Bayes
bound that depends on the Hessian. In Section 3, we present our experiments for validating the proposed
approach. Section 4 presents an analysis of the convergence rate. Section 5 provides a case study of the
regularization effect of the Hessian trace in the over-parameterized matrix sensing problem. In Section 6,
we discuss the related works. Finally, in Section 7, we state the conclusion and the limitations of this work.
We provide complete proof of our theoretical results in Appendix A-B. We provide additional experimental
details in Appendix C.
2 Our Approach
In this section, we present our approach. First, to set up the stage, we will study the straightforward
implementation of noise injection by directly adding noise to the weight matrices of the neural network before
computing thegradients in backpropagation. We term thisprocedure weight-perturbed SGD (or WP-SGDin
short), alsoknownasrandomizedsmoothing(Duchietal.,2012). Wewillcomparetheempiricalperformance
of these two approaches to evaluate the effect of noise injection. Then, we describe our algorithm and provide
empirical measurements of the trace of the Hessian, along with the actual perturbation gaps observed in
practice. Finally, we will show a PAC-Bayes generalization bound that depends on the trace of the Hessian,
which can be measured from data to compare different methods.
2.1 Motivating Experiments
We compare the results from running WP-SGD to standard SGD. We choose the setting of fine-tuning
pretrained foundation models, as overfitting is a common problem for this setting (Wortsman et al., 2022),
and strong regularization is needed (Li & Zhang, 2021; Ju et al., 2022). We will fine-tune a pretrained
ResNet-34 on several image classification datasets, including aircraft recognition (Aircraft) (Maji et al.,
2013), indoor scene recognition (Caltech-256) (Griffin et al., 2007), and medical image classification (retina
images for diabetic retinopathy classification) (Pachade et al., 2021). To implement WP-SGD, we sample
a random vector from Pand add it to the model weights at each iteration before computing the gradient.
We setPas the isotropic Gaussian and adjust its standard deviation between 0.008,0.01,and0.012via
cross-validation.
We report our results in Table 2, which indicate that the performance gap is less than 0.5%, â‰ˆ0.75standard
deviationsbasedonfiveindependentruns. Furthermore, varying Pdoesnotchangetheresults. Inparticular,
we test four types of P, including Gaussian, Laplace, uniform, and Binomial. We adjust standard deviations
between 0.008,0.01, and 0.012via cross-validation. We find that using Laplace and uniform distributions
achieves a performance comparable to that of Gaussian. However, using Binomial results in worse results.
These experiments suggest that the straightforward implementation of noise injection does not offer clear
benefits over SGD.
2.2 Description of Our Algorithm
In our approach, we make two modifications to WP-SGD. First, we add the perturbation from both the
positive and negative directions during the noise injection, as shown in line 5. Second, we average over
multiple noise injections to reduce the variance from noise injection, as described in line 7. As for the first
modification, recall that Pis a symmetric distribution. We use Taylorâ€™s expansion on both f(W+U)and
f(Wâˆ’U)as follows:
f(W+U) =f(W) +âŸ¨U,âˆ‡f(W)âŸ©+1
2UâŠ¤âˆ‡2f(W)U+O(âˆ¥Î£âˆ¥3
2
2), (2)
f(Wâˆ’U) =f(W)âˆ’âŸ¨U,âˆ‡f(W)âŸ©+1
2UâŠ¤âˆ‡2f(W)U+O(âˆ¥Î£âˆ¥3
2
2). (3)
4Published in Transactions on Machine Learning Research (09/2024)
Table 2: Comparing the outcome of running WP-SGD to standard SGD across four different P, measured
over three image classification datasets. Recall that WP-SGD refers to normal weight perturbation (without
the paired perturbation). To be concise, we have included the results of running our approach (i.e., NSO).
All the results and their standard deviations are based on five independent runs.
Aircraft Indoor Retina Disease
P Train Acc. Test Acc. Train Acc. Test Acc. Train Acc. Test Acc.
SGD None 100.0% Â±0.0 59.8%Â±0.7 100.0%Â±0.0 76.0%Â±0.4 100.0%Â±0.0 61.7%Â±0.8
WP-SGD Gaussian 98.4% Â±0.2 60.4%Â±0.1 99.0%Â±0.3 76.3%Â±0.0 100.0%Â±0.0 62.3%Â±0.5
WP-SGD Laplace 98.3% Â±0.1 60.3%Â±0.3 98.9%Â±0.1 76.4%Â±0.3 100.0%Â±0.0 62.0%Â±0.1
WP-SGD Uniform 98.6% Â±0.3 60.3%Â±0.5 98.6%Â±0.3 76.6%Â±0.1 100.0%Â±0.0 62.3%Â±0.0
WP-SGD Binomial 19.6% Â±0.1 11.3%Â±0.1 18.2%Â±0.9 10.7%Â±0.1 58.1%Â±0.1 57.1%Â±0.0
NSO Gaussian 95.8% Â±0.4 62.3%Â±0.3 95.7%Â±0.2 77.4%Â±0.3 100.0%Â±0.0 66.6%Â±0.7
NSO Laplace 96.5% Â±0.3 61.9%Â±0.3 96.1%Â±0.3 77.1%Â±0.1 100.0%Â±0.0 65.9%Â±0.1
NSO Uniform 96.4% Â±0.4 61.9%Â±0.5 96.4%Â±0.2 76.8%Â±0.2 100.0%Â±0.0 65.7%Â±0.1
NSO Binomial 20.1% Â±0.1 14.3%Â±0.3 22.8%Â±0.1 17.9%Â±0.2 59.2%Â±0.1 57.8%Â±0.1
We have that E[U] = 0, andE/bracketleftbig
UUâŠ¤/bracketrightbig
= Î£. Thus, by taking the average of equations (2) and (3), we get
E
Uâˆ¼P/bracketleftbigg1
2(f(W+U) +f(Wâˆ’U))/bracketrightbigg
=F(W) =f(W) +1
2âŸ¨Î£,âˆ‡2f(W)âŸ©+O/parenleftï£¬ig
âˆ¥Î£âˆ¥3
2
2/parenrightï£¬ig
. (4)
We can see that the two-point estimate eliminates the first-order gradient term, potentially reducing its vari-
ance in estimating the Hessian term. The second modification reduces the variance of the stochastic gradient,
using the fact that each perturbation is independent of the others. The entire procedure is summarized in
Algorithm 1. As a remark, two-point gradient estimators are commonly used in zeroth-order optimization
(Duchi et al., 2015). However, their use in designing flat minima optimizers has not been explored much.
Algorithm 1 Noise stability optimization (NSO) for regularizing the Hessian of neural networks
Input: Initialization W0âˆˆRd, a function f:Rdâ†’R
Require : An estimator g:Rdâ†’Rdthat for any W, returnsg(W)s.t.E[g(W)] =âˆ‡f(W)
Parameters: #perturbations k,#epochsT, step sizes Î·0,...,Î·Tâˆ’1
1:fori= 0,1,...,Tâˆ’1do
2:/*Compute the two-point averaged stochastic gradient for each independent noise injection */
3:forj= 0,1,...,kâˆ’1do
4:U(j)
iâ†sampled independently from P
5:G(j)
iâ†g/parenleftbig
Wi+U(j)
i/parenrightbig
+g/parenleftbig
Wiâˆ’U(j)
i/parenrightbig
6:end for
7:Wi+1â†Wiâˆ’Î·i/parenleftï£¬ig
1
2k/summationtextk
j=1G(j)
i/parenrightï£¬ig
8:end for
Measurements of the Hessian trace and the perturbation gap: Next, we provide several examples
to measure the approximation quality of equation (4). Following the experimental setup of Section 2.1, we
will fine-tune a foundation model on a downstream task. After training, we will set Was the model weight
at the last epoch for all the measurements.
To measure equation (4), we then add UtoW, whereUis sampled from an isotropic Gaussian. We will
measuref(W+U)âˆ’f(W), averaged over 100independent samples of U, and we measure this and âˆ‡2f(W)
by taking the average over the training dataset.
The results are shown in Figure 2. We can see that âˆ‡2fprovides an accurate approximation to F(W)âˆ’f(W)
for various values of Ïƒ. In particular, the approximation error of equation (4) using the Hessian trace is less
5Published in Transactions on Machine Learning Research (09/2024)
than 3%. As a remark, the range of Ïƒ2differs across architectures because of the differing scales of their
weights. More details about the neural network architectures can be found in Appendix C.
0.020 0.025 0.030
Ïƒ123Ã—10âˆ’2 MLP
Gap
TraceÃ—Ïƒ2
2
0.0070 0.0075 0.0080
Ïƒ123Ã—10âˆ’2 BERT
Gap
TraceÃ—Ïƒ2
2
0.040 0.045 0.050
Ïƒ246Ã—10âˆ’2 GNN
Gap
TraceÃ—Ïƒ2
2
Figure 2: Illustration of the approximation quality of equation (4). We report all measurements based on the
network weight at the last epoch of fine-tuning. We can see that the perturbation gap (i.e., F(W)âˆ’f(W)
in equation (4)) andÏƒ2
2Tr[âˆ‡2f(W)]are at the same order. Recall that Ïƒrefers to the standard deviation of
the Gaussian noise injected into the weight matrices. More specifically, Ïƒwill decide the strength of noise
injection or the strength of regularization on the Hessian trace.
2.3 Generalization Guarantee and Proof Sketch
Next, we present a PAC-Bayes generalization bound that depends on the trace of the Hessian. Our bound
can be related to the notion of trace norm, which has been used in earlier works for quantifying sample
complexity in the context of matrix recovery (Srebro & Shraibman, 2005).
Concretely, suppose we have a pretrained model in the fine-tuning setting. This can be viewed as our prior
belief of the target hypothesis in PAC-Bayes analysis. Once we have learned a model (though fine-tuning), we
can view this as the posterior in PAC-Bayes analysis. Let DâŠ†XÃ—Y be an unknown data distribution, sup-
ported on the feature space Xand the label space Y. Givennrandom samples (x1,y1),(x2,y2),..., (xn,yn)
drawn fromD, the empirical loss (measured by loss function â„“) applied to a model fW(withWâˆˆRp) is:
Ë†L(W) =1
nn/summationdisplay
i=1â„“(fW(xi),yi).
The population loss is L(W) =E(x,y)âˆ¼D[â„“(fW(x),y)].It is sufficient to think that the empirical loss is less
than the population loss, and the goal is to bound the gap between Ë†L(W)andL(W)from above (Shalev-
Shwartz & Ben-David, 2014).
LetWbe any learned hypothesis within the hypothesis space, denoted as H. Our generalization bound will
apply uniformly to Wwithin the hypothesis space. We state our result, including the required assumptions,
as follows.
Theorem 2.1. Assume that the loss function â„“is bounded between 0andCfor a fixed constant C > 0on the
data distribution D. Supposeâ„“(fW(Â·),Â·)is twice-differentiable in Wand the Hessian matrix âˆ‡2[â„“(fW(Â·),Â·)]
is Lipschitz continuous within the hypothesis space. Suppose for any WinH, the trace norm of the Hessian
is less than Î±:
Î±:= max
WâˆˆHmax
(x,y)âˆ¼DTr/bracketleftbig
âˆ‡2â„“(fW(x),y)/bracketrightbig
, (5)
and theâ„“2-norm ofWis at mostrfor anyWâˆˆH. Then, for any WinH, with probability at least 1âˆ’Î´
for anyÎ´>0, the following must hold, for any Ïµclose to zero:
L(W)â‰¤(1 +Ïµ)Ë†L(W) + (1 +Ïµ)/radicalbigg
CÎ±r2
n+O/parenleftï£¬ig
nâˆ’3
4log(Î´âˆ’1)/parenrightï£¬ig
. (6)
6Published in Transactions on Machine Learning Research (09/2024)
Proof Sketch: We provide a high-level illustration of the proof of Theorem 2.1. Let Qdenote the posterior
distribution. Specifically, we consider Qas being centered at the learned hypothesis W(which could be
anywhere within the hypothesis space), given by a Gaussian distribution N(W,Ïƒ2Idp), where Idpdenotes
thepbypidentity matrix. Given a sample Uâˆ¼N(0,Ïƒ2Idp), let the perturbed loss be given by
â„“Q(fW(x),y) =E
U[â„“(fW+U(x),y)]. (7)
Then, let Ë†LQ(W)be the averaged value of â„“Q(fW(Â·),Â·), taken over nempirical samples from the training
dataset. Likewise, let LQ(W)be the population average of â„“Q(fW(Â·),Â·), in expectation over an unseen data
sample from the underlying data distribution.
Having introduced the notations, we start with the linear PAC-Bayes bound (Catoni, 2007; McAllester, 2013;
Alquier, 2021) (see Theorem A.1 for reference), stated as follows, which holds with probability 1âˆ’Î´for any
Î´âˆˆ(0,1)andÎ²âˆˆ(0,1):
LQ(W)â‰¤1
Î²Ë†LQ(W) +C(KL(Q||P ) + log(Î´âˆ’1))
2Î²(1âˆ’Î²)n, (8)
wherePrefers to the priordistribution, Crefers to the upper bound on the loss value â„“. For analyzing
fine-tuning, we view Pas centered at the pretrained model, with covariance matrix Ïƒ2Idp. By Taylorâ€™s
expansion of â„“Q(see Lemma A.4 for the precise statement), we show that:
LQ(W) =L(W) +Ïƒ2
2E
(x,y)âˆ¼D/bracketleftbig
Tr/bracketleftbig
âˆ‡2â„“(fW(x),y)/bracketrightbig/bracketrightbig
+O(Ïƒ3) (9)
Ë†LQ(W) =Ë†L(W) +Ïƒ2
2nn/summationdisplay
i=1Tr/bracketleftbig
âˆ‡2â„“(fW(xi),yi)/bracketrightbig
+O(Ïƒ3). (10)
Since the Hessian operator is Lipschitz continuous by the assumption of Theorem 2.1, we can bound the gap
between the above two quantities with Ïµ-covering arguments (see Lemma A.5 for the precise statement). By
plugging in these results back to the PAC-Bayes bound of equation (8), after some calculation, we can get:
L(W)â‰¤1
Î²Ë†L(W) +Ïƒ2(1âˆ’Î²)Î±
2Î²+Cr2/2Ïƒ2
2Î²(1âˆ’Î²)n+O/parenleftbigg
Ïƒ3+Ïƒ2âˆšpâˆšn+log(Î´âˆ’1)
n/parenrightbigg
. (11)
In particular, the above uses the fact that the â„“2-norm ofWis less than rfor anyWâˆˆH(the KL divergence
is discussed in Proposition A.2). By choosing Ïƒ2andÎ²to minimize equation (11), we will obtain equation
(6). This summarizes the high-level proof idea. The complete proof can be found in Appendix A.1.
Remark 2.2. We highlight two key aspects of our results. The first is that our PAC-Bayes bound is non-
vacuous, meaning that it matches the scale of empirically observed gaps when measured in practice; this
is based on the trace measurements in Figures 2 and 3. The second is that this non-vacuous bound has
practical implications, meaning that we can utilize this bound to design optimization algorithms that improve
generalization.
These are non-trivial to achieve. To give some context, prior work has provided a PAC-Bayes margin
bound for multi-layer neural networks, which depends on the product of the spectral norm of the network
layers (Neyshabur et al., 2018). While this paper provides important insights regarding the generalization
of deep networks, the bound is vacuous when measured in practice. Arora et al. (2018) provide another
data-dependent PAC-Bayes bound based on compression techniques. Their work started with an experiment
in which they injected noise into the network layers and showed that deep nets can absorb the noise after
retraining. However, their bound remains orders of magnitude higher than the actual generalization errors
observed in practice.
In contrast, our bound matches the scale of empirically observed gaps. To achieve this, we start from the
line of work on data-dependent PAC-Bayes bounds. We build on the line of work on distance from the
initialization (Nagarajan & Kolter, 2020), which is ideal for understanding fine-tuning (Li & Zhang, 2021).
7Published in Transactions on Machine Learning Research (09/2024)
Our key breakthrough is to connect noise stability in PAC-Bayes bound with the loss Hessian matrix (cf.
equations (9)and(10)). Then, we can measure the Hessian of loss landscapes from data.
We additionally note that few existing works have considered using PAC-Bayes bounds to design algorithms.
The reason is that for new algorithm designs, we need to connect the PAC-Bayes bound with data in a non-
vacuous way. The work of Dziugaite & Roy (2017) has provided a computational framework to achieve non-
vacuous generalization bounds. Instead, our result provides an analytical expression that can be leveraged in
algorithm design. To operationalize the design, we utilize the explicit dependence of our result on the Hessian
to design the regularization scheme.
3 Experiments
We now turn to empirical validations of our algorithm. First, we apply our approach to fine-tune pretrained
ResNets on various image classification datasets. We find that NSO can more significantly regularize the
Hessian of the loss surface, resulting in reductions in the trace and the largest eigenvalue by 15.8% and
9.7%, respectively. After controlling computation costs, it can outperform four sharpness-reducing meth-
ods by up to 2.4%. In addition, we justify our algorithm design through detailed ablation analysis. We
also show that our approach is compatible with alternative regularization techniques, including distance-
based regularization and data augmentation, and combining these methods with our approach leads to more
significant regularization and test performance. Second, we show similar results for pretraining and chain-
of-thought fine-tuning. The experiment code for reproducing our empirical findings can be found online at:
https://github.com/VirtuosoResearch/Noise-stability-optimization .
3.1 Comparison with Sharpness Minimization Methods
We now compare Algorithm 1 with five sharpness-reducing training methods, including sharpness-aware
minimization(SAM)(Foretetal.,2021), unnormalizedSAM(USAM)(Agarwala&Dauphin,2023), adaptive
variants of SAM (ASAM), and random SAM (RSAM) (Liu et al., 2022). During the comparison, we control
for the same amount of computation (for Algorithm 1, we will set the number of sampled injections kas
1). Thus, all the methods under consideration will use twice the computation of SGD. For NSO, we sample
perturbation from an isotropic Gaussian distribution and adjust Ïƒbetween 0.008,0.01, and 0.012. For SAM,
we adjust the â„“2norm of the perturbation between 0.01,0.02, and 0.05. For each method, we run it with
both momentum and weight decay. We ensure that all the training methods are carefully adjusted. See
Appendix C for the details.
3.1.1 Empirical Findings
In Table 3, we report the comparison between NSO, SGD, SAM, unnormalized SAM (USAM), and adaptive
SAM (ASAM). We find that our approach reduces the trace of Hessian by 15.8% on average. The largest
eigenvalue of the Hessian is also reduced by 9.7%. This finding is intriguing since SAM has been motivated
by a min-max problem. As for test accuracy, our approach can provide up to 2.4% lift, with an average
improvement of 1.2%. Additional comparisons are deferred to Table 6 in Appendix C.
Figure 3 illustrates the measurements between SGD, WP-SGD, and NSO. Curiously, we find that the trace of
the Hessian also decreases for SGD, possibly due to implicit norm control of SGD. While both WP-SGD and
NSO reduce the trace of the Hessian, our approach penalizes the Hessian more. Besides, the generalization
gap and the test loss are consistently lower during NSO training.
As a remark, the regularization effect of noise injection should be orthogonal to training methods such as
momentum, weight decay, learning rate scheduling, etc. To this end, we performed comparisons without
using either momentum or weight decay. Our approach can again reduce the trace of the Hessian by 17.7%
compared to the five sharpness-reducing methods on average, with up to 1.8% higher test accuracy.
8Published in Transactions on Machine Learning Research (09/2024)
Table 3: Comparison between our approach (NSO) with SGD, sharpness-aware minimization (SAM), un-
normalized SAM (USAM), and adaptive SAM (ASAM). We fine-tune the ResNet-34 network on six image
classification datasets and report the test accuracy and the trace of Hessian using the model in the last epoch
of training. The results are averaged over five random seeds.
CIFAR-10 CIFAR-100 Aircrafts Caltech-256 Indoor Retina
Basic
Statistics# Training 45,000 45,000 3,334 7,680 4,824 1,396
# Validation 5,000 5,000 3,333 5,120 536 248
# Test 10,000 10,000 3,333 5,120 1,340 250
# Classes 10 100 100 256 67 5
Trace
(â†“)SGD 4128 Â±83 13188Â±221 5471Â±65 3674Â±95 3629Â±61 28607Â±226
SAM 2429 Â±87 9227Â±286 4499Â±70 3285Â±95 3159Â±75 15444Â±173
USAM 2352 Â±61 7382Â±222 4298Â±94 3174Â±52 3072Â±51 12068Â±246
ASAM 2445 Â±63 9960Â±313 4475Â±69 3339Â±78 3014Â±53 14155Â±136
NSO 1728 Â±79 5244Â±89 3678Â±83 2958Â±77 2737Â±90 10970Â±146
Test Acc.
(â†‘)SGD 96.1% Â±0.1 82.8%Â±0.1 60.5%Â±0.7 80.0%Â±0.1 76.7%Â±0.4 62.2%Â±0.8
SAM 97.0% Â±0.2 84.0%Â±0.4 62.3%Â±0.3 77.0%Â±0.4 77.2%Â±0.3 65.0%Â±0.3
USAM 96.9% Â±0.2 83.7%Â±0.2 61.9%Â±0.3 76.9%Â±0.2 76.7%Â±0.3 64.7%Â±0.1
ASAM 97.1% Â±0.1 84.2%Â±0.3 62.4%Â±0.5 77.3%Â±0.2 77.2%Â±0.2 65.2%Â±0.3
NSO 97.6% Â±0.4 84.9%Â±0.3 63.2%Â±0.3 78.1%Â±0.5 78.2%Â±0.3 67.0%Â±0.4
0 10 20 30
t0.50.60.70.8Test LossResNet 34
SGD
WP-SGD
NSO
0 10 20 30
t0.51.01.52.0TraceÃ—104 ResNet 34
0 10 20 30
t0.00.20.40.6Generalization GapResNet 34
0 2 4 6
t0.81.01.21.4Test LossBERT Base
SGD
WP-SGD
NSO
0 2 4 6
t0.10.40.71.0TraceÃ—104 BERT Base
0 2 4 6
t0.00.30.60.9Generalization GapBERT Base
Figure 3: Comparison between SGD, WP-SGD, and NSO for fine-tuning ResNet-34 and BERT-Base, re-
spectively, on an image and a text classification dataset. We evaluate the test loss, the trace of the Hessian,
and the generalization gap for the trained model at each epoch. For WP-SGD and NSO, we sample noise
from isotropic Gaussian with standard deviation Ïƒ= 0.01in both settings.
3.1.2 Ablation Analysis
Next, we conduct ablation studies of two modifications in our approach: the use of negative perturbations
and the sampling of multiple perturbations.
Comparing using negative cancellation or not after controlling computation costs: Recall that
our algorithm uses negative perturbations to zero out the first-order term in Taylorâ€™s expansion of F(W).
We validate this by comparing the performance between using or not using the negative perturbation. We
control for the same amount of computation costs to ensure a fair comparison. In particular, we sample
two independent perturbations and take their averaged stochastic gradient. We find that using the nega-
9Published in Transactions on Machine Learning Research (09/2024)
tive perturbation achieves a 3.6% improvement in test accuracy (on average) over not using the negative
perturbation, i.e., randomized smoothing.
As discussed in Section 2.2, our intuition on why NSO can be expected to generalize better than randomized
smoothing is that it can better regularize the Hessian. In particular, even though, in theory, the expectation
off(W+U)and1
2(f(W+U) +f(Wâˆ’U))overUare both equal to F(W). However, the two-point scheme
cancels out the gradient expansion term compared to randomized smoothing at every epoch. More precisely,
we believe that the improved regularization from our approach stems from its better estimate of the Hessian
penalty term. As illustrated in Figure 3, NSO consistently reduces the trace of the Hessian and achieves
lower generalization errors compared to randomized smoothing throughout model training. At the end of
the training, NSO yields 10.6% smaller trace of the Hessian on average than randomized smoothing.
Increasing the number of noise injections k:Recall that increasing the number of perturbations k
can reduce the variance of the estimated gradient. Thus, we consider increasing kin NSO and compare
that with a specific implementation of WP-SGD that uses the same amount of computation. Using k= 2
perturbations improves the test accuracy by 1.2% on average compared to k= 1.
Varying the learning rate and the number of epochs. We provide a detailed comparison between
NSO and WP-SGD when varying the learning rate and the number of epochs. The learning rate is varied be-
tween 0.0001,0.0002,0.0005,0.001,0.002,and 0.005. Thenumberofepochsisvariedbetween 10,20,30,40,50,
and60. We report the test loss from running an image classification task in Figure 4.
1eâˆ’42eâˆ’45eâˆ’41eâˆ’32eâˆ’35eâˆ’3
Learning rate234Test lossWP-SGD
NSO
10 20 30 40 50 60
Number of epochs1.21.41.61.8Test lossWP-SGD
NSO
Figure 4: Results of varying the learning rate and the number of epochs for running our approach and
WP-SGD. We report the test loss from the last epoch and average the results over five random seeds.
Remark 3.1 (Noise variance scheduling as kincreases) .A natural question is whether one can gradually
increase or decrease the regularization strength by Ïƒduring training, similar to learning rate scheduling. To
facilitate this discussion, we test two schedules for adjusting Ïƒ. The first schedule is to increase Ïƒto a
specified value at a linear rate. The second schedule exponentially increases Ïƒto reach a specified value. Our
preliminary experiments show that neither schedule offers significant performance improvements over using
a constant noise variance. One might also consider other scheduling schemes; we leave this to future work.
3.1.3 Detailed Comparison with Sharpness-Aware Minimization (SAM)
Varying the radius of SAM: WeprovideadetailedcomparisontoSAMbyvaryingtheperturbationradius
of SAM (denoted as Ï). To illustrate this comparison, we vary Ïbetween 0.001,0.002,0.005,0.01,0.02,and
0.05. We report both the validation accuracy and the trace of the Hessian for SAM and unnormalized SAM
on an image classification dataset. We present the results in Table 4. We observe that using a smaller Ï(i.e.,
less than 0.01) results in worse results. Thus, we choose Ïbetween 0.01,0.02,and0.05in our experiments.
Varying the batch size of SAM: Next, we measure the sensitivity of our approach concerning the batch
size. In particular, we vary the batch size between 8, 16, 32, and 64 for fine-tuning ResNet-34 on two image
classification datasets. The results are shown in the leftmost two panels of Figure 5. We use the same number
of epochs for each batch size configuration to ensure a fair comparison. On the indoor dataset, our approach
is less sensitive to different batch sizes than SAM. Across all the batch sizes and datasets, our approach
consistently provides a more robust regularization of the Hessian compared to SAM. The best results are
achieved when the batch size is 32. Thus, we use this particular setting in our experiments.
10Published in Transactions on Machine Learning Research (09/2024)
Table 4: Results of varying the perturbation radius of SAM (denoted as Ï) and unnormalized SAM. We
report both the test accuracy and the trace of the Hessian based on the model trained at the last epoch. We
report the averaged results and their standard deviations across five random seeds.
Ï 0.001 0 .002 0 .005 0 .01 0 .02 0 .05
Trace
(â†“)SAM 4920 Â±158 4347Â±166 4016Â±80 3918Â±94 3159Â±753028Â±78
Unnormalized SAM 4352 Â±169 3990Â±70 3723Â±87 3427Â±57 3072Â±513048Â±22
Test Accuracy
(â†‘)SAM 73.6 Â±0.2 74.4Â±0.4 74.8Â±0.6 75.2Â±0.3 76.6Â±0.5 73.8Â±0.7
Unnormalized SAM 74.1 Â±0.1 74.1Â±0.7 74.7Â±0.5 74.6Â±0.3 76.3Â±0.3 73.1Â±0.6
8 16 32 64
Batch size0.81.01.21.4Test lossSAM
NSO
(a) Loss on Indoor dataset
8 16 32 64
Batch size1.01.41.82.2Test lossSAM
NSO (b) Loss on Aircraft dataset
0 10 20 30
t0.81.21.6Test Lossw/o dist.reg.
w/dist.reg. (c) Test loss, w/ dist. reg.
0 10 20 30
t0.81.21.6Test Lossw/o data aug.
w/data aug. (d) Test loss, w/ data aug.
8 16 32 64
Batch size34TraceÃ—103
SAM
NSO
(e) Hessian, Indoor dataset
8 16 32 64
Batch size45TraceÃ—103
SAM
NSO (f) Hessian, Aircraft dataset
0 10 20 30
t357TraceÃ—103
w/o dist.reg.
w/dist.reg. (g) Hessian, w/ dist. reg.
0 10 20 30
t357TraceÃ—103
w/o data aug.
w/data aug. (h) Hessian, w/ data aug.
Figure 5: Results of varying the batch size of our approach and SAM ran on two image classification datasets
(indoor scene recognition and Aircraft detection). We report the test loss and the trace of Hessian using the
model from the last epoch of training. The results are averaged over five random seeds. The regularization
provided by noise injection can be combined with distance-based regularization and data augmentation to
reduce the test loss and the Hessian trace.
3.1.4 Combining Algorithm 1 with Alternative Regularization Methods
In this section, we show that the regularization of the Hessian can serve as a complement to existing, alterna-
tive regularization methods. To validate this, we combine our training approach with data augmentation and
distance-based regularization (Gouk et al., 2022). In particular, the latter approach has been used to regular-
ize fine-tuning algorithms. We use a popular scheme for data augmentation that applies random horizontal
flipping and random cropping sequentially to each training image. As for distance-based regularization, we
penalize the â„“2distance between the fine-tuned model and the pretrained initialization.
The results are shown in Figure 5 within the two rightmost panels. Combining our approach with each
regularization method further reduces the trace of the loss Hessian matrix by 13.6% (on average). This
further leads to 16.3% lower test loss of the fine-tuned network, suggesting that our approach can be used
on top of these preexisting regularization methods.
3.2 Applying Algorithm 1 to Pretraining and Fine-tuning
We apply our approach to pretraining randomly initialized models by replacing SGD to train contrastive
language-image (CLIP) models on a dataset of image-caption pairs. In particular, we use the Conceptual
11Published in Transactions on Machine Learning Research (09/2024)
Caption dataset, which contains 3.3 million image caption pairs. Each caption briefly describes the corre-
sponding image, with ten tokens on average. We use a 12-layer Vision Transformer as the image encoder
and a 12-layer GPT-2 transformer as the text encoder. We train the encoders jointly to maximize the cosine
similarity between the embedding of image caption pairs following the protocol of Radford et al. (2021).
Table 5 presents the results. For each algorithm, we evaluate the trace of the loss Hessian and recall scores
(of the top-10 scored images in retrieving images from texts) on the development set. The results show that
our approach can reduce the trace of the Hessian by 17% compared to both SAM and SGD. In addition,
our approach achieves 1.4% higher recall scores in image retrieval.
Lastly, we apply our algorithm to fine-tuning pretrained language models on chain-of-thought reasoning
datasets. The task is to generate the reasoning process, i.e., a chain of thoughts and the answer for a
given commonsense reasoning question. We fine-tune pretrained GPT-2 models on two question-answering
datasets: Commonsense QA and Strategy QA. Table 5 shows that our approach can yield 25% lower trace
values than SAM and SGD. In addition, we can obtain 5.3% higher test accuracy.
Table 5: Results for pretraining CLIP the Conceptual Caption and chain-of-thought fine-tuning on Com-
monsense/Strategy QA. We report the recall score of image retrieval/test accuracy and trace/ Î»1using the
model at the last epoch. We report the averaged results and standard deviations over five random seeds.
Conceptual Caption Trace(â†“)Î»1(â†“) Recall@10 (â†‘)
SGD 220 Â±24 41Â±2.8 36.1%Â±0.3
SAM 144 Â±20 30Â±1.1 36.9%Â±0.4
NSO 119 Â±3422Â±1.2 37.5%Â±0.3
CommonsenseQA Trace(â†“)Î»1(â†“)Test Accuracy ( â†‘)
SGD 372 Â±34 19Â±0.8 27.7%Â±1.8
SAM 288 Â±15 15Â±0.3 32.7%Â±1.4
NSO 208 Â±3113Â±0.6 39.2%Â±1.4
StrategyQA Trace(â†“)Î»1(â†“)Test Accuracy ( â†‘)
SGD 294 Â±13 44Â±1.5 68.9%Â±1.0
SAM 249 Â±33 42Â±2.6 71.1%Â±1.2
NSO 193 Â±3133Â±1.8 75.2%Â±1.2
4 Convergence Rates
We now study the convergence of Algorithm 1. Recall that our algorithm minimizes f(W)plus a regulariza-
tion term on the Hessian trace. As is typical with regularization, the penalty is usually small relative to the
loss value. Thus, we aim to find a stationary point of F(W)instead off(W)because otherwise, we would
not have the desired regularization. We state the convergence to an approximate stationary point such that
âˆ¥âˆ‡F(W)âˆ¥is small, building on the following gradient oracle assumption (see, e.g., Ghadimi & Lan (2013);
Duchi et al. (2015)).
Assumption 4.1. Given a random seed z, letgz:Rdâ†’Rdbe a continuous function that gives an unbiased
estimate of the gradient: Ez[gz(W)] =âˆ‡f(W), for anyWâˆˆRd. Additionally, the variance is bounded in
the sense that Ez/bracketleftï£¬ig
âˆ¥gz(W)âˆ’âˆ‡f(W)âˆ¥2/bracketrightï£¬ig
â‰¤Ïƒ2.
To help understand the above assumption, suppose there is a dataset of size n. Then, in SGD, the stochastic
gradient would be an unbiased estimate of the gradient of the entire dataset. As for the variance of the
gradient estimator, we note that as long as the â„“2norm of the gradient remains bounded, which will always
hold in practice, then the last equation of the above assumption will hold. We now state an upper bound
on the convergence rate of Algorithm 1.
12Published in Transactions on Machine Learning Research (09/2024)
Proposition 4.2. Suppose Assumption 4.1 holds. Let Pbe a distribution that is symmetric at zero and let
H(P) =E[âˆ¥Uâˆ¥2]. LetCandDbe fixed, positive constants. Let W0âˆˆRddenote an arbitrary initialization.
SupposeF(W0)âˆ’minWâˆˆRdF(W)â‰¤D2,andâˆ‡fisC-Lipschitz continuous. There exists a fixed learning
rateÎ· < Câˆ’1such that if we run Algorithm 1 with Î·i=Î·for alliforTsteps, the algorithm returns Wt
(wheretis a random integer between 1,2,...,T), such that in expectation over the randomness of Wt:
E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¤/radicalbigg
2CD2(Ïƒ2+C2H(P))
kT+2CD2
T. (12)
As a remark, existing sharpness-reducing methods such as SAM seem to suffer from oscillation around the
local basin (Bartlett et al., 2023). Thus, the convergence behavior of SAM seems challenging to analyze for
nonconvex functions. By contrast, our algorithm is amenable to stochastic optimization techniques. Our
proof slightly extends the proof of Theorem 2.1, Ghadimi & Lan (2013), to tackle noise injection and other
variations. For details, see Appendix B.1.
Lower bounds: Next, weconstructanexampletomatchtherateofequation(12), essentiallyshowingthat
this is tight under the same set of assumptions. We use an example from the work of Drori & Shamir (2020).
The difference is that we have to deal with the perturbations added to the objective. For t= 0,1,...,dâˆ’1,
letetâˆˆRdbe the basis vector in dimension d, whoset-th coordinate is 1, while the remaining coordinates
are all zero. Let f:Rdâ†’Rbe defined as
f(W) =1
2GâŸ¨W,e 0âŸ©2+Tâˆ’1/summationdisplay
i=0hi(âŸ¨W,ei+1âŸ©), (13)
wherehiis a piece-wise quadratic function parameterized by Î±i, defined as follow:
hi(x) =ï£±
ï£´ï£´ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£´ï£´ï£³CÎ±2
i
4|x|â‰¤Î±i,
âˆ’C/parenleftbig
|x|âˆ’Î±i/parenrightbig2
2+CÎ±2
i
4Î±iâ‰¤|x|â‰¤3
2Î±i,
C/parenleftbig
|x|âˆ’2Î±i/parenrightbig2
23
2Î±iâ‰¤|x|â‰¤2Î±i,
0 2 Î±iâ‰¤|x|.
One can verify that for each piece above, âˆ‡hiisC-Lipschitz. As a result, provided that Gâ‰¤Câˆ’1,âˆ‡fis
C-Lipschitz, based on the definition of fin equation (13).
The stochastic function Frequires setting the perturbation distribution P. We setPby truncating an
isotropic Gaussian N(0,Ïƒ2Idd)so that the i-th coordinate is at most 2âˆ’1Î±iâˆ’1, fori= 1,...,T. Additionally,
we set the initialization W0to satisfyâŸ¨W0,eiâŸ©= 0for anyiâ‰¥1whileâŸ¨W0,e0âŸ©Ì¸= 0. Finally, we choose the
gradient oracle to satisfy that the i-th stepâ€™s gradient noise Î¾i=âŸ¨Î¾i,ei+1âŸ©ei+1, which means that Î¾iis along
the direction of the basis vector ei+1. In particular, this implies only coordinate i+ 1is updated in step i,
as long asâŸ¨Î¾i,ei+1âŸ©â‰¤2âˆ’1Î±i. With this construction, we state the lower bound below.
Theorem 4.3. Let the learning rates Î·0,...,Î·Tâˆ’1be at mostCâˆ’1. LetD> 0be a fixed value. When either/summationtextTâˆ’1
i=0Î·iâ‰²âˆš
kT, orÎ·i=Î·<Câˆ’1for any epoch i, then for the above construction, the following must hold
min
1â‰¤tâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥D/radicalbigg
CÏƒ2
32kÂ·T. (14)
We remark that the above construction requires Tâ‰¤d. Notice that this is purely for technical reasons. We
briefly illustrate the key steps of the proof. At step i, the gradient noise Î¾iplus the perturbation noise is
less than 2âˆ’1Î±i+ 2âˆ’1Î±i=Î±iat coordinate i+ 1(by triangle inequality). Thus, hâ€²
i(âŸ¨Wt,ei+1âŸ©) = 0, which
holds for all prior update steps. This implies
âˆ‡f(Wi) =Gâˆ’1âŸ¨Wi,e0âŸ©.
13Published in Transactions on Machine Learning Research (09/2024)
Recall that F(W0)â‰¤D2. This condition imposes how large the Î±iâ€™s can be. In particular, we will set
Î±i= 2Î·iÏƒ/âˆš
kin the proof. Then, based on the definition of f(W0),
hi(âŸ¨W0,ei+1âŸ©) =CÎ±2
i
4,sinceâŸ¨W0+U,ei+1âŸ©â‰¤Î±i.
In Lemma B.3, we then argue that the learning rates in this case must satisfy/summationtextTâˆ’1
i=0Î·iâ‰¤O(âˆš
T).When
the learning rate is fixed and at least â„¦(Tâˆ’1/2), we construct a piece-wise quadratic function (similar to
equation (13)), now with a fixed Î±. This is described in Lemma B.4. In this case, the gradient noise grows
by1âˆ’Câˆ’1Î·up toTsteps. We then carefully set Î±to lower bound the norm of the gradient. Combining
these two cases, we conclude the proof of Theorem 4.3. For details, see Appendix B.2. As is typical in
lower-bound constructions, our result holds for a specific instance (with a particular learning rate range).
The proof can also be extended to adaptive learning rate schedules. Notice that the above construction
holds for arbitrary learning rates defined as a function of previous iterates. Then, we set the width of each
functionht,Î±t, proportional to Î·t>0, for anyÎ·tthat may depend on previous iterates, as long as they
satisfy the constraint that/summationtextTâˆ’1
i=0Î·iâ‰¤O(âˆš
T).
Extension: We can show a similar lower bound for the momentum update rule. Recall this is defined as
Mi+1=ÂµMiâˆ’Î·iGi,andWi+1=Wi+Mi+1, (15)
fori= 0,1,...,Tâˆ’1, whereGiis the specific gradient at step i. To handle this case, we will need a more
fine-grained control on the gradient, so we consider a quadratic function as f(W) =C
2âˆ¥Wâˆ¥2.We leave the
result and its proof to Appendix B.3.
Remark 4.4. The novelty of our results lies in analyzing sharpness minimization using techniques from
stochastic optimization. This appears to be new, and we hope our work can inspire further studies, such as
designing accelerated sharpness minimization methods.
5 Regularization Effect of Hessian Trace in Over-Parameterized Matrix Sensing
Before proceeding, let us give an example of the regularization effect of penalizing the Hessian trace. We
consider the matrix sensing problem, whose generalization properties are particularly well-understood in the
nonconvex factorization setting (Li et al., 2018). Let there be an unknown, rank- rpositive semi-definite ma-
trixXâ‹†=Uâ‹†Uâ‹†âŠ¤âˆˆRdÃ—d. Theinputconsistsofalistof dbydGaussianmeasurementmatrix A1,A2,...,An.
The labels are given by yi=âŸ¨Ai,Xâ‹†âŸ©, for everyi= 1,2,...n. The empirical loss is
Ë†L(W) =1
2nn/summationdisplay
i=1/parenleftbig
âŸ¨Ai,WWâŠ¤âŸ©âˆ’yi/parenrightbig2,whereWâˆˆRdÃ—d. (16)
When the loss reaches near zero (which implies the gradient also reaches near zero), it is known that multiple
local minimum solutions exist (Li et al., 2018), and the Hessian becomes
1
nn/summationdisplay
i=1âˆ¥AiWâˆ¥2
Fâ‰ˆdâˆ¥Wâˆ¥2
F=d/vextenddouble/vextenddoubleWWâŠ¤/vextenddouble/vextenddouble
â‹†.
By prior results (Recht et al., 2010), among all X=WWâŠ¤such that Ë†L(W) = 0,Xâ‹†has the lowest nuclear
norm. Thus, the regularization placed on Ë†L(W)is similar to nuclear norm regularization after interpolating
the training dataset. We formalize this discussion and state the result below.
Proposition 5.1. In the setting above, for any Wthat satisfies Ë†L(W) = 0, the following must hold with
high probability:
Tr/bracketleftï£¬ig
âˆ‡2[Ë†L(Uâ‹†)]/bracketrightï£¬ig
â‰¤Tr/bracketleftï£¬ig
âˆ‡2[Ë†L(W)]/bracketrightï£¬ig
+O(nâˆ’1
2). (17)
14Published in Transactions on Machine Learning Research (09/2024)
A similar statement holds if the trace operator is replaced by Î»1in equation (17). To see this, we look at
the quadratic form of the Hessian to find the maximum eigenvalue. Let ube ad2dimension vector with
length equal to one, âˆ¥uâˆ¥= 1. One can derive that:
Î»1[âˆ‡2Ë†L(W)] = max
uâˆˆRd2:âˆ¥uâˆ¥=1uâŠ¤âˆ‡2Ë†L(W)u= max
uâˆˆRd2:âˆ¥uâˆ¥=11
nn/summationdisplay
i=1âŸ¨AiW,uâŸ©2â‰¥1
d2nn/summationdisplay
i=1âˆ¥AiWâˆ¥2
F.
The last step is by setting u=dâˆ’11d2, whose length is equal to one. The proof of Proposition 5.1 can be
found in Appendix A.2.
Simulation: We conduct a numerical simulation to compare algorithmic behaviors. We generate a low-
rank matrix Uâ‹†âˆˆRdÃ—rfrom the isotropic Gaussian. We set d= 100andr= 5. Then, we test three
algorithms: gradient descent (GD), weight-perturbed gradient descent (WP-GD), and Algorithm 1 (NSO).
In particular, we will implement the full gradient update rather than using the stochastic updates. We use an
initialization U0âˆˆRdÃ—dwhere each matrix entry is sampled independently from standard Gaussian N(0,1).
Recall that WP-GD and NSO require setting Ïƒ. We choose Ïƒbetween 0.001,0.002,0.004,0.008,0.0016. NSO
additionally requires setting the number of sampled perturbations k. We setk= 1for faster computing. As
for the learning rate, we choose a fixed Î·for each run and vary its value between 0.001,0.0025,0.005, and
0.01. We find that setting Î·as either 0.005or0.01would be too large, leading the loss values to explode.
Hence, we report the results for setting Î·as0.0025or0.001.
Our findings are illustrated in Figure 6.
â€¢We see that all three algorithms can reduce the training MSE to near zero, as shown in Figure 6a.
From the trends, we can see that the training loss has fully converged for all cases.
â€¢GD suffers from overfitting to training data, while both WP-GD and NSO can generalize to the
validation samples. Moreover, NSO reduces this validation loss further in Figure 6b.
â€¢Finally, we can see in Figure 6c that our algorithm can indeed produce a more accurate estimate of
the ground truth matrix Xâ‹†, as measured by the Frobenius norm distance between WiWâŠ¤
iandXâ‹†.
â€¢The results for varying learning rates can be found in the bottom panel from Figure 6d to 6f. The
comparative results remain consistent.
6 Discussions and Related Work
Using noise injection during neural network training has appeared in very early studies of machine learning
research (Hinton & Van Camp, 1993; An, 1996). Graves (2011) test a variety of variational inference ap-
proaches with different prior and posterior distributions with recurrent neural networks. Cohen et al. (2019)
examine the use of randomized smoothing (with different smoothing distributions) against different â„“pad-
versaries for certified robustness. Camuto et al. (2020) propose a layer-wise regularization scheme motivated
by adaptation patterns of weights through deeper layers. Yang et al. (2020) show how to turn any classifier
that classifies well under Gaussian noise into a new classifier robust to adversarial perturbation under the
â„“2norm. One of the implications of their work is that smoothing with Gaussian noise naturally confers
adversarial robustness in the â„“2norm. Bisla et al. (2022) conduct an extensive empirical study to explore the
connection between sharpness and generalization for training neural networks. Orvieto et al. (2023) analyze
Taylorâ€™s expansion of the stochastic objective after noise injection, examining the induced regularization in
various neural network training settings, and find that layer-wise perturbation can improve generalization.
There is also a line of work on Hessian and sharpness in the edge of stability regime during gradient descent
dynamics (Cohen et al., 2021). In particular, the edge of stability refers to scenarios where the learning
rate goes out of bounds beyond the Lipschitz continuity of a function, which is inversely proportional to
the largest eigenvalue of the Hessian matrix. Long & Bartlett (2024) identify the edge of stability regime
15Published in Transactions on Machine Learning Research (09/2024)
0 1000 2000 3000 4000 5000
Number of epochs0.000.010.020.030.040.050.06Training MSEGD
NSO ( k= 1)
WP -GD
(a) Training loss
0 1000 2000 3000 4000 5000
Number of epochs10âˆ’310âˆ’210âˆ’1100Validation MSEGD
NSO ( k= 1)
WP -GD (b) Validation loss
0 1000 2000 3000 4000 5000
Number of epochs10âˆ’310âˆ’210âˆ’1100Normalized DistanceGD
NSO ( k= 1)
WP GD (c) Normalized distance
0 5000 10000 15000 20000
Number of epochs10âˆ’310âˆ’210âˆ’1100101Training MSENSO ( k= 1)
WP -GD
(d) Training loss
0 5000 10000 15000 20000
Number of epochs10âˆ’310âˆ’210âˆ’1100101Validation MSENSO ( k= 1)
WP -GD (e) Validation loss
0 5000 10000 15000 20000
Number of epochs10âˆ’310âˆ’210âˆ’1100101Normalized DistanceNSO ( k= 1)
WP -GD (f) Normalized distance
Figure 6: Comparing the training loss, validation loss, and the normalized Frobenius norm distance, i.e.,
âˆ¥WiWâŠ¤
iâˆ’Xâ‹†âˆ¥2
F
âˆ¥Xâ‹†âˆ¥2
F, between GD, our approach (NSO), and weight-perturbed (WP) GD (which computes the full
gradient as opposed to the stochastic gradient). For the top panel, the learning rate is fixed at 0.0025for all
the runs. For the bottom panel, the learning rate is set at 0.0001.Ïƒis set as 0.008for WP-GD and NSO.
Also, we trained sufficiently long until the loss curves fully converged.
for the SAM algorithm, highlighting the differences of these regimes between SAM and gradient descent.
Agarwala & Dauphin (2023) present a detailed study of the gradient dynamics of SAM, documenting various
respects of this algorithm. They first analyze the full-batch gradient descent with unnormalized SAM in a
quadratic regression model. This analysis suggests that at initialization, full-batch SAM presents limited
suppression of the largest eigenvalue of the Hessian matrix. They also show that as the batch size decreases,
the regularization of SAM becomes stronger. This work underscores the intricate dynamics of SAM due
to its connection to the min-max problem, which is computationally intractable (Daskalakis et al., 2021).
Dauphin et al. (2024) provide an in-depth comparison between SAM and weight noise by examining the
structure of the Hessian during training. Our results in Section 2.1, which show that weight noise remains
ineffective (for fine-tuning), are consistent with the findings of this work. Wu et al. (2020) study the structure
of the Hessian and conduct experiments on how the Hessian structure changes based on architecture and
the training method.
Randomized smoothing has been studied in stochastic optimization under various contexts, for instance, es-
timating gradients in zeroth-order optimization (Duchi et al., 2015), and for nonsmooth convex optimization
problems (Duchi et al., 2012). In particular, Duchi et al. (2012) analyze the convergence rates of stochastic
optimization algorithms and examine a convolution-based smoothing technique for nonsmooth stochastic op-
timization problems by drawing stochastic gradient samples from the smoothed problem with an appropriate
choice of smoothing density. They show that with the ability to issue several queries to the stochastic oracle,
the original problem can be solved with faster convergence rates than a simple stochastic oracle. Besides,
recent research has investigated the query complexity of finding stationary points of nonconvex functions
(Carmon et al., 2020; Arjevani et al., 2023). These results provide a fine-grained characterization of the
complexity of iterative methods under different orders of gradient oracles.
16Published in Transactions on Machine Learning Research (09/2024)
The findings from our work suggest several avenues that seem ripe for future work. Can recent advancements
in optimization be used to design better noise injection algorithms with faster convergence? Can we better
understand the effect of noise injection on the Hessian during training (e.g., in tensor regression where saddle
points are known to exist (Li et al., 2020))? In particular, our work highlights the need for more accurate
measurements to understand the learning mechanisms of complex models.
7 Conclusion and Limitations
This paper examines the regularization and generalization effects of noise-injection methods for training
neural networks. The study begins by noting that a straightforward implementation of injecting noise into
weight matrices (of a neural network) before computing the gradient in SGD does not perform well in
practice. Thus, an alternative, two-point noise injection scheme is proposed and is shown to be effective
through extensive experiments. In particular, this new algorithm can be used to regularize the Hessian
and improve generalization. The results are tested on fine-tuning, pretraining, and instruction tuning. As
a complement, a PAC-Bayes generalization bound is provided to support the rationale of this approach.
Finally, this paper presents a detailed convergence analysis of the proposed algorithm.
Limitations: In Theorem 2.1, we have shown that the generalization error of a training algorithm can be
bounded by the trace of the Hessian of the loss matrix, scaled by the distance of the hypothesis space. Notice
that this result applies to both Algorithm 1 (NSO) and the naive noise injection algorithm (WP-SGD). As
shown in Figure 3, this result can provide a descriptive measure to explain different algorithms. Since the
Hessian measurements can be used on both algorithms, they can only distinguish one from another after
taking the measurements from the data. Thus, our generalization theory should be interpreted with this
data-dependent lens in mind. We hope future work could work on addressing such limitations, along with
designing more principled optimization algorithms for training neural networks.
Acknowledgements
H. Z. would like to thank Huy Nguyen, Zhiyuan Li, and Guanghui Lan for the discussions and for pointing
out several references during various stages of this work. The authors would also like to thank the anonymous
reviewers and the action editor for their constructive feedback. We acknowledge financial support from NSF
award IIS-2412008.
References
Atish Agarwala and Yann Dauphin. Sam operates far from home: eigenvalue regularization as a dynamical
phenomenon. In International Conference on Machine Learning , pp. 152â€“168. PMLR, 2023. 8, 16
Pierre Alquier. User-friendly introduction to pac-bayes bounds. arXiv preprint arXiv:2110.11216 , 2021. 3, 7
Guozhong An. The effects of adding noise during backpropagation training on a generalization performance.
Neural computation , 8(3):643â€“674, 1996. 2, 15
Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization.
InICML, 2022. 1
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth. Lower
bounds for non-convex stochastic optimization. Mathematical Programming , 199(1):165â€“214, 2023. 16
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets
via a compression approach. In ICML, 2018. 7
Peter L Bartlett, Philip M Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization:
Bouncing across ravines and drifting towards wide minima. Journal of Machine Learning Research , 24
(316):1â€“36, 2023. 1, 13
17Published in Transactions on Machine Learning Research (09/2024)
Devansh Bisla, Jing Wang, and Anna Choromanska. Low-pass filtering sgd for recovering flat optima in the
deep learning optimization landscape. In International Conference on Artificial Intelligence and Statistics ,
pp. 8299â€“8339. PMLR, 2022. 15
Alexander Camuto, Matthew Willetts, Umut Simsekli, Stephen J Roberts, and Chris C Holmes. Explicit
regularisation in gaussian noise injections. Advances in Neural Information Processing Systems , 33:16603â€“
16614, 2020. 15
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary points
i.Mathematical Programming , 184(1-2):71â€“120, 2020. 3, 16
Olivier Catoni. Pac-bayesian supervised classification: the thermodynamics of statistical learning. arXiv
preprint arXiv:0712.0248 , 2007. 3, 7
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.
Ininternational conference on machine learning , pp. 1310â€“1320. PMLR, 2019. 15
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural
networks typically occurs at the edge of stability. ICLR, 2021. 15
Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained min-
max optimization. In Symposium on Theory of Computing , 2021. 16
Yann N Dauphin, Atish Agarwala, and Hossein Mobahi. Neglected hessian component explains mysteries in
sharpness regularization. arXiv preprint arXiv:2401.10809 , 2024. 2, 16
Yoel Drori and Ohad Shamir. The complexity of finding stationary points with stochastic gradient descent.
InICML, 2020. 3, 13
John C Duchi, Peter L Bartlett, and Martin J Wainwright. Randomized smoothing for stochastic optimiza-
tion.SIAM Journal on Optimization , 22(2):674â€“701, 2012. 2, 4, 16
John C Duchi, Michael I Jordan, Martin J Wainwright, and Andre Wibisono. Optimal rates for zero-order
convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory ,
2015. 2, 5, 12, 16
Gintare Karolina Dziugaite and Daniel Roy. Computing nonvacuous generalization bounds for deep (stochas-
tic) neural networks with many more parameters than training data. UAI, 2017. 8
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for
efficiently improving generalization. ICLR, 2021. 1, 2, 3, 8
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM Journal on Optimization , 23(4):2341â€“2368, 2013. 3, 12, 13, 27
Henry Gouk, Timothy M Hospedales, and Massimiliano Pontil. Distance-based regularisation of deep net-
works for fine-tuning. In Ninth International Conference on Learning Representations 2021 , 2022. 3,
11
Alex Graves. Practical variational inference for neural networks. Advances in neural information processing
systems, 24, 2011. 2, 15
Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. 4
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the description
length of the weights. In Proceedings of the sixth annual conference on Computational learning theory , pp.
5â€“13, 1993. 2, 15
Sepp Hochreiter and JÃ¼rgen Schmidhuber. Flat minima. Neural computation , 9(1):1â€“42, 1997. 1
18Published in Transactions on Machine Learning Research (09/2024)
PavelIzmailov, DmitriiPodoprikhin, TimurGaripov, DmitryVetrov, andAndrewGordonWilson. Averaging
weights leads to wider optima and better generalization. UAI, 2018. 1
Haotian Ju, Dongyue Li, and Hongyang R Zhang. Robust fine-tuning of deep neural networks with hessian-
based generalization guarantees. ICML, 2022. 2, 4
Haotian Ju, Dongyue Li, Aneesh Sharma, and Hongyang R Zhang. Generalization in graph neural networks:
Improved pac-bayesian bounds on graph diffusion. AISTATS , 2023. 2
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang.
On large-batch training for deep learning: Generalization gap and sharp minima. ICLR, 2017. 1
Guanghui Lan. First-order and stochastic optimization methods for machine learning , volume 1. Springer,
2020. 3
Dongyue Li and Hongyang R Zhang. Improved regularization and robustness for fine-tuning in neural
networks. Advances in Neural Information Processing Systems , 34:27249â€“27262, 2021. 4, 7
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix
sensing and neural networks with quadratic activations. In Conference On Learning Theory , pp. 2â€“47.
PMLR, 2018. 14
Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural networks
beyond ntk. In Conference on learning theory , pp. 2613â€“2682. PMLR, 2020. 17
Yong Liu, Siqi Mai, Minhao Cheng, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Random sharpness-
aware minimization. Advances in Neural Information Processing Systems , 2022. 8
Philip M Long and Peter L Bartlett. Sharpness-aware minimization and the edge of stability. Journal of
Machine Learning Research , 25(179):1â€“20, 2024. 15
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual
classification of aircraft. arXiv preprint arXiv:1306.5151 , 2013. 4
David McAllester. A pac-bayesian tutorial with a dropout bound. arXiv preprint arXiv:1307.2118 , 2013. 3,
7, 21
Vaishnavh Nagarajan and Zico Kolter. Deterministic pac-bayesian generalization bounds for deep networks
via generalizing noise-resilience. ICLR, 2020. 2, 7
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Representations ,
2018. 7
Antonio Orvieto, Anant Raj, Hans Kersting, and Francis Bach. Explicit regularization in overparametrized
models via noise injection. AISTATS , 2023. 2, 15
Samiksha Pachade, Prasanna Porwal, Dhanshree Thulkar, Manesh Kokare, Girish Deshmukh, Vivek Sa-
hasrabuddhe, Luca Giancardo, GwenolÃ© Quellec, and Fabrice MÃ©riaudeau. Retinal fundus multi-disease
image dataset (rfmid): A dataset for multi-disease detection research. Data, 6(2):14, 2021. 4
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML, 2021. 12
Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix
equations via nuclear norm minimization. SIAM review , 52(3):471â€“501, 2010. 14, 25
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algorithms .
Cambridge university press, 2014. 6
19Published in Transactions on Machine Learning Research (09/2024)
Nathan Srebro and Adi Shraibman. Rank, trace-norm and max-norm. In International conference on
computational learning theory , pp. 545â€“560. Springer, 2005. 6
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48. Cambridge
University Press, 2019. 24, 25
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How does sharpness-aware minimization minimize sharpness?
ICLR, 2023. 1, 3
Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-
cos, HongseokNamkoong, AliFarhadi, YairCarmon, SimonKornblith, andLudwigSchmidt. Modelsoups:
averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In
International conference on machine learning , pp. 23965â€“23998. PMLR, 2022. 1, 4
YikaiWu, XingyuZhu, ChenweiWu, AnnieWang, andRongGe. Dissectinghessian: Understandingcommon
structure of hessian in neural networks. arXiv preprint arXiv:2010.04261 , 2020. 16
Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing
of all shapes and sizes. In International Conference on Machine Learning , pp. 10693â€“10705. PMLR, 2020.
15
Tong Zhang. Mathematical analysis of machine learning algorithms . Cambridge University Press, 2023. 3
20Published in Transactions on Machine Learning Research (09/2024)
A Omitted Proofs from Section 2
We state a few standard notations. Given two matrices X,Yhaving the same dimension, let âŸ¨X,YâŸ©=
Tr[XâŠ¤Y]denote the matrix inner product of XandY. Letâˆ¥Xâˆ¥2denote the spectral norm (largest singular
value) ofX, and letâˆ¥Xâˆ¥Fdenote the Frobenius norm of X. We use the big-O notation f(x) =O(g(x))to
indicate that there exists a fixed constant Cindependent of xsuch thatf(x)â‰¤CÂ·g(x)for large enough x.
A.1 Proof of the PAC-Bayes Bound
We will use the following PAC-Bayes bound. For reference, see, e.g., Theorem 2, McAllester (2013).
Theorem A.1. Suppose the loss function â„“(fW(x),y)lies in a bounded range [0,C]given anyxâˆˆXwith
labely. For anyÎ²âˆˆ(0,1)andÎ´âˆˆ(0,1), with probability at least 1âˆ’Î´, the following holds:
LQ(W)â‰¤1
Î²Ë†LQ(W) +C/parenleftbig
KL(Q||P ) + log1
Î´/parenrightbig
2Î²(1âˆ’Î²)n. (18)
This result provides flexibility in setting Î². Our results will set Î²to balance the perturbation error of Qand
the KL divergence between PandQ. We will need the KL divergence between the prior Pand the posterior
Qin the PAC-Bayesian analysis. This is stated in the following result.
Proposition A.2. SupposeP=N(X,Î£)andQ=N(Y,Î£)are both Gaussian distributions with mean
vectors given by XâˆˆRp,YâˆˆRp, and population covariance matrix Î£âˆˆRpÃ—p. The KL divergence between
PandQis equal to
KL(Q||P ) =1
2(Xâˆ’Y)âŠ¤Î£âˆ’1(Xâˆ’Y).
Specifically, if Î£ =Ïƒ2Idp, then the above simplifies to
KL(Q||P ) =âˆ¥Xâˆ’Yâˆ¥2
2
2Ïƒ2.
We will use Taylorâ€™s expansion on the perturbed loss. This is stated precisely as follows.
Claim A.3. LetfWbe twice-differentiable, parameterized by weight vector WâˆˆRp. LetUâˆˆRpbe another
vector with dimension p. For anyWandU, the following identity holds
â„“(fW+U(x),y) =â„“(fW(x),y) +UâŠ¤âˆ‡â„“(fW(x),y) +UâŠ¤[âˆ‡2â„“(fW(x),y)]U+R2(â„“(fW(x),y)),
whereR2(â„“(fW(x),y)))is a second-order error term in Taylorâ€™s expansion.
Proof.The proof follows by the fact that â„“â—¦fWis twice-differentiable. Let Î·âˆˆRpbe a vector with the same
dimension as WandUfrom the mean value theorem. There must exist an Î·betweenWandU+Wsuch
that the following equality holds:
R2(â„“(fW(x),y)) =UâŠ¤/parenleftï£¬ig
âˆ‡2[â„“(fÎ·(x),y)]âˆ’âˆ‡2[â„“(fW(x),y)]/parenrightï£¬ig
U.
This completes the proof of the claim.
We provide Taylorâ€™s expansion of â„“Qâˆ’â„“based on the above.
Lemma A.4. In the setting of Theorem 2.1, suppose each parameter is perturbed by an independent noise
drawn from N(0,Ïƒ2). Letâ„“Q(fW(x),y)be the perturbed loss with noise perturbation injection vector on W.
There exist some fixed value C1that do not grow with nand1/Î´such that
/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ„“Q(fW(x),y)âˆ’â„“(fW(x),y)âˆ’1
2Ïƒ2Tr/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ‰¤C1Ïƒ3.
21Published in Transactions on Machine Learning Research (09/2024)
Proof.We take the expectation over Ufor both sides of the equation in Claim A.3. The result becomes
E
U[â„“(fW+U(x),y)] =E
U/bracketleftbig
â„“(fW(x),y) +UâŠ¤âˆ‡â„“(fW(x),y) +UâŠ¤âˆ‡2[â„“(fW(x),y)]U+R2(â„“(fW(x),y))/bracketrightbig
.
Then, we use the perturbation distribution QonEU[â„“(fW+U(x),y)], and get
â„“Q(fW(x),y) =E
U[â„“(fW(x),y)] +E
U/bracketleftbig
UâŠ¤âˆ‡â„“(fW(x),y)/bracketrightbig
+E
U/bracketleftbig
UâŠ¤âˆ‡2[â„“(fW(x),y)]U/bracketrightbig
+E
U[R2(â„“(fW(x),y))].
Since E[U] = 0, the first-order term will be zero in expectation. The second-order term becomes equal to
E
U/bracketleftbig
UâŠ¤[âˆ‡2â„“(fW(x),y)]U/bracketrightbig
=Ïƒ2Tr/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig
. (19)
The expectation of the error term R2(â„“(fW(x),y))be
E
U[R2(â„“(fW(x),y))] =E
U/bracketleftbig
UâŠ¤/parenleftbig
âˆ‡2[â„“(fÎ·(x),y)]âˆ’âˆ‡2[â„“(fW(x),y)]/parenrightbig
U/bracketrightbig
â‰¤E
U/bracketleftï£¬ig
âˆ¥Uâˆ¥2
2Â·/vextenddouble/vextenddoubleâˆ‡2[â„“(fÎ·(x),y)]âˆ’âˆ‡2[â„“(fW(x),y)]/vextenddouble/vextenddouble
F/bracketrightï£¬ig
â‰²E
U/bracketleftï£¬ig
âˆ¥Uâˆ¥2
2Â·C1âˆ¥Uâˆ¥2/bracketrightï£¬ig
â‰²C1Ïƒ3.
Thus, the proof is complete.
The last piece we will need is the uniform convergence of the Hessian operator. The result uses the fact that
the Hessian matrix is Lipschitz continuous.
Lemma A.5. In the setting of Theorem 2.1, there exist some fixed values C2,C3that do not grow with
nand1/Î´, such that with probability at least 1âˆ’Î´for anyÎ´ >0, over the randomness of the ntraining
examples, we have
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1âˆ‡2[â„“(fW(xi),yi)]âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤C2/radicalbig
log(C3n/Î´)âˆšn. (20)
The proof will be deferred to Section A.1.2. With these results ready, we will provide proof of the Hessian-
based generalization bound.
A.1.1 Proof of Theorem 2.1
Proof of Theorem 2.1. First, we separate the gap of L(W)and1
Î²Ë†L(W)into three parts:
L(W)âˆ’1
Î²Ë†L(W) =L(W)âˆ’LQ(W) +LQ(W)âˆ’1
Î²Ë†LQ(W) +1
Î²Ë†LQ(W)âˆ’1
Î²Ë†L(W).
By Lemma A.4, we can bound the difference between L(W)andLQ(W)by the Hessian trace plus an error:
L(W)âˆ’1
Î²Ë†L(W)â‰¤âˆ’ E
(x,y)âˆ¼D/bracketleftbiggÏƒ2
2Tr/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/bracketrightbigg
+C1Ïƒ3+/parenleftï£¬ig
LQ(W)âˆ’1
Î²Ë†LQ(W)/parenrightï£¬ig
+1
Î²/parenleftï£¬ig1
nn/summationdisplay
i=1Ïƒ2
2Tr/bracketleftbig
âˆ‡2[â„“(fW(xi),yi)]/bracketrightbig
+C1Ïƒ3/parenrightï£¬ig
.
After re-arranging the terms, we can get the following:
L(W)âˆ’1
Î²Ë†L(W)â‰¤âˆ’ E
(x,y)âˆ¼D/bracketleftbiggÏƒ2
2Tr/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/bracketrightbigg
+1
nÎ²n/summationdisplay
i=1Ïƒ2
2Tr/bracketleftbig
âˆ‡2[â„“(fW(xi),yi)]/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
E1
+1 +Î²
Î²C1Ïƒ3+LQ(W)âˆ’1
Î²Ë†LQ(W)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
E2. (21)
22Published in Transactions on Machine Learning Research (09/2024)
We will examine E1by separating it into two parts:
E1=1
Î²/parenleftï£¬igg
1
nn/summationdisplay
i=1Ïƒ2
2Tr/bracketleftbig
âˆ‡2[â„“(fË†W(xi),yi)]/bracketrightbig
âˆ’E
(x,y)âˆ¼D/bracketleftbiggÏƒ2
2Tr/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/bracketrightbigg/parenrightï£¬igg
(22)
+1âˆ’Î²
Î²Ïƒ2
2E
(x,y)âˆ¼D/bracketleftbig
Tr/bracketleftbig
âˆ‡2â„“(fW(x),y)/bracketrightbig/bracketrightbig
. (23)
We can use the uniform convergence result of Lemma A.5 to bound equation (22), leading to:
Ïƒ2
2Î²/parenleftï£¬igg
1
nn/summationdisplay
i=1Tr/bracketleftbig
âˆ‡2â„“(fW(xi),yi)/bracketrightbig
âˆ’E
(x,y)âˆ¼D/bracketleftbig
Tr/bracketleftbig
âˆ‡2â„“(fW(x),y))/bracketrightbig/bracketrightbig/parenrightï£¬igg
â‰¤Ïƒ2
2Î²Â·âˆšpÂ·/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1âˆ‡2[â„“(fW(xi),yi)]âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
F(by Cauchy-Schwarz)
â‰¤Ïƒ2âˆšpÂ·C2/radicalbig
log(C3n/Î´)
2Î²âˆšn. (24)
In particular, the second step also uses the fact that the Hessian matrix is a symmetric pbypmatrix. As
for equation (23), we recall that
Î±:= max
(x,y)âˆ¼DTr/bracketleftbig
âˆ‡2â„“(fW(x),y)/bracketrightbig
.
Combined with equation (24), we have shown that
E1â‰¤Ïƒ2âˆšpÂ·C2/radicalbig
log(C3n/Î´)
2Î²âˆšn+1âˆ’Î²
Î²Ïƒ2
2Â·Î±. (25)
As forE2, we will use the PAC-Bayes bound of Theorem A.1. In particular, we set the prior distribution P
as the distribution of Uand the posterior distribution Qas the distribution of W+U. Thus,
E2â‰¤C/parenleftbig
KL(Q||P ) + log1
Î´/parenrightbig
2Î²(1âˆ’Î²)nâ‰¤C/parenleftï£¬ig
âˆ¥Wâˆ¥2
2
2Ïƒ2+ log1
Î´/parenrightï£¬ig
2Î²(1âˆ’Î²)nâ‰¤C(r2
2Ïƒ2+ logÎ´âˆ’1)
2Î²(1âˆ’Î²)n. (26)
The last step is because âˆ¥Wâˆ¥2â‰¤rby the assumption of the hypothesis space. Combining equations (21),
(25), (26), we claim that with probability at least 1âˆ’2Î´, the following must be true:
L(W)âˆ’1
Î²Ë†L(W)â‰¤Ïƒ2âˆšpÂ·C2/radicalbig
log(C3n/Î´)
2Î²âˆšn+1âˆ’Î²
Î²Ïƒ2
2Î±+1 +Î²
Î²C1Ïƒ3+C(r2
2Ïƒ2+ log1
Î´)
2Î²(1âˆ’Î²)n.(27)
Thus, we will now choose ÏƒandÎ²âˆˆ(0,1)to minimize the term above. In particular, we will set Ïƒas
Ïƒ2=r
1âˆ’Î²/radicalbigg
C
Î±n. (28)
By plugging in Ïƒto equation (27) and re-arranging terms, the gap between L(W)andË†L(W)
Î²becomes:
L(W)âˆ’1
Î²Ë†L(W)â‰¤1
Î²/radicalbigg
CÎ±r2
n+C2/radicalbig
2plog(C3n/Î´)
2Î²âˆšnÏƒ2+1 +Î²
Î²C1Ïƒ3+C
2Î²(1âˆ’Î²)nlog1
Î´.
LetÎ²be a fixed value close to 1and independent of NandÎ´âˆ’1, and letÏµ= (1âˆ’Î²)/Î². We get
L(W)â‰¤(1 +Ïµ)Ë†L(W) + (1 +Ïµ)/radicalbigg
CÎ±r2
n+Î¾,where
Î¾=C2/radicalbig
2plog(C3n/Î´)
2Î²âˆšnÏƒ2+/parenleftï£¬ig
1 +1
Î²/parenrightï£¬ig
C1Ïƒ3+C
2Î²(1âˆ’Î²)nlog1
Î´.
Notice that Î¾is of orderO(nâˆ’3
4+nâˆ’3
4+ log(Î´âˆ’1)nâˆ’1)â‰¤O(log(Î´âˆ’1)nâˆ’3
4). Therefore, we have finished the
proof of equation (6).
23Published in Transactions on Machine Learning Research (09/2024)
Remark A.6. Whenfis strongly convex, the lowest eigenvalue of the Hessian is bounded from below. Once
the algorithm reaches the global minimizer, our result from Theorem 6 can be used to provide a generalization
bound based on the trace of the Hessian. Notice that the noise injection will add some bias to this minimizer,
leading to a sub-optimal empirical loss. To remedy this issue, one can place the regularization of Hessian as
a constraint, similar to how â„“2-regularization can be implemented as a constraint.
A.1.2 Proof of Lemma A.5
In this section, we provide the proof of Lemma A.5, which shows the uniform convergence of the loss Hessian.
Proof of Lemma A.5. LetC,Ïµ >0, and letS={WâˆˆRp:âˆ¥Wâˆ¥2â‰¤C}. There exists an Ïµ-cover ofS
with respect to the â„“2-norm at most max/parenleftï£¬ig/parenleftbig3C
Ïµ/parenrightbigp,1/parenrightï£¬ig
elements; see, e.g., Example 5.8 (Wainwright, 2019).
LetTâŠ†Sdenote the set of this cover. Recall that the Hessian âˆ‡2[â„“(fW(x),y)]isC1-Lipschitz for all
(W+U)âˆˆS,WâˆˆS. Then we have
/vextenddouble/vextenddoubleâˆ‡2[â„“(fW+U(x),y)]âˆ’âˆ‡2[â„“(fW(x),y)]/vextenddouble/vextenddouble
Fâ‰¤C1âˆ¥Uâˆ¥2.
For parameters Î´,Ïµ> 0, letNbe theÏµ-cover ofSwith respect to the â„“2-norm. Define the event
E=/braceleftï£¬ig
âˆ€WâˆˆT,/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1âˆ‡2[â„“(fW(xi),yi)]âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤Î´/bracerightï£¬ig
.
By the matrix Bernstein inequality, we have
Pr[E]â‰¥1âˆ’4Â·|N|Â·pÂ·exp/parenleftbigg
âˆ’nÎ´2
2Î±2/parenrightbigg
.
Next, for any WâˆˆS, we can pick some W+UâˆˆTsuch thatâˆ¥Uâˆ¥2â‰¤Ïµ. We have
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW+U(x),y)]/bracketrightbig
âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤C1âˆ¥Uâˆ¥2â‰¤C1Ïµ
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1âˆ‡2[â„“(fW+U(xj),yj)]âˆ’1
nn/summationdisplay
j=1âˆ‡2[â„“(fW(xj),yj)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤C1âˆ¥Uâˆ¥2â‰¤C1Ïµ.
Therefore, for any WâˆˆS, we obtain:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1âˆ‡2[â„“(fW(xj),yj)]âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤2C1Ïµ+Î´.
We will also set the value of Î´andÏµ. First, set Ïµ=Î´/(2C1)so that conditional on E,
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1âˆ‡2[â„“(fW(xj),yj)]âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤2Î´.
The eventEhappens with a probability of at least:
1âˆ’4|T|pÂ·exp/parenleftbigg
âˆ’nÎ´2
2Î±2/parenrightbigg
= 1âˆ’4pÂ·exp/parenleftbigg
log|T|âˆ’nÎ´2
2Î±2/parenrightbigg
.
We have log|T|â‰¤plog(3B/Ïµ) =plog(6CC1/Î´). If we set
Î´=/radicalbigg
4pÎ±2log(3Ï„CC 1n/Î±)
n
24Published in Transactions on Machine Learning Research (09/2024)
so that log(3Ï„CC 1n/Î±)â‰¥1(becausenâ‰¥eÎ±
3C1andÏ„â‰¥1), then we get
plog(6CC1/Î´)âˆ’nÎ´2/(2Î±2) =plog/parenleftï£¬igg
6CC1âˆšn/radicalbig
4pÎ±2log(3Ï„CC 1n/Î±)/parenrightï£¬igg
âˆ’2plog (3Ï„CC 1n/Î±)
=plog/parenleftï£¬igg
3CC1âˆšn
Î±/radicalbig
plog(3Ï„CC 1n/Î±)/parenrightï£¬igg
âˆ’2plog (3Ï„CC 1n/Î±)
â‰¤plog (3Ï„CC 1n/Î±)âˆ’2plog (3Ï„CC 1n/Î±)(Ï„â‰¥1,log(3Ï„CC 1n/Î±)â‰¥1)
=âˆ’plog (3Ï„CC 1n/Î±)â‰¤âˆ’plog(eÏ„). (3CC1n/Î±â‰¥e)
Therefore, with a probability greater than
1âˆ’4|N|pÂ·exp(âˆ’nÎ´2/(2Î±2))â‰¥1âˆ’4p(eÏ„)âˆ’p,
the following estimate holds:
/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
j=1âˆ‡2[â„“(fW(xj),yj)]âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤/radicalbigg
16pÎ±2log(3Ï„CC 1n/Î±)
n.
DenoteÎ´â€²= 4p(eÏ„)âˆ’p,C2= 4Î±âˆšp, andC3= 12pCC 1/(eÎ±). With probability greater than 1âˆ’Î´â€², the final
result is:/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
nn/summationdisplay
i=1âˆ‡2[â„“(fW(xi),yi)]âˆ’E
(x,y)âˆ¼D/bracketleftbig
âˆ‡2[â„“(fW(x),y)]/bracketrightbig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
Fâ‰¤C2/radicalbigg
log(C3n/Î´â€²)
n.
This completes the proof of Lemma A.5.
A.2 Proof of Proposition 5.1
Proof of Proposition 5.1. We can calculate the gradient as
âˆ‡Ë†L(W) =1
nn/summationdisplay
i=1(âŸ¨Ai,WWâŠ¤âŸ©âˆ’yi)AiW. (29)
For a particular entry Wj,kofW, for any 1â‰¤j,kâ‰¤d, the derivative of the gradient with respect to Wj,kis
1
nn/summationdisplay
i=1/parenleftbigg
[AiW]j,kAiW+/parenleftï£¬ig
âŸ¨Ai,WWâŠ¤âŸ©âˆ’yi/parenrightï£¬igâˆ‚(AiW)
âˆ‚Wj,k/parenrightbigg
. (30)
When Ë†L(W)is zero, the second term of equation (30) above must be zero, because âŸ¨Ai,WWâŠ¤âŸ©is equal to
yi, for anyi= 1,...,n.
We use the assumption that Aiis a random Gaussian matrix, in which every entry is drawn from a normal
distribution with mean zero and variance one. Notice that the expectation of âˆ¥AiWâˆ¥2
Fsatisfies:
E/bracketleftï£¬ig
âˆ¥AiWâˆ¥2
F/bracketrightï£¬ig
=E/bracketleftbig
Tr/bracketleftbig
WâŠ¤AâŠ¤
iAiW/bracketrightbig/bracketrightbig
= Tr/bracketleftbig
WâŠ¤(dÂ·IddÃ—d)WâŠ¤/bracketrightbig
=dÂ·Tr/bracketleftbig
WâŠ¤W/bracketrightbig
=dâˆ¥Wâˆ¥2
F.
Thus, by concentration inequality for Ï‡2random variables (e.g., Wainwright (2019, equation (2.19))), the
following holds for any 0<Ïµ< 1,
Pr/bracketleftï£¬igg/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
nn/summationdisplay
i=1âˆ¥AiWâˆ¥2
Fâˆ’dâˆ¥Wâˆ¥2
F/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ‰¥Ïµdâˆ¥Wâˆ¥2
F/bracketrightï£¬igg
â‰¤2 exp/parenleftï£¬ig
âˆ’nÏµ2
8/parenrightï£¬ig
. (31)
This implies that Ïµmust be smaller than O(nâˆ’1/2)with high probability. As a result, the average of âˆ¥AiWâˆ¥2
F
must bedâˆ¥Wâˆ¥2
Fplus some deviation error that scales with nâˆ’1/2times the expectation.
By Theorem 3.2, Recht et al. (2010), the minimum Frobenius norm ( âˆ¥Wâˆ¥2
F) solution that satisfies Ë†L(W) = 0
(for Gaussian random matrices) is precisely Uâ‹†. Thus, we conclude that equation (17) holds.
25Published in Transactions on Machine Learning Research (09/2024)
B Omitted Proofs from Section 4
B.1 Proof of Proposition 4.2
Recall that each iteration involves two sources of randomness stemming from gzand{U(j)
i}k
j=1, respectively.
Let us define
Î´i=1
2kk/summationdisplay
j=1/parenleftbig
âˆ‡f/parenleftbig
Wi+U(j)
i/parenrightbig
+âˆ‡f/parenleftbig
Wiâˆ’U(j)
i/parenrightbig/parenrightbig
âˆ’âˆ‡F(Wi),
Î¾i=1
2kk/summationdisplay
j=1/parenleftbig
G(j)
iâˆ’âˆ‡f/parenleftbig
Wi+U(j)
i/parenrightbig
âˆ’âˆ‡f/parenleftbig
Wiâˆ’U(j)
i/parenrightbig/parenrightbig
,
fori= 0,...,Tâˆ’1. One can see that both Î´iandÎ¾ihave mean zero. The former is by the symmetry of P.
The latter is because gzis unbiased under Assumption 4.1. The following result gives their variance.
Lemma B.1. In the setting of Proposition 4.2, for any i= 1,...,T, we have
E/bracketleftï£¬ig
âˆ¥Î¾iâˆ¥2/bracketrightï£¬ig
â‰¤Ïƒ2
kandE/bracketleftï£¬ig
âˆ¥Î´iâˆ¥2/bracketrightï£¬ig
â‰¤C2H(P)
k. (32)
The last step uses smoothness to show that âˆ¥âˆ‡F(Wt)âˆ¥keeps reducing.
Proof.Let us bound the variance of Î´iandÎ¾ifori= 0,1,...,Tâˆ’1. First, we see that
E
U1
i,...,Uk
i/bracketleftï£¬ig
âˆ¥Î´iâˆ¥2/bracketrightï£¬ig
=E
U1
i,...,Uk
iï£®
ï£¯ï£°/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2kk/summationdisplay
j=1/parenleftï£¬ig
âˆ‡f(Wi+Uj
i) +âˆ‡f(Wiâˆ’Uj
i)âˆ’2âˆ‡F(Wi)/parenrightï£¬ig/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble2ï£¹
ï£ºï£»
=1
k2k/summationdisplay
j=1E
Uj
i/bracketleftï£¬igg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftï£¬ig
âˆ‡f(Wi+Uj
i) +âˆ‡f(Wiâˆ’Uj
i)âˆ’2âˆ‡F(Wi)/parenrightï£¬ig/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightï£¬igg
(33)
=1
kE
U1
i/bracketleftï£¬igg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftï£¬ig
âˆ‡f(Wi+U1
i) +âˆ‡f(Wiâˆ’U1
i)/parenrightï£¬ig
âˆ’âˆ‡F(Wi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightï£¬igg
(34)
where in the second line we use that Uj1
iandUj2
iare independent when j1Ì¸=j2, in the last line we use fact
thatU1
i,...,Uk
iare identically distributed. In the second step, we use the fact that for two independent
random variables U,V, and any continuous functions h(U),g(V),h(U)andg(V)are still independent (recall
thatfis continuous since it is twice-differentiable). We include a short proof of this fact for completeness.
IfUandVare independent, we have Pr[UâˆˆA,VâˆˆB] = Pr[UâˆˆA]Â·Pr[VâˆˆB], for anyA,BâˆˆBorel (R).
Thus, ifhandgare continuous functions, we obtain
Pr[h(U)âˆˆA,g(V)âˆˆB] = Pr[Uâˆˆhâˆ’1(A),Vâˆˆgâˆ’1(B)]
= Pr[Uâˆˆhâˆ’1(A)]Â·Pr[Vâˆˆgâˆ’1(B)] = Pr[h(U)âˆˆA]Â·Pr[g(V)âˆˆB].
Thus, we have shown that
E/bracketleftï£¬ig
âˆ¥Î´iâˆ¥2/bracketrightï£¬ig
=1
kE
Uâˆ¼P/bracketleftï£¬igg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftï£¬ig
âˆ‡f(Wi+U) +f(Wiâˆ’U)/parenrightï£¬ig
âˆ’âˆ‡F(Wi)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightï£¬igg
. (35)
Next, we deal with the variance of the two-point stochastic gradient. We will show that
E
U/bracketleftï£¬igg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftï£¬ig
âˆ‡f(W+U) +âˆ‡f(Wâˆ’U)/parenrightï£¬ig
âˆ’âˆ‡F(W)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightï£¬igg
â‰¤C2H(P). (36)
26Published in Transactions on Machine Learning Research (09/2024)
We mainly use the Lipschitz continuity of the gradient of F. The left-hand side of equation (36) is equal to
E
U/bracketleftï£¬igg/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftï£¬ig
âˆ‡f(W+U)âˆ’âˆ‡F(W)/parenrightï£¬ig
+1
2/parenleftï£¬ig
âˆ‡f(Wâˆ’U)âˆ’âˆ‡F(W)/parenrightï£¬ig/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightï£¬igg
â‰¤E
U/bracketleftbigg1
2âˆ¥âˆ‡f(W+U)âˆ’âˆ‡F(W)âˆ¥2+1
2âˆ¥âˆ‡f(Wâˆ’U)âˆ’âˆ‡F(W)âˆ¥2/bracketrightbigg
(by Cauchy-Schwartz)
=1
2E
U/bracketleftï£¬ig
âˆ¥âˆ‡f(W+U)âˆ’âˆ‡F(W)âˆ¥2/bracketrightï£¬ig
(by symmetry of Psince it has mean zero)
=1
2E
U/bracketleftï£¬igg/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE
Uâ€²âˆ¼P[âˆ‡f(W+U)âˆ’âˆ‡f(W+Uâ€²)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble2/bracketrightï£¬igg
â‰¤1
2E
U/bracketleftbigg
E
Uâ€²âˆ¼P/bracketleftï£¬ig
âˆ¥âˆ‡f(W+U)âˆ’âˆ‡f(W+Uâ€²)âˆ¥2/bracketrightï£¬ig/bracketrightbigg
â‰¤1
2E
U,Uâ€²/bracketleftï£¬ig
C2âˆ¥Uâˆ’Uâ€²âˆ¥2/bracketrightï£¬ig
=1
2C2E
U,Uâ€²/bracketleftï£¬ig
âˆ¥Uâˆ¥2+âˆ¥Uâ€²âˆ¥2/bracketrightï£¬ig
=C2H(P) (by equation (38))
As for the variance of Î¾i, we note that U(1)
i,...,U(j)
iare all independent from each other. Therefore,
E/braceleftbig
U(j)
i,z(j)
i/bracerightbigk
j=1/bracketleftï£¬ig
âˆ¥Î¾iâˆ¥2/bracketrightï£¬ig
=1
4kE
U,z/bracketleftï£¬ig
âˆ¥gz(W+U)âˆ’âˆ‡f(W+U) +gz(Wâˆ’U)âˆ’f(Wâˆ’U)âˆ¥2/bracketrightï£¬ig
â‰¤1
2kE
U,z/bracketleftï£¬ig
âˆ¥gz(W+U)âˆ’âˆ‡f(W+U)âˆ¥2+âˆ¥gz(Wâˆ’U)âˆ’âˆ‡f(Wâˆ’U)âˆ¥2/bracketrightï£¬ig
â‰¤Ïƒ2
k.
The first step uses the fact that both gz(Â·)andf(Â·)are continuous functions The second step above uses
Cauchy-Schwartz inequality. The last step uses the variance bound of gz(Â·), Thus, the proof is finished.
In the next step, we use a result from Theorem 2.1, Ghadimi & Lan (2013). Our proof follows from their
work, but we deal with some extra technical details related to the noise injection.
Lemma B.2 (Slightly adapted from Theorem 2.1, Ghadimi & Lan (2013)) .In the setting of Proposition 4.2,
for anyÎ·0,Â·Â·Â·,Î·Tâˆ’1less thanCâˆ’1and a random variable according to a distribution Pr[t=j] =Î·j/summationtextTâˆ’1
i=0Î·i,
for anyj= 0,...,Tâˆ’1, the following holds:
E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¤2C/summationtextTâˆ’1
i=0Î·iD2+C/summationtextTâˆ’1
i=0Î·2
i/parenleftbig
E/bracketleftï£¬ig
âˆ¥Î´iâˆ¥2/bracketrightï£¬ig
+E/bracketleftï£¬ig
âˆ¥Î¾iâˆ¥2/bracketrightï£¬ig/parenrightbig
/summationtextTâˆ’1
i=0Î·i. (37)
Proof.First, let us show that âˆ‡FisC-Lipschitz continuous. To see this, we apply the Lipschitz condition
of the gradient inside the expectation of F(W). For anyW1,W2âˆˆRd, by definition,
âˆ¥âˆ‡F(W1)âˆ’âˆ‡F(W2)âˆ¥=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleâˆ‡E
Uâˆ¼P[f(W1+U)]âˆ’âˆ‡E
Uâˆ¼P[f(W2+U)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleE
Uâˆ¼P[âˆ‡f(W1+U)âˆ’âˆ‡f(W2+U)]/vextenddouble/vextenddouble/vextenddouble/vextenddouble
â‰¤E
Uâˆ¼P[âˆ¥âˆ‡f(W1+U)âˆ’âˆ‡f(W2+U)âˆ¥]â‰¤Câˆ¥W1âˆ’W2âˆ¥.
Sinceâˆ‡F(W)isC-Lipschitz continuous, we have the following domination inequality:
|F(W2)âˆ’F(W1)âˆ’âŸ¨âˆ‡F(W1),W2âˆ’W1âŸ©|â‰¤C
2âˆ¥W2âˆ’W1âˆ¥2. (38)
27Published in Transactions on Machine Learning Research (09/2024)
Based on the above inequality, we have
F(Wi+1)â‰¤F(Wi) +âŸ¨âˆ‡F(Wi),Wi+1âˆ’WiâŸ©+C
2Î·2
i/vextenddouble/vextenddouble/vextenddouble/vextenddouble1
2/parenleftï£¬ig
âˆ‡f(Wi+Ui) +âˆ‡f(Wiâˆ’Ui)/parenrightï£¬ig
+Î¾i/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
=F(Wi)âˆ’Î·iâŸ¨âˆ‡F(Wi),Î´i+Î¾i+âˆ‡F(Wi)âŸ©+CÎ·2
i
2âˆ¥Î´i+Î¾i+âˆ‡F(Wi)âˆ¥2
=F(Wi)âˆ’/parenleftï£¬ig
Î·iâˆ’CÎ·2
i
2/parenrightï£¬ig
âˆ¥âˆ‡F(Wi)âˆ¥2âˆ’/parenleftï£¬ig
Î·iâˆ’CÎ·2
i/parenrightï£¬ig
âŸ¨âˆ‡F(Wi),Î´i+Î¾iâŸ©+CÎ·2
i
2âˆ¥Î´i+Î¾iâˆ¥2.
Summing up the above inequalities for i= 0,1,...,Tâˆ’1, we obtain
Tâˆ’1/summationdisplay
i=0F(Wi+1)â‰¤Tâˆ’1/summationdisplay
i=0F(Wi)âˆ’Tâˆ’1/summationdisplay
i=0/parenleftï£¬ig
Î·iâˆ’CÎ·2
i
2/parenrightï£¬ig
âˆ¥âˆ‡F(Wi)âˆ¥2
âˆ’Tâˆ’1/summationdisplay
i=0/parenleftï£¬ig
Î·iâˆ’CÎ·2
i/parenrightï£¬ig
âŸ¨âˆ‡F(Wi),Î´i+Î¾iâŸ©+Tâˆ’1/summationdisplay
i=0CÎ·2
i
2âˆ¥Î´i+Î¾iâˆ¥2,
which implies that
Tâˆ’1/summationdisplay
i=0/parenleftï£¬ig
Î·iâˆ’CÎ·2
i
2/parenrightï£¬ig
âˆ¥âˆ‡F(Wi)âˆ¥2â‰¤F(W0)âˆ’F(WT)âˆ’Tâˆ’1/summationdisplay
i=0/parenleftï£¬ig
Î·iâˆ’CÎ·2
i/parenrightï£¬ig
âŸ¨âˆ‡F(Wi),Î´i+Î¾iâŸ©+C
2Tâˆ’1/summationdisplay
i=0Î·2
iâˆ¥Î´i+Î¾iâˆ¥2
â‰¤D2âˆ’Tâˆ’1/summationdisplay
i=0/parenleftï£¬ig
Î·iâˆ’CÎ·2
i/parenrightï£¬ig
âŸ¨âˆ‡F(Wi),Î´i+Î¾iâŸ©+C
2Tâˆ’1/summationdisplay
i=0Î·2
iâˆ¥Î´i+Î¾iâˆ¥2. (39)
where in the last step, we use the fact that
F(W0)âˆ’F(WT)â‰¤F(W0)âˆ’min
WâˆˆRdF(W)â‰¤D2.
For anyt= 0,1,...,Tâˆ’1, notice that as long as 0<Î·tâ‰¤1
C, thenÎ·tâ‰¤2Î·tâˆ’CÎ·2
t.Hence, we have
1
2Tâˆ’1/summationdisplay
t=0Î·tâˆ¥âˆ‡F(Wt)âˆ¥2â‰¤Tâˆ’1/summationdisplay
t=0/parenleftï£¬ig
Î·tâˆ’CÎ·2
t
2/parenrightï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2,
which implies that
1
2Tâˆ’1/summationdisplay
i=0Î·iâˆ¥âˆ‡F(Wi)âˆ¥2â‰¤D2âˆ’Tâˆ’1/summationdisplay
i=0/parenleftï£¬ig
Î·iâˆ’CÎ·2
i/parenrightï£¬ig
âŸ¨âˆ‡F(Wi),Î´i+Î¾iâŸ©+C
2Tâˆ’1/summationdisplay
i=0Î·2
iâˆ¥Î´i+Î¾iâˆ¥2.(40)
Additionally, since Utis drawn from a distribution with mean zero. Hence, by symmetry, we get that
E
Ut[Î´t] =1
2E
Ut[âˆ‡f(Wtâˆ’Ut)âˆ’âˆ‡f(Wt+Ut)] = 0. (41)
Thus, ifwetaketheexpectationover U0,U1,...,UTâˆ’1,Î¾0,Î¾1,...,Î¾Tâˆ’1, then E[âŸ¨âˆ‡F(Wi),Î´i+Î¾iâŸ©] = 0.Recall
thattis a random variable whose probability mass is specified in Lemma B.2. We can write equation (40)
equivalently as (below, we take expectation over all the random variables along the update since Wtis a
function of the previous gradient updates, for each t= 0,1,...,Tâˆ’1, recalling that Pr[t=i] =Î·i/summationtextTâˆ’1
j=0Î·j)
E
t;U0,...,U Tâˆ’1,Î¾0,Î¾1,...,Î¾Tâˆ’1/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
=/summationtextTâˆ’1
i=0Î·iE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wi)âˆ¥2/bracketrightï£¬ig
/summationtextTâˆ’1
i=0Î·i
â‰¤2D2+C/summationtextTâˆ’1
i=0Î·2
iE/bracketleftï£¬ig
âˆ¥Î´i+Î¾iâˆ¥2/bracketrightï£¬ig
/summationtextTâˆ’1
i=0Î·i
=2D2+C/summationtextTâˆ’1
i=0Î·2
i/parenleftbig
E/bracketleftï£¬ig
âˆ¥Î´iâˆ¥2/bracketrightï£¬ig
+E/bracketleftï£¬ig
âˆ¥Î¾iâˆ¥2/bracketrightï£¬ig/parenrightbig
/summationtextTâˆ’1
i=0Î·i.
where we use the fact that Î´iandÎ¾iare independent for any i. Hence, equation (37) is proved.
28Published in Transactions on Machine Learning Research (09/2024)
Based on the above result, we now finish the proof of Proposition 4.2.
Proof of Proposition 4.2. Let the step sizes be a fixed Î·for all epochs. Thus, equation (37) becomes
E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¤2
TÎ·D2+CÎ·
TTâˆ’1/summationdisplay
i=0/parenleftï£¬ig
E/bracketleftï£¬ig
âˆ¥Î´iâˆ¥2/bracketrightï£¬ig
+E/bracketleftï£¬ig
âˆ¥Î¾iâˆ¥2/bracketrightï£¬ig/parenrightï£¬ig
. (42)
By Lemma B.1,
Tâˆ’1/summationdisplay
i=0/parenleftï£¬ig
E/bracketleftï£¬ig
âˆ¥Î´iâˆ¥2/bracketrightï£¬ig
+E/bracketleftï£¬ig
âˆ¥Î¾iâˆ¥2/bracketrightï£¬ig/parenrightï£¬ig
â‰¤TÂ·Ïƒ2+C2H(P)
k. (43)
For simplicity, let us denote âˆ† =Ïƒ2+C2H(P)
k. The proof is divided into two cases.
Case 1: âˆ†is large. More precisely, suppose that âˆ†â‰¥2CD2/T. Then, minimizing over Î·above leads us
to the following upper bound on the right-hand side of equation (42):
/radicalbigg
2CD2âˆ†
T, (44)
which is obtained by setting Î·=/radicalï£¬ig
2D2
Câˆ†T.One can verify that this step size is less than1
Csince âˆ†is at least
2CD2. Thus, we conclude that equation (42) must be less than
/radicalbigg
2CD2âˆ†
T=/radicalbigg
2CD2(Ïƒ2+C2H(P)))
kT. (45)
Case 2: âˆ†is small. In this case, suppose âˆ†<2CD2/T. Then, the right-hand side of equation (42) must
be less than
2D2
TÎ·+2C2D2Î·
Tâ‰¤2CD2
T. (46)
Thus, combining equations (45) and (46), we have completed the proof of equation (12).
B.2 Proof of Theorem 4.3
Recall our construction from Section 4 as follows. Let etbe the basis vector for the t-th dimension, for
t= 0,1,...,Tâˆ’1. Definef(W)as
f(W) =1
2GâŸ¨W,e 0âŸ©2+Tâˆ’1/summationdisplay
i=0hi(âŸ¨W,ei+1âŸ©),
wherehia quadratic function parameterized by Î±i, defined as follow:
hi(x) =ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³CÎ±2
i
4|x|â‰¤Î±i
âˆ’C(|x|âˆ’Î±i)2
2+CÎ±2
i
4Î±iâ‰¤|x|â‰¤3
2Î±i
C(|x|âˆ’2Î±i)2
23
2Î±iâ‰¤|x|â‰¤2Î±i
0 2 Î±iâ‰¤|x|.
For technical reasons, we define a truncated perturbation distribution P. Given a sample Ufrom ad-
dimensional isotropic Gaussian N(0,Idd), we truncate the i-th coordinate of Uso that ËœUi= min(Ui,ai), for
some fixed ai>0that we will specify below, for all i= 0,1,...,dâˆ’1. LetPdenote the distribution of ËœU.
The proof of Theorem 4.3 is divided into two cases. First, we examine the case when the averaged learning
rate isO(Tâˆ’1/2).
29Published in Transactions on Machine Learning Research (09/2024)
Lemma B.3. In the setting of Theorem 4.3, suppose the learning rates satisfy that/summationtextTâˆ’1
i=0Î·iâ‰¤/radicalï£¬ig
D2kT
2Ïƒ2C,
consider the function f(W)constructed in equation (13), we have
min
1â‰¤tâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥D/radicalbigg
CÏƒ2
32kT.
Proof.Westartbydefiningagradientoraclebychoosingthenoisevectors {Î¾t}Tâˆ’1
t=0tobeindependentrandom
variables such that
Î¾t=âŸ¨Î¾t,et+1âŸ©et+1and|âŸ¨Î¾t,et+1âŸ©|â‰¤Ïƒâˆš
k, (47)
whereet+1is a basis vector whose (t+ 1)-th entry is one and otherwise is zero. In other words, only the
(t+ 1)-th coordinate of Î¾tis nonzero. Otherwise, the rest of the vector remains zero. We use Â¯Î¾tto denote
the averaged noise variable as
Â¯Î¾t=1
kk/summationdisplay
i=1Î¾(i)
t,
whereÎ¾(i)
tis defined following the condition specified in equation (47). Thus, we can also conclude that
|âŸ¨Â¯Î¾t,et+1âŸ©|â‰¤Ïƒâˆš
k.
We consider the objective function f(W) :Rdâ†’Rdefined above (see also equation (13), Section 4), with
Î±i=2Î·iÏƒâˆš
k,fori= 0,1,...,T. (48)
We will analyze the dynamics of Algorithm 1 with the objective function f(W)and the starting point
W0=Dâˆš
GÂ·e0, whereG= max/braceleftbig
Câˆ’1,2/summationtextTâˆ’1
i=0Î·i/bracerightbig
. For the first iteration, we have
W1=W0âˆ’Î·0/parenleftï£¬ig1
2k/summationdisplay
i=1/parenleftbig
âˆ‡f(W0+U(i)
0) +âˆ‡f(W0âˆ’U(i)
0)/parenrightbig
+Â¯Î¾0/parenrightï£¬ig
= (1âˆ’Î·0Gâˆ’1)W0âˆ’Î·0Â¯Î¾0,
whereUis a random draw from the truncated distribution PwithâŸ¨U,eiâŸ©= min{Pi,ai}forai=Î·iâˆ’1Ïƒâˆš
k.
Next, from the construction of h1, we get
1
2/parenleftbig
âˆ‡f(W1+U) +âˆ‡f(W1âˆ’U)/parenrightbig
=Gâˆ’1âŸ¨W1,e0âŸ©e0+1
2/parenleftï£¬ig
hâ€²
0/parenleftbig
Î·0âŸ¨Â¯Î¾0,e1âŸ©+âŸ¨U,e1âŸ©/parenrightbig
e1+hâ€²
0/parenleftbig
Î·0âŸ¨Â¯Î¾0,e1âŸ©âˆ’âŸ¨U,e1âŸ©/parenrightbig
e1/parenrightï£¬ig
.
Here, using the fact that Î±0=2Î·0Ïƒâˆš
kfrom equation (48) above, and the truncation of U, which implies
|âŸ¨U,e1âŸ©|â‰¤Î·0Ïƒâˆš
k, andâŸ¨Â¯Î¾0,e1âŸ©â‰¤Ïƒâˆš
k, we obtain
/vextendsingle/vextendsingleÎ·0âŸ¨Â¯Î¾0,e1âŸ©+âŸ¨U,e1âŸ©/vextendsingle/vextendsingleâ‰¤2Î·0Ïƒâˆš
k=Î±0,and similarly/vextendsingle/vextendsingleÎ·0âŸ¨Â¯Î¾0,e1âŸ©âˆ’âŸ¨U,e1âŸ©/vextendsingle/vextendsingleâ‰¤2Î·0Ïƒâˆš
k=Î±0,
which implies that
hâ€²
0(Î·0âŸ¨Â¯Î¾0,e1âŸ©+âŸ¨U,e1âŸ©) =hâ€²
0(Î·0âŸ¨Â¯Î¾0,e1âŸ©âˆ’âŸ¨U,e1âŸ©) = 0.
This is the first update. Then, in the next iteration,
W2=W1âˆ’Î·1/parenleftï£¬ig
Gâˆ’1âŸ¨W1,e0âŸ©+Â¯Î¾1/parenrightï£¬ig
=âˆ’(1âˆ’Î·1Gâˆ’1)(1âˆ’Î·0Gâˆ’1)W0âˆ’Î·0Â¯Î¾0âˆ’Î·1Â¯Î¾1.
30Published in Transactions on Machine Learning Research (09/2024)
Similarly, we use the fact that Î±i=2Î·iÏƒâˆš
kand the fact that |âŸ¨U,ei+1âŸ©|â‰¤Î·iÏƒâˆš
k, which renders the gradient as
zero similar to the above reasoning. This holds for any i= 1,2,...,Tâˆ’1.
At thet-th iteration, suppose we have that
Wt=W0tâˆ’1/productdisplay
i=0/parenleftï£¬ig
1âˆ’Î·iGâˆ’1/parenrightï£¬ig
âˆ’tâˆ’1/summationdisplay
i=0Î·iÂ¯Î¾i.
Then by induction, at the (t+ 1)-th iteration, we must have
Wt+1=Wtâˆ’Î·t/parenleftï£¬ig
Gâˆ’1âŸ¨Wt,e0âŸ©+Â¯Î¾t/parenrightï£¬ig
=W0t/productdisplay
i=0/parenleftï£¬ig
1âˆ’Î·iGâˆ’1/parenrightï£¬ig
âˆ’t/summationdisplay
i=0Î·iÂ¯Î¾i. (49)
Next, from the definition of htabove, we have that
F(W0)âˆ’min
WâˆˆRdF(W) =F(W0) (the minimum can be attained at zero)
=1
2G(Dâˆš
G)2+Tâˆ’1/summationdisplay
i=0C
4/parenleftï£¬ig2Î·iÏƒâˆš
k/parenrightï£¬ig2
(sinceâŸ¨W0+U,ei+1âŸ©â‰¤Î±i)
The above must be at most D2, which implies that we should set the learning rates to satisfy (after some
calculation)
1
T/parenleftï£¬igTâˆ’1/summationdisplay
i=0Î·i/parenrightï£¬ig2
â‰¤Tâˆ’1/summationdisplay
i=0Î·2
iâ‰¤kD2
2CÏƒ2. (50)
We note that for all zâˆˆ[0,1],1âˆ’z
2â‰¥exp(logz
2). Thus, applying this to the right-hand side of equation
(49), we obtain that for any t,
t/productdisplay
i=0/parenleftï£¬ig
1âˆ’Î·iGâˆ’1/parenrightï£¬ig
â‰¥1
2, (51)
where we recall that G= max{Câˆ’1,2/summationtextTâˆ’1
i=0Î·i}. Our calculation so far shows that for all the hiexcepth0,
the algorithm has not moved at all from its initialization at W0under the above gradient noise. We thus
conclude that
min
1â‰¤iâ‰¤Tâˆ¥âˆ‡F(Wi)âˆ¥2= min
1â‰¤iâ‰¤T/parenleftï£¬ig
Gâˆ’1âŸ¨W0,e0âŸ©/parenrightï£¬ig2
(by the construction of F(Â·))
â‰¥1
4Gâˆ’2(Dâˆš
G)2(by equations (49) and (51))
=D2
4min/braceleftï£¬ig
C,1
2/summationtextTâˆ’1
i=0Î·i/bracerightï£¬ig
(recall the definition of Gabove)
â‰¥D2
4min/braceleftï£¬ig
C,âˆš
2CÏƒ2
2Dâˆš
kT/bracerightï£¬ig
â‰¥D/radicalbigg
CÏƒ2
32kT. (by equation (50))
In the first step, we use the fact that âŸ¨Â¯Î¾i,e0âŸ©= 0, for all 0 = 1,2,...,Tâˆ’1. Thus, we have proved that
equation (14) holds for Wifor anyi= 1,2,...,T. The proof of Lemma B.3 is finished.
Next, let us consider another case of the lower bound.
Lemma B.4. In the setting of Theorem 4.3, suppose the learning rates satisfy that/summationtextTâˆ’1
i=0Î·iâ‰¥/radicalï£¬ig
D2kT
2Ïƒ2C
andÎ·i=Î·for some fixed Î·â‰¤Câˆ’1. Then, consider the function from equation (13), we have that
min1â‰¤tâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥D/radicalï£¬ig
CÏƒ2
32kT.
31Published in Transactions on Machine Learning Research (09/2024)
Proof.We define the functions g, parametrized by a fixed, positive constants Î±=1âˆ’ÏT
1âˆ’ÏÂ·2cÎ·Ïƒ, as follows:
g(x) =ï£±
ï£²
ï£³âˆ’C
2x2+C
4Î±2|x|â‰¤Î±
2,
C
2(|x|âˆ’Î±)2Î±
2â‰¤|x|â‰¤Î±,
0 Î±â‰¤|x|.
One can verify that âˆ‡gisC-Lipschitz continuous, but gis not twice-differentiable. We also consider a
chain-like function:
f(W) =g(âŸ¨W,e 0âŸ©) +dâˆ’1/summationdisplay
t=0C
2âŸ¨W,et+1âŸ©2. (52)
From the definition of f, its gradient is C-Lipschitz continuous. Similar to equation (47), we define an
adversarial gradient oracle by choosing the noise vectors {Î¾t}Tâˆ’1
t=0to be independent random variables such
that
Î¾t=âŸ¨Î¾t,et+1âŸ©,E/bracketleftbig
âŸ¨Î¾t,et+1âŸ©2/bracketrightbig
=Ïƒ2,and|âŸ¨Î¾t,et+1âŸ©|â‰¤cÏƒ,
wherecis a fixed constant. We use Â¯Î¾tto denote the averaged noise variable as
Â¯Î¾t=k/summationdisplay
i=1Î¾(i)
t.
Suppose{Î¾(i)
t}k
i=1are i.i.d. random variables for any t, we have
|âŸ¨Â¯Î¾t,et+1âŸ©|â‰¤cÏƒandE/bracketleftï£¬ig/vextenddouble/vextenddoubleÂ¯Î¾t/vextenddouble/vextenddouble2/bracketrightï£¬ig
â‰¤Ïƒ2
k. (53)
Next, we analyze the dynamics of Algorithm 1 with the objective function f(W)and the starting point
W0=/summationtextd
i=1/radicalï£¬ig
D2
CdÂ·ei. In this case, by setting Î·i=Î·for alli= 0,1,...,Tâˆ’1. Recall that Î·<Câˆ’1. Denote
byÏ=CÎ·, which is strictly less than one.
Sincehtis an even function, its derivative hâ€²
tis odd. For the first iteration, we have
W1=W0âˆ’Î·/parenleftï£¬ig1
2/parenleftbig
âˆ‡f(W0+U) +âˆ‡f(W0âˆ’U)/parenrightbig
+Â¯Î¾0/parenrightï£¬ig
= (1âˆ’CÎ·)W0âˆ’Î·Â¯Î¾0.
whereUis a truncate distribution of Pâˆ¼N(0,Idd)withâŸ¨U,e0âŸ©= min{P0,a0}anda0=cÎ·Ïƒ.
Using the fact that Î±=1âˆ’ÏT
1âˆ’ÏÂ·2cÎ·Ïƒ,|âŸ¨U,e0âŸ©|â‰¤cÎ·Ïƒ, andâŸ¨Â¯Î¾0,e0âŸ©â‰¤cÏƒ, we have
gâ€²(Î·âŸ¨Â¯Î¾0,e0âŸ©+âŸ¨U,e0âŸ©) +gâ€²(Î·âŸ¨Â¯Î¾0,e0âŸ©âˆ’âŸ¨U,e0âŸ©) =âˆ’2CÎ·âŸ¨Â¯Î¾0,e0âŸ©.
Then, in the next iteration,
W2=W1âˆ’Î·/parenleftï£¬ig
Cd/summationdisplay
i=1âŸ¨W1,eiâŸ©âˆ’CÎ·Â¯Î¾0+Â¯Î¾1/parenrightï£¬ig
= (1âˆ’CÎ·)2W0âˆ’(1âˆ’CÎ·)Î·Â¯Î¾0âˆ’Î·Â¯Î¾1.
Similarly, we use the fact that Î±=1âˆ’ÏT
1âˆ’ÏÂ·2cÎ·Ïƒand the fact that |âŸ¨U,e0âŸ©|â‰¤cÎ·Ïƒ, which renders the gradient
asgâ€²(x) =âˆ’Cx, for anyi= 1,2,...,Tâˆ’1.
At thet-th iteration, suppose that
Wt= (1âˆ’CÎ·)tW0âˆ’tâˆ’1/summationdisplay
i=0(1âˆ’CÎ·)tâˆ’1âˆ’iÎ·Â¯Î¾i.
32Published in Transactions on Machine Learning Research (09/2024)
Then by induction, at the (t+ 1)-th iteration, we have
Wt+1=Wtâˆ’Î·/parenleftï£¬ig
Cd/summationdisplay
i=1âŸ¨Wt,eiâŸ©âˆ’Ctâˆ’1/summationdisplay
i=0(1âˆ’CÎ·)tâˆ’1âˆ’iÎ·Â¯Î¾i+Â¯Î¾t/parenrightï£¬ig
= (1âˆ’CÎ·)t+1W0âˆ’t/summationdisplay
i=0(1âˆ’CÎ·)tâˆ’1âˆ’iÎ·Â¯Î¾i. (54)
Next, from the definition of Fabove, we have that
F(W0)âˆ’min
WâˆˆRdF(W) =F(W0) =dC
2/parenleftï£¬ig/radicalbigg
D2
Cd/parenrightï£¬ig2
+C
4/parenleftï£¬ig2(1âˆ’ÏT)cÎ·Ïƒ
(1âˆ’Ï)/parenrightï£¬ig2
,(sinceâŸ¨W0+U,e0âŸ©â‰¤Î±)
which must be at most D2. Thus, we must have (after some calculation)
c2â‰¤D2(1âˆ’Ï)2
2Ïƒ2Ï2(1âˆ’ÏT)2.
We conclude that
min
1â‰¤iâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wi)âˆ¥2/bracketrightï£¬ig
= min
1â‰¤iâ‰¤TEï£®
ï£°d/summationdisplay
j=1C2âŸ¨Wi,ejâŸ©2+C2âŸ¨Wi,e0âŸ©2ï£¹
ï£»
= min
1â‰¤iâ‰¤T/parenleftï£¬ig
dC2(1âˆ’Ï)2t/parenleftï£¬ig/radicalbigg
D2
Cd/parenrightï£¬ig2
+Ïƒ2
kÂ·Ï2t/summationdisplay
i=0(1âˆ’Ï)2(tâˆ’1âˆ’i)/parenrightï£¬ig
â‰¥min
1â‰¤iâ‰¤T/parenleftï£¬ig
CD2(1âˆ’Ï)2t+Ïƒ2
kÏ
2âˆ’Ï/parenleftbig
1âˆ’(1âˆ’Ï)2t/parenrightbig/parenrightï£¬ig
â‰¥min/braceleftï£¬ig
CD2,Ïƒ2
kÏ
2âˆ’Ï/bracerightï£¬ig
â‰¥Ïƒ2
kC/radicalbigg
kD2
2TÏƒ2C1
2âˆ’C/radicalï£¬ig
kD2
2TÏƒ2Câ‰¥D/radicalbigg
CÏƒ2
16kÂ·T. (after some calculation)
Thus, we have proved this lemma.
Taking both Lemma B.3 and B.4 together, we thus conclude the proof of Theorem 4.3.
B.3 Proof of momentum lower bound
In this section, we prove the following result.
Theorem B.5. There exists a quadratic function fsuch that for the iterates W1,...,WTgenerated by
equation (15), we must have: min1â‰¤tâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥O/parenleftbig
D/radicalï£¬ig
CÏƒ2
kÂ·T/parenrightbig
.
We will focus on a perturbation distribution Pequal to the isotropic Gaussian distribution for this result.
In this case, we know that F(W) =f(W) +d. For the quadratic function f(W) =C
2âˆ¥Wâˆ¥2, its gradient is
C-Lipschitz continuous. We set the initialization W0âˆˆRdsuch that
F(W0)âˆ’min
WâˆˆRdF(W) =D2.
This condition can be met when we set W0as a vector whose Euclidean norm is equal to
D/radicaltp/radicalvertex/radicalvertex/radicalbt2 max/braceleftï£¬ig
Câˆ’1,2Tâˆ’1/summationdisplay
i=0Î·i/bracerightï£¬ig
.
33Published in Transactions on Machine Learning Research (09/2024)
The case when Âµ= 0.We begin by considering the case when Âµ= 0. In this case, the update reduces to
SGD, and the iterate Wt+1evolves as follows:
Wt+1=/parenleftï£¬ig
1âˆ’CÎ·t/parenrightï£¬ig
Wtâˆ’Î·tÂ¯Î¾t, (55)
where we denote Â¯Î¾tas the averaged noise kâˆ’1/summationtextk
j=1Î¾(j)
t, and the noise perturbation U(j)
tcancelled out
between the plus and minus perturbations. The case when Âµ > 0builds on this simpler case, as we will
describe below.
The key observation is that the gradient noise sequence Â¯Î¾1,Â¯Î¾2,..., Â¯Î¾Tforms a martingale sequence:
â€¢For anyi= 1,2,...,T, conditioned on the previous random variables Î¾(j)
iâ€²for anyiâ€²< iand any
j= 1,2,...,k, the expectation of Â¯Î¾iis equal to zero.
â€¢In addition, the variance of Â¯Î¾iis equal tokâˆ’1Ïƒ2, since conditional on the previous random variables,
theÎ¾(j)
is are all independent from each other.
The martingale property allows us to characterize the SGD path of âˆ¥Wtâˆ¥2, as shown in the following result.
Lemma B.6. In the setting of Theorem B.5, for any step sizes Î·0,...,Î·Tâˆ’1less thanCâˆ’1, and anyt=
1,...,T, the expected gradient of Wt,E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
, is equal to
2CD2tâˆ’1/productdisplay
j=0/parenleftbig
1âˆ’CÎ·j/parenrightbig2+CÏƒ2
ktâˆ’1/summationdisplay
i=0Î·2
itâˆ’1/productdisplay
j=i+1/parenleftbig
1âˆ’CÎ·j/parenrightbig2.
Proof.By iterating over equation (55), we can get
Wt=W0tâˆ’1/productdisplay
j=0/parenleftï£¬ig
1âˆ’CÎ·j/parenrightï£¬ig
âˆ’tâˆ’1/summationdisplay
i=0Î·iÂ¯Î¾itâˆ’1/productdisplay
j=i+1/parenleftï£¬ig
1âˆ’CÎ·j/parenrightï£¬ig
.
Meanwhile,
âˆ‡F(Wt) =CWtâ‡’âˆ¥âˆ‡F(Wt)âˆ¥2=C2âˆ¥Wtâˆ¥2.
Thus, by squaring the norm of Wtand taking the expectation, we can get
E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
=C2âˆ¥W0âˆ¥2tâˆ’1/productdisplay
j=0/parenleftbig
1âˆ’CÎ·j/parenrightbig2+C2tâˆ’1/summationdisplay
i=0E/bracketleftï£¬ig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleÎ·iÂ¯Î¾itâˆ’1/productdisplay
j=i+1/parenleftbig
1âˆ’CÎ·j/parenrightbig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle2/bracketrightï£¬ig
. (56)
Above, we use martingale property a), which says the expectation of Â¯Î¾iis equal to zero for all i. In addition,
based on property b), equation (56) is equal to
C2tâˆ’1/summationdisplay
i=0Î·2
iï£«
ï£­tâˆ’1/productdisplay
j=i+1/parenleftï£¬ig
1âˆ’CÎ·j/parenrightï£¬ig2
E/bracketleftï£¬ig/vextenddouble/vextenddoubleÂ¯Î¾i/vextenddouble/vextenddouble2/bracketrightï£¬igï£¶
ï£¸=C2Ïƒ2
ktâˆ’1/summationdisplay
i=0Î·2
itâˆ’1/productdisplay
j=i+1/parenleftï£¬ig
1âˆ’CÎ·j/parenrightï£¬ig2
.
To see this, based on the martingale property of Â¯Î¾again, the cross terms between Â¯Î¾iandÂ¯Î¾jfor different i,j
are equal to zero in expectation:
E/bracketleftbig
âŸ¨Â¯Î¾i,Â¯Î¾jâŸ©|Â¯Î¾j/bracketrightbig
= 0,for all 1â‰¤j <iâ‰¤T.
Additionally, the second moment of Â¯Î¾isatisfies:
E/bracketleftï£¬ig/vextenddouble/vextenddoubleÂ¯Î¾i/vextenddouble/vextenddouble2/bracketrightï£¬ig
=Ïƒ2
k,for anyi= 1,...,T.
34Published in Transactions on Machine Learning Research (09/2024)
Lastly, letW0be a vector such that
âˆ¥W0âˆ¥=Dâˆš
2Câˆ’1â‡’F(W0)âˆ’min
WâˆˆRdF(W)â‰¤D2.
Settingâˆ¥W0âˆ¥=Dâˆš
2Câˆ’1in equation (56) leads to
E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
= 2CD2tâˆ’1/productdisplay
j=0/parenleftï£¬ig
1âˆ’CÎ·j/parenrightï£¬ig2
+C2Ïƒ2
ktâˆ’1/summationdisplay
i=0Î·2
itâˆ’1/productdisplay
j=i+1/parenleftï£¬ig
1âˆ’CÎ·j/parenrightï£¬ig2
.
Thus, we conclude the proof of this result.
We now present the proof for the case when/summationtextTâˆ’1
i=0Î·iâ‰¤O(âˆš
T). For this result, we will use the following
quadratic function:
f(W) =1
2Îºâˆ¥Wâˆ¥2,whereÎº= max{Câˆ’1,2Tâˆ’1/summationdisplay
i=0Î·i}, (57)
Lemma B.7. Considerfgiven in equation (57)above. For any step sizes Î·0,...,Î·Tâˆ’1less thanCâˆ’1, the
following holds for the stochastic objective F:
min
1â‰¤tâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥D2
2 max{Câˆ’1,2/summationtextTâˆ’1
i=0Î·i}.
Proof.The norm of the gradient of F(W)is equal to
âˆ¥âˆ‡F(W)âˆ¥=1
Îºâˆ¥Wâˆ¥. (58)
Following the update rule in NSO, similar to equation (55), Wtevolves as follows:
Wt+1=/parenleftï£¬igg
1âˆ’Î·t
Îº/parenrightï£¬igg
Wtâˆ’Î·tÂ¯Î¾t, (59)
where Â¯Î¾thas variance equal to Ïƒ2/k, according to the proof of Lemma B.6. By iterating equation (59) from
the initialization, we can get a closed-form equation for W(1)
t, for anyt= 1,2,...,T:
Wt=W0tâˆ’1/productdisplay
j=0/parenleftï£¬igg
1âˆ’Î·j
Îº/parenrightï£¬igg
âˆ’tâˆ’1/summationdisplay
k=0Î·kÎ¾ktâˆ’1/productdisplay
j=k+1/parenleftï£¬igg
1âˆ’Î·j
Îº/parenrightï£¬igg
. (60)
Following equation (58), we can show that âˆ¥âˆ‡F(W)âˆ¥2=Îºâˆ’2âˆ¥Wtâˆ¥2.Thus, in expectation,
E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
=Îºâˆ’2E/bracketleftï£¬ig
âˆ¥Wtâˆ¥2/bracketrightï£¬ig
=Îºâˆ’2âˆ¥W0âˆ¥2tâˆ’1/productdisplay
j=0/parenleftï£¬ig
1âˆ’Îºâˆ’1Î·j/parenrightï£¬ig2
+Îºâˆ’2tâˆ’1/summationdisplay
i=0Eï£®
ï£¯ï£°ï£«
ï£­Î·iÂ¯Î¾itâˆ’1/productdisplay
j=i+1/parenleftï£¬ig
1âˆ’Îºâˆ’1Î·j/parenrightï£¬igï£¶
ï£¸2ï£¹
ï£ºï£»
=Îºâˆ’2âˆ¥W0âˆ¥2tâˆ’1/productdisplay
j=0/parenleftï£¬ig
1âˆ’Îºâˆ’1Î·j/parenrightï£¬ig2
+Îºâˆ’2tâˆ’1/summationdisplay
i=0Î·2
itâˆ’1/productdisplay
j=i+1/parenleftï£¬ig
1âˆ’Îºâˆ’1Î·j/parenrightï£¬ig2
E/bracketleftï£¬ig/vextenddouble/vextenddoubleÂ¯Î¾i/vextenddouble/vextenddouble2/bracketrightï£¬ig
= 2D2Îºâˆ’1tâˆ’1/productdisplay
j=0/parenleftï£¬ig
1âˆ’Îºâˆ’1Î·j/parenrightï£¬ig2
+Ïƒ2Îºâˆ’2
ktâˆ’1/summationdisplay
i=0Î·2
itâˆ’1/productdisplay
j=i+1/parenleftï£¬ig
1âˆ’Îºâˆ’1Î·j/parenrightï£¬ig2
, (61)
35Published in Transactions on Machine Learning Research (09/2024)
where we use the definition of initialization W0and the variance of Â¯Î¾iin the last step. In order to tackle
equation (61), we note that for all zâˆˆ[0,1],
1âˆ’z
2â‰¥exp/parenleftï£¬ig
log1
2Â·z/parenrightï£¬ig
. (62)
Hence, applyingequation(62)totheright-handsideofequation(61), weobtainthatforany i= 0,1,...,tâˆ’1,
tâˆ’1/productdisplay
j=i/parenleftï£¬igg
1âˆ’Î·j
max{Câˆ’1,2/summationtextTâˆ’1
j=iÎ·i}/parenrightï£¬igg
â‰¥exp/parenleftï£¬igg
log1
2Â·tâˆ’1/summationdisplay
j=iÎ·j
max{(2C)âˆ’1,/summationtextTâˆ’1
i=0Î·i}/parenrightï£¬igg
â‰¥1
2.
Thus, equation (61) must be at least
E/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥2D2Îºâˆ’1
4+Ïƒ2Îºâˆ’2
ktâˆ’1/summationdisplay
i=0Î·2
i
4. (63)
The above result holds for any t= 1,2,...,T. Therefore, we conclude that
min
1â‰¤tâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥D2
2 max{Câˆ’1,2/summationtextTâˆ’1
i=0Î·i}.
Thus, the proof of Lemma B.7 is finished.
Next, we consider the other case, which is when the learning rates are fixed.
Lemma B.8. There exists convex quadratic functions fsuch that for any gradient oracle satisfying Assump-
tion 4.1 and any distribution Pwith mean zero, if Î·i=Î·<Câˆ’1for anyi= 1,...,T, or if/summationtextTâˆ’1
i=0Î·iâ‰²âˆš
T,
then the following must hold:
min
1â‰¤tâ‰¤TE/bracketleftï£¬ig
âˆ¥âˆ‡F(Wt)âˆ¥2/bracketrightï£¬ig
â‰¥D/radicalbigg
CÏƒ2
32kÂ·T. (64)
Proof.By Lemma B.7, there exists a function such that the left-hand side of equation (64) is at least
D2
2 max{Câˆ’1,2/summationtextTâˆ’1
i=0Î·i}â‰¥CD2
2 max{1,2xâˆ’1âˆš
T}=D2x
4âˆš
T, (65)
which holds if/summationtextTâˆ’1
i=0Î·iâ‰¤âˆš
Txâˆ’1for any fixed x>0.
On the other hand, if/summationtextTâˆ’1
i=0Î·iâ‰¥xâˆ’1âˆš
TandÎ·i=Î·for a fixedÎ·, thenÎ·>xâˆ’1/âˆš
T. By setting Î·i=Î·for
alliin Lemma B.6, the left-hand side of equation (64) is equal to
min
1â‰¤tâ‰¤T/parenleftï£¬ig
2CD2(1âˆ’CÎ·)2t+C2Ïƒ2
ktâˆ’1/summationdisplay
k=0Î·2(1âˆ’CÎ·)2(tâˆ’kâˆ’1)/parenrightï£¬ig
.
Recall that Î· < Câˆ’1. Thus,Ï=CÎ·must be less than one. With some calculations, we can simplify the
above to
min
1â‰¤tâ‰¤T/parenleftbigg
2CD2(1âˆ’Ï)2t+Ïƒ2Ï2
k1âˆ’(1âˆ’Ï)2t
1âˆ’(1âˆ’Ï)2/parenrightbigg
= min
1â‰¤tâ‰¤T/parenleftbiggÏƒ2Ï
k(2âˆ’Ï)+ (1âˆ’Ï)2t/parenleftï£¬ig
2CD2âˆ’Ïƒ2Ï
k(2âˆ’Ï)/parenrightï£¬ig/parenrightbigg
. (66)
If2CD2<Ïƒ2Ï
k(2âˆ’Ï), the above is the smallest when t= 1. In this case, equation (66) is equal to
2CD2(1âˆ’Ï)2+Ïƒ2Ï2
kâ‰¥1
1
2CD2+k
Ïƒ2=O(1).
36Published in Transactions on Machine Learning Research (09/2024)
If2CD2â‰¥Ïƒ2Ï
k(2âˆ’Ï), the above is the smallest when t=T. In this case, equation (66) is at least
Ïƒ2Ï
k(2âˆ’Ï)â‰¥Ïƒ2Ï
2kâ‰¥Ïƒ2Cxâˆ’1
2kÂ·1âˆš
T. (67)
To conclude the proof, we set xso that the right-hand side of equations (65) and (67) match each other.
This leads to
x=/radicalbigg
2Ïƒ2C
kD2.
Thus, by combining the conclusions from both equations (65) and (67) with this value of x, we finally
conclude that if/summationtextTâˆ’1
i=0Î·iâ‰¤âˆš
Txâˆ’1, or for alli= 0,...,Tâˆ’1,Î·i=Î·<Câˆ’1, then in both cases, there exists
a functionfsuch that equation (64) holds. This completes the proof of Lemma B.8.
The case when Âµ > 0.In this case, since the update of Wtalso depends on the momentum update, it
becomes significantly more involved. One can verify that the update from step tto stept+ 1is based on
Xu=/bracketleftbigg1âˆ’CÎ·tÂµ
CÎ·tÂµ/bracketrightbigg
. (68)
Our analysis examines the eigenvalues of the matrix XuXâŠ¤
uand the first entry in the corresponding eigenvec-
tors. Particularly, we show that the two entries are bounded away from zero. Then, we apply the HÃ¶lderâ€™s
inequality to reduce the case of Âµ>0to the case of Âµ= 0, Lemma B.8 in particular.
Proof.First, consider a quadratic function
f(W) =1
2Câˆ¥Wâˆ¥2.
Clearly,f(W)isC-Lipschitz continuous. Further, F(W) =f(W) +d, forPbeing the isotropic Gaussian.
LetW0be a vector whose Euclidean norm equals Dâˆš
2C. Thus,
F(W0)âˆ’min
WâˆˆRdF(W) =D2.
As for the dynamic of momentum SGD, recall that
Mt+1=ÂµMtâˆ’Î·tGtandWt+1=Wt+Mt+1.
We consider the case where Î·t=Î·for all steps t. In this case, we can write the above update into a matrix
notation as follows:
/bracketleftbiggWt+1
Mt+1/bracketrightbigg
=/bracketleftbigg1âˆ’CÎ· Âµ
âˆ’CÎ· Âµ/bracketrightbigg/bracketleftbiggWt
Mt/bracketrightbigg
+CÎ·/bracketleftbiggÂ¯Î¾t
Â¯Î¾t/bracketrightbigg
.
LetXÂµ= [1âˆ’CÎ·,Âµ ;âˆ’CÎ·,Âµ ]denote the 2by2matrix (that depends on Âµ) above. Similar to Lemma B.6,
we can apply the above iterative update to obtain the formula for Wt+1as:
/bracketleftbiggWt+1
Mt+1/bracketrightbigg
=Xt
u/bracketleftbiggW0
M0/bracketrightbigg
+t/summationdisplay
i=0CÎ·Xtâˆ’i
u/bracketleftbiggÂ¯Î¾i
Â¯Î¾i/bracketrightbigg
. (69)
By multiplying both sides by the vector e1= [1,0]âŠ¤, and then taking the Euclidean norm of the vector
(notice that this now only evolves that Wt+1vector on the left, and the Wtvector on the right), we now
obtain that, in expectation over the randomness of the Â¯Î¾iâ€™s, the following holds:
E/bracketleftï£¬ig
âˆ¥Wt+1âˆ¥2/bracketrightï£¬ig
= 2CD2(eâŠ¤
1Xt
ue1)2+C2Î·2Ïƒ2
kt/summationdisplay
i=0/vextenddouble/vextenddoubleeâŠ¤
1Xi
ue/vextenddouble/vextenddouble2. (70)
37Published in Transactions on Machine Learning Research (09/2024)
Above, similar to Lemma B.6, we have set the length of W0appropriately so that its size is equal to Dâˆš
2Câˆ’1,
which has led to the CD2term above. Recall that M0is equal to zero in the beginning. To get the first
term above, we follow this calculation:
/vextenddouble/vextenddouble/vextenddouble/vextenddoubleeâŠ¤
1Xt
Âµ/bracketleftbiggW0
M0/bracketrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
= Tr/bracketleftï£¬igg
eâŠ¤
1Xt
Âµ/bracketleftbiggW0
M0/bracketrightbigg/bracketleftbiggW0
M0/bracketrightbiggâŠ¤
Xt
ÂµâŠ¤e1/bracketrightï£¬igg
= Tr/bracketleftbigg
eâŠ¤
1Xt
Âµ/bracketleftbigg
CD20
0 0/bracketrightbigg
Xt
ÂµâŠ¤e1/bracketrightbigg
= 2CD2(eâŠ¤
1Xt
Âµe1)2.
We usee= [1,1]âŠ¤to denote the vector of ones. Now, we focus on the 2by2matrixXu(recall this is
the coefficient matrix on the right side of equation (69)). Let its singular values be denoted as Î»1andÎ»2.
In addition, to deal with equation (70), let Î±1andÎ±2denote the first entry of Xuâ€™s left singular vectors,
corresponding to aandb, respectively. Thus, we can write
(eâŠ¤
1Xi
Âµe)2=Î±2
1Î»2i
1+Î±2
2Î»2i
2. (71)
Now, one can verify that Î»2
1andÎ»2
2are the roots of the following quadratic equation over x:
x2âˆ’((1âˆ’CÎ·)2+ (CÎ·)2+ 2Âµ2)x+Âµ2= 0. (72)
This can be checked by first taking XutimesXâŠ¤
u, then using the definition of the eigenvalues by calculating
the determinant of XuXâŠ¤
uâˆ’xId = 0. Thus, we have that Î»1andÎ»2are equal to:
Î»1,Î»2=(1âˆ’CÎ·)2+ (CÎ·)2+ 2Âµ2Â±/radicalbig
((1âˆ’CÎ·)2+ (CÎ·)2+ 2Âµ2)2âˆ’4Âµ2
2. (73)
Now,Î±2
1(andÎ±2
2, respectively) satisfies that:
Î±2
1=âˆ’CÎ·(1âˆ’CÎ·) +Âµ2
(1âˆ’CÎ·)2+Âµ2âˆ’Î»1+âˆ’CÎ·(1âˆ’CÎ·) +Âµ2. (74)
By enumerating the possible values of CÎ·between 0and1, one can verify that for a fixed value of Âµ,Î±2
1and
Î±2
2are both bounded below from zero. Therefore, we can claim that from equation (71),
Î±2
1Î»2i
1+Î±2
2Î»2i
2â‰³Î»2i
1+Î»2i
2. (75)
By the HÃ¶lderâ€™s inequality,
(Î»2i
1+Î»2i
2)1
2i(1 + 1)1âˆ’1
2iâ‰¥Î»1+Î»2= (1âˆ’CÎ·)2+ (CÎ·)2+ 2Âµ2(76)
â‰¥(1âˆ’CÎ·)2+ (CÎ·)2, (77)
which implies that
Î»2i
1+Î»2i
2â‰¥((1âˆ’CÎ·)2+ (CÎ·)2)i
2(2iâˆ’1). (78)
Now, we consider two cases. If CÎ· < 1/2, then the above is greater than (1âˆ’CÎ·)2i, which holds for any
i= 0,1,...,Tâˆ’1. By way of reduction, we can follow the proof of Lemma B.8 to complete this proof. If
CÎ·> 1/2, then the above is greater than (CÎ·)2i. Again by following the proof steps in Lemma B.8, we can
show that
T
min
t=1E/bracketleftï£¬ig
âˆ¥Wtâˆ¥2/bracketrightï£¬ig
â‰³D/radicalbigg
CÏƒ2
kÂ·T.
This completes the proof of Theorem B.5.
38Published in Transactions on Machine Learning Research (09/2024)
C Experiment Details
We describe the setup for Figure 2, ran on (1) a two-layer Multi-Layer Perceptron (MLP) trained on the
MNIST digit classification dataset, (2) a twelve-layer BERT-Base model trained on the MRPC sentence
classification dataset from the GLUE benchmark and (3) a two-layer Graph Convolutional Network (GCN)
trained on the COLLAB node classification dataset. We set both MLP and GCN with a hidden dimension
of 128 for model architectures and initialize them randomly. We initialize the BERT model from pretrained
BERT-Base-Uncased. We train each model on the provided training set for the training process until the
training loss is close to zero. Specifically, we train the MLP, BERT, and GCN models for 30, 10, and 100
epochs. We use the model of the last epoch to measure the error in the approximation. We do this for 100
times and again measure the perturbed loss â„“Qon the training set. We take the gap between â„“Qandâ„“. Our
measurements show that the error between the actual gap and the Hessian approximation is within 3%.
Table6reportsadditionalcomparisonsbetweenourapproachandseveralbaselines,includinglabelsmoothing
(LS), random SAM (RSAM), and Bayesian SAM (BSAM). We report the test accuracy and the trace of the
Hessian for the model weights at the last epoch of training on six image classification datasets. We observe
that NSO also further reduces the trace of the Hessian and improves the test accuracy over the baselines.
The largest eigenvalue reduces by 9.7%.
Table 6: Comparison between our approach (NSO), label smoothing (LS), random-SAM (RSAM), and
Bayesian SAM (BSAM). Also included is the largest eigenvalue of the Hessian.
CIFAR-10 CIFAR-100 Aircraft Caltech-256 Indoor Retina
Trace
(â†“)LS 2690Â±85 10669Â±363 5699Â±72 3482Â±85 3650Â±82 17681Â±193
RSAM 2379Â±89 9762Â±422 4665Â±95 3224Â±97 3425Â±70 16950Â±257
BSAM 2768Â±54 9787Â±465 4750Â±55 3498Â±38 3162Â±73 16238Â±286
NSO 1728Â±79 5244Â±89 3678Â±83 2958Â±77 2737Â±90 10970Â±146
Test
Acc
(â†‘)LS 96.9%Â±0.1 83.8%Â±0.1 59.0%Â±0.2 76.6%Â±0.2 76.5%Â±0.3 64.2%Â±0.7
RSAM 96.8%Â±0.1 84.0%Â±0.1 60.9%Â±0.4 76.4%Â±0.1 76.8%Â±0.5 65.9%Â±0.3
BSAM 96.9%Â±0.1 83.9%Â±0.2 61.0%Â±0.3 76.8%Â±0.3 76.4%Â±0.3 65.4%Â±0.2
NSO 97.6% Â±0.4 84.9%Â±0.3 63.2%Â±0.3 78.1%Â±0.5 78.2%Â±0.3 67.0%Â±0.4
CIFAR-10 CIFAR-100 Aircraft Caltech-256 Indoor Retina
Î»1
(â†“)SGD 1442Â±63 4639Â±95 1152Â±40 1064Â±44 1087Â±56 8276Â±91
LS 1311Â±81 3051Â±95 1144Â±88 893Â±79 764Â±75 4296Â±74
SAM 1326Â±72 2625Â±91 890Â±90 948Â±95 887Â±53 4033Â±52
USAM 1245Â±43 2299Â±98 592Â±32 782Â±38 755Â±58 3893Â±55
ASAM 1383Â±73 2638Â±86 615Â±95 795Â±72 697Â±36 3925Â±56
RSAM 1356Â±69 2901Â±121 895Â±74 779Â±68 988Â±65 4537Â±58
BSAM 1375Â±86 2788Â±177 972Â±79 843Â±97 939Â±73 4123Â±87
NSO 1070Â±74 2059Â±45 579Â±59 643Â±57 639Â±72 3681Â±66
Finally, we report the hyper-parameters for the experiments in Section 3. These include a learning rate of
0.0002, momentum of 0.99, weight decay of 0.0001, batch size of 32, and training epochs of 60. We reduce
the learning rate by 0.1every 20epochs. We choose these hyper-parameters based on a grid search on the
validation split. The range in which we conduct a grid search is as follows: Learning rate: 0.005, 0.002,
0.001, 0.0005, 0.0002, and 0.0001; Momentum: 0.9, 0.95, 0.99; Weight decay: 0.01, 0.001, 0.0001; Epochs:
20, 40, and 60; Batch size: 16, 32, and 64.
Each baseline method may have its own set of hyper-parameters, which are adjusted via a grid search. For
label smoothing, we choose the weight of the loss calculated from the incorrect labels between 0.1, 0.2, and
0.3; For SAM and BSAM, we choose the â„“2norm of the perturbation between 0.01, 0.02, and 0.05; For
ASAM, we choose the â„“2norm of the perturbation for the weights between 0.5, 1.0, and 2.0; For RSAM, we
choose theâ„“2norm of the perturbation between 0.01, 0.02, and 0.05 and the standard deviation for sampling
perturbation between 0.008, 0.01, and 0.012.
39