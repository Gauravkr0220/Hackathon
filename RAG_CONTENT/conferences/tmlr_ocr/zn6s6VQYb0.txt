GraphCroc: Cross-Correlation Autoencoder for
Graph Structural Reconstruction
Shijin Duan‚àóRuyi Ding‚àóJiaxing He Aidong Adam Ding Yunsi Fei Xiaolin Xu
Northeastern University
{duan.s, ding.ruy, he.jiaxi, a.ding, y.fei, x.xu}@northeastern.edu
Abstract
Graph-structured data is integral to many applications, prompting the development
of various graph representation methods. Graph autoencoders (GAEs), in particular,
reconstruct graph structures from node embeddings. Current GAE models primarily
utilize self-correlation to represent graph structures and focus on node-level tasks,
often overlooking multi-graph scenarios. Our theoretical analysis indicates that self-
correlation generally falls short in accurately representing specific graph features
such as islands, symmetrical structures, and directional edges, particularly in
smaller or multiple graph contexts. To address these limitations, we introduce a
cross-correlation mechanism that significantly enhances the GAE representational
capabilities. Additionally, we propose the GraphCroc, a new GAE that supports
flexible encoder architectures tailored for various downstream tasks and ensures
robust structural reconstruction, through a mirrored encoding-decoding process.
This model also tackles the challenge of representation bias during optimization by
implementing a loss-balancing strategy. Both theoretical analysis and numerical
evaluations demonstrate that our methodology significantly outperforms existing
self-correlation-based GAEs in graph structure reconstruction. Our code is available
inhttps://github.com/sjduan/GraphCroc .
1 Introduction
Graph-structured data captures the relationships between data points, effectively mirroring the inter-
connectivity observed in various real-world applications, such as web services [ 3], recommendation
systems [ 39], and molecular structures [ 17,18,12]. Beyond the message passing through node
connections [ 50], the exploration of graph structure representation is equally critical [ 38,15,33,19,9].
This representation is extensively utilized in domains including recommendation systems, social
network analysis, and drug discovery [ 42], by leveraging the power of Graph Neural Networks
(GNNs). Specifically with Llayers in GNN, a node assimilates structural information from its L-hop
neighborhood, embedding graph structure in node features.
Graph autoencoders (GAEs)[ 19] have been developed to encode graph structures into node embed-
dings and decode these embeddings back into structural information, such as the adjacency matrix.
This structural reconstruction process can be performed either sequentially along nodes [ 11,22,47] or
in a global fashion [ 33]. While there has been significant advancement in both methods, most studies
primarily focus on node tasks, which involve a single graph, such as link prediction [ 35] and node
classification [ 15], with decoding strategies typically reliant on ‚Äúself-correlation‚Äù. We define this term
as the correlation of node pair from the same node embedding space. Given an n-node graph with
embedding dimension d‚Ä≤on each node, its node embedding is Z‚ààRn√ód‚Ä≤, thus the self-correlation is
expressed as zT
izjbetween two nodes. Correspondingly, the ‚Äúcross-correlation‚Äù depicts the node pair
‚àóequal contribution
38th Conference on Neural Information Processing Systems (NeurIPS 2024).correlation as pT
iqj, which are from two separate embedding spaces, P, Q‚ààRn√ód‚Ä≤. However, studies
are seldom evaluated under graph tasks, which represent graph structure on multiple graphs. The
distinctiveness of each graph presents a significant challenge in accurately representing all graphs.
In this work, we demonstrate the limitations of self-correlation in structure representation, such as
accurately representing islands, topologically symmetric graphs, and directed graphs. Although these
deficiencies may appear infrequently in large, undirected single graphs, they are prevalent and critical
in smaller to moderately-sized multiple graphs, e.g., molecules [ 12]. Conversely, we establish that de-
coding based on cross-correlation can significantly mitigate these limitations, offering improvements
in both undirected and directed graphs. Furthermore, the optimization of self-correlation-based GAE
has to proceed in a restricted space. On the other hand, the cross-correlation can double the argument
space that is not restricted during optimization. It makes the region of attraction smoother and easier
to converge, indicating the superior representational ability of cross-correlation.
Accordingly, we propose a novel GAE model, namely GraphCroc, which leverages cross-correlation
to node embeddings and a U-Net-like encoding-decoding procedure. Previous GAE models carefully
design the encoder for a faithful structure representation, yet keep the decoder as the straightforward
node correlation computation. Differently, GraphCroc retains the freedom of encoder design, facili-
tating its architecture design for downstream tasks, rather than structure representation. We define the
decoder as a mirrored architecture of the encoder, to gradually reconstruct the graph structure. The
encoder shapes the down-sampling of GraphCroc, while the decoder half performs the up-sampling
for structural reconstruction. In addition, regarding the unbalanced population of zeros and ones in
graph structure, i.e., sparse adjacency matrix, we define loss balancing on node connections.
We highlight our contributions as follows:
‚Ä¢We analyze the representational capabilities of self-correlation within GAE encoding, high-
lighting its limitations. Furthermore, we elaborate how cross-correlation addresses these
deficiencies, facilitating a smoother optimization process.
‚Ä¢We propose GraphCroc, a cross-correlation-based GAE that integrates seamlessly with GNN
architectures for graph tasks as its encoder, and structures a mirrored decoder. GraphCroc
offers superior representational capabilities, especially for multiple graphs.
‚Ä¢To the best of our knowledge, this is the first evaluation of structural reconstruction using
GAE models on graph tasks. Besides, we assess the performance of our GraphCroc model
integrated with other GAE strategies and on various downstream tasks.
‚Ä¢We evaluate the potential of GraphCroc in domain-specific applications, such as whether
a GAE focused on structural reconstruction could be an attack surface for edge poisoning
attacks, given the effectiveness and stealth of adversarial attacks using AE in vision tasks.
2 GAE Structural Reconstruction Analysis
2.1 Preliminary
Graph Neural Network As Graph Neural Networks (GNNs) have been defined in various ways,
without loss of generality, we adopt the computing flow in [ 45] to define the graph structure and a
general GNN model. A graph G= (V, E)comprises nnodes, each with a feature represented by a
d-dimensional vector, resulting in a feature matrix X‚ààRn√ód. The set of edges Eis depicted by
an adjacency matrix A‚àà {0,1}n√ón, which indicates the connections between nodes in G. A GNN
model f(X, A)is utilized to summarize graph information for downstream tasks.
The feed-forward propagation of the l-th layer in GNN f(¬∑)is
hl+1=œÉ
ÀÜDl‚àí1
2ÀÜAlÀÜDl‚àí1
2hlWl
(1)
hl‚ààRn√ódlis the input feature of the l-th layer, where h1=Xanddlis the feature dimension of
each node specified by each layer. ÀÜAl=Al+Iis the adjacency matrix (self-loop added) of the
input graph structure in each layer. Note that ÀÜAlwill be consistent in the absented pooling layer
scenario, such as the node classification task, yet it can vary along GNN layers in graph tasks due to
the introduction of graph pooling layers [ 9]. Subsequently, we use the diagonal node degree matrix
2ÀÜDlofÀÜAlto normalize the aggregation. For layer components, Wlis the weight matrix and œÉis the
activation function in the l-th layer.
Graph Autoencoder The na√Øve GAE is proposed to reconstruct the adjacency matrix Aof a graph
Gthrough node embedding Z:
encoder: Z= Œ¶(Z|G) =f(X, A),decoder: ÀúA= Œò( A|Z) =sigmoid (ZZT) (2)
GAE first encodes the graph Gthrough a general GNN model f(¬∑)(a.k.a. inference model), resulting
in node embeddings in the latent space Z‚ààRn√ód‚Ä≤where d‚Ä≤is the latent vector dimension for each
node. The generative model is non-parameterized, but the tensor product of the node embeddings.
It measures the correlation between each node pairs, i.e., sigmoid (zT
izj), normalized as the link
probability by the logistic sigmoid function. Usually, the predicted connection can be defined as an
indicator function I(ÀúAi,j) =I(ÀúAi,j‚â•th), e.g., th= 0.5.
With the same functionality, GAE has been improved with other enhancements, such as variational
embedding [ 19], MLP-based decoder [ 33], masking on features [ 15] and edges [ 23]. Note that other
correlation measurements in the decoder are also proposed, such as Euclidean distance between node
embedding [ 26], i.e., ÀúAi,j=sigmoid (C(1‚àí‚à•zi‚àízj‚à•2
2)), where Cis a temperature hyperparameter.
In general, they predict the connection between node pairs by measuring the similarity, such as inner
product and L2-norm, from the same embedding space. Related work is discussed in Appendix A.
2.2 Deficiencies of Self-Correlation on Graph Structure Representation
To generalize the discussion, we set the representation capability of the encoder as unrestricted, i.e.,
allowing Zto be generated through any potentially optimal encoding method. On the decoding side,
self-correlation is applied following Eq.2. We identify and discuss specific (sub)graph structures that
are poorly addressed by current self-correlation methods:
Islands (Non Self-Loop). Both the inner product and the L2-norm of a node embedding pair
fail to accurately represent nodes without self-loops, where Ai,i= 0. Given that zT
izi‚â•0and
C(1‚àí ‚à•zi‚àízi‚à•2
2) =C >0, the predicted value I(ÀúAi,i)defaults to 1 when threshold 0.5 is applied
to the sigmoid output. This limitation underlies the common homophily assumption in previous
research [ 42], that all nodes contain self-loops; it treats self-loops as irrelevant to the graph‚Äôs structure.
However, there is a huge difference between the self-connection on one node and the inter-connection
between nodes in some scenarios; for example, on heterophilous graphs [ 51], nodes are prone to
connect with other nodes that are dissimilar ‚Äî such as fraudster detection.
ùëã1
ùëãùëô1
ùëãùëô2ùëãùëü1
ùëãùëü2ùëã1ùëã2
ùëã3
ùëã4 ùëã5ùëã6
ùëã2
Figure 1: Two examples
of the topological symmetric
graphs. The left graph is axis-
symmetric; the right graph is
centrosymmetric.Topologically Symmetric Structures. If graph structure is symmet-
ric along an axis or a central pivot, as demonstrated in Figure 1, the
self-correlation method cannot represent these structures as well.
Definition 2.1 (Topologically Symmetric Graph). A symmetric
graph G= (V, E)has the structure and node features topologically
symmetric either along a specific axis or around a central node.
For an axisymmetric graph, the feature matrix is denoted as X=
{X1, . . . , X n1} ‚à™ { Xl1, . . . , X ln2} ‚à™ { Xr1, . . . , X rn2}, such that
n1+ 2n2=n.Xirepresents the node features on the axis, and for
each paired node off the axis, Xli=Xri. The connections are also
symmetric, satisfying Ali,:=Ari,:. In a centrosymmetric graph, the
pivot node has feature X1, and other nodes share the same feature X2=. . .=Xn‚àí1. Additionally,
the adjacency relationships for these nodes are identical, with Ai,:=Aj,:for all i, j‚àà[2, n].
Lemma 2.2. Given an arbitrary topologically symmetric graph G= (V, E)and an encoder f(X, A),
the self-correlation decoder output will always have I(ÀúAli,ri) = 1 .
Proof. Due to the symmetry on graph G= (V, E)from the definition above, we have f(Xi, Ai,:) =
f(Xj, Aj,:)for nodes iandjthat are symmetric about the axis or pivot. Thus, we can derive zi=zj.
For the prediction on link between iandj, we have ÀúAi,j=sigmoid (zT
izj) =sigmoid (zT
izi)‚â•0.5.
Similarly, for the L2-norm method, we have ÀúAi,j=sigmoid (C(1‚àí ‚à•zi‚àízj‚à•2
2)) = sigmoid (C(1‚àí
‚à•zi‚àízi‚à•2
2)) = sigmoid (C)>0.5. In both decoding methods, the decoder is prone to predict the
edge between two symmetric as positive, I(ÀúAi,j) = 1 .
3Consequently, the self-correlation method will indicate a positive connection between two symmetric
nodes, regardless of their real connection status on the graph G. Note that for variational GAE, Zis
sampled from a Gaussian distribution whose mean and variance come from GNN models; thus it still
follows the above lemma, even if randomness is involved during the encoding.
Directed Graph. Another straightforward deficiency of the self-correlation method is that it always
generates a symmetric adjacency matrix, ÀúA, which is not suitable for directed graphs that require an
asymmetric adjacency matrix. This issue is also acknowledged in [ 20], which proposes a solution
involving dual encoding using the Weisfeiler-Leman (WL) algorithm [ 41] and node label coloring.
However, this solution constrains the encoder to a dual-channel structure while maintaining the
same decoding method as described in Eq. 2. This restriction can limit the flexibility of the encoder
architecture, making it less adaptable for various downstream tasks.
Furthermore, we conduct a theoretical analysis of the dimensional requirements of node embedding
to represent graph structures using self-correlation, in Appendix B.
2.3 Our Cross-Correlation Approach for Better Structural Representation
Instead of self-correlation, we advocate cross-correlation to reconstruct the graph structure, denoted
byÀúA=sigmoid (PQT), where P, Q‚ààRn√ód‚Ä≤. This approach allows us to decouple the variables
involved in calculating the correlation, thus overcoming the inherent limitations of self-correlation.
2.3.1 How Does Cross-Correlation Mitigate Deficiencies of Self-Correlation?
Expressing Islands. For each node i‚àà[1, n], the sign of pT
iqican be flexibly determined by pi
andqi, allowing it to be either positive or negative. Consequently, the presence of an island can be
effectively modeled using ÀúAi,j=sigmoid (pT
iqi)orsigmoid (C(1‚àí ‚à•pi‚àíqj‚à•2
2)). This approach
avoids the limitations associated with self-correlation, which restricts the sigmoid input to positive.
Expressing Symmetric Structure. Cross-correlation is particularly effective in capturing topological
symmetric structures. Given a node pair (i, j)that is topologically symmetric about an axis or pivot,
for undirected graphs, we have pi=pjandqi=qj. However, since piandqj(as well as pjandqi)
are not directly dependent on each other, the sign of pT
iqj=pT
jqican be either positive or negative.
Therefore, I(ÀúAi,j) =I(sigmoid (pT
iqj))is able to yield 0 or 1, depending on the specific values of
node embedding piandqj. This flexibility can be supported by the L2-norm decoding as well.
Expressing Directed Graph. A similar interpretation extends to the representation of directed
graphs. For two nodes iandj, the directed edges can be defined by pT
iqjfor one direction and
pT
jqifor the other. Since these four latent vectors do not have explicit dependencies among them,
the directions of the edges can be independently determined using cross-correlation, capturing the
directional connections between nodes.
In Appendix C, we further discuss and visualize how our cross-correlation approach represents these
specific graph structures.
2.3.2 Cross-Correlation Provides Smoother Optimization Trace
We highlight the superiority of cross-correlation over self-correlation in the optimization process of
GAE training. Considering the decoder optimization problem, where we aim to satisfy the constraints:
(self-correlation) I(sigmoid (zT
izj)) =Ai,j,(cross-correlation) I(sigmoid (pT
iqj)) =Ai,j(3)
for each element in matrix A. This involves finding Z‚ààRn√ód‚Ä≤orP, Q‚ààRn√ód‚Ä≤that maximize the
number of satisfied constraints. Additionally, for an undirected graph, the symmetry Ai,j=Aj,i
imposes the requirement that I(sigmoid (pT
iqj)) =I(sigmoid (pT
jqi)), ensuring that both pT
iqjand
pT
iqjshould have the same sign.
In the case of cross-correlation, where PandQare independently determined, and all constraints
can well align with the generation of PandQ. However, this is not the case in self-correlation,
where zT
izj=zT
jziinherently overloads the symmetry constraint. For example, if Ai,j=Aj,i= 1,
we only require pT
iqj>0andpT
jqi>0for cross-correlation, while they become restrictive as
41
 0 1 2
Dim. 1
 (a) Trajectory under self-cor. decoding1
0123Dim. 2Node 1 (z1)
Node 2 (z2)
4
 2
 0
Dim. 1
 (b) Trajectory under cross-cor. decoding012345Dim. 2Node 1 (p1)
Node 2 (p2)
0 20 40 60 80 100
Iteration
 (c) Loss trace over iterations0.40.60.81.01.2BCE Lossself-correlation
cross-correlation
0 20 40 60 80 100
Iteration
 (d) Diagonal value dist. over iterations5
051015Diagonal Value Histogramself-correlation
cross-correlationFigure 2: Training comparison between self-correlation and cross-correlation on PROTEINS subset
(64 graphs). In (a) and (b), we demonstrate the trajectory of the first two node embeddings in the
first graph during training iteration, where the star mark is the end-point of training. We apply PCA
for dimension compression and the Savitzky-Golay filter to help trace visualization. We also set
zi=piÃ∏=qiat the beginning of optimization to ensure that the traces of ziin (a) and piin (b) start
from the same point. (c) provides the BCE loss trace of this graph during training, showing that
cross-correlation can lead the reconstruction to a better solution. (d) demonstrates the distribution of
diagonal elements during training, i.e., zT
izifor self-correlation and pT
iqifor cross-correlation. The
results of other graphs are provided in Appendix G.2.
zT
izj=zT
jzi>0in self-correlation. Cross-correlation offers a broader argument space, providing
more freedom to find solutions that satisfy the constraints for reconstructing A. By employing
gradient method during optimization, this process can be understood as the trajectory of Pand
Qbeing unrestricted in the argument space Rn√ó2d‚Ä≤, while the trajectory of Zis confined within
a restricted space as Rn√ód‚Ä≤. Therefore, cross-correlation facilitates smoother and more efficient
convergence during optimization. Note that this restriction comes from the nature of self-correlation
and cross-correlation, which is interpreted as the space reduction. During optimization, we can first
find (local) optimal PandQ, then apply (PQT+QPT)/2to interpret it as a symmetric matrix ÀúA,
likeZZT; this procedure can still outperform direct optimization on ZZT.
Validation on PROTEINS Here, we make a quick validation of our discussion on a subset of
PROTEINS [ 2] that is a graph task, in Figure 2. By comparing Figure 2(a) and (b), the trajectory of
node embedding during training demonstrates the superiority of cross-correlation over self-correlation.
As the node embeddings are evolving under self-correlation, they frequently change their direction;
this indicates that the region of attraction between the start and end point is not smooth and may follow
a restricted manifold, thus the embedding cannot well converge to the local minimum. On the other
hand, the region of attraction under cross-correlation is much smoother and easy to guide the node
embedding to a better solution. This can also be evidenced by the lower loss under cross-correlation in
Figure 2(c). Another concern of cross-correlation is that pT
iqicannot naturally satisfy Ai,i= 1while
zT
iziin self-correlation is able to. Nevertheless, pT
iqican be encouraged to perform with positive
sign, proved by Figure 2(d), that all the diagonal element reconstruction pT
iqiunder cross-correlation
can achieve positive sign at the end of training, leading to I(sigmoid (pT
iqi)) =Ai,i= 1.
3 GraphCroc: Cross-Correlation-Based Graph Autoencoder
Encoder Architecture Our work scales the normal single-graph representation to multi-graph
for graph tasks, and we do not specify the encoder structure, but free it as the downstream tasks
require. For graph tasks, the GNN model has a sequence of message passing layer and pooling layer
to coarsen the graph to higher-level representation. In the end, a readout layer [ 8,9,31] is applied to
summarize the graph representation to the latent space. We define the encoder as
encoder (ours): Z‚Ä≤= Œ¶(Z‚Ä≤|G) =f(X, A) (4)
where Z‚Ä≤‚ààRn‚Ä≤√ód‚Ä≤has a reduced number n‚Ä≤of node embeddings. Besides, we exclude the readout
layer from the encoder, yet assign it to the start of downstream tasks.
Two-Way Decoder for Cross-Correlation To separately produce two node embeddings, PandQ,
we divide the decoding process into two parallel and self-governed decoders. Unlike [ 20], we leave
5(ùëã,ùê¥)
GCN
Norm(A)
GCN(X,A)
LayerNormùê¥ ‚ÑéGCN(‚Ñé1,ùê¥) (‚Ñé2,ùê¥‚Ä≤2)
GCN &
Pooling (‚Ñéùêø,ùê¥‚Ä≤ùêø)
‚Ä¶GCN
(‚Ñé‚Ä≤ùêø,ùê¥‚Ä≤ùêø)Unpooling
& GCN Linear
LinearùëÉ
ùëÑsigmoid(ùëÉùëÑùëá)GCN
Pooling
‚Ä¶
‚Ä¶GCNUnpoolingEncoder
DecoderDownstream
tasksFigure 3: GraphCroc architecture. The encoder is generally demonstrated as a L+ 1-layer GNN.
The decoder has two paths to generate the node embedding for cross-correlation; each decoder is a
mirrored structure of the encoder. Each decoder layer accepts the node feature and graph structure
information from the corresponding encoder layer. Notably, the GCN module shown on the right
incorporates skip connections and normalization to improve performance.
the encoder design to better suit specific downstream tasks, and focus primarily on the reconstruction
challenges within the decoder design. Still with the latent vector Z=f(X, A)generated by the
encoder, we define our decoder as follow:
decoder (ours): ÀúA=sigmoid (PQT), P =g1(Z,{A‚Ä≤, h‚Ä≤}), Q =g2(Z,{A‚Ä≤, h‚Ä≤})(5)
g1(¬∑)andg2(¬∑)are two individual GNNs with the same structure, which take as input the latent
vectors Z, the adjacency matrix groups {A‚Ä≤}and node feature groups {h‚Ä≤}from the encoder (as
discussed in Section 3). Here, {A‚Ä≤, h‚Ä≤}is required for graph tasks that involves pooling/unpooling. In
Appendix D, we further discuss why the two embeddings, PandQ, in our decoder do not converge
to each other, thereby preventing convergence to self-correlation decoding.
Autoencoder Architecture In Figure 3, we present the autoencoder architecture, GraphCroc. The
design of the encoder/decoder pair is inspired by the Graph U-Net structure [ 9], originally proposed
for classification tasks. While the cross-correlation kernel remains unchanged, the encoder/decoder
configuration can be tailored to various applications. Nevertheless, we utilize the Graph U-Net
structure as a case study to demonstrate the effectiveness of cross-correlation.
The GraphCroc architecture follows the encoder formulations from Eq. 4 for multiple graphs and
the decoder from Eq.5. During the encoding process, the graph architecture A‚Ä≤is captured at each
layer, which is then utilized in the corresponding unpooling layers to reconstruct the graph structure,
as detailed in [ 9]. Additionally, skip connections enhance the model by capturing the node features
h‚Ä≤at each encoder layer and integrating them ‚Äî either through addition or concatenation ‚Äî into
the node features of the corresponding decoder layer. Importantly, we emphasize the significance
of implementing layer normalization [ 1] following each GNN layer. Although often overlooked in
previous GAE studies due to the typically small number of layers, layer normalization is crucial for
regulating gradient propagation as the number of layers increases.
Loss Balancing The training on GAE is highly biased given the sparsity of the adjacency matrix.
In practice, it is quite common that zero elements in Atake the majority, e.g., around 90% in
PROTEINS [ 2]. For a certain Aand its estimation ÀúA, we denote the vanilla loss function as
L=Pc0
i=1L0
i+Pc1
j=1L1
j, where in total c0zeros and c1ones in A, andL0/L1is their corresponding
loss on each element. Since c0‚â´c1, the zero part dominates the loss function, inducing the decoder
to predict zeros. Thus, we apply a scaling factor for each item:
L(ÀúA, A) =Œ±0c0X
i=1L0
i+Œ±1c1X
j=1L1
j, Œ± 0=c0+c1
2c0, Œ± 1=c0+c1
2c1(6)
The derivation is provided in Appendix E.
6Table 1: The AUC score of reconstructing the adjacency matrix in
graph tasks. We reproduce the most representative global GAE
methods with different decoding strategies. The self-correlation
methods include na√Øve GAE, variational GAE [ 19], L2-norm
(EGNN) [ 30], and our GraphCroc under self-correlation; the cross-
correlation methods include directed representation (DiGAE) [ 20]
and our GraphCroc. The best results are in bold , and the second
bests are underlined .
Self-Correlation Cross-Correlation
GAE VGAE EGNN GraphCroc (SC) DiGAE GraphCroc
PROTEINS 0.4750 0.4764 0.9608 0.9781 0.7577 0.9958
IMDB-B 0.7556 0.7105 0.9873 0.9892 0.7500 0.9992
Collab 0.7885 0.7946 0.9947 0.9926 0.7973 0.9989
PPI 0.6330 0.6239 ‚àí‚Ä†0.9764 0.8364 0.9831
QM9 0.5376 0.4852 0.9984 0.9967 0.7791 0.9987
‚Ä†Out of memory during training, see Appendix F.2.
GAEVGAE EGNN
ours(SC)DiGAEours0.00.20.40.60.8WL-testFigure 4: WL-test results on differ-
ent GAE methods, in the IMDB-B
task.
4 Evaluation
4.1 Experimental Setup
Dataset We assess GraphCroc in various graph tasks. Specifically, we utilize datasets for molecule,
scaling from small (PROTEINS [ 2]) to large (Protein-Protein Interactions (PPI) [ 12], and QM9
[29]), for scientific collaboration (COLLAB [ 46]), and for movie collaboration (IMDB-Binary [ 46]).
Further details on these datasets are provided in Appendix F.1.
GraphCroc Structure During evaluation, our GraphCroc structure, especially the GCN layer
number, is determined by the scale of graph. Assuming the average node number in a graph task is ¬Øn,
we set up an empirical layer number Las
¬Øn¬∑Œ†L
i=1(0.9‚àíi)
= 2, so that the number of nodes can
be smoothly reduced to two at the end of encoding [ 9]. Besides, we use node embedding dimension
d‚Ä≤‚âà¬Øn, following analysis in Appendix B. Other detail is provided in Appendix F.2.
4.2 GraphCroc on Structural Reconstruction
Table 1 demonstrates the graph reconstruction capabilities of GAE models for multi-graph tasks by
presenting ROC-AUC scores on adjacency matrix reconstruction. We compare our model against
prevalent self-correlation kernels in GAEs and a cross-correlation method designed for directed graphs.
Comparing the basic inner-product methods, GAE and VGAE [ 19], the variational extension in VGAE
does not obviously improve the performance over GAE in graph tasks. However, by enhancing
the GAE architecture itself, i.e., using our GraphCroc architecture, the reconstruction efficacy is
significantly increased; GraphCroc under self-correlation can achieve 3/5 second bests among all GAE
models. This underscores the graph representation capability of the U-Net structure [ 9]. Additionally,
the L2-norm decoding method generally outperforms the inner-product approach (GAE and VGAE),
although it struggles with large graphs such as PPI, which requires too much GPU memory to
be trained during our reproduction. On the other hand, the cross-correlation method (DiGAE)
provides a consistent albeit modest representation across different graph sizes. This demonstrates
the cross-correlation ability to represent multiple graphs in various scenarios. However, the GNN
architecture limits its capability to capture enough structural information. By integrating the strengths
of cross-correlation with the U-Net architecture, our GraphCroc GAE model consistently excels over
other methods, offering significant advantages in all the graph tasks tested. Even on large graphs,
such as PPI with over 2,000 nodes, GraphCroc can still achieve an AUC score over 0.98. To further
demonstrate the effectiveness of the cross-correlation mechanism, we evaluate GAE models with
alternative architectures under cross-correlation, in Appendix G.1. The reconstruction results for
these architectures are consistent with those in Table 1, though they exhibit slightly lower AUC
compared to GraphCroc.
While the AUC score indicates how well a model reconstructs edges, it does not measure the model‚Äôs
ability to exactly reconstruct a whole graph. To address this, we employ the Weisfeiler-Leman
isomorphism test (WL-test)[ 41], which assesses the structural equivalence between the original and
reconstructed graphs. Figure 4 presents normalized WL-test results, i.e., the pass rates across all test
7PROTEINSGround Truth
 GraphCroc
 GAE
 VGAE
 EGNN
 DIGAE
IMDB-B
 COLLAB
Figure 5: The reconstruction visualization. Other reconstructions are provided in Appendix G.5.
graphs, in the IMDB-B task. Our GraphCroc model significantly outperforms other GAE methods,
while the self-correlation variant also delivers superior performance. Interestingly, while the L2-norm
achieves a high AUC score, it is feeble to well reconstruct the entire graphs; this indicates there are
some connection patterns in graph inherently hard to be captured by L2-norm representation. Other
results of WL test are provided in Appendix G.3, which demonstrates similar observations as above.
In Figure 5, we select a representative graph from each of the PROTEINS, IMDB-B, and COLLAB
tasks to visually compare the structural reconstruction from GAE models. It is evident that GraphCroc
performs well in accurately recovering graph edges. GAE methods with inner-product and vanilla
GNN architectures, such as GAE, VGAE, and DiGAE, tend to overpredict edge connections. Mean-
while, EGNN performs adequately within node clusters, but struggles with inter-cluster connections.
This echos our discussion about the partial representation deficiency in L2-norm.
4.3 GraphCroc on Other GAE Strategies
0 50 100 150 200
Epoch0.800.850.900.951.00ROC-AUC baseline
variational
masking
L2-norm
Figure 6: The AUC score of test-
ing graphs in PROTEINS task, with
different decoding methods.To make cross-correlation more pronounced in our evaluation,
the above experiments implement only the basic inner-product
representation, i.e., sigmoid (PQT). In addition, previous stud-
ies have extensively explored data augmentation and training
enhancements to optimize GAE models. Specifically, we exam-
ine the performance of GAE with cross-correlation applied to
different training strategies in Figure 6, using the PROTEINS
dataset as a case study.
We evaluate three other prevalent decoding enhancements to
compare their performance with the standard inner-product
decoding (baseline) under the cross-correlation method. The
variational decoding [ 19] generates node embeddings from a
Gaussian distribution, with mean/std determined by the decoder
outputs. Although a similar final AUC was achieved, it falls
short of the baseline on the PROTEINS task at early convergence. For the other two strategies, edge
masking [ 35] and L2-norm representation [ 26], they facilitate faster convergence during the initial
training stages. However, we find that the enhancement of these strategies is highly dependent on
graph tasks. Our further analysis on other graph tasks (Appendix G.4) demonstrates that the L2-norm
and masking could converge to worse structural reconstructions than our baseline training. Therefore,
we still advocate our training and representing methods for GAE models on various graph tasks.
8Table 2: Evaluation on graph classification tasks. We compare our model with other GNN meth-
ods, such as unsupervised learning (Infograph [ 34]), contrastive learning (GraphCL [ 48] and In-
foGCL [ 44]), and generative learning (GraphMAE [ 15], S2GAE [ 35] and StructMAE [ 25]). The
encoder of our model is extracted from GraphCroc, and is cascaded with a randomly-initialized
3-layer classifier. We train our models by only fine-tuning for 10 epochs or training for 100 epochs.
The best results are in bold , and the second bests are underlined . Results are averaged on 5 runs.
Infograph GraphCL InfoGCL GraphMAE S2GAE StructMAEours
(10-epoch)ours
(100-epoch)
PROTEINS 74.44 74.39 ‚àí 75.30 76.37 75.97 73.99¬±1.3279.09¬±1.63
IMDB-B 73.03 71.14 75.10 75.52 75.76 75.52 76.69¬±1.0278.75¬±1.35
COLLAB 70.65 71.36 80.00 80.32 81.02 80.53 81.70¬±0.5482.40¬±0.20
4.4 GraphCroc on Graph Classification Tasks
One common application of autoencoder models is leveraging their potent encoders for downstream
tasks. We evaluate our GraphCroc model by employing its encoder in graph classification tasks,
as summarized in Table 2. Notably, generative approaches like GraphMAE, S2GAE, StructMAE,
and our GAE model tend to surpass traditional unsupervised and contrastive learning methods.
Although contrastive learning incorporates negative sampling, its effectiveness is limited in multi-
graph tasks. This finding corroborates the observations in Tab.4 of [ 44], which indicate that while
negative sampling substantially boosts performance in single-graph tasks (e.g., node classification),
it has little impact on graph classification tasks. In contrast, GAE models deliver robust graph
representations, particularly for small-to-moderate-sized graphs, enhancing their utility in graph
classification. Furthermore, our GraphCroc model significantly outperforms self-correlation methods
(GraphMAE and S2GAE) in representing graph structures, demonstrated in Table 1, enabling the
encoder to effectively capture the structural information of input graphs. Consequently, classifiers
leveraging our encoder can achieve high performance with finetuning over only several epochs.
Continued optimization of our classification models promises to further elevate their performance in
graph classification tasks, surpassing other GAE-based models.
4.5 GAE Attack Surface Analysis
Research in vision tasks demonstrates that manipulating the latent space with perturbations enables
AE to produce adversarial examples with stealthiness and semanticity [ 6,16,32,40,43]. Given
AE‚Äôs success in vision domain, we raise the concern ‚Äî whether a GAE can be utilized to generate
adversarial graph structures by modifying the latent vectors? Current graph adversarial attacks
directly modify the input of GNNs, highly inefficient due to the discreteness of graph structures [ 7,37].
By directly conducting attacks in the latent space, GAE could be a potentially efficient attack surface.
Table 3: Accuracy and modified edges for adversarial
attack, leveraging latent perturbation on GAE. A lower
accuracy indicates that the latent perturbation can better
pass to the reconstructed graph. A higher percentage of
modified edges indicates a larger attack cost.
Clean Random PGD [27] C&W [4]
Acc. ‚àÜedge Acc. ‚àÜedge Acc. ‚àÜedge
IMDB-M 55.3 53.5 4.79% 45.7 24.5% 39.7 17.4%
PROTEINS 82.5 77.1 5.01% 57.4 5.63% 41.7 23.7%
COLLAB 81.3 70.0 5.90% 28.8 35.9% 27.3 8.29%In Table 3, we demonstrate the effect of
small perturbations on the latent space us-
ing random noise injection, PGD [ 27], and
C&W adversarial noise injection [ 7] on
graph classification tasks. We limit all at-
tacks on the latent space with a maximum
query number of 400and report the classi-
fication accuracy of perturbed graphs and
the number of edge changes. Note that we
focus solely on the structure modification
without changes on the node features. Our observations indicate that conducting adversarial attacks
on the latent space of the graph autoencoder effectively reduces model accuracy, although it could
yield significant edge changes. Compared to adversarial attacks on graph structures using discrete
optimization methods, our latent attacks demonstrate comparable performance in terms of accuracy
and can be performed in batches with high efficiency. Nevertheless, due to the discrete nature of
graph structures, the distortion in edge changes is hard to be always controlled at a low level. Our
evaluation of GraphCroc‚Äôs latent space reveals a potential vulnerability, indicating that adversarial
attacks on a graph autoencoder‚Äôs latent space can provide efficient structural adversarial attacks. More
detail on the adversarial attack with GAE is discussed in Appendix F.3.
95 Conclusion
Graph autoencoders (GAEs) are increasingly effective in representing graph structures. In our
research, we identify significant limitations in the self-correlation approach employed in the decoding
processes of prevalent GAE models. Self-correlation inadequately represents certain graph structures
and requires optimization within a constrained space. To address these deficiencies, we advocate cross-
correlation as the decoding kernel. We propose a novel GAE model, GraphCroc, which incorporates
cross-correlation decoding and is built upon a U-Net architecture, enhancing the flexibility in GNN
design. Our evaluations demonstrate that GraphCroc outperforms existing GAE methods in terms
of graph structural reconstruction and downstream tasks. In addition, we propose the concern that
well-performed GAEs could be a surface for adversarial attacks.
Acknowledgments and Disclosure of Funding
We thank Shaolei Ren from UC Riverside for the valuable discussions that helped shape this work.
This work is supported in part by the U.S. National Science Foundation under Grants OAC-2319962,
CNS-2239672, CNS-2153690, CNS-2326597, CNS-2247892, and SaTC-1929300.
References
[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450 , 2016.
[2]Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch√∂nauer, SVN Vishwanathan, Alex J
Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics ,
21(suppl_1):i47‚Äìi56, 2005.
[3]Andrei Broder, Ravi Kumar, Farzin Maghoul, Prabhakar Raghavan, Sridhar Rajagopalan,
Raymie Stata, Andrew Tomkins, and Janet Wiener. Graph structure in the web. Computer
networks , 33(1-6):309‚Äì320, 2000.
[4]Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing
ten detection methods. In Proceedings of the 10th ACM workshop on artificial intelligence and
security , pages 3‚Äì14, 2017.
[5]Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. Efficient and degree-guided graph
generation via discrete diffusion modeling. arXiv preprint arXiv:2305.04111 , 2023.
[6]Antonia Creswell, Anil A Bharath, and Biswa Sengupta. Latentpoison-adversarial attacks on
the latent space. arXiv preprint arXiv:1711.02879 , 2017.
[7]Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack
on graph structured data. In International conference on machine learning , pages 1115‚Äì1124.
PMLR, 2018.
[8]Micha√´l Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. Advances in neural information processing
systems , 29, 2016.
[9]Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning ,
pages 2083‚Äì2092. PMLR, 2019.
[10] C Lee Giles, Kurt D Bollacker, and Steve Lawrence. Citeseer: An automatic citation indexing
system. In Proceedings of the third ACM conference on Digital libraries , pages 89‚Äì98, 1998.
[11] Rafael G√≥mez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos√© Miguel Hern√°ndez-Lobato,
Benjam√≠n S√°nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel,
Ryan P Adams, and Al√°n Aspuru-Guzik. Automatic chemical design using a data-driven
continuous representation of molecules. ACS central science , 4(2):268‚Äì276, 2018.
[12] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems , 30, 2017.
10[13] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Advances in neural information processing systems , 30, 2017.
[14] Xu Han, Xiaohui Chen, Francisco JR Ruiz, and Li-Ping Liu. Fitting autoregressive graph
generative models through maximum likelihood estimation. Journal of Machine Learning
Research , 24(97):1‚Äì30, 2023.
[15] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang.
Graphmae: Self-supervised masked graph autoencoders. In Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining , pages 594‚Äì604, 2022.
[16] Surgan Jandial, Puneet Mangla, Sakshi Varshney, and Vineeth Balasubramanian. Advgan++:
Harnessing latent layers for adversary generation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision Workshops , pages 0‚Äì0, 2019.
[17] Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecular
graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design ,
30:595‚Äì608, 2016.
[18] John A Keith, Valentin Vassilev-Galindo, Bingqing Cheng, Stefan Chmiela, Michael Gastegger,
Klaus-Robert Muller, and Alexandre Tkatchenko. Combining machine learning and computa-
tional chemistry for predictive insights into chemical systems. Chemical reviews , 121(16):9816‚Äì
9872, 2021.
[19] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308 , 2016.
[20] Georgios Kollias, Vasileios Kalantzis, Tsuyoshi Id√©, Aur√©lie Lozano, and Naoki Abe. Directed
graph auto-encoders. In Proceedings of the AAAI conference on artificial intelligence , volume 36,
pages 7211‚Äì7219, 2022.
[21] Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B Aditya Prakash, and Chao Zhang.
Autoregressive diffusion model for graph generation. In International conference on machine
learning , pages 17391‚Äì17408. PMLR, 2023.
[22] Matt J Kusner, Brooks Paige, and Jos√© Miguel Hern√°ndez-Lobato. Grammar variational
autoencoder. In International conference on machine learning , pages 1945‚Äì1954. PMLR, 2017.
[23] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng,
Zibin Zheng, and Weiqiang Wang. What‚Äôs behind the mask: Understanding masked graph
modeling for graph autoencoders. In Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining , pages 1268‚Äì1279, 2023.
[24] Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, Will Hamilton, David K Duvenaud, Raquel
Urtasun, and Richard Zemel. Efficient graph generation with graph recurrent attention networks.
Advances in neural information processing systems , 32, 2019.
[25] Chuang Liu, Yuyao Wang, Yibing Zhan, Xueqi Ma, Dapeng Tao, Jia Wu, and Wenbin Hu.
Where to mask: Structure-guided masking for graph masked autoencoders. arXiv preprint
arXiv:2404.15806 , 2024.
[26] Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing
flows. Advances in Neural Information Processing Systems , 32, 2019.
[27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 ,
2017.
[28] Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating
the construction of internet portals with machine learning. Information Retrieval , 3:127‚Äì163,
2000.
[29] Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration
of 166 billion organic small molecules in the chemical universe database gdb-17. Journal of
chemical information and modeling , 52(11):2864‚Äì2875, 2012.
11[30] Vƒ±ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural
networks. In International conference on machine learning , pages 9323‚Äì9332. PMLR, 2021.
[31] Simon Schaefer, Daniel Gehrig, and Davide Scaramuzza. Aegnn: Asynchronous event-based
graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 12371‚Äì12381, 2022.
[32] Nitish Shukla and Sudipta Banerjee. Generating adversarial attacks in the latent space. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
730‚Äì739, 2023.
[33] Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs
using variational autoencoders. In Artificial Neural Networks and Machine Learning‚ÄìICANN
2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October
4-7, 2018, Proceedings, Part I 27 , pages 412‚Äì422. Springer, 2018.
[34] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and
semi-supervised graph-level representation learning via mutual information maximization. arXiv
preprint arXiv:1908.01000 , 2019.
[35] Qiaoyu Tan, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, and Xia Hu. S2gae:
self-supervised graph autoencoders are generalizable learners with graph masking. In Pro-
ceedings of the sixteenth ACM international conference on web search and data mining , pages
787‚Äì795, 2023.
[36] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua
Bengio, et al. Graph attention networks. stat, 1050(20):10‚Äì48550, 2017.
[37] Xingchen Wan, Henry Kenlay, Robin Ru, Arno Blaas, Michael A Osborne, and Xiaowen
Dong. Adversarial attacks on graph classifiers via bayesian optimisation. Advances in Neural
Information Processing Systems , 34:6983‚Äì6996, 2021.
[38] Chun Wang, Shirui Pan, Guodong Long, Xingquan Zhu, and Jing Jiang. Mgae: Marginalized
graph autoencoder for graph clustering. In Proceedings of the 2017 ACM on Conference on
Information and Knowledge Management , pages 889‚Äì898, 2017.
[39] Shoujin Wang, Liang Hu, Yan Wang, Xiangnan He, Quan Z Sheng, Mehmet A Orgun, Longbing
Cao, Francesco Ricci, and Philip S Yu. Graph learning based recommender systems: A review.
arXiv preprint arXiv:2105.06339 , 2021.
[40] Shuo Wang, Shangyu Chen, Tianle Chen, Surya Nepal, Carsten Rudolph, and Marthie Grobler.
Generating semantic adversarial examples via feature manipulation in latent space. IEEE
Transactions on Neural Networks and Learning Systems , 2023.
[41] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra
which appears therein. nti, Series , 2(9):12‚Äì16, 1968.
[42] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE transactions on neural networks and
learning systems , 32(1):4‚Äì24, 2020.
[43] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song. Generating
adversarial examples with adversarial networks. arXiv preprint arXiv:1801.02610 , 2018.
[44] Dongkuan Xu, Wei Cheng, Dongsheng Luo, Haifeng Chen, and Xiang Zhang. Infogcl:
Information-aware graph contrastive learning. Advances in Neural Information Processing
Systems , 34:30414‚Äì30425, 2021.
[45] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826 , 2018.
[46] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM
SIGKDD international conference on knowledge discovery and data mining , pages 1365‚Äì1374,
2015.
12[47] Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generat-
ing realistic graphs with deep auto-regressive models. In International conference on machine
learning , pages 5708‚Äì5717. PMLR, 2018.
[48] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen.
Graph contrastive learning with augmentations. Advances in neural information processing
systems , 33:5812‚Äì5823, 2020.
[49] Chengxi Zang and Fei Wang. Moflow: an invertible flow model for generating molecular graphs.
InProceedings of the 26th ACM SIGKDD international conference on knowledge discovery &
data mining , pages 617‚Äì626, 2020.
[50] Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng
Wang, Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and
applications. AI open , 1:57‚Äì81, 2020.
[51] Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai
Koutra. Graph neural networks with heterophily. In Proceedings of the AAAI conference on
artificial intelligence , volume 35, pages 11168‚Äì11176, 2021.
13A Related Work
Graph representation has been explored through various methods. Auto-regressive models [ 24,49,14]
generate graph structures by sequentially querying the connectivity between node pairs, which can
be computationally expensive for large graphs, e.g., n2queries required for the adjacency matrix.
Similarly, diffusion-based graph models [ 21] construct graph structures through multiple steps, such
as degree matrix reconstruction [ 5]. These methods primarily focus on graph generation, creating
rational graph structures from random noisy node islands.
In contrast, Graph Autoencoder (GAE) methods represent graph structures as node embeddings,
designed to reconstruct the graph either sequentially [ 47,22] or globally [ 33,19,15,30,20]. The
very beginning graph structure representation is proposed in [ 19] with self-correlation (applied with
Eq. 1 and 2) and further expresses the node embedding with a variational approach. Later, GAE
has been widely explored with sequential and global generating methods. For the sequential GAE
models, GraphRNN [ 47] proposes an autoregressive model, which generates graphs by summarizing
a representative set of graphs and decomposes the graph generation into a sequence of node and edge
formations. Similarly, [ 22] targets molecule generation and proposes to regard the graph structure
as a parse tree from a context-free grammar, so that the VGAE can apply encoding and decoding to
these parse trees. However, the sequential graph strategies usually are time-consuming or requiring
expensive processing.
On the other hand, the global methods, which directly encode the graph in latent space and decode
to the entire graph structure, have better scalability on larger and complicated graph structures and
can be time efficient. GraphV AE [ 33] follows the VGAE idea and proposes an autoencoder model
that can generate the graph structure, node features, and edge attributes all at once. EGNN [ 30,26]
decodes the node embedding to graph structures by applying the L2-norm between embeddings.
GraphMAE [ 15] applies the masking strategy and targets to reconstruct the node features of various
scales of graphs, where its GAE architecture also follows the classical GAE model. DiGAE [ 20] lies
in the structural reconstruction on directed graphs, and firstly proposes to use the cross-correlation
to express the node connection from two embedding spaces. Although these methods are effective
recover node connections on a single graph, and even some of them tried the reconstruction of whole
graph on graph tasks, there is no explicit evaluation to demonstrate how the global GAE model
perform when it generate graph structure once and on moderate to large graph tasks. Besides, previous
work follows the self-correlation strategy, which has been proven less effective than cross-correlation
on graph tasks, in our work.
B Dimension Requirement to Well Represent Graph Structure
As the node embedding dimension d‚Ä≤is underexplored before, it is mostly regarded as a hyperpa-
rameter to set up in advance. On the other hand, d‚Ä≤highly effects the encoder representing ability,
which is based on the graph scale. There is necessity to discuss the dimension requirement of node
embeddings for a certain graph scale.
Remark. We share a toy example to demonstrate how the d‚Ä≤design has an impact on the encoder
ability. Assume an extreme case d‚Ä≤= 1, each node is represented by a scalar. The node embeddings
(zi, zj, zk)‚ààR3can never represent a connection set (Ai,j, Ai,k, Aj,k) = (0 ,0,0). Because if
‚àÉ(zi, zj, zk)‚ààR3such that I(ÀúAi,j,ÀúAi,k) = (0 ,0), i.e., zizj<0andzizk<0, then we must have
sign(zjzk) =sign(zjz2
izk) =‚àí1. This will always yield to I(ÀúAj,k) = 0Ã∏=Aj,k. The similar result
can be directly observed when d‚Ä≤= 2and there are four nodes, then ‚àÑ(zi, zj, zk, zl)‚ààR4which can
represent connection set (Ai,j, Ai,k, Ai,l, Aj,k, Aj,l, Ak,l) = (0 ,0,0,0,0,0).
Lemma B.1. For self-correlation method in the decoder, to make the connection constraints always
solvable on the n-node scenario, i.e., requiring zT
izj>0orzT
izj<0for each node pair, the node
embedding dimension d‚Ä≤need to satisfy d‚Ä≤‚â•(n‚àí1)at least.
Proof. We first prove that for nnodes, there is always existing a connection case, such as no
connection on all node pairs, that ‚àÑ{z} ‚ààRn√ód‚Ä≤can represent when d‚Ä≤<(n‚àí1). We consider the
case that Ai,i= 1 andAi,j= 0 for all i, j‚àà[1, n], such as zT
izj<0. When d‚Ä≤<(n‚àí1), e.g.,
d‚Ä≤= (n‚àí2), there will be at most (n‚àí2)linearly independent node vectors. Assume the first
(n‚àí2)vectors z1tozn‚àí2are linearly independent, then we will always find a linear combination
14such thatPn‚àí1
i=1Œ±izi=0, where vector Œ±={Œ±1, ..., Œ± n‚àí1} Ã∏=0. Here, we let the elements of Œ±be
grouped as positives, negatives, and zeros, as Œ±+,Œ±‚àí,Œ±0. Thus, we have
n‚àí1X
i=1Œ±izi=0‚áíX
a‚ààŒ±+aizi=X
b‚ààŒ±‚àí‚àíbjzj=w
a)If both Œ±+,Œ±‚àíare not empty, we do inner product on these two terms:
0<wTw= X
a‚ààŒ±+aizi!T X
b‚ààŒ±‚àí‚àíbjzj!
=X
‚àíaibj¬∑zT
izj‚â§0
This causes conflict on the inequality. b)If one of Œ±+andŒ±‚àíis empty, e.g., Œ±‚àí=‚àÖ, we do inner
product between the positive part and zn:
0 =0Tzn=X
a‚ààŒ±+aizT
izn<0
which also cause inequality conflict. Thus, the assumption on d‚Ä≤<(n‚àí2)does not hold.
Then, we prove that when d‚Ä≤= (n‚àí1), there always exists {z} ‚ààRn√ód‚Ä≤to represent all the
connections through the decoder. We use inductive method to prove it. It is straightforward that
when n= 2,d‚Ä≤= 1can satisfy the representation on any graph A‚àà {0,1}2√ó2. Assuming d‚Ä≤=n‚àí1
is a feasible configuration for an arbitrary n-node graph, we need to prove that d‚Ä≤=nis also
sufficient for (n+ 1) -node graph: We denote a satifiable node embedding from the n-node graph
asZ={zi} ‚ààRn,n‚àí1. By extending it to d‚Ä≤=nfor one more node coming, we specify the node
embedding of the first nnodes as z‚Ä≤
i= [zi,0]fori= [1, n‚àí1]andz‚Ä≤
n= [zn,1]; this specification
can still satisfy arbitrary inequality constraints between node pairs in the first nnode. For the
(n+ 1) -th node, we need to find z‚Ä≤
n+1‚ààRnsuch that z‚Ä≤T
n+1z‚Ä≤
isatisfy arbitrary inequalities. Through
the inductive method, it is also straightforward to prove that there exists Zwith rank (n‚àí1), thus our
extension to one extra dimension will make rank ({z‚Ä≤
i}) =nfori= [1, n]. Therefore, we can always
find a non-zero vector Œ±such that z‚Ä≤
n+1=Pn
i=1Œ±iz‚Ä≤
i. For each constraint z‚Ä≤T
n+1z‚Ä≤
ibeing positive or
negative (in total nconstraints), we can reduce them to system of equations where the constants are
scalars satisfying the constraints:
Œ±1z‚Ä≤T
1z1+...+Œ±nz‚Ä≤T
nz1=c1
...
Œ±1z‚Ä≤T
1zn+...+Œ±nz‚Ä≤T
nzn=cnÔ£º
Ô£Ω
Ô£ænequations
denoted as MŒ±=C. Here, the vector Œ±is solvable as long as rank (M) =rank(M|C), which can
be tuned by specifying C.
Although this theoretical analysis indicates that the node embedding dimension should be large
enough to ensure the graph structure reconstruction, we observed in experiments that the embedding
dimension can usually be smaller (e.g., d‚Ä≤‚âàn/2) because the hard-to-solve structures are not
common in graph tasks. Nevertheless, it provides a preterior node embedding dimension suggestion,
and our evaluation widely adopts this lemma and takes d‚Ä≤‚âànin all experiments.
C Specific Graph Structure Representation
In Sec. 2.2 and 2.3, we explore the limitations of self-correlation and the effectiveness of cross-
correlation in expressing specific graph structures. Given that previous GAE research often evaluates
undirected asymmetric graph structures, the evaluation on special graph structures is usually over-
looked. Hereby we evaluate how our method GraphCroc and other GAE models perform on specific
graph structures as aforementioned.
Island (without self-loop) and symmetric graph structure. We generate 4 topologically symmetric
graphs devoid of self-loops. Thus, the task is to have the evaluated GAE learn to reconstruct these
graph structures and assess their performance. The visualization of their reconstruction is presented
15Ground Truth
 GraphCroc
 GraphCroc(SC)
 GAE
 VGAE
 EGNN
 DiGAE
Figure 7: The graph reconstruction visualization of different models on graphs with symmetric
structure and non-self-looped nodes.
in Fig. 7. The visualization clearly demonstrates that our GraphCroc model effectively reconstructs
these specialized graph structures. For DiGAE which is also based on cross-correlation, it can
also well reconstruct the special graph structures, further supporting our discussion in Sec. 2.3.
In contrast, other self-correlation-based models tend to incorrectly predict connections between
symmetric nodes and islands, and incorrectly introduce self-loops on nodes. Note that for EGNN, it
does not predict positive edges between nodes, which seems not to follow our analysis in Sec. 2.2
with Euclidean encoding sigmoid (C(1‚àí ‚à•zi‚àízj‚à•2)). This is because EGNN slightly improves
this encoding to sigmoid (w‚à•zi‚àízj‚à•2+b), where wandbare learnable. Since no-self-loop nodes
require sigmoid (w‚à•zi‚àízi‚à•2+b) =sigmoid (b)<0.5,bis forced to be negative, inducing negative
prediction on symmetric edges that have zi=zjunder symmetric structures. Therefore, EGNN still
cannot handle well the graph reconstruction on the special graph structures.
Directed graph structure. We conduct an evaluation using datasets of directed graphs. We compare
GraphCroc with DiGAE, as only cross-correlation-based methods are capable of expressing direc-
tional relationships between nodes. To construct the dataset, we sample subgraphs from the directed
Cora_ML [ 28] and CiteSeer [ 10] datasets. Specifically, we randomly select 1,000 subgraphs. Of
these, 800 subgraphs were used for training and 200 for testing. The results are detailed in Table 4,
where ¬ØNrepresents the average number of nodes per graph:
Table 4: The AUC score of reconstructing the adjacency matrix in directed graph tasks.
Cora_ML( ¬ØN= 41 ) Cora_ML( ¬ØN= 77 ) CiteSeer( ¬ØN= 16)
DiGAE 0.6879 0.8296 0.9083
GraphCroc 0.9946 0.9996 0.9999
Our GraphCroc model can well reconstruct the directed graph structure with almost perfect prediction,
which significantly outperforms the DiGAE model. This advantage comes from the expressive model
architecture of our proposed U-Net-like model.
D Node Embedding Divergence in GraphCroc Decoder
The difference between two latent embeddings (denoted as PandQ) is fundamental to cross-
correlation as opposed to self-correlation in which P=Q; therefore, it is necessary to make them
not converge to each other. One method of explicitly controlling this divergence is by incorporating
regularization terms into the loss function, such as cosine similarity (cos (P, Q)).
Our decoder architecture inherently encourages differentiation between PandQsince they are
derived from two separate branches of the decoder. This structure can allow PandQto di-
verge adaptively in response to the specific needs of the graph tasks. If a graph cannot be well
160.4
 0.2
 0.0 0.2 0.4 0.6 0.8
Cosine Similarity0510152025Hist. of Cosine SimilarityCollab
IMDB-B
PPI
PROTEINS
QM9Figure 8: The histogram of cosine
similarity between node embeddings P
andQunder cross-correlation, applying
GraphCroc on graph tasks.represented by self-correlation, our two-branch structure
will encourage sufficient divergence on PandQto suit
structural reconstruction. To evaluate the differentiation
between them, we compute their cosine similarity and
present a histogram of these values for each graph task
in Figure 8. Across all tasks, the cosine similarity be-
tween the node embeddings under cross-correlation is gen-
erally low, typically below 0.6. This shows that our two-
branch decoder effectively maintains the independence
of the node embeddings, which are adaptively optimized
for various graph tasks. Furthermore, this adaptive opti-
mization underscores the superiority of cross-correlation
in real-world applications, as evidenced by GraphCroc‚Äôs
superior performance in graph structural reconstruction
compared to other methods (Table 1).
E Loss Balancing Derivation
We adopt binary cross-entropy (BCE) loss to evaluate reconstruction between prediction ÀúA=
sigmoid (PQT)and ground truth A. Our loss balancing is based on an i.i.d. assumption between
L0andL1, where the loss definition follows L(i, j) =BCE (ÀúAi,j, Ai,j)on the node pair (i, j).
For a certain graph G, we assume there are c0zero elements and c1one elements in A, where
c0‚â´c1. To balance the loss between zeros and ones, we apply scaling factors on each element loss:
L(ÀúA, A) =Œ±0Pc0
i=1L0
i+Œ±1Pc1
j=1L1
j. The scaling has two targets: to keep the loss magnitude and
to balance the zero/one influence. Thus, we construct the following linear equations:

Œ±0c0L0+Œ±1c1L1=c0L0+c1L1
Œ±0c0L0=Œ±1c1L1 ‚áíŒ±0=c0+c1
2c0, Œ± 1=c0+c1
2c1
The scaling factors Œ±0andŒ±1are derived.
F Supplementary Experimental Setup
F.1 Dataset
We provide the graph detail of graph tasks selected in our evaluation, in Table 5. For IMDB-B and
COLLAB without node features, we take the one-hot encoding of degree as the node features.
Table 5: The dataset configuration of selected graph tasks.
# graphs # nodes (avg) # edges (avg) # features # classes/tasks
PROTEINS 1,113 39.1 145.6 3 2
IMDB-B 1,000 19.8 193.1 0 2
COLLAB 5,000 74.5 4914.4 0 3
PPI 22 2245.3 61,318.4 50 121
QM9 130,831 18 37.3 11 19
F.2 Other Experimental Setup Information
We provide a three-layer GraphCroc to demonstrate the detailed data flow and the model structure, in
Figure 9. Besides, we list the detailed configuration of GraphCroc model and corresponding training
setup for all graph tasks in Table 6. The reconstruction results in Table 1 are not provided in average
on multiple runs, because the reproduction on several experiments is too heavy loaded. For example,
due to the large graph size, the default setting (vector dimension of 128 and layer number of 4) in
EGNN when reproducing the PPI task will cause the out-of-memory issue on the 40GB A100 GPU.
While reducing the dimension to 16 and the layer number to 3 allows model and data to fit just into
GPU (38.40GB/40GB), this significantly simplified model fails to adequately learn the structure
of the PPI graphs and performs poorly compared to other GAE methods. In addition, given that
graph reconstruction must be conducted by graph and QM9 task has massive graphs, even one model
training on QM9 will take over 2 GPU days.
17(ùëã,ùê¥)
GCN(‚Ñé1,ùê¥) (‚Ñé2,ùê¥‚Ä≤2) (‚Ñé3,ùê¥‚Ä≤3)
(‚Ñé‚Ä≤3,ùê¥‚Ä≤3)Linear
LinearùëÉ
ùëÑsigmoid(ùëÉùëÑùëá)
GCN
Pooling
GCN
Pooling
GCNUnpooling
GCNUnpooling
GCNUnpoolingGCNUnpoolingGCN
(‚Ñé2‚Ä≤,ùê¥‚Ä≤2)
(‚Ñé2‚Ä≤‚Ä≤,ùê¥‚Ä≤2)(‚Ñé1‚Ä≤,ùê¥)
(‚Ñé1‚Ä≤‚Ä≤,ùê¥)Figure 9: The archtecture example of our GraphCroc model, with 3-layer encoder/decoder.
Table 6: The architecture and training configuration of GraphCroc on selected graph tasks.
input dim. embedding dim. # layers pooling ratetraining config.
(opt., lr, epochs)
PROTEINS 3 128 7 [-, 0.9, 0.8, 0.7, 0.6, 0.5, -] (AdamW, 1e-3, 200)
IMDB-B 135 128 7 [-, 0.9, 0.8, 0.7, 0.6, 0.5, -] (AdamW, 1e-3, 200)
COLLAB 400 128 8 [-, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, -] (AdamW, 1e-3, 200)
PPI 50 1024 11 [-, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, -] (AdamW, 1e-3, 200)
QM9 11 32 6 [-, 0.9, 0.8, 0.7, 0.6, -] (AdamW, 1e-3, 100)
F.3 Adversarial Attack on GraphCroc
In Section 4.5, we evaluate the GAE performance as an adversarial attack surface. Specifically,
given a pretrained encoder Œ¶(Z|G)which encodes the graph into a latent space and a downstream
classifier f(Z)for the graph classification task, we aim to generate a perturbed latent representation
Z‚Ä≤and leverage a reconstructor Œòto rebuild the graph structure G‚Ä≤= (X, A‚Ä≤). The goal of the
adversarial structure is to cause the encoder and downstream classifier to misclassify the graph,
i.e.,f(Œ¶(G‚Ä≤))Ã∏=y, where yis the original label, and the only difference between GandG‚Ä≤is the
adjacency matrix A. We assess two gradient-based adversarial attacks on latent space.
Projected Gradient Descent (PGD) [ 27]: PGD iteratively perturbs the input to maximize the
classification loss of f(Z):
Œ¥t+1=Proj||Œ¥||1‚â§œµ(Œ¥t+Œ±¬∑sign(‚àáŒ¥tL(f(Z), y)))
Carlini & Wagner (C&W) [ 4]: C&W finds adversarial latent vectors by solving an optimization
problem:
Œ¥‚àó= arg min
Œ¥||Œ¥||1+c¬∑(max f(Z)y‚àímax
iÃ∏=y(f(Z)i,‚àík))
Here, f(Z)ydenotes the logits output of the classifier for the true class y, and kis a confidence
parameter that controls the confidence of the misclassification. This optimization can be solved with
gradient descent. Hence, the final adversarial graph structure will be G‚Ä≤= (X,Œò(Z+Œ¥‚àó)). To
enhance the performance of adversarial perturbation, we fine-tune the reconstructor Œòduring the
adversarial attack. Specifically, to ensure the effectiveness of the reconstructed adversarial example,
we optimize Œòby minimizing the distance between the perturbed latent representation and the latent
space of the reconstructed graph structure:
Œò‚àó= arg min
Œò||Z+Œ¥‚àó‚àíŒ¶(X,Œò(Z+Œ¥‚àó)||
G Supplementary Experiment Results
G.1 Structural Reconstruction of Cross-Correlation on Other Architectures
In addition to the GCN kernel used in our GraphCroc model, we extend our analysis to include
other widely used graph architectures such as GraphSAGE [ 13], GAT [ 36], and GIN [ 45]. To
incorporate these architectures into the cross-correlation framework, we replace the GCN module
with corresponding operations while preserving the overarching structure, which includes the encoder,
18the dual-branch decoder, and the skip connections between the encoder and decoder. Furthermore,
we explore how GraphCroc performs without skipping connections. The overall architecture and
training configurations remain consistent with those outlined in Table 6 of our paper. The results,
presented in Table 7, follow the format of Table 1 in our paper, providing a clear comparison across
different architectures.
Table 7: The AUC score of reconstructing the adjacency
matrix in graph tasks, under different architectures in
the cross-correlation framework.
Dataset GraphSAGE GAT GIN GraphCroc‚àóGraphCroc
PROTEINS 0.9898 0.9629 0.9927 0.9934 0.9958
IMDB-B 0.9984 0.9687 0.9980 0.9975 0.9992
Collab 0.9985 0.9627 0.9954 0.9976 0.9989
PPI 0.9774 0.9236 0.9467 0.9447 0.9831
QM9 0.9972 0.9978 0.9974 0.9966 0.9987
‚àówithout skip connectionOverall, all architectures employing cross-
correlation effectively reconstruct graph
structures, underscoring the significance of
cross-correlation as a core contribution of
our work. Given that training each model
requires several hours, particularly for large
datasets such as PPI and QM9, we do not
fine-tune the hyperparameters much during
model training. The results presented here
may represent a lower bound of these archi-
tectures‚Äô potential performance. Therefore, we refrain from ranking these cross-correlation-based
architectures due to their closely matched performance, and we adopt a conservative stance in our
comparisons. Nevertheless, it is evident that most of these architectures (except GAT) generally
surpass the performance of self-correlation models shown in Table 1 of our paper, highlighting the
efficacy of cross-correlation in graph structural reconstruction.
G.2 Node Trajectory during Training
In Figure 10, we demonstrate the converging trajectory of first tow nodes of eight other graphs during
training, as an extension of Figure 2. It aligns with the analysis in main paper that cross-correlation
can provide a much smoother than self-correlation during the GAE training.
0 1 2 3
Dim. 1
 0.5
0.00.51.01.52.0Dim. 2
Graph #1: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
1
 0 1 2 3
Dim. 1
 2
1
012Dim. 2
Graph #1: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
0 1 2 3
Dim. 1
 1.0
0.5
0.00.51.01.5Dim. 2
Graph #2: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
2
 1
 0 1 2 3
Dim. 1
 2
1
012Dim. 2
Graph #2: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
1
 0 1 2
Dim. 1
 0.5
0.00.51.01.52.02.53.0Dim. 2
Graph #3: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
4
 2
 0 2
Dim. 1
 1
0123Dim. 2
Graph #3: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
1
 0 1 2
Dim. 1
 1
0123Dim. 2
Graph #4: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
6
 4
 2
 0 2
Dim. 1
 2
0246Dim. 2
Graph #4: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
1
 0 1 2 3
Dim. 1
 0123Dim. 2
Graph #5: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
4
 2
 0 2
Dim. 1
 2
1
0123Dim. 2
Graph #5: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
1
 0 1 2
Dim. 1
 1
012Dim. 2
Graph #6: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
3
 2
 1
 0 1
Dim. 1
 0.5
0.00.51.01.52.0Dim. 2
Graph #6: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
1
 0 1 2
Dim. 1
 1.0
0.5
0.00.51.01.5Dim. 2
Graph #7: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
3
 2
 1
 0 1
Dim. 1
 2.5
2.0
1.5
1.0
0.5
0.00.51.0Dim. 2
Graph #7: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
1
 0 1 2
Dim. 1
 01234Dim. 2
Graph #8: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (z1)
Node 2 (z2)
6
 4
 2
 0
Dim. 1
 0.51.01.52.02.53.03.54.0Dim. 2
Graph #8: Trajectory under self-cor. (left) and cross-cor. (right)Node 1 (p1)
Node 2 (p2)
Figure 10: The trajectory of the first two nodes of eight graph samples during training.
19G.3 WL-Test Results
In Figure 11, we provide the WL-test results on graph tasks, PROTEINS, COLLAB, PPI, and QM9.
Our GraphCroc can still outperform other GAE models in completely reconstructing the graphs in
most tasks. Notably, all the methods cannot achieve the isomorphism on PPI reconstruction. This is
because PPI only has two test graphs, while they have number of nodes 3324 and 2300, which are
too large to be completely reconstructed.
GAEVGAE EGNN
ours(SC)DiGAEours0.000.050.100.150.20WL-testPROTEINS
(a)
GAEVGAE EGNN
ours(SC)DiGAEours0.00.10.20.3WL-testCOLLAB (b)
GAEVGAE EGNN
ours(SC)DiGAEours0.04
0.02
0.000.020.04WL-testPPI
(c)
GAEVGAE EGNN
ours(SC)DiGAEours0.000.020.040.060.080.10WL-testQM9 (d)
Figure 11: The WL-test results on PROTEINS, COLLAB, PPI, and QM9. ‚Äúours‚Äù refers to GraphCroc.
G.4 GraphCroc on Other GAE Strategies
In Figure 12, we demonstrate more evaluations for GraphCroc on GAE strategies. The AUC score is
further tested on IMDB-B, COLLAB, and PPI tasks. We find that the GAE training integrated with
other enhancements, i.e., variational, edge masking, and L2-norm, performs distinctively on different
graph tasks. They could underperform our baseline training strategy on certain tasks.
0 50 100 150 200
Epoch0.800.850.900.951.00ROC-AUCAUC on IMDB-B
baseline
variational
masking
L2-norm
(a)
0 50 100 150 200
Epoch0.800.850.900.951.00ROC-AUCAUC on COLLAB
baseline
variational
masking
L2-norm (b)
0 50 100 150 200
Epoch0.40.50.60.70.80.91.0ROC-AUCAUC on PPI
baseline
variational
masking
L2-norm (c)
Figure 12: The supplementary AUC scores on other graph tasks, with different decoding methods.
G.5 Reconstruction Visualization
We visualize more graph structure reconstruction results on graph tasks, in Figure 13, 14, 15, and16.
20Ground Truth
 GraphCroc
 GAE
 VGAE
 EGNN
 DIGAE
Figure 13: The structure reconstruction visualization on PROTEINS task.
Ground Truth
 GraphCroc
 GAE
 VGAE
 EGNN
 DIGAE
Figure 14: The structure reconstruction visualization on IMDB-B task.
21Ground Truth
 GraphCroc
 GAE
 VGAE
 EGNN
 DIGAE
Figure 15: The structure reconstruction visualization on COLLAB task.
Ground Truth
 GraphCroc
 GAE
 VGAE
 EGNN
 DIGAE
Figure 16: The structure reconstruction visualization on QM9 task.
22NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: We clearly indicate our contribution on cross-correlation decoding and a novel
GAE model, in [Abstract] and [Introduction].
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: Our approach still requires GAE training by graph, thus time-consuming on
large graph number tasks, such as QM9 [in Appendix F.2]. Further, we observe adversarial
attack based on GAE model could require noticible edge distortion in some cases [in
Evaluation 4.5].
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
23Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We propose Lemma 2.2 and Lemma B.1, which are followed by proper proof.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide training configuration and model architecture [in Secion 4.1 and
Appendix F.2].
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
245.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide sample code along with submission, and will open source full
codes after acceptance.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See answer to Q4.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We provide error bars when possible [in Table 2], and explain when impossible
[in Appendix F.2].
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
25‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We run our model training on an A100 GPU with 40GB [in Appendix F.2],
and the time of model training is also mentioned.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification:
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: Our work does not have ethical or societal concern.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
26‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
27‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification:
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification:
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification:
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
28‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
29