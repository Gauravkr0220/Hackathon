Published in Transactions on Machine Learning Research (06/2023)
POMRL: No-Regret Learning-to-Plan with Increasing
Horizons
Khimya Khetarpalâˆ—â€ khimya.khetarpal@mail.mcgill.ca
Department of Computer Science
McGill University
Claire Vernadeâˆ—â€¡claire.vernade@gmail.com
University of Tuebingen
Brendan Oâ€™Donoghue bodonoghue@google.com
Google Deepmind
Satinder Singh Baveja baveja@google.com
Google Deepmind
Tom Zahavy tomzahavy@google.com
Google Deepmind
Reviewed on OpenReview: https: // openreview. net/ forum? id= brGgOAXYtr
Abstract
We study the problem of planning under model uncertainty in an online meta-reinforcement
learning (RL) setting where an agent is presented with a sequence of related tasks with
limited interactions per task. The agent can use its experience in each task andacross
tasks to estimate both the transition model and the distribution over tasks. We propose an
algorithm to meta-learn the underlying relatedness across tasks, utilize it to plan in each
task, and upper-bound the regret of the planning loss. Our bound suggests that the average
regret over tasks decreases as the number of tasks increases and as the tasks are more similar.
In the classical single-task setting, it is known that the planning horizon should depend on
the estimated modelâ€™s accuracy, that is, on the number of samples within task. We generalize
this ï¬nding to meta-RL and study this dependence of planning horizons on the number of
tasks. Based on our theoretical ï¬ndings, we derive heuristics for selecting slowly increasing
discount factors, and we validate its signiï¬cance empirically.
1 Introduction
Meta-learning (Caruana, 1997; Baxter, 2000; Thrun & Pratt, 1998; Finn et al., 2017; Denevi et al., 2018)
oï¬€ers a powerful paradigm to leverage past experience to reduce the sample complexity of learning future
related tasks. Online meta-learning considers a sequential setting, where the agent progressively accumulates
knowledge and uses past experience to learn good priors and to quickly adapt within each task Finn et al.
(2019); Denevi et al. (2019). Robots acting in real world for instance need to be responsive to and robust
against perturbation inherent in the environment dynamics and their decision making. When the tasks share
a structure i.e. have similar transition dynamics and are related, such approaches enable progressively faster
convergence, or equivalently better model accuracy with better sample complexity (Schmidhuber & Huber,
1991; Thrun & Pratt, 1998; Baxter, 2000; Finn et al., 2017; Balcan et al., 2019).
âˆ—Equal Contribution
â€ Work partially done during an internship at DeepMind. Now at Deepmind.
â€¡Work partially done at DeepMind.
1Published in Transactions on Machine Learning Research (06/2023)
In model-based reinforcement learning (RL), the agent uses an estimated model of the environment to plan
actions ahead towards the goal of maximizing rewards. A key component in the agentâ€™s decision making is
the horizon used during planning. In general, an evaluation horizon is imposed by the task itself, but the
learner may want to use a diï¬€erent and potentially shorter guidance horizon . In the discounted setting, the
size of the evaluation horizon is of order (1âˆ’Î³eval)âˆ’1, for some discount factor Î³evalâˆˆ(0,1), and the agent
may useÎ³/negationslash=Î³evalfor planning. For instance, a classic result known as Blackwell Optimality (Blackwell,
1962) states there exists a discount factor Î³â‹†and a corresponding optimal policy such that the policy is also
optimal for any greater discount factor Î³â‰¥Î³â‹†.Thus, an agent that plans with Î³=Î³â‹†will be optimal for
anyÎ³eval>Î³â‹†.In the Arcade Learning Environment (Bellemare et al., 2013) a discount factor of Î³eval= 1is
used for evaluation, but typically a smaller Î³is used for training (Mnih et al., 2015). Using a smaller discount
factor acts as a regularizer (Amit et al., 2020; Petrik & Scherrer, 2008; Van Seijen et al., 2009; FranÃ§ois-Lavet
et al., 2019; Arumugam et al., 2018) and reduces planner over-ï¬tting in random MDPs (Arumugam et al.,
2018). Indeed, the choice of planning horizon plays a signiï¬cant role in computation (Kearns et al., 2002),
optimality (Kocsis & SzepesvÃ¡ri, 2006), and on the complexity of the policy class (Jiang et al., 2015). In
addition, meta-learning discount factors has led to signiï¬cant improvements in performance (Xu et al., 2018;
Zahavy et al., 2020; Flennerhag et al., 2021; 2022; Luketina et al., 2022).
When doing model-based RL with a learned model, the optimal guidance planning horizon, called eï¬€ective
horizon by Jiang et al. (2015), depends on the accuracy of the model, and so on the amount of data used to
estimate it. Jiang et al. (2015) show that when data is scarce, a guidance discount factor Î³ <Î³ evalshould be
preferred for planning. The reason for this is straightforward; if the model used for planning is inaccurate,
then errors will tend to accumulate along the planned trajectory. A shorter eï¬€ective planning horizon will
accumulate less error and may lead to better performance, even when judged using the true Î³eval. While
that work treated only the batch, single-task setting, the question of eï¬€ective planning horizon remains open
in the online meta-learning setting where the agent accumulates knowledge from many tasks, with limited
interactions within each task.
In this work, we consider a meta-reinforcement-learning problem made of a sequence of related tasks . We
leverage this structural task similarity to obtain model estimators with faster convergence as more tasks are
seen. The central question of our work is:
Can we meta-learn the model across tasks and adapt the eï¬€ective planning horizon accordingly?
We take inspiration from the Average Regret-Upper-Bound Analysis [ARUBA] framework (Khodak et al., 2019)
to generalize planning loss bounds to the meta-RL setting. A high-level, intuitive outline of our approach is
presented in Fig. 1. Our main contributions are as follows:
â€¢We formalize planning in a model-based meta-RL setting as an average planning loss minimization
problem, and we propose an algorithm to solve it.
â€¢Under a structural task-similarity assumption, we prove a novel high-probability task-averaged regret
upper-bound on the planning loss of our algorithm, inspired by ARUBA. We also demonstrate a
way to learn the task-similarity parameter Ïƒon-the-ï¬‚y. To the best of our knowledge, this is a ï¬rst
formal (ARUBA-style) analysis to show that meta-RL can be more eï¬ƒcient than RL.
â€¢Our theoretical result highlights a new dependence of the planning horizon on the size of the within-
task datamandon the number of tasks T. This observation allows us to propose two heuristics to
adapt the planning horizon given the overall sample-size.
2 Preliminaries
ReinforcementLearning. We consider tabular Markov Decision Processes (MDPs) M=/angbracketleftS,A,R,P,Î³ eval/angbracketright,
whereSis a ï¬nite set of states, Ais a ï¬nite set of actions and we denote the set cardinalities as S=|S|and
A=|A|. For each state sâˆˆS, and for each available action aâˆˆA, the probability vector P(Â·|s,a)deï¬nes a
transition model over the state space and is a probability distribution in a set of feasible models DPâŠ‚âˆ†S,
where âˆ†Sthe probability simplex of dimension Sâˆ’1. We denote Î£â‰¤1the diameter ofDP. A policy is a
functionÏ€:Sâ†’Aand it characterizes the agentâ€™s behavior.
2Published in Transactions on Machine Learning Research (06/2023)
Gr o wing Planning Horizon Online Me ta-L earning K e y- T ak ea w a y s 
T ask-Similarity 
m  s amples  
per task 
T T ask s 
Figure 1:Eï¬€ective Planning Horizons in Meta-Reinforcement Learning. The agent faces a sequence
of tasks with transition vector (Pt)tâˆˆ[T](probability vectors represented by blue dots) all close to each other
(Ïƒ<Î£ = 1). The agent builds a transition model for each task and plans with these inaccurate models. By
using data from previous tasks, the agent meta-learns an initialization of the model ( Ë†Po,t), which leads to
better planning in new related but unseen tasks. We show an improved average regret upper bound that
scales with task-similarity parameter Ïƒand inversely with the number of tasks T: as knowledge accumulates,
uncertainty diminishes, and the agent can plan with longer horizons. All tasks Ptâˆ¼Pare centered at some
ï¬xed but unknown Po, depicted here by the shaded red dot and pointed by the arrow.
We consider the bounded reward setting, i.e.,Râˆˆ[0,Rmax]and without loss of generality we set Rmax= 1
(unless stated otherwise). Given an MDP, or task, M, for any policy Ï€, letVÏ€
M,Î³âˆˆRSbe the value
function when evaluated in MDP Mwith discount factor Î³âˆˆ(0,1)(potentially diï¬€erent from Î³eval);
deï¬ned asVÏ€
M,Î³(s) =E/summationtextâˆ
t=0(Î³tRst|s0=s). The goal of the agent is to ï¬nd an optimal policy, Ï€â‹†
M,Î³=
arg maxÏ€Esâˆ¼ÏVÏ€
M,Î³(s)whereÏ>0is any positive measure, denoted Ï€â‹†when there is no ambiguity. For given
state and action spaces and reward function (S,A,R), we denote Î Î³the set of potentially optimal policies for
discount factor Î³:Î Î³={Ï€|âˆƒPs.t.Ï€=Ï€â‹†
M,Î³whereM=/angbracketleftS,A,R,P,Î³/angbracketright}. We use Big-O notation, O(Â·)and
ËœO(Â·), to hide respectively universal constants and poly-logarithmic terms in T,S,AandÎ´>0(the conï¬dence
level).
Model-based Reinforcement Learning. In practice, the true model of the world is unknown and must
be estimated from data. One approach to approximately solve the optimization problem above is to construct
a model,/angbracketleftË†R,Ë†P/angbracketrightfrom data, then ï¬nd Ï€â‹†
Ë†M,Î³for the corresponding MDP Ë†M=/angbracketleftS,A,Ë†R,Ë†P,Î³/angbracketright. This approach is
calledmodel-based RL orcertainty-equivalence (CE) control .
Planning with inaccurate models. In this setting, Jiang et al. (2015) deï¬ne the planning loss as the gap
in expected return in MDP Mwhen using Î³â‰¤Î³evaland the optimal policy for an approximate model Ë†M:
L(Ë†M,Î³|M,Î³ eval) =/bardblVÏ€â‹†
M,Î³ eval
M,Î³ evalâˆ’VÏ€â‹†
Ë†M,Î³
M,Î³ eval/bardblâˆ.
Thus, theoptimaleï¬€ectiveplanninghorizon (1âˆ’Î³â‹†)âˆ’1is deï¬ned using the discount factor that minimizes
the planning loss, i.e.,Î³â‹†:= min 0â‰¤Î³â‰¤Î³evalL(Ë†M,Î³|M,Î³ eval).
Theorem 1. (Jiang et al. (2015)) Let Mbe an MDP with non-negative bounded rewards and evaluation
discount factor Î³eval. Let Ë†Mbe the approximate MDP comprising the true reward function of M and the
approximate transition model Ë†P, estimated from m > 0samples for each state-action pair. Then, with
probability at least 1âˆ’Î´,
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleVÏ€â‹†
M,Î³ eval
M,Î³ evalâˆ’VÏ€â‹†
Ë†M,Î³
M,Î³ eval/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
âˆâ‰¤Î³evalâˆ’Î³
(1âˆ’Î³eval)(1âˆ’Î³)+2Î³Rmax
(1âˆ’Î³)2/parenleftBigg/radicalbigg
Î£
2mlog2SA|Î Î³|
Î´/parenrightBigg
(1)
where Î£is upper-bounded by 1 as P,Ë†Pâˆˆâˆ†S.
This result holds for a count-based model estimator (i.e, empirical average of observed transitions) given
by a generator model for each pair (s,a). It gives an upper-bound on the planning loss as a function of the
3Published in Transactions on Machine Learning Research (06/2023)
 m = 5
 m = 10 m = 20 m = 50
Figure 2: On the role of incorporating a ground truth prior of transition model on planning
horizon. The planning loss is a function of the discount factor Î³and is impacted by incorporating prior
knowledge. The learner has m= 5,10,20,50samples per task to estimate the model, corresponding to the
curves in each sub ï¬gure. Inspecting any of the sub ï¬gures, we observe that larger values of mlead to lower
planning loss and a larger eï¬€ective discount factor. Besides, inspecting one value of macross tasks (e.g.,
m= 5), we see that the same eï¬€ect (lower planning loss and larger eï¬€ective discount) occurs when the
learner puts more weight on the ground truth prior through Î±.
guidance discount factor Î³ <1. The result decomposes the loss into two terms: the constant bias which
decreases as Î³tends toÎ³eval, and the variance (or uncertainty) term which increases with Î³but decreases as
1/âˆšm. Asmâ†’âˆthat second factor vanishes, but in the low-sample regime the optimal eï¬€ective planning
horizon should trade-oï¬€ both terms.
Illustration. These eï¬€ects are illustrated in Fig. 2 on a simple 10-state, 2-action random MDP. The leftmost
plot uses the simple count-based model estimator and reproduces the results from Jiang et al. (2015). We
then incorporate the true prior (mean model Poas in Fig 1 and deï¬ned above Eq. 3 in Assumption 1) in the
estimator with a growing mixing factor Î±âˆˆ(0,1):Ë†P(m) =Î±Po+ (1âˆ’Î±)/summationtextiXi
m. We observe that increasing
the weight Î±âˆˆ(0,1)on good prior knowledge enables longer planning horizons and lower planning loss.
Online Meta-Learning and Regret. We consider an online meta-RL problem where an agent is presented
with a sequence of tasks M1,M2,...,MT, where for each tâˆˆ[T],Mt=/angbracketleftS,A,Pt,R,Î³ eval/angbracketright, that is, the MDPs
only diï¬€er from each other by the transition matrix (dynamics model) Pt. The learner must sequentially
estimate the model Ë†Ptfor each task tfrom a batch of mtransitions simulated for each state-action pair1.
Its goal is to minimize the average planning loss also expressed in the form of task averaged regret suï¬€ered in
planning and deï¬ned as
Â¯L(Ë†M1:T,Î³|M1:T,Î³eval) =1
TT/summationdisplay
t=1L(Ë†Mt,Î³|Mt,Î³eval) =1
TT/summationdisplay
t=1/bardblVÏ€â‹†
Mt,Î³eval
Mt,Î³evalâˆ’VÏ€â‹†
Ë†Mt,Î³
Mt,Î³eval/bardblâˆ (2)
Note that the reference MDP for each term is the true Mt, and the discount factor Î³is the same in all tasks.
One can see this objective as a stochastic dynamic regret: at each task tâˆˆ[T], the learner competes against
the optimal policy for the currenttrue model, as opposed to competing against the best ï¬xed policy in
hindsight used in classical deï¬nitions of regret.
Note that our dynamic regret is diï¬€erent from the one considered in ARUBA (Khodak et al.,
2019). They consider the fully online setting where the data is observed as an arbitrary stream within each
task, and each comparator is simply the minimum of the within-task loss in hindsight. In our model, however,
given access to a simulator (See Sec. 2) allows us to get i.i.d transition samples as a batch at the beginning of
each task, and consequently we deï¬ne our regret with respect to the true generating parameter. One key
consequence of this diï¬€erence is that their regret bounds cannot be directly applied to our setting, and we
prove new results further below.
1So a total of mSAsamples.
4Published in Transactions on Machine Learning Research (06/2023)
3 Planning with Online Meta-Reinforcement Learning
We here formalize planning in a model-based meta-RL setting. We start by specifying all our assumptions in
Sec 3.1 including our main assumption about task relatedness in Sec. 1, present our approach and explain the
proposed algorithms POMRLandada-POMRL in Sec. 3.2. Our main result is a high-probability upper bound on
the average planning loss under the assumed task relatedness, presented as Theorem 2.
3.1 Assumptions
In many real world scenarios such as robotics, it is required to be responsive to changes in the environment
and, at the same time, to be robust against perturbation inherent in the environment and their decision
making. In such practical scenarios, the key reason to employ meta-learning is for the learner to leverage
task-similarity (or task variance) across tasks. Bounded task similarity is becoming a core assumption in
the analysis of recent meta learning (Khodak et al., 2019) and multi-task (Cesa-Bianchi et al., 2021) online
learning algorithms.
Assumption 1 (Structural Assumption Across Tasks: Task Relatedness) .In this work, we exploit the
structural assumption that for all tâˆˆ[T],Ptâˆ¼Pcentered at some ï¬xed but unknown Poâˆˆâˆ†SÃ—A
Sand such
that for any (s,a),
/bardblPt
s,aâˆ’Po
s,a/bardblâˆâ‰¤Ïƒ= max
(s,a)Ïƒ(s,a)a.s. (3)
This also implies that maxt,t/prime/bardblPt
s,aâˆ’Pt/prime
s,a/bardblâˆâ‰¤2Ïƒ, and that the meta-distribution Pis bounded within a
small subset of the simplex. It is immediate to extend our results under a high-probability assumption instead
of the almost sure statement above. In our experiments, we will use Gaussian or Dirichlet priors over the
simplex, whose moments are bounded with high-probability, not almost surely. Importantly, we will say that
a multi-task environment is strongly structured whenÏƒ<Î£,i.e.when the eï¬€ective diameter of the models is
smaller than that of the entire feasible space.
Assumption 2 (Access to a Simulator) .We assume that for each task tâˆˆ[T]we have access to a simulator
of transitions (Kearns et al., 2002) providing mi.i.d. samples (Xt,i
s,a)i=1..mâˆˆSmâˆ¼Pt(Â·|s,a)(categorical
distribution).
Next, for simplicity we assume throughout that the rewards are known and focus on learning and planning
with an approximate dynamics model. Additionally estimating the reward is a straightforward extension of
our analysis and would not change the implications of our main result.
Assumption 3 (Known Rewards) .Given a distribution of tasks, we assume that the rewards are known.
3.2 Our Approach
With access to a simulator (Assumption 2); for each (s,a), we can compute an empirical estimator for
eachs/primeâˆˆ[S]:Â¯Pt
s,a(s/prime) =/summationtextm
i=11{Xt,i
s,a=s/prime}/m, with naturally/summationtext
s/primeÂ¯Pt
s,a(s/prime) = 1. We perform meta-RL via
alternating minimizing a batch within-task regularized least-squares loss, and an outer-loop step where we
optimize the regularization to optimally balance bias and variance of the next estimator.
Estimating dynamics model via regularized least squares. We adapt the standard technique of
meta-learned regularizer (see e.g. Baxter (2000); Cella et al. (2020) for supervised learning and bandit
respectively) to this model estimation problem. At each round t, thecurrent model Ë†Pt
(s,a)is estimated
by minimizing a regularized least square loss : for a given regularizer ht(to be speciï¬ed below)2and
parameterÎ»t>0for each (s,a)âˆˆSÃ—A we solve
Ë†Pt
(s,a)= arg min
P(s,a)âˆˆâˆ†S/vextenddouble/vextenddouble/vextenddouble1
mm/summationdisplay
i=11{Xt,i
s,a}
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
empirical transition prob.âˆ’P(s,a)/vextenddouble/vextenddouble/vextenddouble2
2+Î»t/bardblP(s,a)âˆ’ht/bardbl2
2, (4)
2In principle, this loss is well deï¬ned for any regularizer htbut we specify a meta-learned one and prove that it induces good
performance.
5Published in Transactions on Machine Learning Research (06/2023)
where we use 1{Xt,i
s,a}to denote the one-hot encoding of the state into a vector in RS. Importantly, htand
Î»tare meta-learned in the outer-loop (see below) and aï¬€ect the bias and variance of the resulting estimator.
The solution of equation 4 can be computed in closed form as a convex combination of the empirical average
(count-based) and the prior: Ë†Pt=Î±tht+ (1âˆ’Î±t)Â¯PtwhereÎ±t=Î»t
1+Î»tis the current mixing parameter.
Outer-loop: Meta-learning the regularization. At the beginning of task 1<tâ‰¤T, the learner has
already observed tâˆ’1related but diï¬€erent tasks. We deï¬ne htas anaverage of Means (AoM) :
ht
(s,a)â†Ë†Po,t
(s,a)=1
tâˆ’1tâˆ’1/summationdisplay
j=1/summationtextm
i=11{Xj,i
(s,a)}
m:=1
tâˆ’1tâˆ’1/summationdisplay
j=1Â¯Pj
(s,a). (5)
Deriving the mixing rate. To setÎ±t, we compute the Mean Squared Error (MSE) of Ë†Pt
(s,a), and minimize
an upper bound (see details in Appendix B): MSE (Ë†Pt
(s,a))â‰¤Î±2
tÏƒ2(1 +1
t) + (1âˆ’Î±t)21
m, which leads to
Î±t=1
Ïƒ2(1+1/t)m+1.
Algorithm 1 depicts the complete pseudo code. We note here that POMRL(Ïƒ) assumes, for now, that the
underlying task-similarity parameter Ïƒis known, and we discuss a fully empirical extension further below
(See Sec. 4). The learner does not know the number of tasks a priori and tasks are faced sequentially online.
The learner performs meta-RL alternating between within-task estimation of the dynamics model Ë†Ptvia a
batch ofmsamples for that task, and an outer loop step to meta-update the regularizer Ë†Po,t+1alongside the
mixing rate Î±t+1. For each task, we use a Î³-Selection-Procedure to choose planning horizon Î³âˆ—â‰¤Î³eval.
We defer the details of this step to Sec. 6 as it is non-trivial and only a partial consequence of our theoretical
analysis. Next, the learner performs planning with an imperfect model Ë†Pt. For planning, we use dynamic
programming, in particular policy iteration (a combination of policy evaluation, and improvement), and value
iteration to obtain the optimal policy Ï€â‹†
Ë†Pt,Î³âˆ—for the corresponding MDP Ë†Mt.
Algorithm 1: POMRL(Ïƒ) â€“ Planning with Online Meta-Reinforcement Learning
Input:Given task-similarity (Ïƒ(s,a))a matrix of size SÃ—A. Initialize Ë†Po,1to uniform, Î±1= 0.
fortasktâˆˆ[T]do
fortthbatch ofmsamplesdo
Ë†Pt(m) = (1âˆ’Î±t)1
m/summationtextm
i=1Xi+Î±tË†Po,t//regularized least squares minimizer.
Î³â‹†â† âˆ’Î³-Selection-Procedure (m,Î±t,Ïƒ,T,S,A )
Ï€â‹†
Ë†Pt,Î³âˆ—â†Planning (Ë†Pt(m))//
Output:Ï€â‹†
Ë†Pt,Î³âˆ—
Update Ë†Po,t+1,Î±t+1=1
Ïƒ2(1+1/t)m+1//meta-update AoM (Eq. 5) and mixing rate
3.3 Average Regret Bound for Planning with Online-meta-learning
Our main theoretical result below controls the average regret of POMRL (Ïƒ), a version of Alg. 1 with additional
knowledge of the underlying task relatedness, i.e., the trueÏƒ>0.
Theorem 2. Using the notation of Theorem 1, we bound the average planning loss equation 2 for POMRL (Ïƒ):
Â¯Lâ‰¤Î³evalâˆ’Î³
(1âˆ’Î³eval)(1âˆ’Î³)+2Î³S
(1âˆ’Î³)2ËœOï£«
ï£¬ï£¬ï£­Ïƒ+/radicalBig
1
T/parenleftbigg
Ïƒ+/radicalBig
Ïƒ2+Î£
m/parenrightbigg
Ïƒ2m+ 1+Ïƒ2m/radicalBig
Î£
m
Ïƒ2m+ 1ï£¶
ï£·ï£·ï£¸(6)
with probability at least 1âˆ’Î´, whereÏƒ2<1is the measure of the task-similarity and Ïƒ= max (s,a)Ïƒ(s,a).
The proof of this result is provided in Appendix D and relies on a new concentration bound for the meta-
learned model estimator. The last term on the r.h.s. corresponds to the uncertainty on the dynamics.
6Published in Transactions on Machine Learning Research (06/2023)
First we verify that if T= 1andmgrows large, the second term dominates and is equivalent to ËœO(/radicalBig
Î£
m)
(asÏƒ2/(Ïƒ2m+ 1)â†’0), which is similar to that of Jiang et al. (2015) as there is no meta-learning, with
an additional O(1
m)but second order term due to the introduced bias. Then, if mis ï¬xed and small, for
small enough values of Ïƒ2(typicallyÏƒ < 1/âˆšm), the ï¬rst term dominates and the r.h.s. boils down to
ËœO/parenleftBig
(Ïƒ+1âˆšm)/âˆš
T/parenrightBig
. This highlights the interplay of our structural assumption parameter Ïƒand the amount
of datamavailable at each round. The regimes of the bound for various similarity levels are explored
empirically in Sec. 5 (Q3). We also show the dependence of the regret upper bound on mandTfor a ï¬xedÏƒ,
in Appendix Fig. F3.
Implications for degree of task-similarity i.e.,Ïƒvalues. Our bound suggests that the degree of
improvement you can get from meta learning scales with the task similarity Ïƒinstead of the set size Î£. Thus,
forÏƒâ‰¤Î£, performing meta learning with Algorithm 1 guarantees better learning measured via our improved
regret bound when there is underlying structure in the problem space which we formalize through Eq. 3.
ShouldÏƒbe large, the techniques will still hold and our bounds will simply scale accordingly.
WhenÏƒ= 0, all tasks are exactly the same. Indeed, the mixing rate Î±tâ‰ˆ1for allt, so our algorithm
boils down to returning the average of means Ë†Po,tfor each task, which simply corresponds to solving the
tasks as a continuous, uninterrupted stream of batches from the nearly same model that Ë†Po,taggregates.
Unsurprisingly, our bound recovers that of (Jiang et al., 2015, Theorem 1): the bound below reï¬‚ects that we
have to estimate only one model in a space of â€œsizeâ€ Î£withmTsamples.
Â¯Lâ‰¤Î³evalâˆ’Î³
(1âˆ’Î³eval)(1âˆ’Î³)+2Î³S
(1âˆ’Î³)2ËœO/parenleftBigg/radicalbigg
Î£
mT/parenrightBigg
(7)
WhenÏƒ= 1, thenÏƒ= Î£ = 1, then the meta-learning assumption is not relevant but our bound
remains valid and gracefully degrades to reï¬‚ect it. We need to estimate Tmodels each with m
samples. Then the second term1âˆšmreï¬‚ects the usual estimation error for each task while the ï¬rst term is an
added bias (second order in1
m) due to our regularization to our mean prior Pothat is not relevant here.
Â¯Lâ‰¤Î³evalâˆ’Î³
(1âˆ’Î³eval)(1âˆ’Î³)+2Î³S
(1âˆ’Î³)2ËœO/parenleftBig1
m/parenleftBig
1 +1âˆš
T(1 +/radicalbigg
1 +1
m)/parenrightBig
+1âˆšm/parenrightBig
(8)
Connections to ARUBA. As explained earlier, our metric is not directly comparable to that of
ARUBA (Khodak et al., 2019) but it is interesting to make a parallel with the high-probability aver-
age regret bounds proved in their Theorem 5.1. They also obtain an upper bound in ËœO(1/âˆšm+ 1/âˆš
mT)if
one upper bounds their average within-task regret Â¯Uâ‰¤Bâˆšm.
Remark 1 (Role of the task similarity Ïƒin Eq. 2).WhenÏƒ>0,POMRLnaturally integrates each new data
batch into the model estimation. The knowledge of Ïƒis necessary to obtain this exact and intuitive update
rule, and our theory only covers POMRLequipped with this prior knowledge, but we discuss how to learn and
plug-in Ë†Ïƒtin practice. Note that it would be possible to extend our result to allow for using the empirical
variance estimator with tools like the Bernstein inequality, but we believe this it out of the scope of this work
as it would essentially give a similar bound as obtained in Theorem 2 with an additional lower order term in
O(1/T), and it would not provide much further intuition on the meta-planning problem we study.
4 Practical Considerations: Adaption On-The-Fly
In this section we propose a variant of POMRLthat meta learns the task similarity parameter, which we call
ada-POMRL . We compare the two algorithms empirically in a 10 state, 2 action MDP with closely related
tasks with a total of T= 15tasks (details of the experiment setup are deferred to Sec. 5).
Performance of POMRL.Recall that POMRLis primarily learning the regularizer and assumes the knowledge
of the underlying task similarity (i.e. Ïƒ). We observe in Fig. 3 that with each round tâˆˆTPOMRLis able to
plan better as it learns and adapts the regularizer to the incoming tasks. The convergence rate and ï¬nal
performance corroborates with our theory.
7Published in Transactions on Machine Learning Research (06/2023)
Can we also meta-learn the task-similarity parameter? In practice, the parameter Ïƒ
may not be known and must be estimated online and plugged in (see Appendix C for details).
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
 ada-POMRL
POMRL(Â¾)
Figure 3: ada-POMRL enables
meta-learning the task-similarity
on-the-ï¬‚ywithaperformancegap
for the initial set of tasks as com-
pared to the oracle POMRL, but
improves with more tasksAlg. 2 ada-POMRL uses Welfordâ€™s algorithm to compute an online estimate
of the variance after every task using the model estimators, and simply
plugs-in this estimate wherever POMRLwas using the true value. From
the perspective of ada-POMRL ,POMRLis an "oracle", i.e. the underlying
task-similarity is known. However, in most practical scenarios, the learner
does not have this information a priori.
We compare empirically POMRLand ada-POMRL on a strongly structured
problem (Ïƒâ‰ˆ0.01) in Fig. 3 and observe that meta-learning the under-
lying task relatedness allows ada-POMRL to adapt to the incoming tasks
accordingly. Adaptation on-the-ï¬‚y with ada-POMRL comes at a cost i.e., the
performance gap in comparison to POMRLbut eventually converges albeit
with a slower rate. This is intuitive and a similar theoretical guarantee
applies (See Remark 1).
This online estimation of Ïƒmeans that ada-POMRL now requires an initial
value for Ë†Ïƒ1, which is a choice left to the practitioner, but will only aï¬€ect the results of a ï¬nite number of
tasks at the beginning. Using Ë†Ïƒ1too small will give a slightly increased weight to the prior in initial tasks,
which is not desirable as the latter is not yet learned and will result in an increased bias. On the other
hand, setting Ë†Ïƒ1too large (i.e close to 1/2) will decrease the weight of the prior and increase the variance of
the returned solution; in particular, in cases where the true Ïƒis small, a large initialization will slow down
convergence and we observe empirical larger gaps between POMRLandada-POMRL . In the extreme case where
Ïƒâ‰ˆ0, a large initialization will drastically slow down ada-POMRL as it will take many tasks before it discovers
that the optimal behavior is essentially to aggregate the batches.
Algorithm 2: ada-POMRL â€“ Planning with Online Meta-Reinforcement Learning
Input:Initialize Ë†Po,1to uniform, (Ë†Ïƒ)1as a matrix of size SÃ—A,Î±1= 0.
fortasktâˆˆ[T]do
fortthbatch ofmsamplesdo
Ë†Pt(m) = (1âˆ’Î±t)1
m/summationtextm
i=1Xi+Î±tË†Po,t//regularized least squares minimizer.
Î³â‹†â† âˆ’Î³-Selection-Procedure (m,Î±t,Ïƒt,T,S,A )
Ï€â‹†
Ë†Pt,Î³â‹†â†Planning (Ë†Pt(m))
Output:Ï€â‹†
Ë†Pt,Î³â‹†
Update Ë†Po,t+1,Ë†Ïƒt+1â† âˆ’Welfordâ€™s online algorithm/parenleftBig
( Ë†Ïƒo)t,Ë†Po,t+1,Ë†Po,t/parenrightBig
//meta-update AoM
(Eq. 5) and task-similarity parameter.
UpdateÎ±t+1=1
Ë†Ïƒt+12(1+1/t)m+1//meta-update mixing rate, plug max(ÏƒSÃ—A)
Tasks vary only in certain states and actions. Thus far, we considered a uniform notion of task
similarity as Eq. 3 holds for any (s,a). However, in many practical settings the transition distribution might
remains the same for most part of the state space but only vary on some states across diï¬€erent tasks. These
scenarios are hard to analyse in general because local changes in the model parameters do not always imply
changes in the optimal value function nor necessarily modify the optimal policy. Our Theorem 2 still remains
valid, but it may not be tight when the meta-distribution has non-uniform noise levels. More precisely
Theorem 1 in Appendix D remains locally valid for each (s,a)pair and one could easily replace the uniform
Ïƒwith localÏƒ(s,a), but this cannot directly imply a stronger bound on the average planning loss. Indeed, in
our experiments, in both POMRLandada-POMRL , the parameter ÏƒandË†Ïƒrespectively, are SÃ—Amatrices of
state-action dependent variances resulting in state-action dependent mixing rate Î±t.
8Published in Transactions on Machine Learning Research (06/2023)
5 Experiments
We now study the empirical behavior of planning with online meta-learning in order to answer the following
questions: Q1.Does meta-learning a good initialization of the dynamics model facilitate improved planning
accuracy for the choice of Î³=Î³eval? (Sec. 5.1) Q2.Does meta-learning a good initialization of the dynamics
model enables longer planning horizons? (Sec. 5.2) Q3.How does performance depend on the amount of
shared structure across tasks i.e.,Ïƒ? (Sec. 5.3) Source code is provided in the supplementary material.
Setting: For each experiment, we ï¬x a mean model Poâˆˆâˆ†SÃ—A
S(see below how), and for each new task
tâˆˆ[T], we sample Ptfrom a Dirichlet distribution3centered at Po. As prescribed by theory (see Sec.3.2),
we set4Ïƒâ‰ˆ0.01.1/Sâˆšmunless otherwise speciï¬ed (see Q3). Note that ÏƒandË†Ïƒrespectively, are SÃ—A
matrices of state-action dependent variances that capture the directional variance as we used Dirichlet
distributions as priors and these have non-uniform variance levels in the simplex, depending on how close
to the simplex boundary the mean is located. Aligned with our theory, we use the max of the Ïƒmatrices
resulting in the aforementioned single scalar value. As in Jiang et al. (2015), Po(and eachPt) characterizes
a random chain MDP with S= 10states5andA= 2actions, which is drawn such that, for each stateâ€“action
pair, the transition function P(s,a,s/prime)is constructed by choosing randomly k= 5states whose probability is
set to 0. Then we draw the value of the Sâˆ’kremaining states uniformly in [0,1]and normalize the resulting
vector.
5.1 Meta-reinforcement learning leads to improved planning accuracy for [ Î³eval]. [Q1.]
We consider the aforementioned problem setting with a total of T= 15closely related tasks and focus on the
planning loss gains due to improved model accuracy. We ï¬x Î³=Î³eval, a rather naive Î³-Selection-Procedure
and show the planning loss of POMRL(Alg. 1) with the following baselines : 1)Oracle Prior Knowledge
knows a priori the underlying task structure ( Po,Ïƒ) and uses an estimator (Eq. 4) with exact regularizer
Poand optimal mixing rate Î±t=1
Ïƒ2(1+1/t)m+1, 2)Without Meta-Learning simply uses Ë†Pt=Â¯Pt, the
count-based estimated model using the msamples seen in each task, 3) POMRL(Alg. 1) meta-learns the
regularizer but knows apriori the underlying task structure, and 4) ada-POMRL (Alg. 2) meta-learns not only
the regularizer, but also the underlying task-similarity online. The oracle is a strong baseline that provides
a minimally inaccurate model and should play the role of an "empirical lower bound". For all baselines,
the number of samples per task m= 5. Results are averaged over 100independent runs. Besides, we also
propose and empirically validate competitive heuristics for Î³-Selection-Procedure in Sec. 6. Besides, we
also run another baseline called Aggregating( Î±= 1), that simply ignores the meta-RL structure and just
plans assuming there is a single task (See Appendix F.2).
Inspecting Fig. 4(a) , we can see that our approach ada-POMRL (green) results in decreasing per-task
planning loss as more tasks are seen, and decreasing variance as the estimated model gets more stable and
approaches the optimal value returned by the oracle prior knowledge baseline (blue). On the contrary, without
meta-learning (red), the agent struggles to cope as it faces new tasks every round, and its performance does not
improve. ada-POMRL gradually improves as more tasks are seen whilst adaptation to learned task-similarity
on-the-ï¬‚y which is the primary cause of the performance gap in ada-POMRL and POMRL. Importantly, no
prior knowledge about the underlying task relatedness enables a more practical algorithm with the same
theoretical guarantees (See Sec. 4). Recall that oracle prior knowledge is a strong baseline as it corresponds
to both known task relatedness and regularizer.
5.2 Meta-learning the underlying task relatedness enables longer planning horizons. [Q2.]
We run ada-POMRL forT= 15(withÏƒâ‰ˆ0.01)as above and report planning losses for a range of values of
guidanceÎ³factors. Results are averaged over 100independent runs and displayed on Fig. 4(b). We observe
3The variance of this distribution is controlled by its coeï¬ƒcient parameters Î±1:S: the larger they are, the smaller is the variance.
More details on our choices are given in Appendix F.1. Dirichlet distributions with small variance satisfy the high-probability
version of our structural assumption 3 for Ïƒ= max iÏƒi
4Our priors are multivariate Dirichlet distribution in dimension Sso we divide the theoretical rate by Sto ensure the max
bounded by 1/âˆšm. See App. F for implementation details.
5We provide additional experiments with varying size of the state space in Appendix Fig. F5.
9Published in Transactions on Machine Learning Research (06/2023)
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(Â¾)
(a)
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (b)
123456789101112131415
Task T0.00.20.40.60.81.0Planning Loss
Â°eval=0:99 Planning Loss 
0.00.20.40.60.81.0
Empircally optimal guidance 
 discount factorOptimal Gamma (Â°â‹†) (c)
Figure 4: Planning with Online Meta-Learning. (a) Per-task planning loss of our algorithms
POMRLandada-POMRL compared to an Oracle, and Without Meta-learning baselines. All methods use a ï¬xed
Î³=Î³eval= 0.99.(b)ada-POMRL â€™s planning loss decreases as more tasks are seen. Markers denote the
Î³that minimizes the planning loss in respective tasks. Error bars show standard error. (c)ada-POMRL â€™s
empirically optimal guidance discount factor (right y axis) depicts the eï¬€ective planning horizon, i.e.,
one that minimizes the planning loss. Optimal Î³aka the eï¬€ective planning horizon is larger with online
meta-learning. Planning loss (left y axis) shows the minimum planning loss achieved by the agent in that
roundT. Results are averaged over 100independent runs and error bars represent 1-standard deviation.
in Fig. 4(b) when the agent has seen fewer tasks T, an intermediate value of the discount is optimal, i.e., one
that minimizes the task-averaged planning loss (Î³â‹†<0.5). In the presence of strong underlying structure
across tasks, as the agent sees more tasks, the eï¬€ective planning horizon (Î³â‹†>0.7)shifts to a
larger value - one that is closer to the gamma used for evaluation (Î³eval= 0.99).
As we incorporate the knowledge of the underlying task distribution, i.e., meta-learned initialization of
the dynamics model, we note that the adaptive mixing rate Î±tputs increasing amounts of weight on the
shared task-knowledge. Note that this conforms to the eï¬€ect of increasing weight on the model initialization
that we observed in Fig. 2. As predicted by theory, the per-task planning loss decreases as Tgrows and
is minimized for progressively larger values of Î³, meaning for longer planning horizons (See Fig. 4(c)). In
addition, Appendix Fig. F4 depicts the eï¬€ective planning horizon individually for ada-POMRL , Oracle and
without meta learning baselines.
5.3 POMRL and ada-POMRL perform consistently well for varying task-similarity. [Q3.]
We have thus far studied scenarios where the learner can exploit strong task relatedness, i.e.,Ïƒâ‰ˆ0.01<
1/(Sâˆšm)(for low data per task i.e.,m= 5) is small and we now illustrate the other regimes discussed in
Section 3.2. We show that our algorithms remain consistently good for all amounts of task-similarity.
We letÏƒvary to cover the three regimes :Ïƒâ‰ˆ0.01corresponding to fast convergence, Ïƒ= 0.025is in the
intermediate regime (needs longer T), andÏƒ= 0.047is the loosely structured case where we donâ€™t expect
much meta-learning to help improve model accuracy. The small inset ï¬gures in Fig. 5 represent the task
distribution in the simplex. In all cases, ada-POMRL estimatesÏƒonline and we report the planning losses for
a range ofÎ³â€™s. Inspecting Fig. 5, we observe that while in the presence of closely related tasks (Fig. 5(a))
all methods perform well (except without meta-learning). As the underlying task relatedness decreases (for
intermediate regime in Fig. 5(b)), both POMRLand ada-POMRL remain consistent in their performance as
compared to the Oracle Prior Knowledge baseline. When the underlying tasks are loosely related (as in
Fig. 5(c)), ada-POMRL andPOMRLcan still perform well in comparison to other baselines.
Next, we report and discuss the planning loss plot for ada-POMRL for the three cases are shown in Figures 5(d),
5(e), and 5(f) respectively. An intermediate value of task-similarity (Fig. 5(e)) still leads to gains, albeit at a
lower speed of convergence. In contrast, a large value of Ïƒ= 0.047indicates little relatedness across tasks
resulting in minimal gains from meta-learning here as seen in Fig. 5(f). The learner struggles to learn a good
initialization of the model dynamics as there is no natural one. All planning loss curves remain U-shaped and
overall higher with an intermediate optimal guidance Î³value ( 0.5). However, ada-POMRL does not do worse
overall than the initial run T= 1, meaning that while there is not a signiï¬cant improvement, our method
10Published in Transactions on Machine Learning Research (06/2023)
Strong Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(Â¾)
(a)Medium Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(Â¾)
(b)Loosely Structured
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
POMRL(Â¾)
(c)
3ODQQLQJ/RVV7 7 7 7 7 7 7 7 
(d)
3ODQQLQJ/RVV7 7 7 7 7 7 7 7 
 (e)
3ODQQLQJ/RVV7 7 7 7 7 7 7 7 
 (f)
Figure 5: POMRLand ada-POMRL are robust to varying task-similarity Ïƒfor a small ï¬xed amount of
datam= 5available at each round tâˆˆT. A small value of Ïƒreï¬‚ects the fact that tasks are closely related to
each other and share a good amount of structure whereas a much larger value indicates loosely related tasks
(simplex plots illustrate the meta-distribution in dimension 2). In the former case, meta-learning the shared
structure alongside a good model initialization leads to most gains. In the latter, the learner struggles to cope
with new unseen tasks which diï¬€er signiï¬cantly. Error bars represent 1-standard deviation of uncertainty
across 100independent runs.
does not hurt performance in loosely related tasks6. Recall that ada-POMRL has no apriori knowledge of the
number of tasks ( T), or the underlying task relatedness ( Ïƒ)i.e., adaptation is on-the-ï¬‚y.
6 Adaptation of Planning Horizon Î³
We now propose and empirically validate two heuristics to design an adaptive schedule for Î³based on existing
work (Sec. 6.1) and on our average regret upper bound (Sec. 6.2).
6.1 Schedule adapted from Dong et al. (2021) [ Î³=f(m,Î±t,Ïƒt,T)]
Dong et al. (2021) study a continuous, never-ending RL setting. They divide the time into growing phases
(Tt)tâ‰¥0, and tune a discount factor Î³t= 1âˆ’1/T1/5
t. We adapt their schedule to our problem, where the time
is already naturally divided into tasks: for each tâ‰¥0, we deï¬ne the phase size Ttand the corresponding Î³tas
T0=m, Tt=SA
L/parenleftbig
(1âˆ’Î±t)m+Î±tm(tâˆ’1)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
eï¬ƒcient sample size/parenrightbig
, Î³t= 1âˆ’1
T1/5
t,
whereLis the maximum trajectory length. The size of each Tt,tâ‰¥1, is controlled by an "eï¬ƒcient sample
size" which includes a combination of the current taskâ€™s samples and of the samples observed so far, as used
to construct our estimator in POMRL.
6.2 Using the upper bound to guide the schedule [ Î³= min{1,Î³0+ ËœÎ³}]
Having a second look at Theorem 2, we see that the r.h.s. is a function of Î³of the form
U:Î³/mapstoâ†’1
1âˆ’Î³eval+1
Î³âˆ’1+Cm,T,S,A,Ïƒ,Î´Î³
(1âˆ’Î³)2,
6The theoretical bound may lead to think that the average planning loss is higher due to the introduced bias, but in practice
we do not observe that, which means our bound is pessimistic on the second order terms.
11Published in Transactions on Machine Learning Research (06/2023)
where the ï¬rst term is positive and monotonically decreasing on (0,Î³eval)and the second term is positive and
monotonically increasing on (0,1). We simplify and scale this constant, keeping only problem-related terms:
Ct= (1âˆš
t(Ïƒ+1âˆšm)/(Ïƒ2m+ 1) +Ïƒ2m1âˆšm/(Ïƒ2m+ 1), which is of the order of the constant in equation 6.
Optimizing Î³by using the function Uwith constant Cdoes not lead to a principled analytical value strictly
speaking because Uis derived from an upper bound that may be loose and may not reï¬‚ect the true shape
of the loss w.r.t. Î³, but we may use the resulting growth schedule to guide our choices online. In general,
the existence of a strict minimum for Uin(0,1)is not always guaranteed: depending on the values of
Câ‰ˆCm,T,S,A,Ïƒ , the function may be monotonic and the minimum may be on the edges. We give explicit
ranges in the proposition below, proved in Appendix E.
Proposition 1. The existence of a strict minimum in (0,1)is determined by C=Cm,T,S,A,Ïƒ,Î´ (which can
be computed) as follows:
ËœÎ³=ï£±
ï£´ï£²
ï£´ï£³0ifCâ‰¥1
1ifC < 1/2
1âˆ’C
1+Cotherwise, i.e if 1/2<C < 1
We use these values as a guide. Typically, when T= 1andmis small, the multiplicative term Cis large and
the bound is not really informative (concentration has not happened yet), and Î³should be small, potentially
close to but not equal to zero. As a heuristic, we propose to simply oï¬€set ËœÎ³by an additional Î³0such that the
guidance discount factor is Î³=min{1,Î³0+ËœÎ³}, whereÎ³0should be reasonably chosen by the practitioner to
allow for some short-horizon planning at the beginning of the interaction. Empirically, Î³0=âˆˆ(0.25,0.50)
seems reasonable for our random MDP setting as it corresponds to the empirical minima on Fig 4(b).
123456789101112131415
Number of tasks (T)100Planning LossBest Fixed
Dynamic Best
Â°eval=0:99Â°=f(m;Â®t;Â¾t;T) 
Â°=minÂ©
1;Â°0+~Â°Âª
 
(a)
123456789101112131415
Number of tasks (T)100Planning LossBest Fixed
Dynamic BestÂ°=f(m;Â®t;Â¾t;T) 
Â°=minÂ©
1;Â°0+~Â°Âª
 (b)
123456789101112131415
Number of tasks (T)100Planning LossÂ°=minÂ©
1;Â°0+~Â°ÂªÂ°0=0:35
Â°0=0:25Â°0=0:45
Â°0=0:50 (c)
Figure 6:Adapting the planning horizon during online meta-learning reduces planning loss. (a)
Planning with online-meta learning shows that allbaselines outperform using a constant discount factor.
(b)Zoomed in plot of average planning loss over the progression of tasks T shows competitive performance
with the proposed schedule of Î³=f(m,Î±t,Ïƒt,T)beating best-ï¬xed as more tasks are seen. The Î³schedule
Î³=min{1,Î³0+ËœÎ³}using the upper bound as a guidance beats the best-ï¬xed and is very competitive to the
dynamic-best baseline. (c)Using the upper bound to guide the schedule signiï¬cantly outperforms Î³evaland
is shown for Î³0âˆˆ(0.25,0.50). Error bars depict 1-standard error for 600independent runs.
6.3 Empirical Validation
Next, we empirically test the proposed schedules for adaptation of discount factors. We consider the setup
described in Sec. 5 with 15tasks in a 10-state, 2-action random MDP distribution of tasks with Ïƒâ‰ˆ0.01.
In Fig. 6, we plot the planning loss obtained by POMRLwith our schedules, a ï¬xed Î³evaland two strong
baselines: best ï¬xed which considers the best ï¬xed value of discount over all tasks estimated in hindsight and
dynamic best which considers the best choice if we had used the optimal Î³â‹†in each round as in Fig. 4(c). It
is important to note that dynamic best is a lower bound that we cannot outperform.
We observe in Fig. 6(a) that Î³evalresults in a very high loss, potentially corresponding to trying to plan too far
ahead despite model uncertainty. Upon inspecting Fig. 6(b), we observe that the proposed Î³=f(m,Î±t,Ïƒt,T)
obtains similar performance to best ï¬xed and is within the signiï¬cance range of the lower bound. Our second
heuristic,Î³=min{1,Î³0+ËœÎ³}obtains similarly good performance, as seen in Fig. 6(b). Fig. 6(c) shows the
eï¬€ect of diï¬€erent values of Î³0in the prescribed range. These results provide evidence that it is possible to
adapt the planning horizon as a function of the problemâ€™s structure (meta-learned task-similarity) and sample
sizes. Adapting the planning horizon online is an open problem and beyond the scope of our work.
12Published in Transactions on Machine Learning Research (06/2023)
7 Discussion and Future Work
We presented connections between planning with inaccurate models and online meta-learning via a high-
probability task-averaged regret upper-bound on the planning loss that primarily depends on task-similarity
Ïƒas opposed to the entire search space Î£. Algorithmically, we demonstrate that the agent can use its
experience in each task andacross tasks to estimate both the transition model and the distribution over
tasks. Meta-learning the underlying task similarity and a good initialization of transition model across tasks
enables longer planning horizons.
Beyond the tabular case: Function approximation is at the heart of practical RL so a natural question
is how to extend our work to parametrized models. For linear MDPs, MÃ¼ller & Pacchiano (2022) recently
derived regret bounds in the ï¬xed-horizon setting for an algorithm using meta-regularizers similar to ours.
One question is whether this idea could be extended to inï¬nite horizons and further to non-linear, richer
representations. Another, and perhaps deeper question, is around designing and evaluating better planning
strategies. Should we revisit such line of work under the light of the planning loss rather than the regret? On-
or Oï¬€- Policy Meta-Learning without a simulator: Realistic problem settings in RL involve using
sequentially learnt policies to collect data instead of the simulator. One direction could be to extend our
approach to model-based RL algorithms via meta-gradient updates as in ARUBA or MAML, and seek regret
guarantees induced by our concentration results. Non-stationary meta-distribution : Many real-world
scenarios have (slow or sudden) drifts in the underlying distribution itself, e.g. weather. A promising future
direction is to consider non-stationary environments where the optimal initialization varies over time.
Acknowledgments
The authors would like to thank Zheng Wen, Andras Gyorgy, and anonymous reviewers for valuable feedback
on a draft of this paper, Sebastian Flennerhag, David Abel, and Benjamin Van Roy for insightful discussions
during the course of this project. C.Vernade is funded by the Deutsche Forschungsgemeinschaft (DFG) under
both the project 468806714 of the Emmy Noether Programme and under Germanyâ€™s Excellence Strategy â€“
EXC number 2064/1 â€“ Project number 390727645. CV also thanks the international Max Planck Research
School for Intelligent Systems (IMPRS-IS).
References
Ron Amit, Ron Meir, and Kamil Ciosek. Discount factor as a regularizer in reinforcement learning. In
International conference on machine learning , pp. 269â€“278. PMLR, 2020.
Dilip Arumugam, David Abel, Kavosh Asadi, Nakul Gopalan, Christopher Grimm, Jun Ki Lee, Lucas Lehnert,
and Michael L Littman. Mitigating planner overï¬tting in model-based reinforcement learning. arXiv
preprint arXiv:1812.01129 , 2018.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. In International Conference on Machine Learning , pp. 424â€“433. PMLR, 2019.
Jonathan Baxter. A model of inductive bias learning. Journal of artiï¬cial intelligence research , 12:149â€“198,
2000.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artiï¬cial Intelligence Research , 47:253â€“279, 2013.
David Blackwell. Discrete dynamic programming. The Annals of Mathematical Statistics , pp. 719â€“726, 1962.
Rich Caruana. Multitask learning. Machine learning , 28(1):41â€“75, 1997.
Leonardo Cella, Alessandro Lazaric, and Massimiliano Pontil. Meta-learning with stochastic linear bandits.
InInternational Conference on Machine Learning , pp. 1360â€“1370. PMLR, 2020.
NicolÃ² Cesa-Bianchi, Pierre Laforgue, Andrea Paudice, and Massimiliano Pontil. Multitask online mirror
descent. arXiv preprint arXiv:2106.02393 , 2021.
13Published in Transactions on Machine Learning Research (06/2023)
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around a common
mean.Advances in Neural Information Processing Systems , 31, 2018.
Giulia Denevi, Dimitris Stamos, Carlo Ciliberto, and Massimiliano Pontil. Online-within-online meta-learning.
Advances in Neural Information Processing Systems , 32, 2019.
Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Simple agent, complex environment: Eï¬ƒcient reinforce-
ment learning with agent state. arXiv preprint arXiv:2102.05261 , 2021.
William Fedus, Carles Gelada, Yoshua Bengio, Marc G Bellemare, and Hugo Larochelle. Hyperbolic
discounting and learning over multiple horizons. arXiv preprint arXiv:1902.06865 , 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In International Conference on Machine Learning , pp. 1126â€“1135. PMLR, 2017.
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In International
Conference on Machine Learning , pp. 1920â€“1930. PMLR, 2019.
Sebastian Flennerhag, Yannick Schroecker, Tom Zahavy, Hado van Hasselt, David Silver, and Satinder Singh.
Bootstrapped meta-learning. arXiv preprint arXiv:2109.04504 , 2021.
Sebastian Flennerhag, Tom Zahavy, Brendan Oâ€™Donoghue, Hado van Hasselt, AndrÃ¡s GyÃ¶rgy, and Satinder
Singh. Optimistic meta-gradients. In Sixth Workshop on Meta-Learning at the Conference on Neural
Information Processing Systems , 2022.
Vincent FranÃ§ois-Lavet, Guillaume Rabusseau, Joelle Pineau, Damien Ernst, and Raphael Fonteneau. On
overï¬tting and asymptotic bias in batch reinforcement learning with partial observability. Journal of
Artiï¬cial Intelligence Research , 65:1â€“30, 2019.
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of eï¬€ective planning horizon
on model accuracy. In Proceedings of the 2015 International Conference on Autonomous Agents and
Multiagent Systems , pp. 1181â€“1189. International Foundation for Autonomous Agents and Multiagent
Systems, 2015.
Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-optimal planning
in large markov decision processes. Machine learning , 49(2):193â€“208, 2002.
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar. Adaptive gradient-based meta-learning
methods. arXiv preprint arXiv:1906.02717 , 2019.
Levente Kocsis and Csaba SzepesvÃ¡ri. Bandit based monte-carlo planning. In European conference on
machine learning , pp. 282â€“293. Springer, 2006.
Jelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom Zahavy, and Satinder Singh.
Meta-gradients in non-stationary environments. In ICLR Workshop on Agent Learning in Open-Endedness ,
2022.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex
Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through
deep reinforcement learning. Nature, 518(7540):529, 2015.
Robert MÃ¼ller and Aldo Pacchiano. Meta learning mdps with linear transition models. In International
Conference on Artiï¬cial Intelligence and Statistics , pp. 5928â€“5948. PMLR, 2022.
Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado van Hasselt, Satinder Singh, and
David Silver. Discovering reinforcement learning algorithms. arXiv preprint arXiv:2007.08794 , 2020.
Marek Petrik and Bruno Scherrer. Biasing approximate dynamic programming with a lower discount factor.
Advances in neural information processing systems , 21, 2008.
Joelle Pineau. The machine learning reproducibility checklist. arxiv, 2019.
14Published in Transactions on Machine Learning Research (06/2023)
Juergen Schmidhuber and Rudolf Huber. Learning to generate artiï¬cial fovea trajectories for target detection.
International Journal of Neural Systems , 2(01n02):125â€“134, 1991.
Terence Tao and Van Vu. Random matrices: universality of local spectral statistics of non-hermitian matrices.
The Annals of Probability , 43(2):782â€“874, 2015.
Sebastian Thrun and Lorien Pratt. Learning to learn: Introduction and overview. In Learning to learn , pp.
3â€“17. Springer, 1998.
Harm Van Seijen, Hado Van Hasselt, Shimon Whiteson, and Marco Wiering. A theoretical and empirical
analysis of expected sarsa. In 2009 ieee symposium on adaptive dynamic programming and reinforcement
learning, pp. 177â€“184. IEEE, 2009.
Zhongwen Xu, Hado van Hasselt, and David Silver. Meta-gradient reinforcement learning. arXiv preprint
arXiv:1805.09801 , 2018.
Tom Zahavy, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, Junhyuk Oh, Hado P van Hasselt, David Silver,
and Satinder Singh. A self-tuning actor-critic algorithm. Advances in neural information processing systems ,
33, 2020.
15Published in Transactions on Machine Learning Research (06/2023)
A Additional Related Work
Discount Factor Adaptation. For almost all real-world applications, RL agents operate in a much larger
environment than the agent capacity in the context of both the computational and memory complexity (e.g.
the internet). Inevitably, it becomes crucial to adapt the planning horizon over time as opposed to using a
relatively longer planning horizon from the start (which can be both expensive and sub-optimal). This has
been extensively studied in the context of planning with inaccurate models in reinforcement learning (Jiang
et al., 2015; Arumugam et al., 2018).
Dong et al. (2021) introduced a schedule for Î³that we take inspiration from in Section 6.1. They consider a
â€™never-ending RLâ€™ problem in the inï¬nite-horizon, average-regret setting in which the true horizon is 1, but
show that adopting a diï¬€erent smaller discount value proportional to the time in the agentâ€™s life results in
signiï¬cant gains. Their focus and contributions are diï¬€erent from ours as they are interested in asymptotic
rates, but we believe the connection between our ï¬ndings is an interesting avenue for future research.
Meta-Learning and Meta-RL, orlearning-to-learn has shown tremendous success in online discovery of
diï¬€erent aspects of an RL algorithm, ranging from hyper-parameters (Xu et al., 2018) to complete objective
functions (Oh et al., 2020). In recent years, many deep RL agents (Fedus et al., 2019; Zahavy et al., 2020)
have gradually used higher discounts moving away from the traditional approach of using a ï¬xed discount
factor. However, to the best of our knowledge, existing works do not provide a formal understanding of why
this is helping the agents in better performance, especially across varied tasks. Our analysis is motivated
by the aforementioned empirical success of adapting the discount factor. While there has been signiï¬cant
progress in meta-learning-inspired meta-gradients techniques in RL (Xu et al., 2018; Zahavy et al., 2020;
Flennerhag et al., 2021), they are largely focused on empirical analysis with lot or room for in-depth insights
about the source of underlying gains.
B Closed-form solution of the regularized least squares
We note that each Ë†Pshould be understood as Ë†P(s,a)(s/prime).
âˆ‡/lscript(P|h) =âˆ’2
mm/summationdisplay
i=1(Xiâˆ’P) + 2Î»(Pâˆ’h)
âˆ‡/lscript(P|h) = 0â‡â‡’P(1 +Î») =/summationtext
iXi
m+Î»h
Ë†P(s,a)(s/prime|h) =1
1 +Î»/summationtext
iXi
m+Î»
1 +Î»h (9)
=Î±h+ (1âˆ’Î±)/summationtextiXi
mwhereÎ±=Î»
1 +Î»(10)
Derivation of Mixing Rate Î±t:To chooseÎ±t, we want to minimize the MSE of the ï¬nal estimator.
EXâˆ¼Pt/parenleftBig
Ë†Ptâˆ’Pt/parenrightBig2
=EXâˆ¼Pt/parenleftbig
Î±tht+ (1âˆ’Î±t)Â¯Ptâˆ’Pt/parenrightbig2
=EXâˆ¼Pt/parenleftbig
Î±t(htâˆ’Pt) + (1âˆ’Î±t)(Â¯Ptâˆ’Pt)/parenrightbig2
=Î±2
t(htâˆ’Pt)2+ (1âˆ’Î±t)2EXâˆ¼Pt/parenleftbig
(Â¯Ptâˆ’Pt)/parenrightbig2
where the cross term 2Î±)t(htâˆ’Pt)(1âˆ’Î±t)EXâˆ¼PtE/bracketleftbigÂ¯Ptâˆ’Pt/bracketrightbig
= 0sinceE[Â¯Pt] =Pt. This is the classic
bias-variance decomposition of an estimator and we see that the choice of htplays a role as well as the
variance of Â¯Pt, which is upper bounded by 1/m(because each Xi,tis bounded in (0,1)). For instance, for
the choiceht=Po, by our structural assumption 3 we get:
EXâˆ¼Pt/parenleftBig
Ë†Ptâˆ’Pt/parenrightBig2
â‰¤Î±2Ïƒ2+ (1âˆ’Î±)21
m,
16Published in Transactions on Machine Learning Research (06/2023)
and we minimize this upper bound in Î±to obtain the mixing coeï¬ƒcient with smallest MSE: Î±âˆ—=1
Ïƒ2m+1, or
equivalently Î»âˆ—=1
Ïƒ2m. Recall this is the within-task estimatorâ€™s variance where we consider the true Po.
In practice, however, we meta-learn the prior, so for t>1,ht=Ë†Po,t=1
tâˆ’1/summationtexttâˆ’1
j=1Â¯Pj. Intuitively, as mandt
grow large, Ë†Po,tâ†’Poand we retrieve the result above (we show this formally to prove Eq. 20 in the proof of
our main theorem). To obtain a simple expression for Î±t, we minimize the "meta-MSE" of our estimator:
EPtâˆ¼Po/parenleftBig
Ë†Ptâˆ’Pt/parenrightBig2
=Î±2
tEPtâˆ¼PoEXâˆ¼Pt/parenleftbig
htâˆ’Pt/parenrightbig2+ (1âˆ’Î±t)2EPtâˆ¼PoEXâˆ¼Pt/parenleftbig
(Â¯Ptâˆ’Pt)/parenrightbig2
â‰¤Î±2
tEPtâˆ¼Poï£«
ï£­1
tâˆ’1tâˆ’1/summationdisplay
j=1Pjâˆ’Po+Poâˆ’Ptï£¶
ï£¸2
+ (1âˆ’Î±t)21
m
â‰¤Î±2
tÏƒ2(1 + 1/t) + (1âˆ’Î±t)21
m,
where in the last inequality, we upper bounded the variance of1
tâˆ’1/summationtexttâˆ’1
j=1Pj(the "denoised" Ë†P0,t) byÏƒ2/t
since each Ptis bounded in [Poâˆ’Ïƒ,Po+Ïƒ]by our structural assumption. Minimizing that last upper
bound inÎ±tleads toÎ±t=1
(Ïƒ2)(1+1/t)m+1â‰¤
â†’tÎ±âˆ—, whentâ†’âˆ. This means that the uncertainty on the prior
implies that its weight in the estimator is smaller, but eventually converges at a fast rate to the optimal value
(when the exact optimal prior is known). This inequality holds with probability 1âˆ’Î´because we use the
concentration of Ë†Po,t(see proof of Theorem 17 below)
C Online Estimation
Online Estimation of Prior. At each task, the learner gets minteractions per state-action pair. At task
t= 1, learner can compute the prior based on the samples seen so far, i.e.:
Ë†Pt=1
o(s/prime|s,a) ={/summationtextm
i=1Xi}t=1
m
At subsequent tasks,
Ë†Pt=2
o(s/prime|s,a) ={/summationtext2m
i=1Xi}t=1:2
2m=1
2/parenleftBig{/summationtextm
i=1Xi}
m+{/summationtext2m
i=m+1Xi}
m/parenrightBig
=1
2/parenleftBig
Ë†Pt=1
o(s/prime|s,a) +{/summationtext2m
i=m+1Xi}
m/parenrightBig
Similarly,
Ë†Pt=3
o(s/prime|s,a) ={/summationtext3m
i=1Xi}t=1:3
3m={/summationtext2m
i=1Xi}t=1:2+{/summationtext3m
i=2m+1Xi}t=3
3m
=1
3/parenleftBig2{/summationtext2m
i=1Xi}t=1:2
2m+{/summationtext3m
i=2m+1Xi}t=3
m/parenrightBig
=â‡’Ë†Pt
o(s/prime|s,a) =1
t/parenleftBig
(tâˆ’1)Ë†Ptâˆ’1
o(s/prime|s,a) +/summationtexttm
i=(tâˆ’1)m+1Xi
m/parenrightBig
Therefore,
Ë†Pt
o(s/prime|s,a) =/parenleftBig
1âˆ’1
t/parenrightBig
Ë†Ptâˆ’1
o(s/prime|s,a) +/parenleftBig1
t/parenrightBig/summationtexttm
i=(tâˆ’1)m+1Xi
m(11)
Online Estimation of Variance. Similarly, we can derive the online estimate of the variance:
(Ë†Ïƒ2
o)t= (Ë†Ïƒ2
o)tâˆ’1+(Xmtâˆ’Ë†Ptâˆ’1
o)(Xmtâˆ’Ë†Pt
o)âˆ’(Ë†Ïƒ2
o)tâˆ’1
t(12)
17Published in Transactions on Machine Learning Research (06/2023)
Since the above method is numerically unstable, we will employ Welfordâ€™s online algorithm for variance
estimate.
D Concentration bounds and Proof of Theorem 2
D.1 Proof of Theorem 2
We begin the proof by decomposing each term of the loss:
Lemma 1. For a task t denoted by M, and its estimate denoted by Ë†M,âˆ€sâˆˆS,
VÏ€âˆ—
Pt,Î³eval
Pt,Î³eval(s)âˆ’VÏ€âˆ—
Ë†Pt,Î³
Pt,Î³eval(s) =/parenleftBig
VÏ€âˆ—
Pt,Î³eval
Pt,Î³eval(s)âˆ’VÏ€âˆ—
Pt,Î³eval
Pt,Î³(s)/parenrightBig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
At+/parenleftBig
VÏ€âˆ—
Pt,Î³eval
Pt,Î³(s)âˆ’VÏ€âˆ—
Ë†Pt,Î³
Pt,Î³eval(s)/parenrightBig
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
Bt
We are going to bound each term separately. The term ( At) corresponds to the bias constant due to using Î³
instead ofÎ³evaland was already bounded by Jiang et al. (2015):
Lemma 2. Jiang et al. (2015) For any MDP Ë†Mwith rewards in [0,Rmax],âˆ€Ï€:Sâ†’AandÎ³â‰¤Î³eval,
VÏ€
Pt,Î³â‰¤VÏ€
Pt,Î³evalâ‰¤VÏ€
Pt,Î³+Î³evalâˆ’Î³
(1âˆ’Î³eval)(1âˆ’Î³)Rmax (13)
We denote C(Î³) =Î³evalâˆ’Î³
(1âˆ’Î³eval)(1âˆ’Î³)Rmaxand notice that/summationtext
tAt/T=C(Î³)so that bounds the ï¬rst part of the
average loss.
To bound the second term Bt, we ï¬rst use Lemma 3 (Equation 18) in Jiang et al. (2015) to upper bound
VÏ€âˆ—
Pt,Î³eval
Pt,Î³(s)âˆ’VÏ€âˆ—
Ë†Pt,Î³
Pt,Î³eval(s)â‰¤2 max
sâˆˆS,Ï€âˆˆÎ R,Î³|VÏ€Pt,Î³eval
Pt,Î³(s)âˆ’VÏ€Ë†Pt,Î³
Ë†Pt,Î³eval(s)| (14)
â‰¤2 max
sâˆˆS,aâˆˆA,
Ï€âˆˆÎ R,Î³|QÏ€Pt,Î³eval
Pt,Î³(s,a)âˆ’QÏ€Ë†Pt,Î³
Ë†Pt,Î³eval(s,a)| (15)
Using Lemma 4 from Jiang et al. (2015) and noticing that in our setting we do not estimate RsoË†R=R,
QÏ€
Pt,Î³(s,a) =R(s,a) +Î³/angbracketleftPt(s,a,; ),VÏ€
Pt,Î³/angbracketrightandQÏ€
Ë†Pt,Î³(s,a) =R(s,a) +Î³/angbracketleftË†Pt(s,a,; ),VÏ€
Ë†Pt,Î³/angbracketright, we have
max
sâˆˆS,aâˆˆA,
Ï€âˆˆÎ R,Î³|QÏ€Pt,Î³eval
Pt,Î³(s,a)âˆ’QÏ€Ë†Pt,Î³
Ë†Pt,Î³eval(s,a)|â‰¤1
(1âˆ’Î³)max
sâˆˆS,aâˆˆA,
Ï€âˆˆÎ R,Î³/vextendsingle/vextendsingle/vextendsingleÎ³/angbracketleftË†Pt(s,a,; ),VÏ€
Pt,Î³/angbracketrightâˆ’Î³/angbracketleftPt(s,a,; ),VÏ€
Pt,Î³/angbracketright/vextendsingle/vextendsingle/vextendsingle
(16)
Notice that we are comparing the value functions of two diï¬€erent MDPs which is non-trivial and we leverage
the result of Jiang et al. (2015). We refer the reader to the proof of Lemma 4 therein for intermediate steps.
Now summing over tasks, we have
/summationtext
t(B)t
Tâ‰¤1
TT/summationdisplay
t=12
(1âˆ’Î³)max
sâˆˆS,aâˆˆA,
Ï€âˆˆÎ R,Î³/vextendsingle/vextendsingle/vextendsingleÎ³/angbracketleftË†Pt(s,a,; ),VÏ€
Pt,Î³/angbracketrightâˆ’Î³/angbracketleftPt(s,a,; ),VÏ€
Pt,Î³/angbracketright/vextendsingle/vextendsingle/vextendsingle
â‰¤2Î³
(1âˆ’Î³)1
TT/summationdisplay
t=1max
sâˆˆS,aâˆˆA,
Ï€âˆˆÎ R,Î³/vextendsingle/vextendsingle/vextendsingle/angbracketleftË†Pt(s,a,; )âˆ’Pt(s,a,; ),VÏ€
Pt,Î³/angbracketright/vextendsingle/vextendsingle/vextendsingle
â‰¤2Rmax
(1âˆ’Î³)1
TT/summationdisplay
t=1/summationdisplay
s/primeâˆˆ[S]max
sâˆˆS,aâˆˆA,
Ï€âˆˆÎ R,Î³/vextendsingle/vextendsingle/vextendsingleË†Pt(s,a,s/prime)âˆ’Pt(s,a,s/prime)/vextendsingle/vextendsingle/vextendsingle|VÏ€
Pt,Î³|
â‰¤2RmaxÎ³
(1âˆ’Î³)2S
TT/summationdisplay
t=1max
s,s/primeâˆˆS,aâˆˆA/vextendsingle/vextendsingle/vextendsingleË†Pt(s,a,s/prime)âˆ’Pt(s,a,s/prime)/vextendsingle/vextendsingle/vextendsingle
18Published in Transactions on Machine Learning Research (06/2023)
where we upper-bounded the value function by Rmax/(1âˆ’Î³)and one sum over SbySÃ—maxs/primeâˆˆS.... Note
that this step diï¬€ers from Jiang et al. (2015) and allows us to boil down to an average (worst-case) estimation
error of the transition model. We ï¬nally upper bound the r.h.s using Theorem 1 stated and proved below.
Remark 2. In Jiang et al. (2015), the argument is slightly more direct and involves directly controlling
the deviations of the scalar random variables R(s,a) +Î³/angbracketleftË†Pt(s,a,; ),VÏ€
Ë†Pt,Î³/angbracketright, arguing that it is bounded and
centered at QÏ€
Pt,Î³(s,a). This approach is followed by taking a union bound over the policy space Î RÎ³and
results in a factor log(Î R,Î³)under the square root. We could have followed this approach and obtained a
similar result but we made the alternative choice above as we believe it is informative. In our case, this
factor is replaced (and upper bounded) by the extra Sterm. As a result, we lose the direct dependence on
the size of the policy class, which is a function of Î³and should play a role in the bound. In turn, and at
the price of this extra looseness, we get a slightly more "exploitable" bound (see our heuristic for a gamma
schedule in Section 6). It is easy and straightforward to adapt our concentration bound below to directly bound
R(s,a) +Î³/angbracketleftË†Pt(s,a,; ),VÏ€
Ë†Pt,Î³/angbracketrightâˆ’QÏ€
Pt,Î³(s,a)as in Jiang et al. (2015), and one would obtain a similar bound as
Eq. equation 6 without the factor S, but with an extra log(Î R,Î³).
D.2 Concentration of the model estimator
To avoid clutter in the notation of this section , we drop the (s,a,s/prime)everywhere, as we did in Appendix B
above. All deï¬nitions of Ë†Pand Ë†P0are as stated in the latter section.
Theorem 1. with probability 1âˆ’Î´:
max
s,a,s/prime|Ë†Ptâˆ’Pt|â‰¤1
Ïƒ2m+ 1ï£«
ï£¬ï£­/radicalBigg
log(6T
Î´) log(TS2A
Î´)(Ïƒ2+Î£ log2(6T2
Î´)
m)
T+Ïƒ/radicalBigg
log(3TS2A
Î´)
T+ 2Ïƒï£¶
ï£·ï£¸
+Ïƒ2m
2Ïƒ2m+ 1/radicalBigg
Î£ log(3TS2A
Î´)
2m(17)
For anytâˆˆ[T],s,a,s/primeandÏ€âˆˆÎ R,Î³, deï¬ne Ë†Pt,âˆ—=Î±tPo+ (1âˆ’Î±t)Â¯Pt
mthe optimally regularized estimator
(using the true unknown Pofor eacht). We have
/vextendsingle/vextendsingle/vextendsingleË†Ptâˆ’Pt/vextendsingle/vextendsingle/vextendsingleâ‰¤|Ë†Ptâˆ’Ë†Pt,âˆ—|+|Ë†Pt,âˆ—âˆ’Pt|
â‰¤Î±t|Ë†Po,tâˆ’Po|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(A)+ (1âˆ’Î±t)|Â¯Pt
mâˆ’Pt|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(B)+Î±t|Poâˆ’Pt|/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
â‰¤2Â·Ïƒby assum.(18)
Bounding Term A
Substituting the estimator Ë†Pt=Î±t1
m/summationtextm
iXi+ (1âˆ’Î±t)Ë†Pt
o,
Aâ‰¤Î±tï£«
ï£­|Ë†Pt
oâˆ’1
tâˆ’1tâˆ’1/summationdisplay
j=1Pj|+|1
tâˆ’1tâˆ’1/summationdisplay
j=1Pjâˆ’Po|ï£¶
ï£¸
â‰¤1
Ïƒ2m+ 1ï£«
ï£¬ï£¬ï£¬ï£¬ï£¬ï£­|Ë†Pt
oâˆ’1
tâˆ’1tâˆ’1/summationdisplay
j=1Pj|
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(A1)+|1
tâˆ’1tâˆ’1/summationdisplay
j=1Pjâˆ’Po|
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
A2ï£¶
ï£·ï£·ï£·ï£·ï£·ï£¸
whereÎ±tis simply upper bounded by its initial value1
Ïƒ2m+1and we introduced the denoised (expected)
average1
tâˆ’1/summationtexttâˆ’1
j=1Pj=EP1,...Ptâˆ’1Ë†Po,t. Indeed, by assumption, EPâˆ¼P1
tâˆ’1/summationtexttâˆ’1
j=1Pj=Poand the variance of
19Published in Transactions on Machine Learning Research (06/2023)
this estimator is bounded by Ïƒ2/(tâˆ’1)by our structure assumption. This allows to naturally bound A2
using Hoeï¬€dingâ€™s inequality for bounded random variables: with probability at least 1âˆ’Î´/3,
max
s,a,s/primeA2â‰¤Ïƒ/radicalbigg
log(6S2AT/Î´ )
T(19)
We now bound A1
A1=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
tâˆ’1/summationdisplay
j(Â¯Pj
mâˆ’Pj)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
We note here that the ï¬rst term in A1 is indeed a martingale Mt=/summationtexttâˆ’1
j=1Zj, whereZj=Â¯Pj
mâˆ’Pj, such that
each increment is bounded with high probability: for each j,|Zj|â‰¤cjw.p1âˆ’Î´
6, wherecj=/radicalBig
Î£
mlog(6T2
Î´).
Moreover, the diï¬€erences |Zjâˆ’Zj+1|are also bounded with high probability:
|Zjâˆ’Zj+1|â‰¤|Pjâˆ’Pj+1|+|Â¯Pjâˆ’Â¯Pj+1|<2Ïƒ+ 2cj=Dj= 2/parenleftBigg
Ïƒ+âˆš
Î£ log(6T2
Î´)âˆšm/parenrightBigg
Then by (Tao & Vu, 2015, Prop. 34), for any /epsilon1>0,
P/parenleftBig/vextendsingle/vextendsingle/vextendsingleMt
tâˆ’1/vextendsingle/vextendsingle/vextendsingleâ‰¥/epsilon1
tâˆ’1/radicaltp/radicalvertex/radicalvertex/radicalbttâˆ’1/summationdisplay
j=1D2
j/parenrightBig
â‰¤2 exp(âˆ’2/epsilon12) +tâˆ’1/summationdisplay
j=1Î´
6T2
Choosing/epsilon1=/radicalBig
1
2log(12T
Î´), we get
Pï£«
ï£¬ï£¬ï£­/vextendsingle/vextendsingle/vextendsingle1
tâˆ’1tâˆ’1/summationdisplay
j=1Â¯Pj
mâˆ’Pj/vextendsingle/vextendsingle/vextendsingleâ‰¥/radicaltp/radicalvertex/radicalvertex/radicalbt(Ïƒ+âˆš
Î£ log(6T2
Î´)âˆšm)2log(6T
Î´)
Tï£¶
ï£·ï£·ï£¸â‰¤Î´
6T+Î´
6T=Î´
3T
With a union bound as before, we get that with probability at least 1âˆ’Î´/3,
A1â‰¤/radicalBigg
log(6T
Î´) log(TS2A
Î´)(Ïƒ2+Î£ log2(6T2
Î´)
m)
T(20)
because (Ïƒ+/radicalBig
Î£
m)2â‰¥Ïƒ2+Î£
m.
By combining equation 20 and equation 19, we get:
max
s,a,s/primeÎ±t|Po,tâˆ’Po|â‰¤1
Ïƒ2m+ 1ï£«
ï£¬ï£­/radicalBigg
log(6T
Î´) log(TS2A
Î´)(Ïƒ2+log2(6T2
Î´)
m)
T+Ïƒ/radicalbigg
log(3S2AT/Î´ )
Tï£¶
ï£·ï£¸(21)
Bounding Term B
Term B is simply the concentration of the average of bounded variables Â¯Pt
m=1
m/summationtext
iXi, whose variance is
bounded by 1. So by Hoeï¬€dingâ€™s inequality, and a union bound, with probability at least 1âˆ’Î´/4
max
s,a,s/prime|Â¯Pt
mâˆ’Pt|â‰¤/radicalbigg
Î£ log(4TS2A/Î´)
2m
20Published in Transactions on Machine Learning Research (06/2023)
To bound term B, it remains to upper bound 1âˆ’Î±tfor alltâˆˆ[T]:
1âˆ’Î±t=Ïƒ2(1 +1
t)m
Ïƒ2(1 +1
t)m+ 1â‰¤Ïƒ2m
2Ïƒ2m+ 1
We get that with probability 1âˆ’Î´/3
max
s,a,s/prime(B)â‰¤Ïƒ2m
2Ïƒ2m+ 1/radicalbigg
Î£ log(3TS2A/Î´)
2m(22)
Combining all bounds
To conclude, we combine the bounds on the terms in equation 18, replacing with equation 21,equation 22,
and with a union bound, we get that with probability 1âˆ’Î´,
max
s,a,s/prime|Ë†Ptâˆ’Pt|â‰¤1
Ïƒ2m+ 1ï£«
ï£¬ï£­/radicalBigg
log(6T
Î´) log(TS2A
Î´)(Ïƒ2+Î£ log2(6T2
Î´)
m)
T+Ïƒ/radicalbigg
log(3S2AT/Î´ )
T+ 2Ïƒï£¶
ï£·ï£¸
+Ïƒ2m
2Ïƒ2m+ 1/radicalbigg
Î£ log(3TS2A/Î´)
2m(23)
Discussion
The bound has 4 main terms respectively in ËœO(/radicalBig
1
mT),ËœO(/radicalBig
1
T),ËœO(1
m)and ËœO(/radicalBig
1
m), all scaled by some factor
depending on Ïƒ2andm. A ï¬rst remark is that when mis large and T= 1, the last part in ËœO(/radicalBig
1
m)dominates
due to the factorÏƒ2m
Ïƒ2m+1â†’1, while the coeï¬ƒcient of the ï¬rst two terms goes to 0 fast (in 1/(Ïƒ2m)).
E Proof of Proposition 1
We study the function Udeï¬ned by
U:Î³/mapstoâ†’1
1âˆ’Î³eval+1
Î³âˆ’1+Cm,T,S,A,Ïƒ,Î´Î³
(1âˆ’Î³)2,
whereÎ³evalis a ï¬xed constant and C:=Cm,T,S,A,Ïƒ,Î´ is seen as a parameter whose value controls the general
"shape" of the function. We diï¬€erentiate with respect to Î³:
dU
dÎ³=âˆ’âˆ’C(Î³+ 1) + (1âˆ’Î³)
(1âˆ’Î³)3.
We see that the sign of the derivative is aï¬€ected by the value of the parameter C:
â€¢Ifâˆ€Î³âˆˆ(0,1),âˆ’C(Î³+ 1) + (1âˆ’Î³)>0thenUis monotonically decreasing on (0,1)and the minimum
is reached for Î³= 1,
âˆ€Î³âˆˆ(0,1),âˆ’C(Î³+ 1) + (1âˆ’Î³)>0â‡â‡’ âˆ’ 2C+ 1>0â‡â‡’C < 1/2.
â€¢Similarly, if Cis really large, Umay be monotonically increasing on (0,1):
âˆ€Î³âˆˆ(0,1),âˆ’C(Î³+ 1) + (1âˆ’Î³)<0â‡â‡’Câ‰¥1;
â€¢Finally, ifCâˆˆ(1/2,2), the minimum exists inside (0,1)and is reached for
âˆ’CÎ³âˆ’C+ 1âˆ’Î³= 0â‡â‡’Î³=Î³âˆ—=1âˆ’C
1 +C
21Published in Transactions on Machine Learning Research (06/2023)
F Experiments: Implementation Details, Ablations & Additional Results
F.1 Implementation details
We consider a Dirichlet distribution of tasks such that all tasks tâˆˆ[T],Ptâˆ¼Pare centered at some ï¬xed
meanPoâˆˆâˆ†SÃ—A
Sas shown in Figure F1. The mean of the task distribution Pois chosen as a sampled
random MDP and variance of this distribution is determined such that /bardblPt
s,aâˆ’Po
s,a/bardblâˆâ‰¤Ïƒ<1. Next, we
compute the variance of this distribution Ïƒi=ËœÎ±i(1âˆ’ËœÎ±i)
Î±0+1, where ËœÎ±i=Î±i
Î±0andÎ±0=/summationtextS
iÎ±i.
Î± = (15.000, 15.000, 15.000)
Figure F1: Dirichlet Task Distribution forS= 3states, with Dir(Î±) whereÎ±= [15,15,15], resulting in
our task-similarity measure approximately to be Ïƒ= 0.0129.
F.2 Ablations
We also run ablations with Aggregating( Î±= 1), a naive baseline that simply ignores the meta-RL structure
and just plans assuming there is a single task. We observe in Fig. F2 the aggregating baseline works at-par
with our method POMRLwhich is intuitive when the tasks are strongly related to each other in this case.
However, as the underlying task structure decreases, we note that Aggregating( Î±= 1) as though it is one
single task is problematic and suï¬€ers from a non-vanishing bias due to which for each new task there is on
average an error which does not go to zero. More importantly, the Aggregating( Î±= 1) baseline cannot have
the same guarantees as POMRLandada-POMRL .
Strong Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
Aggregating(Â®=1)
POMRL(Â¾)
(a)Medium Structure
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
Aggregating(Â®=1)
POMRL(Â¾)
(b)Loosely Structured
123456789101112131415
Task (T)0.00.51.01.52.02.53.03.54.0Planning LossÂ°=0:99=Â°eval 
ada-POMRL
Oracle Prior Knowledge
Without Meta-Learning
Aggregating(Â®=1)
POMRL(Â¾)
(c)
Figure F2: Ablations for Eï¬ƒcacy of POMRLand ada-POMRL for varying task-similarity. depicts the
eï¬€ect of the task-similarity parameter Ïƒfor a small ï¬xed amount of data m= 5available at each round. We
run another baseline called Aggregating (orange) that simply ignores the meta-RL structure and acts as if
it is all one single task. In the presence of strong structure, meta-learning the shared structure alongside
a good model initialization leads to most gains and even naively aggregating the tasks transitions might
seem to work well. However, such a naive method is not reliable as the underlying task similarity decreases -
the learner struggles to cope with new unseen tasks which diï¬€er signiï¬cantly and the planning loss doesnâ€™t
improve. Error bars represent 1-standard deviation of uncertainty across 100independent runs.
22Published in Transactions on Machine Learning Research (06/2023)
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.900.99
Î³0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=2 T=3 T=4 T=5
(a)m= 5,T= 5
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.900.99
Î³0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5 (b)m= 20,T= 5
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.900.99
Î³0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2
T=3T=4
T=5
T=12T=15
T=17
T=20T=25
T=27
T=30 (c)m= 5,T= 30
Figure F3: Eï¬€ect of m and T on Average Regret Upper Bound on Planning: for a ï¬xed value of
task similarity Ïƒ, depends on the number of samples per task mand the number of tasks T. (a) For m=T,
smaller loss is obtained with very small discount factor. This implies that with a lot of uncertainty it is not
interesting to plan far too ahead, (b) For m>>T, each task has enough samples to inform itself resulting in
slightly larger eï¬€ective discount factors. Not a lot is gained in this scenario from meta-learning, (c) m/lessmuchT
is the most interesting case as samples seen in each individual task are very limited due to small m. However,
the number of tasks are much more resulting in huge gains from leveraging shared structure across tasks.
F.3 Additional Experiments
We examine more properties of ada-POMRL , namelyEï¬€ect ofm, andTon Planning Loss in Fig. F3,
Individual Baselineâ€™s Performance in Fig. F4, and Varying State Space |S|,m, andTin Fig. F5.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15
(a)ada-POMRL
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (b) Oracle Prior Knowledge
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (c) Without Meta-Learning
Figure F4: Planning with Online Meta Learning - Baselines. (a)ada-POMRL . Meta updates include
learningPo,Ïƒ,Î±as a function of tasks. (b) Oracle Prior Knowledge considers the optimal Î±, true
mean of the task distribution Poand actual underlying task similarity Ïƒas known apriori, (c) Without
Meta-Learning estimates the transition kernel in each round Twithout any meta-learning. All baselines
are obtained with T= 15tasks andm= 5samples per task.
F.4 Reproducibility
We follow the reproducibility checklist by Pineau (2019) to ensure this research is reproducible. For all
algorithms presented, we include a clear description of the algorithm and source code is included with these
supplementary materials. For any theoretical claims, we include: a statement of the result, a clear explanation
of any assumptions, and complete proofs of any claims. For all ï¬gures that present empirical results, we
include: the empirical details of how the experiments were run, a clear deï¬nition of the speciï¬c measure or
statistics used to report results, and a description of results with the standard error in all cases.
F.5 Computing and Open source libraries.
All experiments were conducted using Google Colab instances7.
7https://colab.research.google.com/
23Published in Transactions on Machine Learning Research (06/2023)
|S|= 20
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5
(a)m=T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5 (b)m= 20,T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1 T=3 T=6 T=15 (c)m= 5,T= 30
|S|= 30
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5
(d)m=T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=2T=3 T=4 T=5 (e)m= 20,T= 5
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.91.0
Â°0.00.51.01.52.02.53.03.54.0Planning LossT=1
T=3T=6
T=16T=17
T=18T=19
T=20 (f)m= 5,T= 15
Figure F5: Varying the size of state-space S, number of samples per task m, and number of
tasksT, on Task-averaged Regret Upper Bound on Planning: for a ï¬xed value of task similarity Ïƒ,
We note that despite larger state-space we observe the same eï¬€ect i.e. (a,d,g) For m=T, smaller loss is
obtained with very small discount factor i.e. a lot of uncertainty and inability to plan far too ahead, (b,e,h)
Form>>T, each task has enough samples to inform itself resulting in slightly larger eï¬€ective discount
factors. Not a lot is gained in this scenario from meta-learning. (c,f,i) m/lessmuchTis the most interesting case as
samples seen in each individual task are very limited due to small m. Meta-learning has most signiï¬cant
gains in this case by leveraging the structure across tasks. Results are averaged over 20independent runs and
error bars represent 1-standard deviation.
24