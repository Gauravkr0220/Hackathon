Under review as submission to TMLR
Iterative Machine Teaching for Black-Box Markov Learners
Anonymous authors
Paper under double-blind review
Abstract
Machine teaching has traditionally been constrained by the assumption of a fixed learner
model, where the learnerâ€™s progress follows given rules, such as gradient update with fixed
learning rates and version space update with a given preference function. In this paper, we
consider a generic setting which views the learner as a black box, and the learnerâ€™s dynamics
can be learned during the teaching process. We model the learnerâ€™s dynamics as a Markov
decision process (MDP) with unknown parameters, encompassing a wide range of learner
types studied in the machine teaching literature. In such a setting, machine teaching reduces
to finding an optimal policy for the underlying MDP. We then introduce an algorithm for
teaching such black-box Markov learners, and provide an analysis of the teaching cost under
both discounted and non-discounted settings. The Markov learners considered in this work
can be naturally linked to epiphany learning as studied in decision psychology. Supported
by numerical study results, this paper delivers a novel perspective for machine teaching
under the black-box setting, introducing a robust, versatile learner model with a rigorous
theoretical foundation.
1 Introduction
Machine teaching seeks effective policies for selecting training examples to help a learner learn a target
concept. Over the past few decades, the field of machine teaching has been pushed forward and shown great
promise in various application domains, including those targeting human learners, such as automated tutoring
systems (Rafferty et al., 2016; Sen et al., 2018; Zhu et al., 2018; Hunziker et al., 2019), citizen science and
crowdsourcing services (Sullivan et al., 2009; Nugent, 2018), or those targeting machine learning systems,
such as model compression (Romero et al., 2014) and understanding the vulnerability of data poisoning
attacks (Mei and Zhu, 2015; Zhu, 2018).
Figure 1: An illustration of the teaching framework.
We focus on steps (b), (c) and (d), and assume the
feature mapping (i.e. learned through step (a)) is
known and given.As illustrated in figure 1, a machine teaching frame-
work assumes a computational model of the learnerâ€”
either known or unknown to the teacherâ€”which
typically consists of two components: (a) a model
for representing the learnerâ€™s state (e.g., learnerâ€™s
current hypotheses, as in figure 1 (a)), and (b) a
model for the learning dynamics (e.g. parameters
capturing learnerâ€™s initial knowledge, learning rate,
and learning behavior etc. as in figure 1 (b)). When
both models are known to the teacher, the teaching
problem boils down to an optimal planning problem
as in figure 1 (c). Upon receiving the teaching in-
structions, the learner makes an update according
to its own intrinsic dynamics, and proceeds to the
next knowledge state.
Classical theory of machine teaching often focuses
on specific realizations of such a framework. Depending on the learner type and how much information
of the learner the teacher can access, various teaching models have been proposed. We summarize a few
1Under review as submission to TMLR
representation
model known unknown
knownwhite-box â€œblack-box learnerâ€
(Goldman and Kearns, 1995; Zilles et al., 2011; Chen et al.,
2018; Mansouri et al., 2019; Lessard et al., 2019; Tabibian et al.,
2019; Hunziker et al., 2019)(Dasgupta et al., 2019;
Liu et al., 2018)
unknownblack-box MDP learnerâ€“(this work)
Table 1: A summary of different teaching settings and difference between our work and the existing literature
representative works in table 1. Under the â€œwhite-boxâ€ setting where the teacher has full access to the
learnerâ€™s dynamics and state representation, one may derive strong theoretical guarantees on the complexity
of teaching (Goldman and Kearns, 1995; Zilles et al., 2011; Chen et al., 2018; Mansouri et al., 2019; Lessard
et al., 2019; Tabibian et al., 2019; Hunziker et al., 2019). When the learnerâ€™s representation is unknown but
the learnerâ€™s dynamics (e.g. learning algorithm) are given, it has been shown that the teacher can efficiently
find a set of teaching examples with strong approximation guarantees in finding the optimal set (Dasgupta
et al., 2019) or convergence guarantees (Liu et al., 2018) in teaching the target concept. However, the practical
teaching scenario with unknown learner dynamics has been under-explored so far.
To capture the learnerâ€™s dynamics, we propose to model the learning/teaching problem via a Markov decision
process (MDP), where the learner transits among different hypotheses (states) upon receiving teaching
instructions (actions). The goal of teaching is to steer the learner towards the goal state (e.g. the concept
being taught) via the underlying MDP. As later discussed in section 2, we show that the learner models
in table 1 can be viewed as special cases of Markov learners . Note that the corresponding studies in the
literature are often focused on heuristic learner models or representations. In contrast, the Markov learner
entails a versatile framework generic to a broad class of learner models.
Furthermore, most of the existing learner models in algorithmic machine teaching, such as the preference-based
version space model (Mansouri et al., 2019; Gao et al., 2017) or the gradient-based model (Liu et al., 2017),
assume that the learner follows specific incremental hypothesis update rules which do not capture certain
drastic transitions between hypotheses. These learner models naturally align with the concept of non-epiphany
learners (Chen and Krajbich, 2017; Dufwenberg et al., 2010), a class of learner studied in decision psychology
and neuroscience that was shown not always suitable for modeling human behavior. The restrictions on the
hypothesis update rule hinder their applicability to solving practical problems, where the learner model is
often a complicated black box (e.g. inferred from historic student data (Corbett and Anderson, 1994; Yudelson
et al., 2013; Piech et al., 2015; Settles and Meeder, 2016; Sen et al., 2018; Hunziker et al., 2019)).
Our contributions. In this paper, we set forward a generic teaching framework capable of capturing
unknown complex learner dynamics in real-world teaching applications. For better understanding of the
overall teacher/learner process, we study machine teaching under a generic black-box setting, where the
learnerâ€™s dynamics are modeled by a Markov decision process (MDP) with unknown parameters. We show
that many different learner models can be interpreted as Markov learners, and teaching such learners amounts
to identifying the optimal policy for the underlying MDP. To provide a theoretical understanding, we derive
the teaching cost under the assumption that the learning dynamics can be approximated by a linear function
of the learnerâ€™s state and the teaching instruction. These results are further backed up by a numerical case
study demonstrating the effective of the proposed algorithm.
Our contributions are highlighted below.
â€¢We introduce a generic machine teaching model with parametric Markov learners, which can be used in
place of many existing learner models for characterizing learnerâ€™s transition dynamics. This model allows
us to estimate the learnerâ€™s dynamics from data, providing a versatile approach to machine teaching.
2Under review as submission to TMLR
â€¢As a side product of our model, we establish a natural connection between Markov learners and the
concepts of epiphany and non-epiphany learning in the behavioral science and educational research.
â€¢Under our teaching framework, we provide rigorous analyses on the teaching costs for various teaching
scenarios. Whenthelearningdynamicsislinear, weshowthattheteachingcostsgrowsatmostpolynomially
in the optimal teaching cost and feature dimension d; when the dynamics is non-linear, we show that
teaching is not always feasible, and provide teachability conditions such that the teaching cost becomes
linear (ignoring log factors) in the optimal cost.
â€¢Complementing our theoretical results, we conduct conducted a case study on a numerical example to
demonstrate the use case of our proposed algorithm, and provide guidelines for setting its hyperparameters.
2 Related Work
Markov learners in machine teaching. Various learner models studied in the machine teaching literature
can be viewed as Markov learners. Among these models, most are investigated under a white-box setting,
where both the states st(learnerâ€™s knowledge) and the transition probabilities PÎ¸(learning dynamics) are
observable and given to the teacher (Liu et al., 2017; Lessard et al., 2019; Tabibian et al., 2019; Hunziker et al.,
2019; Chen et al., 2018; Mansouri et al., 2019). Notably, some recent work consider the black-box setting,
where learnerâ€™s states stis unknown but the transition probabilities PÎ¸are given (Dasgupta et al., 2019; Liu
et al., 2018; Yudelson et al., 2013; Rafferty et al., 2016). Additionally, popular models in educational research,
such as Deep Knowledge Tracing (DKT) (Piech et al., 2015), capture the learnerâ€™s hypothesis temporal neural
networks, typically using Recurrent Neural Networks (RNNs). This setup can conceptually be aligned with
the Markov framework by interpreting the decision process as a Partially Observable Markov Decision Process
(POMDP), where the states are represented by RNNs. In this model, transition probabilities are derived
from empirical data, and the belief states correspond to the cumulative observation history. In section 3, we
provide a few more concrete examples, along with their instantiation of the states, actions and transition
dynamics under the MDP model. Nevertheless, these machine teaching models all assume the learnerâ€™s model
is known, and are designed in an ad-hoc way. In contrast, our work focuses on proposing a generic framework
that can capture these heuristic models and allow learning the learnerâ€™s model from data.
Reinforcement learning. Our work is also closely related to the reinforcement learning literature (Jaksch
et al., 2010; Jin et al., 2020; Zhou et al., 2021; Ouyang et al., 2019; Min et al., 2021). In particular, our
algorithm design is built upon the least-squares regression algorithm for estimating the parameter of the
dynamics function, and the extended value iteration (EVI) (Jaksch et al., 2010) for generating the teaching
policy. These two sub-algorithms are commonly used as backbones in algorithm design. In comparison, our
work focuses on the non-episodic setting and takes the initialization into consideration, which better fits in the
machine teaching problem. Specifically, the learners of interest to this work are always resource-constrained
(e.g. by the perceptual capacity of human learning), and such initialization plays a critical role in the final
teaching cost as detailed in section 5. Another related line of works is teaching with reinforcement learning
policy (Wu et al., 2018; Fan et al., 2018; Florensa et al., 2018; Omidshafiei et al., 2019). However, all of these
works focus on improving the training efficiency of neural networks, i.e., whitebox learners. Their major
contributions are developing better state representation and reward shaping functions based on different
heuristics, which can serve as a complement to our work, i.e., the step (a) in figure 1.
Epiphany learning. The concept of Epiphany Learning (EL) has been rigorously studied in behavioral
science (Chen and Krajbich, 2017; Dufwenberg et al., 2010). EL denotes a phenomenon where a learning
agent (for instance, humans) experiences an abrupt enlightenment or comprehension regarding a specific
subject matter. In the context of educational research, EL manifests when students achieve an insightful
moment of comprehension or forge a substantial link between concepts, resulting in profound understanding
of a topic or problem. Conversely, Non-Epiphany Learning implies scenarios in which such transformative
moments do not transpire. Such epiphany/ non-epiphany learners naturally fit into the Markovian framework
considered in this paper (see figure 2). We use the MDP learner model as a computational model to capture
these learners, and subsequently provide an in-depth analysis of the teaching performance.
3Under review as submission to TMLR
Type of the learner States Actions Transition function
Preference-based version space (Mansouri et al., 2019) htâˆˆ2HÃ—HxtâˆˆXht+1â†Ïƒ(ht,xt)
Gradient-based (Liu et al., 2017) htâˆˆRdxtâˆˆXht+1â†htâˆ’Î·Â·âˆ‡wâ„“(ht,xt)
Skill-based (Whitehill and Movellan, 2017) htâˆˆ[0,1]dxtâˆˆXht+1âˆhtâŠ™q(xt)Î±âŠ™(1âˆ’q(xt))(1âˆ’Î±)
Memory-based (Settles and Meeder, 2016; Hunziker et al., 2019) htâˆˆR2xtâˆˆ{0,1}ht+1â†htÂ·HL(xt)
Table 2: Examples of existing sequential learner models that have the Markov property. Detailed discussion
over these models is provided in section 3.1.
3 Problem Formulation
We deal with the black-box setting, where the teacher initially has no knowledge of the learning dynamics (e.g.,
the parameters) of the learner, i.e., how the learner updates its knowledge state upon receiving the teacher
instruction. We assume that the teacher can observe the learnerâ€™s state directly, and also knows the cost
function1. The goal of the teacher is to help the learner reach some target knowledge state with minimal cost.
To assist the learner, the teacher will not only provide informative teaching instructions to the learner but
also needs to learn about the learnerâ€™s dynamics.
Notations. Before we proceed, we first introduce some notation. We use Hto represent the set of all possible
knowledge states of learners, h0denotes the initial knowledge state, and htis the learnerâ€™s knowledge state
at iteration t. At each iteration t, the teacher can choose one teaching instruction xtfrom the teaching set X.
The learnerâ€™s knowledge state will be updated upon receiving the teaching instruction. The teacherâ€™s goal is
to help the learner transit to the target knowledge state hâ‹†with minimal cost. Throughout the entire paper,
we useCâ‹†to denote the tightest upper bound on the expected teaching cost of the optimal teaching policy
by starting from any initial state.
3.1 Parametric Markov Learners
We model the learner as a Markov learner, which is able to cover a broad class of learners considered in the
literature (Gao et al., 2017; Whitehill and Movellan, 2017; Liu et al., 2017; Hunziker et al., 2019; Mansouri
et al., 2019). Specifically, for any given learner, it starts from some initial knowledge state h0, which represents
its current knowledge state. For each iteration t, when the learner receives the teaching instruction xt, it
updates its knowledge based on its transition probability,
ht+1âˆ¼PÎ¸â„“[ht+1|ht,xt], (1)
whereÎ¸â„“refers to the parameters that define the transition probability or learning dynamics of the learner.
Different learners may have different Î¸â„“. The transition probability induces a preference over the next
knowledge states for the learner, which captures the learning dynamics of the learner. Intuitively, for fast
learners, it will assign a higher transition probability to states that are close to the target state hâ‹†upon
receiving the teaching instructions. In contrast, sometimes, a learner may not be able to understand advanced
teaching instructions before it reaches certain knowledge state. To model such scenarios, the learner may
assign a very high probability to remain at the current knowledge state when receiving obscure teaching
instructions (i.e., no learning progress after receiving the teaching instruction).
In the following, we will illustrate through a set of examples how the parametric Markov model described in
equation 1 captures the learnerâ€™s dynamics characterized by several existing sequential learner models, as
summarized in table 2.
Example 1 (Preference-based model for version space learner) For preference-based learners (Chen
et al., 2018; Mansouri et al., 2019), a state ht:= (Ht,ht)is represented as a combination of the learnerâ€™s
current version space, denoted by Ht, and their current hypothesis, denoted by ht. This representation captures
both the set of all hypotheses that are consistent with the observed data (the version space) and the learnerâ€™s
1In practice, the teacher can probe the learnerâ€™s knowledge state by quizzing the learner. The cost could be the price of the
teaching instruction.
4Under review as submission to TMLR
specific hypothesis at any given time. An action xtcorresponds to the provision of a teaching example, which
influences the learnerâ€™s hypothesis. The transition from one state to the next is governed by the preference
functionÏƒ, determining how the learner navigates through Htin response to teaching actions:
ht+1â†Ïƒ(ht,xt)
Mansouri et al. (2019) show that by instantiating the preference-based learner with different preference
functionsÏƒ, it reduces to several classic learner models in algorithmic machine teaching: When Ïƒ(ht,xt) =c
for some constant c, it corresponds to the classic â€œworst-caseâ€ version space model (Goldman and Kearns,
1995); when Ïƒ(ht,xt) =g(Â·)for some function gthat does not depend on the learnerâ€™s current state (i.e.
current hypothesis htand the version space Ht, it corresponds to the (global) preference-based learnerâ€™s
model (Zilles et al., 2011; Gao et al., 2017).
Example 2 (Gradient-based learner)
ht+1â†htâˆ’Î·Â·âˆ‡wâ„“(ht,xt)
whereÎ·denotes the learning rate, htâˆˆRddenotes the learnerâ€™s state at t, andâ„“denotes the loss function.
Liu et al. (2017; 2018) study both the white-box setting and the black-box setting. For the black-box setting,
they assume that the learnerâ€™s state htis unknown but the transition function, defined through the learning
rateÎ·, is known. When Î·is unknown, the teacher needs to spend an extra budget to estimate it, which does
not affect the teaching complexity overall.
Example 3 (Skill-based learner) Skill-based learners (Bower, 1961; Corbett and Anderson, 1994; White-
hill and Movellan, 2017) conceptualize learning as the acquisition of discrete, independent skills. In the
simplest form of such models, the state space corresponds to dindependent skills; each skill is binary, indicating
whether it is â€œlearnedâ€ (1) or â€œnot learnedâ€ (0). At time step t, when an exercise xtis presented, the skill
associated with xtcan jump from 0 to 1 state with some probability. This probabilistic transition is captured
by
ht+1âˆhtâŠ™q(xt)Î±âŠ™(1âˆ’q(xt))(1âˆ’Î±)
Here,q(xt)represents the probability of learning the skill associated with exercise xt, andÎ±is a binary
variable indicating the presence (1) or absence (0) of the skill prior to xt.
Skill-based learners represent a fundamental learnerâ€™s model in intelligent tutoring systems (ITS), which is
integral to the knowledge tracing frameworks (Corbett and Anderson, 1994). More advanced models such as
Deep Knowledge Tracing (Piech et al., 2015) extend beyond binary skill states, employing continuous and
correlated variables to capture more intricate representation of learnersâ€™ skill sets, thereby enhancing the
modelâ€™s capacity to navigate and support the complex landscape of learning trajectories.
Example 4 (Memory-based learner) Classical computational models of human memory, such as the
Half-Life Regression (HLR) model (Settles and Meeder, 2016), have been used in machine teaching to model
the long term learning behavior of a human subject. The HLR model posits an exponential decay of memory
over time, where the probability of correctly recalling an item is influenced by the time elapsed since last
reviewed, and the memory strength, quantified as the half-life ( HLÎ¸(Â·)). A statehtcorresponds to a retention
level and a forgetting rate, and a transition is specified by the half-life dynamics HL:
ht+1â†htÂ·HLÎ¸(xt).
A concrete HLR model studied by Settles and Meeder (2016) calculates the half-life based on a learnerâ€™s
interactions with the teaching example, using the feature representation xtand a parameter vector Î¸, yielding
the estimated half-life as HLÎ¸(xt) = 2Î¸Â·xt. This model extends beyond traditional methods like Leitner
(Leitner and Totter, 1972) and Pimsleur (Pimsleur, 1967) systems by empirically fitting Î¸to actual learning
data, accommodating a wider array of features to more accurately mirror a learnerâ€™s memory dynamics.
5Under review as submission to TMLR
3.2 The Teacherâ€™s Objective
The teacherâ€™s goal is to help the learner learn as fast as possible, i.e., minimizing the cost of steering the
learner to reach the target knowledge state hâ‹†. In order to teach, there are two tasks that the teacher needs
to solve, namely estimating Î¸â„“and generating the teaching instruction. The entire problem can be formulated
as follows, where c(Â·,Â·)is the cost function.
min
x1:TâˆˆXT,TâˆˆZ+E/bracketleftï£¬iggT/summationdisplay
t=1c(ht,xt)/bracketrightï£¬igg
s.t.hT+1=hâ‹†andht+1âˆ¼PÎ¸â„“(h|ht,xt). (2)
If the teacher knows the true parameters, then the above problem becomes a (stochastic) planning problem.
In this work, we assume that the teacher only knows the parametric form of the learnerâ€™s transition function,
and it doesnâ€™t know the true parameters of the learner. This makes our problem formulation more general,
but also introduces an extra challenge in solving the above problem.
4 Preliminaries and Background
Teaching Markov learners can be captured by an MDP M:={H,X,P,c,h0,hâ‹†,Î³}, wherec:HÃ—Xâ†’ R+
is the cost function and hâ‹†is the target knowledge state. For any (h,x,hâ€²)âˆˆHÃ—XÃ—H ,PÎ¸â„“(hâ€²|h,x)
denotes the probability of transiting to knowledge state hâ€²given the teaching instruction xunderh. To
be noted, when the learner reaches the target knowledge state, the cost will be zero for all the teaching
instructions, i.e., c(hâ‹†,x) = 0,âˆ€xâˆˆX, andP(hâ‹†|hâ‹†,x) = 1, which means the target knowledge state is an
absorbing state. Î³âˆˆ(0,1]is the cost discounting factor. In the teaching context, 1âˆ’Î³is the probability of
the learner transiting to the target knowledge state from any other state, i.e., the probability of epiphany
learning (Dufwenberg et al., 2010; Chen and Krajbich, 2017).
Definition 1 (Proper Policy) A stationary policy Ï€is proper if, given any initial state, the probability of
reaching the goal state gwithin a finite number of steps, when following Ï€, is strictly positive.
In the remaining of this section, we introduce the key assumptions that the subsequent sections rely on. Let
us denote by Î proper (M)the set of stationary polices of the underlying MDP Msuch that for any policy
Ï€âˆˆÎ proper (M), the expected time that it takes to reach the target knowledge state hâ‹†from any initial
knowledge state his finite. In the teaching context, the existence of proper polices for a learner means that
there is a way to teach him/her the target knowledge state hâ‹†. In our analysis, we will assume that the
Markov learner is linear and teachable under some known and given feature mapping Ï•:HÃ—XÃ—Hâ†’ Rd.
We summarize the essential idea in the following assumption. Similar assumption has also been studied in
Zhou et al. (2021); Min et al. (2021).
Assumption 1 (Teachable Linear Markov Learners) M:={H,X,PÎ¸â„“,c,h0,hâ‹†,Î³}is a teachable lin-
ear Markov learner, if it satisfies
â€¢Linearity : Given a known feature mapping Ï•, there exists an unknown parameter Î¸â„“âˆˆRd(âˆ¥Î¸â„“âˆ¥2
2â‰¤d)
such that PÎ¸â„“(hâ€²|h,x) =âŸ¨Ï•(hâ€²|h,x),Î¸â„“âŸ©,âˆ€(h,x,hâ€²)âˆˆHÃ—XÃ—H .
â€¢Teachable : There exists at least one proper policy, i.e., Î proper (M)Ì¸=âˆ….
Furthermore, for any bounded value function V:Hâ†’ [0,C]withCâ‹†â‰¤C,âˆ¥Ï•V(h,x)âˆ¥2â‰¤âˆš
dCholds for any
(h,x)âˆˆHÃ—X , whereÏ•V(h,x) =/summationtext
hâ€²Ï•(hâ€²|h,x)V(hâ€²).
For any value function V:Hâ†’R+, we define PV(h,x) =/summationtext
hâ€²P(hâ€²|h,x)V(hâ€²)for any (h,x)âˆˆHÃ—X .
Under the linear MDP assumption, we further have PÎ¸â„“V(h,x) =âŸ¨Ï•V(h,x),Î¸â„“âŸ©. For convenience of notation,
we further define the cost-to-go function for policy Ï€underMÎ¸â„“as
VÏ€(h|Î¸â„“):= lim
Tâ†’+âˆE/bracketleftï£¬iggT/summationdisplay
t=0c(ht,xt)/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleh0=h/bracketrightï£¬igg
,whereht+1âˆ¼PÎ¸â„“(h|ht,xt)andxt=Ï€(ht).
Consequently, the Q-value function of policy Ï€underMÎ¸â„“can be written as
QÏ€(h,x|Î¸â„“):=c(h,x) +PÎ¸â„“VÏ€(h,x|Î¸â„“). (3)
6Under review as submission to TMLR
Subsequently, when there is no ambiguity, we use V(h)andQ(h,x)to simplify notation. Next, we introduce
another assumption tailored to the teaching setting.
Assumption 2 ( Î´0-Closeness) The true parameter Î¸â„“isÎ´0-close to the teacherâ€™s initial estimation Î¸0, i.e.,
âˆ¥Î¸â„“âˆ’Î¸0âˆ¥2â‰¤Î´0âˆš
dwith 0â‰¤Î´0â‰¤1.
The above assumption is natural in the teaching setting. Without such an assumption, the teacher may need
to interact with the learner for a large number of rounds before it can teach in an effective way, which is
impractical for teaching resource-constrained learners, such as humans. In practice, to fulfil Assumption 2, we
can first fit a transition function on the offline teacher-learner interaction data to serve as the initialization.
For simplicity, we denote the associated MDP of a learner with parameter Î¸â„“asMÎ¸â„“and the teacherâ€™s initial
estimation on the parameter as Î¸0.
Lastly, we define two categories of Markov learners depending on their behaviors during learning, which
can be modelled by undiscounted MDP and discounted MDP, respectively. We call a Markov learners an
epiphany learner ifÎ³ <1for its associated MDP. When the learnerâ€™s associated MDP has Î³= 1, we call it a
non-epiphany learner .
<latexit sha1_base64="6wlj5dXMUDK7T8paHZe90b96dxo=">AAAB/nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EEsugjWUE8wHJEfY2e8mS3b1jd08Ix4G/wVZrO7H1r1j6T9wkV5jEBwOP92aYmRfEnGnjut9OYWNza3unuFva2z84PCofn7R1lChCWyTikeoGWFPOJG0ZZjjtxopiEXDaCSZ3M7/zRJVmkXw005j6Ao8kCxnBxkrdfiDScTZgg3LFrbpzoHXi5aQCOZqD8k9/GJFEUGkIx1r3PDc2foqVYYTTrNRPNI0xmeAR7VkqsaDaT+f3ZujCKkMURsqWNGiu/p1IsdB6KgLbKbAZ61VvJv7n9RIT3vgpk3FiqCSLRWHCkYnQ7Hk0ZIoSw6eWYKKYvRWRMVaYGBvR0pZAZDYTbzWBddK+qnq1au3hutK4zdMpwhmcwyV4UIcG3EMTWkCAwwu8wpvz7Lw7H87norXg5DOnsATn6xccM5a+</latexit>hi
<latexit sha1_base64="Ze7v9ouRvvpuLVJRIyBSO9tc17k=">AAAB/nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EomXQxjKC+YDkCHubvWTN7t6xuyeE48DfYKu1ndj6Vyz9J26SK0zig4HHezPMzAtizrRx3W+nsLa+sblV3C7t7O7tH5QPj1o6ShShTRLxSHUCrClnkjYNM5x2YkWxCDhtB+Pbqd9+okqzSD6YSUx9gYeShYxgY6VOLxDpKOs/9ssVt+rOgFaJl5MK5Gj0yz+9QUQSQaUhHGvd9dzY+ClWhhFOs1Iv0TTGZIyHtGupxIJqP53dm6EzqwxQGClb0qCZ+ncixULriQhsp8BmpJe9qfif101MeO2nTMaJoZLMF4UJRyZC0+fRgClKDJ9Ygoli9lZERlhhYmxEC1sCkdlMvOUEVknrourVqrX7y0r9Jk+nCCdwCufgwRXU4Q4a0AQCHF7gFd6cZ+fd+XA+560FJ585hgU4X78dxpa/</latexit>hj<latexit sha1_base64="FqZVKibIX8naCbLoUgKPqtVHL9E=">AAAB/nicbVA9SwNBEJ2LXzF+RS1tFoNgFe5EEsugjWUE8wHJEfY2e8mS3b1jd08Ix4G/wVZrO7H1r1j6T9wkV5jEBwOP92aYmRfEnGnjut9OYWNza3unuFva2z84PCofn7R1lChCWyTikeoGWFPOJG0ZZjjtxopiEXDaCSZ3M7/zRJVmkXw005j6Ao8kCxnBxkrdfiDScTaYDMoVt+rOgdaJl5MK5GgOyj/9YUQSQaUhHGvd89zY+ClWhhFOs1I/0TTGZIJHtGepxIJqP53fm6ELqwxRGClb0qC5+ncixULrqQhsp8BmrFe9mfif10tMeOOnTMaJoZIsFoUJRyZCs+fRkClKDJ9agoli9lZExlhhYmxES1sCkdlMvNUE1kn7qurVqrWH60rjNk+nCGdwDpfgQR0acA9NaAEBDi/wCm/Os/PufDifi9aCk8+cwhKcr18fWZbA</latexit>hk<latexit sha1_base64="yuG35BpwLVA339JSFHkURpHdu9c=">AAACInicbVDLSgMxFM34rPU16tJNsAi1SJkRqS6LblxWsA/ojEOSpm1oMjMkGbEM8w9+hN/gVtfuxJXgxj8xfSxs64HAuefcy809OOZMacf5spaWV1bX1nMb+c2t7Z1de2+/oaJEElonEY9kCyNFOQtpXTPNaSuWFAnMaRMPrkd+84FKxaLwTg9j6gvUC1mXEaSNFNglTyDdxzitZUUPi7Sf3ZegJ1gHTqqAnY7ZY3YS2AWn7IwBF4k7JQUwRS2wf7xORBJBQ004UqrtOrH2UyQ1I5xmeS9RNEZkgHq0bWiIBFV+Or4pg8dG6cBuJM0LNRyrfydSJJQaCmw6RxeoeW8k/ue1E9299FMWxommIZks6iYc6giOAoIdJinRfGgIIpKZv0LSRxIRbWKc2YJFZjJx5xNYJI2zslspV27PC9WraTo5cAiOQBG44AJUwQ2ogTog4Am8gFfwZj1b79aH9TlpXbKmMwdgBtb3L/UNpHY=</latexit>P(hâ‡¤|hi,x)<latexit sha1_base64="3kTEwMXO16RivEYgptDhMf54Qs0=">AAAB/nicbVA9SwNBEJ3zM8avqKXNYhDEItyJRMugjWUE8wHJGfY2e8mS3b1jd08Ix4G/wVZrO7H1r1j6T9wkV5jEBwOP92aYmRfEnGnjut/Oyura+sZmYau4vbO7t186OGzqKFGENkjEI9UOsKacSdowzHDajhXFIuC0FYxuJ37riSrNIvlgxjH1BR5IFjKCjZXa3UCkw+zxvFcquxV3CrRMvJyUIUe9V/rp9iOSCCoN4VjrjufGxk+xMoxwmhW7iaYxJiM8oB1LJRZU++n03gydWqWPwkjZkgZN1b8TKRZaj0VgOwU2Q73oTcT/vE5iwms/ZTJODJVktihMODIRmjyP+kxRYvjYEkwUs7ciMsQKE2MjmtsSiMxm4i0msEyaFxWvWqneX5ZrN3k6BTiGEzgDD66gBndQhwYQ4PACr/DmPDvvzofzOWtdcfKZI5iD8/ULt2OWfg==</latexit>hâ‡¤
<latexit sha1_base64="+QIjkNpZ7ppNxpr8SDHvyT8lHc8=">AAACA3icbVDLSsNAFJ3UV62vqks3g0VwVRKR6rLoxmUF+4AmlMn0ph06M4kzE6GELP0Gt7p2J279EJf+iUmbhW09cOFwzr2cy/EjzrSx7W+rtLa+sblV3q7s7O7tH1QPjzo6jBWFNg15qHo+0cCZhLZhhkMvUkCEz6HrT25zv/sESrNQPphpBJ4gI8kCRonJJM/1RTJOB4kLnKeDas2u2zPgVeIUpIYKtAbVH3cY0liANJQTrfuOHRkvIcowyiGtuLGGiNAJGUE/o5II0F4yezrFZ5kyxEGospEGz9S/FwkRWk+Fn20KYsZ62cvF/7x+bIJrL2Eyig1IOg8KYo5NiPMG8JApoIZPM0KoYtmvmI6JItRkPS2k+CLvxFluYJV0LupOo964v6w1b4p2yugEnaJz5KAr1ER3qIXaiKJH9IJe0Zv1bL1bH9bnfLVkFTfHaAHW1y9PXJkY</latexit>h`
Figure 2: Modeling epiphany learning
as a discounted MDP. The â€˜epiphanyâ€™ or
sudden leap is illustrated by the dashed
arrows with a light bulb. The solid ar-
rows represent normal transitions between
states. The probability of epiphany at
statehicanbeinterpretedastheprobabil-
ityofreachingthegoalstate P(hâˆ—|hi,x).Epiphany learning (Dufwenberg et al., 2010; Chen and Krajbich,
2017) is an observed phenomenon in human learners, which refers
to sudden insights or realizations of human learners that lead to a
rapid increase in understanding or problem-solving ability. Such
learners may not show gradual improvement but instead have
significant leaps in learning after periods of stagnation or slow
progress. In the context of machine teaching (see figure 2), the
â€˜epiphanyâ€™ or sudden leap in understanding can be viewed as a
significant reward. More specifically, such epiphany is specifically
modeled as direct transitions to the goal state , highlighting its
significant impact on the learning process. The discount factor 1âˆ’
Î³could model the decreasing likelihood or value of such epiphanies
over time, or the increasing value of immediate, incremental
learning compared to waiting for less predictable, more significant
breakthroughs. In other words, 1âˆ’Î³can be intuitively understood
as the lower bound of the probability of epiphany learning (i.e.,
transit to the goal knowledge state) at all the knowledge states.
The skill-based learners in table 2 can naturally be considered as
epiphany learners, while the others (e.g., preference-based learners,
gradient learners and memory-based learners) are more suitable to be modeled as non-epiphany learners.
5 Teaching Black-box Markov Learners: Algorithm and Analysis
In this section, we present an algorithm for teaching black-box Markov learners (including epiphany learners
and non-epiphany learners), which takes the initialization into account. We then conduct a rigorous analysis
for upper bounding the teaching cost under different teaching scenarios, including 1) the Markov learner is
linear and teachable; 2) the Markov learner is nonlinear and teachable.
5.1 Black-box Teaching for Linear Markov Learners: Algorithm
We first consider the case where the Markov learner is linear and teachable (see Assumption 1). We first
present an algorithm for solving the teaching problem, which takes the initialization into consideration. The
entire algorithm is built upon solving a regularized least-squares regression (for computing Ë†Î¸), and extended
value iteration (for generating the teaching policy). These two sub-algorithms are often used as backbones
designing RL algorithms (Jaksch et al., 2010; Jin et al., 2020; Zhou et al., 2021; Ouyang et al., 2019; Min
et al., 2021). In contrast to these works, our algorithm 1) takes the initialization Î¸0into account, which
7Under review as submission to TMLR
Algorithm 1 Black-box Teaching Algorithm for Non-Epiphany and Epiphany Learners.
Require: Initial estimation Ë†Î¸0=Î¸0, iterationt= 0, EVI index t0= 0,k= 0,Î£0=Î»I,Âµ0=Î»Î¸0and
discount factor Î³(for epiphany learners).
1:Q0â†EVI ({Î¸âˆˆRd/vextendsingle/vextendsingleâˆ¥Î¸âˆ’Ë†Î¸0âˆ¥2â‰¤Î´0},1
Î»,1
Î»)
2:whilehtÌ¸=hâ‹†do
3:Providext= arg minxâˆˆXQk(ht,x)
4:Receivect=c(ht,xt);ht+1âˆ¼PÎ¸â„“(Â·|ht,xt).
5:Î£tâ†Î£tâˆ’1+Ï•Vk(ht,xt)Ï•Vk(ht,xt)âŠ¤
6:Âµtâ†Âµtâˆ’1+Ï•Vk(ht,xt)Vk(ht+1)
7:ifdet(Î£t)â‰¥2 det( Î£tk)ortâ‰¥2tk+Î»then
8:kâ†k+ 1
9:tkâ†t
10:Qk=EVI (Ct,1
Î»t,1âˆ’1
Î»t)
11:Qk=EVI (Ct,1
Î»t,Î³)
12:end if
13:tâ†t+ 1
14:end while
is crucial to teaching effectively; 2) and applies to both epiphany and non-epiphany learners. Intuitively,
Algorithm 1 can be divided into two parts as described below.
Parameter learning. For parameter learning, once the updating criteria is satisfied, the teacher will update
its estimation of the learnerâ€™s parameter based on the interactions so far. Updating the estimation reduces to
solving the following initialization-regularized least-squares problem:
Ë†Î¸mâ†arg min
Î¸âˆˆRdmâˆ’1/summationdisplay
t=0[âŸ¨Ï•Vk(t)(ht,xt),Î¸âŸ©âˆ’Vk(t)(ht+1)]2+Î»âˆ¥Î¸âˆ’Î¸0âˆ¥2
2, (4)
wherek(t)is the index of the value function at iteration t, e.g., fortjâˆ’1â‰¤tâ‰¤tjâˆ’1, the index is k(t) =jâˆ’1,
andVj(h)is thejthvalue function returned by the extended value iteration (EVI) algorithm (Jaksch et al.,
2010). The above problem has a closed-form solution Ë†Î¸m=Î£âˆ’1
mÂµm, where (see also Lines 4&5 in Algorithm 1),
Î£m=Î»I+mâˆ’1/summationdisplay
t=0Ï•Vk(t)(ht,xt)Ï•Vk(t)(ht,xt)âŠ¤,Âµm=Î»Î¸0+mâˆ’1/summationdisplay
t=0Ï•Vk(t)(ht,xt)Vk(t)(ht+1).
The value of Î»indicates our confidence on the optimality of the initialization Î¸0. When the initialization is
very likely close to the true parameter Î¸â„“, we should set a large Î», otherwise we should set a small Î». In
addition,Î»also affects the updating frequency of the parameter, which is triggered by two criteria, namely 1)
the log-determinant of Î£t; and 2) the number of iterations (see Line 7 in Algorithm 1). When Î»is larger, the
parameter will be updated less frequently, since we trust our current estimate more. To be noted, in our
analysis, we always assume Î»â‰¥12.
Teaching. During the teaching phase, the teacherâ€™s policy is induced by the Q-value function returned by
EVI (see Algorithm 2). After the teacherâ€™s teaching instruction, the teacher will receive a cost incurred by
the teaching instruction, and also observe the learnerâ€™s latest knowledge state,
xt= arg min
xâˆˆXQk(t)(ht,xt),wherect=c(ht,xt),ht+1âˆ¼PÎ¸â„“(h|ht,xt). (5)
In detail, the EVI algorithm takes the confidence set Ct(see Lemma 1 for tâ‰¥1), the error tolerance of the
value iteration Î¾and the cost discounting factor Î½as input. The confidence set Ctis an ellipsoid centered at
the current estimation Ë†Î¸t. With high probability, the true parameter Î¸â„“lies in the intersection of BandCt,
whereBis defined as
B:={Î¸âˆˆRd/vextendsingle/vextendsingleâŸ¨Ï•(Â·|h,x),Î¸âŸ©âˆˆâˆ†d,âˆ€(h,x)âˆˆHÃ—X}.
2This is because we found that Î»âˆ1/Î´2
0is a good choice in practice (see section 6), and Î´0â‰¤1by Assumption 2.
8Under review as submission to TMLR
Algorithm 2 Extended Value Iteration: EVI( C,Î¾,Î½)
Require: Confidence setC, error tolerance of valute iteration Î¾, iterationi= 0, cost discount factor Î½.
1:Q(0)(Â·,Â·) = 0
2:Q(Â·,Â·) = 0
3:V(0)(Â·) = 0
4:V(âˆ’1)(Â·) = +âˆ
5:ifCâˆ©BÌ¸ =âˆ…then
6:whileâˆ¥V(i)âˆ’V(iâˆ’1)âˆ¥âˆâ‰¥Ïµdo
7:Q(i+1)(Â·,Â·)â†c(Â·,Â·) +Î½Â·minÎ¸âˆˆCâˆ©BâŸ¨Î¸,Ï•V(i)(Â·,Â·)âŸ©
8:V(i+1)(Â·)â†minxâˆˆXQ(i+1)(Â·,x)
9:iâ†i+ 1
10:end while
11:Q(Â·,Â·)â†Q(i+1)(Â·,Â·).
12:end if
13:returnQ(Â·,Â·)
The error tolerance parameter is chosen to be Î¾= 1/(Î»t). Intuitively, the error tolerance will be smaller when
we 1) collect more data (i.e., tbecomes large); and 2) start from a better initialization (i.e., Î´0is smaller).
For the cost discount factor Î½, we set it to be 1âˆ’1/(Î»t), when the underlying MDP of the Markov learner
is undiscounted (i.e., non-epiphany learners ). By doing so, the cost discount factor Î½will become closer to
1as the teaching continues, which helps us avoid a teaching cost that is linear in T(i.e, the total number
of teaching instructions) and also ensures the convergence of EVI. When the learnerâ€™s underlying MDP is
discounted (i.e., epiphany learners ), we will set Î½=Î³to be a constant. Intuitively, the cost discount factor Î½
captures the probability of epiphany learning.
Overall, the EVI algorithm adapts the standard value iteration algorithm to incorporate the optimism-in-the-
face-of-uncertainty (OFU) principle (see Line 7 in Algorithm 2) proposed by Abbasi-Yadkori et al. (2011),
which has been demonstrated to be effective in online learning settings.
5.2 Theoretical Analysis for the Linear Case
In this section, we analyze the cost upper bounds of using Algorithm 1 for teaching both non-epiphany and
epiphany learners. The core of the algorithm is to build the confidence set that contains the true parameter
Î¸â„“, which balances exploration (parameter learning) and exploitation (teaching). In general, the smaller
the confidence set that we can construct, the lower the cost. In the following, we present Lemma 1, which
provides a confidence set containing Î¸â„“with high probability.
Lemma 1 Under Assumptions 1 and 2, for any tâ‰¥1, with probability at least 1âˆ’Î´, we have that the true
parameterÎ¸â„“lies in
Ct=/braceleftï£¬ig
Î¸âˆˆRd/vextendsingle/vextendsingle/vextendsingleâˆ¥Ë†Î¸tâˆ’Î¸âˆ¥Î£tâ‰¤C/radicalbig
dlog ((4(t2+t3C2/Î»))/Î´) +âˆš
Î»Î´0/bracerightï£¬ig
. (6)
The confidence set Ctis centered at the current estimation Ë†Î¸t. Its radius is computed based on the iteration
t, feature dimension d, regularization parameter Î», the upper bound of the optimal cost C, and the upper
bound on the distance between Î¸0andÎ¸â„“, i.e.,Î´0. As expected, the confidence set will become smaller as Î´0
decreases, indicating that a good initialization is desired. Now Theorem 1 provides an upper bound on the
cost of teaching non-epiphany learners using Algorithm 1.
Theorem 1 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(Câ‹†),Î»= 1/Î´2
0, and the cost function is bounded from below by cminfor all non-goal knowledge
states (H\{hâ‹†}) and teaching instruction ( X) pairs, then with probability at least 1âˆ’2Î´, the teaching cost
of Algorithm 1 for non-epiphany learners (i.e., Î³= 1) is upper bounded by
O/parenleftï£¬igg/parenleftï£¬igg
1 +d/radicalbigg
log/parenleftï£¬ig
1 +Câ‹†dÎ´0
Î´cmin/parenrightï£¬ig/parenrightï£¬igg
Â·log1.5/parenleftï£¬igCâ‹†d
cminÎ´/parenrightï£¬ig
Â·C2
â‹†d
cmin/parenrightï£¬igg
. (7)
9Under review as submission to TMLR
ğ’‰âˆ—ğ’‰ğŸğ’‰ğŸ1-ğ›œ/ğŸ	|ğ’™=	ğ’™ğŸğŸ.ğŸ	|	ğ’™=ğ’™ğŸğ›œ/ğŸ|	ğ’™=ğ’™ğŸğŸ	|	ğ’™=ğ’™ğŸğŸ-ğ›œ/ğŸ|ğ’™=	ğ’™ğŸ1|ğ’™=ğ’™ğŸ	ğ’ğ’“	ğ’™ğŸğ›œ/ğŸ	|ğ’™=	ğ’™ğŸ1|ğ’™=	ğ’™ğŸ	ğŸ|ğ’™=	ğ’™ğŸğ’‰âˆ—ğ’‰ğŸğ’‰ğŸ1.ğŸ|ğ’™=ğ’™ğŸğŸ-ğ›œ/ğŸ|ğ’™=ğ’™ğŸ0|ğ’™=ğ’™ğŸğ›œ/ğŸ|ğ’™=ğ’™ğŸ1|ğ’™=	ğ’™ğŸ1|ğ’™=ğ’™ğŸ	ğ’ğ’“	ğ’™ğŸ0|ğ’™=	ğ’™ğŸ1-ğ›œ/ğŸ	|ğ’™=	ğ’™ğŸ	ğ›œ/ğŸ	|ğ’™=	ğ’™ğŸMisspecification
Figure 3: An illustration of the failure case under the misspecified setting. The MDP consists of 2 teaching
actionsX={x1,x2}and 3 statesH={h0,h2,hâ‹†}, and the misspecification level is Ïµ. For the teaching
policy induced by the misspecified MDP (right), the learner can get stuck at the state h2with probability
Ïµ/2.
The cost upper bound in Theorem 1 has a polynomial dependency on the expected cost of the optimal policy,
Câ‹†. Itâ€™s worth noting that when Î´0â†’0, the purple term inside the parentheses of Equation 7 will vanish
leaving only the constant term 1. The constant term 1is due to the stochasticity in the transition of the
learnerâ€™s knowledge states, which is independent of the teaching algorithm used.
Next, we consider the case where the learner is an epiphany learner . Intuitively, epiphany learning can be
interpreted as adding a shortcut from the current knowledge state to the target knowledge state in the
underlying MDP, which is equivalent to the discounted MDP case. The following theorem provides an upper
bound on the cost of teaching epiphany learners.
Theorem 2 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(Câ‹†),Î»= 1/Î´2
0, then with probability at least 1âˆ’3Î´, the total cost incurred by running Algorithm 1 for
epiphany learners with Î³ <1, is upper bounded by
O/parenleftï£¬igg
Câ‹†Â·/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +C2â‹†Î´2
0logÎ´
logÎ³/parenrightbigg/parenrightï£¬igg
Â·/radicalï£¬igg
logÎ´
logÎ³log/parenleftbigg
Câ‹†logÎ´
Î´logÎ³/parenrightbigg/parenrightï£¬igg
. (8)
Compared with Theorem 1 for non-epiphany learners, the upper bound of the teaching cost for epiphany
learners is linear (ignoring the log factors) in the expected teaching cost of the optimal policy Câ‹†and the
feature dimension d. Moreover, the dependency on dwill vanish when Î´0â†’0as well. In addition, Theorem 2
does not require the cost function to be bounded from below.
5.3 Theoretical Analysis for the Non-linear Case
In the previous section, we presented the theoretical analysis for both non-epiphany and epiphany learners
when their learning dynamics are linear. One natural follow-up question is: what would happen if the learnerâ€™s
dynamics is non-linear, i.e., the linear model is misspecified? To study this problem, we consider the case
where teaching the learner can be approximately modelled as a linear MDP. This idea is captured in the
following assumption.
Definition 2 ( Ïµ-Approximate Teachable Markov Learners) For anyÏµâˆˆ(0,1], a MDP M=
(H,X,P,c,h0,hâ‹†,Î³)is anÏµ-approximate teachable MDP with a feature map Ï•, if there exists a unknown
teachable linear MDP MÎ¸â‹†such that for any (h,x)âˆˆHÃ—X , we haveâˆ¥P(Â·|h,x)âˆ’âŸ¨Ï•(Â·|h,x),Î¸â‹†âŸ©âˆ¥TVâ‰¤Ïµ,
,whereTVdenotes the total variation distance.
By definition, the learner is an Ïµ-approximate teachable Markov learner if the learning dynamics function
of the learner is close to a linear transition function under the given feature mapping Ï•. We measure the
closeness between the dynamics functions by the total variation distance.
In general, the algorithm designed for the linear case will fail when the transition function is non-linear.
Specifically, for non-epiphany learners, the teaching cost can be unbounded even for a small model misspecifi-
cation level Ïµ. To illustrate this, we present an informal example (see Figure 3), where the teaching policy
10Under review as submission to TMLR
induced by the closest linear MDP to the learnerâ€™s MDP will incur an infinite teaching cost. The intuition
behind such counterexamples is that the teaching policy induced by the misspecified MDP will get trapped
in a circle of the true MDP. Fortunately, for epiphany learners, the teaching cost of Algorithm 1 can still be
bounded well, and it is robust to small misspecification levels. The results are stated in the following theorem.
Theorem 3 ForÏµ-approximate teachable epiphany learners as defined in Definition 2, if âˆ¥Î¸0âˆ’Î¸â‹†âˆ¥2â‰¤Î´0,
the cost function is bounded from above by cmax, the confidence set Ctis constructed according to Lemma 1
withC=O(ÏµÎ³cmax/(1âˆ’Î³)2+Câ‹†), and ifÎ»= 1/Î´2
0, then with probability at least 1âˆ’3Î´, the teaching cost
incurred by running Algorithm 1 is upper bounded by
O/parenleftï£¬igg
CÂ·/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +C2Î´2
0logÎ´
logÎ³/parenrightbigg/parenrightï£¬igg
Â·/radicalï£¬igg
logÎ´
logÎ³log/parenleftbigg
ClogÎ´
Î´logÎ³/parenrightbigg
Â·ÏµlogÎ´
logÎ³C/parenrightï£¬igg
. (9)
In contrast to Theorem 2, the major difference is that there is one extra cost term in Theorem 3 due to the
intrinsic bias of the linear approximation. When Ïµis sufficiently small, those terms with coefficient Ïµcan be
ignored safely, which gives us the following proposition.
Proposition 1 Under the same assumptions as Theorem 3, if Ïµ=O/parenleftbig
Câ‹†(1âˆ’Î³)2/(Î³cmax)/parenrightbig
then with proba-
bility at least 1âˆ’3Î´, the total cost incurred by running Algorithm 1 is upper bounded by
O/parenleftï£¬igg
Câ‹†Â·/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +C2â‹†Î´2
0logÎ´
logÎ³/parenrightbigg/parenrightï£¬igg
Â·/radicalï£¬igg
logÎ´
logÎ³log/parenleftbigg
Câ‹†logÎ´
Î´logÎ³/parenrightbigg
+ÏµlogÎ´
logÎ³Câ‹†/parenrightï£¬igg
.
Hence, as indicated by Theorem 3 and Proposition 1, our algorithm can still attain good theoretical guarantees
when the misspecification level is low.
6 A Numerical Case Study
In this section, we provide a case study on a synthetic learner to illustrate the algorithm. We also evaluate
how the choice of Î»affects the empirical teaching cost, as Î»plays a critical role in our algorithm design.
6.1 Experimental Setup
Knowledge states and teaching instructions . We sample 100 weights {hi}100
i=1uniformly at random
from [âˆ’3,3]dto simulate different knowledge states, each of which corresponds to a linear regressor. We then
pick one of the weights to represent the target knowledge state, denoted as hâ‹†. To generate the teaching
instructions, we first sample 20 points {zi}20
i=1from a normal distribution N(0,I), and their corresponding
labels are generated by yi=âŸ¨hâ‹†,ziâŸ©+Î¶, whereÎ¶âˆ¼N(0,1)is the observation noise. By {xi}20
i=1we denote
the set of teaching instructions, where xi= (zi,yi).
Feature representation . We consider the feature representation for each triplet (h,x,hâ€²)to be
Ï•(hâ€²|h,x) =/bracketleftï£¬ig
1//parenleftï£¬ig
Z(1)
(h,x)Â·âˆ¥hâ€²âˆ’h+Î·âˆ‡hâ„“(h,x)âˆ¥2/parenrightï£¬ig
,1//parenleftï£¬ig
Z(2)
(h,x)Â·âˆ¥âˆ‡hâ„“(h,x)âˆ¥2/parenrightï£¬ig/bracketrightï£¬ig
(10)
whereÎ·is the learning rate, and Z(i)
(h,x)is the normalizing constant for the ithdimension of the feature
representation Ï•(Â·|h,x). The normalizing constants are used to ensure that/summationtext
hâ€²âˆˆHÏ•(hâ€²|h,x) = (1,1).
Therefore, all the feasible Î¸that forms a probabilistic distribution lies in a 1-dsimplex. Intuitively, the first
dimension indicates that the learner is more likely to transit to those knowledge states that align well with
the updated knowledge state, i.e., hâˆ’Î·âˆ‡hâ„“(h,x), whereas the second dimension implies that the learnerâ€™s
knowledge state transition will become more random if the teaching instruction is more difficult, which is
measured by the gradient norm âˆ¥âˆ‡hâ„“(h,x)âˆ¥2.
11Under review as submission to TMLR
Random Blackbox Teaching Optimal
Algorithm050100Cost (#Examples)
1 3 5 6 8 10
Î»60708090
Figure 4: Left: A comparison between random teaching policy, black-box teaching policy and the optimal
teaching policy in terms of the mean of the averaged teaching cost for 99 initial states; Right: Effect of
different values of Î»on the teaching cost (computed with the first 10 states to save computation time).
6.2 Empirical Results
Comparing with baselines. We first evaluate the empirical performance of Algorithm 1 under the above
experimental setup. Specifically, we set the learning rate Î·= 1, and compare it with the random teaching
policy and the optimal policy. We compute the mean of the averaged teaching cost of starting from each
non-goal knowledge state (99 states). The averaged teaching cost is computed with 50 random seeds. The
results are presented in figure 4 left. As expected, the black-box teaching algorithm outperforms the random
teaching policy but underperforms the optimal teaching policy.
How to set Î»?The initialization plays an important role in our algorithm design and theoretical analysis.
Our theoretical analysis has demonstrated the impact of the initialization on the teaching cost. However,
given the initialization, it is still unclear how to set the right regularization parameter Î». We conjecture that
the â€˜optimalâ€™ Î»should be around 1/Î´2
0, which is also adopted in our theoretical analysis. To verify this idea,
we study how the choice of Î»affects the teaching cost. Under the same setting as above, we vary the value of
Î»in{1.0,1.5,...,10.0}. To save computation time, we adopt the first 10states to serve as the initial state
and repeat the previous experiments. The results are reported in the right plot of figure 4. The red dashed
line corresponds to the line of x= 1/Î´2
0withÎ´2
0= 0.18. Based on the empirical results, we can observe that
the best choice of Î»is6, which is close to 1/Î´2
0. In addition, if we set Î»too large or too small, the teaching
cost will increase accordingly.
In summary, our experimental results highlight that modelling the learnerâ€™s learning dynamics is crucial to
achieve a low teaching cost. Furthermore, given the initialization, setting Î»= 1/Î´2
0is a reasonable choice for
obtaining good empirical performance.
7 Conclusion
In this paper, we investigate a generic framework for machine teaching, under which the learnerâ€™s dynamics can
be represented as an MDP with unknown, learnable parameters. To solve the teaching problem, we introduce
an algorithm that accommodates both epiphany and non-epiphany learners, thus bridging a significant gap
in the current literature. Moreover, we furnish a rigorous analysis of the teaching costs associated with
these two types of learners under disparate settings. Complementing our theoretical insights, we conduct
empirical research to demonstrate the efficiency of our proposed algorithm and provide a guideline for setting
hyperparameters. It is our aspiration that this work will stimulate future research in proposing more nuanced
assumptions about the structure of the learnerâ€™s MDP and more efficient algorithms for machine teaching.
References
Yasin Abbasi-Yadkori, DÃ¡vid PÃ¡l, and Csaba SzepesvÃ¡ri. Improved algorithms for linear stochastic bandits.
Advances in neural information processing systems , 24, 2011.
Gordon H Bower. Application of a model to paired-associate learning. Psychometrika , 26(3):255â€“280, 1961.
12Under review as submission to TMLR
Wei James Chen and Ian Krajbich. Computational modeling of epiphany learning. Proceedings of the National
Academy of Sciences , 114(18):4637â€“4642, 2017.
Yuxin Chen, Adish Singla, Oisin Mac Aodha, Pietro Perona, and Yisong Yue. Understanding the role of
adaptivity in machine teaching: The case of version space learners. Advances in Neural Information
Processing Systems 32 , 2018.
Albert T Corbett and John R Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge.
User modeling and user-adapted interaction , 1994.
BalÃ¡zs CsanÃ¡d CsÃ¡ji and LÃ¡szlÃ³ Monostori. Value function based reinforcement learning in changing markovian
environments. Journal of Machine Learning Research , 9(8), 2008.
Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu. Teaching a black-box learner. In International
Conference on Machine Learning , pages 1547â€“1555. PMLR, 2019.
Martin Dufwenberg, Ramya Sundaram, and David Butler. Epiphany in the game of 21. Journal of Economic
Behavior & Organization , 75(2):132â€“143, 2010.
Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, and Tie-Yan Liu. Learning to teach. In International Conference
on Learning Representations , 2018. URL https://openreview.net/forum?id=HJewuJWCZ .
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement
learning agents. In International conference on machine learning , pages 1515â€“1528. PMLR, 2018.
Ziyuan Gao, Christoph Ries, Hans Ulrich Simon, and Sandra Zilles. Preference-based teaching. Journal of
Machine Learning Research , 18:31:1â€“31:32, 2017.
Sally A Goldman and Michael J Kearns. On the complexity of teaching. Journal of Computer and System
Sciences, 50(1):20â€“31, 1995.
Anette Hunziker, Yuxin Chen, Oisin Mac Aodha, Manuel Gomez Rodriguez, Andreas Krause, Pietro Perona,
Yisong Yue, and Adish Singla. Teaching multiple concepts to a forgetful learner. In NeurIPS , 2019.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning.
Journal of Machine Learning Research , 11(51):1563â€“1600, 2010. URL http://jmlr.org/papers/v11/
jaksch10a.html .
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory , pages 2137â€“2143. PMLR, 2020.
S. Leitner and R. Totter. So lernt man lernen . Angewandte Lernpsychologie ein Weg zum Erfolg. Herder,
1972. ISBN 9783451162657.
Laurent Lessard, Xuezhou Zhang, and Xiaojin Zhu. An optimal control approach to sequential machine
teaching. In The 22nd International Conference on Artificial Intelligence and Statistics , pages 2495â€“2503.
PMLR, 2019.
Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B Smith, James M Rehg, and
Le Song. Iterative machine teaching. In International Conference on Machine Learning , pages 2149â€“2158.
PMLR, 2017.
Weiyang Liu, Bo Dai, Xingguo Li, Zhen Liu, James Rehg, and Le Song. Towards black-box iterative machine
teaching. In International Conference on Machine Learning , pages 3141â€“3149. PMLR, 2018.
Farnam Mansouri, Yuxin Chen, Ara Vartanian, Xiaojin Zhu, and Adish Singla. Preference-based batch and
sequential teaching: Towards a unified view of models. Advances in Neural Information Processing Systems
32, 2019.
Shike Mei and Xiaojin Zhu. Using machine teaching to identify optimal training-set attacks on machine
learners. In AAAI, pages 2871â€“2877, 2015.
13Under review as submission to TMLR
Yifei Min, Jiafan He, Tianhao Wang, and Quanquan Gu. Learning stochastic shortest path with linear
function approximation. arXiv preprint arXiv:2110.12727 , 2021.
Jill Nugent. iNaturalist: citizen science for 21st-century naturalists. Science Scope , 41(7):12, 2018.
Shayegan Omidshafiei, Dong-Ki Kim, Miao Liu, Gerald Tesauro, Matthew Riemer, Christopher Amato,
Murray Campbell, and Jonathan P How. Learning to teach in cooperative multiagent reinforcement
learning. In Proceedings of the AAAI conference on artificial intelligence , volume 33, pages 6128â€“6136,
2019.
Yi Ouyang, Mukul Gagrani, and Rahul Jain. Posterior sampling-based reinforcement learning for control of
unknown linear systems. IEEE Transactions on Automatic Control , 65(8):3600â€“3607, 2019.
Chris Piech, Jonathan Bassen, Jonathan Huang, Surya Ganguli, Mehran Sahami, Leonidas J Guibas, and
Jascha Sohl-Dickstein. Deep knowledge tracing. In NIPS, 2015.
Paul Pimsleur. A memory schedule. The Modern Language Journal , 51(2):73â€“75, 1967.
Anna N Rafferty, Emma Brunskill, Thomas L Griffiths, and Patrick Shafto. Faster teaching via pomdp
planning. Cognitive science , 40(6):1290â€“1332, 2016.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 , 2014.
Ayon Sen, Purav Patel, Martina A. Rau, Blake Mason, Robert Nowak, Timothy T. Rogers, and Xiaojin Zhu.
Machine beats human at sequencing visuals for perceptual-fluency practice. In EDM, 2018.
Burr Settles and Brendan Meeder. A trainable spaced repetition model for language learning. In ACL, pages
1848â€“1858, 2016.
Brian L Sullivan, Christopher L Wood, Marshall J Iliff, Rick E Bonney, Daniel Fink, and Steve Kelling. eBird:
A citizen-based bird observation network in the biological sciences. Biological Conservation , 2009.
Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard SchÃ¶lkopf, and Manuel Gomez-
Rodriguez. Enhancing human learning via spaced repetition optimization. Proceedings of the National
Academy of Sciences , 116(10):3988â€“3993, 2019.
Jacob Whitehill and Javier Movellan. Approximately optimal teaching of approximately optimal learners.
IEEE Transactions on Learning Technologies , 11(2):152â€“164, 2017.
Lijun Wu, Fei Tian, Yingce Xia, Yang Fan, Tao Qin, Lai Jian-Huang, and Tie-Yan Liu. Learning to teach
with dynamic loss functions. Advances in Neural Information Processing Systems , 31, 2018.
Michael V Yudelson, Kenneth R Koedinger, and Geoffrey J Gordon. Individualized bayesian knowledge
tracing models. In International conference on artificial intelligence in education , pages 171â€“180. Springer,
2013.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for discounted mdps
with feature mapping. In International Conference on Machine Learning , pages 12793â€“12802. PMLR, 2021.
Xiaojin Zhu. An optimal control view of adversarial machine learning. CoRR, abs/1811.04422, 2018.
Xiaojin Zhu, Adish Singla, Sandra Zilles, and Anna N. Rafferty. An overview of machine teaching. CoRR,
abs/1801.05927, 2018.
Sandra Zilles, Steffen Lange, Robert Holte, and Martin Zinkevich. Models of cooperative teaching and
learning. JMLR, 12(Feb):349â€“384, 2011.
14Under review as submission to TMLR
A Appendix
In the appendix, we present the proofs of our theorems. The proofs of Lemma 1, Theorem 1, Theorem 2 and
Theorem 3 can be found in sections D, E, F and G, respectively. In section H, we provide a reference to the
existing lemmas that we rely on.
B Additional Discussions
Limitations. First of all, our algorithm requires the knowledge of the upper bound of Câ‹†andÎ´0for
setting the hyperparameters. To address this issue, we can use binary search to find a good choice of these
hyperparameters in practice. Secondly, it assumes a pregiven and fixedfeature mapping during the entire
teaching process, which might be suboptimal in reality and lead to biased teaching policies. Lastly, we assume
that the teaching algorithm can observe the learnerâ€™s state at every iteration, which might be expensive for
learners such as humans (i.e., one needs to quiz the learner at every iteration).
Future directions. As future directions, it would be exciting to incorporate representation learning into the
entire framework. Besides, instead of assuming the learnerâ€™s state to be observable, a partially observable
setting could be considered, under which probing the learnerâ€™s state will incur some cost. Therefore, the
algorithm should smartly determine when probing the learnerâ€™s state is necessary. In addition, it would
be interesting to propose more fine-grained and learner-specific structural assumptions on the underlying
MDPs. For example, for version space learners, the knowledge states will form a topological order; for
forgetful learners, there will be a non-zero probability to transit back to the previous knowledge states in
the topological graph. By exploiting the structural assumptions, we will design more efficient algorithms
and achieve stronger theoretical guarantees. Lastly, it would be interesting to incorporate ideas from meta
learning for learning the initialization Î¸0.
C Extended Backgrounds on Various Learnerâ€™s Models
Version-space learner. The version space learner was studied by Goldman and Kearns (1995) for machine
teaching. The hypothesis class of the version space learner is usually a finite set H, which contains a target
hypothesis hâ‹†âˆˆH. The teacher can pick a teaching example from the ground set Xto teach the learner.
Once an example (x,hâ‹†(x))) is provided to the learner, the learner will update its version space by removing
those hypotheses that are not consistent with the example, i.e., Hâ†H\{hâˆˆH|h(x)Ì¸=hâ‹†(x)}. Under this
teacher-learner interaction protocol, the teacher *knows* the aforementioned update rule of the learner. The
entire problem is essentially a set cover problem, which is NP-hard. But a greedy-approximation algorithm
admits a teaching complexity of O(log(H)Â·Câ‹†), whereCâ‹†is the optimal teaching cost. The version space
learner can also be regarded as a tabular case of the machine teaching problem, which falls in the Markov
learner case, i.e., a special case of our teaching framework.
Black-box version-space learner. The black-box version-space learner was studied in Dasgupta et al.
(2019). In this framework, they assume the teacher does not know the hypothesis class Hat the beginning,
but the teacher knows the learnerâ€™s dynamics rule (i.e., how does the learner update the knowledge state).
Then the teaching problem is equivalent to the *online set cover* problem. The analysis of the online set
cover applies to the analysis of the teaching cost. This work can be regarded as a complement to our work,
as they assume the learnerâ€™s model is known, but the state is unknown. Our work assumes the learnerâ€™s state
is observable, but the learnerâ€™s model is unknown.
Black-box iterative learner. The black-box iterative learner Liu et al. (2017) is in the same philosophy
as Dasgupta et al. (2019). The main difference is that, for the black-box iterative learner, it deals with
gradient-based learner, i.e., the learner updates it by following the gradient descent rule. Therefore, this work
still assumes the learnerâ€™s model is known.
Memory-based learner. The memory-based learner was studied in Settles and Meeder (2016); Hunziker
et al. (2019) for modeling the forgetting behavior of human learning. In these works, they used the half-life
15Under review as submission to TMLR
model as a proxy to model the human learnerâ€™s model. Specifically, in Hunziker et al. (2019) the teaching
problem was formulated as a submodular maximization problem (maximizing the memorization utility of the
underlying learner) due to the property of the half-life model.
Bayesian knowledge tracing (BKT) learner. As an instance of skill-based learners (Whitehill and
Movellan, 2017), BKT assumes that student knowledge is represented as a set of binary variables, one per
skill, where the skill is either mastered by the student or not. Observations in BKT are also binary: a
student gets a problem/step either right or wrong. The learnerâ€™s state is updated by Bayes rule given the
new observation. Hence, the teacher still knows the learnerâ€™s model.
D Proof of Lemma 1
Lemma 1 Under Assumptions 1 and 2, for any tâ‰¥1, with probability at least 1âˆ’Î´, we have that the true
parameterÎ¸â„“lies in
Ct=/braceleftï£¬ig
Î¸âˆˆRd/vextendsingle/vextendsingle/vextendsingleâˆ¥Ë†Î¸tâˆ’Î¸âˆ¥Î£tâ‰¤C/radicalbig
dlog ((4(t2+t3C2/Î»))/Î´) +âˆš
Î»Î´0/bracerightï£¬ig
. (6)
Proof:We prove this by induction on k, which is the index of the value functions returned by EVI. By
definition, the fitted value function in the interval [tk,tk+1âˆ’1]isVk(Â·). To be noted, since when t= 0, we
must haveÎ¸â„“âˆˆC0by Assumption 2. Therefore, we abuse the notation a little bit by reloading t0= 1for the
proof. Therefore, we first prove the base step, where tâˆˆ[1,t1âˆ’1]. For notation simplicity, we define
Ï•m=Ï•V0(hm,xm),Î¦t= (Ï•1,....,Ï•t),vt= (V0(h2),...,V 0(ht+1))âŠ¤.
Recall the definition of Ë†Î¸t, by rewriting it in the matrix form, we get
Ë†Î¸t=Î£âˆ’1
tbt=Î£âˆ’1
t/parenleftï£¬igg
Î»Î¸0+t/summationdisplay
m=1Ï•mV0(hm+1)/parenrightï£¬igg
=/parenleftbig
Î»I+Î¦tÎ¦âŠ¤
t/parenrightbigâˆ’1(Î»Î¸0+Î¦tvt)
=/parenleftbig
Î»I+Î¦tÎ¦âŠ¤
t/parenrightbigâˆ’1Î¦t(vtâˆ’Î¦âŠ¤
tÎ¸0) +Î¸0.
=Î£âˆ’1
tÎ¦t(vtâˆ’Î¦âŠ¤
tÎ¸0) +Î¸0.
Next, we define the following random variables
Î·m=V0(sm+1)âˆ’âŸ¨Ï•m,Î¸â„“âŸ©,Î·t= (Î·1,...,Î·t)âŠ¤.
SinceCâ‰¥Câ‹†, the sequence{Î·t}t1
t=1areC-sub-Gaussian. Now, we can rewrite Ë†Î¸tas
Ë†Î¸t=Î£âˆ’1
tÎ¦t/parenleftbig
Î·t+Î¦âŠ¤
t(Î¸â„“âˆ’Î¸0)/parenrightbig
+Î¸0
=Î£âˆ’1
tÎ¦tÎ·t+Î£âˆ’1
tÎ¦tÎ¦âŠ¤
t(Î¸â„“âˆ’Î¸0) +Î¸0.
By subtracting Î¸â„“on both sides, we get
Ë†Î¸tâˆ’Î¸â„“=Î£âˆ’1
tÎ¦tÎ·t+/parenleftbig
Î£âˆ’1
tÎ¦tÎ¦âŠ¤
tâˆ’I/parenrightbig
(Î¸â„“âˆ’Î¸0)
=Î£âˆ’1
tÎ¦tÎ·t+Î£âˆ’1
t/parenleftbig
Î¦tÎ¦âŠ¤
tâˆ’Î£t/parenrightbig
(Î¸â„“âˆ’Î¸0)
=Î£âˆ’1
tÎ¦tÎ·t+Î»Î£âˆ’1
t(Î¸0âˆ’Î¸â„“).
Then, we further obtain the following by the Cauchy-Schwarz inequality,
/vextenddouble/vextenddouble/vextenddoubleË†Î¸tâˆ’Î¸â„“/vextenddouble/vextenddouble/vextenddouble2
Î£t=/angbracketleftï£¬ig
Î£t(Ë†Î¸tâˆ’Î¸â„“),Î¦tÎ·t/angbracketrightï£¬ig
Î£âˆ’1
t+Î»/angbracketleftï£¬ig
Î£t(Ë†Î¸tâˆ’Î¸â„“),Î¸0âˆ’Î¸â„“/angbracketrightï£¬ig
Î£âˆ’1
t
â‰¤/vextenddouble/vextenddouble/vextenddoubleÎ£t(Ë†Î¸tâˆ’Î¸â„“)/vextenddouble/vextenddouble/vextenddouble
Î£âˆ’1
t/parenleftï£¬ig
âˆ¥Î¦tÎ·tâˆ¥Î£âˆ’1
t+Î»âˆ¥Î¸0âˆ’Î¸â„“âˆ¥Î£âˆ’1
t/parenrightï£¬ig
=/vextenddouble/vextenddouble/vextenddoubleË†Î¸tâˆ’Î¸â„“/vextenddouble/vextenddouble/vextenddouble
Î£t/parenleftï£¬ig
âˆ¥Î¦tÎ·tâˆ¥Î£âˆ’1
t+Î»/vextenddouble/vextenddoubleÎ¸0âˆ’Î¸â„“/vextenddouble/vextenddouble
Î£âˆ’1
t/parenrightï£¬ig
.
16Under review as submission to TMLR
By Lemma 6 from Abbasi-Yadkori et al. (2011), for any tâˆˆ[1,t1], we have the following hold with probability
at least 1âˆ’Î´/(t1(t1+ 1)),
âˆ¥Î¦tÎ·tâˆ¥Î£âˆ’1
tâ‰¤C/radicalï£¬igg
2 log/parenleftbiggdet(Î£t)1/2
Î»d/2Â·Î´/(t1(t1+ 1))/parenrightbigg
â‰¤C/radicalï£¬igg
2 log/parenleftbigg(Î»+tC2)d/2
Î»d/2Â·Î´/(t1(t1+ 1))/parenrightbigg
â‰¤C/radicalï£¬igg
dlog/parenleftbigg1 +tC2/Î»
Î´/(t1(t1+ 1))/parenrightbigg
=C/radicalï£¬igg
dlog/parenleftbiggt1(t1+ 1) +tÂ·t1(1 +t1)C2/Î»
Î´/parenrightbigg
.
In the next, we bound âˆ¥Î¸0âˆ’Î¸â„“âˆ¥Î£âˆ’1
t,
/vextenddouble/vextenddoubleÎ¸0âˆ’Î¸â„“/vextenddouble/vextenddouble2
Î£âˆ’1
tâ‰¤1
Î»min(Î£t)âˆ¥Î¸0âˆ’Î¸â„“âˆ¥2
2=1
Î»âˆ¥Î¸0âˆ’Î¸â„“âˆ¥2
2.
Finally, by plugging in the above bounds, we get the desired result for the base step
/vextenddouble/vextenddouble/vextenddoubleË†Î¸tâˆ’Î¸â„“/vextenddouble/vextenddouble/vextenddouble
Î£tâ‰¤C/radicalï£¬igg
dlog/parenleftbiggt1(t1+ 1) +tÂ·t1(1 +t1)C2/Î»
Î´/parenrightbigg
+âˆš
Î»âˆ¥Î¸0âˆ’Î¸â„“âˆ¥2.
Since 2tâ‰¥t1, then we have
/vextenddouble/vextenddouble/vextenddoubleË†Î¸tâˆ’Î¸â„“/vextenddouble/vextenddouble/vextenddouble
Î£tâ‰¤C/radicalï£¬igg
dlog/parenleftbigg4(t2+t3C2)/Î»
Î´/parenrightbigg
+âˆš
Î»Î´0. (11)
Letâ€™s suppose that, for any kâˆˆ{0,...,nâˆ’1}, equation 11 holds for all tâˆˆ[tk,tk+1âˆ’1]. For the induction
step, we define the following notations for any kâˆˆ{0,...,nâˆ’1},
Ë˜Vk(Â·) = min{C,Vk(Â·)}.
Consequently, for any kâˆˆ{0,...,n}andtâˆˆ[tk,tk+1âˆ’1], we further define
Ë˜Î£t=Î»I+t/summationdisplay
i=1Ï•Ë˜Vk(i)(hi,xi)Ï•Ë˜Vk(i)(hi,xi)âŠ¤,Ë˜Âµt=Î»Î¸0+t/summationdisplay
i=1Ï•Ë˜Vk(i)(hi,xi)Ë˜Vk(i)(hi+1),
In analogy, we reload the definition for Ë†Î¸tandÎ·tby
Ë˜Î¸t=Ë˜Î£âˆ’1
tË˜Âµt, Î·t=Ë˜Vk(t)(ht+1)âˆ’âŸ¨Ï•Ë˜Vk(t)(ht,xt),Î¸â„“âŸ©
By the above definition, itâ€™s easy to verify that {Ë˜Î·t}tn
t=1is almost surely C-sub-Gaussian.3Then, we can
apply the Lemma 6 again, and conclude that Î¸â„“âˆˆË˜Ctholds with probability at least 1âˆ’Î´/(tn(tn+ 1))for
anytâˆˆ[tn,tn+1âˆ’1]with
Ë˜Ct=/braceleftï£¬igg
Î¸âˆˆRd/vextendsingle/vextendsingle/vextendsingleâˆ¥Ë˜Î¸tâˆ’Î¸âˆ¥Ë˜Î£tâ‰¤C/radicalï£¬igg
dlog/parenleftbigg4(t2+t3C2/Î»)
Î´/parenrightbigg
+âˆš
Î»Î´0/bracerightï£¬igg
.
By the optimism principle in Algorithm 2 and the base step of induction, we will have Ë˜Vk(Â·) = Ë˜Vkfor
kâˆˆ{0,...,nâˆ’1}, which further gives us that Ë˜Î£t=Î£t,Ë˜Âµt=Âµt,Ë˜Î·t=Î·tand Ë˜Î¸t=Ë†Î¸tfor alltâˆˆ[1,tn+1âˆ’1].
3To be noted, without such construction, if the induction step conditions on the base step, there is no guarantee that the
(conditional) distribution of Î·tisC-sub-Gaussian. This may prevent us from applying the Lemma 6.
17Under review as submission to TMLR
Consequently, we further have Ë˜Ct=Ct. Lastly, by applying the union bound over kâ‰¥0, we will get that the
probability of the event in Lemma 1 holds is at least
1âˆ’/summationdisplay
k=0Î´
tk(tk+ 1)â‰¥1âˆ’Î´.
â–¡
E Proof of Theorem 1
Theorem 1 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(Câ‹†),Î»= 1/Î´2
0, and the cost function is bounded from below by cminfor all non-goal knowledge
states (H\{hâ‹†}) and teaching instruction ( X) pairs, then with probability at least 1âˆ’2Î´, the teaching cost
of Algorithm 1 for non-epiphany learners (i.e., Î³= 1) is upper bounded by
O/parenleftï£¬igg/parenleftï£¬igg
1 +d/radicalbigg
log/parenleftï£¬ig
1 +Câ‹†dÎ´0
Î´cmin/parenrightï£¬ig/parenrightï£¬igg
Â·log1.5/parenleftï£¬igCâ‹†d
cminÎ´/parenrightï£¬ig
Â·C2
â‹†d
cmin/parenrightï£¬igg
. (7)
Proof:To prove Theorem 1, we first bound the teaching cost for running Algorithm 1 for Tsteps. Then, we
can derive a bound for T, and plugging it back to obtain the final result.
For anyT, we can decompose the teaching cost into the following
T/summationdisplay
t=0c(ht,xt)â‰¤T/summationdisplay
t=0c(ht,xt)âˆ’V0(h0) +C. (12)
By Lemma 3, we know that
âˆ’T/summationdisplay
t=1/parenleftbig
Vk(t)(ht)âˆ’Vk(t)(ht+1)/parenrightbig
+ 2dClog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+Clog/parenleftbigg
1 +2T
Î»/parenrightbigg
+V0(h0)â‰¥0.
By adding it to the r.h.s of equation 12, we get
T/summationdisplay
t=0c(ht,xt)â‰¤T/summationdisplay
t=0c(ht,xt)âˆ’V0(h0) +T/summationdisplay
t=1/parenleftbig
Vk(t)(ht+1)âˆ’Vk(t)(ht)/parenrightbig
+ 2dClog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+Clog/parenleftbigg
1 +2T
Î»/parenrightbigg
+V0(h0) +C.
By rearranging the above terms, we can get the following terms
T/summationdisplay
t=0c(ht,xt)â‰¤T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’Vk(t)(ht)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1+T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht+1)âˆ’PÎ¸â„“Vk(t)(ht,xt)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2
+ 2dClog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+Clog/parenleftbigg
1 +2T
Î»/parenrightbigg
+C.
In the next, it remains to bound 1and2. By Lemma 4, we can bound 1by
1â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2(C+ 1)Â·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg
+ 1/parenrightbigg
.
18Under review as submission to TMLR
Then, by Lemma 9, we can bound the martingale difference 2, with probability at least 1âˆ’Î´, by
2â‰¤2C/radicalï£¬igg
2Tlog/parenleftbiggT
Î´/parenrightbigg
.
By merging the terms, we simplify the upper bound of the teaching cost to be
T/summationdisplay
t=1c(ht,xt)â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 15Cdlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2C/radicalï£¬igg
2Tlog/parenleftbiggT
Î´/parenrightbigg
,
whereÎ²T=C/radicalbig
dlog ((4(T2+T3C2/Î»))/Î´)+âˆš
Î»Î´0. Now, it remains to bound T. Since the cost function is
bounded from below by cmin, then we will have
TÂ·cminâ‰¤T/summationdisplay
t=0c(ht,xt).
By replacing the r.h.s. term with the upper bound derived, we get
TÂ·cminâˆ’Câ‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 15Cdlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2C/radicalï£¬igg
2Tlog/parenleftbiggT
Î´/parenrightbigg
.
For the terms on the r.h.s, we can loosely bound the second term by
15Cdlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
â‰¤8Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
.
Then, we can bound Tby
Tâ‰¤1
cmin/parenleftï£¬igg
12Î²T/radicalï£¬igg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2C/radicalï£¬igg
2 log/parenleftbiggT
Î´/parenrightbigg/parenrightï£¬igg
Â·âˆš
T+C
cmin.
Using the fact that câ‰¤aâˆšc+bâ‡’câ‰¤(a+âˆš
b)2fora,bâ‰¥0, we have
Tâ‰¤/parenleftï£¬igg
1
c2
min/parenleftï£¬igg
12Î²T/radicalï£¬igg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2C/radicalï£¬igg
2 log/parenleftbiggT
Î´/parenrightbigg/parenrightï£¬igg
+/radicalbigg
C
cmin/parenrightï£¬igg2
.
By using the inequality (a+b)2â‰¤2a2+ 2b2twice, we get
Tâ‰¤32
c2
min/parenleftbigg
36Î²2
Tdlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+C2log/parenleftbiggT
Î´/parenrightbigg/parenrightbigg
+2C
cmin.
Plugging in the following upper bound of Î²2
T,
Î²2
Tâ‰¤2C2dlog/parenleftbigg4(T2+T3C2/Î»)
Î´/parenrightbigg
+ 2Î»Î´2
0d,
Then, we get
Tâ‰¤32
c2
min/parenleftbigg
72/parenleftbigg
C2d2log/parenleftbigg4T2+ 4T3C2/Î»
Î´/parenrightbigg
+Î»Î´2
0d2/parenrightbigg
Â·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+C2log/parenleftbiggT
Î´/parenrightbigg/parenrightbigg
+2C
cmin.
19Under review as submission to TMLR
By rearranging the terms, we get
Tâ‰¤2304C2d2
c2
minÂ·log/parenleftbigg4T2+ 4T3C2/Î»
Î´/parenrightbigg
Â·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+2304Î»d2C2Î´2
0
c2
minÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+32C2
c2
minÂ·log/parenleftbiggT
Î´/parenrightbigg
+2C
cmin.
SinceÎ»= 1/Î´2
0, we can get the following bound
Tâ‰¤4608C2d2
c2
minÂ·log/parenleftbigg4T2+ 4T3C2Î´2
0
Î´/parenrightbigg
Â·log/parenleftbig
1 +TC2Î´2
0/parenrightbig
+32C2
c2
minÂ·log/parenleftbiggT
Î´/parenrightbigg
+2C
cmin.
We now consider the following cases: when Î´0â‰¤1/(TC2), we will have, for some universal constant C0,
Tâ‰¤C0/parenleftbiggC2d2
c2
minlog2/parenleftbiggT
Î´/parenrightbigg/parenrightbigg
.
WhenÎ´0>1/(TC2), we will have, for some universal constant C1,
Tâ‰¤C1/parenleftbiggC2d2
c2
minlog2/parenleftbiggTC
Î´/parenrightbigg/parenrightbigg
.
According to Lemma 5, we arrive at the desired bound for T
T=O/parenleftbiggC2d2
c2
minlog2/parenleftbiggCd
cminÎ´/parenrightbigg/parenrightbigg
.
BecauseC=O(Câ‹†)and plugging in the bound for Tinto the original bound, we can finally get the desired
bound for the teaching cost hold with probability at least 1âˆ’2Î´by further applying union bound on the two
events (i.e., Lemma 1 and bounding 2),
/summationdisplay
tc(ht,xt) =O/parenleftï£¬igg/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +Câ‹†dÎ´0
Î´cmin/parenrightbigg/parenrightï£¬igg
Â·log1.5/parenleftbiggCâ‹†d
cminÎ´/parenrightbigg
Â·C2
â‹†d
cmin/parenrightï£¬igg
.
â–¡
Lemma 2 Under the same assumptions as Theorem 1, if Algorithm 1 runs for Tsteps, then the total number
of value function updates (i.e., the number of EVI calls) Kis at most
Kâ‰¤2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +2T
Î»/parenrightbigg
.
Proof:The value function update can be triggered by either the determinant criteria ( K1) or the iteration
criteria (K2). We bound each part separately.
Bounding K1: To bound K1, it suffices to bound the determinant of Î£T. By Lemma 7, the fact that
Î£0=Î»I, and the Assumption 1, we have
det(Î£T)â‰¤(Î»+TC2)d.
20Under review as submission to TMLR
Therefore, we can immediately bound K1by
2K1Â·det(Î£0) = 2K1Â·Î»dâ‰¤(Î»+TC2)d
â‡’K1â‰¤2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
.
Bounding K2: To bound K2, we can look at the criteria triggered by it, which immediately gives us that
(1 +Î»)Â·2K2â‰¤T+Î»
â‡’K2â‰¤log/parenleftbiggT+Î»
1 +Î»/parenrightbigg
â‰¤log/parenleftbigg
1 +T
Î»/parenrightbigg
.
SinceK=K1+K2, we can conclude that
Kâ‰¤2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg
.
â–¡
Lemma 3 Under the same assumptions as Theorem 1, for any T, the following holds,
T/summationdisplay
t=0/parenleftbig
Vk(t)(ht)âˆ’Vk(t)(ht+1)/parenrightbig
â‰¤2dClog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+Clog/parenleftbigg
1 +2T
Î»/parenrightbigg
+V0(h0).
Proof:By Lemma 2, we can divide the Tsteps intoK+ 1segments, and within each segment, all the steps
share the same value function. Letâ€™s denote the ending step of kthsegment as tk+1âˆ’1, then we will have (by
canceling out the intermediate terms)
T/summationdisplay
t=0/parenleftbig
Vk(t)(ht)âˆ’Vk(t)(ht+1)/parenrightbig
=K/summationdisplay
k=0Vk(htk)âˆ’Vk(htk+1).
By rearranging terms, we can further get
T/summationdisplay
t=0/parenleftbig
Vk(t)(ht)âˆ’Vk(t)(ht+1)/parenrightbig
=Kâˆ’1/summationdisplay
k=0/parenleftbig
Vk+1(htk+1)âˆ’Vk(htk+1)/parenrightbig
+Kâˆ’1/summationdisplay
k=0/parenleftbig
Vk(htk)âˆ’Vk+1(htk+1)/parenrightbig
+VK(htK)âˆ’VK(htK+1)
=Kâˆ’1/summationdisplay
k=0/parenleftbig
Vk+1(htk+1)âˆ’Vk(htk+1)/parenrightbig
+V0(ht0)âˆ’VK(htK) +VK(htK)âˆ’VK(htK+1)
=Kâˆ’1/summationdisplay
k=0/parenleftbig
Vk+1(htk+1)âˆ’Vk(htk+1)/parenrightbig
+V0(ht0)âˆ’VK(htK+1).
Since the value function is non-negative, then we have
T/summationdisplay
t=1/parenleftbig
Vk(t)(ht)âˆ’Vk(t)(ht+1)/parenrightbig
â‰¤KÂ·max
kâˆ¥Vkâˆ¥âˆ+V0(h0).
By plugging in the upper bound of Kfrom Lemma 2 and the upper bound of the value function, C, we
finally arrive at
T/summationdisplay
t=1/parenleftbig
Vk(t)(ht)âˆ’Vk(t)(ht+1)/parenrightbig
â‰¤2dClog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+Clog/parenleftbigg
1 +2T
Î»/parenrightbigg
+V0(h0).
â–¡
21Under review as submission to TMLR
Lemma 4 Under the same assumptions as Theorem 1, for any T, we can bound 1by,
1=T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’Vk(t)(ht)/bracketrightbig
â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2(C+ 1)Â·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg
+ 1/parenrightbigg
,
whereÎ²T=C/radicalbig
dlog ((4(T2+T3C2/Î»))/Î´) +âˆš
Î»Î´0.
Proof:First of all, by the fact that Vk(t)(ht) = minxâˆˆXQk(t)(ht,x) =Qk(t)(ht,xt), we have
1=T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’Qk(t)(ht,xt)/bracketrightbig
.
Letâ€™s suppose that Qk(t)(Â·,Â·)is the value function at the lk(t)th value iteration of Algorithm 2, i.e., the last
iteration of the while loop. Then, based on the EVI algorithm, we have
Qk(t)(ht,xt) =c(ht,xt) +Î½Â·min
Î¸âˆˆCtâˆ©BâŸ¨Î¸,Ï•V(lk(t)âˆ’1)(ht,xt)âŸ©
=c(ht,xt) +Î½Â·âŸ¨Î¸t,Ï•V(lk(t)âˆ’1)(ht,xt)âŸ©
=c(ht,xt) +Î½Â·âŸ¨Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+Î½Â·âŸ¨Î¸t,[Ï•V(lk(t)âˆ’1)âˆ’Ï•V(lk(t))](ht,xt)âŸ©,
whereÎ¸t=arg minÎ¸âˆˆCtâˆ©BâŸ¨Î¸,Ï•V(lk(t)âˆ’1)(ht,xt)âŸ©. By plugging the above equation into 1to replace
Qk(t)(ht,xt), and then rearrange terms, we get
c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’Qk(t)(ht,xt)
=c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’c(ht,xt)âˆ’Î½Â·âŸ¨Î¸t,Ï•V(lk(t))(ht,xt)âŸ©
âˆ’Î½Â·âŸ¨Î¸t,[Ï•V(lk(t)âˆ’1)âˆ’Ï•V(lk(t))](ht,xt)âŸ©
=âŸ¨Î¸â„“,Ï•V(lk(t))(ht,xt)âŸ©âˆ’Î½Â·âŸ¨Î¸t,Ï•V(lk(t))(ht,xt)âŸ©
âˆ’Î½Â·âŸ¨Î¸t,[Ï•V(lk(t)âˆ’1)âˆ’Ï•V(lk(t))](ht,xt)âŸ©
=âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+ (1âˆ’Î½)Â·âŸ¨Î¸t,Ï•V(lk(t))(ht,xt)âŸ©
âˆ’Î½Â·âŸ¨Î¸t,[Ï•V(lk(t)âˆ’1)âˆ’Ï•V(lk(t))](ht,xt)âŸ©.
By the termination condition of the EVI algorithm, we have
c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’Qk(t)(ht,xt)
â‰¤âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+ (1âˆ’Î½)Â·âŸ¨Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+Î½Â·1
Î»Â·tâ€²
k(t)
â‰¤âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+ (1âˆ’Î½)Â·C+Î½
Î»Â·tâ€²
k(t),
wheretâ€²
k(t)is the time step of k(t)thEVI call, we use tâ€²
k(t)instead oftk(t)to avoid ambiguity. Therefore, we
can bound 1by
1â‰¤T/summationdisplay
t=0âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+ (1âˆ’Î½)Â·C+Î½
Î»Â·tâ€²
k(t)
=T/summationdisplay
t=0âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+T/summationdisplay
t=0/parenleftï£¬igg
Î½
Î»Â·tâ€²
k(t)+ (1âˆ’Î½)Â·C/parenrightï£¬igg
.
22Under review as submission to TMLR
By the fact that both Î¸â„“andÎ¸tare inCtand Lemma 1, we must have
âˆ¥Î¸â„“âˆ’Î¸tâˆ¥Î£tâ‰¤2Î²tâ‰¤2Î²T.
Together with the Cauchy-Schwartz inequality, we obtain
âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©â‰¤âˆ¥Î¸â„“âˆ’Î¸tâˆ¥Î£tÂ·âˆ¥Ï•V(lk(t))(ht,xt)âˆ¥Î£âˆ’1
t
â‰¤2âˆ¥Î¸â„“âˆ’Î¸tâˆ¥Î£tÂ·âˆ¥Ï•V(lk(t))(ht,xt)âˆ¥Î£âˆ’1
t
â‰¤4Î²Tâˆ¥Ï•V(lk(t))(ht,xt)âˆ¥Î£âˆ’1
t
In the meantime, we also have
âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©â‰¤C.
Then, since Câ‰¤Î²T, we get
âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©â‰¤min/braceleftï£¬ig
C,4Î²Tâˆ¥Ï•V(lk(t))(ht,xt)âˆ¥Î£âˆ’1
t/bracerightï£¬ig
.
â‰¤min/braceleftï£¬ig
Î²T,4Î²Tâˆ¥Ï•V(lk(t))(ht,xt)âˆ¥Î£âˆ’1
t/bracerightï£¬ig
.
By Lemma 8, we have
T/summationdisplay
t=0âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©
â‰¤4Î²TT/summationdisplay
t=0min/braceleftï£¬ig
1,âˆ¥Ï•V(lk(t))(ht,xt)âˆ¥Î£âˆ’1
t/bracerightï£¬ig
â‰¤4Î²T/radicaltp/radicalvertex/radicalvertex/radicalbtTÂ·/parenleftï£¬iggT/summationdisplay
t=0min/braceleftï£¬ig
1,âˆ¥Ï•V(lk(t))(ht,xt)âˆ¥Î£âˆ’1
t/bracerightï£¬ig/parenrightï£¬igg
â‰¤4Î²T/radicalï£¬igg
TÂ·/bracketleftbigg
2dlog/parenleftbiggtr(Î»I) +TC2d
d/parenrightbigg
âˆ’log det(Î»I)/bracketrightbigg
â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
.
Next, we will bound the other part. By plugging in 1âˆ’Î½= 1/(Î»Â·tâ€²
k(t)), we have
T/summationdisplay
t=0/parenleftï£¬igg
Î½
Î»Â·tâ€²
k(t)+ (1âˆ’Î½)Â·C/parenrightï£¬igg
â‰¤T/summationdisplay
t=0C+ 1
Î»Â·tâ€²
k(t)= (C+ 1)T/summationdisplay
t=01
Î»Â·tâ€²
k(t).
Considering the iteration triggering criteria, we get
tâ€²
k(t)+1â‰¤2tâ€²
k(t)+Î».
Then, we can conclude that
T/summationdisplay
t=0/parenleftï£¬igg
Î½
Î»Â·tâ€²
k(t)+ (1âˆ’Î½)Â·C/parenrightï£¬igg
â‰¤K/summationdisplay
k=0(C+ 1)Â·/parenleftbigg1
Î»+1
tâ€²
k/parenrightbigg
â‰¤2(K+ 1)Â·(C+ 1)
= 2(C+ 1)Â·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg
+ 1/parenrightbigg
23Under review as submission to TMLR
By combining the two bounds, we get
1â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2(C+ 1)Â·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg
+ 1/parenrightbigg
.
â–¡
Lemma 5 Suppose that Tâ‰¥2,aâ‰¥1andTâ‰¤klog2(aT)for all large enough k. Then, there exists Î·=Î·(a)
such thatTâ‰¤Î·Â·klog2(ak)for all large enough k, i.e.,T=O(klog2(ak)).
Proof:We prove the above lemma by contrapositive. Suppose that there doesnâ€™t exist such an Î·. Then, we
will have, for all large enough k,
Tâ‰¥bkÂ·klog2(ak),
where{bk}âˆ
k=1is a sequence with limkâ†’+âˆbk= +âˆ. The above inequality also implies that
bkâ‰¤T
klog2(ak)â‰¤log2(aT)
log2(ak).
Now, letâ€™s consider the following
log2(aT)â‰¤log2(akÂ·log2(aT)) = (log(ak) + log log2(aT))2.
By the inequality (a+b)2â‰¤2a2+ 2b2, we get
log2(aT)â‰¤2 log2(ak) + 2 log2(log2(aT)).
SinceaTâ‰¥2, we will have
log2(log2(aT))â‰¤1
4log2(aT)â‡’ log2(aT)â‰¤2 log2(ak) +1
2log2(aT).
Therefore, we can get
1
2log2(aT)â‰¤2 log2(ak)â‡’bkâ‰¤log2(aT)
log2(ak)â‰¤4,
which leads to a contradiction with limkâ†’+âˆbk= +âˆ. Hence, we have T=O(klog2(ak)). â–¡
F Proof of Theorem 2
Theorem 2 Under Assumptions 1 and 2, if the confidence set Ctis constructed according to Lemma 1 with
C=O(Câ‹†),Î»= 1/Î´2
0, then with probability at least 1âˆ’3Î´, the total cost incurred by running Algorithm 1 for
epiphany learners with Î³ <1, is upper bounded by
O/parenleftï£¬igg
Câ‹†Â·/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +C2â‹†Î´2
0logÎ´
logÎ³/parenrightbigg/parenrightï£¬igg
Â·/radicalï£¬igg
logÎ´
logÎ³log/parenleftbigg
Câ‹†logÎ´
Î´logÎ³/parenrightbigg/parenrightï£¬igg
. (8)
Proof:The proof for the epiphany learner case mostly follows from the proof of the non-epiphany learner
case, i.e., Theorem 1. In the same way, we can still decompose the cost as in Theorem 1. The only differences
are in the bound of 1in and the upper bound on T.
T/summationdisplay
t=0c(ht,xt)â‰¤T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’Vk(t)(ht)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1+T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht+1)âˆ’PÎ¸â„“Vk(t)(ht,xt)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2
+ 2dClog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+Clog/parenleftbigg
1 +2T
Î»/parenrightbigg
+C.
24Under review as submission to TMLR
In analogy to Lemma 4, we can get the following bound for 1,
1=T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PÎ¸â„“Vk(t)(ht)âˆ’Vk(t)(ht)/bracketrightbig
â‰¤T/summationdisplay
t=0âŸ¨Î¸â„“âˆ’Î¸t,Ï•V(lk(t))(ht,xt)âŸ©+T/summationdisplay
t=0/parenleftï£¬igg
Î³
Î»Â·tâ€²
k(t)+ (1âˆ’Î³)Â·C/parenrightï£¬igg
â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+T/summationdisplay
t=0/parenleftï£¬igg
Î³
Î»Â·tâ€²
k(t)+ (1âˆ’Î³)Â·C/parenrightï£¬igg
.
In the following, we will bound the r.h.s term in the above equation in a similar way to the proof in Lemma 4,
T/summationdisplay
t=0/parenleftï£¬igg
Î³
Î»Â·tâ€²
k(t)+ (1âˆ’Î³)Â·C/parenrightï£¬igg
â‰¤K/summationdisplay
k=0Î³Â·/parenleftbigg1
Î»+1
tâ€²
k/parenrightbigg
+ (1âˆ’Î³)Â·TÂ·C
â‰¤2Î³Â·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg/parenrightbigg
+ (1âˆ’Î³)Â·TÂ·C.
Then, by plugging in the above bounds, we get
1â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2Î³Â·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg/parenrightbigg
+ (1âˆ’Î³)Â·TÂ·C.
The bound for 2in Theorem 1 still holds with probability at least 1âˆ’Î´. Hence, we can merge all the terms
and simply them into
T/summationdisplay
t=0c(ht,xt)â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 9Cdlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2C/radicalï£¬igg
2Tlog/parenleftbiggT
Î´/parenrightbigg
+ (1âˆ’Î³)Â·TÂ·C+C
=O/parenleftbigg
CÂ·/parenleftbigg
1 +d/radicalï£¬ig
log (1 +TC2Î´2
0)/parenrightbigg
Â·/radicalbig
TÂ·log (TC/Î´ ) + (1âˆ’Î³)Â·TÂ·C/parenrightbigg
In the next, itâ€™s easy to show that, with probability at least 1âˆ’Î´, the following holds4
T=O(log(Î´)/log(Î³)).
Lastly, since C=O(Câ‹†), and plugging in the value of T, we have the following hold with probability at least
1âˆ’3Î´by applying the union bound over the three events (i.e., Lemma 1, bounding 2and bounding T),
/summationdisplay
t=0c(ht,xt) =O/parenleftï£¬igg
Câ‹†Â·/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +C2â‹†Î´2
0logÎ´
logÎ³/parenrightbigg/parenrightï£¬igg
Â·/radicalï£¬igg
logÎ´
logÎ³log/parenleftbiggCâ‹†logÎ´
Î´logÎ³/parenrightbigg/parenrightï£¬igg
.
â–¡
G Proof of Theorem 3
Theorem 3 ForÏµ-approximate teachable epiphany learners as defined in Definition 2, if âˆ¥Î¸0âˆ’Î¸â‹†âˆ¥2â‰¤Î´0,
the cost function is bounded from above by cmax, the confidence set Ctis constructed according to Lemma 1
4Without the loss of generality, we assume log(Î´)/log(Î³)â‰¥1.
25Under review as submission to TMLR
withC=O(ÏµÎ³cmax/(1âˆ’Î³)2+Câ‹†), and ifÎ»= 1/Î´2
0, then with probability at least 1âˆ’3Î´, the teaching cost
incurred by running Algorithm 1 is upper bounded by
O/parenleftï£¬igg
CÂ·/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +C2Î´2
0logÎ´
logÎ³/parenrightbigg/parenrightï£¬igg
Â·/radicalï£¬igg
logÎ´
logÎ³log/parenleftbigg
ClogÎ´
Î´logÎ³/parenrightbigg
Â·ÏµlogÎ´
logÎ³C/parenrightï£¬igg
. (9)
Proof:The proof for Ïµ-approximate teachable epiphany learner also follows from the proof of Theorem 1 and
Theorem 2. However, to make the similar proof work, we have to bound the maximum value of the value
function underMÎ¸â‹†. To show this, by Lemma 10 and Definition 2, we have
âˆ¥Vâ‹†(Â·|Î¸â‹†)âˆ’Vâ‹†(Â·)âˆ¥âˆâ‰¤Î³cmaxÏµ
(1âˆ’Î³)2,
where we use Vâ‹†(Â·|Î¸â‹†)andVâ‹†(Â·)to denote the optimal value function under the approximate MDP MÎ¸â‹†
and the true MDP M, respectively. Therefore, we can conclude that
âˆ¥Vâ‹†(Â·|Î¸â‹†)âˆ¥âˆâ‰¤Câ‹†+Î³cmaxÏµ
(1âˆ’Î³)2.
Together with the optimism principle in Algorithm 2, recall that
Î·t=Vk(t)âˆ’âŸ¨Ï•Vk(t)(ht,xt),Î¸â‹†âŸ©.
We will have Î·tis(Câ‹†+Î³cmaxÏµ
(1âˆ’Î³)2)-sub-Gaussian. Therefore, by choosing C=Câ‹†+Î³cmaxÏµ
(1âˆ’Î³)2as assumed, we will
have the following holds with probability at least 1âˆ’Î´by following the same proof as in Lemma 1,
Î¸â‹†âˆˆCtâˆ©B.
Condition on the above event, the same teaching cost decomposition in Theorem 1 still holds,
T/summationdisplay
t=0c(ht,xt)â‰¤T/summationdisplay
t=0/bracketleftbig
c(ht,xt) +PÎ¸â„“Vk(t)(ht,xt)âˆ’Vk(t)(ht)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
1+T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht+1)âˆ’PÎ¸â„“Vk(t)(ht,xt)/bracketrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
2
+ 2dClog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+Clog/parenleftbigg
1 +2T
Î»/parenrightbigg
+C.
To bound 1, the idea is similar to Lemma 4. Due to the model misspecification, there will be one additional
term in the bound,
T/summationdisplay
t=0c(ht,xt) +PVk(t)(ht,xt)âˆ’Qk(t)(ht,xt)
=T/summationdisplay
t=0c(ht,xt) +PVk(t)(ht,xt)âˆ’PÎ¸â‹†Vk(t)(ht,xt) +PÎ¸â‹†Vk(t)(ht,xt)âˆ’Qk(t)(ht,xt)
=T/summationdisplay
t=0/parenleftbig
c(ht,xt) +PÎ¸â‹†Vk(t)(ht,xt)âˆ’Qk(t)(ht,xt)/parenrightbig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
â™£+ [Pâˆ’PÎ¸â‹†]Vk(t)(ht,xt)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
â™¥.
The bound of the â™£term is still the same as it in Theorem 2, and the bound for the term â™¥is
[Pâˆ’PÎ¸â‹†]Vk(t)(ht,xt)â‰¤CÂ·Ïµ.
By putting the two bounds together we get
1â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2Î³Â·/parenleftbigg
2dlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ log/parenleftbigg
1 +T
Î»/parenrightbigg/parenrightbigg
+ (1âˆ’Î³)Â·TÂ·C+ÏµÂ·TÂ·C.
26Under review as submission to TMLR
The bound for 2in Theorem 1 still holds here with probability at least 1âˆ’Î´. Hence, we can merge all the
terms and simply them into
T/summationdisplay
t=0c(ht,xt)â‰¤4Î²T/radicalï£¬igg
2TdÂ·log/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 9Cdlog/parenleftbigg
1 +TC2
Î»/parenrightbigg
+ 2C/radicalï£¬igg
2Tlog/parenleftbiggT
Î´/parenrightbigg
+ (1âˆ’Î³)Â·TÂ·C+C+ÏµÂ·TÂ·C
Following the proof in Theorem 2, we have the following hold with probability at least 1âˆ’Î´,
T=O(log(Î´)/log(Î³)).
By applying the union bound for the three events (i.e., Lemma 1, bounding 2and bounding T), and
plugging in the above TandC=O(Câ‹†+Î³cmaxÏµ
(1âˆ’Î³)2), we can get the final bound for the teaching cost, with
probability at least 1âˆ’3Î´,
T/summationdisplay
t=0c(ht,xt) =O/parenleftï£¬igg
CÂ·/parenleftï£¬igg
1 +d/radicalï£¬igg
log/parenleftbigg
1 +C2Î´2
0logÎ´
logÎ³/parenrightbigg/parenrightï£¬igg
Â·/radicalï£¬igg
logÎ´
logÎ³log/parenleftbiggClogÎ´
Î´logÎ³/parenrightbigg
+ÏµlogÎ´
logÎ³C/parenrightï£¬igg
.
â–¡
H Additional Theorems and Lemmas
Lemma 6 (Abbasi-Yadkori et al. (2011)) Let{Ft}âˆ
t=0be a filtration. Let {Î·t}âˆ
t=1be a real-valued
stochastic process such that Î·tisFt-measurable and Î·tis conditionally B-sub-Gaussian. Let {Ï•t}âˆ
t=1be an
Rd-valued stochastic process such that Ï•tisFtâˆ’1-measurable. Assume that Î£is adÃ—dpositive definite
matrix. For any tâ‰¥0, define
Î£t=Î£+t/summationdisplay
i=1Ï•iÏ•âŠ¤
i,st=t/summationdisplay
i=1Î·iÏ•i.
Then, for any Î´>0, with probability at least 1âˆ’Î´, for alltâ‰¥0,
âˆ¥Î£âˆ’1/2
tstâˆ¥2â‰¤B/radicalï£¬igg
2 log/parenleftbiggdet(Î£t)1/2
Î´Â·det(Î£)1/2/parenrightbigg
.
Lemma 7 (Abbasi-Yadkori et al. (2011)) Suppose that Ï•1,...,Ï•tâˆˆRdand for any 1â‰¤sâ‰¤t, we have
âˆ¥Ï•sâˆ¥â‰¤L. Let Î£t=Î»I+/summationtextt
s=1Ï•sÏ•âŠ¤
sfor someÎ»>0. Then,
det(Î£t)â‰¤(Î»+tL2/d)d.
Lemma 8 (Abbasi-Yadkori et al. (2011)) Let{Ï•t}âˆ
t=1be in Rd, andâˆ¥Ï•tâˆ¥â‰¤Lfor anyt. Then, for
Î£t=Î»I+/summationtextt
s=1Ï•sÏ•âŠ¤
s, we will have
t/summationdisplay
s=1min/braceleftï£¬ig
1,âˆ¥Ï•sâˆ¥Î£âˆ’1
sâˆ’1/bracerightï£¬ig
â‰¤2/bracketleftbigg
dlog/parenleftbiggtr(Î»I) +tL2
d/parenrightbigg
âˆ’log det(Î»I)/bracketrightbigg
.
Lemma 9 (Min et al. (2021)) For a transition function P, a sequence of bounded and non-negative value
functions{Vk}K
k=1under P, and a state action sequence {(ht,xt)}T
t=1, whereâˆ¥Vkâˆ¥âˆâ‰¤Candht+1âˆ¼
P[Â·|ht,xt], we have the following hold with probability at least 1âˆ’Î´,
T/summationdisplay
t=0/bracketleftbig
Vk(t)(ht)âˆ’PVk(t)(ht,xt)/bracketrightbig
â‰¤2C/radicalï£¬igg
2Tlog/parenleftbiggT
Î´/parenrightbigg
.
27Under review as submission to TMLR
Lemma 10 (CsÃ¡ji and Monostori (2008)) For two discounted MDPs with discounting factor Î³, if they
differ only in the transition functions, denoted by P1andP2. If their corresponding optimal value functions
areVâ‹†
1andVâ‹†
2, respectively, and the cost function is bounded from above by cmax, then
âˆ¥Vâ‹†
1âˆ’Vâ‹†
2âˆ¥âˆâ‰¤Î³cmax
(1âˆ’Î³)2âˆ¥P1âˆ’P2âˆ¥âˆ.
28