Published in Transactions on Machine Learning Research (11/2023)
Data pruning and neural scaling laws: fundamental limita-
tions of score-based algorithms
Fadhel Ayedâˆ—fadhel.ayed@gmail.com
Huawei Technologies France
Soufiane Hayouâˆ—hayou@nus.edu.sg
National University of Singapore
Reviewed on OpenReview: https: // openreview. net/ forum? id= iRTL4pDavo
Abstract
Data pruning algorithms are commonly used to reduce the memory and computational cost
of the optimization process. Recent empirical results (Guo, B. Zhao, and Bai, 2022) reveal
that random data pruning remains a strong baseline and outperforms most existing data
pruning methods in the high compression regime, i.e. where a fraction of 30%or less of
the data is kept. This regime has recently attracted a lot of interest as a result of the role
of data pruning in improving the so-called neural scaling laws; see (Sorscher et al., 2022),
where the authors showed the need for high-quality data pruning algorithms in order to beat
the sample power law. In this work, we focus on score-based data pruning algorithms and
show theoretically and empirically why such algorithms fail in the high compression regime.
We demonstrate â€œNo Free Lunch" theorems for data pruning and discuss potential solutions
to these limitations.
1 Introduction
Coreset selection, also known as data pruning, refers to a collection of algorithms that aim to efficiently
select a subset from a given dataset. The goal of data pruning is to identify a small yet representative
sample of the data that accurately reflects the characteristics of the entire dataset. Coreset selection is often
used in cases where the original dataset is too large or complex to be processed efficiently by the available
computational resources. By selecting a coreset, practitioners can reduce the computational cost of their
analyses and gain valuable insights more efficiently. Data pruning has many interesting applications, notably
neural architecture search (NAS), where models trained with a small fraction of the data serve as a proxy
to quickly estimate the performance of a given choice of hyper-parameters (Coleman et al., 2020). Another
application is continual (or incremental) learning in the context of online learning; To avoid the forgetting
problem, one keeps track of the most representative examples of past observations (Aljundi et al., 2019).
Coreset selection is typically performed once during training, and the selected coreset remains fixed until
the end of training. This topic has been extensively studied in classical machine learning and statistics
(Welling, 2009; Chen, Welling, and Smola, 2012; Feldman, Faulkner, and Krause, 2011; Huggins, Campbell,
and Broderick, 2016; Campbell and Broderick, 2019). Recently, many approaches have been proposed to
adapt to the challenges of the deep learning context. Examples include removing the redundant examples
from the feature space perspective (see Sener and Savarese, 2018), finding the hard examples, defined as
the ones for which the model is the least confident (Coleman et al., 2020), or the ones that contribute the
most to the error (Toneva et al., 2019). We refer the reader to Section 6 for a more comprehensive literature
review. Most of these methods use a score function that ranks examples based on their â€œimportance". Given
a desired compression level râˆˆ(0,1)(the fraction of data kept after pruning), the coreset is created by
retaining only the most important examples based on the scores to meet the required compression level. We
âˆ—Equal contribution (Alphabetical order).
1Published in Transactions on Machine Learning Research (11/2023)
Figure 1: Logistic regression: Data distribution alteration due to pruning for different compression ratios.
Here we use GraNdas the pruning algorithm. Blue points correspond to Yi= 0, red points correspond to
Yi= 1. More details in Section 5.
refer to this type of algorithms as score-based pruning algorithms ( SBPA). A formal definition is provided in
Section 2.
1.1 Connection to Neural Scaling Laws
Recently, a stream of empirical works have observed the emergence of power law scaling in different machine
learning applications (see e.g. Hestness et al., 2017; Kaplan et al., 2020; Rosenfeld et al., 2020; Hernandez
et al., 2021; Zhai et al., 2022; Hoffmann et al., 2022). More precisely, these empirical results show that the
performance of the model (e.g. the test error) scales as a power law with either the model size, training
datasetsize, orcompute(FLOPs). InSorscheretal., 2022, theauthorsshowedthatdatapruningcanimprove
the power law scaling of the dataset size. The high compression regime (small r) is of major interest in this
case since it exhibits super-polynomial scaling laws on different tasks. However, as the authors concluded,
improving the power law scaling requires high-quality data pruning algorithms, and it is still unclear what
properties such algorithms should satisfy. Besides scaling laws, small values of rare of particular interest
for tasks such as hyper-parameters selection, where the practitioner wants to select a hyper-parameter from
a grid rapidly. In this case, the smaller the value of r, the better.
In this work, we argue that score-based data pruning is generally not suited for the high compression regime
(starting from râ‰¤30%) and, therefore, cannot be used to beat the power law scaling. In this regime, it
has been observed (see e.g. Guo, B. Zhao, and Bai, 2022) that most SBPAalgorithms underperform random
pruning (randomly selected subset). To understand why this occurs, we analyze the asymptotic behavior of
SBPAalgorithms and identify some of their properties, particularly in the high compression level regime. To
the best of our knowledge, no rigorous explanation for this phenomenon has been reported in the literature.
Our work provides the first theoretical explanation for this behavior and offers insights on how to address it
in practice.
Intuitively, SBPAalgorithms induce a distribution shift that affects the training objective. This can, for
example, lead to the emergence of new local minima where performance deteriorates significantly. To give
a sense of this intuition, we use a toy example in Fig. 1 to illustrate the change in data distribution as the
compression level rdecreases, where we have used GraNd(Paul, Ganguli, and Dziugaite, 2021) to prune the
dataset.
We also report the change in the loss landscape in Fig. 2 as the compression level decreases and the resulting
scaling laws. The results show that such a pruning algorithm cannot be used to improve the scaling laws
since the performance drops significantly in the high compression regime and does not tend to significantly
decrease with sample size.
Motivated by these empirical observations, we aim to understand the behaviour of SBPAalgorithms in the
high compression regime. In Section 3, we analyze the impact of pruning of SBPAalgorithms on the loss
function in detail and link this distribution shift to a notion of consistency. We prove several results showing
the limitations of SBPAalgorithms in the high compression regime, which explains some of the empirical
results reported in Fig. 2. We also propose calibration protocols, that build on random exploration to
address this deterioration in the high compression regime (Fig. 2).
2Published in Transactions on Machine Learning Research (11/2023)
1.2 Contributions
Figure 2: Logistic regression: ris the compression
level,nthe total number of available data and wthe
learnable parameter. ( Left) The loss landscape trans-
formation due to pruning. ( Right) The evolution of
the performance gap as the data budget m:=rÃ—nin-
creases (average over ten runs). Top figures illustrate
theperformanceof GraNd,bottomfiguresillustratethe
performance of GraNdcalibrated with our exact proto-
col: we use 90%of the data budget for the signal, i.e.
points selected by GraNd, and 10%of the data budget
for calibration through random exploration. See Sec-
tions 4 and 5 for more details.Our contributions are as follows:
â€¢We propose a novel formalism to character-
ize the asymptotic properties of data prun-
ingalgorithmsintheabundantdataregime.
â€¢We introduce Score-Based Pruning Algo-
rithms ( SBPA), a class of algorithms that
encompasses a wide range of popular ap-
proaches. By employing our formalism, we
analyze SBPAalgorithms and identify a phe-
nomenon of distribution shift, which prov-
ably impacts generalization error.
â€¢WedemonstrateNo-Free-Lunchresultsthat
characterize when and why score-based
pruningalgorithmsperformworsethanran-
dom pruning. Specifically, we prove that
SBPAare unsuitable for high compression
scenarios due to a significant drop in perfor-
mance. Consequently, SBPAcannot improve
scaling laws without appropriate adapta-
tion.
â€¢Leveraging our theoretical insights, solu-
tions can be designed to address these lim-
itations. As an illustration, we introduce
a simple calibration protocol to correct the
distribution shift by adding noise to the
pruning process. Theoretical and empiri-
cal results support the effectiveness of this
method on toy datasets and show promising
results on image classification tasks.1
2 Learning with Data Pruning
2.1 Setup
Consider a supervised learning task where the inputs and outputs are respectively in XâŠ‚RdxandYâŠ‚Rdy,
both assumed to be compact2. We denote byD=XÃ—Ythe data space. We assume that there exists Âµ, an
atomless probability distribution on Dfrom which input/output pairs Z= (X,Y )are drawn independently
at random. We call such Âµadata generating process . We will assume that Xis continuous while Ycan be
either continuous (regression) or discrete (classification). We are given a family of models
MÎ¸={yout(Â·;w) :Xâ†’Y|wâˆˆWÎ¸}, (1)
parameterised by the parameter space WÎ¸, a compact subspace of RdÎ¸, whereÎ¸âˆˆÎ˜is a fixed hyper-
parameter . For instance,MÎ¸could be a family of neural networks of a given architecture, with weights w,
and where the architecture is given by Î¸. We will assume that youtis continuous onXÃ—WÎ¸3. For a given
1It is important to note that the calibration protocol serves as an example to stimulate further research. We do not claim
that this method systematically allows to outperform random pruning nor to beat the neural scaling laws.
2We further require that the set Xhas no isolated points. This technical assumption is required to avoid dealing with
unnecessary complications in the proofs.
3This is generally satisfied for a large class of models, including neural networks.
3Published in Transactions on Machine Learning Research (11/2023)
continuous loss function â„“:YÃ—Yâ†’ R, the aim of the learning procedure is to find a model that minimizes
the generalization error, defined by
L(w)def=EÂµâ„“/parenleftbig
yout(X;w),Y/parenrightbig
. (2)
We are given a dataset Dncomposed of nâ‰¥1input/output pairs (xi,yi),iidsampled from the data
generating process Âµ. To obtain an approximate minimizer of the generalization error (Eq. (2)), we perform
an empirical risk minimization, solving the problem
min
wâˆˆWÎ¸Ln(w)def=1
nn/summationdisplay
i=1â„“/parenleftbig
yout(xi;w),yi/parenrightbig
. (3)
The minimization problem (3) is typically solved using a numerical approach, often gradient-based, such
as Stochastic Gradient Descent (Robbins and Monro, 1951), Adam (Kingma and Ba, 2017), etc. We refer
to this procedure as the training algorithm . We assume that the training algorithm is exact, i.e. it will
indeed return a minimizing parameter wâˆ—
nâˆˆargminwâˆˆWÎ¸Ln(w).The numerical complexity of the training
algorithms grows with the sample size n, typically linearly or worse. When nis large, it is appealing to
extract a representative subsetofDnand perform the training with this subset, which would reduce the
computational cost of training. This process is referred to as data pruning. However, in order to preserve
the performance, the subset should retain essential information from the original (full) dataset. This is the
primary objective of data pruning algorithms. We begin by formally defining such algorithms.
Notation. IfZis a finite set, we denote by |Z|its cardinal number, i.e. the number of elements in Z. We
denoteâŒŠxâŒ‹the largest integer smaller than or equal to xforxâˆˆR. For some Euclidean space E, we denote by
dthe Euclidean distance and for some set BâŠ‚EandeâˆˆE, we define the distance d(e,B) = infbâˆˆBd(e,b).
Finally, for two integers n1< n 2,[n1:n2]refers to the set{n1,n1+ 1,...,n 2}. We denote the set of all
finite subsets ofDbyC, i.e.C=âˆªnâ‰¥1{{z1,z2,...,zn},z1Ì¸=z2Ì¸=...Ì¸=znâˆˆD}. We callCthe finite power
set ofD.
Definition 1 (Data Pruning Algorithm) We say that a function A:CÃ—(0,1]â†’Cis a data pruning
algorithm if for all ZâˆˆC,râˆˆ(0,1], such that r|Z|is an integer4, we have the following
â€¢A(Z,r)âŠ‚Z
â€¢|A(Z,r)|=r|Z|
where|.|refers to the cardinal number. The number ris called the compression level and refers to the fraction
of the data kept after pruning.
Among the simplest pruning algorithms, we will pay special attention to Randompruning, which selects
uniformly at random a fraction of the elements of Zto meet some desired compression level r.
2.2 ValidandConsistent Pruning Algorithms
Given a pruning algorithm Aand a compression level r, a subset of the training set is selected and the
model is trained by minimizing the empirical loss on the subset. More precisely, the training algorithm finds
a parameter wA,r
nâˆˆargminwâˆˆWÎ¸LA,r
n(w)where
LA,r
n(w)def=1
|A(Dn,r)|/summationdisplay
(x,y)âˆˆA(Dn,r)â„“/parenleftbig
yout(x;w),y/parenrightbig
.
4We make this assumption to simplify the notations. One can take the integer part of rninstead.
4Published in Transactions on Machine Learning Research (11/2023)
This usually requires only a fraction rof the original energy/time5cost or better, given the linear complexity
of the training algorithm with respect to the data size. In this work, we evaluate the quality of a pruning
algorithm by considering the performance gap it induces, i.e. the excess risk of the selected model
gapA,r
n=L(wA,r
n)âˆ’min
wâˆˆWÎ¸L(w). (4)
In particular, we are interested in the abundant data regime: we aim to understand the asymptotic behavior
oftheperformancegapasthesamplesize ngrowstoinfinity. Wedefinethenotionof validpruningalgorithms
as follows.
Definition 2 (Valid pruning algorithm) For a parameter space WÎ¸, a pruning algorithm Ais valid at
a compression level râˆˆ(0,1]iflimnâ†’âˆgapA,r
n= 0almost surely. The algorithm is said to be valid if it is
valid at any compression level râˆˆ(0,1].
We argue that a valid data pruning algorithm for a given generating process Âµand a family of models MÎ¸
should see its performance gap converge to zero almost surely. Otherwise, it would mean that with positive
probability, the pruning algorithm induces a deterioration of the out-of-sample performance that does not
vanish even when an arbitrarily large amount of data is available. This deterioration would not exist without
pruning or if random pruning was used instead (Corollary 1). This means that with positive probability, a
non-valid pruning algorithm will underperform random pruning in the abundant data regime. In the next
result, we show that a sufficient and necessary condition for a pruning algorithm to be valid at compression
levelris thatwA,r
nshould approach the set of minimizers of the original generalization loss function as n
increases.
Proposition 1 (Characterization of valid pruning algorithms) A pruning algorithm Ais valid at a
compression level râˆˆ(0,1]if and only if
d/parenleftï£¬ig
wA,r
n,Wâˆ—
Î¸(Âµ)/parenrightï£¬ig
â†’0a.s.
whereWâˆ—
Î¸(Âµ) =argminwâˆˆWÎ¸L(w)âŠ‚WÎ¸andd/parenleftï£¬ig
wA,r
n,Wâˆ—
Î¸(Âµ)/parenrightï£¬ig
denotes the euclidean distance from the point
wA,r
nto the setWâˆ—
Î¸(Âµ).
With this characterization in mind, the following proposition provides a key tool to analyze the performance
of pruning algorithms. Under some conditions, it allows us to describe the asymptotic performance of any
pruning algorithm via some properties of a probability measure.
Proposition 2 LetAbe a pruning algorithm and râˆˆ(0,1]a compression level. Assume that there exists a
probability measure Î½ronDsuch that
âˆ€wâˆˆWÎ¸,LA,r
n(w)â†’EÎ½râ„“(yout(X;w),Y)a.s. (5)
Then, denotingWâˆ—
Î¸(Î½r) =argminwâˆˆWÎ¸EÎ½râ„“(yout(X;w),Y)âŠ‚WÎ¸, we have that
d/parenleftï£¬ig
wA,r
n,Wâˆ—
Î¸(Î½r)/parenrightï£¬ig
â†’0a.s.
Condition Eq. (5) assumes the existence of a limiting probability measure Î½rthat represents the distribution
of the pruned dataset in the limit of infinite sample size. In Section 3, for a large family of pruning algorithms
called score-based pruning algorithms (a formal definition will be introduced later), we will demonstrate the
existence of such limiting probability measure and derive its exact expression.
Let us now derive two important corollaries; the first gives a sufficient condition for an algorithm to be valid,
and the second a necessary condition. From Proposition 1 and Proposition 2, we can deduce that a sufficient
condition for an algorithm to be valid is that Î½r=Âµsatisfies equation (5). We say that such a pruning
algorithm is consistent .
5Here the original cost refers to the training cost of the model with the full dataset.
5Published in Transactions on Machine Learning Research (11/2023)
Definition 3 (Consistent Pruning Algorithms) We say that a pruning algorithm Ais consistent at
compression level râˆˆ(0,1]if and only if it satisfies
âˆ€wâˆˆWÎ¸,LA,r
n(w)â†’EÂµ[â„“(yout(x,w),y)] =L(w)a.s. (6)
We say thatAis consistent if it is consistent at any compression level râˆˆ(0,1].
Corollary 1 A consistent pruning algorithm Aat a compression level râˆˆ(0,1]is also valid at compression
levelr.
A simple application of the law of large numbers implies that Randompruning is consistent and hence valid
for any generating process and learning task satisfying our general assumptions.
We bring to the readerâ€™s attention that consistency is itself a property of practical interest. Indeed, it not
only ensures that the generalization gap of the learned model vanishes, but it also allows the practitioner to
accurately estimate the generalization error of their trained model from the selected subset. For instance,
consider the case where the practitioner is interested in Khyper-parameter values Î¸1,...,Î¸K; these can
be different neural network architectures (depth, width, etc.). Using a pruning algorithm A, they obtain
a trained model wA,r
n(Î¸k)for each hyper-parameter Î¸k, with corresponding estimated generalization error
LA,r
n/parenleftï£¬ig
wA,r
n(Î¸k)/parenrightï£¬ig
.Hence, the consistency property would allow the practitioner to select the best hyper-
parameter value based on the empirical loss computed with the set of retained points (or a random subset of
which used for validation). From Proposition 1 and Proposition 2, we can also deduce a necessary condition
for an algorithm satisfying (5) to be valid:
Corollary 2 LetAbe any pruning algorithm and râˆˆ(0,1], and assume that (5)holds for a given probability
measureÎ½ronD. IfAis valid, thenWâˆ—
Î¸(Î½r)âˆ©Wâˆ—
Î¸(Âµ)Ì¸=âˆ…; or, equivalently,
min
wâˆˆWâˆ—
Î¸(Î½r)L(w) = min
wâˆˆWL(w).
Corollary2willbeakeyingredientintheproofsonthenon-validityofagivenpruningalgorithm. Specifically,
for all the non-validity results stated in this paper, we prove that Wâˆ—
Î¸(Î½r)âˆ©Wâˆ—
Î¸(Âµ) =âˆ…. In other words, none
of the minimizers of the original problem is a minimizer of the pruned one, and vice-versa.
3 Score-Based Pruning Algorithms and their Limitations
3.1 Score-based Pruning Algorithms
A standard approach to define a pruning algorithm is to assign to each sample zi= (xi,yi)a scoregi=g(zi)
according to some score function g, wheregis a mapping from DtoR.gis also called the pruning criterion.
The score function gcaptures the practitionerâ€™s prior knowledge of the relative importance of each sample.
This function can be defined using a teacher model that has already been trained, for example. In this
work, we use the convention that the lower the score, the more relevant the example. One could of course
adopt the opposite convention by considering âˆ’ginstead ofgin the following. We now formally define this
category of pruning algorithms, which we call score-based pruning algorithms.
Definition 4 (Score-based Pruning Algorithm ( SBPA))LetAbe a data pruning algorithm. We say
thatAis a score-based pruning algorithm ( SBPA) if there exists a function g:Dâ†’ Rsuch that for all
ZâˆˆC, râˆˆ(0,1), we have thatA(Z,r) ={zâˆˆZ,s.t.g(z)â‰¤gr|Z|},wheregr|Z|is(r|Z|)thorder statistic
of the sequence (g(z))zâˆˆZ(first order statistic being the smallest value). The function gis called the score
function.
A significant number of existing data pruning algorithms are score-based (for example
Coleman2020Uncertainty ; Paul, Ganguli, and Dziugaite, 2021; Ducoffe and Precioso, 2018; Sorscher
et al., 2022), among which the recent approaches for modern machine learning. One of the key benefits of
6Published in Transactions on Machine Learning Research (11/2023)
these methods is that the scores are computed independently; these methods are hence parallelizable, and
their complexity scales linearly with the data size (up to log terms). These methods are tailored for the
abundant data regime, which explains their recent gain in popularity.
Naturally, the result of such a procedure highly depends on the choice of the score function g, and different
choices ofgmight yield completely different subsets. The choice of the score function in Definition 4 is not
restricted, and there are many scenarios in which the selection of the score function gmay be problematic.
For example, if ghas discontinuity points, this can lead to instability in the pruning procedure, as close data
points may have very different scores. Another problematic scenario is when gassigns the same score to a
large number of data points. To avoid such unnecessary complications, we define adaptedpruning criteria
as follows:
Definition 5 (Adapted score function) Letgbe a score function corresponding to some pruning algo-
rithmA. We say that gis an adapted score function if gis continuous and for any câˆˆg(D) :={g(z),zâˆˆD},
we haveÎ»(gâˆ’1({c})) = 0, whereÎ»is the Lebesgue measure on D.
In the rest of the section, we will examine the properties of SBPAalgorithms with an adapted score function.
3.2 Asymptotic Behavior of SBPA
Asymptotically, SBPAalgorithms have a simple behavior that mimics rejection algorithms. We describe this
in the following result.
Proposition 3 (Asymptotic behavior of SBPA)LetAbe aSBPAalgorithm and let gbe its corresponding
adapted score function. Consider a compression level râˆˆ(0,1). Denote by qrtherthquantile of the random
variableg(Z)whereZâˆ¼Âµ. DenoteAr={zâˆˆD|g(z)â‰¤qr}. Almost surely, the empirical measure of the
retained data samples converges weakly to Î½r=1
rÂµ|Ar, whereÂµ|Aris the restriction of Âµto the setAr. In
particular, we have that
âˆ€wâˆˆWÎ¸,LA,r
n(w)â†’EÎ½râ„“(yout(X;w),Y)a.s.
The result of Proposition 3 implies that in the abundant data regime, a SBPAalgorithmAacts similarly to a
deterministic rejection algorithm, where the samples are retained if they fall in Ar, and removed otherwise.
The first consequence is that a SBPAalgorithmAis consistent at compression level rif and only if
âˆ€wâˆˆWÎ¸,E1
rÂµ|Arâ„“(yout(X;w),Y) =EÂµâ„“(yout(X;w),Y), (7)
The second consequence is that SBPAalgorithms ignore entire regions of the data space, even when we have
access to unlimited data, i.e. nâ†’âˆ. Moreover, the ignored region can be made arbitrarily large for small
enough compression levels. Therefore, we expect that the generalization performance will be affected and
that the drop in performance will be amplified with smaller compression levels, regardless of the sample size
n. This hypothesis is empirically validated (see Guo, B. Zhao, and Bai, 2022 and Section 5).
In the rest of the section, we investigate the fundamental limitations of SBPAin terms of consistency and
validity; we will show that under mild assumptions, for any SBPAalgorithm with an adapted score function,
there exist compression levels rfor which the algorithm is neither consistent nor valid. Due to the prevalence
of classification problems in modern machine learning, we focus on the binary classification setting and give
specialized results in Section 3.3. In Section 3.4, we provide a different type of non-validity results for more
general problems.
3.3 Binary Classification Problems
In this section, we focus our attention on binary classification problems. The predictions and labels are in
Y= [0,1]. DenotePBthe set of probability distributions on XÃ—{ 0,1}, such that the marginal distribution
on the input space Xis continuous (absolutely continuous with respect to the Lebesgue measure on X) and
for which
pÏ€:xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’PÏ€(Y= 1|X=x)
is upper semi-continuous for any Ï€âˆˆPB. We further assume that:
7Published in Transactions on Machine Learning Research (11/2023)
(i) the loss is non-negative and that â„“(y,yâ€²) = 0if and only if y=yâ€².
(ii) Forqâˆˆ[0,1],yâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’qâ„“(y,1) + (1âˆ’q)â„“(y,0)has a unique minimizer, denoted yâˆ—
qâˆˆ[0,1], that is
increasing with q.
These two assumptions are generally satisfied in practice for the usual loss functions, such as the â„“1,â„“2,
Exponential or Cross-Entropy losses, with the notable exception of the Hinge loss for which (ii) does not
hold.
Under mild conditions that are generally satisfied in practice, we show that no SBPAalgorithm is consistent.
We first define a notion of universal approximation.
Definition 6 (Universal approximation) A family of continuous functions Î¨has the universal approx-
imation property if for any continuous function f:Xâ†’YandÏµ>0, there exists ÏˆâˆˆÎ¨such that
maxxâˆˆX|f(x)âˆ’Ïˆ(x)|â‰¤Ïµ
The next proposition shows that if the set of all models considered âˆªÎ¸âˆˆÎ˜MÎ¸has the universal approximation
property, then no SBPAalgorithm is consistent.
Theorem 1 Consider any generating process for binary classification ÂµâˆˆPB. LetAbe any SBPAalgorithm
with an adapted score function. If âˆªÎ¸MÎ¸has the universal approximation property and the loss satisfies
assumption (i), then there exist hyper-parameters Î¸âˆˆÎ˜for which the algorithm is not consistent.
Even though consistency is an important property, a pruning algorithm can still be valid without being
consistent. Inthisclassificationsetting, wecanfurthershowthat SBPAalgorithmsalsohavestronglimitations
in terms of validity.
Theorem 2 Consider any generating process for binary classification ÂµâˆˆPB. LetAbe a SBPAwith an
adapted score function gthat depends on the labels6. IfâˆªÎ¸MÎ¸has the universal approximation property and
the loss satisfies assumptions (i) and (ii), then there exist hyper-parameters Î¸1,Î¸2âˆˆÎ˜andr0âˆˆ(0,1)such
that the algorithm is not valid for râ‰¤r0for any hyper-parameter Î¸such thatWÎ¸1âˆªWÎ¸2âŠ‚WÎ¸.
This theorem sheds light on a strong limitation of SBPAalgorithms for which the score function depends on
the labels: it states that any solution of the pruned program will induce a generalization error strictly larger
than with random pruning in the abundant data regime. The proof builds on Corollary 2; we show that
for such hyper-parameters Î¸, the minimizers of the pruned problem and the ones of the original (full data)
problem do not intersect, i.e.
Wâˆ—
Î¸(Î½r)âˆ©Wâˆ—
Î¸(Âµ) =âˆ….
SBPAalgorithms usually depend on the labels ( Coleman2020Uncertainty ; Paul, Ganguli, and Dziugaite,
2021; DucoffeandPrecioso, 2018)andTheorem2applies. InSorscheretal., 2022, theauthorsalsoproposeto
use a SBPAthat does not depend on the labels. For such algorithms, the acceptance region Aris characterized
by a corresponding input acceptance region Xr.SBPAindependent of the labels have a key benefit; the
conditional distribution of the output is not altered given that the input is in Xr. Contrary to the algorithms
depending on the labels, the performance will not necessarily be degraded for any generating distribution
given that the family of models is rich enough. It remains that the pruned data give no information outside
ofXr, andyoutcan take any value in X\Xrwithout impacting the pruned loss. Hence, these algorithms can
create new local/global minima with poor generalization performance. Besides, the non-consistency results
of this section and the No-Free-Lunch result presented in Section 3.4 do apply for SBPAindependent of the
labels. For these reasons, we believe that calibration methods (see Section 4) should also be employed for
SBPAindependent of the labels, especially with small compression ratios.
6The score function gdepends on the labels if there exists an input xin the support of the distribution of the input Xand
for whichg(x,0)Ì¸=g(x,1)andP(Y= 1|X=x)âˆˆ(0,1)(both labels can happen at input x)
8Published in Transactions on Machine Learning Research (11/2023)
Applications: Neural Networks
To exemplify the utility of Theorem 1 and Theorem 2, we leverage the existing literature on the universal
approximation properties of neural networks to derive the important corollaries stated below
Definition 7 For an activation function Ïƒ, a real number R > 0, and integers H,Kâ‰¥1, we denote by
FFNNÏƒ
H,K(R)the set of fully-connected feed-forward neural networks with Hhidden layers, each with K
neurons with all weights and biases in [âˆ’R,R].
Corollary 3 (Wide neural networks) LetÏƒbe any continuous non-polynomial function that is continu-
ously differentiable at (at least) one point, with a nonzero derivative at that point. Consider any generating
processÂµâˆˆPB. For any SBPAwith adapted score function and Hâ‰¥1, there exists a radius R0and a width
K0such that the algorithm is not consistent on FFNNÏƒ
H,K(R)for anyKâ‰¥K0andRâ‰¥R0. Besides, if
the score function depends on the labels, then it is also not valid on FFNNÏƒ
H,K(R)for anyKâ‰¥Kâ€²
0and
Râ‰¥Râ€²
0.
Corollary 4 (Deep neural networks) Consider a width Kâ‰¥dx+ 2. LetÏƒbe any continuous non-
polynomial function that is continuously differentiable at (at least) one point, with a nonzero derivative
at that point. Consider any generating process ÂµâˆˆPB. For any SBPAwith an adapted score function,
there exists a radius R0and a number of hidden layers H0such that the algorithm is not consistent on
FFNNÏƒ
H,K(R)for anyHâ‰¥H0andRâ‰¥R0. Besides, if the score function depends on the labels, then it is
also not valid on FFNNÏƒ
H,K(R)for anyHâ‰¥Hâ€²
0andRâ‰¥Râ€²
0
A similar result for convolutional architectures is provided in Appendix C. To summarize, these corollaries
showthatforlargeenoughneuralnetworkarchitectures, any SBPAisnon-consistent. Besides, forlargeenough
neural network architectures, any SBPAthat depends on the label is non-valid, and hence a performance gap
should be expected even in the abundant data regime.
3.4 General Problems
In the previous section, we leveraged the universal approximation property and proved non-validity and
non-consistency results that hold for any data-generating process. In this section, we show a different No-
free-Lunch result in the general setting presented in Section 2. This result does not require the universal
approximation property. More precisely, we show that under mild assumptions, given any SBPAalgorithm,
we can always find a data distribution Âµsuch that the algorithm is not valid (Definition 2). Since random
pruning is valid for any generating process, this means that there exist data distributions for which the SBPA
algorithm provably underperforms random pruning in the abundant data regime.
ForKâˆˆNâˆ—, letPK
Cdenote the set of generating processes for K-classes classification problems, for which
the inputXis a continuous random variable7, and the output Ycan take one of Kvalues inY(the same
set of values for all Ï€âˆˆPK
C). Similarly, denote PR, the set of generating processes for regression problems
for which both the input and output distributions are continuous. Let Pbe any set of generating processes
introduced previously for regression or classification (either P=PK
Cfor someK, orP=PR). In the
next theorem, we show that under minimal conditions, there exists a data generating process for which the
algorithms is not valid.
Theorem 3 LetAbe a SBPAwith an adapted score function. For any hyper-parameter Î¸âˆˆÎ˜, if there exist
(x1,y1),(x2,y2)âˆˆDsuch that
argminwâˆˆWÎ¸â„“(yout(x1;w),y1)âˆ©argminwâˆˆWÎ¸â„“(yout(x2;w),y2) =âˆ…, (H1)
then there exists r0âˆˆ(0,1)and a generating process ÂµâˆˆPfor which the algorithm is not valid for râ‰¤r0.
The rigorous proof of Theorem 3 requires careful manipulations of different quantities, but the intuition is
rather simple. Fig. 3 illustrates the main idea of the proof. We construct a distribution Âµwith the majority
7In the sense that the marginal of the input is dominated by the Lebesgue measure
9Published in Transactions on Machine Learning Research (11/2023)
ğ‘§2 ğ‘§1
ğ‘¤ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘§1ğ‘ğ‘›ğ‘‘ğ‘§2ğ‘ ğ‘¢ğ‘â„ğ‘¡â„ğ‘ğ‘¡ğ‘”ğ‘§1<ğ‘”(ğ‘§2)
ğ‘ƒğ‘¢ğ‘¡ğ‘¡â„ğ‘’ğ‘ğ‘¢ğ‘™ğ‘˜ğ‘œğ‘“ğœ‡ğ‘ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ğ‘§2ğ‘ƒğ‘¢ğ‘¡ğ‘ğ‘ ğ‘šğ‘ğ‘™ğ‘™ğ‘šğ‘ğ‘ ğ‘ ğ‘œğ‘“ğœ‡ğ‘ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ğ‘§1
Figure 3: Graphical sketch of the proof of Theorem 3. The surface represents the loss function f(z,w) =
â„“(yout(x),y)in 2D, where z= (x,y).
Data SBPA Subset
SBPA-CPx Subset
SBPACalibration  
Protocol (CPx)Standard  
method
Our proposed
methodData
Figure 4: An illustration of how the calibration protocols modify SBPAalgorithms.
of the probability mass concentrated around a point where the value of gis not minimal. Consequently,
for sufficiently small r, the distribution of the retained samples will significantly differ from the original
distribution. This shift in data distributions causes the algorithm to be non-valid. We see in the next
section how we can solve this issue via randomization. Finally, notice that Eq. (H1) is generally satisfied in
practice since usually for two different examples (x1,y1)and(x2,y2)in the datasets, the global minimizers
ofâ„“(yout(x1;w),y1)andâ„“(yout(x2;w),y2)are different.
4 Solving Non-Consistency via Randomization
We have seen in Section 3 that SBPAalgorithms inherently transform the data distribution by asymptotically
rejecting all samples in D\Ar. These algorithms are prone to inconsistency; the transformation of the
data distribution translates to a distortion of the loss landscape, potentially leading to a deterioration of
the generalization error. This effect is exacerbated for smaller compression ratios ras the acceptance region
becomes arbitrarily small and concentrated.
10Published in Transactions on Machine Learning Research (11/2023)
With this in mind, one can design practical solutions to mitigate the problem. For illustration, we propose
to resort to a Calibration Protocol to retain information from the previously discarded region D\Ar.The
calibration protocols can be thought of as wrapper modules that can be applied on top of any SBPAalgorithm
to solve the consistency issue through randomization (see Fig. 4 for a graphical illustration). Specifically, we
split the data budget rninto two parts: the first part, allocated for the signal, leverages the knowledge from
theSBPAand its score function g. The second part, allocated for exploration , accounts for the discarded
region and consists of a subset of the rejected points, selected uniformly at random. In other words, we
writer=rsignal +rexploration.With standard SBPAprocedures, rexploration = 0.We defineÎ±=rsignal
rthe
proportion of signal in the overall budget. Accordingly, the set of retained points can be expressed as
Â¯A(Dn,r,Î±) =Â¯As(Dn,r,Î±)âˆªÂ¯Ae(Dn,r,Î±),
where Â¯Adenotes the calibrated version of A, and the indices â€˜sâ€™ and â€˜eâ€™ refer to signal and exploration
respectively. In this work, we consider the simplest approach. The â€œsignal subset" is composed of the Î±rn
points with the highest importance according to g, i.e. Â¯As(Dn,r,Î±) =A(Dn,rÎ±). The â€œexploration subset",
Â¯Ae(Dn,r,Î±)is composed on average of (1âˆ’Î±)rnpoints selected uniformly at random from the remaining
samplesDn\A(Dn,rÎ±), each sample being retained with probability pe=(1âˆ’Î±)r
1âˆ’Î±r, independently. The
calibrated loss is then defined as a weighted sum of the contributions of the signal and exploration budgets,
LÂ¯A,r,Î±
n(w) =1
nï£«
ï£­Î³s/summationdisplay
zâˆˆÂ¯As(Dn,r,Î±)f(z;w) +Î³e/summationdisplay
zâˆˆÂ¯Ae(Dn,r,Î±)f(z;w)ï£¶
ï£¸ (8)
wheref(z;w) =â„“(yout(x),y)forz= (x,y)âˆˆDandwâˆˆWÎ¸.The weights Î³sandÎ³eare chosen so that the
calibrated procedure is consistent; they are inversely proportional to the probability of acceptance within
each region:
Î³s= 1
Î³e=1âˆ’Î±r
(1âˆ’Î±)r
Proposition 4 hereafter states that any SBPAcalibrated with this procedure is made consistent as long as a
non-zero budget is allocated to exploration. For this reason, we refer to this method as the Exact Calibration
protocol (EC). The proof builds on an adapted version of the law of large numbers for sequences of dependent
variables which we prove in the Appendix (Theorem 7).
Proposition 4 (Consistency of Exact Calibration+ SBPA)LetAbe a SBPAalgorithm. Using the Exact
Calibration protocol with signal proportion Î±, the calibrated algorithm Â¯Ais consistent if 1âˆ’Î±>0, i.e. the
exploration budget is not null. Besides, under the same assumption 1âˆ’Î± > 0, the calibrated loss is an
unbiased estimator of the generalization loss at any finite sample size n>0,
âˆ€wâˆˆWÎ¸,âˆ€râˆˆ(0,1),ELÂ¯A,r,Î±
n(w) =L(w).
The proposed EC protocol offers a simple yet effective approach to address the challenges of non-consistency
andnon-validity. Itcanbeseamlesslyappliedinconjunctionwithany SBPA.Thecoreconceptrevolvesaround
the implementation of soft-pruning: any data point is assigned a non-zero selection probability. Samples
with lower scores are given a higher acceptance rate. The contribution of each accepted data point to the
loss is then weighted accordingly. The EC protocol embodies one specific implementation of soft-pruning,
offering the advantage of a single interpretable tuning parameter, the signal proportion Î±âˆˆ[0,1]. By setting
Î±to 1 or 0, one can recover the SBPAandRandompruning as extreme cases.
Proposition 4 states that any SBPAcalibrated with EC is made consistent and valid as long as some budget
is allocated to exploration. This is empirically validated in Section 5 where we show promising results on a
Toy example (Logistic regression) and other image tasks. However, the exact calibration protocol does not
systematically allow to outperform random pruning.
11Published in Transactions on Machine Learning Research (11/2023)
Figure 5: Data distribution alteration due to pruning in the logistic regression setting. Here we use GraNd
as the pruning algorithm. Blue points correspond to Yi= 0, red points correspond to Yi= 1.
Besides, it is worth noting that different implementations of the same general recipe can be considered. It
is reasonable to expect that more tailored protocols can be designed to suit specific pruning algorithms and
problems. Nevertheless, addressing these questions falls outside the scope of the present work which focus
is to provide a framework to analyse data pruning algorithms, as well as to identify and understand their
fundamental limitations. We propose the EC protocol to illustrate how this understanding allows to design
simple yet efficient solutions to address these limitations.
5 Experiments
5.1 Logistic Regression
Figure 6: Evolution of the performance gap
as the data budget m=rnincreases (aver-
age over 10 runs).We illustrate the main results of this work on a logistic regres-
sion task. We consider the following data-generating process
Xiâˆ¼ U/parenleftbig
[âˆ’2.5,2.5]dx/parenrightbig
Yi|Xiâˆ¼ B/parenleftbigg1
1 +eâˆ’wT
0Xi/parenrightbigg
,
wherew0= (1,...,1)âˆˆRdx,UandBare respectively the
uniform and Bernoulli distributions. The class of models is
given by
M=/braceleftbigg
yout(Â·;w) :xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’1
1 +eâˆ’wTXi|wâˆˆW/bracerightbigg
,
whereW= [âˆ’10,10]dx. We train the models using stochastic
gradient descent with the cross entropy loss. For performance
analysis, we take dx= 20andn= 106. For the sake of
visualization, we take dx= 1when we plot the loss landscapes (so that the parameter wis univariate) and
dx= 2when we plot the data distributions.
We use GraNd(Paul, Ganguli, and Dziugaite, 2021) as a pruning algorithm in a teacher-student setting. For
simplicity, we use the optimal model to compute the scores, i.e.
g(Xi,Yi) =âˆ’âˆ¥âˆ‡wâ„“(yout(Xi,w0),Yi)âˆ¥2,
which is proportional to âˆ’(yout(Xi;w0)âˆ’Yi)2.Notice that in this setting, GraNdandEL2N(Paul, Ganguli,
and Dziugaite, 2021) are equivalent8. We bring to the readerâ€™s attention that r= 1corresponds to Random
pruning in our plots. Indeed, we compare models as a function of the data budget m=rn. But notice that
8This is different from the original version of GraNd, here, we have access to the true generating process, which is not the
case in practice.
12Published in Transactions on Machine Learning Research (11/2023)
Figure 8: Pruned data distribution for GraNdcalibrated with exact protocol with Î±= 90%. The top figures
represent the â€™signalâ€™ points. The bottom figures represent the â€™explorationâ€™ points. Blue markers correspond
toYi= 0, and red markers correspond to Yi= 1.
in the case of Random, for a given m, the values of randndo not affect the distribution of the accepted
datapoints, and this distribution is always the same as the original data distribution, i.e. when r= 1.
Distribution shift and performance degradation: In Section 3, we have seen that the pruning
algorithm induces a shift in the data distribution (Fig. 5). This alteration is most pronounced when ris
small; Forr= 20%, the bottom-left part of the space is populated by Y= 1and the top-right by Y= 0.
Notice that it was the opposite in the original dataset ( r= 1). This translates into a distortion of the loss
landscape and the optimal parameters wA,r
nof the pruned empirical loss becomes different from w0= 1.
Hence, even when a large amount of data is available, the performance gap does not vanish (Fig. 6).
Figure 7: Evolution of the performance gap
with calibrated GraNdas the data budget
m=rnincreases (average over 10 runs).Calibration with the exact protocol: To solve the distri-
bution shift, we resort to the exact protocol with Î±= 90%. In
otherwords, 10%ofthebudgetisallocatedtoexploration. The
signal points (top images in Fig. 8) are balanced with the ex-
ploration points (bottom images in Fig. 8). Even though there
are nine times fewer of them, the importance weights allow to
correct the distribution shift, as depicted in Fig. 2 (Introduc-
tion): the empirical losses overlap for all values of r, even for
small values for which the predominant labels are swapped (for
exampler= 20%). Hence, the performance gap vanishes when
enough data is available at any compression ratio (Fig. 7).
Impact of the quality of the pruning algorithm: The
calibration protocols allow the performance gap to eventually
vanish if enough data is provided. However, from a practical
point of view, a natural further requirement is that the pruning
method should be better than Random, in the sense that for a given finite budget rn, the error with the
pruning algorithm should be lower than the one of Random. We argue that this mostly decided by the
quality of the original SBPAand its score function. Let us take a closer look at what happens in the
13Published in Transactions on Machine Learning Research (11/2023)
logistic regression case. For a given Xi, denote ËœYithe most probable label for the input, i.e. /tildewideYi= 1if
yout(Xi,w0)>1/2, and/tildewideYi= 0otherwise. As explained, in this setting, GraNdis equivalent to using the
score function g(Zi) =âˆ’|Yiâˆ’yout(Xi;w0)|. For a given value of r, considerqrtherthquantile of g(Z).
Notice that g(Z)â‰¤qrif and only if
/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleyout(Xi;w0)âˆ’1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ‰¤qr+1
2/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Condition 1or/parenleftbigg/vextendsingle/vextendsingle/vextendsingle/vextendsingleyout(Xi;w0)âˆ’1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingle>/vextendsingle/vextendsingle/vextendsingle/vextendsingleqr+1
2/vextendsingle/vextendsingle/vextendsingle/vextendsingleandYiÌ¸=/tildewideYi/parenrightbigg
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Condition 2
Therefore, the signal acceptance region is the union of two disjoint sets. The first set is composed of all
samples that are close to the decision boundary, i.e. samples for which the true conditional probability
yout(Xi;w0)is close to 1/2. The second set is composed of samples that are further away from the decision
boundary, but the realized labels need to be the least probable ones ( YiÌ¸=/tildewideYi). These two subsets are visible
in Figs. 5 and 8 for r= 70%and even more for r= 50%. The signal points can be divided into two sets:
1. thesetofpointsclosetotheboundaryline y=âˆ’x, wherethecolorsmatchtheoriginalconfigurations
(mostly blue points under the line, red points over the line)
2. the set of points far away from the boundary line, for which the colors are swapped (only red under
the line, blue over the line).
Figure 9: Evolution of the performance gap
for a small value r= 0.1forGraNdand its
calibrated version with Î±= 90%.Hence, the signal subset corresponding to Condition 1 gives
valuable insights; it provides finer-grained visibility in the crit-
ical region. However, the second subset is unproductive, as it
only retains points that are not representative of their region.
Calibration allows mitigating the effect of the second subset
while preserving the benefits of the first subset; in Fig. 7, we
can see that the calibrated GraNdoutperforms random pruning
(which corresponds to the r= 1curve), requiring on average
two to three times fewer data to achieve the same generaliza-
tion error. However, as rbecomes lower, qrwill eventually fall
underâˆ’1/2, and the first subset becomes empty (for example,
r= 0.2in Fig. 8). Therefore, when rbecomes small, GraNd
does not bring valuable information anymore (for this particu-
lar setting). In Fig. 9, we compare GraNdand Calibrated GraNd
(with the exact protocol) to Randomwithr= 10%. We can see
that thanks to the calibration protocol, the performance gap
will indeed vanish if enough data is available. However, Randompruning outperforms both versions of GraNd
at this compression level. This underlines the fact that for high compression levels, (problem-specific) high-
quality pruning algorithms and score functions are required. Given the difficulty of the task, we believe that
in the high compression regime ( râ‰¤10%here), one should allocate a larger budget to random exploration
(take smaller values of Î±).
14Published in Transactions on Machine Learning Research (11/2023)
5.2 Scaling Laws with Neural Networks
103105
training_size10203040Error (%)
Random
r=0.1
r=0.2
r=0.4
r=0.8
r=1.0
103105
training_size
Linear
GraNd
103105
training_size102030405060Error (%)
103105
training_size
103105
training_size
NonLinear+Noise
Calibrated GraNd
103105
Subset size1020304050Error (%)
103105
Subset size
103105
Subset size
NonLinear
Figure 10: Test error on a 3-layers MLP (details are
provided in Appendix D) on different pruned datasets
for compression levels râˆˆ{0.1,0.2,0.4,0.8,1}where
thepruningprocedureisperformedwith Randomprun-
ing or GraNd. The caser= 1corresponds to no prun-
ing. Inalltheexperiments, thenetworkistraineduntil
convergence.The distribution shift is the primary cause of the
observed alteration in the loss function, resulting
in the emergence of new minima. Gradient descent
could potentially converge to a bad minimum, in
which case the performance is significantly affected.
To illustrate this intuition, we report in Fig. 10 the
observed scaling laws for three different synthetic
datasets. Let Ntrain = 106,Ntest= 3Â·104,d= 1000,
andm= 100. Thedatasets are generated as follows:
1.Lineardataset: we first generate a random vec-
torWâˆ¼N (0,dâˆ’1Id). Then, we generate Ntrain
training samples and Ntesttest samples with the
ruley= 1{WâŠ¤x>0}, wherexâˆˆRdis simulated from
N(0,Id).
2.NonLinear dataset (Non-linearity): we first gen-
erate a random matrix Winâˆ¼ N (0,dâˆ’1IdÃ—m)âˆˆ
RdÃ—mand a random vector Woutâˆ¼N (0,mâˆ’1Im).
The samples are then generated with the rule y=
1{WoutâŠ¤Ï•(WâŠ¤
inx)}, wherexâˆˆRdis simulated from
N(0,Id), andÏ•is the ReLU activation function.9
3.NonLinear+Noisy dataset: we first generate
a random vector Wâˆ¼ N (0,dâˆ’1Id). Then, we
generateNtraintraining samples and Ntesttest
samples with the rule y= 1{sin(WâŠ¤x+0.3Ïµ)>0}, where
xâˆˆRdis simulated from N(0,Id)andÏµis simulated from N(0,1)and â€˜sinâ€™ refers to the sine function.
In Fig. 10, we compare the test error of an 3-layers MLP trained on different subsets generated with either
Randompruning, or GraNd. As expected, with random pruning, the results are consistent regardless of the
compression level ras long as the subset size is the same. With GraNdhowever, the results depend on
the difficulty of the dataset. For the linear dataset, it appears that we can indeed beat the power law
scaling, provided that we have access to enough data. In contrast, GraNdseems to perform poorly on the
nonlinear and noisy datasets in the high compression regime. This is due to the emergence of new local
(bad) minima as rdecreases as evidenced in Fig. 1. Calibrated with the exact protocol, GraNdbecomes
valid: we can see that at any compression rate, the error converges to its minimum, which was not the
case forrâ‰¤20%. Whether calibration protocols can allow data pruning algorithms to beat the power law
scaling remains however an open question: further research is needed in this direction. It is also worth noting
that for the Nonlinear datasets, the scaling law pattern exhibits multi-phase behavior. For instance, for the
Nonlinear+Noisy dataset, we can (visually) identify two phases, each one of which follows a different power
law scaling pattern.
5.3 Image Tasks
Through our theoretical analysis, we have concluded that SBPAalgorithms are generally non-consistent.
This effect is most pronounced when the compression level ris small. In this case, the loss landscape can
be significantly altered due to the change in the data distribution caused by the pruning procedure. Given
aSBPAalgorithm, we argue that this alteration in distribution will inevitably affect the performance of the
model trained on the pruned subset, and for small r,Randompruning becomes more effective than the SBPA
algorithm.
9The ReLU activation function is given by Ï•(z) = max(z,0)forzâˆˆR. Here, we abuse the notation a bit and write
Ï•(z) = (Ï•(z1),...,Ï• (zm))forz= (z1,...,zm)âˆˆRm.
15Published in Transactions on Machine Learning Research (11/2023)
In the following, we empirically investigate this behaviour. We evaluate the performance of different SBPA
algorithms from the literature and confirm our theoretical predictions with empirical evidence. We consider
the following SBPAalgorithms:
â€¢GraNd(Paul, Ganguli, and Dziugaite, 2021): with this method, given a datapoint z= (x,y), the
score function gis given by g(z) =âˆ’Ewtâˆ¥âˆ‡wâ„“(yout(x,wt),y)âˆ¥2, whereyoutis the model output and
wtare the model parameters (e.g. the weights in a neural network) at training step t, and where
the expectation is taken with respect to random initialization. GraNdselects datapoints with the
highest average gradient norm (w.r.t to initialization).
â€¢Uncertainty (Coleman2020Uncertainty ): in this method, the score function is designed to cap-
ture the uncertainty of the model in assigning a classification label to a given datapoint10. Different
metrics can be used to measure this assignment uncertainty. We focus here on the entropy approach
in which case the score function gis given by g(z) =/summationtextC
i=1pi(x) log(pi(x))wherepi(x)is the model
output probability that xbelongs to class i. For instance, in the context of neural networks, we have
(pi(x))1â‰¤iâ‰¤C=Softmax (yout(x,wt)), wheretis the training step where data pruning is performed.
â€¢DeepFool (Ducoffe and Precioso, 2018): this method is rooted in the idea that in a classification
problem, data points that are nearest to the decision boundary are, in principle, the most valuable
for the training process. While a closed-form expression of the margin is typically not available,
the authors use a heuristic from the literature on adversarial attacks to estimate the distance to the
boundary. Specifically, given a datapoint z= (x,y), perturbations are added to the input xuntil
the model assigns the perturbed input to a different class. The amount of perturbation required to
change the label for each datapoint defines the score function in this case (see (Ducoffe and Precioso,
2018) for more details).
Weillustrate the limitationsof the SBPAalgorithms above forsmall r, and show that random pruning remains
a strong baseline in this case. We further evaluate the performance of our calibration protocols and show
that the signal parameter Î±can be tuned so that the calibrated SBPAalgorithms outperform random pruning
for smallr. We conduct our experiments using the following setup:
â€¢Datasets and architectures. Our framework is not constrained by the type of the learning task
or the model. However, for our empirical evaluations, we focus on classification tasks with neural
network models. We consider two image datasets: CIFAR10 with ResNet18 and CIFAR100 with
ResNet34 . More datasets and neural architectures are available in our code, which is based on
that of Guo, B. Zhao, and Bai, 2022. The code to reproduce all our experiments will be soon
open-sourced.
â€¢Training. WetrainallmodelsusingSGDwithadecayinglearningrateschedulethatwasempirically
selected following a grid search. This learning rate schedule was also used in Guo, B. Zhao, and Bai,
2022. More details are provided in Appendix D.
â€¢Selection epoch. The selection of the coreset can be performed at differnt training stages. We
consider data pruning at two different training epochs: 1, and 5. We found that going beyond epoch
5(e.g., using a selection epoch of 10) has minimal impact on the performance as compared to using
a selection epoch of 5.
â€¢Pruningmethods. Weconsiderthefollowingdatapruningmethods: Random, GraNd, DeepFool,
Uncertainty . In addition, we consider the pruning methods resulting from applying the proposed
exact calibration protocol to a given SBPAalgorithm. We use the notation SBPA-CP1 to refer to the
resulting method. For instance, DeepFool-CP1 refers to the method resulting from applying (EC)
toDeepFool .
10Uncertainty is specifically designed to be used for classification tasks. This means that it is not well-suited for other types
of tasks, such as regression.
16Published in Transactions on Machine Learning Research (11/2023)
(a) ResNet18 on CIFAR10
 (b) ResNet34 on CIFAR100
Figure 11: Test accuracy for different pruning methods, fractions r, signal parameters Î±, and selection epochs
(se= 1or5). Confidence intervals based on 3 runs are shown.
Poor performance of SBPAin the high compression regime: Fig. 11 shows the results of the data
pruning methods described above with ResNet18 onCIFAR10 and ResNet34 onCIFAR100 . As expected,
we observe a consistent decline in the performance of the trained model when the compression ratio ris
small, typically in the region r<0.3. More importantly, we observe that SBPAmethods ( GraNd,DeepFool ,
Uncertainty in orange) perform consistently worse than Randompruning (in green), confirming our hypoth-
esis. We also observe that amongst the three SBPAmethods, DeepFool is generally the best in the region of
interest ofrand competes with random pruning when the subset selection is performed at training epoch 1.
We noticed that in that setting DeepFool is close to random pruning.
Effect of the calibration protocol Our proposed calibration protocol aim to correct the bias by injecting
some randomness in the selection process and keeping (on average) only a fraction Î±of the SBPAmethod. We
notice that the calibration protocol applied to different SBPAconsistently boosts the performance in the high
compression regime, as can be observed in Fig. 11. Fig. 12 shows that the calibrated SBPAperform better
than Randompruning for specific choices of Î±. However, the difference is not always significant. Besides,
finding the optimal proportion of signal Î±can be difficult in practice.
17Published in Transactions on Machine Learning Research (11/2023)
(a) ResNet18 on CIFAR10
 (b) ResNet34 on CIFAR100
Figure 12: Test accuracy for different pruning methods, fractions r, and selection epochs ( se= 1or5). Best
Î±used for calibration. Different values of rmay have different Î±values.
6 Related Work
As we mentioned in the introduction. The topic of coreset selection has been extensively studied in classical
machine learning and statistics (Welling, 2009; Chen, Welling, and Smola, 2012; Feldman, Faulkner, and
Krause, 2011; Huggins, Campbell, and Broderick, 2016; Campbell and Broderick, 2019). These classical
approaches were either model-independent or designed for simple models (e.g. linear models). The recent
advances in deep learning has motivated the need for new adapted methods for these deep models. Many
approaches have been proposed to adapt to the challenges of the deep learning context. We will cover existing
methods that are part of our framework ( SBPAalgorithms) and others that fall under different frameworks
(non- SBPAalgorithms).
6.1 Score-based Methods
These can generally be categorized into four groups:
1. Geometry based methods: these methods are based on some geometric measure in the feature
space. The idea is to remove redundant examples in this feature space (examples that similar
representations). Examples include Herding ((Chen, Welling, and Smola, 2012)) which aims to
greedily select examples by ensuring that the centers of the coreset and that of the full dataset are
18Published in Transactions on Machine Learning Research (11/2023)
close. A similar idea based on the K-centroids of the input data was used in (Sener and Savarese,
2018; Agarwal et al., 2020; Sinha et al., 2020).
2. Uncertainty based methods: the aim of such methods is to find the most â€œdifficult" examples, defined
as the ones for which the model is the least confident. Different uncertainty measures can be used
for this purpose, see (Coleman et al., 2020) for more details.
3. Error based methods: the goal is to find the most significant examples defined as the ones that
contribute the most to the loss. In Paul, Ganguli, and Dziugaite, 2021, the authors consider the
second norm of the gradient as a proxy to find such examples. Indeed, examples with the highest
gradient norm tends to affect the loss more significantly (a first order Taylor expansion of the loss
function can explain the intuition behind this proxy). This can be thought of as a relaxation of
a Lipschitz-constant based pruning algorithm that was recently introduced in Ayed and Hayou,
2022. Another method consider keeping the most forgettable examples defined as those that change
the most often from being well classified to being mis-classified during the course of the training
(Toneva et al., 2019). Other methods in this direction consider a score function based on the relative
contribution of each example to the total loss over all training examples (see Bachem, Lucic, and
Krause, 2015; Munteanu et al., 2021).
4. Decision boundary based: although this can be encapsulated in uncertainty-based methods, the idea
behind these methods is more specific. The aim is to find the examples near the decision boundary,
the points for which the prediction has the highest variation (e.g. with respect to the input space,
Ducoffe and Precioso, 2018; Margatina et al., 2021).
6.2 Non- SBPAMethods
Other methods in the literature select the coreset based on other desirable properties. For instance, one
could argue that preserving the gradient is an important feature to have in the coreset as it would lead to
similar minima (Killamsetty, Sivasubramanian, Ramakrishnan, De, et al., 2021; Mirzasoleiman, Bilmes, and
Leskovec, 2020). Other work considered the problem of corset selection as a two-stage optimization problem
where the subset selection can be seen also as an optimization problem (Killamsetty, Sivasubramanian,
Ramakrishnan, and Iyer, 2021; Killamsetty, X. Zhao, et al., 2021). Other methods consider conisder the
likelihood and its connection with submodular functions in order to select the subset ( kaushal2021prism ;
Kothawade et al., 2021).
It is worth noting that there exist other approaches to data pruning that involve synthesizing a new dataset
with smaller size that preserves certain desired properties, often through the brute-force construction of
samples that may not necessarily represent the original data. These methods are known as data distillation
methods (see e.g. Wang et al., 2020; B. Zhao, Mopuri, and Bilen, 2021; B. Zhao and Bilen, 2021) However,
these methods have significant limitations, including the difficulty of interpreting the synthesized samples
and the significant computational cost. The interpretability issue is particularly a these approaches to use
in real-world applications, particularly in high-stakes fields such as medicine and financial engineering.
7 Discussion and Limitations
7.1 Extreme Scenarios
Our framework provides insights in the case where both nandrnare large. As a result, there are cases
where this framework is not applicable. We call these cases extreme scenarios.
Extreme scenario 1: small n.Our asymptotic analysis can provide insights when a sufficient number of
samples are available. In the scarce data regime (small n), our theoretical results may not accurately reflect
the impact of pruning on the loss function. It is worth noting, however, that this case is generally not of
practical interest as there is no benefit to data pruning when the sample size is small.
19Published in Transactions on Machine Learning Research (11/2023)
Extreme scenario 2: large nwithr= Î˜(nâˆ’1)).In this case, the â€œeffective" sample size after pruning
isrn= Î˜(1). Therefore, we cannot glean useful information from the asymptotic behavior of LA,r
nin this
case. It is also worth noting that the variance of LA,r
ndoes not vanish in the limit nâ†’âˆ,râ†’0withrn=Î³
fixed, and therefore the empirical mean does not converge to the asymptotic mean.
7.2 Asymptotic Results
Theorem 1 and 2, and the subsequent corollaries are asymptotic results. They essentially reveal the limita-
tions of SBPAfor "large enough models". We decided to take this direction to get results that are as general
as possible, showing that the discussed limitations will appear in most situations. Theorem 1 and 2 apply to
any configuration from a large variety of classes of models, SBPAs, generating processes, and loss functions.
The theory readily covers realistic architectures illustrated by Corollary 3, 4 and 5 (in the appendix) that
cover wide NN, deep neural NN, and convolutional NN with enough filters. However, these limitations could
appear for unrealistically large models. We acknowledge this drawback of the proposed theory and address
it in two ways. First, we experimentally show that for the usual settings (ResNet on Cifar), we already
observe this significant drop in the performance of SBPAs compared to random pruning. This also aligns
with other empirical observations (Guo, B. Zhao, and Bai, 2022). In addition, we provide Theorem 3 to
cover the cases where the class of functions is potentially not rich enough; even in that case, for any SBPA,
one can find datasets for which the pruning algorithm will fail for small compression ratios. Besides, to our
knowledge, the lines of work that allow one to derive explicit bounds for Neural Networks usually require
specific and often overly simplistic architectures (typically one hidden layer feed-forward). In our context,
we additionally expect similar strong restrictions to be required for the SBPAs and generating processes.
This could wrongfully lead practitioners to consider that using a different SBPAor class of models than the
one for which one could derive quantitative results would circumvent the limitations.
7.3 Overparameterized Regime
The scenario in which both the number of parameters pand the sample size ntend to infinity (e.g. with a
constant ratio Î³=p/n) holds practical significance. Our framework does not cover this case and we would
like to elucidate the main point at which our proof machinery encounters challenges under this scenario.
The main issue resides in understanding the asymptotic behaviour of SBPA algorithms in this context,
particularly the extension of Proposition 3. While we can establish concentration with pconstant and n
growing large, achieving this with both nandpgoing to infinity, especially when the underlying model is
a neural network, becomes generally intractable. Nonetheless, under certain supplementary assumptions, it
remains feasible to demonstrate concentration.
RecallthatinProposition3, weessentiallyuseavariationofthelawoflargenumberstoshowtheconvergence
in the infinite sample size limit ( nâ†’âˆ), whilep(and consequently, w) is fixed. However, in the scenario
where both pandntend to infinity, the dependency of wonpcomplicates matters, rendering the used
variation of the LLN inapplicable. An essential condition under such circumstances becomes the convergence
ofwin a certain sense as well, as pâ†’âˆ. For this purpose, a pertinent tool is LLN for Triangular Arrays,
which takes the shape of:
Consider a Triangular Array (Xn,i)iâˆˆ[n],nâ‰¥1of random variables such that for each n, the variables (Xn,i)iâˆˆ[n]
are iid with mean Âµn. Then, under some assumptions (bounded second moments) we have
nâˆ’1/parenleftï£¬iggn/summationdisplay
i=1Xn,iâˆ’Âµn/parenrightï£¬igg
â†’âˆ.
However, note that in our case, the terms Xn,i=â„“(yout(Xi;wp),Yi)1{iis accepted}are not necessarily inde-
pendent, and therefore more advanced LLN for trinagular arrays should be proven and used. We leave this
for future work.
20Published in Transactions on Machine Learning Research (11/2023)
7.4 Pruning Time Vs Training Time
In some cases, the pruning procedure might be compute-intensive, and requires more resources than the
actual training with the full dataset. This is the case when, for instance, the pruning criterion depends on
second order geometry (Hessian etc) and/or multiple perturbations of some quantity (DeepFool), or pruning
is performed in multi-shot settings (dynamical pruning). In this paper, we considered pruning criteria that
can be performed â€œone shotâ€ and rely on criteria that involve either gradients (GraNd) or network outputs
(Uncertainty) or perturbations of the network output (DeepFool). For GraNd and Uncertainty pruning, the
pruning time is typically less than the time required for 1 epoch of training. For DeepFool however, the
pruning time might be comparable to 10 epochs of training.
References
Agarwal, S., H. Arora, S. Anand, and C. Arora (2020). â€œContextual Diversity for Active Learningâ€. In:
European Conference on Computer Vision .
Aljundi, R., M. Lin, B. Goujaud, and Y. Bengio (2019). â€œGradient Based Sample Selection for Online
Continual Learningâ€. In: Advances in Neural Information Processing Systems .
Arkhangelâ€™skiË‡ Ä±, A. V. (2001). â€œFundamentals of General Topology: Problems and Exercisesâ€. In: pp. 123â€“124.
Ayed, F. and S. Hayou (2022). â€œThe Curse of (non)Convexity: The Case of an Optimization-Inspired Data
Pruning Algorithmâ€. In: I Canâ€™t Believe Itâ€™s Not Better Workshop: Understanding Deep Learning Through
Empirical Falsification .
Bachem, O., M. Lucic, and A. Krause (2015). â€œCoresets for Nonparametric Estimation - the Case of DP-
Meansâ€. In: International Conference on Machine Learning .
Campbell, T. and T. Broderick (2019). â€œAutomated Scalable Bayesian Inference via Hilbert Coresetsâ€. In:
The Journal of Machine Learning Research 20.1, pp. 551â€“588.
Chen, Y., M. Welling, and A. Smola (2012). Super-Samples from Kernel Herding . arXiv: 1203.3472 [cs.LG] .
Coleman, C., C. Yeh, S. Mussmann, B. Mirzasoleiman, P. Bailis, P. Liang, J. Leskovec, and M. Zaharia
(2020). â€œSelection via Proxy: Efficient Data Selection for Deep Learningâ€. In: International Conference on
Learning Representations .
Deimling, K. (2010). Nonlinear Functional Analysis . Courier Corporation.
Ducoffe, M. and F. Precioso (2018). Adversarial Active Learning for Deep Networks: a Margin Based Ap-
proach. arXiv: 1802.09841 [cs.LG] .
Feldman, D., M. Faulkner, and A. Krause (2011). â€œScalable Training of Mixture Models via Coresetsâ€. In:
Advances in neural information processing systems 24.
Guo, C., B. Zhao, and Y. Bai (2022). DeepCore: A Comprehensive Library for Coreset Selection in Deep
Learning . arXiv: 2204.08499 [cs.LG] .
Hernandez, D., J. Kaplan, T. Henighan, and S. McCandlish (2021). Scaling Laws for Transfer . arXiv: 2102.
01293 [cs.LG] .
Hestness, J., S. Narang, N. Ardalani, G. Diamos, H. Jun, H. Kianinejad, M. M. A. Patwary, Y. Yang, and
Y. Zhou (2017). Deep Learning Scaling is Predictable, Empirically . arXiv: 1712.00409 [cs.LG] .
Hoffmann, J., S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las Casas, L. A. Hen-
dricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy,
S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre (2022). â€œAn Empirical Analysis
of Compute-optimal Large Language Model Trainingâ€. In: Advances in Neural Information Processing
Systems.
Hornik, K. (1991). â€œApproximation Capabilities of Multilayer Feedforward Networksâ€. In: Neural Networks
4.2, pp. 251â€“257.
Huggins, J., T. Campbell, and T. Broderick (2016). â€œCoresets for Scalable Bayesian Logistic Regressionâ€.
In:Advances in Neural Information Processing Systems 29.
Kaplan, J., S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and
D. Amodei (2020). Scaling Laws for Neural Language Models . arXiv: 2001.08361 [cs.LG] .
Kidger, P. and T. Lyons (2020). â€œUniversal Approximation with Deep Narrow Networksâ€. In: Conference on
learning theory , pp. 2306â€“2327.
21Published in Transactions on Machine Learning Research (11/2023)
Killamsetty, K., D. Sivasubramanian, G. Ramakrishnan, A. De, and R. K. Iyer (2021). â€œGRAD-MATCH:
Gradient Matching based Data Subset Selection for Efficient Deep Model Trainingâ€. In: International
Conference on Machine Learning .
Killamsetty, K., D. Sivasubramanian, G. Ramakrishnan, and R. Iyer (2021). â€œGLISTER: Generalization
based Data Subset Selection for Efficient and Robust Learningâ€. In: Proceedings of the AAAI Conference
on Artificial Intelligence .
Killamsetty, K., X. Zhao, F. Chen, and R. Iyer (2021). RETRIEVE: Coreset Selection for Efficient and
Robust Semi-Supervised Learning . arXiv: 2106.07760 [cs.LG] .
Kingma, D. P. and J. Ba (2017). Adam: A Method for Stochastic Optimization . arXiv: 1412.6980 [cs.LG] .
Kothawade, S. N., N. A. Beck, K. Killamsetty, and R. K. Iyer (2021). â€œSIMILAR: Submodular Information
Measures Based Active Learning In Realistic Scenariosâ€. In: Advances in Neural Information Processing
Systems.
Margatina, K., G. Vernikos, L. Barrault, and N. Aletras (2021). Active Learning by Acquiring Contrastive
Examples . arXiv: 2109.03764 [cs.CL] .
Mirzasoleiman, B., J. Bilmes, and J. Leskovec (2020). â€œCoresets for Data-efficient Training of Machine
Learning Modelsâ€. In: International Conference on Machine Learning , pp. 6950â€“6960.
Munteanu, A., C. Schwiegelshohn, C. Sohler, and D. P. Woodruff (2021). On Coresets for Logistic Regression .
arXiv: 1805.08571 [cs.DS] .
Paul, M., S. Ganguli, and G. K. Dziugaite (2021). â€œDeep Learning on a Data Diet: Finding Important
Examples Early in Trainingâ€. In: Advances in Neural Information Processing Systems .
Robbins, H. and S. Monro (1951). â€œA Stochastic Approximation Methodâ€. In: The annals of mathematical
statistics , pp. 400â€“407.
Rosenfeld, J. S., A. Rosenfeld, Y. Belinkov, and N. Shavit (2020). â€œA Constructive Prediction of the Gener-
alization Error Across Scalesâ€. In: International Conference on Learning Representations .
Sener, O. and S. Savarese (2018). Active Learning for Convolutional Neural Networks: A Core-Set Approach .
arXiv: 1708.00489 [stat.ML] .
Sinha, S., H. Zhang, A. Goyal, Y. Bengio, H. Larochelle, and A. Odena (2020). â€œSmall-GAN: Speeding up
GAN Training using Core-Setsâ€. In: International Conference on Machine Learning .
Sorscher, B., R. Geirhos, S. Shekhar, S. Ganguli, and A. S. Morcos (2022). â€œBeyond Neural Scaling Laws:
Beating Power Law Scaling via Data Pruningâ€. In: Advances in Neural Information Processing Systems .
Toneva, M., A. Sordoni, R. T. des Combes, A. Trischler, Y. Bengio, and G. J. Gordon (2019). An Empirical
Study of Example Forgetting during Deep Neural Network Learning . arXiv: 1812.05159 [cs.LG] .
Varadarajan, V. S. (1958). â€œOn the Convergence of Sample Probability Distributionsâ€. In: SankhyÂ¯ a: The
Indian Journal of Statistics (1933-1960) 19.1/2, pp. 23â€“26.
Wang, T., J.-Y. Zhu, A. Torralba, and A. A. Efros (2020). Dataset Distillation . arXiv: 1811.10959 [cs.LG] .
Welling, M. (2009). â€œHerding dynamical weights to learnâ€. In: Proceedings of the 26th Annual International
Conference on Machine Learning , pp. 1121â€“1128.
Zhai, X., A. Kolesnikov, N. Houlsby, and L. Beyer (2022). â€œScaling Vision Transformersâ€. In: IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) .
Zhao, B. and H. Bilen (2021). â€œDataset Condensation with Differentiable Siamese Augmentationâ€. In: Inter-
national Conference on Machine Learning .
Zhao, B., K. R. Mopuri, and H. Bilen (2021). â€œDataset Condensation with Gradient Matchingâ€. In: Inter-
national Conference on Learning Representations .
Zhou, D.-X. (2020). â€œUniversality of Deep Convolutional Neural Networksâ€. In: Applied and computational
harmonic analysis 48.2, pp. 787â€“794.
8 Acknowledgement
We would like to thank the authors of DeepCore project (Guo, B. Zhao, and Bai, 2022) for open-sourcing
their excellent code11. The high flexibility and modularity of their code allowed us to quickly implement our
calibration protocols on top of existing SBPAalgorithms.
11The code by Guo, B. Zhao, and Bai, 2022 is available at https://github.com/PatrickZH/DeepCore .
22Published in Transactions on Machine Learning Research (11/2023)
A Proofs
A.1 Proofs of Section 2
Propositions 1 and 2 are built on the following lemma.
Lemma 1 LetÏ€be a distribution on Dand(wn)na sequence of parameters in WÎ¸satisfying
EÏ€â„“(yout(X;wn),Y)â†’min
wâˆˆWÎ¸EÏ€â„“(yout(X;w),Y).
Then, it comes that
d(wn,Wâˆ—
Î¸(Ï€))â†’0.
Proof: DenoteLÏ€the function from WÎ¸toRdefined by
LÏ€(w) =EÏ€â„“(yout(X;w),Y).
Notice that under our assumptions, the dominated convergence theorem gives that LÏ€is continuous. This
lemma is a simple consequence of the continuity of LÏ€and the compacity of WÎ¸. Consider a sequence (wn)
such that
LÏ€(wn)â†’min
wâˆˆWÎ¸LÏ€(w).
We can prove the lemma by contradiction. Consider Ïµ >0and assume that there exists infinitely many
indicesnkfor whichd/parenleftï£¬ig
wnk,Wâˆ—
Î¸(Ï€)/parenrightï£¬ig
> Ïµ.SinceWÎ¸is compact, we can assume that wnkis convergent (by
considering a subsequence of which if needed), denote wâˆâˆˆWÎ¸its limit. The continuity of dthen gives
thatd/parenleftï£¬ig
wâˆ,Wâˆ—
Î¸(Ï€)/parenrightï£¬ig
â‰¥Ïµ, and in particular
wâˆÌ¸âˆˆWâˆ—
Î¸(Ï€) =argminwâˆˆWÎ¸LÏ€(w).
But sinceLÏ€is continuous, the initial assumption on (wn)translates to
min
wâˆˆWÎ¸LÏ€(w) = lim
kLÏ€(wnk) =LÏ€(wâˆ),
concluding the proof. â–¡
Proposition 1. A pruning algorithm Ais valid at a compression ratio râˆˆ(0,1]if and only if
d/parenleftï£¬ig
wA,r
n,Wâˆ—
Î¸(Âµ)/parenrightï£¬ig
â†’0a.s.
whereWâˆ—
Î¸(Âµ) =argminwâˆˆWÎ¸L(w)âŠ‚WÎ¸andd/parenleftï£¬ig
wA,r
n,Wâˆ—
Î¸(Âµ)/parenrightï£¬ig
denotes the euclidean distance from the point
wA,r
nto the setWâˆ—
Î¸(Âµ).
Proof: This proposition is a direct consequence of Lemma 1. Consider a valid pruning algorithm A, a
compression ratio rand a sequence of observations (Xk,Yk)such that
L(wA,r
n)â†’min
wâˆˆWÎ¸L(w).
We can apply Lemma 1 on the sequence (wA,r
n)with the distribution Ï€=Âµto get the result. â–¡
Proposition 2. LetAbe a pruning algorithm and râˆˆ(0,1]a compression ratio. Assume that there exists
a probability measure Î½ronDsuch that
âˆ€wâˆˆWÎ¸,LA,r
n(w)â†’EÎ½râ„“(yout(X;w),Y)a.s. (5)
Then, denotingWâˆ—
Î¸(Î½r) =argminwâˆˆWÎ¸EÎ½râ„“(yout(X;w),Y)âŠ‚WÎ¸, we have that
d/parenleftï£¬ig
wA,r
n,Wâˆ—
Î¸(Î½r)/parenrightï£¬ig
â†’0a.s.
23Published in Transactions on Machine Learning Research (11/2023)
Proof: Leveraging Lemma 1, it is enough to prove that
EÎ½râ„“(yout(X;wA,r
n),Y)âˆ’min
wâˆˆWÎ¸EÎ½râ„“(yout(X;w),Y)â†’0a.s.
To simplify the notations, we introduce the function ffromDÃ—WÎ¸toRdefined by
f(z,w) =â„“(yout(x;w),y),
wherez= (x,y).SinceWÎ¸is compact, we can find wâˆ—âˆˆWÎ¸such that EÎ½r[f(z,wâˆ—)] = minwEÎ½r[f(z,w)].
It comes that
0â‰¤EÎ½r[f(z,wA,r
n)]âˆ’EÎ½r[f(z,wâˆ—)]
â‰¤EÎ½r[f(z,wA,r
n)]âˆ’1
rn/summationdisplay
zâˆˆA(Dn,r)f(z,wA,r
n)
+1
rn/summationdisplay
zâˆˆA(Dn,r)f(z,wA,r
n)âˆ’1
rn/summationdisplay
zâˆˆA(Dn,r)f(z,wâˆ—)
+1
rn/summationdisplay
zâˆˆA(Dn,r)f(z,wâˆ—)âˆ’EÎ½r[f(z,wâˆ—)]
The last term converges to zero almost surely by assumption. By definition of wA,r
n, the middle term is
non-positive. It remains to show that the first term also converges to zero. With this, we can conclude that
limnEÎ½r[f(z,wA,r
n)]âˆ’EÎ½r[f(z,wâˆ—)] = 0
To prove that the first term converges to zero, we use the classical result that if every subsequence of a
sequence (un)has a further subsequence that converges to u, then the sequence (un)converges to u. Denote
un=EÎ½r[f(z,wA,r
n)]âˆ’1
rn/summationdisplay
zâˆˆA(Dn,r)f(z,wA,r
n).
Bycompacity ofWÎ¸, fromany subsequence of (un)wecan extractafurthersubsequencewithindicesdenoted
(nk)such thatwâˆ—
nkconverges to some wâˆâˆˆWÎ¸. We will show that (unk)converges to 0. LetÏµ>0, since
fis continuous on the compact set DÃ—WÎ¸, it is uniformly continuous. Therefore, almost surely, for klarge
enough,
sup
z|f(z,wâˆ—
nk)âˆ’f(z,wâˆ)|â‰¤Ïµ.
Denoting
vn=EÎ½r[f(z,wâˆ)]âˆ’1
rn/summationdisplay
zâˆˆA(Dn,r)f(z,wâˆ),
the triangular inequality then gives that, almost surely, for klarge enough
|unkâˆ’vnk|â‰¤2Ïµ.
By assumption, the sequence vnkconverges to zero almost surely, which concludes the proof. â–¡
We now prove Corollary 2, since Corollary 1 is a straightforward application of Proposition 2.
Corollary2. LetAbe any pruning algorithm and râˆˆ(0,1], and assume that (5)holds for a given probability
measureÎ½ronD. IfAis valid, thenWâˆ—
Î¸(Î½r)âˆ©Wâˆ—
Î¸(Âµ)Ì¸=âˆ…; or, equivalently,
min
wâˆˆWâˆ—
Î¸(Î½r)L(w) = min
wâˆˆWL(w).
24Published in Transactions on Machine Learning Research (11/2023)
Proof: This proposition is a direct consequence of Proposition 2 that states that
d(wA,r
n,Wâˆ—
Î¸(Î½r))â†’0a.s.
Since theLis continuous on the compact WÎ¸, it is uniformly continuous. Hence, for any Ïµ>0, we can find
Î·>0such that if d(w,wâ€²)â‰¤Î·, then|L(w)âˆ’L(wâ€²)|â‰¤Ïµfor any parameters w,wâ€²âˆˆWÎ¸. Hence, for nlarge
enough,d(wA,r
n,Wâˆ—
Î¸(Î½r))â‰¤Î·, leading to
L(wA,r
n)â‰¥min
wâˆˆWâˆ—
Î¸(r)L(w)âˆ’Ïµ.
Since the algorithm is valid, we know that L(wA,r
n)converges to minwâˆˆWÎ¸L(w)almost surely. Therefore,
for anyÏµ>0,
min
wâˆˆWÎ¸L(w)â‰¥min
wâˆˆWâˆ—
Î¸(r)L(w)âˆ’Ïµ.
which concludes the proof. â–¡
A.2 Proof of Proposition 3
Proposition 3. [Asymptotic behavior of SBPA]
LetAbe a SBPAalgorithm and let gbe its corresponding score function. Assume that gis adapted, and
consider a compression ratio râˆˆ(0,1). Denote by qrtherthquantile of the random variable g(Z)where
Zâˆ¼Âµ. DenoteAr={zâˆˆD|g(z)â‰¤qr}. Almost surely, the empirical measure of the retained data samples
converges weakly to Î½r=1
rÂµ|Ar, whereÂµ|Aris the restriction of Âµto the setAr. In particular, we have that
âˆ€wâˆˆWÎ¸,LA,r
n(w)â†’EÎ½râ„“(yout(X;w),Y)a.s.
Proof: ConsiderFthe set of functions f:Dâ†’ [âˆ’1,1]that are continuous. We will show that
sup
fâˆˆF/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
|A(Dn,r)|/summationdisplay
zâˆˆA(Dn,r)f(z)âˆ’1
r/integraldisplay
Arf(z)Âµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ†’0a.s. (9)
To simplify the notations, and since|A(Dn,r)|
rnconverges to 1, we will assume that rnis an integer. Denote
qr
nthe(rn)thordered statistic of/parenleftbig
g(zi)/parenrightbig
i=1,...,n, andqrtherthquantile of the random variable g(Z)where
Zâˆ¼Âµ.
We can upper bound the left hand side in equation (9) by the sum of two random terms AnandBndefined
by
Bn=1
rsup
fâˆˆF/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
zâˆˆDnf(z)Ig(z)â‰¤qrnâˆ’1
n/summationdisplay
zâˆˆDnf(z)Ig(z)â‰¤qr/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
Cn=1
rsup
fâˆˆF/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
zâˆˆDnf(z)Ig(z)â‰¤qrâˆ’/integraldisplay
f(z)Ig(z)â‰¤qrÂµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
To conclude the proof, we will show that both terms converge to zero almost surely.
25Published in Transactions on Machine Learning Research (11/2023)
For anyfâˆˆF, denotingGnthe empirical cumulative density function (cdf) of (g(zi))andGthe cdf ofg(Z),
we have that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
zâˆˆDnf(z)Ig(z)â‰¤qrnâˆ’1
n/summationdisplay
zâˆˆDnf(z)Ig(z)â‰¤qr/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ‰¤1
n/summationdisplay
zâˆˆDn|f(z)|Ã—/vextendsingle/vextendsingleIg(z)â‰¤qrnâˆ’Ig(z)â‰¤qr/vextendsingle/vextendsingle
â‰¤1
n/summationdisplay
zâˆˆDn/vextendsingle/vextendsingleIg(z)â‰¤qrnâˆ’Ig(z)â‰¤qr/vextendsingle/vextendsingle
â‰¤ |Gn(qr
n)âˆ’Gn(qr)|
=/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
râˆ’Gn(qr)/vextendsingle/vextendsingle/vextendsingle/vextendsingle
=|G(qr)âˆ’Gn(qr)|.
Therefore, Bnâ‰¤suptâˆˆR|G(t)âˆ’Gn(t)|which converges to zero almost surely by the Glivenko-Cantelli
theorem.
Similarly, the general Glivenko-Cantelli theorem for metric spaces (Varadarajan, 1958) gives that almost
surely,
sup
fâˆˆF/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
zâˆˆDnf(z)âˆ’/integraldisplay
f(z)Âµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ†’0.
Considerkâ‰¥1. Sincegis continuous and Dis compact, the sets Ar(1âˆ’1/k)andAr=D\Arare disjoint
and closed subsets. Using Urysohnâ€™s lemma (Theorem 8 in the Appendix), we can find fkâˆˆFsuch that
fk(z) = 1ifzâˆˆAr(1âˆ’1/k)andfk(z) = 0ifzâˆˆAr. Consider fâˆˆF, it comes that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
zâˆˆDnf(z)Ig(z)â‰¤qrâˆ’/integraldisplay
f(z)Ig(z)â‰¤qrÂµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ‰¤/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
zâˆˆDnfÃ—fk(z)âˆ’/integraldisplay
fÃ—fk(z)Âµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
+1
n/summationdisplay
zâˆˆDnIqr(1âˆ’1/k)â‰¤g(z)â‰¤qr
+/integraldisplay
Iqr(1âˆ’1/k)â‰¤g(z)â‰¤qrÂµ(z)dz
Hence, noticing that fÃ—fkâˆˆF, we find that
Cnâ‰¤supfâˆˆF/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle1
n/summationdisplay
zâˆˆDnf(z)âˆ’/integraldisplay
f(z)Âµ(z)dz/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle+|Gn(qr)âˆ’Gn(qr(1âˆ’1/k))|+r
k.
We can conclude the proof by noticing that |Gn(qr)âˆ’Gn(qr(1âˆ’1/k))|converges tor
kand taking kâ†’âˆ.â–¡
A.3 Proof of Theorem 1
In order to prove the theorem, we will need a few technical results that we state and prove first.
Lemma 2 Consider a set of continuous functions MfromXtoY. Consider Ïˆ0a function in the closure
ofMfor theâ„“âˆnorm. Then for any Ïµ>0, there exists ÏˆâˆˆMsuch that
supx,yâˆˆDâˆ¥â„“(Ïˆ(x),y)âˆ’â„“(Ïˆ0(x),y)âˆ¥â‰¤Ïµ
Proof: Since the loss â„“is continuous on the compact YÃ—Y, it is uniformly continuous. We can therefore
findÎ· >0such that for any y0,y,yâ€²âˆˆY, ifâˆ¥yâˆ’yâ€²âˆ¥â‰¤Î·thenâˆ¥â„“(y0,y)âˆ’â„“(y0,yâ€²)âˆ¥â‰¤Ïµ.We conclude the
proof using by selecting any ÏˆâˆˆMthat is at a distance not larger than Î·fromÏˆ0for theâ„“âˆnorm. â–¡
26Published in Transactions on Machine Learning Research (11/2023)
Lemma 3 Consider a SBPAA. LetMbe a set of continuous functions from XtoY. Consider râˆˆ(0,1)
and assume thatAis consistent onMat levelr, i.e.
âˆ€ÏˆâˆˆM,1
|A(D,r)|/summationdisplay
(x,y)âˆˆA(D,r)â„“(Ïˆ(x),y)â†’EÂµâ„“(Ïˆ(X),Y)a.s.
LetÏˆâˆbe any measurable function from XtoY. If there exists a sequence of elements of Mthat converges
point-wise to Ïˆâˆ, then
E1
rÂµ|Arâ„“(Ïˆâˆ(X),Y) =EÂµâ„“(Ïˆâˆ(X),Y). (10)
In particular, ifMhas the universal approximation property, then (10)holds for any continuous function.
Proof: Le(Ïˆk)kbe a sequence of functions in Mthat converges point-wise to Ïˆâˆ. Consider kâ‰¥0, since
Ais consistent and that Ïˆkis continuous and bounded, Proposition 3 gives that
E1
rÂµ|Arâ„“(Ïˆk(X),Y) =EÂµâ„“(Ïˆk(X),Y).
Sinceâ„“is bounded, we can apply the dominated convergence theorem to both sides of the equation to get
the final result. â–¡
Proposition 5 hereafter proves the final result for SBPAthat do not depend on the labels. The proof of
Theorem 1 that follows essentially deals with the remaining case of SBPAthat depend on the labels.
Proposition 5 LetAbe any SBPAwith an adapted score function gsatisfying
âˆƒËœg:Xâ†’R+, g(x,y) = Ëœg(x)a.s.
Assume that there exists two continuous functions f1andf2such that
EÂµâ„“(f1(X),Y)Ì¸=EÂµâ„“(f2(X),Y).
IfâˆªÎ¸MÎ¸has the universal approximation property, then there exist hyper-parameters Î¸âˆˆÎ˜for which the
algorithm is not consistent.
Proof: Considera compressionratio râˆˆ(0,1). We willprovethe resultby meansof contradiction. Assume
that the SBPAis consistent onâˆªÎ¸MÎ¸. From the universal approximation property and Lemma 3, we get that
1
rEÂµ|Arâ„“(f1(X),Y) =EÂµâ„“(f1(X),Y),
from which we deduce that
EÂµ/bracketleftï£¬ig
â„“(f1(X),Y)I(ZâˆˆAr)/bracketrightï£¬ig
=rEÂµâ„“(f1(X),Y) (11)
EÂµ/bracketleftï£¬ig
â„“(f1(X),Y)I(ZâˆˆD\Ar)/bracketrightï£¬ig
= (1âˆ’r)EÂµâ„“(f1(X),Y) (12)
and similarly for f2.
Notice that since the score function gdoes not depend on Y, there existsXrâŠ‚Xsuch thatAr=XrÃ—Y.
Consider the function defined by
f:xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’f1(x)I(xâˆˆXr) +f2(x) (1âˆ’I(xâˆˆXr)),
we will show that
i)1
rEÂµ|Arâ„“(f(X),Y)Ì¸=EÂµâ„“(f(X),Y)
ii) There exists a sequence of elements in âˆªÎ¸MÎ¸that converges point-wise almost everywhere to f
27Published in Transactions on Machine Learning Research (11/2023)
The conjunction of these two points contradicts Lemma 3, which would conclude the proof.
The first point is obtained through simple derivations, evaluating both sides of the equation i).
1
rEÂµ|Arâ„“(f(X),Y) =1
rEÂµâ„“(f(X),Y)I(ZâˆˆXrÃ—Y)
=1
rEÂµâ„“(f(X),Y)I(XâˆˆXr)
=1
rEÂµâ„“(f1(X),Y)I(XâˆˆXr)
=1
rEÂµâ„“(f1(X),Y)I(ZâˆˆAr)
=EÂµâ„“(f1(X),Y),
where we successively used the definition of fand equation (11). Now, using the definition of f, we get that
EÂµâ„“(f(X),Y) = EÂµâ„“(f1(X),Y)I(XâˆˆXr) +EÂµâ„“(f2(X),Y) (1âˆ’I(XâˆˆXr))
=EÂµâ„“(f1(X),Y)I(ZâˆˆAr) +EÂµâ„“(f2(X),Y)I(ZâˆˆD\Ar)
=rEÂµâ„“(f1(X),Y) + (1âˆ’r)EÂµâ„“(f2(X),Y).
These derivations lead to
1
rEÂµ|Arâ„“(f(X),Y)âˆ’EÂµâ„“(f(X),Y) = (1âˆ’r) [EÂµâ„“(f1(X),Y)âˆ’EÂµâ„“(f2(X),Y)]Ì¸= 0,
by assumption on f1andf2.
For point ii), we will construct a sequence (Ïˆk)kof functions inâˆªÎ¸MÎ¸that converges point-wise to falmost
everywhere, using the definition of the universal approximation property and Urysohnâ€™s lemma (Lemma 8 in
the Appendix). Consider kâ‰¥0and denote Ïµk=1âˆ’r
k+1. Denoteqrandqr+Ïµktherthand(r+Ïµk)thquantile of
the random variable Ëœg(X)where (X,Y )âˆ¼Âµ. DenoteXr={xâˆˆX| Ëœg(x)â‰¤qr}andBr,k={xâˆˆX| Ëœgr(x)â‰¥
qr+Ïµk}. Since Ëœgis continuous and Xis compact, the two sets are closed. Besides, since the random variable
Ëœg(X)is continuous ( gis an adapted score function), both sets are disjoint. Therefore, using Urysohnâ€™s lemma
(Lemma 8 in the Appendix), we can chose a continuous function Ï•k:X â†’ [0,1]such thatÏ•k(x) = 1for
xâˆˆXrandÏ•k(x) = 0forxâˆˆBr,k. Denotefkthe function defined by
Â¯fk(x) =f1(x)Ï•k(x) +f2(x)(1âˆ’Ï•k(x)).
Notice that (Ï•k)kconverges point-wise to I(Â·âˆˆXr), and therefore (Â¯fk)kconverges point-wise to f. Besides,
since Â¯fkis continuous, and âˆªÎ¸MÎ¸has the universal approximation property, we can chose ÏˆkâˆˆâˆªÎ¸MÎ¸such
that
supxâˆˆX|Ïˆk(x)âˆ’Â¯fk(x)|â‰¤Ïµk.
Hence, for any input xâˆˆX, we can upper-bound |Ïˆk(x)âˆ’f(x)|byÏµk+|Â¯fk(x)âˆ’f(x)|, giving that Ïˆk
converges pointwise to fand concluding the proof. â–¡
We are now ready to prove the Theorem 1 that we state here for convenience.
Theorem 1. LetAbe any SBPAalgorithm with an adapted score function. If âˆªÎ¸MÎ¸has the universal
approximation property, then there exist hyper-parameters Î¸âˆˆÎ˜for which the algorithm is not consistent.
Proof: We will use the universal approximation theorem to construct a model for which the algorithm is
biased. Denote supp (Âµ)the support of the generating measure Âµ. We can assume that there exists xâˆˆX
such that (x0,0)âˆˆsupp(Âµ),(x0,1)âˆˆsupp(Âµ), andg(x0,1)Ì¸=g(x0,0),otherwise one can apply Proposition 5
to get the result. Denote y0âˆˆ{0,1}such thatg(x0,y0)>g(x0,1âˆ’y0). Sincegis continuous, we can find
Ïµ>0,r0âˆˆ(0,1)such that
âˆ€xâˆˆB(x0,Ïµ), g(x,y0)>qr0>g(x,1âˆ’y0), (13)
28Published in Transactions on Machine Learning Research (11/2023)
whereqr0is therth
0quantile of g(Z)whereZâˆ¼Âµ.
Since (x0,1âˆ’y0)âˆˆsupp(Âµ), it comes that
âˆ† =1âˆ’r0
2(1 +r0)P/parenleftï£¬ig
XâˆˆB(x0,Ïµ),Y= 1âˆ’y0/parenrightï£¬ig
â„“(y0,1âˆ’y0)>0.
By assumption, the distribution of Xis dominated by the Lebesgue measure, we can therefore find a positive
Ïµâ€²<Ïµsuch that
P/parenleftï£¬ig
XâˆˆB(x0,Ïµ)\B(x0,Ïµâ€²)/parenrightï£¬ig
<âˆ†
2 maxâ„“.
The setsK1=B(x0,Ïµâ€²)andK2=X\Bo(x0,Ïµ)are closed and disjoint sets, Lemma 8 in Appendix insures
the existance of a continuous function hsuch thath(x) =y0forxâˆˆK1, andh(x) = 1âˆ’y0forxâˆˆK2.
We use Lemma 2 to construct ÏˆâˆˆâˆªÎ¸MÎ¸such that for any x,yâˆˆD,|â„“(Ïˆ(x),y)âˆ’â„“(h(x),y)|<âˆ†/2.Let
f1(x,y) =â„“(Ïˆ(x),y)andf2(x,y) =â„“(1âˆ’y0,y). Denotef=f1âˆ’f2. Notice that if we assume that the
algorithm is consistent on âˆªÎ¸MÎ¸, Lemma 3 gives that Ef(X,Y ) =1
r0Ef(X,Y ) 1g(X,Y)â‰¤qr0.We will prove
the non-consistency result by means of contradiction, showing that instead we have
Ef(X,Y )<1
r0Ef(X,Y ) 1g(X,Y)â‰¤qr0. (14)
To do so, we start by noticing three simple results that are going to be used in the following derivations
â€¢âˆ€xâˆˆK2,yâˆˆY,f(x,y) = 0.
â€¢âˆ€xâˆˆK1,f(x,y0) =âˆ’â„“(1âˆ’y0,y0)andf(x,1âˆ’y0) =â„“(y0,1âˆ’y0)
â€¢âˆ€xâˆˆB(x0,Ïµ)\B(x0,Ïµâ€²),yâˆˆY,|f(x,y)|â‰¤maxâ„“
We start be upper bounding the left hand side of (14) as follows:
Ef(X,Y ) = Ef(X,Y )/bracketleftbig
1XâˆˆK1+ 1XâˆˆK2+ 1XâˆˆB(x0,Ïµ)\B(x0,Ïµâ€²)/bracketrightbig
â‰¤P/parenleftï£¬ig
XâˆˆK1,Y= 1âˆ’y0/parenrightï£¬ig
â„“(y0,1âˆ’y0)
âˆ’P/parenleftï£¬ig
XâˆˆK1,Y=y0/parenrightï£¬ig
â„“(1âˆ’y0,y0)
+P/parenleftï£¬ig
XâˆˆB(x0,Ïµ)\B(x0,Ïµâ€²)/parenrightï£¬ig
maxâ„“
<P/parenleftï£¬ig
XâˆˆK1,Y= 1âˆ’y0/parenrightï£¬ig
â„“(y0,1âˆ’y0) +âˆ†
2
Using (13), we can lower bound the right hand side of (14) as follows:
1
r0Ef(X,Y ) 1g(X,Y)â‰¤qr0=1
r0Ef(X,Y )/bracketleftbig
1XâˆˆK1+ 1XâˆˆK2+ 1XâˆˆB(x0,Ïµ)\B(x0,Ïµâ€²)/bracketrightbig
1g(X,Y)â‰¤qr0
â‰¥1
r0P/parenleftï£¬ig
XâˆˆK1,Y= 1âˆ’y0/parenrightï£¬ig
â„“(y0,1âˆ’y0)
âˆ’1
r0P/parenleftï£¬ig
XâˆˆB(x0,Ïµ)\B(x0,Ïµâ€²)/parenrightï£¬ig
maxâ„“
>1
r0/bracketleftï£¬ig
P/parenleftï£¬ig
XâˆˆK1,Y= 1âˆ’y0/parenrightï£¬ig
â„“(y0,1âˆ’y0)âˆ’âˆ†
2/bracketrightï£¬ig
>Ef(X,Y )
+/bracketleftbig1
r0âˆ’1/bracketrightbig
P/parenleftï£¬ig
XâˆˆK1,Y= 1âˆ’y0/parenrightï£¬ig
â„“(y0,1âˆ’y0)
âˆ’1
2/bracketleftbig1
r0+ 1/bracketrightbig
âˆ†
>Ef(X,Y ),
where the last line comes from the definition of âˆ†. â–¡
29Published in Transactions on Machine Learning Research (11/2023)
A.4 Proof of Theorem 2
DenotePBthe set of probability distributions on XÃ—{ 0,1}, such that the marginal distribution on the
input space is continuous (absolutely continuous with respect to the Lebesgue measure on X) and for which
pÏ€:xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’PÏ€(Y= 1|X=x)
is upper semi-continuous. For a probability measure Ï€âˆˆPB, denoteÏ€Xthe marginal distribution on the
input. Denote Î³the function from [0,1]Ã—[0,1]toRdefined by
Î³(p,y) =pâ„“(y,0) + (1âˆ’p)â„“(y,1).
Finally, denoteFthe set of continuous functions from Xto[0,1]. We recall the two assumptions made on
the loss:
(i) The loss is non-negative and that â„“(y,yâ€²) = 0if and only if y=yâ€²
(ii) Forpâˆˆ[0,1],yâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’Î³(p,y) =pâ„“(y,1) + (1âˆ’p)â„“(y,0)has a unique minimizer, denoted yâˆ—
pâˆˆ[0,1], that
is increasing with p.
Lemma 4 Consider a loss â„“that satisfies (ii). Then, for any pâˆˆ[0,1]andÎ´ >0, there exists Ïµ >0such
that for any yâˆˆY= [0,1],
Î³(p,y)âˆ’Î³(p,yâˆ—
p)â‰¤Ïµ=â‡’ |yâˆ’yâˆ—
p|â‰¤Î´.
Proof: Considerpâˆˆ[0,1]andÎ· > 0. Assume that for any Ïµk=1
k+1there exists ykâˆˆYsuch that
|yâˆ’yâˆ—
p|â‰¥Î´and
pâ„“(yk,1) + (1âˆ’p)â„“(yk,0)âˆ’pâ„“(yâˆ—
p,1)âˆ’(1âˆ’p)â„“(yâˆ—
p,0)â‰¤Ïµk
SinceYis compact, we can assume that the sequence (yk)kconverges (taking, if needed, a sub-sequence of
the original one). Denote yâˆthis limit. Since â„“and|Â·|are continuous, it comes that |yâˆâˆ’yâˆ—
p|â‰¥Î´and
pâ„“(yâˆ,1) + (1âˆ’p)â„“(yâˆ,0)âˆ’pâ„“(yâˆ—
p,1)âˆ’(1âˆ’p)â„“(yâˆ—
p,0) = 0,
contradicting the assumption that yâˆ—
pis unique. â–¡
Lemma 5 IfÏˆis a measurable map from Xto[0,1], then there exists a sequence of continuous functions
fnâˆˆFthat converges point-wise to Ïˆ(for the Lebesgue measure)
Proof: This result is a direct consequence of two technical results, the Lusinâ€™s Theorem (Theorem 5 in the
appendix), and the continuous extension of functions from a compact set (Theorem 6 in the appendix). â–¡
Lemma 6 For a distribution Ï€âˆˆPB. defineÏˆâˆ—
Ï€the function from Xto[0,1]by
âˆ€xâˆˆX, Ïˆâˆ—
Ï€(x) =yâˆ—
pÏ€(x)
is measurable. Besides,
inffâˆˆFEÏ€â„“(f(X),Y) =EÏ€â„“(Ïˆâˆ—
Ï€(X),Y)
Proof: The function from [0,1]to[0,1]defined by
pâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’argminyâˆˆ[0,1]Î³(p,y) =yâˆ—
p,
is well defined and increasing from assumption (ii) on the loss. It is, therefore, measurable. Since pÏ€:
xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’PÏ€(Y= 1|X=x)is measurable, we get that Ïˆâˆ—
Ï€is measurable as the composition of two measurable
functions. For the second point, notice that by definition of Ïˆâˆ—
Ï€, for anyfâˆˆF,
EÏ€â„“(f(X),Y) = EÏ€XEÏ€/bracketleftï£¬ig
â„“(f(X),Y)|X/bracketrightï£¬ig
â‰¥EÏ€XEÏ€/bracketleftï£¬ig
â„“(Ïˆâˆ—
Ï€(X),Y)|X/bracketrightï£¬ig
â‰¥EÏ€â„“(Ïˆâˆ—
Ï€(X),Y).
30Published in Transactions on Machine Learning Research (11/2023)
Using Lemma 5, we can take a sequence of continuous functions fnâˆˆFthat converge point-wise to Ïˆâˆ—
Ï€. We
can conclude using the dominated convergence theorem, leveraging that â„“is bounded. â–¡
Lemma 7 LetAaSBPAwith an adapted score function gthat depends on the labels. Then there exists a
compression level r>0andÎµ>0such that for any f0âˆˆF, the two following statements exclude each other
(i)EÎ½râ„“(f0(X),Y)âˆ’inffâˆˆFEÎ½râ„“(f(X),Y)â‰¤Îµ
(ii)EÂµâ„“(f0(X),Y)âˆ’inffâˆˆFEÂµâ„“(f(X),Y)â‰¤Îµ
Proof: Sincegdepends on the labels, we can find x0âˆˆXin the support of ÂµXsuch thatpÂµ(x0) =
PÂµ(Y= 1|X=x0)âˆˆ(0,1)andg(x0,0)Ì¸=g(x0,1). Without loss of generality, we can assume that
g(x0,0)<g(x0,1). Takerâˆˆ(0,1)such that
g(x0,0)<qr<g(x0,1)
By continuity of g, we can find a radius Î·>0such that for any xin the ballBÎ·(x0)of centerx0and radius
Î·, we have that g(x,0)<qr<g(x,1). Besides, since pÂµis upper semi-continuous, we can assume that Î·is
small enough to ensure that for any xâˆˆBÎ·(x0),
pÂµ(x)<1 +pÂµ(x0)
2<1. (15)
Therefore, recalling that Î½r=1
rÂµ|Ar
â€¢PÎ½r(XâˆˆBÎ·(x0)) =1
rPÂµ(XâˆˆBÎ·(x0),Y= 0)>0andPÎ½r(Y= 1|XâˆˆBÎ·(x0)) = 0.
â€¢PÂµ(XâˆˆBÎ·(x0))>0andPÂµ(Y= 1|XâˆˆBÎ·(x0))>0.
Denote âˆ† =PÂµ(XâˆˆBÎ·(x0),Y= 1)>0.Consider the subset Vdefined by
V={xâˆˆBÎ·(x0)s.t. pÂµ(x)â‰¥âˆ†
2}
We can derive a lower-bound on ÂµX(V)as follows:
âˆ† =/integraldisplay
xâˆˆBÎ·(x0)p(x)ÂµX(dx)
=/integraldisplay
xâˆˆBÎ·(x0)p(x) 1p(x)<âˆ†
2ÂµX(dx) +/integraldisplay
xâˆˆBÎ·(x0)p(x) 1p(x)â‰¥âˆ†
2ÂµX(dx)
â‰¤/integraldisplay
xâˆˆBÎ·(x0)âˆ†
2ÂµX(dx) +/integraldisplay
xâˆˆVÂµX(dx)
â‰¤âˆ†
2+ÂµX(V).
The last inequality gives that ÂµX(V)â‰¥âˆ†/2>0. Moreover, we can lower-bound Î½X
r(V)using (15) as follows:
Î½X
r(V) =Î½r(VÃ—{0})
=1
rÂµ(VÃ—{0})
=1
r/integraldisplay
xâˆˆV(1âˆ’pÂµ(x))ÂµX(dx)
â‰¥1âˆ’pÂµ(x0)
2rÂµX(V)
â‰¥1âˆ’pÂµ(x0)
4râˆ†
>0.
31Published in Transactions on Machine Learning Research (11/2023)
Therefore, assumptions i) and ii) on the loss give that Ïˆâˆ—
Î½r(x) = 0andÏˆâˆ—
Âµ(x)â‰¥yâˆ—
âˆ†
2>0for anyxâˆˆV. Using
Lemma 4, take Ïµ1>0such that
â„“(y,0)â‰¤Ïµ1=â‡’yâ‰¤yâˆ—
âˆ†
2
3. (16)
In the following, we will show that there exists Ïµ2>0such that for any pâ‰¥âˆ†
2,
yâ‰¤yâˆ—
âˆ†
2
3=â‡’Î³(p,y)âˆ’Î³(p,yâˆ—
p)â‰¥Ïµ2 (17)
Otherwise, leveraging the compacity of the sets at hand, we can find two converging sequences pkâ†’pâˆâ‰¥âˆ†
2
andykâ†’yâˆâ‰¤yâˆ—
âˆ†
2
3such that
Î³(pk,yk)âˆ’min
yâ€²Î³(pk,yâ€²)â‰¤1
k+ 1.
SinceÎ³is uniformly continuous,
pâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’min
yâ€²Î³(p,yâ€²)
is continuous. Taking the limit it comes that
Î³(pâˆ,yâˆ)âˆ’min
yâ€²Î³(pâˆ,yâ€²) = 0,
and consequently yâˆ=yâˆ—
pâˆ. Sincepâˆâ‰¥âˆ†
2,
yâˆ=yâˆ—
pâˆâ‰¥yâˆ—
âˆ†
2>yâˆ—
âˆ†
2
3
reaching a contradiction.
Now, takeÏµ1andÏµ2satisfying (16) and (17) respectively. Put together, we have that for any pâ‰¥âˆ†
2,
Î³(0,y)âˆ’Î³(0,yâˆ—
0)â‰¤Ïµ1=â‡’Î³(p,y)âˆ’Î³(p,yâˆ—
p)â‰¥Ïµ2.
Using the definition of V, it comes that for any function f0andxâˆˆV
Î³(0,f0(x))â‰¤Ïµ1=â‡’Î³(pÂµ(x),f0(x))âˆ’Î³(pÂµ(x),Ïˆâˆ—
Âµ(x))â‰¥Ïµ2 (18)
LetÎµ=rmin(Ïµ1,Ïµ2)Î½X
r(V)
4>0. Consider f0âˆˆFsatisfying
EÎ½râ„“(f0(X),Y)âˆ’inf
fâˆˆFEÎ½râ„“(f(X),Y)â‰¤Îµ.
We will prove that
EÂµâ„“(f0(X),Y)âˆ’inf
fâˆˆFEÂµâ„“(f(X),Y)>Îµ
to conclude the proof. Denote Uf0is the subset of Vsuch that for any xâˆˆUf0,Î³(0,f0(x))â‰¤2Îµ
Î½Xr(V).We get
that
Îµâ‰¥EÎ½râ„“(f0(X),Y)âˆ’inf
fâˆˆFEÎ½râ„“(f(X),Y)
â‰¥/integraldisplay
X/bracketleftbig
Î³(pÎ½r(x),f0(x))âˆ’Î³(pÎ½r(x),Ïˆâˆ—
Î½r(x))/bracketrightbig
Î½X
r(dx)
â‰¥/integraldisplay
VÎ³(0,f0(x))Î½X
r(dx)
â‰¥2Îµ
Î½Xr(V)Î½X
r(V\Uf0)
32Published in Transactions on Machine Learning Research (11/2023)
Hence we get that Î½X
r(Uf0)â‰¥Î½X
r(V)
2. Since2Îµ
Î½Xr(V)â‰¤Ïµ1,the right hand side of (18) holds. In other words,
âˆ€xâˆˆUf0,Î³(pÂµ(x),f0(x))âˆ’Î³(pÂµ(x),Ïˆâˆ—
Âµ(x))â‰¥Ïµ2,
from which we successively obtain
EÂµâ„“(f0(X),Y)âˆ’inf
fâˆˆFEÂµâ„“(f(X),Y) =/integraldisplay
X/bracketleftbig
Î³(pÂµ(x),f0(x))âˆ’Î³(pÂµ(x),Ïˆâˆ—
Âµ(x))/bracketrightbig
ÂµX(dx)
â‰¥/integraldisplay
U{â€²/bracketleftbig
Î³(pÂµ(x),f0(x))âˆ’Î³(pÂµ(x),Ïˆâˆ—
Âµ(x))/bracketrightbig
ÂµX(dx)
â‰¥ÂµX(Uf0)Ïµ2
â‰¥Âµ(Uf0Ã—{0})Ïµ2
=r Ïµ2Î½X
r(Uf0)
â‰¥rÏµ2Î½X
r(V)
2
> Îµ.
â–¡
We can now ready to prove Theorem 2.
Theorem 2. LetAaSBPAwith an adapted score function gthat depends on the labels. If âˆªÎ¸MÎ¸has
the universal approximation property and the loss satisfies assumptions (i) and (ii), then there exist two
hyper-parameters Î¸1,Î¸2âˆˆÎ˜such that the algorithm is not valid on WÎ¸1âˆªWÎ¸2.
Proof: Denote ËœÎ˜ = Î˜Ã—Î˜, and for ËœÎ¸= (Î¸1,Î¸2)âˆˆËœÎ˜,WËœÎ¸=WÎ¸1âˆªWÎ¸2andMËœÎ¸=MÎ¸1âˆªMÎ¸2. We
will leverage Proposition 1 and Lemma 7 show that there exist a compression ratio râˆˆ(0,1)and a hyper-
parameter ËœÎ¸such that
min
wâˆˆWâˆ—
ËœÎ¸(r)L(w)>min
wâˆˆWËœÎ¸L(w)
which would conclude the proof.
Using Lemma 7, we can find randÏµ >0such that for any continuous function f0âˆˆF, the two following
propositions exclude each other:
(i)EÂµâ„“(f0(X),Y)âˆ’inffâˆˆFEÂµâ„“(f(X),Y)â‰¤Ïµ
(ii)EÎ½râ„“(f0(X),Y)âˆ’inffâˆˆFEÎ½râ„“(f(X),Y)â‰¤Ïµ
SinceâˆªMÎ¸has the universal approximation property, and that Ïˆâˆ—
ÂµandÏˆâˆ—
Î½r(defined as in Lemma 6) are
measurable, we consecutively use Lemma 5 and Lemma 2 to find ËœÎ¸= (Î¸1,Î¸2)such that
1. There exists Ïˆ1âˆˆMÎ¸1such that EÂµâ„“(Ïˆ1(X),Y)âˆ’EÂµâ„“(Ïˆâˆ—
Âµ(X),Y)â‰¤Ïµ/2
2. There exists Ïˆ2âˆˆMÎ¸2such that EÎ½râ„“(Ïˆ2(X),Y)âˆ’EÎ½râ„“(Ïˆâˆ—
Î½r(X),Y)â‰¤Ïµ/2
TakeÏˆ1,Ïˆ2âˆˆM ËœÎ¸two such functions. Consider any parameter wâˆˆargminwâˆˆWâˆ—
ËœÎ¸(r)L(w).By definition, it
comes that
EÎ½râ„“(yout(X;w),Y)âˆ’EÎ½râ„“(Ïˆâˆ—
Î½r(X),Y)â‰¤EÎ½râ„“(Ïˆ2,Y)âˆ’EÎ½râ„“(Ïˆâˆ—
Î½r(X),Y)
â‰¤Ïµ/2
Therefore, since Lemma 6 gives that inffâˆˆFEÎ½râ„“(f(X),Y) =EÎ½râ„“(Ïˆâˆ—
Î½r(X),Y), we can conclude that
EÂµâ„“(yout(X;w),Y)âˆ’inf
fâˆˆFEÂµâ„“(f(X),Y)>Ïµ,
33Published in Transactions on Machine Learning Research (11/2023)
from which we deduce that
EÂµâ„“(yout(X;w),Y)>inf
fâˆˆFEÂµâ„“(f(X),Y) +Ïµ
>EÂµâ„“(Ïˆ1(X),Y) +Ïµ/2
â‰¥ min
wâ€²âˆˆWËœÎ¸L(wâ€²) +Ïµ/2,
which gives the desired result. â–¡
A.5 Proof of the Corollaries 3 and 4
These two corollaries are a straightforward application of Theorem 1 and Theorem 2 as well as the existing
literature on the universal approximation properties of Neural Networks: (Hornik, 1991) and (Kidger and
Lyons, 2020). We give the proof of the result for wide neural networks. Consider any number of hidden
layersHâ‰¥1fixed. Denote Î¸= (K,R)âˆˆNÃ—R= Î˜. (Hornik, 1991) implies that âˆª(K,R)âˆˆÎ˜FFNNÏƒ
H,K(R)
has the universal approximation property. Theorem 1 states that one can find a Î¸0= (K0,R0)such that the
SBPAis not consistent on FFNNÏƒ
H,K 0(R0). Now from the definition of consistency, we get that if a SBPAis
not consistent on M, then it is not consistent on any superset Mâ€²that containsM. Therefore, we get the
non-consistency result by noticing that
FFNNÏƒ
H,K 0(R0)âŠ‚FFNNÏƒ
H,K(R),
for anyKâ‰¥K0andRâ‰¥R0. Similarly, Theorem 2 states that there exist Î¸1= (K1,R1)andÎ¸2= (K2,R2)
such that the model is not valid for any class of model such that
FFNNÏƒ
H,K 1(R1)âˆªFFNNÏƒ
H,K 2(R2)âŠ‚M.
We can conclude noticing that FFNNÏƒ
H,K(R)satisfies this condition if Kâ‰¥max(K1,K2)andRâ‰¥
max(R1,R2).
A.6 Proof of Theorem 3
ForKâˆˆNâˆ—, denotePK
Cthe set of generating processes for K-classes classification problems, for which the
inputXis a continuous random variable (the marginal of the input is dominated by the Lebesgue measure),
and the output Ycan take one of Kvalues inY(the same for all Ï€âˆˆPK
C). Similarly, denote PR, the
set of generating processes for regression problems for which both the input and output distributions are
continuous. LetPbe any set of generating processes introduced previously for regression or classification
(eitherP=PK
Cfor someK, orP=PR).
Assume that there exist (x1,y1),(x2,y2)âˆˆDsuch that
argminwâˆˆWÎ¸â„“(yout(x1;w),y1)âˆ©argminwâˆˆWÎ¸â„“(yout(x2;w),y2) =âˆ…. (H1)
For any SBPAalgorithmAwith adapted criterion, we will show that there exists a generating process ÂµâˆˆP
for whichAis not valid. More precisely, we will show that there exists r0âˆˆ(0,1)such that for any
compression ratio râ‰¤r0, there exists a generating process ÂµâˆˆPfor whichAis not valid. To do so, we
leverage Corollary 2 and prove that for any râ‰¤r0, there exists ÂµâˆˆP, for whichWâˆ—
Î¸(Î½r)âˆ©Wâˆ—
Î¸(Âµ)Ì¸=âˆ…, i.e.
âˆƒr0âˆˆ(0,1),âˆ€râ‰¤r0,âˆƒÂµâˆˆPs.t.âˆ€wâˆ—
râˆˆWâˆ—
Î¸(Î½r),LÂµ(wâˆ—
r)>min
wâˆˆWÎ¸LÂµ(w) (19)
We bring to the readerâ€™s attention that Î½r=1
rÂµ|Ar=Î½r(Âµ)depends on Âµ, and so does the acceptance region
Ar=Ar(Âµ).
The rigorous proof of Theorem 3 requires careful manipulations of different quantities, but the idea is rather
simple. Fig. 13 illustrates the main idea of the proof. We construct a distribution Âµwith the majority of
the probability mass concentrated around a point where the value of gis not minimal.
34Published in Transactions on Machine Learning Research (11/2023)
ğ‘¤ğ‘§
ğ‘§2 ğ‘§1
ğ‘¤ğ‘†ğ‘’ğ‘™ğ‘’ğ‘ğ‘¡ğ‘§1ğ‘ğ‘›ğ‘‘ğ‘§2ğ‘ ğ‘¢ğ‘â„ğ‘¡â„ğ‘ğ‘¡ğ‘”ğ‘§1<ğ‘”(ğ‘§2)
ğ‘ƒğ‘¢ğ‘¡ğ‘¡â„ğ‘’ğ‘ğ‘¢ğ‘™ğ‘˜ğ‘œğ‘“ğœ‡ğ‘ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ğ‘§2ğ‘ƒğ‘¢ğ‘¡ğ‘ğ‘ ğ‘šğ‘ğ‘™ğ‘™ğ‘šğ‘ğ‘ ğ‘ ğ‘œğ‘“ğœ‡ğ‘ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘ğ‘§1
Figure 13: Graphical sketch of the proof of Theorem 3. The surface represents the loss function f(z,w) =
â„“(yout(x),y)in 2D, where z= (x,y).
We start by introducing further notations. For z= (x,y)âˆˆD, andwâˆˆWÎ¸, we denote by fthe function
defined by f(z,w) =â„“(yout(x),y).We will use the generic notation â„“2to refer to the Euclidean norm on
the appropriate space. We denote B(X,Ï)theâ„“2ball with center Xand radius Ï. IfXis a set, then
B(X,Ï) =/uniontext
XâˆˆXB(X,Ï).ForSâŠ‚D, we denote argminwf(S,w) =/uniontext
XâˆˆSargminwf(X,w).
Notice that fis continuous on DÃ—WÎ¸.Besides, the set data generating processes Pis i) convex and ii)
satisfies for all X0âˆˆD,Î´>0andÎ³ <1, there exists a probability measure ÂµâˆˆPsuch that
Âµ(B(X0,Î´))>Î³,
These conditions play a central role in the construction of a generating process for which the pruning
algorithm is not valid. In fact, the non-validity proof applies to any set of generating processes satisfying
conditions i) and ii). To ease the reading of the proof, we break it into multiple steps that we list hereafter.
Steps of the proof:
1. For allz0âˆˆD, the setWz0=argminwf(z0,w)is compact (and non empty).
2. For allz0âˆˆD,Î´> 0, there exists Ï0>0such that for all Ïâ‰¤Ï0,
argminwf(B(z0,Ï),w)âŠ‚B(Wz0,Î´)
3. Under assumption (H1), there exists z1,z2âˆˆDsuch that i) g(X1)<g(X2)and ii)Wz1âˆ©Wz2=âˆ…
4. Forz1,z2as in 3, denoteW1=Wz1andW2=Wz2. There exists Î´,Ï0>0such that for any Ïâ‰¤Ï0
andw1âˆˆB(W1,Î´),andwâˆ—
2âˆˆW 2
inf
zâˆˆB(z2,Ï)f(z,w1)>sup
zâˆˆB(z2,Ï)f(z,wâˆ—
2)
5. For any râˆˆ(0,1), there exits a generating process ÂµâˆˆPsuch that any minimizer of the pruned
programwâˆ—
râˆˆWâˆ—
Î¸(Î½r)necessarily satisfies wâˆ—
râˆˆB(W1,Î´)and such that Âµ(B(z2,Ï))â‰¥1âˆ’2rfor a
givenÏâ‰¤Ï0.
6.âˆƒr0>0such thatâˆ€râ‰¤r0,âˆƒÂµâˆˆPsuch thatLÂµ(wâˆ—
r)>minwâˆˆWÎ¸LÂµ(w)for anywâˆ—
râˆˆWâˆ—
Î¸(Î½r)
35Published in Transactions on Machine Learning Research (11/2023)
Proof: Result 1: LetWz0=argminwf(z0,w)âŠ‚WÎ¸. SinceWÎ¸is compact and functions fz0:wâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’
f(z0,w)is continuous, it comes that Wz0is well defined, non-empty and closed (as the inverse image of a
closed set). Hence it is compact.
Result 2: Letz0âˆˆDandÎ´ >0. We will prove the result by contradiction. Suppose that for any Ï >0,
there exists wâˆˆargminwâ€²f(B(z0,Ï),wâ€²)such thatd(w,Wz0)â‰¥Î´.
It is well known that since fis continuous and that WÎ¸is compact, the function
zâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’min
wâˆˆWÎ¸f(z,w),
is continuous. Therefore, for any k>0, we can find Ïk>0such that for any zâˆˆB(z0,Ïk),
|inf
wf(z,w)âˆ’inf
wf(z0,w)|<1
k
For everyk > 0, letwk,zksuch thatzkâˆˆB(z0,Ïk),wkâˆˆargminwf(zk,w)andd(wk,Wz0)â‰¥Î´. By
definition, limzk=z0. SinceWÎ¸is compact, we can assume that wkconverges to wâˆwithout loss of
generality (taking a sub-sequence of the original one). Now, notice that
|f(zk,wk)âˆ’inf
wf(z0,w)|=|inf
wf(zk,w)âˆ’inf
wf(z0,w)|<1/k,
therefore, since fis continuous, f(z0,wâˆ) = infwf(z0,w)and sowâˆâˆˆWz0, which contradicts the fact that
d(wk,wâˆ)â‰¥Î´for allk. Hence, we can find Ï>0such that for all argminwf(B(z0,Ï))âŠ‚B(Wz0,Î´).
Result 3: Letz1,z2as in (H1) such that g(z1) =g(z2). Sincedis continuous, and W1=Wz1andW2=Wz2
are compact, d(W1Ã—W 2)is also compact. Hence, there exists Î´>0such that
min
w1âˆˆW1, w2âˆˆW2d(w1,w2)â‰¥Î´.
Using the previous result, let Ïsuch that argminwf(B(z1,Ï),w)âŠ‚B(W1,Î´/2),The triangular inequality
yieldsargminwf(B(z1,Ï),w)âˆ©W2=âˆ…. SincegisadaptedandB(z1,Ï)hasstrictlypositiveLebesguemeasure,
we can find zâ€²
1âˆˆB(z1,Ï)such thatg(zâ€²
1)Ì¸=g(z1). Therefore, the points zâ€²
1,z2satisfy the requirements.
Result 4: SinceW1is compact and fz2is continuous, f(z2,W1)is compact, and since W1âˆ©W 2=âˆ…,
minf(z2,W1)>f(z2,wâˆ—
2) = min
wâˆˆWÎ¸f(z2,w),
for anywâˆ—
2âˆˆW 2. Denote âˆ† = minf(z2,W1)âˆ’minwf(z2,w)>0.
Sincefis continuous on the compact space DÃ—WÎ¸, it is uniformly continuous. We can hence take Î´ >0
such that for z,zâ€²âˆˆDandw,wâ€²âˆˆWÎ¸such that
âˆ¥zâˆ’zâ€²âˆ¥â‰¤Î´,âˆ¥wâˆ’wâ€²âˆ¥â‰¤Î´=â‡’ |f(z,w)âˆ’f(zâ€²,wâ€²)|â‰¤âˆ†/3.
Using Result 2, we can find Ï0>0such that for all Ïâ‰¤Ï0,
argminwf(B(z1,Ï),w)âŠ‚argminwf(B(z1,Ï0),w)âŠ‚B(W1,Î´)
We can assume without loss of generality that Ï0â‰¤2Î´. Letw1âˆˆB(W1,Î´). For anywâˆ—
2âˆˆW 2, we conclude
that
min
zâˆˆB(z2,Ï)f(z,w1)â‰¥minf(z2,W1)âˆ’âˆ†/3>f(z2,wâˆ—
2) + âˆ†/3â‰¥sup
zâˆˆB(z2,Ï)f(z,wâˆ—
2).
Result 5: LetÏ0defined previously, k >1andrâˆˆ(0,1). Using the uniform continuity of f, we construct
0<Ïkâ‰¤Ï0such that
âˆ€wâˆˆP,âˆ€z,zâ€²âˆˆD,d(z,zâ€²)â‰¤Ïk=â‡’ |f(z,w)âˆ’f(zâ€²,w)|â‰¤1/k.
36Published in Transactions on Machine Learning Research (11/2023)
ConsiderÂµkâˆˆPsuch thatÂµk/parenleftbig
B(z1,Ïk)/parenrightbig
â‰¥randÂµk/parenleftbig
B(z2,Ïk)/parenrightbig
â‰¥1âˆ’râˆ’r/k. LetÎ½k
r=Î½r(Âµk). It comes
thatÎ½k
r(B(z1,Ïk))â‰¥1âˆ’1
k.Using a proof by contradiction, we will show that there exists k>1such that
argminwEÎ½krf(z,w)âŠ‚B(W1,Î´).
Suppose that the result doesnâ€™t hold, we can define a sequence of minimizers wksuch that wkâˆˆ
argminwEÎ½krf(z,w)andd(wk,W1)>Î´.DenoteM= supz,wf(z,w).Take anywâˆ—
1âˆˆW 1,
EÎ½krf(z,wk)â‰¤EÎ½krf(z,wâˆ—
1) (20)
â‰¤/parenleftbigg
f(z1,wâˆ—
1) +1
k/parenrightbigg
Î½k(B(z1,Ïk)) +M/parenleftbig
1âˆ’Î½k(B(z1,Ïk))/parenrightbig
(21)
â‰¤/parenleftbigg
f(z1,wâˆ—
1) +1
k/parenrightbigg
+M
k(22)
â‰¤/parenleftbigg
min
wf(z1,w) +1
k/parenrightbigg
+M
k(23)
Similarly, we have that
EÎ½k
rf(z,wk)â‰¥/parenleftbigg
f(z1,wk)âˆ’1
k/parenrightbigg
Î½k(B(z1,Ïk)) (24)
â‰¥/parenleftbigg
f(z1,wk)âˆ’1
k/parenrightbigg
(1âˆ’1/k) (25)
â‰¥/parenleftbigg
min
wf(z1,w)âˆ’1
k/parenrightbigg
(1âˆ’1/k). (26)
Putting the two inequalities together, we find
/parenleftbigg
min
wf(z1,w)âˆ’1
k/parenrightbigg
(1âˆ’1/k)â‰¤/parenleftbigg
f(z1,wk)âˆ’1
k/parenrightbigg
(1âˆ’1/k)â‰¤/parenleftbigg
min
wf(z1,w) +1
k/parenrightbigg
+M
k
SinceWÎ¸is compact, we can assume that limkwk=wâˆâˆˆWÎ¸(taking a sub-sequence of the original one).
And sincefz1is continuous, we can deduce that f(z1,wâˆ) = minwf(z1,w), which contradict the fact that
d(wk,wâˆ)>Î´for allk.
Result 6: Letrâˆˆ(0,1)andÎ´,Ï0,Ï,Âµas in the previous results. Let wrâˆˆWâˆ—
Î¸(Î½r)From Result 5, we have
thatwrâˆˆB(W1,Î´).Forwâˆ—
2âˆˆW 2, Result 5 implies that
minzâˆˆB(z2,Ï)f(z,wr)âˆ’supzâˆˆB(z2,Ï)f(z,wâˆ—
2)
â‰¥minzâˆˆB(z2,Ï0)f(z,wr)âˆ’supzâˆˆB(z2,Ï0)f(z,wâˆ—
2) = âˆ†
>0
Therefore,
EÂµf(z,wr)â‰¥ min
zâˆˆB(z2,Ï0)f(z,w1)Ã—Âµ(B(z2,Ïr)) (27)
â‰¥/parenleftï£¬igg
sup
zâˆˆB(z2,Ï0)f(z,wâˆ—
2) + âˆ†/parenrightï£¬igg
Âµ(B(z2,Ï)) (28)
â‰¥EÂµf(z,wâˆ—
2) + âˆ†(1âˆ’2r)âˆ’2rM (29)
â‰¥min
wEÂµf(z,w) + âˆ†(1âˆ’2r)âˆ’2rM. (30)
Therefore,
LÂµ(wr)âˆ’min
wâˆˆWÎ¸LÂµ(w)â‰¥âˆ†(1âˆ’2r)âˆ’2rM,
which is strictly positive for r<âˆ†
2(M+âˆ†)=r0 â–¡
37Published in Transactions on Machine Learning Research (11/2023)
A.7 Proof of Proposition 4
Proposition 4. [Consistency of Exact Calibration+ SBPA]
LetAbe a SBPAalgorithm. Using the Exact Calibration protocol with signal proportion Î±, the calibrated
algorithm Â¯Ais consistent if 1âˆ’Î± > 0, i.e. the exploration budget is not null. Besides, under the same
assumption 1âˆ’Î± > 0, the calibrated loss is an unbiased estimator of the generalization loss at any finite
sample size n>0,
âˆ€wâˆˆWÎ¸,âˆ€râˆˆ(0,1),ELÂ¯A,r,Î±
n(w) =L(w).
Proof: ConsiderÎ±<1. Letf(zi,w) =â„“(yout(xi,w),yi), andpe=(1âˆ’Î±)r
1âˆ’Î±r. Foriâˆˆ{1,...,n}, consider the
independent Bernoulli random variables biâˆ¼B(pe). Notice that
LÂ¯A,r,Î±
n(w) =1
nn/summationdisplay
i=1/parenleftbigg
1ziâˆˆA(Dn,Î±r)+bi
ps1ziÌ¸âˆˆA(Dn,Î±r)/parenrightbigg
f(zi,w),
which gives
ELÂ¯A,r,Î±
n(w) =EDnE/bracketleftï£¬ig
LÂ¯A,r,Î±
n(w)|Dn/bracketrightï£¬ig
=EDnLn(w) =L(w).
Define the random variables
Yn,i=/parenleftbigg
1ziâˆˆA(Dn,Î±r)+bi
ps1ziÌ¸âˆˆA(Dn,Î±r)âˆ’1/parenrightbigg
f(zi,w),
LetFn,i=Ïƒ({Yn,j,jÌ¸=i})be theÏƒ-algebra generated by the random variables {Yn,j,jÌ¸=i}. Let us now
show that the conditions of Theorem 7 hold with this choice of Yn,i.
â€¢Letnâ‰¥1andiâˆˆ{1,...,n}. Similarly to the previous computation, we get that E[Yn,i|Fn,i] = 0.
â€¢Using the compactness assumption on the space WÎ¸andD, we trivially have that supi,nEY4
n,i<âˆ.
â€¢Trivially, for each nâ‰¥1, the variables{Yn,i}1â‰¤iâ‰¤nare identically distributed.
Using Theorem 7 and the standard strong law of large numbers, we have that nâˆ’1/summationtextn
i=1Yn,iâ†’0almost
surely, and nâˆ’1/summationtextn
i=1f(zi,w)â†’EÂµf(z,w)almost surely, which concludes the proof for the consistency.
â–¡
B Technical results
Theorem 4 (Universal Approximation Theorem, (Hornik, 1991)) LetC(X,Y )denote the set of
continuous functions from XtoY. LetÏ•âˆˆC(R,R). Then,Ï•is not polynomial if and only if for ev-
eryn,mâˆˆN, compactKâŠ‚Rn,fâˆˆC(K,Rm),Ïµ >0, there exist kâˆˆN,AâˆˆRkÃ—n,bâˆˆRk,CâˆˆRmÃ—k
such that
sup
xâˆˆKâˆ¥f(x)âˆ’yout(x)âˆ¥â‰¤Ïµ,
whereyout(x) =CâŠ¤Ïƒ(Ax+b).
Lemma 8 (Urysohnâ€™s lemma, (Arkhangelâ€™ski Ë‡Ä±, 2001)) For any two disjoint closed sets AandBof a
topological space X, there exists a real-valued function f, continuous at all points, taking the value 0at all
points ofA, the value 1at all points of B. Moreover, for all xâˆˆX,0â‰¤f(x)â‰¤1.
Theorem 5 (Lusinâ€™s Theorem) IfXis a topological measure space endowed with a regular measure Âµ, if
Yis second-countable and Ïˆ:Xâ†’Yis measurable, then for every Ïµ>0there exists a compact set KâŠ‚X
such thatÂµ(X\K)<Ïµand the restriction of ÏˆtoKis continuous.
38Published in Transactions on Machine Learning Research (11/2023)
Theorem 6 (Continuous extension of functions from a compact, (Deimling, 2010)) LetAâŠ‚Rd
be compact and f:Aâ†’Rbe a continuous function. Then there exists a continuous extension Ëœf:Rdâ†’R
such thatf(x) =Ëœf(x)for allxâˆˆA.
B.1 A generalized Law of Large Numbers
There are many extensions of the strong law of large numbers to the case where the random variables have
some form of dependence. We prove a strong law of large numbers for specific sequences of arrays that satisfy
a conditional zero-mean property.
Theorem 7 Let{Yn,i,1â‰¤iâ‰¤n,nâ‰¥1}be a triangular array of random variables satisfying the following
conditions:
â€¢For allnâ‰¥1andiâˆˆ[n],E[Yn,i|Fn,i] = 0, whereFn,i=Ïƒ({Yn,j,jÌ¸=i}), i.e. theÏƒ-algebra
generated by all the random variables in row nother thanYn,i.
â€¢For allnâ‰¥1, the random variables (Yn,i)1â‰¤iâ‰¤nare identically distributed (but not necessarily
independent).
â€¢supn,iEY4
n,i<âˆ.
Then, we have that
1
nn/summationdisplay
i=1Yn,iâ†’0, a.s.
Proof: The proof uses similar techniques to the standard proof of the strong law of large numbers, with
some key differences, notably in the use of the Chebychev inequality to upper-bound the fourth moment of
the mean. Let Sn=/summationtextn
i=1Yn,i. We want to show that P(limnâ†’âˆSn/n= 0) = 1 . This is equivalent to
showing that for all Ïµ>0,P(Sn>nÏµfor infinitely many n) = 0. This event is nothing but the limsup of the
eventsAn={Sn>nÏµ}. Hence, we can use Borel-Cantelli to conclude if we can show that/summationtext
nP(An)<âˆ.
LetÏµ >0. Using Chebychev inequality with degree 4, we have that P(An)â‰¤(Ïµn)âˆ’4ES4
n. It remains to
bound ES4
nto conclude. We have that ES4
n=E/summationtext
1â‰¤i,j,k,lâ‰¤nYn,iYn,jYn,kYn,l. Using the first condition
(zero-mean conditional distribution), all the terms of the form Yn,iYn,jYn,kYn,l,Y2
n,iYn,jYn,k, andY3
n,iYn,lfor
iÌ¸=jÌ¸=kÌ¸=lvanish and we end up with ES4
n=nEY4
n,1+ 3n(nâˆ’1)EY2
n,1Y2
n,2, where we have used the fact
that the number of terms of the form Y2
n,iY2
n,jin the sum is given by/parenleftbign
2/parenrightbig
Ã—/parenleftbig4
2/parenrightbig
=n(nâˆ’1)
2Ã—6 = 3n(nâˆ’1).
Using the last condition of the fourth moment, we obtain that there exists a constant M > 0such that
ES4
n< Cn2. Using Chebychev inequality, we get that P(An)â‰¤Ïµâˆ’4nâˆ’2, and thus/summationtext
nP(An)<âˆ. We
conclude using the Borel-Cantelli lemma. â–¡
C Additional Theoretical Results
Convolutional neural networks: For an activation function Ïƒ, a real number R> 0, and integers Jâ‰¥1
andsâ‰¥2denoteCNNÏƒ
J,s(R)the set of convolutional neural networks with Jfilters of length s, with all
weights and biases in [âˆ’R,R]. More precisely, for a filter mask w= (w0,..,wsâˆ’1), and a vector xâˆˆRd, the
results of the convolution of wandx, denotedwâˆ—xis a vector in Rd+sdefined by (wâˆ—x)i=i/summationtext
k=iâˆ’s+1wiâˆ’kxk.
A network from CNNÏƒ
J(R)is then defined recursively for xâˆˆX:
â€¢h(0)(x) =x
â€¢Forjâˆˆ[1 :J],h(j)(x) =Ïƒ/parenleftbig
w(j)âˆ—h(jâˆ’1)(x) +b(j)/parenrightbig
, where the filters and biases w(j)andb(j)are in
[âˆ’R,R]
â€¢yout(x) =cTh(J)(x), where the vector chas entries in [âˆ’R,R]
39Published in Transactions on Machine Learning Research (11/2023)
Corollary 5 (Convolutional Neural Networks (Zhou, 2020)) LetÏƒbe the ReLU activation function.
Consider a filter length sâˆˆ[2,dx].For any SBPAwith adapted score function, there exists a number of filters
J0and a radius R0such that the algorithm is not consistent on CNNÏƒ
J,s(R), for anyJâ‰¥J0andRâ‰¥R0.
Besides, if the algorithm depends on the labels, then it is also not valid on CNNÏƒ
J,s(R), for anyJâ‰¥Jâ€²
0and
Râ‰¥Râ€²
0.
D Experimental details
Dataset CIFAR10 CIFAR100
Architecture ResNet18 ResNet34
Methods GraNd(10), Uncertainty ,
DeepFoolGraNd(10), Uncertainty ,
DeepFool
Selection LR 0.1 0.1
Training LR 0.1 0.1
Selection Epochs 1, 5 1, 5
Nb of exps 3 3
Training Epochs 160 160
Optimizer SGD SGD
Batch Size 128 128
The table above contains the different hyper-parameter we used to run the experiments. GraNd(10) refers
to using the GraNdmethod with 10different seeds (averaging over 10 different initializations). Selection LR
refers to the learning rate used for the coreset selection. The training LR follwos a cosine annealing schedule
given by the following:
Î·t=Î·min+1
2(Î·maxâˆ’Î·min)/parenleftbigg
1 + cos/parenleftbiggTcur
TmaxÏ€/parenrightbigg/parenrightbigg
,
whereTcuris the current epoch, Tmaxis the total number of epochs, and Î·max= 0.1andÎ·min= 10âˆ’4.
These are the same hyper-parameter choices used by Guo, B. Zhao, and Bai, 2022.
D.1 MLP for Scaling laws experiments
We consider an MLP given by
y1(x) =Ï•(W1xin+b1),
y2(x) =Ï•(W2y1(x) +b2),
yout(x) =Wouty2(x) +bout,
wherexinâˆˆR1000is the input, W1âˆˆR128Ã—1000,W2âˆˆR128Ã—128,WoutâˆˆR2Ã—128are the weight matrices and
b1,b2,boutare the bias vectors.
40