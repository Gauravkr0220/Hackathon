Published in Transactions on Machine Learning Research (01/2024)
MMD-Regularized Unbalanced Optimal Transport
Piyushi Manupriya cs18m20p100002@iith.ac.in
Department of Computer Science and Engineering, IIT Hyderabad, INDIA.
J. SakethaNath saketha@cse.iith.ac.in
Department of Computer Science and Engineering, IIT Hyderabad, INDIA.
Pratik Jawanpuria pratik.jawanpuria@microsoft.com
Microsoft, INDIA.
Reviewed on OpenReview: https: // openreview. net/ forum? id= eN9CjU3h1b
Abstract
We study the unbalanced optimal transport (UOT) problem, where the marginal constraints
are enforced using Maximum Mean Discrepancy (MMD) regularization. Our work is moti-
vated by the observation that the literature on UOT is focused on regularization based on
Ï•-divergence (e.g., KL divergence). Despite the popularity of MMD, its role as a regular-
izer in the context of UOT seems less understood. We begin by deriving a specific dual of
MMD-regularized UOT (MMD-UOT), which helps us prove several useful properties. One
interesting outcome of this duality result is that MMD-UOT induces novel metrics, which
not only lift the ground metric like the Wasserstein but are also sample-wise efficient to es-
timate like the MMD. Further, for real-world applications involving non-discrete measures,
we present an estimator for the transport plan that is supported only on the given ( m)
samples. Under certain conditions, we prove that the estimation error with this finitely-
supported transport plan is also O(1/âˆšm). As far as we know, such error bounds that
are free from the curse of dimensionality are not known for Ï•-divergence regularized UOT.
Finally, we discuss how the proposed estimator can be computed efficiently using acceler-
ated gradient descent. Our experiments show that MMD-UOT consistently outperforms
popular baselines, including KL-regularized UOT and MMD, in diverse machine learning
applications.
1 Introduction
Optimal transport (OT) is a popular tool for comparing probability measures while incorporating geometry
overtheirsupport. OThaswitnessedalotofsuccessinmachinelearningapplications(PeyrÃ©&Cuturi,2019),
where distributions play a central role. The Kantorovichâ€™s formulation for OT aims to find an optimal plan
forthetransportofmassbetweenthesourceandthetargetdistributionsthatincurstheleastexpectedcostof
transportation. While classical OT strictly enforces the marginals of the transport plan to be the source and
target, one would want to relax this constraint when the measures are noisy (Frogner et al., 2015) or when
the source and target are un-normalized (Chizat, 2017; Liero et al., 2018). Unbalanced optimal transport
(UOT) (Chizat, 2017), a variant of OT, is employed in such cases, which performs a regularization-based
soft-matching of the transport planâ€™s marginals with the source and the target distributions.
Unbalanced optimal transport with Kullback Leibler (KL) divergence and, in general, with Ï•-
divergence (Csiszar, 1967) based regularization is well-explored in literature (Liero et al., 2016; 2018).
Entropy regularized UOT with KL divergence (Chizat et al., 2017; 2018) has been employed in applica-
tions such as domain adaptation (Fatras et al., 2021), natural language processing (Chen et al., 2020b),
and computer vision (De Plaen et al., 2023). Existing works (Piccoli & Rossi, 2014; 2016; Hanin, 1992;
Georgiou et al., 2009) have also studied total variation (TV)-regularization-based UOT formulations. While
1Published in Transactions on Machine Learning Research (01/2024)
MMD-based methods have been popularly employed in several machine learning (ML) applications (Gretton
et al., 2012; Li et al., 2017; 2021; Nguyen et al., 2021), the applicability of MMD-based regularization for
UOT is not well-understood. To the best of our knowledge, interesting questions like the following, have not
been answered in prior works:
â€¢Will MMD regularization for UOT also lead to novel metrics over measures, analogous to the ones
obtained with the KL divergence (Liero et al., 2018) or the TV distance (Piccoli & Rossi, 2014)?
â€¢What will be the statistical estimation properties of these?
â€¢How can such MMD regularized UOT metrics be estimated in practice such that they are suitable
for large-scale applications?
In order to bridge this gap, we study MMD-based regularization for matching the marginals of the transport
plan in the UOT formulation (henceforth termed MMD-UOT).
We first derive a specific dual of the MMD-UOT formulation (Theorem 4.1), which helps further analyze its
properties. One interesting consequence of this duality result is that the optimal objective of MMD-UOT
is a valid distance between the source and target measures (Corollary 4.2), whenever the transport cost is
valid (ground) metric over the data points. Popularly, this is known as the phenomenon of lifting metrics to
measures. This result is significant as it shows that MMD-regularization in UOT can parallel the metricity-
preservation that happens with KL-regularization (Liero et al., 2018) and TV-regularization (Piccoli & Rossi,
2014). Furthermore, our duality result shows that this induced metric is a novel metric belonging to the
family of integral probability metrics (IPMs) with a generating set that is the intersection of the generating
sets of MMD and the Kantorovich-Wasserstein metric. Because of this important relation, the proposed
distance is always smaller than the MMD distance, and hence, estimating MMD-UOT from samples is at
least as efficient as that with MMD (Corollary 4.6). This is interesting as minimax estimation rates for MMD
can be completely dimension-free. As far as we know, there are no such results that show that estimation
with KL/TV-regularized UOT can be as efficient sample-wise. Thus, the proposed metrics not only lift the
ground metrics to measures, like the Wasserstein, but also are sample-wise efficient to estimate, like MMD.
However, like any formulation of optimal transport problems, the computation of MMD-UOT involves op-
timization over all possible joint measures. This may be challenging, especially when the measures are
continuous. Hence, we present a convex program-based estimator, which only involves a search over joints
supported at the samples. We prove that the proposed estimator is statistically consistent and converges to
MMD-UOT between the true measures at a rate O/parenleftï£¬ig
mâˆ’1
2/parenrightï£¬ig
, wheremis the number of samples. Such efficient
estimators are particularly useful in machine learning applications, where typically only samples from the
underlying measures are available. Such applications include hypothesis testing, domain adaptation, and
model interpolation, to name a few. In contrast, the minimax estimation rate for the Wasserstein distance is
itselfO/parenleftï£¬ig
mâˆ’1
d/parenrightï£¬ig
, wheredis the dimensionality of the samples (Niles-Weed & Rigollet, 2019). That is, even if
a search over all possible joints is performed, estimating Wasserstein may be challenging. Since MMD-UOT
can approximate Wasserstein arbitrarily closely (as the regularization hyperparameter goes âˆ), our result
can also be understood as a way of alleviating the curse of dimensionality problem in Wasserstein. We
summarize the comparison between MMD-UOT and relevant OT variants in Table 1.
Finally, our result of MMD-UOT being a metric facilitates its application whenever the metric properties
of OT are desired, for example, while computing the barycenter-based interpolation for single-cell RNA
sequencing (Tong et al., 2020). Accordingly, we also present a finite-dimensional convex-program-based
estimator for the barycenter with MMD-UOT. We prove that this estimator is also consistent with an
efficient sample complexity. We discuss how the formulations for estimating MMD-UOT (and barycenter)
can be solved efficiently using accelerated (projected) gradient descent. This solver helps us scale well
to large datasets. We empirically show the utility of MMD-UOT in several applications including two-
sample hypothesis testing, single-cell RNA sequencing, domain adaptation, and prompt learning for few-
shot classification. In particular, we observe that MMD-UOT outperforms popular baselines such as KL-
regularized UOT and MMD in our experiments.
We summarize our main contributions below:
2Published in Transactions on Machine Learning Research (01/2024)
Table 1: Summarizing interesting properties of MMD and several OT/UOT approaches. ÏµOT (Cuturi, 2013)
andÏµKL-UOT (Chizat, 2017) denote the entropy-regularized scalable variants OT and KL-UOT (Liero
et al., 2018), respectively. MMD and the proposed MMD-UOT are shown with characteristic kernels. By
â€˜finite-parameterization boundsâ€™ we mean results similar to Theorem 4.10.
Property MMD OT ÏµOT TV-UOT KL-UOT ÏµKL-UOT MMD-UOT
Metricity
Lifting of ground metric
No curse of dimensionality
Finite-parametrization bounds N/A
â€¢Dual of MMD-UOT and its analysis. We prove that MMD-UOT induces novel metrics that not only
lift ground metrics like the Wasserstein but also are sample-wise efficient to estimate like the MMD.
â€¢Finite-dimensionalconvex-program-basedestimatorsforMMD-UOTandthecorrespondingbarycen-
ter. We prove that the estimators are both statistically and computationally efficient.
â€¢We illustrate the efficacy of MMD-UOT in several real-world applications. Empirically, we observe
that MMD-UOT consistently outperforms popular baseline approaches.
We present proofs for all our theory results in Appendix B. As a side-remark, we note that most of our
results not only hold for MMD-UOT but also for a UOT formulation where a general IPM replaces MMD.
Proofs in the appendix are hence written for general IPM-based regularization and then specialized to the
case when the IPM is MMD. This generalization to IPMs may itself be of independent interest.
2 Preliminaries
Notations. LetXbe a set (domain) that forms a compact Hausdorff space. Let R+(X),R(X)denote
the set of all non-negative, signed (finite) Radon measures defined over X; while the set of all probability
measures is denoted by R+
1(X). For a measure on the product space, Ï€âˆˆR+(XÃ—X ), letÏ€1,Ï€2denote
the first and second marginals, respectively (i.e., they are the push-forwards under the canonical projection
maps ontoX). LetL(X),C(X)denote the set of all real-valued measurable functions and all real-valued
continuous functions, respectively, over X.
IntegralProbabilityMetric(IPM): GivenasetGâŠ‚L (X),theintegralprobabilitymetric(IPM)(Muller,
1997; Sriperumbudur et al., 2009; Agrawal & Horel, 2020) associated with G, is defined by:
Î³G(s0,t0)â‰¡max
fâˆˆG/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0âˆ’/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingleâˆ€s0,t0âˆˆR+(X). (1)
Gis called the generating set of the IPM, Î³G.
Maximum Mean Discrepancy (MMD) Letkbe a characteristic kernel (Sriperumbudur et al., 2011)
over the domainX, letâˆ¥fâˆ¥kdenote the norm of fin the canonical reproducing kernel Hilbert space (RKHS),
Hk, corresponding to k. MMDkis the IPM associated with the generating set: Gkâ‰¡{fâˆˆHk|âˆ¥fâˆ¥kâ‰¤1}.
Using a characteristic kernel k, MMD metric between s0,t0âˆˆR+(X)is defined as:
MMDk(s0,t0)â‰¡max
fâˆˆGk/vextendsingle/vextendsingle/integraltext
Xfds0âˆ’/integraltext
Xfdt0/vextendsingle/vextendsingle
=âˆ¥Âµk(s0)âˆ’Âµk(t0)âˆ¥k,(2)
whereÂµk(s)â‰¡/integraltext
Ï•k(x)ds(x), is the kernel mean embedding of s(Muandet et al., 2017), Ï•kis the canonical
featuremapof k. Akernelkiscalledacharacteristickernelifthemap Âµkisinjective. MMDcanbecomputed
analytically using evaluations of the kernel k. MMDkis a metric when the kernel kis characteristic. A
continuous positive-definite kernel konXis called c-universal if the RKHS Hkis dense inC(X)w.r.t. the
3Published in Transactions on Machine Learning Research (01/2024)
sup-norm, i.e., for every function gâˆˆC(X)and allÏµ >0, there exists an fâˆˆHksuch thatâˆ¥fâˆ’gâˆ¥âˆâ‰¤Ïµ.
Universal kernels are also characteristic. Gaussian kernel (RBF kernel) is an example of a universal kernel
over the continuous domain. Dirac delta kernel is an example of a universal kernel over the discrete domain.
Optimal Transport (OT) Optimal transport provides a tool to compare distributions while incorporating
the underlying geometry of their support points. Given a cost function, c:XÃ—Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’ R, and two probability
measuress0âˆˆR+
1(X),t0âˆˆR+
1(X), thep-Wasserstein Kantorovich OT formulation is given by:
Â¯Wp
p(s0,t0)â‰¡ min
Ï€âˆˆR+
1(XÃ—X )/integraldisplay
cpdÏ€,s.t.Ï€1=s0, Ï€2=t0, (3)
wherepâ‰¥1. An optimal solution of (3) is called an optimal transport plan. Whenever the cost is a metric,
d, overXÃ—X(ground metric), Â¯Wpdefines a metric over measures, known as the p-Wasserstein metric, over
R+
1(X)Ã—R+
1(X).
Kantorovich metric ( Kc)Kantorovich metric also belongs to the family of integral probability metrics
associated with the generating set Wcâ‰¡/braceleftbigg
f:Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R|max
xâˆˆXÌ¸=yâˆˆX|f(x)âˆ’f(y)|
c(x,y)â‰¤1/bracerightbigg
, wherecis a metric over
XÃ—X. The Kantorovich-Rubinstein duality result shows that the 1-Wasserstein metric is the same as the
Kantorovich metric when restricted to probability measures (refer for e.g. (5.11) in Villani (2009)):
Â¯W1(s0,t0)â‰¡ min
Ï€âˆˆR+
1(XÃ—X )/integraldisplay
cpdÏ€,= max
fâˆˆG/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0âˆ’/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingleâ‰¡ Kc(s0,t0),
s.t.Ï€1=s0, Ï€2=t0
wheres0,t0âˆˆR+
1(X).
3 Related Work
Given the source and target measures, s0âˆˆR+(X)andt0âˆˆR+(X), respectively, the unbalanced opti-
mal transport (UOT) approach (Liero et al., 2018; Chizat et al., 2018) aims to learn the transport plan
by replacing the mass conservation marginal constraints (enforced strictly in â€˜balancedâ€™ OT setting) by a
soft regularization/penalization on the marginals. KL-divergence and, in general, Ï•-divergence (Csiszar,
1967), (Sriperumbudur et al., 2009) based regularizations have been most popularly studied in UOT setting.
TheÏ•-divergence regularized UOT formulation may be written as (Frogner et al., 2015), (Chizat, 2017):
min
Ï€âˆˆR+(XÃ—X )/integraldisplay
cdÏ€+Î»DÏ•(Ï€1,s0) +Î»DÏ•(Ï€2,t0), (4)
wherecis the ground cost metric and DÏ•(Â·,Â·)denotes the Ï•-divergence (Csiszar, 1967; Sriperumbudur
et al., 2009) between two measures. Since in UOT settings, the measures s0, t0may be un-normalized,
following (Chizat, 2017; Liero et al., 2018) the transport plan is also allowed to be un-normalized. UOT
with KL-divergence-based regularization induces the so-called Gaussian Hellinger-Kantorovich metric (Liero
et al., 2018) between the measures whenever 0< Î»â‰¤1and the ground cost cis the squared-Euclidean
distance. Similar to the balanced OT setup (Cuturi, 2013), an additional entropy regularization in KL-UOT
formulation facilitates Sinkhorn iteration (Knight, 2008) based efficient solver for KL-UOT (Chizat et al.,
2017) and has been popularly employed in several machine learning applications (Fatras et al., 2021; Chen
et al., 2020b; Arase et al., 2023; De Plaen et al., 2023).
Total Variation (TV) distance is another popular metric between measures and is the only common member
of theÏ•-divergence family and the IPM family. UOT formulation with TV regularization (denoted by |Â·|TV)
has been studied in (Piccoli & Rossi, 2014):
min
Ï€âˆˆR+(XÃ—X )/integraldisplay
cdÏ€+Î»|Ï€1âˆ’s0|TV+Î»|Ï€2âˆ’t0|TV. (5)
UOT with TV-divergence-based regularization induces the so-called Generalized Wasserstein metric (Piccoli
& Rossi, 2014) between the measures whenever Î» > 0and the ground cost cis a valid metric. As far as
4Published in Transactions on Machine Learning Research (01/2024)
we know, none of the existing works study the sample complexity of estimating these metrics from samples.
More importantly, algorithms for solving (5) with empirical measures that computationally scale well to ML
applications seem to be absent in the literature.
Besides the family of Ï•-divergences, the family of integral probability metrics is popularly used for comparing
measures. AnimportantmemberoftheIPMfamilyistheMMDmetric, whichalsoincorporatesthegeometry
oversupportsthroughtheunderlyingkernel. Duetoitsattractivestatisticalproperties(Grettonetal.,2006),
MMD has been successfully applied in a diverse set of applications including hypothesis testing (Gretton
et al., 2012), generative modelling (Li et al., 2017), self-supervised learning (Li et al., 2021), etc.
Recently, (Nath & Jawanpuria, 2020) explored learning the transport planâ€™s kernel mean embeddings in the
balanced OT setup. They proposed learning the kernel mean embedding of a joint distribution with the least
expected cost and whose marginal embeddings are close to the given-sample-based estimates of the marginal
embeddings. As kernel mean embedding induces MMD distance, MMD-based regularization features in
the balanced OT formulation of (Nath & Jawanpuria, 2020) as a means to control overfitting. To ensure
that valid conditional embeddings are obtained from the learned joint embeddings, (Nath & Jawanpuria,
2020) required additional feasibility constraints that restrict their solvers in scaling well to machine learning
applications. We also note that (Nath & Jawanpuria, 2020) neither analyze the dual of their formulation
nor study its metric-related properties and their sample complexity result of O(mâˆ’1
2)does not apply to our
MMD-UOT estimator as their formulation is different from the proposed MMD-UOT formulation (6).
In contrast, we bypass the issues related to the validity of conditional embeddings as our formulation involves
directly learning the transport plan and avoids kernel mean embedding of the transport plan. We perform
a detailed study of MMD regularization for UOT, which includes analyzing its dual and proving metric
properties that are crucial for optimal transport formulations. To the best of our knowledge, the metricity
of MMD-regularized UOT formulations has not been studied previously. The proposed algorithm scales
well to large-scale machine learning applications. While we also obtain O(mâˆ’1
2)estimation error rate, we
require a different proof strategy than (Nath & Jawanpuria, 2020). Finally, as discussed in Appendix B,
most of our theoretical results apply to a general IPM-regularized UOT formulation and are not limited to
the MMD-regularized UOT formulation. This generalization does not hold for (Nath & Jawanpuria, 2020).
Wasserstein auto-encoders (WAE) also employ MMD for regularization. However, there are some important
differences. The regularization in WAEs is only performed for one of the marginals, and the other marginal is
matched exactly. This not only breaks the symmetry (and hence the metric properties) but also brings back
the curse of dimensionality in estimation (for the same reasons as with unregularized OT). Further, their
work does not attempt to study any theoretical properties with MMD regularization and merely employs
it as a practical tool for matching marginals. Our goal is to theoretically study the metric and estimation
properties with MMD regularization. We present more details in Appendix B.18.
We end this section by noting key differences between MMD and OT-based approaches (including MMD-
UOT). A distinguishing feature of OT-based approaches is the phenomenon of lifting the ground-metric
geometry to that over distributions. One such result is visualized in Figure 2(b), where the MMD-based-
interpolate of the two unimodal distributions comes out to be bimodal. This is because MMDâ€™s interpolation
is the (literal) average of the source and the target densities, irrespective of the kernel. This has been
well-established in the literature (Bottou et al., 2017). On the other hand, OT-based approaches obtain
a unimodal barycenter. This is a â€˜geometricâ€™ interpolation that captures the characteristic aspects of the
source and the target distributions. Another feature of OT-based methods is that we obtain a transport plan
between the source and the target points which can be used for various alignment-based applications, e.g.,
cross-lingual word mapping (Alvarez-Melis & Jaakkola, 2018; Jawanpuria et al., 2020), domain adaptation
(Courty et al., 2017; Courty et al., 2017; Gurumoorthy et al., 2021), etc. On the other hand, it is unclear
how MMD can be used to align the source and target data points.
5Published in Transactions on Machine Learning Research (01/2024)
4 MMD Regularization for UOT
We propose to study the following UOT formulation, where the marginal constraints are enforced using
MMD regularization.
Uk,c,Î» 1,Î»2(s0,t0)â‰¡ min
Ï€âˆˆR+(XÃ—X )/integraltext
cdÏ€+Î»1MMDk(Ï€1,s0) +Î»2MMDk(Ï€2,t0)
= min
Ï€âˆˆR+(XÃ—X )/integraltext
cdÏ€+Î»1âˆ¥Âµk(Ï€1)âˆ’Âµk(s0)âˆ¥k+Î»2âˆ¥Âµk(Ï€2)âˆ’Âµk(t0)âˆ¥k,(6)
whereÂµk(s)is the kernel mean embedding of s(defined in Section 2) induced by the characteristic kernel k
used in the generating set Gkâ‰¡{fâˆˆHk|âˆ¥fâˆ¥kâ‰¤1}, andÎ»1,Î»2>0are the regularization hyper-parameters.
We begin by presenting a key duality result.
Theorem 4.1. (Duality) Wheneverc,kâˆˆC(XÃ—X )andXis compact, we have that:
Uk,c,Î» 1,Î»2(s0,t0) = max
fâˆˆGk(Î»1),gâˆˆGk(Î»2)/integraltext
Xfds0+/integraltext
Xgdt0,
s.t.f(x) +g(y)â‰¤c(x,y)âˆ€x,yâˆˆX.(7)
Here,Gk(Î»)â‰¡{gâˆˆHk|âˆ¥gâˆ¥kâ‰¤Î»}.
The duality result helps us to study several properties of the MMD-UOT (6), discussed in the corollaries
below. The proof of Theorem 4.1 is based on an application of Sionâ€™s minimax exchange theorem (Sion,
1958) and is detailed in Appendix B.1.
Applications in machine learning often involve comparing distributions for which the Wasserstein metric is a
popular choice. While prior works have shown metric-preservation happens under KL-regularization (Liero
et al., 2018) and TV-regularization (Piccoli & Rossi, 2016), it is an open question if MMD-regularization in
UOT can also lead to valid metrics. The following result answers this affirmatively.
Corollary 4.2. (Metricity) In addition to assumptions in Theorem (4.1), whenever cis a metric,Uk,c,Î»,Î»
belongs to the family of integral probability metrics (IPMs). Also, the generating set of this IPM is the
intersection of the generating set of the Kantorovich metric and the generating set of MMD. Finally, Uk,c,Î»,Î»
is a valid norm-induced metric over measures whenever kis characteristic. Thus, Uliftsthe ground metric
cto that over measures.
The proof of Corollary 4.2 is detailed in Appendix B.2. This result also reveals interesting relationships be-
tweenUk,c,Î»,Î», the Kantorovich metric, Kc, and the MMD metric used for regularization. This is summarized
in the following two results.
Corollary 4.3. (Interpolant) In addition to assumptions in Corollary 4.2, if the kernel is c-universal
(continuous and universal), then âˆ€s0,t0âˆˆ R+(X),limÎ»â†’âˆUk,c,Î»,Î» (s0,t0) =Kc(s0,t0). Fur-
ther, if the cost metric, c, dominates the characteristic kernel, k, induced metric, i.e., c(x,y)â‰¥/radicalbig
k(x,x) +k(y,y)âˆ’2k(x,y)âˆ€x,yâˆˆ X, thenUk,c,Î»,Î» (s0,t0) =Î»MMDk(s0,t0)whenever 0< Î»â‰¤1.
Finally, when Î»âˆˆ(0,1), MMD-UOT interpolates between the scaled MMD and the Kantorovich metric. The
nature of this interpolation is already described in terms of generating sets in Corollary 4.2.
We illustrate this interpolation result in Figure 1. Our proof of Corollary 4.3, presented in Appendix B.3,
also shows that the Euclidean distance satisfies such a dominating cost assumption when the kernel employed
is the Gaussian kernel and the inputs lie on a unit-norm ball. The next result presents another relationship
between the metrics in the discussion.
Corollary 4.4.Uk,c,Î»,Î» (s,t)â‰¤min (Î»MMDk(s,t),Kc(s,t)).
TheproofofCorollary4.4isstraightforwardandispresentedinAppendixB.5. Thisresultenablesustoshow
properties like weak metrization and sample efficiency with MMD-UOT. For a sequence snâˆˆR+
1(X), nâ‰¥1,
we say that snweakly converges to sâˆˆ R+
1(X)(denoted as snâ‡€ s), if and only if EXâˆ¼sn[f(X)]â†’
EXâˆ¼s[f(X)]for all bounded continuous functions over X. It is natural to ask when is the convergence in
metric over measures equivalent to weak convergence on measures. The metric is then said to metrize the
6Published in Transactions on Machine Learning Research (01/2024)
0<ğœ†â‰¤1 ; Kernel-based cost.ğœ†âˆˆ(1,âˆ)ğœ†â†’âˆ  (a) MMD                                 (b) Kantorovich                    (c) New UOT metrics
Figure 1: For illustration, the generating set of Kantorovich-Wasserstein is depicted as a triangle, and the
scaled generating set of MMD is depicted as a disc. The intersection represents the generating set of the
IPM metric induced by MMD-UOT. (a) shows the special case when our MMD-UOT metric recovers back
the sample-efficient MMD metric, (b) shows the special case when our MMD-UOT metric reduces to the
Kantorovich-Wasserstein metric that lifts the ground metric to measures, and (c) shows the resulting family
of new UOT metrics which are both sample-efficient and can lift ground metrics to measures.
weak convergence of measures or is equivalently said to weakly metrize measures. The weak metrization
properties of the Wasserstein metric and MMD are well-understood (e.g., refer to Theorem 6.9 in (Villani,
2009) and Theorem 7 in (Simon-Gabriel et al., 2020)). The weak metrization property of Uk,c,Î»,Î»follows
from the above Corollary 4.4.
Corollary 4.5. (Weak Metrization) Uk,c,Î»,Î»metrizes the weak convergence of normalized measures.
The proof is presented in Appendix B.6. We now show that the metric induced by MMD-UOT inherits
the attractive statistical efficiency of the MMD metric. In typical machine learning applications, only
finite samples are given from the measures. Hence, it is important to study statistically efficient metrics
that alleviate the curse of dimensionality problem prevalent in OT (Niles-Weed & Rigollet, 2019). Sample
complexity result with the metric induced by MMD-UOT is presented as follows.
Corollary 4.6. (Sample Complexity) Let us denoteUk,c,Î»,Î», defined in (6), by Â¯U. Let Ë†sm,Ë†tmdenote the
empirical estimates of s0,t0âˆˆR+(X)respectively with msamples. Then, Â¯U(Ë†sm,Ë†tm)â†’Â¯U(s0,t0)at a rate
(apart from constants) same as that of MMDk(Ë†sm,s0)â†’0.
Since the sample complexity of MMD with a normalized characteristic kernel is O(mâˆ’1
2)(Smola et al.,
2007), the same will be the complexity bound for the corresponding MMD-UOT. The proof of Corollary 4.6 is
presented in Appendix B.7. This is interesting because, though MMD-UOT can arbitrarily well approximate
Wasserstein (as Î»â†’âˆ), its estimation can be far more efficient than O/parenleftï£¬ig
mâˆ’1
d/parenrightï£¬ig
, which is the minimax
estimation rate for the Wasserstein (Niles-Weed & Rigollet, 2019). Here, dis the dimensionality of the
samples. Further, in Lemma B4, we show that even when MMDq
k(qâ‰¥2âˆˆN) is used for regularization, the
sample complexity again comes out to be O/parenleftï£¬ig
mâˆ’1
2/parenrightï£¬ig
. We conclude this section with a couple of remarks.
Remark 4.7. As a side result, we prove the following theorem (Appendix B.8) that relates our MMD-UOT to
the MMD-regularized Kantorovich metric. We believe this connection is interesting as it generalizes the pop-
ular Kantorovich-Rubinstein duality result on relating (unregularized) OT to the (unregularized) Kantorovich
metric.
Theorem 4.8. In addition to the assumptions in Theorem 4.1, if cis a valid metric, then
Uk,c,Î» 1,Î»2(s0,t0) = min
s,tâˆˆR(X)Kc(s,t) +Î»1MMDk(s,s0) +Î»2MMDk(t,t0). (8)
Remark 4.9. It is noteworthy that most of our theoretical results presented in this section not only hold
with the MMD-UOT formulation (9) but also with a general IPM-regularized UOT formulation, which we
discuss in Appendix B. This generalization may be of independent interest for future work.
Finally, minor results on robustness and connections with spectral normalized GAN (Miyato et al., 2018)
are discussed in Appendix B.16 and Appendix B.17, respectively.
7Published in Transactions on Machine Learning Research (01/2024)
4.1 Finite-Sample-based Estimation
As noted in Corollary 4.6, MMD-UOT can be efficiently estimated from samples of source and target.
However, one needs to solve an optimization problem over all possible joint (un-normalized) measures. This
can be computationally expensive1(for example, optimization over the set of all joint density functions).
Hence, in this section, we propose a simple estimator where the optimization is only over the joint measures
supported at sample-based points. We show that our estimator is statistically consistent and that the
estimation is free from the curse of dimensionality.
Letmsamples be given from the source, target, s0, t0âˆˆ R+(X)respectively2. We denoteDi=
{xi1,Â·Â·Â·xim},i= 1,2as the set of samples given from s0,t0respectively. Let Ë†sm,Ë†tmdenote the empiri-
cal measures using samples D1,D2. Let us denote the Gram-matrix of DibyGii. LetC12be themÃ—m
cost matrix with entries as evaluations of the cost function over D1Ã—D 2. Following the common practice
in OT literature (Chizat et al., 2017; Cuturi, 2013; Damodaran et al., 2018; Fatras et al., 2021; Le et al.,
2021; Balaji et al., 2020; Nath & Jawanpuria, 2020; PeyrÃ© & Cuturi, 2019), we restrict the transport plan
to be supported on the finite samples from each of the measures in order to avoid the computational issues
in optimizing over all possible joint densities. More specifically, let Î±be themÃ—m(parameter/variable)
matrix with entries as Î±ijâ‰¡Ï€(x1i,x2j)wherei,jâˆˆ{1,Â·Â·Â·,m}. With these notations and the mentioned
restricted feasibility set, Problem (6) simplifies to the following, denoted by Ë†Um(Ë†sm,Ë†tm):
min
Î±â‰¥0âˆˆRmÃ—mTr/parenleftbig
Î±CâŠ¤
12/parenrightbig
+Î»1/vextenddouble/vextenddouble/vextenddoubleÎ±1âˆ’Ïƒ1
m1/vextenddouble/vextenddouble/vextenddouble
G11+Î»2/vextenddouble/vextenddouble/vextenddoubleÎ±âŠ¤1âˆ’Ïƒ2
m1/vextenddouble/vextenddouble/vextenddouble
G22, (9)
where Tr (M)denotes the trace of matrix M,âˆ¥xâˆ¥Mâ‰¡âˆš
xâŠ¤Mx, andÏƒ1,Ïƒ2are the masses of the source,
target measures, s0,t0, respectively. Since this is a Convex Program over a finite-dimensional variable, it
can be solved in a computationally efficient manner (refer Section 4.2).
However, as the transport plan is now supported on the given samples alone, Corollary 4.6 does not apply.
Thefollowingresultshowsthatourestimator(9)isconsistent, andtheestimationerrordecaysatafavourable
rate.
Theorem 4.10. (Consistency of the proposed estimator) Let us denoteUk,c,Î» 1,Î»2, defined in (6),
byÂ¯U. Assume the domain Xis compact, ground cost is continuous, câˆˆC(XÃ—X ), and the kernel kis
c-universal, normalized. Let the source measure ( s0), the target measure ( t0), as well as the corresponding
MMD-UOT transport plan be absolutely continuous. Also assume s0(x),t0(x)>0âˆ€xâˆˆX. Then, we
have w.h.p. and any (arbitrarily small) Ïµ>0that/vextendsingle/vextendsingle/vextendsingleË†Um(Ë†sm,Ë†tm)âˆ’Â¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingleâ‰¤O/parenleftï£¬ig
Î»1+Î»2âˆšm+g(Ïµ)
m+ÏµÏƒ/parenrightï£¬ig
. Here,
g(Ïµ)â‰¡minvâˆˆHkâŠ—Hkâˆ¥vâˆ¥ks.t.âˆ¥vâˆ’câˆ¥âˆâ‰¤Ïµ, andÏƒis the mass of the optimal MMD-UOT transport plan.
Further, if cbelongs toHkâŠ—Hk, then w.h.p./vextendsingle/vextendsingle/vextendsingleË†Um(Ë†sm,Ë†tm)âˆ’Â¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingleâ‰¤O/parenleftï£¬ig
Î»1+Î»2âˆšm/parenrightï£¬ig
.
We discuss the proof of the above theorem in Appendix B.9. Because kis universal, g(Ïµ)<âˆâˆ€Ïµ >0.
The consistency of our estimator as mâ†’ âˆcan be realized, if, for example, one employs the scheme
Î»1=Î»2=O(m1/4)andÏµâ†’0at a slow enough rate such thatg(Ïµ)
mâ†’0. In Appendix B.9.1, we show that
even ifÏµdecays as fast as O/parenleftbig1
m2/3/parenrightbig
, theng(Ïµ)blows-up atmost as O/parenleftbig
m1/3/parenrightbig
. Hence, overall, the estimation
error still decays as O/parenleftbig1
m1/4/parenrightbig
. To the best of our knowledge, such consistency results have not been studied
in the context of KL-regularized UOT.
4.2 Computational Aspects
Problem (9) is an instance of a convex program and can be solved using the mirror descent algorithm detailed
in Appendix B.10. In the following, we propose to solve an equivalent optimization problem which helps us
leverage faster solvers for MMD-UOT:
min
Î±â‰¥0âˆˆRmÃ—mTr/parenleftbig
Î±CâŠ¤
12/parenrightbig
+Î»1/vextenddouble/vextenddouble/vextenddoubleÎ±1âˆ’Ïƒ1
m1/vextenddouble/vextenddouble/vextenddouble2
G11+Î»2/vextenddouble/vextenddouble/vextenddoubleÎ±âŠ¤1âˆ’Ïƒ2
m1/vextenddouble/vextenddouble/vextenddouble2
G22. (10)
1Note that this challenge is inherent to OT (and all its variants). It is not a consequence of our choice of MMD regularization.
2The no. of samples from source and target need not be the same, in general.
8Published in Transactions on Machine Learning Research (01/2024)
Algorithm 1 Accelerated Projected Gradient Descent for solving Problem (10).
Require: Lipschitz constant L, initialÎ±0â‰¥0âˆˆRmÃ—m.
f(Î±) =Tr/parenleftbig
Î±CâŠ¤
12/parenrightbig
+Î»1/vextenddouble/vextenddoubleÎ±1âˆ’Ïƒ1
m1/vextenddouble/vextenddouble2
G11+Î»2/vextenddouble/vextenddoubleÎ±âŠ¤1âˆ’Ïƒ2
m1/vextenddouble/vextenddouble2
G22.
Î³1= 1.
y1=Î±0.
i= 0.
whilenot converged do
Î±i=Projectâ‰¥0/parenleftbig
yiâˆ’1
Lâˆ‡f(yi)/parenrightbig
.
Î³i+1=1+âˆš
1+4Î³2
i
2.
yi+1=Î±i+Î³iâˆ’1
Î³i+1(Î±iâˆ’Î±iâˆ’1).
i=i+ 1.
end while
returnÎ±i.
The equivalence between (9) and (10) follows from standard arguments and is detailed in Appendix B.11.
Our next result shows that the objective in (10) is L-smooth (proof provided in Appendix B.12).
Lemma 4.11. The objective in Problem (10) is L-smooth with L =
2/radicalbig
(Î»1m)2âˆ¥G11âˆ¥2
F+ (Î»2m)2âˆ¥G22âˆ¥2
F+ 2Î»1Î»2(1âŠ¤mG111m+1âŠ¤mG221m).
The above result enables us to use the accelerated projected gradient descent (APGD) algorithm (Nesterov,
2003; Beck & Teboulle, 2009) with fixed step-size Ï„= 1/Lfor solving (10). The detailed steps are pre-
sented in Algorithm 1. The overall computation cost for solving MMD-UOT (10) is O(m2
âˆšÏµ), whereÏµis the
optimality gap. In Section 5, we empirically observe that the APGD-based solver for MMD-UOT is indeed
computationally efficient.
4.3 Barycenter
A related problem is that of barycenter interpolation of measures (Agueh & Carlier, 2011), which has
interesting applications (Solomon et al., 2014; 2015; Gramfort et al., 2015). Given measures s1,...,snwith
total masses Ïƒ1,...,Ïƒnrespectively, and interpolation weights Ï1,...,Ïn, the barycenter sâˆˆ R+(X)is
defined as the solution of Â¯B(s1,Â·Â·Â·,sn)â‰¡minsâˆˆR+(X)/summationtextn
i=1ÏiUk,c,Î» 1,Î»2(si,s).
In typical applications, only sample sets, Di, fromsiare available instead of sithemselves. Let us denote
the corresponding empirical measures by Ë†s1,..., Ë†sn. One way to estimate the barycenter is to consider
Â¯B(Ë†s1,Â·Â·Â·,Ë†sn). However, this may be computationally challenging to optimize, especially when the measures
involved are continuous. So we propose estimating the barycenter with the restriction that the transport
planÏ€icorresponding to Uk,c,Î» 1,Î»2(Ë†si,s)is supported onDiÃ—âˆªn
i=1Di. And, letÎ±iâ‰¥0âˆˆRmiÃ—mdenote the
corresponding probabilities. Following (Cuturi & Doucet, 2014), we also assume that the barycenter, s, is
supported onâˆªn
i=1Di. Let us denote the barycenter problem with this support restriction on the transport
plans and the Barycenter as Ë†Bm(Ë†s1,Â·Â·Â·,Ë†sn). LetGbe the Gram-matrix of âˆªn
i=1DiandCibe themiÃ—m
matrix with entries as evaluations of the cost function.
Lemma 4.12. The barycenter problem Ë†Bm(Ë†s1,Â·Â·Â·,Ë†sn)can be equivalently written as:
minÎ±1,Â·Â·Â·,Î±nâ‰¥0/summationtextn
i=1Ïi/parenleftï£¬ig
Tr/parenleftbig
Î±iCâŠ¤
i/parenrightbig
+Î»1âˆ¥Î±i1âˆ’Ïƒi
mi1âˆ¥2
Gii+Î»2âˆ¥Î±âŠ¤
i1âˆ’/summationtextn
j=1ÏjÎ±âŠ¤
j1âˆ¥2
G/parenrightï£¬ig
.(11)
We present the proof in Appendix B.14.1. Similar to Problem (10), the objective in Problem (11) is a smooth
quadratic program in each Î±iand is jointly convex in Î±iâ€™s. In Appendix B.14.2, we also present the details
for solving Problem (11) using APGD as well as its statistical consistency in Appendix B.14.3.
9Published in Transactions on Machine Learning Research (01/2024)
ÏµKL-UOT plan                  MMD-UOT plan    Barycenter
             (a)           (b)         (c)ğœ–
Figure 2: (a) Optimal Transport plans of ÏµKL-UOT and MMD-UOT; (b) Barycenter interpolating between
Gaussian measures. For the chosen hyperparameter, the barycenters of ÏµKL-UOT and MMD-UOT overlap
and can be looked as smooth approximations of the OT barycenter; (c) Objective vs Time plot comparing
ÏµKL-UOT solved using the popular Sinkhorn algorithm (Chizat et al., 2017; Pham et al., 2020) and MMD-
UOT (10) solved using APGD. A plot showing ÏµKL-UOTâ€™s progress at the initial phase is given in Figure 4.
5 Experiments
In Section 4, we examined the theoretical properties of the proposed MMD-UOT formulation. In this section,
we show that MMD-UOT is a good practical alternative to the popular entropy-regularized ÏµKL-UOT. We
emphasize that our purpose is not to benchmark state-of-the-art performance. Our codes are publicly
available at https://github.com/Piyushi-0/MMD-reg-OT.
5.1 Synthetic Experiments
We present some synthetic experiments to visualize the quality of our solution. Please refer to Appendix C.1
for more details.
Transport Plan and Barycenter We perform synthetic experiments with the source and target as
Gaussian measures. We compare the OT plan of ÏµKL-UOT and MMD-UOT in Figure 2(a). We observe
that the MMD-UOT plan is sparser compared to the ÏµKL-UOT plan. In Figure 2(b), we visualize the
barycenter interpolating between the source and target, obtained with MMD, ÏµKL-UOT and MMD-UOT.
While MMD barycenter is an empirical average of the measures and hence has two modes, the geometry
of measures is considered in both ÏµKL-UOT and MMD-UOT formulations. Barycenters obtained by these
methods have the same number of modes (one) as in the source and the target. Moreover, they appear to
smoothly approximate the barycenter obtained with OT (solved using a linear program).
Visualizing the Level Sets Applications like generative modeling deal with optimization over the pa-
rameter (Î¸) of the source distribution to match the target distribution. In such cases, it is desirable that
the level sets of the distance function over the measures show a lesser number of stationary points that are
not global optima (Bottou et al., 2017). Similar to (Bottou et al., 2017), we consider a model family for
source distributions as F={PÎ¸=1
2(Î´Î¸+Î´âˆ’Î¸) :Î¸âˆˆ[âˆ’1,1]x[âˆ’1,1]}and a fixed target distribution Q
asP(2,2)/âˆˆF. We compute the distances between PÎ¸andQaccording to various divergences. Figure 3
presents level sets showing the set of distances {d(PÎ¸,Q) :Î¸âˆˆ[âˆ’1,1]x[âˆ’1,1]}where the distance d(.,.)is
measured using MMD, Kantorovich metric, ÏµKL-UOT, and MMD-UOT (9), respectively. While all methods
correctly identify global minima (green arrow), level sets with MMD-UOT and ÏµKL-UOT show no local
minima (encircled in red for MMD) and have a lesser number of non-optimal stationary points (marked with
black arrows) compared to the Kantorovich metric in Figure 3(b).
Computation Time In Figure 2(c), we present the objective versus time plot. The source and target
measures are chosen to be the same, in which case the optimal objective is 0. MMD-UOT (10) solved using
10Published in Transactions on Machine Learning Research (01/2024)
(a)                                                  (b)                                                  (c)         (d)   
Figure 3: Level sets of distance function between a family of source distributions and a fixed target distri-
bution with the task of finding the source distribution closest to the target distribution using (a) MMD, (b)
Â¯W2, (c)ÏµKL-UOT, and (d) MMD-UOT. While all methods correctly identify global minima (green arrows),
level sets with MMD-UOT and ÏµKL-UOT show no local minima (encircled in red for MMD) and have a
lesser number of non-optimal stationary points (marked with black arrows) compared to (b).
Table 2: Average Test Power (between 0 and 1; higher is better) on MNIST. MMD-UOT obtains the highest
average test power at all timesteps.
N MMD ÏµKL-UOT MMD-UOT
100 0.137 0.099 0.154
200 0.258 0.197 0.333
300 0.467 0.242 0.588
400 0.656 0.324 0.762
500 0.792 0.357 0.873
10000.909 0.506 0.909
APGD (described in Section 4.2) gives a much faster rate of decrease in objective compared to the Sinkhorn
algorithm used for solving KL-UOT.
5.2 Two-Sample Hypothesis Test
Given two sets of samples {x1,...,xm}âˆ¼s0and{y1,...,ym}âˆ¼t0, the two-sample test aims to determine
whether the two sets of samples are drawn from the same distributions, viz., to predict if s0=t0. The
performance evaluation in the two-sample test relies on two types of errors. Type-I error occurs when
s0=t0, but the algorithm predicts otherwise. Type-II error occurs when the algorithm incorrectly predicts
s0=t0. The probability of Type-I error is called the significance level. The significance level can be
controlled using permutation test-based setups (Ernst, 2004; Liu et al., 2020). Algorithms are typically
compared based on the empirical estimate of their test power (higher is better), defined as the probability
of not making a Type-II error and the average Type-I error (lower is better).
Dataset and experimental setup. Following (Liu et al., 2020), we consider the two sets of samples, one
fromthetrueMNIST(LeCun&Cortes,2010)andanotherfromfakeMNISTgeneratedbytheDCGAN(Bian
et al., 2019). The data lies in 1024 dimensions. We take an increasing number of samples ( N) and compute
the average test power over 100 pairs of sets for each value of N. We repeat the experiment 10 times and
report the average test power in Table 2 for the significance level Î±= 0.05. By the design of the test, the
average Type-I error was upper-bounded, and we noted the Type-II error in our experiment. We detail
the procedure for choosing the hyperparameters and the list of chosen hyperparameters for each method in
Appendix C.2.
Results. In Table 2, we observe that MMD-UOT obtains the highest test power for all values of N. The
average test power of MMD-UOT is 1.5âˆ’2.4times better than that of ÏµKL-UOT across N. MMD-UOT also
outperforms EMD and 2-Wasserstein, which suffer from the curse of dimensionality, for all values of N. Our
results match the sample efficient MMD metricâ€™s result on increasing Nto 1000, but for lesser sample-size,
MMD-UOT is always better than MMD.
11Published in Transactions on Machine Learning Research (01/2024)
Table 3: MMD distance (lower is better) between computed barycenter and the ground truth distribution.
A sigma-heuristics based RBF kernel is used to compute the MMD distance. We observe that MMD-UOTâ€™s
results are closer to the ground truth than the baselinesâ€™ results at all timesteps.
Timestep MMD ÏµKL-UOT MMD-UOT
t1 0.375 0.391 0.334
t2 0.190 0.184 0.179
t3 0.125 0.138 0.116
Avg. 0.230 0.238 0.210
5.3 Single-Cell RNA Sequencing
We empirically evaluate the quality of our barycenter in the Single-cell RNA sequencing experiment. Single-
cell RNA sequencing technique (scRNA-seq) helps us understand how the expression profile of the cells
changes (Schiebingeretal.,2019). BarycenterestimationintheOTframeworkoffersaprincipledapproachto
estimate the trajectory of a measure at an intermediate timestep t(ti<t<tj) when we have measurements
available only at ti(source) and tj(target) time steps.
Dataset and experimental setup. We perform experiments on the Embryoid Body (EB) single-cell
dataset (Moon et al., 2019). The dataset has samples available at five timesteps ( tjwithj= 0,..., 4), which
were collected during a 25-day period of development of the human embryo. Following (Tong et al., 2020),
we project the data onto two-dimensional space and associate uniform measures to the source and the target
samples given at different timesteps. We consider the samples at timestep tiandti+2as the samples from
the source and target measures where 0â‰¤iâ‰¤2and aim at estimating the measure at titimestep as their
barycenter with equal interpolation weights Ï1=Ï2= 0.5.
We compute the barycenters using MMD-UOT (11) and the ÏµKL-UOT (Chizat et al., 2018; Liero et al.,
2018) approaches. For both, a simplex constraint is used to cater to the case of uniform measures. We also
compare against the empirical average of the source and target measures, which is the barycenter obtained
with the MMD metric. The computed barycenter is evaluated against the measure corresponding to the
ground truth samples available at the corresponding timestep. We compute the distance between the two
using the MMD metric with RBF kernel (Gretton et al., 2012). The hyperparameters are chosen based on
the leave-one-out validation protocol. More details and some additional results are in Appendix C.3.
Results. Table 3 shows that MMD-UOT achieves the lowest distance from the ground truth for all the
timesteps, illustrating its superior interpolation quality.
5.4 Domain Adaptation in JUMBOT framework
OT has been widely employed in domain adaptation problems (Courty et al., 2017; Courty et al., 2017; Seguy
et al., 2018; Damodaran et al., 2018). JUMBOT (Fatras et al., 2021) is a popular domain adaptation method
based onÏµKL-UOT that outperforms OT-based baselines. JUMBOTâ€™s loss function involves a cross-entropy
term andÏµKL-UOT discrepancy term between the source and target distributions. We showcase the utility
of MMD-UOT (10) in the JUMBOT (Fatras et al., 2021) framework.
Dataset and experimental setup: We perform the domain adaptation experiment with and Digits
datasets comprising of MNIST (LeCun & Cortes, 2010), M-MNIST (Ganin et al., 2016), SVHN (Netzer
et al., 2011), USPS (Hull, 1994) datasets. We replace the ÏµKL-UOT based loss with the MMD-UOT loss
(10), keeping the other experimental set-up the same as JUMBOT. We obtain JUMBOTâ€™s result with
ÏµKL-UOT with the best-reported hyperparameters (Fatras et al., 2021). Following JUMBOT, we tune
hyperparameters of MMD-UOT for the Digits experiment on USPS to MNIST (U âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’M) domain adaptation
task and use the same hyperparameters for the rest of the domain adaptation tasks on Digits. More details
are in Appendix C.4.
12Published in Transactions on Machine Learning Research (01/2024)
Table 4: Target domain accuracy (higher is better) obtained in domain adaptation experiments. Results for
ÏµKL-UOT are reproduced from the code open-sourced for JUMBOT in (Fatras et al., 2021). MMD-UOT
outperforms ÏµKL-UOT in all the domain adaptation tasks considered.
Source Target ÏµKL-UOT MMD-UOT
M-MNIST USPS 91.53 94.97
M-MNIST MNIST 99.35 99.50
MNIST M-MNIST 96.51 96.96
MNIST USPS 96.51 97.01
SVHN M-MNIST 94.26 95.35
SVHN MNIST 98.68 98.98
SVHN USPS 92.78 93.22
USPS MNIST 96.76 98.53
Avg. 95.80 96.82
Results: Table 4 reports the accuracy obtained on target datasets. We observe that MMD-UOT-based loss
performs better than ÏµKL-UOT-based loss for all the domain adaptation tasks. In Figure 8 (appendix), we
also compare the t-SNE plot of the embeddings learned with the MMD-UOT and the ÏµKL-UOT-based loss
functions. The clusters learned with MMD-UOT are better separated (e.g., red- and cyan-colored clusters).
5.5 More Results on Domain Adaptation
In Section 5.4, we compared the proposed MMD-UOT-based loss function with the ÏµKL-UOT based loss
function in the JUMBOT framework (Fatras et al., 2021). It should be noted that JUMBOT has a ResNet-
50 backbone. Hence, in this section, we also compare with popular domain adaptation baselines having
ResNet-50 backbone. These include DANN (Ganin et al., 2015), CDANN-E (Long et al., 2017), DEEPJ-
DOT(Damodaranetal.,2018), ALDA(Chenetal.,2020a), ROT(Balajietal.,2020), andBombOT(Nguyen
et al., 2022). BombOT is a recent state-of-the-art OT-based method for unsupervised domain adaptation
(UDA). As in JUMBOT (Fatras et al., 2021), BombOT also employs ÏµKL-UOT based loss function. We
also include the results of the baseline ResNet-50 model, where the model is trained on the source and is
evaluated on the target without employing any adaptation techniques.
Office-Home dataset: We evaluate the proposed method on the Office-Home dataset (Venkateswara
et al., 2017), popular for unsupervised domain adaptation. We use the backbone network of ResNet-50
following. The Office-Home dataset has 15,500 images from four domains: Artistic images (A), Clip Art (C),
Product images (P) and Real-World (R). The dataset contains images of 65 object categories common in
office and home scenarios for each domain. Following (Fatras et al., 2021; Nguyen et al., 2022), evaluation is
done in 12 adaptation tasks. Following JUMBOT, we validate the proposed method on the A â†’C task and
use the chosen hyperparameters for the rest of the tasks.
Table 5 reports the target accuracies obtained by different methods. The results of the BombOT method are
quoted from (Nguyen et al., 2022), and the results of other baselines are quoted from (Fatras et al., 2021).
We observe that the proposed MMD-UOT-based method achieves the best target accuracy in 11out of 12
adaptation tasks.
VisDA-2017 dataset: We next consider the next domain adaptation task between the training and
validation sets of the VisDA-2017 (Recht et al., 2018) dataset. We follow the experimental setup detailed in
(Fatras et al., 2021). The source domain of VisDA has 152,397 synthetic images, while the target domain
has 55,388 real-world images. Both the domains have 12 object categories.
Table 6 compares the performance of different methods. The results of the BombOT method are quoted from
(Nguyen et al., 2022), and the results of other baselines are quoted from (Fatras et al., 2021). The proposed
13Published in Transactions on Machine Learning Research (01/2024)
Table 5: Target accuracies (higher is better) on the Office-Home dataset in the UDA setting. The letters
denote different domains: â€˜Aâ€™ for Artistic images, â€˜Pâ€™ for Product images, â€˜Câ€™ for Clip art and â€˜Râ€™ for Real-
World images. The proposed method achieves the highest accuracy on almost all the domain adaptation
tasks and achieves the best accuracy averaged across the tasks.
Method Aâ†’C Aâ†’P Aâ†’R Câ†’A Câ†’P Câ†’R Pâ†’A Pâ†’C Pâ†’R Râ†’A Râ†’C Râ†’PAvg
ResNet-50 34.9 50.0 58.0 37.4 41.9 46.2 38.5 31.2 60.4 53.9 41.2 59.9 46.1
DANN44.3 59.8 69.8 48.0 58.3 63.0 49.7 42.7 70.6 64.0 51.7 78.3 58.3(Ganin et al., 2015)
CDAN-E52.5 71.4 76.1 59.7 69.9 71.5 58.7 50.3 77.5 70.5 57.9 83.5 66.6(Long et al., 2017)
DEEPJDOT50.7 68.7 74.4 59.9 65.8 68.1 55.2 46.3 73.8 66.0 54.9 78.3 63.5(Damodaran et al., 2018)
ALDA52.2 69.3 76.4 58.7 68.2 71.1 57.4 49.6 76.8 70.6 57.3 82.5 65.8(Chen et al., 2020a)
ROT47.2 71.8 76.4 58.6 68.1 70.2 56.5 45.0 75.8 69.4 52.1 80.6 64.3(Balaji et al., 2020)
ÏµKL-UOT (JUMBOT)55.2 75.5 80.8 65.5 74.4 74.9 65.2 52.7 79.2 73.0 59.9 83.4 70.0(Fatras et al., 2021)
BombOT56.2 75.2 80.5 65.8 74.6 75.4 66.2 53.2 80.0 74.2 60.183.3 70.4(Nguyen et al., 2022)
Proposed 56.5 77.2 82.0 70.0 77.1 77.8 69.3 55.1 82.0 75.5 59.3 84.0 72.2
Table 6: Target accuracies (higher is better) on the VisDA-2017 dataset in the UDA setting. The proposed
MMD-UOT method achieves the highest accuracy.
Dataset CDAN-E ALDA DEEPJDOT ROT ÏµKL-UOT (JUMBOT) BombOT Proposed
VisDA-2017 70.1 70.5 68.0 66.3 72.5 74.6 77.0
method achieves the best performance, improving the accuracy obtained by ÏµKL-UOT based JUMBOT and
BombOT methods by 4.5%and2.4%, respectively.
5.6 Prompt Learning for Few-Shot Classification
The task of learning prompts (e.g. â€œa tall bird of [class]â€) for vision-language models has emerged as a
promising approach to adapt large pre-trained models like CLIP (Radford et al., 2021) for downstream
tasks. The similarity between prompt features (which are class-specific) and visual features of a given image
can help us classify the image. A recent OT-based prompt learning approach, PLOT (Chen et al., 2023),
obtainedstate-of-the-artresultsonthe K-shotrecognitiontaskinwhichonly Kimagesperclassareavailable
during training. We evaluate the performance of MMD-UOT following the setup of (Chen et al., 2023) on
the benchmark EuroSAT (Helber et al., 2018) dataset consisting of satellite images, DTD (Cimpoi et al.,
2014) dataset having images of textures and Oxford-Pets (Parkhi et al., 2012) dataset having images of pets.
Results With the same evaluation protocol as in (Chen et al., 2023), we report the classification accuracy
averaged over three seeds in Table 7. We note that MMD-UOT-based prompt-learning achieves better results
than PLOT, especially when Kis less (more challenging case due to lesser training data). With the EuroSAT
dataset, the improvement is as high as 4% for a challenging case of K=1. More details are in Appendix C.5.
14Published in Transactions on Machine Learning Research (01/2024)
Table 7: Average and standard deviation (over 3 runs) of accuracy (higher is better) on the k-shot classifi-
cation task, shown for different values of shots ( k) in the state-of-the-art PLOT framework. The proposed
method replaces OT with MMD-UOT in PLOT, keeping all other hyperparameters the same. The results
of PLOT are taken from their paper (Chen et al., 2023).
Dataset Method 1 2 4 8 16
EuroSATPLOT 54.05 Â±5.95 64.21Â±1.9072.36Â±2.2978.15Â±2.65 82.23Â±0.91
Proposed 58.47Â±1.37 66.0Â±0.93 71.97Â±2.2179.03Â±1.91 83.23Â±0.24
DTDPLOT 46.55 Â±2.6251.24Â±1.9556.03Â±0.43 61.70Â±0.35 65.60Â±0.82
Proposed 47.27Â±1.46 51.0Â±1.7156.40Â±0.73 63.17Â±0.69 65.90Â±0.29
6 Conclusion
The literature on unbalanced optimal transport (UOT) has largely focused on Ï•-divergence-based regu-
larization. Our work provides a comprehensive analysis of MMD-regularization in UOT, answering many
open questions. We prove novel results on the metricity and the sample efficiency of MMD-UOT, propose
consistent estimators which can be computed efficiently, and illustrate its empirical effectiveness on several
machine learning applications. Our theoretical and empirical contributions for MMD-UOT and its corre-
sponding barycenter demonstrate the potential of MMD-regularization in UOT as an effective alternative
toÏ•-divergence-based regularization. Interesting directions of future work include exploring applications of
IPM-regularized UOT (Remark 4.9) and the generalization of Kantorovich-Rubinstein duality (Remark 4.7).
7 Funding Disclosure and Acknowledgements
We thank Kilian Fatras for the discussions on the JUMBOT baseline, and Bharath Sriperumbudur (PSU)
and G. Ramesh (IITH) for discussions related to Appendix B.9.1. We are grateful to Rudraram Siddhi
Vinayaka. We also thank the anonymous reviewers for constructive feedback. PM and JSN acknowledge the
support of Google PhD Fellowship and Fujitsu Limited (Japan), respectively.
References
Rohit Agrawal and Thibaut Horel. Optimal bounds between f-divergences and integral probability metrics.
InICML, 2020.
Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. SIAM Journal on Mathematical
Analysis, 43(2):904â€“924, 2011.
David Alvarez-Melis and Tommi Jaakkola. Gromov-Wasserstein alignment of word embedding spaces. In
EMNLP, 2018.
Yuki Arase, Han Bao, and Sho Yokoi. Unbalanced optimal transport for unbalanced word alignment. In
ACL, 2023.
Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Robust optimal transport with applications in generative
modeling and domain adaptation. In NeurIPS , 2020.
Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences , 2(1):183â€“202, 2009.
A. Ben-Tal and A. Nemirovski. Lectures On Modern Convex Optimization, 2021.
Yuemin Bian, Junmei Wang, Jaden Jungho Jun, and Xiang-Qun Xie. Deep convolutional generative adver-
sarial network (dcgan) models for screening and design of small molecules targeting cannabinoid receptors.
Molecular Pharmaceutics , 16(11):4451â€“4460, 2019.
15Published in Transactions on Machine Learning Research (01/2024)
Alberto Bietti and Julien Mairal. Group invariance, stability to deformations, and complexity of deep
convolutional representations. Journal of Machine Learning Research , 20:25:1â€“25:49, 2017.
Alberto Bietti, GrÃ©goire Mialon, Dexiong Chen, and Julien Mairal. A kernel perspective for regularizing
deep neural networks. In ICML, 2019.
Leon Bottou, Martin Arjovsky, David Lopez-Paz, and Maxime Oquab. Geometrical insights for implicit
generative modeling. Braverman Readings in Machine Learning 2017 , pp. 229â€“268, 2017.
Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Prompt learning
with optimal transport for vision-language models. In ICLR, 2023.
Minghao Chen, Shuai Zhao, Haifeng Liu, and Deng Cai. Adversarial-learned loss for domain adaptation. In
AAAI, 2020a.
Yimeng Chen, Yanyan Lan, Ruinbin Xiong, Liang Pang, Zhiming Ma, and Xueqi Cheng. Evaluating natural
language generation via unbalanced optimal transport. In IJCAI, 2020b.
Xiuyuan Cheng and Alexander Cloninger. Classification logit two-sample testing by neural networks for
differentiating near manifold densities. IEEE Transactions on Information Theory , 68:6631â€“6662, 2019.
L. Chizat, G. Peyre, B. Schmitzer, and F.-X. Vialard. Unbalanced optimal transport: Dynamic and kan-
torovich formulations. Journal of Functional Analysis , 274(11):3090â€“3123, 2018.
LÃ©naÃ¯c Chizat, Gabriel PeyrÃ©, Bernhard Schmitzer, and FranÃ§ois-Xavier Vialard. Scaling algorithms for
unbalanced optimal transport problems. Math. Comput. , 87:2563â€“2609, 2017.
LenaÃ¯c Chizat. Unbalanced optimal transport : Models, numerical methods, applications. Technical report,
Universite Paris sciences et lettres, 2017.
Kacper P. Chwialkowski, Aaditya Ramdas, D. Sejdinovic, and Arthur Gretton. Fast two-sample testing with
analytic representations of probability measures. In NIPS, 2015.
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In CVPR,
2014.
Samuel Cohen, Michael Arbel, and Marc Peter Deisenroth. Estimating barycenters of measures in high
dimensions. arXiv preprint arXiv:2007.07105 , 2020.
N. Courty, R. Flamary, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. IEEE
Transactions on Pattern Analysis and Machine Intelligence , 39(9):1853â€“1865, 2017.
Nicolas Courty, RÃ©mi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal
transportation for domain adaptation. In NIPS, 2017.
I. Csiszar. Information-type measures of difference of probability distributions and indirect observations.
Studia Scientiarum Mathematicarum Hungarica , 2:299â€“318, 1967.
M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.
Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In ICML, 2014.
Bharath Bhushan Damodaran, Benjamin Kellenberger, RÃ©mi Flamary, Devis Tuia, and Nicolas Courty.
DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation. In ECCV,
2018.
Henri De Plaen, Pierre-FranÃ§ois De Plaen, Johan A. K. Suykens, Marc Proesmans, Tinne Tuytelaars, and
Luc Van Gool. Unbalanced optimal transport: A unified framework for object detection. In CVPR, 2023.
Michael D. Ernst. Permutation Methods: A Basis for Exact Inference. Statistical Science , 19(4):676 â€“ 685,
2004.
16Published in Transactions on Machine Learning Research (01/2024)
Kilian Fatras, Thibault SÃ©journÃ©, Nicolas Courty, and RÃ©mi Flamary. Unbalanced minibatch optimal trans-
port; applications to domain adaptation. In ICML, 2021.
RÃ©mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, AurÃ©lie Boisbunon, Stanislas Cham-
bon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, LÃ©o Gautheron, Nathalie T.H.
Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien
Seguy, DanicaJ.Sutherland, RomainTavenard, AlexanderTong, andTitouanVayer. Pot: Pythonoptimal
transport. Journal of Machine Learning Research , 22(78):1â€“8, 2021.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso Poggio. Learning with
a wasserstein loss. In NIPS, 2015.
Yaroslav Ganin, E. Ustinova, Hana Ajakan, Pascal Germain, H. Larochelle, FranÃ§ois Laviolette, Mario
Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. In Journal of
Machine Learning Research , 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois Laviolette,
Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of
Machine Learning Research , 17(1):2096â€“2030, 2016.
Tryphon T. Georgiou, Johan Karlsson, and Mir Shahrouz Takyar. Metrics for power spectra: An axiomatic
approach. IEEE Transactions on Signal Processing , 57(3):859â€“867, 2009.
Alexandre Gramfort, Gabriel PeyrÃ©, and Marco Cuturi. Fast optimal transport averaging of neuroimaging
data. In Proceedings of 24th International Conference on Information Processing in Medical Imaging ,
2015.
Arthur Gretton. A simpler condition for consistency of a kernel independence test. arXiv: Machine Learning ,
2015.
Arthur Gretton, Karsten M. Borgwardt, Malte Rasch, Bernhard SchÃ¶lkopf, and Alexander J. Smola. A
kernel method for the two-sample-problem. In NIPS, 2006.
Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard SchÃ¶lkopf, and Alexander Smola. A
kernel two-sample test. Journal of Machine Learning Research , 13(25):723â€“773, 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved
training of wasserstein gans. In NIPS, 2017.
K. Gurumoorthy, P. Jawanpuria, and B. Mishra. SPOT: A framework for selection of prototypes using
optimal transport. In European Conference on Machine Learning and Knowledge Discovery in Databases
(ECML PKDD) , 2021.
Leonid G. Hanin. Kantorovich-rubinstein norm and its application in the theory of lipschitz spaces. In
Proceedings of the Americal Mathematical Society , volume 115, 1992.
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Introducing eurosat: A novel
dataset and deep learning benchmark for land use and land cover classification. In IGARSS 2018-2018
IEEE International Geoscience and Remote Sensing Symposium , pp. 204â€“207. IEEE, 2018.
J.J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 16(5):550â€“554, 1994.
P. Jawanpuria, M. Meghwanshi, and B. Mishra. Geometry-aware domain adaptation for unsupervised align-
ment of word embeddings. In Annual Meeting of the Association for Computational Linguistics , 2020.
Wittawat Jitkrittum, ZoltÃ¡n SzabÃ³, Kacper P. Chwialkowski, and Arthur Gretton. Interpretable distribution
features with maximum testing power. In NIPS, 2016.
17Published in Transactions on Machine Learning Research (01/2024)
Philip A. Knight. The sinkhornâ€“knopp algorithm: Convergence and applications. SIAM Journal on Matrix
Analysis and Applications , 30(1):261â€“275, 2008.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal
transport: Computational complexity and barycenter computation. In NeurIPS , 2021.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. http://yann.lecun.com/exdb/mnist/,
2010.
Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, and BarnabÃ¡s PÃ³czos. MMD GAN: Towards
Deeper Understanding of Moment Matching Network. In NIPS, 2017.
Yazhe Li, Roman Pogodin, Danica J. Sutherland, and Arthur Gretton. Self-supervised learning with kernel
dependence maximization. In NeurIPS , 2021.
Matthias Liero, Alexander Mielke, and Giuseppe SavarÃ©. Optimal transport in competition with reaction:
The hellinger-kantorovich distance and geodesic curves. SIAM J. Math. Anal. , 48:2869â€“2911, 2016.
Matthias Liero, Alexander Mielke, and Giuseppe SavarÃ©. Optimal entropy-transport problems and a new
hellingerâ€“kantorovich distance between positive measures. Inventiones mathematicae , 211(3):969â€“1117,
2018.
Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, Arthur Gretton, and Danica J. Sutherland. Learning deep
kernels for non-parametric two-sample tests. In ICML, 2020.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I. Jordan. Conditional adversarial domain
adaptation. In NIPS, 2017.
David Lopez-Paz and Maxime Oquab. evisiting classifier two-sample tests. In ICLR, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for gener-
ative adversarial networks. In ICLR, 2018.
Kevin R. Moon, David van Dijk, Zheng Wang, Scott Gigante, Daniel B. Burkhardt, William S. Chen,
Kristina Yim, Antonia van den Elzen, Matthew J. Hirn, Ronald R. Coifman, Natalia B. Ivanova, Guy
Wolf, and Smita Krishnaswamy. Visualizing structure and transitions for biological data exploration.
Nature Biotechnology , 37(12):1482â€“1492, 2019.
Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, and Bernhard SchÃ¶lkopf. Kernel mean em-
bedding of distributions: A review and beyond. Foundations and Trends Â®in Machine Learning , 10(1â€“2):
1â€“141, 2017.
Alfred Muller. Integral probability metrics and their generating classes of functions. Advances in Applied
Probability , 29:429â€“443, 1997.
J. Saketha Nath and Pratik Kumar Jawanpuria. Statistical optimal transport posed as learning kernel
embedding. In NeurIPS , 2020.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course , volume 87. Springer Science
& Business Media, 2003.
Yuval Netzer, Tiejie Wang, Adam Coates, A. Bissacco, Bo Wu, and A. Ng. Reading digits in natural images
with unsupervised feature learning. In NeurIPS , 2011.
Khai Nguyen, Dang Nguyen, Quoc Nguyen, Tung Pham, Hung Bui, Dinh Phung, Trung Le, and Nhat Ho.
On transportation of mini-batches: A hierarchical approach. In ICML, 2022.
Thanh Tang Nguyen, Sunil Gupta, and Svetha Venkatesh. Distributional reinforcement learning via moment
matching. In AAAI, 2021.
18Published in Transactions on Machine Learning Research (01/2024)
Jonathan Niles-Weed and Philippe Rigollet. Estimation of Wasserstein distances in the spiked transport
model. In Bernoulli , 2019.
O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.
Gabriel PeyrÃ© and Marco Cuturi. Computational optimal transport. Foundations and Trends Â®in Machine
Learning , 11(5-6):355â€“607, 2019.
Khiem Pham, Khang Le, Nhat Ho, Tung Pham, and Hung Bui. On unbalanced optimal transport: An
analysis of sinkhorn algorithm. In ICML, 2020.
Benedetto Piccoli and Francesco Rossi. Generalized wasserstein distance and its application to transport
equations with source. Archive for Rational Mechanics and Analysis , 211:335â€“358, 2014.
Benedetto Piccoli and Francesco Rossi. On properties of the generalized wasserstein distance. Archive for
Rational Mechanics and Analysis , 222, 12 2016.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML, 2021.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classifiers gener-
alize to CIFAR-10? arXiv, 2018.
Geoffrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subramanian, Aryeh Solomon, Joshua
Gould, Siyan Liu, Stacie Lin, Peter Berube, Lia Lee, Jenny Chen, Justin Brumbaugh, Philippe Rigollet,
Konrad Hochedlinger, Rudolf Jaenisch, Aviv Regev, and Eric S. Lander. Optimal-transport analysis
of single-cell gene expression identifies developmental trajectories in reprogramming. Cell, 176(4):928â€“
943.e22, 2019.
Vivien. Seguy, Bharath B. Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel.
Large-scale optimal transport and mapping estimation. In ICLR, 2018.
Carl-Johann Simon-Gabriel, Alessandro Barp, Bernhard SchÃ¶lkopf, and Lester Mackey. Metrizing weak
convergence with maximum mean discrepancies. arXiv, 2020.
Maurice Sion. On general minimax theorems. Pacific Journal of Mathematics , 8(1):171 â€“ 176, 1958.
Alexander J. Smola, Arthur Gretton, Le Song, and Bernhard SchÃ¶lkopf. A hilbert space embedding for
distributions. In ALT, 2007.
Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher. Wasserstein propagation for semi-
supervised learning. In ICML, 2014.
Justin Solomon, Fernando de Goes, Gabriel PeyrÃ©, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du,
and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric
domains. ACM Trans. Graph. , 34(4), 2015.
L. Song. Learning via hilbert space embedding of distributions. In PhD Thesis , 2008.
Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions
classes from videos in the wild. CoRR, 2012.
BharathK.Sriperumbudur, KenjiFukumizu, ArthurGretton, BernhardSchÃ¶lkopf, andGertR.G.Lanckriet.
On integral probability metrics, phi-divergences and binary classification. arXiv, 2009.
Bharath K. Sriperumbudur, Kenji Fukumizu, and Gert R. G. Lanckriet. Universality, characteristic kernels
and RKHS embedding of measures. Journal of Machine Learning Research , 12:2389â€“2410, 2011.
Ilya O. Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard SchÃ¶lkopf. Wasserstein auto-encoders. In
ICLR, 2018.
19Published in Transactions on Machine Learning Research (01/2024)
Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. TrajectoryNet: A
dynamic optimal transport network for modeling cellular dynamics. In ICML, 2020.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing
network for unsupervised domain adaptation. In CVPR, 2017.
CÃ©dric Villani. Optimal Transport: Old and New . A series of Comprehensive Studies in Mathematics.
Springer, 2009.
A Preliminaries
A.1 Integral Probability Metric (IPM):
Given a setGâŠ‚L (X), the integral probability metric (IPM) (Muller, 1997; Sriperumbudur et al., 2009;
Agrawal & Horel, 2020) associated with G, is defined by:
Î³G(s0,t0)â‰¡max
fâˆˆG/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0âˆ’/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingleâˆ€s0,t0âˆˆR+(X). (12)
Gis called the generating set of the IPM, Î³G.
In order that the IPM metrizes weak convergence, we assume the following (Muller, 1997):
Assumption A.1. GâŠ†C (X)and is compact.
Since the IPM generated by Gand its absolute convex hull is the same (without loss of generality), we
additionally assume the following:
Assumption A.2. Gis absolutely convex.
Remark A.3. We note that the assumptions A.1 and A.2 are needed only to generalize our theoretical results
to an IPM-regularized UOT formulation (Formulation 13). These assumptions are satisfied whenever the
IPM employed for regularization is the MMD (Formulation 6) with a kernel that is continuous and universal
(i.e., c-universal).
A.2 Classical Examples of IPMs
â€¢Maximum Mean Discrepancy (MMD): Letkbe a characteristic kernel (Sriperumbudur et al.,
2011) over the domain X, letâˆ¥fâˆ¥kdenote the norm of fin the canonical reproducing kernel Hilbert
space (RKHS),Hk, corresponding to k. MMDkis the IPM associated with the generating set:
Gkâ‰¡{fâˆˆHk|âˆ¥fâˆ¥kâ‰¤1}.
MMDk(s0,t0)â‰¡max
fâˆˆGk/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0âˆ’/integraldisplay
Xfdt0/vextendsingle/vextendsingle/vextendsingle/vextendsingle.
â€¢Kantorovich metric ( Kc):Kantorovich metric also belongs to the family of integral probability
metrics associated with the generating set Wcâ‰¡/braceleftbigg
f:Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R|max
xâˆˆXÌ¸=yâˆˆX|f(x)âˆ’f(y)|
c(x,y)â‰¤1/bracerightbigg
, wherec
is a metric overX. The Kantorovich-Fenchel duality result shows that the 1-Wasserstein metric is
the same as the Kantorovich metric when restricted to probability measures.
â€¢Dudley: This is the IPM associated with the generating set:
Ddâ‰¡ {f:Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R|âˆ¥fâˆ¥âˆ+âˆ¥fâˆ¥dâ‰¤1},wheredis a ground metric over X Ã—X. The
so-called Flatmetric is related to the Dudley metric. Itâ€™s generating set is: Fdâ‰¡
{f:Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R|âˆ¥fâˆ¥âˆâ‰¤1,âˆ¥fâˆ¥dâ‰¤1}.
â€¢Kolmogorov: LetX=Rn. Then, theKolmogorovmetricistheIPMassociatedwiththegenerating
set: Â¯Kâ‰¡/braceleftbig
1(âˆ’âˆ,x)|xâˆˆRn/bracerightbig
.
20Published in Transactions on Machine Learning Research (01/2024)
â€¢Total Variation (TV): This is the IPM associated with the generating set: T â‰¡
{f:Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R|âˆ¥fâˆ¥âˆâ‰¤1},whereâˆ¥fâˆ¥âˆâ‰¡max
xâˆˆX|f(x)|. Total Variation metric over measures s0,t0âˆˆ
R+(X)is defined as:
TV(s,t)â‰¡/integraltext
Yd|sâˆ’t|(y), where|sâˆ’t|(y)â‰¡/braceleftbigg
s(y)âˆ’t(y)ifs(y)â‰¥t(y)
t(y)âˆ’s(y)otherwise
B Proofs and Additional Theory Results
AsmentionedinthemainpaperandRemark4.9, mostofourproofsholdevenwithageneralIPM-regularized
UOT formulation (13) under mild assumptions. We restate such results and give a general proof
that holds for IPM-regularized UOT (Formulation 13), of which MMD-regularized UOT (For-
mulation 6) is a special case.
The proposed IPM-regularized UOT formulation is presented as follows.
UG,c,Î»1,Î»2(s0,t0)â‰¡ min
Ï€âˆˆR+(XÃ—X )/integraldisplay
cdÏ€+Î»1Î³G(Ï€1,s0) +Î»2Î³G(Ï€2,t0), (13)
whereÎ³Gis defined in equation (12).
We now present the theoretical results and proofs with IPM-regularized UOT (Formulation 13), of which
MMD-regularized UOT (Formulation 6) is a special case. To the best of our knowledge, such an analysis for
IPM-regularized UOT has not been done before.
B.1 Proof of Theorem 4.1
Theorem 4.1. (Duality) WheneverGsatisfies Assumptions A.1 and A.2, c,kâˆˆC(XÃ—X )andXis
compact, we have that:
UG,c,Î»1,Î»2(s0,t0) = max
fâˆˆG(Î»1),gâˆˆG(Î»2)/integraltext
Xfds0+/integraltext
Xgdt0,
s.t.f(x) +g(y)â‰¤c(x,y)âˆ€x,yâˆˆX. (14)
Proof.We begin by re-writing the RHS of (13) using the definition of IPMs given in (12):
UG,c,Î»1,Î»2(s0,t0)â‰¡ min
Ï€âˆˆR+(XÃ—X )/integraldisplay
XÃ—XcdÏ€+Î»1/parenleftbigg
max
fâˆˆG/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xfds0âˆ’/integraldisplay
XfdÏ€1/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
+Î»2/parenleftbigg
max
gâˆˆG/vextendsingle/vextendsingle/vextendsingle/vextendsingle/integraldisplay
Xgdt0âˆ’/integraldisplay
XgdÏ€2/vextendsingle/vextendsingle/vextendsingle/vextendsingle/parenrightbigg
âˆµ(A.2)= min
Ï€âˆˆR+(XÃ—X )/integraldisplay
XÃ—XcdÏ€+Î»1/parenleftbigg
max
fâˆˆG/integraldisplay
Xfds0âˆ’/integraldisplay
XfdÏ€1/parenrightbigg
+Î»2/parenleftbigg
max
gâˆˆG/integraldisplay
Xgdt0âˆ’/integraldisplay
XgdÏ€2/parenrightbigg
= min
Ï€âˆˆR+(XÃ—X )/integraldisplay
XÃ—XcdÏ€+/parenleftbigg
max
fâˆˆG(Î»1)/integraldisplay
Xfds0âˆ’/integraldisplay
XfdÏ€1/parenrightbigg
+/parenleftbigg
max
gâˆˆG(Î»2)/integraldisplay
Xgdt0âˆ’/integraldisplay
XgdÏ€2/parenrightbigg
= max
fâˆˆG(Î»1),gâˆˆG(Î»2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0+ min
Ï€âˆˆR+(XÃ—X )/integraldisplay
XÃ—XcdÏ€âˆ’/integraldisplay
XfdÏ€1âˆ’/integraldisplay
XgdÏ€2
= max
fâˆˆG(Î»1),gâˆˆG(Î»2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0+ min
Ï€âˆˆR+(XÃ—X )/integraldisplay
XÃ—Xcâˆ’Â¯fâˆ’Â¯gdÏ€
= max
fâˆˆG(Î»1),gâˆˆG(Î»2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0+/braceleftbigg0iff(x) +g(y)â‰¤c(x,y)âˆ€x,yâˆˆX,
âˆ’âˆ otherwise.
= max
fâˆˆG(Î»1),gâˆˆG(Î»2)/integraldisplay
Xfds0+/integraldisplay
Xgdt0,
s.t.f(x) +g(y)â‰¤c(x,y)âˆ€x,yâˆˆX.
(15)
Here, Â¯f(x,y)â‰¡f(x),Â¯g(x,y)â‰¡g(y). The min-max interchange in the third equation is due to Sionâ€™s
minimax theorem: (i) since R(X)is a topological dual of C(X)wheneverXis compact, the objective is
21Published in Transactions on Machine Learning Research (01/2024)
bilinear (inner-product in this duality), whenever c,f,gare continuous. This is true from Assumption A.1
andcâˆˆC(XÃ—X ). (ii)oneofthefeasibilitysetsinvolves G, whichisconvexcompactbyAssumptionsA.1,A.2.
The other feasibility set is convex (the closed conic set of non-negative measures).
RemarkB.1. Whenever the kernel, k, employed is continuous, the generating set of the corresponding MMD
satisfies assumptions A.2 and GkâŠ†C(X). Hence, the above proof also works in our case of MMD-regularized
UOT (i.e., to prove Theorem 4.1 in the main paper).
B.2 Proof of Corollary 4.2
We first derive an equivalent re-formulation of 13, which will be used in our proof.
Lemma B1.
UG,c,Î»1,Î»2(s0,t0)â‰¡ min
s,tâˆˆR+(X)|s|W1(s,t) +Î»1Î³G(s,s0) +Î»2Î³G(t,t0), (16)
whereW1(s,t)â‰¡/braceleftbiggÂ¯W1(s
|s|,t
|t|)if|s|=|t|,
âˆ otherwise., with Â¯W1as the 1-Wasserstein metric.
Proof.
min
s,tâˆˆR+(X)|s|W1(s,t) +Î»1Î³G(s,s0) +Î»2Î³G(t,t0)
= min
s,tâˆˆR+(X);|s|=|t||s|min
Â¯Ï€âˆˆR+
1(XÃ—X )/integraldisplay
cdÂ¯Ï€+Î»1Î³G(s,s0) +Î»2Î³G(t,t0)s.t.Â¯Ï€1=s
|s|,Â¯Ï€2=t
|t|
= min
Î·>0Î· min
Â¯Ï€âˆˆR+
1(XÃ—X )/integraldisplay
cdÂ¯Ï€+Î»1Î³G(Î·Â¯Ï€1,s0) +Î»2Î³G(Î·Â¯Ï€2,t0)
= min
Î·>0min
Â¯Ï€âˆˆR+
1(XÃ—X )/integraldisplay
c Î·dÂ¯Ï€+Î»1Î³G(Î·Â¯Ï€1,s0) +Î»2Î³G(Î·Â¯Ï€2,t0)
= min
Ï€âˆˆR+(XÃ—X )/integraldisplay
cdÏ€+Î»1Î³G(Ï€1,s0) +Î»2Î³G(Ï€2,t0)
The first equality holds from the definition of W1:W1(s,t)â‰¡/braceleftbiggÂ¯W1(s
|s|,t
|t|)if|s|=|t|,
âˆ otherwise.. Eliminating
normalized versions sandtusing the equality constraints and introducing Î·to denote their common mass
gives the second equality. The last equality comes after changing the variable of optimization to Ï€âˆˆ
R+(XÃ—X )â‰¡Î·Â¯Ï€. Recall thatR+(X)denotes the set of all non-negative Radon measures defined over X;
while the set of all probability measures is denoted by R+
1(X).
Corollary 4.2 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.2. (Metricity) In addition to assumptions in Theorem (4.1), whenever cis a metric,UG,c,Î»,Î»
belongs to the family of integral probability metrics (IPMs). Also, the generating set of this IPM is the
intersection of the generating set of the Kantorovich metric and the generating set of the IPM used for
regularization. Finally, UG,c,Î»,Î»is a valid norm-induced metric over measures whenever the IPM used for
regularization is norm-induced (e.g. MMD with a characteristic kernel). Thus, Uliftsthe ground metric c
to that over measures.
Proof.The constraints in dual, (7), are equivalent to: g(y)â‰¤min
xâˆˆXc(x,y)âˆ’f(x)âˆ€yâˆˆX. The RHS is
nothing but the c-conjugate ( c-transform) of f. From Proposition 6.1 in (PeyrÃ© & Cuturi, 2019), whenever
cis a metric we have: min
xâˆˆXc(x,y)âˆ’f(x) =/braceleftbigg
âˆ’f(y)iffâˆˆWc,
âˆ’âˆotherwise.Here,Wcis the generating set of the
Kantorovich metric lifting c. Thus the constraints are equivalent to: g(y)â‰¤âˆ’f(y)âˆ€yâˆˆX,fâˆˆWc.
22Published in Transactions on Machine Learning Research (01/2024)
Now, since the dual, (7), seeks to maximize the objective with respect to g, and monotonically increases
with values of g; at optimality, we have that g(y) =âˆ’f(y)âˆ€yâˆˆX. Note that this equality is possible to
achieve as both g,âˆ’fâˆˆG(Î»)âˆ©Wc(these sets are absolutely convex). Eliminating g, one obtains:
UG,c,Î»,Î» (s0,t0) = max
fâˆˆG(Î»)âˆ©Wc/integraltext
Xfds0âˆ’/integraltext
Xfdt0,
Comparing this and the definition of IPMs 12, we have that UG,c,Î»,Î»belongs to the family of IPMs. Since
any IPM is a pseudo-metric (induced by a semi-norm) over measures (Muller, 1997), the only condition left
to be proved is positive definiteness with UG,c,Î»,Î»(s0,t0). Following Lemma B1, we have that for optimal
sâˆ—,tâˆ—in (16),UG,c,Î»,Î»(s0,t0) = 0â‡â‡’ (i)W1(sâˆ—,tâˆ—) = 0,(ii)Î³G(sâˆ—,s0) = 0,(iii)Î³G(tâˆ—,t0) = 0as each term
in the RHS is non-negative. When the IPM used for regularization is a norm-induced metric (e.g. the MMD
metric or the Dudley metric), the conditions (i),(ii),(iii)â‡â‡’sâˆ—=tâˆ—=s0=t0, which proves the positive
definiteness. Hence, we proved that UG,c,Î»,Î»is a norm-induced metric over measures whenever the IPM used
for regularization is a metric.
Remark B.2. Recall that MMD is a valid norm-induced IPM metric whenever the kernel employed is char-
acteristic. Hence, our proof above also shows the metricity of the MMD-regularized UOT (as per corollary 4.2
in the main paper).
Remark B.3. IfGis the unit uniform-norm ball (corresponding to TV), our result specializes to that
in (Piccoli & Rossi, 2016), which proves that UG,c,Î»,Î»coincides with the so-called Flat metric (or the bounded
Lipschitz distance).
Remark B.4. If the regularizer is the Kantorovich metric3, i.e.,G=Wc, andÎ»1=Î»2=Î»â‰¥1, then
UWc,c,Î»,Î»coincides with the Kantorovich metric. In other words, the Kantorovich-regularized OT is the same
as the Kantorovich metric. Hence providing an OT interpretation for the Kantorovich metric that is valid
for potentially un-normalized measures in R+(X).
B.3 Proof of Corollary 4.3
Proof.As discussed in Theorem 4.1 and Corollary 4.2, the MMD-regularized UOT (Formulation 6) is an
IPM with the generating set as an intersection of the generating sets of the MMD and the Kantorovich-
Wasserstein metrics. We now present special cases when MMD-regularized UOT (Formulation 6) recovers
back the Kantorovich-Wasserstein metric and the MMD metric.
Recovering Kantorovich. Recall thatGk(Î») ={Î»g|gâˆˆGk}. From the definition of Gk(Î»),fâˆˆGk(Î») =â‡’
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤Î». Hence, as Î»â†’âˆ,Gk(Î») =Hk. Using this in the duality result of Theorem 4.1, we have
the following.
lim
Î»â†’âˆUk,c,Î»,Î» (s0,t0) = lim
Î»â†’âˆmax
fâˆˆGk(Î»)âˆ©Wc/integraldisplay
fds0âˆ’/integraldisplay
fdt0= max
fâˆˆHkâˆ©Wc/integraldisplay
fds0âˆ’/integraldisplay
fdt0
(1)= max
fâˆˆC(X)âˆ©Wc/integraldisplay
fds0âˆ’/integraldisplay
fdt0
(2)= max
fâˆˆWc/integraldisplay
fds0âˆ’/integraldisplay
fdt0
Equality (1)holds becauseHkis dense in the set of continuous functions, C(X). For equality (2), we use
thatWcconsists of only 1-Lipschitz continuous functions. Thus, âˆ€s0,t0âˆˆR+(X),limÎ»â†’âˆUk,c,Î»,Î» (s0,t0) =
Kc(s0,t0).
Recovering MMD. We next show that when 0<Î»1=Î»2=Î»â‰¤1and the cost metric cis such that
c(x,y)â‰¥/radicalbig
k(x,x) +k(y,y)âˆ’2k(x,y) =âˆ¥Ï•(x)âˆ’Ï•(y)âˆ¥kâˆ€x,y(Dominating cost assumption discussed
in B.4), thenâˆ€s0,t0âˆˆR+(X),Uk,c,Î»,Î» (s0,t0) =MMDk(s0,t0).
3The ground metric in UG,c,Î»,Î»must be the same as that defining the Kantorovich regularizer.
23Published in Transactions on Machine Learning Research (01/2024)
LetfâˆˆGk(Î») =â‡’f=Î»gwheregâˆˆHk,âˆ¥gâˆ¥â‰¤1. This also implies that Î»gâˆˆHkasÎ»âˆˆ(0,1].
|f(x)âˆ’f(y)|=|âŸ¨Î»g,Ï• (x)âˆ’Ï•(y)âŸ©|(RKHS property)
â‰¤|âŸ¨g,Ï•(x)âˆ’Ï•(y)âŸ©|(âˆµ0<Î»â‰¤1)
â‰¤âˆ¥gâˆ¥kâˆ¥Ï•(x)âˆ’Ï•(y)âˆ¥k(Cauchy Schwarz)
â‰¤âˆ¥Ï•(x)âˆ’Ï•(y)âˆ¥k(âˆµâˆ¥gâˆ¥â‰¤1)
â‰¤c(x,y)(Dominating cost assumption, discussed in B.4)
=â‡’fâˆˆWc
Therefore,Gk(Î»)âŠ†WCand hence,Gk(Î»)âˆ©WC=Gk(Î»). This relation, together with the metricity result
shown in Corollary 4.2, implies that Uk,c,Î»,Î» (s0,t0) =Î»MMDk(s0,t0). In B.4, we show that the Euclidean
distance satisfies the dominating cost assumption when the kernel employed is the Gaussian kernel and the
inputs lie on a unit-norm ball.
B.4 Dominating Cost Assumption with Euclidean cost and Gaussian Kernel
We present a sufficient condition for the Dominating cost assumption (used in Corollary 4.3) to be satisfied
while using a Euclidean cost and a Gaussian kernel based MMD. We consider the characteristic RBF kernel,
k(x,y) = exp (âˆ’sâˆ¥xâˆ’yâˆ¥2), and show that for the hyper-parameter, 0< sâ‰¤0.5, the Euclidean cost is
greater than the Kernel cost when the inputs are normalized, i.e., âˆ¥xâˆ¥=âˆ¥yâˆ¥= 1.
âˆ¥xâˆ’yâˆ¥2â‰¥k(x,x) +k(y,y)âˆ’2k(x,y)
â‡â‡’âˆ¥xâˆ¥2+âˆ¥yâˆ¥2âˆ’2âŸ¨x,yâŸ©â‰¥2âˆ’2k(x,y)
â‡â‡’âŸ¨x,yâŸ©â‰¤exp (âˆ’2s(1âˆ’âŸ¨x,yâŸ©))(Assuming normalized inputs)(17)
From Cauchy Schwarz inequality, âˆ’âˆ¥xâˆ¥âˆ¥yâˆ¥â‰¤âŸ¨x,yâŸ©â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥. With the assumption of normalized inputs,
we have thatâˆ’1â‰¤âŸ¨x,yâŸ©â‰¤1. We consider two cases based on this.
Case 1:âŸ¨x,yâŸ©âˆˆ[âˆ’1,0]In this case, condition (17) is satisfied âˆ€sâ‰¥0becausek(x,y)â‰¥0âˆ€x,ywith a
Gaussian kernel.
Case 2:âŸ¨x,yâŸ©âˆˆ(0,1]In this case, our problem in condition (17) is to find sâ‰¥0such that lnâŸ¨x,yâŸ©â‰¤
âˆ’2s(1âˆ’âŸ¨x,yâŸ©). We further consider two sub-cases and derive the required condition as follows:
Case 2A:âŸ¨x,yâŸ©âˆˆ(0,1
e/bracketrightbig
We re-parameterize âŸ¨x,yâŸ©=eâˆ’nfornâ‰¥1. With this, we need to find sâ‰¥0such
thatâˆ’nâ‰¤âˆ’2s(1âˆ’eâˆ’n)â‡â‡’nâ‰¥2s(1âˆ’eâˆ’n). This is satisfied when 0<sâ‰¤0.5becauseeâˆ’nâ‰¥1âˆ’n.
Case 2B:âŸ¨x,yâŸ©âˆˆ(1
e,âˆ)We re-parameterize âŸ¨x,yâŸ©=eâˆ’1
nforn >1. With this, we need to find sâ‰¥0
such that1
n/parenleftï£¬ig
1âˆ’eâˆ’1
n/parenrightï£¬igâ‰¥2s. We consider the function f(n) =n/parenleftï£¬ig
1âˆ’eâˆ’1
n/parenrightï£¬ig
fornâ‰¥1. We now show that fis
an increasing function by showing that the gradientdf
dn= 1âˆ’/parenleftbig
1 +1
n/parenrightbig
eâˆ’1
nis always non-negative.
df
dnâ‰¥0
â‡â‡’e1
nâ‰¥/parenleftbigg
1 +1
n/parenrightbigg
â‡â‡’1
nâˆ’ln/parenleftbigg
1 +1
n/parenrightbigg
â‰¥0
â‡â‡’1
nâˆ’(ln (n+ 1)âˆ’ln (n))â‰¥0
24Published in Transactions on Machine Learning Research (01/2024)
Applying the Mean Value Theorem on g(n) = lnn, we get
ln (n+ 1)âˆ’lnn= (n+ 1âˆ’n)1
z,wherenâ‰¤zâ‰¤n+ 1
=â‡’ln/parenleftbigg
1 +1
n/parenrightbigg
=1
zâ‰¤1
n
=â‡’df
dn=1
nâˆ’ln/parenleftbigg
1 +1
n/parenrightbigg
â‰¥0
The above shows that fis an increasing function of n. We note that limnâ†’âˆf(n) = 1, hence,1
f(n)=
1
n/parenleftï£¬ig
1âˆ’eâˆ’1
n/parenrightï£¬igâ‰¥1which implies that condition (17) is satisfied by taking 0<sâ‰¤0.5.
B.5 Proof of Corollary 4.4
Corollary 4.4 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.4.UG,c,Î»,Î»(s,t)â‰¤min (Î»Î³G(s,t),Kc(s,t)).
Proof.Theorem 4.1 shows that UG,c,Î»,Î»is an IPM whose generating set is the intersection of the generating
sets of Kantorovich and the scaled version of the IPM used for regularization. Thus, from the definition of
max, we have that UG,c,Î»,Î»(s,t)â‰¤Î»Î³G(s,t)andUG,c,Î»,Î»(s,t)â‰¤Kc(s,t). This implies that UG,c,Î»,Î»(s,t)â‰¤
min (Î»Î³G(s,t),Kc(s,t)). As a special case, Uk,c,Î»,Î» (s,t)â‰¤min (Î»MMDk(s,t),Kc(s,t)).
B.6 Proof of Corollary 4.5
Corollary 4.5 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.5. (Weak Metrization) UG,c,Î»,Î»metrizes the weak convergence of normalized measures.
Proof.For convenience of notation, we denote UG,c,Î»,Î»byU. From Corollary 4.4 in the main paper,
0â‰¤U(Î²n,Î²)â‰¤Kc(Î²n,Î²)
From Sandwich theorem, limÎ²nâ‡€Î²U(Î²n,Î²)â†’0aslimÎ²nâ‡€Î²Kc(Î²n,Î²))â†’0by Theorem 6.9 in (Villani,
2009).
B.7 Proof of Corollary 4.6
Corollary 4.6 in the main paper is restated below with the IPM-regularized UOT formulation (13), followed
by its proof.
Corollary 4.6. (Sample Complexity) Let us denoteUG,c,Î»,Î», defined in 13, by Â¯U. Let Ë†sm,Ë†tmdenote the
empirical estimates of s0,t0âˆˆR+(X)respectively with msamples. Then, Â¯U(Ë†sm,Ë†tm)â†’Â¯U(s0,t0)at a rate
(apart from constants) same as that of Î³G(Ë†sm,s0)â†’0.
Proof.We use metricity of Â¯Uproved in Corrolary 4.2. From triangle inequality of the metric Â¯Uand Corol-
lary 4.4 in the main paper, we have that
0â‰¤|Â¯U(Ë†sm,Ë†tm)âˆ’Â¯U(s0,t0)|â‰¤Â¯U(Ë†sm,s0) +Â¯U(t0,Ë†tm)â‰¤Î»/parenleftbig
Î³G(Ë†sm,s0) +Î³G(Ë†tm,t0)/parenrightbig
.
Hence, by Sandwich theorem, Â¯U(Ë†sm,Ë†tm)â†’Â¯U(s0,t0)at a rate at which Î³G(Ë†sm,s0)â†’0andÎ³G(Ë†tm,t0)â†’0. If
the IPM used for regularization is MMD with a normalized kernel, then MMD k(s0,Ë†sm)â‰¤/radicalï£¬ig
1
m+/radicalï£¬ig
2 log(1/Î´)
m
with probability at least 1âˆ’Î´(Smola et al., 2007).
Fromtheunionbound, withprobabilityatleast 1âˆ’Î´,|Â¯U(sm,tm)âˆ’Â¯U(s0,t0)|â‰¤2Î»/parenleftbigg/radicalï£¬ig
1
m+/radicalï£¬ig
2 log(2/Î´)
m/parenrightbigg
.
25Published in Transactions on Machine Learning Research (01/2024)
B.8 Proof of Theorem 4.8
We first restate the standard Moreau-Rockafellar theorem, which we refer to in this discussion.
Theorem B2. LetXbe a real Banach space and f,g:Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’Râˆª{âˆ}be closed convex functions such that
dom(f)âˆ©dom(g)is not empty, then: (f+g)âˆ—(y) = min
x1+x2=yfâˆ—(x1)+gâˆ—(x2)âˆ€yâˆˆXâˆ—. Here,fâˆ—is the Fenchel
conjugate of f, andXâˆ—is the topological dual space of X.
Theorem 4.8 in the main paper is restated below with the IPM-regularized UOT formulation 13, followed
by its proof.
Theorem 4.8. In addition to the assumptions in Theorem 4.1, if cis a valid metric, then
UG,c,Î»1,Î»2(s0,t0) = min
s,tâˆˆR(X)Kc(s,t) +Î»1Î³G(s,s0) +Î»2Î³G(t,t0). (18)
Proof.Firstly, the result in the theorem is not straightforward and is not a consequence of Kantorovich-
Rubinstein duality. This is because the regularization terms in our original formulation (13, 16) enforce
closeness to the marginals of a transport plan and hence necessarily must be of the same mass and must
belong toR+(X). Whereas in the RHS of 18, the regularization terms enforce closeness to marginals that
belong toR(X)and more importantly, they could be of different masses.
We begin the proof by considering indicator functions FcandFGdefined overC(X)Ã—C(X)as:
Fc(f,g) =/braceleftï£¬ig0iff(x) +g(y)â‰¤c(x,y)âˆ€x,yâˆˆX,
âˆ otherwise.,FG,Î»1,Î»2(f,g) =/braceleftï£¬ig0iffâˆˆG(Î»1),gâˆˆG(Î»2),
âˆ otherwise.
Recall that the topological dual of C(X)is the set of regular Radon measures R(X)and the duality product
âŸ¨f,sâŸ©â‰¡/integraltext
fdsâˆ€fâˆˆC(X),sâˆˆR(X). Now, from the definition of Fenchel conjugate in the (direct sum)
spaceC(X)âŠ•C(X), we have: Fâˆ—
c(s,t) = max
fâˆˆC(X),gâˆˆC(X)/integraltext
fds+/integraltext
gdt,s.t.f(x) +g(y)â‰¤c(x,y)âˆ€x,yâˆˆX,
wheres,tâˆˆR(X). Under the assumptions that Xis compact and cis a continuous metric, Proposition 6.1
in (PeyrÃ© & Cuturi, 2019) shows that Fâˆ—
c(s,t) = max
fâˆˆWc/integraltext
fdsâˆ’/integraltext
fdt=Kc(s,t).
On the other hand, FG,Î»1,Î»2(f,g) =/parenleftbigg
max
fâˆˆG(Î»1)/integraltext
fds+ max
gâˆˆG(Î»2)/integraltext
gdt/parenrightbigg
=Î»1Î³G(s,0) +Î»2Î³G(t,0). Now,
we have that the RHS of 18 is min
s,t,s 1,t1âˆˆR(X):(s,t)+(s1,t1)=(s0,t0)Fâˆ—
c(s,t) +Fâˆ—
G,Î»1,Î»2(s1,t1). This is be-
causeÎ³G(s0âˆ’s,0) =Î³G(s0,s). Now, observe that the indicator functions FG,Î»1,Î»2,Fcare closed, con-
vex functions because their domains are closed, convex sets. Indeed, Gis a closed, convex set by As-
sumptions A.1, A.2. Also, it is simple to verify that the set {(f,g)|f(x) +g(y)â‰¤c(x,y)âˆ€x,yâˆˆX}
is closed and convex. Hence by applying the Moreau-Rockafellar formula (Theorem B2), we have that
the RHS of 18 is equal to (Fc+FG,Î»1,Î»2)âˆ—(s0,t0). But from the definition of conjugate, we have that
(Fc+FG,Î»1,Î»2)âˆ—(s0,t0)â‰¡ max
fâˆˆC(X),gâˆˆC(X)/integraltext
Xfds0+/integraltext
Xgdt0âˆ’Fc(f,g)âˆ’FG,Î»1,Î»2(f,g).Finally, from the
definition of the indicator functions Fc,FG,Î»1,Î»2, this is same as the final RHS in 15. Hence Proved.
Remark B.5. Whenever the kernel, k, employed is continuous, the generating set of the corresponding
MMD satisfies assumptions A.1, A.2 and GkâŠ†C(X). Hence, the above proof also works in our case of
MMD-UOT.
B.9 Proof of Theorem 4.10: Consistency of the Proposed Estimator
Proof.From triangle inequality,
|Ë†Um(Ë†sm,Ë†tm)âˆ’Â¯U(s0,t0)|â‰¤| Ë†Um(Ë†sm,Ë†tm)âˆ’Ë†Um(s0,t0)|+|Ë†Um(s0,t0)âˆ’Â¯U(s0,t0)|, (19)
where Ë†Um(s0,t0)is same as Â¯U(s0,t0)except that it employs the restricted feasibility set, F(Ë†sm,Ë†tm), for the
transport plan: set of all joints supported using the samples in Ë†sm,Ë†tmalone i.e.,
F(Ë†sm,Ë†tm)â‰¡/braceleftï£¬ig/summationtextm
i=1/summationtextm
j=1Î±ijÎ´(x1i,x2j)|Î±ijâ‰¥0âˆ€i,j= 1,...,m/bracerightï£¬ig
. Here,Î´zis the Dirac measure at z. We
begin by bounding the first term in RHS of (19).
26Published in Transactions on Machine Learning Research (01/2024)
We denote the (common) objective in Ë†Um(Â·,Â·),Â¯U(Â·,Â·)as a function of the transport plan, Ï€, byh(Ï€,Â·,Â·).
Then,
Ë†Um(Ë†sm,Ë†tm)âˆ’Ë†Um(s0,t0) = min
Ï€âˆˆF(Ë†sm,Ë†tm)h(Ï€,Ë†sm,Ë†tm)âˆ’ min
Ï€âˆˆF(Ë†sm,Ë†tm)h(Ï€,s0,t0)
â‰¤h(Ï€0âˆ—,Ë†sm,Ë†tm)âˆ’h(Ï€0âˆ—,s0,t0)/parenleftï£¬igg
whereÏ€0âˆ—= arg min
Ï€âˆˆF(Ë†sm,Ë†tm)h(Ï€,s0,t0)/parenrightï£¬igg
=Î»1/parenleftbig
MMDk(Ï€0âˆ—
1,Ë†sm)âˆ’MMDk(Ï€0âˆ—
1,s0)/parenrightbig
+Î»2/parenleftbig
MMDk(Ï€0âˆ—
2,Ë†tm)âˆ’MMDk(Ï€0âˆ—
2,t0)/parenrightbig
â‰¤Î»1MMDk(s0,Ë†sm) +Î»2MMDk(t0,Ë†tm) (âˆµMMDksatisfies triangle inequality )
Similarly, one can show that Ë†Um(s0,t0)âˆ’Ë†Um(Ë†sm,Ë†tm)â‰¤Î»1MMDk(s0,Ë†sm) +Î»2MMDk(t0,Ë†tm). Now, (Muan-
det et al., 2017, Theorem 3.4) shows that, with probability at least 1âˆ’Î´, MMDk(s0,Ë†sm)â‰¤1âˆšm+/radicalï£¬ig
2 log(1/Î´)
m,
wherekis a normalized kernel. Hence, the first term in inequality (19) is upper-bounded by (Î»1+
Î»2)/parenleftbigg
1âˆšm+/radicalï£¬ig
2 log 2/Î´
m/parenrightbigg
, with probability at least 1âˆ’Î´.
We next look at the second term in inequality (19): |Ë†Um(s0,t0)âˆ’Â¯U(s0,t0)|. Let Â¯Ï€mbe the optimal transport
plan in definition of Ë†Um(s0,t0). LetÏ€âˆ—be the optimal transport plan in the definition of Â¯U(s0,t0). Consider
another transport plan: Ë†Ï€mâˆˆF(Ë†sm,Ë†tm)such that Ë†Ï€m(xi,yj) =Î·(xi,yj)
m2whereÎ·(xi,yj) =Ï€âˆ—(xi,yj)
s0(xi)t0(yj)for
i,jâˆˆ[1,m].
|Ë†Um(s0,t0)âˆ’Â¯U(s0,t0)|=Ë†Um(s0,t0)âˆ’Â¯U(s0,t0)
=h(Â¯Ï€m,s0,t0)âˆ’h(Ï€âˆ—,s0,t0)
â‰¤h(Ë†Ï€m,s0,t0)âˆ’h(Ï€âˆ—,s0,t0) (âˆµÂ¯Ï€mis optimal, )
â‰¤/integraldisplay
cdË†Ï€mâˆ’/integraldisplay
cdÏ€âˆ—+Î»1âˆ¥Âµk(Ë†Ï€m
1)âˆ’Âµk(Ï€âˆ—
1)âˆ¥k+Î»2âˆ¥Âµk(Ë†Ï€m
2)âˆ’Âµk(Ï€âˆ—
2)âˆ¥k
(âˆµTriangle inequality )
To upper bound these terms, we utilize the fact that the RKHS, Hk, corresponding to a c-universal kernel,
k, is dense inC(X)wrt. the supnorm (Sriperumbudur et al., 2011) and like-wise the direct-product space,
HkâŠ—Hk, is dense inC(XÃ—X )(Gretton, 2015). Given any fâˆˆC(X)Ã—C(X), and arbitrarily small Ïµ>0,
we denote by fÏµ,fâˆ’Ïµthe functions inHkâŠ—Hkthat satisfy the condition:
fâˆ’Ïµ/2â‰¤fâˆ’Ïµâ‰¤fâ‰¤fÏµâ‰¤f+Ïµ/2.
Such anfÏµâˆˆHkâŠ—Hkwill exist because: i) f+Ïµ/4âˆˆC(X)Ã—C(X)and ii)HkâŠ—HkâŠ†C(X)Ã—C(X)is
dense. So there must exist some fÏµâˆˆHkâŠ—Hksuch that|f(x,y) +Ïµ/4âˆ’fÏµ(x,y)|â‰¤Ïµ/4âˆ€x,yâˆˆX â‡â‡’
f(x,y)â‰¤fÏµ(x,y)â‰¤f(x,y) +Ïµ/2âˆ€x,yâˆˆX. Analogously, fâˆ’Ïµexists. In other words, fÏµ,fâˆ’ÏµâˆˆHkâŠ—Hkare
arbitrarily close upper-bound (majorant), lower-bound (minorant) of fâˆˆC(X)Ã—C(X).
We now upper-bound the first of the set of terms (denote s0(x)t0(y)byÎ¾(x,y)and Ë†Î¾m(x,y)is the corre-
sponding empirical measure):
/integraldisplay
cdË†Ï€mâˆ’/integraldisplay
cdÏ€âˆ—â‰¤/integraldisplay
cÏµdË†Ï€mâˆ’/integraldisplay
câˆ’ÏµdÏ€âˆ—
=âŸ¨cÏµ,Âµk(Ë†Ï€m)âŸ©âˆ’âŸ¨câˆ’Ïµ,Âµk(Ï€âˆ—)âŸ©
=âŸ¨cÏµ,Âµk(Ë†Ï€m)âŸ©âˆ’âŸ¨cÏµ,Âµk(Ï€âˆ—)âŸ©+âŸ¨cÏµ,Âµk(Ï€âˆ—)âŸ©âˆ’âŸ¨câˆ’Ïµ,Âµk(Ï€âˆ—)âŸ©
=âŸ¨cÏµ,Âµk(Ë†Ï€m)âˆ’Âµk(Ï€âˆ—)âŸ©+âŸ¨cÏµâˆ’câˆ’Ïµ,Âµk(Ï€âˆ—)âŸ©
â‰¤âŸ¨cÏµ,Âµk(Ë†Ï€m)âˆ’Âµk(Ï€âˆ—)âŸ©+ÏµÏƒÏ€âˆ—
(âˆµâˆ¥cÏµâˆ’câˆ’Ïµâˆ¥âˆâ‰¤Ïµand defineÏƒsas the mass of measure s)
â‰¤âˆ¥cÏµâˆ¥kâˆ¥Âµk(Ë†Ï€m)âˆ’Âµk(Ï€âˆ—)âˆ¥k+ÏµÏƒÏ€âˆ—
27Published in Transactions on Machine Learning Research (01/2024)
One can obtain the tightest upper bound by choosing cÏµâ‰¡arg minvâˆˆHkâŠ—Hkâˆ¥vâˆ¥ks.t.câ‰¤vâ‰¤c+Ïµ/2.
Accordingly, we replace âˆ¥câˆ¥kbyg(Ïµ)in the theorem statement4. Further, we have:
âˆ¥Âµk(Ë†Ï€m)âˆ’Âµk(Ï€âˆ—)âˆ¥2
k=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
Ï•k(x)âŠ—Ï•k(y)dË†Ï€m(x,y)âˆ’/integraldisplay
Ï•k(x)âŠ—Ï•k(y)dÏ€âˆ—(x,y)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
Ï•k(x)âŠ—Ï•k(y)d(Ë†Ï€m(x,y)âˆ’Ï€âˆ—(x,y))/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/angbracketleftbigg/integraldisplay
Ï•k(x)âŠ—Ï•k(y)d(Ë†Ï€m(x,y)âˆ’Ï€âˆ—(x,y)),/integraldisplay
Ï•k(xâ€²)âŠ—Ï•k(yâ€²)d(Ë†Ï€m(xâ€²,yâ€²)âˆ’Ï€âˆ—(xâ€²,yâ€²))/angbracketrightbigg
=/angbracketleftbigg/integraldisplay
Ï•k(x)âŠ—Ï•k(y)Î·(x,y)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
,/integraldisplay
Ï•k(xâ€²)âŠ—Ï•k(yâ€²)Î·(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig/angbracketrightbigg
=/integraldisplay /integraldisplay
âŸ¨Ï•k(x)âŠ—Ï•k(y),Ï•k(xâ€²)âŠ—Ï•k(yâ€²)âŸ©Î·(x,y)Î·(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig
=/integraldisplay /integraldisplay
âŸ¨Ï•k(x),Ï•k(xâ€²)âŸ©âŸ¨Ï•k(y),Ï•k(yâ€²)âŸ©Î·(x,y)Î·(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig
=/integraldisplay /integraldisplay
k(x,xâ€²)k(y,yâ€²)Î·(x,y)Î·(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig
Now, observe that Ëœk:XÃ—XÃ—XÃ—X defined by Ëœk((x,y),(xâ€²,yâ€²))â‰¡k(x,xâ€²)k(y,yâ€²)Î·(x,y)Î·(xâ€²,yâ€²)is a valid
kernel. This is because Ëœk=kakbkc, whereka((x,y),(xâ€²,yâ€²))â‰¡k(x,xâ€²)is a kernel, kb((x,y),(xâ€²,yâ€²))â‰¡
k(y,yâ€²)is a kernel, and kc((x,y),(xâ€²,yâ€²))â‰¡Î·(x,y)Î·(xâ€²,yâ€²)is a kernel (the unit-rank kernel), and product of
kernels is indeed a kernel. Let Ïˆ(x,y)be the feature map corresponding to Ëœk. Then, the final RHS in the
above set of equations is:
=/integraldisplay /integraldisplay
âŸ¨Ïˆ(x,y),Ïˆ(xâ€²,yâ€²)âŸ©d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig
=/angbracketleftbigg/integraldisplay
Ïˆ(x,y)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
,/integraldisplay
Ïˆ(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig/angbracketrightbigg
.
Hence, we have that: âˆ¥Âµk(Ë†Ï€m)âˆ’Âµk(Ï€âˆ—)âˆ¥k=/vextenddouble/vextenddouble/vextenddoubleÂµËœk(Ë†Î¾m)âˆ’ÂµËœk(Î¾)/vextenddouble/vextenddouble/vextenddoubleËœk. Again, using (Muandet et al., 2017,
Theorem 3.4), with probability at least 1âˆ’Î´,/vextenddouble/vextenddouble/vextenddoubleÂµËœk(Ë†Î¾m)âˆ’ÂµËœk(Î¾)/vextenddouble/vextenddouble/vextenddoubleËœkâ‰¤CËœk
m+âˆš
2CËœklog(1/Î´)
m, whereCËœk=
max
x,y,xâ€²,yâ€²âˆˆXËœk((x,y),(xâ€²,yâ€²)). Note that CËœk<âˆasXis compact and s0,t0are assumed to be positive
measures and kis normalized.
Now the MMD-regularizer terms can be bounded using a similar strategy. Recall that, Ë†Ï€m
1(xi) =/summationtextn
j=1Ï€âˆ—(xi,yj)
m2s0(xi)t0(yj), so we have the following.
âˆ¥Âµk(Ë†Ï€m
1)âˆ’Âµk(Ï€âˆ—
1)âˆ¥2
k=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
Ï•k(x)dË†Ï€m
1(x)âˆ’/integraldisplay
Ï•k(x)dÏ€âˆ—
1(x)/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/integraldisplay
Ï•k(x)d(Ë†Ï€m
1(x)âˆ’Ï€âˆ—
1(x))/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
k
=/angbracketleftbigg/integraldisplay
Ï•k(x)d(Ë†Ï€m
1(x)âˆ’Ï€âˆ—
1(x)),/integraldisplay
Ï•k(xâ€²)d(Ë†Ï€m
1(xâ€²)âˆ’Ï€âˆ—
1(xâ€²))/angbracketrightbigg
=/angbracketleftbigg/integraldisplay
Ï•k(x)Î·(x,y)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
,/integraldisplay
Ï•k(xâ€²)Î·(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig/angbracketrightbigg
=/integraldisplay /integraldisplay
âŸ¨Ï•k(x),Ï•k(xâ€²)âŸ©Î·(x,y)Î·(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig
=/integraldisplay /integraldisplay
k(x,xâ€²)Î·(x,y)Î·(xâ€²,yâ€²)d/parenleftï£¬ig
Ë†Î¾m(x,y)âˆ’Î¾(x,y)/parenrightï£¬ig
d/parenleftï£¬ig
Ë†Î¾m(xâ€²,yâ€²)âˆ’Î¾(xâ€²,yâ€²)/parenrightï£¬ig
.
4This leads to a slightly weaker bound, but we prefer it for ease of presentation
28Published in Transactions on Machine Learning Research (01/2024)
Now, observe that Â¯k:XÃ—XÃ—XÃ—X defined by Â¯k((x,y),(xâ€²,yâ€²))â‰¡k(x,xâ€²)Î·(x,y)Î·(xâ€²,yâ€²)is a valid ker-
nel. This is because Â¯k=k1k2, wherek1((x,y),(xâ€²,yâ€²))â‰¡k(x,xâ€²)is a kernel and k2((x,y),(xâ€²,yâ€²))â‰¡
Î·(x,y)Î·(xâ€²,yâ€²)is a kernel (the unit-rank kernel), and product of kernels is indeed a kernel. Hence,
we have that:âˆ¥Âµk(Ë†Ï€m
1)âˆ’Âµk(Ï€âˆ—
1)âˆ¥k=/vextenddouble/vextenddouble/vextenddoubleÂµÂ¯k(Ë†Î¾m)âˆ’ÂµÂ¯k(Î¾)/vextenddouble/vextenddouble/vextenddoubleÂ¯k. Similarly, we have: âˆ¥Âµk(Ë†Ï€m
2)âˆ’Âµk(Ï€âˆ—
2)âˆ¥k=/vextenddouble/vextenddouble/vextenddoubleÂµÂ¯k(Ë†Î¾m)âˆ’ÂµÂ¯k(Î¾)/vextenddouble/vextenddouble/vextenddoubleÂ¯k. Again, using (Muandet et al., 2017, Theorem 3.4), with probability at least 1âˆ’Î´,
/vextenddouble/vextenddouble/vextenddoubleÂµÂ¯k(Ë†Î¾m)âˆ’ÂµÂ¯k(Î¾)/vextenddouble/vextenddouble/vextenddoubleÂ¯kâ‰¤CÂ¯k
m+âˆš
2CÂ¯klog(1/Î´)
m, whereCÂ¯k= max
x,y,xâ€²,yâ€²âˆˆXÂ¯k((x,y),(xâ€²,yâ€²)). Note that CÂ¯k<
âˆasXis compact, s0,t0are assumed to be positive measures, and kis normalized. From the
union bound, we have:/vextendsingle/vextendsingle/vextendsingleË†Um(Ë†sm,Ë†tm)âˆ’Â¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingleâ‰¤(Î»1+Î»2)/parenleftbigg
1âˆšm+/radicalï£¬ig
2 log (5/Î´)
m+CÂ¯k
m+âˆš
2CÂ¯klog(5/Î´)
m/parenrightbigg
+
g(Ïµ)/parenleftbigg
CËœk
m+âˆš
2CËœklog (5/Î´)
m/parenrightbigg
+ÏµÏƒÏ€âˆ—, with probability at least 1âˆ’Î´. In other words, w.h.p. we have:
/vextendsingle/vextendsingle/vextendsingleË†Um(Ë†sm,Ë†tm)âˆ’Â¯U(s0,t0)/vextendsingle/vextendsingle/vextendsingleâ‰¤O/parenleftï£¬ig
Î»1+Î»2âˆšm+g(Ïµ)
m+ÏµÏƒÏ€âˆ—/parenrightï£¬ig
for anyÏµ>0. Hence proved.
B.9.1 Bounding g(Ïµ)
Let the target function to be approximated be hâˆ—âˆˆC(X)âŠ‚L2(X), which is the set of square-integrable
functions (wrt. some measure). Since Xis compact, kbeing c-universal, it is also L2-universal.
Consider the inclusion map Î¹:Hkâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’L2(X), defined by Î¹g=g. Letâ€™s denote the adjoint of Î¹byÎ¹âˆ—. Consider
the regularized least square approximation of hâˆ—defined by htâ‰¡(Î¹âˆ—Î¹+t)âˆ’1Î¹âˆ—hâˆ—âˆˆHk, wheret>0. Now,
using standard results, we have:
âˆ¥Î¹htâˆ’hâˆ—âˆ¥L2=âˆ¥/parenleftbig
Î¹(Î¹âˆ—Î¹+t)âˆ’1Î¹âˆ—âˆ’I/parenrightbig
hâˆ—âˆ¥L2
=âˆ¥/parenleftbig
Î¹ Î¹âˆ—(Î¹ Î¹âˆ—+t)âˆ’1âˆ’I/parenrightbig
hâˆ—âˆ¥L2
=âˆ¥/parenleftbig
Î¹ Î¹âˆ—(Î¹ Î¹âˆ—+t)âˆ’1âˆ’(Î¹ Î¹âˆ—+t)(Î¹ Î¹âˆ—+t)âˆ’1/parenrightbig
hâˆ—âˆ¥L2
=tâˆ¥(Î¹ Î¹âˆ—+t)âˆ’1hâˆ—âˆ¥L2
â‰¤tâˆ¥(Î¹ Î¹âˆ—)âˆ’1hâˆ—âˆ¥L2
The last inequality is true because the operator Î¹ Î¹âˆ—is PD andt >0. Thus, iftâ‰¡Ë†t=Ïµ
âˆ¥(Î¹ Î¹âˆ—)âˆ’1hâˆ—âˆ¥L2, then
âˆ¥Î¹hË†tâˆ’hâˆ—âˆ¥âˆâ‰¤âˆ¥Î¹hË†tâˆ’hâˆ—âˆ¥L2â‰¤Ïµ. Clearly,
g(Ïµ)â‰¤âˆ¥hË†tâˆ¥Hk
=/radicalï£¬ig
âŸ¨hË†t,hË†tâŸ©Hk
=/radicalï£¬ig
âŸ¨(Î¹âˆ—Î¹+Ë†t)âˆ’1Î¹âˆ—hâˆ—,(Î¹âˆ—Î¹+Ë†t)âˆ’1Î¹âˆ—hâˆ—âŸ©Hk
=/radicalï£¬ig
âŸ¨Î¹âˆ—(Î¹ Î¹âˆ—+Ë†t)âˆ’1hâˆ—,Î¹âˆ—(Î¹ Î¹âˆ—+Ë†t)âˆ’1hâˆ—âŸ©Hk
=/radicalï£¬ig
âŸ¨(Î¹ Î¹âˆ—+Ë†t)âˆ’1Î¹ Î¹âˆ—(Î¹ Î¹âˆ—+Ë†t)âˆ’1hâˆ—,hâˆ—âŸ©L2
=/radicalï£¬ig
âŸ¨(Î¹ Î¹âˆ—)1
2(Î¹ Î¹âˆ—+Ë†t)âˆ’1hâˆ—,(Î¹ Î¹âˆ—)1
2(Î¹ Î¹âˆ—+Ë†t)âˆ’1hâˆ—âŸ©L2
=âˆ¥(Î¹ Î¹âˆ—)1
2(Î¹ Î¹âˆ—+Ë†t)âˆ’1hâˆ—âˆ¥L2.
Now, consider the spectral function f(Î») =Î»1
2
Î»+Ë†t. This is maximized when Î»=Ë†t. Hence,f(Î»)â‰¤1
2âˆš
Ë†t. Thus,
g(Ïµ)â‰¤âˆ¥hâˆ—âˆ¥L2âˆš
âˆ¥(Î¹ Î¹âˆ—)âˆ’1hâˆ—âˆ¥L2
2âˆšÏµ. Therefore, as Ïµdecays as1
m2/3, then,g(Ïµ)
mâ‰¤O/parenleftbig1
m2/3/parenrightbig
.
29Published in Transactions on Machine Learning Research (01/2024)
B.10 Solving Problem (9) using Mirror Descent
Problem (9) is an instance of a convex program and can be solved using Mirror Descent (Ben-Tal & Ne-
mirovski, 2021), presented in Algorithm 2.
Algorithm 2 Mirror Descent for solving Problem (9)
Require: InitialÎ±1â‰¥0, max iterations N.
f(Î±) =Tr/parenleftbig
Î±CâŠ¤
12/parenrightbig
+Î»1/vextenddouble/vextenddoubleÎ±1âˆ’Ïƒ1
m1/vextenddouble/vextenddouble
G11+Î»2/vextenddouble/vextenddoubleÎ±âŠ¤1âˆ’Ïƒ2
m1/vextenddouble/vextenddouble
G22.
foriâ†1toNdo
ifâˆ¥âˆ‡f(Î±i)âˆ¥Ì¸=0then
si= 1/âˆ¥âˆ‡f(Î±i)âˆ¥âˆ.
else
returnÎ±i.
end if
Î±i+1=Î±iâŠ™eâˆ’siâˆ‡f(Î±i).
end for
returnÎ±i+1.
B.11 Equivalence between Problems (9) and (10)
We comment on the equivalence between Problems (9) and (10) based on the equivalence of their Ivanov
forms:
Ivanov form for Problem (9) is
min
Î±â‰¥0âˆˆRm1Ã—m2Tr/parenleftbig
Î±CâŠ¤
12/parenrightbig
s.t./vextenddouble/vextenddouble/vextenddouble/vextenddoubleÎ±1âˆ’Ïƒ1
m11/vextenddouble/vextenddouble/vextenddouble/vextenddouble
G11â‰¤r1,/vextenddouble/vextenddouble/vextenddouble/vextenddoubleÎ±âŠ¤1âˆ’Ïƒ2
m21/vextenddouble/vextenddouble/vextenddouble/vextenddouble
G22â‰¤r2,
wherer1,r2>0.
Similarly, the Ivanov form for Problem (10) is
min
Î±â‰¥0âˆˆRm1Ã—m2Tr/parenleftbig
Î±CâŠ¤
12/parenrightbig
s.t./vextenddouble/vextenddouble/vextenddouble/vextenddoubleÎ±1âˆ’Ïƒ1
m11/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
G11â‰¤Â¯r1,/vextenddouble/vextenddouble/vextenddouble/vextenddoubleÎ±âŠ¤1âˆ’Ïƒ2
m21/vextenddouble/vextenddouble/vextenddouble/vextenddouble2
G22â‰¤Â¯r2,
where Â¯r1,Â¯r2>0.
As we can see, the Ivanov forms are the same with Â¯r1=r2
1,Â¯r2=r2
2, the solutions obtained for Problems (9)
and (10) are the same.
B.12 Proof of Lemma 4.11
Proof.Letf(Î±)denote the objective of Problem (10), G11,G22are the Gram matrices over the source and
target samples, respectively and m1,m2as the number of source and target samples respectively.
âˆ‡f(Î±) =C12+ 2/parenleftï£¬igg
Î»1G11/parenleftbigg
Î±1m2âˆ’Ïƒ1
m11m1/parenrightbigg
1âŠ¤
m2+Î»21m1/parenleftbigg
1âŠ¤
m1Î±âˆ’1âŠ¤
m2Ïƒ2
m2/parenrightbigg
G22/parenrightï£¬igg
We now derive the Lipschitz constant of this gradient.
âˆ‡f(Î±)âˆ’âˆ‡f(Î²) = 2/parenleftbig
Î»1G11(Î±âˆ’Î²)1m21âŠ¤
m2+1m11âŠ¤
m1Î»2(Î±âˆ’Î²)G22/parenrightbig
vec/parenleftbig
(âˆ‡f(Î±)âˆ’âˆ‡f(Î²))âŠ¤/parenrightbig
= 2/parenleftï£¬ig
Î»1vec/parenleftbig
(G11(Î±âˆ’Î²)1m21âŠ¤
m2/parenrightbigâŠ¤) +Î»2vec/parenleftbig
(1m11âŠ¤
m1(Î±âˆ’Î²)G22)âŠ¤/parenrightbig/parenrightï£¬ig
= 2/parenleftbig
Î»11m21âŠ¤
m2âŠ—G11+Î»2G22âŠ—1m11âŠ¤
m1/parenrightbig
vec(Î±âˆ’Î²)
whereâŠ—denotes Kronecker product.
30Published in Transactions on Machine Learning Research (01/2024)
âˆ¥vec(âˆ‡f(Î±)âˆ’âˆ‡f(Î²))âˆ¥F=âˆ¥vec/parenleftbig
(âˆ‡f(Î±)âˆ’âˆ‡f(Î²))âŠ¤/parenrightbig
âˆ¥F
â‰¤2âˆ¥Î»11m21âŠ¤
m2âŠ—G11+Î»2G22âŠ—1m11âŠ¤
m1âˆ¥Fâˆ¥vec(Î±âˆ’Î²)âˆ¥F(Cauchy Schwarz).
This implies the Lipschitz smoothness constant
L= 2âˆ¥Î»11m21âŠ¤
m2âŠ—G11+Î»2G22âŠ—1m11âŠ¤
m1âˆ¥F
= 2/radicalï£¬ig
(Î»1m2)2âˆ¥G11âˆ¥2
F+ (Î»2m1)2âˆ¥G22âˆ¥2
F+ 2Î»1Î»2/angbracketleftbig
1m21âŠ¤m2âŠ—G11,G22âŠ—1m11âŠ¤m1/angbracketrightbig
F
= 2/radicalï£¬ig
(Î»1m2)2âˆ¥G11âˆ¥2
F+ (Î»2m1)2âˆ¥G22âˆ¥2
F+ 2Î»1Î»2(1âŠ¤m1G111m1) (1âŠ¤m2G221m2).
For the last equality, we use the following properties for Kronecker products-
Mixed product property: (AâŠ—B)âŠ¤=AâŠ¤âŠ—BâŠ¤,(AâŠ—B)(CâŠ—D) = (AC)âŠ—(BD)and
Spectrum property: Tr ((AC)âŠ—(BD)) =Tr(AC)Tr(BD).
B.13 Solving Problem (10) using Accelerated Projected Gradient Descent
In Algorithm 1, we present the accelerated projected gradient descent (APGD) algorithm that we use to solve
Problem (10), as discussed in Section 4.2. The projection operation involved is Projectâ‰¥0(x) = max( x,0).
B.14 More on the Barycenter problem
B.14.1 Proof of Lemma 4.12
Proof.Recall that we estimate the barycenter with the restriction that the transport plan Ï€icorresponding
toË†U(Ë†si,s)is supported onDiÃ—âˆªn
i=1Di. LetÎ²â‰¥0âˆˆRmdenote the probabilities parameterizing the
barycenter, s. With Ë†UmasdefinedinEquation(9), theMMD-UOTbarycenterformulation, Ë†Bm(Ë†s1,Â·Â·Â·,Ë†sn) =
min
Î²â‰¥0/summationtextn
i=1ÏiË†Um(Ë†si,s(Î²)), becomes
min
Î±1,Â·Â·Â·,Î±n,Î²â‰¥0n/summationdisplay
i=1Ïi/braceleftï£¬igg
Tr/parenleftbig
Î±iCâŠ¤
i/parenrightbig
+Î»1âˆ¥Î±i1âˆ’Ïƒi
mi1âˆ¥Gii+Î»2âˆ¥Î±âŠ¤
i1âˆ’Î²âˆ¥G/bracerightï£¬igg
. (20)
Following our discussion in Sections 4.2 and B.11, we present an equivalent barycenter formulation with
squared-MMD regularization. This not only makes the objective smooth, allowing us to exploit accelerated
solvers, but also simplifies the problem, as we discuss next.
Bâ€²
m(Ë†s1,Â·Â·Â·,Ë†sn)â‰¡ min
Î±1,Â·Â·Â·,Î±n,Î²â‰¥0n/summationdisplay
i=1Ïi/braceleftï£¬igg
Tr/parenleftbig
Î±iCâŠ¤
i/parenrightbig
+Î»1âˆ¥Î±i1âˆ’Ïƒi
mi1âˆ¥2
Gii+Î»2âˆ¥Î±âŠ¤
i1âˆ’Î²âˆ¥2
G/bracerightï£¬igg
.(21)
The above problem is a least squares problem in terms of Î²with a non-negativity constraint. Equating the
gradient wrt Î²as 0, we get G(Î²âˆ’/summationtextn
j=1ÏjÎ±âŠ¤
j1) = 0. As the Gram matrices of universal kernels are full-rank
(Song, 2008, Corollary 32), this implies Î²=/summationtextn
j=1ÏjÎ±âŠ¤
j1, which also satisfies the non-negativity constraint.
Substituting Î²=/summationtextn
j=1ÏjÎ±âŠ¤
j1in 21 gives us the MMD-UOT barycenter formulation:
Bâ€²
m(Ë†s1,Â·Â·Â·,Ë†sn)â‰¡ min
Î±1,Â·Â·Â·,Î±n,Î²â‰¥0n/summationdisplay
i=1Ïi/braceleftï£¬igg
Tr/parenleftbig
Î±iCâŠ¤
i/parenrightbig
+Î»1âˆ¥Î±i1âˆ’Ïƒi
mi1âˆ¥2
Gii+Î»2âˆ¥Î±âŠ¤
i1âˆ’n/summationdisplay
j=1ÏjÎ±âŠ¤
j1âˆ¥2
G/bracerightï£¬igg
.(22)
31Published in Transactions on Machine Learning Research (01/2024)
B.14.2 Solving the Barycenter Formulation
The objective of 22, as a function of Î±i, has the following smoothness constant (derivation analogous to
Lemma 4.11 in the main paper).
Li= 2Ïi/radicalï£¬ig
(Î»1m)2âˆ¥Giiâˆ¥2
F+ (Î·imi)2âˆ¥Gâˆ¥2
F+ 2Î»1Î·i(1âŠ¤miGii1mi)(1âŠ¤mG1m)
whereÎ·i=Î»2(1âˆ’Ïi). We jointly optimize for Î±iâ€™s using accelerated projected gradient descent with step-size
1/Li.
B.14.3 Consistency of the Barycenter estimator
Similar to Theorem 4.10, we show the consistency of the proposed sample-based barycenter estimator. Let
Ë†sibe the empirical measure supported over msamples from si. From the proof of Lemma 4.12 and 22, recall
that,
Bâ€²
m(s1,Â·Â·Â·,sn) = min
Î±1,Â·Â·Â·,Î±nâ‰¥0n/summationdisplay
i=1Ïi/parenleftï£¬ig
Tr/parenleftbig
Î±iCâŠ¤
i/parenrightbig
+Î»1âˆ¥Î±i1âˆ’Ë†siâˆ¥2
Gii+Î»2âˆ¥Î±âŠ¤
i1âˆ’n/summationdisplay
j=1ÏjÎ±âŠ¤
j1âˆ¥2
G/parenrightï£¬ig
.
Now let us denote the true Barycenter with squared-MMD regularization by B(s1,Â·Â·Â·,sn)â‰¡
min
sâˆˆR+(X)/summationtextn
i=1ÏiU(si,s)whereU(si,s)â‰¡ min
Ï€iâˆˆR+(X)/integraltext
cdÏ€i+Î»1MMD2
k(Ï€i
1,si) +Î»2MMD2
k(Ï€i
2,s). Let
Ï€1âˆ—,...,Ï€nâˆ—,sâˆ—be the optimal solutions corresponding to B(s1,Â·Â·Â·,sn). It is easy to see that sâˆ—=/summationtextn
j=1ÏjÏ€jâˆ—
2(for e.g. refer (Cohen et al., 2020, Sec C)). After eliminating s, we have:B(s1,Â·Â·Â·,sn) =
min
Ï€1,...,Ï€nâˆˆR+(X)/summationtextn
i=1Ïi/parenleftï£¬ig/integraltext
cdÏ€i+Î»1MMD2
k(Ï€i
1,si) +Î»2MMD2
k(Ï€i
2,/summationtextn
j=1ÏjÏ€j
2)/parenrightï£¬ig
.
Theorem B3. LetÎ·i(x,z)â‰¡Ï€iâˆ—(x,z)
si(x)sâ€²(z)wheresâ€²is the mixture density sâ€²â‰¡/summationtextn
i=11
nsi. Under
mild assumptions that the functions, Î·i,câˆˆ HkâŠ—Hk, we have that w.h.p., the estimation error,
|Bâ€²
m(Ë†s1,Â·Â·Â·,Ë†sm)âˆ’B(s1,Â·Â·Â·,sn)|â‰¤O ( max
iâˆˆ[1,n]/parenleftbig
âˆ¥Î·iâˆ¥kâˆ¥câˆ¥k/parenrightbig
/m).
Proof.From triangle inequality,
|Bâ€²
m(Ë†s1,Â·Â·Â·,Ë†sn)âˆ’B(s1,Â·Â·Â·,sn)|â‰¤|Bâ€²
m(Ë†s1,Â·Â·Â·,Ë†sn)âˆ’Bâ€²
m(s1,Â·Â·Â·,sn)|+|Bâ€²
m(s1,Â·Â·Â·,sn)âˆ’B(s1,Â·Â·Â·,sn)|,(23)
whereBâ€²
m(s1,Â·Â·Â·,sn)is the same as B(s1,Â·Â·Â·,sn)except that it employs restricted feasibility sets,
Fi(Ë†s1,Â·Â·Â·,Ë†sn)for corresponding Î±ias the set of all joints supported at the samples in Ë†s1,Â·Â·Â·,Ë†snalone.
LetDi={xi1,Â·Â·Â·,xim}and the union of all samples, âˆªDn
i=1={z1,Â·Â·Â·,zmn}.
Fi(Ë†s1,Â·Â·Â·,Ë†sn)â‰¡/braceleftï£¬ig/summationtextm
l=1/summationtextmn
j=1Î±ljÎ´(xil,zj)|Î±ljâ‰¥0âˆ€l= 1,...,m ;j= 1,...,mn/bracerightï£¬ig
. Here,Î´ris the Dirac
measure at r. We begin by bounding the first term.
We denote the (common) objective in Bâ€²
m(Â·),B(Â·)as a function of the transport plans, (Ï€1,Â·Â·Â·,Ï€n), by
h(Ï€1,Â·Â·Â·,Ï€n,Â·).
Bâ€²
m(Ë†s1,Â·Â·Â·,Ë†sn)âˆ’Bâ€²
m(s1,Â·Â·Â·,sn) = min
Ï€iâˆˆFi(Ë†s1,Â·Â·Â·,Ë†sn)h(Ï€1,Â·Â·Â·,Ï€n,Ë†s1,Â·Â·Â·,Ë†sn)âˆ’ min
Ï€iâˆˆFi(Ë†s1,Â·Â·Â·,Ë†sn)h(Ï€1,Â·Â·Â·,Ï€n,s1,Â·Â·Â·,sn)
â‰¤h(Â¯Ï€1âˆ—,Â·Â·Â·,Â¯Ï€nâˆ—,Ë†s1,Â·Â·Â·,Ë†sn)âˆ’h(Â¯Ï€1âˆ—,Â·Â·Â·,Â¯Ï€nâˆ—,s1,Â·Â·Â·,sn)/parenleftï£¬ig
where Â¯Ï€iâˆ—= arg minÏ€iâˆˆFi(Ë†s1,Â·Â·Â·,Ë†sn)h(Ï€1,Â·Â·Â·,Ï€n,s1,Â·Â·Â·,sn)foriâˆˆ[1,n]/parenrightï£¬ig
=/summationtextn
i=1Î»1Ïi/parenleftbig
MMD2
k(Â¯Ï€iâˆ—
1,Ë†si)âˆ’MMD2
k(Â¯Ï€iâˆ—
1,si)/parenrightbig
=/summationtextn
i=1ÏiÎ»1/parenleftbig
MMDk(Â¯Ï€iâˆ—
1,Ë†si)âˆ’MMDk(Â¯Ï€iâˆ—
1,si)/parenrightbig/parenleftbig
MMDk(Â¯Ï€iâˆ—
1,Ë†si) +MMDk(Â¯Ï€iâˆ—,si)/parenrightbig
(1)
â‰¤2Î»1M/summationtextn
i=1Ïi/parenleftbig
MMDk(Â¯Ï€iâˆ—
1,Ë†si)âˆ’MMDk(Â¯Ï€iâˆ—
1,si)/parenrightbig
â‰¤2Î»1M/summationtextn
i=1ÏiMMDk(Ë†si,si)(As MMD satisfies Triangle Inequality) ,
â‰¤2Î»1Mmax
iâˆˆ[1,n]MMDk(Ë†si,si)
32Published in Transactions on Machine Learning Research (01/2024)
where for inequality (1) we use that max
s,tâˆˆR+
1(X)MMDk(s,t) =M <âˆas the generating set of MMD is
compact.
As with probability at least 1âˆ’Î´, MMDk(Ë†si,si)â‰¤1âˆšm+/radicalï£¬ig
2 log(1/Î´)
m(Smola et al., 2007), with union
bound, we get that the first term in inequality (23) is upper-bounded by 2Î»1M/parenleftbigg
1âˆšm+/radicalï£¬ig
2 log 2n/Î´
m/parenrightbigg
, with
probability at least 1âˆ’Î´.
We next look at the second term in inequality (23): |Bâ€²
m(s1,Â·Â·Â·,sn)âˆ’B(s1,Â·Â·Â·,sn)|. Let (Â¯Ï€1,Â·Â·Â·,Â¯Ï€n)be
the solutions ofBâ€²
m(s1,Â·Â·Â·,sn). Let (Ï€1âˆ—,Â·Â·Â·,Ï€nâˆ—)be the solutions of B(s1,Â·Â·Â·,sn). Recall that sâ€²denotes
the mixture density sâ€²â‰¡/summationtextn
i=11
nsi. Let us denote the empirical distribution of sâ€²byË†sâ€²(i.e., uniform samples
fromâˆªn
i=1Di). Consider the transport plans: Ë†Ï€imâˆˆFi(Ë†s1,Â·Â·Â·,Ë†sn)such that Ë†Ï€im(l,j) =Î·i(xl,zj)
m2nwhere
33Published in Transactions on Machine Learning Research (01/2024)
Î·i(xl,zj) =Ï€iâˆ—(xl,zj)
si(xl)sâ€²(zj), forlâˆˆ[1,m];jâˆˆ[1,mn].
|Bâ€²
m(s1,Â·Â·Â·,sn)âˆ’B(s1,Â·Â·Â·,sn)|=Bâ€²
m(s1,Â·Â·Â·,sn)âˆ’B(s1,Â·Â·Â·,sn)
=h(Â¯Ï€1m,Â·Â·Â·,Â¯Ï€nm,s1,Â·Â·Â·,sn)âˆ’h(Ï€1âˆ—,Â·Â·Â·,Ï€nâˆ—,s1,Â·Â·Â·,sn)
â‰¤h(Ë†Ï€1m,Â·Â·Â·,Ë†Ï€nm,s1,Â·Â·Â·,sn)âˆ’h(Ï€1âˆ—,Â·Â·Â·,Ï€nâˆ—,s1,Â·Â·Â·,sn)
=n/summationdisplay
i=1Ïi/braceleftï£¬igg
/angbracketleftbig
Âµk(Ë†Ï€im)âˆ’Âµk(Ï€iâˆ—),ci/angbracketrightbig
+ 2Î»1M/parenleftï£¬ig
âˆ¥Âµk(Ë†Ï€im
1)âˆ’Âµk(si)âˆ¥k
âˆ’âˆ¥Âµk(Ï€iâˆ—
1)âˆ’Âµk(si)âˆ¥k/parenrightï£¬ig
+
2Î»2Mï£«
ï£­/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleÂµk(Ë†Ï€im
2)âˆ’Âµkï£«
ï£­n/summationdisplay
j=1ÏjË†Ï€jm
2ï£¶
ï£¸/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
kâˆ’/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleÂµk(Ï€iâˆ—
2)âˆ’Âµkï£«
ï£­n/summationdisplay
j=1ÏjÏ€jâˆ—
2ï£¶
ï£¸/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
kï£¶
ï£¸/bracerightï£¬igg
(Upper-bounding the sum of two MMD terms by 2M)
â‰¤n/summationdisplay
i=1Ïi/braceleftï£¬igg
/angbracketleftbig
Âµk(Ë†Ï€im)âˆ’Âµk(Ï€iâˆ—),ci/angbracketrightbig
+ 2Î»1M/vextenddouble/vextenddoubleÂµk(Ë†Ï€im
1)âˆ’Âµk(Ï€iâˆ—
1)/vextenddouble/vextenddouble
k+
2Î»2Mï£«
ï£­/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleÂµk(Ë†Ï€im
2)âˆ’Âµkï£«
ï£­n/summationdisplay
j=1ÏjË†Ï€jm
2ï£¶
ï£¸/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
kâˆ’/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddoubleÂµk(Ï€iâˆ—
2)âˆ’Âµkï£«
ï£­n/summationdisplay
j=1ÏjÏ€jâˆ—
2ï£¶
ï£¸/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
kï£¶
ï£¸/bracerightï£¬igg
(Using triangle inequality)
â‰¤n/summationdisplay
i=1Ïi/braceleftï£¬igg
/angbracketleftbig
Âµk(Ë†Ï€im)âˆ’Âµk(Ï€iâˆ—),ci/angbracketrightbig
+ 2Î»1Mâˆ¥Âµk(Ë†Ï€im
1)âˆ’Âµk(Ï€iâˆ—
1)âˆ¥k+
2Î»2Mï£«
ï£­âˆ¥Âµk(Ë†Ï€im
2)âˆ’Âµk(Ï€iâˆ—
2)âˆ¥k+n/summationdisplay
j=1Ïjâˆ¥Âµk(Ë†Ï€jm
2)âˆ’Âµk(Ï€jâˆ—
2)âˆ¥kï£¶
ï£¸/bracerightï£¬igg
(Triangle Inequality and linearity of the kernel mean embedding)
â‰¤n/summationdisplay
i=1Ïi/braceleftï£¬igg
âˆ¥Âµk(Ë†Ï€im)âˆ’Âµk(Ï€iâˆ—)âˆ¥kâˆ¥ciâˆ¥k+ 2Î»1Mâˆ¥Âµk(Ë†Ï€im
1)âˆ’Âµk(Ï€iâˆ—
1)âˆ¥k+
2Î»2Mï£«
ï£­âˆ¥Âµk(Ë†Ï€im
2)âˆ’Âµk(Ï€iâˆ—
2)âˆ¥k+n/summationdisplay
j=1Ïjâˆ¥Âµk(Ë†Ï€jm
2)âˆ’Âµk(Ï€jâˆ—
2)âˆ¥kï£¶
ï£¸/bracerightï£¬igg
(Cauchy Schwarz)
â‰¤max
iâˆˆ[1,n]/braceleftï£¬igg
âˆ¥Âµk(Ë†Ï€im)âˆ’Âµk(Ï€iâˆ—)âˆ¥kâˆ¥ciâˆ¥k+ 2Î»1Mâˆ¥Âµk(Ë†Ï€im
1)âˆ’Âµk(Ï€iâˆ—
1)âˆ¥k+
2Î»2M/parenleftbigg
âˆ¥Âµk(Ë†Ï€im
2)âˆ’Âµk(Ï€iâˆ—
2)âˆ¥k+ max
jâˆˆ[1,n]âˆ¥Âµk(Ë†Ï€jm
2)âˆ’Âµk(Ï€jâˆ—
2)âˆ¥k/parenrightbigg/bracerightï£¬igg
34Published in Transactions on Machine Learning Research (01/2024)
We now repeat the steps similar to B.9 (for bounding the second term in the proof of Theorem 4.10) and
get the following.
âˆ¥Âµk(Ë†Ï€im)âˆ’Âµk(Ï€iâˆ—)âˆ¥k= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1/vextendsingle/vextendsingle/vextendsingle/integraltext
fdË†Ï€imâˆ’/integraltext
fdÏ€iâˆ—/vextendsingle/vextendsingle/vextendsingle
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1/integraltext
fdË†Ï€imâˆ’/integraltext
fdÏ€iâˆ—
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1/summationtextm
l=1/summationtextmn
j=1f(xl,zj)Ï€iâˆ—(xl,zj)
m2nsi(xl)sâ€²(zj)âˆ’/integraltext/integraltext
f(x,z)Ï€iâˆ—(x,z)
si(x)sâ€²(z)si(x)sâ€²(z)dxdz
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²/bracketleftï£¬ig
f(X,Z)Ï€iâˆ—(X,Z)
si(X)sâ€²(Z)/bracketrightï£¬ig
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²/bracketleftbig
f(X,Z)Î·i(X,Z)/bracketrightbig
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²/bracketleftbig/angbracketleftbig
fâŠ—Î·i,Ï•(X)âŠ—Ï•(Z)âŠ—Ï•(X)âŠ—Ï•(Z)/angbracketrightbig/bracketrightbig
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1/angbracketleftbig
fâŠ—Î·i,EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(X)âŠ—Ï•(Z)âŠ—Ï•(X)âŠ—Ï•(Z)]/angbracketrightbig
â‰¤ max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1âˆ¥fâŠ—Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(X)âŠ—Ï•(Z)âŠ—Ï•(X)âŠ—Ï•(Z)]âˆ¥k
(âˆµCauchy Schwarz)
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1âˆ¥fâˆ¥kâˆ¥Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(X)âŠ—Ï•(X)âŠ—Ï•(Z)âŠ—Ï•(Z)]âˆ¥k
(âˆµproperties of norm of tensor product)
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1âˆ¥fâˆ¥kâˆ¥Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si[Ï•(X)âŠ—Ï•(X)]âŠ—EZâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(Z)âŠ—Ï•(Z)]âˆ¥k
â‰¤âˆ¥Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si[Ï•(X)âŠ—Ï•(X)]âˆ¥kâˆ¥EZâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(Z)âŠ—Ï•(Z)]âˆ¥k
=âˆ¥Î·iâˆ¥kâˆ¥Âµk2(Ë†si)âˆ’Âµk2(si)âˆ¥k2âˆ¥Âµk2(Ë†sâ€²)âˆ’Âµk2(sâ€²)âˆ¥k2
(âˆµÏ•(Â·)âŠ—Ï•(Â·)is the feature map corresponding to k2.)
Similarly, we have the following for the marginals.
âˆ¥Âµk(Ë†Ï€im
1)âˆ’Âµk(Ï€iâˆ—
1)âˆ¥k= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1/vextendsingle/vextendsingle/vextendsingle/integraldisplay
fdË†Ï€im
1âˆ’/integraldisplay
fdÏ€iâˆ—
1/vextendsingle/vextendsingle/vextendsingle
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1/integraldisplay
fdË†Ï€im
1âˆ’/integraldisplay
fdÏ€iâˆ—
1
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1m/summationdisplay
l=1mn/summationdisplay
j=1f(xl)Ï€iâˆ—(xl,zj)
m2nsi(xl)sâ€²(zj)âˆ’/integraldisplay /integraldisplay
f(x)Ï€iâˆ—(x,z)
si(x)sâ€²(z)si(x)sâ€²(z)dxdz
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²/bracketleftbigg
f(X)Ï€iâˆ—(X,Z)
si(X)sâ€²(Z)/bracketrightbigg
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²/bracketleftbig
f(X)Î·i(X,Z)/bracketrightbig
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²/bracketleftbig/angbracketleftbig
fâŠ—Î·i,Ï•(X)âŠ—Ï•(X)âŠ—Ï•(Z)/angbracketrightbig/bracketrightbig
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1/angbracketleftbig
fâŠ—Î·i,EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(X)âŠ—Ï•(X)âŠ—Ï•(Z)]/angbracketrightbig
â‰¤ max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1âˆ¥fâŠ—Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(X)âŠ—Ï•(X)âŠ—Ï•(Z)]âˆ¥k
(âˆµCauchy Schwarz)
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1âˆ¥fâˆ¥kâˆ¥Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si,Zâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(X)âŠ—Ï•(X)âŠ—Ï•(Z)]âˆ¥k
(âˆµproperties of norm of tensor product)
= max
fâˆˆHk,âˆ¥fâˆ¥kâ‰¤1âˆ¥fâˆ¥kâˆ¥Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si[Ï•(X)âŠ—Ï•(X)]âŠ—EZâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(Z)]âˆ¥k
â‰¤âˆ¥Î·iâˆ¥kâˆ¥EXâˆ¼Ë†siâˆ’si[Ï•(X)âŠ—Ï•(X)]âˆ¥kâˆ¥EZâˆ¼Ë†sâ€²âˆ’sâ€²[Ï•(Z)]âˆ¥k
=âˆ¥Î·iâˆ¥kâˆ¥Âµk2(Ë†si)âˆ’Âµk2(si)âˆ¥k2âˆ¥Âµk(Ë†sâ€²)âˆ’Âµk(sâ€²)âˆ¥k
(âˆµÏ•(Â·)âŠ—Ï•(Â·)is the feature map corresponding to k2.)
35Published in Transactions on Machine Learning Research (01/2024)
Thus, with probability at least 1âˆ’Î´,|Bâ€²
m(s1,Â·Â·Â·,sn)âˆ’B(s1,Â·Â·Â·,sn)| â‰¤/parenleftbigg
max
iâˆˆ[1,n]/braceleftï£¬ig
âˆ¥Î·iâˆ¥kâˆ¥ciâˆ¥k+ 2Î»1Mâˆ¥Î·iâˆ¥k+ 2Î»2M(âˆ¥Î·iâˆ¥k+ max
jâˆˆ[1,n]âˆ¥Î·jâˆ¥k)/bracerightï£¬ig/parenrightbigg/parenleftbigg
1âˆšm+/radicalï£¬ig
2 log (2n+2)/Î´
m/parenrightbigg2
. Ap-
plying union bound again for the inequality in 23, we get that with probability at least 1âˆ’Î´,
|Bâ€²
m(Ë†s1,Â·Â·Â·,Ë†sn)âˆ’B(s1,Â·Â·Â·,sn)| â‰¤/parenleftbigg
1âˆšm+/radicalï£¬ig
2 log (2n+4)/Î´
m/parenrightbigg/parenleftbigg
2Î»1M+Î¶/parenleftbigg
1âˆšm+/radicalï£¬ig
2 log (2n+4)/Î´
m/parenrightbigg/parenrightbigg
, where
Î¶=/parenleftbigg
max
iâˆˆ[1,n]/braceleftï£¬ig
âˆ¥Î·iâˆ¥kâˆ¥ciâˆ¥k+ 2Î»1Mâˆ¥Î·iâˆ¥k+ 2Î»2M(âˆ¥Î·iâˆ¥k+ max
jâˆˆ[1,n]âˆ¥Î·jâˆ¥k)/bracerightï£¬ig/parenrightbigg
.
B.15 More on Formulation (10)
Analogous to Formulation (10) in the main paper, we consider the following formulation where an IPM
raised to the qthpower with q>1âˆˆZis used for regularization.
UG,c,Î»1,Î»2,q(s0,t0)â‰¡ min
Ï€âˆˆR+(XÃ—X )/integraldisplay
cdÏ€+Î»1Î³q
G(Ï€1,s0) +Î»2Î³q
G(Ï€2,t0) (24)
Formulation (10) in the main paper is a special case of Formulation (24), when IPM is MMD and q= 2.
Following the proof in Lemma B1, one can easily show that
UG,c,Î»1,Î»2,q(s0,t0)â‰¡ min
s,tâˆˆR+(X)|s|W1(s,t) +Î»1Î³q
G(s,s0) +Î»2Î³q
G(t,t0). (25)
To simplify notations, we denote UG,c,Î»,Î», 2byUin the following. It is easy to see that Usatisfies the
following properties by inheritance.
1.Uâ‰¥0as each of the terms in the objective in Formulation (25) is greater than 0.
2.U(s0,t0) = 0â‡â‡’s0=t0, whenever the IPM used for regularization is a norm-induced metric. As
W1,Î³Gare non-negative terms, U(s0,t0) = 0â‡â‡’s=t,Î³G(s,s0) = 0,Î³G(t,t0) = 0. If IPM used
for regularization is a norm-induced metric, the above condition reduces to s0=t0.
3.U(s0,t0) =U(t0,s0)as each term in Formulation (25) is symmetric.
We now derive sample complexity with Formulation (24).
Lemma B4. Let us denote UG,c,Î»1,Î»2,qdefined in Formulation (9) by U, whereq>1âˆˆZ. Let Ë†sm,Ë†tmdenote
the empirical estimates of s0,t0âˆˆR+
1(X)respectively with msamples. Then, U(Ë†sm,Ë†tm)â†’U(s0,t0)at a
rate same as that of Î³G(Ë†sm,s0)â†’0.
Proof.
U(s0,t0)â‰¡ min
Ï€âˆˆR+(XÃ—X )h(Ï€,s0,t0)â‰¡/integraldisplay
cdÏ€+Î»Î³q
G(Ï€1,s0) +Î»Î³q
G(Ï€2,t0)
We have,
U(sm,tm)âˆ’U(s0,t0) = min
Ï€âˆˆR+(XÃ—X )h(Ï€,Ë†sm,Ë†tm)âˆ’ min
Ï€âˆˆR+(XÃ—X )h(Ï€,s0,t0)
â‰¤h(Ï€âˆ—,Ë†sm,Ë†tm)âˆ’h(Ï€âˆ—,s0,t0)/parenleftï£¬igg
whereÏ€âˆ—= arg min
Ï€âˆˆR+(XÃ—X )h(Ï€,s0,t0)/parenrightï£¬igg
=Î»/parenleftbig
Î³q
G(Ï€âˆ—
1,Ë†sm)âˆ’Î³q
G(Ï€âˆ—
1,s0) +Î³q
G(Ï€âˆ—
2,Ë†tm)âˆ’Î³q
G(Ï€âˆ—
2,t0)/parenrightbig
=Î»/parenleftï£¬igg
(Î³G(Ï€âˆ—
1,Ë†sm)âˆ’Î³G(Ï€âˆ—
1,s0))/parenleftï£¬iggqâˆ’1/summationdisplay
i=0Î³i
G(Ï€âˆ—
1,Ë†sm)Î³qâˆ’1âˆ’i
G (Ï€âˆ—
1,s0)/parenrightï£¬igg/parenrightï£¬igg
+
Î»/parenleftï£¬igg
/parenleftbig
Î³G(Ï€âˆ—
2,Ë†tm)âˆ’Î³G(Ï€âˆ—
2,t0)/parenrightbig/parenleftï£¬iggqâˆ’1/summationdisplay
i=0Î³i
G(Ï€âˆ—
2,Ë†tm)Î³qâˆ’1âˆ’i
G (Ï€âˆ—
2,t0)/parenrightï£¬igg/parenrightï£¬igg
36Published in Transactions on Machine Learning Research (01/2024)
â‰¤Î»/parenleftï£¬igg
Î³G(s0,Ë†sm)/parenleftï£¬iggqâˆ’1/summationdisplay
i=0Î³i
G(Ï€âˆ—
1,Ë†sm)Î³qâˆ’1âˆ’i
G (Ï€âˆ—
1,s0)/parenrightï£¬igg/parenrightï£¬igg
+
Î»/parenleftï£¬igg
Î³G(t0,Ë†tm)/parenleftï£¬iggqâˆ’1/summationdisplay
i=0Î³i
G(Ï€âˆ—
2,Ë†tm)Î³qâˆ’1âˆ’i
G (Ï€âˆ—
2,t0)/parenrightï£¬igg/parenrightï£¬igg
(âˆµÎ³Gsatisfies triangle inequality )
â‰¤Î»/parenleftï£¬igg
Î³G(s0,Ë†sm)qâˆ’1/summationdisplay
i=0/parenleftbigg/parenleftbiggqâˆ’1
i/parenrightbigg
Î³i
G(Ï€âˆ—
1,Ë†sm)Î³qâˆ’1âˆ’i
G (Ï€âˆ—
1,s0)/parenrightbigg/parenrightï£¬igg
+
Î»/parenleftï£¬igg
(Î³G(t0,Ë†tm)/parenleftï£¬iggqâˆ’1/summationdisplay
i=0/parenleftbiggqâˆ’1
i/parenrightbigg
Î³i
G(Ï€âˆ—
2,Ë†tm)Î³qâˆ’1âˆ’i
G (Ï€âˆ—
2,t0)/parenrightï£¬igg/parenrightï£¬igg
=Î»/parenleftï£¬ig
Î³G(s0,Ë†sm) (Î³G(Ï€âˆ—
1,Ë†sm) +Î³G(Ï€âˆ—
1,s0))qâˆ’1
+Î³G(t0,Ë†tm)/parenleftbig
Î³G(Ï€âˆ—
2,Ë†tm) +Î³G(Ï€âˆ—
2,t0)/parenrightbigqâˆ’1/parenrightï£¬ig
â‰¤Î»(2M)qâˆ’1/parenleftbig
Î³G(s0,Ë†sm) +Î³G(t0,Ë†tm)/parenrightbig
.
For the last inequality, we use that max
aâˆˆR+
1(X)max
bâˆˆR+
1(X)Î³G(a,b) =M <âˆas the domain is compact.
Similarly, one can show the other way inequality, resulting in the following.
|U(s0,t0)âˆ’U(sm,tm)|â‰¤Î»(2M)qâˆ’1/parenleftbig
Î³G(s0,Ë†sm) +Î³G(t0,Ë†tm)/parenrightbig
. (26)
The rate at which |U(sm,tm)âˆ’U(s0,t0)|goes to zero is hence the same as that with which either of the
IPM terms goes to zero. For example, if the IPM used for regularization is MMD with a normalized kernel,
then MMD k(s0,Ë†sm)â‰¤/radicalï£¬ig
1
m+/radicalï£¬ig
2 log(1/Î´)
mwith probability at least 1âˆ’Î´(Smola et al., 2007).
From the union bound, with probability at least 1âˆ’Î´,|U(sm,tm)âˆ’U(s0,t0)| â‰¤
2Î»(2M)qâˆ’1/parenleftbigg/radicalï£¬ig
1
m+/radicalï£¬ig
2 log(2/Î´)
m/parenrightbigg
. Thus,O/parenleftï£¬ig
1âˆšm/parenrightï£¬ig
is the common bound for the rate at which the
LHS as well as the MMD k(s0,Ë†sm)decays to zero.
B.16 Robustness
We show the robustness property of IPM-regularized UOT 13 with the same assumptions on the noise model
as used in (Fatras et al., 2021, Lemma 1) for KL-regularized UOT.
Lemma B5. (Robustness) Lets0,t0âˆˆR+
1(X). Consider sc=Ïs0+ (1âˆ’Ï)Î´z(Ïâˆˆ[0,1]), a distribution
perturbed by a Dirac outlier located at some zoutside of the support of t0. Letm(z) =/integraltext
c(z,y)dt0(y).
We have that,UG,c,Î»1,Î»2(sc,t0)â‰¤ÏUG,c,Î»1,Î»2(s0,t0) + (1âˆ’Ï)m(z).
37Published in Transactions on Machine Learning Research (01/2024)
Proof.LetÏ€be the solution of UG,c,Î»1,Î»2(s0,t0). Consider ËœÏ€=ÏÏ€+ (1âˆ’Ï)Î´zâŠ—t0. It is easy to see that
ËœÏ€1=ÏÏ€1+ (1âˆ’Ï)Î´zandËœÏ€2=ÏÏ€2+ (1âˆ’Ï)t0.
UG,c,Î»1,Î»2(sc,t0)â‰¤/integraldisplay
c(x,y)dËœÏ€(x,y) +Î»1Î³G(ËœÏ€1,sc) +Î»2Î³G(ËœÏ€2,t0)(Using the definition of min)
â‰¤/integraldisplay
c(x,y)dËœÏ€(x,y) +Î»1(ÏÎ³G(Ï€1,s0) + (1âˆ’Ï)Î³G(Î´z,Î´z)) +Î»2(ÏÎ³G(Ï€2,t0) + (1âˆ’Ï)Î³G(t0,t0))
(âˆµIPMs are jointly convex)
=/integraldisplay
c(x,y)dËœÏ€(x,y) +Ï(Î»1Î³G(Ï€1,s0) +Î»2Î³G(Ï€2,t0))
=Ï/integraldisplay
c(x,y)dÏ€(x,y) +/integraldisplay
(1âˆ’Ï)c(z,y)d(Î´zâŠ—t0)(z,y) +Ï(Î»1Î³G(Ï€1,s0) +Î»2Î³G(Ï€2,t0))
=Ï/integraldisplay
c(x,y)dÏ€(x,y) +/integraldisplay
(1âˆ’Ï)c(z,y)dt0(y) +Ï(Î»1Î³G(Ï€1,s0) +Î»2Î³G(Ï€2,t0))
=ÏUG,c,Î»1,Î»2(s0,t0) + (1âˆ’Ï)m(z).
We note that m(z)is finite ast0âˆˆR+
1(X).
We now present robustness guarantees with a different noise model.
Corollary B6. We say a measure qâˆˆR+(X)is corrupted with Ïâˆˆ[0,1]fraction of noise when q=
(1âˆ’Ï)qc+Ïqn, whereqcis the clean measure and qnis the noisy measure.
Lets0,t0âˆˆR+(X)be corrupted with Ïfraction of noise such that |scâˆ’sn|TVâ‰¤Ïµ1and|tcâˆ’tn|TVâ‰¤Ïµ2. We
have thatUG,c,Î»,Î»(s0,t0)â‰¤UG,c,Î»,Î»(sc,tc) +ÏÎ²(Ïµ1+Ïµ2), whereÎ²= max
fâˆˆG(Î»)âˆ©Wcâˆ¥fâˆ¥âˆ.
Proof.We use our duality result of UG,c,Î»,Î», from Theorem 4.1. We first upper-bound UG,c,Î»,Î» (sn,tn)which
is later used in the proof.
UG,c,Î»,Î» (sn,tn) = max
fâˆˆG(Î»)âˆ©Wc/integraldisplay
fdsnâˆ’/integraldisplay
fdtn
= max
fâˆˆG(Î»)âˆ©Wc/integraldisplay
fd(snâˆ’sc) +/integraldisplay
fdscâˆ’/integraldisplay
fd(tnâˆ’tc)âˆ’/integraldisplay
fdtc
â‰¤ max
fâˆˆG(Î»)âˆ©Wc/integraldisplay
fd(snâˆ’sc) + max
fâˆˆG(Î»)âˆ©Wc/integraldisplay
fd(tnâˆ’tc) + max
fâˆˆG(Î»)âˆ©Wc/parenleftbigg/integraldisplay
fdscâˆ’/integraldisplay
fdtc/parenrightbigg
â‰¤Î²(|scâˆ’sn|TV+|tcâˆ’tn|TV) +UG,c,Î»,Î»(sc,tc)
=Î²(Ïµ1+Ïµ2) +UG,c,Î»,Î»(sc,tc). (27)
We now show the robustness result as follows.
UG,c,Î»,Î»(s0,t0) = max
fâˆˆG(Î»)âˆ©Wc/integraldisplay
fds0âˆ’/integraldisplay
fdt0
= max
fâˆˆG(Î»)âˆ©Wc(1âˆ’Ï)/integraldisplay
fdsc+Ï/integraldisplay
fdsnâˆ’(1âˆ’Ï)/integraldisplay
fdtcâˆ’Ï/integraldisplay
fdtn
= max
fâˆˆG(Î»)âˆ©Wc(1âˆ’Ï)/parenleftbigg/integraldisplay
fdscâˆ’/integraldisplay
fdtc/parenrightbigg
+Ï/parenleftbigg/integraldisplay
fdsnâˆ’/integraldisplay
fdtn/parenrightbigg
â‰¤ max
fâˆˆG(Î»)âˆ©Wc(1âˆ’Ï)/parenleftbigg/integraldisplay
fdscâˆ’/integraldisplay
fdtc/parenrightbigg
+ max
fâˆˆG(Î»)âˆ©WcÏ/parenleftbigg/integraldisplay
fdsnâˆ’/integraldisplay
fdtn/parenrightbigg
= (1âˆ’Ï)UG,c,Î»,Î» (sc,tc) +ÏUG,c,Î»,Î» (sn,tn)
â‰¤(1âˆ’Ï)UG,c,Î»,Î» (sc,tc) +Ï(UG,c,Î»,Î» (sc,tc) +Î²(Ïµ1+Ïµ2)) (Using 27)
=UG,c,Î»,Î» (sc,tc) +ÏÎ²(Ïµ1+Ïµ2).
38Published in Transactions on Machine Learning Research (01/2024)
We note that Î²= max
fâˆˆG(Î»)âˆ©Wcâˆ¥fâˆ¥âˆâ‰¤max
fâˆˆWcâˆ¥fâˆ¥âˆ<âˆ. Also, as Î²â‰¤min/parenleftbigg
max
fâˆˆGk(Î»)âˆ¥fâˆ¥âˆ,max
fâˆˆWcâˆ¥fâˆ¥âˆ/parenrightbigg
â‰¤
min/parenleftbigg
Î»,max
fâˆˆWcâˆ¥fâˆ¥âˆ/parenrightbigg
(for a normalized kernel).
B.17 Connections with Spectral Normalized GAN
We comment on the applicability of MMD-UOT in generative modelling and draw connections with the
Spectral Norm GAN (SN-GAN) (Miyato et al., 2018) formulation.
A popular approach in generative modelling is to define a parametric function gÎ¸:Z âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’X that takes a
noise distribution and generates samples from PÎ¸distribution. We then learn Î¸to makePÎ¸closer to the real
distribution, Pr. On formulating this problem with the dual of MMD-UOT derived in Theorem 4.1, we get
min
Î¸max
fâˆˆWcâˆ©Gk(Î»)/integraldisplay
fdPÎ¸âˆ’/integraldisplay
fdPr (28)
We note that in the above optimization problem, the critic function or the discriminator fshould satisfy
âˆ¥fâˆ¥câ‰¤1andâˆ¥fâˆ¥kâ‰¤Î»whereâˆ¥fâˆ¥cdenotes the Lipschitz norm under the cost function c. Let the critic func-
tion befW, parametrized using a deep convolution neural network (CNN) with weights W={W1,Â·Â·Â·,WL},
whereLis the depth of the network. Let Fbe the space of all such CNN models, then Problem (28) can be
approximated as follows.
min
Î¸max
fWâˆˆF;âˆ¥fWâˆ¥câ‰¤1,âˆ¥fWâˆ¥kâ‰¤Î»/integraldisplay
fWdPÎ¸âˆ’/integraldisplay
fWdPr (29)
The constraintâˆ¥fâˆ¥câ‰¤1is popularly handled using a penalty on the gradient, âˆ¥âˆ‡fWâˆ¥(Gulrajani et al.,
2017). The constraint on the RKHS norm, âˆ¥fâˆ¥k, is more challenging for an arbitrary neural network.
Thus, we follow the approximations proposed in (Bietti et al., 2019). (Bietti et al., 2019) use the result
derived in (Bietti & Mairal, 2017) that constructs a kernel whose RKHS contains a CNN, Â¯f, with the same
architecture and parameters as fbut with activations that are smooth approximations of ReLU. With this
approximation, (Bietti et al., 2019) shows tractable bounds on the RKHS norm. We consider their upper
bound based on spectral normalization of the weights in fW. With this, Problem (29) can be approximated
with the following.
min
Î¸max
fWâˆˆF/integraldisplay
fWdPÎ¸âˆ’/integraldisplay
fWdPr+Ï1âˆ¥âˆ‡fWâˆ¥+Ï2L/summationdisplay
i=11
Î»âˆ¥Wiâˆ¥2
sp, (30)
whereâˆ¥.âˆ¥spdenotes the spectral norm and Ï1,Ï2>0. Formulations like (30) have been successfully applied
as variants of Spectral Normalized GAN (SN-GAN). This shows the utility of MMD-regularized UOT in
generative modelling.
B.18 Comparison with WAE
The OT problem in WAE (RHS in Theorem 1 in (Tolstikhin et al., 2018)) using our notation is:
min
Ï€âˆˆR+
1(XÃ—Z )/integraldisplay
c(x,G(z))dÏ€(x,z),s.t.Ï€1=PX, Ï€2=PZ, (31)
whereX,Zare the input and latent spaces, Gis the decoder, and PX,PZare the probability measures
corresponding to the underlying distribution generating the given training set and the latent prior (e.g.,
Gaussian).
(Tolstikhin et al., 2018) employs a one-sided regularization. More specifically, (Tolstikhin et al., 2018, eqn.
(4)) in our notation is:
min
Ï€âˆˆR+
1(XÃ—Z )/integraldisplay
c(x,G(z))dÏ€(x,z) +Î»2MMDk(Ï€2,PZ),s.t.Ï€1=PX. (32)
39Published in Transactions on Machine Learning Research (01/2024)
However, in our work, the proposed MMD-UOT formulation corresponding to (31) reads as:
min
Ï€âˆˆR+
1(XÃ—Z )/integraldisplay
c(x,G(z))dÏ€(x,z) +Î»1MMDk(Ï€1,PX) +Î»2MMDk(Ï€2,PZ). (33)
It is easy to see that the WAE formulation (32) is a special case of our MMD-UOT formulation (33). Indeed,
asÎ»1â†’âˆ, both formulations are the same.
The theoretical advantages of MMD-UOT over WAE are that MMD-UOT induces a new family of metrics
and can be efficiently estimated from samples at a rate O(1âˆšm)whereas WAE is not expected to induce
a metric as the symmetry is broken. Also, WAE is expected to be cursed with dimensions in terms of
estimation, as a marginal is exactly matched, similar to unregularized OT.
We now present the details of estimating (33) in the context of VAEs. The transport plan Ï€is factorized as
Ï€(x,z)â‰¡Ï€1(x)Ï€(z|x), whereÏ€(z|x)is the encoder. For the sake of fair comparison, we choose this encoder
and the decoder, G, to be exactly the same as that in (Tolstikhin et al., 2018). Since Ï€1(x)is not modelled by
WAE, we fall back to the default parametrization in our paper of distributions supported over the training
points. More specifically, if D={x1,...,xm}is the training set (sampled from PX), then our formulation
reads as:
min
Ï€(z|x),Î±âˆˆâˆ†mm/summationdisplay
i=1Î±i/integraldisplay
c(xi,G(z))dÏ€(z|xi) +Î»1MMD2
k/parenleftbigg
Î±,1
m1/parenrightbigg
+Î»2MMD2
k/parenleftï£¬iggm/summationdisplay
i=1Î±iÏ€(z|xi),PZ/parenrightï£¬igg
,(34)
whereGis the gram-matrix over the training set D. We solve (34) using SGD, where the block over the Î±
variables can employ accelerated gradient steps.
C Experimental Details and Additional Results
We present more experimental details and additional results in this section. We have followed standard
practices to ensure reproducibility. We will open-source the codes to reproduce all our experiments upon
acceptance of the paper.
C.1 Synthetic Experiments
We present more details for the experiments in Section 5.1, along with additional experimental results.
Transport Plan and Barycenter We use squared-Euclidean cost as the ground metric. We take points
[1,2,Â·Â·Â·,50]and consider Gaussian distribution over them with mean, and standard deviation as (15, 5)
and (35, 3), respectively. The hyperparameters for MMD-UOT are Î»as 100 and Ïƒ2in the RBF kernel
(k(x,y) = exp/parenleftï£¬ig
âˆ’âˆ¥xâˆ’yâˆ¥2
2Ïƒ2/parenrightï£¬ig
) as 1. The hyperparameters for ÏµKL-UOT are Î»andÏµas 1.
For the barycenter experiment, we take points [1,2,Â·Â·Â·,100]and consider Gaussian distribution over them
with mean, and standard deviation as (20, 5) and (60, 8), respectively. The hyperparameters for MMD-UOT
areÎ»as 100 and Ïƒ2in the RBF kernel as 10. The hyperparameters for ÏµKL-UOT are Î»as 100 and Ïµas
10âˆ’3.
Visualizing the Level Sets For all OT variants squared-Euclidean is used as a ground metric. For the
level set with MMD, RBF kernel is used with Ïƒ2as 3. For MMD-UOT, Î»is 1 and RBF kernel is used with
Ïƒ2as 1. For plotting the level set contours, 20 lines are used for all methods.
Computation Time The source and target measures are Uniform distributions from which we sample
5,000 points. The dimensionality of the data is 5. The experiment is done with hyper-parameters as
squared-Euclidean distance, squared-MMD regularization with RBF kernel, sigma as 1 and lambda as 0.1.
ÏµKL-UOTâ€™s entropic regularization coefficient is 0.01, and lambda is 1. We choose entropic regularization
coefficient from the set {1eâˆ’3,1eâˆ’2,1eâˆ’1}and lambda from the set {1eâˆ’2,1eâˆ’1,1}. This hyper-
parameter resulted in the fastest convergence. This experiment was done on an NVIDIA-RTX 2080 GPU.
40Published in Transactions on Machine Learning Research (01/2024)
ğœ–ğœ–
Figure 4: Computation time: Convergence plots with m= 5000 for the case of the same source and
target measures where the optimal objective is expected to be 0. Left: MMD-UOT Problem (10) solved
with accelerated projected gradient descent. Right: ÏµKL-UOTâ€™s convergence plot is shown separately. We
observe that ÏµKL-UOTâ€™s objective plateaus in 0.3 seconds. We note that our convergence to the optimal
objective is faster than that of ÏµKL-UOT.
Figure 5: Sample efficiency: Log-log plot of optimal objective vs number of samples. The optimal objective
values of MMD-UOT and ÏµKL-UOT formulation are shown as the number of samples increases. The data
lies in 10 dimensions, and the source and target measures are both Uniform. MMD-UOT can be seen to
have a better rate of convergence.
Sample Complexity In Theorem 4.10 in the main paper, we proved an attractive sample complexity of
O/parenleftï£¬ig
mâˆ’1
2/parenrightï£¬ig
for our sample-based estimators. In this section, we present a synthetic experiment to show that
the convergence of MMD-UOTâ€™s metric towards the true value is faster than that of ÏµKL-UOT. We sample
10-dimensional sources and target samples from Uniform sources and target marginals, respectively. As the
marginals are equal, the metrics over measures should converge to 0 as the number of samples increases.
We repeat the experiment with an increasing number of samples. We use squared-Euclidean cost. For
ÏµKL-UOT,Î»= 1,Ïµ= 1eâˆ’2. For MMD-UOT, Î»= 1and RBF kernel with Ïƒ= 1is used. In Figure 5,
we plot MMD-UOTâ€™s objective and the square root of the ÏµKL-UOT objective on increasing the number of
samples. It can be seen from the plot that the MMD-UOT achieves a better rate of convergence compared
toÏµKL-UOT.
EffectofRegularization InFigures7and6, wevisualizematchingthemarginalsofMMD-UOTâ€™soptimal
transport plan. We show the results with both RBF kernel k(x,y) = exp/parenleftï£¬ig
âˆ’âˆ¥xâˆ’yâˆ¥2
2âˆ—10âˆ’6/parenrightï£¬ig
and the IMQ kernel
k(x,y) =/parenleftbig
10âˆ’6+âˆ¥xâˆ’yâˆ¥2/parenrightbigâˆ’0.5. As we increase Î», the matching becomes better for unnormalized measures,
and the marginals exactly match the given measures when the measures are normalized. We have also shown
the unbalanced case results with KL-UOT. As the POT library (Flamary et al., 2021) doesnâ€™t allow including
a simplex constraint for KL-UOT, we do not show this.
41Published in Transactions on Machine Learning Research (01/2024)
                MMD-UOT with RBF kernel                MMD-UOT with IMQ kernel
                KL-UOT
Figure 6: (With unnormalized measures) Visualizing the marginals of transport plans learnt by MMD-UOT
and KL-UOT, on increasing Î».
                MMD-UOT with IMQ kernel
                MMD-UOT with RBF kernel
Figure 7: (With normalized measures) Visualizing the marginals of MMD-UOT (solved with simplex con-
straints) plan on increasing Î». We do not show KL-UOT here as the Sinkhorn algorithm for solving KL-UOT
in the POT library (Flamary et al., 2021) does not incorporate the Simplex constraints on the transport
plan.
C.2 Two-sample Test
Following (Liu et al., 2020), we repeat the experiment 10 times, and in each trial, we randomly sample a
validation subset and a test subset of size Nfrom the given real and fake MNIST datasets. We run the
two-sample test experiment for type-II error on the test set for a given trial using the hyperparameters
chosen for that trial. The hyperparameters were tuned for N= 100for each trial. The hyperparameters
42Published in Transactions on Machine Learning Research (01/2024)
Table 8: Test power (higher is better) for the task of CIFAR-10.1 vs CIFAR 10. The proposed MMD-UOT
method achieves the best results.
ME SCF C2ST-S C2ST-L MMD ÏµKL-UOT MMD-UOT
0.588 0.171 0.452 0.529 0.316 0.132 0.643
for a given trial were chosen based on the average empirical test power (higher is better) over that trialâ€™s
validation dataset.
We use squared-Euclidean distance for MMD-UOT and ÏµKL-UOT formulations. RBF kernel, k(x,y) =
exp/parenleftï£¬ig
âˆ’âˆ¥xâˆ’yâˆ¥2
2Ïƒ2/parenrightï£¬ig
, is used for MMD and for MMD-UOT formulation. The hyperparameters are chosen from
the following set. For the MMD-UOT and MMD, Ïƒwas chosen from {median, 40, 60, 80, 100} where the
median is the median-heuristic (Gretton et al., 2012). For the MMD-UOT an ÏµKL-UOT,Î»is chosen from
{0.1, 1, 10}. For ÏµKL-UOT,Ïµwas chosen from {1, 10âˆ’1,10âˆ’2,10âˆ’3,10âˆ’4}. Based on validation, Ïƒas the
median is chosen for MMD at all trials. For ÏµKL-UOT, the best hyperparameters (Î»,Ïµ)are (10, 0.001) for
trial number 3, (0.1, 0.1) for trial number 10 and (1, 0.1) for the remaining the 8 trials. For MMD-UOT,
the best hyperparameters (Î»,Ïƒ2)are(0.1,60)for trial number 9 and (1,median2)for the remaining 9 trials.
Additional Results Following (Liu et al., 2020), we consider the task of verifying that the datasets
CIFAR-10 (Krizhevsky, 2009) and CIFAR-10.1 (Recht et al., 2018) are statistically different. We follow the
same experimental setup as given in (Liu et al., 2020). The training is done on 1,000 images from each
dataset, and the test is on 1,031 images. The experiment is repeated 10 times, and the average test power is
compared with the results shown in (Liu et al., 2020) with the popular baselines: ME (Chwialkowski et al.,
2015; Jitkrittum et al., 2016), SCF (Chwialkowski et al., 2015; Jitkrittum et al., 2016), C2ST-S (Lopez-Paz &
Oquab, 2017), C2ST-L (Cheng & Cloninger, 2019). We repeat the experiment following the same setup for
the MMD and ÏµKL-UOT baselines. The chosen hyperparameters (Î»,Ïµ)for the 10 different experimen-
tal runsÏµKL-UOT are (0.1,0.1),(1,0.1),(1,0.1),(1,0.01),(1,0.1),(1,0.1),(1,0.1),(0.1,0.1),(1,0.1),(1,0.1)
and (1,0.1). The chosen (Î»,Ïƒ2)for the 10 different experimental runs of MMD-UOT are
(0.1,median ),(1,60),(10,100),(0.1,80),(0.1,40),(0.1,40),(0.1,40),(1,median ),(0.1,80)and(1,40). Table 8
shows that the proposed MMD-UOT obtains the highest test power.
C.3 Single-Cell RNA sequencing
scRNA-seq helps us understand how the expression profile of the cells changes over stages (Schiebinger
et al., 2019). A population of cells is represented as a measure of the gene expression space, and as they
grow/divide/die, and the measure evolves over time. While scRNA-seq records such a measure at a time
stamp, it does so by destroying the cells (Schiebinger et al., 2019). Thus, it is impossible to monitor how
the cell population evolves continuously over time. In fact, only a few measurements at discrete timesteps
are generally taken due to the cost involved.
We perform experiments on the Embryoid Body (EB) single-cell dataset (Moon et al., 2019). The Embryoid
Bodydatasetcomprisesdataat5timestepswithsamplesizesas2381,4163,3278,3665and3332,respectively.
The MMD barycenter interpolating between measures s0,t0has the closed form solution as1
2(s0+t0). For
evaluating the performance at timestep ti, we select the hyperparameters based on the task of predicting for
{t1,t2,t3}\ti. We use IMQ kernel k(x,y) =/parenleftï£¬ig
1+âˆ¥xâˆ’yâˆ¥2
K2/parenrightï£¬igâˆ’0.5
. TheÎ»hyperparameter for the validation of
MMD-UOT is chosen from {0.1,1,10}andK2is chosen from{1eâˆ’4,1eâˆ’3,1eâˆ’2,1eâˆ’1,median}, where
median denotes the median of { 0.5âˆ¥xâˆ’yâˆ¥2âˆ€x,yâˆˆDs.t.xÌ¸=y} over the training dataset ( D). The chosen
(Î»,K2)for timesteps t1,t2,t3are (1, 0.1), (1, median) and (1, median), respectively. The Î»hyperparameter
for the validation of ÏµKL-UOT is chosen from {0.1,1,10}andÏµis chosen from{1eâˆ’5,1eâˆ’4,1eâˆ’3,1eâˆ’
2,1eâˆ’1}. The chosen (Î»,Ïµ)for timesteps t1,t2,t3are (10, 0.01), (1, 0.1) and (1, 0.1) respectively. In Table 9,
we compare against additional OT-based methods Â¯W1,Â¯W2,ÏµOT.
43Published in Transactions on Machine Learning Research (01/2024)
Table 9: Additional OT-based baselines for two-sample test: Average Test Power (between 0 and 1; higher
is better) on MNIST. MMD-UOT obtains the highest average test power at all timesteps even with the
additional baselines.
N Â¯W1Â¯W2ÏµOT MMD-UOT
100 0.111 0.099 0.108 0.154
200 0.232 0.207 0.191 0.333
300 0.339 0.309 0.244 0.588
400 0.482 0.452 0.318 0.762
500 0.596 0.557 0.356 0.873
1000 0.805 0.773 0.508 0.909
Figure 8: (Best viewed in color) The t-SNE plots of the source and target embeddings learnt for the M-
MNIST to USPS domain adaptation task. Different cluster colors imply different classes. The quality of
the learnt representations can be judged based on the separation between clusters. The clusters obtained by
MMD-UOT seem better separated (for example, the red and the cyan-colored clusters).
C.4 Domain Adaptation in JUMBOT framework
The experiments are performed with the same seed as used by JUMBOT. For the experiment on the Digits
dataset, the chosen hyper-parameters for MMD-UOT are K2in the IMQ kernel k(x,y) =/parenleftï£¬ig
1+âˆ¥xâˆ’yâˆ¥2
K2/parenrightï£¬igâˆ’0.5
as10âˆ’2andÎ»as 100. In Figure 8, we also compare the t-SNE plot of the embeddings learnt with the MMD-
UOT andÏµKL-UOT-based loss. The clusters formed with the proposed MMD-UOT seem better separated
(for example, the red and the cyan-colored clusters). For the experiment on the Office-Home dataset, the
chosen hyperparameters for MMD-UOT are/parenleftï£¬ig
Î»= 100, IMQ kernel with K2= 0.1/parenrightï£¬ig
. For the VisDA-2017
dataset, the chosen hyperparameters for MMD-UOT are/parenleftï£¬ig
Î»= 1, IMQ kernel with K2as 10/parenrightï£¬ig
.
For the validation phase on the Digits and the Office-Home datasets, we choose Î»from the set{1,10,100}
andK2from the set{0.01,0.1,10,100,median}. For the validation phase on VisDA, we choose Î»from the
set{1,10,100}andK2from the set{0.1,10,100}.
44Published in Transactions on Machine Learning Research (01/2024)
   PLOT
      Proposed MMD-UOT-based Prompt LearningOriginal image
Figure 9: The attention maps corresponding to each of the four prompts for the baseline (PLOT) and
the proposed method. The prompts learnt using the proposed MMD-UOT capture diverse attributes for
identifying the cat (Oxford-Pets dataset): lower body, upper body, image background and the area near the
mouth.
Original image   PLOT
      Proposed MMD-UOT-based Prompt Learning
Figure 10: The attention maps corresponding to each of the 4 prompts for the baseline (PLOT) and the
proposed method. The prompts learnt using the proposed MMD-UOT capture diverse attributes for identi-
fying the dog (Oxford-Pets dataset): the forehead and the nose, the right portion of the face, the head along
with the left portion of the face, and the ear.
C.5 Prompt Learning
LetF={fm|M
m=1}denote the set of visual features for a given image and Gr={gn|N
n=1}denote the set of
textual prompt features for class r. Following the setup in the PLOT baseline, an OT distance is computed
between empirical measures over 49 image features and 4 textual prompt features, taking cosine similarity
cost. LetdOT(x,r)denote the OT distance between the visual features of image xand prompt features of
classr. The prediction probability is given by p(y=r|x) =exp ((1âˆ’dOT(x,r)/Ï„))/summationtextT
r=1exp ((1âˆ’dOT(x,r)/Ï„)),whereTdenotes the
total no. of classes and Ï„is the temperature of softmax. The textual prompt embeddings are then optimized
with the cross-entropy loss. Additional results on Oxford-Pets (Parkhi et al., 2012) and UCF101 (Soomro
et al., 2012) datasets are shown in Table 11.
45Published in Transactions on Machine Learning Research (01/2024)
Table 10: Hyperparameters (kernel type, kernel hyperparameter, Î») for the prompt learning experiment.
Dataset 1 2 4 8 16
EuroSAT (imq2, 10âˆ’3,500) (imq1, 104,103) (imq1, 10âˆ’2, 500) (imq1, 104,500) (rbf, 1, 500)
DTD (imq1, 10âˆ’2, 10) (rbf, 100, 100) (imq2, 10âˆ’2, 10) (rbf, 10âˆ’2,10) (rbf, 0.1, 1)
Oxford-Pets (imq2, 0.01, 500) (rbf, 10âˆ’3,10) (imq, 1, 10) (imq1, 0.1, 10) (imq1, 0.01, 1)
UCF101 (rbf, 1, 100) (imq2, 10, 100) (rbf, 0.01, 1000) (rbf, 10âˆ’4, 10) (rbf, 100, 103)
Table 11: Additional Prompt Learning results. Average and standard deviation (over 3 runs) of accuracy
(higher is better) on the k-shot classification task, shown for different values of shots ( k) in the state-of-
the-art PLOT framework. The proposed method replaces OT with MMD-UOT in PLOT, keeping all other
hyperparameters the same. The results of PLOT are taken from their paper (Chen et al., 2023).
Dataset Method 1 2 4 8 16
EuroSATPLOT 54.05 Â±5.95 64.21Â±1.9072.36Â±2.2978.15Â±2.65 82.23Â±0.91
Proposed 58.47Â±1.37 66.0Â±0.93 71.97Â±2.2179.03Â±1.91 83.23Â±0.24
DTDPLOT 46.55 Â±2.6251.24Â±1.9556.03Â±0.43 61.70Â±0.35 65.60Â±0.82
Proposed 47.27Â±1.46 51.0Â±1.7156.40Â±0.73 63.17Â±0.69 65.90Â±0.29
Oxford-PetsPLOT 87.49 Â±0.57 86.64Â±0.63 88.63Â±0.2687.39Â±0.7487.21Â±0.40
Proposed 87.60Â±0.65 87.47Â±1.04 88.77Â±0.4687.23Â±0.3488.27Â±0.29
UCF101PLOT 64.53Â±0.7066.83Â±0.43 69.60Â±0.67 74.45Â±0.50 77.26Â±0.64
Proposed 64.2 Â±0.7367.47Â±0.82 70.87Â±0.48 74.87Â±0.33 77.27Â±0.26
Avg acc.PLOT 63.16 67.23 71.66 75.42 78.08
Proposed 64.38 67.98 72.00 76.08 78.67
Following the PLOT baseline, we use the last-epoch model. The authors empirically found that learning 4
prompts with the PLOT method gave the best results. In our experiments, we keep the number of prompts
and the other neural network hyperparameters fixed. We only choose Î»and the kernel hyperparameters
for prompt learning using MMD-UOT. For this experiment, we also validate the kernel type. Besides RBF,
we consider two kernels belonging to the IMQ family: k(x,y) =/parenleftï£¬ig
1+âˆ¥xâˆ’yâˆ¥2
K2/parenrightï£¬igâˆ’0.5
(referred to as imq1) and
k(x,y) = (K2+âˆ¥xâˆ’yâˆ¥2)âˆ’0.5(referred to as imq2). We choose Î»from {10, 100, 500, 1000} and kernel
hyperparameter ( K2orÏƒ2) from { 1eâˆ’3,1eâˆ’2,1eâˆ’1,1,10,1e+ 2,1e+ 3}. The chosen hyperparameters
are included in Table 10.
46