Leveraging Drift to Improve Sample Complexity of
Variance Exploding Diffusion Models
Ruofeng Yang
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
wanshuiyin@sjtu.edu.cnZhijie Wang
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
violetevergarden@sjtu.edu.cn
Bo Jiang
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
bjiang@sjtu.edu.cnShuai Liâˆ—
John Hopcroft Center for Computer Science
Shanghai Jiao Tong University
shuaili8@sjtu.edu.cn
Abstract
Variance exploding (VE) based diffusion models, an important class of diffusion
models, have shown state-of-the-art (SOTA) performance. However, only a few
theoretical works analyze VE-based models, and those works suffer from a worse
forward convergence rate 1/poly(T)than the exp (âˆ’T)of variance preserving
(VP) based models, where Tis the forward diffusion time and the rate measures
the distance between forward marginal distribution qTand pure Gaussian noise.
The slow rate is due to the Brownian Motion without a drift term. In this work, we
design a new drifted VESDE forward process, which allows a faster exp (âˆ’T)for-
ward convergence rate. With this process, we achieve the first efficient polynomial
sample complexity for a series of VE-based models with reverse SDE under the
manifold hypothesis. Furthermore, unlike previous works, we allow the diffusion
coefficient to be unbounded instead of a constant, which is closer to the SOTA mod-
els. Besides the reverse SDE, the other common reverse process is the probability
flow ODE (PFODE) process, which is deterministic and enjoys faster sample speed.
To deepen the understanding of VE-based models, we consider a more general
setting considering reverse SDE and PFODE simultaneously, propose a unified
tangent-based analysis framework, and prove the first quantitative convergence
guarantee for SOTA VE-based models with reverse PFODE. We also show that the
drifted VESDE can balance different error terms and improve generated samples
without training through synthetic and real-world experiments.
1 Introduction
Recently, diffusion modeling has shown impressive performance in different areas [Ho et al., 2022,
Rombach et al., 2022, Esser et al., 2024, Li et al., 2024]. Diffusion models consist of two processes:
the forward and reverse process. The forward process gradually converts data q0to Gaussian noise,
which can be described by an intermediate marginal distribution sequence {qt}tâˆˆ[0,T]. The reverse
process sequentially predicts noise and removes it from data to generate samples.
There are two common forward processes: (1) Variance preserving (VP) SDE and (2) variance
exploding (VE) SDE. The VPSDE corresponds to an Ornstein-Uhlenbeck process, and the stationary
distribution is N(0,I). The VESDE has an exploding variance in the forward process. In earlier
âˆ—Corresponding author
38th Conference on Neural Information Processing Systems (NeurIPS 2024).times, VP-based models [Ho et al., 2020, Lu et al., 2022] provide an important boost for developing
diffusion models. Recently, VE-based models have shown the ability to generate data distribution
supported on low-dimensional manifolds [Song and Ermon, 2019, 2020]. Since image and text
datasets typically exhibit a low-dimensional manifold nature [Pope et al., 2021, Tang and Yang,
2024], VE-based models have achieved great performance in image generation, one-step generation,
and reinforcement learning [Teng et al., 2023, Song et al., 2023, Ding and Jin, 2023]. Furthermore,
Karras et al. [2022] unify VP and VESDE and prove that the ODE solution trajectory of a specific
VESDE is linear and directly towards the data manifold, which makes the denoise process easy.
After determining a forward SDE, diffusion models reverse it and generate samples by running
the corresponding reverse process. Since the reverse drift term contains the gradient of forward
logarithmic density âˆ‡logqt(a.k.a. score function), we estimate it by using the score matching
technique [Vincent, 2011]. After that, diffusion models discretize the continuous reverse process and
run this discrete process starting from pure Gaussian. There are two widely used reverse processes:
reverse SDE [Ho et al., 2020] and probability flow ODE (PFODE) [Song et al., 2020a]. The reverse
SDE usually generates higher quality samples [Kim et al., 2022]. The reverse PFODE always has
a faster generation speed and is useful in other aspects such as calculating likelihoods [Song et al.,
2020b] or obtaining one-step generation models [Song et al., 2023]. Hence, these processes are both
critical, and providing the guarantee for VE-based models with these processes is necessary.
Recently, many works analyze the convergence guarantee of the VP-based diffusion models under the
reverse SDE setting and prove that the VP-based models can sample from the target data distribution
with polynomial complexity [Chen et al., 2023c,a,b, Lee et al., 2023, Benton et al., 2023]. As the
first step of this work, we also analyze VE-based models under the reverse SDE setting. Different
from the VP-based models, only a few works consider VE-based models and all of them suffer from
slow1/Poly(T)forward convergence rate [Lee et al., 2022, Gao et al., 2023, Gao and Zhu, 2024],
which is worse than exp(âˆ’T)one for VPSDE. A slow forward convergence rate makes a large
distance between qTand pure Gaussian noise, which leads to a large reverse beginning error. From
the theoretical perspective, this error introduces hardness to balance three error sources, as shown in
Section 5. From the empirical perspective, Lin et al. [2024] show that this error introduces a data
information leakage problem, which leads to bad performance. To deal with this problem, De Bortoli
et al. [2021] introduce a small drift term to obtain a exp (âˆ’âˆš
T)reverse beginning error. However,
they introduce an additional exp (T)in the discretization error term. Furthermore, their results do not
allow unbounded Î²t, which is the key point of the optimal solution trajectory and used by the SOTA
models [Karras et al., 2022, Song et al., 2023]. Therefore, the following question remains open:
Is it possible to design a VESDE with a faster forward convergence rate than 1/poly(T)and achieve
the polynomial sample complexity when the diffusion coefficient is unbounded?
In this work, for the first time, we propose a new drifted VESDE forward process, which enjoys a
faster forward convergence rate and allows unbounded coefficients. We first show that the drifted
VESDE has similar trends but performs better than the original SOTA VESDE on synthetic data
(Section 7). After that, we analyze the sample complexity of drifted VESDE under the realistic
manifold hypothesis. The manifold hypothesis means the data q0is supported on a lower dimensional
compact set M, and much empirical evidence shows that image and text dataset satisfy this hypothesis
[Fefferman et al., 2016, Pope et al., 2021, Tang and Yang, 2024]. Furthermore, as shown in Section 2,
the manifold hypothesis is more realistic than previous data assumptions since it allows the blow-up
phenomenon of the score function at the end of the reverse process, which matches the empirical
observation [Kim et al., 2021]. Under the manifold hypothesis, we prove that the drifted VESDE with
a suitable larger Î²tbalances the reverse beginning, discretization, and approximated score errors and
achieves the first efficient polynomial sample complexity for VE-based models with reverse SDE.
To better understand VE-based models, we analyze reverse SDE and PFODE simultaneously after
obtaining polynomial complexity for reverse SDE. Despite the great performance, a few theoretical
works consider reverse PFODE [Chen et al., 2023d,b, Gao and Zhu, 2024], and these works either
focus on VPSDE or have strong assumptions. Hence, we propose the tangent-based framework for
VE-based models and achieve the first quantitative convergence for the SOTA VE-based models with
reverse PFODE. In conclusion, we accomplish the following results under the manifold hypothesis:
1.We propose a new drifted VESDE forward process, which allows exp (âˆ’T)forward conver-
gence guarantee with suitable Î²t. With this process, we achieve the first polynomial sample
complexity for a series of VE-based models under the reverse SDE setting.
22.When considering the general setting, we propose the tangent-based unified framework and
analyze reverse SDE and PFODE simultaneously. Under this framework, we prove the first
quantitative guarantee for SOTA VE-based models with reverse PFODE.
3.We show that the drifted VESDE balances different error terms and improves generated
samples without training via synthetic and real-world experiments.
2 Related Work
Before providing current results, we first discuss different assumptions about the data distribution
from strong to weak. The strongest assumption is the log-concave distribution. While the log-Sobelev
inequality (LSI) assumption is slightly weaker, it does not allow the presence of substantial non-
convexity, which is far away from the multi-modal distribution. Recently, some works assume the
score function is L-Lipschitz to allow the multi-modal distribution. However, this assumption can
not explain the blow-up phenomenon of score [Kim et al., 2021]. The last assumption is the manifold
hypothesis, which is supported by much empirical evidence and allows the blow-up score.
Analyses for VP-based models. For the reverse SDE, Lee et al. [2022] achieve the first polynomial
complexity with strong LSI assumption. Chen et al. [2023c] remove the LSI assumption, assume the
Lipschitz score and achieve polynomial complexity. Bortoli [2022] is the first work to focus on the
sample complexity of diffusion models under the manifold hypothesis, and it is the most relevant
work to our unified framework. However, as discussed in Section 6.1, the original tangent-based
lemma can not deal with reverse PFODE even in VPSDE. We carefully control the tangent process
to avoid additional exp (T)by using the exploding variance property of VESDE. Recently, Chen
et al. [2023a] and Benton et al. [2023] also remove the Lipschitz score assumption, and Benton et al.
[2023] achieve optimal dependence on d. More recently, Conforti et al. [2023] use bounded Fisher
information assumption and replace dwith a Fisher information term.
For the PFODE, Chen et al. [2023d] propose the first quantitative result with exponential dependence
on the Lipschitz constant. Chen et al. [2023b] achieve polynomial complexity by introducing a
corrector component to inject suitable noise. More recently, Li et al. [2023] remove the additional
corrector. However, their results rely heavily on the very specific Î²t, which goes to 0asTâ†’+âˆ.
Since VE-based models have an unbounded Î²t, this method is not suitable for our models.
Analyses for VE-based models. When considering VESDE, most works focus on constant Î²t
and reverse SDE. De Bortoli et al. [2021] provide the first convergence guarantee with exponential
dependence on T. Lee et al. [2022] analyze a constant diffusion coefficient VESDE and achieve
polynomial sample complexity under the LSI assumption. When considering the reverse PFODE,
Chen et al. [2023d] only consider the discretization error and provide a quantitative convergence
guarantee. However, their results introduce additional exp (T)compared to ours (Section 6.1).
Recently, Gao et al. [2023] and Gao and Zhu [2024] provide the polynomial results for a series of
VESDE with reverse SDE and reverse PFODE under the log-concave assumption, respectively.
3 The Drifted Variance Exploding (VE) SDE
Diffusion models usually consist of a forward process and a reverse process. The forward process
gradually injects noise to convert the data distribution to pure noise. To generate samples, diffusion
models reverse the forward process and run the corresponding reverse process.
This section first recalls two previous forward processes: VPSDE and VESDE. Recently, the VE-
based models achieve great performance in application [Karras et al., 2022, Song et al., 2023].
However, unlike the widely analyzed VP-based models [Benton et al., 2023, Chen et al., 2023b], the
VE-based models suffer from challenges in obtaining an efficient sample complexity due to the slow
forward convergence rate. To address this limitation, we introduce a new drifted VESDE forward
process, which has a faster forward convergence rate, balances different error terms and achieves the
first efficient polynomial sample complexity (see Section 5). Finally, we introduce how to reverse
this new forward process and obtain an implementable algorithm.
3.1 The VP and VESDE of Diffusion Models
We first introduce the general form of the forward process and then recall two common forward
processes, VPSDE and VESDE, adopted in previous works [Ho et al., 2020, Karras et al., 2022]. Let
3q0be the data distribution. Given X0âˆˆRd, the forward process is defined by
dXt=f(Xt, t) dt+g(t) dBt,X0âˆ¼q0,
where (Bt)tâ‰¥0is the standard Brownian motion in Rd,f(Xt, t)is a drift coefficient and g(t)is a
diffusion coefficient. Let qtbe the density function of Xtat time t. With a suitable choice of drift and
diffusion terms (e.g. Section 3.1 and 3.2), the forward process gradually converts the data distribution
into Gaussian noise. More specifically, the conditional distribution Xt|X0is exactly N(mtX0, Ïƒ2
tI)
given X0, where mtis determined by the drift term and Ïƒ2
tis determined by the diffusion term.
The VPSDE forward process. Let{Î²t}tâ‰¥0be a non-decreasing sequence with bounded range
[1/Â¯Î²,Â¯Î²]. The VPSDE has the following formula:
dXt=âˆ’Î²tXtdt+p
2Î²tdBt,where X0âˆ¼q0. (1)
In this case, mt= exp( âˆ’Rt
0Î²sds)andÏƒ2
t= 1âˆ’m2
t. Note that mTâ‰¤exp (âˆ’T/Â¯Î²),which indicates
a fast forward convergence rate TV(qT|N(0,I))â‰¤exp (âˆ’T/Â¯Î²)[Chen et al., 2023c].
The VESDE forward process. The VESDE forward process is defined without a drift term:
dXt=q
dÏƒ2
t/dtdBt,where X0âˆ¼q0. (2)
Two common choices for Ïƒ2
taretandt2, with the latter achieving SOTA performance [Karras et al.,
2022, Teng et al., 2023]. However, VESDE only has a slow polynomial-decay forward convergence
rate (Theorem 4.2), which introduces hardness to obtain an efficient sample complexity (see Section 5).
This motivates us to design an improved VESDE process with a fast forward convergence rate.
3.2 The Drifted VESDE Forward Process
Note that the forward convergence rate of the general process is upper bounded by mT/Ïƒ2
T(Theo-
rem 4.2). In practical applications [Ho et al., 2020, Karras et al., 2022, Song et al., 2023], the variance
of the forward process Ïƒ2
tat time T, which is determined by the diffusion term, does not exceed T2.
This indicates the contribution of Ïƒ2
Tto the forward convergence rate is only 1/Poly(T). Hence, the
exponential-decay forward convergence rate of VPSDE comes from the drift term, which introduces
an exponential-decay mtâ‰¤exp (âˆ’T/Â¯Î²). Due to the absence of the drift term in VESDE, the data
information, such as expectation E[q0]and covariance Cov[q0], does not decay and mtâ‰¡1, which is
a key to an only polynomial-decay forward convergence rate mT/Ïƒ2
Tâ‰¤1/Poly(T). With the drift
term, the VPSDE gradually removes the data information from qtduring the process, which makes
qtquickly converge to pure Gaussian noise. Inspired by this elimination effect of the drift term, we
propose a drifted VESDE forward process:
dXt=âˆ’1
Ï„Î²tXtdt+p
2Î²tdBt,X0âˆ¼q0, (3)
where Ï„âˆˆ[T, T2]is the coefficient used to balance the drift and diffusion term2, and{Î²t}tâ‰¥0is a
positive non-decreasing sequence. In this case,
mt= exp
âˆ’Zt
0Î²s/Ï„ds
andÏƒ2
t=Ï„ 
1âˆ’m2
t
. (4)
We show that the drifted VESDE is not only an effective representation of the existing VESDE but
also extends beyond it (see Section 7 and Appendix A.1). We also prove that this process with suitable
Î²thas a exp (âˆ’T)forward convergence rate and enjoys an efficient polynomial sample complexity.
3.3 The Reverse Process of the Drifted VESDE
To generate samples from Gaussian noise, a diffusion model reverses the forward process. Let qÏ„
tbe
the density function of the drifted VESDE forward process at time tand(Yt)tâˆˆ[0,T]= (XTâˆ’t)tâˆˆ[0,T].
As shown in Cattiaux et al. [2021], the reverse process of drifted VESDE has the following form3:
dYt=Î²Tâˆ’t
Yt/Ï„+ (1 + Î·2)âˆ‡logqÏ„
Tâˆ’t(Yt)	
dt+Î·p
2Î²Tâˆ’tdBt. (5)
2We note that the choice Ï„âˆˆ[T, T2]is used to guarantee the exploding variance of the forward process. In
fact, our drifted VESDE is a general formula that covers the VP and VESDE by choosing Ï„âˆˆ[1, T2]. More
details are shown in Section 5.
3Now(Bt)tâ‰¥0is the reversed Brownian motion, and we abuse this notation here for ease of notation.
4The parameter Î·âˆˆ[0,1]is used to determine the type of reverse processes. There are two common
reverse processes in the application: reverse probability flow ODE (PFODE) ( Î·= 0) [Song et al.,
2020b, 2023] and reverse SDE ( Î·= 1) [Ho et al., 2020].
To generate distribution q0through running the above reverse process, diffusion models need the
true score function âˆ‡logqÏ„
Tâˆ’t(Yt)and the accurate reverse beginning distribution qÏ„
T. However,
âˆ‡logqÏ„
Tâˆ’t(Yt)andqÏ„
Tcontain the data information and usually can not be exactly calculated. For the
score function, diffusion models approximate it using a score network s(Tâˆ’t,Â·)by minimizing the
score matching objective function [Vincent, 2011]. For the initial distribution of the reverse process,
since qÏ„
Tshould be close to a pure Gaussian, we choose qÏ„
âˆ=N(0, Ïƒ2
TI)as an approximation. Then,
the continuous reverse process (bYt)tâˆˆ[0,T], incorporating s(Tâˆ’t,Â·)andqÏ„
âˆ, is defined as:
dbYt=Î²Tâˆ’tbYt/Ï„+ (1 + Î·2)s(Tâˆ’t,bYt)	
dt+Î·p
2Î²Tâˆ’tdBt,wherebY0âˆ¼qÏ„
âˆ.
Since diffusion models can not run a continuous process due to the nonlinear score function, these
models usually discretize the above continuous process and freeze the approximated score at the
beginning of each interval. Let {Î³k}kâˆˆ[K]be the stepsize and tk+1=Pk
j=0Î³j. As shown in Kim
et al. [2021], âˆ‡logqÏ„
Tâˆ’t(Yt)goes to +âˆat the end of the reverse process. To mitigate this issue,
they use the early stopping technique tK=Tâˆ’Î´, and we also employ this technique in this work.
With the stepsize, we choose the exponential integrator discretization scheme [Zhang and Chen,
2022] to discretize the above process, which runs the following process:
deYt=Î²Tâˆ’teYt/Ï„+ (1 + Î·2)s(Tâˆ’tk,eYtk)	
dt+Î·p
2Î²Tâˆ’tdBt,where tâˆˆ[tk, tk+1].(6)
As shown in Karras et al. [2022], the choice of Î²tsignificantly affects the performance of models,
and we need to determine Î²tbefore running the reverse process. The state-of-the-art diffusion models
adopt Î²t=t, which increases rapidly and has an unbounded range. However, current theoretical
works assume Î²tto be a constant [Chen et al., 2023c] or confined to a bounded interval [1/Â¯Î²,Â¯Î²]
[Bortoli, 2022] to match the setting of VPSDE. To align more closely with practical applications
of VE-based models, we allow an unbounded Î²tin this work. Furthermore, we make a detailed
assumption on Î²twhen considering different reverse processes.
Assumption 3.1. Let{Î²t}tâ‰¥0be a positive, non-decreasing sequence. For any Ï„âˆˆ[T, T2], there
exists constants Â¯Î²andC, such that for any tâˆˆ[0, T]: (1) for Î·= 0, then 1/Â¯Î²â‰¤Î²tâ‰¤max{Â¯Î², t}
andRT
0Î²t/Ï„dtâ‰¤C; (2) for Î·= 1, then 1/Â¯Î²â‰¤Î²tâ‰¤max{Â¯Î², t2}.
As shown in Chen et al. [2023b], due to the absence of the stochasticity, the small errors for quickly
accumulate and are magnified. Hence, we assume a conservative Î²tfor the reverse PFODE, whose
growth rate is at most t, to avoid an additional exp (T)in the convergence guarantee (see Section 6.1).
We note that this choice of Î²tis satisfied in practical applications [Song et al., 2020b, Karras et al.,
2022]. For the reverse SDE setting, we assume the growth rate of Î²tcan depend on Ï„instead of at
most linear. For example, when Ï„=T2, we can choose Î²t=t2, which has the same order as Ï„. As
shown in Theorem 4.2, the drifted VESDE with aggressive Î²t=t2has an exponential-decay forward
convergence rate, which leads to the first efficient polynomial complexity for VE-based models.
Notations. ForxâˆˆRdandAâˆˆRdÃ—d, we denote by âˆ¥xâˆ¥andâˆ¥Aâˆ¥the Euclidean norm for vector
and the spectral norm for matrix. We denote by Â¯Î³K=argmaxkâˆˆ{0,...,Kâˆ’1}Î³kthe maximum stepsize
forkâˆˆ[0, Kâˆ’1]. We denote by q0PTthe distribution of XT,QqÏ„
âˆ
tKthe distribution of YtK,RqÏ„
âˆ
K
the distribution of eYtKandQq0PT
tKthe distribution which does reverse process starting from qÏ„
T(Eq.
5). We denote by W1andW2the Wasserstein distance of order one and two, respectively.
4 The Faster Forward Convergence Rate for the Drifted VESDE
This section shows that the drifted VESDE has a fast forward convergence rate. Since qÏ„
Tcontains
the data information, we first introduce the manifold hypothesis before controlling TV(qÏ„
T, qÏ„
âˆ).
Assumption 4.1. q0is supported on a compact set Mand0âˆˆ M .
We denote Rthe diameter of the manifold by R= sup{âˆ¥xâˆ’yâˆ¥:x, yâˆˆ M} and assume R > 1.
As shown in Section 1, the manifold hypothesis is supported by much empirical evidence [Bengio
5et al., 2013, Fefferman et al., 2016, Pope et al., 2021] and allows the blow-up phenomenon of the
score. Recently, Tang and Yang [2024] show that diffusion models can adapt to the intrinsic manifold
structure. With Assumption 4.1, we obtain the forward process guarantee for the drifted VESDE.
Theorem 4.2. Assume Assumption 4.1 and 3.1. Let qÏ„
âˆ=N(0, Ïƒ2
TI). With mT, ÏƒTdefined in
Equation (4), we have TV(qÏ„
T, qÏ„
âˆ)â‰¤âˆšmTÂ¯D/Ïƒ T, where Â¯D=d|c|+E[q0] +Randcis the
eigenvalue of Cov [q0]with the largest absolute value.
Recall that mT= exp( âˆ’RT
0Î²t/Ï„dt), the previous VESDE [Song et al., 2020b, Karras et al., 2022,
Lee et al., 2022] chooses a conservative Î²tsatisfiesRT
0Î²t/Ï„dtâ‰¤C. However, with an aggressive Î²t,
the drifted VESDE will have a faster convergence rate. To illustrate the accelerated forward process,
we use Ï„=T2as an example and discuss different Î²t=tÎ±1, Î±1âˆˆ[1,2]. Due to the definition of ÏƒT,
ÏƒTâ‰ˆT, and the forward convergence rate mainly depends onâˆšmT. When Î±1= 1is conservative,
mTis a constant, and the convergence rate is 1/T. When Î±1= 1 + ln(2 rln(T))/ln(T)is slightly
aggressive, the convergence rate is 1/Tr+1forr >0. When Î±1â‰¥1 + ln( Tâˆ’ln(T))/ln(T)is
aggressive, the convergence rate is faster than exp(âˆ’T). In our analysis, whether Î²tcan be aggressive
depends on the reverse process (see Section 6). When choosing an aggressive Î²t, the drifted VESDE
achieves an improved sample complexity compared with pure VESDE (see Section 5).
5 The Polynomial Complexity for a Series of VESDE with Reverse SDE
In this section, we first pay attention to the reverse SDE ( Î·= 1) to show the power of the drifted
VESDE. More specifically, we show that our general drifted VESDE form covers the current models
(VP and VESDE). After that, we show that drifted VESDE can go beyond the current models and
achieve an improved complexity with an aggressive Î²t. Since the objective function minimizes the
L2distance between the ground truth and the approximated score, we assume that the approximated
score is L2-accuracy, which is exactly the same with Chen et al. [2023c] and Benton et al. [2023].
Assumption 5.1. Eqtkhstkâˆ’ âˆ‡lnqÏ„
tk2i
â‰¤Ïµ2
score forâˆ€kâˆˆ[K].
With this assumption, we provide a universe convergence guarantee for Ï„âˆˆ[1, T2]andÎ²tâˆˆ[1, t2]
and discuss the sample complexity of VP and VE-based models in detail.
Theorem 5.2. Assume Assumption 3.1, 4.1, 5.1. Let Â¯Ddefined in Theorem 4.2, Â¯Î³K=
argmaxkâˆˆ{0,...,Kâˆ’1}Î³k,Ï„=T2andÎ²tâˆˆ[1, t2]. Then, we have that
TV
RqÏ„
âˆ
K, qÎ´
â‰¤Â¯DâˆšmT
ÏƒT+R2âˆš
d
Ïƒ4
Î´p
Â¯Î³KÎ²TÏ„T+Ïµscorep
Î²TT .
To guarantee the above convergence guarantee smaller than eO(ÏµTV), each component of the result
needs to be smaller than ÏµTV. As shown in Remark 5.3, it is difficult for pure VESDE to balance the
approximated score and the first two error terms to achieve an efficient sample complexity. Hence, we
discuss how to balance the reverse beginning and discretization error. More specifically, we require
Â¯DâˆšmT/ÏƒTâ‰¤ÏµTVandÂ¯Î³Kâ‰¤Ïƒ8
Î´Ïµ2
TV/ 
R4dÎ²TÏ„T
, where the first inequality determines the order
ofTand the second inequality determines the stepsize Â¯Î³K. After that, with sample complexity
K=T/Â¯Î³K, we have that TV(RqÏ„
âˆ
K, qÎ´)â‰¤eO(ÏµTV). The last step is to guarantee qÎ´andq0is close
enough W2
2(q0, qÎ´)â‰¤Ïµ2
W2, which requires Ïƒ2
Î´â‰¤Ïµ2
W2/(d+Râˆš
d).
Following the above process, this general convergence guarantee leads to the polynomial sample
complexity for VP and VE-based models. When Î²t= 1andÏ„= 1, the drifted VESDE becomes
VPSDE and achieve the complexity eO(1/(Ïµ8
W2Ïµ2
TV)), which achieve exactly the same order compared
with Chen et al. [2023c]. When Î²t= 1andÏ„=T, our formula is similar but slightly better (Figure 2
and 3) to pure VESDE ( Ïƒ2
t=t) and achieves O(1/Ïµ8
W2Ïµ8
TV)result. For Î²t=tandÏ„=T2, the
general formula is similar to SOTA pure VESDE ( Ïƒ2
t=t2) and achieves the first polynomial results
O(1/Ïµ8
W2Ïµ7
TV)for this model under the manifold hypothesis4.
Although we achieve the first polynomial sample complexity for VE-based models under the manifold
hypothesis, it is clear that the results of the VE-based models are significantly worse than the result
4We note these results still hold for pure VESDE with Ïƒ2
t=tandt2, and we use the general drifted VESDE
for simplicity. Here, we only consider the dependence of Ïµ. Readers can find detailed results in Appendix C.1.
6of VP-based models since the slow 1/Poly(T)forward convergence guarantee. More specifically,
the forward convergence rate of VPSDE is exp (âˆ’T), which means Thas the order log(1/ÏµTV)and
can be ignore. When considering the pure VESDE with Ïƒ2
t=t2, the forward convergence rate is
Â¯D/T , which indicates Tâ‰¥Â¯D/Ïµ TVis a polynomial term and can not be ignored. Hence, the results
K=R4dT5/(Ïƒ8
Î´Ïµ2
TV)of pure VESDE ( Ïƒ2
t=t2) is heavily influenced by T. When considering
pure VESDE with Ïƒ2
t=t, it suffers from a slower forward convergence guarantee Â¯D/âˆš
T, which
indicates Tâ‰¥Â¯D2/Ïµ2
TVandK=R4dT3/(Ïƒ8
Î´Ïµ2
TV). This is the first work to explain why pure
VESDE ( Ïƒ2
t=t2) performs better than pure VESDE ( Ïƒ2
t=t2) from a theoretical perspective.
Remark 5.3.In this part, we explain the reason why the pure VESDE fails to balance the reverse
beginning and the approximated score. We use the pure VESDE with Ïƒ2
t=t2as an example. Under
this setting, the guarantee has the form 1/T+p
Â¯Î³KT4/Î´4+Ïµscoreâˆš
T2, which requires Tâ‰¥1/ÏµTV.
Then, Ïµscoreâˆš
T2is larger than Ïµscore/p
Ïµ2
TV. Hence, it is hard to achieve non-asymptotic results.
Drifted VESDE with an aggressive Î²tbalances different error terms. This part shows that
our drifted VESDE with a suitable aggressive Î²tcan balance the above three error terms. More
specifically, we show that introducing aggressive Î²tonly slightly affects the discretization error and
significantly benefits in balancing reverse beginning and approximated errors. As a result, we obtain
an efficient polynomial complexity for a series of VE-based models with unbounded Î²t.
Corollary 5.4. Following the setting of Theorem 5.2. When considering Ï„=T2, Î²t=t2, by
choosing Î´â‰¤Ïµ2/3
W2
(d+Râˆš
d)1/3,Tâ‰¥2 ln(Â¯D/ÏµTV),Â¯Î³Kâ‰¤Î´12Ïµ2
TVln5(Â¯D/Ïµ TV)/(R4d)and assuming
Ïµscoreâ‰¤eO(ÏµTV),RqÏ„
âˆ
Kis(ÏµTV+Ïµscore)close to qÎ´, which is ÏµW2close to q0, with sample complexity
Kâ‰¤eO 
dR4(d+Râˆš
d)4
Ïµ8
W2Ïµ2
TV!
.
Defined by RqÏ„
âˆ
K,R 0the output RqÏ„
âˆ
Kprojected onto B (0, R0)forR0=eÎ˜(R). Then, we achieve pure
W2guarantee W2(RqÏ„
âˆ
K,R 0, q0)â‰¤ÏµW2with sample complexity eO
dR8(d+Râˆš
d)4
Ïµ12
W2
.
Since our drifted VESDE with Ï„=T2andÎ²t=t2has a fast forward convergence rate exp (âˆ’T)/T2,
Tbecomes a logarithmic term and does not influence the discretization term, which is the source of
the improved sample complexity. Furthermore, the requirement of Ïµscore has the same order with ÏµTV,
which indicates the drifted VESDE balances the reverse beginning and approximated score error. In
fact, we only require the forward convergence rate of drifted VESDE is exp (âˆ’T), which indicates
a series of VE-based models can achieve this sample complexity. We use Ï„=T2as an example.
When considering the Î²t=tÎ±1, we require 2â‰¥Î±1â‰¥1 + ln( Tâˆ’ln(T))/ln(T)to enjoy exp (âˆ’T)
forward convergence rate and achieve Kâ‰¤eO 
1/(Ïµ8
W2Ïµ2
TV)
sample complexity. For Î²t=tand
Ï„=T, we also obtain complexity Kâ‰¤eO 
1/ 
Ïµ8
W2Ïµ2
TV
(Appendix C.1).
Remark 5.5.Recently, Lee et al. [2022] and Gao et al. [2023] consider the sample complexity of
VESDE with reverse SDE under strong assumption. Lee et al. [2022] consider VESDE with Ïƒ2
t=t
and achieve ËœO(L2/Ïµ4
TV)result under the LSI assumption. Under the manifold hypothesis, the result
isËœO(1/Ïµ8
W2Ïµ4
TV), which is worse than Corollary 5.4. Gao et al. [2023] achieve pure W2guarantee
eO(1/Ïµ2.5
W2)under the log-concave distribution, which is even stronger than LSI assumption and ignore
the influence of Î´. Hence, they ignore an additional 1/Poly(W2)(Detail in Appendix A.2).
6 The Tangent-based Analysis Framework
To deepen the understanding of VE-based models instead of the specific reverse process, we introduce
the unified framework for VESDE with reverse SDE and PFODE. Similar to previous PFODE work
[Chen et al., 2023d], we assume an accurate score and consider the other errors.
Theorem 6.1. Assume Assumption 3.1 and 4.1, Î´â‰¤1/32andÎ³ksupvâˆˆ[Tâˆ’tk+1,Tâˆ’tk]Î²v/Ïƒ2
vâ‰¤1/28
forâˆ€kâˆˆ {0, ..., K âˆ’1}. LetÎ³K=Î´. Then, for âˆ€Ï„âˆˆ[T, T2]:
7(1) If Î·= 1(the reverse SDE), choosing Î²t=t2,W1
RqÏ„
âˆ
K, q0
is bounded by
(R
Ï„+âˆš
d)âˆš
Î´+ expR2
2(Â¯Î²
Î´3+1
Ï„) 
C1(Ï„)TÎº2
1(Ï„)
(Â¯Î²
Î´3+1
Ï„)Â¯Î³1/2
K+ 1
Â¯Î³1/2
K+Â¯Deâˆ’T/2
âˆšÏ„!
,
where Îº1(Ï„) =T2(1/Ï„+Â¯Î²/Î´3)andC1(Ï„)is linear in Ï„2.
(2) If Î·= 0(PFODE), choosing a conservative Î²t(Assumption 3.1), W1
RqÏ„
âˆ
K, q0
is bounded by
(R
Ï„+âˆš
d)âˆš
Î´+ expR2
2(Â¯Î²
Î´2+1
Ï„) 
C2(Ï„)Îº2
2(Ï„)T
(Â¯Î²
Î´2+1
Ï„)Â¯Î³1/2
K+ 1
Â¯Î³1/2
K+Â¯DâˆšÏ„!
,
where Îº2(Ï„) =T 
1/Ï„+Â¯Î²/Î´2
andC2(Ï„)is linear in Ï„2.
Theorem 6.1 proves the first quantitative guarantee for VE-based models with reverse PFODE using
the unified tangent-based framework. Correspondingly, the Girsanov-based method [Chen et al.,
2023c,a] can not deal with reverse PFODE since the reverse process diffusion term is not well-defined.
Recently, Chen et al. [2023d] employ the Restoration-Degradation framework to analyze VESDE
with reverse PFODE, which also has exponential dependence on RandÎ´. Furthermore, their results
have exponential dependence on Î²t(gmaxin Chen et al. [2023d]), which corresponds to Ï„. However,
our dependence on Ï„appears in the polynomial term. Hence, our framework is a suitable unified
framework. Furthermore, we emphasize that our tangent-based unified framework is not a simple
extension of Bortoli [2022]. We carefully control the tangent process according to the variance
exploding property of VESDE to avoid exp (T)term when considering PFODE (Section 6.1).
Theorem 6.1 has exponential dependence on RandÎ´, which is introduced by the tangent process.
Similar to Bortoli [2022], if we assume the Hessianâˆ‡2logqt(xt)â‰¤Î“/Ïƒ2
t, we obtain a better con-
trol on the tangent process and replace the exponential dependence on Î´by a polynomial dependence
onÎ´and exponential dependence on Î“when considering reverse PFODE.
Corollary 6.2. Assume Assumption 3.1, 4.1 andâˆ‡2logqt(xt)â‰¤Î“/Ïƒ2
t. Let Î·= 0 (reverse
PFODE), Î´âˆˆ(0,1/32), Ï„=T2,Î²t=tandÎº2(Ï„), C2(Ï„)defined in Theorem 6.1, we have
W1
RqÏ„
âˆ
K, q0
â‰¤(R
Ï„+âˆš
d)âˆš
Î´+Â¯Î²Î“
2
Î´Î“expÎ“ + 2
2 
C2(Ï„)Îº2
2(Ï„)T((Â¯Î²
Î´2+1
Ï„)Â¯Î³1/2
K+ 1)Â¯Î³1/2
K+Â¯DâˆšÏ„!
.
Though the additional assumption is strong, many special cases, such as hypercube M=
[âˆ’1/2,1/2]psatisfy this assumption. We emphasize that our analysis also holds for VESDE ( Ïƒ2
t=t2)
with reverse PFODE, which means our results can explain the SOTA model in Karras et al. [2022].
6.1 The Discussion on the Unified Framework
In this section, we introduce the unified tangent-based framework for reverse SDE and PFODE and
discuss key steps to achieve the quantitative guarantee for PFODE. Firstly, we decompose the goal
W1
RqÏ„
âˆ
K, q0
into three terms: W1
RqÏ„
âˆ
K, QqÏ„
âˆ
tK
+W1
QqÏ„
âˆ
tK, Qq0PT
tK
+W1
Qq0PT
tK, q0
.
These terms correspond to the discretization scheme, reverse beginning distribution, and the early
stopping parameter Î´. We focus on most difficult discretization term and first recall the stochastic
flow of the reverse process for any xâˆˆRdands, tâˆˆ[0, T]withtâ‰¥s:
dYx
s,t=Î²Tâˆ’t
Yx
s,t/Ï„+ 
1 +Î·2
âˆ‡logqTâˆ’t 
Yx
s,t	
dt+Î·p
2Î²Tâˆ’tdBt,where Yx
s,s=x .
Withâˆ‡Yx
s,s=I, the corresponding tangent process is
dâˆ‡Yx
s,t=Î²Tâˆ’tâˆ‡Yx
s,t/Ï„dt+Î²Tâˆ’t 
1 +Î·2
âˆ‡2logqTâˆ’t(Yx
s,t)âˆ‡Yx
s,tdt .
The key of the discretization error is to bound tangent processâˆ‡Yx
s,tK. For this term, we consider
the reverse SDE and PFODE simultaneously and propose a general version of Bortoli [2022].
Lemma 6.3. Assume Assumption 3.1 and 4.1. For âˆ€sâˆˆ[0, tK]andxâˆˆRd, we have
âˆ¥âˆ‡Yx
s,tKâˆ¥ â‰¤expR2
2Ïƒ2
Tâˆ’tK+(1âˆ’Î·2)
2ZtK
0Î²Tâˆ’u
Ï„du
.
8(a) Original Figure
 (b) VESDE ( ğ‘‡=100)
 (c) Drifted VESDE ( ğ‘‡=100)Figure 1: Experiment results of Swiss roll with Euler Maruyama Method (Reverse SDE)
We emphasize that the general bound for the tangent process is the key to achieving the guarantee for
VESDE with the reverse ODE. Recall that in the original lemma for the tangent processes, since Ï„
is independent of TandÎ²tis bounded in a small interval [1/Â¯Î²,Â¯Î²],RtK
0Î²Tâˆ’u/Ï„du= Î˜( T), which
means there is an additional exp (T)when considering VPSDE with revere PFODE. However, our
tangent-based lemma makes use of the variance exploding property of VESDE to guarantee thatRT
0Î²t/Ï„dtâ‰¤Cwith a conservative Î²t=twhen considering reverse PFODE. When Î·= 1, we
choose aggressive Î²t=t2since the choice of Î²tdoes not affect the bound of the tangent process.
For the early stopping term, it corresponds to Î´and is smaller than 2(R/Ï„+âˆš
d)âˆš
Î´. Since we can
not use the data processing inequality in Wasserstein distance, the reverse beginning terms consists of
the bound of tangent process term and the forward process term:
W1
QqÏ„
âˆ
tK, Qq0PT
tK
â‰¤âˆšmTÂ¯D
ÏƒTexpR2
2Ïƒ2
Tâˆ’tK+(1âˆ’Î·2)
2ZtK
0Î²Tâˆ’u
Ï„du
.
One notable future work is introducing the short regularization technique [Chen et al., 2023b] and
suitable corrector to remove the above exponential dependence.
7 Experiments
In this section, we show the power of the drifted VESDE forward process through experiments.
Section 7.1 shows that aggressive one achieves good balance in different error terms. After that, we
consider the approximated score and show that the conservative one can improve the quality of the
generated distribution without training in the synethetic and real-world setting.
7.1 The Aggressive Drifted VESDE Balances Errors
In this section, we do experiments on 2-D Gaussian to show that the aggressive drifted VESDE
balances different errors. Since the ground truth score of the Gaussian can be directly calculated, we
use the accurate score function to discuss the balance between the other two error terms clearly. We
show how to use approximated score in Section 7.2.
Figure 2: Results of 2-D GaussianAs shown in Figure 2, the process with aggres-
siveÎ²t=t2achieves the best and second per-
formance in EI and EM discretization, which
supports our theoretical result (Corollary 5.4).
The third best process is conservative Î²t=t
with the small drift term. The reason is that
though it can not achieve a exp (âˆ’T)forward
guarantee, it also has a constant decay on prior
information, which slightly reduces the effect of
the reverse beginning error (Section 3.1). The
worst process is pure VESDE since it is hard to
balance different error sources. Our experimen-
tal results also show that EI is better than EM.
9(a) Pure VESDE
(b) Drifted VESDEFigure 3: Experiment results of CelebA dataset
7.2 The Conservative Drifted VESDE Benefits from VESDE without Training
As shown in Figure 2, the red and orange lines have similar trends. Hence, for conservative drifted
VESDE, which satisfies (2) of Assumption 3.1, we can directly use the models trained by pure
VESDE to improve the quality of generated distribution. We confirm our intuition by training the
model with pure VESDE with Ïƒ2
t=tand directly use the models to conservative drifted VESDE
withÎ²t= 1andÏ„=T. From the experimental results (Figure 1), it is clear that pure VESDE has a
low density on the Swiss roll except for the center one, which indicates pure VESDE can not deal
with large dataset variance, as we discuss in Section 4. For conservative drift VESDE, as we discuss
in the above section, it can reduce the influence of the dataset information. Figure 1 (c) supports our
augmentation and shows that the density of the generated distribution is more uniform compared to
pure VESDE, which means that the drift VESDE can deal with large dataset mean and variance.
Beyond the synthetic data, we show that our conservative drifted VESDE can improve the generated
images of pure VESDE without training on the real-world CelebA256 dataset. From the qualitative
perspective, as shown in Figure 3, the images generated by our drifted VESDE have more detail (such
as hair and beard details). On the contrary, since pure VESDE can not deal with large variance, the
images generated by pure VESDE appear blurry and unrealistic in these details. From the quantitative
results, we use aesthetic score [Schuhmann et al., 2022] and Inception Score to measure the quality
of generated images. Our drifted VESDE achieves aesthetic score 5.813, and IS 4.174, which is
better than the results of baseline pure VESDE (aesthetic score 5.807and IS 4.082). There are more
examples on CelebA256 and more experiments on Swiss roll and 1D-GMM to explore different
sampling methods (RK45, PFODE) and different T. We refer to Appendix G for more details.
8 Conclusion
In this work, we analyze the VE-based models under the manifold hypothesis. Firstly, we propose a
new forward drifted VESDE process, which enjoys a faster forward convergence rate. Then, we show
that with an aggressive Î²t, the new process balances different errors and achieve the first efficient
polynomial sample complexity for a series of VE-based models with reverse SDE.
After achieving the above results, we go beyond the reverse SDE and propose the tangent-based unified
framework, which considers reverse SDE and PFODE at the same time. Under this framework, we
make use of the variance exploding property of VESDE and achieve the first quantitative convergence
guarantee for SOTA VE-based models with reverse PFODE. Finally, we show the power of the new
drifted forward process through synthetic and real-world experiments.
Future Work and Limitation. This work proposes the first unified framework for VE-based models
with an accurate score. After that, we plan to consider the approximated score error and provide a
polynomial complexity for the VE-based models with reverse PFODE under the manifold hypothesis.
Broader Impact. Our work focuses on the convergence guarantee of the SOTA diffusion models and
deepens the understanding of diffusion models. Therefore, this work can be viewed as a fundamental
step in improving the quality of diffusion models and the societal impact is similar to general
generative models [Mirsky and Lee, 2021].
10Acknowledgments and Disclosure of Funding
The author Bo Jiang is supported by National Natural Science Foundation of China (62072302).
References
Vladimir M Alekseev. An estimate for the perturbations of the solutions of ordinary differential equations. Vestn.
Mosk. Univ. Ser. I. Math. Mekh , 2:28â€“36, 1961.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives.
IEEE transactions on pattern analysis and machine intelligence , 35(8):1798â€“1828, 2013.
Joe Benton, Valentin De Bortoli, Arnaud Doucet, and George Deligiannidis. Linear convergence bounds for
diffusion models via stochastic localization. arXiv preprint arXiv:2308.03686 , 2023.
Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. Trans. Mach.
Learn. Res. , 2022, 2022. URL https://openreview.net/forum?id=MhK5aXo3gB .
Patrick Cattiaux, Giovanni Conforti, Ivan Gentil, and Christian LÃ©onard. Time reversal of diffusion processes
under a finite entropy condition. arXiv preprint arXiv:2104.07708 , 2021.
Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-
friendly bounds under minimal smoothness assumptions. In International Conference on Machine Learning ,
pages 4735â€“4763. PMLR, 2023a.
Sitan Chen, Sinho Chewi, Holden Lee, Yuanzhi Li, Jianfeng Lu, and Adil Salim. The probability flow ode is
provably fast. arXiv preprint arXiv:2305.11798 , 2023b.
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the
score: theory for diffusion models with minimal data assumptions. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023 . OpenReview.net, 2023c. URL
https://openreview.net/pdf?id=zyLVMgsZ0U_ .
Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: A non-
asymptotic analysis for ddim-type samplers. In International Conference on Machine Learning , pages
4462â€“4484. PMLR, 2023d.
Giovanni Conforti, Alain Durmus, and Marta Gentiloni Silveri. Score diffusion models without early stopping:
finite fisher information is all you need. arXiv preprint arXiv:2308.12240 , 2023.
Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrÃ¶dinger bridge with
applications to score-based generative modeling. Advances in Neural Information Processing Systems , 34:
17695â€“17709, 2021.
Pierre Del Moral and Sumeetpal Sidhu Singh. Backward itÃ´â€“ventzell and stochastic interpolation formulae.
Stochastic Processes and their Applications , 154:197â€“250, 2022.
Zihan Ding and Chi Jin. Consistency models as a rich and efficient policy class for reinforcement learning.
arXiv preprint arXiv:2309.16984 , 2023.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image
synthesis. arXiv preprint arXiv:2403.03206 , 2024.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis. Journal of the
American Mathematical Society , 29(4):983â€“1049, 2016.
Xuefeng Gao and Lingjiong Zhu. Convergence analysis for general probability flow odes of diffusion models in
wasserstein distances. arXiv preprint arXiv:2401.17958 , 2024.
Xuefeng Gao, Hoang M Nguyen, and Lingjiong Zhu. Wasserstein convergence guarantees for a general class of
score-based generative models. arXiv preprint arXiv:2311.11003 , 2023.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural
Information Processing Systems , 33:6840â€“6851, 2020.
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video
diffusion models. arXiv preprint arXiv:2204.03458 , 2022.
11Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based
generative models. Advances in Neural Information Processing Systems , 35:26565â€“26577, 2022.
Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A universal
training technique of score-based diffusion model for high precision score estimation. arXiv preprint
arXiv:2106.05527 , 2021.
Dongjun Kim, Yeongmin Kim, Wanmo Kang, and Il-Chul Moon. Refining generative process with discriminator
guidance in score-based diffusion models. arXiv preprint arXiv:2211.17091 , 2022.
Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Fp-
diffusion: Improving score-based diffusion models by enforcing the underlying score fokker-planck equation.
In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , volume 202 of Proceedings of Machine Learning Research , pages 18365â€“18398. PMLR, 2023.
Jean-FranÃ§ois Le Gall. Brownian motion, martingales, and stochastic calculus . Springer, 2016.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial
complexity. Advances in Neural Information Processing Systems , 35:22870â€“22882, 2022.
Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data
distributions. In International Conference on Algorithmic Learning Theory , pages 946â€“985. PMLR, 2023.
Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Towards faster non-asymptotic convergence for diffusion-based
generative models. arXiv preprint arXiv:2306.09251 , 2023.
Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Pengyuan Zhou.
Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. arXiv preprint
arXiv:2404.03575 , 2024.
Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps
are flawed. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages
5404â€“5411, 2024.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for
diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927 , 2022.
Yisroel Mirsky and Wenke Lee. The creation and detection of deepfakes: A survey. ACM Computing Surveys
(CSUR) , 54(1):1â€“41, 2021.
Jakiw Pidstrigach. Score-based generative models detect manifolds. arXiv preprint arXiv:2206.01018 , 2022.
Phillip Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic dimension of
images and its impact on learning. arXiv preprint arXiv:2104.08894 , 2021.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 10684â€“10695, 2022.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset
for training next generation image-text models. Advances in Neural Information Processing Systems , 35:
25278â€“25294, 2022.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502 , 2020a.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances
in neural information processing systems , 32, 2019.
Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in
neural information processing systems , 33:12438â€“12448, 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 ,
2020b.
12Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International
Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , volume 202 of
Proceedings of Machine Learning Research , pages 32211â€“32252. PMLR, 2023.
Rong Tang and Yun Yang. Adaptivity of diffusion models to manifold structures. In International Conference
on Artificial Intelligence and Statistics , pages 1648â€“1656. PMLR, 2024.
Jiayan Teng, Wendi Zheng, Ming Ding, Wenyi Hong, Jianqiao Wangni, Zhuoyi Yang, and Jie Tang. Relay
diffusion: Unifying diffusion process across resolutions for image synthesis. arXiv preprint arXiv:2309.03350 ,
2023.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation , 23(7):
1661â€“1674, 2011.
Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv
preprint arXiv:2204.13902 , 2022.
13Appendix
A More Discussion on Drifted VESDE and Current Works
A.1 The Drifted VESDE is Representative Enough.
In this work, we consider Ï„âˆˆ[T, T2]and show that this choice is enough to represent the current
VESDE. More specifically, since VESDE with Ïƒ2
t=thasqT=N(E[q0],Cov[q0] +TI)and
drifted VESDE with Ï„=T2andÎ²tâ‰¡1/2hasqÏ„
T=N(exp (âˆ’1
2T)E[q0],exp (âˆ’1
T)Cov[q0] + (1âˆ’
exp (âˆ’1
T))T2I), these two setting is almost identical when Tâ†’+âˆ. The second choice of VESDE
Ïƒ2
t=t2, which achieves the state-of-the-art performance [Karras et al., 2022], is almost identical
toÏ„=T2,Î²t=t. The simulation experiments also show that VESDE and drifted VESDE with
specific Î²tandÏ„have similar performance (Figure 2).
A.2 The Detailed Calculation of Previous work
The results of Lee et al. [2022] Lee et al. [2022] consider VESDE ( Ïƒ2
t=t) with reverse SDE
under the LSI assumption with parameter CLS. The LSI assumption does not allow the presence of
substantial non-convexity and is far away from the multi-modal real-world distribution. Furthermore,
they use unrealistic assumption Ïµscoreâ‰¤1/(CLS+T)to avoid the effect of the approximated score,
which is stronger than Assumption 5.1. Under the above strong assumption, Lee et al. [2022] achieve
the polynomial sample complexity ËœO(L2d(d|c|+R)2/Ïµ4
TV). Under the manifold hypothesis, by
Lemma E.2, we know that
L=R2d2/Ïµ4
W2
Then, the result is ËœO(R4d5(d|c|+R)2/Ïµ8
W2Ïµ4
TV), which is worse than Corollary 5.4.
The results of Gao et al. [2023]. Gao et al. [2023] analyze a series of VESDE with reverse SDE and
achieve 1/Ïµ2.5
W2sample compelxity for VESDE with Ïƒ2
t=t2in 2-Wasserstein distance. However, they
assume the data distribution is log-concave, which is even stronger than LSI assumption. Furthermore,
under this assumption, âˆ‡logqTâˆ’t(Â·)do not blow-up at the end of the reverse process, which do not
match the empirical phenomenon and ignore the influence of early stopping parameters. To transfer
their results to our the results under the manifold hypothesis, we need to consider the influence of Î´,
which would introduce an additional 1/Poly(W2)term.
B The Proof for the Faster Forward Process
Lemma B.1. The minimization problem min Â¯mt,VtKL (qÏ„
t| N( Â¯mt, Vt))is minimized by Â¯mt=
mtE[q0]andVt=m2
tCov [q0] +Ïƒ2
tI, where mtandÏƒtdefined in Equation (4).
Proof .For simplicity, we denote the mean and covariance of q0byaandCâ€². We also define the
optimize variable nt=N( Â¯mt, Ct). We can directly compute the KL divergence KL(qt|nt):
KL (qt|nt) =âˆ’H(qt)âˆ’Z
log (nt(x))qt(x)dx
=âˆ’H(qt) +d
2log(2Ï€) +1
2log (det ( Vt)) +1
2Z
(xâˆ’Â¯mt)TVâˆ’1
t(xâˆ’Â¯mt)qt(x)dx .
For the last term, we directly compute
Z
(xâˆ’Â¯mt)TVâˆ’1
t(xâˆ’Â¯mt)pt(x)dx
=Eh
(Xtâˆ’Â¯mt)TVâˆ’1
t(Xtâˆ’Â¯mt)i
=Eh
(mtX0+ÏƒtZâˆ’Â¯mt)TVâˆ’1
t(mtX0+ÏƒtZâˆ’Â¯mt)i
=Eh
m2
t(X0âˆ’a)TVâˆ’1
t(X0âˆ’a)i
+ (mtaâˆ’Â¯mt)TVâˆ’1
t(mtaâˆ’Â¯mt) +Ïƒ2
tE
ZTVâˆ’1
tZ
=m2
ttr 
Câ€²Vâˆ’1
t
+Ïƒ2
ttr 
Vâˆ’1
t
+ (mtaâˆ’Â¯mt)TVâˆ’1
t(mtaâˆ’Â¯mt),
14where the second inequality follows that Xt=mtX0+ÏƒtZ. It is clear that the optimal solution of
Â¯mtismta. In the next step, we focus on the optimization problem for Vt:
L 
Vâˆ’1
t
= log (det ( Vt)) + tr  
m2
tCâ€²+Ïƒ2
tI
Vâˆ’1
t
=âˆ’log 
det 
Vâˆ’1
t
+ tr  
m2
tCâ€²+Ïƒ2
tI
Vâˆ’1
t
.
Since the above optimization is a convex optimization problem, we use the method similar to
Pidstrigach [2022], we obtain that the optimal solution of Vtism2
tCâ€²+Ïƒ2
tI. â– 
Lemma B.2. LetÂ¯mtandVtbe the optimal mean and covariance operator from Lemma B.1. Then
KL (qt|N( Â¯mt, Vt))â‰¤1
2log Qd
i=1 
m2
tci+Ïƒ2
t
(Ïƒ2
t)d!
+R2mt
Ïƒ2
t
â‰¤dm2
tc
2Ïƒ2
t+R2mt
Ïƒ2
t+o(m2
tc
Ïƒ2
t),
KL 
N( Â¯mt, Vt)|(N(0, Ïƒ2
t)
â‰¤m2
tPd
i=1ci
2Ïƒ2
t+m2
t(E[q0])2
2Ïƒ2
t+1
2log 
(Ïƒ2
t)d
Qd
i=1(m2
tci+Ïƒ2
t)!
â‰¤m2
tPd
i=1ci
2Ïƒ2
t+m2
t(E[q0])2
2Ïƒ2
t+dm2
tc
2Ïƒ2
t+o(m2
tc
Ïƒ2
t),
where ciare the eigenvalues of Cov [q0], and cis the eigenvalue with the largest absolute value.
Proof .Fortâ‰¥0, we directly calculate the KL divergence for this term:
KL(qt| N( Â¯mt, Vt)) =âˆ’H(qt) +1
2log (det (2 Ï€Vt)) +1
2tr  
m2
tCâ€²+Ïƒ2
tI
Vâˆ’1
t
=âˆ’H(qt) +1
2log (det (2 Ï€Vt)) +d
2
=âˆ’H(qt) +d
2log(2Ï€) +1
2log dY
i=1 
m2
tci+Ïƒ2
t!
+d
2,
where ciare the eigenvalues of Cov [q0]. Now, we only need to calculate H(qt):
âˆ’H(qt) =EXt[logqt(Xt)] =EXt
log
EX0
(2Ï€Ïƒ2
t)âˆ’d/2exp
âˆ’1
2Ïƒ2
tâˆ¥Xtâˆ’X0âˆ¥2
.
By Assumption 4.1, it is clear that
exp
âˆ’1
2Ïƒ2
tâˆ¥Xtâˆ’X0âˆ¥2
â‰¤exp
âˆ’1
2Ïƒ2
t
âˆ¥Xtâˆ¥2+ 2âŸ¨Xt, X0âŸ©
.
Then, we know that
E
log
EX0
(2Ï€Ïƒ2
t)âˆ’d/2exp
âˆ’1
2Ïƒ2
tâˆ¥Xtâˆ’X0âˆ¥2
â‰¤E
log
(2Ï€Ïƒ2
t)âˆ’d/2
âˆ’1
2Ïƒ2
t
âˆ¥Xtâˆ¥2+ 2âŸ¨Xt, X0âŸ©
â‰¤ âˆ’d
2log(2Ï€)âˆ’1
2log 
(Ïƒ2
t)d
âˆ’1
2Ïƒ2
tEh
âˆ¥Xtâˆ¥2i
+R2mt
Ïƒ2
t.
we also know that
Eh
âˆ¥Xtâˆ¥2i
=m2
tEh
âˆ¥X0âˆ¥2i
+Ïƒ2
tE
âˆ¥Zâˆ¥2
=Eh
âˆ¥X0âˆ¥2i
+tE
âˆ¥Zâˆ¥2
= Â¯m2
0+V0+Ïƒ2
td .
Finally, put these terms together, we have:
KL(qt|N( Â¯mt, Vt))â‰¤1
2log Qd
i=1 
m2
tci+Ïƒ2
t
(Ïƒ2
t)d!
+R2mt
Ïƒ2
t,
where ciare the eigenvalues of Cov [q0]. Then by choosing the largest absolute value eigenvalue
largest absolute value, we can use the Taylor expansion to obtain the first results of this lemma. For
the second result of this lemma, we directly compute the KL divergence between N( Â¯mt, Vt)and
N(0, Ïƒ2
t)to obtain the final results. â– 
15Theorem 4.2. Assume Assumption 4.1 and 3.1. Let qÏ„
âˆ=N(0, Ïƒ2
TI). With mT, ÏƒTdefined in
Equation (4), we have TV(qÏ„
T, qÏ„
âˆ)â‰¤âˆšmTÂ¯D/Ïƒ T, where Â¯D=d|c|+E[q0] +Randcis the
eigenvalue of Cov [q0]with the largest absolute value.
Proof .We know that
âˆ¥qTâˆ’qÏ„
âˆâˆ¥TV
â‰¤ âˆ¥qTâˆ’ N(mTE[q0], m2
TCov[q0] +Ïƒ2
TI)âˆ¥TV+âˆ¥N(mTE[q0], m2
TCov[q0] +Ïƒ2
TI)âˆ’qÏ„
âˆâˆ¥TV.
By directly using the Pinskerâ€™s inequality and Lemma B.2, we complete the proof. â– 
C The Proof of the Polynomial Complexity for Reverse SDE
In this section, we prove Corollary 5.4. First, we recall the Girsanovâ€™s Theorem [Le Gall, 2016] used
in Chen et al. [2023c]:
Lemma C.1 (Girsanovâ€™s theorem) .LetPTandQTbe two probability measures on path space
C 
[0, T];Rd
. Suppose that under PT, the process (Xt)tâˆˆ[0,T]follows
dXt=Ëœbtdt+Î±tdËœBt
where ËœBis aPT-Brownian motion, and under QT, the process (Xt)tâˆˆ[0,T]follows
dXt=btdt+Î±tdBt
where Bis aQT-Brownian motion. We assume that for each t >0, Î±tis adÃ—dsymmetric positive
definite matrix. Then, provided that Novikovâ€™s condition holds,
EQTexp 
1
2ZT
0Î±âˆ’1
t
Ëœbtâˆ’bt2
dt!
<âˆ,
we have that
dPT
dQT= exp ZT
0Î±âˆ’1
t
Ëœbtâˆ’bt
dBtâˆ’1
2ZT
0Î±âˆ’1
t
Ëœbtâˆ’bt2
dt!
.
If the Novikovâ€™s condition is satisfied, we apply the Girsanov theorem by choosing PT=
RqÏ„
T
K, QT=QqÏ„
T
tK,Ëœbt=Î²Tâˆ’tn
1
Ï„eYt+ 2s(Tâˆ’tk,eYt)o
(for tâˆˆ[tk, tk+1]),bt=
Î²Tâˆ’t1
Ï„Yt+ 
1 +Î·2
âˆ‡logqTâˆ’t(Yt)	
, and Î±t=p
2Î²Tâˆ’tId.
Then, similar to Chen et al. [2023c], we have the following lemma.
Lemma C.2. Assuming that RqÏ„
T
KandQqÏ„
T
tKsatisfy Novikovâ€™s condition, it holds that
KL
QqÏ„
T
tKâˆ¥RqÏ„
T
K
=EQqÏ„
T
tKlndQqÏ„
T
tK
dRqÏ„
T
K=Kâˆ’1X
k=0EQqÏ„
T
tKZtk+1
tk2Î²Tâˆ’tâˆ¥s(Tâˆ’tk,Ytk)âˆ’ âˆ‡lnqTâˆ’t(Yt)âˆ¥2dt .
Before using the Girsanovâ€™s Theorem, we need to check the Novikovâ€™s condition. We use almost
the same proof process compare to Chen et al. [2023c]. The key proof of the Novikovâ€™s condition
is Lemma 19 of Chen et al. [2023c]. Hence, we give a complete proof of Lemma 19 in Chen et al.
[2023c] under our drifted VESDE. Before the proof, we first introduce a smooth cuttoff function for
truncating the drift terms.
Lemma C.3 (lemma 17 of Chen et al. [2023c]) .For any Â¯R > 0, there is a smooth function
Ï•R:Rdâ†’[0,1]satisfying: 1. Ï•Â¯R(x) = 1 for all âˆ¥xâˆ¥ â‰¤Â¯R, 2.Ï•Â¯R(x) = 0 for all âˆ¥xâˆ¥ â‰¥2Â¯R, 3.Ï•Â¯R
isO(1/Â¯R)-Lipschitz.
16Note that Â¯Ris not Rin Assumption 4.1 and will goes to +âˆin the proof of Chen et al. [2023c].
Similar to Chen et al. [2023c], we also introduce a Lâˆand a modified process with truncation
argument for tâˆˆ[tk, tk+1]. Define the bad set
Bt:={âˆ¥stâˆ’ âˆ‡lnqtâˆ¥ â‰¥Îµscore,âˆ},
where Îµscore,âˆ>0is a parameter to be chosen later. We define the Lâˆ-accurate score estimate to be
sâˆ
t:=st1Bc
t+âˆ‡lnqt1Bt.
We note that âˆ¥sâˆ
tâˆ’ âˆ‡lnqtâˆ¥ â‰¤Îµscore,âˆ.
The modified process with truncation argument for tâˆˆ[tk, tk+1]is
dYâˆ
t=Î²Tâˆ’t
Yâˆ
t/Ï„+ 2âˆ‡logqÏ„
Tâˆ’t(Yâˆ
t)	
dt+p
2Î²Tâˆ’tdBt,
deYâˆ
t=Î²Tâˆ’teYâˆ
t/Ï„+ 2s(Tâˆ’tk,eYâˆ
t)	
dt+p
2Î²Tâˆ’tdBt,
where Yâˆ
0=eYâˆ
0is obtained by sampling Yâˆ
0âˆ¼qÏ„
Tand setting eYâˆ
0=XTifâˆ¥XTâˆ¥ â‰¤ Â¯Rand
setting eYâˆ
0= 0otherwise. Then, we ara ready to prove the following lemma.
Lemma C.4 (Modified key lemma for Novikovâ€™s condition) .
EQqÏ„
T,âˆ
tKexp Kâˆ’1X
k=0Ztk+1
tkÎ²Tâˆ’tkÏ•Â¯R 
Yâˆ
Tâˆ’tk
sâˆ
Tâˆ’tk 
Yâˆ
Tâˆ’tk
âˆ’Ï•Â¯R 
Yâˆ
Tâˆ’tk
âˆ‡lnqTâˆ’t 
Yâˆ
Tâˆ’tk2dt!
<âˆ
Proof .We note that due to the manifold hypothesis (Assumption 4.1), if Â¯Râ‰¥âˆš
dT2+R2, then the
marginal distribution of Yâˆ
Tâˆ’tkis exactly the same compared to Xtk. We also recall that Â¯Râ†’+âˆ
in Chen et al. [2023c] (Theorem 21 of Chen et al. [2023c]). Hence, we can use Lemma E.1 to prove
that
p
Î²Tâˆ’tkÏ•Â¯R 
Yâˆ
Tâˆ’tk
sâˆ
Tâˆ’tk 
Yâˆ
Tâˆ’tkâ‰¤ sup
tâˆ—âˆˆ[0,Tâˆ’Î´]p
Î²Tâˆ’tâˆ—sâˆ
Tâˆ’tâˆ—(YTâˆ’tâˆ—)=:Aâ€²<âˆ.
and
p
Î²Tâˆ’tkÏ•Â¯R 
Yâˆ
Tâˆ’tk
âˆ‡lnqTâˆ’t 
Yâˆ
Tâˆ’tkâ‰¤ sup
tâˆ—âˆˆ[0,Tâˆ’Î´]p
Î²Tâˆ’tâˆ—âˆ¥âˆ‡lnqTâˆ’tâˆ—(YTâˆ’tâ‹†)âˆ¥=:Bâ€²<âˆ.
Then, the left hand of this lemma is at most exp 
2T 
Aâ€²2+Bâ€²2
<âˆas claimed. â– 
After obtaining the above inequality, the remaining proof for Novikovâ€™s condition are exactly com-
pared to Chen et al. [2023c].
Since we assume the accurate score function in this work, this lemma need to control
sup
xâˆ—âˆˆB(0,R),tâˆ—âˆˆ[0,Tâˆ’Î´]2Î²Tâˆ’tâˆ—âˆ¥âˆ‡lnqTâˆ’tâˆ—(xâˆ—)âˆ¥=:B <âˆ.
As we shown in Lemma E.1, we know that with the early stopping parameter Î´,âˆ¥âˆ‡lnqTâˆ’tâˆ—(xâˆ—)âˆ¥
is controlled. By using Assumption 4.1, we know that1
Î²Tâˆ’tâˆ—â‰¤Â¯Î². Finally, with similar process to
Chen et al. [2023c], we can proof that the Novikovâ€™s condition is satisfied. The following lemma
show the discretization error for our drifted VESDE with reverse SDE.
Lemma C.5 (Discretization) .Suppose that Assumption 4.1 and Assumption 5.1 holds. Let Â¯Î³K=
argmaxkâˆˆ{0,...,Kâˆ’1}Î³k, Î³K=Î´. IfÏ„âˆˆ[1, T2]andÎ²tâˆˆ[1, t2], then with QqÏ„
T
tKandRqÏ„
T
Kdefined in
Lemma C.2,
TV
RqÏ„
T
K, QqÏ„
T
tK2
â‰²R4TÏ„Î² Td
Ïƒ8
Î´Â¯Î³K+R6TÏ„Î² T
Ïƒ8
Î´Â¯Î³2
K+Ïµ2
scoreTÎ²T.
17Proof .First, we control the discretization error in an interval tâˆˆ[tk, tk+1]:
EQqÏ„
T
tKh
âˆ¥s(Tâˆ’tk,Ytk)âˆ’ âˆ‡lnqTâˆ’t(Yt)âˆ¥2i
â‰²Ïµ2
score+EQqÏ„
T
tKh
âˆ¥âˆ‡lnqTâˆ’tk(Ytk)âˆ’ âˆ‡lnqTâˆ’t(Ytk)âˆ¥2i
+EQqÏ„
T
tKh
âˆ¥âˆ‡lnqTâˆ’t(Ytk)âˆ’ âˆ‡lnqTâˆ’t(Yt)âˆ¥2i
â‰²EQqÏ„
T
tK"âˆ‡lnqTâˆ’tk
qTâˆ’t(Ytk)2#
+L2EQqÏ„
T
tKh
âˆ¥Ytkâˆ’Ytâˆ¥2i
+Ïµ2
score
â‰²Ï„L2dÂ¯Î³K+Ï„L2Â¯Î³2
K 
dÏ„+R2
+Ï„L3Â¯Î³2
K+L2(Î²TdÂ¯Î³K+R2Â¯Î³2
K) +Ïµ2
score
â‰²Ï„L2dÂ¯Î³K+Ï„L2R2Â¯Î³2
K+Ïµ2
score,
where L= max tâˆˆ[0,Tâˆ’Î´]âˆ‡2logqTâˆ’t(Yt)â‰¤ 
1 +R2
/Ïƒ4
Î´and the third inequality follows
Lemma E.5. Then, we know that
Kâˆ’1X
k=0EQqÏ„
T
tKZtk+1
tk2Î²Tâˆ’tâˆ¥s(Tâˆ’tk,Ytk)âˆ’ âˆ‡lnqTâˆ’t(Yt)âˆ¥2dt
â‰²Ï„TÎ² TL2dÂ¯Î³K+L2R2Ï„TÎ² TÂ¯Î³2
K+Ïµ2
scoreTÎ²T
â‰²R4Ï„TÎ² Td
Ïƒ8
Î´Â¯Î³K+R6Ï„TÎ² T
Ïƒ8
Î´Â¯Î³2
K+Ïµ2
scoreTÎ²T.
After obtaining the general discretization error for our drifted VESDE, we focus on two special cases.
ForÏ„=T2andÎ²t=t2, we have that
Kâˆ’1X
k=0EQqÏ„
T
tKZtk+1
tk2Î²Tâˆ’tâˆ¥s(Tâˆ’tk,Ytk)âˆ’ âˆ‡lnqTâˆ’t(Yt)âˆ¥2dt
â‰²T3Î²TL2dÂ¯Î³K+L2R2T3Î²TÂ¯Î³2
K+Ïµ2
scoreTÎ²T
â‰²R4T3Î²Td
Ïƒ8
Î´Â¯Î³K+R6T3Î²T
Ïƒ8
Î´Â¯Î³2
K+Ïµ2
scoreTÎ²T
=R4T5d
Ïƒ8
Î´Â¯Î³K+R6T5
Ïƒ8
Î´Â¯Î³2
K+Ïµ2
scoreT3.
ForÏ„=TandÎ²t=t, we know that
Kâˆ’1X
k=0EQqÏ„
T
tKZtk+1
tk2Î²Tâˆ’tâˆ¥s(Tâˆ’tk,Ytk)âˆ’ âˆ‡lnqTâˆ’t(Yt)âˆ¥2dt
â‰²T3L2dÂ¯Î³K+L2R2T3Â¯Î³2
K+Ïµ2
scoreT2
â‰²R4T3d
Ïƒ8
Î´Â¯Î³K+R6T3
Ïƒ8
Î´Â¯Î³2
K+Ïµ2
scoreT2.
â– 
Combined with the reversing beginning error controlled by Theorem 4.2, we can obtain the conver-
gence guarantee for the general drifted VESDE with reverse SDE.
Theorem 5.2. Assume Assumption 3.1, 4.1, 5.1. Let Â¯Ddefined in Theorem 4.2, Â¯Î³K=
argmaxkâˆˆ{0,...,Kâˆ’1}Î³k,Ï„=T2andÎ²tâˆˆ[1, t2]. Then, we have that
TV
RqÏ„
âˆ
K, qÎ´
â‰¤Â¯DâˆšmT
ÏƒT+R2âˆš
d
Ïƒ4
Î´p
Â¯Î³KÎ²TÏ„T+Ïµscorep
Î²TT .
18Proof .By the data processing inequality, we know that
TV
RqÏ„
âˆ
K, qÎ´
â‰¤TV
RqÏ„
âˆ
K, RqÏ„
T
K
+ TV
RqÏ„
T
K, QqÏ„
T
tK
â‰¤TV (qÏ„
T, qÏ„
âˆ) + TV
RqÏ„
âˆ
K, QqÏ„
T
tK
.
Combined with Theorem 4.2 and Lemma C.5, we achieve the final result. â– 
C.1 The Sample Complexity for Drifted VESDE
As shown in Theorem 5.2, the general convergence guarantee is
Â¯DâˆšmT
ÏƒT+R2âˆš
d
Ïƒ4
Î´p
Â¯Î³KÎ²TÏ„T+Ïµscorep
Î²TT .
In this section, we provide the sample complexity for different Î²tandÏ„.
The results of Ï„= 1 andÎ²t= 1.When considering Î²t= 1 andÏ„= 1, the drifted VESDE
becomes VPSDE and mT= exp ( âˆ’T), which indicates Tis a logarithmic term and the dominated
term of the convergence guarantee is the discretization term eO(R2âˆšdÂ¯Î³K/Ïƒ4
Î´). To make this term
smaller than ÏµTV, we require Â¯Î³Kâ‰¤Ïƒ8
Î´Ïµ2
TV/(R4d). To make sure that W2
2(q0, qÎ´)â‰¤Ïµ2
W2, we require
Ïƒ2
Î´â‰¤Ïµ2
W2
d+Râˆš
d. Then, we achieve the final sample complexity
Kâ‰¤eO 
dR4(d+Râˆš
d)4
Ïµ8
W2Ïµ2
TV!
.
We note that this results is exactly the same with Chen et al. [2023c], which means our drifted VESDE
covers VPSDE setting.
C.1.1 The results of Ï„=T2with different Î²t
In this part, we analyze the influence of Î²tunder setting Ï„=T2and show the power of our drifted
VESDE.
The aggressive Î²t(Ï„=T2).When considering aggressive Î²t=tÎ±1where 2â‰¥Î±1â‰¥1 + ln( Tâˆ’
ln(T))/ln(T),âˆšmT/ÏƒTâ‰¥exp (âˆ’T/2), which means Tis a logarithmic term and can be ignored.
After that, the analysis process is exactly the same with the above VPSDE setting, and we achieve the
sample complexity
Kâ‰¤eO 
dR4(d+Râˆš
d)4
Ïµ8
W2Ïµ2
TV!
.
Defined by RqÏ„
âˆ
K,R 0the output RqÏ„
âˆ
Kprojected onto B (0, R0)forR0=eÎ˜(R). Following exactly the
same proof process of Chen et al. [2023c] (Corollary 5), we have that
W2(RqÏ„
âˆ
K,R 0, q0)â‰¤ÏµW2
with sample complexity
Kâ‰¤eO 
dR8(d+Râˆš
d)4
Ïµ12
W2!
.
The conservative Î²t=t.In this case, the first term is Â¯D/T . To make this term smaller than ÏµTV,
we require Tâ‰¥Â¯D/Ïµ TV. For the stepsize, we require Â¯Î³Kâ‰¤Ïƒ8
Î´Ïµ2
TV/(R4dT4), which means the
sample complexity is
Kâ‰¤O 
dR4(d+Râˆš
d)4Â¯D5
Ïµ8
W2Ïµ7
TV!
.
19The most conservative Î²t= 1.In this case, mT= exp ( âˆ’1/T)andÏƒ2
t=Ï„(1âˆ’m2
t) =
T2(1âˆ’exp (âˆ’2/T). When Tis large enough, mT= Î˜(1) andÏƒ2
T=T, which indicates the first
term is Â¯D/âˆš
T. To make this term smaller than ÏµTV, we require Tâ‰¥Â¯D2/Ïµ2
TV. For the second term,
we require Â¯Î³Kâ‰¤Ïƒ8
Î´Ïµ2
TV/(R4dT3)Then, the final complexity is
Kâ‰¤O 
dR4(d+Râˆš
d)4Â¯D8
Ïµ8
W2Ïµ10
TV!
.
C.1.2 The results of Ï„=Twith different Î²t
At the remaining part, we show the sample complexity of setting Ï„=Twith different Î²t.
The results for setting Ï„=TandÎ²t=t.For this setting, as shown in Lemma C.5, the
discretization error is
TV
RqÏ„
T
K, QqÏ„
T
tK2
â‰²R4T3d
Ïƒ8
Î´Â¯Î³K+R6T3
Ïƒ8
Î´Â¯Î³2
K+Ïµ2
scoreT2.
Furthermore, we choose an aggressive Î²t, which indicates Tis a logarithmic term. Then, by choosing
Ïƒ2
Î´â‰¤Ïµ2
W2
(d+Râˆš
d)andÂ¯Î³Kâ‰¤Ïƒ8
Î´Ïµ2
TVln3 Â¯D/Ïµ TV
/ 
R4d
, we obtain the sample complexity
K=T
Â¯Î³Kâ‰¤eO 
dR4(d+Râˆš
d)4
Ïµ8
W2Ïµ2
TV!
.
The results for setting Ï„=TandÎ²t= 1.In this setting, the reverse beginning error is
bounded by Â¯D/âˆš
T, which indicates Tâ‰¥Â¯D2/Ïµ2
TV. For the discretization term, we require
Â¯Î³Kâ‰¤Ïƒ8
Î´Ïµ2
TV/(R4dT2). Then, the sample complexity is bounde by
K=T
Â¯Î³Kâ‰¤O 
dR4(d+Râˆš
d)4Â¯D6
Ïµ8
W2Ïµ8
TV!
.
D The Proof of the Convergence Guarantee in the Unified Framework
In this work, we introduce an indicator iâˆˆ {1,2}forÏƒTâˆ’tKto represent different Î²t. We use Ï„=T2
as an example. When Î²t=t2is aggressive, we choose i= 1,Î·= 1andÏƒâˆ’2
Tâˆ’tK(i= 1)â‰¤1
Ï„+Â¯Î²
Î´3.
When Î²t=tis conservative, we choose i= 2,Î·âˆˆ[0,1)andÏƒâˆ’2
Tâˆ’tK(i= 2)â‰¤1
Ï„+Â¯Î²
Î´2. In the proof
process of Lemma 6.3, Lemma D.1, Lemma D.2 and Lemma D.3, we ignore the indicator isince
this lemma does not involve the specific value of Ïƒ2
Tâˆ’tK(i). Before the proof of this section, we first
recall the stochastic flow of the reverse process for any xâˆˆRdands, tâˆˆ[0, T]withtâ‰¥s:
dYx
s,t=Î²Tâˆ’t
Yx
s,t/Ï„+ 
1 +Î·2
âˆ‡logqTâˆ’t 
Yx
s,t	
dt+Î·p
2Î²Tâˆ’tdBt, Yx
s,s=x ,
and the interpolation of its discretization for any kâˆˆ {0, ..., K}andtâˆˆ[sk, tk+1):
dÂ¯Yx
s,t(k) =Î²Tâˆ’tÂ¯Yx
s,t/Ï„+ 
1 +Î·2
s 
Tâˆ’sk,Â¯Yx
s,t	
dt+Î·p
2Î²Tâˆ’tdBt, Â¯Yx
s,s=x ,
where sk= max ( s, tk). To deal with the discretization error, we use the approximation technique
used in Bortoli [2022]. Hence, we introduce the tangent process:
dâˆ‡Yx
s,t=Î²Tâˆ’t
I/Ï„+ 
1 +Î·2
âˆ‡2logqTâˆ’t(Yx
s,t)	
âˆ‡Yx
s,tdt, âˆ‡Yx
s,s=I.
Then, we discuss the interpolation formula, which is used to control the discretization error.
Proposition 1. Fors, tâˆˆ[0, T)withs < t , anykâˆˆ {0, ..., K}and(Ï‰v)vâˆˆ[s,T], we define that
bu(Ï‰) =Î²Tâˆ’u(1
Ï„Ï‰u+ (1 + Î·2)âˆ‡logqTâˆ’u(Ï‰u)),
Â¯bu(Ï‰) =Î²Tâˆ’u(1
Ï„Ï‰u+ (1 + Î·2)s(Tâˆ’sk, Ï‰sk)),âˆ†bu(Ï‰) =bu(Ï‰)âˆ’Â¯bu(Ï‰),
20where sk= max( s, tk)anduâˆˆ[sk, tk+1). Then, for any xâˆˆRd, we have that
Yx
s,tâˆ’Â¯Yx
s,t=Zt
sâˆ‡Yx
u,t Â¯Yx
s,uâŠ¤âˆ†bu Â¯Yx
s,v
vâˆˆ[s,T]
du,
where for any uâˆˆ[0, T), there exists a kâˆˆ {0, ..., K}satisfies uâˆˆ[sk, tk+1).
For reverse SDE, the augmentation is similar to Bortoli [2022] (Appendix E). When Î·= 0, the
stochastic extension of the Alekseevâ€“GrÃ¶bner formula [Del Moral and Singh, 2022] degenerates into
the original version [Alekseev, 1961]. After that, we control the tangent process.
Lemma 6.3. Assume Assumption 3.1 and 4.1. For âˆ€sâˆˆ[0, tK]andxâˆˆRd, we have
âˆ¥âˆ‡Yx
s,tK,iâˆ¥ â‰¤expR2
2Ïƒ2
Tâˆ’tK+(1âˆ’Î·2)
2ZtK
0Î²Tâˆ’u
Ï„du
.
Ifâˆ‡2logqt(xt)â‰¤Î“/Ïƒ2
t,âˆ¥âˆ‡Yx
s,tK,iâˆ¥ â‰¤Ïƒâˆ’(1+Î·2)Î“
Tâˆ’tKexp  
1 +Î·2
Î“ + 2RtK
0Î²Tâˆ’u
Ï„du
.
Proof .Using the definition of the tangent process and Lemma E.1, we have
dâˆ‡Yx
s,t2
â‰¤2Î²Tâˆ’t1
Ï„âˆ‡Yx
s,t2âˆ’ 
1 +Î·2 
1âˆ’m2
Tâˆ’tR2/ 
2Ïƒ2
Tâˆ’t
/Ïƒ2
Tâˆ’tâˆ‡Yx
s,t2
dt .
Using Lemma F.1, we have
Zt
sÎ²Tâˆ’u1
Ï„âˆ’ 
1 +Î·2
/Ïƒ2
Tâˆ’u+ 
1 +Î·2
m2
Tâˆ’uR2/2Ïƒ4
Tâˆ’u
du
â‰¤  
1 +Î·2
R2/4 
Ïƒâˆ’2
Tâˆ’tâˆ’Ïƒâˆ’2
Tâˆ’s
+1âˆ’Î·2
2Zt
sÎ²Tâˆ’u
Ï„du
â‰¤ 
1 +Î·2
R2
4Ïƒ2
Tâˆ’t+1âˆ’Î·2
2Zt
sÎ²Tâˆ’u
Ï„du .
Note that âˆ‡Ys,s=I, we get
âˆ¥âˆ‡Yx
s,tKâˆ¥2â‰¤exp" 
1 +Î·2
R2
2Ïƒ2
Tâˆ’t+ (1âˆ’Î·2)ZtK
0Î²Tâˆ’u
Ï„du#
.
When we assumeâˆ‡logq2
t(xt)â‰¤Î“/Ïƒ2
t, we know that
dâˆ‡Yx
s,t2â‰¤2Î²Tâˆ’t 
1
Ï„âˆ’ 
1 +Î·2
Î“
Ïƒ2
Tâˆ’t!
âˆ‡Yx
s,t2dt .
Using Lemma F.1, we have
2Zt
sÎ²Tâˆ’u/Ïƒ2
Tâˆ’udu
â‰¤log 
exp"
2ZTâˆ’s
0Î²Tâˆ’u
Ï„du#
âˆ’1!
âˆ’log 
exp"
2ZTâˆ’t
0Î²Tâˆ’u
Ï„du#
âˆ’1!
â‰¤log 
Ïƒ2
Tâˆ’s
âˆ’log 
Ïƒ2
Tâˆ’t
+ZTâˆ’s
Tâˆ’tÎ²u
Ï„du .
Then we have
âˆ¥âˆ‡Yx
s,tKâˆ¥2â‰¤Ïƒâˆ’(1+Î·2)Î“
Tâˆ’tKexp  
1 +Î·2
Î“ + 2ZtK
0Î²Tâˆ’u
Ï„du
.
Thus we complete our proof. â– 
21After bounding the gradient of the tangent process, the remaining term is âˆ¥âˆ†bâˆ¥:
âˆ¥âˆ†bâˆ¥ â‰¤ âˆ¥ âˆ†(a,b)bâˆ¥+âˆ¥âˆ†(b,c)bâˆ¥+âˆ¥âˆ†(c,d)bâˆ¥, (7)
where b(a)=bandb(d)=Â¯b. Moreover,
b(b)
u(Ï‰) =Î²Tâˆ’u(1
Ï„Ï‰u+ (1 + Î·2)âˆ‡logqTâˆ’sk(Ï‰u)),
b(c)
u(Ï‰) =Î²Tâˆ’u(1
Ï„Ï‰u+ (1 + Î·2)âˆ‡logqTâˆ’sk(Ï‰sk)),
âˆ†a,b
b=b(a)âˆ’b(b),âˆ†b,c
b=b(b)âˆ’b(c),âˆ†c,d
b=b(c)âˆ’b(d).
We then control âˆ¥âˆ†(a,b)bâˆ¥,âˆ¥âˆ†(b,c)bâˆ¥,âˆ¥âˆ†(c,d)bâˆ¥separately. In this section, âˆ¥âˆ†(c,d)bâˆ¥= 0 since
we assume that the accurate score function is achieved. Forâˆ†(a,b)bu(Ï‰), we have the following
lemma.
Lemma D.1. Fors, uâˆˆ[0, T)such that uâ‰¥s, uâˆˆ[sk, tk+1)andÏ‰= (Ï‰v)vâˆˆ[s,T]we have
âˆ¥âˆ†(a,b)bu(Ï‰)âˆ¥
â‰¤ 
1 +Î·2
Î²Tâˆ’u sup
vâˆˆ[Tâˆ’u,Tâˆ’tk] 
Î²v/Ïƒ6
v 
2 +R2
(R+âˆ¥Ï‰uâˆ¥)Î³k.
Proof .Without loss of generality, we assume sâ‰¤tk. Then
âˆ¥âˆ†(a,b)bu(Ï‰)âˆ¥ â‰¤ 
1 +Î·2
Î²Tâˆ’uâˆ¥âˆ‡logqTâˆ’u(Ï‰u)âˆ’ âˆ‡logqTâˆ’tk(Ï‰u)âˆ¥
â‰¤ 
1 +Î·2
Î²Tâˆ’uÎ³k sup
vâˆˆ[Tâˆ’u,Tâˆ’tk]âˆ¥âˆ‚vâˆ‡logqTâˆ’v(Ï‰u)âˆ¥.
Then by Lemma E.4, we have
âˆ¥âˆ†(a,b)bu(Ï‰)âˆ¥
â‰¤ 
1 +Î·2
Î²Tâˆ’u sup
vâˆˆ[Tâˆ’u,Tâˆ’tk] 
Î²v/Ïƒ6
v 
2 +R2
(R+âˆ¥Ï‰uâˆ¥)Î³k.
â– 
Forâˆ†(b,c)bu(Ï‰), we have the following lemma.
Lemma D.2. Fors, uâˆˆ[0, T)such that uâ‰¥s, uâˆˆ[sk, tk+1)andÏ‰= (Ï‰v)vâˆˆ[s,T]we have
âˆ¥âˆ†(b,c)bu(Ï‰)âˆ¥ â‰¤ 
1 +Î·2 
Î²Tâˆ’u/Ïƒ4
Tâˆ’u 
1 +R2
âˆ¥Ï‰uâˆ’Ï‰skâˆ¥.
Proof .Without loss of generality, we assume sâ‰¤tk. In this case sk=tk, Then
âˆ¥âˆ†(b,c)bu(Ï‰)âˆ¥ â‰¤ 
1 +Î·2
Î²Tâˆ’uâˆ¥âˆ‡logqTâˆ’tk(Ï‰tk)âˆ’ âˆ‡logqTâˆ’tk(Ï‰u)âˆ¥
â‰¤ 
1 +Î·2
Î²Tâˆ’usup
vâˆˆ[u,Tâˆ’tk]âˆ¥âˆ‡2logqTâˆ’tk(Ï‰v)âˆ¥âˆ¥Ï‰uâˆ’Ï‰tkâˆ¥.
Using Lemma E.2, we have that
âˆ¥âˆ†(b,c)bu(Ï‰)âˆ¥ â‰¤ 
1 +Î·2 
Î²Tâˆ’u/Ïƒ4
Tâˆ’u 
1 +R2
âˆ¥Ï‰uâˆ’Ï‰tkâˆ¥.
Then the proof is complete. â– 
We need to control the reverse process when dealing with âˆ†b. The following lemma shows an upper
bound for the reverse Yk.
Lemma D.3. Assume Assumption 3.1 ,Assumption 4.1, and there exists Î´ >0such thatÎ³kÎ²Tâˆ’tk
Ïƒ2
Tâˆ’tkâ‰¤
Î´â‰¤1/28for any kâˆˆ {0,Â·Â·Â·, K}, then we have
E[âˆ¥Ykâˆ¥2]â‰¤U(Ï„) =Ï„d+B(1/A+Î´),
22where
A= 4Î·2+ 2âˆ’2Î´âˆ’4(1 + Î·2)(1 + Î´)ÂµR
B= 4(1 + Î·2)R2Î´+ 2(1 + Î·2)(1 + Î´)R
Âµ+ 4Î·2Ï„d
andÂµis an arbitrary positive number which makes A >0. In particular, if Î´â‰¤1/28, then
E[âˆ¥Ykâˆ¥2]â‰¤U0(Ï„) = 111 R2+ 13Ï„d .
Proof .Recall the discretization of the backward process (the explicit form of Equation (6))
Yk+1=Yk+Î³1,k1
Ï„Yk+ (1 + Î·2)s(Tâˆ’tk, Yk)
+Î·p
2Î³2,kZk,
Î³1,k= exp"ZTâˆ’tk
Tâˆ’tk+1Î²sds#
âˆ’1, Î³ 2,k= 
exp"
2ZTâˆ’tk
Tâˆ’tk+1Î²sds#
âˆ’1!
/2,
where {Zk}kâˆˆKare independent Gaussian random variables. It is clear that Î³1,kâ‰¤Î³2,kâ‰¤2Î³1,k,
and using Lemma E.1 we have
âŸ¨xt,s(t, xt)âŸ©=âŸ¨xt,âˆ‡logqt(xt)âŸ©
â‰¤ âˆ’âˆ¥ xtâˆ¥2/Ïƒ2
t+mtRâˆ¥xtâˆ¥/Ïƒ2
t
â‰¤(âˆ’1 +ÂµmtR)âˆ¥xtâˆ¥2/Ïƒ2
t+ (mtR/Âµ)/Ïƒ2
t,
where the first equality follows that we assume the accurate score function. For any Âµ >0. Again
using Lemma E.1, we have
âˆ¥s(t, xt)âˆ¥2=âˆ¥âˆ‡logqt(xt)âˆ¥2
â‰¤2âˆ¥xtâˆ¥2/Ïƒ4
t+ 2m2
tR2/Ïƒ4
t.
Combining the results above, we have
E[âˆ¥Yk+1âˆ¥2] = (1 +Î³1,k
Ï„)2E[âˆ¥Ykâˆ¥2] + (1 + Î·2)2Î³2
1,kE[âˆ¥s(Tâˆ’tk, Yk)âˆ¥2]
+ 2(1 + Î·2)(1 +Î³1,k
Ï„)Î³1,kE[âŸ¨Yk, s(Tâˆ’tk, Yk)âŸ©] + 2Î·2Î³2,kd
â‰¤((1 +Î³1,k
Ï„)2+ 2(1 + Î·2)2Î³2
1,k/Ïƒ4
Tâˆ’tk
+ 2(1 + Î·2)(1 +Î³1,k
Ï„)Î³1,k(âˆ’1 +ÂµmTâˆ’tkR)/Ïƒ2
Tâˆ’tk)E[âˆ¥Ykâˆ¥2]
+2m2
Tâˆ’tkR2
Ïƒ4
Tâˆ’tk(1 +Î·2)2Î³2
1,k+mTâˆ’tkR
ÂµÏƒ2
Tâˆ’tk(1 +Î·2)(1 +Î³1,k
Ï„)Î³1,k+ 4Î·2Î³1,kd .
If we denote Î´k=Î³1,k/Ïƒ2
Tâˆ’tkand notice the fact that mtâˆˆ[0,1], Ïƒ2
tâˆˆ[0, Ï„], Î·âˆˆ[0,1], then we
have
E[âˆ¥Yk+1âˆ¥2]â‰¤(1 + 2 Î´k+Î´2
k)E[âˆ¥Ykâˆ¥2] + 8Î´2
kE[âˆ¥Ykâˆ¥2]
+ 2(1 + Î´k)Î´k(âˆ’1 +ÂµR)E[âˆ¥Ykâˆ¥2] + 8R2Î´2
k+2R
ÂµÎ´k(1 +Î´k) + 4Ï„Î´kd .
We also have that
Î³1,k= exp[ZTâˆ’tk
Tâˆ’tk+1Î²sds]âˆ’1â‰¤exp[Î²Tâˆ’tkÎ³k]âˆ’1â‰¤2Î²Tâˆ’tkÎ³k,
where the last inequality follows that Î³k= exp ( âˆ’T),Î²Tâˆ’tkÎ³kâ‰¤1/2for small enough stepsize,
andeÏ‰âˆ’1â‰¤2Ï‰for any Ï‰âˆˆ[0,1/2]. We get Î´kâ‰¤2Î³kÎ²Tâˆ’tk/Ïƒ2
Tâˆ’tkâ‰¤2Î´. Thus
E[âˆ¥Yk+1âˆ¥2]â‰¤(1 + 2 Î´k+ 2Î´kÎ´)E[âˆ¥Ykâˆ¥2] + 16 Î´kÎ´E[âˆ¥Ykâˆ¥2]
+ 4(1 + Î´)(âˆ’1 +ÂµR)Î´kE[âˆ¥Ykâˆ¥2] + 16 R2Î´kÎ´+ 4(1 + Î´)R
ÂµÎ´k+ 4Ï„dÎ´k.
23Hence, we have
E[âˆ¥Yk+1âˆ¥2]â‰¤(1 +Î´k[âˆ’2 + 14 Î´+ 4(1 + Î´)ÂµR])E[âˆ¥Ykâˆ¥2]
+Î´k[16R2Î´+ 4(1 + Î´)R
Âµ+ 4Ï„d].
We denote A= 2âˆ’14Î´âˆ’4(1 + Î´)ÂµRandB= 16R2Î´+ 4(1 + Î´)R
Âµ+ 4Ï„d, then
E[âˆ¥Yk+1âˆ¥2]â‰¤(1âˆ’Î´kA)E[âˆ¥Ykâˆ¥2] +Î´kB .
Notice that E[âˆ¥Y0âˆ¥2] =dÏ„and if E[âˆ¥Ykâˆ¥2]â‰¥B/A it is decreasing, if E[âˆ¥Ykâˆ¥2]â‰¤B/A we have
E[âˆ¥Yk+1âˆ¥2]â‰¤B/A +Î´B. so
E[âˆ¥Ykâˆ¥2]â‰¤Ï„d+B(1/A+Î´).
Notice that when Î´â‰¤1/28,if we choose Âµ= 1/(4(1 + Î´)R),Aâ‰¥1/2, and
Bâ‰¤37R2+ 4Ï„d .
Then, the proof is complete. â– 
The following lemma shows a discretization error in the k-the interval.
Lemma D.4. Assume Assumption 3.1,Assumption 4.1 and Î³kÎ²Tâˆ’tk/Ïƒ2
Tâˆ’tkâ‰¤1/28for any kâˆˆ
{0,Â·Â·Â·, Kâˆ’1}. Then for any k,tâˆˆ[tk, tk+1]andiâˆˆ {1,2}, we have that
E[âˆ¥Â¯Ytâˆ’Â¯Ytkâˆ¥2]â‰¤Li(Ï„)Î²Tâˆ’tkÎ³k,
where Li(Ï„) = Â¯ Î³KÎºi(Ï„)(64
Ïƒ2
Tâˆ’tK(i)+8
Ï„)U0(Ï„) + 64 R2Â¯Î³KÎºi(Ï„)
Ïƒ2
Tâˆ’tK(i)+ 4d,Â¯Î³K,Îºi(Ï„)is defined in
Lemma D.5 and U0(Ï„)is defined in Lemma D.3.
Proof .Recall the discretized backward process
Â¯Yt=Â¯Ytk+ (exp[ZTâˆ’tk
Tâˆ’tÎ²sds]âˆ’1)(1
Ï„Â¯Ytk+ (1 + Î·2)s(Tâˆ’tk,Â¯Ytk))
+Î·(exp[2ZTâˆ’tk
Tâˆ’tÎ²sdsâˆ’1])1/2Z ,
where Zis a standard Gaussian random variable. By directly calculating, we have that
E[âˆ¥Â¯Ytâˆ’Â¯Ytkâˆ¥2] = 2(exp[ZTâˆ’tk
Tâˆ’tÎ²sds]âˆ’1)2(1
Ï„2E[âˆ¥Â¯Ytkâˆ¥2] + (1 + Î·2)2E[âˆ¥s(Tâˆ’tk,Â¯Ytk)âˆ¥2])
+Î·2(exp[2ZTâˆ’tk
Tâˆ’tÎ²sds]âˆ’1)d .
By Lemma E.1 and accurate score function assumption,
âˆ¥s(Tâˆ’tk,Â¯Ytk)âˆ¥2â‰¤2âˆ¥Â¯Ytkâˆ¥2/Ïƒ4
Tâˆ’tk(i) + 2m2
Tâˆ’tkR2/Ïƒ4
Tâˆ’tk(i).
So we have that
E[âˆ¥Â¯Ytâˆ’Â¯Ytkâˆ¥2]â‰¤2(exp[ZTâˆ’tk
Tâˆ’tÎ²sds]âˆ’1)2((8
Ïƒ4
Tâˆ’tk(i)+1
Ï„2)E[âˆ¥Â¯Ytkâˆ¥2] +8R2
Ïƒ4
Tâˆ’tk(i))
+ (exp[2ZTâˆ’tk
Tâˆ’tÎ²sds]âˆ’1)d .
Bye2wâˆ’1â‰¤1 + 4 wfor any wâˆˆ[0,1/2]andÎ³ksupvâˆˆ[Tâˆ’tk+1,Tâˆ’tk]Î²v/Ïƒ2
vâ‰¤1/28for any
kâˆˆ {0, ..., K âˆ’1}, we have
exp[ÏZTâˆ’tk
Tâˆ’tÎ²sds]âˆ’1â‰¤2ÏÎ²Tâˆ’tkÎ³k.
24forÏ= 1,2. And using Lemma D.3 and Lemma F.2 we have
E[âˆ¥Â¯Ytâˆ’Â¯Ytkâˆ¥2]
â‰¤(64Î³k
Ïƒ4
Tâˆ’tk(i)+8Î²Tâˆ’tkÎ³k
Ï„2)U0(Ï„)Î²Tâˆ’tkÎ³k+ 64R2Î³k
Ïƒ4
Tâˆ’tk(i)Î²Tâˆ’tkÎ³k+ 4dÎ²Tâˆ’tkÎ³k.
We denote Li(Ï„) = Â¯Î³KÎºi(Ï„)(64
Ïƒ2
Tâˆ’tK(i)+8
Ï„)U0(Ï„) + 64 R2Â¯Î³KÎºi(Ï„)
Ïƒ2
Tâˆ’tK(i)+ 4dforiâˆˆ {1,2}and the
proof is complete. â– 
Lemma D.5. Assume Assumption 3.1 and Assumption 4.1, Î³ksupvâˆˆ[Tâˆ’tk+1,Tâˆ’tk]Î²v/Ïƒ2
vâ‰¤1/28
for any kâˆˆ {0, ..., K âˆ’1}. Let Â¯Î³K=argmaxkâˆˆ{0,...,Kâˆ’1}Î³k,Îºi(Ï„) = max {Â¯Î²,T2
Tâˆ’1+i}Ïƒâˆ’2
Tâˆ’tK(i),
and
Ci(Ï„) = 2(2 + R2)(R+U1/2
0(Ï„)) + 2 L1/2
i(Ï„)Ï„3/2(1 +R2),
foriâˆˆ {1,2}. Then, for any s, uâˆˆ[0, tK]withuâ‰¥sandiâˆˆ {1,2}, we have
E[âˆ¥âˆ†bu,i((Â¯Ys,v)vâˆˆ[s,T])âˆ¥]â‰¤Ci(Ï„)[Îº2
i(Ï„)Ïƒâˆ’2
Tâˆ’tK(i)Â¯Î³1/2
K+Îº2
i(Ï„)]Â¯Î³1/2
K,
where Â¯Ys,sâˆ¼N(0,I).
Proof .Combining Lemma D.1, Lemma D.2 and the exact score function, we get
âˆ¥âˆ†bu,i(Ï‰)âˆ¥ â‰¤(1 +Î·2) sup
vâˆˆ[Tâˆ’tk+1,Tâˆ’tk](Î²2
v/Ïƒ6
v(i))(2 + R2)(diam( M+âˆ¥Ï‰uâˆ¥))Î³k
+ (1 + Î·2)(Î²Tâˆ’u/Ïƒ4
Tâˆ’u(i))(1 + diam( M2))âˆ¥Ï‰uâˆ’Ï‰skâˆ¥.
For any uâˆˆ[Tâˆ’tK, T], using Lemma F.3 we have Î²u/Ïƒ2
u(i)â‰¤Îºi(Ï„). Hence,
âˆ¥âˆ†bu,i(Ï‰)âˆ¥ â‰¤(1 +Î·2) sup
vâˆˆ[Tâˆ’tk+1,Tâˆ’tk](Î²2
v/Ïƒ6
v(i))(2 + diam( M2))(R+âˆ¥Ï‰uâˆ¥)Î³k
+ (1 + Î·2)(Î²Tâˆ’u/Ïƒ4
Tâˆ’u(i))(1 + diam( M2))(âˆ¥Ï‰uâˆ’Ï‰tkâˆ¥)
â‰¤(1 +Î·2)(Îº2
i(Ï„)/Ïƒ2
Tâˆ’tk+1(i))Î³k(2 + diam( M2))(R+âˆ¥Ï‰uâˆ¥)
+ (1 + Î·2)Îº2
i(Ï„)(1 + R2)âˆ¥Ï‰uâˆ’Ï‰tkâˆ¥/Î²Tâˆ’u.
Combining this with Lemma D.3 and Lemma D.4,
E[âˆ¥âˆ†bu,i((Â¯Ys,v)vâˆˆ[s,T])âˆ¥]â‰¤(1 +Î·2)(Îº2
i(Ï„)/Ïƒ2
Tâˆ’tk+1(i))Â¯Î³K(2 +R2)(R+U1/2
0(Ï„))
+ (1 + Î·2)Îº2
i(Ï„)(1 + R2)L1/2
i(Ï„) max{Â¯Î², Ï„}3/2Â¯Î³1/2
K.
We denote Ci(Ï„) = 2(2 + R2)(R+U1/2
0(Ï„)) + 2 L1/2
i(Ï„)Ï„3/2(1 +R2), foriâˆˆ {1,2}, then we have
E[âˆ¥âˆ†bu,i((Â¯Ys,v)vâˆˆ[s,T])âˆ¥]â‰¤Ci(Ï„)((Îº2
i(Ï„)/Ïƒ2
Tâˆ’tk+1)Â¯Î³K+Îº2
i(Ï„)Â¯Î³1/2
K).
â– 
Lemma D.6. Assume Assumption 3.1 and Assumption 4.1, Î³ksupvâˆˆ[Tâˆ’tk+1,Tâˆ’tk]Î²v/Ïƒ2
vâ‰¤1/28
for any kâˆˆ {0, ..., K âˆ’1}. LetÂ¯Î³K=argmaxkâˆˆ{0,...,Kâˆ’1}Î³k,Î³K=Î´, and Î´â‰¤1/32. Then
W1
RqÏ„
âˆ
K, QqÏ„
âˆ
tK
â‰¤Ci(Ï„)Îº2
i(Ï„)TexpR2
2Ïƒ2
Tâˆ’tK(i)+(1âˆ’Î·2)
2
[Â¯Î³1/2
K
Ïƒ2
Tâˆ’tK(i)+ 1]Â¯Î³1/2
K,
where Ci(Ï„), Îºi(Ï„)foriâˆˆ {1,2}are the same terms to Theorem 6.1.
25Proof .ByProposition 1 we have
âˆ¥YtKâˆ’YKâˆ¥=âˆ¥YtKâˆ’Â¯YtKâˆ¥ â‰¤ZtK
0âˆ¥âˆ‡Yu,tK,i(Â¯Y0,u)âˆ¥âˆ¥âˆ†bu,i((Â¯Y0,v)vâˆˆ[0,T])âˆ¥du.
âˆ¥YtKâˆ’YKâˆ¥
â‰¤exp" 
1 +Î·2
R2
4Ïƒ2
Tâˆ’t(i)+(1âˆ’Î·2)
2ZtK
0Î²Tâˆ’u
Ï„du#ZtK
0âˆ¥âˆ†bu,i((Â¯Y0,v)vâˆˆ[0,T])âˆ¥du .
Then by definition of Wasserstein distance, we have
W1(qâˆQtK, qâˆRK)
â‰¤E[âˆ¥YtKâˆ’YKâˆ¥]
â‰¤exp" 
1 +Î·2
R2
4Ïƒ2
Tâˆ’tK(i)+(1âˆ’Î·2)
2ZtK
0Î²Tâˆ’u
Ï„du#ZtK
0E[âˆ¥âˆ†bu,i((Â¯Y0,v)vâˆˆ[0,T]âˆ¥]du
â‰¤Ci(Ï„)Texp" 
1 +Î·2
R2
4Ïƒ2
Tâˆ’tK(i)+(1âˆ’Î·2)
2#
[Îº2
i(Ï„)Ïƒâˆ’2
Tâˆ’tK(i)Â¯Î³1/2
K+Îº2
i(Ï„)]Â¯Î³1/2
K.
â– 
Theorem 6.1. Assume Assumption 3.1 and 4.1, Î´â‰¤1/32andÎ³ksupvâˆˆ[Tâˆ’tk+1,Tâˆ’tk]Î²v/Ïƒ2
vâ‰¤1/28
forâˆ€kâˆˆ {0, ..., K âˆ’1}. LetÎ³K=Î´. Then, for âˆ€Ï„âˆˆ[T, T2]:
(1) If Î·= 1(the reverse SDE), choosing Î²t=t2,W1
RqÏ„
âˆ
K, q0
is bounded by
(R
Ï„+âˆš
d)âˆš
Î´+ expR2
2(Â¯Î²
Î´3+1
Ï„) 
C1(Ï„)TÎº2
1(Ï„)
(Â¯Î²
Î´3+1
Ï„)Â¯Î³1/2
K+ 1
Â¯Î³1/2
K+Â¯Deâˆ’T/2
âˆšÏ„!
,
where Îº1(Ï„) =T2(1/Ï„+Â¯Î²/Î´3)andC1(Ï„)is linear in Ï„2.
(2) If Î·= 0(PFODE), choosing a conservative Î²t(Assumption 3.1), W1
RqÏ„
âˆ
K, q0
is bounded by
(R
Ï„+âˆš
d)âˆš
Î´+ expR2
2(Â¯Î²
Î´2+1
Ï„) 
C2(Ï„)Îº2
2(Ï„)T
(Â¯Î²
Î´2+1
Ï„)Â¯Î³1/2
K+ 1
Â¯Î³1/2
K+Â¯DâˆšÏ„!
,
where Îº2(Ï„) =T 
1/Ï„+Â¯Î²/Î´2
andC2(Ï„)is linear in Ï„2.
Proof .To obtain the convergence guarantee, we need to control three error terms:
W1
RqÏ„
âˆ
K, q0
â‰¤W1
RqÏ„
âˆ
K, QqÏ„
âˆ
tK
+W1
QqÏ„
âˆ
tK, Qq0PT
tK
+W1
Qq0PT
tK, q0
.
For term W1
RqÏ„
âˆ
K, QqÏ„
âˆ
tK
, we use Lemma D.6.
For the second term, we define 
Yx
0,t
tâˆˆ[0,T]and 
Yy
0,t
tâˆˆ[0,T]be the reverse processes with initial
condition xandy. Then we have
âˆ¥Yx
0,tâˆ’Yy
0,tâˆ¥ â‰¤ âˆ¥ xâˆ’yâˆ¥Z1
0âˆ¥âˆ‡YzÎ»
0,tâˆ¥dÎ» ,
where zÎ»=Î»x+ (1âˆ’Î»)y. In this work, we choose xâˆ¼qÏ„
âˆandyâˆ¼q0PT. Combined with the
above inequality, Theorem 4.2 and Lemma 6.3, we know that:
W1
QqÏ„
âˆ
tK, Qq0PT
tK
â‰¤expR2
2Ïƒ2
Tâˆ’tK(i)+(1âˆ’Î·2)
2ZtK
0Î²Tâˆ’u
Ï„du
âˆ¥q0PTâˆ’qÏ„
âˆâˆ¥
â‰¤âˆšmTÂ¯D
ÏƒTexpR2
2Ïƒ2
Tâˆ’tK(i)+(1âˆ’Î·2)
2ZtK
0Î²Tâˆ’u
Ï„du
.
26For the last term, we use exactly the same process with Bortoli [2022] with bounded Ïƒ2
Tâˆ’tK:
W1
Qq0PT
tK, q0
â‰¤E[âˆ¥Xâˆ’mTâˆ’tKX+ÏƒTâˆ’tKZâˆ¥]
â‰¤(R
Ï„+âˆš
d)ÏƒTâˆ’tK
â‰¤2(R
Ï„+âˆš
d)âˆš
Î´ ,
where the second inequality follows that Ïƒ2
Tâˆ’tK+Ï„mTâˆ’tK=Ï„. â– 
In the end of the section, we provide the proof of Corollary 6.2.
Corollary D.7. Assume Assumption 3.1, 4.1 andâˆ‡2logqt(xt)â‰¤Î“/Ïƒ2
t. Let Î·= 0 (reverse
PFODE), Î´âˆˆ(0,1/32), Ï„=T2,Î²t=tandÎº2(Ï„), C2(Ï„)defined in Theorem 6.1, we have
W1
RqÏ„
âˆ
K, q0
â‰¤(R
Ï„+âˆš
d)âˆš
Î´+Â¯Î²Î“
2
Î´Î“expÎ“ + 2
2 
C2(Ï„)Îº2
2(Ï„)T((Â¯Î²
Î´2+1
Ï„)Â¯Î³1/2
K+ 1)Â¯Î³1/2
K+Â¯DâˆšÏ„!
.
Proof .The proof of this corollary is almost identical to the proof of Theorem 6.1. We just need to
replace the first bound for the tangent process in Lemma 6.3 by the second bound. â– 
E Lemmas for the Logarithmic Density
In this section, we introduce auxiliary lemmas to control the gradient and Hessian of the logarithmic
density under the manifold hypothesis. Lemma E.1, Lemma E.2 and Lemma E.3 come from
Lemma C.1, Lemma C.2, and Lemma C.5 of Bortoli [2022]. Since these lemmas do not involve the
relationship between mtandÏƒt, we can directly use the results from Bortoli [2022]. Following Bortoli
[2022], we also define a empirical version of q0withNdatapoints, i.e. qN
0= (1/N)PN
k=1Xk, with
Xk	N
k=1âˆ¼qâŠ—N
0. We denote by 
qN
t
t>0such that for any t >0the density w.r.t. the Lebesgue
measure of the distribution of XN
t, and when Nâ†’+âˆ,qN
t=qt.
Lemma E.1. Assume Assumption 4.1. Then for any tâˆˆ(0, T]andxtâˆˆRdwe have that
âŸ¨âˆ‡logqt(xt), xtâŸ© â‰¤ âˆ’âˆ¥ xtâˆ¥2/Ïƒ2
t+mRâˆ¥xtâˆ¥/Ïƒ2
t.
In addition, we have
âˆ¥âˆ‡logqt(xt)âˆ¥2â‰¤2âˆ¥xtâˆ¥2/Ïƒ4
t+ 2m2
tR2/Ïƒ4
t.
Lemma E.2. Assume Assumption 4.1. Then for any tâˆˆ(0, T],xtâˆˆRdandMâˆˆ M d 
Rd

M,âˆ‡2logqt(xt)M
â‰¤ âˆ’ 
1âˆ’m2
tR2/ 
2Ïƒ2
t
/Ïƒ2
tâˆ¥Mâˆ¥2.
In addition, we have
âˆ‡2logqt(xt)â‰¤ 
1 +R2
/Ïƒ4
t.
The following lemma shows that the derivatives up to the fourth order are uniformly bounded since
Ï„âˆˆ[T, T2]. Thus we can use the stochastic extension of the Alekseevâ€“GrÃ¶bner formula [Del Moral
and Singh, 2022].
Lemma E.3. Assume Assumption 4.1. Then, there exists Â¯Câ‰¥0such that for any tâˆˆ(0, T]we have
âˆ‡2logqt(x)+âˆ‡3logqt(x)+âˆ‡4logqt(x)â‰¤Â¯C/Ïƒ8
t.
The following lemma shows that âˆ¥âˆ‚tâˆ‡logqt(xt)âˆ¥is bounded. The proof before using the relation-
ship between Ïƒtandmtis identical compared to Lemma C.3 in Bortoli [2022]. For the sake of
completeness, we also give the proof process of this part.
Lemma E.4. Assume Assumption 4.1. Then for any tâˆˆ(0, T]andxtâˆˆRdwe have
âˆ¥âˆ‚tâˆ‡logqt(xt)âˆ¥ â‰¤ 
Î²t/Ïƒ6
t 
2 +R2
(R+âˆ¥xtâˆ¥).
27Proof .LetNâˆˆNandtâˆˆ(0, T]. We denote for any xâˆˆRd,qN
t(x) = Â¯qN
t(x)/ 
2Ï€Ïƒ2
td/2with
Â¯qN
t(x) = (1 /N)NX
k=1ek
t(x), ek
t(x) = exp
âˆ’âˆ¥xâˆ’mtXkâˆ¥2/ 
2Ïƒ2
t
.
Next we denote fk
tâ‰œlogek
t. Then we have
âˆ‚tlog Â¯qN
t(x)NX
k=1âˆ‚tfk
t(x)ek
t(x)/NX
k=1ek
t(x).
Therefore we have
âˆ‚tâˆ‡log Â¯qN
t(x)
=NX
k=1âˆ‚tâˆ‡fk
t(x)ek
t(x)/NX
k=1ek
t(x) +NX
k=1âˆ‚tfk
t(x)âˆ‡fk
t(x)ek
t(x)/NX
k=1ek
t(x)
âˆ’NX
k,j=1âˆ‚tfk
t(x)âˆ‡fj
t(x)ek
t(x)ej
t(x)/NX
k,j=1ek
t(x)ej
t(x)
=NX
k=1âˆ‚tâˆ‡fk
t(x)ek
t(x)/NX
k=1ek
t(x)
+ (1/2)NX
k,j=1
âˆ‚tfk
t(x)âˆ’âˆ‚tfj
t(x)
âˆ‡fk
t(x)âˆ’ âˆ‡fj
t(x)
ek
t(x)ej
t(x)/NX
k,j=1ek
t(x)ej
t(x).
In what follows, we provide upper bounds for |âˆ‚tfk
tâˆ’âˆ‚tfj
t|,âˆ¥âˆ‡fk
tâˆ’ âˆ‡fj
tâˆ¥andâˆ‚tâˆ‡fk
t. First we
notice that âˆ‡fk
t(x) =âˆ’ 
xâˆ’mtXk
/Ïƒ2
t, and using mtâ‰¤1we get
âˆ¥âˆ‡fk
t(x)âˆ’ âˆ‡fj
t(x)âˆ¥ â‰¤mR/Ïƒ2
tâ‰¤R/Ïƒ2
t.
and
âˆ‚tfk
t(t) =âˆ‚tÏƒ2
t/ 
2Ïƒ4
t
âˆ¥xâˆ’mtXkâˆ¥2+âˆ‚tmt/Ïƒ2
t
Xk, xâˆ’mtXk
.
Notice the fact that âˆ‚tÏƒ2
t=âˆ’2Ï„mtâˆ‚tmt= 2Î²tm2
tandâˆ‚tmt=âˆ’Î²t
Ï„mt, combined with the above
equality, we know that
âˆ‚tfk
t(t) =âˆ’Î²tmt/Ïƒ2
t
âˆ’ 
mt/Ïƒ2
t
âˆ¥xâˆ’mtXkâˆ¥2+1
Ï„
xâˆ’mtXk, Xk
=âˆ’Î²tmt/Ïƒ2
t
xâˆ’mtXk,âˆ’ 
mt/Ïƒ2
t 
xâˆ’mtXk
+1
Ï„Xk
=âˆ’Î²tmt/Ïƒ4
t
xâˆ’mtXk,âˆ’mtx+
m2
t+Ïƒ2
t
Ï„
Xk
=Î²tmt/Ïƒ4
t
mtâˆ¥xâˆ¥2+mtXk2+ 
1 +m2
t
x, Xk
,
where the last equality holds that Ï„m2
t+Ïƒ2
t=Ï„. The rest of the proof is identical to the Lemma C.3
in Bortoli [2022].
So using mtâ‰¤1we have
âˆ‚tfk
t(x)âˆ’âˆ‚tfj
t(x)â‰¤2Î²tm2
tR2/Ïƒ4
t+Î²tmt 
1 +m2
t
Râˆ¥xâˆ¥/Ïƒ4
t
â‰¤2 
Î²t/Ïƒ4
t
R(R+âˆ¥xâˆ¥)
Now we compute âˆ‡âˆ‚tfk
t(x)for any xâˆˆRd
âˆ‡âˆ‚tfk
t(x) = 2 Î²tm2
t/Ïƒ4
tx+ 
Î²tmt/Ïƒ4
t 
1 +m2
t
Xk.
28So we can bound the norm of it by
âˆ¥âˆ‚tâˆ‡fk
t(x)âˆ¥ â‰¤2 
Î²t/Ïƒ4
t
(R+âˆ¥xâˆ¥).
Combining results above we get for any xâˆˆRd
âˆ‚tâˆ‡log Â¯qN
t(x)â‰¤2 
Î²t/Ïƒ4
t
(R+âˆ¥xâˆ¥) + 
Î²t/Ïƒ6
t
R2(R+âˆ¥xâˆ¥)
â‰¤ 
Î²t/Ïƒ6
t 
2 +R2
(R+âˆ¥xâˆ¥)
Note that
lim
Nâ†’+âˆâˆ‚tâˆ‡logqN
t(xt) =âˆ‚tâˆ‡logqt
and the proof is complete. â– 
In the following lemma, similar to Chen et al. [2023c], we obtain a better control on the time
discretization error instead of controlling âˆ¥âˆ‚tâˆ‡logqt(xt)âˆ¥forâˆ€xtâˆˆRd.
Lemma E.5. Assume Assumption 4.1 and Xtsatisfies the forward process Equation (3). Define
L= max tâˆˆ[0,Tâˆ’Î´]âˆ‡2logqTâˆ’t(Yt)â‰¤ 
1 +R2
/Ïƒ4
Î´, then we have that
EQqÏ„
T
tK"âˆ‡lnqTâˆ’tk
qTâˆ’t(Ytk)2#
â‰²Ï„L2dÂ¯Î³K+Ï„L2Â¯Î³2
K(dÏ„+R2) +Ï„L3Â¯Î³2
K+Ï„L4Â¯Î³2
K(Î²TdÂ¯Î³K+R2Â¯Î³2
K).
Proof .Due to the property of the forward process, we know that if S:Rdâ†’Rdis the mapping
S(x) := exp( âˆ’(tâˆ’tk))x, then qTâˆ’tk=S#qTâˆ’tâˆ—normal
0, Ï„
1âˆ’exp(âˆ’2Rtk+1
tkÎ²s/Ï„ds)
Similar to Chen et al. [2023c], we define Î±= exphRtk+1
tkÎ²s
Ï„dsi
= 1 + O(Â¯Î³K)andÏƒ2=
Ï„
1âˆ’exp(âˆ’2Rtk+1
tkÎ²s/Ï„ds)
=O(Ï„Â¯Î³K). Then we can use Lemma C.12 of Lee et al. [2022] to
obtain
EQqÏ„
T
tK"âˆ‡lnqTâˆ’tk
qTâˆ’t(Ytk)2#
â‰²Ï„L2dÂ¯Î³K+Ï„L2Â¯Î³2
Kâˆ¥Ytkâˆ¥2+Ï„L2Â¯Î³2
Kâˆ¥âˆ‡lnqTâˆ’t(Ytk)âˆ¥2
â‰²Ï„L2dÂ¯Î³K+Ï„L2Â¯Î³2
K(dÏ„+R2) +Ï„L3Â¯Î³2
K+Ï„L4Â¯Î³2
K(Î²TdÂ¯Î³K+R2Â¯Î³2
K).
The last inequality follows Lemma F.4 and the fact that
âˆ¥âˆ‡lnqTâˆ’t(Ytk)âˆ¥2â‰²âˆ¥âˆ‡lnqTâˆ’t(Yt)âˆ¥2+âˆ¥âˆ‡lnqTâˆ’t(Ytk)âˆ’ âˆ‡lnqTâˆ’t(Yt)âˆ¥2
â‰²âˆ¥âˆ‡lnqTâˆ’t(Ytk)âˆ¥2+L2(Î²TdÂ¯Î³K+R2Â¯Î³2
K)
â‰²L+L2(Î²TdÂ¯Î³K+R2Â¯Î³2
K).
â– 
F Auxiliary Lemmas
Lemma F.1. For any s, tâˆˆ[0, T]we have
Zt
sÎ²Tâˆ’u/Ïƒ2
Tâˆ’udu="
âˆ’1
2log 
exp"
2ZTâˆ’u
0Î²v
Ï„dv#
âˆ’1!#t
s,
Zt
sÎ²Tâˆ’um2
Tâˆ’u/Ïƒ4
Tâˆ’udu="
(1/2Ï„)/ 
1âˆ’exp"
âˆ’2ZTâˆ’u
0Î²v
Ï„dv#!# t
s.
29Proof .We directly compute
Zt
sÎ²Tâˆ’u/Ïƒ2
Tâˆ’udu=1
Ï„Zt
sÎ²Tâˆ’u/ 
1âˆ’exp"
âˆ’2ZTâˆ’u
0Î²v
Ï„dv#!
du
=1
Ï„Zt
sÎ²Tâˆ’uexp"
2ZTâˆ’u
0Î²v
Ï„dv#
/ 
exp"
2ZTu
0Î²v
Ï„dv#
âˆ’1!
du
=âˆ’1
2Zt
sâˆ‚ulog 
exp"
2ZTâˆ’u
0Î²v
Ï„dv#
âˆ’1!
du .
Similarly
Zt
sÎ²Tâˆ’um2
Tâˆ’u/Ïƒ4
Tâˆ’u
=1
Ï„2Zt
sÎ²Tâˆ’uexp"
âˆ’2ZTâˆ’u
0Î²v
Ï„dv#
/ 
1âˆ’exp"
âˆ’2ZTâˆ’u
0Î²v
Ï„dv#!2
du
= (1/2Ï„)Zs
tâˆ‚u 
1âˆ’exp"
âˆ’2ZTâˆ’u
0Î²v
Ï„dv#!âˆ’1
du.
â– 
Lemma F.2. Assume Assumption 3.1. For iâˆˆ {1,2}, we have Ïƒ2
Tâˆ’tK(i)â‰¤2Î´andÏƒâˆ’2
u(i)â‰¤
Ïƒâˆ’2
Tâˆ’tK(i)â‰¤1
Ï„+Â¯Î²
Î´4âˆ’i,âˆ€uâˆˆ[Tâˆ’tK, T].
Proof .
Ïƒ2
Tâˆ’tK(i) =Ï„ 
1âˆ’exp"
âˆ’2ZTâˆ’tK
0Î²s
Ï„ds#!
â‰¤2ZTâˆ’tK
0Î²sdsâ‰¤2Î´ ,
where the first inequality follows from for any aâ‰¥0,exp[âˆ’a]â‰¥1âˆ’a; the second inequlity follows
from Assumption 3.1 and Î´â‰¤1.
Ïƒâˆ’2
Tâˆ’tK(i) =1
Ï„ 
1âˆ’exp"
âˆ’2ZTâˆ’tK
0Î²s
Ï„ds#!âˆ’1
â‰¤1
Ï„ï£«
ï£­1 + 
2ZTâˆ’tK
0Î²s
Ï„ds!âˆ’1ï£¶
ï£¸
â‰¤1
Ï„+Â¯Î²
Î´4âˆ’i,
where the first inequality follows from for any aâ‰¥0,1/(1+exp[ âˆ’a])â‰¤1+1/a, the second inequal-
ity follows from Assumption 3.1. It is easy to check that Ïƒâˆ’2
u(i)â‰¤Ïƒâˆ’2
Tâˆ’tK(i),âˆ€uâˆˆ[Tâˆ’tK, T].
â– 
Using the bound on Ïƒâˆ’2
Tâˆ’tK(i)immediately yields the following control of Î²u/Ïƒ2
u(i).
Lemma F.3. Assume Assumption 3.1. Then, we have for any uâˆˆ[Tâˆ’tK, T]: (1) if i= 1, then
Î²u
Ïƒ2u(i= 1)â‰¤Îº1(Ï„) = max {Â¯Î², T2}1
Ï„+Â¯Î²
Î´3
;
(2) if i= 2, then
Î²u
Ïƒ2u(i= 2)â‰¤Îº2(Ï„) = max {Â¯Î², T}1
Ï„+Â¯Î²
Î´2
.
30Generally speaking, Tâ‰¥Â¯Î²â‰¥1. Hence, We can further simplify the above inequality by removing
max.
In the rest of this section, we provide the useful lemma to achieve polynomial sample complexity for
VE-based models with reverse SDE. As shown in Lemma E.1, we also need to control E[âˆ¥Xtâˆ¥2]in
the forward process. The following lemmas shows that this term is bounded by the R2and exploding
variance.
Lemma F.4. Suppose that Assumption 4.1hold. Let (Xt)tâˆˆ[0,T]denote the forward process Equa-
tion(3). Then, for all tâ‰¥0,
Eh
âˆ¥Xtâˆ¥2i
â‰¤dÏƒ2
tâˆ¨R2.
Proof .As shown in Equation (4),
Eh
âˆ¥Xtâˆ¥2i
â‰¤E
âˆ¥X0âˆ¥2
+Ïƒ2
tdâ‰¤dÏƒ2
tâˆ¨R2.
â– 
Lemma F.5 (movement bound for VESDE) .Let(Xt)tâˆˆ[0,T]denote the forward process Equation (3).
For0â‰¤s < t withÎ´:=tâˆ’s, ifÎ´â‰¤1, then
Eh
âˆ¥Xtâˆ’Xsâˆ¥2i
â‰²2Î²tÎ´d+Î´2R2.
Proof .
Eh
âˆ¥Xtâˆ’Xsâˆ¥2i
â‰²Ep
2Î²t(Btâˆ’Bs)2
+Î´Zt
sEh
âˆ¥Xrâˆ¥2i
drâ‰²2Î²tÎ´d+Î´2R2.
â– 
Similar to Chen et al. [2023c], we can also show that if we do forward process for time Î´,qÎ´will be
close to q0inW2distance.
Lemma F.6. Suppose Assumption 4.1 holds. Let ÏµW2>0. IfÎ²2
t=t2andÏ„=T2, we choose
the early stopping parameter Î´â‰¤Ïµ2/3
W2
(d+Râˆš
d)1/3. IfÎ²t=tandÏ„=T, we choose Î´â‰¤ÏµW2
(d+Râˆš
d)1/2.
If consider pure VESDE (SMLD) (Equation (2)) with Ïƒ2
t=t, we choose Î´â‰¤Ïµ2
W2
d. Then we have
W2(qÎ´, q0)â‰¤ÏµW2.
Proof .For the forward process Equation (3), we know that Xt:=mtX0+ÏƒtZ, where Zâˆ¼
normal (0 , Id)is independent of X0andmtâ‰¤1. Hence, for Î´â‰²1,
W2
2(q0, qÎ´)â‰¤(1âˆ’mt)2Eh
âˆ¥X0âˆ¥2i
+Eh
âˆ¥ÏƒÎ´Zâˆ¥2i
.
ForÎ²t=t2andÏ„=T2, we have that
W2
2(q0, qÎ´)â‰¤Î´3d+R2Î´6
T2
Hence, we can take Î´â‰¤Ïµ2/3
W2
(d+Râˆš
d)1/3. For Î²t=tandÏ„=T, we have that
W2
2(q0, qÎ´)â‰¤Î´2d+R2Î´4
T2
Hence, we can take Î´â‰¤ÏµW2
(d+Râˆš
d)1/2. For pure VESDE (Equation (2)) with Ïƒt=t, we have
W2
2(q0, qÎ´)â‰¤Î´d .
â– 
31G Additional Synthetic Experiments
In this section, we do synthetic experiments to show the power of our new forward process with small
drift term in different setting.
G.1 The Synthetic experiments with accurate score function
In this section, we do numerical experiments on 2-dimension Gaussian distribution to show the power
of our new VESDE forward process in balancing different error sources.
Experiment Setting. We set the mean of target distribution E[q0] = [6 ,8], the covariance matrix
Cov[q0] =
25 5
5 4
, the diffusion time T= 2,Ï„=T2and the reverse beginning distribution is
N(0, T2I). We choose uniform stepsize Î³k=h,âˆ€kâˆˆ[K]where hâˆˆ {0.005,0.01,0.02,0.04}. For
score functions, we directly calculate the ground truth score function instead of learning it by the
score matching objective. We calculate the KL divergence between the generation distribution and
target distribution q0as the experiments.
The implementable algorithm. We choose three different VESDE forward processes in the
experiments: (1) aggressive Î²t=t2withÏ„=T2; (2) conservative Î²t=twithÏ„=T2and (3)
VESDE without drift term Equation (2) with Ïƒ2
t=t2. After determining the forward process, we
run the reverse SDE with the above Î³k, kâˆˆ[K]. For the discretization scheme, we choose two
common method: exponential integrator (EI) [Zhang and Chen, 2022] and Euler-Maruyama (EM)
discretization [Ho et al., 2020].
Observations. The experimental results are shown in Figure 2. We note that the red line (EI,
VESDE without drift, Ïƒ2
t=t2) and orange line (conservative drift VESDE, Î²t=tandÏ„=T2) has
a similar trend. Furthermore, the conservative drift VESDE has better performance compared to pure
VESDE without drift term. Hence, our new forward process is representative enough to represent
current VESDE, as discussed in Section 3.1.
The experimental results also support our theoretical results and show the power of the new forward
process in balancing different error terms. As shown in Figure 2, the process with aggressive Î²t=t2
with small drift term achieves the best and second performance in EI and EM discretization since it
can balance the reverse beginning and discretization. The third best process is conservative Î²t=t
with the small drift term. The reason is that though it can not achieve a exp (âˆ’T)forward process
guarantee, it also has a constant decay on prior information, as shown in Section 3.1. This decay
slightly reduces the effect of the reverse beginning error. The worse process is VESDE without drift
term since it is hard to balance different error sources. Our experimental results also show that EI
discretization is better than EM discretization.
G.2 The Synthetic experiments with approximated score function
In this section, instead of using an accurate score function, we train an approximated score function
on the pure VESDE (Equation (2)) without drift term on two synthetic datasets: multiple Swiss rolls
and 1-D GMM. Then, for the drift VESDE, we do not train the approximated score corresponding to
Equation (3); we directly use the approximated score learned by pure VESDE and show that the drift
VESDE can improve the generated distribution without the training process.
Datasets. The 1-D GMM distribution contains three modes:
3
10N(âˆ’8,0.01) +3
10N(âˆ’4,0.01) +4
10N(3,1).
For multiple Swiss rolls, we use a similar code compared to Listing 2 of Lai et al. [2023], except
Line 6. We change Line 6. to data /=10. to obtain a larger variance dataset. Each dataset contains
50000 datapoints.
The implementable algorithm. In this subsection, we choose two forward processes: (1) conserva-
tiveÎ²t= 1withÏ„=T; (2) pure VESDE without drift term (Equation (2)) with Ïƒ2
t=t. To match
32Table 1: The KL divergence for pure VESDE (Equation (2)) and conservative drift VESDE with
different sampling method.
Forward Process1-D GMM Swiss roll
Reverse SDE PFODE Reverse SDE PFODE
Pure VESDE ( T= 100 ) 0.082 0.434 9.58 21.05
Drift VESDE ( T= 100 ) 0.043 0.249 8.71 7.77
Pure VESDE ( T= 625 ) 0.027 0.057 8.00 8.20
Drift VESDE ( T= 625 ) 0.025 0.031 7.95 7.21
our analysis, we choose two sampling methods for the reverse process: Euler-Maruyama method for
reverse SDE and RK45 ODE solver for the reverse PFODE method.
We note that although aggressive setting Î²t=tandÏ„=Thas shown its power in theory (Lemma C.2)
and the experiments with accurate score (Figure 2), other sampling issues may arise in practice. We
leave the experimental exploration for drift VESDE with aggressive Î²tas a future work.
The training detail. For each dataset, we train a score function with pure VESDE (Equation (2),
Ïƒ2
t=t). We train for 200epochs with batch size 200and learning rate 10âˆ’4. For both training and
inference, the start time is Î´= 10âˆ’5. For the conservative VESDE, we directly adapt the checkpoint
learned by the pure VESDE since the conservative drift VESDE has a similar trend compared to pure
VESDE, as shown in Figure 2. The above experiments are runned over 5random seed and we present
the average over these seeds in Table 1.
The above experiments are conduct on a GeForce RTX 4090. It takes 25 minutes to train a score
function of pure VESDE.
Observation. We do experiments with T= 100 and lager T= 625 and these two choice show
similar phenomenon. In this paragraph, we first use T= 100 as an example to discuss the results. As
shown in Table 1, the conservative drift VESDE has smaller KL divergence compared to pure VESDE
under all sampling methods and datasets. From Figure 1 and Figure 4, it is clear that pure VESDE
has low density on the Swiss roll except the center one, which means that though pure VESDE can
deal with small E[q0], it is hard to deal with large dataset variance Cov[q0], as we discuss in Section 4.
For conservative drift VESDE ( Î²t= 1andÏ„=T), as we discuss in Section 3.1, there is a constant
decay on the prior information E[q0]andCov[q0], which is helpful in deal with large dataset mean
and variance. The experimental results support our augmentation. Figure 1 (c), Figure 4 (c) and
Figure 5 (c) show that the density of the generated distribution is more uniform compared to pure
VESDE, which means that the drift VESDE can deal with large dataset mean and variance.
We also do experiments with larger T= 625 . As we discuss in Section 4, larger Twill reduce the
influence of the prior data information and have greater generated distribution, as shown in Figure 4
(c) and Figure 4 (e). The experiments of 1D-GMM (Figure 5) show a similar phenomenon compared
to the multi Swiss rolls.
G.3 The Real-World Experiments on CelebA 256
After achieving great performance under the synthetic data, we show that our conservative drifted
VESDE can improve the results of pure VESDE without training.
Setting. In this experiment, we adapt well-known VESDE implementation [Song et al., 2020b]
and do experiments on CelebA datasets (size: 256âˆ—256âˆ—3). More specifically, we use
ve/celebahq _256_ncsnpp _continuous checkpoints provided by [Song et al., 2020b] and modify
the sampling process strictly according to our drifted VESDE. To do a fair comparsion, we fix the
random seed and use the reverse PFODE process. Then, we generate 10000 face images to calculate
the metrics. We note that when using this checkpoint and pure VESDE pipeline provided by [Song
et al., 2020b], the models would generate almost pure noise with a certain probability. Hence, we use
an aesthetic predictor [Schuhmann et al., 2022] (aesthetic score â‰¥5.5) to filter the generated images
to ensure that the images are clear faces.
33(a) Original Figure (b) VESDE ( ğ‘‡=100) (c) Drifted VESDE ( ğ‘‡=100)
(d) VESDE ( ğ‘‡=625) (e) Drifted VESDE ( ğ‘‡=625)Figure 4: Experiment results of Swiss roll with reverse PFODE
(a) Original Figure (b) VESDE ( ğ‘‡=625) (c) Drifted VESDE ( ğ‘‡=625)
Figure 5: Experiment results of 1D-GMM with reverse PFODE
Discussion. From the qualitative perspective, as shown in Figure 3 (Figure 6 and 7), the images
generated by our drifted VESDE have more detail (such as hair and beard details). On the contrary,
since pure VESDE can not deal with large variance, the images generated by pure VESDE appear
blurry and unrealistic in these details. From the quantitative results, our drifted VESDE achieves
aesthetic score 5.813, and IS 4.174, which is better than the results of baseline pure VESDE (aesthetic
score 5.807 and IS: 4.082). In conclusion, the real-world experiments show the potential of our
drifted VESDE.
We note that the goal of these experiments is to show that our conservative drifted VESDE is plug-
and-play without training instead of achieving a SOTA performance. Hence, we focus on the relative
improvement compared to the baseline [Song et al., 2020b]. There are two interesting empirical
future works. For the conservative drifted VESDE, we will do experiments on the SOTA pure VESDE
models [Karras et al., 2022] and improve their results without training. For the aggressive drifted
VESDE, since this process makes a larger modification compared with the conservative one, we need
to train a new score function instead of directly using a pre-train one to achieve better results.
34Figure 6: The real-world experiments on CelebA256 dataset (More examples)
Figure 7: The real-world experiments on CelebA256 dataset (Detail)
NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: We propose a new drifted VESDE forward process and achieve the first
polynomial complexity for reverse SDE (Corollary 5.4). For reverse PFODE, we propose
the firs t quantitative convergence guarantee for VESDE (SOTA) (Theorem 6.1). We also do
synthetic experiments to support our results (Section 7).
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
35Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We discuss future work and limitation at Section 8.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All assumptions, Theorem, Corollary, and proof sketch have been clearly
stated in the main content. The detailed proof appears in the appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
36We has shown all experiments detail including dataset and training detail in Appendix G.
Furthermore, we discuss why these experiments results support our theoretical results in
Section 7.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: As a theoretical work, we only do simple synthetic experiments to support our
results. All detail and the used checkpoint are shown in Appendix G.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
37â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We has shown all experiments detail including dataset and training detail in
Appendix G.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The results of Figure 2 and Table 1 are calculated over 5random seed.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We have shown the compute works and computation time in Appendix G.
38Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have checked the code of ethics and make sure that our work satisfies the
code of ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We have discussed the broader impacts of our work at the end of main paper.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
39Answer: [NA]
Justification: This paper poses no such risks.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: The paper does not use existing assets.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
40Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
41