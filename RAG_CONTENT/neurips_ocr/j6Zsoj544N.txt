Does Worst-Performing Agent Lead the Pack?
Analyzing Agent Dynamics in Unified Distributed SGD
Jie Hu Yi-Ting Ma Do Young Eun
Department of Electrical and Computer Engineering
North Carolina State University
{jhu29, yma42, dyeun}@ncsu.edu
Abstract
Distributed learning is essential to train machine learning algorithms across hetero-
geneous agents while maintaining data privacy. We conduct an asymptotic analysis
of Unified Distributed SGD (UD-SGD), exploring a variety of communication pat-
terns, including decentralized SGD and local SGD within Federated Learning (FL),
as well as the increasing communication interval in the FL setting. In this study,
we assess how different sampling strategies, such as i.i.d. sampling, shuffling, and
Markovian sampling, affect the convergence speed of UD-SGD by considering
the impact of agent dynamics on the limiting covariance matrix as described in
the Central Limit Theorem (CLT). Our findings not only support existing theories
on linear speedup and asymptotic network independence, but also theoretically
and empirically show how efficient sampling strategies employed by individual
agents contribute to overall convergence in UD-SGD. Simulations reveal that a few
agents using highly efficient sampling can achieve or surpass the performance of
the majority employing moderately improved strategies, providing new insights
beyond traditional analyses focusing on the worst-performing agent.
1 Introduction
Distributed learning deals with the training of models across multiple agents over a communication
network in a distributed manner, while addressing the challenges of privacy, scalability, and high-
dimensional data [ 11,55]. Each agent iâˆˆ[N]holds a private dataset Xiand an agent-specified
loss function Fi:RdÃ— Xiâ†’Rthat depends on the model parameter Î¸âˆˆRdand a data point
Xâˆˆ Xi. The goal is then to find a local minima Î¸âˆ—of the objective function f(Î¸)â‰œ1
NPN
i=1fi(Î¸),
where agent iâ€™s loss function fi(Î¸)â‰œEXâˆ¼Di[Fi(Î¸, X)]andDirepresents the target distribution
of data for agent i.1Each agent ican locally compute the gradient âˆ‡Fi(Î¸, X)âˆˆRdw.r.t. Î¸for
every sampled data point Xâˆˆ Xi. Due to the distributed nature, {Di}iâˆˆ[N]and{Xi}iâˆˆ[N]are not
necessarily identically distributed over [N]so that the minima of each local function fi(Î¸)can be far
away from L. This is particularly relevant in decentralized training data, e.g., Federated Learning
(FL) with heterogeneous data across data centers or devices [81, 31].
In this paper, we focus on Unified Distributed SGD (UD-SGD), where each agent iâˆˆ[N]updates its
model parameter Î¸i
n+1in a two-step process:
Local update: Î¸i
n+1/2=Î¸i
nâˆ’Î³n+1âˆ‡Fi(Î¸i
n, Xi
n), (1a)
1Throughout the paper we donâ€™t impose convexity assumption on f(Î¸). For convex f(Î¸),Lis the global
minima. For non-convex f(Î¸),Lrepresents the collection of local minima, which is of great interest in
neural network training for sufficiently good performance [ 20,19]. With an additional condition such as the
Polyak-Lojasiewicz inequality, non-convex f(Î¸)is ensured to have a unique minima [1, 75, 78].
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Aggregation: Î¸i
n+1=PN
j=1wn(i, j)Î¸j
n+1/2, (1b)
where Î³ndenotes the step size, Xi
nis the data sampled by agent iat time n(i.e., agent dynamics), and
Wn=[wn(i, j)]i,jâˆˆ[N]represents the doubly-stochastic communication matrix satisfying wn(i, j)â‰¥
0and1TWn=1T,Wn1=1. In the special case of N= 1,(1)simplifies to the vanilla SGD where
Wn= 1for all n. UD-SGD covers a wide range of distributed algorithms, e.g., decentralized SGD
(DSGD) [ 71,80,61,68], distributed SGD with changing topology (DSGD-CT) [ 24,43], local SGD
(LSGD) in FL [55, 76], and its variant aimed at reducing communication costs (LSGD-RC) [51].
ð’Š
ð‘¿ð’ð’Š
ð‘¿ð’+ðŸð’Šð’‹ð’˜ð’(ð’Š,ð’‹)
ð’˜ð’(ð’‹,ð’Š)
ð‘¿ð’ð’‹ð‘¿ð’+ðŸð’‹
dataset ð’³ð’Š dataset ð’³ð’‹Communication 
pattern ð‘¾ð‘›
Heterogeneous 
sampling 
strategy ð‘‹ð‘–,ð‘‹ð‘—
Figure 1: GD-SGD algorithm with a communi-
cation network of N= 5 agents, each holding
potentially distinct datasets; e.g., agent j(in blue)
samples Xji.i.d. and agent i(in red) samples Xi
via Markovian trajectory.Versatile Communication Patterns {Wn}:
For visualization, we depict the scenarios of
UD-SGD (1)in Figure 1. In DSGD, each agent
(node) in the graph communicates with its neigh-
bors after each SGD computation via Wn, rep-
resenting the underlying network topology. As
a special case, central server-based aggregation,
forming a fully connected network, translates
Wninto a rank-1 matrix Wn=11T/N. To
minimize communication expenses, FL variants
allow each agent to perform multiple SGD steps
before aggregation [ 55,67,76], resulting in a
communication interval of length Kand a con-
sistent pattern Wn=Wforn=mK,âˆ€mâˆˆ
N, andWn=INotherwise. In particular, i)
W=11T/Ncorresponds to LSGD with full agent participation (LSGD-FP) [ 76,42,51]; ii)Wis a
random matrix generated by partial agent participation (LSGD-PP) [ 55,18,74]; iii)Wis generated
by Metropolis-Hasting algorithm in decentralized setting, e.g., hybrid LSGD (HLSGD) [ 37,32] and
decentralized FL (DFL) [46, 77, 16]. We defer further discussion of Wto Appendix F.1.
Markovian vs i.i.d. Sampling: Agents typically employ i.i.d. or Markovian sampling, as illustrated
in the bottom brown box of Figure 1. In cases where agents have full access to their data, DSGD with
i.i.dsampling has been extensively studied [ 60,43,61,47]. In FL, many application-oriented LSGD
variants have been investigated [ 51,18,77,32,37,53]. However, these works solely focus on i.i.d.
sampling, restricting their applicability to Markovian sampling scenarios.
Markovian sampling, which has received increased attention in limited settings (see Table 1), is
vital where agents lack independent data access. For instance, in statistical applications, agents with
an unknown a priori distribution often use Markovian sampling over i.i.d. sampling [ 40,63]. In
HLSGD across device-to-device (D2D) networks [ 32,37], random walks reduce communication costs
compared to the frequent aggregations required by Gossip algorithms [ 38,28,4]. For single-agent
scenarios, vanilla SGD with Markovian noise, as applied in a D2D network, has shown improved
communication efficiency and privacy [ 69,28,35]. In contrast, for agents with full data access,
Markov Chain Monte Carlo (MCMC) methods can be more efficient than i.i.d. sampling, especially
in high-dimensional spaces with constraints [ 27,40], where acceptance-rejection methods [ 12] lead to
computational inefficiency (e.g., wasted samples) due to multiple rejections before obtaining a sample
that satisfies constraints [ 26,69]. In addition, shuffling methods can be considered as high-order
Markov chains [38], which achieves faster convergence than i.i.d. sampling [1, 79, 78].
Limitations of Non-Asymptotic Analysis on Agentâ€™s Sampling Strategy: Recent studies on the
non-asymptotic behavior of DSGD and LSGD variants under Markovian sampling, as summarized in
Table 1, have made significant strides. However, these works often fall short in accurately revealing
the statistical influence of each agent dynamics {Xi
n}on the performance of UD-SGD. For instance,
[71,68] proposed the error bound O(1/log2(1/Ï)
n1âˆ’a), where aâˆˆ(0.5,1]andÏdenotes the identical
mixing rate for all agents, overlooking agent heterogeneity in sampling strategy. A similar assumption
toÏis also evident in [ 42]. More recent contributions from [ 80,72] have attempted to relax these
constraints by considering a finite-time bound of O(Ï„2
mix/(n+ 1)) , where Ï„mixis the mixing time
of the slowest agent. This approach, however, inherently focuses on the worst-performing agent ,
neglecting how other agents with faster mixing rates might positively influence the system.2Such
an analysis fails to capture the collective impact of other agents on the overall system performance,
2Although improving the finite-time upper bound to distinguish each agent may not be the focus of the
aforementioned works, their analyses require every Markov chain to be close to some neighborhood of its
2Table 1: Comparison of recent works in distributed learning: We classify the communication patterns
into seven categories, i.e., DSGD, DSGD-CT, LSGD-FP, LSGD-PP, LSGD-RC, HLSGD and DFL. We
mark â€˜UD-SGDâ€™ when all aforementioned patterns are included and the detailed discussion on {Wn}
is referred to Appendix F.1. Abbreviations: â€˜ Asym. â€™=â€˜Asymptoticâ€™, â€˜ D.A.B â€™=â€˜Differentiating
Agent Behaviorâ€™, â€˜ L.S.â€™=â€˜Linear Speedupâ€™, â€˜ A.N.I. â€™=â€˜Asymptotic Network Independenceâ€™.
Reference Analysis Sampling Communication Pattern D.A.B. L.S. A.N.I.
[58] Asym. i.i.d. DSGD âœ“ âœ“ âœ“
[51] Asym. i.i.d. LSGD-RC âœ“ âœ“ N/A
[43, 47] Non-Asym. i.i.d. DSGD-CT Ã— âœ“ Ã—
[61] Non-Asym. i.i.d. DSGD Ã— Ã— âœ“
[18, 53] Non-Asym. i.i.d. LSGD-PP Ã— âœ“ N/A
[37, 32] Non-Asym. i.i.d. HLSGD Ã— âœ“ Ã—
[77, 16] Non-Asym. i.i.d. DFL Ã— âœ“ Ã—
[71, 80, 68] Non-Asym. Markov DSGD Ã— Ã— Ã—
[42, 72] Non-Asym. Markov LSGD-FP Ã— âœ“ N/A
[69, 4, 28] Non-Asym. Markov N/A (single agent) N/A N/A N/A
[38, 52] Asym. Markov N/A (single agent) N/A N/A N/A
Our Work Asym. Markov UD-SGD âœ“ âœ“ âœ“
a crucial aspect in large-scale applications where identifying and managing the worst-performing
agent is challenging due to privacy concerns or sporadic unreachability. Since agents in distributed
learning have the freedom to choose their sampling strategies, itâ€™s vital to understand how each
agentâ€™s improved sampling approach contributes to the overall convergence speed of the UD-SGD
algorithm. This understanding is key to enhancing system performance, particularly in large-scale
machine learning scenarios where agent heterogeneity is a defining feature.
Rationale for Asymptotic Analysis: Recent trends in convergence analysis have leaned towards
non-asymptotic methods, yet itâ€™s crucial to recognize the complementary role of asymptotic analysis
for a better understanding of convergence behaviors, as highlighted in [ 9,56,25,39]. For vanilla SGD,
[59,17] emphasized that central limit theorem (CLT) is far less asymptotic than it may appear under
both i.i.d. and Markovian sampling. Notably, the limiting covariance matrix, a key statistical feature
in vanilla SGDâ€™s CLT, also prominently features in high-probability bound [ 59], explicit finite-time
bound [ 17] and 1-Wasserstein distance in the non-asymptotic CLT [ 66]. [38] further underscored
this by numerically showing that the limiting covariance matrix provides a more precise depiction of
convergence than the mixing rates often used in finite-time upper bounds [ 26,69]. Moreover, they
argued that finite-time analysis may not suitably apply to certain efficient high-order Markov chains,
due to the lack of comparative mixing-rate metrics.
Our Contributions: We present an asymptotic analysis of the UD-SGD algorithm (1)under hetero-
geneous agent dynamics {Xi
n}and a large family of communication patterns {Wn}. Specifically,
â€¢Under appropriate assumptions, all agents performing (1)asymptotically reach the consensus and
findÎ¸âˆ—:âˆ€iâˆˆ[N],Î¸nâ‰œ1
NPN
i=1Î¸i
ndenotes the average model parameter among all agents, we have
lim
nâ†’âˆžâˆ¥Î¸i
nâˆ’Î¸nâˆ¥=0,lim
nâ†’âˆžâˆ¥Î¸nâˆ’Î¸âˆ—âˆ¥=0 a.s. (2)
Moreover, we derive the CLT of UD-SGD in the form of
Î³âˆ’1/2
n(Î¸nâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ âˆ’ â†’
nâ†’âˆžN(0,V). (3)
Our framework addresses technical challenges in quantifying consensus error under various commu-
nication patterns and slowly increasing communication interval. This shows a substantial extension
compared to previous studies [ 58,43,51], particularly in regulating the growth of communication
stationary distribution. This naturally incurs a maximum operator , and thus convergence is strongly influenced
by the slowest mixing rate, i.e., the worst-performing agent.
3intervals (Assumption 2.3-ii) and in proving the scaled consensus errorâ€™s boundedness (Lemma B.1).
Furthermore, we reformulate UD-SGD as a stochastic approximation-like iteration and tackle the
Markovian noise term using the Poisson equation, a technique previously confined only to vanilla
SGD with Markovian sampling [ 17,38,52]. The key here is to devise the noise decomposition that
separates the consensus error among all agents from the error caused by the bias from the Markov
chain, which aligns with the target distribution only asymptotically at infinity, not at finite times.
â€¢In analyzing (3), we derive the exact form of Vas1
N2PN
i=1Vi. Here, Viis the limiting covariance
matrix of agent i, which depends mainly on its sampling strategy {Xi
n}. This allows us to show
that improving individual agentsâ€™ sampling strategy can reduce the covariance in CLT, which in turn
implies a smaller mean-square error (MSE) for large time n. This is a significant advancement over
previous finite-sample bounds that only account for the worst-performing agent and do not fully
capture the effect of individual agent dynamics on overall system performance. Our CLT result (3)
also treats recent findings in [ 38] as a very special case with N= 1, where the relationship therein
between the sampling efficiency of the Markov chain and the limiting covariance matrix in the CLT
of vanilla SGD, can carry over to our UD-SGD.
â€¢We demonstrate that our analysis supports recent findings from studies such as [ 42], which exhibited
linear speedup scaling with the number of agents under LSGD-FP with Markovian sampling; and
[62,61], which examined the notion of â€˜asymptotic network independenceâ€™ for DSGD with i.i.d.
sampling, where the convergence of the algorithm (1)at large time ndepends solely on the left
eigenvector of Wn(1
N1considered in this paper) rather than the specific communication network
topology encoded in Wn, but now under Markovian sampling. We extend these findings in view of
CLT to a broader range of communication patterns {Wn}and general sampling strategies {Xi
n}.
â€¢We conduct numerical experiments using logistic regression and neural network training with
several choices of agentsâ€™ sampling strategies, including a recently proposed one via nonlinear
Markov chain [ 25]. Our results uncover a key phenomenon: a handful of compliant agents adopting
highly efficient sampling strategies can match or exceed the performance of the majority using
moderately improved strategies. This finding is crucial for practical optimization in large-scale
learning systems, moving beyond the current literature that only considers the worst-performing
agent in more restrictive settings.
2 Preliminaries
Basic Notations: We use âˆ¥vâˆ¥to indicate the Euclidean norm of a vector vâˆˆRdandâˆ¥Mâˆ¥to indicate
the spectral norm of a matrix MâˆˆRdÃ—d. The identity matrix of dimension dis denoted by Id, and
the all-one (resp. all-zero) vector of dimension Nis denoted by 1(resp. 0). Let Jâ‰œ11T/N. The
diagonal matrix with the entries of von the main diagonal is written as diag(v). We also use â€˜ âª°â€™ for
Loewner ordering such that Aâª°Bis equivalent to xT(Aâˆ’B)xâ‰¥0for any xâˆˆRd.
Asymptotic Covariance Matrix: Asymptotic variance is a widely used metric for evaluating the
second-order properties of Markov chains associated with a scalar-valued test function in the MCMC
literature, e.g., Chapter 6.3 [ 12], and asymptotic covariance matrix is its multivariate version for a
vector-valued function. Specifically, we consider a finite, irreducible, aperiodic and positive recurrent
(ergodic) Markov chain {Xn}nâ‰¥0with transition matrix Pand stationary distribution Ï€, and the
estimator Ë†Âµn(g)â‰œ1
nPnâˆ’1
s=0g(Xs)for any vector-valued function g: [N]â†’Rd. According to
the ergodic theorem [ 12,13], we have limnâ†’âˆžË†Âµn(g) =EÏ€(g)a.s.. As defined in [ 13,38], the
asymptotic covariance matrix Î£X(g)for a vector-valued function g(Â·)is given by
Î£X(g)â‰œlim
nâ†’âˆžnÂ·Var(Ë†Âµn(g))= lim
nâ†’âˆž1
nÂ·E
âˆ†nâˆ†T
n	
, (4)
where âˆ†nâ‰œPnâˆ’1
s=0(g(Xs)âˆ’EÏ€(g)). By following the algebraic manipulations in [ 12, Theorem
6.3.7] for asymptotic variance (univariate version), we can rewrite (4) in a matrix form such that
Î£X(g) =GTdiag(Ï€) 
Zâˆ’IN+1Ï€T
G, (5)
where Gâ‰œ[g(1),Â·Â·Â·,g(N)]TâˆˆRNÃ—dandZâ‰œ[INâˆ’P+1Ï€T]âˆ’1. This matrix form explicitly
shows the dependence on the transition matrix Pand its stationary distribution Ï€, and will be utilized
in our Theorem 3.3.
4Model Description: The UD-SGD in (1)can be expressed in a compact iterative form, i.e., we have
Î¸i
n+1=PN
j=1wn(i, j)(Î¸j
nâˆ’Î³n+1âˆ‡Fj(Î¸j
n, Xj
n)), (6)
at each time n, where each agent isamples according to its own Markovian trajectory {Xi
n}nâ‰¥0
with stationary distribution Ï€isuch that EXâˆ¼Ï€i[Fi(Î¸, X)]=fi(Î¸). LetKldenote the communication
interval between the (lâˆ’1)-th and l-th aggregation among Nagents, and nlâ‰œPl
m=1Kmbe the
time instance for the l-th aggregation. We also define Ï„nâ‰œminl{l:nlâ‰¥n}as the index of the
upcoming aggregation at time nsuch that KÏ„nindicates the communication interval for the Ï„n-th
aggregation, or more precisely, the length of the communication interval that includes the time index
n. The communication pattern follows that Wn=InifnÌ¸=nlandWn=Wotherwise for
lâ‰¥1, where the examples of Wwill be discussed in Appendix F.1. Note that i) when Kl= 1,(6)
reduces to DSGD; ii) when Kl=K > 1,(6)becomes the local SGD in FL. iii) When Klincreases
withl, we recover some choices of Klstudied in [ 51] beyond LSGD-RC with i.i.d. sampling. This
increasing communication interval aims to further reduce the frequency of aggregation among agents
for lower communication costs, but now under a Markovian sampling setting and a wider range of
communication patterns. We below state the assumptions needed for the main theoretical results.
Assumption 2.1 (Regularity of the gradient) .For each iâˆˆ[N]andXâˆˆ Xi, the function Fi(Î¸, X)
is L-smooth in terms of Î¸, i.e., for any Î¸1, Î¸2âˆˆRd,
âˆ¥âˆ‡Fi(Î¸1, X)âˆ’ âˆ‡Fi(Î¸2, X)âˆ¥ â‰¤Lâˆ¥Î¸1âˆ’Î¸2âˆ¥. (7)
In addition, we assume that the objective function fis twice continuously differentiable and Âµ-strongly
convex only around the local minima Î¸âˆ—âˆˆ L, i.e.,
Hâ‰œâˆ‡2f(Î¸âˆ—)âª°ÂµId. (8)
Assumption 2.1 imposes the regularity conditions on the gradient âˆ‡Fi(Â·, X)and Hessian matrix
of the objective function f(Â·), as is commonly assumed in [ 10,45,29,38]. Note that (7)requires
per-sample Lipschitzness of âˆ‡Fiand is stronger than the Lipschitzness of its expected version âˆ‡fi,
which is commonly assumed under i.i.dsampling setting [ 73,50,30]. However, we remark that this
is in line with previous work on DSGD and LSGD-FP under Markovian sampling as well [ 71,42,80],
because âˆ‡Fi(Î¸, X)is no longer the unbiased stochastic version of âˆ‡fi(Î¸)and the effect of {Xi
n}
has to be taken into account in the analysis. The local strong convexity at the minimizer is commonly
assumed to analyze the convergence of the algorithm under both asymptotic and non-asymptotic
analysis [10, 29, 38, 45, 52, 80].
Assumption 2.2 (Ergodicity of Markovian sampling) .{Xi
n}nâ‰¥0is an ergodic Markov chain
with stationary distribution Ï€isuch that EXâˆ¼Ï€i[Fi(Î¸, X)] = fi(Î¸), and is independent from
{Xj
n}nâ‰¥0, jÌ¸=i.
The ergodicity of the underlying Markov chains, as stated in Assumption 2.2, is commonly assumed
in the literature [ 26,69,80,42,38]. This assumption ensures the asymptotic unbiasedness of the loss
function Fi(Î¸,Â·), which takes i.i.d. sampling as a special case.
Assumption 2.3 (Decreasing step size and slowly increasing communication interval) .i) For bounded
communication interval KÏ„nâ‰¤K,âˆ€n, we assume the polynomial step size Î³n= 1/naandaâˆˆ
(0.5,1]; Or ii) If KÏ„nâ†’ âˆž asnâ†’ âˆž , we assume Î³n= 1/nand define Î·n=Î³nKL+1
Ï„n, where the
sequence {Kl}lâ‰¥0satisfiesP
nÎ·2
n<âˆž,KÏ„n=o(Î³âˆ’1/2(L+1)
n ), and limlâ†’âˆžÎ·nl+1/Î·nl+1+1= 1.
In Assumption 2.3, the polynomial step size Î³nis standard in the literature and it has the propertyP
nÎ³n=âˆž,P
nÎ³2
n<âˆž[17,38]. Inspired by [ 51], we introduce Î·nto control the step size
within each l-th communication interval with length Klto restrict the growth of Kl. Specifically,P
nÎ·2
n<âˆžandKÏ„n=o(Î³âˆ’1/2(L+1)
n )ensure that Î·nâ†’0andKÏ„ndoes not increase too fast
inn.limlâ†’âˆžÎ·nl+1/Î·nl+1+1= 1 sets the restriction on the increment from nltonl+1. Several
practical forms of Klsuggested by [ 51], including Klâˆ¼log(l)andKlâˆ¼log log( l), also satisfy
Assumption 2.3-ii). We defer to Appendix A the mathematical verification of these two types of Kl,
together with the practical implications of increasing communication interval Kl.
Remark 1. In Assumption 2.3, we incorporate an increasing communication interval along with
a step size Î³n= 1/n. This complements the choice of step size Î³nin [51, Assumption 3.3], where
Î³n= 1/naforaâˆˆ(0.5,1). It is important to note, however, that the increasing communication
interval specified in [ 51, Assumption 3.2] is applicable only in i.i.d sampling. Under the Markovian
5sampling framework, the expression âˆ‡Fi(Î¸, X)âˆ’âˆ‡fi(Î¸)loses its unbiased and Martingale difference
properties. Consequently, the Martingale CLT application as utilized by [ 51] does not directly extend
to Markovian sampling. To address this, we adapted techniques from [ 29,58] to accommodate the
increasing communication interval within the Markovian sampling setting and various communication
patterns. This adaptation necessitates Î³n= 1/n, a specification not covered in [ 51]. Exploring more
general forms of Klthat could relax this assumption is outside the scope of our current study.
Assumption 2.4 (Stability on model parameter) .We assume supnâˆ¥Î¸i
nâˆ¥<âˆžalmost surely âˆ€iâˆˆ[N].
Assumption 2.4 claims that the sequence of {Î¸i
n}always remains in a path-dependent compact set. It
is to ensure the stability of the algorithm that serves the purpose of analyzing the convergence, which
is often assumed under the asymptotic analysis of vanilla SGD with Markovian noise [ 23,29,52]. As
mentioned in [ 58,70], checking Assumption 2.4 is challenging and requires case-by-case analysis,
even under i.i.d. sampling. Only recently the stability of SGD under Markovian sampling has been
studied in [ 9], but the result for UD-SGD remains unknown in the literature. Thus, we analyze each
agentâ€™s sampling strategy in the asymptotic regime under this stability condition.
Assumption 2.5 (Contraction property of communication matrix) .i).{Wn}nâ‰¥0is independent of
the sampling strategy {Xi
n}nâ‰¥0for all iâˆˆ[N]and is assumed to be doubly-stochastic for all n; ii).
At each aggregation step nl,Wnlis independently generated from some distribution Pnlsuch that
âˆ¥EWâˆ¼Pnl[WTW]âˆ’Jâˆ¥â‰¤C1<1for some constant C1.
The doubly-stochasticity of Wnin Assumption 2.5-i) is widely assumed in the literature [ 54,24,
43,80]. Assumption 2.5-ii) is a contraction property to ensure that agents employing UD-SGD will
asymptotically achieve the consensus, which is also common in [ 7,24,80]. Examples of Wthat
satisfy Assumption 2.5-ii), e.g., Metropolis-Hasting matrix, partial agent participation in FL, are
deferred to Appendix F.1 due to space constraint.
3 Asymptotic Analysis of UD-SGD
Almost Sure Convergence: LetÎ¸nâ‰œ1
NPN
i=1Î¸i
nrepresent the consensus among all the agents at
timen, we establish the asymptotic consensus of the local parameters Î¸i
n, as stated in Lemma 3.1.
Lemma 3.1. Under Assumptions 2.1, 2.3, 2.4 and 2.5, the consensus error Î¸i
nâˆ’Î¸ndiminishes to zero
at the rate specified below: Almost surely, for every agent iâˆˆ[N],
Î¸i
nâˆ’Î¸n=O(Î³n)under Assum. 2.3-i) ,
O(Î·n)under Assum. 2.3-ii) .(9)
Lemma 3.1 indicates that all agents asymptotically reach consensus at a rate of O(Î³n)(orO(Î·n)).
This finding extends the scope of [ 58, Proposition 1], incorporating considerations for Markovian
sampling, FL settings, and increasing communication interval Kl. The proof, detailed in Appendix B,
primarily tackles the challenge of establishing the boundedness of the sequences {Î³âˆ’1
n(Î¸i
nâˆ’Î¸n)}(or
{Î·âˆ’1
n(Î¸i
nâˆ’Î¸n)}) almost surely for all iâˆˆ[N]. This is specifically analyzed in Lemma B.1. Next,
with additional Assumption 2.2, we are able to obtain the almost sure convergence of Î¸ntoÎ¸âˆ—âˆˆ L.
Theorem 3.2. Under Assumptions 2.1 - 2.5, the consensus Î¸nconverges to Lalmost surely, i.e.,
lim supninfÎ¸âˆ—âˆˆLâˆ¥Î¸nâˆ’Î¸âˆ—âˆ¥= 0 a.s. (10)
Theorem 3.2 is achieved by decomposing the Markovian noise term âˆ‡Fi(Î¸i
n, Xi
n)âˆ’ âˆ‡fi(Î¸i
n), using
the Poisson equation technique as discussed in [ 6,29,17], into a Martingale difference noise term,
along with additional noise terms. We then reformulate (6)into an iteration akin to stochastic
approximation, as depicted in (56). The subsequent step involves verifying the conditions on these
noise terms under our stated assumptions. Crucially, this theorem also establishes that UD-SGD
ensures an almost sure convergence of each agent to a local minimum Î¸âˆ—âˆˆ L, even in scenarios
where the communication interval Klgradually increases, in accordance with Assumption 2.3-ii).
The detailed proof of this theorem is provided in Appendix C.
Central Limit Theorem: LetUiâ‰œÎ£Xi(âˆ‡Fi(Î¸âˆ—,Â·))represent the asymptotic covariance matrix
(defined in (5)) associated with each agent iâˆˆ[N], given their sampling strategy {Xi
n}and function
âˆ‡Fi(Î¸âˆ—,Â·). Define Uâ‰œ1
N2PN
i=1Ui. We assume the polynomial step-size Î³nâˆ¼Î³â‹†/na,aâˆˆ(0.5,1]
andÎ³â‹†>0. In the case of a= 1, we further assume Î³â‹†>1/2Âµ, where Âµis defined in (8).
For notational simplicity, and without loss of generality, our remaining CLT result is stated while
conditioning on the event that {Î¸nâ†’Î¸âˆ—}for some Î¸âˆ—âˆˆ L.
6Theorem 3.3. Let Assumptions 2.1 - 2.5 hold. Then,
Î³âˆ’1/2
n(Î¸nâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ âˆ’ â†’
nâ†’âˆžN(0,V), (11)
where the limiting covariance matrix Vis in the form of
V=Râˆž
0eMtUeMtdt. (12)
Here, we have M=âˆ’Hifaâˆˆ(0.5,1), orM=Id/2Î³â‹†âˆ’Hifa= 1, where His defined in (8).
Moreover, let Â¯Î¸n=1
nPnâˆ’1
s=0Î¸sandVâ€²=Hâˆ’1UHâˆ’1. For aâˆˆ(0.5,1), we have
âˆšn(Â¯Î¸nâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ âˆ’ â†’
nâ†’âˆžN(0,Vâ€²), (13)
The proof, presented in Appendix D, addresses the technical challenges in deriving the CLT for
UD-SGD, specifically the second-order conditions in decomposing the Markovian noise term, which
is not present in the i.i.d. sampling case [ 58,43,51]. We decompose âˆ‡Fi(Î¸n, Xi
n)âˆ’âˆ‡fi(Î¸n)into
three parts in (48) using Poisson equation: ei
n+1, Î½i
n+1, Î¾i
n+1. The consensus error Î¸i
nâˆ’Î¸nembedded
in noise terms ei
n+1andÎ¾i
n+1is a new factor, whose characteristics have been quantified in our
Lemma 3.1 but are not present in the single-agent scenario analyzed as an application of stochastic
approximation in [ 22,29]. The specifics of this analysis are expanded upon in Appendices D.1 to
D.3. We require Î³â‹†>1/2Âµfora=1to ensure that the largest eigenvalue of Mis negative, as this
is a necessary condition for the existence of Vin(12) (otherwise integration diverges). In the case
where there is only one agent ( N=1),VandVâ€²reduce to the matrices specified in the CLT result
of vanilla SGD [ 29,38,52]. In addition, for a special case of constant communication interval in
Assumption 2.3-i) and i.i.d. sampling as shown in Table 1, we recover the CLT of LSGD-RC in [ 51].
See Appendix E for detailed discussions.
Theorem 3.3 has significant implications for the MSE of {Î¸n}for large time n, i.e.,E[âˆ¥Î¸nâˆ’Î¸âˆ—âˆ¥2]=Pd
i=1eT
iE[(Î¸nâˆ’Î¸âˆ—)(Î¸nâˆ’Î¸âˆ—)T]eiâ‰ˆÎ³nPd
i=1eT
iVei=Î³nTr(V), where eiis the d-dimensional
vector of all zeros except 1at the i-th entry. This indicates that a smaller limiting covariance matrix
V, according to the Loewner order, results in a smaller trace of Vand consequently in a reduced
MSE for large n. Consideration for smaller Vwill be presented in the next section, where agents
have the opportunity to improve their sampling strategies.
Remark 2. Studies by [ 62,61] have shown that in DSGD with a fixed doubly-stochastic matrix
W, the influence of communication topology diminishes after a transient period. Our Theorem 3.3
extends these findings to Markovian sampling and a broader spectrum of communication patterns as
in Table 1. This extension is based on the fact that the consensus error, impacted primarily by the
communication pattern, decreases faster than the CLT scale O(âˆšÎ³n)and is thus not the dominant
factor in the asymptotic regime, as suggested by Lemma 3.1.
Remark 3. Recent studies have highlighted linear speedup with increasing number of agents Nin
the dominant term of their finite-sample error bounds under DSGD-CT with i.i.d. sampling [ 43] and
LSGD-FC with Markovian sampling [ 42]. However, our Theorem 3.3 demonstrates this phenomenon
under more diverse communication patterns and Markovian sampling in Table 1 via the leading term
Vin our CLT. Specifically, it scales with 1/N, i.e.V=Â¯V/N, where Â¯V=1
NPN
i=1Videnotes the
average limiting covariance matrices across all Nagents and Vi=Râˆž
0eMtUieMtdt, suggesting
that the MSE E[âˆ¥Î¸nâˆ’Î¸âˆ—âˆ¥2]will be improved by 1/N. A similar argument also applies to Vâ€²in(13),
i.e.,Vâ€²=Â¯Vâ€²/N, where Â¯Vâ€²=1
NPN
i=1Vâ€²
iandVâ€²
i=Hâˆ’1UiHâˆ’1.
Impact of Agentâ€™s Sampling Strategy: In the literature, the mixing time-based technique has
been widely used in the non-asymptotic analysis in SGD, DSGD and various LSGD variants in FL
[26, 69, 68, 80, 42], i.e., for each agent iâˆˆ[N]and some constant C,
âˆ¥âˆ‡Fi(Î¸, Xi
n)âˆ’ âˆ‡fi(Î¸)âˆ¥ â‰¤Câˆ¥Î¸âˆ¥Ïn
i, (14)
where Ïiis the mixing rate of the underlying Markov chain. However, typical non-asymptotic analyses
often rely on Ïâ‰œmax iÏiamong Nagents, i.e., the worst-performing agent in their finite-time
bounds [80, 72], or assume an identical mixing rate across all Nagents [42, 68].
In contrast, Remark 3 highlights that each agent holds its own limiting covariance matrices Viand
Vâ€²
i, which are predominantly governed by the matrix Ui, capturing the agentâ€™s sampling strategy
7{Xi
n}andcontributing equally to the overall performance of UD-SGD. For each agent i, denote by
UX
iandUY
ithe asymptotic covariance matrices associated with two candidate sampling strategies
{Xi
n}and{Yi
n}, respectively. Let VXandVYbe the limiting covariance matrices of the distributed
system in (12), where agent iemploys {Xi
n}and{Yi
n}, respectively, while keeping other agentsâ€™
sampling strategies unchanged. Then, we have the following result.
Corollary 3.4. For agent i, if there exist two sampling strategies {Xi
n}nâ‰¥0and{Yi
n}nâ‰¥0such that
UX
iâª°UY
i, we have VXâª°VY.
Corollary 3.4 directly follows from the definition of Loewner ordering, and Loewner ordering being
closed under addition (i.e., Aâª°Bimplies A+Câª°B+C). It demonstrates that even a single agent
improves its sampling strategy from {Xi
n}to{Yi
n}, it leads to an overall reduction in V(in terms
of Loewner ordering), thereby decreasing the MSE and benefiting the entire group of Nagents.
The subsequent question arises: How do we identify an improved sampling strategy {Yi
n}over the
baseline {Xi
n}?
This question has been partially addressed by [ 57,48,38], which qualitatively investigates the
â€˜efficiency orderingâ€™ of two sampling strategies. In particular, [ 38, Theorem 3.6(i)] shows that
sampling strategy {Yn}is more efficient than {Xn}if and only if Î£X(g)âª°Î£Y(g)for any
vector-valued function g(Â·)âˆˆRd. Consequently, in the UD-SGD framework, employing a more
efficient sampling strategy {Yi
n}over the baseline {Xi
n}by agent ileads to Î£Xi(âˆ‡Fi(Î¸âˆ—,Â·))âª°
Î£Yi(âˆ‡Fi(Î¸âˆ—,Â·)), thus satisfying UX
iâª°UY
i. This finding, as per Corollary 3.4, implies an overall
improvement in UD-SGD.
For illustration purposes, we list a few examples where two competing sampling strategies follow
efficiency ordering: i) When an agent has complete access to the entire dataset (e.g., deep learning),
shuffling techniques like single shuffling and random reshuffling are more efficient than i.i.d. sampling
[38,78]; ii) When an agent works with a graph-like data structure and employs a random walk, e.g.,
agent iin Figure 1, using non-backtracking random walk (NBRW) is more efficient than simple
random walk (SRW) [48]. iii) A recently proposed self-repellent random walk (SRRW) is shown to
achieve near-zero sampling variance, indicating even higher sampling efficiency than NBRW and
SRW [ 25].3This random-walk-based sampling finds a particular application in large-scale FL within
D2D networks (e.g., mobile networks, wireless sensor networks), where each agent acts as an edge
server or access point, gathering information from the local D2D network [ 37,32]. Employing a
random walk over local D2D network for each agent constitutes the sampling strategy.
Theorem 3.3 and Corollary 3.4 not only qualitatively compare these sampling strategies but also
allow for a quantitative assessment of the overall system enhancement. Since every agent contributes
equally to the limiting covariance matrix Vof the distributed system as in Remark 3, a key application
scenario is to encourage a subset of compliant agents to adopt highly efficient strategies like SRRW,
potentially yielding better performance than universally upgrading to slightly improved strategies like
NBRW. This approach, more feasible and impactful in large-scale machine learning scenarios where
some agents cannot freely modify their sampling strategies, is a unique aspect of our framework not
addressed in previous works focusing on the worst-performing agent [80, 42, 68, 72].
4 Experiments
In this section, we empirically evaluate the effect of agentsâ€™ sampling strategies under various
communication patterns in UD-SGD. We consider the L2-regularized binary classification problem
min
Î¸f(Î¸)â‰œ1
NNX
i=1fi(Î¸),withfi(Î¸)=1
BBX
j=1log
1+eÎ¸Txi,j
âˆ’yi,j 
Î¸Txi,j
+Îº
2âˆ¥Î¸âˆ¥2, (15)
where the feature vector xi,jand its corresponding label yi,jare held by agent i, with a penalty
parameter Îºset to 1. We use the ijcnn1 dataset [ 14] with 22features in each data point and 50k
data points in total, which is evenly distributed to two groups with 50agents each ( N= 100 agents
3Note that SRRW is a nonlinear Markov chain that depends on the relative visit counts of each node in the
graph. While its application in single-agent optimization has been studied in [ 39], expanding the theoretical
examination of SRRW to multi-agent scenarios is beyond the scope of this paper. However, we can still
numerically evaluate the performance of UD-SGD with multiple agents on general communication matrices
using SRRW as a highly efficient sampling strategy in Section 4.
810000 12500 15000 17500 20000 22500 25000 27500 30000
Number of steps (n)1.01.52.02.53.0MSE n*2
1e5
50 SRW
50 NBRW
40 SRW, 10 SRRW
30 SRW, 20 SRRW
101102103
Number of steps (n)105
104
103
102
101
100MSE n*2
Centralized Learning
DSGD-VTLSGD-FP
DFL
Figure 2: Binary classification problem. From left to right: (a) Impact of efficient sampling strategies
on convergence. (b) Performance gains from partial adoption of efficient sampling. (c) Comparative
advantage of SRRW over NBRW in a small subset of agents. (d) Asymptotic network independence
of four algorithms under UD-SGD framework with fixed sampling strategy (shuffling, SRRW). (e)
Different sampling strategies in the DSGD algorithm with time-varying topology (DSGD-VT). (f)
Different sampling strategies in the DFL algorithm with increasing communication interval.
in total) and each agent holds B= 500 distinct data points. Each agent in the first group has full
access to its entire dataset, and thus can employ i.i.d. sampling (baseline) or single shuffling. On
the other hand, each agent in the other group has a graph-like structure and uses SRW (baseline),
NBRW or SRRW with reweighting to sample its local dataset with uniform weight. In this simulation,
we assume that agents can only communicate through a communication network using the DSGD
algorithm. This scenario with heterogeneous agents, as depicted in Figure 1, is of great interest in
large-scale machine learning [ 37,32]. In addition, we employ a decreasing step size Î³n= 1/nin our
UD-SGD framework (1)because it is typically used for the strongly convex objective function and is
tested to have the fastest convergence in this simulation setup. Due to space constraints, we defer
detailed simulation setup, including the introduction of SRW, NBRW, and SRRW, to Appendix G.1.
The simulation results are obtained through 120independent trials. In Figure 2(a), we assume that
the first group of agents perform either i.i.d. sampling or shuffling method, while the other group of
agents all change their sampling strategies from baseline SRW to NBRW and SRRW, as shown in the
legend. This plot shows that improved sampling strategy leads to overall convergence speedup since
NBRW and SRRW are more efficient than SRW [ 38,25]. Furthermore, it illustrates that SRRW is
significantly more efficient than NBRW in this simulation setup, i.e., SRRW â‰«NBRW >SRW
in terms of sampling efficiency. While keeping the second group of agents unchanged, we can see
that shuffling method outperforms i.i.d. sampling with smaller asymptotic MSE. However, shuffling
method may not perform perfectly for small time ndue to slow mixing behavior in the initial period,
which is also observed in the single-agent scenario in [ 65,1,38]. The error bar therein also indicates
that the random-walk sampling strategy has a significant impact on the overall system performance
and SRRW has smaller variance than NBRW and SRW.
In Figure 2(b), we let the first group of agents perform i.i.d. sampling while only changing a portion
of agents in the second group to upgrade from SRW to SRRW, e.g., 30SRW 20SRRW in the legend
means that there are 30 agents using SRW while the rest 20agents in the second group upgrade
to SRRW. We observe that more agents willing to upgrade from SRW to SRRW lead to smaller
asymptotic MSE, as predicted by Theorem 3.3 and Remark 3. This improvement in MSE reduction
doesnâ€™t scale linearly with more agents adopting SRRW because each agent holds its own dataset that
are not necessarily identical, resulting in different individual limiting covariance matrices ViÌ¸=Vj.
While maintaining i.i.d. sampling for the first group of agents, we compare the performance when the
second group of agents in Figure 2(c) employ NBRW or SRRW. Remarkably, the case with only 10
agents out of 50agents in the second group adopting far more efficient sampling strategy ( 40SRW,
10SRRW) through incentives or compliance already produces a smaller MSE than all 50agents
using slightly better strategy ( 50NBRW). The performance gap becomes even more pronounced
9when 20agents upgrade from SRW to SRRW ( 30SRW, 20SRRW). We show that the performance
of a distributed system can be improved significantly when a small proportion of agents adopt highly
efficient sampling strategies.
Figure 2(d) empirically illustrates the asymptotic network independence property via four algorithms
under our UD-SGD framework: Centralized SGD (communication interval K= 1, communication
matrix W=11T/N); LSGD-FP (FL with full client participation, K= 5,W=11T/N); DSGD-
VT (DSGD with time-varying topologies, randomly chosen from 5doubly stochastic matrices);
DFL (decentralized FL with fixed MH-generated Wand increasing communication interval Kl=
max{1,log(l)}afterl-th aggregation). We fix the sampling strategy (shuffling, SRRW) throughout
this plot. All four algorithms overlap around 1000 steps, implying that they have entered the
asymptotic regime with similar performance where the CLT result dominates, implying the asymptotic
network independence in the long run.
Figure 2(e) and 2(f) show the performance of different sampling strategies in DSGD-VT and DFL
algorithms in terms of MSE. Both plots consistently demonstrate that improving agentâ€™s sampling
strategies (e.g., shuffling > iid sampling, and SRRW > NBRW > SRW) leads to faster convergence
with smaller MSE, supporting our theory.
Furthermore, in Appendix G.2, we simulate an image classification task with CIFAR-10 dataset [ 44]
by training a 5-layer CNN and ResNet-18 model collaboratively through a 10-agent network. The
result is illustrated in Figure 3, where SRRW outperforms NBRW and SRW as expected. In summary,
we find that upgrading even a small portion of agents to efficient sampling strategies (e.g., shuffling
method, NBRW, SRRW under different dataset structures) improves system performance in UD-SGD.
These results are consistent in binary and image classification tasks, underscoring that every agent
matters in distributed learning.
5 Conclusion
In this work, we develop an UD-SGD framework that establishes the CLT of various distributed algo-
rithms with Markovian sampling. We overcome technical challenges such as quantifying consensus
error under very general communication patterns and decomposing Markovian noise through the
Poisson equation, which extends the analysis beyond the single-agent scenario. We demonstrate that
even if only a few agents optimize their sampling strategies, the entire distributed system will benefit
with a smaller limiting covariance in the CLT, suggesting a reduced MSE. This finding challenges the
current established upper bounds where the worst-performing agent leads the pack. Future studies
could pivot towards developing fine-grained finite-time bounds to individually characterize each
agentâ€™s behavior, and theoretically analyze the effect of SRRW in UD-SGD.
6 Acknowledgments and Disclosure of Funding
We thank the anonymous reviewers for their constructive comments. This work was supported in part
by National Science Foundation under Grant Nos. CNS-2007423, IIS-1910749, and IIS-2421484.
References
[1]Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. Sgd with shuffling: optimal rates without
component convexity and large epoch requirements. In Proceedings of the 34th International
Conference on Neural Information Processing Systems , pages 17526â€“17535, 2020.
[2]Noga Alon, Itai Benjamini, Eyal Lubetzky, and Sasha Sodin. Non-backtracking random walks
mix faster. Communications in Contemporary Mathematics , 9(04):585â€“603, 2007.
[3]Christophe Andrieu, Ã‰ric Moulines, and Pierre Priouret. Stability of stochastic approximation
under verifiable conditions. SIAM Journal on control and optimization , 44(1):283â€“312, 2005.
[4]Ghadir Ayache, Venkat Dassari, and Salim El Rouayheb. Walk for learning: A random walk
approach for federated learning from heterogeneous data. IEEE Journal on Selected Areas in
Communications , 41(4):929â€“940, 2023.
10[5]Anna Ben-Hamou, Eyal Lubetzky, and Yuval Peres. Comparing mixing times on sparse
random graphs. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete
Algorithms , pages 1734â€“1740. SIAM, 2018.
[6]Albert Benveniste, Michel MÃ©tivier, and Pierre Priouret. Adaptive algorithms and stochastic
approximations , volume 22. Springer Science & Business Media, 2012.
[7]Pascal Bianchi, Gersende Fort, and Walid Hachem. Performance of a distributed stochastic
approximation algorithm. IEEE Transactions on Information Theory , 59(11):7405â€“7418, 2013.
[8] Patrick Billingsley. Convergence of probability measures . John Wiley & Sons, 2013.
[9]Vivek Borkar, Shuhang Chen, Adithya Devraj, Ioannis Kontoyiannis, and Sean Meyn. The ode
method for asymptotic statistics in stochastic approximation and reinforcement learning. arXiv
preprint arXiv:2110.14427 , 2021.
[10] V .S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint: Second Edition . Texts
and Readings in Mathematics. Hindustan Book Agency, 2022.
[11] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed opti-
mization and statistical learning via the alternating direction method of multipliers. Foundations
and TrendsÂ® in Machine learning , 3(1):1â€“122, 2011.
[12] Pierre BrÃ©maud. Markov chains: Gibbs fields, Monte Carlo simulation, and queues , volume 31.
Springer Science & Business Media, 2013.
[13] Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. Handbook of markov chain
monte carlo . CRC press, 2011.
[14] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM
transactions on intelligent systems and technology (TIST) , 2(3):1â€“27, 2011.
[15] VijaySekhar Chellaboina and Wassim M Haddad. Nonlinear dynamical systems and control: A
Lyapunov-based approach . Princeton University Press, 2008.
[16] Vishnu Pandi Chellapandi, Antesh Upadhyay, Abolfazl Hashemi, and Stanislaw H Zak. On the
convergence of decentralized federated learning under imperfect information sharing. arXiv
preprint arXiv:2303.10695 , 2023.
[17] Shuhang Chen, Adithya Devraj, Ana Busic, and Sean Meyn. Explicit mean-square error bounds
for monte-carlo and linear stochastic approximation. In International Conference on Artificial
Intelligence and Statistics , pages 4173â€“4183. PMLR, 2020.
[18] Wenlin Chen, Samuel HorvÃ¡th, and Peter RichtÃ¡rik. Optimal client sampling for federated
learning. Transactions on Machine Learning Research , 2022.
[19] Anna Choromanska, Mikael Henaff, Michael Mathieu, GÃ©rard Ben Arous, and Yann LeCun.
The loss surfaces of multilayer networks. In Artificial intelligence and statistics , pages 192â€“204,
2015.
[20] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and
Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-
convex optimization. In Advances in neural information processing systems , volume 27, 2014.
[21] Burgess Davis. On the intergrability of the martingale square function. Israel Journal of
Mathematics , 8:187â€“190, 1970.
[22] Bernard Delyon. Stochastic approximation with decreasing gain: Convergence and asymptotic
theory. Technical report, 2000.
[23] Bernard Delyon, Marc Lavielle, and Eric Moulines. Convergence of a stochastic approximation
version of the em algorithm. Annals of statistics , pages 94â€“128, 1999.
11[24] Thinh Doan, Siva Maguluri, and Justin Romberg. Finite-time analysis of distributed td (0)
with linear function approximation on multi-agent reinforcement learning. In International
Conference on Machine Learning , pages 1626â€“1635. PMLR, 2019.
[25] Vishwaraj Doshi, Jie Hu, and Do Young Eun. Self-repellent random walks on general graphsâ€“
achieving minimal sampling variance via nonlinear markov chains. In International Conference
on Machine Learning . PMLR, 2023.
[26] John C Duchi, Alekh Agarwal, Mikael Johansson, and Michael I Jordan. Ergodic mirror descent.
SIAM Journal on Optimization , 22(4):1549â€“1578, 2012.
[27] Martin Dyer, Alan Frieze, Ravi Kannan, Ajai Kapoor, Ljubomir Perkovic, and Umesh Vazirani.
A mildly exponential time algorithm for approximating the number of solutions to a multi-
dimensional knapsack problem. Combinatorics, Probability and Computing , 2(3):271â€“284,
1993.
[28] Mathieu Even. Stochastic gradient descent under markovian sampling schemes. In International
Conference on Machine Learning , 2023.
[29] Gersende Fort. Central limit theorems for stochastic approximation with controlled markov
chain dynamics. ESAIM: Probability and Statistics , 19:60â€“80, 2015.
[30] Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi. A general theory for client
sampling in federated learning. In International Workshop on Trustworthy Federated Learning
in conjunction with IJCAI , 2022.
[31] Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework
for clustered federated learning. In Proceedings of the 34th International Conference on Neural
Information Processing Systems , pages 19586â€“19597, 2020.
[32] Yuanxiong Guo, Ying Sun, Rui Hu, and Yanmin Gong. Hybrid local SGD for federated learning
with heterogeneous communications. In International Conference on Learning Representations ,
2022.
[33] P. Hall, C.C. Heyde, Z.W. Birnbauam, and E. Lukacs. Martingale Limit Theory and Its
Application . Communication and Behavior. Elsevier Science, 2014.
[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,
pages 770â€“778, 2016.
[35] Hadrien Hendrikx. A principled framework for the design and analysis of token algorithms. In
International Conference on Artificial Intelligence and Statistics , pages 470â€“489. PMLR, 2023.
[36] Roger A. Horn and Charles R. Johnson. Topics in Matrix Analysis . Cambridge University Press,
1991.
[37] Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G Brinton, Nicolo Michelusi,
Vaneet Aggarwal, David J Love, and Huaiyu Dai. Multi-stage hybrid federated learning over
large-scale d2d-enabled fog networks. IEEE/ACM Transactions on Networking , 2022.
[38] Jie Hu, Vishwaraj Doshi, and Do Young Eun. Efficiency ordering of stochastic gradient descent.
InAdvances in Neural Information Processing Systems , 2022.
[39] Jie Hu, Vishwaraj Doshi, and Do Young Eun. Accelerating distributed stochastic optimiza-
tion via self-repellent random walks. In The Twelfth International Conference on Learning
Representations , 2024.
[40] Mark Jerrum and Alistair Sinclair. The markov chain monte carlo method: an approach to
approximate counting and integration. Approximation Algorithms for NP-hard problems, PWS
Publishing , 1996.
[41] Thomas Kailath, Ali H Sayed, and Babak Hassibi. Linear estimation . Prentice Hall, 2000.
12[42] Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated rein-
forcement learning: Linear speedup under markovian sampling. In International Conference on
Machine Learning , pages 10997â€“11057, 2022.
[43] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A
unified theory of decentralized sgd with changing topology and local updates. In International
Conference on Machine Learning , pages 5381â€“5393, 2020.
[44] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, 2009.
[45] Harold Kushner and G George Yin. Stochastic approximation and recursive algorithms and
applications , volume 35. Springer Science & Business Media, 2003.
[46] Anusha Lalitha, Shubhanshu Shekhar, Tara Javidi, and Farinaz Koushanfar. Fully decentralized
federated learning. In Advances in neural information processing systems , 2018.
[47] Batiste Le Bars, AurÃ©lien Bellet, Marc Tommasi, Erick Lavoie, and Anne-Marie Kermarrec.
Refined convergence and topology learning for decentralized sgd with heterogeneous data. In
International Conference on Artificial Intelligence and Statistics , pages 1672â€“1702. PMLR,
2023.
[48] Chul-Ho Lee, Xin Xu, and Do Young Eun. Beyond random walk and metropolis-hastings
samplers: why you should not backtrack for unbiased graph sampling. ACM SIGMETRICS
Performance evaluation review , 40(1):319â€“330, 2012.
[49] Jure Leskovec and Andrej Krevl. Snap datasets: Stanford large network dataset collection,
2014.
[50] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence
of fedavg on non-iid data. In International Conference on Learning Representations , 2020.
[51] Xiang Li, Jiadong Liang, Xiangyu Chang, and Zhihua Zhang. Statistical estimation and online
inference via local sgd. In Proceedings of Thirty Fifth Conference on Learning Theory , volume
178 of Proceedings of Machine Learning Research , pages 1613â€“1661, 02â€“05 Jul 2022.
[52] Xiang Li, Jiadong Liang, and Zhihua Zhang. Online statistical inference for nonlinear stochastic
approximation with markovian data. arXiv preprint arXiv:2302.07690 , 2023.
[53] Yunming Liao, Yang Xu, Hongli Xu, Lun Wang, and Chen Qian. Adaptive configuration for
heterogeneous participants in decentralized federated learning. In IEEE INFOCOM 2023 . IEEE,
2023.
[54] Adwaitvedant S Mathkar and Vivek S Borkar. Nonlinear gossip. SIAM Journal on Control and
Optimization , 54(3):1535â€“1557, 2016.
[55] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial
intelligence and statistics , pages 1273â€“1282. PMLR, 2017.
[56] Sean Meyn. Control systems and reinforcement learning . Cambridge University Press, 2022.
[57] Antonietta Mira. Ordering and improving the performance of monte carlo markov chains.
Statistical Science , pages 340â€“350, 2001.
[58] Gemma Morral, Pascal Bianchi, and Gersende Fort. Success and failure of adaptation-diffusion
algorithms with decaying step size in multiagent networks. IEEE Transactions on Signal
Processing , 65(11):2798â€“2813, 2017.
[59] Wenlong Mou, Chris Junchi Li, Martin J Wainwright, Peter L Bartlett, and Michael I Jordan. On
linear stochastic approximation: Fine-grained polyak-ruppert and non-asymptotic concentration.
InConference on Learning Theory , pages 2947â€“2997. PMLR, 2020.
13[60] Giovanni Neglia, Chuan Xu, Don Towsley, and Gianmarco Calbi. Decentralized gradient
methods: does topology matter? In International Conference on Artificial Intelligence and
Statistics , pages 2348â€“2358. PMLR, 2020.
[61] Alex Olshevsky. Asymptotic network independence and step-size for a distributed subgradient
method. Journal of Machine Learning Research , 23(69):1â€“32, 2022.
[62] Shi Pu, Alex Olshevsky, and Ioannis Ch Paschalidis. Asymptotic network independence in
distributed stochastic optimization for machine learning: Examining distributed and centralized
stochastic gradient descent. IEEE signal processing magazine , 37(3):114â€“122, 2020.
[63] Christian P Robert, George Casella, and George Casella. Monte Carlo statistical methods ,
volume 2. Springer, 1999.
[64] Sheldon M Ross, John J Kelly, Roger J Sullivan, William James Perry, Donald Mercer, Ruth M
Davis, Thomas Dell Washburn, Earl V Sager, Joseph B Boyce, and Vincent L Bristow. Stochastic
processes , volume 2. Wiley New York, 1996.
[65] Itay Safran and Ohad Shamir. How good is sgd with random shuffling? In Conference on
Learning Theory , pages 3250â€“3284. PMLR, 2020.
[66] R. Srikant. Rates of Convergence in the Central Limit Theorem for Markov Chains, with an
Application to TD Learning. arXiv e-prints , page arXiv:2401.15719, January 2024.
[67] Sebastian U Stich. Local sgd converges fast and communicates little. In International Conference
on Learning Representations , 2018.
[68] Tao Sun, Dongsheng li, and Bao Wang. On the decentralized stochastic gradient descent with
markov chain sampling. IEEE Transactions on Signal Processing , PP, 07 2023.
[69] Tao Sun, Yuejiao Sun, and Wotao Yin. On markov chain gradient descent. In Advances in
neural information processing systems , volume 31, 2018.
[70] M Vidyasagar. Convergence of stochastic approximation via martingale and converse lyapunov
methods. arXiv preprint arXiv:2205.01303 , 2022.
[71] Hoi-To Wai. On the convergence of consensus algorithms with markovian noise and gradient
bias. In 2020 59th IEEE Conference on Decision and Control (CDC) , pages 4897â€“4902. IEEE,
2020.
[72] Han Wang, Aritra Mitra, Hamed Hassani, George J Pappas, and James Anderson. Federated tem-
poral difference learning with linear function approximation under environmental heterogeneity.
arXiv preprint arXiv:2302.02212 , 2023.
[73] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling the
objective inconsistency problem in heterogeneous federated optimization. In Advances in
Neural Information Processing Systems , volume 33, pages 7611â€“7623, 2020.
[74] Shiqiang Wang and Mingyue Ji. A unified analysis of federated learning with arbitrary client
participation. In Advances in neural information processing systems , 2022.
[75] Stephan Wojtowytsch. Stochastic gradient descent with noise of machine learning type part i:
Discrete time analysis. Journal of Nonlinear Science , 33(3):45, 2023.
[76] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan
Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In
International Conference on Machine Learning , pages 10334â€“10343. PMLR, 2020.
[77] Hao Ye, Le Liang, and Geoffrey Ye Li. Decentralized federated learning with unreliable
communications. IEEE Journal of Selected Topics in Signal Processing , 16(3):487â€“500, 2022.
[78] Chulhee Yun, Shashank Rajput, and Suvrit Sra. Minibatch vs local SGD with shuffling: Tight
convergence bounds and beyond. In International Conference on Learning Representations ,
2022.
14[79] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Open problem: Can single-shuffle sgd be better
than reshuffling sgd and gd? In Proceedings of Thirty Fourth Conference on Learning Theory ,
volume 134, pages 4653â€“4658. PMLR, Aug 2021.
[80] Sihan Zeng, Thinh T Doan, and Justin Romberg. Finite-time convergence rates of decentralized
stochastic approximation with applications in multi-agent and multi-task learning. IEEE
Transactions on Automatic Control , 2022.
[81] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582 , 2018.
15A Discussion of Assumption 2.3-ii)
A.1 Suitable choices of Kl
When wet let Klâˆ¼log(l)(resp. Klâˆ¼log log( l)), as suggested by [ 51], it trivially satisfies
KÏ„n=o(Î³âˆ’1/2(L+1)
n ) =o(n1/2(L+1))since by definition KÏ„n< K nâˆ¼log(n)(resp. log log( n)),
andlog(n) =o(nÏµ)(resp. log log( n) =o(nÏµ)) for any Ïµ >0. Besides,P
nÎ·2
n=P
nÎ³2
nK2(L+1)
Ï„nâ‰²P
nnâˆ’2n2(L+1)Ïµ=P
nn2(L+1)Ïµâˆ’2. To ensureP
nÎ·2
n<âˆž, it is sufficient to have 2(L+1)Ïµâˆ’2<
âˆ’1, or equivalently, Ïµ <1/2(L+ 1) . Since Ïµcan be arbitrarily small to satisfy the condition,P
nÎ·2
n<âˆžis satisfied. When Klâˆ¼log(l), we can rewrite the last condition as
Î·nl+1
Î·nl+1+1=Î³nl+1
Î³nl+1+1KL+1
l
KL+1
l+1=nl+1+ 1
nl+ 1log(l+ 1) + 1
log(l) + 1L+1
=
1 +Kl+1
nl+ 1log(l+ 1) + 1
log(l) + 1L+1
,(16)
where we have nlâˆ¼log(l!)such that Kl+1/nl= log( l+1)/log(l!)â†’0andlog(l+1)/log(l)â†’1
aslâ†’ âˆž , which leads to limnâ†’âˆžÎ·nl+1/Î·nl+1+1= 1. Similarly, for Klâˆ¼log log( l), we
havenlâˆ¼log(Ql
s=1log(s))such that Kl+1/nlâˆ¼log log( l+ 1)/log log(Ql
s=1log(s))â†’0and
log log( l+ 1)/log log( l)â†’1aslâ†’ âˆž , which also leads to limnâ†’âˆžÎ·nl+1/Î·nl+1+1= 1.
A.2 Practical implications of Assumption 2.3-ii)
In this assumption, we allow the number of local iterations to go to infinity asymptotically. In
distributed learning environments such as mobile, IoT, and wireless sensor networks, where nodes
are often constrained by battery life, increasing communication interval in Assumption 2.3-ii) plays
a crucial role in balancing energy costs with communication effectiveness. It allows agents to
communicate more frequently early on, leading to a faster initial convergence to the neighborhood of
Î¸âˆ—. Then, we slow down the communication frequency between agents to conserve energy, leveraging
the diminishing returns on accuracy improvements from additional communications.
Consider the scenario where devices across multiple clusters collaborate on a distributed optimization
task, utilizing local datasets. Devices within each cluster form a communication network that allows
a virtual agent to perform a heterogeneous Markov chain trajectory via random walk, or an i.i.d.
sequence in a complete graph with self-loops, depending on the application context. Each cluster
features an edge server that supports the exchange of model estimates with neighboring clusters. By
performing Klocal updates before uploading these to the clusterâ€™s edge server, the model benefits
from reduced communication overhead. As the frequency of updates between devices and edge
servers decreases â€” optimized by gradually increasing Kâ€” we effectively lower communication
costs, particularly as the model estimation Î¸nis close to Î¸âˆ—.
B Proof of Lemma 3.1
LetJâŠ¥â‰œINâˆ’JâˆˆRNÃ—NandJâŠ¥â‰œJâŠ¥âŠ—IdâˆˆRNdÃ—Nd, where âŠ—is the Kronecker product.
LetÎ˜n= [(Î¸1
n)T,Â·Â·Â·,(Î¸N
n)T]TâˆˆRNd. Then, motivated by [ 58], we define a sequence Ï•nâ‰œ
Î·âˆ’1
n+1JâŠ¥Î˜nâˆˆRNdin the increasing communication interval case (resp. Ï•nâ‰œÎ³âˆ’1
n+1JâŠ¥Î˜nin the
bounded communication interval case ), where Î·n+1is defined in Assumption 2.3-ii). JâŠ¥Î˜n=
Î˜nâˆ’1
N(11TâŠ—Id)Î˜nrepresents the consensus error of the model.
We first give the following lemma that shows the pathwise boundedness of Ï•n.
Lemma B.1. Let Assumptions 2.1, 2.3, 2.4 and 2.5 hold. For any compact set â„¦âŠ‚RNd, the
sequence Ï•nsatisfies supnE[âˆ¥Ï•nâˆ¥21âˆ©jâ‰¤nâˆ’1{Î˜jâˆˆâ„¦}]<âˆž.
Lemma B.1 and Assumption 2.3-ii) imply that for any nâ‰¥0,E[âˆ¥JâŠ¥Î˜nâˆ¥21âˆ©jâ‰¤nâˆ’1{Î˜jâˆˆâ„¦}] =
Î·2
n+1E[âˆ¥Ï•nâˆ¥21âˆ©jâ‰¤nâˆ’1{Î˜jâˆˆâ„¦}]â‰¤CÎ·2
n+1for some constant Cthat depends on C1andâ„¦. Along
with Assumption 2.4 such that âˆ¥JâŠ¥Î˜nâˆ¥is always bounded per each trajectory, it means
âˆ¥JâŠ¥Î˜nâˆ¥ 1âˆ©jâ‰¤nâˆ’1{Î˜jâˆˆâ„¦}=O(Î·n)a.s.
16Let{â„¦m}mâ‰¥0be a sequence of increasing compact subset of RNdsuch thatS
mâ„¦m=RNd. Then,
we know that for any mâ‰¥0,
âˆ¥JâŠ¥Î˜nâˆ¥ 1âˆ©jâ‰¤nâˆ’1{Î˜jâˆˆâ„¦m}=O(Î·n)a.s. (17)
(17) indicates either one of the following two cases:
â€¢there exists some trajectory-dependent index mâ€²such that each trajectory {Î˜n}nâ‰¥0is
always within the compact set â„¦mâ€², i.e., 1âˆ©jâ‰¤n{Î˜jâˆˆâ„¦mâ€²}= 1(satisfied by the construction
of increasing compact sets {â„¦m}mâ‰¥0and Assumption 2.4), and we have âˆ¥JâŠ¥Î˜nâˆ¥=O(Î·n)
such that limnâ†’âˆžJâŠ¥Î˜n=0;
â€¢Î˜nwill escape the compact set â„¦meventually for any mâ‰¥0in finite time such that
1âˆ©jâ‰¤nâˆ’1{Î˜jâˆˆâ„¦m}= 0when nis large enough.
We can see the second case contradicts Assumption 2.4 because we assume every trajectory {Î˜n}nâ‰¥0
is within some compact set. Therefore, (17) for any mâ‰¥0is equivalent to showing âˆ¥JâŠ¥Î˜nâˆ¥=
O(Î·n)andlimnâ†’âˆžJâŠ¥Î˜n=0. Under Assumption 2.3-i) we can obtain similar result âˆ¥JâŠ¥Î˜nâˆ¥=
O(Î³n)by following the same steps as above, which completes the proof of Lemma 3.1.
Proof of Lemma B.1. We begin by rewriting (6) in the matrix form,
Î˜n+1=Wn(Î˜nâˆ’Î³n+1âˆ‡F(Î˜n,Xn)), (18)
where Xnâ‰œ(X1
n, X2
n,Â·Â·Â·, XN
n)andâˆ‡F(Î˜n,Xn)â‰œ[âˆ‡F1(Î¸1
n, X1
n)T,Â·Â·Â·,âˆ‡FN(Î¸N
n, XN
n)T]Tâˆˆ
RNd. Recall Î¸nâ‰œ1
NPN
i=1Î¸i
nâˆˆRdand we have [Î¸T
n,Â·Â·Â·, Î¸T
n]T=1
N(11TâŠ—Id)Î˜nâˆˆRNd.
Case 1 (Increasing communication interval KÏ„n):By left multiplying (18) with1
N(11TâŠ—Id),
along with Î³n+1=Î·n+1/KL+1
Ï„n+1in Assumption 2.3-ii), we have the following iteration
1
N(11TâŠ—Id)Î˜n+1=1
N(11TâŠ—Id)Î˜nâˆ’Î·n+11
N(11TâŠ—Id)âˆ‡F(Î˜n,Xn)
KL+1Ï„n+1, (19)
where the equality comes from1
N(11TâŠ—Id)Wn=1
N(11TWnâŠ—Id) =1
N(11TâŠ—Id). With (18)
and (19), we have
Î˜n+1âˆ’1
N(11TâŠ—Id)Î˜n+1
=
Wnâˆ’1
N(11TâŠ—Id)
Î˜nâˆ’Î·n+1
Wnâˆ’1
N(11TâŠ—Id)âˆ‡F(Î˜n,Xn)
KL+1Ï„n+1
=(JâŠ¥WnâŠ—Id)JâŠ¥Î˜nâˆ’Î·n+1(JâŠ¥WnâŠ—Id)âˆ‡F(Î˜n, Xn)
KL+1Ï„n+1
=Î·n+1(JâŠ¥WnâŠ—Id) 
Î·âˆ’1
n+1JâŠ¥Î˜nâˆ’âˆ‡F(Î˜n,Xn)
KL+1Ï„n+1!
,(20)
where the second equality comes from Wnâˆ’1
N(11TâŠ—Id) = (Wnâˆ’1
N11T)âŠ—Id=JâŠ¥WnâŠ—Id
and(JâŠ¥WnâŠ—Id)JâŠ¥=JâŠ¥WnJâŠ¥âŠ—Id=JâŠ¥WnâŠ—Id. Let anâ‰œÎ·n/Î·n+1, dividing both sides
of (20) by Î·n+2gives
Ï•n+1=an+1(JâŠ¥WnâŠ—Id) 
Ï•nâˆ’âˆ‡F(Î˜n,Xn)
KL+1Ï„n+1!
. (21)
17Define the filtration {Fn}nâ‰¥0asFnâ‰œÏƒ{Î˜0,X0,W0,Î˜1,X1,W1,Â·Â·Â·,Xnâˆ’1,Wnâˆ’1,Î˜n,Xn}.
Recursively computing (21) w.r.t the time interval [nl, nl+1]gives
Ï•nl+1="nl+1Y
k=nl+1ak# "
JâŠ¥nl+1âˆ’1Y
k=nlWk#
âŠ—Id!
Ï•nl
âˆ’nl+1âˆ’1X
k=nl"nl+1Y
i=k+1ai# "
JâŠ¥nl+1âˆ’1Y
i=kWi#
âŠ—Id!
âˆ‡F(Î˜k,Xk)
KL+1
l+1
=Î·nl+1
Î·nl+1+1(JâŠ¥WnlâŠ—Id)Ï•nlâˆ’nl+1âˆ’1X
k=nlÎ·nl+1
Î·k+2(JâŠ¥WnlâŠ—Id)âˆ‡F(Î˜k,Xk)
KL+1
l+1,(22)
whereQis the backward multiplier, the second equality comes from JâŠ¥WnJâŠ¥=JâŠ¥Wn
andWk=INfork /âˆˆ {nl}. In Assumption 2.5, we have âˆ¥EWâˆ¼Pnl[WTJâŠ¥W]âˆ¥=
âˆ¥EWâˆ¼Pnl[WTWâˆ’J]âˆ¥ â‰¤C1<1. Then,
E[âˆ¥Ï•nl+1âˆ¥2|Fnl]
=Î·nl+1
Î·nl+1+12
Ï•T
nlEWnlâˆ¼Pnlh
(JâŠ¥WnlâŠ—Id)T(JâŠ¥WnlâŠ—Id)i
Ï•nl
âˆ’2E"nl+1âˆ’1X
k=nlÎ·2
nl+1
Î·nl+1+1Î·k+2Ï•T
nl(JâŠ¥WnlâŠ—Id)T(JâŠ¥WnlâŠ—Id)âˆ‡F(Î˜k,Xk)
KL+1
l+1Fnl#
+Eï£®
ï£°nl+1âˆ’1X
k=nlÎ·nl+1
Î·k+2(JâŠ¥WnlâŠ—Id)âˆ‡F(Î˜k,Xk)
KL+1
l+12Fnlï£¹
ï£»
â‰¤Î·nl+1
Î·nl+1+12
Ï•T
nlEWnlâˆ¼Pnl 
WT
nlJâŠ¥WnlâŠ—Id
Ï•nl
âˆ’2Î·nl+1
Î·nl+1+12
E"nl+1âˆ’1X
k=nlÏ•T
nl 
WT
nlJâŠ¥WnlâŠ—Idâˆ‡F(Î˜k,Xk)
KL+1
l+1Fnl#
+Î·nl+1
Î·nl+1+12
Eï£®
ï£°(JâŠ¥WnlâŠ—Id)nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)
KL+1
l+12Fnlï£¹
ï£»
â‰¤Î·nl+1
Î·nl+1+12
C1âˆ¥Ï•nlâˆ¥2+ 2Î·nl+1
Î·nl+1+12
C1âˆ¥Ï•nlâˆ¥E"nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)
KL+1
l+1Fnl#
+Î·nl+1
Î·nl+1+12
C1Eï£®
ï£°nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)
KL+1
l+12Fnlï£¹
ï£»,(23)
where the first inequality comes from JT
âŠ¥JâŠ¥=JâŠ¥andÎ·k+2â‰¥Î·nl+1+1forkâˆˆ[nl, nl+1âˆ’1].
Then, we analyze the norm of the gradient âˆ¥âˆ‡F(Î˜k,Xk)âˆ¥in the second term on the RHS of (23)
conditioned on Fnl. By Assumption 2.4, we assume Î˜nlis within some compact set â„¦at time
nlsuch that supiâˆˆ[N],XiâˆˆXiâˆ‡Fi(Î¸i
nl, Xi)â‰¤Câ„¦for some constant Câ„¦. For n=nl+ 1and any
Xâˆˆ X1Ã— X2Ã— Â·Â·Â· Ã— X N, we have
âˆ¥âˆ‡F(Î˜nl+1,X)âˆ¥ â‰¤ âˆ¥âˆ‡ F(Î˜nl+1,X)âˆ’ âˆ‡F(Î˜nl,X)âˆ¥+âˆ¥âˆ‡F(Î˜nl,X)âˆ¥.
Considering âˆ¥âˆ‡F(Î˜nl,X)âˆ¥, we have supXâˆ¥âˆ‡F(Î˜nl,X)âˆ¥2â‰¤PN
i=1supXiâˆˆXiâˆ¥âˆ‡Fi(Î¸i
nl, Xi)âˆ¥2â‰¤NC2
â„¦such that âˆ¥âˆ‡F(Î˜nl,X)âˆ¥ â‰¤âˆš
NCâ„¦. In addition, we
18have
âˆ¥âˆ‡F(Î˜nl+1,X)âˆ’ âˆ‡F(Î˜nl,X)âˆ¥2=NX
i=1âˆ¥âˆ‡Fi(Î¸i
nl+1, Xi)âˆ’ âˆ‡Fi(Î¸i
nl, Xi)âˆ¥2
â‰¤NX
i=1L2âˆ¥Î¸i
nl+1âˆ’Î¸i
nlâˆ¥2
â‰¤NX
i=1Î³2
nl+1L2âˆ¥âˆ‡Fi(Î¸i
nl, Xi
nl)âˆ¥2
â‰¤Î³2
nl+1C2
â„¦NL2(24)
such that âˆ¥âˆ‡F(Î˜nl+1,X)âˆ’ âˆ‡F(Î˜nl,X)âˆ¥ â‰¤Î³nl+1Câ„¦âˆš
NL. Thus, for any X,
âˆ¥âˆ‡F(Î˜nl+1,X)âˆ¥ â‰¤(1 +Î³nl+1L)âˆš
NCâ„¦. (25)
Forn=nl+ 2and any X, we have
âˆ¥âˆ‡F(Î˜nl+2,X)âˆ¥ â‰¤ âˆ¥âˆ‡ F(Î˜nl+2,X)âˆ’ âˆ‡F(Î˜nl+1,X)âˆ¥+âˆ¥âˆ‡F(Î˜nl+1,X)âˆ¥.
Similar to the steps in (24), we have
âˆ¥âˆ‡F(Î˜nl+2,X)âˆ’ âˆ‡F(Î˜nl+1,X)âˆ¥2â‰¤NX
i=1Î³2
nl+2L2âˆ¥âˆ‡Fi(Î¸i
nl+1, Xi
nl+1)âˆ¥2
=Î³2
nl+2L2âˆ¥âˆ‡F(Î˜nl+1,Xnl+1)âˆ¥2.(26)
Then,âˆ¥âˆ‡F(Î˜nl+2,X)âˆ¥ â‰¤(1 +Î³nl+2L) supXâˆ¥âˆ‡F(Î˜nl+1,X)âˆ¥and, together with (25), we have
âˆ¥âˆ‡F(Î˜nl+2,X)âˆ¥ â‰¤(1 +Î³nl+2L)(1 + Î³nl+1L)âˆš
NCâ„¦. (27)
By induction, âˆ¥âˆ‡F(Î˜nl+m,X)âˆ¥ â‰¤Qm
s=1(1 +Î³nl+sL)âˆš
NCâ„¦formâˆˆ[1, Kl+1âˆ’1].
The next step is to analyze the growth rate ofQm
s=1(1 +Î³nl+sL). By1 +xâ‰¤exforxâ‰¥0, we have
mY
s=1(1 +Î³nl+sL)â‰¤eLPm
s=1Î³nl+s.
For step size Î³n= 1/n, we have LPm
s=1Î³nl+s=LPm
s=11/(nl+s)< LPm
s=11/s < L (log(m)+
1)such thatQm
s=1(1 +Î³nl+sL)<(em)L. Then,
nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)
KL+1
l+1â‰¤1
KL+1
l+1nl+1âˆ’1X
k=nlâˆ¥âˆ‡F(Î˜k,Xk)âˆ¥ â‰¤1
KL+1
l+1âˆš
NeLCâ„¦Kl+1âˆ’1X
m=0mL
â‰¤âˆš
NeLCâ„¦,(28)
where the last inequality comes fromPKl+1âˆ’1
m=0mL< K l+1(Kl+1âˆ’1)L< KL+1
l+1. We can see the
sum of the norm of the gradients are bounded byâˆš
NeLCâ„¦, which only depends on the compact set
â„¦at time n=nl.
LetÎ´1âˆˆ(C1,1). Since from Assumption 2.3-ii), limlâ†’âˆžÎ·nl+1/Î·nl+1+1= 1, there exists some
large enough l0such that (Î·nl+1
Î·nl+1+1)2C1< Î´1< Î´2:= (Î´1+ 1)/2<1for any l > l 0. Note that Î´1
depends only on C1and is independent of Fn. Then, let ËœCâ„¦:=âˆš
NeLCâ„¦, we can rewrite (23) as
E[âˆ¥Ï•nl+1âˆ¥2|Fnl]â‰¤Î´1âˆ¥Ï•nlâˆ¥2+ 2Î´1ËœCâ„¦âˆ¥Ï•nlâˆ¥+Î´1ËœC2
â„¦
â‰¤Î´2âˆ¥Ï•nlâˆ¥2+Mâ„¦,(29)
where Mâ„¦satisfies Mâ„¦>8ËœC2
â„¦/(1âˆ’Î´1) +Î´1ËœC2
â„¦, which is derived from rearranging (29) as
Mâ„¦â‰¥(Î´1âˆ’Î´2)âˆ¥Ï•nlâˆ¥2+ 2Î´1ËœCâ„¦âˆ¥Ï•nlâˆ¥+Î´1ËœC2
â„¦and upper bounding the RHS. Upon noting that
1âˆ©jâ‰¤nl{Î˜jâˆˆâ„¦}â‰¤ 1âˆ©jâ‰¤nlâˆ’1{Î˜jâˆˆâ„¦}, we obtain
Eh
âˆ¥Ï•nl+1âˆ¥21âˆ©jâ‰¤nl{Î˜jâˆˆâ„¦}i
â‰¤Î´2Eh
âˆ¥Ï•nlâˆ¥21âˆ©jâ‰¤nlâˆ’1{Î˜jâˆˆâ„¦}i
+Mâ„¦. (30)
19The induction leads to E[âˆ¥Ï•nl+1âˆ¥21âˆ©jâ‰¤nl{Î˜jâˆˆâ„¦}]â‰¤Î´nl+1âˆ’nl0
2 E[âˆ¥Ï•nl0âˆ¥21âˆ©jâ‰¤nl0âˆ’1{Î˜jâˆˆâ„¦}] +
M/(1âˆ’Î´2)<âˆžfor any lâ‰¥l0. Besides, for mâˆˆ(nl, nl+1), by following the above steps
(23) applied to (21), we have
E[âˆ¥Ï•mâˆ¥2|Fnl]â‰¤Î·nl+1
Î·m+12
âˆ¥Ï•nlâˆ¥2+ 2Î·nl+1
Î·m+12
âˆ¥Ï•nlâˆ¥E"mâˆ’1X
k=nlâˆ‡F(Î˜k,Xk)
KL+1
l+1Fnl#
+Î·nl+1
Î·m+12
Eï£®
ï£°mâˆ’1X
k=nlâˆ‡F(Î˜k,Xk)
KL+1
l+12Fnlï£¹
ï£».(31)
By(28) we already show that âˆ¥Pnl+1âˆ’1
k=nlâˆ‡F(Î˜k,Xk)
KL+1
l+1âˆ¥<âˆžconditioned on Fnl. Therefore,
E[âˆ¥Ï•mâˆ¥21âˆ©jâ‰¤nl{Î˜jâˆˆâ„¦}]<âˆžformâˆˆ(nl, nl+1). This completes the boundedness analysis of
E[âˆ¥Ï•nâˆ¥21âˆ©jâ‰¤nâˆ’1{Î˜jâˆˆâ„¦}].
Case 2 (Bounded communication interval KÏ„nâ‰¤K):In this case, we do not need the auxiliary
step size Î·nand can directly work on Î³n= 1/naforaâˆˆ(0.5,1]. Similar to (20), we have
Î˜n+1âˆ’1
N(11TâŠ—Id)Î˜n+1=Î³n+1(JâŠ¥WnâŠ—Id) 
Î³âˆ’1
n+1JâŠ¥Î˜nâˆ’ âˆ‡F(Î˜n,Xn)
, (32)
and let bnâ‰œÎ³n/Î³n+1, dividing both sides of above equation by Î³n+2gives
Ï•n+1=bn+1(JâŠ¥WnâŠ—Id) (Ï•nâˆ’ âˆ‡F(Î˜n,Xn)). (33)
Then, by following the similar steps in (22) and (23), we obtain
E[âˆ¥Ï•nl+1âˆ¥2|Fnl]â‰¤Î³nl+1
Î³nl+1+12
C1 
âˆ¥Ï•nlâˆ¥2+ 2âˆ¥Ï•nlâˆ¥E"nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)Fnl#
+Eï£®
ï£°nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)2Fnlï£¹
ï£»!
.(34)
Also similar to (25) - (28), we can bound the sum of the norm of the gradients as
nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)â‰¤nl+1âˆ’1X
k=nl"kY
s=nl(1 +Î³s+1L)#
âˆš
NCâ„¦. (35)
Now that Klis bounded above by K,Qk
s=nl(1 +Î³s+1L)â‰¤eLPk
s=nlÎ³s+1< eLPKâˆ’1
s=0Î³s+1:=CK.
Then, we further bound (35) as
nl+1âˆ’1X
k=nlâˆ‡F(Î˜k,Xk)â‰¤âˆš
NKC KCâ„¦. (36)
The subsequent proof is basically a replication of (29) - (31) and is therefore omitted.
C Proof of Theorem 3.2
We focus on analyzing the convergence property of Î¸, which is obtained by left multiplying (18) with
1
N(1TâŠ—Id), i.e.,
Î¸n+1=1
N(1TâŠ—Id)Î¸n+1
=Î¸nâˆ’Î³n+11
N(1TâŠ—Id)âˆ‡F(Î˜n,Xn).(37)
where the second equality comes from Wnbeing doubly stochastic and1
N(1TâŠ—Id)Wn=
1
N(1TWnâŠ—Id) =1
N(1TâŠ—Id).
For self-contained purpose, we first give the almost sure convergence result for the stochastic
approximation that will be used in our proof.
20Theorem C.1 (Theorem 2 [23]) .Consider the stochastic approximation in the form of
Î¸n+1=Î¸n+Î³n+1h(Î¸n) +Î³n+1en+1+Î³n+1rn+1. (38)
Assume that
C1. w.p.1, the closure of {Î¸n}nâ‰¥0is a compact subset of Rd;
C2.{Î³n}is a decreasing sequence of positive number such thatP
nÎ³n=âˆž;
C3. w.p.1, limpâ†’âˆžPp
n=1Î³nenexists and is finite. Moreover, limnâ†’âˆžrn= 0.
C4. vector-valued function his continuous on Rdand there exists a continuously differentiable
function V:Rdâ†’Rsuch that âŸ¨âˆ‡V(Î¸), h(Î¸)âŸ© â‰¤0for all Î¸âˆˆRd. Besides, the interior of
V(L)is empty where Lâ‰œ{Î¸âˆˆRd:âŸ¨âˆ‡V(Î¸), h(Î¸)âŸ©= 0}.
Then, w.p.1, lim supnd(Î¸n,L) = 0 .
We can rewrite (37) as
Î¸n+1=Î¸nâˆ’Î³n+11
N(1TâŠ—Id)âˆ‡F(Î˜n,Xn)
=Î¸nâˆ’Î³n+1âˆ‡f(Î¸n)âˆ’Î³n+1 
1
NNX
i=1âˆ‡fi(Î¸i
n)âˆ’ âˆ‡f(Î¸n)!
âˆ’Î³n+1 
1
NNX
i=1âˆ‡Fi(Î¸i
n, Xi
n)âˆ’1
NNX
i=1âˆ‡fi(Î¸i
n)!
,(39)
and work on the converging behavior of the third and fourth term. By definition of function âˆ‡f(Â·),
we have
rnâ‰œ1
NNX
i=1âˆ‡fi(Î¸i
n)âˆ’ âˆ‡f(Î¸n) =1
NNX
i=1
âˆ‡fi(Î¸i
n)âˆ’ âˆ‡fi(Î¸n)
. (40)
By the Lipschitz continuity of function âˆ‡Fi(Â·, X)in (7), we have
âˆ¥rnâˆ¥ â‰¤1
NNX
i=1Lâˆ¥Î¸i
nâˆ’Î¸nâˆ¥ â‰¤Lâˆš
NÎ˜nâˆ’1
N(11TâŠ—Id)Î˜n=Lâˆš
Nâˆ¥JâŠ¥Î˜nâˆ¥, (41)
where the second inequality comes from the Cauchy-Schwartz inequality. In Appendix B, we have
shown limnJâŠ¥Î˜n=0almost surely such that limnâ†’âˆžrn= 0almost surely.
Next, we further decompose the fourth term in (39). For an ergodic transition matrix Pand a function
vassociated with the same state space X, define the operator Pkv(x)â‰œP
yâˆˆXPk(x, y)v(y)for the
k-step transition probability Pk(x, y). Denote by P1,Â·Â·Â·,PNthe underlying transition matrices
of all Nagents with corresponding stationary distribution Ï€1,Â·Â·Â·,Ï€N. Then, for every function
âˆ‡Fi(Î¸i,Â·) :Xiâ†’Rd, there exists a corresponding function mÎ¸i(Â·) :Xiâ†’Rdsuch that
mÎ¸i(x)âˆ’PimÎ¸i(x) =âˆ‡Fi(Î¸i, x)âˆ’ âˆ‡fi(Î¸i). (42)
The solution of the Poisson equation (42) has been studied in the literature, e.g., [ 17,38]. For
self-contained purpose, we derive the closed-form mÎ¸i(x)from scratch. First of all, we can obtain
function mÎ¸i(x)in the recursive form as follows,
mÎ¸i(x) =âˆ‡Fi(Î¸i, x)âˆ’âˆ‡fi(Î¸i)+Pi[âˆ‡Fi(Î¸i,Â·)âˆ’âˆ‡fi(Î¸i)](x)+P2
i[âˆ‡Fi(Î¸i,Â·)âˆ’âˆ‡fi(Î¸i)](x)+Â·Â·Â·.
(43)
It is not hard to check that (43) satisfies (42). Note that by induction we get
Pk
iâˆ’1(Ï€i)T= 
Piâˆ’1(Ï€i)Tk,âˆ€kâˆˆN, kâ‰¥1. (44)
21Then, we can further simplify (43), and the closed-form expression of mÎ¸i(x)is given as
mÎ¸i(x) =X
yâˆˆXi
Piâˆ’1(Ï€i)T0(x, y)(âˆ‡Fi(Î¸i, y)âˆ’ âˆ‡fi(Î¸i))
+X
yâˆˆXi
P1
iâˆ’1(Ï€i)T
(x, y)(âˆ‡Fi(Î¸i, y)âˆ’ âˆ‡fi(Î¸i)) +Â·Â·Â·
=X
yâˆˆXi"âˆžX
k=0
Piâˆ’1(Ï€i)Tk#
(x, y)(âˆ‡Fi(Î¸i, y)âˆ’ âˆ‡fi(Î¸i))
=X
yâˆˆXi 
Iâˆ’Pi+1(Ï€i)Tâˆ’1(x, y)(âˆ‡Fi(Î¸i, y)âˆ’ âˆ‡fi(Î¸i)),(45)
where the fourth equality comes from (44). Note that the so-called â€˜fundamental matrixâ€™ (Iâˆ’Pi+
1(Ï€i)T)âˆ’1exists for every ergodic Markov chain Xifrom Assumption 2.2. Since function âˆ‡Fiis
Lipschitz continuous, we have the following lemma.
Lemma C.2. Under assumption (A1), functions mÎ¸i(x)andPimÎ¸i(x)are both Lipschitz continuous
inÎ¸ifor any xâˆˆ Xi.
Proof. By (45), for any Î¸i
1, Î¸i
2âˆˆRdandxâˆˆ Xi, we have
mÎ¸i
1(x)âˆ’mÎ¸i
2(x)â‰¤X
yâˆˆXi 
Iâˆ’Pi+1(Ï€i)Tâˆ’1(x, y)
âˆ‡Fi(Î¸i
1, y)âˆ’ âˆ‡Fi(Î¸i
2, y)
+âˆ‡fi(Î¸i
1)âˆ’ âˆ‡fi(Î¸i
2)
â‰¤Cimax
yâˆˆXiâˆ‡Fi(Î¸i
1, y)âˆ’ âˆ‡Fi(Î¸i
2, y)+âˆ‡fi(Î¸i
1)âˆ’ âˆ‡fi(Î¸i
2)
â‰¤(CiL+ 1)âˆ¥Î¸i
1âˆ’Î¸i
2âˆ¥,(46)
where the second inequality holds for a constant Cithat is the largest absolute value of the entry
in the matrix (Iâˆ’Pi+1(Ï€i)T)âˆ’1. Therefore, mÎ¸i(x)is Lipschitz continuous in Î¸i. Moreover,
following the similar steps as above, we have
PimÎ¸i
1(x)âˆ’PimÎ¸i
2(x)=X
yâˆˆXiPi(x, y)mÎ¸i
1(y)âˆ’X
yâˆˆXiPi(x, y)mÎ¸i
2(y)
=X
yâˆˆXiPi(x, y)
mÎ¸i
1(y)âˆ’mÎ¸i
2(y)
â‰¤X
yâˆˆXiPi(x, y)mÎ¸i
1(y)âˆ’mÎ¸i
2(y)
â‰¤ |X i|mÎ¸i
1(y)âˆ’mÎ¸i
2(y)
â‰¤ |X i|(CiL+ 1)âˆ¥Î¸i
1âˆ’Î¸i
2âˆ¥(47)
such that PimÎ¸i(x)is also Lipschitz continuous in Î¸i, which comletes the proof.
Now with (42) we can decompose âˆ‡Fi(Î¸i
n, Xi
n)âˆ’ âˆ‡fi(Î¸i
n)as
âˆ‡Fi(Î¸i
n, Xi
n)âˆ’ âˆ‡fi(Î¸i
n) =mÎ¸in(Xi
n)âˆ’PimÎ¸in(Xi
n)
=mÎ¸in(Xi
n)âˆ’PimÎ¸in(Xi
nâˆ’1)
| {z }
ei
n+1
+PimÎ¸in(Xi
nâˆ’1)
|{z }
Î½inâˆ’PimÎ¸i
n+1(Xi
n)
|{z }
Î½i
n+1
+PimÎ¸i
n+1(Xi
n)âˆ’PimÎ¸in(Xi
n)
| {z }
Î¾i
n+1.(48)
22Here{Î³nei
n}is a Martingale difference sequence and we need the martingale convergence theorem
in Theorem C.3.
Theorem C.3 (Theorem 6.4.6[64]).For an Fn-Martingale Sn, setXnâˆ’1=Snâˆ’Snâˆ’1. If for some
1â‰¤pâ‰¤2,
âˆžX
n=1E[âˆ¥Xnâˆ’1âˆ¥p|Fnâˆ’1]<âˆža.s. (49)
thenSnconverges almost surely.
We want to show thatP
nÎ³2
n+1E[âˆ¥ei
n+1âˆ¥2|Fn]<âˆžsuch thatP
nÎ³nei
nconverges almost surely
by Theorem C.3. As we can see in (45), with Lemma C.2 and Assumption 2.4, for a sample
path ( Î˜nwithin a compact set â„¦),supnâˆ¥mÎ¸in(x)âˆ¥<âˆžandsupnâˆ¥PimÎ¸in(x)âˆ¥<âˆžalmost
surely for all xâˆˆ Xi. This ensures that ei
n+1is anL2-bounded martingale difference sequence, i.e.,
supnâˆ¥ei
n+1âˆ¥ â‰¤supn(âˆ¥mÎ¸i
n(Xi
n+1)âˆ¥+âˆ¥PimÎ¸i
n(Xi
n)âˆ¥)â‰¤Dâ„¦<âˆž. Together with Assumption 2.3,
we get
X
nÎ³2
n+1E[âˆ¥ei
n+1âˆ¥2|Fn]â‰¤Dâ„¦X
nÎ³2
n+1<âˆža.s. (50)
and thusP
nÎ³nei
nconverges almost surely.
Next, for the term Î½i
nwe have
pX
k=0Î³k+1(Î½i
kâˆ’Î½i
k+1) =pX
k=0(Î³k+1âˆ’Î³k)Î½i
k+Î³0Î½i
0âˆ’Î³p+1Î½i
p+1. (51)
As is shown before, for a given sample path, âˆ¥PimÎ¸in(x)âˆ¥is bounded almost surely for all nand
xâˆˆ Xisuch that supnâˆ¥Î½i
nâˆ¥<âˆžalmost surely. Since limnâ†’âˆž(Î³n+1âˆ’Î³n) = 0 , we have
limnâ†’âˆž(Î³n+1âˆ’Î³n)Î½i
n= 0. Note that there exists a path-dependent constant C(that bounds âˆ¥Î½i
nâˆ¥)
such that for any nâ‰¥m,
nX
k=m(Î³k+1âˆ’Î³k)Î½i
kâ‰¤CnX
k=m(Î³kâˆ’Î³k+1) =C(Î³mâˆ’Î³n+1)< CÎ³ m. (52)
Since limnâ†’âˆžÎ³n= 0, there exists a positive integer Msuch that for all nâ‰¥mâ‰¥M,Î³m< Ïµ/C
andâˆ¥Pn
k=m(Î³k+1âˆ’Î³k)Î½i
kâˆ¥< Ïµfor every Ïµ >0. Therefore, {Pp
k=0(Î³k+1âˆ’Î³k)Î½i
k}pâ‰¥0is a Cauchy
sequence andPâˆž
k=0(Î³k+1âˆ’Î³k)Î½i
kconverges by Cauchy convergence criterion. The last term of
(51) tends to zero. Therefore,Pâˆž
k=0Î³k+1(Î½i
kâˆ’Î½i
k+1)converges and is finite.
For the last term Î¾i
n, Lemma C.2 leads to
1
NNX
i=1Î¾i
n+1â‰¤Câ€²
NNX
i=1âˆ¥Î¸i
n+1âˆ’Î¸i
nâˆ¥ â‰¤Câ€²
âˆš
Nâˆ¥Î˜n+1âˆ’Î˜nâˆ¥. (53)
for the Lipschitz constant Câ€²ofPimÎ¸i(x). However, the relationship between Î¸nandÎ¸n+1is
not obvious in the D-SGD and FL setting due to the update rule (18) with communication matrix
Wn, unlike the classical stochastic approximation shown in (38). We come up with the novel
decomposition of Î¾i
n, which takes the consensus error into account, to solve this issue, i.e.,
Î¾i
n+1=h
PimÎ¸i
n+1(Xi
n)âˆ’PimÎ¸n+1(Xi
n)i
+
PimÎ¸n(Xi
n)âˆ’PimÎ¸in(Xi
n)
+
PimÎ¸n+1(Xi
n)âˆ’PimÎ¸n(Xi
n)
.(54)
23Using the Lipschitzness property of PimÎ¸(X)in Lemma C.2, we have
1
NNX
i=1Î¾i
n+1â‰¤Câ€²
NNX
i=1 Î¸i
n+1âˆ’Î¸n+1+âˆ¥Î¸n+1âˆ’Î¸nâˆ¥+Î¸nâˆ’Î¸i
n
â‰¤Câ€²
âˆš
NÎ˜n+1âˆ’1
N 
11TâŠ—Id
Î˜n+1+Câ€²
âˆš
NÎ˜nâˆ’1
N 
11TâŠ—Id
Î˜n
+Câ€²âˆ¥Î¸n+1âˆ’Î¸nâˆ¥
=Câ€²
âˆš
N(âˆ¥JâŠ¥Î˜n+1âˆ¥+âˆ¥JâŠ¥Î˜nâˆ¥) +Câ€²âˆ¥Î¸n+1âˆ’Î¸nâˆ¥
=Câ€²
âˆš
N(âˆ¥JâŠ¥Î˜n+1âˆ¥+âˆ¥JâŠ¥Î˜nâˆ¥) +Câ€²Î³n+11
N(1TâŠ—Id)âˆ‡F(Î˜n,Xn).
(55)
In Appendix B we have shown limnâ†’âˆžJâŠ¥Î˜n=0almost surely. Moreover, âˆ¥1
N(1TâŠ—
Id)âˆ‡F(Î˜n,Xn)âˆ¥is bounded per sample path. Therefore, limnâ†’âˆž1
NPN
i=1âˆ¥Î¾i
n+1âˆ¥= 0 such
thatlimnâ†’âˆž1
NPN
i=1Î¾i
n+1= 0almost surely.
To sum up, we decompose (39) into
Î¸n+1=Î¸nâˆ’Î³n+1âˆ‡f(Î¸n)âˆ’Î³n+1rnâˆ’Î³n+11
NNX
i=1 
ei
n+1+Î½i
nâˆ’Î½i
n+1+Î¾i
n+1
. (56)
Now that limpâ†’âˆžPp
n=11
NPN
i=1Î³nei
nandlimpâ†’âˆžPp
n=01
NPN
i=1Î³n+1(Î½i
nâˆ’Î½i
n+1)converge
and are finite, limnâ†’âˆžrn= 0,limnâ†’âˆž1
NPN
i=1Î¾i
n= 0 , all the conditions of C3 in Theorem
C.1 are satisfied. Additionally, Assumption 2.4 corresponds to C1, Assumption 2.3 meets C2, and
C4 is automatically satisfied when we choose the lyapunov function V(Î¸) =f(Î¸). Therefore,
lim supninfÎ¸âˆ—âˆˆLâˆ¥Î¸nâˆ’Î¸âˆ—âˆ¥= 0.
D Proof of Theorem 3.3
To obtain Theorem 3.3, we need to utilize the existing CLT result for general SA in Theorem D.1 and
check all the necessary conditions therein.
Theorem D.1 (Theorem 2.1 [29]) .Consider the stochastic approximation iteration (38), assume
C1.LetÎ¸âˆ—be the root of function h, i.e., h(Î¸âˆ—) = 0 , and assume limnâ†’âˆžÎ¸n=Î¸âˆ—. Moreover,
assume the mean field his twice continuously differentiable in a neighborhood of Î¸âˆ—, and
the Jacobian Hâ‰œâˆ‡h(Î¸âˆ—)is Hurwitz, i.e., the largest real part of its eigenvalues B <0;
C2.The step sizeP
nÎ³n=âˆž,P
nÎ³2
n<âˆž, and either (i). log(Î³nâˆ’1/Î³n) =o(Î³n), or (ii).
log(Î³nâˆ’1/Î³n)âˆ¼Î³n/Î³â‹†for some Î³â‹†>1/2|B|;
C3.supnâˆ¥Î¸i
nâˆ¥<âˆžalmost surely for any iâˆˆ[N];
C4. (a) {en}nâ‰¥0is anFn-Martingale difference sequence, i.e., E[en|Fnâˆ’1] = 0 , and there
exists Ï„ >0such that supnâ‰¥0E[âˆ¥enâˆ¥2+Ï„|Fnâˆ’1]<âˆž;
(b)E[en+1eT
n+1|Fn] =U+D(A)
n+D(B)
n, where Uis a symmetric positive semi-definite
matrix and (
D(A)
nâ†’0almost surely ,
limnÎ³nEhPn
k=1D(B)
ki
= 0.(57)
C5. Let rn=r(1)
n+r(2)
n,rnisFn-adapted, and
ï£±
ï£²
ï£³r(1)
n=o(âˆšÎ³n)a.s.
âˆšÎ³nPn
k=1r(2)
k=o(1) a.s.(58)
24Then,
1âˆšÎ³n(Î¸nâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ âˆ’ â†’
nâ†’âˆžN(0,V), (59)
where VHT+HV=âˆ’U in case C2 (i) ,
V(Id+ 2Î³â‹†HT) + (Id+ 2Î³â‹†H)V=âˆ’2Î³â‹†U in case C2 (ii) .(60)
Note that the matrix Uin the condition C4(b) of Theorem D.1 was assumed to be positive definite in
the original Theorem 2.1 [ 29]. It was only to ensure that the solution Vto the Lyapunov equation (60)
is positive definite, which was only used for the stability of the related autonomous linear ODE (e.g.,
Theorem 3.16 [ 15] or Theorem 2.2.3 [ 36]). However, in this paper, we do not need strict positive
definite matrix V. Therefore, we extend Uto be positive semi-definite such that Vis also positive
semi-definite (see Lemma D.2 for the closed form of matrix V). Such kind of extension does not
change any of the proof steps in [29].
D.1 Discussion about C1-C3
Our Assumption 2.1 corresponds to C1 by letting function h(Î¸) =âˆ’âˆ‡f(Î¸)therein. We can also let
Î³â‹†in Theorem 3.3 large enough to satisfy C2. The typical form of step size, also indicated in [ 29], is
polynomial step size Î³nâˆ¼Î³â‹†/naforaâˆˆ(0.5,1]. Note that aâˆˆ(0.5,1)satisfies C2 (i) and a= 1
satisfies C2 (ii). Assumption 2.4 corresponds to C3.4
D.2 Analysis of C4
To check condition C4, we need to analyze the Martingale difference sequence {ei
n}. Recall
ei
n+1=mÎ¸in(Xi
n)âˆ’PimÎ¸in(Xi
nâˆ’1)such that there exists a constant C,
Ehei
n+12+Ï„|Fni
â‰¤CEhmÎ¸in(Xi
n)2+Ï„+PimÎ¸in(Xi
nâˆ’1)2+Ï„Fni
=CX
YâˆˆXiPi(Xi
nâˆ’1, Y)mÎ¸in(Y)2+Ï„+CPimÎ¸in(Xi
nâˆ’1)2+Ï„.(61)
Sinceâˆ¥mÎ¸in(Y)âˆ¥<âˆžalmost surely by Assumption 2.4 and Xiis a finite state space, at all time n,
we have X
YâˆˆXiPi(Xi
nâˆ’1, Y)mÎ¸in(Y)2+Ï„<âˆža.s. (62)
and there exists another constant Câ€²such that by definition of PimÎ¸in(Xi
nâˆ’1), we have
PimÎ¸in(Xi
nâˆ’1)2+Ï„â‰¤Câ€²X
YâˆˆXiPi(Xi
nâˆ’1, Y)mÎ¸in(Y)2+Ï„<âˆža.s. (63)
Therefore, E[âˆ¥ei
n+1âˆ¥2+Ï„|Fn]<âˆža.s. for all nand C4.(a) is satisfied.
We now turn to C4.(b). Note that for any iÌ¸=j, we have E[ei
n+1(ej
n+1)T|Fn] =E[ei
n+1|Fn]Â·
E[(ej
n+1)T|Fn] = 0 due to the independence between agent iandj, andE[ei
n+1|Fn] = 0 . Then, we
have
Eï£®
ï£° 
1
NNX
i=1ei
n+1! 
1
NNX
i=1ei
n+1!TFnï£¹
ï£»=1
N2NX
i=1E
ei
n+1(ei
n+1)TFn
. (64)
The analysis of E[ei
n+1(ei
n+1)T|Fn]is inspired by Section 4 [ 29] and Section 4.3.3 [ 22], where they
constructed another Poisson equation to further decompose the noise terms therein.5Here, expanding
4Theorem D.1 is slightly modified in terms of condition C3, which is mentioned as a special case in Section
2.2 [29]. For the sake of mathematical simplicity, we stick to condition C3 in the proof.
5However, we note that [ 29,22] considered the Lipschitz continuity of function Fi
Î¸i(x)defined in (68) as an
assumption instead of a conclusion, where we give a detailed proof for this. We also obtain matrix Uiin an
explicit form, which coincides with the definition of asymptotic covariance matrix and was not simplified in
[29]. The discussion on the improvement of Uiis outlined in Section 3.2, which was not the focus of [ 29,22]
and was not covered therein.
25E[ei
n+1(ei
n+1)T|Fn]gives
E
ei
n+1(ei
n+1)TFn
=E[mÎ¸in(Xi
n)mÎ¸in(Xi
n)T|Fn] +PimÎ¸in(Xi
nâˆ’1) 
PimÎ¸in(Xi
nâˆ’1)T
âˆ’E[mÎ¸in(Xi
n)|Fn] 
PimÎ¸in(Xi
nâˆ’1)Tâˆ’PimÎ¸in(Xi
nâˆ’1)E[mÎ¸in(Xi
n)T|Fn]
=X
yâˆˆXiPi(Xnâˆ’1, y)mÎ¸in(y)mÎ¸in(y)Tâˆ’PimÎ¸in(Xi
nâˆ’1) 
PimÎ¸in(Xi
nâˆ’1)T.
(65)
Denote by
Gi(Î¸i, x)â‰œX
yâˆˆXiPi(x, y)mÎ¸i(y)mÎ¸i(y)Tâˆ’PimÎ¸i(x) (PimÎ¸i(x))T, (66)
and let its expectation w.r.t the stationary distribution Ï€ibegi(Î¸i)â‰œExâˆ¼Ï€i[Gi(Î¸i, x)], we can
construct another Poisson equation, i.e.,
E
ei
n+1(ei
n+1)TFn
âˆ’X
XinâˆˆXiÏ€(Xi
n)E
ei
n+1(ei
n+1)TFn
=Gi(Î¸i
n, Xi
nâˆ’1)âˆ’gi(Î¸i
n)
=Ï†i
Î¸in(Xi
nâˆ’1)âˆ’PiÏ†i
Î¸in(Xi
nâˆ’1),(67)
for some matrix-valued function Ï†i:RdÃ— Xiâ†’RdÃ—d. Following the similar steps shown in (42) -
(45), we can obtain the closed-form expression
Ï†i
Î¸i(x) =X
yâˆˆXi 
Iâˆ’Pi+1(Ï€i)Tâˆ’1(x, y)Gi(Î¸i, x)âˆ’gi(Î¸i). (68)
Then, we can decompose (65) into
Gi(Î¸i
n, Xi
nâˆ’1) =gi(Î¸âˆ—)|{z}
Ui+gi(Î¸i
n)âˆ’gi(Î¸âˆ—)|{z }
D(1)
i,n+Ï†i
Î¸in(Xi
n)âˆ’PiÏ†i
Î¸in(Xi
nâˆ’1)
| {z }
D(2,a)
i,n+Ï†i
Î¸in(Xi
nâˆ’1)âˆ’Ï†i
Î¸in(Xi
n)
| {z }
D(2,b)
i,n.
(69)
LetUâ‰œ1
N2PN
i=1Ui,D(1)
nâ‰œ1
N2PN
i=1D(1)
1,n,D(2,a)
nâ‰œ1
N2PN
i=1D(2,a)
i,n, and D(2,b)
nâ‰œ
1
N2PN
i=1D(2,b)
i,n, we want to prove that D(1)
nsatisfies the first condition in C4, and D(2,a)
n,D(2,b)
n
meet the second condition in C4.
We now show that for all i,Gi(Î¸i, x)is Lipschitz continuous in Î¸iâˆˆâ„¦for some compact subset
â„¦âŠ‚Rd. For any xâˆˆ XiandÎ¸i
1, Î¸i
2âˆˆâ„¦, we can get
âˆ¥mÎ¸i
1(x)mÎ¸i
1(x)Tâˆ’mÎ¸i
2(x)mÎ¸i
2(x)Tâˆ¥
=âˆ¥mÎ¸i
1(x)(mÎ¸i
1(x)âˆ’mÎ¸i
2(x))Tâˆ’(mÎ¸i
1(x)âˆ’mÎ¸i
2(x))mÎ¸i
2(x)Tâˆ¥
â‰¤âˆ¥mÎ¸i
1(x)âˆ’mÎ¸i
2(x)âˆ¥(mÎ¸i
1(x)âˆ¥+âˆ¥mÎ¸i
2(x)âˆ¥)
â‰¤Câˆ¥Î¸i
1âˆ’Î¸i
2âˆ¥,(70)
for some constant C, where the last inequality comes from âˆ¥mÎ¸i
1(x)âˆ¥<âˆžsince Î¸i
1âˆˆâ„¦and the
Lipschitz continuous function mÎ¸i(x). Similarly, we can get âˆ¥PimÎ¸i
1(x)âˆ’PimÎ¸i
2(x)âˆ¥ â‰¤Câˆ¥Î¸i
1âˆ’Î¸i
2âˆ¥.
Therefore, Gi(Î¸i, x)andgi(Î¸i)are Lipschitz continuous in Î¸iâˆˆâ„¦for any xâˆˆ Xi.
For the sequence {D(1)
i,n}nâ‰¥0, by applying Theorem 3.2 and conditioned on limnâ†’âˆžÎ¸n=Î¸âˆ—for
an optimal point Î¸âˆ—âˆˆ L, we have limnâ†’âˆžâˆ¥gi(Î¸i
n)âˆ’gi(Î¸âˆ—)âˆ¥ â‰¤limnâ†’âˆžCâˆ¥Î¸i
nâˆ’Î¸âˆ—âˆ¥= 0. This
implies D(1)
i,nâ†’0for every iâˆˆ[N]and thus D(1)
nâ†’0asnâ†’ âˆž almost surely, which satisfies the
first condition in (57).
For the Martingale difference sequence {D(2,a)
i,n}nâ‰¥0, we use Burkholder inequality (e.g., Theorem
2.10 [33], [21]) such that for pâ‰¥1and some constant Cp,
E"nX
i=1D(2,a)
i,np#
â‰¤CpEï£®
ï£° nX
i=1D(2,a)
i,n2!p/2ï£¹
ï£». (71)
26By the definition (66) and Assumption 2.4, for a sample path, supnâˆ¥Gi(Î¸i
n, x)âˆ¥<âˆžfor any xâˆˆ Xi,
as well as supnâˆ¥gi(Î¸i
n)âˆ¥<âˆž, which leads to supnâˆ¥Ï†i
Î¸in(x)âˆ¥<âˆžfor any xâˆˆ Xibecause of (68).
Then, we have supnâˆ¥D(2,a)
i,nâˆ¥ â‰¤C <âˆžfor the path-dependent constant C. Taking p= 1and we
have
lim
nâ†’âˆžÎ³nCpvuutnX
i=1D(2,a)
i,n2
â‰¤lim
nâ†’âˆžCpCÎ³nâˆšn= 0 a.s. (72)
Thus, Lebesgue dominated convergence theorem gives
lim
nâ†’âˆžÎ³nCpEï£®
ï£°vuutnX
i=1âˆ¥D(2,a)
i,nâˆ¥2ï£¹
ï£»=Eï£®
ï£°lim
nâ†’âˆžÎ³nCpvuutnX
i=1âˆ¥D(2,a)
i,nâˆ¥2ï£¹
ï£»= 0
and we have limnâ†’âˆžÎ³nE[âˆ¥Pn
i=1D(2,a)
i,nâˆ¥] = 0 .
For the sequence {D(2,b)
i,n}nâ‰¥0, we have
nX
k=1D(2,b)
i,k=nX
k=1
Ï†i
Î¸i
k(Xi
kâˆ’1)âˆ’Ï†i
Î¸i
kâˆ’1(Xi
kâˆ’1)
+Ï†i
Î¸i
0(Xi
0)âˆ’Ï†i
Î¸i
n(Xi
n)
=nX
k=1
Ï†i
Î¸i
k(Xi
kâˆ’1)âˆ’Ï†i
Î¸k(Xi
kâˆ’1)+Ï†i
Î¸k(Xi
kâˆ’1)âˆ’Ï†i
Î¸kâˆ’1(Xi
kâˆ’1)+Ï†i
Î¸kâˆ’1(Xi
kâˆ’1)âˆ’Ï†i
Î¸i
kâˆ’1(Xi
kâˆ’1)
+Ï†i
Î¸i
0(Xi
0)âˆ’Ï†i
Î¸in(Xi
n).
(73)
Since Gi(Î¸i, x)andgi(Î¸i)are Lipschitz continuous in Î¸iâˆˆâ„¦,Ï†i
Î¸i(x)is also Lipschitz continuous
inÎ¸iâˆˆâ„¦and is bounded. We havenX
k=1D(2,b)
i,kâ‰¤nX
k=1Ï†i
Î¸i
k(Xi
kâˆ’1)âˆ’Ï†i
Î¸i
kâˆ’1(Xi
kâˆ’1)+Ï†i
Î¸i
0(Xi
0)+Ï†i
Î¸in(Xi
n)
â‰¤nX
k=1Ï†i
Î¸i
k(Xi
kâˆ’1)âˆ’Ï†i
Î¸i
kâˆ’1(Xi
kâˆ’1)+D1
â‰¤nX
k=1D2Dâ„¦Î³k+D1(74)
where âˆ¥Ï†i
Î¸i
0(Xi
0)âˆ¥+âˆ¥Ï†i
Î¸in(Xi
n)âˆ¥ â‰¤D1for a given sample path, D2is the Lipschitz constant of
Ï†i
Î¸i(x), andâˆ¥âˆ‡Fi(xi, Xi)âˆ¥ â‰¤Dâ„¦for any xiâˆˆâ„¦andXiâˆˆ Xi. Then,
Î³nnX
k=1D(2,b)
i,kâ‰¤D2Dâ„¦Î³nnX
k=1Î³k+Î³nD1â†’0asnâ†’ âˆž (75)
because Î³nPn
k=1Î³k=O(n1âˆ’2a)by assumption 2.3. Therefore, the second condition of C4 is
satisfied.
D.3 Analysis of C5
We now analyze condition C5. The decreasing rate of each term in (56) has been proved in Appendix C.
Specifically, by assumption 2.4, there exists a compact subset for a given sample path, and
â€¢ we have shown thatr(A)
n=O(Î·n)a.s., which impliesr(A)
n=o(âˆšÎ³n)a.s.
â€¢For1
NPN
i=1Î¾i
n, in the case of increasing communication interval,1
NPN
i=1Î¾i
n=O(Î³n+
Î·n), by Assumption 2.3-ii), we know (Î³n+Î·n)/âˆšÎ³n=âˆšÎ³n+âˆšÎ³nKL+1
Ï„n=o(1)such
thatâˆ¥1
NPN
i=1Î¾i
nâˆ¥=o(âˆšÎ³n)almost surely. On the other hand, in the case of bounded
communication interval,1
NPN
i=1Î¾i
n=O(Î³n)such that âˆ¥1
NPN
i=1Î¾i
nâˆ¥=o(âˆšÎ³n)a.s.
27â€¢Since supnâˆ¥Î½i
nâˆ¥<âˆžalmost surely, we have suppâˆ¥1
NPN
i=1Pp
k=0(Î½i
kâˆ’Î½i
k+1)âˆ¥=
suppâˆ¥1
NPN
i=1(Î½i
0âˆ’Î½i
p+1)âˆ¥<âˆžalmost surely. Then,âˆšÎ³pâˆ¥1
NPN
i=1Pp
k=0(Î½i
kâˆ’
Î½i
k+1)âˆ¥=O(âˆšÎ³p)leads toâˆšÎ³pâˆ¥1
NPN
i=1Pp
k=0(Î½i
kâˆ’Î½i
k+1)âˆ¥=o(1)a.s.
Letr(1)
nâ‰œr(A)
n+1
NPN
i=1Î¾i
nandr(2)
nâ‰œ1
NPN
i=1(Î½i
kâˆ’Î½i
k+1). From above, we can see that C5 in
Theorem D.1 is satisfied and we show that all the conditions in Theorem D.1 have been satisfied.
D.4 CLosed Form of Limitimg Covariance Matrix
Lastly, we need to analyze the closed-form expression of Uas in C4 (b) of Theorem D.1. Recall
thatU=1
N2PN
i=1UiandUi=gi(Î¸âˆ—)in(69). We now give the exact form of function gi(Î¸âˆ—)as
follows:
gi(Î¸âˆ—) =X
xâˆˆXiÏ€i(x)ï£®
ï£¯ï£°mÎ¸âˆ—(x)mÎ¸âˆ—(x)Tâˆ’ï£«
ï£­X
yâˆˆXiPi(x, y)mÎ¸âˆ—(y)ï£¶
ï£¸ï£«
ï£­X
yâˆˆXiPi(x, y)mÎ¸âˆ—(y)ï£¶
ï£¸Tï£¹
ï£ºï£»
=Eï£®
ï£° âˆžX
s=0[âˆ‡Fi(Î¸âˆ—, Xs)âˆ’ âˆ‡fi(Î¸âˆ—)]! âˆžX
s=0[âˆ‡Fi(Î¸âˆ—, Xs)âˆ’ âˆ‡fi(Î¸âˆ—)]!Tï£¹
ï£»
âˆ’Eï£®
ï£° âˆžX
s=1[âˆ‡Fi(Î¸âˆ—, Xs)âˆ’ âˆ‡fi(Î¸âˆ—)]! âˆžX
s=1[âˆ‡Fi(Î¸âˆ—, Xs)âˆ’ âˆ‡fi(Î¸âˆ—)]!Tï£¹
ï£»
=Eh 
âˆ‡Fi(Î¸âˆ—, Xi
0)âˆ’ âˆ‡fi(Î¸âˆ—) 
âˆ‡Fi(Î¸âˆ—, Xi
0)âˆ’ âˆ‡fi(Î¸âˆ—)Ti
+Eï£®
ï£° 
âˆ‡Fi(Î¸âˆ—, Xi
0)âˆ’ âˆ‡fi(Î¸âˆ—) âˆžX
s=1[âˆ‡Fi(Î¸âˆ—, Xs)âˆ’ âˆ‡fi(Î¸âˆ—)]!Tï£¹
ï£»
+E" âˆžX
s=1[âˆ‡Fi(Î¸âˆ—, Xs)âˆ’ âˆ‡fi(Î¸âˆ—)]!
 
âˆ‡Fi(Î¸âˆ—, Xi
0)âˆ’ âˆ‡fi(Î¸âˆ—)T#
=Cov(âˆ‡Fi(Î¸âˆ—, X0),âˆ‡Fi(Î¸âˆ—, X0))
+âˆžX
s=1[Cov(âˆ‡Fi(Î¸âˆ—, X0),âˆ‡Fi(Î¸âˆ—, Xs)) + Cov(âˆ‡Fi(Î¸âˆ—, Xs),âˆ‡Fi(Î¸âˆ—, X0))],
=Î£(âˆ‡F(Î¸âˆ—,Â·)).
(76)
where the second equality comes from the recursive form of mÎ¸i(x)in(45), and that the process
{Xn}nâ‰¥0is in its stationary regime, i.e., X0âˆ¼Ï€ifrom the beginning. The last equality comes
from rewriting Cov(âˆ‡Fi(Î¸âˆ—, Xi),âˆ‡Fi(Î¸âˆ—, Xj))in a matrix form. Note that gi(Î¸âˆ—)is exactly the
asymptotic covariance matrix of the underlying Markov chain {Xi
n}nâ‰¥0associated with the test
function âˆ‡Fi(Î¸âˆ—,Â·). By utilizing the following lemma, we can obtain the explicit form of Vas
defined in (60).
Lemma D.2 (Lemma D.2.2 [ 41]).If all the eigenvalues of matrix Mhave negative real part, then for
every positive semi-definite matrix Uthere exists a unique positive semi-definite matrix Vsatisfying
U+MV +VMT=0. The explicit solution Vis given as
V=Zâˆž
0eMtUe(MT)tdt. (77)
D.5 CLT of Polyak-Ruppert Averaging
We now consider the CLT result of Polyak-Ruppert averaging Â¯Î¸n=1
nPnâˆ’1
k=0Î¸k. The steps follow
similar way by verifying that the conditions in the related CLT of Polyak-Ruppert averaging for the
stochastic approximation are satisfied. The additional assumption is given below.
28C6. For the sequence {rn}in (38), nâˆ’1/2Pn
k=0r(1)
kâ†’0with probability 1.
Then, the CLT of Polyak-Ruppert averaging is as follows.
Theorem D.3 (Theorem 3.2 of [ 29]).Consider the iteration (38), assume C1, C3, C4, C5 in
Theorem D.1 are satisfied. Moreover, assume C6 is satisfied. Then, with step size Î³nâˆ¼Î³â‹†/nafor
aâˆˆ(0.5,1), we have
âˆšn(Â¯Î¸nâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ âˆ’ â†’
nâ†’âˆžN(0,Vâ€²), (78)
where Vâ€²=Hâˆ’1UHâˆ’T.
Discussion about C1 and C3 can be found in Section D.1. Condition C4 has been analyzed in Section
D.2 and condition C5 has been examined in Section D.3. The only condition left to analyze is C6,
which is based on the results obtained in Section D.3. In view of (56),r(1)
n=r(A)
n+1
NPN
i=1Î¾i
n+1,
so C6 is equivalent to
nâˆ’1/2nX
k=1"
r(A)
k+1
NNX
i=1 
Î¾i
k+1#
â†’0w.p.1. (79)
In Section D.3, we have shown thatr(A)
n=O(Î·n),1
NPN
i=1Î¾i
n=O(Î³n). Note that by Assump-
tion 2.3, we consider bounded communication interval for step size Î³nâˆ¼Î³â‹†/naforaâˆˆ(0.5,1), and
hence, Î·n=O(Î³n)such thatr(A)
n=O(Î³n). We then know that
nX
k=1r(A)
n=O(n1âˆ’a),nX
k=1âˆ¥1
NNX
i=1Î¾i
nâˆ¥=O(n1âˆ’a), (80)
such that
nâˆ’1/2nX
k=1r(A)
k+1
NNX
i=1 
Î¾i
k+1=O(n1/2âˆ’a) =o(1), (81)
which proved (79) and C6 is verified. Therefore, Theorem D.3 is proved under our Assumptions 2.1 -
2.5.
E Discussion on the comparison of Theorem 3.3 to the CLT result in [51]
As a byproduct of our Theorem 3.3, we have the following corollary.
Corollary E.1. Under Assumptions 2.1 - 2.5, for the sub-sequence {nl}lâ‰¥0where Kl=Kfor all l,
we have
1âˆšnllX
k=1(Â¯Î¸nkâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ â†’
lâ†’âˆžN(0,Vâ€²) (82)
Proof. Since Kl=Kfor all l, we have nl=Kl. There is an existing result showing the CLT result
of the partial sum of a sub-sequence (after normalization) has the same normal distribution as the
partial sum of the original sequence.
Theorem E.2 (Theorem 14.4 of [ 8]).Given a sequence of random variable Î¸1, Î¸2,Â·Â·Â·with partial
sumSnâ‰œPn
k=1Î¸ksuch that1âˆšnSndist.âˆ’ âˆ’ âˆ’ âˆ’ â†’
nâ†’âˆžN(0,V). Let nlbe some positive random variable
taking integer value such that Î¸nlis on the same space as Î¸n. In addition, for some sequence {bl}lâ‰¥0
going to infinity, nl/blâ†’cfor a positive constant c. Then,1âˆšnlSnldist.âˆ’ âˆ’ âˆ’ â†’
lâ†’âˆžN(0,V).
From Theorem E.2 and our Theorem 3.3, we have1âˆšnlPl
k=1(Â¯Î¸nkâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ â†’
lâ†’âˆžN(0,Vâ€²).
29Recently, [ 51] studied the CLT result under the LSGD-FP algorithm with i.i.dsampling (with slightly
different setting of the step size). We are able recover Theorem 3.1 of [ 51] under the constant
communication interval while adjusting their step size to make a fair comparison. We state their
algorithm below for self-contained purpose. During each communication interval nâˆˆ(nl, nl+1],
Î¸i
n+1=(
Î¸i
nâˆ’Î³lâˆ‡Fi(Î¸i
n, Xi
n) ifnâˆˆ(nl, nl+1),
1
NPN
i=1(Î¸i
nâˆ’Î³lâˆ‡Fi(Î¸i
n, Xi
n)) ifn=nl+1.(83)
The CLT result associated with (83) is given below.
Theorem E.3 (Theorem 3.1 of [51]) .Under LSGD-FP algorithm with i.i.d. sampling, we have
âˆšnl
llX
k=1 Â¯Î¸nkâˆ’Î¸âˆ—dist.âˆ’ âˆ’ âˆ’ â†’
lâ†’âˆžN(0, Î½Vâ€²), (84)
where Î½â‰œlimlâ†’âˆž1
l2(Pl
k=1Kl)(Pl
k=1Kâˆ’1
l).
Note that Î½= 1for constant K. We can rewrite (84) as
âˆšnl
llX
k=1 Â¯Î¸nkâˆ’Î¸âˆ—
=âˆšnlâˆš
l1âˆš
llX
k=1(Â¯Î¸nkâˆ’Î¸âˆ—) =âˆš
K1âˆš
llX
k=1(Â¯Î¸nkâˆ’Î¸âˆ—) (85)
such that
1âˆš
llX
k=1(Â¯Î¸nkâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ â†’
lâ†’âˆžN(0,1
KVâ€²). (86)
Note that the step size in (83) keeps unchanged during each communication interval, while ours in
(1) keeps decreasing even in the same communication interval. This makes our step size decreasing
faster than theirs. To make a fair comparison, we only choose a sub-sequence {nKl}lâ‰¥0in(86) such
that it is â€˜equivalentâ€™ to see that our step sizes become the same at each aggregation step. In this case,
we again use Theorem E.2 to obtain
1âˆš
KllX
s=1(Â¯Î¸nKsâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ â†’
lâ†’âˆžN(0,1
KVâ€²), (87)
such that
1âˆš
llX
s=1(Â¯Î¸nKsâˆ’Î¸âˆ—) =âˆš
K1âˆš
KllX
s=1(Â¯Î¸nKsâˆ’Î¸âˆ—)dist.âˆ’ âˆ’ âˆ’ â†’
lâ†’âˆžN(0,Vâ€²). (88)
Therefore, our Corollary E.1 also recovers Theorem 3.1 of [ 51] under the constant communication
interval K, but with more general communication patterns and Markovian sampling.
F Discussion on Communication Patterns
F.1 Examples of Communication Matrix W
Metropolis Hasting Algorithm: In the decentralized learning such as D-SGD, HLSGD and DFL,
Wat the aggregation step can be generated locally using the Metropolis Hasting algorithm based on
the underlying communication topology, and is deterministic [ 62,43,80]. Specifically, each agent i
exchanges its degree diwith its neighbors jâˆˆN(i), forming the weight W(i, j) = min {1/di,1/dj}
forjâˆˆN(i)andW(i, i) = 1 âˆ’P
jÌ¸=N(i)W(i, j). In this case, Wis doubly stochastic and
symmetric. By Perron-Frobenius theorem, its SLEM Î»2(W)<1. Then, âˆ¥WTWâˆ’Jâˆ¥=
âˆ¥W2âˆ’Jâˆ¥=Î»2
2(W)<1, which satisfies Assumption 2.5-ii). It is worth noting that this algorithm
is robust to time-varying communication topologies.
Client Sampling in FL: For LSGD-FP studied in [ 67,76,42],W=11T/Ntrivially satisfies
Assumption 2.5-ii). For LSGD-PP on the other hand, only a small fraction of agents participate in
each aggregation step for consensus [ 50,30]. Denote by Sa randomly selected set of agents (without
replacement) of fixed size |S| âˆˆ { 1,2,Â·Â·Â·, N}at time nandWSplays a role of aggregating Î¸i
n
30for agent iâˆˆ S. Additionally, the central server needs to broadcast updated parameter Î¸n+1to
the newly selected set Sâ€²with the same size, which results in a bijective mapping Ïƒ(forS â†’ Sâ€²
and[N]/S â†’ [N]/Sâ€²) and a corresponding permutation matrix TSâ†’Sâ€². Then, the communication
matrix becomes W=TSâ†’Sâ€²WS.6Specifically, TSâ†’Sâ€²(i, j) = 1 ifj=Ïƒ(i)andTSâ†’Sâ€²(i, j) = 0
otherwise. Besides, WS(i, j) = 1 /|S|fori, jâˆˆ S,WS(i, i) = 1 fori /âˆˆ S, andWS(i, j) = 0
otherwise. Note that WSis now a random matrix, since Sis a randomly chosen subset of size
|S|. Clearly, for each choice of S,WSis doubly stochastic, symmetric and W2
S=WS. Taking
the expectation of WSw.r.t the randomly selected set SgivesES[WS](i, i) = 1âˆ’(|S| âˆ’ 1)/N
foriâˆˆ[N]andES[WS](i, j) = (|S| âˆ’ 1)/N(Nâˆ’1)foriÌ¸=j. Note that ES[WS]has all
positive entries. Therefore, we use the fact TTT=Ifor permutation matrix Tsuch that âˆ¥E[W]âˆ’
Jâˆ¥=âˆ¥ES,Sâ€²[WT
STT
S,Sâ€²TS,Sâ€²WS]âˆ’Jâˆ¥=âˆ¥ES[WT
SWS]âˆ’Jâˆ¥=âˆ¥ES[WS]âˆ’Jâˆ¥<1by
Perronâ€“Frobenius theorem and eigendecomposition, which satisfies Assumption 2.5-ii).
F.2 Discussion on partial client sampling
The commonly used partial client sampling algorithm in the FL literature [ 50,30] is FedAvg as
follows:
1.At time n, the central server updates its global parameter Î¸n=1
|S|P
iâˆˆSÎ¸i
nfrom the
agents in the previous set S. Then, the central server selects a new subset of agents Sâ€²and
broadcasts Î¸nto agent iâˆˆ Sâ€², i.e., Î¸i
n=Î¸n;
2.Each selected agent icomputes Ksteps of SGD locally and consecutively updates its local
parameter Î¸i
n+1,Â·Â·Â·, Î¸i
n+Kaccording to (1a);
3. Each selected agent iâˆˆ Sâ€²uploads Î¸i
n+Kto the central server.
Then, the central server repeats the above three steps with Î¸n+Kand a new set of selected agents.
In our client sampling scheme, at the aggregation step n, the design of WSresults in ËœÎ¸i
n=
1
|S|P
jâˆˆSÎ¸j
nfor a selected agent iâˆˆ S, and ËœÎ¸i
n=Î¸i
nfor an unselected agent i /âˆˆ S. Mean-
while, the central server updates the global parameter ËœÎ¸n=ËœÎ¸i
nforiâˆˆ S. Then, the permutation
matrix TSâ†’Sâ€²ensures that the newly selected agent iâˆˆ Sâ€²will use ËœÎ¸nas the initial point for its subse-
quent SGD iterations. Consequently, from the selected agentsâ€™ perspective, the communication matrix
W=TSâ†’Sâ€²WScorresponds to step 1 in FedAvg. As we can observe, both algorithms update
the global parameter identically from the central serverâ€™s viewpoint, rendering them mathematically
equivalent regarding the global parameter update.
We acknowledge that under the i.i.d sampling strategy, the behavior of unselected agents in our
algorithm differs from FedAvg. Specifically, unselected agents are idle in FedAvg, while they
continue the SGD computation in our algorithm (despite not contributing to the global parameter
update). Importantly, when an unselected agent is later selected, the central server overwrites its local
parameter during the broadcasting process. This ensures that the activities of agents when they are
unselected have no impact on the global parameter update.
To our knowledge, the FedAvg algorithm under the Markovian sampling strategy remains unexplored
in the FL literature. Extrapolating the behavior of unselected agents in FedAvg from i.i.dsampling to
Markovian sampling suggests that unselected agents would remain idle. In contrast, our algorithm
enables unselected agents to continue evolving Xi
n. These additional transitions contribute to faster
mixing of the Markov chain for each unselected agent and a smaller bias of Fi(Î¸, Xi
n)relative to its
mean field fi(Î¸), potentially accelerating the convergence.
G Additional Simulation
G.1 Simulation Setup in Section 4
This simulation is performed on a PC with an AMD R9 5950X, RTX 3080 and 128 GB RAM. In
this simulation, we assume that agents follow the DSGD algorithm (1). In Figure 2(a) - 2(c), each
6In Appendix F.2, we will discuss the mathematical equivalence between our client sampling strategy and
the commonly used one in the FL literature [50, 30].
31agent holds a disjoint local dataset (non-overlapping data points for every agent), while we distribute
the ijcnn1 dataset [ 14] with more varied distribution among 100agents by leveraging Dirichlet
distribution with the default alpha value of 0.5in Figure 2(d) - 2(f).
Moreover, we assume that all agents are distributed over a communication network. In order to create
this network among 100agents and the graph-like dataset structure held by each agent, we utilize
connected sub-graphs from the real-world graph Facebook in SNAP [ 49]. All 100agents collaborate
together to generate a deterministic communication matrix W= [Wij]with Metropolis Hasting
algorithm of the following form: For i, jâˆˆ[N], we have
Wij=(
minn
1
di,1
djo
if agent jis the neighbor of agent i,
0 otherwise ,
Wii= 1âˆ’X
jâˆˆ[N]Wij,
where direpresents the degree of agent iin the graph. The communication interval Kis set to 1, as
is the usual choice in DSGD [71, 80, 61, 68].
For the first group of agents, we assume they have full access to their datasets, thus performing i.i.d.
sampling or single shuffling. In particular,
â€¢i.i.d. sampling employed by agent imeans that the data point Xi
nis independently and
uniformly sampled from its dataset Xiat each time n.
â€¢Single shuffling, by its name, only shuffles the dataset once and adheres to that specific order
throughout the training process.
On the other hand, within the second group of agents, we assume that they hold graph-like datasets.
Now, we introduce simple random walk (SRW), non-backtracking random walk (NBRW), and
self-repellent random walk (SRRW) in order:
â€¢ SRW refers to the walker that chooses one of the neighboring nodes uniformly at random.
â€¢NBRW, as studied in [ 2,48,5], is a variation of SRW, which selects one of the neighbors
uniformly at random, with the exception of the one visited in the last step.
â€¢SRRW, recently proposed by [ 25], is designed with a nonlinear transition kernel K[x]âˆˆ
[0,1]NÃ—Nof the following form:
Kij[x]â‰œPij(xj/Âµj)âˆ’Î±
P
kâˆˆ[N]Pik(xk/Âµk)âˆ’Î±,âˆ€i, jâˆˆ[N], (89)
where matrix P= [Pij]is the transition kernel of the baseline Markov chain and Âµ= [Âµi]is
its corresponding stationary distribution. Additionally, Î±denotes the force of self repellence,
and larger Î±leads to stronger force of self repellence, thus higher sampling efficiency
[25, Corollary 4.3]. Moreover, vector xâˆˆRNis in the interior of probability simplex,
representing the empirical distribution, in other words, the visit frequency of each node in
the graph. The update rule of this empirical distribution is in the following form:
xn+1=xn+Î²n+1(Î´Xn+1âˆ’xn), (90)
where Î²nâ‰œ(n+ 1)âˆ’bis the step size of SRRW iterates. b= 1was original proposed in
[25] and is recently extended to bâˆˆ(0.5,1)in [39]. In this simulation, we use SRW as the
baseline Markov chain of SRRW, and in turn Âµis proportional to the degree distribution.
We also assume x0=1/N, i.e., each node has been visited once, and choose the step
sizeÎ²n= (n+ 1)âˆ’0.8, force of self repellence Î±= 20 according to the suggestion in [ 39,
Section 4].
Since SRW, NBRW, and SRRW all admit the stationary distribution that is proportional to degree
distribution, in order to obtain unfirom target in (15), we need to reweight the gradient computed by
each agent iin order to maintain an asymptotic unbiased gradient. Thus, agent ishould modify its
SGD update from (1a) to the following:
Î¸i
n+1/2=Î¸i
nâˆ’Î³n+1âˆ‡Fi(Î¸i
n, Xi
n)/d(Xi
n). (91)
320 25 50 75 100 125 150 175 200
Number of rounds (n)0.60.81.01.21.41.61.82.02.2training lossShuffling, SRW
Shuffling, NBRW
Shuffling, SRRW
0 25 50 75 100 125 150 175 200
Number of rounds (n)0.00.51.01.52.0training lossShuffling, SRRW with 10 clients
Shuffling, SRRW with 20 clients
Shuffling, SRRW with 30 clients
Shuffling, SRRW with 40 clients
0 25 50 75 100 125 150 175 200
Number of rounds (n)0.100.150.200.250.300.350.400.450.50training lossShuffling, SRW
Shuffling, NBRW
Shuffling, SRRW(=10)
Figure 3: Image classification experiment. From left to right: (a) Comparison of various sampling
strategies in image classification problem using 5-layer neural network. (b) Train a 5-layer CNN
model with different number of total agents (clients) to show the linear speedup effect. (c) Train
ResNet- 18model with different sampling strategies among 10agents with participation ratio 0.4.
G.2 Image Classification Task
In this part, we perform the image classification task through a 5-layer neural network, where the
CIFAR10 dataset [ 44] with 50k image data is evenly distributed to 10agents. Each agent possesses
5k images, which are further divided into 200batches, each batch with 25images.
The Convolutional Neural Network (CNN) model used in this simulation encompasses:
â€¢Two convolutional layers (i.e., nn.Conv2d(3, 6, 5) andnn.Conv2d(6, 16, 5) ), each followed by
ReLU activation functions to introduce non-linearity and max pooling (i.e., nn.MaxPool2d(2,
2)) to reduce spatial dimensions.
â€¢Three fully connected (linear) layers, concluding with a softmax output to handle the
multi-class classification problem.
Similar to the simulation setup in Section 4, among the 10participating agents, five have unrestricted
access to their respective data allocations, enabling them to utilize the shuffling method to iterate
through their batches. The other five agents are designed to simulate limited data access scenarios.
Their data access is structured using five distinct graph topologies extracted from the SNAP dataset
collection [ 49], each graph simulating a unique communication pattern among the batches (nodes) of
data. Within these topologies, agents adopt one of three random walk strategies â€” SRW, NBRW, and
SRRW, all with importance reweighting â€” to sample the batches for training.
Local model training is conducted for five epochs at each agent before aggregating the updates at a
central server â€” a process repeated for a total of 200communication rounds. Each epoch consists of a
full traversal of the local dataset of agents in the first group, or 200batches sampled for training in the
second group of agents. To mimic realistic conditions, we also introduce partial agent participation
where only 40% of agents are selected randomly to transmit their updates in each round, reflecting
the intermittent communication in real-world FL deployments. Lastly, the selection of the step size
Î²nfor SRRW iterates (90) is a critical aspect of our experiments. In this simulation, we experiment
with various values of bâˆˆ {0.501,0.6,0.7,0.8,0.9}to determine the most advantageous setting for
maximizing the benefits of the SRRW strategy. Based on our findings, the best choice for the SRRW
step size is b= 0.501, in other words, Î²n= (n+ 1)âˆ’0.501.
The simulation result is quantified by averaging the training loss across ten repeated trials for each
configuration. As depicted in Figure 3(a), the training results are consistent with our previous findings
in Figure 2(a) in the context of the FL framework and the training of neural networks: the use of a
more effective sampling approach, even for a portion of the agents, results in significant enhancements
in the overall training of the model, and this improvement is further enhanced through the highly
efficient sampling strategy SRRW.
In Figure 3(b) and 3(c), we perform image classification experiments in the FL setting with partial
client participation. Only 4random agents will participate in the training process at each aggregation
phase. In Figure 3(b), we fix the sampling strategy (shuffling, SRRW with Î±= 10 ) and test
the linear speedup effect for the 5-layer CNN model by duplicating 10agents to Nagents with
Nâˆˆ {10,20,30,40}, keeping the same participation ratio 0.4. As can be seen from the plot, the
training loss is inversely proportional to the number of agents, i.e., at 200rounds, the training loss
is0.52for10agents, 0.23for20agents, 0.18for30agents, and 0.12for40agents. In Figure 3(c),
we extend the current simulation from 5-layer CNN model to ResNet-18 model [ 34] in order to
33numerically test the performance of different sampling strategies in a more complex neural network
training task. By fixing the shuffling in the first group of agents, we observe that improving Markovian
sampling from SRW to NBRW, then to SRRW, gives accelerated training process with smaller training
loss.
H Limitations
Our study provides crucial insights into the identification of nuanced agentsâ€™ sampling behaviors in
UD-SGD, where improving each agentâ€™s sampling strategy speeds up overall system performance
without additional computational burden except the additional storage for the visit counts used for
sampling their datasets. Our UD-SGD is scalable in terms of larger datasets as the sampling strategy
(i.e., random walk) utilized by each agent leverages only local information for its dataset. However,
our work has two limitations that should be acknowledged.
1.Assumption 2.4 posits that the parameter trajectory {Î¸n}is almost surely bounded, which is
a strong assumption. This is crucial for guaranteeing the well-defined nature of all related
quantities. Some mechanisms such as projections onto a compact subset [ 45, Chapter 5.1],
or truncation-based method with expanding compact subsets can do the trick to ensure that
the iteration is always bounded [ 3]. As mentioned in our discussion after Assumption 2.4,
only recently the stability of SGD under Markovian sampling has been guaranteed to hold
for some class of objective function f[9], while the discussion on stability issue under
multi-agent scenario with Markovian sampling remains an open problem and we do not
pursue to remove this stability assumption in this paper.
2.Asymptotic analysis: The main results of our work, i.e., almost sure convergence and
central limit theorem in distributed optimization, are based on asymptotic analysis and
might not accurately represent the finite-sample performance of each contributing agent
in the system. The state-of-the-art finite-sample analysis in the literature only focuses on
the worst-performing agent that cannot capture the nuanced differences between agentsâ€™
sampling strategies, with the explanation detailed in Footnote 2. Thus, a finite-sample error
bound that distinguish every agentâ€™s dynamics is still unknown and regarded as a future
direction.
Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: The abstract and the introduction clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. The claims
made match theoretical and experimental results, and reflect how much the results can be
expected to generalize to other settings.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: The â€˜Limitationsâ€™ section is provided in Appendix H.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: All the theorems, formulas, and proofs in the paper have been numbered and
cross-referenced. All assumptions have been clearly stated or referenced in the statement of
any theorems. The complete and correct proofs have been provided in appendices.
4.Experimental Result Reproducibility
34Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The detailed simulation setups have been provided in Appendix G.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: We do not provide clean source codes, but detailed instructions to perform the
experiment have been provided in Appendix G.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: The full details have been provided in Appendix G.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We plot the error bars in Figure 2(a). However, error bars are intentionally
omitted in Figure 2 in the main body to avoid a cluttered plot and deliver the main message.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We specify the platform that runs the simulation in Appendix G.1.
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips .cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We fully obey the NeurIPS Code of Ethics.
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: This work mainly aims to demonstrate how the improvement of partial agentsâ€™
sampling strategies can accelerate the overall systemâ€™s performance. It advocates for
improving common wealth and has no negative social impact.
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks.
3512.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have cited correctly the datasets ijcnn1 andCIFAR-10 used for experiments
in our paper.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
36