Towards Neuron Attributions in Multimodal
Large Language Models
Junfeng Fang, Zongze Bi, Ruipeng Wang, Houcheng Jiang, Yuan Gao, Kun Wangâˆ—
University of Science and Technology of China
{fjf,zacb2018,wrp20021021,janghc,gaoy,wk520529}@mail.ustc.edu.cn
An Zhang
National University of Singapore
an_zhang@nus.edu.sgJie Shi
Huawei
shi.jie1@huawei.com
Xiang Wangâˆ—
University of Science and Technology of China
xiangwang1223@gmail.comTat-Seng Chua
National University of Singapore
dcscts@nus.edu.sg
Abstract
As Large Language Models (LLMs) demonstrate impressive capabilities, demys-
tifying their internal mechanisms becomes increasingly vital. Neuron attribution,
which attributes LLM outputs to specific neurons to reveal the semantic properties
they learn, has emerged as a key interpretability approach. However, while neuron
attribution has made significant progress in deciphering text-only LLMs, its appli-
cation to Multimodal LLMs (MLLMs) remains less explored. To address this gap,
we propose a novel Neuron Attribution method tailored for MLLMs, termed NAM .
Specifically, NAM not only reveals the modality-specific semantic knowledge
learned by neurons within MLLMs, but also highlights several intriguing properties
of neurons, such as cross-modal invariance and semantic sensitivity. These proper-
ties collectively elucidate the inner workings mechanism of MLLMs, providing a
deeper understanding of how MLLMs process and generate multi-modal content.
Through theoretical analysis and empirical validation, we demonstrate the efficacy
of NAM and the valuable insights it offers. Furthermore, leveraging NAM, we
introduce a multi-modal knowledge editing paradigm, underscoring the practical
significance of our approach for downstream applications of MLLMs. Our code is
available at https://github.com/littlelittlenine/NAM_1.
1 Introduction
As Large Language Models (LLMs) demonstrate impressive capabilities [ 1,2,3,4], demystifying their
internal mechanisms becomes increasingly vital, particularly in applications emphasizing fairness,
trust, and ethical decision-making [ 5,6,7]. To interpret LLMs, â€œneuron attributionâ€ stands out as
a pivotal approach. This method involves attributing text outputs to individual model components
(e.g., neurons and hidden layers) to reveal the knowledge and linguistic properties they learn [ 5,8,
9,10,11,12]. Such insights not only facilitate tasks like model editing and pruning [ 13,14,15],
but also offer a deeper understanding of how LLMs internalize knowledge. For instance, leading
neuron-attribution studies [ 16,17,13,14,18,19] suggest that this capability of internalization may
predominantly originate from their Feedforward Neural Networks (FFNs).
âˆ—Corresponding authors.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Give me an image like:
GILL:
CLIPOPT
(Base LLM)Projector
Here is an image of a Dog.
Projector Stable Diffusion
(a)
Dog
Here is 
an image 
of a Dog.
FFN
Dogs  
are 
human's 
friends.FFN
Text-only LLM
(b)T or I -Neurons?
 Here is 
an image 
of a Dog.
Lawn
 (Noise)Lawn
 (Noise)
MLLM MLLMI-NeuronsT-Neurons
+
(c) (d)T-NeuronsFigure 1: Illustration of neuron attribution methods for interpreting LLMs. (a) The paradigm of GILL;
(b) current attribution methods tailored for text-only LLMs; (c) the challenges of extending current
attribution methods to MLLMs; (d) the paradigm of our NAM. Best viewed in color.
Recently, the rapid development of multi-modal large language models (MLLMs) [ 20,21,22,23,24]
is sparking the interest of interpretability [ 18,19]. Here we focus on the MLLMs that can perceive,
generate texts and images simultaneously. Scrutinizing prior studies, we can summarize two common
components beyond the LLM base: image encoding and generating modules. Specifically, the image
encoding module projects the input image into the representation space of the base LLM; hereafter,
the image generating module generates image outputs conditioned on the representations given by
the base LLM. Take GILL [ 25] as an example. As shown in Figure 1 (a), it hires OPT [ 26] as the
LLM base, CLIP Vit-L [ 27] with a cross-modal projector as the image encoding module, and Stable
Diffusion [28] with another projector as the image generating module.
While this expanded capacity endows GILL with versatility suitable for a variety of downstream tasks,
it concurrently presents challenges for interpretation, particularly concerning neuron attributions.
Specifically, we outline these challenges through three progressive points:
â€¢Source of Attribution: Semantic Noise. As shown in Figure 1 (b), for semantics like dog, current
methods typically attribute the output to neurons directly in text-only LLMs [ 16,5,13]. However,
in MLLMs, attributing the entire generated image to neurons directly might result in inaccuracies.
As shown in Figure 1 (c), when GILL is tasked with drawing a dog, the generated image might
contain other semantic elements like lawn, introducing noise and distorting attribution.
â€¢Process of Attribution: Inefficiency. Leading attribution methods typically rely on gradients
[16,19] or causal effects [ 13,14], requiring extensive forward/backward propagation processes,
which are inherently time-consuming and storage-intensive. The added complexity of encoding
and generation modules in MLLMs further exacerbates this challenge.
â€¢Results of Attribution: Intermingled Neurons. In text-only LLMs, attributing the concept dog
involves identifying neurons crucial for outputting the word â€œdogâ€, termed T-neurons . In contrast,
MLLMs also require identifying neurons crucial for image generation, called I-neurons . As
illustrated in Figure 1 (c), the distribution of T-neurons and I-neurons differs for the same concept,
leading to conflicting results that complicate further analysis and applications.
In sight of this, we introduce a new Neuron Attribution paradigm tailored for MLLMs, termed
NAM , to reveal the modality-specific semantic properties learned by neurons within the FFN layers.
Specifically, to address the above challenges, NAM undertakes the following efforts:
â€¢Image Segmentation for Semantic Noise: As shown in Figure 1 (d), NAM employs the image
segmentation model to distinguish regions containing the target semantics from other noisy semantic
areas, and attributes these regions to the neurons, rather than the entire image, to ensure accuracy.
â€¢Activation-based Scores for Inefficiency: Drawing inspiration from prior studies on neuron
activations [ 18,5], NAM introduces a new attribution score that relies on neuron activations,
eliminating the need for additional forward/backward propagation or gradient calculations.
â€¢Modality Decoupling for Intermingled Neurons: NAM assigns modality-specific attribution
scores to neurons to prevent cross-modal disturbances during attributions. This paradigm facilitates
the decoupling analysis of T-neurons and I-neurons, as depicted in Figure 1 (d).
Furthermore, based on the empirical results of NAM, we reveal several intriguing neuron properties
within MLLMs. These properties collectively elucidate the inner workings of MLLMs, enhancing our
2EVA02
Diffuser
Interpreter
0.4 0.7
â€œ â€¦Dog. â€
Image generation 
module
Give me 
an image    
like:
ğ–inğ‘™ğ¡0 ğ–outğ‘™ğ€ğ‘™Attentionğ¡ ğ‘™ğ¡ ğ¿ğ–u 
ğ‘1ğ‘™ğ‘ğ‘‘ğ‘™â„1ğ‘™â„ğ»ğ‘™ğ»Ã—ğ‘‘ ğ‘‘Ã—ğ»
ğ¡ ğ‘™âˆ’1
ğ–inğ‘™ğ–outğ‘™ğ€ğ‘™Residual
ğ–outğ‘™ğ‘ğ‘˜ğ‘™
(ğ–outğ‘™)ğ’Œ Streamğ¡ ğ‘™
ğ¡ ğ¿Image generation 
module
Contribution  of ğ‘ğ‘˜ğ‘™ to â€œdog regionâ€: .ğ‘ğ“Importance of â„1ğ‘™~â„ğ»ğ‘™
to â€œdog regionâ€:
Refine to neuron ğ’–ğ’Œğ’ğ‘
ğ–inğ‘™ğ–outğ‘™ğ€ğ‘™ Residualğ–u ğ–outğ‘™
(ğ–outğ‘™)ğ’Œ 
(ğ–u )ğ’‘ ğ¡ ğ‘™
ğ’šğ’‘ 
ğ’„ğ’Œ, ğ’‘ğ’Streamğ¡ ğ¿
ğ–u 
Vocabulary : â€œDogâ€
Refine to neuron ğ’–ğ’Œğ’ğ‘¦1 ğ‘¦2 ğ‘¦|ğ‘‰| ğ’‚ğ’Œğ’(a)
(b) (c)Figure 2: Illustration of neuron attribution methods for interpreting LLMs. (a) The paradigm of
current attribution methods tailored for text-only LLMs. (b) The challenges of extending current
attribution methods to MLLMs. (c) The paradigm of our NAM.
understanding of their capacity to process and generate multi-modal content. Interestingly, among
these insights, a pivotal finding exhibits that when generating multi-model content for the same
semantics ( e.g., the word â€œdogâ€ & an image of a dog), the crucial neurons ( i.e.,T-Neurons & I-
Neurons) are typically not identical. This distinction underscores the complex nature of neurons
within MLLMs, and highlights the necessity of neuron attribution across modalities. Additionally, by
applying NAM to enhance image editing tasks, we further underscore the significance and potential
applications of our NAM for MLLM community.
2 Preliminary
Transformer-Based LLMs. An autoregressive transformer language model G:X â†’ Y operates
over the vocabulary V. It receives a token sequence xâˆˆ X and generates a probability distribution
y= [y1, y2, ..., y |V|]âˆˆ Y to predict the next token [ 29,13]. Each token is represented as a series of
representations hlâˆˆRHinl-th layer, where h0is the embedding of the token in x. The modelâ€™s
final output, y=Wu 
hL
, is derived from the last representation hL=
hL
1, hL
2, ..., hL
HâŠ¤in layer
Lusing the unembedding matrix Wu. Figure 2 (a) exhibits a visualization of how hlare computed
within layer l. The representation in each layer results from the combination of the global attention
al, the local MLP output ml, and the representation hlâˆ’1from the previous layer. Formally,
hl=ml+hlâˆ’1+al,ml=Wl
outÏƒ 
Wl
inÎ³ 
al+hlâˆ’1
, (1)
where WinâˆˆRdÃ—HandWoutâˆˆRHÃ—dare the first and second linear layer in FFN with the
dimensionality of the FFNâ€™s intermediate layer d;ÏƒandÎ³are rectifying and normalizing nonlinearity.
For further background on transformers, we refer to [ 29]. Additionally, focusing on the the k-th
neuron ul
kin the l-th FFN layers, we simplify the definition by considering its activation al
kas:
Al= [a1, a2, ..., a d]âŠ¤=Ïƒ 
Wl
outÎ³ 
al+hlâˆ’1
(2)
MLLMs. Here, we take GILL as an example to illustrate a common paradigm of MLLMs. As shown
in Figure 2 (a), GILL incorporates the following modifications on the above text-only LLMs: (1) In
addition to the textual prompt, the input token sequence xalso includes the encoding of the input
image produced by the image encoding module. (2) The representation hLof the last hidden layer is
utilized as input to the image generation module, facilitating conditional image generation.
3 Method
This section delineates the implementation of NAM within MLLMs. Specifically, Section 3.1 and
3.2 introduce the attribution process for image and text outputs, respectively; In Section 3.3, we
3explore a significant application of NAM, i.e.,editing images generated by MLLMs. To illustrate
these processes, we utilize GILL [ 25], a representative MLLM capable of image generation. For a
detailed introduction to GILL, please refer to Appendix B.
3.1 Neuron Attribution for Image Generation
We first detail how to attribute the output images to the specific neurons within FFN layers. Retro-
specting the challenges highlighted in Introduction, using the MLLM to generate images of a specific
concept ( e.g., dog) often results in outputs that include extraneous, noisy elements ( e.g., lawn). To
mitigate the negative effects of semantic noise on neuron attribution, we propose a two-step approach
to extract I-Neurons. Specifically, (1) the first step focuses on attributing the output of the image
generation module ( i.e.,images) to the input of the image generation module ( i.e.,last representation
hL), and (2) the second step endeavors to attribute the input of the image generation module to the
specific neurons. Next, we provide detailed descriptions of these two steps.
3.1.1 STEP1: Attribution From Images to Representation hL
The purpose of this step is to attribute the image to hLâˆˆRH. That is, to identify the contribution of
each element in hLfor image generation. We define these contribution scores as RâˆˆRH. As shown
in Figure 2 (b), NAM acquires Rby sequentially executing the following processes:
â€¢Given an image generated by prompting MLLM with the semantics dog, NAM first employs the
leading segmentation model, EV A02 [ 30], to identify regions specifically related to dog. This is
crucial for minimizing interference from extraneous semantics, such as lawn in the background;
â€¢Subsequently, NAM utilizes the advanced attribution algorithm of the diffusion model, Diffuser-
Interpreter [ 31], to access the relevance of each dimension in the input of the image generation
module to the dog region in the generated image.
â€¢Ultimately, by normalizing these relevance scores to (0,1), we obtain the importance scores
R= [r1, r2, ..., r H]âŠ¤ofhL= [h1, h2, ..., h H]âŠ¤w.r.t the target semantics in the output image.
Due to the established applications of EV A02 and Diffuser-Interpret, we provide detailed introductions
in Appendix B. Furthermore, it is worth mentioning that NAM can be transferred to any other modality
by utilizing the (1) semantic segmentation algorithms and (2) attribution algorithms of generation
modules tailored for other modalities ( e.g., audio and video). After obtaining Rby these advanced
modality-specific algorithms, the subsequent attribution steps are universal across all transformer-
based MLLMs.
3.1.2 STEP2: Attribution From Representation hLto Neuron ul
k
This step involves attributing the representation hLin the last layer to the specific neuron ul
kwithin
the FFNs of the base LLM. To this end, NAM aims to trace each neuronâ€™s contribution to hL, and
identify the neurons with significant contributions as I-Neurons for the semantic of interest. Hence, a
fair and efficient contribution scoring method is crucial.
Direct Contributions through Residual Stream. Current methods for scoring contributions often
rely on gradients, such as the product of gradients and activations [ 19] or the integration of gradients
[16]. However, these methods are computationally intensive, particularly for large-scale models with
extensive parameters. In sight of this, drawing inspiration from prior studies on neuron activation
[18,19], we first introduce a new attribution score that relies on the neuron activation al
k. Specifically,
we first try to disassemble and deduce the generation procedure of hLby expanding hLas follows:
hL=mL+hLâˆ’1+aL=LX
l=1ml+h0+LX
l=1al
=LX
l=1Wl
outAl+h0+LX
l=1al=LX
l=1dX
k=1al
k(Wl
out)k+h0+LX
l=1al,(3)
where (Wl
out)kâˆˆRHis the k-th column of the weight matrix Wl
outcorresponding to the index
of neuron ul
k, as shown in Figure 2 (b). Note that the first term of Equation (3)reflects the direct
contribution of the neuron ul
kto the last representation hL,i.e.,the contribution through the residual
stream [19] of the base LLM.
4Hence, we employ al
k(Wl
out)kas the indicator for the neuron ul
kâ€™s contribution to hL.
Furthermore, by integrating Rin Section 3.1.1, we can establish a complete attribution pipeline ( i.e.,
targeted semantic region in generated image â‡’representation hLâ‡’neuron ul
k). Recall that Rhas
assigned the contribution for each dimension in hL, the neuron ul
kâ€™s contribution sl
kcan be defined
asal
k(Wl
out)kweighted by elements in R. That is, sl
k=al
k(Wl
out)kRâŠ¤, as illustrated in Figure 2 (b).
Contribution Score Considering Indirect Influence. While sl
kquantifies the neuron ul
kâ€™s direct con-
tribution to hLthrough the residual stream, it does not account for all influential factors. Specifically,
it overlooks the indirect contributions that neurons make through the attention mechanisms within
subsequent FFN layers. Supporting evidence exhibited in Appendix B verifies that this oversight
might lead to a bias. To address this issue, and in line with our objective to eschew complex computa-
tions like gradient, we implement a heuristic optimization of the current indicator sl
k. Specifically, we
employ the relative magnitude of neuron activation as another indicator to identify neurons that may
have a significant indirect contribution to hLâ€“ contribution which sl
kmight overlook.
Furthermore, considering the computation of sl
kalready incorporates al
k, the contributions reflected
by these two indicators may overlap. To prevent redundancy from summing or multiplying these two
metrics, we utilize the maximum function in our final score design:
Ë†sl
k=max{esl
k
PL
l=1Pd
k=1esl
k,eal
k
PL
l=1Pd
k=1eal
k}, (4)
where the normalization operation ensures fair competition between sl
kandal
k. Note that our experi-
ments in Section 4 have shown that this combined scoring approach is more effective than utilizing sl
k
oral
kalone, verifying their complementary nature. By computing contributions following Equation
(4)for various semantics across all layers, NAM identifies neurons that consistently demonstrate the
highest contributions to the generated images. These neurons, distinguished by their significant roles,
are designated as I-neurons responsible for targeted semantics in MLLMs.
3.2 Neuron Attribution for Text Generation
We then focus on how to acquire the neuronâ€™s contribution to the text outputs. Similar to the derivation
process of contribution score for image output, here we first focus on the contribution for output y
through the residual stream. Specifically, we expand hLas follows:
y=Wu 
mL+hLâˆ’1+aL
=LX
l=1WuWl
outAl+Wu(h0+LX
l=1al). (5)
According to a commonly used assumption for analyzing the internal mechanisms of LLMs, represen-
tations at any layer within the language models can be transformed into a distribution over the token
vocabulary Vusing the output embeddings [ 18,19,32,16,13,14]. Hence, WuWl
outâˆˆR|V|Ã—Hcan
be considered as the new unembedding matrix at the end of the residual stream, and Alcontributes to
the model output distribution yover the vocabulary through WuWl
outAl, as shown in Figure 2 (c).
Refined Contribution of Individual Neuron. We further disassemble Equation (5)to refine individ-
ual neuron ul
kâ€™s contribution to the output word. Specifically, denoting pas the index of the word
â€œdogâ€ in the vocabulary V, we have:
yp= (Wu)pWl
outAl+ (Wu)p(h0+LX
l=1al) =dX
k=1al
k(Wu)p(Wl
out)k+ (Wu)p(h0+LX
l=1al),(6)
where (Wu)pâˆˆR1Ã—dis the p-th row of Wu. According to Equation (6), the neuron ul
kâ€™s contribution
cl
k,pto the p-th word â€œdogâ€ on vocabulary can be obtained by cl
k,p=al
k(Wu)p(Wl
out)k, as illustrated
in Figure 2 (c). Furthermore, we would like to encourage the semantic specificity of the identified
crucial neurons â€“ that is, only preserving a single semantic concept with the maximum contribution,
while discarding other semantics. Formally, for the semantics of p-th word on vocabulary, the neuron
ul
kâ€™s contribution sl
kcan be defined as:
pâˆ—= arg max
pcl
k,p, sl
k=
cl
k,p ifp=pâˆ—,
0 otherwise .(7)
5By substituting sl
kto Equation (4), we can acquire the final attribution score of neuron ul
kfor
text output. The neurons that consistently exhibit the highest contributions are then designated as
T-neurons.
3.3 Image Editing Enhanced by NAM
Knowledge editing methods based on neuron attribution have already been explored for text outputs
[13,14,33,34]. Here, we focus on how to leverage attribution results to facilitate knowledge editing
of images. This objective requires replacing some semantics ( e.g., dog) with another semantics
(e.g., cat). To this end, we leverage the I-Neurons identified by NAM for image editing through a
straightforward, training-free approach. Specifically, we first construct the set of I-Neurons, U, for the
semantics like dog. Then, we collect the positions (l, k)of the neurons ul
kinU, and construct the set
of these position indices, I. For(l, k)âˆˆ I, we add a perturbation âˆ†(Wl
out)kto(Wl
out)kfollowing:
W={âˆ†(Wl
out)kfor(l, k)âˆˆ I}
âˆ†(Wl
out)k=arg min
W||X
(l,k)âˆˆIal
kâˆ†(Wl
out)k,(Ë†hLâˆ’hL)âŠ¤||2+Ï„||X
(l,k)âˆˆIâˆ†(Wl
out)kÂ·1âŠ¤||2,(8)
where hLandË†hLis the last representation of the base LLM when generating the image of dog and
cat, respectively; Ï„serves as a trade-off parameter. In Equation (8), the first term aims to facilitate a
shift in the image generation moduleâ€™s input from hLtoË†hL, while the second term is the â„“2norm
constraint for preventing drastic edits that might affect images containing other semantics. According
to this method, NAM can be utilized to enable simple and efficient images editing, underscoring the
significance and potential applications of the NAM for MLLMs2.
4 Experiment
In this section, we aim to validate the effectiveness of NAM from three aspects:
â€¢ What is the distribution of T/I-Neurons identified by NAM?
â€¢What properties do the T/I-neurons identified by NAM have? How to verify that the T/I-neurons
identified by NAM are more critical compared to the neurons identified by baseline methods?
â€¢ Can the T/I-Neurons identified by NAM facilitate the image editing within MLLMs?
4.1 Investigation Setup
Target Models & Datasets. Our research focuses on GILL [ 25] and NExT-GPT [ 35], two representa-
tive MLLMs with the capability of image generation. All experiments are conducted on the Common
Objects in Context (COCO) [ 36], a large-scale object detection, segmentation, and captioning dataset
including 80 object categories and five captions per image to conduct our experiments. Due to space
limitations, we only present the experimental results on GILL in this section. The remains and the
detailed implementations, such as the setting of hyper-parameters, can be found in Appendix B.
Baselines. We collect five advanced neuron attribution methods across three categories (gradient-
, activation-, and causality-based attribution). Specifically, their abbreviations and the attribution
scores they employ are: (1) AcT: neuron activation [ 5]; (2) AcU: The product of activation and the
unembedding matrix, focused on the dimension corresponding to the output word [ 18]; (3) GraD:
The gradient of the output dimension corresponding to the output word w.r.t activation. (4) GraT: The
product of the gradients and activation [ 19]; (5) GraI: The integral of the gradients [ 16]; (6) CE: The
causal effect of activation on outputs [ 13,14]. See detailed description in Appendix B.1. For NAM
and baselines stand and their role in rich literature, please refer to Appendix A ( i.e.,Related Work).
4.2 RQ1: Distribution of T/I-Neurons
We first focus on the distribution of T/I-neurons identified by NAM. Specifically, we randomly select
1000 images from the COCO dataset. Then, we feed each image to GILL individually and instruct
GILL to generate a similar image. The distributions of T/I-neurons are exhibited in Figures 3 (a) and
2The extension of NAM to broader scenarios will be detailed in Appendix C.
6Table 1: The semantics of T/I-neurons with the highest attribution scores identified by different
attribution methods, when the output of MLLMs containing the targeted semantics. (L l,Uk) denotes
thek-th neuron at layer l. For each method, we report semantics with top-4 probabilities.
Output Segmentation Category Method Location Semantics
â€œA girl is riding a horse .â€
T-NeuronsGrad L28.U4786 {â€˜dogâ€™, â€˜sharkâ€™, â€˜catâ€™, â€˜birdâ€™}
AcT L30.U13868 {â€˜animalsâ€™, â€˜animalâ€™, â€˜Animalâ€™, â€˜Animalsâ€™}
CE L27.U14262 {â€˜vehiclesâ€™, â€˜trucksâ€™, â€˜carsâ€™, â€˜boatsâ€™}
NAM L23.U5318 {â€˜horsesâ€™, â€˜horseâ€™, â€˜Horsesâ€™, â€˜Horsesâ€™ }
I-NeuronsGrad L29.U14374 {â€˜farmingâ€™, â€˜farmâ€™, â€˜farmsâ€™, â€˜agâ€™}
AcT L26.U12957 {â€˜animalâ€™, â€˜animalsâ€™, â€˜veterinâ€™, â€˜veterinaryâ€™}
CE L28.U1208 {â€˜Kinnikumanâ€™, â€˜cffffâ€™, â€˜Nantoâ€™, â€˜Vaultsâ€™}
NAM L23.U5318 {â€˜horsesâ€™, â€˜horseâ€™, â€˜Horsesâ€™, â€˜Horsesâ€™ }
â€œAdogis running on the lawn. â€
T-NeuronsGrad L28.U12056 {â€˜childâ€™, â€˜Childâ€™, â€˜childrenâ€™, â€˜maleâ€™}
AcT L24.U12845 {â€˜dogsâ€™, â€˜dogâ€™, â€˜Dogâ€™, â€˜canineâ€™}
CE L25.U3655 {â€˜thoseâ€™, â€˜Thoseâ€™, â€˜thatâ€™, â€˜thisâ€™}
NAM L24.U10710 {â€˜dogsâ€™, â€˜Dogâ€™, â€˜Dogsâ€™, â€˜petsâ€™ }
I-NeuronsGrad L26.U1135 {â€˜adoptâ€™, â€˜petâ€™, â€˜adoptingâ€™, â€˜adoptionâ€™}
AcT L30.U13868 {â€˜animalsâ€™, â€˜animalâ€™, â€˜Animalâ€™, â€˜Animalsâ€™}
CE L31.U1135 {â€˜weedsâ€™, â€˜chickensâ€™, â€˜compostâ€™, â€˜trashâ€™}
NAM L24.U12845 {â€˜dogsâ€™, â€˜dogâ€™, â€˜Dogâ€™, â€˜canineâ€™ }
â€œA small ship on the sea. â€
T-NeuronsGrad L26.U3972 {â€˜divingâ€™, â€˜diggingâ€™, â€˜dredâ€™, â€˜drillingâ€™}
AcT L28.U11438 {â€˜boatâ€™, â€˜carâ€™, â€˜phoneâ€™, â€˜vehicleâ€™}
CE L30.U3335 {â€˜inarâ€™, â€˜setâ€™, â€˜Setâ€™, â€˜camâ€™}
NAM L30.U2503 {â€˜shipâ€™, â€˜shipsâ€™, â€˜Shipâ€™, â€˜shippingâ€™ }
I-NeuronsGrad L27.U8984 {â€˜bushâ€™, â€˜treeâ€™, â€˜brushâ€™, â€˜shrâ€™}
AcT L25.U2539 {â€˜swimâ€™, â€˜swimmingâ€™, â€˜Swimâ€™, â€˜underwaterâ€™}
CE L25.U5113 {â€˜bronzeâ€™, â€˜signâ€™, â€˜boxâ€™, â€˜SIGNâ€™}
NAM L28.U10626 {â€˜shipâ€™, â€˜shipsâ€™, â€˜seaâ€™, â€˜oceanâ€™ }
Layer001000200030004000(a) I-Neurons
05 10 15 20 25 30
Layer00
Top 100(b) T -Neurons
05 10 15 20 25 30Top 300
Top 600
Layer00(c) T -Neurons & I-Neurons
05 10 15 20 25 301500300045006000
Top 100
Top 300
Top 600
1000200030004000T âˆ© I - Neurons
TâˆªI - NeuronsNumber
Figure 3: Distribution of (a) I-neurons, (b) T-neurons, and (c) intersection and subset of I-neurons
and T-neurons per layer identified by NAM, chosen by different number of neurons with top scores
on average. Best viewed in color.
(b), while Figure 3 (c) illustrates the distribution of intersections of T- and I-neurons. These results
demonstrate the following observations:
Observation 1: Within MLLMs, the crucial neurons for the text and image output containing
specific semantics predominantly occur in the middle and high layers of the base LLM. Note that
this observation is consistent with the previous works involving neuron attributions within LLMs
[5,13,18]. Additionally, the similar distribution of T and I neurons suggests that the formation time
of semantic concepts across different modalities in MLLMs may be consistent.
Observation 2: Figure 3 (c) reveals a partial overlap between T and I neurons. However, it also pro-
nounces distinctions between them. This finding substantiates the claim presented in the Introduction
of this paper: even for the same semantics, critical neurons are modality-specific within MLLMs.
4.3 RQ2: Properties of T/I-Neurons & Effectiveness of NAM
We then explore the following properties of T/I-neurons through comprehensive quantitative and
qualitative experiments: Semantic Relevance ,Cross-Sample Invariance , and Concept Specificity .
Additionally, by comparing these properties with those of neurons identified by various baselines, we
validate the effectiveness of our NAM. Note that we only present the results of the best-performing
baseline for each class, and remains are shown in Appendix B.
4.3.1 Semantic Relevance
Following previous studies [ 19,18], we treat the unembedding matrix and the second linear layer
matrix in the FFN as a projection from neuron activation to the probability distributions of the
vocabulary. Based on this, words with the average highest probability can be regarded as the relevant
7Table 2: Consistency between the neuronâ€™s semantics and the images/captions. Grad. ,Act. and
Ca.denote gradient-, activation- and causality-based methods, respectively. â€¡andâ‹†represent the
CLIPScore w.r.t input and output images. We use background highlights the best performance.
Class T-Neurons
Method Grad. Act. Ca.â€¡CLipScoreâ‹†CLipScore BERTScore MoveScore BLEURT
CE âœ“ 0.264Â±0.015 0.251Â±0.022 0.273Â±0.029 0.257Â±0.019 0.040Â±0.005
GraI âœ“ âœ“ 0.239Â±0.018 0.244Â±0.020 0.276Â±0.032 0.296Â±0.030 0.039Â±0.005
GraD âœ“ 0.378Â±0.047 0.396Â±0.032 0.457Â±0.027 0.436Â±0.030 0.064Â±0.008
GraT âœ“ âœ“ 0.425Â±0.040 0.422Â±0.029 0.486Â±0.018 0.477Â±0.036 0.072Â±0.009
AcT âœ“ 0.556Â±0.037 0.594Â±0.046 0.624Â±0.057 0.653Â±0.054 0.139Â±0.013
AcU âœ“ 0.543Â±0.051 0.624Â±0.057 0.618Â±0.054 0.609Â±0.038 0.135Â±0.014
NAM âœ“ 0.562Â±0.054 0.637Â±0.047 0.640Â±0.039 0.657Â±0.048 0.148Â±0.013
Class I-Neurons
Method Grad. Act. Ca.â€¡CLipScoreâ‹†CLipScore BERTScore MoveScore BLEURT
CE âœ“ 0.228Â±0.017 0.219Â±0.026 0.245Â±0.033 0.250Â±0.021 0.044Â±0.003
GraI âœ“ âœ“ 0.230Â±0.021 0.235Â±0.021 0.259Â±0.027 0.278Â±0.037 0.035Â±0.004
GraD âœ“ 0.370Â±0.042 0.377Â±0.026 0.432Â±0.040 0.409Â±0.041 0.058Â±0.006
GraT âœ“ âœ“ 0.432Â±0.038 0.394Â±0.032 0.453Â±0.042 0.437Â±0.038 0.068Â±0.008
AcT âœ“ 0.547Â±0.043 0.580Â±0.051 0.597Â±0.056 0.623Â±0.061 0.128Â±0.013
AcU âœ“ 0.501Â±0.061 0.601Â±0.054 0.559Â±0.039 0.548Â±0.052 0.137Â±0.013
NAM âœ“ 0.558Â±0.053 0.613Â±0.052 0.611Â±0.048 0.630Â±0.056 0.144Â±0.014
15 20 25 30 35 40 45bikecatbusairplaneboatwomandog
(b)dog
house
boat
bus
airplaneapplecatbeach
30035040045010%
15%
20%
25%
30%35%40%45%
120160200240
10 (a)Ratio of 
neuron overlap (%)
GraD GraT
AcT AcU CENAMGraI Semantics Ratios
 AcT
NAMAcU
Figure 4: Cross-sample invariance and semantic specificity of T/I-neurons. (a) exhibits invariance by
calculating the average ratio between T/I-neuronsâ€™ subset and intersection across different text/image
output; (b) quantifies specificity by showing: 1. the number of neurons crucial for specific semantics
solely and 2. the average number of neurons whose probability of being crucial to other semantics is
lower than a certain value. Best viewed in color.
semantics of neurons, as shown in Table 1. Note that the results of AcU are not exhibited since
its attribution score is exactly this probability. Furthermore, to explore the semantic relevance of
the neurons quantitatively, we calculate the consistency between the semantics of neurons and the
input/output images employing CLIPScore [ 37]. BERTScore [ 38], MoverScore [ 39], and BLEURT
[40] are also employed to quantify their consistency with (1) input imageâ€™s caption provided by the
dataset and (2) output imageâ€™s caption given by GPT [ 1]. Table 2 exhibits the average quantified
results. According to Table 1 and 2, we have the following observation:
Observation 3: The semantics of T/I-neurons identified by NAM align more closely with the
input/output images and their captions, while the other attribution methods typically identify the
neurons that are hardly correlated with the targeted semantics. The quantitative results share a similar
tendency, confirming the high semantic relevance of T/I-neurons and the effectiveness of our NAM.
4.3.2 Cross-sample Invariance
For different text/image outputs containing the same semantics, the T/I-neurons identified by the
attribution methods shall be consistent. To quantify this consistency, we instruct GILL to describe
and generate images for the same semantics ten times, and collect the set of T/I-neurons each time.
We then calculate the proportion of neurons that appeared in all ten sets as the quantification of
8cross-sample invariance. The average invariance Figure 4 (a) presents the average invariance for
different concepts. Specifically,
Pre-editing Post-editing Method Value ( â†“)
â€œboyâ€ â€œgirlâ€ CE 0.726
GraI 0.625
GraD 0.471
GraT 0.502
AcT 0.390
AcU 0.362
NAM 0.350
â€œdogâ€ â€œcatâ€ CE 0.601
GraI 0.540
GraD 0.458
GraT 0.321
AcT 0.259
AcU 0.288
NAM 0.255
â€œcantaloupeâ€ â€œappleâ€ CE 0.493
GraI 0.360
GraD 0.357
GraT 0.401
AcT 0.328
AcU 0.324
NAM 0.316
Table 3: Results of image editing. The Value rep-
resents the â„“2norm of the perturbation added to
the I-neurons, demonstrating that NAM necessi-
tates minimal perturbations for the editing.Observation 4: NAM outperforms all baselines
by an average of 16.83% w.r.t cross-sample invari-
ance across all semantics. This demonstrates that
NAM extracts the critical neurons for the targeted
semantics across samples, effectively filtering out
the neurons sensitive to sample-specific noise.
4.3.3 Semantic Specificity
Neurons that are crucial for specific semantics
should not be indiscriminately crucial across oth-
ers. Therefore, we study the neuronâ€™s specificity
in this part. We identify the Top-500 T/I-neurons
for the specific semantics. Then, we show (1) the
number of neurons that are crucial for specific se-
mantics solely and (2) the average number of neu-
rons whose probability of being crucial to other se-
mantics is lower than Îºâˆˆ {10%,15%, . . . , 45%}
in Figure 4 (b). The results of the three best-
performing methods are exhibited here. These re-
sults highlight that:
Observation 5: The T/I-neurons identified by our
NAM are specialized and not commonly sensitive
across different semantics, verifying their speci-
ficity across the semantics.
4.4 RQ3: Image Editing Enhanced by NAM
Lastly, we aim to edit the images generated by MLLMs through perturbing I-neurons identified by
NAM, as outlined in Section 3.3. Table 3 exhibits the pre- and post-editing semantics, the selected
images for collecting hLandË†hL, and the magnitude of the perturbations. Furthermore, for fair
comparisons, we also perturb I-neurons identified by baselines to achieve similar editing results.
According to Table 3 we can find that:
Observation 6: NAM-enhanced editing methods can not only replace the original semantics with the
target semantics precisely within the outputs of MLLMs, but also necessitate minimal perturbations.
Specifically, the perturbation it added is 40.2% less than the baselines on average, and nearly 15%
less than the best baseline, underscoring its significance and potential applications for MLLMs.
5 Limitations & Future Work
This study provides new insights into interpreting MLLMs, enhancing the understanding of their inner
working mechanize. However, while our experiments thoroughly investigated the neuron properties
within GILL and NExTGPT, they did not extend to a broader range of models. Additionally, although
the proposed attribution method can be transferred to any other modality, as demonstrated in Section
3.1.1, our experiments focused on text and image outputs solely. Looking forward, we plan to
incorporate more MLLMs and modalities into our research, and streamline our attribution method to
eliminate the reliance on external interpreters. By expanding the scope of our study and refining our
method, we aim to uncover more valuable insights that will benefit the MLLM community.
6 Conclusion
We propose NAM, a novel neuron attribution method tailored for MLLMs. Specifically, NAM is
tailored for multi-modal attribution, revealing the modality-specific semantic properties learned by
neurons within the FFN layers.To address the challenges of extending attribution methods from text-
9only LLMs to MLLMs, NAM first employs a leading image segmentation model to remove the noisy
semantics, then proposes a new attribution score to eliminate the need for additional forward/backward
propagation or gradient calculations. Based on NAM, we highlights several intriguing properties of
neurons, elucidating the inner workings mechanism of MLLMs.
Acknowledgments
This research is supported by the National Science and Technology Major Project (2023ZD0121102),
and National Natural Science Foundation of China (92270114).
References
[1]Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS ,
2020.
[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, AurÃ©lien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models. CoRR , abs/2302.13971, 2023.
[3]Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,
Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov,
Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily
Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Is-
ard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds,
Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira,
Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr,
Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von
Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski,
and et al. Gemini: A family of highly capable multimodal models. CoRR , abs/2312.11805,
2023.
[4]Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano,
Jan Leike, and Ryan Lowe. Training language models to follow instructions with human
feedback. In NeurIPS , 2022.
[5]Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li. Finding
skill neurons in pre-trained transformer-based language models. In EMNLP , pages 11132â€“11152.
Association for Computational Linguistics, 2022.
[6]Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, Yucheng Shi, Fan Yang, Tianming Liu, Xiaoming
Zhai, Wenlin Yao, Jundong Li, Mengnan Du, and Ninghao Liu. Usable XAI: 10 strategies
towards exploiting explainability in the LLM era. CoRR , abs/2403.08946, 2024.
[7]Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. Neuron-level interpretation of deep NLP
models: A survey. Trans. Assoc. Comput. Linguistics , 10:1285â€“1303, 2022.
[8]Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform
editing? surprising differences in causality-based localization vs. knowledge editing in language
models. In NeurIPS , 2023.
[9]Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer feed-forward
layers build predictions by promoting concepts in the vocabulary space. In EMNLP , pages
30â€“45. Association for Computational Linguistics, 2022.
10[10] Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, and Dimitris Bert-
simas. Finding neurons in a haystack: Case studies with sparse probing. CoRR , abs/2305.01610,
2023.
[11] Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, and Owen Lewis. Localizing paragraph
memorization in language models. CoRR , abs/2403.19851, 2024.
[12] Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xiaozhi Wang, Xu Han, Zhiyuan
Liu, Ruobing Xie, Maosong Sun, and Jie Zhou. Emergent modularity in pre-trained transformers.
InACL (Findings) , pages 4066â€“4083. Association for Computational Linguistics, 2023.
[13] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual
associations in GPT. In NeurIPS , 2022.
[14] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. Mass-
editing memory in a transformer. In ICLR . OpenReview.net, 2023.
[15] Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun
Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities.
InEMNLP , pages 10222â€“10240. Association for Computational Linguistics, 2023.
[16] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons
in pretrained transformers. In ACL (1) , pages 8493â€“8502. Association for Computational
Linguistics, 2022.
[17] Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual
associations in auto-regressive language models. In EMNLP , pages 12216â€“12235. Association
for Computational Linguistics, 2023.
[18] Haowen Pan, Yixin Cao, Xiaozhi Wang, and Xun Yang. Finding and editing multi-modal
neurons in pre-trained transformer. CoRR , abs/2311.07470, 2023.
[19] Sarah Schwettmann, Neil Chowdhury, Samuel Klein, David Bau, and Antonio Torralba. Multi-
modal neurons in pretrained text-only transformers. In ICCV (Workshops) , pages 2854â€“2859.
IEEE, 2023.
[20] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu.
Mm-llms: Recent advances in multimodal large language models. CoRR , abs/2401.13601, 2024.
[21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-
image pre-training with frozen image encoders and large language models. In ICML , volume
202 of Proceedings of Machine Learning Research , pages 19730â€“19742. PMLR, 2023.
[22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In
NeurIPS , 2023.
[23] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:
Enhancing vision-language understanding with advanced large language models. CoRR ,
abs/2304.10592, 2023.
[24] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani
Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith,
Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo:
An open-source framework for training large autoregressive vision-language models. CoRR ,
abs/2308.01390, 2023.
[25] Jing Yu Koh, Daniel Fried, and Russ Salakhutdinov. Generating images with multimodal
language models. In NeurIPS , 2023.
[26] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam
Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke
Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.
11[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. In ICML ,
volume 139 of Proceedings of Machine Learning Research , pages 8748â€“8763. PMLR, 2021.
[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-
resolution image synthesis with latent diffusion models. In CVPR , pages 10674â€“10685. IEEE,
2022.
[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , pages 5998â€“6008,
2017.
[30] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. EV A-02:
A visual representation for neon genesis. CoRR , abs/2303.11331, 2023.
[31] JoaoLages. diffusers-interpret. https://github.com/JoaoLages/diffusers-interpret ,
2019.
[32] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers
are key-value memories. In EMNLP (1) , pages 5484â€“5495. Association for Computational
Linguistics, 2021.
[33] Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Xiang Wang, Xiangnan He, and Tat
seng Chua. Alphaedit: Null-space constrained knowledge editing for language models, 2024.
[34] Houcheng Jiang, Junfeng Fang, Tianyu Zhang, An Zhang, Ruipeng Wang, Tao Liang, and Xiang
Wang. Neuron-level sequential editing for large language models, 2024.
[35] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any
multimodal LLM. CoRR , abs/2309.05519, 2023.
[36] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan,
Piotr DollÃ¡r, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV
(5), volume 8693 of Lecture Notes in Computer Science , pages 740â€“755. Springer, 2014.
[37] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore:
A reference-free evaluation metric for image captioning. In EMNLP (1) , pages 7514â€“7528.
Association for Computational Linguistics, 2021.
[38] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore:
Evaluating text generation with BERT. In ICLR . OpenReview.net, 2020.
[39] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. Mover-
score: Text generation evaluating with contextualized embeddings and earth mover distance. In
EMNLP/IJCNLP (1) , pages 563â€“578. Association for Computational Linguistics, 2019.
[40] Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. BLEURT: learning robust metrics for text
generation. In ACL, pages 7881â€“7892. Association for Computational Linguistics, 2020.
[41] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey
on multimodal large language models. CoRR , abs/2306.13549, 2023.
[42] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. In ICLR . OpenReview.net, 2021.
[43] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 , 2024.
[44] W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang, B Li, P Fung, and S Hoi. Instructblip: Towards
general-purpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint
arXiv:2305.06500 .
12[45] Yiren Jian, Chongyang Gao, and Soroush V osoughi. Bootstrapping vision-language learning
with decoupled language pre-training. In NeurIPS , 2023.
[46] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
Kosmos-2: Grounding multimodal large language models to the world. CoRR , abs/2306.14824,
2023.
[47] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,
Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. CoRR ,
abs/2307.05222, 2023.
[48] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language
generation via generative vokens. CoRR , abs/2310.02239, 2023.
[49] Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, HervÃ© JÃ©gou, and LÃ©on Bottou. Birth of a
transformer: A memory viewpoint. In NeurIPS , 2023.
[50] Cheongwoong Kang and Jaesik Choi. Impact of co-occurrence on factual knowledge of large
language models. In EMNLP (Findings) , pages 7721â€“7735. Association for Computational
Linguistics, 2023.
[51] Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, and Srijan
Kumar. Mysterious projections: Multimodal llms gain domain-specific visual capabilities
without richer cross-modal projections. CoRR , abs/2402.16832, 2024.
[52] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse
autoencoders find highly interpretable features in language models. CoRR , abs/2309.08600,
2023.
[53] Omer Antverg and Yonatan Belinkov. On the pitfalls of analyzing individual neurons in language
models. In ICLR . OpenReview.net, 2022.
[54] Chandan Singh, Aliyah R. Hsu, Richard Antonello, Shailee Jain, Alexander G. Huth, Bin Yu,
and Jianfeng Gao. Explaining black box text modules in natural language with language models.
CoRR , abs/2305.09863, 2023.
[55] Tom Lieberum, Matthew Rahtz, JÃ¡nos KramÃ¡r, Neel Nanda, Geoffrey Irving, Rohin Shah, and
Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice
capabilities in chinchilla. CoRR , abs/2307.09458, 2023.
[56] Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding
space. In ACL (1) , pages 16124â€“16170. Association for Computational Linguistics, 2023.
[57] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang,
Dawei Yin, and Mengnan Du. Explainability for large language models: A survey. CoRR ,
abs/2309.01029, 2023.
13A Related Work
Multi-modal Large Language Models. In recent years, MLLMs have made significant progress,
continually pushing the boundaries of performance in various downstream tasks [ 41]. The launch
of models like GPT-4 (Visual) and Gemini, with their impressive multimodal (MM) understanding
and generation capabilities, has sparked intense research interest in MM-LLMs [ 20]. In terms of
understanding multimodal content, preliminary research often utilizes multimodal encoders like the
ViT [ 42] and CLIP [ 27] ViT to capture representations, and projectors like Q-Former [ 43,44] and
P-Former [ 45] to align these representations with the embedding space of foundational LLMs. This
approach covers tasks such as image-text understanding, with representative models including BLIP-2
[21], MiniGPT-4 [23], LLA V A [22], and OpenFlamingo [24].
Recently, the capabilities of MM-LLMs have expanded to support specific modal outputs. These
methods align certain embeddings from the foundational LLM with the input space of a well-trained
multimodal generator through another projector. This extension includes tasks with image-text
outputs as demonstrated by models like GILL [ 25], Kosmos-2 [ 46], Emu [ 47], NExT-GPT [ 35]
and MiniGPT-5 [ 48], which is the focus of this paper. Recent research endeavors have focused on
mimicking human-like any-to-any modality conversion, shedding light on the path to artificial general
intelligence [20].
Interpretability of Pre-trained Transformers. Demystifying the internal mechanisms of LLMs
becomes increasingly vital, particularly in applications emphasizing fairness, trust, and ethical
decision-making [ 49,50,51,52,53,54,55]. In recent years, many works have focused on explaining
pre-trained transformers. For instance, [ 32] regards the FFN as unnormalized Key-Value Memories;
[56] presents a conceptual framework where all parameters are interpreted by projecting them into the
embedding space; [ 9] analyzes the FFN updates in the vocabulary space, showing that each update
can be decomposed to sub-updates; [ 17] studies how the model aggregates information about the
subject and relation to predict the correct attribute; while [ 11] localizes the weights and mechanisms
used by a language model to memorize and recite entire paragraphs of its training data.
Neuron Attribution. Neuron attribution aims to reveal the black box of pre-trained transformers
by answering the following questions [ 7,57,6]: (1) What concepts are learned within neurons
of the network? (2) Are there neurons that specialize in learning particular concepts? (3) How
localized/distributed and redundantly is the knowledge preserved within neurons of the network?
To achieve these goals, many recent works have focused on exploring the properties of neurons in
LLMs. Specifically, [ 16] present preliminary studies on how factual knowledge is stored in LLMs by
introducing the concept of knowledge neurons; [ 5] finds the special neurons whose activations on
soft prompts are highly predictive of the task labels of inputs, and dub them skill neurons.
For MLLMs, [ 18] employs the neuron contribution within the residual stream to identify multi-modal
neurons in Transformer-based multi-modal LLMs, while [ 19] claims that image prompts cast into
the transformer embedding space do not encode interpretable semantics, and translation between
modalities occurs inside the transformer. Additionally, despite the aforementioned methods are all
based on activation and gradients to identify target neurons, some recent works focused on knowledge
editing have discovered an alternative approach [ 15,13]. This involves using causal effect methods to
identify key neurons by perturbing neurons and observing changes in the output [14].
For the detailed contribution scores these works employed and their relationship with our NAM,
please see Appendix B.1.
B More Experiments
B.1 Experimental Settings
Baseline. Here we first detail the contribution score of the baseline methods used. We will use the
formulas and symbols from the paper to provide a formal explanation. Specifically, for the p-th word
in V ocabulary Vand the k-th neuron ul
kin layer l, we have:
â€¢ AcT takes the neuron activation as the contribution score following:
Ë†sl
k=ak
l. (9)
14â€¢AcU takes the product of the neuron activation and the unembedding matrix as the contribution
score following:
Ë†sl
k=cl
k,p=al
k(Wu)p(Wl
out)k. (10)
â€¢GraD takes the gradient of the output dimension corresponding to the output word w.r.t activation
as the contribution score following:
Ë†sl
k=âˆ‚yp
âˆ‚ak
l. (11)
â€¢ GraT takes the product of the gradients and activation as the contribution score following:
Ë†sl
k=ak
lâˆ‚yp
âˆ‚ak
l. (12)
â€¢GraI takes the product of the integral of the gradients and the activation as the contribution score
following:
Ë†sl
k=ak
lZ1
Î±=0âˆ‚yâ€²
p
âˆ‚ak
ldÎ±, (13)
where yâ€²
pis the p-th dimension of the output ywhen we fix the activation of neuron ul
kasÎ±Â·ak
l.
Note that perturbing and performing forward propagation for each neuron multiple times is time-
consuming. Therefore, we add the same coefficient Î±to multiple neurons simultaneously and
calculate their integrals to obtain their respective contribution scores.
â€¢CE takes the causal effect of activation on outputs, quantified by the output change when activation
is perturbed, as the contribution score following:
Ë†sl
k=âˆ‚yâ€²â€²
p
âˆ‚ak
l, (14)
where yâ€²â€²
pis the p-th dimension of the output ywhen we add the activation of neuron ul
kwith a
Gaussian Noise. We set the mean of the Gaussian noise to 0 and the variance to the total variance
of the neuron activation values in the respective layer. Similar to GraI, for each individual neuron,
performing forward propagation for each neuron multiple times is costly. Therefore, we add
perturbations to multiple neurons simultaneously and evenly distribute the resulting output changes
among them as their respective contribution scores.
Furthermore, it is important to note that the above baselines all attribute text outputs to neurons.
Therefore, we have adapted all these baselines to image outputs for comprehensive validation. For
example, the attribution score of the baseline Grad for text output is the gradient of activation when
the output is the word â€œdogâ€. When transferred to adapt the image output, its attribution score is the
gradient of activation when the output is an image of a dog. Specifically, since the last representation
his the input of the image generation module, we have:
Ë†sl
k=1Â·âˆ‚h
âˆ‚ak
l. (15)
Hyperparameter Configuration. For our experiments, we sourced the training and testing data for
the COCO dataset directly from its website3. Similarly, we obtained the source code for the target
models GILL4and NExT-GPT5, the image segmentation model EV A026, and the attribution algorithm
Diffuser Interpreter7used for stable diffusion from the links cited in their respective publications
All hyperparameter settings, such as the division of training and testing datasets, learning rate,
and optimizer, are consistent with the original configurations of the above link unless otherwise
stated. Additionally, it is important to note that, unless explicitly mentioned, the samples used in the
experiments were 500 images randomly selected from the COCO dataset.
Furthermore, we use Quadro RTX6000 GPUs with 24GB of memory as a representative example of
consumer-level GPUs; 40GB A100s and 80GB H100s to provide datacenter-level benchmarks.
3COCO: http://images.cocodataset.org
4GILL: https://github.com/kohjingyu/gill
5NExT-GPT: https://github.com/NExT-GPT
6EV A-02: https://github.com/baaivision/EV A/
7Diffuser-Interpreter: https://github.com/JoaoLages/diffusers-interpret
15Table 4: The semantics of T/I-neurons with the highest attribution scores identified by different
attribution methods, when the output of MLLMs containing the targeted semantics. (L l,Uk) denotes
thek-th neuron at layer l. For each method, we report semantics with top-4 probabilities.
Output Segmentation Category Method Location Semantics
T-NeuronsGrad L30.U10180 {â€˜cardâ€™, â€˜screenâ€™,â€˜Cardâ€™}
AcT L26.U8761 {â€˜coffeeâ€™, â€˜teaâ€™, â€˜brewâ€™}
CE L27.U8520 {â€˜CLSIDâ€™, â€˜Nantoâ€™, â€˜Kinnikumanâ€™}
NAM L27.U9810 {â€˜glassâ€™, â€˜Glassâ€™, â€˜budâ€™ }
I-NeuronsGrad L28.U3335 {â€˜spiritsâ€™, â€˜spiritâ€™, â€˜runâ€™}
AcT L30.U5844 {â€˜bitâ€™, â€˜lotâ€™, â€˜fewâ€™}
CE L31.U3393 {â€˜orderâ€™, â€˜Orderâ€™, â€˜ordersâ€™}
NAM L27.U413 {â€˜drinkâ€™, â€˜drinkingâ€™, â€˜drinksâ€™ }
T-NeuronsGrad L30.U4516 {â€˜rollâ€™, â€˜rollsâ€™, â€˜Rollâ€™}
AcT L28.U6023 {â€˜eatâ€™, â€˜eatingâ€™, â€˜eatenâ€™}
CE L25.U11833 {â€˜flexibleâ€™, â€˜brittleâ€™, â€˜bendâ€™}
NAM L30.U8704 {â€˜tasteâ€™, â€˜tastedâ€™, â€˜tastingâ€™ }
I-NeuronsGrad L30.U14400 {â€˜fruitâ€™, â€˜roseâ€™, â€˜appleâ€™}
AcT L27.U6886 {â€˜snapâ€™, â€˜takenâ€™, â€˜snappedâ€™}
CE L25.U14616 {â€˜Kodâ€™, â€˜negativeâ€™, â€˜developsâ€™}
NAM L27.U2615 {â€˜cookiesâ€™, â€˜pancakesâ€™, â€˜bakedâ€™ }
T-NeuronsGrad L31.U404 {â€˜vaultsâ€™, â€˜70710â€™, â€˜20439â€™}
AcT L27.U8950 {â€˜driverâ€™, â€˜deriversâ€™, â€˜vehicleâ€™}
CE L24.U15330 {â€˜fewâ€™, â€˜lotâ€™, â€˜bitâ€™}
NAM L25.U3913 {â€˜bikeâ€™, â€˜bikesâ€™, â€˜cycleâ€™ }
I-NeuronsGrad L28.U1208 {â€˜20439â€™, â€˜Kinnikumanâ€™, â€˜Nantoâ€™}
AcT L27.U6437 {â€˜gunâ€™, â€˜carâ€™, â€˜bikeâ€™}
CE L25.U4343 {â€˜CLSIDâ€™, â€˜shapeshifterâ€™, â€˜coupleâ€™}
NAM L25.U3913 {â€˜bikeâ€™, â€˜bikesâ€™, â€˜cycleâ€™ }
Ablation Study. Our attribution score design primarily consists of two components: activation
and its mapped values on the target semantic dimension in the vocabulary. We aim to leverage
the complementary effects of both components, using them as indicators of indirect and direct
contributions, respectively. The results of baseline methods AcT and AcU can be approximated as
the attribution effects when using each component separately. Therefore, comparing our method with
AcT and AcU can also be considered as an ablation study. This comparison also reflects that using
only the mapped values (AcU) can introduce certain biases, leading to poorer performance across
various metrics.
External Model & Algorithm. Then, we introduce the semantic segmentation model EV A02 and
attribution algorithm for the stable diffusion model Diffuser Interpret. Specifically, EV A02 [ 30]
comprises a series of robustly optimized plain Vision Transformers (ViTs) of moderate model sizes,
featuring transferable bidirectional visual representations learned from a powerful CLIP vision
encoder via masked image modeling (MIM) pre-training. Compared to current leading vision models
with billions of parameters, the EV A-02 variants necessitate significantly fewer computational
resources, enabling a more in-depth exploration of often-overlooked aspects. Furthermore, Diffuser
Interpret [ 31] attributes the pixel values of the generated image to the input embedding of the stable
diffusion using gradient information. We also apply this strategy to the attribution of the projector
used by GILL.
B.2 More Experimental Results
Results on Semantic Relevance. To better illustrate the comparison between our attribution method
and the baseline attribution methods, we present additional qualitative experimental results to provide
a more intuitive comparison, as shown in Table 4. The effects demonstrated by these methods are
similar to those shown in the experimental results in the main text, indicating that the neurons
identified by our NAM method are semantically closer to the target semantics.
Results on NExTGPT. In the main body of the text, we have completed testing the effectiveness of
various neuron attribution algorithms with GILL as the target model. In this section, we expand our
experiments to include NExTGPT as the target model. Below, we present the key neurons identified
in NExTGPT and their semantic correlations with images and captions. The results can be found in
Table 5.
Consistency Between Neuronâ€™s Semantics and Caption Given by GPT. The main text only
presents the average consistency of the T/I-neurons identified by various attribution methods with the
captions of the input/output images. To refine our comparison, we provide the specific consistency
16Table 5: Consistency between the neuronâ€™s semantics and the images/captions. Grad. ,Act. and
Ca.denote gradient-, activation- and causality-based methods, respectively. â€¡andâ‹†represent the
CLIPScore w.r.t input and output images. We use background highlights the best performance.
Class T-Neurons
Method Grad. Act. Ca.â€¡CLipScoreâ‹†CLipScore BERTScore MoveScore BLEURT
CE âœ“ 0.258Â±0.017 0.244Â±0.020 0.257Â±0.032 0.261Â±0.025 0.038Â±0.006
GraI âœ“ âœ“ 0.228Â±0.018 0.233Â±0.022 0.268Â±0.034 0.290Â±0.035 0.042Â±0.006
GraD âœ“ 0.369Â±0.051 0.387Â±0.029 0.456Â±0.029 0.402Â±0.024 0.059Â±0.011
GraT âœ“ âœ“ 0.406Â±0.034 0.409Â±0.033 0.485Â±0.026 0.480Â±0.042 0.068Â±0.008
AcT âœ“ 0.607Â±0.036 0.587Â±0.044 0.608Â±0.065 0.632Â±0.055 0.120Â±0.014
AcU âœ“ 0.593Â±0.048 0.615Â±0.053 0.592Â±0.046 0.611Â±0.048 0.141Â±0.020
NAM âœ“ 0.614Â±0.055 0.620Â±0.055 0.616Â±0.042 0.633Â±0.036 0.145Â±0.021
Class I-Neurons
Method Grad. Act. Ca.â€¡CLipScoreâ‹†CLipScore BERTScore MoveScore BLEURT
CE âœ“ 0.217Â±0.024 0.210Â±0.019 0.238Â±0.036 0.254Â±0.039 0.041Â±0.005
GraI âœ“ âœ“ 0.226Â±0.035 0.242Â±0.018 0.246Â±0.031 0.265Â±0.035 0.033Â±0.005
GraD âœ“ 0.362Â±0.037 0.369Â±0.031 0.418Â±0.044 0.388Â±0.042 0.055Â±0.007
GraT âœ“ âœ“ 0.417Â±0.043 0.386Â±0.030 0.466Â±0.039 0.421Â±0.045 0.069Â±0.009
AcT âœ“ 0.533Â±0.042 0.564Â±0.048 0.568Â±0.062 0.608Â±0.073 0.119Â±0.015
AcU âœ“ 0.592Â±0.055 0.583Â±0.051 0.582Â±0.042 0.535Â±0.057 0.130Â±0.017
NAM âœ“ 0.608Â±0.047 0.585Â±0.049 0.592Â±0.041 0.635Â±0.067 0.142Â±0.018
scores w.r.t the captions generated by GPT for the output images. The detailed results are shown in
Table 6 and 7. These results also demonstrate the highest consistency scores of our attribution method
compared to the baselines.
C Broader Impact
In this paper, we present a novel neuron attribution method NAM to interpret the MLLMs. Based on
NAM, we reveal several intriguing neuron properties within MLLMs. These properties collectively
elucidate the inner workings of neurons within MLLMs, enhancing our understanding of their
capacity to process and generate multi-modal content. This approach can contribute to a wide range
of applications of MLLMs, boosting the MLLMs across various downstream tasks such as knowledge
editing [ 15,13,14]. We believe that the neuron properties drawn from NAM can shed light for future
research on MLLM community, and inspire further exploration into understanding neurons within
other pre-trained transformers.
17Table 6: Consistency between the neuronâ€™s semantics within GILL and the captions given by GPT.
Grad. ,Act.andCa.denote gradient-, activation- and causality-based methods, respectively. â€¡and
â‹†represent the CLIPScore w.r.t input and output images. We use background highlights the best
performance.
Class T-Neurons (GILL)
Method Grad. Act. Ca. BERTScore MoveScore BLEURT
CE âœ“ 0.254Â±0.035 0.263Â±0.029 0.036Â±0.013
GraI âœ“ âœ“ 0.262Â±0.038 0.278Â±0.028 0.040Â±0.008
GraD âœ“ 0.452Â±0.027 0.392Â±0.019 0.062Â±0.013
GraT âœ“ âœ“ 0.476Â±0.029 0.475Â±0.038 0.066Â±0.011
AcT âœ“ 0.599Â±0.057 0.623Â±0.046 0.113Â±0.010
AcU âœ“ 0.584Â±0.043 0.615Â±0.055 0.134Â±0.020
NAM âœ“ 0.601Â±0.037 0.618Â±0.028 0.142Â±0.019
Class I-Neurons (GILL)
Method Grad. Act. Ca. BERTScore MoveScore BLEURT
CE âœ“ 0.241Â±0.037 0.248Â±0.042 0.038Â±0.009
GraI âœ“ âœ“ 0.237Â±0.033 0.267Â±0.032 0.027Â±0.004
GraD âœ“ 0.409Â±0.036 0.382Â±0.036 0.053Â±0.008
GraT âœ“ âœ“ 0.470Â±0.035 0.408Â±0.041 0.070Â±0.011
AcT âœ“ 0.554Â±0.050 0.602Â±0.062 0.117Â±0.019
AcU âœ“ 0.571Â±0.037 0.528Â±0.062 0.124Â±0.014
NAM âœ“ 0.584Â±0.039 0.622Â±0.052 0.137Â±0.021
Table 7: Consistency between the neuronâ€™s semantics within NExTGPT and captions given by GPT.
Grad. ,Act.andCa.denote gradient-, activation- and causality-based methods, respectively. â€¡and
â‹†represent the CLIPScore w.r.t input and output images. We use background highlights the best
performance.
Class T-Neurons (NExTGPT)
Method Grad. Act. Ca. BERTScore MoveScore BLEURT
CE âœ“ 0.263Â±0.028 0.259Â±0.024 0.036Â±0.007
GraI âœ“ âœ“ 0.264Â±0.037 0.288Â±0.035 0.040Â±0.008
GraD âœ“ 0.455Â±0.025 0.393Â±0.028 0.060Â±0.014
GraT âœ“ âœ“ 0.482Â±0.024 0.472Â±0.039 0.070Â±0.010
AcT âœ“ 0.602Â±0.057 0.624Â±0.043 0.114Â±0.007
AcU âœ“ 0.585Â±0.037 0.623Â±0.042 0.135Â±0.016
NAM âœ“ 0.613Â±0.034 0.624Â±0.028 0.140Â±0.017
Class I-Neurons (NExTGPT)
Method Grad. Act. Ca. BERTScore MoveScore BLEURT
CE âœ“ 0.234Â±0.029 0.243Â±0.028 0.037Â±0.007
GraI âœ“ âœ“ 0.239Â±0.028 0.271Â±0.025 0.036Â±0.007
GraD âœ“ 0.417Â±0.040 0.382Â±0.037 0.054Â±0.008
GraT âœ“ âœ“ 0.458Â±0.034 0.406Â±0.041 0.070Â±0.014
AcT âœ“ 0.570Â±0.055 0.597Â±0.059 0.114Â±0.008
AcU âœ“ 0.579Â±0.039 0.528Â±0.054 0.126Â±0.018
NAM âœ“ 0.588Â±0.033 0.627Â±0.057 0.138Â±0.020
18NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: In this paper, we introduce the neuron attribution method for MLLMs aimed at
enhancing the transparency and reliability of MLLMs (See Abstraction and Introduction
Section).
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In this work, we systematically discuss the limitations of our research and
outline directions for future work (See Introduction Section).
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [NA]
Justification: This paper does not include experimental results related to theoretical aspects.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
19â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: We provide the code necessary for replicating the studies described in this
paper via an anonymous link, and we detail the experimental setup for the replication in the
article itself (See Abstraction Section).
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: For the datasets disclosed in the article, we have provided information regarding
their sources and origins (See Experimental Section).
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
20â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: we have specified all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: In this paper, we have reported error bars suitably and correctly defined or
other appropriate information about the statistical significance of the experiments.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
218.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the
experiments?
Answer: [Yes]
Justification: In this paper, we provide detailed information about the experimental resources,
including GPU configurations used in our studies.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The study presented in this paper conforms to the NeurIPS Code of Ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We have provided the societal impacts of the work.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
22Answer: [NA]
Justification: This paper does not address issues related to this aspect.
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All creators and original owners of the assets used in our paper, such as code,
data, and models, have been properly credited. We have explicitly mentioned the licenses
and terms of use for each asset and have ensured full compliance with these terms throughout
our research.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The research presented in this paper is not concerned with new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: This paper does not involve experiments or research related to human subjects.
Guidelines:
23â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not address potential risks incurred by study participants.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
24