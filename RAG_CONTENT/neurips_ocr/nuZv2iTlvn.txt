Non-Euclidean Mixture Model for Social Network
Embedding
Roshni G. Iyer
Computer Science Department
University of California, Los Angeles
Los Angeles, California, USA
roshnigiyer@cs.ucla.eduYewen Wang
Computer Science Department
University of California, Los Angeles
Los Angeles, California, USA
wyw10804@gmail.com
Wei Wang
Computer Science Department
University of California, Los Angeles
Los Angeles, California, USA
weiwang@cs.ucla.eduYizhou Sun
Computer Science Department
University of California, Los Angeles
Los Angeles, California, USA
yzsun@cs.ucla.edu
Abstract
It is largely agreed that social network links are formed due to either homophily
or social influence. Inspired by this, we aim at understanding the generation of
links via providing a novel embedding-based graph formation model. Different
from existing graph representation learning, where link generation probabilities are
defined as a simple function of the corresponding node embeddings, we model the
link generation as a mixture model of the two factors. In addition, we model the
homophily factor in spherical space and the influence factor in hyperbolic space to
accommodate the fact that (1) homophily results in cycles and (2) influence results
in hierarchies in networks. We also design a special projection to align these two
spaces. We call this model Non-Euclidean Mixture Model, i.e., NMM . We further
integrate NMM with our non-Euclidean graph variational autoencoder (V AE)
framework, NMM-GNN .NMM-GNN learns embeddings through a unified
framework which uses non-Euclidean GNN encoders, non-Euclidean Gaussian
priors, a non-Euclidean decoder, and a novel space unification loss component to
unify distinct non-Euclidean geometric spaces. Experiments on public datasets
show NMM-GNN significantly outperforms state-of-the-art baselines on social
network generation and classification tasks, demonstrating its ability to better
explain how the social network is formed.
1 Introduction
Social networks are omnipresent because they are used for modeling interactions among users on
social platforms. Social network analysis plays a key role in several applications, including detecting
underlying communities among users [ 1], classifying people into meaningful social classes [ 2],
and predicting user connectivity [ 3]. Most existing embedding models are designed based on the
homophily aspect of social networks [ 4,5]. They utilize the intuition that associated nodes in a
social network imply feature similarity, and an edge is usually generated between similar nodes.
Prior works have used shallow embedding models to represent homophily, like matrix factorization
and random-walk (Section 2), which are parameter intensive and do not employ message passing.
As an improvement, graph neural network (GNN) models (Section 2) have been proposed to more
effectively capture homophily by representing a node through its local neighborhood context.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).However, research of RaRE[6] and work of [ 7] show homophily is insufficient, and social influence
is also critical in forming connections. This is due to popular nodes having direct influence in forming
links [ 8]. For example, in Twitter network, users tend to follow celebrities in addition to users
who share similar interests [ 9]. Though RaREjointly models both factors, it has limitations in
modeling capabilities. Specifically, it is parameter intensive as each node embedding is fully parame-
terized through a Bayesian framework. Further, RaREassumes graphs are transductive, limiting
its performance in the practical inductive setting where new links not seen during training must be
predicted. Moreover, nearly all works embedding social networks utilize a single zero-curvature
Euclidean space, when in reality, network factors may create different topologies. Specifically, edges
generated by homophily tend to form cycles [ 10], while edges generated by social influence tend to
form tree structures [ 11,12]. From Riemannian geometry, people have found that networks with
cycles are best represented by spherical space embeddings [ 13], while tree structured networks are
best represented by hyperbolic space embeddings [ 14]. Thus, an end-to-end model to bridge social
network embeddings of distinct non-Euclidean geometric spaces is a promising direction.
Our motivation is two-fold: (1) We aim to understand how the social network is generated e.g., which
factors affect node connectivity and what topological patterns emerge in the network as a result. (2)
Using our learning from (1), we aim to design a more realistic deep learning model to explain how
the network is generated (inferring new connections). We summarize our contributions as follows:
‚Ä¢We propose Graph-based Non-Euclidean Mixture Model ( NMM ) to explain social network
generation. NMM represents nodes via joint influence by homophily (modeled in spherical
space) and social influence (modeled in hyperbolic space), while seamlessly unifying
embeddings via our space unification loss.
‚Ä¢To our knowledge, we are also the first to couple NMM with a graph-based V AE learning
framework, NMM-GNN . Specifically, we introduce a novel non-Euclidean V AE framework
where node embeddings are learned with a powerful encoder of GNNs using spherical and
hyperbolic spaces, non-Euclidean Gaussian priors, and unified non-Euclidean optimization.
‚Ä¢Extensive experiments on several real-world datasets on large-scale social networks,
Wikipedia networks, and attributed graphs demonstrate effectiveness of NMM-GNN in
social network generation and classification, which outperforms state-of-the-art (SOTA)
network embedding models.
2 Preliminary and Related Work
We provide an overview of social network embedding models and discuss advancements in non-
Euclidean graph learning.
2.1 Social Network Embedding
Several works that embed social networks merely model homophily [ 15], capturing node-node
similarity, without also considering a node‚Äôs social influence or node popularity e.g., a celebrity.
Homophily-based models include shallow embedding models and GNN embedding models. Most
shallow embedding models are either based on matrix factorization [ 16] or random-walk [ 17]. Though
GNN models [ 18] effectively learn on large networks and in inductive settings, they still fail to model
the social influence factor in the network. Further, even the model that captures both homophily
and social influence, RaRE[6], has limitations in that its fully-parameterized node embeddings
require large parameter size to be learned, and it models all nodes in Euclidean space, an approach
not effective in capturing different topologies (e.g., cycles and hierarchy) in the social network.
2.2 Non-Euclidean Geometry for Graphs
Non-Euclidean geometric spaces, commonly used to model surfaces in mathematics and physics, are
curved geometries that include spherical spaces with positive curvature, and hyperbolic spaces with
negative curvature [ 19]. Works have recently found Euclidean space modeling to be insufficient for
non-Euclidean graph-structured data [ 20]. Namely, spherical spaces have been shown to effectively
embed graphs with cyclic structure due to their positive curvature [ 21,13], while hyperbolic spaces
have been shown to effectively embed graphs with hierarchical structure due to their negative
curvature and exponential or "tree-like" growth of the space [ 22,23]. HGCN [ 24] and [ 25] are critical
works exploring GNNs in non-Euclidean spaces. Both these hyperbolic GNN methods have shown
2significant improvement on benchmark datasets by preserving hierarchical structure of graphs. In
knowledge graphs, [ 26] has achieved notable performance by modeling graphs in non-Euclidean
spaces of various curvatures, using both hyperbolic and spherical space.
3 Methodology
ùê¥
Encoder Decoder·àöùê¥
ùíõùëñùëÜùëùùëíùëñùëó=1dists(ùíõùëñùëÜ,ùíõùëóùëÜ))Reconstruction loss
Loss termsùëùùúÉùëíùëñùëó=1
Mixture Modelùëùùëíùëñùëó=1disth(ùíõùëñùêª,ùíõùëóùêª))KL 
Divergence
ùíõùëñùêªspace 
unification
Spherical 
Gaussian Priorùëûùúì(ùíõùëñùêª|ùê¥,ùë£ùëñ)
Hyperbolic GNN
ùëû‚àÖ(ùíõùëñùëÜ|ùê¥,ùë£ùëñ)
Spherical GNN‚àÄùëñ‚ààùê¥input graphrecovered graph
ùëù(ùíõùëñùêª)
ùëù(ùíõùëñùëÜ)KL 
Divergence1‚àíùõæ
ùõæ‚äó
‚äó‚äï
‚äï
Hyperbolic 
Gaussian Prior
(a)
ùíõùëñùëÜ‚Ä≤=projs(ùíõùëñùêª)
ùíõùëñùêª
ùíõùëñùëÜ‚Ä≤
ùíõùëñùëÜmin(dists(ùíõùëñùëÜ,ùíõùëñùëÜ‚Ä≤))
Surface of
Spherical ball (2D)Poincar√© ball (3D) (b)
Figure 1: Model Architecture. (a) Architecture overview of NMM-GNN , a non-Euclidean Mixture Model
with non-Euclidean V AE framework. (b) Illustration of space unification loss of NMM-GNN .zH
ifrom the
hyperbolic space is projected to its corresponding point in the spherical space, zS‚Ä≤
i, such that its geodesic
distance is ensured to be close to vi‚Äôs existing spherical space representation, zS
i.
A social network G= (V, A)consists of a set of vertices V={vi}N
i=1, and associated adjacency
matrix eij‚ààA, where eijis an edge from vitovj. We aim to design a model to jointly learn both
node homophily and social influence representation, denoted zS
iandzH
irespectively, that can best
explain the social network in terms of link reconstruction. In this section, we describe our architecture.
First, we introduce a novel non-Euclidean mixture model, called NMM , to model the probability of
a new link. Second, we add a non-Euclidean GNN encoder to enrich NMM , called NMM-GNN ,
which is connected to the variational autoencoder framework. Source code is in the Appendix. For
details on future directions, the reader is also referred to the Limitations section in the Appendix.
3.1 Overview
To model both factors of homophily and social influence that affect graph connectivity, we model each
node viwith homophily regulated representation, zS
i, and social influence regulated representation,
zH
i1. A unified framework is designed such that zS
iandzH
iinfluence each other bi-directionally
and are seamlessly merged through NMM .NMM utilizes non-Euclidean geometric spaces to better
represent homophily and social influence components which produce curved structures like cycles
and trees. To improve NMM , we enrich its encoder by GNNs, qœà(zH
i|G)andqœï(zS
i|G). To ensure
generated embeddings are in spherical and hyperbolic spaces respectively, non-Euclidean GNNs are
adopted. The enhanced model is NMM-GNN , which has a clear connection to the V AE framework.
Figure 1(a) illustrates the architecture overview of NMM-GNN . Specifically, the encoder component
maps nodes into homophily based embedding, zS, in spherical space (for homophily generated
cycles) and social influence based embedding, zH, in hyperbolic space (for social influence generated
trees), which follow non-Euclidean prior distributions. The embeddings are passed into our mixture
model decoder, which models the probability of a link as a mixture of a homophily based distribution
component and a social influence based distribution component. The objective is to maximize the
likelihood to observe the links, or equivalently to minimize the link reconstruction loss. In addition,
the two geometric spaces are ensured to be aligned together via a space unification regularization
term, to make sure the two embeddings of the same node are corresponding to each other.
3.2 Modeling via Non-Euclidean Mixture Model (NMM)
Geometrically, the space of NMM is a spherical surface inside a unit Poincar√© ball. Each node
vicorresponds to (1) a point in the Poincar√© ball, zH
i, and (2) a point on the spherical surface,
1The geometric space community conventionally uses Sto represent spherical space and Hto represent
hyperbolic space.
3zS
i, where both spaces are embedded inside a Euclidean space. These two points are aligned by
enforcing projection of zH
ionto the spherical surface to be close to zS
i, as projecting a star onto
earth‚Äôs atmosphere, shown in Fig. 1(b). We note that the space alignment does not mean embeddings
are enforced to be the same in two spaces2. Rather, we require the projection of zH
iis close to i‚Äôs
embedding in spherical space zS
i. In this case, the two distances of spherical and hyperbolic spaces
are also different from each other (norm space difference vs. spherical geodesic distance). Without
the space alignment, zH
ihas too much degree of freedom, which can move freely as long its norm is
kept the same. Lastly, the probability of generating a link is a mixture of the probability of generating
the link in each non-Euclidean space.
3.2.1 Modeling for homophily
Representation of homophily regulated nodes. We embed homophily based representation of
node vi, orzS
i, on the surface of the spherical ball in spherical space, Sd. Formally, Sd={zS
i‚àà
Rd‚à•zS
i‚à•=wS}is the d-dimensional wS-norm ball, where ‚à•¬∑‚à•is the Euclidean norm and wS‚àà[0,1)
is a constant to ensure the spherical surface is inside the unit Poincar√© ball. To better capture
homophily distribution, p(zS
i), we use spherical Gaussian distribution GS(¬∑)as spherical prior,
described in Table 1. Œ≤is the axis or direction of the lobe controlling where the lobe is located on the
sphere, and points towards the center of the lobe; Œªis the sharpness of the lobe such that as this value
increases, the lobe will become narrower in width; and ais the amplitude or intensity of the lobe,
corresponding to the height of the lobe at its peak.
Link prediction using homophily based distribution. The probability of link eij= 1 between
nodes viandvj, also described in Table 1, is determined by the geodesic distance between zS
iandzS
j.
dists(zS
i,zS
j) = arccos( ‚ü®zS
i,zS
j‚ü©)is the geodesic distance between zS
iandzS
j;J >0andB‚â•0
are model parameters; and ‚ü®¬∑,¬∑‚ü©denotes the inner product. The probabilistic model is designed so
nodes greatly dissimilar (exhibiting low homophily), or dists(zS
i,zS
j)‚Üí+‚àû, have low probability
of a link being generated, or phom(eij= 1)‚Üí0. In the case of nodes exhibiting high homophily,
they either (1) may be connected due to highly similar characteristics, or (2) may not be connected
simply because the individuals do not know each other. Our distribution models both scenarios. Note
when dists(zS
i,zS
j)‚Üí0,phom(eij= 1)‚Üí1
1+eB, which can be interpreted as a factor to control
sparsity of the network.
Table 1: Homophily regulated nodes: Distribution Prior and Link Generation Probability
(a) Spherical Distribution Prior
p(zS
i) =GS(zS
i;Œ≤, Œª, a )
=aeŒª(Œ≤¬∑zS
i‚àí1)(b) Link Generation Probability
phom(eij= 1) = p(eij= 1|dist s(zS
i,zS
j))
=1
1 +eJ√ódists(zS
i,zS
j)+B
3.2.2 Modeling for social influence
Representation of social influence regulated nodes. We embed social influence based representa-
tions of node vi, orzH
i, on the Poincar√© ball from hyperbolic space, Hd+1, to better capture resulting
hierarchical structures. Social influence regulated nodes are represented as points, zH
i, belonging in-
side the Poincar√© (open) ball in Hd+1. Formally, Hd+1={zH
i‚ààRd+1‚à•zH
i‚à•=wH
zi}, wH
zi‚àà[0,1),
is(d+1)-dimensional wH
zi-norm ball, where ‚à•¬∑‚à•is Euclidean norm. We assume center of the Poincar√©
ball is aligned with center of the sphere, and is one dimension larger than the sphere to ensure the
spherical surface is inside the Poincar√© ball. To better capture a social influence regulated distribution,
p(zH
i), we use Hyperbolic Gaussian distribution GH(¬∑)as non-Euclidean Gaussian prior, described
in Table 2. zH
iis the origin of (r, œâ)for radius rand angle œâin polar coordinates. zH
iis the center of
mass and Œ∂ >0is the dispersion parameter, where the dispersion dependent normalization constant
Z(Œ∂)accounts for the underlying non-Euclidean geometry. Z(Œ∂)is partitioned into angular œâand
radial rcomponents. Œì(¬∑)is Euler‚Äôs gamma function, and distH(¬∑)is the hyperbolic distance between
two hyperbolic space node embeddings, xandy, where ‚à•¬∑‚à•denotes Euclidean norm.
2Note that it is impossible to equate these two embeddings directly as they are in different geometric spaces.
4Link prediction using social influence based distribution. We model existence of an edge, eij= 1
between nodes viandvj, also described in Table 2, as a function of norm space difference, disth(¬∑),
between two hyperbolic space node embeddings, zH
iandzH
j. We utilize norm space difference as
opposed to hyperbolic distance, since nodes of similar social influence status may have large distance
in the Poincar√© (open) ball due to nodes possibly being placed towards the ball‚Äôs boundary. This
is because, at the boundary of the ball, nodes become infinitely distanced apart. Thus, to allow for
numerical stability and to capture social influence difference (in which the higher the social influence
of nodes indicated by large in-degree and smaller out-degree, the closer they are embedded towards the
center of the ball), we utilize norm space as indicator. Consistent with the notation of [ 6], a node with
higher social influence is associated with a smaller norm value. CandDare learned model parameters
and norm space of a vector is norm( x) =‚à•x‚à•.disth(zH
i,zH
j) =|norm( zH
i)‚àínorm( zH
j)|is
the norm difference between zH
iandzH
j;C > 0andD‚â•0are model parameters; and norm( ¬∑)
denotes the L1 normalization function. The probabilistic model is designed such that nodes largely
different in popularity (exhibiting high social influence), or disth(zH
i,zH
j)‚Üí+‚àû, have high
probability of a link being generated, or prank(eij= 1)‚Üí1. In the case both nodes exhibit low
social influence (such as having similar social rank), they either (1) may be connected due to highly
similar characteristics, or (2) may not be connected simply because individuals do not know each
other. Our distribution models both scenarios. When disth(zH
i,zH
j)‚Üí0,prank(eij= 1)‚ÜíeD
1+eD,
which can be interpreted as another factor to control sparsity of the network.
Table 2: Social influence regulated nodes: Distribution Prior and Link Generation Probability
(a) Hyperbolic Distribution Prior
p(zH
i) =GH(zH
i;zH
i, Œ∂)
=1
Z(Œ∂)e‚àídistH(zH
i,zH
i)2
2Œ∂2(1)
Z(Œ∂) =Zœâ(Œ∂)Zr(Œ∂) (2)
Zœâ(Œ∂) = Vol( Hd)
=œÄd
2
Œì(d
2+ 1)(3)
Zr(Œ∂) =Z+‚àû
0e‚àír2
2Œ∂2sinhd(r)dr
=1
2ddX
k=0 
d
k!
(‚àí1)kr
œÄ
2Œ∂e(2k‚àíd)2Œ∂2
2 erfc (2k‚àíd)Œ∂‚àö
2(4)
dist H(x,y) = arrcosh 
1 +2‚à•x‚àíy‚à•2
(1‚àí ‚à•x‚à•2)(1‚àí ‚à•y‚à•2)
(5)(b) Link Generation Probability
prank(eij= 1)
=p(eij= 1|dist h(zH
i,zH
j))
=eC√ódisth(zH
i,zH
j)+D
1 +eC√ódisth(zH
i,zH
j)+D
3.2.3 Non-Euclidean Mixture Model
Link Prediction Using Mixed Space Distribution. Since both homophily and social influence
affect the connectivity structure of social networks, we model existence of a new link between nodes,
pŒ∏(eij= 1) , as a weighted combination distribution of these factors. Specifically, our non-Euclidean
mixture model is a weighted combination of homophily based distribution, phom(eij= 1) , and social
influence based distribution, prank(eij= 1) , with learned weight Œ≥:
pŒ∏(eij= 1) = pŒ∏(eij= 1|zS
i,zS
j,zH
i,zH
j)
=Œ≥¬∑phom(eij= 1) + (1 ‚àíŒ≥)¬∑prank(eij= 1)(6)
where phom(eij= 1) is modeled in positively curved spherical space since homophily based links
may form cycles due to similarity connections between node clusters. prank(eij= 1) is modeled in
negatively curved hyperbolic space since social influence based links may form tree-like structures
due to popularity-based social hierarchy between node clusters. We would like to highlight that
the link between nodes iandjis a mixture model because each link is a weighted combination of
influence from both spherical and hyperbolic spaces (not one or the other) as evidenced in Equation 6.
As shown in Figure 1b, the same node has two representations ‚Äì one in the spherical space and one in
the hyperbolic space, and because they represent the same underlying node, they need to be aligned.
Therefore, these two network factors do not contradict each other, but rather work together to explain
how links are formed between users.
53.3 Modeling via Non-Euclidean V AE on Graphs
We enrich the encoder of NMM to generate better embeddings. To do so, we explore GNN methods
which have been shown to be more effective than shallow embedding methods (see Experiments
and Ablation Studies). We refer to this enriched NMM model as NMM-GNN .NMM-GNN uses
non-Euclidean V AE as its learning framework. The framework integrates a mixture of different non-
Euclidean geometric spaces e.g., hyperbolic and spherical spaces, for learning of encoder, decoder,
and node prior distributions, and ensures geometric spaces are unified during training.
Encoder model. The encoder learns two corresponding embedding representations per node vito
produce spherical embedding zS
iand hyperbolic embedding zH
i. For homophily regulated nodes
in spherical space, Sd, any spherical space GNN (SGNN) can be applied, and for social influence
regulated nodes in hyperbolic space, Hd+1, any hyperbolic space GNN (HGNN) can be applied.
The general framework for SGNN and HGNN are shown in Tables 3 and 4. zS(l)
i‚ààRd(l)and
zH(l)
i‚ààRd+1(l)are spherical and hyperbolic feature representations of node viat layer l, with
dimensionality dand(d+ 1) respectively. fis a message-specific neural network function of
incoming messages to vifrom neighborhood context Ni, and activation function œÉ(¬∑), typically
ReLU( ¬∑)for all layers but the last one being softmax( ¬∑).
Table 3: General Framework and Example Model for Spherical Graph Neural Network.
(a) General Framework: Spherical GNN
zS(l+1)
i =qSGNN (zS
i|A, vi)
=qœï(zS
i|A, vi)
=œÉ X
j‚ààNif(zS(l)
i,zS(l)
j)(b) Example: Spherical GCN
zS(l+1)
i =qœï(zS
i|A, vi)
= expS
0
œÉ
WT
l X
j‚ààNi‚à™{i}ej,i‚àömjmilogS
0(zS(l)
j)
Table 4: General Framework and Example Model for Hyperbolic Graph Neural Network.
(a) General Framework: Hyperbolic GNN
zH(l+1)
i =qHGNN (zH
i|A, vi)
=qœà(zH
i|A, vi)
=œÉ X
j‚ààNif(zH(l)
i,zH(l)
j)(b) Example: Hyperbolic GCN
zH(l+1)
i =qœà(zH
i|A, vi)
= expH
0
œÉ
WT
l X
j‚ààNi‚à™{i}ej,i‚àömjmilogH
0(zH(l)
j)
Example Non-Euclidean Encoder models. We illustrate Non-Euclidean Graph Convolutional
Neural Network (Non-Euclidean GCN) as an example GNN. Tables 3 and 4 describe the model
architectures for both Spherical GCN and Hyperbolic GCN respectively. Node features are initialized
by sampling each embedding dimension from a distribution uniformly at random for all nodes where
zS(0)
i‚ààRd(l)‚ÜêUnif([0 ,1))dandzH(0)
i‚ààRd+1(l)‚ÜêUnif([0 ,1))d+1respectively. The retraction
operator, R(¬∑), involves mapping between spaces. For non-Euclidean spaces, retraction is performed
between non-Euclidean space and approximate tangent Euclidean space using logarithmic and
exponential map functions. Specifically, logH
0(zH
i) = tanh‚àí1(i¬∑ ‚à•zH
i‚à•)zH
i
i¬∑‚à•zH
i‚à•is a logarithmic map
at center 0from hyperbolic space to Euclidean tangent space, and expH
0(zH
i) = tanh(i ¬∑‚à•zH
i‚à•)zH
i
i¬∑‚à•zH
i‚à•
is an exponential map at center 0from Euclidean tangent space to hyperbolic space. logS
0(zS
i) =
tanh‚àí1(‚à•zS
i‚à•)zS
i
‚à•zS
i‚à•is a logarithmic map at center 0from spherical space to Euclidean tangent space
andexpS
0(zS
i) = tanh( ‚à•zS
i‚à•)zS
i
‚à•zS
i‚à•is an exponential map at center 0from Euclidean tangent space
to spherical space. where zS(l)
i,zH(l)
iare embeddings of node viat layer l‚àà[0, L),L= 2;Wlis
a layer-specific learnable weight matrix; Niis the set of nodes in the neighborhood context of vi;
ej,iis the edge-weight between nodes vj‚Üívi, with default edge weight being 1.0if an edge exists.
mi,mjare entries of the degree matrix, with mi= 1 +P
j‚ààNiej,i.
Decoder model. NMM can be considered as a probabilistic decoder for link generation, which
maps embeddings of two nodes into the probablity to generate a link between them.
6Joint loss function. The training loss involves components of reconstruction loss (to ensure the
generated graph is consistent with the original graph), KL divergence loss (to ensure predicted em-
beddings zS
iandzH
iclosely match their non-Euclidean Gaussian distributions), and space unification
loss (to ensure zS
iandzH
imap to the same node vi).
Reconstruction Loss. Table 5 shows reconstruction loss, which minimizes the upper bound on the
negative log-likelihood. ŒªAis a hyperparameter; A‚Ä≤=XAXT, given X‚àà0,1k√ódwhere Xa,i= 1
only if node a‚ààÀúGis assigned to i‚ààGandXa,i= 0otherwise, where ÀúGis the predicted graph.
Table 5: Description for Reconstruction Loss.
p(A‚Ä≤|zS,zH) =1
k(k‚àí1)X
aÃ∏=bA‚Ä≤
a,blogÀúAa,b+ (1‚àíA‚Ä≤
a,b)log(1 ‚àíÀúAa,b) (7)
‚àílogpŒ∏(G|zS
i,zS
j,zH
i,zH
j) =‚àíŒªAlogp(A‚Ä≤|zS
i,zS
j,zH
i,zH
j) (8)
LS
recon(œï, Œ∏;G) =Eqœï(zS
i|G)[‚àílogpŒ∏(G|zS
i,zS
j,zH
i,zH
j)] (9)
LH
recon(œà, Œ∏;G) =Eqœà(zH
i|G)[‚àílogpŒ∏(G|zS
i,zS
j,zH
i,zH
j)] (10)
KL Divergence Loss. The KL divergence loss is formed by minimizing the equations described in
Table 6. Minimizing the KL divergence loss ensures that the homophily regulated nodes and social
influence regulated nodes closely align to their underlying non-Euclidean distribution priors. As
described in Section 3, these distributions are designed to appropriately capture the distinct topologies
that emerge as a result of the respective social network factors.
Space Unification Loss. The space unification loss is formed by minimizing the equations described
in Table 6. Minimizing the space unification loss ensures that the hyperbolic space representation of
node viin spherical space, projS(zH
i), is close to the corresponding learned representation of node
viin the spherical space, zS
i. Note that using a normalized hyperbolic disk is not a substitute for the
projection operator from the social influence hyperbolic space onto the homophily spherical space.
The projection operation solely projects out-of-sphere nodes onto the wSnorm space, or the norm
at the surface of the spherical ball. A normalization operator would instead change the embedding
values of all nodes. More importantly, the space unification loss component ensures minimal spherical
geodesic distance between zH
i‚Äôs representation on the spherical ball and zS
i, illustrated in Figure 1(b).
Table 6: Description of KL Divergence Loss and Space Unification Loss.
(a) KL Divergence Loss
LS
KL(œï;G) = KL[ qœï(zS
i|G)||p(zS
i)] (11)
LH
KL(œà;G) = KL[ qœà(zH
i|G)||p(zH
i)] (12)(b) Space Unification Loss
projS(x) =(
wS¬∑x
‚à•x‚à•if‚à•x‚à• Ã∏=wS
x otherwise(13)
Lunify(G) = dist s(projS(zH
i),zS
i) (14)
Total Loss. The overall loss function for homophily regulated and social influence regulated nodes
respectively is then a summation of the above loss components given by:
LS=LS
recon(œï, Œ∏;G) +LS
KL(œï;G) +Lunify(G) (15)
LH=LS
recon(œà, Œ∏;G) +LH
KL(œà;G) +Lunify(G) (16)
3.4 Training
This section details NMM-GNN ‚Äôs training framework, using non-Euclidean V AE, for representing
social networks. We describe the optimization method for variables from Sections 3.2 and 3.3.
Embedding Initialization. We randomly initialize all embeddings of zS
iandzH
i. For homophily
regulated nodes, we choose a value for norm wS, that is sampled from uniform distribution: wS:
wS‚àà[0,1)‚ÜíUnif([0 ,1))for all nodes, and for social influence regulated nodes, we choose a
value for norm wH
zi, assigned uniformly at random per node. We set curvature values of spherical
and hyperbolic spaces as KS= 1andKH=‚àí1respectively. We leave the non-trivial problem of
learning optimal curvatures as future work.
Training procedure for homophily regulated nodes. Parameter optimization for learning node
embeddings is performed using Riemannian stochastic gradient descent (RSGD) for the spherical
space as shown in Table 7. To ensure the updated node embeddings remain in norm- wSspace, we
perform a rescaling operation, projS, to project out-of-boundary embeddings back to the surface of
7Table 7: Training Procedure for homophily regulated nodes and social influence regulated nodes.
(a) Homophily Regulated Nodes
r(zS
i,t, LS) = 
1 +zST
i,t‚àáLS(zS
i,t)
‚à•‚àáLS(zS
i,t)‚à•
(I‚àízS
i,tzST
i,t) (17)
zS
i,t+1‚ÜêprojS 
‚àíŒ∑t¬∑r(zS
i,t, LS)‚àáLS(zS
i,t)
(18)
SGDS(x) :xt+1‚Üêxt‚àíŒ∑t‚àáLS(xt) (19)(b) Social Influence Regulated Nodes
zH
i,t+1‚ÜêzH
i,t‚àíŒ∑t(1‚àí ‚à•zH
i,t‚à•2
2)2‚àáLH(zH
i,t) (20)
SGDH(x) :xt+1‚Üêxt‚àíŒ∑t‚àáLH(xt) (21)
Table 8: Dataset statistics for evaluation datasets.
Dataset # Vertices # Edges Type # Classes
BlogCatalog 10.3K 334.0K undirected 39
LiveJournal 4.8M 69.0M directed 10
Friendster 65.6M 1.8B undirected ‚Äì
thewS-ball. We further update scalar parameters JandB(from the homophily regulated distribution)
andŒ≤, Œª, a (from the spherical gaussian prior) through stochastic gradient descent (SGD) as defined
below via SGDS(J),SGDS(B),SGDS(Œ≤),SGDS(Œª),SGDS(a).
Training procedure for social influence regulated nodes. Parameter optimization for learning
node embeddings is performed using RSGD for the hyperbolic space as shown in Table 7. The
corresponding norm space, wH
zi, is also learned through RSGD by updating embeddings of zH
i. We
further update scalar parameters CandD(from the social influence regulated distribution) and Œ∂(from
the hyperbolic gaussian prior) through SGD as defined below via SGDH(C),SGDH(D),SGDH(Œ∂).
Training procedure for NMM-GNN weights. Parameter optimization for Œ≥uses SGD as follows,
where Ltotal=LS+LH:
Œ≥t+1‚ÜêŒ≥t‚àíŒ∑t‚àáLtotal(Œ≥t) (22)
4 Experiments
We comprehensively evaluate NMM-GNN on social network generation for popular large-scale
social networks through multi-label classification and link prediction tasks against competitive SOTA
baselines in various categories. The Appendix section further details Ablation Studies where we test
quality of using (1) a mixture model, (2) distinct non-Euclidean geometric spaces, (3) non-Euclidean
GNN-based encoders and non-Euclidean GraphV AE framework (through the inductive setting), and
(4) space unification loss component. All experiments and each of the ablation studies consistently
show NMM-GNN outperforms baseline network embedding models on all metrics for all datasets.
4.1 Datasets
For comprehensive evaluation, we assess our models on real-world datasets from well-known social
media venues: BlogCatalog (BC) [27],LiveJournal (LJ) [28], and Friendster (F) [29] which are
friendship networks among bloggers. Table 8 provides statistics of the datasets. In the Appendix,
we also include experiments for Wikipedia datasets, to show that our model can also benefit other
networks. Our research goal is to design an embedding model to better explain how the social network
is formed. Thus, to separate model contribution from the learned representation, we focus on the
setting of featureless graphs since quality of node features can be a confounding factor in determining
quality of our model. However, since our model can handle feature graphs, we also provide these
experiments in the Appendix, with our model outperforming all baselines on attributed networks.
4.2 Models
Baselines. We compare NMM-GNN to SOTA network embedding models, in Table 9, and report
results in Table 10. We omit comparison to prior models e.g., LINE [30] due to lower performance.
We also highlight that unlike prior works like Œ∫-GCN , our model overcomes limitations of the product
space, e.g., where the entire model belongs to a Cartesian product of non-Euclidean geometric spaces
by default. Our work is in a category called mixed space model that uses a multi-geometric space
framework where different portions of the graph may possibly belong to different spaces (based
on the amount of impact each of homophily and social influence has for that personalized pair of
8Table 9: Category and description of baseline models.
Category Description
Structural GraRep [31], shallow embedding integrating global structural information
Embedding RolX [32], unsupervised learning approach using structural role based similarity
Models GraphWave [33], shallow embedding model using spectral graph wavelet diffusion patterns
GraphSAGE [34], inductive framework using node features and neighbor aggregation
GNN Embedding GCN [35], semi-supervised learning model via graph convolution on local neighborhoods
Models GAT [36], graph attention model using mask self-attention layers on local neighborhoods
(Euclidean space) GIN [37], graph embedding model based on the Weisfeiler-Lehman (WL) graph isomorphism test
GRAPH CL[38], graph contrastive learning framework for unsupervised graph data
Homophily-based GELTOR [17], embedding method using learning-to-rank with AdaSim* similarity metric
Embedding Models NRP [27], embedding model using pairwise personalized PageRank on the global graph
GNN Embedding Models HGCN [24], hyperbolic GCN model utilizing Riemannian geometry and hyperboloid model
(non-Euclidean space) Œ∫-GCN [39], GCN model using product space e.g., product of constant curvature spaces
Mixture Models RaRE [6], Bayesian probabilistic modelfor node proximity/popularity via posterior estimation
(homophily and NMM , our non-Euclidean mixture model (see Eqn. 6), without use of GraphV AE framework
social influence) NMM-GNN , our non-Euclidean mixture model with non-Euclidean GraphV AE framework
nodes). In the extreme case (Case 1) where only social influence is at play, e.g., weight of homophily
representation is learned close to 0, the hyperbolic space will be used. On the other hand if only
homophily is at play (Case 2), e.g., weight of homophily representation is learned close to 0, the
spherical space will be used. In the normal case of both factors at play (Case 3), then both spaces will
be used and can be jointly aligned with our space alignment mechanism. When using product space,
Cases 1, 2, and 3 will all not be distinguished from each other as all cases will be modeled by one
complex non-Euclidean geometric space as a Cartesian product of spherical and hyperbolic spaces.
Time Complexity of NMM-GNN .Regarding our model, NMM is highly efficient, with time
complexity O(ed+nd), where nis number of nodes, eis number of edges, and dis dimension size.
In comparison, the time complexity analyses for the remaining baseline models are as follows: the
mixture model of RaREisO(ed+nd)which is comparable to the NMM mixture model, and the
GNN embedding models of GCN ,GAT (with one-head attention), and Œ∫-GCN (forŒ∫= 0) are
O(ed+nd2).Œ∫-GCN (forŒ∫Ã∏= 0) and HGCN ‚Äôs time complexities are O(ed+a¬∑nd2), where ais
the filter length, and NMM-GNN isO(ed+nd2)which is comparable to GNN embedding models.
As our work focuses on improving accuracy of learned embeddings, we further use GraphV AE
training with NMM to achieve SOTA performance. We would like to point out that GraphV AE
(ofNMM-GNN ) training is also designed to be highly parallelizable, which allows for scalability.
Moreover, our model is capable of learning on real-world, highly large-scale graphs on the order of
millions of nodes and billion of edges, e.g., Friendster, while achieving the best performance, which
attests to its practical value to the network science community.
4.3 Evaluation
We detail our evaluation procedure for multi-label classification and link prediction. For all experi-
ments, for fairness of comparison to baselines, we utilize the experiment procedure of [ 6]. Specifically,
90% of links are randomly sampled as training data. We do not perform cross-validation, since it may
cause overfitting to occur as our framework uses learnable parameters e.g., zS,zH,J,B,C,D,Œ≥,Œ≤,
Œª,Œ±,Wl, andŒ∂which is a function of zHequivalently interpreted as mean square error. Per dataset,
we choose hyperparameter values for ŒªAin reconstruction loss: {0, 1, 2, 4, 8, 16, 32, 64}, step sizes
Œ∑t: {0.005, 0.001, 0.01, 0.05, 0.1}, and experiments are performed on AWS cluster (8 Nvidia GPUs).
4.3.1 Classification
Evaluation results are in Table 10. We observe that mixture models (homophily and social influence),
achieve better performance on all datasets for all metrics. Specifically, it improves over structural em-
bedding models ( Graph Wave), GNNs ( GAT ,HGCN ), and homophily-based models ( GELTOR ,
NRP ). We also see learning embeddings in non-Euclidean geometric spaces helps better represent
structures in social networks ( HGCN vs.GAT ,RaREvs.NMM ). We further observe using GNN-
based encoders with GraphV AE learning yields additional improvement ( NMM vs.NMM-GNN ).
4.3.2 Link Prediction
As from [ 6], we measure quality of link prediction by sorting probability scores of every pair of nodes
per model and evaluating them using area under the ROC curve (AUC) score. Specifically, 10% of
existing edges and non-existing edges are hidden from training set, and probabilities are examined by
9Table 10: Results of social network classification and link prediction for Jaccard Index (JI) ,Hamming
Loss (HL) ,F1 Score (F1) , and AUC in % using embedding dimension 64. Our NMM and its variants are
in gray shading. For each group of models, the best results are bold-faced. The overall best results on each
dataset are underscored.‚Ä†Ablation study variant models using distinct non-Euclidean geometric spaces for
NMM (homophily/social influence) where E,S, andHdenote Euclidean, Spherical, and Hyperbolic spaces.
Datasets BlogCatalog LiveJournal Friendster
Metrics JI HL F1 AUC JI HL F1 AUC JI HL F1 AUC
GraRep 36.0 28.2 45.6 87.9 40.1 41.1 35.2 56.7 53.6 34.2 40.6 89.8
RolX 37.2 25.4 48.7 90.4 40.9 38.0 35.6 60.1 58.8 33.9 40.9 90.3
GraphWave 39.5 22.8 48.9 92.3 42.2 37.6 35.9 60.1 59.0 31.5 41.1 90.5
GraphSAGE 45.4 20.1 49.3 92.0 45.5 34.7 34.1 59.0 64.1 28.7 43.4 90.5
GCN 47.3 19.5 55.1 91.6 46.7 31.2 47.8 62.6 66.5 28.0 47.2 91.9
GAT 47.9 19.3 54.5 91.4 47.4 28.5 49.0 65.3 66.3 28.0 46.8 92.0
GIN 47.1 19.7 56.2 91.5 48.6 28.3 48.1 67.2 66.0 27.7 48.1 92.3
GRAPH CL 47.5 19.4 55.8 91.3 49.7 27.9 49.0 69.4 68.1 25.5 49.9 92.8
GELTOR 47.4 19.3 54.9 92.0 51.0 28.9 48.6 65.3 66.7 27.9 47.5 91.7
NRP 61.6 20.4 65.2 95.5 69.7 24.5 64.0 78.7 72.2 22.6 52.8 92.2
HGCN 56.7 19.2 60.9 92.7 58.8 27.1 57.7 68.5 69.9 24.3 49.9 93.3
Œ∫-GCN 61.6 20.7 65.4 95.3 63.6 27.3 57.2 69.1 69.4 24.1 50.3 93.1
RaRE 61.4 20.6 65.6 95.1 74.2 23.8 65.1 79.9 75.7 22.5 55.0 94.4
NMM (Hd/Sd)‚Ä†56.6 19.8 62.3 95.1 74.0 28.4 55.5 68.8 74.6 26.9 50.6 93.0
NMM (Sd/Sd)‚Ä†57.1 19.6 65.9 94.0 74.7 27.6 57.1 69.0 75.3 26.2 52.5 93.4
NMM (Ed/Ed)‚Ä†57.9 19.5 66.3 95.4 75.1 25.0 58.4 71.2 77.0 24.7 52.8 94.5
NMM (Sd/Ed)‚Ä†59.2 19.2 67.1 95.5 75.3 24.4 59.3 74.5 77.5 23.3 54.3 94.5
NMM (Hd/Hd)‚Ä†58.4 19.0 66.7 95.3 75.6 24.6 61.9 76.0 78.8 23.3 55.0 94.7
NMM (Ed/Hd)‚Ä†60.3 19.1 67.8 95.7 76.2 23.2 64.4 79.2 79.1 22.6 55.4 94.5
NMM (ours) 62.7 19.0 70.9 95.8 76.5 22.7 67.3 84.2 79.8 22.1 56.3 94.8
NMM-GNN (ours) 62.6 17.3 78.8 96.9 78.6 20.4 67.3 86.8 83.3 21.8 57.7 94.9
the model. Further, 10% of non-training edges are used for validation. For fairness against baselines
on undirected networks, we treat all directed networks as undirected. Table 10 shows evaluation results
for AUC score. Comparing relative score differences between best performing homophily embedding
model ( NRP ) toRaRE, we observe that LiveJournal andFriendster datasets contain relatively more
node social influence than BlogCatalog , which homophily-based models do not capture. Due to
the above observation, it is likely that LiveJournal andFriendster (which are also larger datasets)
show more realistic heterogeneity in network structure compared to BlogCatalog dataset e.g., cyclic
structures produced by homophily based nodes and tree-like structures produced by social influence
based nodes. Thus, modeling these structures in non-Euclidean spaces ( RaREvs.NMM ) also
shows more improvement. Moreover, in our NMM variant models, every node is influenced by both
factors of homophily and social influence through our non-Euclidean mixture model. It is a weighted
combination personalized per node of these factors that influence the links formed, rather than being
generated solely through homophily vs. social influence.
5 Conclusions
We are among the first to explore a Graph-based non-Euclidean mixture model for social networks.
As social networks are influenced by homophily and social influence, we design a model to represent
both factors jointly for nodes. Further, we model resulting unique network topologies (cycles and
trees) using distinct non-Euclidean geometric spaces and introduce a GNN-based non-Euclidean
variational autoencoder framework for our model, to effectively learn embeddings. The resulting
model, NMM-GNN , significantly outperforms various state-of-the-art models for social networks.
As future work, we hope to explore alternatives to graph-based V AE methods for improved learning.
106 Acknowledgments and Disclosure of Funding
This work was partially supported by DARPA HR0011-24-9-0370; NSF 1937599, 2106859, 2119643,
2200274, 2211557, 2303037, 2312501; NIH U54HG012517, U24DK097771; Optum AI; NASA;
SRC JUMP 2.0 Center; Amazon Research Awards; and Snapchat Gifts.
References
[1]Huiwen Xiang, Mingjing Xie, and Yiyi Fang. Study on the architecture space-social network
characteristics based on social network analysis: A case study of anshun tunpu settlement.
Elsevier , 2024.
[2]Farzana Afridi, Amrita Dhillon, and Swati Sharma. The ties that bind us: Social networks and
productivity in the factory. Elsevier , 2024.
[3]Austin P. Logan, Phillip M. LaCasse, and Brian J. Lunday. Social network analysis of twitter
interactions: a directed multilayer network approach. Springer , 2023.
[4]Wei Jiang, Xinyi Gao, Guandong Xu, Tong Chen, and Hongzhi Yin. Challenging low homophily
in social recommendation. WWW , pages 3476‚Äì3484, 2024.
[5]Kazi Khanam, Gautam Srivastava, and Vijay Mago. The homophily principle in social network
analysis: A survey. Springer , 2023.
[6]Yupeng Gu, Yizhou Sun, Yanen Li, and Yang Yang. Rare: Social rank regulated large-scale
network embedding. In WWW , pages 359‚Äì368, 2018.
[7]Liye Ma, Ramayya Krishnan, and Alan L. Montgomery. Latent homphily or social influence?
an empirical analysis of purchase within a social network. In Management Science INFORMS ,
pages 454‚Äì473, 2015.
[8]A.L. Barabasi, H. Jeong, Z. Neda, E. Ravasz, A. Schubert, and T. Vicsek. Evolution of the
social network of scientific collaborations. Elsevier Science , 2002.
[9]Julian McAuley and Jure Leskovec. Learning to discover social circles in ego networks.
NeurIPS , 2012.
[10] Li Sun, Mengjie Li, Yong Yang, Xiao Li, Lin Liu, Pengfei Zhang, and Haohua Du. Rcoco:
contrastive collective link prediction across multiplex network in riemannian space. Springer ,
2024.
[11] Xuelian Ni, Fei Xiong, Yu Zheng, and Liang Wang. Graph contrastive learning with kernel
dependence maximization for social recommendation. WWW , pages 481‚Äì492, 2024.
[12] Zitai Qiu, Congbo Ma, Jia Wu, and Jian Yang. An efficient automatic meta-path selection for
social event detection via hyperbolic space. WWW , pages 2519‚Äì2529, 2024.
[13] Cosimo Gregucci, Mojtaba Nayyeri, Daniel Hern√°ndez, and Steffen Staab. Link prediction with
attention applied on multiple knowledge graph embedding models. WWW , pages 2600‚Äì2610,
2023.
[14] Min Zhou, Menglin Yang, Bo Xiong, Hui Xiong, and Irwin King. Hyperbolic graph neural
networks: A tutorial on methods and applications. KDD , pages 5843‚Äì5844, 2023.
[15] Azad Noori, Mohammad Ali Balafar, Asgarali Bouyer, and Khosro Salmani. Review of
heterogeneous graph embedding methods based on deep learning techniques and comparing
their efficiency in node classification. Springer , 2024.
[16] Xin Liu, Tsuyoshi Murata, Kyoung-Sook Kim, Chatchawan Kotarasu, and Chenyi Zhuang. A
general view for network embedding as matrix factorization. In WSDM , pages 375‚Äì383, 2019.
[17] Masoud R. Hamedani, Jin-Su Ryu, and Sang-Wook Kim. Geltor: A graph embedding method
based on listwise learning to rank. WWW , pages 6‚Äì16, 2023.
11[18] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to
train deeper gcns. IEEE TPAMI , 2023.
[19] Peter Petersen. Riemannian geometry , volume 171. Springer, 2006.
[20] Gabriel Moreira, Manuel Marques, Jo√£o P. Costeira, and Alexander Hauptmann. Hyperbolic vs
euclidean embeddings in few-shot learning: Two sides of the same coin. IEEE WACV , pages
2082‚Äì2090, 2024.
[21] Yansong Ning, Hao Liu, Hao Wang, Zhenyu Zeng, and Hui Xiong. Uukg: Unified urban
knowledge graph dataset for urban spatiotemporal prediction. NeurIPS , 2024.
[22] Jongmin Park, Seunghoon Han, Soohwan Jeong, and Sungsu Lim. Hyperbolic heterogeneous
graph attention networks. WWW , pages 561‚Äì564, 2024.
[23] Fragkiskos Papadopoulos, Maksim Kitsak, M. √Ångeles Serrano, Mari√°n Bogu√±√°, and Dmitri
Krioukov. Popularity versus similarity in growing networks. Nature , pages 537‚Äì540, 2012.
[24] Ines Chami, Zhitao Ying, Christopher R√©, and Jure Leskovec. Hyperbolic graph convolutional
neural networks. NeurIPS , 32, 2019.
[25] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. NeurIPS , 32,
2019.
[26] Roshni G. Iyer, Yunsheng Bai, Wei Wang, and Yizhou Sun. Dual-geometric space embedding
model for two-view knowledge graphs. In KDD , pages 676‚Äì686, 2022.
[27] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, and Sourav S. Bhowmick. Homogeneous
network embedding for massive graphs via reweighted personalized pagerank. VLDB , pages
670‚Äì683, 2020.
[28] Feng Xia, Lei Wang, Tao Tang, Xin Chen, Xiangjie Kong, Giles Oatley, and Irwin King. Cengcn:
Centralized convolutional networks with vertex imbalance for scale-free graphs. IEEE TKDE ,
pages 4555‚Äì4569, 2022.
[29] Danah M. Boyd. Friendster and publicly articulated social networking. ACM CHI , 2004.
[30] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-
scale information network embedding. In WWW , pages 1067‚Äì1077, 2015.
[31] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global
structural information. CIKM , pages 891‚Äì900, 2015.
[32] Keith Henderson, Brian Gallagher, Tina Eliassi-Rad, Hanghan Tong, Sugato Basu, Leman
Akoglu, Danai Koutra, Christos Faloutsos, and Lei Li. Rolx: structural role extraction & mining
in large graphs. KDD , pages 1231‚Äì1239, 2012.
[33] Claire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node
embeddings via diffusion wavelets. KDD , pages 1320‚Äì1329, 2018.
[34] William L. Hamilton, Rex Ying, and Jure Lescovec. Inductive representation learning on large
graphs. NeurIPS , 30, 2017.
[35] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. ICLR , 2017.
[36] Petar Veli Àáckovi ¬¥c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. ICLR , 2018.
[37] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? ICLR , 2019.
[38] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen.
Graph contrastive learning with augmentations. NeurIPS , 2020.
12[39] Gregor Bachmann, Gary B√©cigneul, and Octavian Ganea. Constant curvature graph convolu-
tional networks. In ICML , pages 486‚Äì496. PMLR, 2020.
[40] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Lescovek. Graphrnn:
Generating realistic graphs with deep auto-regressive models. ICML , 2018.
[41] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Sourav S. Bhowmick, and Juncheng Liu.
Scaling attributed network embedding to massive graphs. VLDBJ , pages 37‚Äì49, 2023.
[42] Qiaoyu Tan, Xin Zhang, Xiao Huang, Hao Chen, Jundong Li, and Xia Hu. Collaborative graph
neural networks for attributed network embedding. IEEE TKDE , 2023.
[43] Jure Leskovec, Kevin J. Lang, Anirban Dasgupta, and Michael W. Mahoney. Community
structure in large networks: Natural cluster sizes and the absence of large well-defined clusters.
Internet Mathematics , 2009.
13Appendix
A Limitations
NMM is highly efficient, with time complexity O(ed+nd), where nis number of nodes, eis
number of edges, and dis dimension size. As our work focuses on improving accuracy of learned
embeddings, we further use GraphV AE training with NMM to achieve SOTA performance, called
NMM-GNN . Using the GraphV AE training pipeline may be seen as a limitation as it is less efficient
compared to GNNs for training time. However, in the worst case scenario, NMM-GNN still achieves
efficiency comparable to popular heterogeneous graph models including GraphRNN [ 40], and training
is designed to be highly parallelizable, which allows for scalability. Moreover, our model is capable
of learning on real-world, highly large-scale graphs on the order of millions of nodes and billion of
edges, e.g., Friendster , while achieving the SOTA performance, which attests to its practical value to
the network science community.
Our model also makes the assumption the homophily regulated nodes lie on the surface of the
spherical ball and the social influence regulated nodes lie on the open Poincare ball, which are
both natural representations for modeling nodes. People have observed that more popular celebrity
nodes tend to have smaller norm space and are embedded towards the center of the ball (similar to
higher order concepts in the knowledge graph space), while less popular nodes (containing less social
influence) are embedded towards the boundary of the ball (similar to entities in the knowledge graph
space). To ensure that we can compute an intersection space (for the space unification component
to unify the homophily and social influence representation for the same node), we enforce that the
spherical surface norm is in the Poincare ball e.g., any learned soft value norm space between 0 and 1.
In compute, while we evaluate on a cluster of 8 GPUs, at least one GPU is necessary for running our
experiments (which may been seen as compute limitation).
Regarding ethical considerations and fairness, privacy and fairness are often topics of focus regarding
social networks. Our model uses information like node popularity based on graph structure (in degree
vs. out degree ratio), as well as attributed features if available. Further it also infers links through
neighborhood context by looking at connected nodes. In general, a network embedding model uses
personal information from the user which may impinge on their privacy. However, we make every
attempt in our model to allow the user to select their granular choice of model personalization (e.g.,
we provide the option to choose from whether or not users want to enable their attributed information
like profile interest and history being learned).
B A Note on Graph-Level Learning
Our work can be generalized to the graph-level because our method learns to represent social science
network factors based on topologies in the graph on clusters of nodes and edges. Thus, if the cluster
of nodes and edges comprised of the entire graph and we subsequently applied graph pooling per
node embedding (that we currently learn), we can reduce the embedding from node level to graph
level. In this way, homophily and social influence can be modeled at the graph level. That said, it is
unclear whether graph-level modeling would be specifically useful or interpretable for social network
embedding models as compared to node-level modeling. This is due to the social network setting
requiring links to be generated that are per node and not at the graph level, because a user (modeled
as a node) is recommended to another specific user in the practical social network setting. This is
different from other network domains like molecular classification where the entire graph represents
one molecule e.g., atoms form individual nodes and chemical bonds form the edges. For this reason,
node-level learning is in fact consistent with the recent state-of-the-art NN methods in the network
science community though NMM-GNN can still be generalized to learning at the graph-level.
C Additional Experiments
In this section, we provide experiment results on Wikipedia networks and on attributed graphs. As
in the case for social networks, we also evaluate on multi-label classification and link prediction
for Wikipedia networks and attributed graphs. For all experiments, for fairness of comparison to
baselines, we utilize experiment procedure of [ 6]. Specifically, 90% of links are randomly sampled
14as training data. We do not perform cross-validation, since it may cause overfitting to occur as our
framework uses learnable parameters e.g., zS,zH,J,B,C,D,Œ≥,Œ≤,Œª,Œ±,Wl, and Œ∂which is a
function of zHequivalently interpreted as mean square error. Per dataset, we choose hyperparameter
values for ŒªAin reconstruction loss: {0, 1, 2, 4, 8, 16, 32, 64}, and step sizes Œ∑t: {0.005, 0.001, 0.01,
0.05, 0.1}.
Evaluation on Wikipedia networks. In Table 12, we additionally present multi-label classification
and link prediction experiments on Wikipedia datasets to show the benefits of our model on other
networks. These include Wikipedia Clickstream [6], which contains counts of (referrer, resource)
pairs extracted from the request logs of Wikipedia, and Wikipedia Hyperlink [6], which contains
edges as hyperlinks from one page to another. Dataset statistics for the Wikipedia datasets are
summarized in Table 11. Our NMM model variants consistently achieves the best performance
over all the competitive baseline models belonging to categories of (1) structural embedding models,
(2) GNN embedding models (Euclidean space), (3) homophily-based embedding models, (4) GNN
embedding models (non-Euclidean space), and (5) mixture models. This shows that NMM ‚Äôs model
is generalizable and widely applicable because it can learn effective representations on online
information networks that go beyond social networks.
Table 11: Dataset statistics for evaluation datasets.
Dataset # Vertices # Edges Type # Classes
Wikipedia Clickstream 2.4M 15.0M directed 6
Wikipedia Hyperlink 488K 5.5M directed 6
Table 12: Results of social network classification and link prediction for Jaccard Index (JI) ,Hamming
Loss (HL) ,F1 Score (F1) , and AUC in % using embedding dimension 64. Our NMM and variants are
in gray shading. For each group of models, best results are bold-faced. The overall best results on each
dataset are underscored.‚Ä†Ablation study variant models using distinct non-Euclidean geometric spaces for
NMM (homophily/social influence) where E,S, andHdenote Euclidean, Spherical, and Hyperbolic spaces.
Datasets Wikipedia Clickstream Wikipedia Hyperlink
Metrics JI HL F1 AUC JI HL F1 AUC
GraRep 31.9 28.3 44.4 79.6 48.3 22.7 50.2 76.8
RolX 32.2 28.1 44.9 85.4 57.1 17.6 56.0 82.6
GraphWave 32.8 27.0 44.6 86.1 55.3 18.1 54.8 81.8
GraphSAGE 33.1 26.6 45.1 89.3 62.8 8.3 68.7 89.4
GCN 33.1 24.1 50.7 89.5 63.7 6.5 76.4 92.0
GAT 33.4 24.2 51.2 89.0 64.2 6.5 76.2 92.1
GIN 33.7 24.4 50.6 91.2 65.1 6.9 76.1 92.3
GRAPH CL 33.3 24.1 51.0 90.9 64.5 6.6 76.4 92.7
GELTOR 33.2 24.1 50.9 91.4 64.0 6.8 76.7 92.3
NRP 46.5 20.9 57.1 91.8 75.5 7.1 81.1 96.9
HGCN 38.1 20.6 54.3 89.9 69.8 6.0 78.4 94.1
Œ∫-GCN 37.6 20.9 55.0 90.2 75.0 7.1 81.6 96.9
RaRE 47.8 20.7 58.0 93.0 75.7 6.8 82.3 97.5
NMM (Hd/Sd)‚Ä†44.8 27.8 44.2 86.5 76.0 8.2 76.4 92.1
NMM (Sd/Sd)‚Ä†45.1 22.9 55.7 90.3 78.4 7.1 79.7 97.0
NMM (Ed/Ed)‚Ä†46.2 21.7 56.0 90.6 79.2 6.8 82.0 97.5
NMM (Sd/Ed)‚Ä†47.7 20.9 56.2 91.0 79.9 6.6 82.3 97.6
NMM (Hd/Hd)‚Ä†47.4 20.9 56.8 91.2 81.0 6.6 81.6 97.3
NMM (Ed/Hd)‚Ä†48.1 20.7 57.9 91.7 80.5 6.2 82.4 97.8
NMM (ours) 49.2 18.5 59.0 92.8 80.3 5.8 82.5 98.0
NMM-GNN (ours) 49.7 16.5 60.8 95.6 81.7 5.5 83.0 97.3
Evaluation on attributed graphs. As our model can also handle attributed graphs, we provide
experiments against SOTA attributed network embedding models that are summarized below. Since
these models require the presence of network attributes, we do not include them in our experiments
on featureless graphs for fairness of comparison. We conduct evaluation on well-known large-
scale attributed graph datasets which include Facebook [9] and Google+ [9] social networks where
consistent with [ 27], we treat each ego-network as a label and extract attributes from their user
profiles. Dataset statistics are in Table 13, and results are reported in Table 14. It can be seen that
ourNMM-GNN model consistently achieves the best performance in both the large-scale graph
datasets, indicating that our model‚Äôs inherent learning ability is effective to learn graph structure and
topology , which goes beyond simply exploiting information from node and edge attributes. Below is
a summary of the baseline attributed network embedding models:
15(a)
 (b)
Figure 2: Ablation studies. (a) Quality of mixture model, where NMM hom andNMM rank are homophily-
only and social influence-only deconstructed NMM components. (b) Inductive reasoning for NMM-GNN and
RaREon LiveJournal. % nodes{10,30,50,70,90}are sampled ensuring no overlap with test. (c) Ablation
study on quality of using space unification loss (SUL) component.
‚Ä¢PANE [41], random-walk based attirubted network embedding (ANE) model
‚Ä¢NRP [27], described in Table 9 of the main paper
‚Ä¢CONN [42], GNN for attributed networks via collaborative aggregation on bipartite graphs
Table 13: Dataset statistics for evaluation datasets.
Dataset # Vertices # Edges #Attributes Type # Classes
Facebook 4.0K 88.2K 1.3K undirected 193
Google+ 107.6K 13.7M 15.9K directed 468
Table 14: Results of social network classification and link prediction for Jaccard Index (JI) ,Hamming Loss
(HL) ,F1 Score (F1) , and AUC in % using embedding dimension 64. Our NMM and variants are in gray
shading. The overall best results on each dataset are bold-faced.
Datasets Facebook Google+
Metrics JI HL F1 AUC JI HL F1 AUC
NRP 48.0 19.7 55.8 96.1 40.7 26.5 51.1 81.3
PANE 49.3 15.9 64.3 96.4 45.1 23.8 60.2 92.2
CONN 51.2 16.4 61.2 96.8 44.9 23.3 61.3 90.8
NMM-GNN (ours) 52.6 14.2 67.1 96.8 47.3 20.4 66.0 93.9
The source code and datasets for our work can be found at:
https://github.com/roshnigiyer/nmm .
D Ablation Studies
This section details ablation studies where we test quality of using (1) a mixture model, (2) distinct non-
Euclidean geometric spaces, (3) non-Euclidean GNN-based encoders and non-Euclidean GraphV AE
framework (through the inductive setting), and (4) space unification loss component. Each of the
ablation studies provides insight to motivate our architecture design choices, as well as consistently
shows NMM-GNN outperforms baseline network embedding models.
Quality of using a mixture model architecture. We deconstruct our mixture model of NMM ,
to observe the effect different network factors have on learning embeddings, where Œ±andŒ≤are
learnable. We report results on embedding dimension 64, evaluated on AUC score, with the models
summarized below:
‚Ä¢NMM hom, deconstructed homophily component: pŒ∏(eij= 1) = Œ±¬∑phom(eij= 1)
‚Ä¢NMM rank, deconstructed social influence component: pŒ∏(eij= 1) = Œ≤¬∑prank(eij= 1)
‚Ä¢NMM , which is our mixture model defined by Eqn. 6
As shown in Figure 2(a), the mixture model of NMM outperforms that of its subcomponents
NMM hom andNMM rank on all datasets for link prediction. This validates the effectiveness of
using a mixture model architecture for modeling both homophily and social influence factors jointly.
16Quality of using distinct non-Euclidean geometric spaces. We study combinations of geomet-
ric spaces to model NMM (homophily/social influence) to observe the effect it has on learning
topological structure, denoted with NMM (¬∑)‚Ä†in Table 10 (main paper). As shown, the choice of
modeling homophily based nodes in spherical space and modeling social influence based nodes in
hyperbolic space leads to the best performance. Further, NMM outperforms its Euclidean space
counterpart showing that the social network exhibits structures (cycles and hierarchy) that need to
be appropriately represented in curved geometric spaces. There is also evidence of non-Euclidean
topologies in the datasets. For example, the average node on LiveJournal has in-degree of 17 but
outdegree of 25, showing several hierarchical structures present, and 17.7% of LiveJournal data
contains cycles [43].
Link prediction on unseen nodes (inductive task). We study ability of NMM-GNN to learn on
the inductive setting (in addition to the standard transductive setting of Table 10 of the main paper).
To do so, we randomly sample % of nodes being{10,30,50,70,90}and their corresponding links
as our training set. Test nodes are ensured to have no overlap with training nodes, to allow for link
prediction on unseen graphs. Figure 2(b) reports results on NMM-GNN andRaREforLiveJournal
on embedding dimension of 64 for AUC score. NMM-GNN outperforms RaREon all settings
of training nodes. This shows the effectiveness of NMM-GNN in using non-Euclidean GNN-
based encoders and a non-Euclidean GraphV AE training framework during the learning process,
two components that RaRElacks. Further, as less training nodes are observed, NMM-GNN
outperforms RaREby larger margins (e.g., 10% vs. 70% training nodes), showing NMM-GNN
better generalizes to unseen graphs.
Quality of using space unification loss. We test quality of using our proposed space unification loss
(SUL) in our NMM-GNN , by conducting experiments for with the loss component ( NMM-GNN )
and without it ( NMM-GNN -no-SUL ). We report results on embedding dimension 64, evaluated on
AUC score in Figure 2(c), for datasets Wikipedia Clickstream (WC) ,BlogCatalog (BC) ,LiveJournal
(LJ),Wikipedia Hyperlink (WH) , and Friendster (F) . As shown, using SUL improves performance
on all datasets, indicating importance of bridging together representations of distinct non-Euclidean
spaces (spherical and hyperbolic space) at node level.
17NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: The abstract summarizes the main paper claims and we also provide a con-
tribution summary with bullet points in the Introduction. Further, all these main claims of
the paper are supported through Section 3 (detailing our model architecture), Section 4 on
Experiments, as well as our (four) ablation studies in the Appendix.
Our model novelly represents both homophily and social influence factors when modeling
the social network, an advancement over the recent network embedding methods. Further,
the novel utilization of non-Euclidean geometric spaces to model the resulting topologies due
to network factors through appropriate positive and negative curvatures naturually exibiting
properties of cycles and hierarchy (the observed topological heterogeneity), tremendously
improves the representation capability of network embedding modes. This is evidenced by
the empirical results as ablation studies for our work. Moreover, NMM-GNN not only
improves against SOTA models in the performance metrics (Jaccard Index, Hamming Loss, ,
F1 score, AUC Score), but is also applicable to the Inductive Setting, other large-scale infor-
mation networks like Wikipedia networks, and attributed networks (Facebook and Google+),
while consistently achieving the best performance indicating model generalizability.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: We have an entire section on Limitations in the Appendix (Section A) that
addresses all the guidelines below. Our model also makes the assumption the homophily
regulated nodes lie on the surface of the spherical ball and the social influence regulated
nodes lie on the open Poincare ball, which are both natural representations for modeling
nodes. People have observed that more popular celebrity nodes tend to have smaller norm
space and are embedded towards the center of the ball (similar to higher order concepts
in the knowledge graph space), while less popular nodes (containing less social influence)
are embedded towards the boundary of the ball (similar to entities in the knowledge graph
space). To ensure that we can compute an intersection space (for the space unification
component to unify the homophily and social influence representation for the same node),
we enforce that the spherical surface norm is in the Poincare ball e.g., any learned soft value
norm space between 0 and 1. In compute, while we evaluate on a cluster of 8 GPUs, at
least one GPU is necessary for running our experiments (which may been seen as compute
limitation).
Regarding ethical considerations and fairness, privacy and fairness are often topics of focus
regarding social networks. Our model uses information like node popularity based on graph
structure (in degree vs. out degree ratio), as well as attributed features if available. Further it
also infers links through neighborhood context by looking at connected nodes. In general,
a network embedding model uses personal information from the user which may impinge
on their privacy. However, we make every attempt in our model to allow the user to select
their granular choice of model personalization (e.g., we provide the option to choose from
18whether or not users want to enable their attributed information like profile interest and
history being learned).
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: The assumptions about the social network graph as well as our model are
comprehensively listed in Section 3 which contains the "Methodology" and "Overview"
assumptions as well as subsections that each list the associated assumptions for that par-
ticular model component. Moreover, Section 4 (Experiments) and Appendix (Additional
Experiments + Ablation Studies) detail the entire experiment evaluation procedure and
design as well as all the corresponding graph assumptions.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
19Answer: [Yes]
Justification: The full details of experiments for train and test sampling and evaluation
etc. as well as the comprehensive experiment design procedure are provided in Section
4 (Experiments). We also provide all hyperparameter setting details. Further, we provide
all the details for each of the baseline models evaluated for both Experiments as well as
Ablation Studies. Moreover, we release all source code and datasets used in the paper for
enhanced reproducibility.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide open access to all our source code as well as for the datasets in the
Supplemental Material that has been uploaded. As part of this, we also include a README
file to details the setup/requirement as well as commands instructions. We also refer to
this in Section 3 (Methodology) and Section 4 (Experiments). In addition, we detail all the
hyperparameter settings in the main paper (Section 4).
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/
guides/CodeSubmissionPolicy) for more details.
20‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run
to reproduce the results. See the NeurIPS code and data submission guidelines (https:
//nips.cc/public/guides/CodeSubmissionPolicy) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: We have an entire training procedure section and loss function information
inside Section 3 (Methodology) which comprehensively describes our model architecture.
All optimizer and loss function details are presented here. Our Section 4 (Experiments)
and Appendix (which contains our four Ablation Studies) detail the experiment evaluation
for the corresponding experiments and detail all the training and test details as well as any
additional information needed to understand the results. We provide detailed information
about hyperparameter settings (Section 4), which were selected based on performance on
the validation set (train/validation/test split details also in this section). Further, the use of
all evaluation metrics from the experiments are clearly defined (paper resource references
pointed) and justified.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: We provide all details about distribution priors, random embedding initializa-
tion etc., and evaluate our model on standard experiment metrics important to the social
network community for determining model quality. Error bars are not applicable in this
scenario but our paper details all experiment conditions.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
21‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: The paper has a section devoted to analyzing the memory complexity of the
model (Section 4: Experiments), as well as also provide compute and hyperparameter details
in this section and Section 3 (model architecture). Additionally, we release all of our source
code and datasets for maximal reproducibility (Supplement Material).
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: All paper guidelines have been carefully followed regarding page limit, for-
matting, content, research topic and relevance, writing, figures etc.
All privacy and data security concerns are also addressed as we utilize public benchmark
datasets that do not contain any sensitive user information. Our Limitations section further
provides details regarding ethical considerations.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
22Justification: The paper provides clear motivation of the importance of the research problem
of investigation and further identifies key challenges in the network science community in
state-of-the-art embedding models, which thus motivates our model architecture. Further,
our paper provides clear intuition and justification for each design choice decision in our
model components. Additionally, impact of our model is discussed with respect to the social
network community and more broadly for other online information networks e.g., Wikipedia
datasets and attributed networks (in the Appendix section on Ablation studeies). Lastly, we
also discuss Limitations of our model in Appendix Section A.
This model effectively also learns without needed data attributes, which could beneficially
influence privacy considerations in social networks (that users do not need to have their
profile information and history being stored). Of course, any network science model could
have potential for misuse in areas like surveillance or manipulation of online communities,
so litigation and action should be taken to safeguard against misuse.
Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We have been extremely careful on the type of data and model we use and to
consider all ethical aspects respectively. No human subjects were used in our research study
(N/A), and all of our datasets for evaluation are large-scale public benchmark popular social
media datasets evaluated on numerous papers in the recent years. As such, these datasets
are highly suitable for direct comparisons against various SOTA models as they have been
widely tested on.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
23‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: All source codes are attributed to the respective authors and we even identify
in our paper the training procedures if used from prior work for fairness of comparison
e.g., RaRE [6] that we indicate in Section 4 (Experiments). We also release all our source
code and datasets (in addition to referencing each of them in our main paper for the source
information) for reproducibility under MIT and CC-by licences.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the package
should be provided. For popular datasets, paperswithcode.com/datasets has curated
licenses for some datasets. Their licensing guide can help determine the license of a
dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper is about improved model development for enhancing representation
learning of nodes in the social network to better explain how links in the social network are
formed. No new assets or datasets etc. are released as part of this work.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: As mentioned in previous questions this is not applicable for this paper and all
datasets evaluated are popular large-scale networks from social media that have been widely
evaluated on in recent years in the network science community.
24Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: This paper does not involve any research or testing of human subjects. All
training and testing procedures are solely conducted on publically released benchmark
datasets that do not involve intervention from human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
25