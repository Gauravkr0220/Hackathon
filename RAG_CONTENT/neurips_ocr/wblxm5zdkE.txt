Real-Time Selection Under General Constraints via
Predictive Inference
Yuyang Huo1âˆ—Lin Lu1âˆ—Haojie Ren2â€ Changliang Zou1â€ 
1School of Statistics and Data Sciences, LPMC, KLMDASR and LEBPS,
Nankai University, Tianjin, China
2School of Mathematical Sciences, Shanghai Jiao Tong University, Shanghai, China
huoyynk@gmail.com ,linlu102099@gmail.com
haojieren@sjtu.edu.cn ,zoucl@nankai.edu.cn
Abstract
Real-time decision-making gets more attention in the big data era. Here, we con-
sider the problem of sample selection in the online setting, where one encounters a
possibly infinite sequence of individuals collected over time with covariate infor-
mation available. The goal is to select samples of interest that are characterized by
their unobserved responses until the user-specified stopping time. We derive a new
decision rule that enables us to find more preferable samples that meet practical
requirements by simultaneously controlling two types of general constraints: indi-
vidual and interactive constraints, which include the widely utilized False Selection
Rate (FSR), cost limitations, diversity of selected samples, etc. The key elements
of our approach involve quantifying the uncertainty of response predictions via
predictive inference and addressing individual and interactive constraints in a se-
quential manner. Theoretical and numerical results demonstrate the effectiveness
of the proposed method in controlling both individual and interactive constraints.
1 Introduction
In recent times, the field of real-time decision has flourished significantly, primarily driven by the
exponential growth of available data in both the tech industry and computer science. We consider
here a typical application of real-time decision, the problem of online sample selection [2,5]. For
instance, online recruitment systems utilize machine learning algorithms to sequentially choose
qualified candidates rather than waiting for all (future) candidatesâ€™ information to be collected
[14]. Additionally, recommendation systems have now become commonplace in providing real-time
suggestions for content (e.g., news articles, short videos) with potential high click-through rates to
users [24]. Common situations also can be found in real-time precision marketing [43].
We describe the online sample selection problem as follows: samples (individuals) characterized by
covariates XtâˆˆRdarrive sequentially while their responses YtâˆˆRremain unobserved throughout
the process. The data pairs (X, Y)of each time are i.i.d. random vectors. At each time point, the
analyst is faced with the task of deciding whether to select the current observation based on certain
predetermined criteria related to Yt, and this selection process continues until a specific stopping rule
is triggered. For example, Ytis the score that measures how one candidate fits a given job position in
the recruitment system and the human resource agencies aim to prioritize candidates with higher Yt,
such as Ytâ‰¥b. OrYtis a binary variable where Yt= 1/0means accepting or rejecting the offer,
and the companies wish to find those individuals with Yt= 1based on the Xt.
*Equal contribution, and the first two authors are listed in alphabetical order.
â€ Correspondence to: Haojie Ren <haojieren@sjtu.edu.cn>, Changliang Zou <zoucl@nankai.edu.cn.>
38th Conference on Neural Information Processing Systems (NeurIPS 2024).Our Goal and Motivation. Our goal is to sequentially select samples whose unobserved responses
Ytâ€™s are in the specified target region. A natural idea is to make decisions based on the prediction
value of Ytfrom models associated with (X, Y)built on some historical data. However, neglecting
the uncertainty in predictions could result in numerous false decisions, i.e., selecting those samples
whose true responses are beyond the specified region. To measure the selection uncertainty, some
existing works reformulate sample selection as a hypothesis testing problem and focus on controlling
the online false discovery rate (FDR) [ 16,33]; see more discussions and literature in Section 1.2.
However, in addition to quantifying statistical uncertainty, online selections often need to take into
account various constraints to find informative samples in practice, for example, cost limitations,
the impacts of some covariates or the diversity of candidates in the online recruitment [ 39]. Hence,
itâ€™s necessary to explore the covariate space to satisfy these requirements. This motivates us to
investigate how to efficiently implement online sample selection with statistical guarantees under
various constraints.
To address this issue, we summarize common constraints into two types, i.e., individual constraints
andinteractive constraints . The former one is relevant to the cost of selected samples, and one typical
example is the fundamental and crucial criterion, false selection rate (FSR), which quantifies the
proportion of falsely selected samples and is equivalent to the well-adopted FDR. The latter constraint
captures the interactive influence among selected samples and is regarded as some kind of quadratic
constraint on some pairwise functions, such as the similarity or diversity among selected samples.
While the individual constraint associated with online FDR control has gained some attentions
[16,18], it alone fails to capture the nuanced pairwise relationships among different samples. To
bridge this gap, we introduce interactive constraints, which are pivotal within our framework. Building
upon individual constraints, the interactive criteria significantly expand the range of constraints our
method can control. Integrating two distinct types of constraints into a unified framework makes it
easier to create practical algorithms and ensures theoretical guarantees.
A motivating example: candidate screening. As an example, in recruitment, screening from the
resumes arriving sequentially to determine viable candidates who can get into interview processes is
an important problem in human resource management [ 13,35]. In this case, one may be interested in:
(1) controlling the online FSR to enhance resource efficiency [ 21], and (2) maintaining a desired level
of candidate diversity during the screening process, thereby reducing bias [ 23,44]. The individual
and interactive constraints and the novel real-time sample selection procedure we propose can solve
this problem precisely. See Section 4.2 for the details and more real data examples.
1.1 Our Contributions
In this paper, we design a novel and flexible online selection rule to effectively ensure the above two
types of constraints are under control at pre-specified levels simultaneously, named as â€œIndividual
and Interactive Constrained Online Selection" (II-COS). The main idea stems from an oracle model
based on the local false discovery rate (lFDR) [12, 38], which is involved in offering valid evidence
on whether Ytis our interest at each time point. With some appropriately chosen evaluating functions,
the II-COS procedure entails validating whether the estimates of constraints are controlled. Simulated
and real-data examples clearly demonstrate the superiority of the II-COS in terms of both online
individual and interactive criteria control.
To the best of our knowledge, this is the first work to systematically bridge the predictive inference
and online selection procedure with various constraints. Our contributions are summarized as follows:
â€¢Under a unified framework, the II-COS addresses how to implement online predictive
selection sufficiently in consideration of both individual and interactive constraints. It is
flexible to characterize the selective uncertainty and trade-off the sampling efficiency and
practical limitations in the covariate space.
â€¢Under mild conditions, we establish the theoretical guarantee that the II-COS is able to
control both individual and interactive constraints simultaneously and asymptotically under
one given stopping rule.
â€¢The II-COS is model-agnostic in the sense that its implementation is applicable to any
(appropriate) learning algorithms. Extensive numerical experiments indicate that the II-COS
can significantly outperform existing ones while yielding effective constraints control.
21.2 Related Works
Our proposed method is built upon two fundamental pillars: (1) quantifying the uncertainty of
response predictions using predictive inference; and (2) systematically addressing individual and
interactive constraints in a sequential manner. Our work is intricately connected to the fields of
predictive inference and online multiple testing. Here we briefly review literature on these two topics.
Predictive Inference. One key ingredient of our proposed method is predictive/conformal inference.
Conformal inference [ 42,34] provides a powerful and flexible tool to achieve algorithm-agnostic
uncertainty quantification of predictions. Conventionally, conformal inference aims to build the pre-
diction intervals and enjoys valid and distribution-free properties by leveraging data exchangeability
[25,3]. Taking a different but related perspective from multiple-testing, Bates et al. [7]pioneered a
method to construct conformal p-values to detect outliers with finite-sample FDR control. Building
upon this, recent advancements have improved detection power by involving more information or
performing model selection [ 50,26,27,51,46]. The most related works are Jin and CandÃ¨s [21] and
Wu et al. [45], which considered a similar scenario that one would like to select some individuals of
interest by controlling FDR or maximizing the diversity of selected samples in an off-line setting.
Their methods are based on the conformal p-values or lFDR constructed with the predicted response
values, respectively. Besides the fundamental difference between online and offline paradigms, our
framework for characterizing various constraints poses additional challenges in how to select samples
sequentially since we have multiple goals to achieve.
Online Multiple Testing. When only considering individual constraint as FSR control, the online
sample selection can be reformulated as online multiple testing problem. Methods for online multiple
testing have received much recent attention and were pioneered by Foster and Stine [16] who proposed
the so-called Î±-investing strategy, which was later built upon and generalized [ 1,29,30,18,19]. The
key idea in Î±-investing and its generalizations is to compare p-values with dynamic thresholds and
gain some extra Î±-wealth for each rejection. We refer to Robertson et al. [33] for a thorough overview.
Those rules suffer from the â€œalpha-death" issue to some extent [ 29], which means a permanent end to
decision-making when the decision threshold is too small, i.e., the online procedure stops early. This
phenomenon occurs in many existing online multiple testing algorithms, as discussed in [ 29]. Along
a different direction, Gang et al. [17] developed a new class of structure-adaptive sequential testing
(SAST) rules built on the lFDR to avoid the alpha-death issue. The SAST serves as a building block
for developing our II-COS procedure and can be essentially seen as a special case of ours. Later
on, Ao et al. [4]reformulated online multiple testing procedure into an online knapsack problem,
providing novel policies with near-optimal regret guarantees. Additionally, Xu and Ramdas [48]
proposed to use e-values [ 41] for online multiple testing to address the dependence. However, those
existing works do not take predictive inference into account and are concerned only with online error
rate control without exploration of the covariate space, which may greatly hamper their applicability.
2 Individual and Interactive Constrained Online Selection Procedure
2.1 Problem Formulation
Assume there exists a historical labeled dataset as D={ËœXi,ËœYi}n
i=1, where (ËœXi,ËœYi)â€™s are independent
and identically distributed (i.i.d.) from (X, Y). A sequence of unlabeled data X1,X2,Â·Â·Â· âˆ¼ X=
(X1,Â·Â·Â·, Xd)âŠ¤arrives in a stream with unknown responses Y1, Y2,Â·Â·Â·. At each time t, one must
make a real-time decision about whether or not to select the t-th individual, which is determined by
some pre-specified requirement on Yt. Denote Aas the target region of Yt, which differs depending
on usersâ€™ specifications. For example, in a regression setting, the requirement could be of the form
Ytâˆˆ[a, b],(âˆ’âˆž, a)orYtâ‰¥b.
LetÎ¸t=I{Ytâˆˆ A} describe the true state of Yt. Denote a decision rule as Î´tâˆˆ {0,1}, where Î´t= 1
indicates that the Xtis selected and Î´t= 0otherwise. A false selection is made if Î´t= 1butÎ¸t= 0.
Denote Î´t={Î´i:iâ‰¤t}as the decision rule and Tas the time that the procedure stops. Our goal
is to build a decision rule Î´tto select samples with {Ytâˆˆ A} up to stopping time Tsuch that the
following two general types of constraints hold simultaneously.
Individual Constraint. In practice, one main concern is to control the cost of selecting samples of
interest. For example, in online recruitment, companies need to control the proportion of selected
3unqualified candidates or the average loss when hiring someone who rejects the offer. In such cases,
we can assign each selected sample a cost associated with some pre-specified function of the covariate
Xand control the expected cost associated with time Tat the target level. We refer to this requirement
asindividual constraint and write it as:
C1(Î´t) =E"P
iâ‰¤t{(1âˆ’Î¸i)G0(Xi) +Î¸iG1(Xi)}Î´i
(P
iâ‰¤tÎ´i)âˆ¨1#
, (1)
where aâˆ¨b= max {a, b}andG0(X)â‰¥0andG1(X)â‰¥0withG0Ì¸=G1are the costs corresponding
toÎ¸= 0andÎ¸= 1, respectively. Here, we take expectation due to the randomness of Î¸1,Â·Â·Â·, Î¸t.
For example, when we simply choose G0(X) = 1 andG1(X) = 0 , the individual constraint is the
popular false selection rate (FSR), i.e.
C1(Î´t) = FSR( Î´t) =E"P
iâ‰¤t(1âˆ’Î¸i)Î´i
(P
iâ‰¤tÎ´i)âˆ¨1#
. (2)
The FSR is essentially equivalent to the well-adopted FDR in multiple testing literature, which is
a useful tool to maintain the ability to reliably select samples of interest without excessively false
selections [9]. Some works on online FDR control have been well studied. [16, 1].
The individual constraints alone cannot capture the pairwise relationship among different samples.
We address this by introducing interactive constraints below.
Interactive Constraint Another common concern is the interactive constraint , which involves
choosing more preferable samples. For example, companies would like to retain candidates with a
diverse range of backgrounds and experiences in online recruitment, or real-time suggested contents
are required to avoid homogeneity in recommendation systems. Here, we introduce a bi-variate
weight function g(X,Xâ€²)to evaluate the interaction between selected samples. Denote PC(Î´t) =PP
1â‰¤i<jâ‰¤tg(Xi,Xj)Î¸iÎ¸jÎ´iÎ´j,PS(Î´t) =PP
1â‰¤i<jâ‰¤tÎ¸iÎ¸jÎ´iÎ´j. We define the interactive constraint as
ËœC2(Î´t) =EPC(Î´t)
PS(Î´t)
. (3)
Here, since only the correctly selected samples are of interest, the constraint is concerned with the
average mutual effects between the correctly selected ones rather than all selected ones. When
choosing the function gas some similarities, controlling ËœC2(Î´t)at a specified constant K, i.e.,
ËœC2(Î´t)â‰¤K, is controlling the expected similarity (ES). It is equivalent to requiring that correctly
selected samples exhibit certain diversity and rich information in the covariate space of interest.
Typically, one useful choice for g(X,Xâ€²)is the weighted RBF kernel g(X,Xâ€²) =
expn
âˆ’1
Ïƒ2Pd
k=1wk(Xkâˆ’Xâ€²
k)2o
with parameter Ïƒ > 0to measure the similarity between two
independent XandXâ€². The RBF kernel is a common and widely embraced choice in machine
learning [ 49,28]. Here, {w1, . . . , w d}are some given weights per usersâ€™ needs. For instance, if
one is just interested in the effects of the k-th feature, then simply wk= 1andwj= 0forjÌ¸=k.
Specifically, the case that wk= 1for all k= 1, . . . , d is chosen in Section 4. We also consider other
similarity choices of g(X,Xâ€²), such as the cosine similarity g(X,Xâ€²) =XâŠ¤Xâ€²/(âˆ¥Xâˆ¥2âˆ¥Xâ€²âˆ¥2)[52].
Due to the randomness in the denominator, it turns out controlling (3)directly is not easy. Instead,
we employ a modified interactive constraint,
C2(Î´t) =E
PC(Î´t)
E
PS(Î´t). (4)
The constraint (4)aims to control a ratio of expectations, which is still a reasonable interactive
measure. In numerical studies, we see that ËœC2(Î´t)in(3)andC2(Î´t)in(4)yield almost identical
patterns. An illustrative example can be found in Appendix D.1.
In sum, the goal is to select samples of interest by a decision rule Î´Tcontrolling both the individual
and interactive constraints until stopping time T, i.e., C1(Î´T)â‰¤Î±and C2(Î´T)â‰¤K.We
emphasize that C1(Î´T)andC2(Î´T)as well as their pre-specified levels Î±andKcan be chosen up
to the practical applications.
42.2 Oracle Selection Procedure
To design a general rule that is valid for any arbitrary stopping time T, we consider controlling the
constraints at each time tin an online fashion, such that suptâˆˆNC1(Î´t)â‰¤Î±andsuptâˆˆNC2(Î´t)â‰¤K.
Since Ytis unavailable, we consider utilizing predictive inference to measure the suspicious patterns.
LetÂµ(x) := Y|X=xbe the regression or classification model associated with (Xt, Yt), and
one reliable estimate as bÂµ(Â·), being estimated on the labeled data Dwith some machine learning
algorithm. Denote Wt=bÂµ(Xt)as a predicted value of Ytand assume that bÂµ(Â·)is a bijection almost
surely. The bijection assumption is considerably mild and widely adopted for the identyification of
eachXtin the predictive inference framework [ 45]. The Î¸t=I(Ytâˆˆ A)is Bernoulli( Ï€) distributed
withÏ€= Pr( Ytâˆˆ A), and Wtcan be viewed as generated from one two-group model
Wt|Î¸tâˆ¼(1âˆ’Î¸t)f0+Î¸tf1,
where f0andf1denote the probability distribution functions of Wtconditional on Yt/âˆˆ A (i.e.,
Î¸t= 0) and Ytâˆˆ A, respectively. Then, the conditional probability of Yt/âˆˆ A is
Lt= Pr( Î¸t= 0|Wt) =(1âˆ’Ï€)f0(Wt)
f(Wt), (5)
where f= (1âˆ’Ï€)f0+Ï€f1. The Ltcoincides with the local FDR in multiple testing literature
[12,17]. With the two-group model (5), we have E[Î¸t|Xt] = 1âˆ’Ltand further notice that the
individual constraint C1(Î´t)in (1) can be exactly satisfied if
Vt
Rt:=P
iâ‰¤t{LiG0(Xi) + (1 âˆ’Li)G1(Xi)}Î´i
(P
iâ‰¤tÎ´i)âˆ¨1â‰¤Î±,
holds. Here, we denote Vt=P
iâ‰¤t{LiG0(Xi) + (1 âˆ’Li)G1(Xi)Î´i}and the number of selected
ones as Rt=P
iâ‰¤tÎ´iâˆ¨1for notational convenience. Especially, when G0(X) = 1 ,G1(X) = 0 ,
thenFSR( Î´t)in (2) can be exactly controlled.
Accordingly, the interactive constraint C2(Î´t)â‰¤Kin (4) can be achieved if
TSt
NSt:=PP
1â‰¤i<jâ‰¤tg(Xi,Xj)(1âˆ’Li)(1âˆ’Lj)Î´iÎ´j
PP
1â‰¤i<jâ‰¤t(1âˆ’Li)(1âˆ’Lj)Î´iÎ´jâ‰¤K,
where the expected total mutual effects conditional on {Xi}iâ‰¤tand the expected number are denoted
as TS tand NS t, respectively.
Therefore, if Ltis known, when a new sample Xtarrives at time point t, we can perform the decision
rule as follows. Note that there is no need to consider interactive effects before the first selection.
When tcomes before the first selection (i.e, Rtâˆ’1= 0), the decision rule is Î´t= 1if
Vtâˆ’1+LtG0(Xt) + (1 âˆ’Lt)G1(Xt)
Rtâˆ’1+ 1â‰¤Î±, (6)
holds; otherwise, Î´t= 0 which means Xtis not selected. When Xtarrives with Rtâˆ’1â‰¥1, then
Î´t= 1if (6) and
TStâˆ’1+"
P
iâ‰¤tâˆ’1g(Xi,Xt)(1âˆ’Li)Î´i#
(1âˆ’Lt)
NStâˆ’1+"
P
iâ‰¤tâˆ’1(1âˆ’Li)Î´i#
(1âˆ’Lt)â‰¤K (7)
hold simultaneously; otherwise, Î´t= 0. Note that if we set C1(Î´t)as FSR( Î´t) and choose K= +âˆž,
then our method essentially reduces to the same manner as the controlling step of the SAST in Gang
et al. [17]. Our proposed method can be seen as a much more generalized and flexible framework for
controlling both the individual and the interactive constraints simultaneously in an online fashion.
We call this method the oracle II-COS (Individual and Interactive Constrained Online Selection). The
workflow in Figure 1 shows the procedure of the oracle II-COS. The following result shows that it
can exactly achieve our goal.
Proposition 2.1. Assume Ltvalues are known. Then the oracle II-COS selection rule controls both
constraints at any stopping time T, i.e., C1(Î´T)â‰¤Î±and C2(Î´T)â‰¤K.
5Oracle II -COS ð‘‹ð‘¡
Input: 
ð‘Šð‘¡,ð¿ð‘¡,
ð‘”(ð‘‹ð‘–,ð‘‹ð‘¡)ð¶1(ð‘¡)
ð¶2(ð‘¡)Yesð›¿ð‘¡=0
Yes
ð›¿ð‘¡=1Update:
ð‘‰ð‘¡,ð‘…ð‘¡,
ð‘‡ð‘†ð‘¡,ð‘ð‘†ð‘¡No
No
ð‘‹ð‘¡Figure 1: The implementation flowchart of the oracle II-COS procedure.
2.3 Data-driven II-COS Procedure
AsLtis unknown in practice, we propose a data-driven II-COS procedure, which uses a reliable
estimation bLtfor implementation. We resort to a data-splitting strategy: randomly split historical
dataDinto two parts, the training set Dtrand the calibration one Dcalof sizes n0andn1respectively,
where Dtris used for training a predictive model and Dcalis to estimate those unknown parameters.
Specifically, we first fit a regression or classification model bÂµ(Â·)onDtr, and then obtain predicted
values on Dcal, i.e.{bÂµ(ËœXi) : (ËœXi,ËœYi)âˆˆ D cal}. Note that conditional on Dtr,{bÂµ(ËœXi) : (ËœXi,ËœYi)âˆˆ
Dcal}are i.i.d. random variables and have the same distribution as Wt=bÂµ(Xt), so that it can be
utilized to estimate (5).
Therefore, the estimators of f0andf,bf0andbf, can be obtained by applying the kernel density
estimation method to the data {bÂµ(ËœXi) : ( ËœXi,ËœYi)âˆˆ D cal,ËœYi/âˆˆ A} and{bÂµ(ËœXi) : ( ËœXi,ËœYi)âˆˆ
Dcal}, respectively. And the probability Ï€= Pr( Ytâˆˆ A)can be approximated by bÏ€=
nâˆ’1
1P
(ËœXi,ËœYi)âˆˆDcalI(ËœYiâˆˆ A).Further the Ltin (5) can be estimated by
bLi=(1âˆ’bÏ€)bf0(Wi)
bf(Wi)âˆ§1. (8)
The data-driven II-COS procedure is summarized in Algorithm 1, and it indeed consists of two phases:
offline estimation and online decision. The running time of offline estimation is not critical. At each
timet, the computational complexity is a linear function of the currently selected number Rt. More
implementation details can be found in Section 4 and Appendix B.1.
In fact, the proposed II-COS is flexible to trade off the individual and interactive constraints by
adjusting the thresholds Î±andK. If one is concerned only with individual cost control, then we can
setK= +âˆž, with which the interactive constraint is out of work. Similarly, only the interactive
effect is of interest when Î±= 1. Appendix D.2 provides a toy example to illustrate this.
Before further pursuing, we would discuss the stopping time in practice. Itâ€™s worth noting that the
specific choice of stopping rule (and thus stopping time) is completely up to the user. For example,
when mâ‰¥2is the desired number of selections, one can set T= inf t{t:Pt
i=1Î´i=m}. Or when
sis the total wages for recruitment, one can set T= inf t{t:Pt
i=1Î´isi=s}where siis the payroll
for each selected candidate. Or Tis just chosen as one given deadline. With the use of II-COS,
practitioners have the flexibility to design diverse stopping strategies that can adapt seamlessly to
their specific applications. In brief, our method is flexible and is appropriate for various goals based
on the userâ€™s requirements by choosing different G0(X), G1(X), g(Xi,Xj)and varied target levels
(Î±, K)and a user-specified stopping time.
6Algorithm 1 The data-driven II-COS procedure
Input: Target levels Î±andK, pairwise function g, cost G0(X)andG1(X), stopping time T,
interested region A, labeled data D, prediction algorithm H.
Initialization: t= 0,Vt=Rt= 0,TSt= NS t= 0; Decision rule Î´t=âˆ…
Estimation: Randomly split Dinto training set Dtrand calibration set Dcal. OnDtr, fitbÂµ(x)with
H. Obtain bÏ€,bf0andbffromDcal.
Online decisions: while tâ‰¤Tdo
Sett=t+ 1. Compute Wt=bÂµ(Xt),bLtby (8) and the similarities g(Xi,Xt)for those Î´i= 1.
Replace LtbybLtin (6) and (7).
ifRtâˆ’1<1and (6) holds thenÎ´t= 1;
elseÎ´t= 0;
ifRtâˆ’1â‰¥1, (6) and (7)hold thenÎ´t= 1;
elseÎ´t= 0;
Update: Î´t=Î´tâˆ’1S{Î´t};Rt=Rtâˆ’1+Î´t;Vt=Vtâˆ’1+{bLtG0(Xt) + (1 âˆ’bLt)G1(Xt)}Î´t;
TSt=TStâˆ’1+"X
i<tg(Xi,Xt)(1âˆ’bLi)Î´i#
(1âˆ’bLt)Î´t;NSt=NStâˆ’1+"X
i<t(1âˆ’bLi)Î´i#
(1âˆ’bLt)Î´t;
end
Output: Selection set {Xi:Î´i= 1, Î´iâˆˆÎ´T}.
Extension to varying proportion case. In practice, the distribution of (Xt, Yt)may vary smoothly
over time. In Appendix C, we consider the probability of Ytâˆˆ A (i.e. the proportion of samples in
the specified region) varying over time and extend the proposed II-COS to learn Ï€t= Pr( Ytâˆˆ A)
continuously over time and we also construct the corresponding theoretical guarantees.
3 Statistical Performance Guarantees
In this section, we provide statistical guarantees for the data-driven II-COS procedure. The main
difficulties lie in the quantification of data-driven estimation error of Ltand we utilize the classical
kernel density estimation theory along with the structure of our online procedure to effectively
characterize it. For simplicity, we consider that the training data set is given such that the estimated
modelbÂµis fixed. Before presenting our theoretical results, we state the following regularity conditions.
Assumption 3.1 (Density functions and kernel) .The density functions and kernel function satisfy
(1) The f1(Â·)andf0(Â·)are upper bounded by M > 0, and the f(Â·)is lower bounded by â„“ >0.
(2)Thef0andf1are HÃ¶lder-continuous, i.e. |f0(w)âˆ’f0(wâ€²)| â‰¤cÎ²|wâˆ’wâ€²|Î²for any w, wâ€²âˆˆ
R,and the same for f1with some fixed 0< Î²â‰¤1and constant cÎ².
(3) Kernel K(Â·)is a bounded symmetric function and enjoys exponential decay.
Assumption 3.2 (Weight functions) .There exists constants cG>0andcg>0such that 0<
G0(X)â‰¤cG,0< G 1(X)â‰¤cGfor any Xand0< g(X,Xâ€²)â‰¤cgfor any XÌ¸=Xâ€².
Assumption 3.1 is considerably mild and widely adopted in the uniform convergence of kernel
density estimation [ 36]. Iff0andf1have bounded first-order derivatives, the HÃ¶lder-continuous
assumption would hold with Î²= 1. The lower bound of fis to ensure the uniform convergence
of the estimated lFDR . Assumption 3.2 is mild since the weight functions are required only to be
positive and bounded. It can be satisfied by a large category of G0,G1andg. For example, we can
takeGj(X) =ajâˆ¥Xâˆ¥2
2forj= 0or 1 and cGexists when Xis bounded. And we can set gas the
RBF and orthogonal similarities with cg= 1.
With those regularity conditions, we establish the validity of the II-COS procedure for the individual
constraint control.
7Theorem 3.3 (Bound for individual constraint) .Suppose Assumptions 3.1 and 3.2 hold and take
the bandwidths for estimating fandf0in the order of nâˆ’1/(2Î²+1)
1 . Then for any given time t, the
individual constraint of the II-COS procedure (Algorithm 1) satisfies C1(Î´t)â‰¤Î±+ âˆ† n1,where
âˆ†n1=Dnâˆ’Î²
2Î²+1
1âˆšlogn1andDis a constant depending on M,â„“,cÎ²,Î²,Ï€,cGandK(Â·).
Although the C1(Â·)of II-COS might be slightly larger than the target level in finite samples, this
gap converges to 0asymptotically as n1increases. In the numerical studies, we find that a small
calibration size of around 200is enough to control the C1in a reasonable range. Taking the FSR as
an individual constraint, Theorem 3.3 indicates that our method can provide asymptotic online FSR
control similar to an online FDR control procedure [16].
The next theorem examines the performance of the II-COS in terms of interactive constraint.
Theorem 3.4 (Bound for interactive constraint) .Suppose Assumptions 3.1-3.2 hold and take the
bandwidths for estimating fandf0in the order of nâˆ’1/(2Î²+1)
1 . LetTs= inf{t:Pt
i=1Î´i=s}for
s >2and assume there exists a constant Î±â€²âˆˆ(0,1)such thatP
iâ‰¤tbLiÎ´i/(1âˆ¨Rt)â‰¤Î±â€². Then for
any given time tâ‰¥Tm, the interactive constraint of the II-COS satisfies
C2(Î´t)â‰¤K+(K+cg)âˆ†n1
0.5âˆ’mÎ±â€²
mâˆ’1âˆ’âˆ†n1.
The term mÎ±â€²/(mâˆ’1)is used to characterize the lower bound of the denominator term of the
interactive constraint. Specifically, when we choose FSR as the individual constraint, we have
Î±â€²=Î±, which demonstrates the interdependence between controlling individual and interactive
constraints. Furthermore, under arbitrary stopping strategies with a stopping time T, we can have the
asymptotic guarantee.
Corollary 3.5. Suppose the conditions in Theorem 3.4 hold, the stopping moment Tâ‰¥Tmand
Î±â€²<(1âˆ’1/m)/2. Then the II-COS procedure controls the individual and interactive constraints
asymptotically at T, i.e.limn1â†’âˆžC1(Î´T)â‰¤Î±and limn1â†’âˆžC2(Î´T)â‰¤K.
4 Experiments and Evaluation
We illustrate the breadth of applicability of the II-COS procedure by experiments on simulated data
and real-data applications. As an example, we set the stopping rule as selecting total m= 100
samples, i.e., T=Tm= inf t{t:Pt
i=1Î´i=m}. Additional experiments including the extended II-
COS in Appendix C are shown in Appendix D.9. Code for implementing II-COS and reproducing the
experiments and figures in our paper is available at https://github.com/lulin2023/II-COS .
Implementation of II-COS . To our best knowledge, online selection with uncertainty qualification
has only been studied in the field of online multiple testing, which aims to control online FDR. Hence,
we focus on using FSR as the individual criterion and modified ES as the interactive criterion. As
Xmay be measured on scales with widely differing ranges in different dimensions, we assume that
Xâ€™s have been properly scaled in each dimension before computing g. We choose gas the weighted
RBF kernel with Ïƒ= 1,wk= 1here. Other choices for individual and interactive constraints are
considered in Appendix D.5.
Benchmarks . We compare the II-COS procedure with four benchmarks from online multiple testing.
The first one is a structure-adaptive sequential testing rule, the SAST [17], which is implemented
withbLt. It can achieve the FSR control but ignore the interactive constraint. As mentioned earlier
in Section 2, SAST can also be considered as a special case of our II-COS with K= +âˆž. Its
details are deferred to Appendix B.2. The other competitors are three well-known online FDR control
algorithms LOND [18],SAFFRON [30] and ADDIS [40] implemented with the conformal p-values
suggested by Bates et al. [7]. Refer more information in Appendix B.3. All the benchmarks can only
control FDR, which demonstrates the flexibility of our method for different constraints.
Performance Measures . The empirical FSR, ES and stopping time ( Tm) are evaluated using the
average values of the false selection proportions, the similarity and the stopping time from 500
replications, respectively, where Tmserves as a criterion for assessing selection efficiency.
84.1 Results on Synthetic Data
Data Description. We consider a classification model: X|Y= 0âˆ¼ N 4(Âµ1,I4), andX|Y=
1âˆ¼ N 4(Âµ2,I4), where Âµ1= (5,0,0,0)âŠ¤,Âµ2= (0,0,âˆ’3,âˆ’2)âŠ¤. We set Pr(Y= 1) = 0 .2. The
information set is A={1}. The predictor His taken as random forest with defaulted parameters.
We also consider a regression setting and conduct additional experiments in Appendix D.
Firstly, we observe that methods relying on conformal p-values, such as LOND, SAFFRON, and
ADDIS, encounter the alpha-death (stop early) issue [ 29]. These methods struggle to select an
adequate number of samples, especially in small calibration sets. In contrast, II-COS ensures the
control of both individual and interactive constraints even with a small ncal(e.g., 200). See more
details and results in Appendix D.3. Hence, to make a fair comparison, we consider a relatively large
size of the calibration set, ncal= 4,000. We fix training data size ntr= 1,000.
0.000.050.100.150.20
100 200 300
TimeC1(FSR)
0.0250.0500.075
100 200 300
TimeC~
2(ES)
Method IIâˆ’COS SAST LOND SAFFRON ADDIS
Figure 2: The values of FSR( Î´t)andES(Î´t)over time tfor II-COS, SAST, LOND, SAFFRON and ADDIS.
The black dashed lines denote the FSR level Î±= 0.1and the ES level K= 0.045. Shading represents error
bars of one standard error above and below.
0.00.10.20.3
IIâˆ’COSSASTLONDSAFFRONADDISC1(FSR)
0.000.050.100.15
IIâˆ’COSSASTLONDSAFFRONADDISC~
2(ES)
02505007501000
IIâˆ’COSSASTLONDSAFFRONADDISTm
Figure 3: Boxplots of FSR( Î´Tm),ES(Î´Tm)and stopping time Tmfor II-COS, SAST, LOND, SAFFRON and
ADDIS. The black dashed lines indicate the corresponding nominal levels.
Results. Figure 2 presents the online FSR (false selection rate) and ES (expected similarity) values of
five methods against time t, which are individual and interactive constraints, respectively. The FSR
levels of II-COS and SAST are closer to the nominal level than the other three methods. As expected,
only the empirical ES levels of II-COS are controlled under the pre-specified level Kover time t.
The LOND and SAFFRON lead to slightly conservative FSR values, while the FSR levels of ADDIS
are inflated compared to the target level. Figure 3 further displays the boxplots of empirical FSR,
ES at stopping time Tm. We observe that only II-COS achieves satisfactory ES values compared
to the nominal level. Moreover, the II-COS has a relatively larger value of Tmcompared to those
of other benchmarks. This is consistent with the fact that the II-COS spends more time exploring
the structure information inside the covariate space due to the requirement of interactive constraint.
Similar conclusions for the regression example in Appendix D.4 can be drawn.
Regarding efficiency, we also conducted an experiment to compare the effectiveness of II-COS with
an oracle method possessing knowledge of true state Î¸t. The Tmof II-COS is very close to the oracle.
This close proximity indicates the high efficiency of II-COS. See Appendix D.8 for the details.
4.2 Results on Real Data
We next demonstrate the performance of the II-COS in two real-world applications. Since those
online multiple testing methods based on conformal p-values yield few selected individuals, we focus
9on comparing the II-COS with the SAST. For comparison, we also include the offline method using
conformal p-values, where Î´t= 1ifbptâ‰¤Î±. We denote it as CP.
Datasets. We consider the recruitment dataset from Kaggle [22] that contains 45,372 candidates after
removing the missing data and records a binary response indicating whether the candidate passes the
job interview, and other 11 attributes including education status, handicapped or not, and gender. The
other problem is to use 1994 Census Bureau dataset [ 8] to select a subset of individuals who may
have high incomes in precision marketing. This census dataset records 32,561 individuals with their
14 attributes, including gender, race, marriage, education length and so on.
For each dataset, we randomly partition the data into three parts: ntr= 1,000training data, ncal=
1,000calibration data and the rest which are used as the online observations. The categorical
attributes are converted into one-hot codes and then are treated as numerical attributes for computing
similarity measures. The prediction algorithm His random forest with defaulted parameters.
Results. Table 1 reports the results among 500 repetitions. Both the II-COS and SAST enjoy valid
FSR control, but CP yields an inflated FSR level in income investigation. The II-COS performs well
in terms of similarities. To further compare the diversities, we present the proportions of different
education status in in Figure 4. It can be seen that the proposed II-COS demonstrates its superior
diversity in the specific attributes. See Appendix D.6 for more results for the real data. In summary,
the proposed II-COS works well for selecting individuals of interest to achieve various constraints in
practical applications.
Table 1: Average values with candidate dataset and income dataset: FSR( Î´Tm),ES(Î´Tm)(Ã—10âˆ’3) and
stopping time Tm. The target FSR level is Î±= 0.2for both. For the candidate data, the target ESlevel
K= 1Ã—10âˆ’3; For the income data, K= 6Ã—10âˆ’3. The bracket contains the standard error.
(a) Candidate dataset [22]
Method FSR ES Tm
II-COS 0.19 (0.002) 0.98 (0.005) 2227 (91.6)
SAST 0.19 (0.002) 8.73 (0.269) 310 (35.2)
CP 0.16 (0.002) 10.34 (0.084) 277 (1.49)(b) Income dataset [8]
Method FSR ES Tm
II-COS 0.16 (0.007) 5.56 (0.128) 2760 (227.0)
SAST 0.19 (0.008) 30.90 (1.078) 1200 (202.3)
CP 0.42 (0.006) 18.84 (0.640) 283 (3.883)
0.20.210.210.49
0.440.48
0.020.07
0.020.3
0.260.3
0.010.030.01
0.00.10.20.30.40.5
No Qual High School Matriculation Bachelors Master
Education StatusPropotionMethod IIâˆ’COS SAST CP
0.350.45
0.00.10.20.30.40.5
4 8 12 16
Education LengthPropotionMethod IIâˆ’COS SAST
Figure 4: Left: Education status composition of the correctly selected samples (II-COS, SAST and CP) in
candidate dataset; Right: Education length (year) composition of the correctly selected samples (II-COS, SAST)
in income dataset. The plots have error bar to show the variation across the 500 runs.
5 Concluding Remarks
Broader Impacts. This work focuses on creating reliable machine learning tools for making real-time
decisions. One key achievement is a new algorithm called II-COS, designed to select informative
samples in real-time while meeting two types of general constraints. II-COS allows for both individual
and interactive control, validated through theoretical analysis and numerical tests. Our method is
model-agnostic and easily applicable to many real-world cases such as producing diversified results
while controlling FSR for online recruitment. One potential negative impact of our work is that
researchers will apply the algorithm without sufficient scrutiny. We emphasize that itâ€™s important to
use caution when applying this method to complex real-world scenarios to prevent misuse.
Limitations. Firstly, we mainly consider binary functions as the interactive constraint. How to adapt
the II-COS to other popular constraints, such as the Gini index, deserves further study. Secondly,
in certain practical scenarios, it is possible to obtain feedback after decisions. Incorporating the
feedback information into our method to enhance its performance warrants future research.
10Acknowledgments and Disclosure of Funding
We thank anonymous area chair and reviewers for their helpful comments. Zou was supported
by the National Key R&D Program of China (Grant Nos. 2022YFA1003703, 2022YFA1003800),
the National Natural Science Foundation of China (Grant Nos. 11925106, 12231011, 11931001,
12226007, 12326325). Ren was supported by the National Natural Science Foundation of China
(Grant Nos. 12101398, 12471262), and Young Elite Scientists Sponsorship Program by CAST.
References
[1]Ehud Aharoni and Saharon Rosset. Generalized Î±-investing: definitions, optimality results and
application to public databases. Journal of the Royal Statistical Society: Series B (Statistical
Methodology) , 76(4):771â€“794, 2014.
[2]Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection
for online continual learning. Advances in Neural Information Processing Systems , 32, 2019.
[3]Anastasios N Angelopoulos and Stephen Bates. Conformal prediction: A gentle introduction.
Foundations and TrendsÂ® in Machine Learning , 16(4):494â€“591, 2023.
[4]Ruicheng Ao, Hongyu Chen, David Simchi-Levi, and Feng Zhu. Online local false discovery
rate control: A resource allocation approach. arXiv preprint arXiv:2402.11425 , 2024.
[5]Eric Bach, Shuchi Chawla, and Seeun Umboh. Threshold rules for online sample selection.
Discrete Mathematics, Algorithms and Applications , 2(4):625â€“642, 2010.
[6]Peter L. Bartlett. Learning with a slowly changing distribution. In Proceedings of the Fifth
Annual Workshop on Computational Learning Theory , page 243â€“252, 1992.
[7]Stephen Bates, Emmanuel CandÃ¨s, Lihua Lei, Yaniv Romano, and Matteo Sesia. Testing for
outliers with conformal p-values. Annals of Statistics , 51(1):149â€“178, 2023.
[8]Barry Becker and Ronny Kohavi. Adult income investigation. UCI Machine Learning Repository
https://archive.ics.uci.edu/dataset/2/adult , 1996.
[9]Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: a practical and
powerful approach to multiple testing. Journal of the Royal Statistical Society: Series B
(Statistical Methodology) , 57(1):289â€“300, 1995.
[10] Yoav Benjamini and Daniel Yekutieli. The control of the false discovery rate in multiple testing
under dependency. Annals of Statistics , 29(4):1165â€“1188, 2001.
[11] Rick Durrett. Probability: theory and examples , volume 49. Cambridge University Press, 2019.
[12] Bradley Efron, Robert Tibshirani, John D Storey, and Virginia Tusher. Empirical bayes analysis
of a microarray experiment. Journal of the American Statistical Association , 96(456):1151â€“
1160, 2001.
[13] Evanthia Faliagka, Kostas Ramantas, Athanasios Tsakalidis, and Giannis Tzimas. Application of
machine learning algorithms to an online recruitment system. In Proc. International Conference
on Internet and Web Applications and Services , pages 215â€“220, 2012.
[14] Evanthia Faliagka, Lazaros Iliadis, Ioannis Karydis, Maria Rigou, Spyros Sioutas, Athanasios
Tsakalidis, and Giannis Tzimas. On-line consistent ranking on e-recruitment: seeking the truth
behind a well-formed cv. Artificial Intelligence Review , 42(3):515â€“528, 2014.
[15] Kai-Tai Fang, Runze Li, and Agus Sudjianto. Design and modeling for computer experiments .
Chapman and Hall/CRC, 2005.
[16] Dean P Foster and Robert A Stine. Î±-investing: a procedure for sequential control of expected
false discoveries. Journal of the Royal Statistical Society: Series B (Statistical Methodology) ,
70(2):429â€“444, 2008.
11[17] Bowen Gang, Wenguang Sun, and Weinan Wang. Structureâ€“adaptive sequential testing for
online false discovery rate control. Journal of the American Statistical Association , 118(541):
732â€“745, 2023.
[18] Adel Javanmard and Andrea Montanari. On online control of false discovery rate. arXiv preprint
arXiv:1502.06197 , 2015.
[19] Adel Javanmard and Andrea Montanari. Online rules for control of false discovery rate and
false discovery exceedance. The Annals of Statistics , 46(2):526â€“554, 2018.
[20] Heinrich Jiang. Uniform convergence rates for kernel density estimation. In International
Conference on Machine Learning , pages 1694â€“1703. PMLR, 2017.
[21] Ying Jin and Emmanuel J CandÃ¨s. Selection by prediction with conformal p-values. Journal of
Machine Learning Research , 24(244):1â€“41, 2023.
[22] Kaggle. Candidate selection dataset. https://www.kaggle.com/datasets/
tarunchilkur/client , 2020.
[23] G Kanagavalli, R Seethalakshmi, and T Sowdamini. A systematic review of literature on
recruitment and selection process. Humanities & Social Sciences Reviews , 7(2):01â€“09, 2019.
[24] Mozhgan Karimi, Dietmar Jannach, and Michael Jugovac. News recommender systemsâ€“survey
and roads ahead. Information Processing & Management , 54(6):1203â€“1227, 2018.
[25] Jing Lei, Max Gâ€™Sell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman.
Distribution-free predictive inference for regression. Journal of the American Statistical Associ-
ation , 113(523):1094â€“1111, 2018.
[26] Ziyi Liang, Matteo Sesia, and Wenguang Sun. Integrative conformal p-values for out-of-
distribution testing with labelled outliers. Journal of the Royal Statistical Society Series B:
Statistical Methodology , 86(3):671â€“693, 01 2024.
[27] Ariane Marandon, Lihua Lei, David Mary, and Etienne Roquain. Adaptive novelty detection
with false discovery rate guarantee. The Annals of Statistics , 52(1):157â€“183, 2024.
[28] Houwen Peng, Bing Li, Haibin Ling, Weiming Hu, Weihua Xiong, and Stephen J. Maybank.
Salient object detection via structured matrix decomposition. IEEE Transactions on Pattern
Analysis and Machine Intelligence , 39(4):818â€“832, 2017.
[29] Aaditya Ramdas, Fanny Yang, Martin J Wainwright, and Michael I Jordan. Online control of
the false discovery rate with decaying memory. Advances in Neural Information Processing
Systems , 30:5655â€“5664, 2017.
[30] Aaditya Ramdas, Tijana Zrnic, Martin Wainwright, and Michael Jordan. Saffron: an adaptive
algorithm for online control of the false discovery rate. In International Conference on Machine
Learning , pages 4286â€“4294. PMLR, 2018.
[31] Bradley Rava, Wenguang Sun, Gareth M James, and Xin Tong. A burden shared is a burden
halved: A fairness-adjusted approach to classification. arXiv preprint arXiv:2110.05720 , 2021.
[32] David S Robertson, Jan Wildenhain, Adel Javanmard, and Natasha A Karp. onlineFDR: an R
package to control the false discovery rate for growing data repositories. Bioinformatics , 35
(20):4196â€“4199, 2019.
[33] David S Robertson, James MS Wason, and Aaditya Ramdas. Online multiple hypothesis testing.
Statistical Science , 38(4):557, 2023.
[34] Glenn Shafer and Vladimir V ovk. A tutorial on conformal prediction. Journal of Machine
Learning Research , 9(3):371â€“421, 2008.
[35] Muhammad Ahmad Shehu and Faisal Saeed. An adaptive personnel selection model for
recruitment using domain-driven data mining. Journal of Theoretical and Applied Information
Technology , 91(1):117, 2016.
12[36] Bernard W. Silverman. Weak and strong uniform consistency of the kernel estimate of a density
and its derivatives. The Annals of Statistics , 6(1):177â€“184, 1978.
[37] John D Storey. A direct approach to false discovery rates. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) , 64(3):479â€“498, 2002.
[38] Wenguang Sun and T. Tony Cai. Oracle and adaptive compound decision rules for false
discovery rate control. Journal of the American Statistical Association , 102(479):901â€“912,
2007.
[39] Stephen Taylor. Resourcing and talent management . Kogan Page Publishers, 2018.
[40] Jinjin Tian and Aaditya Ramdas. Addis: an adaptive discarding algorithm for online fdr control
with conservative nulls. Advances in Neural Information Processing Systems , 32:9388â€“9396,
2019.
[41] Vladimir V ovk and Ruodu Wang. E-values: Calibration, combination and applications. The
Annals of Statistics , 49(3):1736â€“1754, 2021.
[42] Vladimir V ovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random
world . New York: Springer, 2005.
[43] Michel Wedel and PK Kannan. Marketing analytics for data-rich environments. Journal of
Marketing , 80(6):97â€“121, 2016.
[44] Sang Eun Woo, James M LeBreton, Melissa G Keith, and Louis Tay. Bias, fairness, and validity
in graduate-school admissions: A psychometric perspective. Perspectives on Psychological
Science , 18(1):3â€“31, 2023.
[45] Xiaoyang Wu, Yuyang Huo, Haojie Ren, and Changliang Zou. Optimal subsampling via
predictive inference. Journal of the American Statistical Association , pages 1â€“29, 2023.
[46] Xiaoyang Wu, Lin Lu, Zhaojun Wang, and Changliang Zou. Conditional testing based on
localized conformal p-values. arXiv preprint arXiv:2409.16829 , 2024.
[47] Pengtao Xie, Yuntian Deng, and Eric Xing. Latent variable modeling with diversity-inducing
mutual angular regularization. arXiv preprint arXiv:1512.07336 , 2015.
[48] Ziyu Xu and Aaditya Ramdas. Online multiple testing with e-values. In International Conference
on Artificial Intelligence and Statistics , pages 3997â€“4005. PMLR, 2024.
[49] Yi Yang, Zhigang Ma, Feiping Nie, Xiaojun Chang, and Alexander G Hauptmann. Multi-class
active learning by uncertainty sampling with diversity maximization. International Journal of
Computer Vision , 113(2):113â€“127, 2015.
[50] Yifan Zhang, Haiyan Jiang, Haojie Ren, Changliang Zou, and Dejing Dou. Automs: automatic
model selection for novelty detection with error rate control. Advances in Neural Information
Processing Systems , 35:19917â€“19929, 2022.
[51] Zinan Zhao and Wenguang Sun. False discovery rate control for structured multiple testing:
Asymmetric rules and conformal q-values. Journal of the American Statistical Association ,
pages 1â€“24, 2024.
[52] Ping Zhong, Zhiqiang Gong, Shutao Li, and Carola-Bibiane SchÃ¶nlieb. Learning to diversify
deep belief networks for hyperspectral image classification. IEEE Transactions on Geoscience
and Remote Sensing , 55(6):3516â€“3530, 2017.
13Supplementary Material for â€œReal-Time Selection Under General
Constraints via Predictive Inferenceâ€
This supplementary material contains:
â€¢ Preliminary terms for self-containment (Appendix A);
â€¢ Implementation details (Appendix B);
â€¢ An extension algorithm to varying proportion case (Appendix C);
â€¢ Additional experiments (Appendix D);
â€¢ The proofs of all the theoretical results. (Appendix E).
A Preliminary Terms for Self-Containment
Here, we list the preliminary terms we use in the paper for the sake of clarity and self-containment.
â€¢FDR [9], false discovery rate, a widely-adopted error rate notion in the field of multiple
testing, is defined as the expected proportion of incorrectly rejected null hypotheses as
follows:
FDR( t) =E|H0âˆ© R(t)|
|R(t)| âˆ¨1
,
whereH0is the unknown set of true null hypotheses, R(t)represents the set of rejected null
hypotheses until time tand then H0âˆ© R(t)is the set of false discoveries.
â€¢FSR , false selection rate, defined as the expected proportion of individuals being not of
interest among the selected subset of individuals. It is in fact equivalent to the definition of
FDR. In our framework in this paper, we describe it equivalently as:
FSR( t) =E"P
iâ‰¤t(1âˆ’Î¸i)Î´i
(P
iâ‰¤tÎ´i)âˆ¨1#
.
B Implementation Details of Algorithms
ð’Ÿ
ð’Ÿð‘¡ð‘Ÿ ð’Ÿð‘ð‘Žð‘™
â„‹
Æ¸ðœ‡(ð‘‹) à·œðœ‹,à·¡ð‘“0,à·¡ð‘“1Offline EstimationOnline Decision ð‘‹ð‘¡
Compute: 
ð‘Šð‘¡,à·¡ð¿ð‘¡,
ð‘”(ð‘‹ð‘–,ð‘‹ð‘¡)ð¶1(ð‘¡)
ð¶2(ð‘¡)Yesð›¿ð‘¡=0
Yes
ð›¿ð‘¡=1Update:
ð‘‰ð‘¡,ð‘…ð‘¡,
ð‘‡ð‘†ð‘¡,ð‘ð‘†ð‘¡No
No
ð‘‹ð‘¡
Figure 5: The implementation flowchart of the data-driven II-COS procedure.
14B.1 Implementation Details of II-COS
Figure 5 shows the implementation flowchart of our proposed data-driven II-COS procedure, including
the offline estimation step and online decision step. Here we introduce more implementation details
to reproduce the results.
About the classifier. For the classification problem, we use the predicted conditional probability
cPr(Yt= 1|Xt=x)asbÂµ(x)in the procedure of II-COS. Many commonly used algorithms such as
random forest and neural networks can provide such probability estimators. In fact, the choice of bÂµ
is not restricted to probability. For example, the support vector machine outputs the distance to the
separating hyperplane for classification, and such distance can be chosen as bÂµ.
However, for probability estimators, one potential problem is that when the classifier is quite accurate,
most of Wiâ€™s are concentrated at 0or1and few of them take values between (0,1). In such a
situation, it is difficult to accurately and stably estimate lFDR Liâ€™s since the density functions f0(w)
andf(w)are not smooth enough and are not lower bounded. This yields that our assumptions are
violated, but the II-COS can still perform satisfactorily with some corrections on the estimation bLiâ€™s.
Notice that the larger Wiis, the more likely Yiâˆˆ A and hence the smaller Liis. That is bLishould be
monotonically decreasing as Wiincreases. Observing this, we can make a monotonization correction
onbLiâ€™s. To be specific, we rearrange bLiby the decreasing order of Wi. IfbL(W(iâˆ’1))<bL(W(i))
which violates the monotonicity, we revise it by bL(W(iâˆ’1)) =bL(W(i)), where W(i)is the ith
smallest among W. This monotonization correction enables us to avoid obvious errors due to
unstable estimation and improve the performances of all the methods utilizing bLiâ€™s.
Choice of K. A useful interactive constraint needs an appropriate specification of K. For any
two i.i.d. observations XandXâ€²with corresponding Î¸andÎ¸â€²respectively, the expected C2of the
individuals of interest is given as C2:=E[g(X,Xâ€²)|Î¸= 1, Î¸â€²= 1] , which can be estimated by
cC2=PP
i<j;i,jâˆˆLg(ËœXi,ËœXj)/{|L|(|L| âˆ’ 1)},where L={i:ËœYiâˆˆ A} . It is reasonable to set
K=acC2, where a >0is user-specific to control the interactive constraint level. Our numerical
evidence reveals that aâˆˆ(0.1,0.5)works generally well.
B.2 Implementation Details of SAST
Gang et al. [17] proposed a structure-adaptive sequential testing (SAST) rule for online false dis-
covery rate control. In their work, the rejecting rule is as follows: If Lt< Î³ tand{|Rtâˆ’1|+
1}âˆ’1P
iâˆˆRtâˆ’1Li+Lt
â‰¤Î±, then Î´t= 1. Otherwise Î´t= 0, where Rtâˆ’1={iâ‰¤tâˆ’1 :Î´i= 1}
andÎ³tis a barrier estimated from an "offline" procedure.
The implementation details of SAST for comparisons in our simulations in Section 4 are different
from the original one. Firstly, the original SAST assumes the null density function f0is already
known while in our setting f0remains unknown. Secondly, in our predictive inference setting, the
density functions and the null proportion are directly estimated via calibration set as the offline
estimation procedure in Algorithm 1, not from current rejection sets. Besides, considering the
time-varying structures of the data stream in their setting, Gang et al. [17] incorporated the barrier
strategy in their method, which is not necessary to be adopted here.
B.3 Implementation Details of Conformal p-values
The notion of conformal p-values was originally proposed by V ovk et al. [42] to construct prediction
interval. Recently, there exist some works to apply conformal p-values to implement sample selection
from a multiple-testing perspective, such as Bates et al. [7], Rava et al. [31] and Jin and CandÃ¨s [21].
In the sample selection problem, the hypothesis has the following form for each t,
H0t:Ytâˆˆ Acv.s. H1t:Ytâˆˆ A.
There are two types of conformal p-values and we adopt the one in Bates et al. [7]which utilizes the
same class calibration. Recall that for Wt, its conformal p-value ptis defined as
bpt=1 +P
iâˆˆD calI{Q(ËœWi)â‰¤Q(Wt)}
1 +|Dcal|.
15The nonconformity score function Q(Wt)is used to indicate the possibility of Î¸t= 0. For example,
in regression settings, if A= [b,+âˆž), we can use Q(Wj) =bâˆ’Wj. IfA= (âˆ’âˆž, a]âˆª[b,+âˆž),
then we can choose Q(Wt) = max {Wtâˆ’a, bâˆ’Wt}. And in binary classification settings, if A= 1
andWtindicates the probability of Yt= 1, we set Q(Wt) = 1âˆ’Wt.
Even though conformal p-values are correlated, using conformal p-values to conduct multiple testing
can control the FDR level with finite sample guarantee since they are positive regression dependent
on a subset (PRDS) [ 10]. However, the conformal p-values are lower bounded by 1/(|Dcal|+ 1) ,
which leads to unsatisfactory performance for online multiple testing methods based on p-values.
Since these methods require sufficiently small p-values to make rejections.
In our simulations, we implement LOND, SAFFRON and ADDIS for online sample selection by
R package OnlineFDR [32] with Î±= 0.1. Other parameters are defaulted. Here we introduce
the details about these online FDR control methods. Ramdas et al. [29] proposed a â€œstatistical
perspective" to control FDR in online setting, which is to keep an estimate of the FDP less than Î±
similar to the offline setting. Specifically, for offline FDR, let the rejection set R(s) ={i|piâ‰¤s}.
An oracle estimate for FDP is given by FDPâˆ—(s) :=|H0|Â·s
|R(s)|âˆ¨1.For online FDR, an oracle estimate of
FDPâˆ—(t)isP
jâ‰¤t,jâˆˆH0Î±j
R(t)âˆ¨1. Table 2 lists a comparison of estimating FDP in classical offline methods
and online methods for FDR control in multiple testing. For the online methods, denote the decision
rule as Î´t={ptâ‰¤Î±t}, where ptis the corresponding conformal p-value at time tfor our problem.
The test levels {Î±t}for LOND [18], SAFFRON [30] and ADDIS [40] are listed as follows:
Table 2: A comparison of [FDP in offline methods v.s. online methods for FDR control.
Offline dFDP dFDP(t) Online
BH [9]nÂ·s
|R(s)|âˆ¨1P
jâ‰¤tÎ±j
R(t)âˆ¨1LOND [18]
Storey-BH [37]nÂ·sÂ·Ë†Ï€0
|R(s)|âˆ¨1,Ë†Ï€0=Pn
i=11(pi>Î»)
n(1âˆ’Î»)P
jâ‰¤tÎ±j1{pj>Î»j}
(1âˆ’Î»j)
R(t)âˆ¨1SAFFRON [30]
P
jâ‰¤tÎ±j1{Î»j<pjâ‰¤Ï„j}
Ï„jâˆ’Î»j
R(t)âˆ¨1ADDIS [40]
â€¢LOND: Î±t=Î³t(R(tâˆ’1)+1) , where {Î³t}âˆž
t=1is a given infinite non-increasing sequence of
positive constants that sums to Î±andR(n) =Pn
t=1Rtdenotes the number of discoveries
in the first nhypotheses tested.
â€¢SAFFRON: At each time t, define Cj+=Cj+(t) =Ptâˆ’1
i=Ï„j+1, where Ct=I{ptâ‰¤Î»}.
Fort= 1,Î±1= min {Î³1W0, Î»}; Fort= 2,3, . . .,Î±t:= min {Î»,ËœÎ±t}, where
ËœÎ±t=W0Î³tâˆ’C0++ ((1âˆ’Î»)Î±âˆ’W0)Î³tâˆ’Ï„1âˆ’C1++ (1âˆ’Î»)Î±X
jâ‰¥2Î³tâˆ’Ï„jâˆ’Cj+.
â€¢ ADDIS: The testing levels for ADDIS are given by Î±t= min {Î»,Ë†Î±t}, where
Ë†Î±t= (Î·âˆ’Î»)[Ï‰0Î³Stâˆ’C0++ (Î±âˆ’Ï‰0)Î³Stâˆ’Ï„âˆ—
1âˆ’C1++Î±X
jâ‰¥2Î³Stâˆ’Ï„âˆ—
jâˆ’Cj+]
andSt=P
i<tI{piâ‰¤Î·},Ï„âˆ—
j=P
iâ‰¤Ï„jI{piâ‰¤Î·}.
B.4 Experiments Compute Resources
All the experiments were conducted on 3.11 GHz Intel Gen i5-11300H processors with 16 Gb memory
at a Lenovo personal computer and the R platform with version 4.2.1. The time of execution for
each of the individual experimental runs is about 6.686 seconds. And the total compute time for the
synthetic classification example in Section 4 for 500 replications is about 63.877 minutes.
16B.5 A Toy Example for Illustration in Section 2.3
We illustrate the idea of the II-COS procedure via a binary classification example. We aim to select
m= 50 data points of a specific class from unlabeled data arriving sequentially. We choose FSR as
the individual constraint and mES as the interactive constraint.
The data is generated as follows. The 4-dimensional covariates X= (X1, X2, X3, X4)âŠ¤are
generated from a mixture of multivariate normal distributions with mean (0,0,0,0)ifY= 0and
mean (âˆ’3,âˆ’3,0,0)ifY= 1. The proportions of Y= 0andY= 1are 80% and 20% respectively.
In this illustrative example, we set the historical data size n= 1,000with calibration size n1= 500
and would like to select m= 50 data points from the interest region A={1}with the target FSR
levelÎ±= 0.1. The mES threshold Kis set at 0.01for II-COS.
Figure 6 depicts the scatterplot of the first two dimensions of the covariates X, with green dots and
red triangles denoting correctly selected points and falsely selected ones, respectively. The SAST
method proposed by Gang et al. [17] is taken as one benchmark, which considers only the FSR
control. We observe that the selected points of II-COS enjoy significant diversity among the covariate
space and only a few false selections are contained. In contrast, the SAST is inclined to choose
similar samples concentrated at the center of the concerned group and stops too early to fully explore
the covariate space with sequentially arriving samples.
Correctlyâˆ’selected Falselyâˆ’selected
âˆ’5.0âˆ’2.50.0
âˆ’6âˆ’4âˆ’2 0
X1X2IIâˆ’COS
âˆ’5.0âˆ’2.50.0
âˆ’6âˆ’4âˆ’2 0
X1X2SAST
Figure 6: Scatter plots of selected points of the II-COS and SAST. It stops when selecting 50samples . Left:
the selection results of the II-COS; Right: the results of SAST. Green dots and red triangles indicate correct and
false selections, respectively.
C Extension to Varying Proportion Case
In practice, the distribution of (Xt, Yt)may vary smoothly over time. Due to the unknown Yt, it is
unrealistic to re-estimate parameters (i.e., Estimation Step in Algorithm 1) on both labeled data
and the most recent data. To mitigate this problem, we consider the probability of Ytâˆˆ A (i.e. the
proportion of samples in the specified region) varying over time and extend the proposed II-COS to
learn Ï€t= Pr( Ytâˆˆ A)continuously over time.
LetQ(Wt)be one score function, which is high when the possibility of Î¸t= 0is large and otherwise
low. For example, in regression settings, if A= [a,+âˆž), we can take Q(Wt) =aâˆ’Wt. And in
binary classification, if A= 1andWtindicates the probability of Yt= 1, we can set Q(Wt) = 1âˆ’Wt.
One valid conformal p-value for Q(Wt)can be obtained as Bates et al. [7],
bpt=1 +P
iâˆˆD calI{Q(ËœWi)â‰¤Q(Wt)}
1 +|Dcal|.
Inspired by the techniques for null proportion estimation in multiple testing literature [ 37], we note
thatPr(bptâ‰¥Î»)â‰ˆPr(bptâ‰¥Î», Î¸t= 0)â‰ˆÏ€t(1âˆ’Î»)for large Î»âˆˆ(0,1)(i.e.,Î»= 0.5).
Thus, we consider using some recent bptto estimate a reliable Ï€t. Denote qas the size of a neighbor-
hood{tâˆ’q, . . . , t âˆ’1}and fix Î». We employ an exponential weighted scheme to estimate Ï€twhere
17the more recent samples will contribute more to the estimation:
bÏ€Î»
t= 1âˆ’Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)I{bpj> Î»}
(1âˆ’Î»)Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t),
where Îºb(s) = exp {âˆ’|s|/b}andbis the bandwidth parameter. Then, we can compute the distribution
ofWtasbft=bf0(w)(1âˆ’bÏ€t) +bf1(w)bÏ€tand estimate LtbybLÎ»
t=(1âˆ’bÏ€Î»
t)bf0(Wt)
bft(Wt)âˆ§1.
The extended II-COS is to substitute bLÎ»
tforbLtin Algorithm 1. The following theorem establishes
the guarantee by assuming the slow change of distribution [6].
Theorem C.1. Assume Ï€tsatisfies |Ï€t+1âˆ’Ï€t| â‰¤ Î·for any t, and the bandwidth pa-
rameter bsatisfies bÎ¶â‰¤qfor some Î¶ > 1. Denote âˆ†â€²=c1nâˆ’Î²/(2Î²+1)
1âˆšlogn1+
c2max{bâˆ’1/3,(logn1/n1)1/6,(bÎ·)2/3}with constants c1,c2. Suppose ptandÎ»satisfy Pr(pt> Î»|
Î¸t= 1) = 0 . Under Assumption 3.1-3.2, we have:
(a) For any given time t, the individual constraint of extended II-COS satisfies C1(Î´t)â‰¤Î±+ âˆ†â€²;
(b) Furthermore, if the conditions in Theorem 3.4 hold, then for any given time t > T m, the
interactive constraint of the extended II-COS satisfies C2(Î´t)â‰¤K+(K+cg)âˆ†â€²
0.5âˆ’mÎ±â€²
mâˆ’1âˆ’âˆ†â€².
Besides the part similar to âˆ†n1in Theorem 3.3, the bound âˆ†â€²owns an additional term that can be
decomposed into three parts. The bâˆ’1/3indicates how many valid samples we use to estimate Ï€t. The
second part comes from the approximation error of p-values. The last one characterizes the effects of
distribution shift. If we properly choose bsuch that bÎ·=o(1), this term is negligible. Thus when b
andn1both tend to infinity, âˆ†â€²converges to 0and the individual and interactive constraints will be
controlled asymptotically.
D Additional Experiments
D.1 Illustration of the Similar Patterns between ËœC2andC2
We calculate the empirical ËœC2andC2during the online sample selection procedure under regression
setting in Section D.4 with II-COS from 500 replications. The results are summarized in Figure 7.
0.0080.0100.0120.014
200 400 600
TimeValue
Figure 7: Illustration of the similar patterns between ËœC2andC2. Line charts of ËœC2(Î´t)(red line) and C2(Î´t)
(blue line) over time tfor the regression example. Experiment details are in Appendix D.4. The black dashed
line is K= 0.015. The two measures yield almost identical patterns.
D.2 An Illustration Example of the Flexibility for Choices of Î±andK.
There exists some trade-off of stopping time and two criteria. In fact, II-COS could result in a short
stopping time when only one criterion is considered. Typically, one could choose K= +âˆžfor the
case the interactive constraint is out of work and only individual criterion control is considered, and
meanwhile one can set Î±= 1with which the interactive constraint is the only concern. The results in
Table 3 evaluated this conclusion where C1is the FSR and ËœC2is the ES.
18Table 3: Results of flexible choice of Î±andKfor the classification example. Average values among 500
repetitions: FSR( Î´Tm),ES(Î´Tm)and stopping time Tm.
Î±, K C 1(FSR) ËœC2(ES) Tm
Î±= 0.1, K= +âˆž 0.101 0.073 446.51
Î±= 0.1, K= 0.045 0.102 0.044 628.82
Î±= 1, K= 0.045 0.849 0.038 107.79
Table 4: Average number of selected samples when stopping for II-COS, LOND, SAFFRON and ADDIS.
Classification Regression
ncal II-COS LOND SAFFRON ADDIS II-COS LOND SAFFRON ADDIS
500 100 0.20 0.70 100 100 0.20 21.82 24.05
1000 100 0.72 10.40 100 100 0.87 56.72 59.20
1500 100 11.40 36.33 100 100 12.28 74.70 80.05
2000 100 44.20 50.79 100 100 40.99 80.03 82.02
2500 100 68.33 60.95 100 100 59.72 92.01 91.04
D.3 Effects of the Calibration Size
We perform some additional simulations under the classification setting, to study the stopping early
issue for those methods based on the conformal p-values.
Take Î±= 0.1andK= 0.045for FSR and mES, respectively. In Table 4, we first fix ntr= 1,000
andm= 100 and vary ncalfrom 500 to 2,500 to compare the average numbers of selected samples of
these three methods with II-COS until stopping. It is clear that all the three benchmarks are unable to
select enough samples across all the settings, especially with a small calibration set. As the calibration
sizen1increases, their selected numbers tend to be close to the target. This can be understood
because a larger calibration size would generally yield more accurate detection of the individual
of interest and thus alleviate the alpha-death issue to some extent. In contrast, the performances of
II-COS, in terms of the number of selected samples until stopping, would be much less influenced
by the size of calibration data. The II-COS only stops when msamples are obtained under all the
scenarios.
Furthermore, to verify that II-COS can guarantee both FSR( Î´Tm)andES(Î´Tm)control with a
relatively small calibration size ncal(such as 200), we apply II-COS in synthetic data. The details of
the data generation process can be found in Section 4 and D.4.
We fix ntr= 1000 ,m= 100 andncalvaries from 200 to 800. The average of FSR( Î´Tm)and
ES(Î´Tm)with II-COS for both scenarios are calculated from 500 replications. The results are
summarized as Table 5. Itâ€™s obvious that for different ncal, both FSR( Î´Tm)andES(Î´Tm)are
controlled under the pre-specified constant Î±andKrespectively under all scenarios with our II-COS
method.
Table 5: Average values of FSR( Î´Tm)(%) andES(Î´Tm)(Ã—10âˆ’2)(with standard errors in parentheses) for
II-COS with different ncalunder classification setting ( Î±= 10% , K= 4.50Ã—10âˆ’2) and regression setting
(Î±= 10% , K= 1.50Ã—10âˆ’2).
Classification Regression
ncalFSR ( Tm) ES ( Tm) FSR ( Tm) ES ( Tm)
200 9.22(0.73) 4 .41(0.08) 5 .74(0.18) 1 .49(0.03)
400 9.22(0.69) 4 .42(0.07) 4 .84(0.19) 1 .49(0.04)
600 9.47(0.80) 4 .39(0.09) 5 .18(0.19) 1 .49(0.04)
800 9.36(0.69) 4 .40(0.08) 5 .79(0.19) 1 .49(0.04)
19D.4 Results on Synthetic Data for Regression Setting
The following regression setting is considered: Y=âˆ’7X2
1+ 5 exp X2+ 10( X3+X4)2+Îµ,with
Xâˆ¼ N 4(0,I4)andÎµâˆ¼ N(0,1). The informative set is A= (c,âˆž), where cis the 80% quantile
ofY. The FSR and mES are considered as C1andC2, respectively. The prediction algorithm H
is taken as neural network, with a single hidden layer and 10 hidden neurons, implemented by R
package nnet , and Kis chosen as 0.015.
The simulation results are summarized in Figure 8 and 9 from 500 replications. The results are similar
to those for classification setting. Itâ€™s further verified that our proposed II-COS outperforms all the
benchmarks under both classification and regression scenarios.
0.000.050.100.150.20
100 200 300
TimeC1(FSR)
0.010.020.030.040.05
100 200 300
TimeC~
2(ES)
Method IIâˆ’COS SAST LOND SAFFRON ADDIS
Figure 8: Simulation results for regression setting. Line charts of FSR( Î´t)(Left) and ES(Î´t)(Right)
for II-COS, SAST, LOND, SAFFRON and ADDIS over time t. The black dashed lines are the
corresponding FSR level Î±= 0.1and the ES level K= 0.015. Shading represents error bars of one
standard error above and below.
0.000.050.100.150.20
IIâˆ’COS
SAST
LOND
SAFFRON
ADDISC1(FSR)
0.000.030.060.090.12
IIâˆ’COS
SAST
LOND
SAFFRON
ADDISC~
2(ES)
050010001500
IIâˆ’COS
SAST
LOND
SAFFRON
ADDISTm
Figure 9: Simulation results for regression setting. Boxplots of FSR( Î´Tm)(Left), ES(Î´Tm)(Middle)
andTm(Right) for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines are the
corresponding FSR level Î±= 0.1and ES level K= 0.015.
D.5 Experiments for Other Individual and Interactive Constrains.
Other individual constraints To better illustrate the performance of our proposed method, we
also conduct an experiment for the general individual constraint. We choose G0(X) =|Pd
i=1Xi|,
G1(X) =|Pd
i=1Xi|/2for EC (expected cost), and choose G0(X) = 1 ,G1(X) = 0 for FSR. Other
settings are the same as the classification model. The results are shown in Figure 10. We can see that
only the proposed II-COD can guarantee all EC (expected cost), FSR and ES control, while all the
benchmarks are out of control for EC or ES.
Other pairwise function g.Besides the RBF kernel, popular choices include the cosine similarity
[47,52] with adjustment g(X,Xâ€²) =XâŠ¤Xâ€²/(âˆ¥Xâˆ¥2âˆ¥Xâ€²âˆ¥2) + 1 , and the absolute value of cosine
similarity, i.e., g(X,Xâ€²) =|XâŠ¤Xâ€²|/(âˆ¥Xâˆ¥2âˆ¥Xâ€²âˆ¥2)which characterizes the orthogonality between
200.00.51.01.5
IIâˆ’COSSASTLONDSAFFRONADDISC1(EC)
0.00.20.4
IIâˆ’COSSASTLONDSAFFRONADDISC1(FSR)
0.000.050.100.15
IIâˆ’COSSASTLONDSAFFRONADDISC~
2(ES)Figure 10: Boxplots of EC(Î´Tm),FSR( Î´Tm), and ES(Î´Tm)for II-COS, SAST, LOND, SAFFRON and
ADDIS under classification model. The black dashed lines indicate the corresponding nominal levels Î±1=
0.5, Î±2= 0.2, K= 0.045.
XandXâ€²and is often considered in the field of design of experiments [ 15]. The similarity functions
mentioned above all satisfy Assumption 3.2. As a supplement to the experiments in the main text, we
choose the cosine similarity with adjustment g(X,Xâ€²) =XâŠ¤Xâ€²/(âˆ¥Xâˆ¥2âˆ¥Xâ€²âˆ¥2)+1 for classification
setting. Notice that the original cosine similarity XâŠ¤Xâ€²/(âˆ¥Xâˆ¥2âˆ¥Xâ€²âˆ¥2)can be negative sometimes.
Hence we add a constant 1which guarantees g(X,Xâ€²)â‰¥0and does not change the final results. The
simulation results are summarized in Figure 11 from 500 replications. As can be seen, the simulation
results are similar to those when choosing the RBF kernel in Section 4.1.
0.00.10.2
II-COSSASTLONDSAFFRONADDIS
MethodFSR
0.951.001.051.10
II-COSSASTLONDSAFFRONADDIS
MethodES
0200400600800
II-COSSASTLONDSAFFRONADDIS
MethodTm
Figure 11: Boxplots of FSR( Tm),ES(Tm)andTmwith random forest algorithms under classification setting
for II-COS, SAST, LOND, SAFFRON and ADDIS. The similarity function gis chosen as the cosine similarity
with adjustment g(X,Xâ€²) =XâŠ¤Xâ€²/(âˆ¥Xâˆ¥2âˆ¥Xâ€²âˆ¥2) + 1 . The black dashed lines are the corresponding FSR
levelÎ±= 0.1and the ES level K= 1.0.
D.6 Additional Results for Real Data Analysis
In Table 6, the average proportions of handicapped and female in the selected candidates are in the
last two columns of (a). The average proportions of minority and female among the correctly selected
individuals are in the last two columns of (b).
D.7 Additional Experiments for Different Learning Algorithms
Except for the random forest algorithm, we further apply two different learning algorithms Hfor
classification setting to estimate the model Ë†Âµ(X): Support vector machine (SVM) and Neural network
(NN) with a single hidden layer and 10 hidden neurons, which are implemented by R packages
kernlab andnnet , respectively. For NN algorithm, the entropy fitting is used for classification
setting. The empirical FSR( Tm)andES(Tm)levels are estimated by the average of the false
selection proportion and the expected similarity respectively from 500 replications. The results are
summarized in Figure 12. As can be seen, the simulation results are similar as those when applying
random forest in Section 4.1.
21Table 6: Average values with candidate dataset and income dataset: FSR( Î´Tm),ES(Î´Tm)(Ã—10âˆ’3) and
stopping time Tm. The average proportions of handicapped and female in the selected candidates are in the last
two columns of (a). The average proportions of minority and female among the correctly selected individuals
are in the last two columns of (b). The target FSR level is Î±= 0.2for both. For the candidate data, the target
ESlevelK= 1Ã—10âˆ’3; For the income data, K= 6Ã—10âˆ’3.
(a) Candidate dataset [22]
Method FSR ES Tm Handicap Female
II-COS 0.19 0.98 2227 0.15 0.48
SAST 0.19 8.73 310 0.05 0.47
CP 0.16 10.34 277 0.04 0.46(b) Income dataset [8]
Method FSR ES Tm Minority Female
II-COS 0.16 5.56 2760 0.05 0.09
SAST 0.19 30.90 1200 0.03 0.05
CP 0.42 18.84 283 0.06 0.13
NN SVM
IIâˆ’COS SAST LONDSAFFRONADDIS IIâˆ’COS SAST LONDSAFFRONADDIS0.000.100.200.300.400.50
MethodFSR
NN SVM
IIâˆ’COS SAST LONDSAFFRONADDIS IIâˆ’COS SAST LONDSAFFRONADDIS0.000.030.060.09
MethodES
Figure 12: Boxplots of FSR( Tm)andES(Tm)with two different learning algorithms under classification
setting for II-COS, SAST, LOND, SAFFRON and ADDIS. The black dashed lines are the corresponding FSR
levelÎ±= 0.1and the ES level K= 0.045.
D.8 Experiments for the comparison with oracle procedure
Regarding efficiency, we conducted an experiment to compare the effectiveness of II-COS with an
oracle method possessing knowledge of true state Î¸t. This can be succinctly formulated as follows.
At time t,Î´t= 1ifÎ¸t= 1andPC(Î´Tm)/PS(Î´Tm)â‰¤K,the process halts whenP
iâ‰¤tÎ´i=m. To
ensure a fair comparison, we fix Î±= 0.01for II-COS and choose K= 0.045for both methods. The
results are shown in Table 7. The FSR of the oracle procedure is 0 as expected. The Tmof II-COS is
very close to the oracle. This close proximity indicates the high efficiency of II-COS. Both FSR and
ES are effectively controlled by II-COS.
D.9 Experiments for the Extended II-COS under Varying Proportion Case
Table 7: FSR( Î´Tm),ES(Î´Tm)(Ã—10âˆ’2)
andTmof the II-COS and the oracle proce-
dure under classification model.
Method FSR ES Tm
II-COS 0.0097 4.41 686.245
Oracle 0 4.41 681.665LetNbe a prespecified large integer denoting the number
of samples arriving sequentially. Denote nas the size of
available labeled history dataset D. We fix N= 5000 ,
n= 5000 . Consider three different varying patterns for Ï€t:
1.Blocks pattern: Ï€t= 0.3, for tâˆˆ[1,100]âˆª
[501,600]âˆª[1001 ,1100] âˆª[1501 ,2000] âˆª
[2001 ,2100] âˆª[2501 ,2600] âˆª[3001 ,3100] âˆª
[3501 ,4000] ;Ï€t= 0.2fortâˆˆ[101,500]âˆª
[601,1000] âˆª[1101 ,1500] âˆª[2101 ,2500] âˆª
[2601 ,3000]âˆª[3101 .3500]âˆª[4000 ,5000] .
222.Linear pattern: Vary Ï€tlinearly from 0 to 0.5,
Ï€t= 0.5t/N.
3.Sine pattern: Ï€t={sin(8Ï€t/N ) + 1}/4,Ï€t
ranges between 0 and 0.5.
The other settings are the same as those in classification setting. For the historical dataset, we fix
Ï€t= 0.2.
In simulation we generate 500 data points prior to t= 1to form an initial proportion estimate. The
varying proportion estimates are updated every 200 time points. The bandwidth parameter bfor
estimating Ï€Î»
tare chosen based on normal reference rule. We set b= 38 . The window size qare
chosen by a rule of thumb in practice. Let q=Cb, where Cis a fixed constant between 10 and 20.
In our simulations, we fix q= 500 andÎ»= 0.5. As for the density estimation, we first estimate f0
andf1on the calibration data, then compute bft=bf0(w)(1âˆ’bÏ€t) +bf1(w)bÏ€t. Thus accordingly, we
estimate LtbybLÎ»
t=(1âˆ’bÏ€Î»
t)bf0(Wt)
bft(Wt)âˆ§1.As for those methods based on conformal p-values, they do
not need modification since they only require the exchangeability of data in the null hypothesis. The
varying proportion case does not violate such a condition.
The simulation results are shown in Figures 13-15, from which we can observe that the extended
version of II-COS performs better than the benchmarks under all three different varying settings for
bothC1(FSR) and C2(ES) control.
0.000.050.100.150.200.25
II-COSSASTLONDSAFFRONADDIS
MethodFSR
0.000.050.100.15
II-COSSASTLONDSAFFRONADDIS
MethodES
010002000300040005000
II-COSSASTLONDSAFFRONADDIS
MethodTm
Figure 13: Boxplots of FSR( Î´Tm),ES(Î´Tm)andTmfor II-COS, SAST, LOND, SAFFRON and ADDIS
(Blocks pattern). The black dashed lines indicate the corresponding nominal levels.
0.0000.0250.0500.0750.100
II-COSSASTLONDSAFFRONADDIS
MethodFSR
0.000.050.100.15
II-COSSASTLONDSAFFRONADDIS
MethodES
010002000300040005000
II-COSSASTLONDSAFFRONADDIS
MethodTm
Figure 14: Boxplots of FSR( Î´Tm),ES(Î´Tm)andTmfor II-COS, SAST, LOND, SAFFRON and ADDIS
(Linear pattern). The black dashed lines indicate the corresponding nominal levels.
230.000.050.100.150.20
II-COSSASTLONDSAFFRONADDIS
MethodFSR
0.000.050.100.15
II-COSSASTLONDSAFFRONADDIS
MethodES
010002000300040005000
II-COSSASTLONDSAFFRONADDIS
MethodTmFigure 15: Boxplots of FSR( Î´Tm),ES(Î´Tm)andTmfor II-COS, SAST, LOND, SAFFRON and ADDIS
(Sine pattern). The black dashed lines indicate the corresponding nominal levels.
E Proofs
E.1 Auxiliary Lemmas
The following results include a standard uniform bound for kernel density estimator [ 36,20] and a
simple corollary from the central limit theorem [11].
Lemma E.1. If Assumption 3.1 hold and we take the bandwidth h=nâˆ’1/(2Î²+1)
1 , then with probabil-
ity at least 1âˆ’1/n1
sup
wâˆˆR|bf(w)âˆ’f(w)| â‰¤D1nâˆ’Î²
2Î²+1
1p
logn1,
where D1=D1(M, c Î², Î², K )is a positive constant depending on MandÎ², cÎ²of HÃ¶lder continuity
and the kernel K(Â·).
Lemma E.2. The estimation bÏ€satisfies
|bÏ€âˆ’Ï€| â‰¤p
Ï€(1âˆ’Ï€)nâˆ’1
2+Î³
1 ,
with probability 1âˆ’nâˆ’2Î³
1for any constant 0< Î³ < 1/2.
Next lemma characterizes the uniform convergence of bL(w).
Lemma E.3 (Uniform convergence of bL(w)).Suppose Assumption 3.1 holds. Taking the bandwidth
of kernel density estimator as h=nâˆ’1/(2Î²+1)
1 , then we have
sup
wâˆˆR|bL(w)âˆ’L(w)| â‰¤D2nâˆ’Î²
2Î²+1
1p
logn1,
for some positive constant D2=D2(M, D 1, â„“, Ï€)with probability 1âˆ’2/n1âˆ’1/n1/3
1.
Proof .
24Note that bL(w) = (1 âˆ’bÏ€)bf0(w)
bf(w)âˆ§1and
sup
wâˆˆR|bL(w)âˆ’L(w)| â‰¤sup
wâˆˆR(1âˆ’bÏ€)bf0(w)
bf(w)âˆ’L(w)= sup
wâˆˆR(1âˆ’bÏ€)bf0(w)f(w)âˆ’(1âˆ’Ï€)f0(w)bf(w)
bf(w)f(w)
â‰¤sup
wâˆˆR1
bf(w)f(w)n
(1âˆ’bÏ€)f(w)|bf0(w)âˆ’f0(w)|+|Ï€âˆ’bÏ€|f0(w)f(w) + (1 âˆ’Ï€)f0(w)|bf(w)âˆ’f(w)|o
â‰¤sup
wâˆˆR|bf0(w)âˆ’f0(w)|+f0(w)|Ï€âˆ’bÏ€|
bf(w)+ sup
wâˆˆRL(w)|bf(w)âˆ’f(w)|
bf(w)
(i)
â‰¤1
infwâˆˆRbf(w){D1nâˆ’Î²
2Î²+1
1p
logn1+Mp
Ï€(1âˆ’Ï€)nâˆ’1
2+Î³
1 +D1Â·nâˆ’Î²
2Î²+1
1p
logn1}
(ii)
â‰¤2
â„“{Dâˆ—
1nâˆ’Î²
2Î²+1
1p
logn1+Mp
p(1âˆ’p)nâˆ’1
2+Î³
1 +D1Â·nâˆ’Î²
2Î²+1
1p
logn1}
(iii)
â‰¤D2nâˆ’Î²
2Î²+1
1p
logn1,
where Dâˆ—
1=Dâˆ—
1(D1, Ï€)is a positive constant depending on D1andÏ€because bf0is estimated by
about n1(1âˆ’Ï€)samples. The (i)follows the results directly in Lemma E.1 and Lemma E.2 and
the bounds of f(w)andf0(w)in Assumption 3.1-(1) with probability 1âˆ’2/n1âˆ’1/n2Î³
1. The (ii)
holds since
bf(w)â‰¥f(w)âˆ’D1nâˆ’Î²
2Î²+1
1p
logn1â‰¥â„“âˆ’D1nâˆ’Î²
2Î²+1
1p
logn1> â„“/2
for sufficiently large n1. Due to the fact Î²â‰¤1and taking Î³= 1/6in Lemma E.2, the (iii)holds
withD2= 2{Dâˆ—
1+Mp
Ï€(1âˆ’Ï€) +D1}/â„“. Hence we verify the conclusion. â–¡
The next lemma shows the property of data-driven II-COS procedure which is useful to give the
bound of the mES gap.
Lemma E.4. Suppose Î´tis the decision result at time tâ‰¥Tsby the data-driven II-COS procedure
withP
iâ‰¤tbLiÎ´i/1âˆ¨P
iâ‰¤tÎ´iâ‰¤Î±â€², where Ts= inf{t:Pt
i=1Î´i=s}andsâ‰¥2. Then we have
E(PP
1â‰¤i<jâ‰¤tÎ´iÎ´j)
EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)oâ‰¤ 
1âˆ’2s
sâˆ’1Î±â€²âˆ’1.
Proof . It suffices to show
XX
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)â‰¥(1âˆ’2s
sâˆ’1Î±â€²)XX
1â‰¤i<jâ‰¤tÎ´iÎ´j.
By the selection procedure, we have
tX
i=1Î´i(1âˆ’bLi)â‰¥(1âˆ’Î±â€²)tX
i=1Î´i. (9)
Squaring both sides of (9) and making some decomposition, we have
XX
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)â‰¥(1âˆ’Î±â€²)2XX
1â‰¤i<jâ‰¤tÎ´iÎ´j+1
2tX
i=1{(1âˆ’Î±â€²)2âˆ’(1âˆ’bLi)2}Î´i.
Notice that
tX
i=1Î´i=2
sâˆ’1XX
1â‰¤i<jâ‰¤tÎ´iÎ´j.
25We have
(1âˆ’Î±â€²)2XX
1â‰¤i<jâ‰¤tÎ´iÎ´j+1
2tX
i=1{(1âˆ’Î±â€²)2âˆ’(1âˆ’bLi)2}Î´i
â‰¥(1âˆ’Î±â€²)2XX
1â‰¤i<jâ‰¤tÎ´iÎ´j+1
2tX
i=1{(1âˆ’Î±â€²)2âˆ’1}Î´i
â‰¥(1âˆ’Î±â€²)2XX
1â‰¤i<jâ‰¤tÎ´iÎ´jâˆ’Î±â€²(2âˆ’Î±â€²)
22
sâˆ’1XX
1â‰¤i<jâ‰¤tÎ´iÎ´j
â‰¥(1âˆ’2s
sâˆ’1Î±â€²)XX
1â‰¤i<jâ‰¤tÎ´iÎ´j.
â–¡
The next two lemmas will be used in the analysis of the extended II-COS for varying proportion case.
Lemma E.5. Assume that random variables Y1,Y2, have density functions bounded by a constant
c > 0, and other two random variables Y3, and Y4satisfy P(|Y1âˆ’Y3| â‰¤Îµ)â‰¥1âˆ’Î¶and
P(|Y2âˆ’Y4| â‰¤Ïµ)â‰¥1âˆ’Ï‚, where Îµ >0,Ïµ >0,Î¶ >0andÏ‚ >0. Then for all t >0,
|P(Y1> t, Y 2> t)âˆ’P(Y3> t, Y 4> t)| â‰¤2c(Îµ+Ïµ) + 3( Î¶+Ï‚).
Proof . Define the events E1={|Y1âˆ’Y3| â‰¤Îµ},E2={|Y2âˆ’Y4| â‰¤Ïµ}, andE=E1SE2. Note that
P(Y1+Îµ > t,E1)â‰¤P(Y3> t,E1)â‰¤P(Y1âˆ’Îµ > t,E1),
and
P(Y2+Ïµ > t,E2)â‰¤P(Y4> t,E2)â‰¤P(Y2âˆ’Ïµ > t,E2),
we have
P(Y1> t, Y 2> t)âˆ’P(Y3> t, Y 4> t)
â‰¤ |P(Y1> t, Y 2> t)âˆ’P(Y3> t, Y 4> t,E)|+P(Ec
1) +P(Ec
2)
â‰¤ |P(Y1> t, Y 2> t)âˆ’P(Y1âˆ’Îµ > t, Y 2âˆ’Ïµ > t,E)|
+|P(Y1> t, Y 2> t)âˆ’P(Y1+Îµ > t, Y 2+Ïµ > t,E)|+P(Ec
1) +P(Ec
2)
â‰¤ |P(Y1> t, Y 2> t)âˆ’P(Y1âˆ’Îµ > t, Y 2âˆ’Ïµ > t)|+ 3P(Ec
1) + 3P(Ec
2)
+|P(Y1> t, Y 2> t)âˆ’P(Y1+Îµ > t, Y 2+Ïµ > t)|
â‰¤2c(Îµ+Ïµ) + 3( Î¶+Ï‚),
the last inequality holds because the density function is bounded.
â–¡
Lemma E.6. IfbÎ¶â‰¤qfor some Î¶ >1andbis sufficiently large, the exponential weight Îºb(s) =
exp{âˆ’|s|/b}satisfiesPq
s=1Îº2
b(s)
{Pq
s=1Îºb(s)}2â‰¤Cb1
b
andPq
s=1sÎºb(s)Pq
s=1Îºb(s)â‰¤Cbb,
where Cb>0is constant determined by b.
Proof . Take Zb= exp{âˆ’1/b}. Notice that
jX
s=iÎºb(s) =exp{âˆ’(i+ 1)/b} âˆ’exp{âˆ’(j+ 1)/b}
1âˆ’exp{âˆ’1/b}=Zi+1
bâˆ’Zj+1
b
1âˆ’Zb.
26Thus for a constant Cb>0. we have
Pq
s=1Îº2
b(s)
{Pq
s=1Îºb(s)}2=Z4
bâˆ’Z2q
b
1âˆ’Z2
b
/Z4
bâˆ’2Zq+2
b+Z2q
b
1 +Z2
bâˆ’2Zb
(i)
â‰¤Cb(1âˆ’Zb)
(ii)
â‰¤Cb1
b,
where ( i) holds since bis sufficiently large such that Zq
b= exp {âˆ’q/b} â‰¤ exp{âˆ’bÎ¶âˆ’1}can be
eliminated and Zb<1. The last inequality ( ii) holds by exp{x} â‰¥x+ 1.
By the same discussion, we have
Pq
s=1sÎºb(s)Pq
s=1Îºb(s)â‰¤Zâˆž
1sZs
bds/Z2
bâˆ’Zq
b
1âˆ’Zb
â‰¤Zb
(1âˆ’Zb)2
/Z2
bâˆ’Zq
b
1âˆ’Zb
â‰¤Cb1
1âˆ’Zb
â‰¤Cb(b+ 1).
The last inequality holds since exp{x} â‰¤1/(1âˆ’x)forx <1. For simplicity, we use the same
notation Cbto denote the constants. â–¡
E.2 Proof of Proposition 2.1
Proof . For the part of individual constraint control, note that Liis defined as
Li= Pr( Î¸i= 0|Wi) =E[(1âˆ’Î¸i)|Wi].
Define Wâˆ—=Ïƒ(W1,Â·Â·Â·). The stopping time Tis measurable respect to Wâˆ—. The individual
constraint at time Tsatisfies
C1(Î´T) = E(P
iâ‰¤T{(1âˆ’Î¸i)G0(Xi) +Î¸iG1(Xi)}Î´i
(P
iâ‰¤TÎ´i)âˆ¨1)
=Eï£®
ï£°E
(RTâˆ¨1)âˆ’1X
iâ‰¤t{(1âˆ’Î¸i)G0(Xi) +Î¸iG1(Xi)}Î´i| Wâˆ—	ï£¹
ï£»
(i)=Eï£®
ï£°(Rtâˆ¨1)âˆ’1X
iâ‰¤t{E
(1âˆ’Î¸i)|Wi	
G0(Xi) +E
Î¸i|Wi	
G1(Xi)}Î´iï£¹
ï£»
=E
(Rtâˆ¨1)âˆ’1X
iâ‰¤t{LiG0(Xi) + (1 âˆ’Li)G1(Xi)}Î´i	
.
The(i)holds since Xiâ€™s are independent of each other. By construction of the selection rule, we have
that at the stopping time T,((RTâˆ¨1)âˆ’1P
iâ‰¤T{LiW0(Xi) + (1 âˆ’Li)W1(Xi)}Î´iâ‰¤Î±. It follows
thatC1(Î´T)â‰¤Î±at a random time T. By construction of the selection rule, we have for stopping
timeT,
XX
1â‰¤i<jâ‰¤Tg(Xi,Xj)Î´iÎ´j(1âˆ’Li)(1âˆ’Lj)â‰¤KÃ—XX
1â‰¤i<jâ‰¤TÎ´iÎ´j(1âˆ’Li)(1âˆ’Lj).(10)
Taking expectations on both sides of inequality (10) and by double expectation theorem, we finally
obtain
EnXX
1â‰¤i<jâ‰¤Tg(Xi,Xj)Î´iÎ´jÎ¸iÎ¸jo
â‰¤KÃ—EnXX
1â‰¤i<jâ‰¤TÎ´iÎ´jÎ¸iÎ¸jo
.
27It follows that for every time tâ‰¥T2,
C2(Î´T) =EnPP
1â‰¤i<jâ‰¤Tg(Xi,Xj)Î´iÎ´jÎ¸iÎ¸jo
En
(P
iâ‰¤TÎ´iÎ¸i)(P
iâ‰¤TÎ´iÎ¸iâˆ’1)oâ‰¤K
â–¡
E.3 Proof of Theorem 3.3
Proof . In the data-driven II-COS procedure, Î´iis determined by the estimated lFDR bLiand we have
1
RtX
iâ‰¤t{bLiG0(Xi) + (1 âˆ’bLi)G1(Xi)}Î´iâ‰¤Î±.
Note that
C1(Î´t) =En1
RtX
iâ‰¤t{LiG0(Xi) + (1 âˆ’Li)G1(Xi)}Î´io
=En1
RtX
iâ‰¤t{bLiG0(Xi) + (1 âˆ’bLi)G1(Xi)}Î´io
+En1
RtX
iâ‰¤t{(Liâˆ’bLi)G0(Xi) + (bLiâˆ’Li)G1(Xi)}Î´io
â‰¤Î±+En1
RtX
iâ‰¤t{(Liâˆ’bLi)G0(Xi) + (bLiâˆ’Li)G1(Xi)}Î´io
.
It suffices to bound the absolute value of the second term. We have
En1
RtX
iâ‰¤t{(Liâˆ’bLi)G0(Xi) + (bLiâˆ’Li)G1(Xi)}Î´io
â‰¤En1
RtX
iâ‰¤t{|Liâˆ’bLi||G0(Xi)|+|bLiâˆ’Li||G1(Xi)|}Î´io
â‰¤En1
RtX
iâ‰¤t{sup
wâˆˆR|Liâˆ’bLi||G0(Xi)|+ sup
wâˆˆR|bLiâˆ’Li||G1(Xi)|}Î´io
â‰¤2cGD2nâˆ’Î²
2Î²+1
1p
logn1+ 2nâˆ’1
1+nâˆ’1
3
1.
The last inequality follows from Lemma E.3 and max w|bL(w)âˆ’L(w)| â‰¤1. Notice that nâˆ’1
3
1â‰¤
nâˆ’Î²
2Î²+1
1 byÎ²â‰¤1. We have
C1(Î´t)â‰¤Î±+Dnâˆ’Î²
2Î²+1
1p
logn1,
where D= 2cGD2+ 3. â–¡
E.4 Proof of Theorem 3.4
Proof . Notice that
EnXX
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(1âˆ’bLi)(1âˆ’bLj)âˆ’XX
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(1âˆ’Li)(1âˆ’Lj)o
â‰¤EnXX
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(|bLiâˆ’Li|+|bLjâˆ’Lj|+bLi|bLjâˆ’Lj|+Lj|bLiâˆ’Li|)o
(i)
â‰¤2cgEnXX
1â‰¤i<jâ‰¤tÎ´iÎ´j(|bLiâˆ’Li|+|bLjâˆ’Lj|)o
(ii)
â‰¤2cgDnâˆ’Î²
2Î²+1
1p
logn1E(XX
1â‰¤i<jâ‰¤tÎ´iÎ´j). (11)
28The(i)holds by Assumption 3.2 and L(w),bL(w)â‰¤1even when tis random, as the uniform
convergence of bL(w). And (ii)follows from Lemma E.3. By the similar arguments we have
EnXX
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)âˆ’XX
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’Li)(1âˆ’Lj)o
â‰¤2Dnâˆ’Î²
2Î²+1
1p
logn1E(XX
1â‰¤i<jâ‰¤tÎ´iÎ´j). (12)
Denote âˆ†n1=Dnâˆ’Î²
2Î²+1
1âˆšlogn1. Combining (11) and (12), it follows that for every time tâ‰¥Ts,
C2(Î´t) =EnPP
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(1âˆ’Li)(1âˆ’Lj)o
EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’Li)(1âˆ’Lj)o
(i)
â‰¤EnPP
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(1âˆ’bLi)(1âˆ’bLj)o
+ 2cgâˆ†n1E(PP
1â‰¤i<jâ‰¤tÎ´iÎ´j)
EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’Li)(1âˆ’Lj)o
(ii)
â‰¤h
K+ 2cgâˆ†n1E(PP
1â‰¤i<jâ‰¤tÎ´iÎ´j)
EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)oi
Ã—EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)o
EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’Li)(1âˆ’Lj)o
(iii)
â‰¤n
K+2cgâˆ†n1
1âˆ’2mÎ±â€²
mâˆ’1o
Ã—EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)o
EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’Li)(1âˆ’Lj)o
(iv)
â‰¤n
K+2cgâˆ†n1
1âˆ’2mÎ±â€²
mâˆ’1o
Ã—h
1âˆ’2âˆ†n1E(PP
1â‰¤i<jâ‰¤tÎ´iÎ´j)
EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)oiâˆ’1
(v)
â‰¤n
K+2cgâˆ†n1
1âˆ’2mÎ±â€²
mâˆ’1on
1âˆ’2âˆ†n1
1âˆ’2mÎ±â€²
mâˆ’1oâˆ’1
.
The(i)follows from (11) , and (ii)comes from the operation of our algorithm, where
EnPP
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(1âˆ’bLi)(1âˆ’bLj)o
â‰¤KÂ·EnPP
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’bLi)(1âˆ’bLj)o
.
The(iii)and(v)are directly from Lemma E.4. The (iv)holds due to (12). Thus we have
C2(Î´t)â‰¤K+(K+cg)âˆ†n1
0.5âˆ’mÎ±â€²
mâˆ’1âˆ’âˆ†n1
â–¡
E.5 Proof of Corollary 3.5
Proof . Define the C1constraint conditional on Was
Câ€²
1(Î´t) =1
RtX
iâ‰¤t{(Liâˆ’bLi)G0(Xi) + (bLiâˆ’Li)G1(Xi)}Î´i.
By the proofs of Theorem 3.3, we have for any t
Câ€²
1(Î´t)â‰¤Î±+ 2cGsup
wâˆˆR|bL(w)âˆ’L(w)|.
29Hence at stopping time Tm, we have
Câ€²
1(Î´Tm) =E[Câ€²
1(Î´Tm)]â‰¤E[sup
tCâ€²
1(Î´t)]â‰¤Î±+ 2cGE[ sup
wâˆˆR|bL(w)âˆ’L(w)|]â‰¤Î±+ âˆ† n1.
By the proof of Theorem 3.4, we can also drop off the expectation and replace âˆ†n1with
supwâˆˆR|bL(w)âˆ’L(w)|. That is
{1âˆ’2 supwâˆˆR|bL(w)âˆ’L(w)|
1âˆ’2mÎ±
mâˆ’1oXX
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(1âˆ’Li)(1âˆ’Lj)
â‰¤n
K+2cgsupwâˆˆR|bL(w)âˆ’L(w)|
1âˆ’2mÎ±
mâˆ’1oXX
1â‰¤i<jâ‰¤tÎ´iÎ´j(1âˆ’Li)(1âˆ’Lj). (13)
Hence at stopping time t=Tm, (13) still holds. Take expectations at both sides and we will get
C2(Î´Tm)â‰¤K+(K+cg)âˆ†n1
0.5âˆ’mÎ±â€²
mâˆ’1âˆ’âˆ†n1.
Notice that âˆ†n1=Dnâˆ’Î²
2Î²+1
1âˆšlogn1converges to 0asn1â†’ âˆž andÎ±â€²<(1âˆ’1/m)/2by the
condition. It follows that
lim
n1â†’âˆžC1(Î´Tm)â‰¤Î±and lim
n1â†’âˆžC2(Î´Tm)â‰¤K.
E.6 Proof of Theorem C.1
Proof . We first denote some notations. For notational simplicity, we use cto denote constants in the
following proofs.
Recall the conformal score function Q(Xt). The conformal p-value is
bpt=1 +P
iâˆˆLI{Q(ËœWi)â‰¤Q(Wt)}
1 +|L|=bFn2(Zt),
where n2=|L|.
Denote the population p-value as pt=F(Q(Wt)), where F(Â·)is the distribution of Q(Wt)condi-
tional on Î¸t= 0. Denote Ï€t= Pr( Î¸t= 1) and
Ï€Î»
t= 1âˆ’Pr(pt> Î»)
1âˆ’Î».
The corresponding local FDR is defined as
LÎ»
t=Ï€Î»
tf0(Wt)
ft(Wt).
It follows the definition of CÎ»
1(Î´t)andCÎ»
2(Î´t).
CÎ»
1(Î´t) =En1
RtX
iâ‰¤t{LÎ»
iG0(Xi) + (1 âˆ’LÎ»
i)G1(Xi)}Î´io
and
CÎ»
2(Î´t) =E{PP
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j(1âˆ’LÎ»
i)(1âˆ’LÎ»
j)}
E{PP
1â‰¤i<jâ‰¤tg(Xi,Xj)Î´iÎ´j}.
The estimated proportion is defined as
bÏ€Î»
t= 1âˆ’Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)I{bpj> Î»}
(1âˆ’Î»)Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t).
30And denote the ideal estimated proportion via population p-values as
ËœÏ€Î»
t= 1âˆ’Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)I{pj> Î»}
(1âˆ’Î»)Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t),
where Îºb(s) = exp {âˆ’|s|/b}andqis the size of a neighborhood for estimation before time t.
We first claim the following proposition and the proof is deferred in Section E.7.
Proposition E.7. Suppose the assumptions in Theorem 3 hold. Then
|bÏ€Î»âˆ’Ï€Î»| â‰¤cmaxn1
b,âˆšlogn1
n1/2
1,(bÎ·)2o1/2âˆ’Î³
(14)
with probability 1âˆ’maxn
1/b,p
logn1/n1,(bÎ·)2o2Î³
, where c >0is a constant and 0< Î³ < 1/2.
Notice that
sup
w|bft(w)âˆ’ft(w)|
= sup
w|bf0(w)(1âˆ’bÏ€t) +bf1(w)bÏ€tâˆ’f0(w)(1âˆ’Ï€t)âˆ’f1(w)Ï€t|
â‰¤sup
w|bf0(w)âˆ’f0(w)|+ sup
w|bf1(w)âˆ’f1(w)|+ 2M|bÏ€tâˆ’Ï€t|.
The last inequality holds since bÏ€tâˆˆ[0,1]andf0(w)andf1(w)are upper bounded by M.
Thus by Lemma E.1 and take Î³in Proposition E.7 at 1/6, with probability 1âˆ’
cmaxn
1/b,p
logn1/n1,(bÎ·)2o1/3
we have
sup
w|bft(w)âˆ’ft(w)| â‰¤cnâˆ’Î²
2Î²+1
1p
logn1+cmaxn
bâˆ’1
3,logn1
n11
6,(bÎ·)2
3o
.
By the procedure of Lemma E.3 directly, with probability 1âˆ’cmaxn
1/b,p
logn1/n1,(bÎ·)2o1/3
we have
sup
w|bLÎ»
t(w)âˆ’LÎ»
t(w)| â‰¤cnâˆ’Î²
2Î²+1
1p
logn1+cmaxn
bâˆ’1
3,logn1
n11
6,(bÎ·)2
3o
.
It follows that
CÎ»
1(Î´t)â‰¤Î±+cnâˆ’Î²
2Î²+1
1p
logn1+ 2ccGmaxn
bâˆ’1
3,logn1
n11
6,(bÎ·)2
3o
.
Denote âˆ†â€²
n1= 2ccGnâˆ’Î²
2Î²+1
1âˆšlogn1+cmaxn
bâˆ’1
3,
logn1
n11
6,(bÎ·)2
3o
. By our assumption, bÎ·=
o(1). So
lim
b,n1â†’âˆžâˆ†â€²
n1= 0.
By the additional assumption that Pr(pt> Î»|Î¸t= 1) = 0 , we have Ï€Î»
t=Ï€t. Thus Hence the first
part of Theorem 3 is completed.
At last, we can use the same procedure to prove that
CÎ»
2(Î´t)â‰¤K+(K+cg)âˆ†â€²
n1
0.5âˆ’mÎ±â€²
mâˆ’1âˆ’âˆ†â€²n1.
Since Ï€Î»
t=Ï€t,CÎ»
2(Î´t) =C2(Î´t)and the results directly follow.
31E.7 Proof of Proposition E.7
Proof. Notice that
Pr(|bÏ€Î»
tâˆ’Ï€Î»
t|> Îµ)â‰¤Pr
|bÏ€Î»
tâˆ’ËœÏ€Î»
t|>Îµ
2
+ Pr
|ËœÏ€Î»
tâˆ’Ï€Î»
t|>Îµ
2
.
We discuss for the two parts respectively. For the first term of the above inequality, it suffices to show
the upper bound of the second moment of |bÏ€Î»
tâˆ’ËœÏ€Î»
t|.
For convenience, denote Uj=I(pj> Î»), and Vj=I(bpj> Î»). It follows
E{|bÏ€Î»
tâˆ’ËœÏ€Î»
t|2}
=1
{Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)}2(1âˆ’Î»)2Eï£®
ï£°tâˆ’1X
j=tâˆ’qÎºb(jâˆ’t)(Ujâˆ’Vj)ï£¹
ï£»2
=Ptâˆ’1
j=tâˆ’qÎº2
b(jâˆ’t)E{(Ujâˆ’Vj)2}
{Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)}2(1âˆ’Î»)2+P
i,jâˆˆNq(t),qÌ¸=jÎºb(iâˆ’t)Îºb(jâˆ’t)E{(Uiâˆ’Vi)(Ujâˆ’Vj)}
{Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)}2(1âˆ’Î»)2
=1
{Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)}2(1âˆ’Î»)2htâˆ’1X
j=tâˆ’qÎº2
b(jâˆ’t)n
Pr(pj> Î») + Pr( bpj> Î»)âˆ’2 Pr(pj> Î»,bpj> Î»)o
+2X
tâˆ’qâ‰¤i<jâ‰¤tâˆ’1Îºb(iâˆ’t)Îºb(jâˆ’t)n
Pr(pi> Î», p j> Î»)âˆ’2 Pr(bpi> Î», p j> Î») + Pr( bpi> Î»,bpj> Î»)oi
.
Now we check the upper bound of |pjâˆ’bpj|. We can rewrite them as pj=F(Q(Wj))and
bpj=bFn2(Q(Wj)). Even though n2is a random variable, due to the two-group model, bFn2(Â·)is still
an empirical distribution function composed by i.i.d. samples conditional on {ËœÎ¸i}iâˆˆCwhere Cis the
index set of calibration set. Thus by DKW inequality, for any Îµ1>0
Pr (|pjâˆ’bpj| â‰¤Îµ1) =Eh
Pr
|pjâˆ’bpj| â‰¤Îµ1| {ËœÎ¸i}iâˆˆCi
â‰¤Eh
Pr
sup
zF(z)âˆ’bFn2(z)â‰¤Îµ1| {ËœÎ¸i}iâˆˆCi
â‰¤Eh
1âˆ’2 exp{âˆ’n2Îµ2
1}i
â‰¤1âˆ’2 exp{âˆ’(1âˆ’Ï€)n1Îµ2
1} âˆ’4Ï€/{n1(1âˆ’Ï€)}.
The last inequality holds since n2â‰¥(1âˆ’Ï€)n1/2with probability 1âˆ’4Ï€/{n1(1âˆ’Ï€)}.
Thus by Lemma E.5 and the fact that pjhas a bounded density function, we obtain
|Pr(pj> Î»)âˆ’Pr(bpj> Î»)| â‰¤2Îµ1+ 6 exp {âˆ’(1âˆ’Ï€)n1Îµ2
1}+ 12Ï€/{n1(1âˆ’Ï€)}.
TakeÎµ1=p
logn1/{n1(1âˆ’Ï€)}, for sufficient large n1such thatâˆšn1logn1â‰¥6 + 6 Ï€we have
|Pr(pj> Î»)âˆ’Pr(bpj> Î»)| â‰¤2âˆšlogn1
(1âˆ’Ï€)n1/2+6 + 12 Ï€/(1âˆ’Ï€)
n1â‰¤3âˆšlogn1
(1âˆ’Ï€)n1/2.
Again by Lemma E.5 and the same Îµ1, for any i, jâˆˆ {tâˆ’q, . . . , t âˆ’1}:=Nq(t), we have
|Pr(pj> Î», p i> Î»)âˆ’Pr(bpj> Î»,bpi> Î»)| â‰¤4Îµ1+12 exp {âˆ’(1âˆ’Ï€)n1Îµ2
1}+24Ï€
n1(1âˆ’Ï€)â‰¤6âˆšlogn1
(1âˆ’Ï€)n1/2
1.
Therefore by Lemma E.6, we have
E(|bÏ€Î»
tâˆ’ËœÏ€Î»
t|)â‰¤9Cbâˆšlogn1
bn1/2
1(1âˆ’Ï€)(1âˆ’Î»)2+12âˆšlogn1
n1/2
1(1âˆ’Ï€)(1âˆ’Î»)2.
32So by definition and Markovâ€™ inequality, we have
Pr
|bÏ€Î»
tâˆ’ËœÏ€Î»
t|>Îµ
2
â‰¤4E(|bÏ€Î»
tâˆ’ËœÏ€Î»
t|)
Îµ2
â‰¤36Cbâˆšlogn1
bn1/2
1(1âˆ’Ï€)(1âˆ’Î»)2Îµ2+48âˆšlogn1
n1/2
1(1âˆ’Ï€)(1âˆ’Î»)2Îµ2
For the second part, we have
Pr
|ËœÏ€Î»
tâˆ’Ï€Î»
t|>Îµ
2
= Pr Pr(pt> Î»)
1âˆ’Î»âˆ’Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)I{pj> Î»}
{Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)}2(1âˆ’Î»)>Îµ
2!
â‰¤4EhPtâˆ’1
j=tâˆ’qÎºb(jâˆ’t){Pr(pt> Î»)âˆ’I(pj> Î»)}i2
{Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)}2Îµ2(1âˆ’Î»)2
=4Ptâˆ’1
j=tâˆ’qÎº2
b(jâˆ’t)
Var{I(pj> Î»)}+{Pr(pj> Î»)âˆ’Pr(pt> Î»)}2
{Ptâˆ’1
j=tâˆ’qÎºb(jâˆ’t)}2Îµ2(1âˆ’Î»)2
(i)
â‰¤Cb
bÎµ2(1âˆ’Î»)2+4Ptâˆ’1
j=tâˆ’qÎº2
b(jâˆ’t){Pr(pj> Î»)âˆ’Pr(pt> Î»)}2
{Ptâˆ’1
j=tâˆ’qÎº2
b(jâˆ’t)}2Îµ2(1âˆ’Î»)2
(ii)
â‰¤Cb
bÎµ2(1âˆ’Î»)2+16Î·2Ptâˆ’1
j=tâˆ’q(jâˆ’t)2Îº2
b(jâˆ’t)
{Ptâˆ’1
j=tâˆ’qÎº2
b(jâˆ’t)}2Îµ2(1âˆ’Î»)2
(iii)
â‰¤Cb
bÎµ2(1âˆ’Î»)2+16(CbbÎ·)2
{Ptâˆ’1
j=tâˆ’qÎº2
b(jâˆ’t)}2Îµ2(1âˆ’Î»)2(15)
The(i)holds since Var{I(pj> Î»)} â‰¤1/4. As for ( ii), consider AÎ»={w:F(Q(w))> Î»}. Then
by the two-group model we have Pr(pt> Î») =R
AÎ»{f0(w)(1âˆ’Ï€t) +f1(w)Ï€t}dw.Hence
|Pr(pj> Î»)âˆ’Pr(pt> Î»)|=Z
AÎ»n
f0(w)|Ï€jâˆ’Ï€t|+f1(w)|Ï€tâˆ’Ï€j|o
dwâ‰¤2|jâˆ’t|Î·
by our assumptions. And ( iii) is directly from the second part of Lemma E.6.
Above all, we finally conclude that for all Îµ >0and sufficiently large n1, we have
Pr(|bÏ€Î»
tâˆ’Ï€Î»
t|> Îµ)â‰¤Cb
bÎµ2(1âˆ’Î»)2+36Cbâˆšlogn1
bn1/2
1(1âˆ’Ï€)(1âˆ’Î»)2Îµ2+48âˆšlogn1
n1/2
1(1âˆ’Ï€)(1âˆ’Î»)2Îµ2+16(CbbÎ·)2
Îµ2(1âˆ’Î»)2
â‰¤maxn1
b,âˆšlogn1
n1/2
1,(bÎ·)2oc
Îµ2
for some constant c >0.
Thus take Îµ= maxn
1/b,âˆšlogn1/n1/2
1,(bÎ·)2o1/2âˆ’Î³âˆšcand we obtain
|bÏ€Î»âˆ’Ï€Î»| â‰¤cmaxn1
b,âˆšlogn1
n1/2
1,(bÎ·)2o1/2âˆ’Î³
with probability 1âˆ’maxn
1/b,p
logn1/n1,(bÎ·)2o2Î³
, where 0< Î³ < 1/2. Hence we complete
the proof.
33NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: See the end of Sect. 1, we summarize our main contributions. The experimental
results in Sect. 4 and Appendix D validate the theoretical results in Sect. 3 and Appendix C.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In Sect. 5, we discuss the limitations of our II-COS algorithm. In Sect. 3, we
have explained why our assumptions are weak piece by piece. And see Subsect. 2.2 for the
details about our computational complexity.
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
34Answer: [Yes]
Justification: For the theory assumptions, see Assumption 3.1-Assumption 3.2 in Sect. 3.
For the proofs of each theoretical result, see Appendix E in the Supplementary material.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Appendix B for the implementation details to reproduce our experimental
results.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
35Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: We provide the data and code in the supplemental materials, including sufficient
instructions in the zip file.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: In Sect.4 and Appendix B, we provide the experimental setting/details.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: We calculate the standard error across 500 replications and show the error bars
in experimental results, which is calculated by mean +/âˆ’se.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
36â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: See Appendix B.4 for the compute resources and more details about the
execution time.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: All the authors have reviewed the NeurIPS Code of Ethics guidelines and
ensured that our paper conforms to them.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We have discussed the broader impacts of our work in Section 5.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
37â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: We have cited and provided the URL of the dataset we use in the paper, see
Sect. 4 and the references.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
38â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
39