Carrot and Stick:
Eliciting Comparison Data and Beyond
Yiling Chen
Harvard University
yiling@seas.harvard.eduShi Feng
Harvard University
shifeng@fas.harvard.eduFang-Yi Yu
George Mason University
fangyiyu@gmu.edu
Abstract
Comparison data elicited from people are fundamental to many machine learning
tasks, including reinforcement learning from human feedback for large language
models and estimating ranking models. They are typically subjective and not
directly verifiable. How to truthfully elicit such comparison data from rational
individuals? We design peer prediction mechanisms for eliciting comparison data
using a bonus-penalty payment [ 11]. Our design leverages on the strong stochastic
transitivity for comparison data [ 60,13] to create symmetrically strongly truthful
mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and
2) yields the highest payment among all symmetric equilibria. Each individual only
needs to evaluate one pair of items and report her comparison in our mechanism.
We further extend the bonus-penalty payment concept to eliciting networked data,
designing a symmetrically strongly truthful mechanism when agentsâ€™ private signals
are sampled according to the Ising models. We provide the necessary and sufficient
conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian
Nash equilibrium. Experiments on two real-world datasets further support our
theoretical discoveries.
1 Introduction
In the past two decades, researchers have been embracing the challenge of eliciting private information
from individuals when there is no ground truth available to evaluate the quality of elicited contribu-
tions, and have made amazing progress. Many mechanisms, collectively called peer prediction [42],
have been developed to incentivize individuals to strictly truthfully report their information at a
Bayesian Nash equilibrium (BNE), by artful design of payment functions that only depend on reports
from individuals. Moreover, in multi-task peer prediction mechanisms, the truthful BNE gives each
individual the highest expected payoff among all BNEs (i.e. itâ€™s a strongly truthful BNE). [ 11,57,30]
However, all prior multi-task peer prediction mechanisms require tasks being ex-ante identical, and
hence individualsâ€™ private information is independently and identically distributed (iid) for each task.
Multi-task peer prediction leverages this structure of information to succeed at truthful elicitation.
But what if such structure of information doesnâ€™t hold for an information elicitation problem?
One notable application is to elicit pair-wise comparisons of multiple alternatives, such as preferences
for consumer products [ 53], translation [ 34], peer grading [ 55], and relevance of language model
outputs [ 9,10]. Such pair-wise comparison data are crucial for estimating a ranking of the alternatives
and for devising reward functions for reinforcement learning. Comparison tasks for different pairs
are clearly not ex-ante identical â€” answers to the tasks demonstrate a certain degree of transitivity
(e.g. ifğ‘is preferred to ğ‘â€²andğ‘â€²is preferred to ğ‘â€²â€², then itâ€™s more likely that ğ‘is preferred to ğ‘â€²â€²),
rendering existing peer prediction mechanisms not applicable.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).In this paper, we design a peer prediction mechanism for eliciting comparison data. We model
individualsâ€™ private information of pair-wise comparisons as Bayesian strongly stochastically tran-
sitive (Bayesian SST), which takes many widely used models (e.g. Thurstone [ 58], Bradley-Terry-
Luce [ 4,38], and Mallows [ 39]) as special cases. Our mechanism uses a simple bonus-penalty
payment [ 11] (hence carrot and stick) that takes three reports as inputs and admits a strongly truthful
symmetric BNE. The key insight that we develop is a condition of information structure that we call
uniform dominance . When uniform dominance is satisfied, the bonus-penalty payment is the only
type of payment that induces a strictly truthful BNE. Information of individuals, ğ‘–,ğ‘—, andğ‘˜, satisfies
uniform dominance if, conditioned on any realization of agent ğ‘–â€™s information, the probability for
ğ‘—â€™s information to agree with ğ‘–â€™s is higher than the probability for ğ‘˜â€™s information to agree with ğ‘–â€™s.
Bayesian SST allows us to group three pairwise comparisons, (ğ‘,ğ‘â€²),(ğ‘â€²â€²,ğ‘â€²)and(ğ‘â€²â€²,ğ‘), together
such that private information about these pairs satisfies uniform dominance. After identifying uniform
dominance as a central structure for incentivizing truthful elicitation, we further generalize the
bonus-penalty payment to truthfully elicit private information over social networks that demonstrate
homophily (i.e. friends tend to have similar opinions than non-friends) [ 40], and our mechanism can
be integrated with common survey techniques such as snowball sampling [24].
Our contributions. Our work is a leap forward for designing mechanisms for complex information
elicitation settings where ground truth verification is not available.
â€¢We are the first to design mechanisms to truthfully elicit pairwise comparison data under Bayesian
SST and networked data under Ising models. In our mechanisms, truthful reporting forms a BNE
and yields a strictly higher payoff than any symmetric non-permutation equilibrium.
â€¢We identify a key structure of information, uniform dominance, as a lever such that the simple
bonus-penalty payment is the unique payment inducing a strictly truthful BNE. This identification
may offer a path for developing truthful elicitation mechanisms for other settings in the future.
â€¢We use Griffithsâ€™ inequality and Weitzâ€™s self-avoiding walk [ 65] to prove the uniform dominance
property in the Ising model. The resulting correlation bounds may be of independent interest.
â€¢We test our mechanisms on real-world data (sushi preference dataset [ 26,27] and Last.fm
dataset [ 8]). Even though these datasets do not perfectly satisfy our theoretical assumptions, our
mechanisms still provide a stronger incentive for truthful reporting compared to misreporting.
Related work. Information elicitation has two settings according to whether verification is possible.
Our paper focuses on elicitation without verification.
For information elicitation without verification, Miller et al. [41] introduce the first mechanism
for single task signal elicitation that has truth-telling as a strict Bayesian Nash equilibrium but
requires full knowledge of the common prior. Bayesian truth serum (BTS) [ 47] is the first strongly
truthful peer prediction mechanism, but requires complicated reports from agents (their private signal
and predictions on otherâ€™s reports). A series of works [ 48,49,67,66,68,3,51,31] relax certain
assumptions of BTS but still require complicated reports from agents. Dasgupta and Ghosh [11]
introduces the multi-task setting where agents are assigned batch iid tasks and only report their
signals. Several works extend this to multiple-choice questions [29, 33, 56, 11], predictions [37], or
even continuous value [ 50], and investigate the limitation and robustness [ 52,6,70,17,70]. Another
related line of work is co-training and federated learning, which wants to elicit models [ 32,36], or
samples [ 64] when multiple iid data or feature of data are available. For more related works, see
Faltings [16].
One popular line of work considers information elicitation when verification is possible. Spot-
checking requires direct verification of the agentâ€™s report [ 21]. Recent work on comparison data
elicitation [ 19] utilizes spot-checking concepts and focuses on incentivizing effort. Another form
of verification involves using additional samples to evaluate how the agentâ€™s reports improve model
performance [ 1,28]. Additionally, the verification may have a general relation to the agentâ€™s signal,
e.g., proper scoring rules [23, 46, 35, 20].
2 Problem Formulation
We discuss our model for eliciting comparison data in this section and defer the extensions to
Section 5. Given a collection of items Aand a set of strategic agents N. Agents privately observe
2noisy comparisons between pairs of items. Our goal is to design mechanisms to truthfully elicit agentsâ€™
private information. We will first introduce the information structure of agentsâ€™ private information of
pairwise comparisons in Section 2.1 and then define the information elicitation problem in Section 2.2.
2.1 Bayesian SST Models for Comparison Data
We introduce Bayesian Strong Stochastic Transitivity (Bayesian SST) models to capture the structure
of agentsâ€™ private information for comparison data.
Given the set of items A, the underlying unknown state about the items is ğœƒâˆˆÎ˜.ğœƒcan be the
vector of quality scores for the items (Example 2.2) or a reference ranking (Example 2.4). ğœƒis drawn
according to a common prior ğ‘ƒÎ˜:ğœƒâˆ¼ğ‘ƒÎ˜. Any realized ğœƒhas an associated stochastic comparison
functionğ‘‡ğœƒ:A2â†’{âˆ’ 1,1}. For comparisons of two items ğ‘andğ‘â€²,ğ‘‡ğœƒ(ğ‘,ğ‘â€²)andğ‘‡ğœƒ(ğ‘â€²,ğ‘)
stochastically take value 1orâˆ’1, with Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1]=1âˆ’Pr[ğ‘‡ğœƒ(ğ‘â€²,ğ‘)=âˆ’1]. For anyğœƒ,ğ‘‡ğœƒis
strongly stochastically transitive as defined below.
Definition 2.1 ([60,13]).A stochastic comparison function, ğ‘‡:A2â†’ {âˆ’ 1,1}, isstrongly
stochastically transitive (SST) if for allğ‘,ğ‘â€²,ğ‘â€²â€²âˆˆ A with Pr[ğ‘‡(ğ‘,ğ‘â€²)=1]>1/2and
Pr[ğ‘‡(ğ‘â€²,ğ‘â€²â€²)=1]>1/2, we have
Pr[ğ‘‡(ğ‘,ğ‘â€²â€²)=1]>max{Pr[ğ‘‡(ğ‘,ğ‘â€²)=1],Pr[ğ‘‡(ğ‘â€²,ğ‘â€²â€²)=1]}.
Intuitively, a comparison function is SST when for any three items ğ‘,ğ‘â€²,ğ‘â€²â€², ifğ‘is more favorable
thanğ‘â€²andğ‘â€²is more favorable than ğ‘â€²â€², thenğ‘is even more favorable than ğ‘â€²â€². The concept of SST
is a well-established property of comparisons in social science and psychology [18].
Each agentğ‘–âˆˆ N has the knowledge of (ğ‘‡ğœƒ)ğœƒâˆˆÎ˜andğ‘ƒÎ˜. When asked to compare a pair of
items(ğ‘,ğ‘â€²), the agent observes an independent draw according to the stochastic comparison
function:ğ‘†ğ‘–=ğ‘‡ğœƒ(ğ‘,ğ‘â€²), where realization ğ‘ ğ‘–=1represent item ğ‘is preferred over item ğ‘â€²
by agentğ‘–. We assume items are a priori similar but ex-post distinct so that for all ğ‘,ğ‘â€²âˆˆA ,
E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)]=E[E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]]=0andE[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]â‰ 0for allğœƒ.
Examples of Bayesian SST models. Bayesian SST models are a general family of models that take
many classical parametric ranking models, including Bradley-Terry Luce [ 4,38], Thurstone (Case
V) [58], and Mallows ğœ‚-model [39], as special cases.
Example 2.2 (Bradley-Terry-Luce, Thurstone model, and more [ 59]).LetğœƒâˆˆRA=Î˜where each
coordinate is independently and identically sampled from a fixed non-atomic distribution ğœˆonR, and
each itemğ‘have a scalar quality ğœƒğ‘âˆˆR. Letğ¹:Râ†’[0,1]be any strictly increasing function such
thatğ¹(ğ‘¡)=1âˆ’ğ¹(âˆ’ğ‘¡)for allğ‘¡âˆˆR. Conditional on a fixed ğœƒ,
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1]=ğ¹(ğœƒğ‘âˆ’ğœƒğ‘â€²)for allğ‘,ğ‘â€²âˆˆA.
This model recovers the Thurstone model [ 58] by settingğ¹(ğ‘¡)=Î¦(ğ‘¡)whereÎ¦is the Gaussian CDF,
and the Bradley-Terry-Luce model [ 4] by settingğ¹(ğ‘¡)=ğ‘’ğ‘¡
1+ğ‘’ğ‘¡, the sigmoid function. Moreover, this
model also contains any additive random utility model [ 2] whereğ‘‡(ğ‘,ğ‘â€²)=1ifğœƒğ‘+ğ‘ >ğœƒğ‘â€²+ğ‘â€²
with iid noise ğ‘andğ‘â€², because we can set ğ¹to be the CDF of the difference of two iid noise.
Proposition 2.3. For any strictly increasing ğ¹and non-atomic ğœˆonR, the parametric model in
Example 2.2 is a Bayesian SST model.
Example 2.4 (Mallowsğœ‚-model [ 39]).LetÎ˜be the set of rankings on Aandğœ‚>0be a dispersion
parameter. Given a reference ranking ğœƒâˆˆÎ˜, theMallowsğœ‚-model generate a ranking ğœ™âˆˆÎ˜with prob-
ability Pr(ğœ™)âˆexp(âˆ’ğœ‚ğ‘‘(ğœƒ,ğœ™))whereğ‘‘(ğœƒ,ğœ™)={(ğ‘,ğ‘â€²)âˆˆA2:ğœƒ(ğ‘)<ğœƒ(ğ‘â€²)andğœ™(ğ‘)>ğœ™(ğ‘â€²)}
is Kendallâ€™s tau distance, and ğœƒ(ğ‘)is the rank of item ğ‘. Therefore, to generate comparisons, we first
sample a uniform ğœƒand
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1]=âˆ‘ï¸
ğœ™:ğœ™(ğ‘)>ğœ™(ğ‘â€²)Pr(ğœ™),for allğ‘,ğ‘â€²âˆˆA.
Proposition 2.5. For anyğœ‚ > 0, Mallowsğœ‚-model in Example 2.4 with uniform distribution on
reference ranking is an Bayesian SST model.
The proofs for propositions 2.3 and 2.5 are closely related to strong stochastic transitivity [ 54,7], but
are provided in the appendix for completeness.
32.2 Peer Prediction Mechanism Design
To truthfully elicit comparison data from agents, a peer prediction mechanism creates a game between
the agents outlined below: First, we choose an assignmentE={ğ‘’ğ‘–=(ğ‘ğ‘¢ğ‘–,ğ‘ğ‘£ğ‘–):ğ‘–âˆˆN} where
agentğ‘–âˆˆN gets a pair of items ğ‘’ğ‘–=(ğ‘ğ‘¢ğ‘–,ğ‘ğ‘£ğ‘–)âˆˆA2to compare. Then each agent ğ‘–âˆˆN privately
observes the realization of the comparison (signal) ğ‘ ğ‘–âˆˆ{âˆ’1,1}, which is an independent realization
ofğ‘‡ğœƒ(ğ‘ğ‘¢ğ‘–,ğ‘ğ‘£ğ‘–), and reports Ë†ğ‘ ğ‘–âˆˆ{âˆ’1,1}potentially different from her signal. We use ğ‘†ğ‘–=ğ‘†(ğ‘ğ‘¢ğ‘–,ğ‘ğ‘£ğ‘–)
to denote the random variable of agent ğ‘–â€™s signal, where the randomness of ğ‘†(Â·,Â·)comes from both ğœƒ
andğ‘‡ğœƒ. Let Srepresent the random vector of all agentsâ€™ signals, s=(ğ‘ ğ‘–)ğ‘–âˆˆNbe all agentsâ€™ realized
private signals and Ë†s=(Ë†ğ‘ ğ‘–)ğ‘–âˆˆNbe all agentsâ€™ reports. Finally, a peer prediction mechanism (ğ‘€ğ‘–)ğ‘–âˆˆN
takes all agentsâ€™ reports Ë†sand pays agent ğ‘–withğ‘€ğ‘–(Ë†s)âˆˆR.
Each agentğ‘–â€™s strategy is a random function from her signal to a report ğœğ‘–:ğ‘ ğ‘–â†¦â†’Ë†ğ‘ ğ‘–, and the
randomness of their strategies is independent of each otherâ€™s and all signals. With slight abuse of
notation, we write ğœğ‘–(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)=Pr[Ë†ğ‘†ğ‘–=Ë†ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]as the conditional probability of reporting Ë†ğ‘ ğ‘–
given private signal ğ‘ ğ‘–. A strategy profile ğˆis a collection of all agentâ€™s strategies. All agents are
rational and risk-neutral, so they want to maximize their expected payments. Thus, given prior ğ‘ƒÎ˜,
randomness of ğ‘‡ğœƒand a strategy profile ğˆ, agentğ‘–wants to maximize her ex-ante payment denoted
asEğˆ,ğœƒ,ğ‘‡ğœƒ[ğ‘€ğ‘–(Ë†S)]where Ë†Sis the random vector of all agentsâ€™ report that depends on the signals S
and strategy profile ğˆ.
We introduce three families of strategies, truth-telling, permutation, and uninformed strategy profiles,
which are central to understanding effective peer prediction mechanisms.
â€¢A strategyğœğ‘–istruthful (or truth-telling) if it is a deterministic identity map, ğœğ‘–(ğ‘ ğ‘–)=ğ‘ ğ‘–. A
strategy profile is truthful if all agentsâ€™ strategies are truthful.
â€¢Apermutation strategy profile is where agents simultaneously relabel their signals and then report
the relabeled ones. A permutation strategy is indistinguishable from truth-telling unless the peer
prediction mechanism has additional knowledge about the prior signal distribution.[33]
â€¢Finally, a strategy is uninformed if it has the same report distribution across all signals, and it is
informed otherwise. Common examples include consistently reporting all signals as a constant
value, such as 1orâˆ’1, or using a random report regardless of the signal. Uninformed strategies
are undesirable as the reports bear no relationship to the private signals.
A strategy profile is symmetric if all agents use the same strategy. For example, both truth-telling and
permutation strategy profiles are symmetric.
We now introduce goals for a peer prediction mechanism that favors truth-telling more than other
strategies. First, we want the truth-telling (strategy profile) to be a strict Bayesian Nash equilibrium
(BNE) so that any agentâ€™s payment would strictly decrease if she unilaterally changes to any non-
truthful strategy. Moreover, there may be multiple equilibria, and a desirable mechanism should
ensure that truth-telling is better than all other equilibria. In this paper, we aim for symmetrically
strongly truthful mechanisms defined below.
Definition 2.6. A peer prediction mechanism is symmetrically strongly truthful if truth-telling is a
BNE, and each agentâ€™s expected payment in truth-telling is no less than the payment in any other
symmetric equilibrium with equality for the equilibrium with a permutation strategy profile.1
3 Bonus-penalty Payment Mechanism for Comparison Data
We now propose a bonus-penalty payment mechanism for eliciting comparison data. The mechanism
makes use of a bonus-penalty payment function, which can be seen as an agreement payment
and introduced by Dasgupta and Ghosh [11] in a different context (see discussion in appendix A).
Formally, for any Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜âˆˆ{âˆ’1,1}, the bonus-penalty payment function is
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜)=Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘—âˆ’Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘˜=2 1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘—]âˆ’1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘˜], (1)
which rewards when the first input agrees with the second but punishes when it agrees with the third.
1Kong and Schoenebeck [30] shows that it is impossible to pay the truth-telling strategy profile strictly better
than other permutation strategy profiles without additional knowledge of the prior signal distribution.
4Mechanism 1 uses the bonus-penalty payment eq. (1) for each agent ğ‘–by carefully choosing agent ğ‘—
andğ‘˜such that agent ğ‘—â€™s signal is more likely to agree with agent ğ‘–â€™s than agent ğ‘˜â€™s signal is. The
crux of finding such pair of agents is to show that if agent ğ‘–prefers item ğ‘overğ‘â€², she would expect
that others will prefer any third item ğ‘â€²â€²overğ‘â€², and prefer ğ‘overğ‘â€²â€². Thus, if agent ğ‘—has pair
(ğ‘â€²â€²,ğ‘â€²)and agentğ‘˜has pair(ğ‘â€²â€²,ğ‘), then agent ğ‘—â€™s signal is more likely to take the same value
asğ‘–â€™s than agent ğ‘˜â€™s signal is. This is the main idea behind the proof of Theorem 3.1, where we
establish the symmetrically strongly truthfulness of Mechanism 1. To ensure the existence of such
pairs are assigned, we require the assignment Eto be admissible where for all(ğ‘,ğ‘â€²)âˆˆE , there
existsğ‘â€²â€²âˆˆA so that(ğ‘â€²â€²,ğ‘â€²)and(ğ‘â€²â€²,ğ‘)âˆˆE .
Mechanism 1: BPP mechanism for comparison data
Input: LetAbe a collection of items, Ebe an admissible assignment, and Ë†sbe agentsâ€™ reports.
foragentğ‘–âˆˆN with pairğ‘’ğ‘–=(ğ‘ğ‘¢ğ‘–,ğ‘ğ‘£ğ‘–)=(ğ‘,ğ‘â€²)do
Findğ‘â€²â€²âˆˆA and two agents ğ‘—andğ‘˜so thatğ‘’ğ‘—=(ğ‘â€²â€²,ğ‘â€²)andğ‘’ğ‘˜=(ğ‘â€²â€²,ğ‘), and pay agent ğ‘–
ğ‘€ğ‘–(Ë†s)=ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜)=Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘—âˆ’Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘˜. (2)
Theorem 3.1. Given a collection of items Aand a set of agents Nwith|A|,|N|â‰¥ 3, for any
admissible assignment matrix Eand Bayesian SST model with (ğ‘‡ğœƒ)ğœƒâˆˆÎ˜andğ‘ƒÎ˜, the BPP mechanism
for comparison (Mechanism 1) is symmetrically strongly truthful.
We defer the proof of theorem 3.1 to section 4. The admissible condition imposes little overhead on
downstream learning problems, including rank recovery [ 25] and identification of the top ğ‘˜items [ 15].
Specifically, the size of assignment Eis the number of comparisons and corresponds to the sample
complexity for these learning problems. If a learning algorithm requires a set of pairs to compare
Eğ‘€ğ¿, we can construct an admissible superset Ethat introduces a constant factor overhead and can
recoverEğ‘€ğ¿âŠ†E.2
We remark that the bonus-penalty payment function eq. (2) can be seen as a boolean function for
transitivity [45]; see remark 3.2 for a formal statement. Hence, theorem 3.1 implies that agentsâ€™
manipulations can only decrease the probability of transitivity among their reports.
Remark 3.2. Note that a deterministic comparison function ğ‘¡:AÃ—Aâ†’{âˆ’ 1,1}satisfies transitivity
on three items ğ‘,ğ‘â€²,ğ‘â€²â€²âˆˆ A if and only if ğ‘¡(ğ‘,ğ‘â€²),ğ‘¡(ğ‘â€²,ğ‘â€²â€²),ğ‘¡(ğ‘â€²â€²,ğ‘)are not all equal, that is
ğ‘ğ´ğ¸(ğ‘¡(ğ‘,ğ‘â€²),ğ‘¡(ğ‘â€²,ğ‘â€²â€²),ğ‘¡(ğ‘â€²â€²,ğ‘))=1where
ğ‘ğ´ğ¸(ğ‘¤1,ğ‘¤2,ğ‘¤3)=3
4âˆ’1
4ğ‘¤1ğ‘¤2âˆ’1
4ğ‘¤1ğ‘¤3âˆ’1
4ğ‘¤2ğ‘¤3.
The agentâ€™s random noisy comparisons may or may not satisfy transitivity. The probability of
transitivity is the probability that they do.
We can show that the bonus-penalty payment in eq. (2) is equivalent to the above transitivity test
when agents are truth-telling. Formally,
ğ‘ğ´ğ¸(ğ‘†(ğ‘,ğ‘â€²),ğ‘†(ğ‘â€²,ğ‘â€²â€²),ğ‘†(ğ‘â€²â€²,ğ‘))
=3
4âˆ’1
4(ğ‘†(ğ‘,ğ‘â€²)ğ‘†(ğ‘â€²,ğ‘â€²â€²)+ğ‘†(ğ‘,ğ‘â€²)ğ‘†(ğ‘â€²â€²,ğ‘)+ğ‘†(ğ‘â€²,ğ‘â€²â€²)ğ‘†(ğ‘â€²â€²,ğ‘))
=1
4(ğ‘†(ğ‘,ğ‘â€²)ğ‘†(ğ‘â€²â€²,ğ‘â€²)âˆ’ğ‘†(ğ‘,ğ‘â€²)ğ‘†(ğ‘â€²â€²,ğ‘))+3
4+1
4ğ‘†(ğ‘â€²â€²,ğ‘â€²)ğ‘†(ğ‘â€²â€²,ğ‘)(ğ‘†(ğ‘â€²,ğ‘â€²â€²)=âˆ’ğ‘†(ğ‘â€²â€²,ğ‘â€²))
=1
4 ğ‘ ğ‘–ğ‘ ğ‘—âˆ’ğ‘ ğ‘–ğ‘ ğ‘˜+3
4+1
4ğ‘ ğ‘—ğ‘ ğ‘˜=1
4ğ‘€ğ‘–(s)+3
4+1
4ğ‘ ğ‘—ğ‘ ğ‘˜ (truth-telling)
Therefore, arg maxË†ğ‘ ğ‘–E[ğ‘ğ´ğ¸(Ë†ğ‘ ğ‘–,âˆ’ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–]=arg maxË†ğ‘ ğ‘–E1
4ğ‘€ğ‘–(Ë†ğ‘ ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)+3
4+1
4ğ‘†ğ‘—ğ‘†ğ‘˜|ğ‘†ğ‘–=ğ‘ ğ‘–
=
arg maxË†ğ‘ ğ‘–E[ğ‘€ğ‘–(Ë†ğ‘ ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–].
2Specifically, given any assignment E0, we can construct a superset Eso that for any(ğ‘,ğ‘â€²)âˆˆE0find an
arbitraryğ‘â€²â€²â‰ ğ‘,ğ‘â€²and add(ğ‘,ğ‘â€²),(ğ‘â€²,ğ‘),(ğ‘,ğ‘â€²â€²),(ğ‘â€²â€²,ğ‘),(ğ‘â€²,ğ‘â€²â€²),(ğ‘â€²â€²,ğ‘â€²)intoE. Thus,Eis admissible
and at most six times larger than E0.
54 Proof of Theorem 3.1: from Bayesian SST Model to Uniform Dominance
To prove theorem 3.1, we formalize the idea that agent ğ‘—â€™s signal is more likely to agree with agent ğ‘–â€™s
than agentğ‘˜â€™s is as what we call uniform dominance in definition 4.1. Weâ€™ll show that any Bayesian
SST model satisfies this property. Then, weâ€™ll prove that BPP mechanism is symmetrically strongly
truthful when agentsâ€™ private signals satisfy uniform dominance.
Definition 4.1. Given a random vector (ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)âˆˆ{âˆ’ 1,1}3with joint distribution ğ‘ƒ,ğ‘†ğ‘—uniformly
dominatesğ‘†ğ‘˜forğ‘†ğ‘–ifPr[ğ‘†ğ‘—=1|ğ‘†ğ‘–=1]>Pr[ğ‘†ğ‘˜=1|ğ‘†ğ‘–=1]andPr[ğ‘†ğ‘—=âˆ’1|ğ‘†ğ‘–=âˆ’1]>
Pr[ğ‘†ğ‘˜=âˆ’1|ğ‘†ğ‘–=âˆ’1]. We call such an ordered tuple âŸ¨ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜âŸ©a uniformly dominant tuple.3
Lemma 4.2 shows how to identify uniformly dominant tuples under Bayesian SST models.
Lemma 4.2. Under any Bayesian SST model, for any agent ğ‘–and itemsğ‘,ğ‘â€²andğ‘â€²â€², agentğ‘—â€™s signal
ğ‘†ğ‘—=ğ‘†(ğ‘â€²â€²,ğ‘â€²)uniformly dominates agent ğ‘˜â€™s signalğ‘†ğ‘˜=ğ‘†(ğ‘â€²â€²,ğ‘)for signalğ‘†ğ‘–=ğ‘†(ğ‘,ğ‘â€²).
In other words, under any Bayesian SST model, the distribution of ğ‘†(ğ‘,ğ‘â€²),ğ‘†(ğ‘â€²â€²,ğ‘â€²),ğ‘†(ğ‘â€²â€²,ğ‘)
satisfies uniform dominance for any ğ‘,ğ‘â€²,ğ‘â€²â€². In the rest of this section, we can view (ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)as
an abstract random vector with some joint distribution ğ‘ƒ.
We now establish some implications of uniform dominance on the bonus-penalty payment. Lemma
4.3 shows that truth-telling is the best response if other signals are reported truthfully. Lemma 4.4
states that the expected payment is zero if everyone uses uninformed strategies (random functions
independent of input). Lemma 4.5 characterizes the best response under symmetric strategy profiles
(the same random function on each coordinate).
Lemma 4.3 (Truthfulness) .Given a uniformly dominant tuple âŸ¨ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜âŸ©with distribution ğ‘ƒ, for all
ğ‘ ğ‘–âˆˆ{âˆ’ 1,1},ğ‘ ğ‘–=arg maxË†ğ‘ ğ‘–âˆˆ{âˆ’1,1}Eğ‘ƒ
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–andEğ‘ƒ
ğ‘ˆğµğ‘ƒğ‘ƒ(ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)
>
0.
Lemma 4.4. Given a uniformly dominant tuple âŸ¨ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜âŸ©, with joint distribution ğ‘ƒif agentğ‘—and
ğ‘˜both use an uninformed strategy ğœso that Ë†ğ‘†ğ‘—=ğœ(ğ‘†ğ‘—)andË†ğ‘†ğ‘˜=ğœ(ğ‘†ğ‘˜), for allğ‘ ğ‘–andË†ğ‘ ğ‘–in{âˆ’1,1},
Eğœ,ğ‘ƒ
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=0.
Lemma 4.5. Given a uniformly dominant tuple âŸ¨ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜âŸ©with distribution ğ‘ƒ, for any strategy ğœand
ğ‘ ğ‘–âˆˆ{âˆ’ 1,1}when agent ğ‘—andğ‘˜both useğœ,arg maxË†ğ‘ ğ‘–âˆˆ{âˆ’1,1}Eğœ,ğ‘ƒ
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=
arg maxË†ğ‘ ğ‘–âˆˆ{âˆ’1,1}{ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)âˆ’ğœ(âˆ’ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)}.
Weâ€™d like to highlight that lemmas 4.3 to 4.5 as well as the proof of theorem 3.1 below hold for any
uniformly dominant tuple âŸ¨ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜âŸ©, not necessarily derived from the Bayesian SST model. This
offers a path to generalize our mechanism for comparison data to other settings.
Proof of theorem 3.1. By lemma 4.2, for any agent ğ‘–, the associated agent ğ‘—â€™s signalğ‘†ğ‘—=ğ‘†(ğ‘â€²â€²,ğ‘â€²)
uniformly dominates the associated ğ‘˜â€™s signalğ‘†ğ‘˜=ğ‘†(ğ‘â€²â€²,ğ‘)for signalğ‘†ğ‘–=ğ‘†(ğ‘,ğ‘â€²). By lemma 4.3,
if agentğ‘—andğ‘˜are truthful, agent ğ‘–â€™s best response is truthful reporting, so truth-telling is a BNE.
Now we show that all other symmetric equilibria are permutation or uninformed equilibria. For any
symmetric equilibrium ğˆ=(ğœğœ„)ğœ„âˆˆNso that everyone uses the same strategy ğœğœ„=ğœfor allğœ„âˆˆN. If
ğœis not deterministic so that ğœ(ğ‘ ,ğ‘ ),ğœ(ğ‘ ,âˆ’ğ‘ )>0for someğ‘ âˆˆ{âˆ’1,1}, agentğ‘–must be indifferent
between reporting ğ‘ andâˆ’ğ‘ when getting ğ‘†ğ‘–=ğ‘ .ğœ(ğ‘ ,ğ‘ )âˆ’ğœ(âˆ’ğ‘ ,ğ‘ )=ğœ(ğ‘ ,âˆ’ğ‘ )âˆ’ğœ(âˆ’ğ‘ ,âˆ’ğ‘ )by
lemma 4.5. This means ğœ(ğ‘ ,ğ‘ )=ğœ(âˆ’ğ‘ ,ğ‘ )andğœ(ğ‘ ,âˆ’ğ‘ )=ğœ(âˆ’ğ‘ ,âˆ’ğ‘ ), andğœis an uninformed
strategy. If the strategy is deterministic, there are two cases. If ğœ(ğ‘ )=ğœ(âˆ’ğ‘ ), the strategy is also
uninformed. If ğœ(ğ‘ )â‰ ğœ(âˆ’ğ‘ ),ğœis either truth-telling ğ‘ â†¦â†’ğ‘ or flippingğ‘ â†¦â†’âˆ’ğ‘ for allğ‘ .
Finally, by lemma 4.4, any uninformed equilibriumâ€™s expectation is zero. Additionally, because
eq. (1) is invariant when all inputs are flipped, the truth-telling and flipping/permutation equilibria
has the same expected payment which is positive by lemma 4.3. â–¡
3If we viewğ‘†ğ‘—andğ‘†ğ‘˜as two statistical tests for a binary event ğ‘†ğ‘–, the two inequalities in definition 4.1 say
thatğ‘†ğ‘—has a better type II and type I error than ğ‘†ğ‘˜respectively.
65 Generalization of Bonus-penalty Payment Mechanisms
We now leverage the key idea of uniform dominance to design peer prediction mechanisms for
networked data in section 5.1. In section 5.2, we summarize our design approach as a general scheme
that first identifies uniform dominance structures and then engages the bonus-penalty payment. We
prove the uniqueness of bonus-penalty payment: it is the only payment function, up to some positive
affine transformation, that induces truth-telling as a strict BNE for all uniform dominant tuples.
5.1 Bonus-penalty Payment Mechanisms for Networked Data
Uniform dominance implies agent ğ‘–â€™s signal is more likely to agree with agent ğ‘—â€™s than with agent
ğ‘˜â€™s. Social networks are another natural domain exhibiting this property, as homophily [ 40] suggests
that agentsâ€™ opinions or signals in a social network are more likely to agree with their friends than
with non-friends. Leveraging this insight, we use a bonus-penalty payment scheme to elicit binary
networked data.
Mechanism 2: BPP mechanism for networked data
Input: Let(ğ‘‰,ğ¸)be a graph of agents in ğ‘‰,Ë†sâˆˆ{âˆ’1,1}ğ‘‰from all agentâ€™s reports.
foragentğ‘–âˆˆğ‘‰do
Find agents ğ‘—(friend) and ğ‘˜(non-friend) so that (ğ‘–,ğ‘—)âˆˆğ¸but(ğ‘–,ğ‘˜)âˆ‰ğ¸, and pay agent ğ‘–
ğ‘€ğ‘–(Ë†s)=ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜)=Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘—âˆ’Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘˜. (3)
Below, we provide a theoretical guarantee for our mechanism under a popular graphical model
for social network data, Ising model [14,43], which captures the correlation between agents and
their friends. Formally, an Ising model consists of an undirected graph (ğ‘‰,ğ¸)and correlation
parameterğ›½ğ‘–,ğ‘—â‰¥0for each edge(ğ‘–,ğ‘—)âˆˆğ¸. Each agent is a node in the graph, N=ğ‘‰, and has a
binary private signal ( 1orâˆ’1) jointly distributed as the following: For all s=(ğ‘ ğ‘–)ğ‘–âˆˆğ‘‰âˆˆ{âˆ’ 1,1}ğ‘‰,
Prğœ·[S=s]âˆexp(ğ»(s))where the energy function is ğ»(s)=Ã
(ğ‘–,ğ‘—)âˆˆğ¸ğ›½ğ‘–,ğ‘—ğ‘ ğ‘–ğ‘ ğ‘—.
Theorem 5.1. If agentsâ€™ signals are sampled from an Ising model on undirected graph (ğ‘‰,ğ¸)with
correlation parameters ğœ·, Mechanism 2 is symmetrically strongly truthful, when2ğ›½
ğ‘‘>lnğ‘’2(ğ‘‘+1)ğ›½+1
ğ‘’2ğ›½+ğ‘’2ğ‘‘ğ›½
whereğ›½=min(ğ‘–,ğ‘—)âˆˆğ¸ğ›½ğ‘–,ğ‘—,ğ›½=max(ğ‘–,ğ‘—)âˆˆğ¸ğ›½ğ‘–,ğ‘—, andğ‘‘is the maximal degree of graph (ğ‘‰,ğ¸).
Mechanism 2 does not require knowledge about parameters of the Ising model, but only the connection
of the network(ğ‘‰,ğ¸). Social network platforms, which already possess this knowledge, can easily
integrate our mechanism when conducting surveys. Additionally, snowball sampling [ 24], which
relies on participants referring their friends, is also naturally compatible with our mechanism.
The complete proof of theorem 5.1 is quite technical and is deferred to the appendix, where we also
explain why the bound between ğœ·andğ‘‘is necessary. Below, we provide a sketch of the proof.
Proof sketch for theorem 5.1. As discussed in section 4, we only need to show that for any agent ğ‘–,
for all agent ğ‘—with(ğ‘–,ğ‘—)âˆˆğ¸andğ‘˜with(ğ‘–,ğ‘˜)âˆ‰ğ¸,ğ‘—â€™s signal uniformly dominates ğ‘˜â€™s signal for
ğ‘–â€™s signal. Because the energy function ğ»(s)above remains invariant when the signs are flipped,
Pr[ğ‘†ğ‘–=1]=Pr[ğ‘†ğ‘—=1]=Pr[ğ‘†ğ‘˜=1]=1/2, it is sufficient to prove that
Pr[ğ‘†ğ‘–=1|ğ‘†ğ‘—=1]>Pr[ğ‘†ğ‘–=1|ğ‘†ğ‘˜=1]. (4)
We then prove a lower bound for the left-hand side and an upper bound for the right-hand side
separately. For the left-hand side, we use the Griffithsâ€™ inequality [ 44] to show that the minimum
value of Pr[ğ‘†ğ‘–=1|ğ‘†ğ‘—=1]happens when ğ‘—is the only friend of ğ‘–. For the right-hand side, we use
Weitzâ€™s self-avoiding walk [ 65] and reduce any graph with maximum degree ğ‘‘into ağ‘‘-ary tree. â–¡
5.2 General Design Scheme and Uniqueness
The design of BPP mechanisms for comparison data and networked data has suggested a general
design scheme for other elicitation settings. That is, if one can identify a uniformly dominant tuple for
7Mechanism 3: General design scheme using BPP
Input: LetË†sbe reports from agents in N.
foragentğ‘–âˆˆN do
Find two agents ğ‘—andğ‘˜so thatğ‘—â€™s signal uniformly dominates ğ‘˜â€™s forğ‘–â€™s, and pay agent ğ‘–
ğ‘€ğ‘–(Ë†s)=ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜)=Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘—âˆ’Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘˜.
each agent , adopting the bonus-penalty payment gives a symmetrically strongly truthful mechanism.
We further show that the bonus-penalty payment is in some sense unique.
Theorem 5.2. If for each agent ğ‘–the associated agent ğ‘—â€™s signal uniformly dominates ğ‘˜â€™s signal for
ğ‘–â€™s signal, the above scheme is symmetrically strongly truthful.
When an agent ğ‘–has multiple pairs of (ğ‘—1,ğ‘˜1),...,(ğ‘—â„“,ğ‘˜â„“)so thatğ‘—ğ‘™â€™s signal uniformly dominates
ğ‘˜ğ‘™â€™s forğ‘–â€™s for eachğ‘™=1,...,â„“ , we may pay agent ğ‘–the average of bonus-penalty payment on all
pairsğ‘€ğ‘–(Ë†s)=1
â„“Ãâ„“
ğ‘™=1ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—ğ‘™,Ë†ğ‘ ğ‘˜ğ‘™). This average maintains our symmetrically strongly truthful
guarantee while potentially reducing the variance in payments.
Theorem 5.2 shows that bonus-penalty payment is a sufficient condition for designing good elicitation
mechanisms for information structures with uniform dominance. We now prove it is also a necessary
condition: any payment that induces truth-telling as a strict BNE under all uniformly dominant tuples
must be an affine transformation of the bonus-penalty payment.
Theorem 5.3 (Uniqueness) .A paymentğ‘ˆ:{âˆ’1,1}3â†’Rsatisfies that, for all uniformly dominant
tuplesâŸ¨ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜âŸ©,ğ‘ ğ‘–=arg maxË†ğ‘ ğ‘–âˆˆ{âˆ’1,1}E
ğ‘ˆ(Ë†ğ‘ ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–, if and only if there exist ğœ† >0
andğœ‡:{âˆ’1,1}2â†’Rso that
ğ‘ˆ(ğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜)=ğœ†ğ‘ˆğµğ‘ƒğ‘ƒ(ğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜)+ğœ‡(ğ‘ ğ‘—,ğ‘ ğ‘˜),for allğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜âˆˆ{âˆ’1,1}
where choice of ğœ‡does not affect the set of equilibria.
6 Experiments
We present experiments on real-world data to evaluate our models and mechanisms. We hope to cast
insights on two questions empirically. Does our mechanism provide better rewards when all agents
report truthfully than when all agents report randomly? Does our mechanism incentivize truth-telling
for each agent if all other agents are truthful? We evaluate Mechanism 1 and 2 by comparing three
settings, truth-telling, uninformed, and unilateral deviation, using empirical cumulative distribution
functions (ECDF) on agentsâ€™ payments. Each point on ECDF denotes the fraction of agents who get
paid less than a particular value.
For both comparison and networked datasets (figs. 1 and 2), we find our mechanisms provide better
payments to agents under truthful settings than the other two settings. The ECDF under truth-telling
lies below the other two ECDFs, which is known as first-order stochastic dominance. This implies
that the truth-telling strategy results in higher average, quantiles (e.g., first quartile, median, and third
quartile), and a greater expectation of any monotone function on the empirical distribution than the
other two settings. We provide additional
6.1 SUSHI Preference Dataset for Comparison Data
We consider preference data for a collection of 10sushi items (item set A) [ 26,27], and focus on a
group of 249agents. Each agent provides a complete ranking of all 10types of sushi in the dataset.
These agents are female, aged thirty to forty-nine, who took more than three hundred seconds to rank
the items and mostly lived in Kanto and Shizuoka until age fifteen. We restrict the set of agents to
avoid significant violations of transitivity across different agents and to better align with our model
assumptions. In the appendix, we will present the experimental results for other groups of users and
further test whether the dataset satisfies transitivity.
For the first question, we use Mechanism 1 to compute each agentâ€™s payment under the truth-telling
or uninformed strategy profile. For each agent ğ‘–, we 1) randomly sample three items ğ‘,ğ‘â€²,ğ‘â€²â€²and
8two agentsğ‘—,ğ‘˜, 2) derive agent ğ‘–â€™s comparison on the first two items (ğ‘,ğ‘â€²)from her ranking, (and
similarly for agent ğ‘—â€™s comparison on(ğ‘â€²,ğ‘â€²â€²), and agentğ‘˜â€™s comparison on(ğ‘,ğ‘â€²â€²)), 3) compute
bonus-penalty payment on these three comparisons, 4) repeat the above procedure 100times and
pay agentğ‘–with the average of those 100trials. For the uninformed strategy setting, we replace
every agentâ€™s comparisons with uniform random bits and compute the payment. The left column of
fig. 1 presents the ECDF of payments for the agents in both settings. The figure shows that in the
uninformed random strategy setting only about 50% of the agents receive positive payments, while in
the original dataset (truthful strategy setting) over 75% of the users receive positive payments. The
right column of fig. 1 tests the second question if the agent has the incentive to deviate when every
other agent is truthful. The truth-telling curve is identical to the left column of fig. 1. For unilateral
deviation, each agent gets the above bonus-penalty payment when her comparisons are replaced by
uniform random bits. We plot the ECDFs of payments for both settings in the right column of fig. 1.
The figure shows that the ECDF of the unilateral deviation payments is above the ECDF of human
usersâ€™ payments, indicating that our mechanism pays more to the truth-telling agents.
Figure 1: SUSHI preference dataset
6.2 Last.fm Dataset for Networked Data
We test our BPP mechanism on the Last.fm dataset from Cantador et al. [8]. This dataset consists
of1892 agents on Last.fm, forming a social network with 12704 edges and an average degree of
6.71. It records agentsâ€™ top fifty favorite artists whom they have listened to the most. We note that, in
the dataset, listener fractions for all artists are much smaller than non-listener fractions. This bias
differs from our Ising model in section 5.1 where every agent has the same chance to get both signals.
Thus, the result can be seen as a stress test for our mechanism even when the data deviate from the
assumption of our theoretical results.
Figure 2 focuses on the most popular artist in the dataset, Lady Gaga, who has a listener fraction of
32.3%. The results for additional artists are presented in the supplementary material. The left column
of fig. 2 tests the first question. Each agent has a binary signal about whether or not she listens to a
particular artist (Lady Gaga in this section). For the truth-telling setting, everyone reports her signal
truthfully and gets payment by the bonus-penalty payment (formally defined in section 5.1). For the
uninformed setting, everyone gets the bonus-penalty payment when all reports are iid according to
the prior ( 0.322for Lady Gaga). When everyone is truthful, more than 76% of agents get positive
payments and have an average payment of 0.37for Lady Gaga, while when agents report randomly,
only half get positive payments, and have a near zero average payment. These results suggest that
agents got more incentive to choose the truth-telling equilibrium than the uninformed equilibrium.
The right column of fig. 2 tests the second question. The truth-telling curve is identical to the left
column of fig. 2. For the unilateral deviation setting, each agent gets the bonus-penalty payment
when she reports listener/non-listener uniformly at random. The unilateral deviationâ€™s payment is
worse than the payments for truth-telling, decreasing from 0.37to near zero for Lady Gaga.
9Figure 2: Last.fm dataset for Lady Gaga
7 Conclusion and Discussion
We introduce a symmetrically strongly truthful peer prediction mechanism for eliciting comparison
data without verification and extend it to eliciting networked data under Ising models. Our mech-
anisms are evaluated using real-world data. A key insight from our work is the identification of a
structure we term â€œuniform dominance,â€ which suggests a path for designing mechanisms in more
complex elicitation settings. For example, in time-series data, adjacent points tend to be more related
than distant ones, and in contextual settings, feedback from similar contexts is typically more related
than from different contexts.
A central assumption in this study is that agents are a priori similar . Hence, noisy comparisons
of item pairs are independent of the assigned agentâ€™s identity. This assumption is reasonable for
items with widely agreed-upon rankings, such as quality assessments of large language model (LLM)
outputs. However, it may break down in settings where preferences are highly polarized, such as
political opinions or social choice problems4. Despite this, our additional experiments in appendix F,
which relax the selection rule used in obtaining fig. 1, show that the mechanism remains robust even
when some dissimilarities among agents exist.
Agents in our model are assumed to focus solely on maximizing their payments, without accounting
for efforts or external incentives such as minimizing othersâ€™ rewards or intentionally distorting
rankings. While our mechanism may be extended to handle binary effort as suggested in previous
work [ 11,57], accommodating more than two effort levels would require additional assumptions
[69]. Moreover, one may hope to incorporate the designerâ€™s utility, by factoring in downstream
learning problems along with elicitation payments. This would necessitate a significant overhaul of
the existing learning framework.
Our mechanisms achieve a symmetric, strongly truthful equilibrium. This does not rule out the
existence of non-symmetric equilibria with potentially higher utility. However, such equilibria would
require complex coordination among agents, making them less likely to arise naturally.
From a technical standpoint, our approach involves several assumptions that can be generalized
or relaxed. Our Bayesian SST model, which relies on strong stochastic transitivity, serves as a
non-parametric extension of several widely used parametric ranking models. In appendix C.2, we
present both positive and negative results regarding weaker notions of transitivity (e.g., [ 5]). While
we assume admissible assignments, this can be relaxed to random assignments with full support.
Additionally, limited liability can be ensured in our mechanism. For example, adding a constant of 1
to the payment function in eq. (2) ensures that the payment is either 2or0.
4For example, when ranking phone features (e.g., innovation, performance, brand reputation, price, ease
of use), consumers often fall into two groups: early adopters, who prioritize cutting-edge technology, and late
adopters, who favor stability, affordability, and ease of use. Their opposing preferences violate the a priori
similarity assumption. Imagine an early adopter whose payment in eq. (2) depends on two late adopters. Since
their preferences may differ significantly, the early adopter might have an incentive to misreport her preferences.
10Acknowledgments
This research was partially supported by the National Science Foundation under grant no. IIS-
2147187.
References
[1]Jacob D Abernethy and Rafael Frongillo. A collaborative mechanism for crowdsourcing
prediction problems. Advances in neural information processing systems , 24, 2011.
[2]Hossein Azari, David Parks, and Lirong Xia. Random utility theory for social choice. Advances
in Neural Information Processing Systems , 25, 2012.
[3]AurÃ©lien Baillon. Bayesian markets to elicit private information. Proceedings of the National
Academy of Sciences , 114(30):7958â€“7962, 2017.
[4]Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the
method of paired comparisons. Biometrika , 39(3/4):324â€“345, 1952.
[5]Mark Braverman and Elchanan Mossel. Noisy sorting without resampling. In Proceedings
of the Nineteenth Annual ACM-SIAM Symposium on Discrete Algorithms , SODA â€™08, page
268â€“276, USA, 2008. Society for Industrial and Applied Mathematics.
[6]Noah Burrell and Grant Schoenebeck. Measurement integrity in peer prediction: A peer assess-
ment case study. In Proceedings of the 24th ACM Conference on Economics and Computation ,
pages 369â€“389, 2023.
[7]RÃ³bert Busa-Fekete, Eyke HÃ¼llermeier, and BalÃ¡zs SzÃ¶rÃ©nyi. Preference-based rank elicitation
using statistical models: The case of mallows. In International conference on machine learning ,
pages 1071â€“1079. PMLR, 2014.
[8]IvÃ¡n Cantador, Peter Brusilovsky, and Tsvi Kuflik. Second workshop on information hetero-
geneity and fusion in recommender systems. In Proceedings of the fifth ACM conference on
Recommender systems , pages 387â€“388, 2011.
[9]Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li,
Zhenya Huang, and Enhong Chen. Towards personalized evaluation of large language models
with an anonymous crowd-sourcing platform. In Companion Proceedings of the ACM on Web
Conference 2024 , pages 1035â€“1038, 2024.
[10] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li,
Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al. Chatbot arena:
An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132 ,
2024.
[11] Anirban Dasgupta and Arpita Ghosh. Crowdsourced judgement elicitation with endogenous
proficiency. In Proceedings of the 22nd international conference on World Wide Web , pages
319â€“330, 2013.
[12] Constantinos Daskalakis, Nishanth Dikkala, and Gautam Kamath. Concentration of multilinear
functions of the ising model with applications to network data. Advances in Neural Information
Processing Systems , 30, 2017.
[13] Donald Davidson and Jacob Marschak. Experimental tests of a stochastic decision theory.
Measurement: Definitions and theories , 17(2), 1959.
[14] Glenn Ellison. Learning, local interaction, and coordination. Econometrica: Journal of the
Econometric Society , pages 1047â€“1071, 1993.
[15] Brian Eriksson. Learning to top-k search using pairwise comparisons. In Artificial Intelligence
and Statistics , pages 265â€“273. PMLR, 2013.
[16] Boi Faltings. Game-theoretic mechanisms for eliciting accurate information. In IJCAI , 2022.
11[17] Shi Feng, Fang-Yi Yu, and Yiling Chen. Peer prediction for learning agents. Advances in Neural
Information Processing Systems , 35:17276â€“17286, 2022.
[18] Peter C Fishburn. Binary choice probabilities: on the varieties of stochastic transitivity. Journal
of Mathematical psychology , 10(4):327â€“352, 1973.
[19] Kiriaki Frangias, Andrew Lin, Ellen Vitercik, and Manolis Zampetakis. Algorithmic contract
design for crowdsourced ranking. arXiv preprint arXiv:2310.09974 , 2023.
[20] Rafael Frongillo and Ian A Kash. Vector-valued property elicitation. In Conference on Learning
Theory , pages 710â€“727. PMLR, 2015.
[21] Xi Alice Gao, James R Wright, and Kevin Leyton-Brown. Incentivizing evaluation with peer
prediction and limited access to ground truth. Artificial Intelligence , 275:618â€“638, 2019.
[22] Hans-Otto Georgii. Gibbs measures and phase transitions . Walter de Gruyter GmbH & Co.
KG, Berlin, 2011.
[23] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American statistical Association , 102(477):359â€“378, 2007.
[24] Leo A Goodman. Snowball sampling. The annals of mathematical statistics , pages 148â€“170,
1961.
[25] David R Hunter. Mm algorithms for generalized bradley-terry models. The annals of statistics ,
32(1):384â€“406, 2004.
[26] Toshihiro Kamishima. Nantonac collaborative filtering: recommendation based on order
responses. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge
discovery and data mining , pages 583â€“588, 2003.
[27] Toshihiro Kamishima and Shotaro Akaho. Efficient clustering for orders. In Sixth IEEE
International Conference on Data Mining-Workshops (ICDMWâ€™06) , pages 274â€“278. IEEE,
2006.
[28] Sai Praneeth Karimireddy, Wenshuo Guo, and Michael I Jordan. Mechanisms that incentivize
data sharing in federated learning. arXiv preprint arXiv:2207.04557 , 2022.
[29] Yuqing Kong. Dominantly truthful multi-task peer prediction with a constant number of tasks.
InProceedings of the fourteenth annual acm-siam symposium on discrete algorithms , pages
2398â€“2411. SIAM, 2020.
[30] Yuqing Kong and Grant Schoenebeck. A framework for designing information elicitation
mechanisms that reward truth-telling. CoRR , abs/1605.01021, 2016. URL http://arxiv.
org/abs/1605.01021 .
[31] Yuqing Kong and Grant Schoenebeck. Equilibrium selection in information elicitation without
verification via information monotonicity. In 9th Innovations in Theoretical Computer Science
Conference (ITCS 2018) . Schloss-Dagstuhl-Leibniz Zentrum fÃ¼r Informatik, 2018.
[32] Yuqing Kong and Grant Schoenebeck. Water from two rocks: Maximizing the mutual infor-
mation. In Proceedings of the 2018 ACM Conference on Economics and Computation , pages
177â€“194, 2018.
[33] Yuqing Kong and Grant Schoenebeck. An information theoretic framework for designing
information elicitation mechanisms that reward truth-telling. ACM Transactions on Economics
and Computation (TEAC) , 7(1):1â€“33, 2019.
[34] Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. Reliability and learnability of human bandit
feedback for sequence-to-sequence reinforcement learning. arXiv preprint arXiv:1805.10627 ,
2018.
[35] Nicolas S Lambert, David M Pennock, and Yoav Shoham. Eliciting properties of probability
distributions. In Proceedings of the 9th ACM Conference on Electronic Commerce , pages
129â€“138, 2008.
12[36] Yang Liu, Rixing Lou, and Jiaheng Wei. Auditing for federated learning: A model elicitation
approach. In Proceedings of the Fifth International Conference on Distributed Artificial
Intelligence , DAI â€™23, New York, NY , USA, 2023. Association for Computing Machinery.
ISBN 9798400708480. doi: 10.1145/3627676.3627683. URL https://doi.org/10.1145/
3627676.3627683 .
[37] Yang Liu, Juntao Wang, and Yiling Chen. Surrogate scoring rules. ACM Transactions on
Economics and Computation , 10(3):1â€“36, 2023.
[38] R Duncan Luce. Individual choice behavior: A theoretical analysis . Courier Corporation, 2005.
[39] C. L. Mallows. Non-null ranking models. i. Biometrika , 44(1/2):114â€“130, 1957. ISSN
00063444. URL http://www.jstor.org/stable/2333244 .
[40] Miller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in
social networks. Annual review of sociology , 27(1):415â€“444, 2001.
[41] N. Miller, P. Resnick, and R. Zeckhauser. Eliciting informative feedback: The peer-prediction
method. Management Science , pages 1359â€“1373, 2005.
[42] Nolan Miller, Paul Resnick, and Richard Zeckhauser. Eliciting informative feedback: The
peer-prediction method. Management Science , 51(9):1359â€“1373, 2005.
[43] Andrea Montanari and Amin Saberi. The spread of innovations in social networks. Proceedings
of the National Academy of Sciences , 107(47):20196â€“20201, 2010.
[44] Marc MÃ©zard and Andrea Montanari. Information, Physics, and Computation . Oxford Univer-
sity Press, 01 2009. ISBN 9780198570837. doi: 10.1093/acprof:oso/9780198570837.001.0001.
URL https://doi.org/10.1093/acprof:oso/9780198570837.001.0001 .
[45] Ryan Oâ€™Donnell. Analysis of boolean functions . Cambridge University Press, 2014.
[46] Kent Harold Osband. Providing Incentives for Better Cost Forecasting (Prediction, Uncertainty
Elicitation) . University of California, Berkeley, 1985.
[47] Drazen Prelec. A bayesian truth serum for subjective data. science , 306(5695):462â€“466, 2004.
[48] Goran Radanovic and Boi Faltings. A robust bayesian truth serum for non-binary signals. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 27, pages 833â€“839,
2013.
[49] Goran Radanovic and Boi Faltings. Incentives for truthful information elicitation of continuous
signals. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence , pages
770â€“776, 2014.
[50] Grant Schoenebeck and Fang-Yi Yu. Learning and strongly truthful multi-task peer prediction:
A variational approach. In 12th Innovations in Theoretical Computer Science Conference (ITCS
2021) . Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik, 2021.
[51] Grant Schoenebeck and Fang-Yi Yu. Two strongly truthful mechanisms for three heterogeneous
agents answering one question. ACM Transactions on Economics and Computation , 10(4):1â€“26,
2023.
[52] Grant Schoenebeck, Fang-Yi Yu, and Yichi Zhang. Information elicitation from rowdy crowds.
InProceedings of the Web Conference 2021 , pages 3974â€“3986, 2021.
[53] SÃ¶ren W Scholz, Martin Meissner, and Reinhold Decker. Measuring consumer preferences
for complex products: A compositional approach basedonpaired comparisons. Journal of
Marketing Research , 47(4):685â€“698, 2010.
[54] Nihar Shah, Sivaraman Balakrishnan, Aditya Guntuboyina, and Martin Wainwright. Stochas-
tically transitive models for pairwise comparisons: Statistical and computational issues. In
International Conference on Machine Learning , pages 11â€“20. PMLR, 2016.
13[55] Nihar B Shah, Joseph K Bradley, Abhay Parekh, Martin Wainwright, and Kannan Ramchandran.
A case for ordinal peer-evaluation in moocs. In NIPS workshop on data driven education ,
volume 15, page 67, 2013.
[56] Victor Shnayder, Arpit Agarwal, Rafael Frongillo, and David C. Parkes. Informed truthfulness
in multi-task peer prediction. In Proceedings of the 2016 ACM Conference on Economics and
Computation , EC â€™16, pages 179â€“196, New York, NY , USA, 2016. ACM. ISBN 978-1-4503-
3936-0.
[57] Victor Shnayder, Arpit Agarwal, Rafael M. Frongillo, and David C. Parkes. Informed
truthfulness in multi-task peer prediction. CoRR , abs/1603.03151, 2016. URL http:
//arxiv.org/abs/1603.03151 .
[58] Louis L Thurstone. A law of comparative judgment. In Scaling , pages 81â€“92. Routledge, 2017.
[59] Amos Tversky and J Edward Russo. Substitutability and similarity in binary choices. Journal
of Mathematical psychology , 6(1):1â€“12, 1969.
[60] Stephan Vail. A stochastic model for utilities. Unpublished manuscript , 1953.
[61] Luis V on Ahn and Laura Dabbish. Labeling images with a computer game. In Proceedings of
the SIGCHI conference on Human factors in computing systems , pages 319â€“326, 2004.
[62] Luis von Ahn and Laura Dabbish. Designing games with a purpose. Commun. ACM , 51(8):
58â€“67, aug 2008. ISSN 0001-0782. doi: 10.1145/1378704.1378719. URL https://doi.
org/10.1145/1378704.1378719 .
[63] Bo Waggoner and Yiling Chen. Output agreement mechanisms and common knowledge. In
Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , volume 2,
pages 220â€“226, 2014.
[64] Jiaheng Wei, Zuyue Fu, Yang Liu, Xingyu Li, Zhuoran Yang, and Zhaoran Wang. Sample
elicitation. In International Conference on Artificial Intelligence and Statistics , pages 2692â€“
2700. PMLR, 2021.
[65] Dror Weitz. Counting independent sets up to the tree threshold. In Proceedings of the thirty-
eighth annual ACM symposium on Theory of computing , pages 140â€“149, 2006.
[66] Jens Witkowski and David C. Parkes. A Robust Bayesian Truth Serum for Small Populations.
InProceedings of the 26th AAAI Conference on Artificial Intelligence (AAAI 2012) , 2011.
[67] Jens Witkowski and David C. Parkes. Peer prediction without a common prior. In Proceedings
of the 13th ACM Conference on Electronic Commerce, EC 2012, Valencia, Spain, June 4-8,
2012 , pages 964â€“981. ACM, 2012.
[68] Peter Zhang and Yiling Chen. Elicitability and knowledge-free elicitation with peer prediction.
InProceedings of the 2014 international conference on Autonomous agents and multi-agent
systems , pages 245â€“252. International Foundation for Autonomous Agents and Multiagent
Systems, 2014.
[69] Yichi Zhang and Grant Schoenebeck. High-effort crowds: Limited liability via tournaments.
InProceedings of the ACM Web Conference 2023 , WWW â€™23, page 3467â€“3477, New York,
NY , USA, 2023. Association for Computing Machinery. ISBN 9781450394161. doi: 10.1145/
3543507.3583334. URL https://doi.org/10.1145/3543507.3583334 .
[70] Shuran Zheng, Fang-Yi Yu, and Yiling Chen. The limits of multi-task peer prediction. In
Proceedings of the 22nd ACM Conference on Economics and Computation , pages 907â€“926,
2021.
14A Further discussion on BPP payment
In this section, we discuss the connection of bonus-penalty payment and existing peer prediction
mechanisms. First, if we substitute the third input with a uniformly random bit, denoted as Ë†ğ‘ ğ‘˜=ğ‘âˆ¼ğ‘¢
{âˆ’1,1}, the bonus-penalty payment simplifies to the agreement mechanism [62,61,63], one of the
most basic peer prediction mechanisms,
E
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,ğ‘)
=Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘—=21[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘—]âˆ’1.
However, the agreement mechanism is not symmetrically strongly truthful, as all agents always
reporting 1andâˆ’1can result in higher payments than truth-telling.
The bonus-penalty payment eq. (1) is originally proposed by [ 11,57] for the multi-task setting.
Our BPP mechanism in Mechanism 3 can be seen as a generalization of multi-task setting. In the
multi-task setting, agents works on multiple tasks and for each task the private signals are jointly
identically and independently (iid) sampled from a fixed distribution and the each agentâ€™s strategy
also are iid. Take two agents (Isabel and Julia) and two tasks as an example: Isabel has a private
signal(ğ‘ 1
ğ‘–,ğ‘ 2
ğ‘–)and reports(Ë†ğ‘ 1
ğ‘–,Ë†ğ‘ 2
ğ‘–)and Julia has(ğ‘ 1
ğ‘—,ğ‘ 2
ğ‘—)and reports(Ë†ğ‘ 1
ğ‘—,Ë†ğ‘ 2
ğ‘—)where(ğ‘ ğ‘™
ğ‘–,ğ‘ ğ‘™
ğ‘—)are iid
from random vector (ğ‘†ğ‘–,ğ‘†ğ‘—). Isabel and Julia decide their reports on each task using random function
ğœğ‘–,ğœğ‘—:{âˆ’1,1}â†¦â†’{âˆ’ 1,1}respectively. Dasgupta and Ghosh [11] use the following payments for
Isabel
1[Ë†ğ‘ 1
ğ‘–=Ë†ğ‘ 1
ğ‘—]âˆ’1[Ë†ğ‘ 1
ğ‘–=Ë†ğ‘ 2
ğ‘—]=1
2ğ‘ˆğµğ‘ƒğ‘ƒ
Ë†ğ‘ 1
ğ‘–,Ë†ğ‘ 1
ğ‘—,Ë†ğ‘ 2
ğ‘—
.
The payment is a special case of Mechanism 3 by taking the second input as Ë†ğ‘ 1
ğ‘—and the third input as
Ë†ğ‘ 2
ğ‘—. Additionally, ğ‘†1
ğ‘—uniform dominates ğ‘†2
ğ‘—forğ‘†1
ğ‘–if and only if
Pr[ğ‘†ğ‘—=1|ğ‘†ğ‘–=1]>Pr[ğ‘†ğ‘—=1],andPr[ğ‘†ğ‘—=âˆ’1|ğ‘†ğ‘–=âˆ’1]>Pr[ğ‘†ğ‘—=âˆ’1]
which is called categorical signal distributions [57].
Finally, similar to Shnayder et al. [57], we may extend to non-binary signal setting by extending the
payment to
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜)=2 1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘—]âˆ’1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘˜]
and the definition of uniform dominance to the following.
Definition A.1. Given a random vector (ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)âˆˆÎ©3on a discrete domain, we say ğ‘†ğ‘—uniformly
dominatesğ‘†ğ‘˜forğ‘†ğ‘–if
Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ]>0and
Pr[ğ‘†ğ‘—=ğ‘ â€²|ğ‘†ğ‘–=ğ‘ ]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ â€²|ğ‘†ğ‘–=ğ‘ ]<0
for allğ‘ ,ğ‘ â€²âˆˆÎ©withğ‘ â‰ ğ‘ â€².
However, the guarantee for truth-telling ( informed truthfulness ) is weaker than the binary setting.
Theorem A.2. Given any discrete domain Î©, if for each agent ğ‘–the associated agent ğ‘—â€™s signal
uniformly dominates ğ‘˜â€™s signal forğ‘–â€™s signal (definition A.1), Mechanism 3â€™s scheme is symmetrically
informed truthful so that
1. truth-telling is a strict equilibrium, and
2.each agentâ€™s expected payment in truth-telling is no less than the payment in any other
symmetric equilibria and strictly better than any uninformed equilibriumâ€™s.
Proof. First truth-telling is a strict equilibrium, because if ğ‘†ğ‘–=ğ‘ ,
arg max
Ë†ğ‘ E
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ 
=arg max
Ë†ğ‘ Pr[ğ‘†ğ‘—=Ë†ğ‘ |ğ‘†ğ‘–=ğ‘ ]âˆ’Pr[ğ‘†ğ‘˜=Ë†ğ‘ |ğ‘†ğ‘–=ğ‘ ]
=ğ‘  (by definition A.1)
15Additionally, because Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ]>Pr[ğ‘†ğ‘—=ğ‘ â€²|ğ‘†ğ‘–=ğ‘ ]âˆ’Pr[ğ‘†ğ‘˜=
ğ‘ â€²|ğ‘†ğ‘–=ğ‘ ]for allğ‘ â€²â‰ ğ‘ , summing over all possible ğ‘ â€²âˆˆÎ©on both sides gets Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=
ğ‘ ]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ]>0and
E
ğ‘ˆğµğ‘ƒğ‘ƒ(ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)
>0.
For any informed equilibrium, by a direct computation E
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘†ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)
=0.
Finally, we show that the truth-telling has the maximum expected payment for each agents. When all
agent use a strategy ğœ:Î©â†’Î©, agentğ‘–â€™s expected payment is
âˆ‘ï¸
ğ‘ ğ‘–,Ë†ğ‘ ğ‘–âˆˆÎ©Pr[ğ‘†ğ‘–=ğ‘ ğ‘–]ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)E
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=2âˆ‘ï¸
ğ‘ ğ‘–,Ë†ğ‘ ğ‘–âˆˆÎ©Pr[ğ‘†ğ‘–=ğ‘ ğ‘–]ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)âˆ‘ï¸
ğ‘ âˆˆÎ©(Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–])ğœ(ğ‘ ,Ë†ğ‘ ğ‘–)
=2âˆ‘ï¸
ğ‘ ğ‘–âˆˆÎ©Pr[ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ‘ï¸
Ë†ğ‘ ğ‘–,ğ‘ âˆˆÎ©ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)ğœ(ğ‘ ,Ë†ğ‘ ğ‘–)(Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–])
Letğ‘“ğ‘ ğ‘–(ğ‘ ):=Ã
Ë†ğ‘ ğ‘–âˆˆÎ©ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)ğœ(ğ‘ ,Ë†ğ‘ ğ‘–)which is between 0and 1, because ğ‘“ğ‘ ğ‘–(ğ‘ ) â‰¤Ã
Ë†ğ‘ ğ‘–âˆˆÎ©ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)Ã
Ë†ğ‘ ğ‘–âˆˆÎ©ğœ(ğ‘ ,Ë†ğ‘ ğ‘–)=1. Then the expectation becomes
âˆ‘ï¸
ğ‘ ğ‘–,Ë†ğ‘ ğ‘–âˆˆÎ©Pr[ğ‘†ğ‘–=ğ‘ ğ‘–]ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)E
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=2âˆ‘ï¸
ğ‘ ğ‘–âˆˆÎ©Pr[ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ‘ï¸
ğ‘ âˆˆÎ© Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–]ğ‘“ğ‘ ğ‘–(ğ‘ )
â‰¤2âˆ‘ï¸
ğ‘ ğ‘–âˆˆÎ©Pr[ğ‘†ğ‘–=ğ‘ ğ‘–] Pr[ğ‘†ğ‘—=ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]
=E
ğ‘ˆğµğ‘ƒğ‘ƒ(ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)
The inequality holds because ğ‘“ğ‘ ğ‘–âˆˆ[0,1]and definition A.1. Therefore, we complete the proof. â–¡
B Proofs in Section 2: Bayesian SST model and other models
The proofs of propositions 2.3 and 2.5 are standard, and variations can be found in related literature.
We include proofs here for completeness.
Proof of proposition 2.3. First given ğœƒâˆˆRA, for all distinct ğ‘,ğ‘â€²,ğ‘â€²â€²âˆˆ A ,Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=
1],Pr[ğ‘‡ğœƒ(ğ‘â€²,ğ‘â€²â€²)=1]>1/2implies that ğœƒğ‘âˆ’ğœƒğ‘â€²>0andğœƒğ‘â€²âˆ’ğœƒğ‘â€²â€²>0becuaseğ¹is strictly
increasing and ğ¹(0)=1/2. Becauseğœƒğ‘âˆ’ğœƒğ‘â€²â€²=ğœƒğ‘âˆ’ğœƒğ‘â€²+ğœƒğ‘â€²âˆ’ğœƒğ‘â€²â€²>max(ğœƒğ‘âˆ’ğœƒğ‘â€²,ğœƒğ‘â€²âˆ’ğœƒğ‘â€²â€²),
we have
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²â€²)=1]=ğ¹(ğœƒğ‘âˆ’ğœƒğ‘â€²â€²)
>maxğ¹(ğœƒğ‘âˆ’ğœƒğ‘â€²),ğ¹(ğœƒğ‘â€²âˆ’ğœƒğ‘â€²â€²)
=max Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1],Pr[ğ‘‡ğœƒ(ğ‘â€²,ğ‘â€²â€²)=1]
and thusğ‘‡ğœƒis strongly stochastically transitive for all ğœƒwith distinct coordinates which happens
surely asğœˆis non-atomic. Finally, since the distribution on ğœƒis exchangeable on each coordinate,
E[E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)]]=0for allğ‘,ğ‘â€². â–¡
Proof of proposition 2.5. First givenğœƒâˆˆÎ˜, for all distinct ğ‘,ğ‘â€²âˆˆA, if the rank of ğ‘is higher than
ğ‘â€²,
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1]=â„ğœ‚(ğœƒ(ğ‘â€²)âˆ’ğœƒ(ğ‘)+1)âˆ’â„ğœ‚(ğœƒ(ğ‘â€²)âˆ’ğœƒ(ğ‘))
whereâ„ğœ‚(ğ‘¥)=ğ‘¥
1âˆ’exp(âˆ’ğœ‚ğ‘¥)by Busa-Fekete et al. [7].
Claim B.1. For anyğœ‚>0andğ‘¥âˆˆZ>0, the difference â„ğœ‚(ğ‘¥+1)âˆ’â„ğœ‚(ğ‘¥)is increasing and larger
than 1/2whereâ„ğœ‚(ğ‘¥)=ğ‘¥
1âˆ’exp(âˆ’ğœ‚ğ‘¥).
16By claim B.1, Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1],Pr[ğ‘‡ğœƒ(ğ‘â€²,ğ‘â€²â€²)=1]>1/2implies that ğœƒ(ğ‘â€²)âˆ’ğœƒ(ğ‘)>0and
ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘â€²)>0. Thus,ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘)>max(ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘â€²),ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘â€²)), and
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²â€²)=1]=â„(ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘)+1)âˆ’â„(ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘))
>maxâ„(ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘â€²)+1)âˆ’â„(ğœƒ(ğ‘â€²â€²)âˆ’ğœƒ(ğ‘â€²)),â„(ğœƒ(ğ‘â€²)âˆ’ğœƒ(ğ‘)+1)âˆ’â„(ğœƒ(ğ‘â€²)âˆ’ğœƒ(ğ‘))
=max Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1],Pr[ğ‘‡ğœƒ(ğ‘â€²,ğ‘â€²â€²)=1]
where the second inequality is due to claim B.1. Therefore, ğ‘‡ğœƒis strongly stochastically transitive for
allğœƒ. Finally, E[E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)]]=0for allğ‘,ğ‘â€²sinceğœƒis an uniform distribution on rankings. â–¡
Proof of claim B.1. We first prove that the function â„ğœ‚(ğ‘¥)=ğ‘¥
1âˆ’exp(âˆ’ğœ‚ğ‘¥)is increasing and strictly
convex onğ‘¥â‰¥0. Becauseâ„ğœ‚(ğ‘¥)=1
ğœ‚â„1(ğœ‚ğ‘¥), for allğœ‚,ğ‘¥, it is sufficient to consider ğœ‚=1. First,
â„â€²
1(ğ‘¥)=1âˆ’(ğ‘¥+1)ğ‘’âˆ’ğ‘¥
(1âˆ’ğ‘’âˆ’ğ‘¥)2>0, soâ„1is increasing. Second, as â„â€²â€²
1(ğ‘¥)=ğ‘’âˆ’ğ‘¥((ğ‘¥âˆ’2)+(ğ‘¥+2)ğ‘’âˆ’ğ‘¥)
(1âˆ’ğ‘’âˆ’ğ‘¥)3 , to show
â„â€²â€²
1(ğ‘¥)>0for allğ‘¥ > 0, it is sufficient to show that ğ‘”(ğ‘¥)=(ğ‘¥âˆ’2)+(ğ‘¥+2)ğ‘’âˆ’ğ‘¥>0. Because
ğ‘”(0)=0andğ‘”â€²(ğ‘¥)=1âˆ’(ğ‘¥+1)ğ‘’âˆ’ğ‘¥>0,ğ‘”(ğ‘¥)>0for allğ‘¥ >0. Therefore, â„1is strictly convex.
On the other hand, â„ğœ‚(ğ‘¥+2)âˆ’â„ğœ‚(ğ‘¥+1)> â„ğœ‚(ğ‘¥+1)âˆ’â„ğœ‚(ğ‘¥)for allğ‘¥by convexity, and â„ğœ‚(2)âˆ’â„ğœ‚(1)=
1
1+ğ‘’âˆ’ğœ‚>1
2which completes the proof. â–¡
C Proofs in Section 3 and 4
C.1 Uniform dominance from Bayesian SST
Proof of lemma 4.2. With a prior similar assumption for Bayesian SST model, we only need to show
Pr[ğ‘†(ğ‘â€²â€²,ğ‘â€²)=1|ğ‘†(ğ‘,ğ‘â€²)=1]>Pr[ğ‘†(ğ‘â€²â€²,ğ‘)=1|ğ‘†(ğ‘,ğ‘â€²)=1], (5)
and the other case Pr[ğ‘†(ğ‘â€²â€²,ğ‘â€²)=âˆ’1|ğ‘†(ğ‘,ğ‘â€²)=âˆ’1]>Pr[ğ‘†(ğ‘â€²â€²,ğ‘)=âˆ’1|ğ‘†(ğ‘,ğ‘â€²)=âˆ’1]follows
by symmetry. To prove eq. (5), we can rewrite the conditional probability in expectations of ğ‘‡ğœƒ.
Pr[ğ‘†(ğ‘â€²â€²,ğ‘â€²)=1|ğ‘†(ğ‘,ğ‘â€²)=1]
=âˆ«
Pr[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)=1,ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1|ğœƒ]ğ‘‘ğ‘ƒÎ˜âˆ«
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1|ğœƒ]ğ‘‘ğ‘ƒÎ˜
=âˆ«
Pr[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)=1|ğœƒ]Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1|ğœƒ]ğ‘‘ğ‘ƒÎ˜âˆ«
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1|ğœƒ]ğ‘‘ğ‘ƒÎ˜(conditional independent)
=2âˆ«
Pr[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)=1|ğœƒ]Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1|ğœƒ]ğ‘‘ğ‘ƒÎ˜ (a prior similar)
=2âˆ«E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)|ğœƒ]+1
2E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]+1
2ğ‘‘ğ‘ƒÎ˜ (binary value)
=1
2âˆ«
E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)|ğœƒ]E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]+E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)|ğœƒ]+E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]+1ğ‘‘ğ‘ƒÎ˜
=1
2âˆ«
E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)|ğœƒ]E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]+1ğ‘‘ğ‘ƒÎ˜. (a prior similar)
Claim C.1. For any strongly stochastically transitive ğ‘‡ğœƒonA, and distinct ğ‘,ğ‘â€²,ğ‘â€²â€²âˆˆA
E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)|ğœƒ]>E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘)|ğœƒ].
With claim C.1, we have
Pr[ğ‘†(ğ‘â€²â€²,ğ‘â€²)=1|ğ‘†(ğ‘,ğ‘â€²)=1]=1
2âˆ«
E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)|ğœƒ]E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]+1ğ‘‘ğ‘ƒÎ˜
>1
2âˆ«
E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘)|ğœƒ]E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]+1ğ‘‘ğ‘ƒÎ˜=Pr[ğ‘†(ğ‘â€²â€²,ğ‘)=1|ğ‘†(ğ‘,ğ‘â€²)=1].
This completes the proof of eq. (5), and thus the uniform dominance. â–¡
17Proof of claim C.1. We letğ‘„(ğ›¼,ğ›¼â€²):=E[ğ‘‡ğœƒ(ğ›¼,ğ›¼â€²)|ğœƒ]=2 Pr[ğ‘‡ğœƒ(ğ›¼,ğ›¼â€²)=1|ğœƒ]âˆ’1for allğ›¼,ğ›¼â€².
Note thatğ‘„(ğ›¼,ğ›¼â€²)>0if and only if Pr[ğ‘‡ğœƒ(ğ›¼,ğ›¼â€²)=1|ğœƒ]>1/2andğ‘„(ğ›¼,ğ›¼â€²)=âˆ’ğ‘„(ğ›¼â€²,ğ›¼).
By symmetry, let ğ‘„(ğ‘,ğ‘â€²)>0. It is sufficient to show that
ğ‘„(ğ‘â€²â€²,ğ‘â€²)>ğ‘„(ğ‘â€²â€²,ğ‘).
Ifğ‘„(ğ‘â€²,ğ‘â€²â€²)>0, by definition 2.1 ğ‘„(ğ‘,ğ‘â€²â€²)> ğ‘„(ğ‘â€²,ğ‘â€²â€²)>0soğ‘„(ğ‘â€²â€²,ğ‘â€²)> ğ‘„(ğ‘â€²â€²,ğ‘). Now
considerğ‘„(ğ‘â€²,ğ‘â€²â€²)<0. Ifğ‘„(ğ‘â€²â€²,ğ‘)<0,ğ‘„(ğ‘â€²â€²,ğ‘â€²)>0> ğ‘„(ğ‘â€²â€²,ğ‘). Ifğ‘„(ğ‘â€²â€²,ğ‘)>0, we have
ğ‘„(ğ‘â€²â€²,ğ‘)>0,ğ‘„(ğ‘,ğ‘â€²)>0, and thusğ‘„(ğ‘â€²â€²,ğ‘â€²)>ğ‘„(ğ‘â€²â€²,ğ‘)by definition 2.1 â–¡
C.2 Uniform dominance and weak notions of stochastic transitivity
There are weaker forms of stochastic transitivity, raising the question of whether they are sufficient
for uniform dominance as in lemma 4.2. We show that general weak stochastic transitivity is not
sufficient. Additionally, we show that although the noisy sorting model from [ 5] is only weakly
stochastically transitive but does not satisfy definition 2.1, it exhibits uniform dominance.
Definition C.2 ([13]).A stochastic comparison function, ğ‘‡:A2â†’{âˆ’ 1,1}, isweakly stochastically
transitive if for allğ‘,ğ‘â€²,ğ‘â€²â€²âˆˆA with Pr[ğ‘‡(ğ‘,ğ‘â€²)=1]>1/2andPr[ğ‘‡(ğ‘â€²,ğ‘â€²â€²)=1]>1/2,
Pr[ğ‘‡(ğ‘,ğ‘â€²â€²)=1]>1/2.
Compared to definition 2.1, the weak stochastic transitivity only require the item ğ‘is favorable than
ğ‘â€²â€². Below we provide a simple weakly stochastically transitive example with a prior similar property
that does not satisfy the uniform dominance in eq. (5).
Example C.3. Consider the set of three items and Î˜consists of all ranking on Awith uniform prior
whereğœƒmaps each items to its value. Given ğœƒâˆˆÎ˜so that ifğœƒ(ğ‘)>ğœƒ(ğ‘â€²)>ğœƒ(ğ‘â€²â€²),
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1]=Pr[ğ‘‡ğœƒ(ğ‘â€²,ğ‘â€²â€²)=1]=0.9andPr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²â€²)=1]=0.6.
Note that the model is weakly stochastically transitive, because an item with a larger value is more fa-
vorable and the weak stochastic transitivity is reduced to transitivity on the values. However, the model
is not strongly stochastically transitive, because Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²â€²)=1]=0.6<max{Pr[(ğ‘‡(ğ‘,ğ‘â€²)=
1],Pr[(ğ‘‡(ğ‘â€²,ğ‘â€²â€²)=1]]}=0.9. Finally, as the rank ğœƒhas a uniform prior, the model satisfies a prior
similar assumption.
To conclude the example, we show that eq. (5) does not hold for the above model. By direct
computation over all six possible ranking ğœƒ, we have
Pr[ğ‘†(ğ‘â€²â€²,ğ‘â€²)=1|ğ‘†(ğ‘,ğ‘â€²)=1]
=1
2âˆ«
E[ğ‘‡ğœƒ(ğ‘â€²â€²,ğ‘â€²)|ğœƒ]E[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)|ğœƒ]+1ğ‘‘ğ‘ƒÎ˜
=1
2
1âˆ’64
6
,
butPr[ğ‘†(ğ‘â€²â€²,ğ‘)=1|ğ‘†(ğ‘,ğ‘â€²)=1]=1
2
1+64
6
. Therefore, we have Pr[ğ‘†(ğ‘â€²â€²,ğ‘â€²)=1|ğ‘†(ğ‘,ğ‘â€²)=
1]<Pr[ğ‘†(ğ‘â€²â€²,ğ‘)=1|ğ‘†(ğ‘,ğ‘â€²)=1], and show that eq. (5) does not hold.
Though the above example shows that weak stochastic transitivity is not sufficient.5Below we show a
popular weakly stochastically transitive model in Braverman and Mossel [5]has uniform dominance
as in lemma 4.2.
Example C.4. LetÎ˜be the set of rankings on Aandğœ‚ > 0be a parameter. Given a uniformly
distributed reference ranking ğœƒâˆˆÎ˜, the noise ranking model [5] ensures that for all ğœƒ(ğ‘)>ğœƒ(ğ‘â€²)
Pr[ğ‘‡ğœƒ(ğ‘,ğ‘â€²)=1]=1
2+ğœ‚
Note that the above model does not satisfy the strict inequality in definition 2.1, but by direct
computation, Pr[ğ‘†(ğ‘â€²â€²,ğ‘â€²)=1|ğ‘†(ğ‘,ğ‘â€²)=1]=1
2
1+4ğ›¾2
3
andPr[ğ‘†(ğ‘â€²â€²,ğ‘)=1|ğ‘†(ğ‘,ğ‘â€²)=1]=
1
2
1âˆ’4ğ›¾2
3
, which satisfies lemma 4.2.
5In the above example, we can also decrease 0.9to a smaller number that satisfies both uniform dominance
and weak stochastic transitivity.
18C.3 Symmetrically strongly truthful from uniform dominance
Proof of lemma 4.3. Supposeğ‘†ğ‘–=1. Because Pr[ğ‘†ğ‘—=1|ğ‘†ğ‘–=1]>Pr[ğ‘†ğ‘˜=1|ğ‘†ğ‘–=1],Pr[ğ‘†ğ‘—=
âˆ’1|ğ‘†ğ‘–=1]<Pr[ğ‘†ğ‘˜=âˆ’1|ğ‘†ğ‘–=1]. Therefore, arg maxË†ğ‘ âˆˆ{âˆ’1,1}Pr[ğ‘†ğ‘—=Ë†ğ‘ |ğ‘†ğ‘–=1]âˆ’Pr[ğ‘†ğ‘˜=Ë†ğ‘ ğ‘–|ğ‘†ğ‘–=
1]=1. Identical argument holds for the case of ğ‘†ğ‘–=âˆ’1which completes the proof.
Additionally, the expected payment of truth-telling is
E
ğ‘ˆğµğ‘ƒğ‘ƒ(ğ‘†ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)
=âˆ‘ï¸
ğ‘Pr[ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ‘ï¸
ğ‘ ğ‘—,ğ‘ ğ‘˜Pr[ğ‘†ğ‘—=ğ‘ ğ‘—,ğ‘†ğ‘˜=ğ‘ ğ‘˜|ğ‘†ğ‘–=ğ‘ ğ‘–]ğ‘ˆğµğ‘ƒğ‘ƒ(ğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜)
=2âˆ‘ï¸
ğ‘Pr[ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ‘ï¸
ğ‘ ğ‘—,ğ‘ ğ‘˜Pr[ğ‘†ğ‘—=ğ‘ ğ‘—,ğ‘†ğ‘˜=ğ‘ ğ‘˜|ğ‘†ğ‘–=ğ‘ ğ‘–](1[ğ‘ ğ‘–=ğ‘ ğ‘˜]âˆ’1[ğ‘ ğ‘–=ğ‘ ğ‘˜])
=2âˆ‘ï¸
ğ‘Pr[ğ‘†ğ‘–=ğ‘ ğ‘–] Pr[ğ‘†ğ‘—=ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]
>0
The last inequality holds due to definition 4.1. â–¡
Proof of lemma 4.4. Asğœis uninformed, let ğœ‡(ğ‘ )=ğœ(ğ‘ ,ğ‘ )=ğœ(âˆ’ğ‘ ,ğ‘ )andğœ‡(âˆ’ğ‘ )=ğœ(ğ‘ ,âˆ’ğ‘ )=
ğœ(âˆ’ğ‘ ,âˆ’ğ‘ )for allğ‘ .
E
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=âˆ‘ï¸
Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜ğœ‡(Ë†ğ‘ ğ‘—)ğœ‡(Ë†ğ‘ ğ‘˜)ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)=âˆ‘ï¸
Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜ğœ‡(Ë†ğ‘ ğ‘—)ğœ‡(Ë†ğ‘ ğ‘˜)(Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘—âˆ’Ë†ğ‘ ğ‘–Ë†ğ‘ ğ‘˜)=0
The first equality holds as the reports are independent of signals. â–¡
Proof of lemma 4.5.
Eğ‘ƒ,ğœ
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=âˆ‘ï¸
ğ‘ ğ‘—,ğ‘ ğ‘˜,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜Pr[ğ‘†ğ‘—=ğ‘ ğ‘—,ğ‘†ğ‘˜=ğ‘ ğ‘˜|ğ‘†ğ‘–=ğ‘ ğ‘–]ğœ(ğ‘ ğ‘—,Ë†ğ‘ ğ‘—)ğœ(ğ‘ ğ‘˜,Ë†ğ‘ ğ‘˜)ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜)
=2âˆ‘ï¸
ğ‘ ğ‘—,ğ‘ ğ‘˜,Ë†ğ‘ ğ‘—,Ë†ğ‘ ğ‘˜Pr[ğ‘†ğ‘—=ğ‘ ğ‘—,ğ‘†ğ‘˜=ğ‘ ğ‘˜|ğ‘†ğ‘–=ğ‘ ğ‘–]ğœ(ğ‘ ğ‘—,Ë†ğ‘ ğ‘—)ğœ(ğ‘ ğ‘˜,Ë†ğ‘ ğ‘˜) 1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘—]âˆ’1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘˜]
(by eq. (1))
=2âˆ‘ï¸
ğ‘ ğ‘—,Ë†ğ‘ ğ‘—Pr[ğ‘†ğ‘—=ğ‘ ğ‘—|ğ‘†ğ‘–=ğ‘ ğ‘–]ğœ(ğ‘ ğ‘—,Ë†ğ‘ ğ‘—)1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘—]âˆ’2âˆ‘ï¸
ğ‘ ğ‘˜,Ë†ğ‘ ğ‘˜Pr[ğ‘†ğ‘˜=ğ‘ ğ‘˜|ğ‘†ğ‘–=ğ‘ ğ‘–]ğœ(ğ‘ ğ‘˜,Ë†ğ‘ ğ‘˜)1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ğ‘˜]
=2âˆ‘ï¸
ğ‘ ,Ë†ğ‘ (Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–])ğœ(ğ‘ ,Ë†ğ‘ )1[Ë†ğ‘ ğ‘–=Ë†ğ‘ ](renaming dummy variables)
=2âˆ‘ï¸
ğ‘ (Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–])ğœ(ğ‘ ,Ë†ğ‘ ğ‘–)
Letğ›¿=Pr[ğ‘†ğ‘—=ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]>0, becauseğ‘†ğ‘—uniformly dominates ğ‘†ğ‘˜
forğ‘†ğ‘–. Additionally, Pr[ğ‘†ğ‘—=âˆ’ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=âˆ’ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]=1âˆ’Pr[ğ‘†ğ‘—=ğ‘ ğ‘–|ğ‘†ğ‘–=
ğ‘ ğ‘–]âˆ’1+Pr[ğ‘†ğ‘˜=ğ‘ ğ‘–|ğ‘†ğ‘–=ğ‘ ğ‘–]=âˆ’ğ›¿. We have
Eğ‘ƒ,ğœ
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=2âˆ‘ï¸
ğ‘ (Pr[ğ‘†ğ‘—=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–]âˆ’Pr[ğ‘†ğ‘˜=ğ‘ |ğ‘†ğ‘–=ğ‘ ğ‘–])ğœ(ğ‘ ,Ë†ğ‘ ğ‘–)
=2ğ›¿(ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)âˆ’ğœ(âˆ’ğ‘,Ë†ğ‘ ğ‘–)),
soarg maxË†ğ‘ ğ‘–âˆˆ{âˆ’1,1}Eğ‘ƒ,ğœ
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,Ë†ğ‘†ğ‘—,Ë†ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=arg maxË†ğ‘ ğ‘–âˆˆ{âˆ’1,1}{ğœ(ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)âˆ’ğœ(âˆ’ğ‘ ğ‘–,Ë†ğ‘ ğ‘–)}
which completes the proof. â–¡
D Proofs in Section 5.1
Before diving into the proof, we introduce some notations. We further introduce Ising models with
bias parameter ğœ¶âˆˆRğ‘‰
â‰¥0in addition to ğœ·where
ğ»(s)=âˆ‘ï¸
ğ‘–,ğ‘—âˆˆğ‘‰ğ›½ğ‘–,ğ‘—ğ‘ ğ‘–ğ‘ ğ‘—+âˆ‘ï¸
ğ‘–âˆˆğ‘‰ğ›¼ğ‘–ğ‘ ğ‘–
19andPrğœ¶,ğœ·[S=s]âˆexp(ğ»(s)), for all configuration s. Givenğ‘–âˆˆğ‘‰, let the expectation and ratio be
ğœˆğ‘–(ğœ¶,ğœ·)=Eğœ¶,ğœ·[ğ‘†ğ‘–]=Pr
ğœ¶,ğœ·[ğ‘†ğ‘–=1]âˆ’Pr
ğœ¶,ğœ·[ğ‘†ğ‘–=âˆ’1]andğœŒğ‘–(ğœ¶,ğœ·)=Prğœ¶,ğœ·[ğ‘†ğ‘–=1]
Prğœ¶,ğœ·[ğ‘†ğ‘–=âˆ’1]
respectively which are monotone to each other. We will omit ğœ¶,ğœ·when clear. Given a subset ğ‘ˆâŠ†ğ‘‰,
sğ‘ˆâˆˆ{âˆ’ 1,1}ğ‘ˆis a configuration over the nodes in ğ‘ˆ, and sğ‘ˆ=1ifğ‘¥ğœ„=1for allğœ„âˆˆğ‘ˆ. We write
Pr[Â·|Sğ‘ˆ=sğ‘ˆ],ğœˆğ‘–|Sğ‘ˆ=sğ‘ˆ, andğœŒğ‘–|Sğ‘ˆ=sğ‘ˆfor the conditional probability, expectation and ratio when the
configuration in ğ‘ˆis fixed as specified by sğ‘ˆ.
A lower bound for LHS Informally, we want to lower bound the correlation between adjacent ğ‘–and
ğ‘—(friends). Note that as we remove edges (setting coordinates of ğœ·to zeros), the correlation should
decrease, and the smallest correlation between neighboring nodes ğ‘–andğ‘—happens when ğ¸={(ğ‘–,ğ‘—)}.
Lemma D.2 formalizes this idea using the following monotone inequality [44, Theorem 17.2].
Theorem D.1 (Griffithsâ€™ inequality) .For anyğ‘–âˆˆğ‘‰,ğœˆğ‘–(ğœ¶,ğœ·)=Eğœ¶,ğœ·[ğ‘†ğ‘–]is non-negative and
non-decreasing in all ğ›½ğ‘—,ğ‘˜â‰¥0andğ›¼ğ‘—â‰¥0withğ‘—,ğ‘˜âˆˆğ‘‰.
Lemma D.2. Givenğ‘‰andğ‘–,ğ‘—âˆˆğ‘‰, for all ğœ¶,ğœ·, and ğœ·â€², ifğ›½â€²
ğ‘’=ğ›½ğ‘’whenğ‘’=(ğ‘–,ğ‘—)andğ›½â€²
ğ‘’=0
otherwise, we have
ğœˆğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·)â‰¥ğœˆğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·â€²)andğœŒğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·)â‰¥ğœŒğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·â€²).
Proof. First, note that we can write the conditional expectation Eğœ¶,ğœ·
ğ‘†ğ‘–|ğ‘†ğ‘—=1as marginal
expectation. Formally, consider ğœ¶ğœ‚so thatğ›¼ğœ‚
ğœ„=ğ›¼ğœ„ifğœ„â‰ ğ‘—andğ›¼ğœ‚
ğ‘—=ğ›¼ğ‘—+ğœ‚. Because
ğœ‚â†’ğœ¶ğœ‚is non-decreasing, ğœ‚â†’ğœˆğ‘–(ğœ¶ğœ‚,ğœ·)is non-decreasing by theorem D.1. In addition,
Prğœ¶ğœ‚,ğœ·[ğ‘†ğ‘–|ğ‘†ğ‘—=ğ‘ ]=Prğœ¶,ğœ·[ğ‘†ğ‘–|ğ‘†ğ‘—=ğ‘ ]for allğ‘ , and Prğœ¶ğœ‚,ğœ·[ğ‘†ğ‘—=âˆ’1]=ğ‘‚(ğ‘’âˆ’2ğœ‚), so
ğœˆğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·)=Eğœ¶,ğœ·[ğ‘†ğ‘–|ğ‘†ğ‘—=1]=lim
ğœ‚â†’+âˆğœˆğ‘–(ğœ¶ğœ‚,ğœ·).
Similarly,
ğœˆğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·â€²)=Eğœ¶,ğœ·â€²[ğ‘†ğ‘–|ğ‘†ğ‘—=1]=lim
ğœ‚â†’+âˆğœˆğ‘–(ğœ¶ğœ‚,ğœ·â€²).
On the other hand, consider ğœ·ğœ†so thatğ›½ğœ†
ğ‘’=ğ›½ğ‘’ifğ‘’â‰ (ğ‘–,ğ‘—)andğ›½ğœ†
ğ‘–,ğ‘—=ğ›½ğ‘–,ğ‘—+ğœ†. By theorem D.1,
ğœˆğ‘–(ğœ¶ğœ‚,ğœ·ğœ†)is non-decreasing in ğœ†for allğœ‚. Because ğœ·0=ğœ·â€²andğœ·1=ğœ·, we have
ğœˆğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·â€²)=lim
ğœ‚â†’+âˆğœˆğ‘–(ğœ¶ğœ‚,ğœ·â€²)â‰¤ lim
ğœ‚â†’+âˆğœˆğ‘–(ğœ¶ğœ‚,ğœ·)=ğœˆğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·)
BecauseğœŒğ‘–=1+ğœˆğ‘–
1âˆ’ğœˆğ‘–is monotone in ğœˆğ‘–, the second part follows. â–¡
Given ğœ·â€²defined in lemma D.2, by some direct computation with ğœ¶=0
ğœŒğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·)â‰¥ğœŒğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·â€²)=ğ‘’2ğ›¼ğ‘–+2ğ›½ğ‘–,ğ‘—=ğ‘’2ğ›½. (6)
An upper bound for RHS Now, we need to upper bound the correlation between non-adjacent
ğ‘–andğ‘˜(non-friends). We will use Weitzâ€™s self-avoiding walks reduction [ 65] to upper bound the
correlation on general graph ğºby the correlation on trees.
Given a general graph ğº, and an arbitrary node ğ‘–, we can construct the Self Avoiding Walk Tree
ofğºrooted atğ‘–, denotedğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–), so that Pr[ğ‘†ğ‘–=1|Sğ‘ˆ=sğ‘ˆ]is the same in ğºas in the tree.
We outline the construction. ğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)enumerates all self-avoiding walks in ğºstarting atğ‘–which
terminates when it revisits a previous node (closes a cycle). Then, ğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)introduces a leaf with
a certain boundary condition. The self-avoiding walk never revisits a node immediately, so there all
the leaves with fixed boundary conditions are at least three hops away from node ğ‘–. Note that if ğºhas
maximum degree ğ‘‘,ğ‘‡ğ‘†ğ´ğ‘Š is ağ‘‘-ary tree.
Theorem D.3 ([65]) .For any ğœ¶,ğœ·, nodeğ‘–âˆˆğ‘‰, and configuration sğ‘ˆonğ‘ˆâŠ‚ğ‘‰,
Pr
ğœ¶,ğœ·[ğ‘†ğ‘–=1|Sğ‘ˆ=sğ‘ˆ]= Pr
ğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)[ğ‘†ğ‘–=1|Sğ‘ˆ=sğ‘ˆ].
20First, with the above theorem, we have ğœˆğ‘–|ğ‘†ğ‘˜=1(ğœ¶,ğœ·)=Eğœ¶,ğœ·[ğ‘†ğ‘–|ğ‘†ğ‘˜=1]=Eğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)[ğ‘†ğ‘–|ğ‘†ğ‘˜=1].
By the monotone property in theorem D.1, setting all two-hop neighbors ğ‘ˆinğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)to1
(recalled that any boundary conditions for ğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)being at least three hops away) increases the
conditional expectation,
Eğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)[ğ‘†ğ‘–|ğ‘†ğ‘˜=1]â‰¤Eğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)[ğ‘†ğ‘–|Sğ‘ˆ=1,ğ‘†ğ‘˜=1].
Letğ‘‡be the tree by truncating ğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)at level 2. By the Markov property of Ising models, the
expectation is equal to the expectation on ğ‘‡.
Eğœ¶,ğœ·[ğ‘†ğ‘–|ğ‘†ğ‘˜=1]â‰¤Eğ‘‡ğ‘†ğ´ğ‘Š(ğº,ğ‘–)[ğ‘†ğ‘–|Sğ‘ˆ=1]=Eğ‘‡[ğ‘†ğ‘–|Sğ‘ˆ=1]. (7)
Finally, we can recursively compute the probability ratio ğœŒğ‘–(and thus expectation ğœˆğ‘–) on trees.
Specifically, given a rooted tree ğ‘‡â€², we defineğœŒğ‘‡â€²as the ratio of probabilities for the root to be
+1andâˆ’1respectively, and ğœŒğ‘‡â€²|Sğ‘ˆ=sğ‘ˆfor the ratio of conditional probabilities. As stated in the
following lemma, it is well known (see, for example, [ 22]) that the ratio of each node can be computed
recursively over the childrenâ€™s ratio.
Lemma D.4. Given a tree ğ‘‡rooted atğ‘–with parameter(ğœ¶,ğœ·)and boundary condition sğ‘ˆ,
ğœŒğ‘‡|Sğ‘ˆ=sğ‘ˆ=ğ‘’2ğ›¼ğ‘–ğ‘‘Ã–
ğ‘™=1ğœŒğ‘‡ğ‘™|Sğ‘ˆ=sğ‘ˆğ‘’2ğ›½ğ‘–,ğ‘—ğ‘™+1
ğ‘’2ğ›½ğ‘–,ğ‘—ğ‘™+ğœŒğ‘‡ğ‘™|Sğ‘ˆ=sğ‘ˆ
whereğ‘—1,...,ğ‘—ğ‘‘are children of ğ‘–andğ‘‡ğ‘™is the subtree rooted at ğ‘—ğ‘™for allğ‘™.
By the monotone property in theorem D.1, the maximum of right-hand side of eq. (7) happens when
ğ‘‡is a complete ğ‘‘-ary tree with ğœ·=ğ›½. Therefore,
ğœŒğ‘–|ğ‘†ğ‘˜=1(ğœ¶,ğœ·)â‰¤ 
ğ‘’2(ğ‘‘+1)ğ›½+1
ğ‘’2ğ›½+ğ‘’2ğ‘‘ğ›½!ğ‘‘
. (8)
Finally, with eqs. (6) and (8), we have ğœŒğ‘–|ğ‘†ğ‘—=1(ğœ¶,ğœ·)â‰¥ğ‘’2ğ›½â‰¥
ğ‘’2(ğ‘‘+1)ğ›½+1
ğ‘’2ğ›½+ğ‘’2ğ‘‘ğ›½ğ‘‘
â‰¥ğœŒğ‘–|ğ‘†ğ‘˜=1(ğœ¶,ğœ·)which
implies eq. (4).
Remark D.5. Note that for any graph ğºthere exists small enough ğ›½,ğ›½so that the condition in
theorem 5.1 is satisfied, because the inequality become equality when ğ›½=ğ›½=0, andğœ•
ğœ•ğ›½2ğ›½
ğ‘‘>0=
ğœ•
ğœ•ğ›½lnğ‘’2(ğ‘‘+1)ğ›½+1
ğ‘’2ğ›½+ğ‘’2ğ‘‘ğ›½.
The bound between ğœ·andğ‘‘is necessary as shown in fig. 3. On the other hand, by the Markov
property of the Ising model, the majority of all neighborâ€™s signals is a sufficient statistic, and we
can show the majority of all neighborâ€™s signals are uniformly dominant to a non-neighborâ€™s signal.
Therefore, we can get a symmetrically strongly truthful mechanism by replacing ğ‘—â€™s reports with the
majority of reports from ğ‘–â€™s neighbors.
E Proof of Theorem 5.3
The sufficient condition is done by lemma 4.3, because
arg max
Ë†ğ‘ ğ‘–âˆˆ{âˆ’1,1}E
ğœ†ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)+ğœ‡(ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=arg max
Ë†ğ‘ ğ‘–âˆˆ{âˆ’1,1}E
ğœ†ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
=arg max
Ë†ğ‘ ğ‘–âˆˆ{âˆ’1,1}E
ğ‘ˆğµğ‘ƒğ‘ƒ(Ë†ğ‘ ğ‘–,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=ğ‘ ğ‘–
(ğœ†>0)
=ğ‘ ğ‘– (by lemma 4.3)
For the necessary, given ğ‘ˆ, defineğ·(ğ‘ ğ‘—,ğ‘ ğ‘˜)=1
2 ğ‘ˆ(1,ğ‘ ğ‘—,ğ‘ ğ‘˜)âˆ’ğ‘ˆ(âˆ’1,ğ‘ ğ‘—,ğ‘ ğ‘˜)andğœ‡(ğ‘ ğ‘—,ğ‘ ğ‘˜)=
1
2(ğ‘ˆ(1,ğ‘ ğ‘—,ğ‘ ğ‘˜)+ğ‘ˆ(âˆ’1,ğ‘ ğ‘—,ğ‘ ğ‘˜))for allğ‘ ğ‘—andğ‘ ğ‘˜in{âˆ’1,1}. Hence
ğ‘ˆ(ğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜)=ğ‘ ğ‘–Â·ğ·(ğ‘ ğ‘—,ğ‘ ğ‘˜)+ğœ‡(ğ‘ ğ‘—,ğ‘ ğ‘˜),âˆ€ğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜âˆˆ{âˆ’1,1} (9)
21Figure 3: As fixing any ğ›½,ğ›½, we can construct a simple graph with ğ‘‰={ğ‘£0,...,ğ‘£ğ‘›âˆ’1}andğ¸=
{(ğ‘£0,ğ‘£ğ‘™),(ğ‘£ğ‘™,ğ‘£ğ‘›âˆ’1):ğ‘™=1,...,ğ‘›âˆ’2}where agent ğ‘£0andğ‘£ğ‘›âˆ’1are not connected but share ğ‘›âˆ’2
common friends. We can show that the correlation between ğ‘†0andğ‘†ğ‘›âˆ’1converge to 1as the number
of common friends ğ‘‘increases, while the correlation between ğ‘†0andğ‘†1is bounded away from 1.
Given a joint distribution satisfying definition 4.1, we let ğ‘ğ‘ ğ‘–(ğ‘ ğ‘—,ğ‘ ğ‘˜)=Pr[ğ‘†ğ‘—=ğ‘ ğ‘—,ğ‘†ğ‘˜=ğ‘ ğ‘˜|ğ‘†ğ‘–=ğ‘ ğ‘–]
and additionally write ğ‘ğ‘ ğ‘–=
ğ‘ğ‘ ğ‘–(1,1)ğ‘ğ‘ ğ‘–(1,âˆ’1)
ğ‘ğ‘ ğ‘–(âˆ’1,1)ğ‘ğ‘ ğ‘–(âˆ’1,âˆ’1)
. Then definition 4.1 ensures that
ğ‘1(1,âˆ’1)> ğ‘1(âˆ’1,1)andğ‘âˆ’1(1,âˆ’1)< ğ‘âˆ’1(âˆ’1,1).
Becauseğ‘ˆis truthful for all uniformly dominant tuples, we have
0<E
ğ‘ˆ(1,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=1
âˆ’E
ğ‘ˆ(âˆ’1,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=1
=2âˆ‘ï¸
ğ‘ ğ‘—,ğ‘ ğ‘˜ğ·(ğ‘ ğ‘—,ğ‘ ğ‘˜)ğ‘1(ğ‘ ğ‘–,ğ‘ ğ‘—)
0>E
ğ‘ˆ(1,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=âˆ’1
âˆ’E
ğ‘ˆ(âˆ’1,ğ‘†ğ‘—,ğ‘†ğ‘˜)|ğ‘†ğ‘–=âˆ’1
=2âˆ‘ï¸
ğ‘ ğ‘—,ğ‘ ğ‘˜ğ·(ğ‘ ğ‘—,ğ‘ ğ‘˜)ğ‘âˆ’1(ğ‘ ğ‘–,ğ‘ ğ‘—).(10)
Suppose the following are true
ğ·(1,âˆ’1)=âˆ’ğ·(âˆ’1,1)>0 (11)
ğ·(1,1)=ğ·(âˆ’1,âˆ’1)=0 (12)
Letğœ†=ğ·(1,âˆ’1)>0. By eqs. (11) and (12), we have
ğ‘ˆ(ğ‘ ğ‘–,ğ‘ ğ‘—,ğ‘ ğ‘˜)=ğ‘ ğ‘–Â·ğ·(ğ‘ ğ‘—,ğ‘ ğ‘˜)+ğœ‡(ğ‘ ğ‘—,ğ‘ ğ‘˜) (by eq. (9))
=ğœ†Â·ğ‘ ğ‘–(ğ‘ ğ‘—âˆ’ğ‘ ğ‘˜)+ğœ‡(ğ‘ ğ‘—,ğ‘ ğ‘˜) (by eqs. (9) and (11))
which completes the proof. Thus, we will construct three joint distributions satisfying definition 4.1
to prove eqs. (11) and (12).
The first joint distribution ğ‘ğ‘ ğ‘–
1(ğ‘ ğ‘—,ğ‘ ğ‘˜)with 0<ğ›¿â‰¤1/2
ğ‘1=
0 1/2+ğ›¿
1/2âˆ’ğ›¿ 0
andğ‘âˆ’1=
0 1/2âˆ’ğ›¿
1/2+ğ›¿ 0
.
Then eq. (10) on the first distribution reduces to
0<ğ·(1,âˆ’1)ğ‘1
1(1,âˆ’1)+ğ·(âˆ’1,1)ğ‘1
1(âˆ’1,1)=1
2(ğ·(1,âˆ’1)+ğ·(âˆ’1,1))+ğ›¿(ğ·(1,âˆ’1)âˆ’ğ·(âˆ’1,1))
0>ğ·(1,âˆ’1)ğ‘âˆ’1
1(1,âˆ’1)+ğ·(âˆ’1,1)ğ‘âˆ’1
1(âˆ’1,1)=1
2(ğ·(1,âˆ’1)+ğ·(âˆ’1,1))âˆ’ğ›¿(ğ·(1,âˆ’1)âˆ’ğ·(âˆ’1,1)).
As we takeğ›¿to zero, we prove ğ·(1,âˆ’1)=âˆ’ğ·(âˆ’1,1). Then plugging in with nonzero ğ›¿, we have
ğ·(1,âˆ’1)>0and complete the proof of eq. (11).
22The second joint distribution ğ‘ğ‘ ğ‘–
2(ğ‘ ğ‘—,ğ‘ ğ‘˜)with 0â‰¤ğœ–â‰¤1is
ğ‘1=
1âˆ’ğœ–3
4ğœ–
ğœ–
40
andğ‘âˆ’1=1âˆ’ğœ–ğœ–
43ğœ–
40
.
With eq. (11), eq. (10) reduces to
0<(1âˆ’ğœ–)ğ·(1,1)+ğœ–
4(ğ·(1,âˆ’1)âˆ’ğ·(âˆ’1,1))
0>(1âˆ’ğœ–)ğ·(1,1)âˆ’ğœ–
4(ğ·(1,âˆ’1)âˆ’ğ·(âˆ’1,1)).
By takingğœ–to zero, we prove ğ·(1,1)=0. We can prove ğ·(âˆ’1,âˆ’1)=0using the similar trick and
complete the proof of eq. (12).
F Additional empirical results
F.1 Comparison data
Here we test if the dataset satisfy transitivity property. We denote the proportion of rankings such that
itemğ‘is higher than item ğ‘â€²in the dataset by ğ‘ğ‘>ğ‘â€². Ifğ‘ğ‘>ğ‘â€²>1/2,ğ‘ğ‘â€²>ğ‘â€²â€²>1/2, andğ‘ğ‘>ğ‘â€²â€²>1/2,
we say the triple of items {ğ‘,ğ‘â€²,ğ‘â€²â€²}empirically satisfies transitivity. If ğ‘ğ‘>ğ‘â€²>1/2,ğ‘ğ‘â€²>ğ‘â€²â€²>1/2,
andğ‘ğ‘>ğ‘â€²â€²>max{ğ‘ğ‘>ğ‘â€²,ğ‘ğ‘â€²>ğ‘â€²â€²}, we say the triple of items {ğ‘,ğ‘â€²,ğ‘â€²â€²}empirically satisfies strong
transitivity. We first test the transitivity of the SUSHI subdataset selected in section 6.1. We find that
100% of the item triples empirically satisfy transitivity, and 69.17% of the item triples empirically
satisfy strong transitivity. This suggests that our transitivity assumption for the comparison data is
mostly aligned.
Moreover, we conducted an experiment on the entire SUSHI dataset without any selection criteria
and demonstrated the results in fig. 4. Observe that the ECDF of payments from original human users
also dominates the payments under the uninformed strategy and the unilateral deviating strategy. This
is consistent with our experimental results in section 6.1. However, there are two minor difference.
First the separation of truth-telling from the other two is slightly less prominent than fig. 1 with
the selection criteria. This may be due to a slightly lower degree of transitivity across agents with
different backgrounds. In particular, we found the average value of ğ‘ğ‘>ğ‘â€²â€²âˆ’max{ğ‘ğ‘>ğ‘â€²,ğ‘ğ‘â€²>ğ‘â€²â€²}is
0.0559 without the selection criteria which is less than 0.0604 with the selection criteria in fig. 1.
Second, the fraction of agents receiving positive payments is slightly higher than in fig. 1 ( 0.785
and0.763respectively). This aligned with or empirical (strong) transitively which are 1and0.7667
compared to the above 1and0.69117 . Furthermore, we also conducted experiments on other groups
of users by changing the selection criteria. Those interested can refer to fig. 5, fig. 6 and table 1 for
the results, which further verify the effectiveness of our mechanism.
Selection criteria Number of users Average utility Fraction of positive utility
All (No selection) 5000 0 .138 78 .5%
Female, 30-49, Kanto/Shizuoka 249 0 .137 76 .3%
Male, 30-49, Kanto/Shizuoka 185 0 .167 82 .2%
Female, 5-29, Kanto/Shizuoka 146 0 .175 84 .2%
Female, 50+, Kanto/Shizuoka 26 0 .13 80 .8%
Female, 30-49, Tohoku 30 0 .174 83 .3%
Female, 30-49, Hokuriku 23 0 .105 69 .6%
Table 1: Summary of truth-telling utility in appendix F.1.
F.2 Networked data
Alongside fig. 2, Figure 7 and table 2 present empirical results for the top five popular artists in the
dataset, excluding Lady Gaga, who are Britney Spears, Rihanna, The Beatles, and Katy Perry. All
these settings show similar results. However, the Beatlesâ€™ data is less conclusive as the payment
distribution under the uninformed strategy profile is close to the truth-telling. This observation is
23Figure 4: ECDF comparisons on all users without any selection.
also documented in Daskalakis et al. [12] which notes that the Ising model performs much better for
rock artists than for pop artists. The authors conjecture that this may be due to the highly divisive
popularity of pop artists like Lady Gaga and Britney Spears, whose listeners may form dense cliques
within the graph.
Note that there is a buck of agent with a payment of around 0.5under the truth-telling. This is because
many non-listeners have no listener friends, and payment is 1âˆ’[(1âˆ’ğ‘)âˆ’ğ‘]=2ğ‘is twice the
popularityğ‘â‰ˆ0.25. Moreover, the jump is most minor for the Beatles, and indicates less agreement
between non-listeners. Additionally, by the definition of bonus-penalty payment, we can see the
payment of deviation is the minus of the truthful payment, so that the ECDF is symmetric around
(0,0.5).
Artists Fraction of listener Average utility Fraction of positive utility
Lady Gaga 32.2% 0 .37 76%
Britney Spears 27.6% 0 .420 82 .6%
Rihanna 25.6% 0 .422 83 .4%
The Beatles 25.4% 0 .137 68 .5%
Katy Perry 25.0% 0 .361 79 .9%
Table 2: Summary of truth-telling utility in appendix F.2.
Figure 8 further shows the scatter plot of average payment and fraction of agents with positive
payments across the top fifty popular artists where all settings have more than 60% percent of agents
get positive payment. However, for less popular artists, the performance of our mechanism declines.
This is expected, as we cannot provide effective incentives when only one agent listens to an artist.
24Figure 5: In each of the rows, we present the ECDF comparisons after changing the selection criteria
for the user group as follows: from female to male, from ages 30â€“49 to ages 5â€“29, from ages 30â€“49
to ages 50+, respectively.
25Figure 6: In each of the rows, we present the ECDF comparisons after changing the location criteria
for the user group as follows: from mostly living in Kanto or Shizuoka to Tohoku until age 15, and
from mostly living in Kanto or Shizuoka to Hokuriku until age 15, respectively.
26Figure 7: Last.fm dataset for other top five popular artists excluding Lady Gaga.
27Figure 8: Average payment and fraction of positive payment under the truth-telling across top fifty
popular artists.
28NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
â€¢ You should answer [Yes] , [No] , or [NA] .
â€¢[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
â€¢ Please provide a short (1â€“2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
â€¢Delete this instruction block, but keep the section heading â€œNeurIPS paper checklist" ,
â€¢Keep the checklist subsection headings, questions/answers and guidelines below.
â€¢Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: Our claims accurately reflect the contributions and scope of the paper.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In section 7, we discuss potential future research directions, which are the
limitations of our current work.
29Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: We present all the assumptions. The complete proofs are provided in the
appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: The code is uploaded in the supplementary material. All the information
required to reproduce the experimental results is provided.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
30â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [Yes]
Justification: The code is uploaded in the supplementary material.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
31â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: Most of these details are explained in section 6 and in the provided code.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [No]
Justification: We believe the error bars are not relevant to our empirical metrics.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: We believe the computer resources are not relevant to our main contributions.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
32â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research conducted in our paper conforms with the NeurIPS Code of
Ethics.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: Our work contributes to the theory of information elicitation. We discussed
the applicability and limitations for elicitation settings.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: We believe this paper poses no such risks.
33Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The datasets used in this paper are mentioned with URLs and the licenses and
terms of use are properly respected.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
34Answer: [NA]
Justification: The paper does not involve crowdsourcing or research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing or research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
35