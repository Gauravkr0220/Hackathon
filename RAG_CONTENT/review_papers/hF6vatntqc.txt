Transformers are Minimax Optimal
Nonparametric In-Context Learners
Juno Kim1,2âˆ—Tai Nakamaki1Taiji Suzuki1,2
1University of Tokyo2Center for Advanced Intelligence Project, RIKEN
âˆ—junokim@g.ecc.u-tokyo.ac.jp
Abstract
In-context learning (ICL) of large language models has proven to be a surprisingly
effective method of learning a new task from only a few demonstrative exam-
ples. In this paper, we study the efficacy of ICL from the viewpoint of statistical
learning theory. We develop approximation and generalization error bounds for
a transformer composed of a deep neural network and one linear attention layer,
pretrained on nonparametric regression tasks sampled from general function spaces
including the Besov space and piecewise Î³-smooth class. We show that sufficiently
trained transformers can achieve â€“ and even improve upon â€“ the minimax optimal
estimation risk in context by encoding the most relevant basis representations
during pretraining. Our analysis extends to high-dimensional or sequential data
and distinguishes the pretraining andin-context generalization gaps. Furthermore,
we establish information-theoretic lower bounds for meta-learners w.r.t. both the
number of tasks and in-context examples. These findings shed light on the roles of
task diversity and representation learning for ICL.
1 Introduction
Large language models (LLMs) have demonstrated remarkable capabilities in understanding and
generating natural language data. In particular, the phenomenon of in-context learning (ICL) has
recently garnered widespread attention. ICL refers to the ability of pretrained LLMs to perform a new
task by being provided with a few examples within the context of a prompt, without any parameter
updates or fine-tuning. It has been empirically observed that few-shot prompting is especially effective
in large-scale models (Brown et al., 2020) and requires only a couple of examples to consistently
achieve high performance (GarcÃ­a et al., 2023). In contrast, Raventos et al. (2023) demonstrate that
sufficient pretraining task diversity is required for the emergence of ICL. However, we still lack a
comprehensive understanding of the statistical foundations of ICL and few-shot prompting.
A vigorous line of research has been directed towards understanding ICL of single-layer linear
attention models pretrained on the query prediction loss of linear regression tasks (Garg et al., 2022;
AkyÃ¼rek et al., 2023; Zhang et al., 2023; Ahn et al., 2023; Mahankali et al., 2023; Wu et al., 2024).
It has been shown that the global minimizer of the L2pretraining loss implements one step of GD
on a least-squares linear regression objective (Mahankali et al., 2023) and is nearly Bayes optimal
(Wu et al., 2024). Moreover, risk bounds with respect to the context length (Zhang et al., 2023) and
number of tasks (Wu et al., 2024) have been obtained.
Other works have examined ICL of more complex multi-layer transformers. Bai et al. (2023); von
Oswald et al. (2023) give specific transformer constructions which simulate GD in context, however
it is unclear how such meta-algorithms may be learned. Another approach is to study learning with
representations , where tasks consist of a fixed nonlinear feature map composed with a varying linear
function. Guo et al. (2023) empirically found that trained transformers exhibit a separation where
lower layers transform the input and upper layers perform linear ICL. Recently, Kim and Suzuki
38th Conference on Neural Information Processing Systems (NeurIPS 2024).(2024) analyzed a model consisting of a shallow neural network followed by a linear attention layer
and proved that the MLP component learns to encode the true features during pretraining. However,
they assumed the infinite task and sample size limit and did not study generalization capabilities.
Our contributions. In this paper, we analyze the optimality of ICL from the perspective of statistical
learning theory. Our object of study is a transformer consisting of a deep neural network with N-
dimensional output followed by one linear attention layer. The model is pretrained on ninput-output
samples from Tnonparametric regression tasks, generated from a suitably decaying distribution on a
general function space. Compared to previous works, we take a crucial step towards understanding
practical multi-layer transformers by incorporating the representation learning capabilities of the
DNN module. From a more abstract perspective, this work can also be situated as a nonlinear
extension of meta-learning. Our contributions are highlighted below.
â€¢We develop a general framework for upper bounding the in-context estimation error of the
empirical risk minimizer in terms of the approximation error of the neural network and separate
in-context andpretraining generalization gaps depending on n, T, respectively.
â€¢In the Besov space setting, we show that ICL achieves nearly minimax optimal risk nâˆ’2Î±
2Î±+d
when Tis sufficiently large. Since LLMs are pretrained on vast amounts of data in practice, T
can be taken to be nearly infinite, justifying the emergence of ICL at large scales. We extend the
optimality guarantees to nearly dimension-free rates in the anisotropic Besov space and also to
learning sequential data with deep transformers in the piecewise Î³-smooth function class.
â€¢We show that ICL can improve upon the a priori optimal rate when the task class basis resides
in a coarser Besov space by learning to encode informative basis representations, emphasizing
the importance of pretraining on diverse tasks.
â€¢We also derive information-theoretic lower bounds for the minimax risk in both n, T, rigorously
confirming that ICL is jointly optimal when Tis large (Besov space setting), while any meta-
learning method is jointly suboptimal when Tis small (coarser space setting). This separation
aligns with empirical observations of a task diversity threshold (Raventos et al., 2023).
The paper is structured as follows. In Section 2, the regression tasks and transformer model are
defined in an abstract setting. In Section 3, we present the general framework for estimating the ICL
approximation and generalization error. In Section 4, we specialize to the Besov-type and piecewise
Î³-smooth class settings and show that transformers can achieve or exceed the minimax optimal rate
in context. In Section 5, we derive minimax lower bounds. All proofs are deferred to the appendix;
moreover, we provide numerical experiments validating our results in Appendix E.
1.1 Other Related Works
Meta-learning. The theoretical setting of ICL is closely related to meta-learning , where the goal
is to infer a shared representation Ïˆâ—¦with samples from a set of transformed tasks Î²âŠ¤
iÏˆâ—¦. When
Ïˆâ—¦is linear, fast rates have been established by Tripuraneni et al. (2020); Du et al. (2021), while
the nonlinear case has been studied by Meunier et al. (2023) where Ïˆâ—¦is a feature projection into
a reproducing kernel Hilbert space. Our results can be viewed as extending this body of work to
function spaces of generalized smoothness with a specific deep transformer architecture.
Optimal rates for DNNs. Our analysis extends established optimality results for classes of DNNs
in ordinary supervised regression settings to ICL. Suzuki (2019) has shown that deep feedforward
networks with the ReLU activation can efficiently approximate functions in the Besov space and thus
achieve the minimax optimal rate. This has been extended to the anisotropic Besov space (Suzuki and
Nitanda, 2021), convolutional neural networks for infinite-dimensional input (Okumoto and Suzuki,
2022), and transformers for sequence-to-sequence functions (Takakura and Suzuki, 2023). We remark
that a work in progress (Imaizumi, 2024) also studies the sample complexity of ICL of transformers
in a Sobolev space setting.
Pretraining dynamics for ICL. While how ICL arises from optimization is not fully understood,
there are encouraging developments in this direction. Zhang et al. (2023) has shown for one layer of
linear attention that running GD on the population risk always converges to the global optimum. This
was extended to incorporate a linear output layer by Zhang et al. (2024), and to softmax attention
2by Huang et al. (2023); Li et al. (2024); Chen et al. (2024). Kim and Suzuki (2024) considered
a compound transformer equivalent to ours with a shallow MLP component and proved that the
loss landscape becomes benign in the mean-field limit, deriving convergence guarantees for the
corresponding gradient dynamics. These analyses indicate that the attention mechanism, while highly
nonconvex, may possess structures favorable for gradient-based optimization.
2 Problem Setup
2.1 Nonparametric Regression
In this paper, we analyze the ability of a transformer to solve nonparametric regression problems in
context when pretrained on examples from a family of regression tasks, which we describe below.
LetX âŠ†Rdbe the input space ( dis allowed to be infinite), PXa probability distribution on X,
and(Ïˆâ—¦
j)âˆ
j=1a fixed countable subset of L2(PX). A regression function Fâ—¦
Î²:X â†’Ris randomly
generated for each task by sampling the sequence of coefficients Î²âˆˆRâˆfrom a distribution PÎ²on
B(Râˆ); the class of tasks Fâ—¦is defined as
Fâ—¦=n
Fâ—¦
Î²=Pâˆ
j=1Î²jÏˆâ—¦
jÎ²âˆˆsuppPÎ²o
,
endowed with the induced distribution. Each task prompt contains nexample input-response pairs
{(xk, yk)}n
k=1. The covariates xkare i.i.d. drawn from PXand the responses are generated as
yk=Fâ—¦
Î²(xk) +Î¾k,1â‰¤kâ‰¤n, (1)
where the noise Î¾kis assumed to be i.i.d. with mean zero and |Î¾k| â‰¤Ïƒalmost surely.1In addition,
we independently generate a query token Ëœxand corresponding output Ëœyin the same manner.
We proceed to state our assumptions for the regression model. Informally, we suppose a relaxed
version of sparsity and orthonormality of Ïˆâ—¦
jand suitable decay rates for the basis expansion. These
will be subsequently verified for specific function spaces of interest with their natural decay rate.
Assumption 1 (relaxed sparsity and orthonormality of basis functions) .ForNâˆˆN, there exist inte-
gersÂ¯N < Â¯Nâ‰²Nwith Â¯Nâˆ’Â¯N+1 = Nsuch that Ïˆâ—¦
Â¯N,Â·Â·Â·, Ïˆâ—¦
Â¯Nare independent and Ïˆâ—¦
1,Â·Â·Â·, Ïˆâ—¦
Â¯Nâˆ’1
are all contained in the linear span of Ïˆâ—¦
Â¯N,Â·Â·Â·, Ïˆâ—¦
Â¯N. Moreover, there exist r, C1, C2, Câˆ>0such
thatÎ£Î¨,N:= 
Exâˆ¼PX[Ïˆâ—¦
j(x)Ïˆâ—¦
k(x)]Â¯N
j,k=Â¯Nsatisfies C1INâª¯Î£Î¨,Nâª¯C2INand
PÂ¯N
j=Â¯N(Ïˆâ—¦
j)21/2
Lâˆ(PX)â‰¤CâˆNr. (2)
Denoting the Â¯N-basis approximation of Fâ—¦
Î²asFâ—¦
Î²,Â¯N:=PÂ¯N
j=1Î²jÏˆâ—¦
j, by Assumption 1 there exist
â€˜aggregatedâ€™ coefficients Â¯Î²
Â¯N,Â·Â·Â·,Â¯Î²Â¯Nuniquely determined by Î²such that Fâ—¦
Î²,Â¯N=PÂ¯N
j=Â¯NÂ¯Î²jÏˆâ—¦
j. We
define two types of coefficient covariance matrices
Î£Î²,Â¯N:= (EÎ²[Î²jÎ²k])Â¯N
j,k=1âˆˆRÂ¯NÃ—Â¯Nand Î£Â¯Î²,N:= 
EÎ²[Â¯Î²jÂ¯Î²k]Â¯N
j,k=Â¯NâˆˆRNÃ—N.
Assumption 2 (decay of Î²).Fors, B > 0it holds that âˆ¥Fâ—¦
Î²âˆ¥Lâˆ(PX)â‰¤Bfor all Fâ—¦
Î²âˆˆ Fâ—¦and
âˆ¥Fâ—¦
Î²âˆ’Fâ—¦
Î²,Nâˆ¥2
L2(PX)â‰²Nâˆ’2s(3)
uniformly over Î²âˆˆsuppPÎ². Furthermore, Tr(Î£ Â¯Î²,N)is bounded for all Nand
0â‰ºÎ£Î²,Â¯Nâ‰¾diagh
(jâˆ’2sâˆ’1(logj)âˆ’2)Â¯N
j=1i
. (4)
Remark 2.1. In the simple case where (Ïˆâ—¦
j)âˆ
j=1is a basis for L2(PX), we may setÂ¯N= 1,Â¯N=N
so that the dependency condition of Assumption 1 is trivially satisfied, moreover, Î£Â¯Î²,N= Î£ Î²,N
and boundedness of Tr(Î£ Â¯Î²,N)automatically follows from (4). However, the assumptions in the
stated form also allow for hierarchical bases with dependencies such as wavelet systems. We
also note that (3)and(4)entail basically the same rate but are not equivalent: the uniform bound
|Î²j|2â‰²jâˆ’2sâˆ’1(logj)âˆ’2along with Assumption 1 implies (3). The (logj)âˆ’2term can be replaced
with any g(j)such thatPâˆ
j=1jâˆ’1g(j)is convergent.
1This implies VarÎ¾kâ‰¤Ïƒ2. We require boundedness since the values ykform part of the prompt input, and
we wish to utilize sup-norm covering number estimates for the attention map; this technicality can be removed
with more careful analysis. However, for the information-theoretic lower bound we assume Gaussian noise.
32.2 In-Context Learning
We now describe our transformer model, which takes ncontext pairs X= (x1,Â·Â·Â·, xn)âˆˆRdÃ—n,
y= (y1,Â·Â·Â·, yn)âŠ¤âˆˆRnand a query token Ëœxas input and returns a prediction for the corresponding
output. The covariates are first passed through a nonlinear representation or feature mapping Ï•:
X â†’RN, which we assume belongs to a sufficiently powerful class of estimators FN. Specifically:
Assumption 3 (expressivity of FN).âˆ¥Ï•(x)âˆ¥2â‰¤Bâ€²
Nfor some Bâ€²
N>0for all xâˆˆ X,Ï•âˆˆ F N.
Moreover for some Î´N>0, there exist Ï•âˆ—
Â¯N,Â·Â·Â·, Ï•âˆ—
Â¯Nâˆˆ FNsatisfying
max
Â¯Nâ‰¤jâ‰¤Â¯Nâˆ¥Ïˆâ—¦
jâˆ’Ï•âˆ—
jâˆ¥Lâˆ(PX)â‰¤Î´N.
By choosing FNandÎ´Nto satisfy the above assumption, we will be able to utilize established
approximation and generalization guarantees for families of deep neural networks in Section 4.
The extracted representations Ï•(X) = (Ï•(x1),Â·Â·Â·, Ï•(xn))are then mapped to a scalar output via a
linear attention layer parametrized by a matrix Î“âˆˆ SNforSNâŠ‚RNÃ—N,
Ë‡fÎ˜(X,y,Ëœx) :=1
nnX
k=1ykÏ•(xk)âŠ¤Î“âŠ¤Ï•(Ëœx) =Î“Ï•(X)y
n, Ï•(Ëœx)
,where Î˜ = (Î“ , Ï•)âˆˆ SNÃ—FN.
Finally, the output is constrained to lie on [âˆ’Â¯B,Â¯B]by applying clip Â¯B(u) := max {min{u,Â¯B},âˆ’Â¯B},
yielding fÎ˜(X,y,Ëœx) := clip Â¯B(Ë‡fÎ˜(X,y,Ëœx)). We set SN={Î“âˆˆRNÃ—N|0âª¯Î“âª¯C3IN}for
some C3>0and fix Â¯B=Bfor simplicity.
The above setup is a restricted reparametrization of linear attention widely used in theoretical analyses
(see e.g. Zhang et al., 2023; Wu et al., 2024, for more details), where the values only refer to yand
the query and key matrices are consolidated into one matrix Î“. The form is equivalent to one step
of GD with matrix step size and has been shown to be optimal for a single layer of linear attention
for linear regression tasks (Ahn et al., 2023; Mahankali et al., 2023). The placement of the attention
layer after the DNN module Ï•is justified by the observation that lower layers of trained transformers
act as data representations on top of which upper layers perform ICL (Guo et al., 2023).
During pretraining, the model is presented with Tprompts {(X(t),y(t),Ëœx(t))}T
t=1where the tasks
Fâ—¦
Î²(t)âˆˆ Fâ—¦,Î²(t)âˆ¼ P Î²and tokens X(t)= (x(t)
1,Â·Â·Â·, x(t)
n),y(t)= (y(t)
1,Â·Â·Â·, y(t)
n)âŠ¤,Ëœx(t)andËœy(t)
are independently generated as described in Section 2.1, and is trained to minimize the empirical risk
bÎ˜ = arg min
Î˜âˆˆSNÃ—FNbR(Î˜),bR(Î˜) =1
TTX
t=1
Ëœy(t)âˆ’fÎ˜(X(t),y(t),Ëœx(t))2
.
Our goal is to verify the efficiency of ICL as a learning algorithm and show that learning the optimal
bÎ˜allows the transformer to solve new random regression problems y=Fâ—¦
Î²(x) +Î¾forFâ—¦
Î²âˆˆ Fâ—¦in
context. To this end, we evaluate the convergence of the mean-squared risk or estimation error,
Â¯R(bÎ˜) := E(X(t),y(t),Ëœx(t),Ëœy(t))T
t=1[R(bÎ˜)], R (Î˜) := EX,y,Ëœx,Î²
(Fâ—¦
Î²(Ëœx)âˆ’fÎ˜(X,y,Ëœx))2
.
Note that we do not study whether the transformer always converges to bÎ˜; the training dynamics of a
DNN is already a very difficult problem. For the attention layer, see the discussion in Section 1.1.
3 Risk Bounds for In-Context Learning
In this section, we outline our framework for analyzing the in-context estimation error Â¯R(bÎ˜). Some
additional definitions are in order. The Ïµ-covering number N(C, Ï, Ïµ)of a metric space Cequipped
with a metric ÏforÏµ >0is defined as the minimal number of balls in Ïwith radius Ïµneeded to
coverC(van der Vaart and Wellner, 1996). The Ïµ-covering entropy ormetric entropy is given as
V(F, Ï, Ïµ) := log N(F, Ï, Ïµ). The Ïµ-packing number M(Ïµ,C, Ï)is given as the maximal cardinality
of aÏµ-separated set {c1, . . . , c M} âŠ† C such that Ï(ci, cj)â‰¥Î´for all iÌ¸=j. The transformer model
class is defined as TN:={fÎ˜|Î˜âˆˆ SNÃ— F N}.
To bound the overall risk, we first decompose into the approximation and generalization gaps.
4Theorem 3.1 (Schmidt-Hieber (2020), Lemma 4, adapted) .There exists a universal constant Csuch
that for any Ïµ >0such that V(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰¥1,
Â¯R(bÎ˜)â‰¤2 inf
Î˜âˆˆSNÃ—FNR(Î˜) + CB2+Ïƒ2
TV(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ) + (B+Ïƒ)Ïµ
.
Proof. The convergence rate of the empirical risk minimizer is established for a fixed regression
problem y=fâ—¦(z) +Î¾in Schmidt-Hieber (2020) when Î¾is Gaussian; we modify the proof to
incorporate bounded noise in Appendix B.1. The ICL setup can be reduced to the ordinary case as
follows. We consider the entire batch (Î²,X, Î¾1:n,Ëœx)including the hidden coefficient Î²as a single
datum zwith output Ëœy. The true function is given as fâ—¦(z) =Fâ—¦
Î²(Ëœx)and the model class is taken to
beTNimplicitly concatenated with the generative process (Î²,X, Î¾1:n,Ëœx)7â†’(X,y,Ëœx). Then R(Î˜),
V(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)andTagree with the ordinary L2risk, model class entropy and sample size.
Here, the second term is the pretraining generalization error dependent on the number of tasks T; the
in-context generalization error dependent on the prompt length nmanifests as part of the first term.
This separation allows us to compare the relative difficulty of the two types of learning.
Bounding approximation error. In order to bound the first term, we analyze the risk of the choice
Î˜âˆ—= (Î“âˆ—, Ï•âˆ—) :=
Î£Î¨,N+1
nÎ£âˆ’1
Â¯Î²,Nâˆ’1
, Ï•âˆ—
Â¯N:Â¯N
where Ï•âˆ—is given as in Assumption 3 for a suitable Î´Nto be determined. The definition of Î“âˆ—
approximately generalizes the global optimum Î“ = 
(1 +1
n)Î› +1
ntr(Î›)Idâˆ’1for the Gaussian
linear regression setup where xâˆ¼ N(0,Î›)(Zhang et al., 2023). Since Î£Î¨,Nâª°C1INwe have
Î“âˆ—âª¯Câˆ’1
1INand hence we may assume Î“âˆ—âˆˆ SNby replacing C3withC3âˆ¨Câˆ’1
1if necessary.
Proposition 3.2. Under Assumptions 1-3, it holds that
inf
Î˜âˆˆSNÃ—FNR(Î˜)â‰¤R(Î˜âˆ—)â‰²N2r
nlogN+N4r
n2log2N+N
n+Nâˆ’2s+N2Î´4
N+N2r+1Î´2
N.
The proof is presented throughout Appendix A. The overall scheme is to approximate Fâ—¦
Î²(Ëœx)by its
truncation Fâ—¦
Î²,Â¯N(Ëœx)and the finite basis Ïˆâ—¦
Â¯N:Â¯NbyÏ•âˆ—
Â¯N:Â¯N, which incurs errors Nâˆ’2sand the terms
pertaining to Î´N, respectively. The first three terms arise from the concentration of the ntoken
representations Ïˆâ—¦
Â¯N:Â¯N(xk). All hidden constants are at most polynomial in problem parameters.
Bounding generalization error. To estimate the metric entropy of TN, we first reduce to the
metric entropy of the representation class FN. Here, âˆ¥Â·âˆ¥Lâˆrefers to the essential supremum over
the support of PXand also over all Ncomponents for FN. The proof is given in Appendix B.2.
Lemma 3.3. Under Assumptions 1-3, there exists D > 0such that for all Ïµsufficiently small,
V(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²N2logBâ€²2
N
Ïµ+V
FN,âˆ¥Â·âˆ¥Lâˆ,Ïµ
DBâ€²
Nâˆš
N
.
4 Minimax Optimality of In-Context Learning
4.1 Besov Space and DNNs
We now apply our theory to study the sample complexity of ICL when FNconsists of (clipped, see
(6)) deep neural networks. These can be also seen as simplified transformers with attention layers
and skip connections removed. To be precise, we define the set of DNNs with depth L, width W,
sparsity S, norm bound Mand ReLU activation Î·(x) =xâˆ¨0(applied element-wise) as
FDNN(L, W, S, M ) =
(W(L)Î·+b(L))â—¦Â·Â·Â·â—¦ (W(1)x+b(1))W(1)âˆˆRWÃ—d,W(â„“)âˆˆRWÃ—W,
W(L)âˆˆRW, b(â„“)âˆˆRW, b(L)âˆˆR,LX
â„“=1âˆ¥W(â„“)âˆ¥0+âˆ¥b(â„“)âˆ¥0â‰¤S,max
1â‰¤â„“â‰¤Lâˆ¥W(â„“)âˆ¥âˆâˆ¨âˆ¥b(â„“)âˆ¥âˆâ‰¤M
.
5TheBesov space is a very general class of functions including the HÃ¶lder and Sobolev spaces which
captures spatial inhomogeneity in smoothness, and provides a natural setting in which to study the
expressive power of deep neural networks (Suzuki, 2019). Here, we fix X= [0,1]dfor simplicity.
Definition 4.1 (Besov space) .For2â‰¤pâ‰¤ âˆ ,0< qâ‰¤ âˆ , fractional smoothness Î± > 0and
r=âŒŠÎ±âŒ‹+ 1, therth modulus of fâˆˆLp(X)is defined using the difference operator âˆ†r
h, hâˆˆRdas
wr,p(f, t) := supâˆ¥hâˆ¥2â‰¤tâˆ¥âˆ†r
h(f)âˆ¥p,âˆ†r
h(f)(x) = 1 {x,x+rhâˆˆX}Pr
j=0 r
j
(âˆ’1)râˆ’jf(x+jh).
Also, the Besov (quasi-)norm is given as âˆ¥Â·âˆ¥BÎ±p,q=âˆ¥Â·âˆ¥Lp+|Â·|BÎ±p,qwhere
|f|BÎ±p,q:=( Râˆ
0tâˆ’qÎ±wr,p(f, t)qdt
t1/qq <âˆ
supt>0tâˆ’Î±wr,p(f, t) q=âˆ
and the Besov space is defined as BÎ±
p,q(X) ={fâˆˆLp(X)| âˆ¥fâˆ¥BÎ±p,q<âˆ}. We write U(BÎ±
p,q(X))
for the unit ball in (BÎ±
p,q(X),âˆ¥Â·âˆ¥BÎ±p,q).
We have that the HÃ¶lder space CÎ±(X) =BÎ±
âˆ,âˆ(X)for order Î± >0, Î± /âˆˆNand the Sobolev space
Wm
2(X) =Bm
2,2(X)formâˆˆNas well as the embeddings Bm
p,1(X),â†’Wm
p(X),â†’Bm
p,âˆ(X); if
Î± > d/p ,BÎ±
p,q(X)compactly embeds into the space of continuous functions on X. See Triebel
(1983); GinÃ© and Nickl (2015) for more details. The difficulty of learning a regression function in the
Besov is quantified by the minimax risk; the following rate is classical.
Proposition 4.2 (Donoho and Johnstone (1998)) .The minimax risk for an estimator bfnwithni.i.d.
samples Dn={(xi, yi)}n
i=1overU(BÎ±
p,q(X))satisfies
infbfn:Dnâ†’Rsupfâ—¦âˆˆU(BÎ±p,q(X))EDn[âˆ¥fâ—¦âˆ’bfnâˆ¥2
L2(X)]â‰nâˆ’2Î±
2Î±+d.
A natural basis system for BÎ±
p,q(X)is formed by the B-splines , which can be seen as a type of wavelet
decomposition or multiresolution analysis (DeVore and Popov, 1988). As B-splines are piecewise
polynomials, they can be efficiently approximated by DNNs with at most log depth (Suzuki, 2019).
Definition 4.3 (B-spline wavelet basis) .The tensor product B-spline of order mâˆˆNsatisfying
m > Î± + 1âˆ’1/p, at resolution kâˆˆZd
â‰¥0and location â„“âˆˆId
k=Qd
i=1[âˆ’m: 2ki]is
Ï‰d
k,â„“(x) =dY
i=1Î¹m(2kixiâˆ’â„“i),where Î¹m(x) = (Î¹âˆ—Î¹âˆ— Â·Â·Â· âˆ— Î¹|{z}
m+1)(x), Î¹(x) = 1 {xâˆˆ[0,1]}.
When k1=Â·Â·Â·=kd, we abuse notation and write Ï‰d
k,â„“forkâˆˆZâ‰¥0in place of Ï‰d
(k,Â·Â·Â·,k),â„“.
4.2 Estimation Error Analysis
To apply our framework, we set the task class as the unit ball Fâ—¦=U(BÎ±
p,q(X))and take as basis
{Ïˆâ—¦
j|jâˆˆN}={2kd/2Ï‰d
k,â„“|kâˆˆZâ‰¥0, â„“âˆˆId
k}the set of all B-spline wavelets ordered primarily
by increasing kand scaled to counteract the dilation in x. Abusing notation, we also write Î²k,â„“
to denote the coefficient in Î²corresponding to 2kd/2Ï‰d
k,â„“. The set of B-splines at each resolution
are independent, while those of lower resolution can always be decomposed into a linear sum of
B-splines of higher resolution satisfying certain decay rates, which we prove in Proposition C.10.
From this setup, in Appendix C.1.1, we verify Assumptions 1 and 2withr= 1/2, s=Î±/d under:
Assumption 4. Fâ—¦=U(BÎ±
p,q(X)),Î± > d/p andPXhas positive Lebesgue density ÏXbounded
above and below on X. Also, all coefficients Î²k,â„“are independent and
EÎ²[Î²k,â„“] = 0,0<EÎ²[Î²2
k,â„“]â‰²2âˆ’k(2Î±+d)kâˆ’2,âˆ€kâ‰¥0, â„“âˆˆId
k. (5)
We can check that we have not given ourselves an easier learning problem with (5): the assumed
variance decay rate is tight (up to the logarithmic factor kâˆ’2) in the sense that any fâˆˆU(BÎ±
p,q(X))
can indeed be expanded into a sum of wavelets with the same coefficient decay when averaged over
â„“âˆˆId
k. See Lemma C.1 and the following discussion. We also obtain the following in-context
approximation and entropy bounds in Appendix C.1.2.
6Lemma 4.4. For any Î´N>0, Assumption 3 is satisfied by taking
FN={Î Bâ€²
Nâ—¦Ï•|Ï•= (Ï•j)N
j=1, Ï•jâˆˆ F DNN(L, W, S, M )} (6)
where Î Bâ€²
Nis the projection in RNto the centered ball of radius Bâ€²
N=O(âˆš
N)and each Ï•jis a
ReLU network such that L=O(logN+ log Î´âˆ’1
N)andW, S, M =O(1). Also, the metric entropy of
FNis bounded as V(FN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²NlogN
Î´NÏµ.
Hence Assumptions 1-3 all follow from Assumption 4, and we conclude in Appendix C.1.3:
Theorem 4.5 (minimax optimality of ICL in Besov space) .Under Assumption 4, if nâ‰³NlogN,
Â¯R(bÎ˜)â‰²Nâˆ’2Î±
d DNN
approximation
error!
+NlogN
n in-context
generalization
error!
+N2logN
T pretraining
generalization
error!
.
Hence if Tâ‰³n2Î±+2d
2Î±+dandNâ‰nd
2Î±+d, in-context learning achieves the minimax optimal rate
nâˆ’2Î±
2Î±+dup to a log factor.
The first term arises from the N-term truncation and oracle approximation error of the DNN module,
and is equal to the N-term optimal error (D Ëœung, 2011a). The second and third term each correspond
to the in-context and pretraining generalization gap. With regard to N, we see that n=eâ„¦(N)
is enough to learn the basis expansion in context, while T=eâ„¦(N2)is necessary to learn the
attention layer. However if T/N =o(n), the third term dominates and the overall complexity scales
suboptimally as Tâˆ’Î±
Î±+d, illustrating the importance of sufficient pretraining. This also aligns with
the task diversity threshold observed by Raventos et al. (2023). Since the amount of training data for
LLMs is practically infinite in practice, our result justifies the effectiveness of ICL at large scales
with only a small number of in-context samples.
A limitation of ICL. In the regime 1â‰¤p <2, the approximation error is strictly worse without
anadaptive representation scheme and the resulting rate is suboptimal (see Remark C.3). While
DNNs can adapt to task smoothness in supervised settings (Suzuki, 2019), ICL and any other meta-
learning methods are fundamentally constrained to non-adaptive representations since they cannot
update at inference time, and hence are bounded below by the best linear approximation rate or
Kolmogorov width, which is strictly worse than the minimax optimal rate when p <2. Indeed, for
anyN-dimensional subspace LNâŠ‚BÎ±
p,q(X)it holds that (VybÃ­ral, 2008)
infLNsupfâ—¦âˆˆU(BÎ±p,q(X))infâ„“nâˆˆLNâˆ¥fâ—¦âˆ’â„“nâˆ¥L2(PX)â‰³Nâˆ’Î±/d+(1/pâˆ’1/2)+.
Remark 4.6. TheN2logNterm in the pretraining generalization gap is due to the covering bound of
the attention matrix Î“, while the entropy of the DNN class is only NlogN. Hence the task diversity
requirement may be lessened to the latter by considering low-rank structure or approximation of
attention heads (Bhojanapalli et al., 2020; Chen et al., 2021).
4.3 Avoiding the Curse of Dimensionality
The above rate inevitably suffers from the curse of dimensionality as dappears in the exponent of
the optimal rate. We also consider the anisotropic Besov space (Nikolâ€™skii, 1975), a generalization
allowing for different degrees of smoothness (Î±1,Â·Â·Â·, Î±d)in each coordinate. Then the optimal
rate is nearly dimension-free in the sense that the rate only depends on dthrough the quantity
eÎ±:= (P
iÎ±âˆ’1
i)âˆ’1, and becomes independent of dimension if only a few directions are important i.e.
have small Î±i. Rigorous definitions, statements and proofs are provided in Appendix C.2.
Extending Theorem 4.5, we show that ICL again attains near-optimal estimation error in the
anisotropic Besov space, circumventing the curse of dimensionality and theoretically establishing the
efficiacy of in-context learning in high-dimensional settings.
Theorem 4.7 (informal version of Theorem C.7) .For the anisotropic Besov space of smoothness
(Î±1,Â·Â·Â·, Î±d), assume variance decay (4)withs=eÎ± >1/p. IfTâ‰³nNandNâ‰n1
2eÎ±+1, in-context
learning achieves the minimax optimal rate nâˆ’2eÎ±
2eÎ±+1up to a log factor.
74.4 Learning a Coarser Basis
Thus far, we have demonstrated the importance of sufficient pretraining to achieve optimal risk;
as another application of our framework, we illustrate how pretraining can actively mitigate the
complexity of in-context learning. Consider the case where (Ïˆâ—¦
j)âˆ
j=1is no longer the B-spline basis
ofBÎ±
p,q(X)but instead is chosen from some wider function space, say the unit ball of BÏ„
p,q(X)for a
smaller smoothness Ï„ < Î± . Without knowledge of the basis, the sample complexity of learning any
regression function Fâ—¦
Î²is a priori lower bounded by the minimax rate nâˆ’2Ï„
2Ï„+dby Proposition 4.2.
For ICL, this difficulty manifests as an increase in the metric entropy of the class FNwhich must be
powerful enough to approximate Ïˆâ—¦
1:N(Corollary C.12), giving rise to the modified risk bound:
Corollary 4.8 (ICL for coarser basis) .Suppose Î± > Ï„ > d/p , the basis (Ïˆâ—¦
j)âˆ
j=1âŠ‚U(BÏ„
p,q(X))
and Assumptions 1, 2 hold with r= 1/2, s=Î±/d. Then if nâ‰³NlogN,
Â¯R(bÎ˜)â‰²Nâˆ’2Î±
d+NlogN
n+N1+Î±
Ï„+d
Ï„log3N
T.
Hence if Tâ‰³n1+d
2Î±+dÎ±+d
Ï„andNâ‰nd
2Î±+d, the risk converges as nâˆ’2Î±
2Î±+dlogn.
The pretraining generalization gap is now dominated by the higher complexity N1+Î±
Ï„+d
Ï„log3Nof
the DNN class and strictly worse compared to N2logNfor Theorem 4.5. The required number of
tasks also suffers and the exponent is no longer2Î±+2d
2Î±+dâˆˆ(1,2)but scales as O(d). Nevertheless,
observe that the burden of complexity is entirely carried by T; with sufficient pretraining, the third
term can be made arbitrarily small and the ICL risk again attains nâˆ’2Î±
2Î±+d. Hence ICL improves
upon the a priori lower bound nâˆ’2Ï„
2Ï„+dat inference time by encoding information on the coarser basis
during pretraining. We remark that the result is also readily adapted to the anisotropic setting.
4.5 Sequential Input and Transformers
We now consider a more complex setting where the inputs xâˆˆ[0,1]dÃ—âˆare bidirectional sequences
of tokens (e.g. entire documents) and Ï•is itself a transformer network.2In this infinite-dimensional
setting, transformers can still circumvent the curse of dimensionality and in fact achieve near-optimal
sample complexity due to their parameter sharing and feature extraction capabilities (Takakura and
Suzuki, 2023). Our goal in this section is to extend this guarantee to ICL of trained transformers.
For sequential data, it is natural to suppose the smoothness w.r.t. each coordinate can vary depending
on the input. For example, the position of important tokens in a sentence will change if irrelevant
strings are inserted. To this end, we adopt the piecewise Î³-smooth function class introduced by
Takakura and Suzuki (2023), which allows for arbitrary bounded permutations among input tokens;
see Appendix D.1 for definitions. Also borrowing from their setup, we consider multi-head sliding
window self-attention layers with window size U, embedding dimension D, number of heads Hwith
key, query, value matrices K(h), Q(h)âˆˆRDÃ—d, V(h)âˆˆRdÃ—dand norm bound Mdefined as3
FAttn(U, D, H, M ) =
g:RdÃ—âˆâ†’RdÃ—âˆmax
1â‰¤hâ‰¤Hâˆ¥K(h)âˆ¥âˆâˆ¨ âˆ¥Q(h)âˆ¥âˆâˆ¨ âˆ¥V(h)âˆ¥âˆâ‰¤M,
g(x)i=xi+HX
h=1V(h)xiâˆ’U:i+USoftmax
(K(h)xiâˆ’U:i+U)âŠ¤Q(h)xi
.
We also consider a linear embedding layer Enc(x) =Ex+P,EâˆˆRDÃ—dwith absolute positional
encoding PâˆˆRDof bounded norm. Then the class of depth Jtransformers is defined as
FTF(J, U, D, H, L, W, S, M ) :=
fJâ—¦gJâ—¦ Â·Â·Â· â—¦ f1â—¦g1â—¦Enc| âˆ¥Eâˆ¥ â‰¤M,
fiâˆˆ F DNN(L, W, S, M ), giâˆˆ F Attn(U, D, H, M )	
.
Our result, proved in Appendix D.2, reads:
2We clarify that this is notequivalent to a multi-layer transformer setting where Ï•is the rest of the transformer.
Instead, Ï•operates on individual tokens xiseparately, which may now themselves be sequences of unbounded
dimension. The extracted per-token features are cross-referenced only at the final attention layer fÎ˜.
3Here the ith column and (j, i)th component of xâˆˆRdÃ—âˆforiâˆˆZ, jâˆˆ[d]are denoted by xiandxij,
respectively. These are not to be confused with sample indexes (1) as those will not be used in this section.
8Theorem 4.9 (informal version of Theorem D.1) .Suppose Fâ—¦consists of functions on [0,1]dÃ—âˆ
of bounded piecewise Î³-smooth and Lâˆ-norm with smoothness Î±âˆˆRdÃ—âˆ
>0, and let Î³be mixed
or anisotropic smoothness with Î±â€ = max i,jÎ±ijor(P
i,jÎ±âˆ’1
ij)âˆ’1, respectively. Under suitable
regularity and decay assumptions, by taking FNto be a class of clipped transformers it holds that
Â¯R(bÎ˜)â‰²Nâˆ’2Î±â€ +NlogN
n+N2âˆ¨(1+1/Î±â€ )polylog( N)
T.
Hence if Tâ‰³nN1âˆ¨1/Î±â€ andNâ‰n1
2Î±â€ +1, ICL achieves the rate nâˆ’2Î±â€ 
2Î±â€ +1polylog( n).
This matches the optimal rate in finite dimensions independently of the (possibly infinite) length of
the input or context window. The dynamical feature extraction ability of attention layers in the FTF
class is essential in dealing with input-dependent smoothness, further justifying the efficiacy of ICL
of sequential data.
5 Minimax Lower Bounds
In this section, we provide lower bounds for the minimax rate in both n, T by extending the theory of
Yang and Barron (1999), which can be leveraged to yield results stronger than optimality in merely n.
The bound is purely information-theoretic and hence applies to not just ICL but any meta-learning
scheme for the regression problem of Section 2.1 from the data Dn,T={(X(t),y(t))}T+1
t=1, where
the index T+ 1corresponds to the test task.
For this section we assume that the noise (1)is i.i.d. Gaussian, Î¾kâˆ¼ N(0, Ïƒ2), instead of bounded;
while the exact shape of the noise distribution is not important, having restricted support may convey
additional information and affect the minimax rate. We also suppose for simplicity that the support
ofPÎ²is included in B:={Î²âˆˆRâˆ| |Î²j|2â‰²jâˆ’2sâˆ’1(logj)âˆ’2, jâˆˆN}and that the aggregated
coefficients Â¯Î²jforÂ¯Nâ‰¤jâ‰¤Â¯Nsatisfy EÎ²[Â¯Î²2
j]â‰¤Ïƒ2
Î²for some ÏƒÎ²dependent on N. The proof of the
following statement is given in Appendix F.1.
Proposition 5.1. ForÎµn,1, Îµn,2, Î´n>0, letQ1andQ2be the Îµn,1- and Îµn,2-covering numbers
ofFNandBrespectively, and Mbe the Î´n-packing number of Fâ—¦. Suppose that the following
conditions are satisfied:
1
2Ïƒ2
n(T+ 1)Ïƒ2
Î²Îµ2
n,1+C2nÎµ2
n,2
â‰¤logQ1+ log Q2â‰¤1
8logM, 4 log 2 â‰¤logM. (7)
Then the minimax rate is lower bounded as
infbf:Dn,Tâ†’Rsupfâ—¦âˆˆFâ—¦EDn,T[âˆ¥bfâˆ’fâ—¦âˆ¥2
L2(PX)]â‰¥1
4Î´2
n.
Finally, Proposition 5.1 is applied to obtain concrete lower bounds for the settings studied in Section
4 throughout Appendices F.2-F.4.
Corollary 5.2 (minimax lower bound) .The minimax rates in the previous regression settings are
lower bounded as follows.
(i)Besov space (Section 4.2): infbfsupfâ—¦EDn,T[âˆ¥bfâˆ’fâ—¦âˆ¥2]â‰³nâˆ’2Î±
2Î±+d,
(ii)Coarser basis (Section 4.4): infbfsupfâ—¦EDn,T[âˆ¥bfâˆ’fâ—¦âˆ¥2]â‰³nâˆ’2Î±
2Î±+d+ (nT)âˆ’2Ï„
2Ï„+d,
(iii) Sequential input (Section 4.5): infbfsupfâ—¦EDn,T[âˆ¥bfâˆ’fâ—¦âˆ¥2]â‰³nâˆ’2Î±â€ 
2Î±â€ +1.
These results match the upper bounds for (i), (iii) and show that ICL is provably jointly optimal in
n, T in the â€˜large Tâ€™ regime. Moreover, we can check for the coarser basis setting that insufficient
pretraining T=O(1)indeed leads to the worse complexity nâˆ’2Ï„
2Ï„+d, while the faster rate nâˆ’2Î±
2Î±+dis
retrieved when Tâ‰³n(Î±âˆ’Ï„)d
(2Î±+d)Ï„. This aligns with the discussion in Section 4.4, showing that ICL is
provably suboptimal in the â€˜small Tâ€™ regime.
Remark 5.3. The obtained upper and lower bounds in the coarser basis setting are not tight as T
varies, hence it remains to be shown whether there exists a meta-learning algorithm that attains the
lower bound (ii). The task diversity threshold for optimal learning suggested by the bounds are also
different ( n1+d(Î±+d)
(2Î±+d)v.s.n(Î±âˆ’Ï„)d
(2Î±+d)Ï„); it would be interesting for future work to resolve this gap.
96 Conclusion
In this paper, we performed a learning-theoretic analysis of ICL of a transformer consisting of a DNN
and a linear attention layer pretrained on nonparametric regression tasks. We developed a general
framework for bounding the in-context estimation error of the empirical risk minimizer in terms of
both the number of tasks and samples, and proved that ICL can achieve nearly minimax optimal rates
in the Besov space, anisotropic Besov space and Î³-smooth class. We also demonstrated that ICL can
improve upon the a priori optimal rate by learning informative representations during pretraining.
We supplemented our analyses with corresponding minimax lower bounds jointly in n, T and also
performed numerical experiments validating our findings. Our work opens up interesting approaches
of adapting classical learning theory to study emergent phenomena of foundation models.
Limitations. Our transformer model is limited to a single layer of linear self-attention and does not
consider more complex in-context learning behavior which may arise in transformers with multiple
attention layers. Moreover, the obtained upper and lower bounds are not tight in certain regimes,
suggesting future research directions for meta-learning.
Acknowledgments
JK was partially supported by JST CREST (JPMJCR2015). TS was partially supported by JSPS
KAKENHI (24K02905, 20H00576) and JST CREST (JPMJCR2115). We would like to express our
gratitude to Masaaki Imaizumi for valuable and insightful discussions on the topic in relation to his
work in progress (Imaizumi, 2024).
References
K. Ahn, X. Cheng, H. Daneshmand, and S. Sra. Transformers learn to implement preconditioned
gradient descent for in-context learning. arXiv preprint arXiv:2306.00297 , 2023.
E. AkyÃ¼rek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-
context learning? Investigations with linear models. In International Conference on Learning
Representations , 2023.
Y . Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: provable in-
context learning with in-context algorithm selection. In ICML Workshop on Efficient Systems for
Foundation Models , 2023.
S. Bhojanapalli, C. Yun, A. S. Rawat, S. J. Reddi, and S. Kumar. Low-rank bottleneck in multi-head
attention models. In International Conference on Machine Learning , 2020.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,
D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot
learners. In Advances in Neural Information Processing Systems , 2020.
B. Chen, T. Dao, E. Winsor, Z. Song, A. Rudra, and C. RÃ©. Scatterbrain: unifying sparse and low-rank
attention approximation. In Advances in Neural Information Processing Systems , 2021.
S. Chen, H. Sheen, T. Wang, and Z. Yang. Training dynamics of multi-head softmax attention for
in-context learning: emergence, convergence, and optimality. arXiv preprint arXiv:2402.19442 ,
2024.
R. A. DeVore and V . A. Popov. Interpolation of Besov spaces. Transactions of the American
Mathematical Society , 305(1):397â€“414, 1988.
D. L. Donoho and I. M. Johnstone. Minimax estimation via wavelet shrinkage. The Annals of
Statistics , 26(3):879â€“921, 1998.
S. Du, W. Hu, S. Kakade, J. Lee, and Q. Lei. Few-shot learning via learning the representation,
provably. In International Conference on Learning Representations , 2021.
10D. D Ëœung. Optimal adaptive sampling recovery. Advances in Computational Mathematics , 34:1â€“41,
2011a.
D. D Ëœung. B-spline quasi-interpolant representations and sampling recovery of functions with mixed
smoothness. Journal of Complexity , 27(6):541â€“567, 2011b.
X. GarcÃ­a, Y . Bansal, C. Cherry, G. F. Foster, M. Krikun, F. Feng, M. Johnson, and O. Firat.
The unreasonable effectiveness of few-shot learning for machine translation. In International
Conference on Machine Learning , 2023.
S. Garg, D. Tsipras, P. Liang, and G. Valiant. What can Transformers learn in-context? A case study
of simple function classes. In Advances in Neural Information Processing Systems , 2022.
E. GinÃ© and R. Nickl. Mathematical foundations of infinite-dimensional statistical models . Cambridge
Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2015.
T. Guo, W. Hu, S. Mei, H. Wang, C. Xiong, S. Savarese, and Y . Bai. How do Transformers learn
in-context beyond simple functions? A case study on learning with representations. arXiv preprint
arXiv:2310.10616 , 2023.
S. Hayakawa and T. Suzuki. On the minimax optimality and superiority of deep neural network
learning over sparse parameter spaces. Neural Networks , 123:343â€“361, 2020.
Y . Huang, Y . Cheng, and Y . Liang. In-context convergence of Transformers. arXiv preprint
arXiv:2310.05249 , 2023.
M. Imaizumi. Statistical analysis on in-context learning, 2024. Personal communication.
J. Kim and T. Suzuki. Transformers learn nonlinear features in context: nonconvex mean-field
dynamics on the attention landscape. In International Conference on Machine Learning , 2024.
H. Li, M. Wang, S. Lu, X. Cui, and P.-Y . Chen. Training nonlinear Transformers for efficient in-context
learning: a theoretical learning and generalization analysis. arXiv preprint arXiv:2402.15607 ,
2024.
A. Mahankali, T. B. Hashimoto, and T. Ma. One step of gradient descent is provably the optimal
in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576 , 2023.
D. Meunier, Z. Li, A. Gretton, and S. Kpotufe. Nonlinear meta-learning can guarantee faster rates.
arXiv preprint arXiv:2307.10870 , 2023.
S. M. Nikolâ€™skii. Approximation of functions of several variables and imbedding theorems , volume
205 of Grundlehren der mathematischen Wissenschaften . Springer Berlin, 1975.
Y . Nishimura and T. Suzuki. Minimax optimality of convolutional neural networks for infinite dimen-
sional input-output problems and separation from kernel methods. In The Twelfth International
Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=
EW8ZExRZkJ .
S. Okumoto and T. Suzuki. Learnability of convolutional neural networks for infinite dimensional
input via mixed and anisotropic smoothness. In International Conference on Learning Representa-
tions , 2022.
A. Raventos, M. Paul, F. Chen, and S. Ganguli. Pretraining task diversity and the emergence of
non-Bayesian in-context learning for regression. In Advances in Neural Information Processing
Systems , 2023.
J. Schmidt-Hieber. Nonparametric regression using deep neural networks with ReLU activation
function. The Annals of Statistics , 48(4), 2020.
T. Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov spaces:
optimal rate and curse of dimensionality. In International Conference on Learning Representations ,
2019.
11T. Suzuki and A. Nitanda. Deep learning is adaptive to intrinsic dimensionality of model smoothness
in anisotropic Besov space. In Advances in Neural Information Processing Systems , 2021.
S. J. Szarek. Nets of Grassmann manifold and orthogonal group. Proceedings of Banach Space
Workshop , pages 169â€“186, 1981.
S. Takakura and T. Suzuki. Approximation and estimation ability of Transformers for sequence-
to-sequence functions with infinite dimensional input. In International Conference on Machine
Learning , 2023.
H. Triebel. Theory of function spaces . Monographs in mathematics. BirkhÃ¤user Verlag, 1983.
H. Triebel. Entropy numbers in function spaces with mixed integrability. Revista Matematica
Complutense , 24:169â€“188, 2011.
N. Tripuraneni, C. Jin, and M. Jordan. Provable meta-learning of linear representations. In Interna-
tional Conference on Machine Learning , 2020.
J. A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine
Learning , 8(1â€“2):1â€“230, May 2015. ISSN 1935-8237.
A. W. van der Vaart and J. A. Wellner. Weak convergence and empirical processes: with applications
to statistics . Springer, New York, 1996.
J. von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and
M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference
on Machine Learning , 2023.
J. VybÃ­ral. Function spaces with dominating mixed smoothness , volume 30 of Lectures in Mathematics .
European Mathematical Society, 2006.
J. VybÃ­ral. Widths of embeddings in function spaces. Journal of Complexity , 24(4):545â€“570, 2008.
J. Wu, D. Zou, Z. Chen, V . Braverman, Q. Gu, and P. L. Bartlett. How many pretraining tasks are
needed for in-context learning of linear regression? In International Conference on Learning
Representations , 2024.
Y . Yang and A. Barron. Information-theoretic determination of minimax rates of convergence. The
Annals of Statistics , 27(5):1564â€“1599, 1999.
D. Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks , 94:
103â€“114, 2016.
R. Zhang, S. Frei, and P. L. Bartlett. Trained Transformers learn linear models in-context. arXiv
preprint arXiv:2306.09927 , 2023.
R. Zhang, J. Wu, and P. L. Bartlett. In-context learning of a linear Transformer block: benefits of the
MLP component and one-step GD initialization. arXiv preprint arXiv:2402.14951 , 2024.
12Table of Contents
1 Introduction 1
1.1 Other Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2 Problem Setup 3
2.1 Nonparametric Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.2 In-Context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 Risk Bounds for In-Context Learning 4
4 Minimax Optimality of In-Context Learning 5
4.1 Besov Space and DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.2 Estimation Error Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
4.3 Avoiding the Curse of Dimensionality . . . . . . . . . . . . . . . . . . . . . . . . 7
4.4 Learning a Coarser Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.5 Sequential Input and Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 Minimax Lower Bounds 9
6 Conclusion 10
A Proof of Proposition 3.2 15
A.1 Decomposing Approximation Error . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 Bounding Term (8) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 Bounding Term (9) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.4 Bounding Terms (10)-(12) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.5 Proof of Lemma A.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B Proofs on Metric Entropy 22
B.1 Modified Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2 Proof of Lemma 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Proof of Lemma B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C Details on Besov-type Spaces 24
C.1 Besov Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.1.1 Verification of Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.1.2 Proof of Lemma 4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.1.3 Proof of Theorem 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2 Anisotropic Besov Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2.1 Definitions and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
C.2.2 Proof of Theorem C.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
C.3 Wavelet Refinement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
C.4 Proof of Corollary 4.8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
D Details on Sequential Input 33
D.1 Definitions and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
13D.2 Proof of Theorem D.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
E Numerical Experiments 36
F Proofs of Minimax Lower Bounds 37
F.1 Proof of Proposition 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
F.2 Lower Bound in Besov Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.3 Lower Bound with Coarser Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
F.4 Lower Bound in Piecewise Î³-smooth Class . . . . . . . . . . . . . . . . . . . . . 39
14Appendix
A Proof of Proposition 3.2
A.1 Decomposing Approximation Error
Recall that
Î˜âˆ—= (Î“âˆ—, Ï•âˆ—) = 
Î£Î¨,N+1
nÎ£âˆ’1
Â¯Î²,Nâˆ’1
, Ï•âˆ—
Â¯N:Â¯N!
.
We introduce some additional notation in the following fashion. For brevity, we write Â¯N:âˆinstead
of(Â¯N+ 1) : âˆas an exception.
Î¦âˆ—= (Ï•âˆ—
Â¯N:Â¯N(x1),Â·Â·Â·, Ï•âˆ—
Â¯N:Â¯N(xn))âˆˆRNÃ—n, Î¾ = (Î¾1, . . . , Î¾ n)âŠ¤âˆˆRn,
Î¨â—¦= (Ïˆâ—¦(x1), . . . , Ïˆâ—¦(xn))âˆˆRâˆÃ—n,Î¨â—¦
Â¯N:Â¯N= (Ïˆâ—¦
Â¯N:Â¯N(x1),Â·Â·Â·, Ïˆâ—¦
Â¯N:Â¯N(xn))âˆˆRNÃ—n,
Î¨â—¦
Â¯N:âˆ= (Ïˆâ—¦
Â¯N:âˆ(x1),Â·Â·Â·, Ïˆâ—¦
Â¯N:âˆ(xn))âˆˆRâˆÃ—n.
Since clipping Ë‡fÎ˜does not make its difference with Fâ—¦
Î²âˆˆ[âˆ’B, B]larger, it holds that
R(Î˜âˆ—) =Eh 
Fâ—¦
Î²(Ëœx)âˆ’fÎ˜âˆ—(X,y,Ëœx)2i
â‰¤Eh 
Fâ—¦
Î²(Ëœx)âˆ’Ë‡fÎ˜âˆ—(X,y,Ëœx)2i
â‰¤2E
Fâ—¦
Î²(Ëœx)âˆ’Fâ—¦
Î²,Â¯N(Ëœx)2
+ 2E
Fâ—¦
Î²,Â¯N(Ëœx)âˆ’Ë‡fÎ˜âˆ—(X,y,Ëœx)2
â‰²Nâˆ’2s+E"
Fâ—¦
Î²,Â¯N(Ëœx)âˆ’Ï•âˆ—(Ëœx)âŠ¤Î“âˆ—Î¦âˆ—y
n2#
due to Assumption 2 and Â¯Nâ‰N. Expanding the attention output as
Ï•âˆ—(Ëœx)âŠ¤Î“âˆ—Î¦âˆ—y
n= (Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx) +Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—(Î¦âˆ—âˆ’Î¨â—¦
Â¯N:Â¯N+ Î¨â—¦
Â¯N:Â¯N)y
n
= (Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—(Î¦âˆ—âˆ’Î¨â—¦
Â¯N:Â¯N)y
n
+ (Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯Ny
n
+Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—(Î¦âˆ—âˆ’Î¨â—¦
Â¯N:Â¯N)y
n
+Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯Ny
n,
and the final term further as
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯Ny
n=Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯N(Î¨â—¦âŠ¤Î²+Î¾)
n
=Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯NÂ¯Î²
Â¯N:Â¯N
n+Ïˆâ—¦
1:N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯N(Î¨â—¦âŠ¤
Â¯N:âˆÎ²Â¯N:âˆ+Î¾)
n,
we obtain that
E"
Fâ—¦
Î²,Â¯N(Ëœx)âˆ’Ï•âˆ—(Ëœx)âŠ¤Î“âˆ—Î¦âˆ—y
n2#
â‰²Eï£®
ï£° 
Fâ—¦
Î²,Â¯N(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯NÂ¯Î²
Â¯N:Â¯N
n!2ï£¹
ï£» (8)
15+Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯N(Î¨â—¦âŠ¤
Â¯N:âˆÎ²Â¯N:âˆ+Î¾)
n!2ï£¹
ï£» (9)
+Eï£®
ï£° 
(Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—(Î¦âˆ—âˆ’Î¨â—¦
Â¯N:Â¯N)y
n!2ï£¹
ï£» (10)
+Eï£®
ï£° 
(Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯Ny
n!2ï£¹
ï£» (11)
+Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—(Î¦âˆ—âˆ’Î¨â—¦
Â¯N:Â¯N)y
n!2ï£¹
ï£». (12)
We proceed to control each term separately, from which the statement of Proposition 3.2 will follow.
A.2 Bounding Term (8)
Since Fâ—¦
Î²,Â¯N(Ëœx) =Ïˆâ—¦
1:Â¯N(Ëœx)âŠ¤Î²1:Â¯N=Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Â¯Î²
Â¯N:Â¯N, we can introduce a (Î“âˆ—)âˆ’1factor to bound
Eï£®
ï£° 
Fâ—¦
Î²,Â¯N(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯NÂ¯Î²
Â¯N:Â¯N
n!2ï£¹
ï£»
=Eï£®
ï£° 
Fâ—¦
Î²,Â¯N(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ— Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nâˆ’(Î“âˆ—)âˆ’1+ (Î“âˆ—)âˆ’1!
Â¯Î²
Â¯N:Â¯N!2ï£¹
ï£»
=Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ— Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nâˆ’Î£Î¨,Nâˆ’1
nÎ£âˆ’1
Â¯Î²,N!
Â¯Î²
Â¯N:Â¯N!2ï£¹
ï£»
â‰¤2Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ— Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nâˆ’Î£Î¨,N!
Â¯Î²
Â¯N:Â¯N!2ï£¹
ï£» (13)
+ 2E"
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î£âˆ’1
Â¯Î²,N
nÂ¯Î²
Â¯N:Â¯N2#
. (14)
Denote the operator norm (with respect to L2norm of vectors) by âˆ¥Â·âˆ¥op. For (13), noting that Î£Î¨,N
is positive definite,
Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ— Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nâˆ’Î£Î¨,N!
Â¯Î²
Â¯N:Â¯N!2ï£¹
ï£»
=Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î£1/2
Î¨,N 
Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN!
Î£1/2
Î¨,NÂ¯Î²
Â¯N:Â¯N!2ï£¹
ï£»
=EX,Î²"
Â¯Î²âŠ¤
Â¯N:Â¯NÎ£1/2
Î¨,N 
Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN!
Î£1/2
Î¨,NÎ“âˆ—EËœxh
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤i
Ã—Î“âˆ—Î£1/2
Î¨,N 
Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN!
Î£1/2
Î¨,NÂ¯Î²
Â¯N:Â¯N#
=EX"
Tr" 
Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN!
Î£1/2
Î¨,NEÎ²h
Â¯Î²
Â¯N:Â¯NÂ¯Î²âŠ¤
Â¯N:Â¯Ni
Î£1/2
Î¨,N
Ã— 
Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN!
Î£1/2
Î¨,NÎ“âˆ—Î£Î¨,NÎ“âˆ—Î£1/2
Î¨,N##
16â‰²EXï£®
ï£°Tr" 
Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN!2
Î£1/2
Î¨,NÎ£Â¯Î²,NÎ£1/2
Î¨,N#ï£¹
ï£»
due to the independence of X,ËœxandÎ². For the last inequality, we have used the fact that both the
Î£1/2
Î¨,NÎ“âˆ—Î£Î¨,NÎ“âˆ—Î£1/2
Î¨,Nterm and the matrix multiplied to it are positive semi-definite, and the former
is bounded above as â‰¾INby Assumption 1.
Furthermore, we utilize the following result proved in Appendix A.5:
Lemma A.1. EXï£®
ï£°Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN2
opï£¹
ï£»â‰²N2r
nlogN+N4r
n2log2N.
It follows that (13) is bounded as
Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ— Î¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nâˆ’Î£Î¨,N!
Â¯Î²
Â¯N:Â¯N!2ï£¹
ï£»
â‰²EXï£®
ï£°Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN2
opï£¹
ï£»Trh
Î£1/2
Î¨,NÎ£Â¯Î²,NÎ£1/2
Î¨,Ni
â‰²N2r
nlogN+N4r
n2log2N
âˆ¥Î£Î¨,Nâˆ¥opTr(Î£ Â¯Î²,N)
â‰²N2r
nlogN+N4r
n2log2N
since Tr(Î£ Â¯Î²,N)is bounded by Assumption 2.
Moreover for (14), we compute
E"
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î£âˆ’1
Â¯Î²,N
nÂ¯Î²
Â¯N:Â¯N2#
=E"
Â¯Î²âŠ¤
Â¯N:Â¯NÎ£âˆ’1
Â¯Î²,N
nÎ“âˆ—Ïˆâ—¦
Â¯N:Â¯N(Ëœx)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î£âˆ’1
Â¯Î²,N
nÂ¯Î²
Â¯N:Â¯N#
=E"
Tr"Î£âˆ’1
Â¯Î²,N
nÎ“âˆ—Ïˆâ—¦
Â¯N:Â¯N(Ëœx)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î£âˆ’1
Â¯Î²,N
nÂ¯Î²
Â¯N:Â¯NÂ¯Î²âŠ¤
Â¯N:Â¯N##
= Tr"Î£âˆ’1
Â¯Î²,N
nÎ“âˆ—EËœxh
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤i
Î“âˆ—Î£âˆ’1
Â¯Î²,N
nEÎ²h
Â¯Î²
Â¯N:Â¯NÂ¯Î²âŠ¤
Â¯N:Â¯Ni#
= Tr"Î£âˆ’1
Â¯Î²,N
nÎ“âˆ—Î£Î¨,NÎ“âˆ—
|{z}
positive definiteÎ£âˆ’1
Â¯Î²,N
nÎ£Â¯Î²,N#
â‰¤1
nTr"
Î£Î¨,N+1
nÎ£âˆ’1
Â¯Î²,N
Î“âˆ—Î£Î¨,NÎ“âˆ—#
=1
nTr"
Î£Î¨,N
Î£Î¨,N+1
nÎ£âˆ’1
Â¯Î²,Nâˆ’1#
â‰¤N
n.
A.3 Bounding Term (9)
Since the sequence of covariates x1,Â·Â·Â·, xnand noise Î¾1,Â·Â·Â·, Î¾nare each i.i.d.,
Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯N(Î¨â—¦âŠ¤
Â¯N:âˆÎ²Â¯N:âˆ+Î¾)
n!2ï£¹
ï£»
17=Eï£®
ï£° nX
i=1Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xi)(Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(xi) +Î¾i)
n!2ï£¹
ï£»
=1
n2Eï£®
ï£° nX
i=1Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xi)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(xi) +nX
i=1Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xi)Î¾i!2ï£¹
ï£»
â‰¤1
nE
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)2
(15)
+nâˆ’1
nEh
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xâ€²)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(xâ€²)i
(16)
+Ïƒ2
nE
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)2
(17)
for independent samples x, xâ€²,Ëœxâˆ¼ PX. We now bound the three terms separately below.
For (15), we have that
1
nE
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)2
=1
nEh
Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(Ëœx)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:âˆ(x)âŠ¤Î²Â¯N:âˆÎ²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)i
=1
nExh
Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î“âˆ—Î£Î¨,NÎ“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:âˆ(x)âŠ¤EÎ²
Î²Â¯N:âˆÎ²âŠ¤
Â¯N:âˆ
Ïˆâ—¦
Â¯N:âˆ(x)i
â‰²1
nExh
âˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2Ïˆâ—¦
Â¯N:âˆ(x)âŠ¤EÎ²
Î²Â¯N:âˆÎ²âŠ¤
Â¯N:âˆ
Ïˆâ—¦
Â¯N:âˆ(x)i
â‰²1
nsup
xâˆˆsuppPXâˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2Â·lim
Mâ†’âˆTr 
EÎ²
Î²Â¯N:MÎ²âŠ¤
Â¯N:M
Ex
Ïˆâ—¦
Â¯N:M(x)Ïˆâ—¦
Â¯N:M(x)âŠ¤
â‰²1
nsup
xâˆˆsuppPXâˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2Â·lim
Mâ†’âˆTr 
EÎ²
Î²Â¯N:MÎ²âŠ¤
Â¯N:M
since Î£Î¨,Mâª¯C2INasMâ†’ âˆ by Assumption 2. As supxâˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2â‰²Nâˆ’2rby(2)and the
diagonal of EÎ²[Î²Â¯N:MÎ²âŠ¤
Â¯N:M]decays faster than Â¯Nâˆ’2sMâˆ’1(logM)âˆ’2when M > Â¯Ndue to (4), it
follows that
1
nsup
xâˆˆsuppPXâˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2Â·lim
Mâ†’âˆTr 
EÎ²
Î²Â¯N:MÎ²âŠ¤
Â¯N:M
â‰²1
nN2rNâˆ’2s.
Similarly, for (16), we have that
nâˆ’1
nEh
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xâ€²)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(xâ€²)i
â‰¤E
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)âŠ¤
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xâ€²)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(xâ€²)
=Ex,xâ€²,Î²h
Ïˆâ—¦
Â¯N:âˆ(x)âŠ¤Î²Â¯N:âˆÏˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î“âˆ—Î£Î¨,NÎ“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xâ€²)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(xâ€²)i
=EÎ²
Exh
Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)iâŠ¤
Î“âˆ—Î£Î¨,NÎ“âˆ—Exh
Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)i
â‰²EÎ²Exh
Ïˆâ—¦
Â¯N:Â¯N(x)Î²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)i2
â‰²EÎ²
Ïˆâ—¦
Â¯N:âˆ(x)âŠ¤Î²Â¯N:âˆÎ²âŠ¤
Â¯N:âˆÏˆâ—¦
Â¯N:âˆ(x)
â‰²Nâˆ’2s.
And for (17), we have that
Ïƒ2
nE
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(x)2
18=Ïƒ2
nTrh
Î“âˆ—EËœxh
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤i
Î“âˆ—Exh
Ïˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤ii
=Ïƒ2
nTr
Î“âˆ—Î£Î¨,NÎ“âˆ—
|{z}
positive definiteÎ£Î¨,N
â‰¤Ïƒ2
nTr
Î“âˆ—Î£Î¨,NÎ“âˆ—
Î£Î¨,N+1
nÎ£âˆ’1
Â¯Î²,N
=Ïƒ2
nTr [Î“âˆ—Î£Î¨,N]â‰¤Ïƒ2N
n.
Therefore, we obtain the following bound:
Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯N(Î¨â—¦âŠ¤
Â¯N:âˆÎ²Â¯N:âˆ+Î¾)
n!2ï£¹
ï£»â‰²N2rNâˆ’2s
n+Nâˆ’2s+Ïƒ2N
n.
A.4 Bounding Terms (10)-(12)
For (10), we use the Cauchy-Schwarz inequality and Assumption 3 to bound
Eï£®
ï£° 
(Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—(Î¦âˆ—âˆ’Î¨â—¦
Â¯N:Â¯N)y
n!2ï£¹
ï£»
â‰¤1
nnX
i=1E
(Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—(Ï•âˆ—(xi)âˆ’Ïˆâ—¦
Â¯N:Â¯N(xi))yi2
â‰¤1
nnX
i=1Eh
âˆ¥Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âˆ¥2âˆ¥Î“âˆ—(Ï•âˆ—(xi)âˆ’Ïˆâ—¦
Â¯N:Â¯N(xi))âˆ¥2y2
ii
â‰¤ âˆ¥Î“âˆ—âˆ¥2
op
sup
xâˆˆsuppPXâˆ¥Ï•âˆ—(x)âˆ’Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥22
E
y2
â‰¤ âˆ¥Î“âˆ—âˆ¥2
op
Nmax
Â¯Nâ‰¤jâ‰¤Â¯Nâˆ¥Ï•âˆ—
jâˆ’Ïˆâ—¦
jâˆ¥2
Lâˆ(PX)2
E
y2
â‰²N2Î´4
N(B2+Ïƒ2).
Similarly for (11) and (12), we have
Eï£®
ï£° 
(Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—Î¨â—¦
Â¯N:Â¯Ny
n!2ï£¹
ï£»
â‰¤1
nnX
i=1E
(Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx))âŠ¤Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xi)yi2
â‰¤1
nnX
i=1Eh
âˆ¥Ï•âˆ—(Ëœx)âˆ’Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âˆ¥2âˆ¥Î“âˆ—Ïˆâ—¦
Â¯N:Â¯N(xi)âˆ¥2y2
ii
â‰¤ sup
xâˆˆsuppPXâˆ¥Ï•âˆ—(x)âˆ’Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2âˆ¥Î“âˆ—âˆ¥2
op sup
xâˆˆsuppPXâˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2E
y2
â‰²NÎ´2
NÂ·N2r(B2+Ïƒ2) =N2r+1Î´2
N(B2+Ïƒ2)
and
Eï£®
ï£° 
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—(Î¦âˆ—âˆ’Î¨â—¦
Â¯N:Â¯N)y
n!2ï£¹
ï£»
â‰¤1
nnX
i=1E
Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âŠ¤Î“âˆ—(Ï•âˆ—(xi)âˆ’Ïˆâ—¦
Â¯N:Â¯N(xi))yi2
19â‰¤1
nnX
i=1Eh
âˆ¥Ïˆâ—¦
Â¯N:Â¯N(Ëœx)âˆ¥2âˆ¥Î“âˆ—(Ï•âˆ—(xi)âˆ’Ïˆâ—¦
Â¯N:Â¯N(xi))âˆ¥2y2
ii
â‰¤ sup
xâˆˆsuppPXâˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2âˆ¥Î“âˆ—âˆ¥2
op sup
xâˆˆsuppPXâˆ¥(Ï•âˆ—(x)âˆ’Ïˆâ—¦
Â¯N:Â¯N(x))âˆ¥2E
y2
â‰²N2r+1Î´2
N(B2+Ïƒ2).
This concludes the proof of Proposition 3.2.
A.5 Proof of Lemma A.1
We will make use of the following concentration bound and its corollary, proved at the end of this
subsection.
Theorem A.2 (matrix Bernstein inequality) .LetS1,Â·Â·Â·,SnâˆˆRNÃ—Nbe independent random
matrices such that E[Si] = 0 andâˆ¥Siâˆ¥opâ‰¤Lalmost surely for all i. Define Z=Pn
i=1Snand the
matrix variance statistic v(Z)as
v(Z) = max
âˆ¥E[ZZâŠ¤]âˆ¥op,âˆ¥E[ZâŠ¤Z]âˆ¥op	
= maxnPn
i=1E[SiSâŠ¤
i]
op,Pn
i=1E[SâŠ¤
iSi]
opo
.
Then it holds for all t >0that
Pr (âˆ¥Zâˆ¥opâ‰¥t)â‰¤2Nexp
âˆ’t2
2v(Z) +2
3Lt
.
Proof. See Tropp (2015), Theorem 6.1.1.
Corollary A.3. For matrices S1,Â·Â·Â·,Snsatisfying the conditions of Theorem A.2, it holds that
E1
n2âˆ¥Zâˆ¥2
op
â‰¤4v(Z)
n2(1 + log 2 N) +16L2
9n2(2 + 2 log 2 N+ (log 2 N)2).
We will apply Corollary A.3 to the matrices
Si:= Î£âˆ’1/2
Î¨,NÏˆâ—¦
Â¯N:Â¯N(xi)Ïˆâ—¦
Â¯N:Â¯N(xi)âŠ¤Î£âˆ’1/2
Î¨,Nâˆ’IN.
It is straightforward to verify that
E[Si] = Î£âˆ’1/2
Î¨,NÎ£Î¨,NÎ£âˆ’1/2
Î¨,Nâˆ’IN= 0
and
âˆ¥Siâˆ¥opâ‰²âˆ¥Ïˆâ—¦
Â¯N:Â¯N(xi)Ïˆâ—¦
Â¯N:Â¯N(xi)âŠ¤âˆ¥op+ 1 =Â¯NX
j=Â¯NÏˆâ—¦
j(xi)2+ 1â‰²N2r
almost surely by Assumption 1.
Next, we evaluate the matrix variance statistic. Since each Siis symmetric,
v(Z) =nX
i=1E[SiSâŠ¤
i]
op
=nX
i=1
EXh
Î£âˆ’1/2
Î¨,NÏˆâ—¦
Â¯N:Â¯N(xi)Ïˆâ—¦
Â¯N:Â¯N(xi)âŠ¤Î£âˆ’1
Î¨,NÏˆâ—¦
Â¯N:Â¯N(xi)Ïˆâ—¦
Â¯N:Â¯N(xi)âŠ¤Î£âˆ’1/2
Î¨,Ni
âˆ’2EXh
Î£âˆ’1/2
Î¨,NÏˆâ—¦
Â¯N:Â¯N(xi)Ïˆâ—¦
Â¯N:Â¯N(xi)âŠ¤Î£âˆ’1/2
Î¨,Ni
+IN
op
=nX
i=1EXh
Î£âˆ’1/2
Î¨,NÏˆâ—¦
Â¯N:Â¯N(xi)Ïˆâ—¦
Â¯N:Â¯N(xi)âŠ¤Î£âˆ’1
Î¨,NÏˆâ—¦
Â¯N:Â¯N(xi)Ïˆâ—¦
Â¯N:Â¯N(xi)âŠ¤Î£âˆ’1/2
Î¨,Ni
âˆ’nIN
op
â‰¤n+nExh
Î£âˆ’1/2
Î¨,NÏˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î£âˆ’1
Î¨,NÏˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î£âˆ’1/2
Î¨,Ni
op
20for a single sample xâˆ¼ PX. The second term is further bounded asExh
Î£âˆ’1/2
Î¨,NÏˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î£âˆ’1
Î¨,NÏˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î£âˆ’1/2
Î¨,Ni
op
â‰¤Exh
Î£âˆ’1/2
Î¨,NÏˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤Î£âˆ’1/2
Î¨,Ni
opÂ·Ïˆâ—¦âŠ¤
Â¯N:Â¯NÎ£âˆ’1
Î¨,NÏˆâ—¦
Â¯N:Â¯N
Lâˆ(PX)
â‰²âˆ¥INâˆ¥opÂ·Â¯NX
j=Â¯N(Ïˆâ—¦
j)2
Lâˆ(PX)
â‰²N2r,
again by Assumption 1.
Hence we may apply Corollary A.3 with v(Z)â‰²nN2r,Lâ‰²N2rto conclude:
EXï£®
ï£°Î£âˆ’1/2
Î¨,NÎ¨â—¦
Â¯N:Â¯NÎ¨â—¦âŠ¤
Â¯N:Â¯N
nÎ£âˆ’1/2
Î¨,Nâˆ’IN2
opï£¹
ï£»=E"1
nnX
i=1Si2
op#
â‰²N2r
nlogN+N4r
n2log2N.
Proof of Corollary A.3. From Theorem A.2 and with the change of variables Î»=t2/n2, we have
Pr1
n2âˆ¥Zâˆ¥2
opâ‰¥Î»
â‰¤2Nexp
âˆ’n2Î»
2v(Z) +2
3Lnâˆš
Î»
.
Since the probability is also bounded above by 1, it follows that
Pr1
n2âˆ¥Zâˆ¥2
opâ‰¥Î»
â‰¤1âˆ§2Nexp
âˆ’n2Î»
2(2v(Z)âˆ¨2
3Lnâˆš
Î»)
â‰¤1âˆ§2Nexp
âˆ’n2Î»
4v(Z)
+ 1âˆ§2Nexp
âˆ’3nâˆš
Î»
4L
,
and the expectation can be controlled as
E1
n2âˆ¥Zâˆ¥2
op
=Zâˆ
0Pr1
n2âˆ¥Zâˆ¥2
opâ‰¥Î»
dÎ»
â‰¤Zâˆ
01âˆ§2Nexp
âˆ’n2Î»
4v(Z)
dÎ»+Zâˆ
01âˆ§2Nexp
âˆ’3nâˆš
Î»
4L
dÎ».
For the first integral, we truncate at Î»1:=4v(Z)
n2log 2Nso that
Zâˆ
01âˆ§2Nexp
âˆ’n2Î»
4v(Z)
dÎ»=Î»1+Zâˆ
Î»12Nexp
âˆ’n2Î»
4v(Z)
dÎ»
=4v(Z)
n2log 2N+4v(Z)
n2.
For the second integral, we truncate at Î»2:= 4L
3nlog 2N2so that
Zâˆ
01âˆ§2Nexp
âˆ’3nâˆš
Î»
4L
dÎ»
=Î»2+Zâˆ
Î»22Nexp
âˆ’3nâˆš
Î»
4L
dÎ»
=Î»2âˆ’16LN
3nâˆš
Î»+4L
3n
exp
âˆ’3nâˆš
Î»
4Lâˆ
Î»=Î»2
=4L
3nlog 2N2
+8L
3n4L
3nlog 2N+4L
3n
.
Adding up, we conclude that
E1
n2âˆ¥Zâˆ¥2
op
â‰¤4v(Z)
n2(1 + log 2 N) +16L2
9n2(2 + 2 log 2 N+ (log 2 N)2)
as was to be shown.
21B Proofs on Metric Entropy
B.1 Modified Proof of Theorem 3.1
For a full proof of the original statement, we refer the reader to Section B.1 of Hayakawa and Suzuki
(2020), which corrects some technical flaws in the original proof. Here we only outline the necessary
modification to incorporate the bounded noise setting.
Denote an Ïµ-cover of the model space by f1,Â·Â·Â·, fM. The only step which relies on the normality of
noise Î¾1:nis a concentration result for the random variables
Îµj:=Pn
i=1Î¾i(fj(xi)âˆ’fâ—¦(xi))

(Pn
i=1(fj(xi)âˆ’fâ—¦(xi))21/2,1â‰¤jâ‰¤M,
where it is shown via the normality of Îµjthat
E
max
1â‰¤jâ‰¤MÎµ2
j
â‰¤4Ïƒ2(logM+ 1).
We will instead rely on Hoeffdingâ€™s inequality. By writing Îµj=wâŠ¤
jÎ¾1:n=Pn
i=1wj,iÎ¾iwhere
wj,i=fj(xi)âˆ’fâ—¦(xi)

(Pn
i=1(fj(xi)âˆ’fâ—¦(xi))21/2,
since|wj,iÎ¾i| â‰¤Ïƒ|wj,i|a.s. it follows that
Pr (|Îµj| â‰¥u)â‰¤2 exp
âˆ’2u2
Pn
i=1(2Ïƒ|wj,i|)2
= 2 exp
âˆ’u2
2Ïƒ2
.
for all u >0. Then the squared-exponential moment of each Îµjis bounded as
E
exp(tÎµ2
j)
= 1 +Zâˆ
1Pr 
exp(tÎµ2
j)â‰¥Î»
dÎ»
â‰¤1 +Zâˆ
12 exp
âˆ’logÎ»
2Ïƒ2t
dÎ»
â‰¤1 + 2Zâˆ
1Î»âˆ’1
2Ïƒ2tdÎ»â‰¤3
by setting t=1
4Ïƒ2. Hence via Jensenâ€™s inequality we obtain
exp
tE
max
1â‰¤jâ‰¤MÎµ2
j
â‰¤E
max
1â‰¤jâ‰¤Mexp(tÎµ2
j)
â‰¤MX
j=1E
exp(tÎµ2
j)
â‰¤3M
and thus
E
max
1â‰¤jâ‰¤MÎµ2
j
â‰¤4Ïƒ2log 3M,
retrieving the original result up to a constant.
B.2 Proof of Lemma 3.3
Let us take two functions fÎ˜1, fÎ˜2âˆˆ TNforÎ˜i= (Î“ i, Ï•i),i= 1,2separated as
âˆ¥Î“1âˆ’Î“2âˆ¥opâ‰¤Î´1,max
1â‰¤jâ‰¤Nâˆ¥Ï•1,jâˆ’Ï•2,jâˆ¥Lâˆ(PX)â‰¤Î´2.
Then it holds that
|fÎ˜1(X,y,Ëœx)âˆ’fÎ˜2(X,y,Ëœx)|
â‰¤ |Ë‡fÎ˜1(X,y,Ëœx)âˆ’Ë‡fÎ˜2(X,y,Ëœx)|
=Î“1Ï•1(X)y
n, Ï•1(Ëœx)
âˆ’Î“2Ï•2(X)y
n, Ï•2(Ëœx)
22=1
nÏ•1(Ëœx)âŠ¤Î“1Ï•1(X)yâˆ’Ï•2(Ëœx)âŠ¤Î“1Ï•1(X)y+Ï•2(Ëœx)âŠ¤Î“1Ï•1(X)y
âˆ’Ï•2(Ëœx)âŠ¤Î“2Ï•1(X)y+Ï•2(Ëœx)âŠ¤Î“2Ï•1(X)yâˆ’Ï•2(Ëœx)âŠ¤Î“2Ï•2(X)y
â‰¤1
nâˆ¥Ï•1(Ëœx)âˆ’Ï•2(Ëœx)âˆ¥âˆ¥Î“1âˆ¥opâˆ¥Ï•1(X)yâˆ¥+1
nâˆ¥Ï•2(Ëœx)âˆ¥âˆ¥Î“1âˆ’Î“2âˆ¥opâˆ¥Ï•1(X)yâˆ¥
+1
nâˆ¥Ï•2(Ëœx)âˆ¥âˆ¥Î“2âˆ¥opâˆ¥(Ï•1(X)âˆ’Ï•2(X))yâˆ¥
â‰¤âˆš
NÎ´2C2
n+Bâ€²
NÎ´1
nnX
i=1âˆ¥Ï•1(xi)âˆ¥|yi|+Bâ€²
NC2
nnX
i=1âˆ¥Ï•1(xi)âˆ’Ï•2(xi)âˆ¥|yi|
â‰¤(âˆš
NÎ´2C2+Bâ€²
NÎ´1)Bâ€²
N(B+Ïƒ) +Bâ€²
NC2âˆš
NÎ´2(B+Ïƒ)
=Bâ€²2
N(B+Ïƒ)Î´1+ 2Bâ€²
N(B+Ïƒ)C2âˆš
NÎ´2.
Hence to construct an Ïµ-cover GTofTN, it suffices to exhibit a Î´1-cover GSofSNand a Î´2-cover
GFofFNfor
Î´1=Ïµ
2Bâ€²2
N(B+Ïƒ), Î´ 2=Ïµ
4Bâ€²
N(B+Ïƒ)C2âˆš
N
and set GT=GSÃ—GF. For the metric entropy, this implies
V(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰¤ V(SN,âˆ¥Â·âˆ¥op, Î´1) +V(FN,âˆ¥Â·âˆ¥Lâˆ, Î´2).
We further bound the metric entropy of SNwith the following result, proved in Appendix B.3.
Lemma B.1. ForÎ´â‰¤1
2it holds that V(SN,âˆ¥Â·âˆ¥op, Î´)â‰²N2log1
Î´.
Substituting into the above, we conclude that
V(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²N2logBâ€²2
N
Ïµ+V 
FN,âˆ¥Â·âˆ¥Lâˆ,Ïµ
4Bâ€²
N(B+Ïƒ)C2âˆš
N!
.
The choice of Î´2is not important as long as the metric entropy of FNis at most polynomial in Î´2.
B.3 Proof of Lemma B.1
LetÎ“1,Î“2âˆˆ SNand consider their diagonalizations
Î“i=UiÎ›iUâŠ¤
i,Î›i= diag( Î»i,1,Â·Â·Â·, Î»i,N), i= 0,1,
where Uiâˆˆ O N, the orthogonal group in dimension N, and 0â‰¤Î»i,jâ‰¤C3for each 1â‰¤jâ‰¤N.
Assuming
âˆ¥U1âˆ’U2âˆ¥opâ‰¤Î´
4C3,|Î»1,jâˆ’Î»2,j| â‰¤Î´
2âˆ€jâ‰¤N,
it follows that
âˆ¥Î“1âˆ’Î“2âˆ¥op
=âˆ¥U1Î›1UâŠ¤
1âˆ’U1Î›1UâŠ¤
2+U1Î›1UâŠ¤
2âˆ’U2Î›1UâŠ¤
2+U2Î›1UâŠ¤
2âˆ’U2Î›2UâŠ¤
2âˆ¥op
â‰¤2âˆ¥Î›1âˆ¥Lâˆâˆ¥U1âˆ’U2âˆ¥op+âˆ¥Î›1âˆ’Î›2âˆ¥Lâˆ
â‰¤2C3Â·Î´
4C3+Î´
2=Î´.
Moreover, the covering number of ONin operator norm is given by the following result.
Theorem B.2 (Szarek (1981), Proposition 6) .There exist universal constants c1, c2>0such that
for all NâˆˆNandÎ´âˆˆ(0,2],
c1
Î´N(Nâˆ’1)
2â‰¤ N(ON,âˆ¥Â·âˆ¥op, Î´)â‰¤c2
Î´N(Nâˆ’1)
2.
23Hence we obtain that
V(SN,âˆ¥Â·âˆ¥op, Î´)â‰¤ V
ON,âˆ¥Â·âˆ¥op,Î´
4C3
+V
[0, C3]N,âˆ¥Â·âˆ¥Lâˆ,Î´
2
â‰¤N(Nâˆ’1)
2logc2
Î´+Nlog2C3
Î´
â‰²N2log1
Î´.
Finally, we remark that if elements of the domain SNare not constrained to be symmetric, we can
alternatively consider the singular value decomposition and separately bound entropy of the two
rotation components, giving the same result up to constants.
C Details on Besov-type Spaces
C.1 Besov Space
C.1.1 Verification of Assumptions
We first give some background on wavelet decomposition. The decay rate s=Î±/d is intrinsic to
the Besov space as shown by the following result, which allows us to translate between functions
fâˆˆBÎ±
p,q(X)and their B-spline coefficient sequences.
Lemma C.1 (DeVore and Popov (1988), Corollary 5.3) .IfÎ± > d/p andm > Î± + 1âˆ’1/p, a
function fâˆˆLp(X)is inBÎ±
p,q(X)if and only if fcan be represented as
f=âˆX
k=0X
â„“âˆˆId
kËœÎ²k,â„“Ï‰d
k,â„“
such that the coefficients satisfy
âˆ¥ËœÎ²âˆ¥bÎ±p,q:="âˆX
k=0"
2k(Î±âˆ’d/p)X
â„“âˆˆId
k|ËœÎ²k,â„“|p1/p#q#1/q
<âˆ,
with appropriate modifications if p=âˆorq=âˆ. Moreover, the two norms âˆ¥ËœÎ²âˆ¥bÎ±p,qandâˆ¥fâˆ¥BÎ±p,q
are equivalent.
In particular, this implies that for any fâˆˆU(BÎ±
p,q(X))thep-norm average of ËœÎ²k,â„“forâ„“âˆˆId
kat
resolution kis bounded as
 
1
|Id
k|X
â„“âˆˆId
k|ËœÎ²k,â„“|p!1/p
â‰²(2âˆ’kd)1/pÂ·2k(d/pâˆ’Î±)âˆ¥fâˆ¥BÎ±p,qâ‰¤2âˆ’kÎ±,
and the coefficients Î²k,â„“= 2âˆ’kd/2ËœÎ²k,â„“w.r.t. the scaled basis (2kd/2Ï‰d
k,â„“)k,â„“satisfy
 
1
|Id
k|X
â„“âˆˆId
k|Î²k,â„“|p!1/p
â‰²(2kd)âˆ’Î±/dâˆ’1/2. (18)
Thus it is natural in a probabilistic sense to assume E[Î²p
k,â„“]1/pâ‰²(2kd)âˆ’Î±/dâˆ’1/2. This will be
the case if for instance we sample (Î²k,â„“)â„“âˆˆId
kuniformly from the p-norm ball (18). This matches
Assumption 4 up to the logarithmic factor and hence the rate is nearly tight in variance, even though
(18) only applies to the average over locations rather than each coefficient explicitly. See also the
discussion in Lemma 2 of Suzuki (2019).
24Assumption 1. We take mto be even for simplicity. The wavelet system (Ï‰d
K,â„“)â„“âˆˆId
Kat each
resolution Kis linearly independent; for any gâˆˆL2(X)that can be expressed as
g=X
â„“âˆˆId
KÎ²K,â„“2Kd/2Ï‰d
K,â„“
we have the quasi-norm equivalence (D Ëœung, 2011b, 2.15)
âˆ¥gâˆ¥2â‰2âˆ’Kd/2X
â„“âˆˆId
K2KdÎ²2
K,â„“1/2
=âˆ¥(Î²K,â„“)â„“âˆˆId
Kâˆ¥2
which implies that the covariance matrix Exâˆ¼Unif([0 ,1]d)[Ïˆâ—¦
Â¯N:Â¯N(x)Ïˆâ—¦
Â¯N:Â¯N(x)âŠ¤]is bounded above and
below. Since we assume PXhas Lebesgue density bounded above and below, it follows that Î£Î¨,Nis
uniformly bounded above and below for all Kâ‰¥0.
In contrast, any B-spline at a lower resolution k < K can be exactly expressed as a linear combination
of elements of (Ï‰d
K,â„“)â„“âˆˆId
Kby repeatedly applying the following relation.
Lemma C.2 (refinement equation) .For even mandr= (r1,Â·Â·Â·, rd)âŠ¤,1= (1,Â·Â·Â·,1)âŠ¤âˆˆRdit
holds that
Ï‰d
k,â„“=mX
r1,Â·Â·Â·,rd=02(âˆ’m+1)ddY
i=1m
ri
Â·Ï‰d
k+1,2â„“+râˆ’m
21. (19)
Proof. The relation for one-dimensional wavelets is given in equation (2.21) of D Ëœung (2011b),
Î¹m(x) = 2âˆ’m+1mX
r=0m
r
Î¹m
2xâˆ’r+m
2
,
from which it follows that
Ï‰d
k,â„“(x) =dY
i=1Î¹m(2kxiâˆ’â„“i)
=mX
r1,Â·Â·Â·,rd=02(âˆ’m+1)ddY
i=1m
ri
Î¹m
2k+1xiâˆ’2â„“iâˆ’ri+m
2
=mX
r1,Â·Â·Â·,rd=02(âˆ’m+1)ddY
i=1m
ri
Â·Ï‰d
k+1,2â„“+râˆ’m
21(x)
as was to be shown.
Therefore we select all B-splines at a fixed resolution Kto approximate the target tasks,
N=Id
K= (m+ 1 + 2K)dâ‰2Kd
and
Â¯N=Kâˆ’1X
k=0Id
k+ 1â‰N, Â¯N=KX
k=0Id
kâ‰N.
It is straightforward to see that 0â‰¤Ï‰d
k,â„“(x)â‰¤1for all xâˆˆ X and moreover the B-splines (extended
to all â„“âˆˆZd) form a partition of unity of Rdat all resolutions:
X
â„“âˆˆZdÏ‰d
k,â„“(x)â‰¡1,âˆ€xâˆˆRd,âˆ€kâ‰¥0.
Then for all xâˆˆ X we have the bound
Â¯NX
j=Â¯NÏˆâ—¦
j(x)2=X
â„“âˆˆId
K2KdÏ‰d
K,â„“(x)2â‰¤2KdX
â„“âˆˆZdÏ‰d
K,â„“(x) = 2Kdâ‰²N,
and hence (2) holds with r= 1/2.
25Assumption 2. For any Î²âˆˆsuppPÎ²we have that
âˆ¥Fâ—¦
Î²âˆ¥Lâˆ(PX)â‰¤âˆX
k=0X
â„“âˆˆId
kÎ²k,â„“Â·2kd/2Ï‰d
k,â„“
Lâˆ(PX)
â‰¤âˆX
k=02kd/2max
â„“âˆˆId
k|Î²k,â„“| Â·X
â„“âˆˆId
kÏ‰d
k,â„“
Lâˆ(X)
â‰¤âˆX
k=02kd(1/2+1/p) 
1
|Id
k|X
â„“âˆˆId
k|Î²k,â„“|p!1/pX
â„“âˆˆId
kÏ‰d
k,â„“
Lâˆ(X)
â‰²âˆX
k=02kd(1/2+1/p)Â·(2kd)âˆ’Î±/dâˆ’1/2âˆ¥Fâ—¦
Î²âˆ¥BÎ±p,q
â‰²(1âˆ’2d/pâˆ’Î±)âˆ’1=:B.
Furthermore, the convergence rate of the truncated approximation Fâ—¦
Î²,Â¯Nis determined by the decay
rate of Î²in Lemma C.1 as follows (it does not matter whether we bound Fâ—¦
Î²,Â¯NorFâ—¦
Î²,Nsince Â¯Nâ‰N).
We consider a truncation of all resolutions lower than Kso that Â¯N, Nâ‰2Kd. Then it holds that
âˆ¥Fâ—¦
Î²âˆ’Fâ—¦
Î²,Â¯Nâˆ¥2
L2(PX)=Z
XâˆX
j=Â¯N+1Î²jÏˆâ—¦
j(x)2
PX(dx) =âˆX
j,k=Â¯N+1Î²jÎ²kEx[Ïˆâ—¦
j(x)Ïˆâ—¦
k(x)]
â‰¤lim
Mâ†’âˆC2âˆ¥Î²Â¯N:Mâˆ¥2=C2âˆX
k=KX
â„“âˆˆId
kÎ²2
k,â„“
â‰²âˆX
k=K|Id
k|1âˆ’2/p X
â„“âˆˆId
k|Î²k,â„“|p!2/p
â‰²âˆX
k=K2kdÂ·(2kd)âˆ’2Î±/dâˆ’1
=2âˆ’2Î±K
1âˆ’2âˆ’2Î±â‰Nâˆ’2Î±/d,
where for the last two inequalities we have used the inequality âˆ¥zâˆ¥2â‰¤D1/2âˆ’1/pâˆ¥zâˆ¥pforzâˆˆRD
andpâ‰¥2in conjunction with (18). Thus our choice of s=Î±/d is justified. Under this choice, (5)
directly implies (4) as
EÎ²[Î²2
K,â„“]â‰²2âˆ’Kd(2s+1)Kâˆ’2â‰Â¯Nâˆ’2sâˆ’1(logÂ¯N)âˆ’2
holds for the basis elements at each resolution K, that is for those numbered betweenÂ¯NandÂ¯N.
Remark C.3. When 1â‰¤p <2, the truncation up to Â¯Ndoes not suffice to achieve the Nâˆ’2Î±/d
approximation rate, and basis elements must be judiciously selected from a wider resolution range.
More concretely, a size Nsubset of all wavelets up to resolution Kâ€²=âŒˆK(1 + Î½âˆ’1)âŒ‰where
Î½=pÎ±/2dâˆ’1/2>0must be used (Suzuki and Nitanda, 2021, Lemma 2). Hence the exponent is a
factor of 1 +Î½âˆ’1worse w.r.t. Nâ€²â‰2Kâ€²d, leading to the inevitable suboptimal rate.
To show boundedness of Tr(Î£ Â¯Î²,N), we analyze the composition of the aggregated coefficients Â¯Î²
using the following result.
Corollary C.4. For any 0â‰¤k < kâ€²there exists constants Î³k,kâ€²,â„“,â„“â€²â‰¥0forâ„“âˆˆId
k,â„“â€²âˆˆId
kâ€²such
that X
â„“âˆˆId
kÎ²k,â„“2kd/2Ï‰d
k,â„“=X
â„“â€²âˆˆId
kâ€²Â¯Î²kâ€²,â„“â€²2kâ€²d/2Ï‰d
kâ€²,â„“â€²,Â¯Î²kâ€²,â„“â€²=X
â„“âˆˆId
kÎ³k,kâ€²,â„“,â„“â€²Î²k,â„“
holds for all (Î²k,â„“)â„“âˆˆId
k. Moreover, it holds that
X
â„“âˆˆId
kÎ³k,kâ€²,â„“,â„“â€²â‰¤2(kâˆ’kâ€²)d/2,X
â„“â€²âˆˆId
kâ€²Î³k,kâ€²,â„“,â„“â€²â‰¤2(kâ€²âˆ’k)d/2.
26The statement follows directly from the more general Proposition C.10, stated and proved in Appendix
C.3 below, by restricting to wavelets with uniform resolution across dimensions. Using Corollary
C.4, we can refine each lower resolution component of Fâ—¦
Î²to resolution K:
Fâ—¦
Î²,Â¯N=Â¯NX
j=1Î²jÏˆâ—¦
j=KX
k=0X
â„“âˆˆId
kÎ²k,â„“2kd/2Ï‰d
k,â„“=KX
k=0X
â„“â€²âˆˆId
KX
â„“âˆˆId
kÎ³k,K,â„“,â„“â€²Î²k,â„“2Kd/2Ï‰d
K,â„“â€².
Thus each aggregated coefficient, indexed here by â„“âˆˆId
K, can be expressed as
Â¯Î²K,â„“=KX
k=0X
â„“âˆˆId
kÎ³k,K,â„“,â„“â€²Î²k,â„“.
Hence it follows that
EÎ²[Â¯Î²2
K,â„“] =KX
k=0X
â„“âˆˆId
kÎ³2
k,K,â„“,â„“â€²EÎ²[Î²2
k,â„“]â‰²KX
k=0 X
â„“âˆˆId
kÎ³k,K,â„“,â„“â€²!2
2âˆ’k(2Î±+d)kâˆ’2
â‰¤KX
k=02(kâˆ’K)dÂ·2âˆ’k(2Î±+d)kâˆ’2
â‰²2âˆ’KdÂ·KX
k=02âˆ’2kÎ±kâˆ’2â‰Nâˆ’1,
from which we conclude that Tr(Î£ Â¯Î²,N) =P
â„“âˆˆId
KEÎ²[Â¯Î²2
K,â„“]â‰²NÂ·Nâˆ’1is uniformly bounded.
Finally for the verification of Assumption 3, see Appendix C.1.2.
C.1.2 Proof of Lemma 4.4
We use the following result to approximate each wavelet Ï‰d
K,â„“at resolution Kwith the class (6). The
proof, in turn, relies on the construction by Yarotsky (2016) of DNNs which efficiently approximates
the multiplication operation.
Lemma C.5 (Suzuki (2019), Lemma 1) .For all Î´ > 0, there exists a ReLU neural network
ËœÏ‰âˆˆ F DNN(L, W, S, M )with
L= 3 + 2l
log2
3dâˆ¨m(1 +dmâˆ’1/2(2e)m+1)Î´âˆ’1
+ 5m
âŒˆlog2(dâˆ¨m)âŒ‰,
W=W0= 6dm(m+ 2) + 2 d, S =LW2, M = 2(m+ 1)m
satisfying supp Ëœ Ï‰âŠ†[0, m+ 1]dandâˆ¥Ï‰d
0,0âˆ’ËœÏ‰âˆ¥Lâˆ(X)â‰¤Î´.
Here, Î´is also dependent on N.
Now consider Nidentical copies of ËœÏ‰in parallel, where each module is preceded by the scaling
(xi)d
i=17â†’(2Kxiâˆ’â„“i)d
i=1forâ„“âˆˆId
Kand whose output is scaled by 2Kd/2. In particular, these
operations can be implemented by Kâ‰²logNconsecutive additional layers with norm bounded by a
constant. Hence each module Ï•âˆ—
Â¯N,Â·Â·Â·, Ï•âˆ—
Â¯Napproximates the basis 2Kd/2Ï‰d
K,â„“with2Kd/2Î´â‰²âˆš
NÎ´
accuracy, and substituting Î´N=âˆš
NÎ´gives that
âˆ¥Ïˆâ—¦
jâˆ’Ï•âˆ—
jâˆ¥Lâˆ(PX)â‰¤Î´N,Â¯Nâ‰¤jâ‰¤Â¯N,
withLâ‰²logÎ´âˆ’1+ log Nâ‰²logÎ´âˆ’1
N+ log N. Note that the sparsity Sis only multiplied by a factor
ofNsince different modules do not share any connections. Moreover the target basis has 2-norm
bounded as
âˆ¥Ïˆâ—¦
Â¯N:Â¯N(x)âˆ¥2â‰² X
â„“âˆˆId
K2KdÏ‰K,â„“(x)2!1/2
â‰¤ 
2KdX
jâˆˆZdÏ‰K,â„“(x)!1/2
â‰âˆš
N,
where we have again used the sparsity of Ï‰d
k,â„“at each resolution. Hence we may clip the magnitude
of the vector output Ï•byBâ€²
Nand the approximation guarantee remains unchanged.
To bound the covering number of FN, we directly apply the following result.
27Lemma C.6 (Suzuki (2019), Lemma 3) .The covering number of FDNNis bounded as
N(FDNN(L, W, S, M ),âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰¤L(Mâˆ¨1)Lâˆ’1(W+ 1)2L
ÏµS
.
Since clipping the magnitude of the outputs does not increase the covering number, we conclude:
V(FN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰¤NÂ· V(FDNN(L, W, S, M ),âˆ¥Â·âˆ¥Lâˆ, Ïµ)
â‰¤SNlogL+SLN logM+ 2SLN log(W+ 1) + SNlog1
Ïµ
â‰²NlogN
Î´N+Nlog1
Ïµ.
C.1.3 Proof of Theorem 4.5
By Lemma 3.3 and Lemma 4.4, the metric entropy of TNis bounded as
V(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²N2logN
Ïµ+NlogN2
Î´NÏµ.
Combining with Theorem 3.1 and Proposition 3.2 with r= 1/2ands=Î±/d gives
Â¯R(bÎ˜)â‰²N
nlogN+N2
n2log2N+Nâˆ’2Î±/d+N2Î´2
N+1
T
N2logN
Ïµ+NlogN2
Î´NÏµ
+Ïµ.
Substituting Î´Nâ‰Nâˆ’1âˆ’Î±/dandÏµâ‰Nâˆ’2Î±/dyields the desired bound.
C.2 Anisotropic Besov Space
C.2.1 Definitions and Results
For1â‰¤p, qâ‰¤ âˆ , directional smoothness Î±= (Î±1,Â·Â·Â·, Î±d)âˆˆRd
>0andr= max iâ‰¤dâŒŠÎ±iâŒ‹+ 1, we
define âˆ¥Â·âˆ¥BÎ±p,q=âˆ¥Â·âˆ¥Lp+|Â·|BÎ±p,qwhere
|f|BÎ±p,q:=( Pâˆ
k=0
2kwr,p(f,(2âˆ’k/Î±1,Â·Â·Â·,2âˆ’k/Î±d))q1/qq <âˆ
supkâ‰¥02kwr,p(f,(2âˆ’k/Î±1,Â·Â·Â·,2âˆ’k/Î±d)) q=âˆ.
The anisotropic Besov space is defined as
BÎ±
p,q(X) ={fâˆˆLp(X)| âˆ¥fâˆ¥BÎ±p,q<âˆ}.
The definition reduces to the usual Besov space if Î±1=Â·Â·Â·=Î±d; see VybÃ­ral (2006); Triebel (2011)
for details. We also write Î±= max iÎ±i,Î±= min iÎ±iand the harmonic mean smoothness as
eÎ±:=dX
i=1Î±âˆ’1
iâˆ’1
.
For the anisotropic Besov space, we need to redefine the wavelet basis so that the sensitivity to
resolution kâˆˆZâ‰¥0differs for each component depending on Î±. Define the quantities
âˆ¥kâˆ¥Î±/Î±:=dX
i=1âŒŠkÎ±/Î±iâŒ‹, Id,Î±
k:=dY
i=1{âˆ’m,âˆ’m+ 1,Â·Â·Â·,2âŒŠkÎ±/Î±iâŒ‹} âŠ‚Zd.
We then set for each kâ‰¥0andâ„“âˆˆId,Î±
k
Ï‰d,Î±
k,â„“(x) :=Ï‰d
(âŒŠkÎ±/Î±1âŒ‹,Â·Â·Â·,âŒŠkÎ±/Î±dâŒ‹),â„“(x) =dY
i=1Î¹m(2âŒŠkÎ±/Î±iâŒ‹xiâˆ’â„“i),
28and take the scaled basis
{Ïˆâ—¦
j|jâˆˆN}={2âˆ¥kâˆ¥Î±/Î±/2Ï‰d,Î±
k,â„“|kâˆˆZâ‰¥0, â„“âˆˆId,Î±
k}
with the natural hierarchy induced by k.
The minimax optimal rate for the anisotropic Besov space is equal to nâˆ’2eÎ±
2eÎ±+1(Suzuki and Nitanda,
2021, Theorem 4). Our result for in-context learning is as follows.
Theorem C.7 (minimax optimality of ICL in anisotropic Besov space) .LetÎ±âˆˆRd
>0witheÎ± >1/p
andFâ—¦=U(BÎ±
p,q(X)). Suppose that PXhas positive Lebesgue density ÏXbounded above and
below on X. Also suppose that all coefficients are independent and
EÎ²[Î²k,â„“] = 0,EÎ²[Î²2
k,â„“]â‰²2âˆ’kÎ±(2+1/eÎ±)kâˆ’2,âˆ€kâ‰¥0, â„“âˆˆId,Î±
k. (20)
Then for nâ‰³NlogNwe have
Â¯R(bÎ˜)â‰²Nâˆ’2eÎ±+NlogN
n+N2logN
T.
Hence if Tâ‰³nNandNâ‰n1
2eÎ±+1, in-context learning achieves the rate nâˆ’2eÎ±
2eÎ±+1lognwhich is
minimax optimal up to a polylog factor.
C.2.2 Proof of Theorem C.7
The overall approach is similar to Appendix C.1. The decay rate of functions in the anisotropic Besov
space is characterized by the following result which extends Lemma C.1.
Lemma C.8 (Suzuki and Nitanda (2021), Lemma 2) .IfeÎ± >1/pandm > Î±+ 1âˆ’1/p, a function
fâˆˆLp(X)is inMBÎ±
p,q(X)if and only if fcan be represented as
f=X
kâˆˆZd
â‰¥0X
â„“âˆˆId,Î±
kËœÎ²k,â„“Ï‰d,Î±
k,â„“(x)
such that the coefficients satisfy
âˆ¥ËœÎ²âˆ¥bÎ±p,q:="âˆX
k=0"
2kÎ±âˆ’âˆ¥kâˆ¥Î±/Î±/pX
â„“âˆˆId,Î±
k|ËœÎ²k,â„“|p1/p#q#1/q
â‰²âˆ¥fâˆ¥BÎ±p,q.
Moreover, the two norms âˆ¥ËœÎ²âˆ¥bÎ±p,qandâˆ¥fâˆ¥BÎ±p,qare equivalent.
We again select all B-splines (Ï‰d,Î±
K,â„“)â„“âˆˆId
Kat each resolution Kto approximate the target functions. By
repeatedly applying the refinement equation for one-dimensional wavelets as many times as needed
to each dimension separately, we may express any B-spline at a lower resolution k < K as a linear
combination of (Ï‰d,Î±
K,â„“)â„“âˆˆId
Ksimilarly to Lemma C.2. See Proposition C.10 for details. We thus have
N=|Id,Î±
K|=dY
i=1(m+ 1 + 2âŒŠKÎ±/Î±iâŒ‹)â‰2âˆ¥Kâˆ¥Î±/Î±.
Sinceâˆ¥kâˆ¥Î±/Î±=kÎ±/eÎ±+Ok(1)always holds, it also follows that
Â¯N=KX
k=0|Id,Î±
k|+ 1â‰KX
k=02âˆ¥kâˆ¥Î±/Î±â‰KX
k=0(2Î±/eÎ±)kâ‰2KÎ±/eÎ±â‰N
and similarlyÂ¯Nâ‰N. Therefore,
Â¯NX
j=Â¯NÏˆâ—¦
j(x)2â‰¤2âˆ¥Kâˆ¥Î±/Î±X
â„“âˆˆId,Î±
kÏ‰d,Î±
K,â„“(x)2â‰¤2âˆ¥Kâˆ¥Î±/Î±â‰N
29and the scaled coefficients decay in average as
 
1
|Id,Î±
k|X
â„“âˆˆId,Î±
k|Î²k,â„“|p!1/p
â‰² dY
i=12âŒŠkÎ±/Î±iâŒ‹!âˆ’1/p
2âˆ’âˆ¥kâˆ¥Î±/Î±/2 X
â„“âˆˆId,Î±
k|ËœÎ²k,â„“|p!1/p
â‰²2âˆ’âˆ¥kâˆ¥Î±/Î±/2âˆ’kÎ±âˆ¥fâˆ¥BÎ±
p,q
â‰Nâˆ’(eÎ±+1/2)âˆ¥fâˆ¥BÎ±p,q.
For Assumption 2, we can check that
âˆ¥Fâ—¦
Î²âˆ¥Lâˆ(PX)â‰¤âˆX
k=0X
â„“âˆˆId,Î±
kÎ²k,â„“Â·2âˆ¥kâˆ¥Î±/Î±/2Ï‰d,Î±
k,â„“
Lâˆ(PX)
â‰¤âˆX
k=02(1/2+1/p)âˆ¥kâˆ¥Î±/Î± 
1
|Id,Î±
k|X
â„“âˆˆId,Î±
k|Î²k,â„“|p!1/p
â‰²âˆX
k=02(1/2+1/p)âˆ¥kâˆ¥Î±/Î±Â·2âˆ’âˆ¥kâˆ¥Î±/Î±/2âˆ’kÎ±
â‰²
1âˆ’2Î±/eÎ±(1/pâˆ’eÎ±)âˆ’1
=:B
and for a resolution cutoff K > 0,Â¯Nâ‰2âˆ¥Kâˆ¥Î±/Î±the truncation error satisfies
âˆ¥Fâ—¦
Î²âˆ’Fâ—¦
Î²,Â¯Nâˆ¥2
L2(PX)â‰²âˆX
k=K+1X
â„“âˆˆId,Î±
kÎ²2
k,â„“â‰²âˆX
k=K+1|Id,Î±
k|1âˆ’2/p X
â„“âˆˆId,Î±
k|Î²k,â„“|p!2/p
â‰²âˆX
k=K+12âˆ¥kâˆ¥Î±/Î±Â·2âˆ’âˆ¥kâˆ¥Î±/Î±âˆ’2kÎ±â‰2âˆ’2KÎ±â‰Nâˆ’2eÎ±.
Thus we may set r= 1/2, s=eÎ±and take the variance decay rate (20) as
EÎ²[Î²2
k,â„“]â‰²2âˆ’âˆ¥kâˆ¥Î±/Î±(2eÎ±+1)kâˆ’2â‰2âˆ’kÎ±(2+1/eÎ±))kâˆ’2.
For boundedness of Tr(Î£ Â¯Î²,N), we use the following result which is also obtained from Proposition
C.10 by considering resolution vectors (âŒŠkÎ±/Î±1âŒ‹,Â·Â·Â·,âŒŠkÎ±/Î±dâŒ‹)and(âŒŠkâ€²Î±/Î±1âŒ‹,Â·Â·Â·,âŒŠkâ€²Î±/Î±dâŒ‹).
Corollary C.9. For any 0â‰¤k < kâ€²there exists constants Î³k,kâ€²,â„“,â„“â€²â‰¥0forâ„“âˆˆId,Î±
k,â„“â€²âˆˆId,Î±
kâ€²such
thatX
â„“âˆˆId,Î±
kÎ²k,â„“2âˆ¥kâˆ¥Î±/Î±/2Ï‰d,Î±
k,â„“=X
â„“â€²âˆˆId,Î±
kâ€²Â¯Î²kâ€²,â„“â€²2âˆ¥kâ€²âˆ¥Î±/Î±/2Ï‰d,Î±
kâ€²,â„“â€²,Â¯Î²kâ€²,â„“â€²=X
â„“âˆˆId,Î±
kÎ³k,kâ€²,â„“,â„“â€²Î²k,â„“
holds for all (Î²k,â„“)â„“âˆˆId,Î±
k. Moreover, it holds that
X
â„“âˆˆId,Î±
kÎ³k,kâ€²,â„“,â„“â€²â‰¤2(âˆ¥kâˆ¥Î±/Î±âˆ’âˆ¥kâ€²âˆ¥Î±/Î±)/2,X
â„“â€²âˆˆId,Î±
kâ€²Î³k,kâ€²,â„“,â„“â€²â‰¤2(âˆ¥kâ€²âˆ¥Î±/Î±âˆ’âˆ¥kâˆ¥Î±/Î±)/2.
We apply Corollary C.9 to refine all components of Fâ—¦
Î²,Â¯Nto resolution K:
Fâ—¦
Î²,Â¯N=KX
k=0X
â„“âˆˆId,Î±
kÎ²k,â„“2âˆ¥kâˆ¥Î±/Î±/2Ï‰d,Î±
k,â„“=KX
k=0X
â„“â€²âˆˆId,Î±
KX
â„“âˆˆId,Î±
kÎ³k,K,â„“,â„“â€²Î²k,â„“2âˆ¥Kâˆ¥Î±/Î±/2Ï‰d,Î±
K,â„“â€².
Hence it follows that
EÎ²[Â¯Î²2
K,â„“] =KX
k=0X
â„“âˆˆId,Î±
kÎ³2
k,K,â„“,â„“â€²EÎ²[Î²2
k,â„“]â‰²KX
k=0 X
â„“âˆˆId,Î±
kÎ³k,K,â„“,â„“â€²!2
2âˆ’kÎ±(2+1/eÎ±)kâˆ’2
30â‰¤KX
k=02âˆ¥kâˆ¥Î±/Î±âˆ’âˆ¥Kâˆ¥Î±/Î±Â·2âˆ’kÎ±(2+1/eÎ±)kâˆ’2
â‰²2âˆ’âˆ¥Kâˆ¥Î±/Î±Â·KX
k=02âˆ’2kÎ±kâˆ’2â‰Nâˆ’1,
and we again conclude that Tr(Î£ Â¯Î²,N)is uniformly bounded.
The rest of the proof proceeds similarly to the ordinary Besov space.
C.3 Wavelet Refinement
In this subsection, we present and prove an auxiliary result concerning the refinement of B-spline
wavelets and the recurrence relations satisfied by their coefficient sequences.
Proposition C.10. For any k, kâ€²âˆˆZd
â‰¥0such that kâ€²âˆ’kâˆˆZd
â‰¥0there exists constants Î³k,kâ€²,â„“,â„“â€²â‰¥0
forâ„“âˆˆId
k,â„“â€²âˆˆId
kâ€²such that
X
â„“âˆˆId
kÎ²k,â„“2âˆ¥kâˆ¥1/2Ï‰d
k,â„“=X
â„“â€²âˆˆId
kâ€²Â¯Î²kâ€²,â„“â€²2âˆ¥kâ€²âˆ¥1/2Ï‰d
kâ€²,â„“â€²,Â¯Î²kâ€²,â„“â€²=X
â„“âˆˆId
kÎ³k,kâ€²,â„“,â„“â€²Î²k,â„“ (21)
holds for all (Î²k,â„“)â„“âˆˆId
k. Moreover, it holds that
X
â„“âˆˆId
kÎ³k,kâ€²,â„“,â„“â€²â‰¤2âˆ’âˆ¥kâ€²âˆ’kâˆ¥1/2,X
â„“â€²âˆˆId
kâ€²Î³k,kâ€²,â„“,â„“â€²â‰¤2âˆ¥kâ€²âˆ’kâˆ¥1/2.
Proof. We proceed by induction on âˆ¥kâ€²âˆ’kâˆ¥1. When kâ€²=k+ejfor some 1â‰¤jâ‰¤d, we can refine
eachÏ‰d
k,â„“using equation (2.21) of D Ëœung (2011b) as
Ï‰d
k,â„“(x) =dY
i=1Î¹m(2kixiâˆ’â„“i)
= 2âˆ’m+1Y
iÌ¸=jÎ¹m(2kixiâˆ’â„“i)mX
r=0m
r
Î¹m
2kj+1xjâˆ’2â„“jâˆ’r+m
2
= 2âˆ’m+1mX
r=0m
r
Ï‰d
k+ej,â„“+(â„“j+râˆ’m
2)ej(x).
Since â„“+ (â„“j+râˆ’m
2)ejmatches a given location vector â„“â€²âˆˆId
k+ejif and only if â„“i=â„“â€²
i(iÌ¸=j)
andâ„“â€²
j= 2â„“j+râˆ’m
2, comparing coefficients in (21) yields
Î³k,k+ej,â„“,â„“â€²= 2âˆ’m+1/21{â„“i=â„“â€²
i(iÌ¸=j)}m
â„“â€²
jâˆ’2â„“j+m
2
.
Here,1Adenotes the indicator function for condition A. It follows that Î³k,k+ej,â„“,â„“â€²â‰¥0and
X
â„“âˆˆId
kÎ³k,k+ej,â„“,â„“â€²â‰¤X
â„“jâˆˆZ2âˆ’m+1/2m
â„“â€²
jâˆ’2â„“j+m
2
â‰¤2âˆ’1/2,
X
â„“â€²âˆˆId
k+ejÎ³k,k+ej,â„“,â„“â€²â‰¤X
â„“â€²
jâˆˆZ2âˆ’m+1/2m
â„“â€²
jâˆ’2â„“j+m
2
â‰¤21/2,
by considering parities.
Now suppose the claim holds for a fixed difference âˆ¥kâ€²âˆ’kâˆ¥1. Applying the above derivation to
further refine resolution kâ€²tokâ€²â€²=kâ€²+ejfor arbitrary jgives
X
â„“âˆˆId
kÎ²k,â„“2âˆ¥kâˆ¥1/2Ï‰d
k,â„“=X
â„“â€²âˆˆId
kâ€²Â¯Î²kâ€²,â„“â€²2âˆ¥kâ€²âˆ¥1/2Ï‰d
kâ€²,â„“â€²=X
â„“â€²â€²âˆˆId
kâ€²+1Â¯Â¯Î²kâ€²+1,â„“â€²â€²2(âˆ¥kâ€²âˆ¥1+1)/2Ï‰d
kâ€²+ej,â„“â€²â€²
31where
Â¯Â¯Î²kâ€²+ej,â„“â€²â€²=X
â„“â€²âˆˆId
kâ€²2âˆ’m+1/21{â„“â€²
i=â„“â€²â€²
i(iÌ¸=j)}m
â„“â€²â€²
jâˆ’2â„“â€²
j+m
2
Â¯Î²kâ€²,â„“â€²
=X
â„“âˆˆId
kX
â„“â€²âˆˆId
kâ€²2âˆ’m+1/21{â„“â€²
i=â„“â€²â€²
i(iÌ¸=j)}m
â„“â€²â€²
jâˆ’2â„“â€²
j+m
2
Î³k,kâ€²,â„“,â„“â€²Î²k,â„“.
Hence we obtain the recurrence relation
Î³k,kâ€²+ej,â„“,â„“â€²â€²=X
â„“â€²âˆˆId
kâ€²2âˆ’m+1/21{â„“â€²
i=â„“â€²â€²
i(iÌ¸=j)}m
â„“â€²â€²
jâˆ’2â„“â€²
j+m
2
Î³k,kâ€²,â„“,â„“â€²,
from which we verify that Î³k,kâ€²+ej,â„“,â„“â€²â€²â‰¥0and
X
â„“âˆˆId
kÎ³k,kâ€²+ej,â„“,â„“â€²â€²=X
â„“â€²âˆˆId
kâ€²2âˆ’m+1/21{â„“â€²
i=â„“â€²â€²
i(iÌ¸=j)}m
â„“â€²â€²
jâˆ’2â„“â€²
j+m
2X
â„“âˆˆId
kÎ³k,kâ€²,â„“,â„“â€²
â‰¤2âˆ’m+1/2âˆ’âˆ¥kâ€²âˆ’kâˆ¥1/2X
â„“â€²
jâˆˆZm
â„“â€²â€²
jâˆ’2â„“â€²
j+m
2
= 2âˆ’(âˆ¥kâ€²âˆ’kâˆ¥1+1)/2,
and furthermore
X
â„“â€²â€²âˆˆId
kâ€²+ejÎ³k,kâ€²+ej,â„“,â„“â€²â€²=X
â„“â€²âˆˆId
kâ€²X
â„“â€²â€²âˆˆId
kâ€²+ej2âˆ’m+1/21{â„“â€²
i=â„“â€²â€²
i(iÌ¸=j)}m
â„“â€²â€²
jâˆ’2â„“â€²
j+m
2
Î³k,kâ€²,â„“,â„“â€²
â‰¤2âˆ’m+1/2X
â„“â€²âˆˆId
kâ€²X
â„“â€²â€²
jâˆˆZm
â„“â€²â€²
jâˆ’2â„“â€²
j+m
2
Î³k,kâ€²,â„“,â„“â€²
â‰¤21/2X
â„“â€²âˆˆId
kâ€²Î³k,kâ€²,â„“,â„“â€²â‰¤2(âˆ¥kâ€²âˆ’kâˆ¥1+1)/2.
This concludes the proof.
C.4 Proof of Corollary 4.8
In order to approximate arbitrary Ïˆâ—¦
jâˆˆU(BÏ„
p,q(X)), we need the following construction instead of
Lemma C.5. Note that Nâ€²corresponds to the number of B-splines used to approximate the target
function and can be freely chosen to match the desired error, which however affects the covering
number of FN.
Lemma C.11 (Suzuki (2019), Proposition 1) .SetmâˆˆN,m > Ï„ + 2âˆ’1/pandÎ½= (pÏ„âˆ’d)/2d.
For all Nâ€²âˆˆNsufficiently large and Ïµ=Nâ€²âˆ’Ï„/d(logNâ€²)âˆ’1, for any fâ—¦âˆˆU(BÏ„
p,q(X))there exists
a ReLU network Ëœfâˆˆ F DNN(L, W, S, M )with
L= 3 + 2l
log2
3dâˆ¨m(1 +dmâˆ’1/2(2e)m+1)Ïµâˆ’1
+ 5m
âŒˆlog2(dâˆ¨m)âŒ‰,
W=Nâ€²W0, S = ((Lâˆ’1)W2
0+ 1)Nâ€², M =O(Nâ€²1/Î½+1/d)
satisfying âˆ¥fâ—¦âˆ’Ëœfâˆ¥Lâˆ(X)â‰¤Nâ€²âˆ’Ï„/d.
Also note that from Assumption 1 it follows that âˆ¥Ïˆjâˆ¥Lâˆ(PX)â‰¤CâˆN1/2. Setting Nâ€²â‰Î´âˆ’d/Ï„
N
and applying the covering number bound in Lemma C.6, after some algebra we obtain the following
counterpart to Lemma 4.4.
Corollary C.12. For any Î´N>0, Assumption 3 is satisfied by taking
FN={Î Bâ€²
Nâ—¦Ï•|Ï•= (Ï•j)N
j=1, Ï•jâˆˆ F DNN(L, W, S, M )}
where Bâ€²
N=CâˆN1/2and
L=O(logÎ´âˆ’1
N), W =O(Î´âˆ’d/Ï„
N), S =O(Î´âˆ’d/Ï„
N logÎ´âˆ’1
N),logM=O(logÎ´N).
32Also, the metric entropy of FNis bounded as
V(FN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²NÎ´âˆ’d/Ï„
N log1
Î´N
log1
Ïµ+ log21
Î´N
.
Then by combining with Lemma 3.3 and Proposition 3.2 via Theorem 3.1, it follows that
V(TN,âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²N2logN
Ïµ+NÎ´âˆ’d/Ï„
N log1
Î´N
logN
Ïµ+ log21
Î´N
.
and
Â¯R(bÎ˜)â‰²N
nlogN+N2
n2log2N+Nâˆ’2Î±/d+N2Î´2
N
+N2
TlogN
Ïµ+N
TÎ´âˆ’d/Ï„
N log1
Î´N
logN
Ïµ+ log21
Î´N
+Ïµ.
Substituting Î´Nâ‰Nâˆ’1âˆ’Î±/dandÏµâ‰Nâˆ’2Î±/dconcludes the desired bound.
D Details on Sequential Input
D.1 Definitions and Results
Î³-smooth class. We first define the Î³-smooth function class introduced by Okumoto and Suzuki
(2022). Let râˆˆZdÃ—âˆ
0 andsâˆˆÂ¯NdÃ—âˆ
0 , where Â¯N=Nâˆª{0}and the subscript 0indicates restriction
to the subset of elements with a finite number of nonzero components. Consider the orthonormal
basis (Ïˆr)rofL2([0,1]dÃ—âˆ)given as
Ïˆr(x) =Y
iâˆˆZdY
j=1Ïˆrij(xij), Ïˆ rij(xij) =ï£±
ï£²
ï£³âˆš
2 cos(2 Ï€rijxij)rij<0
1 rij= 0âˆš
2 sin(2 Ï€rijxij)rij>0.
The frequency scomponent Î´s(f)offâˆˆL2([0,1]dÃ—âˆ)is defined as
Î´s(f) :=X
âŒŠ2sijâˆ’1âŒ‹â‰¤|rij|<2sijâŸ¨f, ÏˆrâŸ©Ïˆr.
For a monotonically nondecreasing function Î³:Â¯NdÃ—âˆ
0â†’Randpâ‰¥2, qâ‰¥1, theÎ³-smooth norm
and function class are defined as
âˆ¥fâˆ¥FÎ³
p,q(PX):= X
sâˆˆÂ¯NdÃ—âˆ
02qÎ³(s)âˆ¥Î´s(f)âˆ¥q
p,PX!1/q
and
FÎ³
p,q(PX) :=
fâˆˆL2([0,1]dÃ—âˆ)| âˆ¥fâˆ¥FÎ³
p,q(PX)<âˆ	
.
TheÎ³-smooth class over finite-dimensional input space [0,1]dÃ—mis similarly defined.
In particular, we consider two specific cases of Î³for the component-wise smoothness parameter
Î±âˆˆRdÃ—âˆ
>0, for which we also define the corresponding degrees of smoothness Î±â€ âˆˆR>0. Denote by
(ËœÎ±j)âˆ
j=1all components of Î±sorted by ascending magnitude.
â€¢ Mixed smoothness: Î³(s) =âŸ¨Î±, sâŸ©,Î±â€ = ËœÎ±1= max i,jÎ±ij.
â€¢ Anisotropic smoothness: Î³(s) = max i,jÎ±ijsij,Î±â€ = (P
i,jÎ±âˆ’1
ij)âˆ’1.
Furthermore, the weak lÎ·-norm of Î±is defined as âˆ¥Î±âˆ¥wlÎ·:= supjjÎ·ËœÎ±âˆ’1
jforÎ· >0.
33Piecewise Î³-smooth class. The piecewise Î³-smooth class is an extension of the Î³-smooth class
allowing for arbitrary bounded permutations of the tokens of an input (Takakura and Suzuki, 2023).
For a threshold VâˆˆNand an index set Î›, let{â„¦Î»}Î»âˆˆÎ›be a disjoint partition of suppPXand
{Ï€Î»}Î»âˆˆÎ›a set of bijections from [2V+ 1] to[âˆ’V:V]. Further define the permutation operator
Î  : supp PXâ†’RdÃ—(2V+1)as
Î (x) = (xÏ€Î»(1),Â·Â·Â·, xÏ€Î»(2V+1)),ifxâˆˆâ„¦Î».
Then the piecewise Î³-smooth function class is defined as
PÎ³
p,q(PX) :=
g=fâ—¦Î |fâˆˆ FÎ³
p,q(PX),âˆ¥gâˆ¥PÎ³
p,q(PX)<âˆ	
,
where
âˆ¥gâˆ¥PÎ³
p,q(PX):= X
sâˆˆÂ¯NdÃ—[âˆ’V:V]
02qÎ³(s)âˆ¥Î´s(f)â—¦Î âˆ¥q
p,PX!1/q
.
Next, we state the set of assumptions inherited from Takakura and Suzuki (2023). In particular, the
importance function makes precise a notion of relative importance between tokens which is preserved
by permutations.
Assumption 5 (smoothness and importance function) .1< qâ‰¤2and:
1.The smoothness parameter Î±satisfies âˆ¥Î±âˆ¥wlÎ·â‰¤1andÎ±ij= â„¦(|i|Î·)for some Î· >1. For
mixed smoothness, we also require ËœÎ±1<ËœÎ±2.
2.There exists a shift-equivariant map Âµ: supp PXâ†’Râˆsuch that Âµ0âˆˆU(FÎ³
âˆ,q),
âˆ¥Âµ0âˆ¥ â‰¤1andâ„¦Î»={xâˆˆsuppPX|Âµ(x)Ï€Î»(1)>Â·Â·Â·> Âµ(x)Ï€Î»(2V+1)}for all Î»âˆˆÎ›.Âµ
is moreover well-separated, that is Âµ(x)Ï€Î»(v)âˆ’Âµ(x)Ï€Î»(v+1)â‰¥CÂµvâˆ’Ï±forCÂµ, Ï± > 0.
We focus on parameter ranges 1< qâ‰¤2andÎ· >1strictly for simplicity of presentation, but the
cases q= 1, q > 2andÎ· >0can be handled with some more analysis. Note that Î· >1ensures
Î±â€ >0for anisotropic smoothness.
Additionally, the assumption pertaining to our ICL setup is stated as follows.
Assumption 6. ForrâˆˆZdÃ—âˆ
0 the coefficients Î²rcorresponding to Ïˆrare independent and satisfy
forsâˆˆÂ¯NdÃ—âˆ
0 such that the frequency component Î´s(f)contains the element Ïˆr,
EÎ²[Î²r] = 0,EÎ²[Î²2
r]â‰²2âˆ’(2+1/Î±â€ )Î³(s)Î³(s)âˆ’2. (22)
AlsoÎ£Î¨,Nâ‰INholds, for example PXis bounded above and below with respect to the product
measure Î»dÃ—âˆonB([0,1]dÃ—âˆ)of the uniform measure Î»onB([0,1]).
We then obtain the following result for ICL with transformers:
Theorem D.1 (minimax optimality of ICL for sequential input) .LetFâ—¦={fâˆˆU(PÎ³
p,q(PX))|
âˆ¥fâˆ¥Lâˆ(PX)â‰¤B}for some B > 0where Î³corresponds to mixed or anisotropic smoothness.
Suppose Assumptions 5 and 6 hold. Then for nâ‰³NlogNwe have
Â¯R(bÎ˜)â‰²Nâˆ’2Î±â€ +NlogN
n+N2âˆ¨(1+1/Î±â€ )polylog( N)
T.
Hence if Tâ‰³nN1âˆ¨1/Î±â€ andNâ‰n1
2Î±â€ +1, ICL achieves the rate nâˆ’2Î±â€ 
2Î±â€ +1polylog( n).
D.2 Proof of Theorem D.1
Since the system (Ïˆr)ris orthonormal, we may takeÂ¯N= 1,Â¯N=Nfollowing Remark 2.1. We
mainly utilize the following approximation and covering number bounds.
Theorem D.2 (Takakura and Suzuki (2023), Theorem 4.5) .For a function Fâ—¦âˆˆU(PÎ³
p,q(PX)),
âˆ¥Fâ—¦âˆ¥Lâˆ(PX)â‰¤Band any K > 0, there exists a transformer bFâˆˆ F TF(J, U, D, H, L, W, S, M )
such that
âˆ¥bF0âˆ’Fâ—¦âˆ¥L2(PX)â‰²2âˆ’K,
34where
Jâ‰²K1/Î·,logUâ‰²logKâˆ¨logV, D â‰²K2(1+Ï±)/Î·logV, H â‰²(logK)1/Î·,
Lâ‰²K2, Wâ‰²2K/Î±â€ K1/Î·, Sâ‰²2K/Î±â€ K2+2/Î·,logMâ‰²Kâˆ¨log log V.
Theorem D.3 (Takakura and Suzuki (2023), Theorem 5.3) .ForÏµ >0andBâ‰¥1it holds that
logN(FTF(J, U, D, H, L, W, S, M ),âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²J3L(S+HD2) logDHLWM
Ïµ
.
To analyze the decay rate in the Î³-smooth class, we approximate a function fâˆˆL2([0,1]dÃ—âˆ)by
the partial sum of its frequency components up to â€˜resolutionâ€™ K, measured via the Î³function:
RK(f) :=X
Î³(s)<KÎ´s(f).
The basis functions Ïˆrare thus ordered primarily ordered by increasing Î³(s).
Lemma D.4 (Okumoto and Suzuki (2022), Lemma 17) .For1â‰¤qâ‰¤2it holds that
âˆ¥fâˆ’RK(f)âˆ¥L2(PX)â‰²2âˆ’Kâˆ¥fâˆ¥FÎ³
p,q(PX).
Note that if Î³(s)< K thensij< K/a ijâ‰²K/|i|Î·for all i, jfor both types of smoothness and so
âˆ¥sâˆ¥0â‰²dK1/Î·. In addition, the number of basis functions Ïˆrused in the sum for Î´s(f)is exactly
2âˆ¥sâˆ¥1=Q
i,j2sij. Theorem D.3 of Takakura and Suzuki (2023) shows that the number of basis
elements used in the sum RK(f)satisfies
Nâ‰X
Î³(s)<K2âˆ¥sâˆ¥1â‰²2K/Î±â€ 
for both mixed and anisotropic smoothness. Hence the N-term approximation error decays as Nâˆ’Î±â€ 
so that the choice s=Î±â€ leading to the assumed variance bound Nâˆ’2Î±â€ âˆ’1â‰2âˆ’(2+1/Î±â€ )Kin(22)
is justified. Moreover for large K,
X
Î³(s)<KX
âŒŠ2sijâˆ’1âŒ‹â‰¤|rij|<2sijâˆ¥Ïˆrâˆ¥2
Lâˆ(PX)â‰¤X
Î³(s)<K2âˆ¥sâˆ¥1(âˆš
2âˆ¥sâˆ¥0)2â‰²2K/Î±â€ +O(K1/Î·)
sinceâˆ¥râˆ¥0=âˆ¥sâˆ¥0, so that (2)of Assumption 1 is satisfied with r= 1/2. The second part of
Assumption 1 holds since (Ïˆr)râˆˆZdÃ—âˆ
0is orthonormal w.r.t. Î»dÃ—âˆ. Furthermore, the discussion thus
far immediately extends to the piecewise Î³-smooth class for any partition {â„¦Î»}Î»âˆˆÎ›by composing
with the permutation operator Î .
We proceed to use Theorem D.2 to approximate each basis function Ïˆrâ—¦Î up to resolution K.
Moreover, we can see from the proof of Lemma 17 of Okumoto and Suzuki (2022) that we do not
need to account for the sup-norm scaling of Ïˆrand thus it suffices to find the parameter Kâ€²âˆˆNsuch
that the approximation error 2âˆ’Kâ€²â‰Î´N. Hence combining Theorems D.2 and D.3, we conclude that
V(FTF(J, U, D, H, L, W, S, M ),âˆ¥Â·âˆ¥Lâˆ, Ïµ)â‰²Kâ€²3/Î·Kâ€²2Â·2Kâ€²/Î±â€ Kâ€²2+2/Î·Â·Kâ€²log1
Ïµ
â‰²1
Î´N1/Î±â€ 
polylog
N,1
Î´N
log1
Ïµ
is sufficient to satisfy Assumption 3. Therefore, we can now apply our framework with Bâ€²
Nâ‰Nto
obtain the bound
Â¯R(bÎ˜)â‰²N
nlogN+N2
n2log2N+Nâˆ’2Î±â€ +N2Î´2
N
+N2
TlogN
Ïµ+1
TÎ´âˆ’1/Î±â€ 
N polylog
N,1
Î´N
log1
Ïµ+Ïµ.
Substituting Î´Nâ‰Nâˆ’1âˆ’Î±â€ andÏµâ‰Nâˆ’2Î±â€ concludes the theorem.
35Figure 1: Architecture of the compared models. Each model contains two MLP components, all
attention layers are single-head and LayerNorm is not included. (a),(b) implement the simplified
reparametrization for attention, while all layers in (c) utilize the full embeddings. The input dimension
is 8 and all hidden layer and DNN output widths are 32. The query prediction is read off the last entry
of the output at the query position.
Figure 2: Training and test curves for the ICL pretraining objective. We use the Adam optimizer with
a learning rate of 0.02 for all layers. For the task class we take Î±= 1,p=q=âˆ,T=n= 512
and generate samples from random combinations of order 2 wavelets.
E Numerical Experiments
In this section, we connect our theoretical contributions to practical transformers by conducting
experiments verifying our results as well as justifying the simplified model setup and the empirical risk
minimization assumption. We implement and compare the following toy models: (a) the simplified
architecture studied in our paper; (b) the same model with linear attention replaced by softmax; and
(c) a full transformer with 2 stacked encoder layers. The number of feedforward layers, widths of
hidden layers, learning rate, etc. are set to equal for a fair comparison, see Figure 1 for details.
Figure 2 shows training (solid) and test loss curves (dashed) during pretraining. All 3 architectures
exhibit similar behavior and converge to near zero training loss, justifying the use of our simplified
model and also supporting the assumption that the empirical risk is minimized. Moreover, Figure
3 shows the converged losses over a wide range of N, n, T values. We verify that increasing N, n
Figure 3: Training and test losses of the three models after 50 epochs while varying (a) DNN width
N; (b) number of in-context samples n; (c) number of tasks T. For (a), the widths of all hidden layers
also vary with N. We take the median over 5 runs for robustness.
36leads to decreasing train and test error, corresponding to the approximation error of Theorem 3.1. We
also observe that increasing Ttends to improve the pretraining generalization gap up to a threshold,
confirming our theoretical analysis of task diversity. Again, this behavior is consistent across the
3 architectures. We note that in the overparametrized regime when the number of total parameters
â‰³nT, the trained model is likely not the empirical risk minimizer, which may also contribute to the
large error for large Nor small n, T.
F Proofs of Minimax Lower Bounds
F.1 Proof of Proposition 5.1
In this section, we develop our framework for obtaining minimax lower bounds in the ICL setup by
adapting the information-theoretic approach of Yang and Barron (1999).
Let{(Ïˆ(j), Î²(j)
T+1)}M
j=1be aÎ´n-packing of the class Fâ—¦with respect to the L2(X)-norm such that
âˆ¥Î²(j)âŠ¤
T+1Ïˆ(j)âˆ’Î²(jâ€²)âŠ¤
T+1Ïˆ(jâ€²)âˆ¥2
L2(X)â‰¥Î´2
n,1â‰¤j < jâ€²â‰¤M,
where Mis the corresponding packing number. Then we have the following proposition as an
application of Fanoâ€™s inequality (Yang and Barron, 1999).
Proposition F.1. LetÎ˜be a random variable uniformly distributed over {(Ïˆ(j), Î²(j)
T+1)}M
j=1. Then,
it holds that
inf
bfn:Dn,Tâ†’Rsup
fâ—¦âˆˆFâ—¦EDn,T[âˆ¥bfâˆ’fâ—¦âˆ¥2
L2(PX)]â‰¥Î´2
n
2
1âˆ’EX[IX(1:T+1)(Î˜,y(1:T+1))] + log 2
logM
,
where IX(1:T+1)(Î˜,y(1:T+1))is the mutual information between Î˜,y(1:T+1)for given X(1:T+1).
The mutual information IX(1:T+1)(Î˜,y(1:T+1))is formulated more concretely as
X
Î¸âˆˆsupp Î˜w(Î¸)Z
p(y(1:T+1)|Î¸,X(1:T+1)) logp(y(1:T+1)|Î¸,X(1:T+1))
pw(y(1:T+1)|X(1:T+1))
dy(1:T+1),
where p(y|Î¸,X)is the probability density of yconditioned on Î¸,Xandpwis the marginal distri-
bution of y(1:T+1)where w(Â·)â‰¡1
Mis the probability mass function of Î˜(i.e.,pw(Â·|X(1:T+1)) =P
Î¸âˆˆsupp Î˜w(Î¸)p(Â·|Î¸,X(1:T+1))). We let Py(t)|Î¸(andPy(1:t)|Î¸) be the distribution of y(t)condi-
tioned on Î¸,X(t)(andX(1:t)) respectively, and let
Â¯Py(1:T+1)=1
MMX
j=1Py(1:T+1)|Î¸(j)
be the marginal distribution of y(1:T+1)conditioned on X(1:T+1).
Next, we define the set {ËœÏˆ(j)}Q1
j=1to be a Îµn,1-covering of FNw.r.t. the norm d(Ïˆ, Ïˆâ€²) :=p
Ex[âˆ¥Ïˆ(x)âˆ’Ïˆâ€²(x)âˆ¥2]withÎµn,1-covering number Q1, and{ËœÎ²(j)}Q2
j=1to be a Îµn,2-covering of
Bw.r.t. the L2norm with Îµn,2-covering number Q2. By taking all combinations of (ËœÏˆ(j),ËœÎ²(jâ€²))
for1â‰¤jâ‰¤Q1and1â‰¤jâ€²â‰¤Q2, we obtain the covering {ËœÎ¸(j)}Q
j=1with respect to the quantity
Îµ2
n=Ïƒ2
Î²Îµ2
n,1+C2Îµ2
n,2where Q=Q1Q2and each ËœÎ¸(j)is given by ËœÎ¸(j)= (ËœÏˆ(j1),ËœÎ²(j2))for some
indices j1andj2.
Then as in the discussion of Yang and Barron (1999), the mutual information is bounded by
IX(1:T+1)(Î˜,y(1:T+1)) =1
MMX
j=1D(Py(1:T+1)|Î¸(j)âˆ¥Â¯Py(1:T+1))
â‰¤1
MMX
j=1D(Py(1:T+1)|Î¸(j)âˆ¥ËœPy(1:T+1)),
37where D(Â·âˆ¥Â·)is the Kullback-Leibler divergence and ËœPy(1:T+1)=1
QPQ
j=1Py(1:T+1)|ËœÎ¸(j)because
Â¯Py(1:T+1)minimizes the right hand side. If we let
Îº(j) := arg min
1â‰¤kâ‰¤QD(Py(1:T+1)|Î¸(j)âˆ¥Py(1:T+1)|ËœÎ¸(k)),
then each summand of the right-hand side is further bounded by
logQ+D(Py(1:T+1)|Î¸(j)âˆ¥Py(1:T+1)|ËœÎ¸Îº(j)).
Moreover, for Î¸= (Ïˆ, Î²(T+1))it holds that
p(y(1:T+1)|Î¸,X(1:T+1))
=TY
t=1p(y(t)|Ïˆ,X(t))Â·p(y(T+1)|Ïˆ, Î²(T+1),X(T+1))
=TY
t=1Z
p(y(t)|Ïˆ, Î²(t),X(t))pÎ²(Î²(t))dÎ²(t)Â·p(y(T+1)|Ïˆ, Î²(T+1),X(T+1)).
Then the KL-divergence can be bounded as
D(Py(1:T+1)|Î¸(j)âˆ¥Py(1:T+1)|ËœÎ¸Îº(j))
=TX
t=1D
Py(t)|Ïˆ(j)âˆ¥Py(t)|ËœÏˆ(Îº(j))
+D
Py(T+1)|Ïˆ(j),Î²(j)
T+1âˆ¥Py(T+1)|ËœÏˆ(Îº(j)),ËœÎ²(Îº(j))
T+1
â‰¤TX
t=1Z
D
Py(t)|Ïˆ(j),Î²(t)âˆ¥Py(t)|ËœÏˆ(Îº(j)),Î²(t)
pÎ²(Î²(t))dÎ²(t)
+D
Py(T+1)|Ïˆ(j),Î²(j)
T+1âˆ¥Py(T+1)|ËœÏˆ(Îº(j)),ËœÎ²(Îº(j))
T+1
,
where the joint convexity of KL-divergence was used for the last inequality. Since the observation
noise is assumed to be normally distributed, the integrand KL-divergence can be bounded as
D
Py(t)|Ïˆ(j),Î²âˆ¥Py(t)|ËœÏˆÎº(j),Î²
=nX
i=11
2Ïƒ2
Î²âŠ¤Ïˆ(j)(x(t)
i)âˆ’Î²âŠ¤ËœÏˆ(Îº(j))(x(t)
i)2
.
Hence, its expectation with respect to Î²,X(t)becomes
EX(t),Î²h
D
Py(t)|Ïˆ(j),Î²âˆ¥Py(t)|ËœÏˆÎº(j),Î²i
=nÏƒ2
Î²
2Ïƒ2âˆ¥Ïˆ(j)âˆ’ËœÏˆÎº(j)âˆ¥2
L2(PX)â‰¤nÏƒ2
Î²
2Ïƒ2Îµ2
n,1,
In the same manner, we have that
D
Py(T+1)|Ïˆ(j),Î²(j)
T+1âˆ¥Py(T+1)|ËœÏˆÎº(j),ËœÎ²(Îº(j))
T+1
=1
2Ïƒ2nX
i=1
Î²(j)âŠ¤
T+1Ïˆ(j)(x(t)
i)âˆ’ËœÎ²(Îº(j))âŠ¤
T+1ËœÏˆ(Îº(j))(x(t)
i)2
â‰¤nX
i=11
2Ïƒ2
Î²(j)
T+1âˆ’ËœÎ²(Îº(j))
T+1âŠ¤ËœÏˆ(Îº(j))(x(t)
i)2
+nX
i=11
2Ïƒ2h
Î²(j)âŠ¤
T+1(Ïˆ(j)(x(t)
i)âˆ’ËœÏˆ(Îº(j))(x(t)
i))i2
.
The expectation of the right-hand side with respect to X(T+1), Î²(j)
T+1is bounded as
EX(T+1),Î²(j)
T+1h
D
Py(T+1)|Ïˆ(j),Î²(j)
T+1âˆ¥Py(t)|ËœÏˆÎº(j),ËœÎ²(Îº(j))
T+1i
â‰¤C2n
2Ïƒ2âˆ¥Î²(j)
T+1âˆ’ËœÎ²(Îº(j))
T+1âˆ¥2+n
2Ïƒ2Ïƒ2
Î²âˆ¥Ïˆ(j)âˆ’ËœÏˆ(Îº(j))âˆ¥2
L2(PX)
â‰¤C2n
2Ïƒ2Îµ2
n,2+n
2Ïƒ2Ïƒ2
Î²Îµ2
n,1=n
2Ïƒ2Îµ2
n.
Therefore, the expected mutual information can be bounded as
EX[IX(1:T+1)(Î˜,y(1:T+1))]â‰¤logQ1+ log Q2+nT
2Ïƒ2Ïƒ2
Î²Îµ2
n,1+n
2Ïƒ2Îµ2
n.
Applying Proposition F.1 together with (7) concludes the proof.
38F.2 Lower Bound in Besov Space
Here, we derive the minimax lower bound when Fâ—¦=U(BÎ±
p,q(X)). Recall that in this setting
s=Î±/d. We fix a resolution Kand then consider the set of B-splines Ï‰d
K,â„“, â„“âˆˆId
Kof cardinality
Nâ€²â‰2Kd. Considering the basis pairs (Ï‰d
K,1, Ï‰d
K,2), . . . , (Ï‰d
K,Nâ€²âˆ’1, Ï‰d
K,Nâ€²), we can determine
which one is employed to construct the basis Ïˆ(j). The Varshamov-Gilbert bound yields that for
â„¦ ={0,1}Nâ€²/2, we can construct a subset â„¦â€²={w1, . . . , w2Nâ€²/16} âŠ‚â„¦such that |â„¦|= 2Nâ€²/16
andwÌ¸=wâ€²âˆˆâ„¦â€²has a Hamming distance not less than Nâ€²/16. Using this â„¦â€², we set N=Nâ€²/2
andM= 2Nâ€²/16and define (Ïˆ(j))M
j=1asÏˆ(j)
i=Ï‰d
K,2iâˆ’1ifwj,i= 0andÏˆ(j)
i=Ï‰d
K,2iifwj,i= 1.
We use the same B-spline bases with resolution more than KforÏˆ(j)
i(iâ‰¥N)across all j.
By the construction of (Ïˆ(j)), if we set Î²(1)= (ÏƒÎ², . . . , Ïƒ Î²,0,0, . . .), then
âˆ¥Î²(1)âŠ¤Ïˆ(j)âˆ’Î²(1)âŠ¤Ïˆ(jâ€²)âˆ¥2
L2(PX)â‰¥Ïƒ2
Î²N/8.
Hence, for Î´2
nâ‰¤Ïƒ2
Î²N/8â‰²1, theÎ´n-packing number is not less than 2N/8. Moreover, the logarithmic
Î´n-packing number of {Î²âŠ¤Ïˆ(j)|Î²âˆˆ B} for a fixed jisÎ˜(min {Î´âˆ’1/s
n, Nlog(1/Î´n)})by the
standard argument.
Hence taking Î´n=Nâˆ’s, we obtain logMâ‰³Nand the upper bound of the covering numbers
logQ1+ log Q2â‰²NforÏƒ2
Î²Îµ2
n,1â‰¤Î´2
nandÎµ2
n,2=CÎ´2
nwhere Cis a constant. Then, by choosing C
appropriately and Îµn,1â‰²Nâˆ’1âˆ’s(so that logQ1â‰²N), as long as
nTÏƒ2
Î²Îµ2
n,1+nÎ´2
nâ‰²logQ1+ log Q2â‰²N
is satisfied, the minimax rate is lower bounded by Î´2
n. Taking Nâ‰n1
2s+1, we obtain the lower bound
Î´2
nâ‰³nâˆ’2s
2s+1. (23)
F.3 Lower Bound with Coarser Basis
We consider a generalized setting where X=RdÃ—RdÃ—Â·Â·Â· Ã— Rd
| {z }
(N+1) timesand take Ïˆ(j)
iâˆˆU(BÏ„
p,q(Rd))
and assume that Î²1âˆˆ[âˆ’1,1]andÎ²jâˆˆ[âˆ’ÏƒÎ², ÏƒÎ²]where Ïƒ2
Î²=ËœÎ˜(Nâˆ’2sâˆ’1). Since the logarithmic
ËœÎµ1-covering and packing numbers of U(BÏ„
p,q(Rd))areÎ˜(ËœÎµâˆ’d/Ï„
1), those for the basis functions on
j= 2, . . . , N + 1become Î˜(NËœÎµâˆ’d/Ï„
1), and those for BareÎ˜(Nlog(1 +NÏƒ2
Î²
Îµ2
n,2)). Therefore, by
taking Îµ2
n,1=NËœÎµ2
1we see that
nT(Îµ2
n,1+Ïƒ2
Î²Îµ2
n,1) +nÎµ2
n,2â‰²Îµâˆ’d/Ï„
n,1+NÎµn,1âˆš
Nâˆ’d/Ï„
+Nlog 
1 +NÏƒ2
Î²
Îµ2
n,2!
should be satisfied. Moreover, by taking Îµ(2Ï„+d)/Ï„
n,1 â‰1/nT andÎµ2
n,2â‰Nlog(1 + Nâˆ’2s/Îµ2
n,2)/n
we can balance both sides. In particular, we may set
Îµ2
n,1â‰(nT)âˆ’2Ï„
2Ï„+d, Îµ2
n,2â‰N
nâˆ§Nâˆ’2s.
Taking the balance with respect to Nto maximize Îµ2
n,2, we have Nâ‰nd
2Î±+dandÎµ2
n,1â‰(nT)âˆ’2Ï„
2Ï„+d.
Therefore, the minimax rate is lower bounded as
Î´2
nâ‰ƒ(1 +Ïƒ2
Î²)Îµ2
n,1+Îµ2
n,2â‰ƒnâˆ’2Î±
2Î±+d+ (nT)âˆ’2Ï„
2Ï„+d. (24)
F.4 Lower Bound in Piecewise Î³-smooth Class
Suppose that we utilize the basis functions up to resolution K. Then, by the argument by Nishimura
and Suzuki (2024), the number of basis functions Ïˆrin the K-th resolution is Nâ€²â‰2K/aâ€ . Moreover,
theÎ´n-packing number of the Î³-smooth class is also lower bounded by
logMâ‰¥Nâ€²polylog( Î´n, Nâ€²). (25)
39Here, by noticing the approximation error bound in Appendix D.2, we take Nâ€²â‰Î´âˆ’1/aâ€ 
n where the
basis functions are chosen from the Kth resolution. As in the case of the Besov space, we construct
(Ïˆ(j))Mâ€²
j=1where Mâ€²= 2Nâ€²/16andÏˆ(j)(x)âˆˆRNforN=Nâ€²/2andâˆ¥Ïˆ(j)âˆ’Ïˆ(jâ€²)âˆ¥2
L2(PX)â‰¥N/8
forjÌ¸=jâ€². Following the same argument as in the Besov case, we need to take Îµn,1andÎµn,2as
nTÏƒ2
Î²Îµ2
n,1+nÎµ2
n,2â‰²Î´âˆ’1/aâ€ 
n
up to logarithmic factors. This is satisfied by taking Îµ2
n,2=CÎ´2
nâ‰nâˆ’2aâ€ 
2aâ€ +1with a constant Cand
balancing Nso that Ïƒ2
Î²Îµ2
n,1= (nT)âˆ’2aâ€ 
2aâ€ +1Ïƒ2
2aâ€ +1
Î²â‰Tâˆ’1nâˆ’2aâ€ 
2aâ€ +1. Combining this evaluation and
(25) yields that the minimax lower bound is given by
Î´2
nâ‰³nâˆ’2aâ€ 
2aâ€ +1. (26)
40NeurIPS Paper Checklist
The checklist is designed to encourage best practices for responsible machine learning research,
addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove
the checklist: The papers not including the checklist will be desk rejected. The checklist should
follow the references and follow the (optional) supplemental material. The checklist does NOT count
towards the page limit.
Please read the checklist guidelines carefully for information on how to answer these questions. For
each question in the checklist:
â€¢ You should answer [Yes] , [No] , or [NA] .
â€¢[NA] means either that the question is Not Applicable for that particular paper or the
relevant information is Not Available.
â€¢ Please provide a short (1â€“2 sentence) justification right after your answer (even for NA).
The checklist answers are an integral part of your paper submission. They are visible to the
reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it
(after eventual revisions) with the final version of your paper, and its final version will be published
with the paper.
The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.
While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a
proper justification is given (e.g., "error bars are not reported because it would be too computationally
expensive" or "we were unable to find the license for the dataset we used"). In general, answering
"[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we
acknowledge that the true answer is often more nuanced, so please just use your best judgment and
write a justification to elaborate. All supporting evidence can appear either in the main paper or the
supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification
please point to the section(s) where related material for the question can be found.
IMPORTANT, please:
â€¢Delete this instruction block, but keep the section heading â€œNeurIPS paper checklist" ,
â€¢Keep the checklist subsection headings, questions/answers and guidelines below.
â€¢Do not modify the questions and only use the provided macros for your answers .
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paperâ€™s contributions and scope?
Answer: [Yes]
Justification: See the Our Contributions paragraph for details.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See the Limitations paragraph.
41Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that arenâ€™t acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
Answer: [Yes]
Justification: See Assumptions 1, 2, 3, 4, 5, 6. All proofs were provided in the Appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
â€¢All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
â€¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Appendix E, Figure 1.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
42â€¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [NA]
Justification: All experiments are toy simulations and data is i.i.d. random.
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be
possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
â€¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
â€¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
43â€¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: See Appendix E, Figure 2.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
â€¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: The median over 5 runs is reported in Figure 3.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [NA]
Justification: All experiments are toy simulations.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
44â€¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didnâ€™t make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: The research is theoretical and raises no ethical concerns.
Guidelines:
â€¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [NA]
Justification: The research is theoretical and raises no concerns on societal impact.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
45â€¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
â€¢The authors should state which version of the asset is used and, if possible, include a
URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to
the assetâ€™s creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
â€¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: [NA]
46Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: [NA]
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
47