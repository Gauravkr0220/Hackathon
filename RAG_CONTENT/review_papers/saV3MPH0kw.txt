Published in Transactions on Machine Learning Research (09/2024)
Unsupervised Domain Adaptation
by Learning Using Privileged Information
Adam Breitholtzâˆ—adambre@chalmers.se
Department of Computer Science
Chalmers University of Technology
Anton Matssonâˆ—antmats@chalmers.se
Department of Computer Science
Chalmers University of Technology
Fredrik D. Johansson fredrik.johansson@chalmers.se
Department of Computer Science
Chalmers University of Technology
Reviewed on OpenReview: https: // openreview. net/ forum? id= saV3MPH0kw
Abstract
Successful unsupervised domain adaptation is guaranteed only under strong assumptions
such as covariate shift and overlap between input domains. The latter is often violated in
high-dimensional applications like image classification which, despite this limitation, con-
tinues to serve as inspiration and benchmark for algorithm development. In this work, we
show that training-time access to side information in the form of auxiliary variables can
help relax restrictions on input variables and increase the sample efficiency of learning at
the cost of collecting a richer variable set. As this information is assumed available only
during training, not in deployment, we call this problem unsupervised domain adaptation
by learning using privileged information (DALUPI). To solve this problem, we propose a
simple two-stage learning algorithm, inspired by our analysis of the expected error in the
target domain, and a practical end-to-end variant for image classification. We propose three
evaluation tasks based on classification of entities in photos and anomalies in medical im-
ages with different types of available privileged information (binary attributes and single or
multiple regions of interest). We demonstrate across these tasks that using privileged infor-
mation in learning can reduce errors in domain transfer compared to baselines, be robust to
spurious correlations in the source domain, and increase sample efficiency.
1 Introduction
Deploymentofmachinelearning(ML)systemsreliesongeneralizationfromtrainingsamplestonewinstances
in a target domain. When these new instances differ in distribution from the source of training data,
performance tends to degrade and guarantees are often weak. For example, a supervised ML model trained
to identify medical conditions in X-ray images from one hospital may work poorly in another hospital if
the two sites have different equipment or examination protocols (Zech et al., 2018). In the unsupervised
domain adaptation (UDA) problem (Ben-David et al., 2006), nolabeled examples are available from the
target domain and strong assumptions are needed for success. In this work, we ask: How can access to
auxiliary variables during training help solve the UDA problem and weaken the assumptions necessary to
guarantee domain transfer?
In standard UDA, a common assumption is that the object of the learning task is identical in source and
target domains but that input distributions differ (Shimodaira, 2000). This â€œcovariate shiftâ€ assumption is
âˆ—Equal contribution.
1Published in Transactions on Machine Learning Research (09/2024)
plausible in our X-ray example above: Doctors are likely to give the same diagnosis based on X-rays of the
same patient from similar but different equipment. However, guarantees for consistent domain adaptation
alsorequireeitherdistributionaloverlapbetweeninputsfromsourceandtargetdomainsorknownparametric
forms of the labeling function (Ben-David & Urner, 2012; Wu et al., 2019; Johansson et al., 2019). Without
these, adaptation cannot be verified or guaranteed by statistical means.
Input domain overlap is implausible for the high-dimensional tasks that have become standard benchmarks
in the UDA community, including image classification (Long et al., 2013; Ganin et al., 2016) and sentence
labeling (Orihashi et al., 2020). If hospitals have different X-ray equipment, the probability of observing
(near-)identical images from source and target domains is zero (Zech et al., 2018). Even when covariate shift
andoverlaparesatisfied, largedomaindifferencescanhaveadramaticeffectonsamplecomplexity(Breitholtz
& Johansson, 2022). Despite promising developments (Shen et al., 2022), realistic guarantees for practical
domain transfer remain elusive.
In supervised ML without domain shift, incorporating auxiliary variables in the training of models has
been proposed to improve out-of-sample generalization. For example, learning using privileged informa-
tion(Vapnik & Vashist, 2009; Lopez-Paz et al., 2016), variables available during training but unavailable in
deployment, has been proven to require fewer examples compared to learning without these variables (Karls-
son et al., 2021). In X-ray classification, privileged information (PI) can come from graphical annotations
or clinical notes made by radiologists that are unavailable when the system is used. While PI has begun to
see use in domain adaptation, see e.g., Sarafianos et al. (2017) or Vu et al. (2019), and a theoretical analysis
exists for linear classifiers (Xie et al., 2020), the literature has yet to fully characterize the benefits of this
practice.
We introduce unsupervised domain adaptation by learning using privileged information (DALUPI), in which
auxiliary variables, related to the outcome of interest, are leveraged during training to improve test-time
adaptation when the variables are unavailable. We summarize our contributions below:
â€¢We formalize the DALUPI problem and give conditions under which it is possible to solve it con-
sistently, i.e., to learn a model using privileged information that predicts optimally in the target
domain. Importantly, these conditions do not rely on distributional overlap between source and
target domains in the input variable (Section 2.1), making consistent learning without privileged
information (PI) generally infeasible.
â€¢We propose practical learning algorithms for image classification in the DALUPI setting (Section 3),
designed to handle problems with three different types of PI, see Figure 1 for examples. As common
UDA benchmarks lack auxiliary variables related to the learning problem, we propose three new
evaluation tasks spanning the three types of PI using data sets with real-world images and auxiliary
variables.
â€¢Onthesetasks,wecompareourmethodstosupervisedlearningbaselinesandwell-knownmethodsfor
unsupervised domain adaptation (Section 4). We find that our proposed models perform favorably
to the alternatives for all types of PI, particularly when input overlap is violated and when training
sets are small.
2 Privileged Information in Domain Adaptation
In unsupervised domain adaptation (UDA), the goal is to learn a hypothesis hto predict outcomes (or
labels)YâˆˆYfor problem instances represented by input covariates XâˆˆX, drawn from a target domain
with densityT(X,Y ). During training, we have access to labeled samples (x,y)only from a source domain
S(X,Y )and unlabeled samples ËœxfromT(X). As a running example, we think of SandTas radiology
departments at two different hospitals, of Xas the X-ray image of a patient, and of Yas the diagnosis made
by a radiologist after analyzing the image.
2Published in Transactions on Machine Learning Research (09/2024)
Digit	classification	with	single	region	of	interest	as	PI
Image	fromCIFAR-10â€™slast	fiveclassesImages	from	CIFAR-10â€™s	first	fiveand	last	fiveclasses
Bounding	box	indicating	the	location	of	the	MNIST	digit
3MNISTdigit
Entity	classification	with	multiple	regionofinterestsas	PIIndoorand	outdoorimages
Bounding	boxes	indicating	the	location	of	the	entities
OutdoorimagedogpersonEntitiesInput	samplesğ‘‹~ğ’®,	ğ‘‹~ğ’¯Privileged		information	ğ‘Š~ğ’®,	ğ‘Š~ğ’¯Labelğ‘Œ~ğ’®Test	sampleğ‘‹~ğ’¯
Celebrity	photo	classification	with	binary	attributesas	PI
Image	of	personwithhatImages	of	personswithand	withouthats
3Reportedgender
Binary	attributesmustachemakeupâˆ’1âˆ’1âˆ’1âˆ’1
Figure 1: Examples of domain adaptation tasks with different types of privileged information (PI). During
training, input samples Xand PIWare drawn from both source and target domains. Labels Yare only
available from the source domain. At test time, a target sample Xis observed. We consider three types of
PI: binary attribute vectors, a single region of interest, and multiple regions of interest.
We aim to learn a hypothesis hâˆˆHfrom a hypothesis set Hthat minimizes the expected target-domain
prediction error (risk) RT, with respect to a loss function L:YÃ—Yâ†’ R+, i.e., to solve
min
hâˆˆHRT(h),RT(h):=ET(X,Y )[L(h(X),Y)], (1)
where we use the subscript convention Ep(X)[f(X)] =/integraltext
xâˆˆXp(x)f(x)dxto denote an expectation of some
functionfover a density pon the domainX. A consistent solution to the UDA problem returns a minimizer
of Equation 1 without ever observing labeled samples from T. However, ifSandTare allowed to differ
arbitrarily, finding such a solution cannot be guaranteed (Ben-David & Urner, 2012). To make the problem
feasible, we assume that covariate shift (Shimodaira, 2000) holdsâ€”that the labeling function is the same in
both domains, but the covariate distributions differ.
Assumption 1 (Covariate shift) .For domainsS,TonXÃ—Y,covariate shift holds with respect to Xif
âˆƒxâˆˆX:T(X=x)Ì¸=S(X=x)andâˆ€xâˆˆX:T(Y|x) =S(Y|x).
In our example, covariate shift means that radiologists at either hospital would diagnose two patients with
the same X-ray in the same way, but that the radiologists may encounter different distributions of patients
and images. To guarantee consistent learning without further assumptions, these distributions cannot be too
differentâ€”the source input domain S(x)must sufficiently overlapthe target input domain T(x).
Assumption 2 (Domain overlap) .A domainSoverlaps another domain Twith respect to XonXif
âˆ€xâˆˆX:T(X=x)>0 =â‡’ S (X=x)>0.
3Published in Transactions on Machine Learning Research (09/2024)
Table 1: A summary of the different settings we consider in this work, what data is assumed to be available
during training and if guarantees for identification are known for the setting under the assumptions of
Proposition 1. The parentheses around source samples for DALUPI indicate that we need not necessarily
observe these for the setting. Note that at test time only xfromTis observed.âˆ—Under the more generous
assumption of overlapping support in the input space X, guarantees exist for all these settings.
Setting Observed SObservedTGuarantee
x w y Ëœx Ëœw ËœyforRT
SL-T âœ“ âœ“ âœ“
SL-S âœ“ âœ“âˆ—
UDA âœ“ âœ“ âœ“âˆ—
LUPI âœ“ âœ“ âœ“âˆ—
DALUPI ( âœ“)âœ“ âœ“ âœ“ âœ“ âœ“
Covariate shift and domain overlap with respect to Xguarantee that the target risk RTcan be identified
by the sampling distribution described above, and thus, that a solution to Equation 1 may be found. Hence,
they have become standard assumptions, used by most informative guarantees (Zhao et al., 2019).
Overlap is often violated in high-dimensional problems such as image classification, partly due to irrelevant
information that has a spurious association with the label Y(Beery et al., 2018; Dâ€™Amour et al., 2021).
In X-ray classification, it may be possible to perfectly distinguish hospitals (domains) based on protocol or
equipment differences manifesting in the images (Zech et al., 2018). There are no guarantees for optimal
UDA in this case. Some guarantees based on distributional distances do not rely on overlap (Ben-David
et al., 2006; Long et al., 2013), but do not guarantee optimal learning either (Johansson et al., 2019).
Still, an image Xmaycontaininformation Wwhich is both sufficient for prediction andsupported in
both domains . For X-rays, this could be a region of pixels indicating a medical condition, ignoring parts
that merely indicate differences in protocol (Zech et al., 2018). The learner does not know how to find this
information a priori, but it can be supplied during training as added supervision. A radiologist could indicate
regions of interest Wusing bounding boxes during training (Irvin et al., 2019), but would not be available
to annotate images at test time. As such, Wisprivileged information (Vapnik & Vashist, 2009).
2.1 Unsupervised Domain Adaptation With Privileged Information
Learning using privileged information, variables that are available only during training but not at test
time, has been shown to improve sample efficiency in diverse settings (Vapnik & Izmailov, 2015; Pechyony &
Vapnik, 2010; Jung & Johansson, 2022). Next, we show that privileged information can also improve UDA by
providing identifiability of the target riskâ€”allowing it to be computed from the sampling distributionâ€”even
when overlap is not satisfied in X.
We define domain adaptation by learning using privileged information (DALUPI) as follows. During train-
ing, learners observe samples of covariates X, labelsYand privileged information Wâˆˆ WfromSin
a datasetDS={(xi,wi,yi)}m
i=1, as well as samples of covariates and privileged information from T,
DT={(Ëœxi,Ëœwi)}n
i=1.At test time, trained models only observe covariates Ëœxâˆ¼ T (X)and our learning
goal remains to minimize the target risk, see Equation 1 . We justify access to privileged information from T,
but not labels, by pointing out that it is often easier to annotate observations with privileged information W
than with labels Y. For example, a non-expert may be able to reliably recognize the outline of an animal in
an image, indicating the pixels Wcorresponding to it, but not identify its species ( Y); see Figure 2, where
it would likely be easier to identify the location of the cat in the image than to identify its breed.
To identify RT(Equation 1) without overlap in X, we make the assumption that Wis sufficient to predict
Yin the following sense.
Assumption 3 (Sufficiency of privileged information) .Privileged information Wis sufficient for the out-
comeYgiven covariates XifYâŠ¥X|Win bothSandT.
4Published in Transactions on Machine Learning Research (09/2024)
ğ‘‹
ğ’³
ğ‘¤!
ğ’²ğ‘¡!ğ‘¥!
Figure 2: An illustration of domain overlap being more plausible when we consider appropriate forms of
privileged information W, such as a region of interest of an image. Source and target domains S,Tare here
indoor and outdoor images Xand the task is to identify the animal Yin the image.
Assumption 3 is satisfied when Xprovides no more information about Yin the presence of W. If we
considerWto be a subset of image pixels corresponding to an area of interest, the other pixels in Xmay be
unnecessary to predict Y. This is illustrated in Figure 2 where the privileged information wiis the region of
interest indicated by the bounding box ti. Here, overlap is more probable in Wthan inX, as the extracted
pixels mostly show cats. Moreover, when Wretains more information, sufficiency becomes more plausible
but domain overlap in Wis reduced. The sufficiency assumption is used to replace T(y|x)withT(y|w)in
Proposition 1. If sufficiency is violated but it is plausible that the degree of insufficiency is comparable across
domains, we can still obtain a bound on the target risk which may be estimated from observed quantities.
We give such a result in Appendix F.
We expect that some PI can be selected to be sufficient for a given task. However, if this sufficiency cannot
be ensured, the overall performance may decrease, assuming covariate shift with respect to Wis not violated.
Even so, we still anticipate the generalization error to remain of a comparable magnitude. If covariate shift
is violated in W, further performance declines are expected, as the problem becomes more complex and we
are not guaranteed to identify the optimal hypothesis (Johansson et al., 2019).
Assumptions 1â€“2 holding with respect to privileged information Winstead ofX, along with Assumption 3,
allow us to identify the target risk even for models hâˆˆHthat do not use Was input:
Proposition 1. Let Assumptions 1 and 2 be satisfied with respect to W(not necessarily with respect to X)
and let Assumption 3 hold as stated. Then, the target risk RTis identified for hypotheses h:Xâ†’Y,
RT(h) =ET(X)/bracketleftbig
ET(W|X)/bracketleftbig
ES(Y|W)[L(h(X),Y)|X,W ]|X/bracketrightbig/bracketrightbig
=/integraldisplay
xT(x)/integraldisplay
wT(w|x)/integraldisplay
yS(y|w)L(h(x),y)dydwdx ,
and forLthe squared loss, a minimizer of RTis the function
hâˆ—
T(x) =ET(W|x)[ES(Y|W)[Y|W]|x] =/integraldisplay
wT(w|x)/integraldisplay
yS(y|w)ydydw .
Proof sketch. RT(h) =/integraltext
x,yT(x,y)L(h(x),y)dxdy. We can then marginalize over Wto getT(x,y) =
T(x)ET(W|x)[T(y|W)|x] =T(x)/integraltext
w:S(w)>0T(w|x)S(y|w)dw, where the first equality follows by
sufficiency and the second by covariate shift and overlap in W.T(x),T(w|x)andS(y|w)are observable
throughtrainingsamples. That hâˆ—
Tisaminimizerfollowsfromthefirst-ordercondition. SeeAppendixC.
Proposition 1 shows that there are conditions where privileged information allows for identification of target-
optimal hypotheses where identification is not possible without it, i.e., when overlap is violated in X.W
5Published in Transactions on Machine Learning Research (09/2024)
guides the learner toward the information in Xthat is relevant for the label Y. WhenWis deterministic
inX, overlap in Ximplies overlap in W, but not vice versa. In the same case, under Assumption 3, if
covariate shift holds for X, it holds also for W. Hence, if sufficiency can be justified, the requirements on X
are weaker than in standard UDA, at the cost of collecting W. Surprisingly, Proposition 1 does not require
thatXis observed in the source domain as the result does not depend on S(x).
Figure 1 gives examples of problems with the DALUPI structure which we consider in this work. For
comparison, we list related learning paradigms in Table 1. Supervised learning (SL-S) refers to learning from
labeled samples from Swithout privileged information. SL-T refers to supervised learning with (infeasible)
access to labeled samples from T. UDA refers to the setting at the start of Section 2 and LUPI to learning
using privileged information without data from T(Vapnik & Vashist, 2009). We compare DALUPI to these
alternative settings in our experiments in Section 4.
2.2 A Two-stage Algorithm and Its Risk
In light of Proposition 1, a natural learning strategy is to model privileged information as a function of the
input,T(W|x), and the outcome as a function of privileged information, Ë†g(w)â‰ˆES[Y|w], and combining
these. In the case where Wis a deterministic function of X,T(W|x)is a mapf:Xâ†’W, which may be
estimated as a regression Ë†fand combined with the outcome regression to form Ë†h= Ë†g(Ë†f(X)). We may find
such functions Ë†f,Ë†gby separately minimizing the empirical risks
Ë†RW
T(f) =1
nn/summationdisplay
i=1LW(f(Ëœxi),Ëœwi)and Ë†RY
S(g) =1
mm/summationdisplay
i=1LY(g(wi),yi). (2)
Hypothesis classes F,Gmay be chosen so that H={h=gâ—¦f; (f,g)âˆˆFÃ—G} has a desired form. Note
thatLWandLYmay in general be different loss functions.
We can bound the generalization error of estimators Ë†h= Ë†gâ—¦Ë†fwhenWâˆˆRdWand the loss is the squared
loss. We do this by placing an assumption of Lipschitz smoothness on the space of prediction functions:
âˆ€gâˆˆG,w,wâ€²âˆˆW :âˆ¥g(w)âˆ’g(wâ€²)âˆ¥2â‰¤Mâˆ¥wâˆ’wâ€²âˆ¥2. To arrive at a bound, we first define the Ï-weighted
empirical risk of the outcome model gin the source domain, Ë†RY,Ï
S(g) =1
m/summationtextm
i=1Ï(wi)L_W(g(wi),yi)where
Ïis the density ratio of TandS,Ï(w) =T(w)
S(w). When the density ratio Ïis unknown, we may use density
estimation (Sugiyama et al., 2012) or probabilistic classifiers to estimate it. We arrive at the following result,
proven for univariate Ybut generalizable to multivariate outcomes.
Proposition2. Suppose that WandYare deterministic in XandW, respectively, and that Assumptions 1â€“
3 hold with respect to W. LetGcompriseM-Lipschitz mappings g:Wâ†’YwithWâŠ†RdW, and let the loss
be the squared Euclidean distance, assumed to be uniformly bounded over W. LetÏ(w) =T(w)/S(w)andd
anddâ€²be the pseudo-dimensions of GandF, respectively. Assume that there are mlabeled samples from S
andnunlabeled samples from T. Then, for any h=gâ—¦fâˆˆGÃ—F, with probability at least 1âˆ’Î´,
RT(h)
2â‰¤Ë†RY,Ï
S(g) +M2Ë†RW
T(f) +Oï£«
ï£­3/8/radicalï£¬igg
dlogm
d+ log4
Î´
m+/radicalï£¬igg
dâ€²logn
dâ€²+ logdW
Î´
nï£¶
ï£¸.
Proof sketch. Decomposing the risk of hâ—¦Ï•, we get
RT(h) =ET[(g(f(X))âˆ’Y)2]
â‰¤2ET[(g(W)âˆ’Y)2+ (g(f(X))âˆ’g(W))2]
â‰¤2ET[(g(W)âˆ’Y)2+M2âˆ¥f(X))âˆ’g(W)âˆ¥2]
â‰¤2ET[(g(W)âˆ’Y)2] + 2M2ET[âˆ¥(f(X)âˆ’W)âˆ¥2]
= 2RY
T(g) + 2M2RW
T(f) = 2RY,Ï
S(g) + 2M2RW
T(f).
The first inequality follows the relaxed triangle inequality, the second from the Lipschitz property, and
the third equality from Overlap and Covariate shift. Treating each component of Ë†was independent, using
6Published in Transactions on Machine Learning Research (09/2024)
standard PAC learning results, and application of Theorem 3 from Cortes et al. (2010) allows us to reweight
the risk with the density ratio Ïby also adding an additional term which contains the RÃ©nyi divergence.
Then with a union bound argument, we get the stated result. See Appendix D for a more detailed proof.
WhenFandGcontain the ground-truth mappings between XandWand between WandY, in the infinite-
sample limit, minimizers of Equation 2 minimize RTas well. Our approach is not limited to classical PAC
analysis but could, under suitable assumptions, be carried out under another framework, e.g. using PAC-
Bayes analysis to obtain a bound that contains different sample complexity terms. However, such a bound
would then hold in expectation over a posterior distribution on Hinstead of uniformly over H. We sketch a
proof of such a bound in Appendix E.
Furthermore, if sufficiency is violated but it is plausible that the degree of insufficiency is comparable across
domains, we can still obtain a bound on the target risk which may be estimated from observed quantities.
We give such a result in Appendix F.
3 Image Classification With Privileged Information
Weuseimageclassification, where Xisanimageand Yisadiscretelabel, asproofofconceptforDALUPI.To
show the versatility of our approach, we consider three different instantiations of privileged information W:
a binary attribute vector, a single region of interest, or multiple regions of interest. The two-stage estimator,
see Figure 3a, is used in the first two cases. With multiple regions of interest as privileged information, we
use an end-to-end model based on Faster-R-CNN (Ren et al., 2016), see Figure 3b. We detail each setting
below and illustrate them in Figure 1.
3.1 Binary Attributes as PI
First, we consider the case where each image xiis accompanied by privileged information in the form of
a binary vector wiâˆˆ{0,1}dindicating the presence of dattributes in the image. In this setting, we can
directly apply our two-stage estimator (Equation 2). For the first estimator Ë†f, we use a convolutional neural
network (CNN) trained on observations from T(and possiblyS) to output a vector of attributes Ë†wifrom the
inputxi. For the second estimator Ë†g, we use a multi-layer perceptron classifier, trained on source domain
observations, that predicts the image label Ë†yigiven the vector of attributes wi. We use the categorical
cross-entropy loss to train both Ë†fandË†g. The resulting classifier, Ë†h(x) = Ë†g(Ë†f(x)), is subsequently evaluated
on target domain images.
3.2 Single Region of Interest as PI
Next, we consider privileged information as a subset of pixels wi, taken from the image xiand associated
with an object or feature that determines the label yiâˆˆ{1,...,K}. In our experiments, this PI is provided
as asinglebounding box with coordinates tiâˆˆR4enclosing the region of interest wi. Here, we use two
CNNs, Ë†dandË†g, and a deterministic function Ï•to approximate the two-stage estimator (Equation 2). The
network Ë†dis trained to output bounding box coordinates Ë†tias a function of the input xi, and the pixels
Ë†wiwithin the bounding box are extracted from xiand resized to pre-specified dimensions through Ï•. The
composition of these two functions, Ë†f(xi) =Ï•(xi,Ë†d(xi)), returns Ë†wi. The second network Ë†gis trained to
predictyigiven the pixels wicontained in a bounding box tibased on observations from S. We use the mean
squared error loss for Ë†dand the categorical cross-entropy loss for Ë†g. Finally, Ë†h(x) = Ë†g(Ë†f(x))is evaluated on
target domain images where the output of Ë†fis used for prediction with Ë†g. See Appendix A.1 for further
details.
3.3 Multiple Regions of Interest as PI
Finally, we consider a setting where privileged information indicates multiple regions of interest in an image.
We use this PI in multi-label classification problems where the image xiis associated with one or more
categorieskfrom a set{1,...,K}, encoded in a multi-category label yiâˆˆ{0,1}K(e.g., indicating findings
7Published in Transactions on Machine Learning Research (09/2024)
ğ‘“":ğ’³â†’ğ’²ğ‘Š(
ğ‘”*:ğ’²â†’ğ’´ğ‘Š,ğ‘Œ~ğ’®ğ‘Œ/ğ‘Š(
ğ‘Œ/Trainstage	Iğ¿!ğ‘Š(,ğ‘Šğ‘‹~ğ’¯
ğ¿"ğ‘Œ/,ğ‘Œğ‘‹,ğ‘Š~ğ’¯ğ‘‹,ğ‘Š~ğ’®Trainstage	IITest
(a) Two-stage estimator.
ğ‘‹~ğ’¯ğ¿!"#$%&ğ‘ƒ#,ğ‘ƒğ¿'()$%&ğ‘‡#,ğ‘‡ğ¿!"#*(+ğ‘ƒ#,ğ¿'()*(+ğ‘‡#,,ğ‘‡ğ‘”%:ğ’²â†’ğ’´ğ‘Š+
ğ‘Œ-ğ‘Š+
ğ‘Œ-ğ‘“/:ğ’³â†’ğ’²Train	E2Eğ‘‹,ğ‘Š,ğ‘Œ~ğ’®ğ‘‹,ğ‘Š~ğ’¯Test (b) End-to-end estimator.
Figure 3: A schematic representation of the train and test flow for DALUPI using (a) the two-stage estimator
presented in Section 2.2 and (b) an end-to-end architecture based on Faster R-CNN (Ren et al., 2016). In
the two-stage procedure, the networks Ë†fand Ë†gare learned through empirical risk minimization of LW
andLY, respectively. At test time, Ë†fandË†gare combined into Ë†h= Ë†g(Ë†f(X)). The end-to-end estimator
uses a region proposal network (RPN) to produce regions of interest in the input image X. The RPN,
which serves as the network Ë†f, is followed by a detection network Ë†gthat predicts the class of any object
within a region proposal. Training is guided by regression losses LPRN
reg(Ë†T,T)andLdet
reg(Ë†TU,T), as well as by
classification losses LPRN
cls(Ë†P,P)andLdet
cls(Ë†PU). Here,Tand Ë†Tdenote ground-truth and predicted bounding
box coordinates, respectively, and Ë†TUare the predicted coordinates for a region proposal with ground-truth
labelU. Further, Ë†Pis the RPNâ€™s predicted probability that a region proposal contains an object, Pis a
binary label assigned to the proposal based on its overlap with ground-truth bounding boxes, and Ë†Puis the
probability of the ground-truth class Uwithin the proposal, as predicted by the detection network.
of one or more diseases). The partial label yi(k) = 1indicates the presence of features or objects in the
image from category k. In our entity classification experiment, an object jof classkâˆˆ[K]in the image, say
â€œBirdâ€, will be annotated by a bounding box tijâˆˆR4surrounding the pixels of the bird, and an object label
uij=k. In X-ray classification, tijcan indicate an abnormality jin the X-ray image, and uijâˆˆ{1,...,K}
the label of the finding (e.g., â€œPneumoniaâ€).
To make full use of privileged information, we train a deep neural network Ë†h(x) = Ë†g(Ë†f(x)), where Ë†fproduces
a setof bounding box coordinates Ë†tijand extracts the pixels Ë†wijassociated with each Ë†tij, and where Ë†g
predicts a label Ë†uijfor each Ë†wij. To this end, we adapt the Faster R-CNN architecture (Ren et al., 2016)
which uses a region proposal network (RPN) to generate regions that are fed to a detection network for
classification and refined bounding box regression. A CNN backbone in combination with the RPN region
of interest pooling serves as the subnetwork Ë†f, producing estimates Ë†wiof the privileged information for an
imagexi. For the detection network, which corresponds to the subnetwork Ë†g, we use Fast-RCNN (Girshick,
2015).
Privileged information adds supervision through regression losses LRPN
reg(Ë†t,t)andLdet
reg(Ë†tu,t)for region propos-
alsË†tand class-specific bounding box coordinates Ë†tu. We use the smooth L1 loss defined by Girshick (2015) for
8Published in Transactions on Machine Learning Research (09/2024)
bothLRPN
regandLdet
reg. Training is further guided by classification losses LRPN
cls(Ë†p,p) =âˆ’(plog Ë†p+(1âˆ’p) log Ë†p)
andLdet
cls(Ë†pu) =âˆ’log Ë†pu, where Ë†pis the RPNâ€™s predicted probability that a region proposal contains an
object,pis a binary label assigned to the proposal based on its overlap with ground-truth bounding boxes,
and Ë†puis the probability of the ground-truth class uwithin the proposal, as predicted by the detection
network.
In Appendix A.2, we provide details of the learning objective and architecture and describe small modifica-
tions to the training procedure of Faster R-CNN to accommodate the DALUPI setting. Unlike the two-stage
estimator, we train Faster R-CNN (both Ë†fandË†g) end-to-end, minimizing both losses at once. In entity
classification experiments (see Table 3 and Figure 5), we also train this model in a LUPI setting, where no
information from the target domain is used, but privileged information from the source domain is used.
4 Experiments
We evaluate the empirical benefits of learning using privileged information, compared to the other data
availability settings in Table 1, across four UDA image classification tasks where PI is available in the forms
described in Section 3. Widely used datasets for UDA evaluation like OfficeHome (Venkateswara et al.,
2017) and large-scale benchmark suites like DomainBed (Gulrajani & Lopez-Paz, 2021), VisDA (Peng et al.,
2017) and WILDS (Koh et al., 2021) do notinclude privileged information and cannot be used for evaluation
here. Thus, we first compare our method to baselines on the recent CelebA task (Xie et al., 2020) which
includes PI in the form of binary attributes (Section 4.1). Additionally, we propose three new tasks based on
well-known image classification data sets with regions of interest as PI (Section 4.2â€“4.4). In Section 4.1 and
4.2, we use the two-stage estimator with the subnetwork Ë†fbased on the ResNet-18 architecture (He et al.,
2016a). In Section 4.3 and 4.4, we use our variant of Faster R-CNN with a ResNet-50 backbone.
Our goal is to collect evidence that DALUPI improves adaptation bias and sample efficiency compared to
methods that do not make use of PI. We choose baselines to illustrate these two disparate settings. First,
we compare DALUPI to supervised learning baselines, SL-S and SL-T, trained on labeled examples from
the source and target domain, respectively. SL-S is a simple but strong baseline: On benchmark suites like
DomainBed and WILDS, there is still no UDA method that consistently outperforms SL-S (ERM) without
transfer learning (Gulrajani & Lopez-Paz, 2021; Koh et al., 2021). SL-T serves as an oracle comparison since
it uses labels from the target domain which are normally unavailable in UDA. Second, we compare DALUPI
to two UDA methodsâ€”domain adversarial neural networks (DANN) (Ganin et al., 2016) and the margin
disparity discrepancy (MDD) (Zhang et al., 2019)â€”which have theoretical guarantees but do not make use
of PI. These baselines are all based on the ResNet architecture. In Section 4.1, we compare DALUPI also to
In-N-Out (Xie et al., 2020), which was designed to make use of auxiliary (privileged) attributes for training
domain adaptation models. We do not include this model in other experiments as it was not designed to
use regions of interest as privileged information. The exact architectures of all models and baselines are
described in Appendix A, along with details on experimental setup and hyperparameters.
Foreachtaskandtask-specificsetting(labelskew, amountofprivilegedinformation, etc.), wetrain10models
from each relevant class using hyperparameters randomly selected from given ranges (see Appendix A). For
DANN and MDD, the trade-off parameter, which regularizes domain discrepancy in representation space,
increases from 0 to 0.1 during training; for MDD, the margin parameter is set to 3. All models are evaluated
on a held-out validation set from the source domain and the best-performing model in each class is then
evaluated on held-out test sets from both domains. For SL-T, we use a held-out validation set from the target
domain. We repeat this procedure over 5 or 10 seeds, controlling the data splits and the random number
generation. We report accuracy and area under the ROC curve (AUC) with 95 %confidence intervals
computed by bootstrapping over the seeds.
4.1 Celebrity Photo Classification With Binary Attributes as PI
In the case where privileged information is available as binary attributes, we follow Xie et al. (2020) who
introduced a binary classification task based on the CelebA dataset (Liu et al., 2015), where the goal is
to predict whether the person in an image has been identified as male or female ( Y) in one of the binary
9Published in Transactions on Machine Learning Research (09/2024)
Table 2: Celebrity photo classification. DALUPI per-
forms comparably to the In-N-Out models in Xie et al.
(2020). Note: In-N-Out results are reported as the av-
erage of 5 trials with 90 %confidence intervals.
Target accuracy
SL-T 86.6 (86.3, 86.9)
SL-S 78.4 (77.1, 80.0)
DANN 78.2 (76.2, 80.3)
MDD 78.3 (77.5, 79.1)
In-N-Out (w/o pretraining) 78.5 (77.2, 79.9)
In-N-Out (w. pretraining) 79.4 (78.7, 80.1)
In-N-Out (rep. self-training) 80.4 (79.7, 81.1)
DALUPI (Wâˆ¼T) 76.4 (73.8, 78.6)
DALUPI (Wâˆ¼S,T) 80.3 (77.9, 82.7)
0.25 0.50 0.75 1.00
Skew/epsilon10.20.40.60.81.0AccuracySL-T
SL-S
DALUPI
DANN
MDDFigure 4: Digit classification. Target domain accu-
racy as a function of association Ïµbetween back-
ground and label in S. As the skew increases,
thetarget-domainperformanceofthenon-privileged
models deteriorates.
attributes that accompanies the data setâ€™s photos of celebrities ( X). Like Xie et al. (2020), we use 7 of the
40 other attributes ( Bald,Bangs,Mustache ,Smiling,5_o_Clock Shadow ,Oval_Face , and Heavy_Makeup )
as a vector of privileged information Wâˆˆ{0,1}7. The target and source domains are defined by people
wearing (T) and not wearing ( S) a hat. The respective datasets contain 3,000 and 2,000 images. An extra
30,000 unlabeled source samples are available to train estimators (DALUPI and In-N-Out) that can utilize
privileged information from both source and target. More details can be found in (Xie et al., 2020) and in
Appendix A.3.
Table 2 shows the target accuracy for each model. We observe that when DALUPI is provided with PI from
both source and target, it performs comparably to the best-performing In-N-Out model proposed by Xie
et al. (2020), while outperforming other feasible baselines on average. Confidence intervals overlap for all
feasible models. Notably, the best-performing In-N-Out models require four or more rounds of training to
achieve their results (baseline, auxiliary input, auxiliary output pre-training, tuning and self-training) (Xie
et al., 2020). Both DALUPI and In-N-Out benefit from access to privileged information from both the source
and target domain (pre/self-training for In-N-Out).
Finally, it is worth noting that neither covariate shift, nor sufficiency are likely to hold with respect to W
in this task. Specifically, photos with none of the 7 attributes active, w=0, have different label rates and
majority label in SandT(the rates of labels are Â¯YS= 0.64and Â¯YT= 0.46, respectively) and therefore
P(Y|W)is not constant, i.e. covariate shift is violated. In addition, the best model we have found trained
onWalone achieves only 65 %accuracy, compared to the results in Table 2â€”sufficiency is unlikely to hold.
Thus, DALUPI is robust to violations of these assumptions.
4.2 Digit Classification With Single Region of Interest as PI
We construct a synthetic image dataset, based on the assumptions of Proposition 1, to verify that there
are problems where DALUPI is guaranteed successful transfer but standard UDA is not. Starting from
CIFAR-10 (Krizhevsky, 2009) images upscaled to 128Ã—128, we insert a random 28Ã—28digit image from
the MNIST dataset (Lecun, 1998), with a label in the range 0â€“4, into a random location of each CIFAR-10
image, forming the input image X(see Figure 1 (top) for examples). The label Yâˆˆ{0,..., 4}is determined
by the MNIST digit. We store the bounding box around the inserted digit image and use the pixels contained
within it as privileged information Wduring training. The domains are constructed using CIFAR-10â€™s first
five and last five classes as source and target backgrounds, respectively. Both source and target datasets
contain 15,298 images each. To increase the difficulty of the task, we make the digit be the mean color of the
10Published in Transactions on Machine Learning Research (09/2024)
Table 3: Entity classification. UDA models have
accesstoallunlabeledtargetsamples, LUPItoallPI
(source), and DALUPI to all PI (source and target).
Source AUC Target AUC
SL-T 60.1 (58.7, 61.5) 69.0 (68.1, 69.9)
SL-S 69.5 (68.6, 70.4) 63.0 (61.6, 64.2)
DANN 68.1 (67.5, 68.7) 62.5 (61.9, 63.1)
MDD 62.4 (61.1, 63.9) 57.7 (56.3, 59.2)
LUPI 69.3 (68.5, 70.1) 65.9 (65.0, 66.8)
DALUPI 71.4 (70.3, 72.4) 68.2 (66.3, 70.1)
0.0 0.5 1.0
nPI(S)
nPI(T) = 00.50.60.70.8AUCSL-T
SL-S
DALUPI
LUPI
DANN
MDD
0.0 0.5 1.0
nPI(T)
nPI(S) = 1Figure 5: Entity classification. Target domain AUC.
The performance of SL-S and SL-T is extended across
the x-axes for visual purposes. DANN and MDD use
an increasing fraction of target samples Ëœxbut no PI.
dataset and make the digit background transparent so that the border of the image is less distinct. This may
slightly violate Assumption 2 with respect to the region of interest Wsince the backgrounds differ between
domains.
To understand how successful transfer depends on domain overlap and access to sufficient privileged infor-
mation, we include a skew parameter Ïµâˆˆ[1
c,1], wherec= 5is the number of digit classes, which determines
the correlation between digits and backgrounds. For a source image iwith digit label Yiâˆˆ {0,..., 4},
we select a random CIFAR-10 image with class Biâˆˆ{0,..., 4}with probability P(Bi=b|Yi=y) =
{Ïµ,ifb=y; (1âˆ’Ïµ)/(câˆ’1),otherwise}. For target images, digits and backgrounds are matched uniformly
at random. The choice Ïµ=1
cyields a uniform distribution and Ïµ= 1is equivalent to the background carrying
as much signal as the privileged information. We hypothesize that Ïµ= 1is the worst possible case where
confusion of the model is likely, which would lead to poor adaptation under domain shift.
In Figure 4, we observe the conjectured behavior. As the skew Ïµand the association between background
and label increases, the performance of SL-S decreases rapidly on the target domain. At Ïµ= 1, it performs
no better than random guessing, likely because the model has learned to associate spurious features in the
background with the label of the digit. We also observe that DANN and MDD deteriorate in performance
with increased correlation between the label and the background. In contrast, DALUPI is unaffected by the
skew as the subset of pixels extracted by Ë†fonly carries some of the background with it, while containing
sufficient information to make good predictions. Interestingly, DALUPI also seems to be as good or slightly
better than the oracle SL-T in this setting. This may be due to improved sample efficiency from using PI.
4.3 Entitity Classification With Multiple Regions of Interest as PI
Next, we consider multi-label classification of the presence of four types of entities (persons, cats, dogs, and
birds) indicated by a binary vector Yâˆˆ{0,1}4for imagesXfrom the MS-COCO dataset (Lin et al., 2014).
PI is used to localize regions of interest Wrelated to the entities, provided as bounding box annotations. We
define source and target domains SandTas indoor and outdoor images, respectively. Indoor images are
extracted by filtering out images from the MS-COCO super categories â€œindoorâ€ and â€œapplianceâ€ that also
contain at least one of the four main label classes. Outdoor images are extracted using the super categories
â€œvehicleâ€ and â€œoutdoorâ€. In total, there are 5,231 images in the source and 5,719 images in the target domain;
the distribution of labels is provided in Appendix A.5.
Sufficiency is likely to hold in this task because the pixels contained in a bounding box should be sufficient
for an annotator to classify the entity according to the four categories above, irrespective of the pixels outside
11Published in Transactions on Machine Learning Research (09/2024)
Table 4: X-ray task. Test AUC for the three pathologies in the
target domain for all considered models. Boldface indicates the
best-performing feasible model; SL-T uses target labels.
ATL CM PE
SL-T 57 (56, 58) 59 (55, 63) 79 (78, 80)
SL-S 55 (55, 56) 61 (58, 64) 73 (70, 75)
DANN 53 (51, 55) 55 (53, 58) 55 (51, 61)
MDD 49 (48, 50) 51 (51, 52) 51 (48, 54)
DALUPI 55 (55, 56) 72 (71, 73) 74 (72, 76)
Figure 6: Left: Example from the X-ray
target test set with label CM. The red rect-
angle indicates the bounding box predicted
by DALUPI. Right: saliency map for CM
for SL-S.
of the box. Similarly, covariate shift is likely to hold since the label attributed to the pixels in a bounding
box should be the same, whether the entity is indoor or outdoor.
We study the effect of adding privileged information by first training the end-to-end model in a LUPI setting,
using all (x,y)samples from the source domain and increasing the fraction of inputs for which PI is available,
nPI(S), from 0 to 1. We then train the model in a DALUPI setting, increasing the fraction of (Ëœx,Ëœw)samples
from the target domain, nPI(T), from 0 to 1, while keeping nPI(S) = 1. We train SL-S and SL-T using all
available data and increase the fraction of unlabeled target samples used by DANN and MDD from 0.2 to 1
while using all data from the source domain.
Table 3 shows the modelsâ€™ source and target domain AUC, averaged over the four entity classes, when
the UDA models have access to all unlabeled target samples, LUPI to all PI from the source domain, and
DALUPI to all PI from both domains. Clearly, DALUPI yields a substantial gain in adaptation. As we see
in Figure 5, the performance of LUPI increases as nPI(S)increases. When additional (Ëœx,Ëœw)samples from
the target domain are added, DALUPI outperforms SL-S and approaches the performance of SL-T. We note
that DANN and MDD do not benefit as much from added unlabeled target samples as DALUPI does. Their
weak performance could be explained by difficulties in adversarial training. The gap between LUPI and
SL-S fornPI(S) = 0is anticipated; we do not expect the detection network to work well without bounding
box supervision.
4.4 X-ray Classification With Multiple Regions of Interest as PI
As a real-world application, we study detection of pathologies in chest X-ray images. We use the ChestX-
ray8 dataset (Wang et al., 2017) as source domain and the CheXpert dataset (Irvin et al., 2019) as target
domain.1As PI, we use the regions of pixels associated with each found pathology, as annotated by domain
experts using bounding boxes. For the CheXpert dataset, only pixel-level segmentations are available, and
we create bounding boxes that tightly enclose the segmentations. It is not obvious that the pixels within
such a bounding box are sufficient for classifying the pathology. For this reason, we suspect that some of
the assumptions of Proposition 1 may be violated. However, as we find below, DALUPI improves empirical
performance compared to baselines for small training sets, thereby demonstrating increased sample efficiency.
We consider the three pathologies that exist in both datasets and for which there are annotated findings:
atelectasis(ATL:collapsedlung), cardiomegaly(CM:enlargedheart), andpleuraleffusion(PE:wateraround
the lung). There are 457 and 118 annotated images in the source and target domain, respectively. We train
DALUPI, DANN and MDD using all these images. SL-S is trained with the 457 source images and SL-T with
the 118 target images as well as 339 labeled but non-annotated target images. Neither SL-S, SL-T, DANN,
nor MDD support using privileged information. The distributions of labels and bounding box annotations
are given in Appendix A.6.
1This study was granted IRB approval.
12Published in Transactions on Machine Learning Research (09/2024)
In Table 4, we present the per-class AUCs in the target domain. DALUPI outperforms all baseline models,
including the target oracle, in detecting CM. For ATL and PE, it performs similarly to or better than the
other feasible models. That SL-T is better at predicting PE is not surprising because this pathology is most
prevalent in the target domain. In Figure 6, we show a single-finding image from the target test set with
ground-truth label CM. The predicted bounding box of DALUPI with the highest probability is added to
the image. DALUPI identifies the region of interest (the heart) and makes a correct classification. The
rightmost panel shows the saliency map for the ground truth class for SL-S. We see that the gradients are
mostly constant, indicating that the model is uncertain. In Appendix B, we show AUC for CM for the
models trained with additional examples withoutbounding box annotations. We find that SL-S reaches
the performance of DALUPI when a large amount of labeled examples are provided. This indicates that
identifiability is not the main obstacle for adaptation and that PI improves sample efficiency.
5 Related Work
Learning using privileged information was first introduced by Vapnik & Vashist (2009) for support vector
machines (SVMs), and was later extended to empirical risk minimization (Pechyony & Vapnik, 2010). Meth-
ods using PI, which is sometimes called hidden information or side information, has since been applied in
many diverse settings such as healthcare (Shaikh et al., 2020), finance (Silva et al., 2010), clustering (Fey-
ereisl & Aickelin, 2012) and image recognition (Vu et al., 2019; Hoffman et al., 2016). Related concepts
include knowledge distillation (Hinton et al., 2015; Lopez-Paz et al., 2016), where a teacher model trained
on additional variables adds supervision to a student model, and weak supervision (Robinson et al., 2020)
where so-called weak labels are used to learn embeddings, subsequently used for the task of interest. Further-
more, in the realm of NLP, there is the related concept of learning using feature feedback, where additional
annotations that are related to the associated task label are provided (Katakkar et al., 2022; Kaushik et al.,
2021). These works are mostly of an empirical nature, and theoretical work on the subject either considers
linear models/SVMs (Poulis & Dasgupta, 2017) or a teacher/student-type setup where additional supervi-
sion is given when the model predicts incorrectly (Dasgupta et al., 2018). The use of PI for deep image
classification has been investigated by Chen et al. (2017) and Han et al. (2023) but these works only cover
regular supervised learning where source and target domains coincide. Further, Sharmanska et al. (2014)
used regions of interest in images as privileged information to improve the accuracy of image classifiers, but
did not consider domain shift either.
Domain adaptation using PI has been considered before with SVMs (Li et al., 2022; Sarafianos et al., 2017),
but not with more complex classifiers such as neural networks. Vu et al. (2019) used scene depth as PI in
semantic segmentation using deep neural networks. However, they only used PI from the source domain
and they did not provide any theoretical analysis. Xie et al. (2020) provide some theoretical results for a
similar setup to ours. However, these are specifically for linear classifiers while our approach holds for any
type of classifier. Motiian (2019) investigated PI and domain adaptation using the information bottleneck
method for visual recognition. However, their setting differs from ours in that each observation comprises
source-domain and target-domain features, a label and PI. Another related approach is that of subsidiary
tasks (Kundu et al., 2022; Ye et al., 2022). However, in these settings the additional tasks performed are
used to build a representation that helps with the main task through domain alignment. Our approach
instead seeks to use information which directly relates to the main task.
6 Discussion
We have presented DALUPI: unsupervised domain adaptation by learning using privileged information (PI).
The framework provides provable guarantees for adaptation under relaxed assumptions on the input features,
at the cost of collecting a larger variable set, such as attribute or bounding box annotations, during training.
Our analysis inspired practical algorithms for image classification which we evaluated using three kinds of
privileged information. In our experiments, we demonstrated tasks where our approach is successful while
existingadaptationmethodsfail. Weobservedempiricallyalsothatmethodsusingprivilegedinformationare
more sample-efficient than comparable non-privileged learners, in line with the literature. In fact, DALUPI
13Published in Transactions on Machine Learning Research (09/2024)
models occasionally even outperform oracle models trained using target labels due to their sample efficiency.
Thus, we recommend considering these methods in small-sample settings.
The main contribution of the paper is the proposed learning paradigm for domain adaptation with privileged
information. Since common benchmark datasets in UDA lack privileged information related to the learning
problem, we created three new tasks for evaluating our framework, see Section 4.2â€“4.4, which itself is a
notable contribution. We hope that this work inspires the community to develop additional datasets for
UDA using privileged information.
Toavoidassumingthatdomainoverlapissatisfiedwithrespecttoinputcovariates, werequirethatthelabelis
conditionally independent of the input features given the PIâ€”that the PI is â€œsufficientâ€. This is a limitation
whenever sufficiency is difficult to verify. However, in our motivating example of image classification, a
domain expert could choosePI so that sufficiency is reasonably justified. Moreover, in experiments on
CelebA, we see empirical gains from our approach even when sufficiency is known to be violated. Another
limitation is that we still rely on overlap in the privileged information, W, which may also be violated in some
circumstances. It is more likely that overlap holds for Wwhen, for example, it is a subset of X, as argued
in Figure 2. Designing experiments to test how sensitive DALUPI is to violations of these assumptions is an
interesting direction for future work.
The use of regions of interest as privileged information brings up an interesting point concerning the rela-
tionship between the label and the privileged information. In object detection tasks, it is natural to treat the
bounding box coordinates as label information. In this work, however, the learning tasks were multi-class
and multi-label image classification, not object detection. Producing a perfect box Wwas not the goal of the
learning task, and the bounding boxes were therefore neither critical for the task nor for the labels. Instead,
the bounding boxes were privileged information and our experiments in Section 4.2â€“4.4 sought to quantify
the value of this added information, compared to not having it. Therefore, we compared our method to
image classification baselines. It is not obvious a priori that learning from object locations improves the
adaptation of image classifiers.
If there is a lack of PI available to the models one might mitigate this by either 1) using the limited amount
of PI that is available to learn Ë†gand assume that it is good enough to achieve reasonable overall performance;
or 2) using the learned Ë†fto create â€œweakâ€ PI labels for the inputs that are missing PI, similar to the work
of e.g. Robinson et al. (2020). However, one should note that the latter approach might bias the model in
unintended ways and, as such, should be undertaken with some caution.
In future work, our framework could be applied to a more diverse set of tasks, with different modalities
of inputs and privileged information to investigate if the findings here can be replicated and extended.
Moreover, such work could consider different types and degrees of shifts to further corroborate the stability
and resistance to noise which we observe here. More broadly, using PI may be viewed as â€œbuilding inâ€
domain knowledge in the structure of the adaptation problem and we see this as a promising direction for
further research.
Acknowledgments
This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program
(WASP) funded by the Knut and Alice Wallenberg Foundation
The computations and data handling were enabled by resources provided by the National Academic Infras-
tructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through
grant agreement no. 2022-06725.
References
Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the
European conference on computer vision (ECCV) , pp. 456â€“473, 2018.
Shai Ben-David and Ruth Urner. On the hardness of domain adaptation and the utility of unlabeled target
samples. In International Conference on Algorithmic Learning Theory , pp. 139â€“153. Springer, 2012.
14Published in Transactions on Machine Learning Research (09/2024)
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain
adaptation. Advances in neural information processing systems , 19, 2006.
Adam Breitholtz and Fredrik Daniel Johansson. Practicality of generalization guarantees for unsupervised
domain adaptation with neural networks. Transactions on Machine Learning Research , 2022.
YunpengChen, XiaojieJin, JiashiFeng, andShuichengYan. Traininggrouporthogonalneuralnetworkswith
privileged information. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial
Intelligence, IJCAI-17 , pp. 1532â€“1538, 2017.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting. In
Advances in Neural Information Processing Systems , volume 23, 2010.
Sanjoy Dasgupta, Akansha Dey, Nicholas Roberts, and Sivan Sabato. Learning from discriminative feature
feedback. In Advances in Neural Information Processing Systems 31 (NeurIPS 2018) , 2018.
Antoine de Mathelin, FranÃ§ois Deheeger, Guillaume Richard, Mathilde Mougeot, and Nicolas Vayatis.
ADAPT: Awesome Domain Adaptation Python Toolbox. arXiv preprint arXiv:2107.03049 , 2021.
Alexander Dâ€™Amour, Peng Ding, Avi Feller, Lihua Lei, and Jasjeet Sekhon. Overlap in observational studies
with high-dimensional covariates. Journal of Econometrics , 221(2):644â€“654, 2021.
Jan Feyereisl and Uwe Aickelin. Privileged information for data clustering. Information Sciences , 194:4â€“23,
2012.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, FranÃ§ois Lavi-
olette, Mario Marchand, and Victor Lempitsky. Domain-Adversarial Training of Neural Networks.
arXiv:1505.07818 [cs, stat] , May 2016. arXiv: 1505.07818.
Pascal Germain, Amaury Habrard, FranÃ§ois Laviolette, and Emilie Morvant. PAC-Bayes and Domain Adap-
tation.Neurocomputing , 379:379â€“397, February 2020. arXiv: 1707.05712.
Ross Girshick. Fast R-CNN. In Proceedings of the IEEE international conference on computer vision , pp.
1440â€“1448, 2015.
Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International Conference
on Learning Representations , 2021.
Dongyoon Han, Junsuk Choe, Seonghyeok Chun, John Joon Young Chung, Minsuk Chang, Sangdoo Yun,
Jean Y. Song, and Seong Joon Oh. Neglected free lunch - learning image classifiers using annotation
byproducts. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp.
20200â€“20212, October 2023.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 770â€“778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770â€“778, 2016b.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
Judy Hoffman, Saurabh Gupta, and Trevor Darrell. Learning with Side Information through Modality
Hallucination. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp.
826â€“834. IEEE, 2016.
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Mark-
lund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset
with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intel-
ligence, volume 33, pp. 590â€“597, 2019.
15Published in Transactions on Machine Learning Research (09/2024)
Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-invariant
representations. In The 22nd International Conference on Artificial Intelligence and Statistics , pp. 527â€“
536. PMLR, 2019.
BastianJungandFredrikDanielJohansson. Efficientlearningofnonlinearpredictionmodelswithtime-series
privileged information. In Advances in Neural Information Processing Systems , 2022.
Rickard Karlsson, Martin Willbo, Zeshan Hussain, Rahul G. Krishnan, David A. Sontag, and Fredrik D.
Johansson. Using time-series privileged information for provably efficient learning of prediction models.
InProceedings of The 25th International Conference on Artificial Intelligence and Statistics 2022 , 2021.
Anurag Katakkar, Clay H. Yoo, Weiqin Wang, Zachary Lipton, and Divyansh Kaushik. Practical benefits of
feature feedback under distribution shift. In Proceedings of the Fifth BlackboxNLP Workshop on Analyzing
and Interpreting Neural Networks for NLP , pp. 346â€“355, 2022.
D. Kaushik, A. Setlur, E. H. Hovy, and Z. C. Lipton. Explaining the efficacy of counterfactually augmented
data. In International Conference on Learning Representations , 2021.
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,
Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-
the-wild distribution shifts. In International Conference on Machine Learning , pp. 5637â€“5664. PMLR,
2021.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of
Toronto, 2009.
Jogendra Nath Kundu, Suvaansh Bhambri, Akshay Kulkarni, Hiran Sarkar, Varun Jampani, and
R. Venkatesh Babu. Concurrent subsidiary supervision for unsupervised source-free domain adaptation.
InProceedings of the 17th European Conference on Computer Vision (ECCV) , pp. 177â€“194, 2022.
Yann Lecun. Gradient-Based Learning Applied to Document Recognition. proceedings of the IEEE , 86(11):
47, 1998.
Yanmeng Li, Huaijiang Sun, and Wenzhu Yan. Domain adaptive twin support vector machine learning using
privileged information. Neurocomputing , 469:13â€“27, 2022.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar,
and Larry Zitnick. Microsoft COCO: Common Objects in Context. In ECCV. European Conference on
Computer Vision, September 2014.
Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature
pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pp. 2117â€“2125, 2017.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In 2015
IEEE International Conference on Computer Vision (ICCV) , pp. 3730â€“3738, 2015.
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip S. Yu. Transfer feature learning
withjointdistributionadaptation. In Proceedings of the 2013 IEEE International Conference on Computer
Vision, ICCV â€™13, pp. 2200â€“2207, USA, 2013. IEEE Computer Society.
David Lopez-Paz, Leon Bottou, Bernhard SchÃ¶lkopf, and Vladimir Vapnik. Unifying distillation and privi-
leged information. In International Conference on Learning Representations (ICLR 2016) , 2016.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning . MIT Press,
second edition, 2018.
Saeid Motiian. Domain Adaptation and Privileged Information for Visual Recognition . PhD thesis, West
Virginia University, 2019. Graduate Theses, Dissertations, and Problem Reports. 6271.
16Published in Transactions on Machine Learning Research (09/2024)
Shota Orihashi, Mana Ihori, Tomohiro Tanaka, and Ryo Masumura. Unsupervised Domain Adaptation
for Dialogue Sequence Labeling Based on Hierarchical Adversarial Training. In Interspeech 2020 , pp.
1575â€“1579. ISCA, October 2020.
Dmitry Pechyony and Vladimir Vapnik. On the theory of learnining with privileged information. In Advances
in Neural Information Processing Systems , volume 23. Curran Associates, Inc., 2010.
Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The
visual domain adaptation challenge. arXiv preprint arXiv:1710.06924 , 2017.
Stefanos Poulis and Sanjoy Dasgupta. Learning with feature feedback: from theory to practice. In Interna-
tional Conference on Artificial Intelligence and Statistics (AISTATS) , 2017.
S Ren, K He, R Girshick, and J Sun. Faster R-CNN: Towards real-time object detection with region proposal
networks. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(6):1137â€“1149, 2016.
Joshua Robinson, Stefanie Jegelka, and Suvrit Sra. Strength from Weakness: Fast Learning Using Weak
Supervision. In Proceedings of the 37th International Conference on Machine Learning , pp. 8127â€“8136.
PMLR, November 2020.
Nikolaos Sarafianos, Michalis Vrigkas, and Ioannis A. Kakadiaris. Adaptive SVM+: Learning with privileged
information for domain adaptation. In Proceedings of the IEEE International Conference on Computer
Vision (ICCV) Workshops , Oct 2017.
Tawseef Ayoub Shaikh, Rashid Ali, and M. M. Sufyan Beg. Transfer learning privileged information fuels
CAD diagnosis of breast cancer. Machine Vision and Applications , 31(1):9, February 2020.
Viktoriia Sharmanska, Novi Quadrianto, and Christoph H. Lampert. Learning to Transfer Privileged Infor-
mation.arXiv:1410.0389 [cs, stat] , October 2014. arXiv: 1410.0389.
Kendrick Shen, Robbie M Jones, Ananya Kumar, Sang Michael Xie, Jeff Z. Haochen, Tengyu Ma, and Percy
Liang. Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation. In
Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),
Proceedings of the 39th International Conference on Machine Learning , volume 162 of Proceedings of
Machine Learning Research , pp. 19847â€“19878. PMLR, 17â€“23 Jul 2022.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood
function. Journal of statistical planning and inference , 90(2):227â€“244, 2000.
Catarina Silva, Armando Vieira, Antonio Gaspar-Cunha, and Joao Carvalho das Neves. Financial distress
model prediction using SVM+. In Proceedings of the International Joint Conference on Neural Networks ,
pp. 1â€“7, July 2010.
M. Sugiyama, T. Suzuki, and T. Kanamori. Density Ratio Estimation in Machine Learning . Cambridge
books online. Cambridge University Press, 2012.
Marian Tietz, Thomas J. Fan, Daniel Nouri, Benjamin Bossan, and skorch Developers. skorch: A scikit-learn
compatible neural network library that wraps PyTorch , July 2017.
Vladimir Vapnik and Rauf Izmailov. Learning Using Privileged Information: Similarity Control and Knowl-
edge Transfer. Journal of Machine Learning Research , 16:2023â€“2049, 2015.
Vladimir Vapnik and Akshay Vashist. A new learning paradigm: Learning using privileged information.
Neural Networks , 22(5):544â€“557, July 2009.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing
network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 5018â€“5027, 2017.
17Published in Transactions on Machine Learning Research (09/2024)
Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick PÃ©rez PÃ©rez. Dada: Depth-
aware domain adaptation in semantic segmentation. In 2019 IEEE/CVF International Conference on
Computer Vision (ICCV) , pp. 7363â€“7372, 2019.
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-
ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and local-
ization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 2097â€“2106, 2017.
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with asymmetrically-
relaxed distribution alignment. In International conference on machine learning , pp. 6872â€“6881. PMLR,
2019.
Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out:
Pre-training and self-training using auxiliary information for out-of-distribution robustness. arXiv preprint
arXiv:2012.04550 , 2020.
Yalan Ye, Ziqi Liu, Yangwuyong Zhang, Jingjing Li, and Hengtao Shen. Alleviating style sensitivity then
adapting: Source-free domain adaptation for medical image segmentation. In Proceedings of the 30th ACM
International Conference on Multimedia , MM â€™22, pp. 1935â€“1944, New York, NY, USA, 2022. Association
for Computing Machinery.
John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann.
Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs:
a cross-sectional study. PLoS medicine , 15(11):e1002683, 2018.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain
adaptation. In International conference on machine learning , pp. 7404â€“7413. PMLR, 2019.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant representa-
tions for domain adaptation. In International Conference on Machine Learning , pp. 7523â€“7532. PMLR,
2019.
A Experimental Details
In this section, we give further details of the experiments. All code is written in Python and we
mainly use PyTorch in combination with skorch (Tietz et al., 2017) for our implementations of the net-
works. For Faster R-CNN, we adapt the implementation provided by torchvision through the func-
tion fasterrcnn_resnet50_fpn . For DANN and MDD, we use the ADAPT TensorFlow implementation
(de Mathelin et al., 2021) with a ResNet-50-based encoder. We initially set the trade-off parameter Î», which
controls the amount of domain adaption regularization, to 0 and then increase it to 0.1 in 10,000 gradient
steps according to the formula Î»=Î²(2/(1 +eâˆ’p)âˆ’1)/C, wherepincreases linearly from 0 to 1, Î²is a
parameter specified for each experiment, and C= 2/(1 +eâˆ’1)âˆ’1. For MDD, we fix the margin parameter
Î³to 3. The source and target baselines are based on the ResNet-50 architecture when PI is provided as
multiple regions of interest; otherwise, the ResNet-18 architecture is used. The architecture of DALUPI in
each experiment is specified in the respective subsection below.
We use the Adam optimizer in all experiments. Learning rate decay is treated as a hyperparameter. For
ADAPT models (DANN and MDD), the learning rate is either constant or decayed according to Âµ0/(1 +
Î±p)3/4, whereÂµ0is the initial learning rate, pincreases linearly from 0 to 1, and Î±is a parameter specified
in each experiment (see below). For non-ADAPT models, the learning rate is either constant or decayed by
a factor 0.1everynth epoch, where nis another hyperparameter.
For all models except LUPI and DALUPI, the classifier network following the encoder is a simple MLP with
two possible settings: Either it is a single linear layer from inputs to outputs or a three-layer network with
ReLU activations between the layers. This choice is treated as a hyperparameter in our experiments. The
nonlinear case has the following structure where nis the number of input features:
18Published in Transactions on Machine Learning Research (09/2024)
â€¢fully connected layer with nneurons
â€¢ReLU activation layer
â€¢fully connected layer with n//2neurons
â€¢ReLU activation layer
â€¢fully connected layer with n//4neurons.
All models were trained using NVIDIA Tesla A40 GPUs and the development and evaluation of this study
required approximately 30,000 hours of GPU training. The code is available on GitHub: https://github.
com/Healthy-AI/dalupi .
A.1 DALUPI With Two-stage Classifier
Here, we describe in more detail how we construct our two-stage classifier for image classification when
privileged information is provided as a single region of interest as in the digit classification task (Section
4.2). When privileged information is provided as binary attributes, we can directly learn the two-stage
estimator according to Equation 2. In this task, it was found that using the cross entropy loss and using
continuous outputs from fprovided superior performance compared to other losses. In the digit classification
task, each image xihas a single label yiâˆˆ{0,..., 4}determined by the MNIST digit. Privileged information
is given by a single bounding box with coordinates tiâˆˆR4enclosing a subset of pixels wicorresponding to
the digit. The training procedure is summarized in Algorithm 1 and further described below.
We first learn Ë†dwhich is a function that takes target image data, Ëœxi, and bounding box coordinates, ti, and
learns to output bounding box coordinates, Ë†ti, which should contain the privileged information wi. Note
that we do not exactly follow the setup in Equation 2 since we do not need to actually predict the pixel
values within the bounding box. If we find a good enough estimator of tiwe should minimize the loss of fin
Equation 2. To obtain the privileged information we apply a deterministic function Ï•which crops and scales
an image using the associated bounding box, ti. We can now write the composition of these two functions as
Ë†f(xi) =Ï•(xi,Ë†d(xi))which outputs the privileged information. The function Ï•is hard-coded and therefore
not learned.
In the second step, we learn Ë†gto predict the label from the privileged information wi, which is a cropped
version ofxiwhere the cropping is defined by the bounding box tiaround the digit. This cropping and
resizing is performed by Ï•. When we evaluate the performance of this classifier we combine the two models
into one, Ë†h(x) = Ë†g(Ï•(x,Ë†d(x))). We use the mean squared error loss for learning Ë†dand categorical cross-
entropy (CCE) loss for Ë†g.
Algorithm 1 Training of the two-stage model.
1:procedure Two_stage (Ëœxi,wi,ti,yi)
2:Empirically minimize1
m/summationtextm
i=1âˆ¥d(Ëœxi)âˆ’tiâˆ¥2and obtain Ë†d.
3:Empirically minimize1
n/summationtextn
i=1CCE (g(wi),yi)and obtain Ë†g.
4:Compose Ë†d,Ë†gandÏ•intoË†h(x) = Ë†g(Ï•(x,Ë†d(x))).
5:end procedure
A.2 DALUPI With Faster R-CNN
For multi-label classification, we adapt Faster R-CNN (Ren et al., 2016) outlined in Figure 7 and described
below. Faster R-CNN uses a region proposal network (RPN) to generate region proposals which are fed to a
detection network for classification and bounding box regression. This way of solving the task in subsequent
steps has similarities with our two-stage algorithm although Faster R-CNN can be trained end-to-end. We
make small modifications to the training procedure of the original model in the end of this section.
19Published in Transactions on Machine Learning Research (09/2024)
â‹¯RPNResNetFeature	MapsRoIPoolingğ¿clsRPNğ¿regRPNğ¿clsdetğ¿regdetObject	proposals
Reference	boxes
Figure 7: Faster R-CNN (Ren et al., 2016) architecture. The RoI pooling layer and the classification and
regression layers are part of the Fast R-CNN detection network (Girshick, 2015).
The RPN generates region proposals relative to a fixed number of reference boxesâ€”anchorsâ€”centered at
the locations of a sliding window moving over convolutional feature maps. Each anchor is assigned a binary
labelpâˆˆ{0,1}based on its overlap with ground-truth bounding boxes; positive anchors are also associated
with a ground-truth box with location t. The RPN loss for a single anchor is
LRPN(Ë†p,p,Ë†t,t) :=LRPN
cls(Ë†p,p) +pLRPN
reg(Ë†t,t), (3)
where Ë†trepresents the refined location of the anchor and Ë†pis the estimated probability that the anchor
contains an object. The binary cross-entropy loss and a smooth L1loss are used for the classification loss
LRPN
clsand the regression loss LRPN
reg, respectively. For a mini-batch of images, the total RPN loss is computed
based on a subset of all anchors, sampled to have a ratio of up to 1:1 between positive and negative ditto.
A filtered set of region proposals are projected onto the convolutional feature maps. For each proposal, the
detection networkâ€”Fast R-CNN (Girshick, 2015)â€”outputs a probability Ë†p(k)and a predicted bounding box
location Ë†t(k)for each class k. Let Ë†p= (Ë†p(0),..., Ë†p(K)), where/summationtext
kË†p(k) = 1,Kis the number of classes
and0represents a catch-all background class. For a single proposal with ground-truth coordinates tand
multi-class label uâˆˆ{0,...,K}, the detection loss is
Ldet(Ë†p,u,Ë†tu,t) =Ldet
cls(Ë†p,u) +Iuâ‰¥1Ldet
reg(Ë†tu,t), (4)
whereLdet
cls(Ë†p,u) =âˆ’log Ë†p(u)andLdet
regis a smooth L1loss. To obtain a probability vector for the entire
image, we maximize, for each class k, over the probabilities of all proposals.
During training, Faster R-CNN requires that all input images xcome with at least one ground-truth anno-
tation (bounding box) wand its corresponding label u. To increase sample-efficiency, we enable training the
model using non-annotated but labeled samples (x,y)from the source domain and annotated but unlabeled
samples (Ëœx,Ëœw)from the target domain. In the RPN, no labels are needed, and we simply ignore anchors from
non-annotated images when sampling anchors for the loss computation. For the computation of Equation
4, we handle the two cases separately. We assign the label u=âˆ’1to all ground-truth annotations from the
target domain and multiply Ldet
clsby the indicator Iuâ‰¥0. For non-annotated samples (x,y)from the source
domain, there are no box-specific coordinates tor labelsubut only the labels yfor the entire image. In this
case, 4 is undefined and we instead compute the binary cross-entropy loss between the per-image label and
the probability vector for the entire image.
We train the RPN and the detection network jointly as described in Ren et al. (2016). To extract feature
maps, we use a Feature Pyramid Network (Lin et al., 2017) on top of a ResNet-50 architecture He et al.
(2016b). We use the modified model in the experiments in Section 4.3 and 4.4. In Section 4.3, we also train
this model in a LUPI setting, where no information from the target domain is used.
20Published in Transactions on Machine Learning Research (09/2024)
A.3 Celebrity Photo Classification With Binary Attribute Vector
In our experiment based on CelebA (Liu et al., 2015), the input xis an RGB image which has been resized
to 64Ã—64 pixels, the target yis a binary label for gender of the subject of the image, and the privileged
information ware 7 binary-valued attributes. The attributes used in this experiment are: Bald,Bangs,
Mustache ,Smiling,5_o_Clock_Shadow ,Oval_Face and Heavy_Makeup . We use a subset of the CelebA
dataset with 2,000 labeled source examples and 3,000 unlabeled target examples. We use 1,000 samples
each for the source validation set, source test set, and target test set, respectively. The target oracle, SL-T,
is trained using labels provided for the 3,000 target examples, with 20 %of these examples set aside for
validation. The same unlabeled validation set is used to validate the first DALUPI network, Ë†f. When using
privileged information from the source domain to train Ë†f, we use 30,000 extra samples (x,w)with PI.
For DALUPI, we use the two-stage estimator with the network Ë†fbased on ResNet-18 followed by a non-linear
MLP. The network Ë†gis an MLP with two hidden layers of with 256 neurons each. We train the models for
100 epochs. If the validation accuracy (or validation AUC for Ë†f) does not improve for 10 subsequent epochs,
we stop the training earlier. For DALUPI, the early stopping patience is 15 for each network. We treat
the problem as multi-class classification with two classes and use the categorical cross entropy loss for SL-S,
SL-T, DANN, and MDD.
A.3.1 Hyperparameters
We randomly choose hyperparameters from the following predefined sets of values:
â€¢SL-S and SL-T:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“step sizenfor learning rate decay: (15, 30, 100)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0, 0.1, 0.2, 0.5)
â€“nonlinear classifier: ( True,False).
â€¢DALUPI:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’5,1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“step sizenfor learning rate decay: (15, 30, 100)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3).
â€¢DANN:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“width of discriminator network: (64,128,256)
â€“depth of discriminator network: (2,3)
â€“nonlinear classifier: ( True,False)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
â€¢MDD:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
21Published in Transactions on Machine Learning Research (09/2024)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“nonlinear classifier: ( True,False)
â€“maximum norm value for classifier weights: (0.5, 1.0, 2.0)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
A.4 Digit Classification With Single Bounding Box as PI
In the digit classification task, we separate 20 %of the available source and target data into a test set. We
likewise use 20 %of the training data for validation purposes. For DALUPI we use ResNet-18 for the function
Ë†f. We replace the default fully connected layer with a fully connected layer with 4 neurons to predict the
coordinates of the bounding box. The predicted bounding box is resized to a 28Ã—28square no matter the
initial size. We use a simple convolutional neural network for the function Ë†gwith the following structure:
â€¢convolutional layer with 16 output channels, kernel size of 5, stride of 1, and padding of 2
â€¢max pooling layer with kernel size 2, followed by a ReLU activation
â€¢convolutional layer with 32 output channels, kernel size of 5, stride of 1, and padding of 2
â€¢max pooling layer with kernel size 2, followed by a ReLU activation
â€¢dropout layer with p= 0.4
â€¢fully connected layer with 50 out features, followed by ReLU activation
â€¢dropout layer with p= 0.2
â€¢fully connected layer with 5 out features.
The model training is stopped when the best validation accuracy (or validation loss for Ë†f) does not improve
over 10 epochs or when the model has been trained for 100 epochs, whichever occurs first. All models are
trained from scratch, without pretrained weights. We use the categorical cross entropy loss for SL-S, SL-T,
DANN, and MDD.
A.4.1 Hyperparameters
We randomly choose hyperparameters from the following predefined sets of values:
â€¢SL-S and SL-T:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“step sizenfor learning rate decay: (15, 30, 100)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0, 0.1, 0.2, 0.5)
â€“nonlinear classifier: ( True,False).
â€¢DALUPI:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“step sizenfor learning rate decay: (15, 30, 100)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3).
22Published in Transactions on Machine Learning Research (09/2024)
Table 5: Marginal label distribution in source and target domains for the entity classification task based on
the MS-COCO dataset. The background class contains images where none of the four entities are present.
Domain Person Dog Cat Bird Background
Source 2,963 569 1,008 213 1,000
Target 3,631 1,121 423 712 1,000
â€¢DANN:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“width of discriminator network: (64,128,256)
â€“depth of discriminator network: (2,3)
â€“nonlinear classifier: ( True,False)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
â€¢MDD:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“nonlinear classifier: ( True,False)
â€“maximum norm value for classifier weights: (0.5, 1.0, 2.0)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
A.5 Entitity Classification With Multiple Regions of Interest as PI
In the entity classification experiment, we train all models for at most 50 epochs. If the validation AUC
does not improve for 10 subsequent epochs, we stop the training earlier. No pretrained weights are used in
this experiment since we find that the task is too easy to solve with pretrained weights. For DALUPI and
LUPI, we use the end-to-end solution based on Faster R-CNN (see Section A.2). We use the default anchor
sizes for each of the feature maps (32, 64, 128, 256, 512), and for each anchor size we use the default aspect
ratios (0.5, 1.0, 2.0). We use the binary cross entropy loss for SL-S, SL-T, DANN, and MDD.
We use the 2017 version of the MS-COCO dataset (Lin et al., 2014). As decribed in Section 4.3, we extract
indoor images by sorting out images from the super categories â€œindoorâ€ and â€œapplianceâ€ that also contain
at least one of the entity classes. Outdoor images are extracted in the same way using the super categories
â€œvehicleâ€ and â€œoutdoorâ€. Images that match both domains (for example an indoor image with a toy car)
are removed, as are any gray-scale images. We also include 1,000 negative examples, i.e., images with none
of the entities present, in both domains. In total, there are 5,231 images in the source domain and 5,719
images in the target domain. From these pools, we randomly sample 3,000, 1,000, and 1,000 images for
training, validation, and testing, respectively. In Table 5 we describe the label distribution in both domains.
All images are resized to 320Ã—320.
A.5.1 Hyperparameters
We randomly choose hyperparameters from the following predefined sets of values. For information about
the specific parameters in LUPI and DALUPI, we refer to the paper by Ren et al. (2016). Here, RoI and
NMS refer to region of interest and non-maximum suppression, respectively.
23Published in Transactions on Machine Learning Research (09/2024)
â€¢SL-S and SL-T:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“step sizenfor learning rate decay: (15, 30, 100)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0, 0.1, 0.2, 0.5)
â€“nonlinear classifier: ( True,False).
â€¢DANN:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“width of discriminator network: (64,128,256)
â€“depth of discriminator network: (2,3)
â€“nonlinear classifier: ( True,False)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
â€¢MDD:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“nonlinear classifier: ( True,False)
â€“maximum norm value for classifier weights: (0.5, 1.0, 2.0)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
â€¢LUPI and DALUPI:
â€“batch size: (16,32,64)
â€“learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“step sizenfor learning rate decay: (15, 30, 100)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“IoU foreground threshold (RPN): (0.6, 0.7, 0.8, 0.9)
â€“IoU background threshold (RPN): (0.2, 0.3, 0.4)
â€“batchsize per image (RPN): (32, 64, 128, 256)
â€“fraction of positive samples (RPN): (0.4, 0.5, 0.6, 0.7)
â€“NMS threshold (RPN): (0.6, 0.7, 0.8)
â€“RoI pooling output size (Fast R-CNN): (5, 7, 9)
â€“IoU foreground threshold (Fast R-CNN): (0.5, 0.6)
â€“IoU background threshold (Fast R-CNN): (0.4, 0.5)
â€“batchsize per image (Fast R-CNN): (16, 32, 64, 128)
â€“fraction of positive samples (Fast R-CNN): (0.2, 0.25, 0.3)
â€“NMS threshold (Fast R-CNN): (0.4, 0.5, 0.6)
â€“detections per image (Fast R-CNN): (25, 50, 75, 100).
24Published in Transactions on Machine Learning Research (09/2024)
Table 6: Marginal distribution of labels of images and bounding boxes in the source and target do-
main, respectively, for the chest X-ray classification experiment. ATL=Atelectasis; CM=Cardiomegaly;
PE=Effusion; NF=No Finding.
Data ATL CM PE NF
xâˆ¼S11,559 2,776 13,317 60,361
wâˆ¼S180 146 153 -
Ëœxâˆ¼T14,278 20,466 74,195 16,996
Ëœwâˆ¼T75 66 64 -
A.6 X-ray Classification With Multiple Regions of Interest as PI
In the X-ray classification experiment, we train all models for at most 50 epochs, using pre-trained weights
in the ResNet architecture of each model. If the validation AUC does not improve for 10 subsequent epochs,
we stop the training earlier. We then fine-tune all models, except DANN and MDD, for up to 20 additional
epochs. The number of encoder layers that are fine-tuned is a hyperparameter for which we consider different
values. We start the training with weights pretrained on ImageNet. For DALUPI, we use the end-to-end
solution based on Faster R-CNN (see Section A.2). We use the default anchor sizes for each of the feature
maps (32, 64, 128, 256, 512), and for each anchor size we use the default aspect ratios (0.5, 1.0, 2.0). We
use the binary cross entropy loss for SL-S, SL-T, DANN, and MDD.
In total, there are 83,519 (457) and 120,435 (118) images (annotated images) in the source and target
domain, respectively. The distributions of labels and bounding box annotations are provided in Table 6.
Here, â€œNFâ€ refers to images with no confirmed findings. In the annotated images, there are 180/146/153
and 75/66/64 examples of ATL/CM/PE in each domain respectively. Validation and test sets are sam-
pled from non-annotated images and contain 10,000 samples each. All annotated images are reserved for
training. We merge the default training and validation datasets before splitting the data and resize all
images to 320Ã—320. For the source dataset (ChestX-ray8), the bounding boxes can be found together with
the dataset. The target segmentations can be found here: https://stanfordaimi.azurewebsites.net/
datasets/23c56a0d-15de-405b-87c8-99c30138950c .
A.6.1 Hyperparameters
We choose hyperparameters randomly from the following predefined sets of values. For information about
the specific parameters in DALUPI, we refer to the paper by Ren et al. (2016). RoI and NMS refer to region
of interest and non-maximum suppression, respectively.
â€¢SL-S and SL-T:
â€“batch size: (16, 32, 64)
â€“learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“dropout (encoder): (0, 0.1, 0.2, 0.5)
â€“nonlinear classifier: ( True,False)
â€“number of layers to fine-tune: (3, 4, 5)
â€“learning rate (fine-tuning): ( 1.0Ã—10âˆ’5,1.0Ã—10âˆ’4).
â€¢DANN:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
25Published in Transactions on Machine Learning Research (09/2024)
â€“number of trainable layers (encoder): (1, 2, 3, 4, 5)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“width of discriminator network: (64,128,256)
â€“depth of discriminator network: (2,3)
â€“nonlinear classifier: ( True,False)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
â€¢MDD:
â€“batch size: (16,32,64)
â€“initial learning rate: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“parameterÎ±for learning rate decay: (0, 1.0)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“number of trainable layers (encoder): (1, 2, 3, 4, 5)
â€“dropout (encoder): (0,0.1,0.2,0.5)
â€“nonlinear classifier: ( True,False)
â€“maximum norm value for classifier weights: (0.5, 1.0, 2.0)
â€“parameterÎ²for adaption regularization decay: (0.1, 1.0, 10.0).
â€¢DALUPI:
â€“batch size: (16,32,64)
â€“learning rate: ( 1.0Ã—10âˆ’4)
â€“weight decay: ( 1.0Ã—10âˆ’4,1.0Ã—10âˆ’3)
â€“IoU foreground threshold (RPN): (0.6, 0.7, 0.8, 0.9)
â€“IoU background threshold (RPN): (0.2, 0.3, 0.4)
â€“batchsize per image (RPN): (32, 64, 128, 256)
â€“fraction of positive samples (RPN): (0.4, 0.5, 0.6, 0.7)
â€“NMS threshold (RPN): (0.6, 0.7, 0.8)
â€“RoI pooling output size (Fast R-CNN): (5, 7, 9)
â€“IoU foreground threshold (Fast R-CNN): (0.5, 0.6)
â€“IoU background threshold (Fast R-CNN): (0.4, 0.5)
â€“batchsize per image (Fast R-CNN): (16, 32, 64, 128)
â€“fraction of positive samples (Fast R-CNN): (0.2, 0.25, 0.3)
â€“NMS threshold (Fast R-CNN): (0.4, 0.5, 0.6)
â€“detections per image (Fast R-CNN): (25, 50, 75, 100)
â€“learning rate (fine-tuning): ( 1.0Ã—10âˆ’5,1.0Ã—10âˆ’4)
â€“number of layers to fine-tune: (3, 4, 5).
B Additional Results
In Figure 8a and 8b, we show some example images from the digit classification task with associated saliency
maps from the source-only model for different values of the skew parameter Ïµ. We can see that for a lower
value of epsilon the SL-S model activations seem concentrated on the area with the digit, while when the
correlation with the background is large the model activations are more spread out.
In Figure 9, we show the averageAUC when additional training data of up to 30,000 samples are added in
the chest X-ray experiment. We see that, once given access to a much larger amount of labeled samples,
SL-S and DALUPI perform comparably in the target domain.
In Figure 10, we show AUC for the pathology CM when additional training data withoutbounding box
annotations are added. We see that SL-S catches up to the performance of DALUPI when a large amount
of labeled examples are provided. These results indicate that identifiability is not the primary obstacle for
adaptation, and that PI improves sample efficiency.
26Published in Transactions on Machine Learning Research (09/2024)
(a) SL-S,Ïµ= 0.2
 (b) SL-S,Ïµ= 1.0
Figure 8: Example images (top) and saliency maps (bottom) from SL-S when trained with source skew
Ïµ= 0.2(a) andÏµ= 1(b).
0 10000 20000 30000
Extra (x,y) samples0.20.40.60.81.0AUCSL-T
SL-S
DALUPI
DANN
MDD
(a) Source test AUC.
0 10000 20000 30000
Extra (x,y) samples0.20.40.60.81.0AUCSL-T
SL-S
DALUPI
DANN
MDD (b) Target test AUC.
Figure 9: Classification of chest X-ray images. Model performance on source (a) and target (b) domains. The
AUC is averaged over the three pathologies: ATL, CM and PE. The 95 %confidence intervals are computed
using bootstrapping the results over five seeds.
C Proof of Proposition 1
Proposition. LetAssumptions1and2besatisfiedw.r.t. W(notnecessarilyw.r.t. X)andletAssumption3
hold as stated. Then, the target risk RTis identified for hypotheses h:Xâ†’Y,
RT(h) =/integraldisplay
xT(x)/integraldisplay
wT(w|x)/integraldisplay
yS(y|w)L(h(x),y)dydwdx .
and, forLthe squared loss, a minimizer of RTishâˆ—
T(x) =/integraltext
wT(w|x)ES[Y|w]dw .
Proof.By definition, RT(h) =/integraltext
x,yT(x,y)L(h(x),y)dydx. We marginalize over Wto get
T(x,y) =T(x)ET(W|x)[T(y|W,x)|x]]
=T(x)ET(W|x)[T(y|W)|x]
=T(x)/integraldisplay
w:T(w)>0T(w|x)S(y|w)dw
=T(x)/integraldisplay
w:S(w)>0T(w|x)S(y|w)dw .
wherethesecondequalityfollowsbysufficiencyandthethirdbycovariateshiftandoverlapin W.T(x),T(w|
x)andS(y|w)are observable through training samples. That hâˆ—
Tis a minimizer follows from the first-order
27Published in Transactions on Machine Learning Research (09/2024)
0 5000 10000
Extra (x,y) samples0.20.40.60.81.0AUC (CM)SL-T
SL-S
DALUPI
DANN
MDD
Figure 10: Test AUC for CM in T. DALUPI outperforms the other models when no extra (x,y)samples
are provided. Adding examples without bounding box annotations improves the performance of SL-S and
SL-T, eventually causing the latter to surpass DALUPI.
condition of setting the derivative of the risk with respect to hto 0. This strategy yields the well-known
result that
hâˆ—
T= arg min
hET[(h(X)âˆ’Y2)] =ET[Y|X].
By definition and the previous result, we have that
ET[Y|X=x] =/integraldisplay
yyT(x,y)
T(x)dy
=/integraldisplay
y/integraldisplay
w:S(w)>0T(w|x)S(y|w)ydwdy
=/integraldisplay
wT(w|x)ES[Y|x]dw
and we have the result.
D Proof of Proposition 2
Proposition 2. Assume thatGcomprises M-Lipschitz mappings from the privileged information space
W âŠ† RdWtoY. Further, assume that both the ground truth privileged information Wand labelYare
deterministic in XandWrespectively. Let Ïbe the domain density ratio of Wand let Assumptions 1â€“3
(Covariate shift, Overlap and Sufficiency) hold w.r.t. W. Further, let the loss Lbe uniformly bounded by
some constant Band letdanddâ€²be the pseudo-dimensions of GandFrespectively. Assume that there are
nobservations from the source (labeled) domain and mfrom the target (unlabeled) domain. Then, with L
the squared Euclidean distance, for any h=hâ—¦fâˆˆGÃ—F, w.p. at least 1âˆ’Î´,
RT(h)
2â‰¤Ë†RY,Ï
S(g) +M2Ë†RW
T(f)
+ 25/4/radicalbig
d2(Tâˆ¥S)3
8/radicalï£¬igg
dlog2me
d+ log4
Î´
m
+dWBM2ï£«
ï£­/radicalbigg
2dâ€²logen
dâ€²
n+/radicalï£¬igg
logdW
Î´
2nï£¶
ï£¸.
28Published in Transactions on Machine Learning Research (09/2024)
Proof.Decomposing the risk of hâ—¦Ï•, we get
RT(h) =ET[(g(f(X))âˆ’Y)2]
â‰¤2ET[(g(W)âˆ’Y)2+ (g(f(X))âˆ’g(W))2]
â‰¤2ET[(g(W)âˆ’Y)2+M2âˆ¥f(X))âˆ’g(W)âˆ¥2]
â‰¤2ET[(g(W)âˆ’Y)2] + 2M2ET[âˆ¥(f(X)âˆ’W)âˆ¥2]
= 2RY
T(g) + 2M2RW
T(f) = 2RY,Ï
S(g)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(I)+2M2RW
T(f)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(II).
The first inequality follows from the relaxed triangle inequality, the second inequality from the Lipschitz
property and the third equality from Overlap and Covariate shift. We will bound these quantities separately
starting with (I).
We assume that the pseudo-dimension of G,dis bounded. Further, we assume that the second moment
of the density ratios, equal to the RÃ©nyi divergence d2(Tâˆ¥S) = Î£wâˆˆcGT(w)T(w)
S(w)are bounded and that the
density ratios are non-zero for all wâˆˆG. LetD1={wi,yi}m
i=0be a dataset drawn i.i.d from the source
domain. Then by application of Theorem 3 from Cortes et al. (2010) we obtain with probability 1âˆ’Î´over
the choice of D1,
(I) =RY,Ï
S(g)â‰¤Ë†RY,Ï
S(g) + 25/4/radicalbig
d2(Tâˆ¥S)3/8/radicalï£¬igg
dlog2me
d+ log4
Î´
m
Now for (II)we treat each component of wâˆˆWas a regression problem independent from all the others.
So we can therefore write the risk as the sum of the individual component risks
RW
T(f) = Î£dW
i=1RW
T,i(f)
Let the pseudo-dimension of Fbe denoted d,D2={xi,wi}n
i=0be a dataset drawn i.i.d from the target
domain. Then, using theorem 11.8 from Mohri et al. (2018) we have that for any Î´>0, with probability at
least 1âˆ’Î´over the choice of D2, the following inequality holds for all hypotheses fâˆˆFfor each component
risk
RW
T,i(f)â‰¤Ë†RW
T,i(f) +Bï£«
ï£­/radicalbigg
2dâ€²logen
dâ€²
n+/radicalï£¬igg
log1
Î´
2nï£¶
ï£¸
We then simply make all the bounds hold simultaneously by applying the union bound and having it so that
each bound must hold with probability 1âˆ’Î´
dWwhich results in
RW
T(f) = Î£dW
i=1RW
T,i(f)â‰¤Î£dW
i=1Ë†RW
T,i(f) + Î£dW
i=1Bï£«
ï£­/radicalbigg
2dâ€²logen
dâ€²
n+/radicalï£¬igg
logdW
Î´
2nï£¶
ï£¸
=Ë†RW
T(f) +dWBï£«
ï£­/radicalbigg
2dâ€²logen
dâ€²
n+/radicalï£¬igg
logdW
Î´
2nï£¶
ï£¸
Combination of these two results then yield the proposition statement.
Consistency follows as Yis a deterministic function of WandWis a deterministic fundtion of Xand both
HandFare well-specified. Thus both empirical risks and sample complexity terms will converge to 0 in
the limit of infinite samples.
The parts of the bound shown above can be described as falling into three main categories: Empirical risk(s),
domain shift and sample complexity components. A central term that figures both in the weighted empirical
29Published in Transactions on Machine Learning Research (09/2024)
risk and the RÃ©nyi divergence is the density ratioT(w)
S(w). Therefore, the size of the bound is governed at
least in part based on the proximity in W-space the source and target domains are. This is similar to other
importance weighting bounds, however, since the experiment designer may choose the form of PI this can
be more well-behaved than the density ratio in the input space.
E Proof Sketch for PAC-Bayes Bound
We will here detail a proof sketch for a PAC-Bayes version of the bound we propose in the main text. For
the purposes of this bound we will consider the quantity Ehâˆ¼ÏˆRT(h), whereÏˆis a posterior distribution
over classifiers hâˆ¼Ïˆ. As we are basing the bound on the two-step methodology where we train two different
classifiers on separate datasets we assume that we can obtain the posteriors over the component functions
separately and independently i.e. h=fâ—¦gâˆ¼Ïˆ=ÏˆfÃ—Ïˆg, wherefâˆ¼Ïˆfandgâˆ¼Ïˆg. Let the assumptions
from proposition 2 hold here. Similar to the previous section we decompose the risk into two parts
Ehâˆ¼ÏˆRT(h) =Ehâˆ¼ÏˆET[(g(f(X))âˆ’Y)2]
=Ehâˆ¼Ïˆ[2RY
T(g) + 2M2RW
T(f)] = 2Ehâˆ¼ÏˆRY
T(g)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(I)+2M2Ehâˆ¼ÏˆRW
T(f)/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
(II).
We note that since we now have expectations over the composite function hon expressions which depend
on only one of the components we can, for example, write the following:
Ehâˆ¼ÏˆRY
T(g) =Egâˆ¼ÏˆgRY
T(g)
This holds as we assume that f and g are not dependent on each other. Therefore, we can just marginalize
out the part which is not in use. From this point we can use some of the available bounds from the literature
to estimate the resulting part e.g. Corollary 1 from Breitholtz & Johansson (2022). Application of this result
yields the following bound on the first term
E
gâˆ¼ÏˆgRY
T(g)â‰¤1
Î³E
gâˆ¼ÏˆgË†RY,Ï
S(g) +Î²âˆKL(Ïˆgâˆ¥Ï€g) + ln(1
Î´)
2Î³(1âˆ’Î³)m.
Thereafter we can use another bound from the literature to estimate the second term, e.g. Theorem 6 from
Germain et al. (2020). Using this we obtain the following:
E
fâˆ¼ÏˆfRW
T(f)â‰¤Î±
1âˆ’eâˆ’Î±/parenleftbigg
E
fâˆ¼ÏˆfË†RW
T(f) +KL(Ïˆfâˆ¥Ï€f) + ln(1
Î´)
nÎ±/parenrightbigg
.
Then a bound can be constructed by combining these two results using a union bound argument.
Ehâˆ¼ÏˆRT(h)â‰¤2
Î³E
gâˆ¼ÏˆgË†RY,Ï
S(g) +Î²âˆKL(Ïˆgâˆ¥Ï€g) + ln(2
Î´)
2Î³(1âˆ’Î³)m
+2M2Î±
1âˆ’eâˆ’Î±/parenleftbigg
E
fâˆ¼ÏˆfË†RW
T(f) +KL(Ïˆfâˆ¥Ï€f) + ln(2
Î´)
nÎ±/parenrightbigg
F A Bound on the Target Risk Without Suffiency
The sufficiency assumption is used to replace T(y|x)withT(y|w)in the proof of Proposition 1. If
sufficiency is violated but it is plausible that the degree of insufficiency is comparable across domains, we
can still obtain a bound on the target risk which may be estimated from observed quantities. One way to
formalize such an assumption is that there is some Î³â‰¥1, for which
sup
xâˆˆT(x|w)T(y|w,x)/T(y|w)â‰¤Î³sup
xâˆˆS(x|w)S(y|w,x)/S(y|w) (5)
30Published in Transactions on Machine Learning Research (09/2024)
This may be viewed as a relaxation of suffiency. If Assumption 3 holds, both left-hand and right-hand sides
of the inequality are 1. Under Equation 5, with âˆ†Î³(w,y)equal to the right-hand side the inequality,
RT(h)â‰¤/integraldisplay
xT(x)/integraldisplay
wT(w|x)/integraldisplay
yâˆ†Î³(w,y)S(y|w)L(h(x),y)dydwdx .
However, the added assumption is not verifiable statistically.
31