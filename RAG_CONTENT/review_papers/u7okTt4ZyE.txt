Taming Diffusion Prior for Image Super-Resolution
with Domain Shift SDEs
Qinpeng Cui‚àó12, Yixuan Liu1, Xinyi Zhang2, Qiqi Bao2, Qingmin Liao2
Li Wang1, Tian Lu1, Zicheng liu1, Zhongdao Wang‚Ä†2, Emad Barsoum1
1Advanced Micro Devices Inc.2Tsinghua University
{qinpeng.cui, yixuan.liu, li.wang, lu.tian, zicheng.liu, ebarsoum}@amd.com;
{cqp22, xinyi-zh22, bqq19, liaoqm, wcd17}@tsinghua.edu.cn
Abstract
Diffusion-based image super-resolution (SR) models have attracted substantial
interest due to their powerful image restoration capabilities. However, prevailing
diffusion models often struggle to strike an optimal balance between efficiency
and performance. Typically, they either neglect to exploit the potential of existing
extensive pretrained models, limiting their generative capacity, or they necessitate
a dozens of forward passes starting from random noises, compromising inference
efficiency. In this paper, we present DoSSR, a Domain Shift diffusion-based SR
model that capitalizes on the generative powers of pretrained diffusion models
while significantly enhancing efficiency by initiating the diffusion process with
low-resolution (LR) images. At the core of our approach is a domain shift equa-
tion that integrates seamlessly with existing diffusion models. This integration
not only improves the use of diffusion prior but also boosts inference efficiency.
Moreover, we advance our method by transitioning the discrete shift process to
a continuous formulation, termed as DoS-SDEs. This advancement leads to the
fast and customized solvers that further enhance sampling efficiency. Empirical
results demonstrate that our proposed method achieves state-of-the-art performance
on synthetic and real-world datasets, while notably requiring only 5 sampling
steps . Compared to previous diffusion prior based methods, our approach achieves
a remarkable speedup of 5-7 times, demonstrating its superior efficiency. Code:
https://github.com/AMD-AIG-AIMA/DoSSR
1 Introduction
Image super-resolution (SR) is a classical task in computer vision that involves enhancing a low-
resolution (LR) image to create a perceptually convincing high-resolution (HR) image [ 28]. Tradi-
tionally, this field has operated under the assumption of simple image degradations, such as bicubic
down-sampling, which has led to the development of numerous effective SR models [ 6,25,57,12].
However, these models often fall short when confronted with real-world degradations, which are
typically more complex than those assumed in academic settings. Recently, diffusion models has
emerged as a pivotal research direction in real-world SR, using their robust generative capabilities to
enhance perceptual quality. This shift highlights their superior performance in practical applications.
Currently, diffusion-based SR strategies can be broadly categorized into two approaches. The
first approach leverages large-scale pretrained diffusion models ( e.g., Stable Diffusion [ 41]) as
generative prior, using LR images (or preprocessed LR images) as conditional inputs to generate
HR images [ 45,27,51]. Despite achieving remarkable results, it exhibits low inference efficiency,
‚àóWork done during an internship at AMD.
‚Ä†Corresponding author.
38th Conference on Neural Information Processing Systems (NeurIPS 2024).0 2 4 6 8
Latency(s)0.30.40.50.6MANIQA
SeeSR-50
DiffBIR-50ResShift-15DoSSR(ours)-5
DoSSR(ours)-1
Real-ESRGANLDL
DASR
17.5 20.0StableSR-200
Diameter
[#Params:1000M]Latency vs. MANIQA vs. Parameters      (a)
LR Image
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 SeeSR-50
 DoSSR(ours)-5 (b)
Figure 1: (a) Latency, MANIQA, and complexity of model comparison on RealLR200 [51] dataset in
x4 SR task (for 128 √ó128 LR images). (b) Qualitative comparisons of DoSSR and recent state-of-
the-art methods on one typical real-world example. For diffusion-based methods, the suffix "-N"
appended to the method name indicates the number of inference steps. Zoom in for a better view.
as the inference starting point is a random Gaussian noise instead of the LR image. Although
techniques such as sampler optimization [ 33,38,30,9,22] or model distillation [ 32,37] have been
proposed to mitigate this issue, they inevitably compromise SR performance. The second approach
involves redefining the diffusion process and retraining a model from scratch for the SR task [ 53,36].
Consequently, the generative prior from pretrained diffusion models is not leveraged. ResShift [ 53],
as a typical representative, revises the forward process of DDPM [ 16] to better accommodate the SR
task. By starting from LR rather than Gaussian noise, it improves inference efficiency. However, its
modification of the diffusion pattern, which deviates significantly from existing noise schedules in
diffusion models, hinders its integration with large-scale pretrained diffusion models for leveraging
their generative prior. The diffusion generative prior has been proven to be highly beneficial for SR
tasks [ 45], enabling models to transcend the limitations of knowledge learned solely from the training
dataset, thereby equipping them to handle various complex real-world scenarios. Thus, crafting a
diffusion process tailored for the SR task that also remains compatible with established diffusion
prior presents a significant challenge.
To tackle this challenge, we propose DoSSR, a Domain Shift diffusion-based SR model. We
initially view the SR task as a gradual shift from the LR domain to the HR domain, describing this
transition with a linear equation, which is called domain shift equation . Then, we combine this
domain shift equation with existing diffusion equations, facilitating the fine-tuning of large-scale
pretrained diffusion models to harness diffusion prior effectively. Moreover, by carefully designing a
shifting sequence, inference can begin from LR images rather than Gaussian noises, thereby boosting
inference efficiency. To further enhance efficiency, we employ sampler optimization techniques,
extensively explored in image generation [38, 30, 9], but not previously tailored for diffusion-based
SR tasks. Specifically, we expand the customized diffusion equation from discrete to continuous,
enabling its formulation as stochastic differential equations (SDEs). We subsequently present the
corresponding backward-time SDE as Domain Shift SDE in the reverse process and provide an
exact formulation of its solution. Based on our formulation, we customize fast solvers for sampling.
Experimental results demonstrate that our method achieves superior or comparable performance
compared to current state-of-the-art methods on both synthetic and real-world datasets, with only 5
sampling steps , striking an optimal balance between efficiency and effectiveness. Furthermore, our
approach can match the performance of previous methods even with just a single step .
In summary, the main contributions of our work are as follows:
‚Ä¢We propose a novel diffusion equation, which models SR from the perspective of domain
shift, enabling inference to start from LR images and leveraging diffusion prior to ensure both
efficiency and performance.
‚Ä¢We further propose the SDEs to describe the process of domain shift and provide an exact
solution for the corresponding reverse-time SDEs. Based on the solution, we design customized
fast samplers, resulting in even higher efficiency, thereby achieving the state-of-the-art efficiency-
performance trade-off.
22 Related work
Neural Network-based Super-Resolution. Neural network-based methods have emerged as the
dominant approach in image SR tasks. The introduction of convolutional neural networks (CNNs)
and Transformer architecture, with the primary focus on network architecture design [ 12,10,25,26,
58,21,56,57], have demonstrated superior performance over traditional methods. This improvement
is facilitated by the introduction of residual blocks, dense blocks and attention mechanisms. These
methods primarily aim for better image fidelity measures such as PSNR and SSIM [ 49] indices,
therefore, they often yield over-smoothed outcomes. To enhance visual perception, Generative
adversarial network (GAN)-based SR methods have been developed. By incorporating adversarial
loss during training, many SR models [ 13,23,17] can generate perceptually realistic details, thereby
enhancing visual quality. To further study SR problems in real-world scenarios, some studies [ 54,46,
24] have proposed simulating the intricate real-world image degradation process through random
combinations of fundamental degradation operations. Despite the remarkable advancements, GAN-
based SR methods can introduce undesirable visual artifacts.
Diffusion-based Super-Resolution. Recently, diffusion-based SR methods [ 35,36,8,7,45,27]
have demonstrated excellent performance, especially in terms of perceptual quality. These methods
can generate more authentic details while avoiding unpleasant visual artifacts like GAN-based
methods. Current diffusion models for super-resolution can be broadly categorized into two main
approaches. The first approach involves leveraging large-scale pretrained diffusion models, such
as Stable Diffusion [ 41], as prior, and then using LR images as conditional inputs to generate HR
images. StableSR [ 45] and DiffBIR [ 27] represent representative works that leverage diffusion
prior, leading to enhanced fidelity when conditioning on LR or preprocessed LR. SeeSR [ 51] and
CoSeR [ 42] demonstrate that extracting semantic text information from LR images as additional
control conditions for the T2I model helps improve performance. The second approach involves
redefining the diffusion process and retraining a model from scratch for SR [ 18,36]. To address the
slow inference speed issue of diffusion-based SR methods, ResShift [ 53] constructs a Markov chain
that transitions between HR and LR images by shifting residuals between them, enabling accelerated
sampling. SinSR [ 48] proposed a method of distilling ResShift to achieve comparable performance in
a single step. Despite the remarkable advancements achieved by ResShift and SinSR, they necessitate
retraining from scratch for SR tasks (or further distillation) and are unable to leverage diffusion prior.
Therefore, improving the inference efficiency while leveraging the potential of large-scale pretrained
diffusion models to assist SR requires thorough investigation, which is the goal of this work.
3 Methodology
We aim to optimize the balance between efficiency and performance in diffusion-based super-
resolution (SR) models. Our approach is grounded in two key principles: First, initiating inference
from LR images rather than noise; Second, effectively harnessing pretrained diffusion prior. In
Section 3.1, we introduce a novel diffusion equation designed to fulfill both criteria simultaneously.
Subsequently, in Section 3.2, we extend this diffusion process to continuous scenarios, formulating it
through Stochastic Differential Equations (SDEs). Building on these SDEs, we develop an efficient
solver detailed in Section 3.3, further enhancing inference efficiency.
3.1 Diffusion Process with Domain Shift
Our goal is to characterize the shift from the source domain to the target domain as a diffusion
process. In the task of SR, the distribution of LR images pdata(ÀÜx0)represents the source domain,
while the distribution of HR images pdata(x0)represents the target domain. Firstly, we conceptualize
domain shift as a gradual transition from the source domain to the target domain through a linear
drift coefficient Œ∑t, the domain shift equation is formulated as
D(ÀÜx0,x0) =Œ∑tÀÜx0+ (1‚àíŒ∑t)x0,0‚â§Œ∑t‚â§1, t= 1,2,¬∑¬∑¬∑, T, (1)
where shifting sequence {Œ∑t}T
t=1monotonically non-decreases with timestep t. In order to enable
linear combination, we can interpolate ÀÜx0to match the same dimensions as x0if necessary. Secondly,
we combine this domain shift with the diffusion equation. To integrate with pretrained diffusion
models, we adopt the most commonly used diffusion scheme from DDPM [ 16] and express the
3HR: ùë•0 ùë•ùë°=ùõºùë°ùíü‡∑úùë•0,ùë•0+ùúéùë°ùúÄ
LR: ‡∑úùë•0
ùíü‡∑úùë•0,ùë•0=‡∑úùë•0ùë•ùë°=ùõºùë°ùíü‡∑úùë•0,ùë•0+ùúéùë°ùúÄ ùë•ùëá
ùë•0 ùë•ùëá ùë•ùë° ùë•ùë°‚àí1 ùë•ùë° ùë•ùë°‚àí1
DoSSR_sampler (ùë•ùë°,‡∑úùë•0,ùúÄ)ùë°=ùëá(a)DoSSR forward
ùë°=ùë°1
ùë•ùë°1
ùë•ùë°1ùë°=0
DoSSR_sampler (ùë•ùë°,ùúÄ)‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶Optional starting point (b)DoSSR backward
ùë°=ùë°1 ùë°=ùëáStarting point
ùë°=0ùíü‡∑úùë•0,ùë•0=ùúÇùë°‡∑úùë•0+1‚àíùúÇùë°ùë•0
LR domain
HR domainLR domain
HR domain(c) Score field of SD ( left) and DoSSR  (right )
(d) Shifting sequence ùúÇùë°
1.0
0ùúÇùë°
ùë°1 ùëáùúÇùë°=1 ùúÇùë°=1‚àícos(ùúãùë°/ùë°1)
2Learned 
diffusion 
path for SR Figure 2: Illustration of the proposed diffusion process with domain shift. (a) In the forward process,
we merge the gradual shift from HR to LR domain with standard diffusion process. (b) In the
reverse process, we initiate inference from LR domain ( t=t1) and use our fast sampler to generate
SR images. (c) Comparison of the estimated score between SD and DoSSR. DoSSR inherits the
capability of SD in ambient space and enhances learning a pathway from LR to HR domain. (d) The
design of the shifting sequence which enables us to initiate inference from t1.
formula of marginal distribution at any timestep tas follows:
q(xt|x0,ÀÜx0) =N(xt;Œ±tD(ÀÜx0,x0), œÉ2
tI), t= 1,2,¬∑¬∑¬∑, T, (2)
where Œ±t, œÉt‚â•0andŒ±2
t+œÉ2
t= 1,Iis the identity matrix. Based on our proposed marginal
distribution Eq. (2), we demonstrate the transition distribution as follows:
q(xt|xt‚àí1,ÀÜx0) =N(xt;Œ±t
Œ±t‚àí1xt‚àí1+Œ±t(Œ∑t‚àíŒ∑t‚àí1)e0,1‚àíŒ±2
t
Œ±2
t‚àí1I), t= 1,2,¬∑¬∑¬∑, T, (3)
where e0=ÀÜx0‚àíx0is the residual between the source and target domain.
Relation to DDPM [16]. The formulation of Eq. (2) is based on the DDPM [16] forward process,
with a crucial difference lying in its mean Œ±tD(ÀÜx0,x0)instead of Œ±tx0. This integration encapsulates
the domain shift within the variation of its mean, while the diffusion process with added noise
maintains consistency with it, thereby smoothing this transformation. Meanwhile, it enhances the
diffusion model to learn the pathway from the source domain to the target domain. For an intuitive
understanding, we plot and compare the score function ‚àáxlogqt(xt)learned by a vanilla Stable
Diffusion (SD) model and DoSSR in Fig. 2(c). While SD learns reasonable score field in the whole
space, DoSSR inherits its capability in the ambient space and further learns more accurate scores
along the path between LR and HR domains. Therefore sampling efficiency is improved. Furthermore,
the choice of Œ±tandœÉt, referred to as the noise schedule , follows the existing pretrained diffusion
model, allowing us to fine-tune it rather than training it from scratch.
Relation to ResShift [ 53].The form of equation Eq. (3)suggests that this shift essentially constructs
a Markov chain in a manner similar to that described in ResShift [ 53]. However, the equation
constructed by ResShift adopts an entirely different noise schedule compared to the pretrained
diffusion model. This makes it difficult to apply pretrained diffusion models for subsequent fine-
tuning, necessitating training from scratch instead. Therefore, it is unable to utilize the diffusion prior,
thereby limiting the model‚Äôs performance. See Appendix A.7 for detailed theoretical differences from
ResShift. In Appendix C.1, we present experimental results on ImageNet [ 11] showing that DoSSR
uses two orders of magnitude less training data than ResShift while achieving superior performance.
Shifting Sequence. The parameter Œ∑tplays a crucial role in guiding the diffusion process, serving
as a bridge between the source and target domains. Specifically, Œ∑t= 1represents standard diffusion
4forward perturbations in the source domain, whereas Œ∑t= 0corresponds to the target domain. The
transition between these domains occurs for 0< Œ∑t<1, indicating a domain shift. To effectively
utilize the diffusion prior, we adopt the noise schedule from DDPM. This adoption dictates that as
tapproaches the final time step T, the scale parameter Œ±Ttends towards zero, and the distribution
q(xT)approximates a standard Gaussian, N(0,I). To retain prior information from the source
domain while shortening the diffusion path, we set Œ∑t= 1fort‚àà[t1, T], as defined by:
Œ∑t=1‚àícos(œÄt
t1)
2ift‚àà[0, t1], Œ∑ t= 1ift‚àà[t1, T]. (4)
The advantage of such a setting lies in the fact that during the reverse process, the values of xtfor
t‚àà[t1, T]are known and can be obtained through the forward process Eq. (2). Consequently, the
inference does not need to start from time step T, but can commence at t1, thereby preserving the
prior information of the source domain while enhancing the efficiency of inference. An overview of
the impact of Œ∑tis presented in Fig. 2.
3.2 Diffusion DoS-SDEs
To improve the efficiency of inference in diffusion models, many prior works [ 30,31,9] have
designed efficient samplers by solving the diffusion SDEs. Therefore, in this section, we extend the
aforementioned discrete shift process to an SDE for description, in preparation for designing efficient
samplers in the following section. Specifically, inspired by the work of [ 40], we generalize this finite
shift process further to an infinite number of noise scales, such that the data distribution of domain
shift evolves according to an SDE as noise intensifies. Then we provide the corresponding reverse-
time SDE and elucidate the training of diffusion models from the perspective of score matching [ 39].
Next, we will elaborate extensively on how to describe diffusion models using SDEs.
Forward Process. Expanding the time variable tin Eq. (2)to a continuous range, t‚àà[0, T], we
have that Œ±t, œÉt, Œ∑tare differentiable functions of twith bounded derivatives. Furthermore, Song
et al. [40] have demonstrated that the diffusion process can be modeled as the solution to an It√¥ SDE
and we formulate the SDE as follows:
dxt= [f(t)xt+h(t)ÀÜx0]dt+g(t)dwt,x0‚àºq0(x0), (5)
where wtis the standard Wiener process, and q0(x0)is the target domain data distribution. It has
the same marginal distribution q(xt|x0,ÀÜx0)as in Eq. (2)for any t‚àà[0, T]with the coefficients
satisfying (proof in Appendix A.2)
f(t) =dlogŒ±t(1‚àíŒ∑t)
dt, h(t) =Œ±t
1‚àíŒ∑tdŒ∑t
dt, g(t) =r
dœÉ2
t
dt‚àí2dlogŒ±t(1‚àíŒ∑t)
dtœÉ2
t.(6)
Reverse Process. The reverse of a diffusion process is also a diffusion process [ 2] which can
similarly be described by a reverse-time SDE (proof in Appendix A.3):
dxt=h
f(t)xt+h(t)ÀÜx0‚àíg2(t)‚àáxlogqt(xt)i
dt+g(t)dwt (7)
where wtis also a standard Wiener process when time flows backwards. In this paper, we refer to
this SDE as Domain ShiftSDE (DoS-SDE).
Score Matching. The only unknown term in Eq. (7)is the score function ‚àáxlogqt(xt)that can be
estimated by training a score-based model on samples with score matching [ 39]. In practice, we use
a neural network œµŒ∏(xt,ÀÜx0, t)conditioned on ÀÜx0, parameterized by Œ∏, to estimate the scaled score
function (alternatively referred to as noise), following [ 16,40]. The parameter Œ∏is optimized by
minimizing the following objectives:
Œ∏‚àó= arg min
Œ∏Etn
w(t)Eqt(xt)
||œµŒ∏(xt,ÀÜx0, t) +œÉt‚àáxlogqt(xt)||o
= arg min
Œ∏Etn
w(t)Eq0(x0)Eq(œµ)
||œµŒ∏(xt,ÀÜx0, t)‚àíœµ||o
,(8)
where w(t)is a weighting function, xt=Œ±t(Œ∑tÀÜx0+ (1‚àíŒ∑t)x0) +œÉtœµ, andœµ‚àº N(0,I).
Thus, we have completed the expression of the diffusion model using SDEs. Sampling from diffusion
models can alternatively be seen as solving the corresponding diffusion DoS-SDEs.
53.3 Solvers for Diffusion DoS-SDEs
In this section, we present an exact formulation of the solution of diffusion DoS-SDEs and design
efficient samplers for fast sampling. To facilitate the solution of equation Eq. (7), we utilize the data
prediction model xŒ∏(xt,ÀÜx0, t), which directly estimates the original target data x0from the noisy
samples. The relationship between score function and data prediction model is as follows (proof in
Appendix A.4):
‚àáxlogqt(xt) =‚àíxt‚àí(Œ±t(1‚àíŒ∑t)xŒ∏(xt,ÀÜx0, t) +Œ±tŒ∑tÀÜx0)
œÉ2
t. (9)
In practice, we employ our trained noise prediction model œµŒ∏(xt,ÀÜx0, t)for data prediction
xŒ∏(xt,ÀÜx0, t)as described in Appendix A.4. By substituting Eq. (6)and Eq. (9)into Eq. (7)
and introducing the substitutions Œªt=œÉt
Œ±t(1‚àíŒ∑t)andyt=xt
Œ±t(1‚àíŒ∑t)along with the notation
dwŒªt:=q
dŒªt
dtdwt,xŒª:=xt(Œª),wŒª:=wŒªt, we rewrite Eq. (7) w.r.t Œªas
dyŒª=2
ŒªyŒªdŒª+h1
(1‚àíŒ∑Œª)2dŒ∑Œª‚àíŒ∑Œª
1‚àíŒ∑Œª2
ŒªdŒªi
ÀÜx0‚àí2
ŒªxŒ∏(xŒª,ÀÜx0, Œª)dŒª+‚àö
2ŒªdwŒª (10)
We propose the exact solution for Eq. (10) using the variation-of-constants formula, following [ 31,9].
Proposition 3.1 (Exact solution of diffusion DoS-SDEs) .Given an initial value xsat time s >0,
the solution xtfor the diffusion DoS-SDEs defined in Eq. (7)at time t‚àà[0, s]is as follows:
xt=Œ±t(1‚àíŒ∑t)
Œ±s(1‚àíŒ∑s)Œª2
t
Œª2sxs+Œ±t(1‚àíŒ∑t)(Œ∑t
1‚àíŒ∑t‚àíŒ∑s
1‚àíŒ∑sŒª2
t
Œª2s)ÀÜx0
‚àíŒ±t(1‚àíŒ∑t)ZŒªt
Œªs2Œª2
t
Œª3xŒ∏(xŒª,ÀÜx0, Œª)dŒª+Œ±t(1‚àíŒ∑t)s
Œª2
t‚àíŒª4
t
Œª2szs,(11)
where Œªt=œÉt
Œ±t(1‚àíŒ∑t)andzs‚àº N(0,I).
The detailed derivation of this proposition is provided in Appendix A.5. Notably, the nonlinear
term in Eq. (11) involves the integration of a non-analytical neural network xŒ∏(xŒª,ÀÜx0, Œª), which
can be challenging to compute. For practical applicability, we employ It√¥-Taylor expansion to
approximate the integral of xŒ∏fromŒªstoŒªtto compute Àúxt, thereby approximating xt. Additionally,
we approximate the derivatives of xŒ∏using the forward differential method . These approximations
allow us to derive SDE solvers of any order for diffusion DoS-SDEs. For the sake of brevity, we
employ a first-order solver for demonstration. In this case, Eq. (11) becomes
Àúxt=Œ±t(1‚àíŒ∑t)
Œ±s(1‚àíŒ∑s)Œª2
t
Œª2sxs+Œ±t(1‚àíŒ∑t)(Œ∑t
1‚àíŒ∑t‚àíŒ∑s
1‚àíŒ∑sŒª2
t
Œª2s)ÀÜx0
| {z }
Domain Shift Guidance(DoSG)
+Œ±t(1‚àíŒ∑t)(1‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s) +Œ±t(1‚àíŒ∑t)s
Œª2
t‚àíŒª4
t
Œª2szs.(12)
The detailed derivation, as well as high-order solvers, are provided in Appendix A.6, and detailed
algorithms are proposed in Appendix B. Typically, higher-order solvers converge even faster because
of more accurate estimation of the the nonlinear integral term. The solvers provided for sampling
allow us to iteratively generate HR images using a trained diffusion model. It is worth noting that
Eq.(12) comprises four terms, including the additional linear term ÀÜx0, as compared to the ancestral
sampling algorithm [ 16]. We refer to this additional term as the domain shiftguidance (DoSG) which
leverages prior information from the source domain and enhances the efficiency of inference.
4 Experiments
4.1 Experimental setup
For training, we train our DoSSR using a variety of datasets including DIV2K [ 1], DIV8K [ 15],
Flickr2K [ 43], and OST [ 47]. To synthesize LR and HR training pairs, we adopt the degradation
6Table 1: Quantitative comparison with state-of-the-art methods on both synthetic and real-world
benchmarks, as well as comparison of latency and number of model parameters. NFE represents the
number of function evaluations in the inference of diffusion models. The best and second best results
of each metric are highlighted in redand blue , respectively.
Datasets Metrics BSRGAN [54]Real- [46]
ESRGANLDL [23] DASR [24] StableSR [45] ResShift [53] DiffBIR [27] SeeSR [51] DoSSR
DIV2k-ValPSNR ‚Üë 24.58 24.29 23.83 24.47 23.36 24.65 23.67 23.68 23.98
SSIM‚Üë 0.6241 0.6338 0.6312 0.6277 0.5654 0.6148 0.5592 0.5987 0.6073
LPIPS ‚Üì 0.3351 0.3112 0.3256 0.3543 0.3114 0.3349 0.3516 0.3195 0.3371
CLIPIQA ‚Üë 0.5246 0.5276 0.5179 0.5036 0.6771 0.6065 0.6693 0.6935 0.7014
MUSIQ ‚Üë 61.19 61.06 60.04 55.19 65.92 61.07 65.78 68.68 66.54
MANIQA ‚Üë 0.3547 0.3795 0.3736 0.3165 0.4193 0.4107 0.4568 0.5041 0.5294
TOPIQ ‚Üë 0.5456 0.5294 0.5142 0.4530 0.5974 0.5383 0.6142 0.6854 0.6766
RealSRPSNR ‚Üë 26.38 25.69 25.28 27.02 24.65 26.26 24.81 25.14 24.18
SSIM‚Üë 0.7655 0.7615 0.7565 0.7714 0.7060 0.7404 0.6571 0.7194 0.6839
LPIPS ‚Üì 0.2656 0.2709 0.2750 0.3134 0.3002 0.3469 0.3607 0.3007 0.3374
CLIPIQA ‚Üë 0.5114 0.4485 0.4556 0.3198 0.6234 0.5473 0.6448 0.6699 0.7025
MUSIQ ‚Üë 63.28 60.37 60.93 41.21 65.88 58.47 64.94 69.82 69.42
MANIQA ‚Üë 0.3764 0.3733 0.3792 0.2461 0.4260 0.3836 0.4539 0.5406 0.5781
TOPIQ ‚Üë 0.5502 0.5147 0.5124 0.3207 0.5743 0.4883 0.5722 0.6887 0.6985
DRealSRPSNR ‚Üë 28.74 28.62 28.17 29.72 28.03 28.42 26.67 27.89 26.82
SSIM‚Üë 0.8033 0.8050 0.8126 0.8264 0.7523 0.7629 0.6548 0.7565 0.7298
LPIPS ‚Üì 0.2858 0.2818 0.2792 0.3099 0.3284 0.4036 0.4517 0.3273 0.3689
CLIPIQA ‚Üë 0.5091 0.4507 0.4473 0.3813 0.6357 0.5286 0.6391 0.6708 0.6776
MUSIQ ‚Üë 57.16 54.28 53.95 42.41 58.51 49.73 60.91 65.09 64.40
MANIQA ‚Üë 0.3424 0.3436 0.3444 0.2845 0.3867 0.3322 0.4486 0.5115 0.5214
TOPIQ ‚Üë 0.5058 0.4621 0.4518 0.3482 0.5320 0.4380 0.5819 0.6574 0.6618
Real200CLIPIQA ‚Üë 0.5910 0.5554 0.5508 0.5157 0.7272 0.6759 0.7170 0.7167 0.7437
MUSIQ ‚Üë 67.65 66.12 65.80 61.26 70.63 66.98 68.92 72.14 71.62
MANIQA ‚Üë 0.3882 0.3861 0.3921 0.3196 0.4838 0.4713 0.4869 0.5588 0.5794
TOPIQ ‚Üë 0.5966 0.5530 0.5478 0.4793 0.6517 0.6124 0.6235 0.7142 0.7176
NFE‚Üì - - - - 200 15 50 50 5
# Parameters 16.70M 16.70M 16.70M 8.07M 1409.1M 173.9M 1716.7M 2283.7M 1716.6M
Latency/Image ‚Üì 0.06s 0.08s 0.08s 0.04s 18.90s 1.12s 5.85s 7.24s 1.03s
pipeline from RealESRGAN [ 46]. For the network architecture, we employ the LAControlNet [ 27]
with SD 2.1-base3as the pretrained T2I model. In cases where LR images are severely degraded,
potentially leading to the diffusion model mistaking degradation for semantic content, we implement
RealESRNet [ 46] as a preprocessing step. This ensures our source domain consists of preprocessed
LR images, thereby refining the input quality for better model training and performance. The model
is fine-tuned for 50k iterations using the Adam optimizer [ 20], with a batch size of 32 and a learning
rate set to 5√ó10‚àí5, on512√ó512resolution images.
For testing, we evaluate our method on both synthetic and real-world datasets, employing the same
configuration as StableSR4. For synthetic data, we randomly crop 3K patches with a resolution of
512√ó512from the DIV2K validation set [ 1], and degrade them following the degradation pipeline of
RealESRGAN [ 46]. For real-world datasets, we generate LR images with a resolution of 128√ó128
by center-cropping on RealSR [4], DRealSR [50] and RealLR200 [51].
4.2 Comparisons with State-of-the-Arts
We compare DoSSR with the state-of-the-art real-world SR methods, including BSRGAN [ 54], Real-
ESRGAN [ 46], LDL [ 23], DASR [ 24], StableSR [ 45], ResShift [ 53], DiffBIR [ 27], and SeeSR [ 51].
We use the publicly available codes and pretrained models to facilitate fair comparisons.
Quantitative Comparison. We show the quantitative comparison on the four synthetic and real-
world datasets in Table 1. To comprehensively evaluate the performance of various methods, we
utilize the following metrics5for quantitative comparison: reference-based metrics PSNR, SSIM [ 49],
LPIPS [ 55], and non-reference metrics CLIPIQA [ 44], MUSIQ [ 19], MANIQA [ 52], TOPIQ [ 5].
3https://huggingface.co/stabilityai/stable-diffusion-2-1-base
4https://huggingface.co/datasets/Iceclear/StableSR-TestSets
5We use the repository available at https://github.com/chaofengc/IQA-PyTorch
7LR Image
Zoomed LR
 BSRGAN
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 DiffBIR-50
 SeeSR-50
 DoSSR(ours)-5
LR Image
Zoomed LR
 BSRGAN
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 DiffBIR-50
 SeeSR-50
 DoSSR(ours)-5Figure 3: Qualitative comparisons of different steps of our DoSSR and other diffusion-based SR
methods. The "-N" suffix denotes inference steps. Please zoom in for a better view.
Notably, DoSSR consistently achieves the highest scores in CLIPIQA, MANIQA, and TOPIQ, with
the exception of being second in TOPIQ on DIV2K, and attains the second highest score in MUSIQ
across all four datasets. At the same time, we also note that diffusion-based methods generally
achieve poorer performance in reference metrics compared to GAN-based methods due to their ability
to generate more realistic details at the expense of fidelity. Additionally, our DoSSR manages to
achieve improved no-reference metric performance compared to the data presented in Table 1 as NFE
increases slightly, a detail further elaborated on in Section 4.3.
Qualitative Comparison. Figs. 1(b), 3 present visual comparisons on real-world images. By
leveraging learning of domain shift and introducing DoSG, our DoSSR efficiently generates high-
quality texture details consistent with contents of the LR image. In the example of Fig. 1(b),
GAN-based methods fail to faithfully reconstruct the grid texture of clothing, leading to notable
degradation. StableSR and ResShift produce specific erroneous textures. Both SeeSR and ours
successfully restore correct textures, while our results display clearer textures. Similarly, in the
first example of Fig. 3, our DoSSR generates a more perceptually convincing Spider-Man face as
well as textures, while in the second example, it produces more realistic and high-quality details of
ground-laid bricks compared to other methods. More visual examples are provided in Fig. 7.
Efficiency Comparison. The comparative analysis of model parameters and latency for competing
SR models is shown in Fig. 1(a) and Table 1. The latency is calculated on the √ó4 SR task for 128 √ó128
LR images with V100 GPU. StableSR, DiffBIR, SeeSR, and our DoSSR utilize the pretrained SD
model, resulting in a similar parameter count, with SeeSR incorporating a prompt extractor to enhance
SR results, making it the largest among these methods. ResShift, utilizing the network structure
from LDM [ 35], is trained from scratch and has significantly fewer parameters. It employs a 15-
step process to achieve faster inference speeds. Among the pretrained SD-based methods, DoSSR
demonstrates superior performance efficiency, requiring only 5 function evaluations to achieve
speeds 5-7 times faster than previous SD-based models such as SeeSR. Additionally, DoSSR not
only demonstrates faster or comparable latency to ResShift but also achieves significantly better
super-resolution performance.
82 3 4 5 6 7
Step0.600.650.700.750.80SSIM
0.7475
0.7369
0.7080
0.6839
0.6498
0.6377SSIM
MUSIQ
60657075
MUSIQ
63.1564.3767.9769.4270.92 71.10(a)
Zoomed LR
 LDL
 SeeSR-10
 SeeSR-50
DoSSR-onestep
 DoSSR-3
 DoSSR-5
 DoSSR-7 (b)
Figure 4: (a) Quality metrics vs. steps on RealSR Dataset. (b) Qualitative comparisons of different
steps of our DoSSR with other methods. The suffix "-N" appended to the method name indicates the
number of inference steps. Please zoom in for a better view.
4.3 Ablation Study
Effectiveness of DoSG. To verify the effectiveness of the DoSG introduced in the diffusion
equation, we conduct an experiment using identical network architectures but with two different
diffusion equations: the original diffusion equation as described by Ho et al. [16] and our newly
formulated equation (Eq. (2)). To isolate the impact of DoSG from our shifting sequence design,
we set t1=T, ensuring that the starting point of our inference in both scenarios approximates
Gaussian noise. Quantitative comparisons can be found in the first two rows of Table 2. It is evident
that the introduction of DoSG leads to a significant improvement across all metrics in the table,
highlighting the effectiveness of DoSG in enhancing the performance of diffusion-based SR models.
Additionally, it is worth noting that the original diffusion equation can be considered a special case
within our framework where Œ∑t= 0 andt1=T. Therefore, our sampler can accommodate the
original diffusion equation, and for a fair comparison, we employ the same sampler for both models.
More comprehensive comparison is provided in Appendix Table 6, where it can be seen that our
DoSSR demonstrates superior performance compared to the corresponding order solver with DDPM,
benefiting from the inclusion of DoSG in our DoS SDE-Solver.
Method CLIPIQA ‚ÜëMUSIQ ‚ÜëMANIQA ‚ÜëTOPIQ ‚Üë
DDPM [16] 0.5379 54.09 0.3932 0.5180
Domain Shift
Diffusion- t1T 0.5776 55.69 0.4181 0.5427
2T/3 0.6337 59.30 0.4589 0.5987
‚úìT/2 0.6776 64.40 0.5214 0.6618
T/3 0.6490 61.76 0.4895 0.6260
Table 2: Comparison across various selections of start-
ing point t1, evaluated on the DRealSR dataset. The
baseline method is DDPM, which employs the original
diffusion equation. In all setups, inference is carried out
over 5 steps.The selection of t1.The strating point
t1serves as a pivotal parameter in DoSSR.
We explore several options on the value of
t1and show the corresponding final SR per-
formance in Table 2. It can be observed that
SR performance improves as t1gradually
decreases from TtoT/2. However, further
decreasing t1fromT/2to3/Tconversely
compromises SR performance. Intuitively,
a larger t1means less LR prior is preserved
due to a larger magnitude of added noises,
and the model behaves more like the vanilla
pretrained model by hallucinating plausi-
ble HR contents; In contrast, a smaller t1
means less noises, so the prediction is prone to be more consistent with the LR image, but without
HR details. Hence, we set t1=T/2by default for a good trade-off.
The number of step. We assess the impact of different inference steps on DoSSR by analyzing
changes in representative metrics for both reference-based and non-reference-based evaluations,
as shown in Fig. 4(a). As the number of inference steps increases, reference-based metrics tend
to decline, suggesting a loss in fidelity, while non-reference metrics improve, indicating enhanced
realism and detail in the generated images. We also conduct visual comparisons in Fig. 4(b). Our
DoSSR achieves performance comparable to SeeSR in just 5 steps and produces more realistic
details in 7 steps. Remarkably, DoSSR is capable of delivering satisfactory results even with just a
9single step , achieving 0.5115 MANIQA score and 0.6258 CLIPIQA score on the RealSR dataset,
significantly boosting the efficiency of diffusion-based methods. More visual examples are provided
in Fig. 9, where it can be observed that increasing the number of steps yields more realistic details.
Order CLIPIQA ‚ÜëMUSIQ ‚ÜëMANIQA ‚ÜëTOPIQ ‚Üë
1 0.5907 59.12 0.4686 0.5907
2 0.6749 64.09 0.5196 0.6571
3 0.6776 64.40 0.5214 0.6618
Table 3: Comparison of performance of
different sampler orders on the DRealSR
dataset. In all setups, inference is carried
out over 5 steps.The order of our sampler. We provide a suite of
solvers for sampling in our DoSSR model, including
a first-order solver presented in Eq. (11), and more ad-
vanced second- and third-order solvers detailed in Ap-
pendix A.6. We investigate the impact of samplers with
different orders on our experimental results through
qualitative and quantitative comparisons, as illustrated
in Table 3 and Fig. 10. From Table 3, it becomes ev-
ident that high-order samplers can achieve superior
non-reference metrics under the same limited inference
step conditions. This is because the acceleration of
higher-order samplers allows diffusion models to generate more details, as demonstrated in the first
example of Fig. 10, where the tower generated by the high-order sampler exhibits richer textures.
More comprehensive comparison is provided in Appendix Table 6. In our implementation, we use
third-order sampler by default.
5 Conclusion
In this paper, we present DoSSR, a diffusion-based super-resolution framework that significantly
enhances both efficiency and performance by integrating a domain shift strategy with pretrained
diffusion models. This approach not only enhances generative capacity but also enhances further
inference efficiency through our novel proposed DoS-SDEs formulation and customized solvers.
Empirical validation on diverse SR benchmarks confirms that DoSSR achieves a 5-7 times speed
improvement over existing methods, setting a new state-of-the-art. Our work paves the way for more
efficient diffusion-based solutions in image super-resolution.
Limitation. Despite the strong overall performance demonstrated by the proposed DoSSR, it
occasionally generates visually unfriendly details when employing an unfavorable random seed, a
challenge also encountered by other diffusion-based methods. Typically, we fix the random seed for
all image super-resolution tasks to stabilize the results, but this particular seed may not be suitable
for certain specific images. As depicted in Fig. 8, different initializations of random seeds result in
significant variations in the details of the lion‚Äôs eyes. Some of the initialized random seeds produce
eyes that are reasonable and acceptable, while others exhibit noticeable inconsistencies with LR.
For bad cases, we can also obtain a satisfactory result by adjusting the random seed multiple times.
However, this often requires numerous attempts, and the quality of the results heavily relies on luck.
This inspires us to find a suitable initialization for each specific LR image, which can enhance the
performance of the model. Hence, for diffusion-based methods, exploring how to obtain a reasonable
random seed based on known LR images may be a future research direction.
Societal impact. Our advancements in the diffusion-based image super-resolution model, DoSSR,
present both positive and negative societal impacts. On the positive side, it enhances medical imaging,
potentially leading to more accurate diagnoses and reducing the need for invasive procedures. In
surveillance, it aids in better identification and tracking, improving public safety. Moreover, in
remote sensing and environmental monitoring, it facilitates informed decision-making for disaster
management and environmental conservation. However, there are concerns regarding privacy and
surveillance. Enhanced resolution capabilities could infringe upon privacy rights and lead to increased
surveillance in public spaces, raising questions about civil liberties. Additionally, in digital media,
while high-resolution imagery enhances visual content, it may perpetuate unrealistic beauty standards
and digital manipulation, impacting self-esteem. In summary, while DoSSR brings promising
advancements, it‚Äôs crucial to address concerns around privacy, security, and digital ethics to ensure
responsible and ethical deployment of the technology.
10References
[1]Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution: Dataset
and study. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops ,
pages 126‚Äì135, 2017.
[2]Brian DO Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications ,
12(3):313‚Äì326, 1982.
[3]Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie Li, Hamid Kazemi, Furong Huang, Micah Goldblum,
Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise.
Advances in Neural Information Processing Systems , 36, 2024.
[4]Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image
super-resolution: A new benchmark and a new model. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 3086‚Äì3095, 2019.
[5]Chaofeng Chen, Jiadi Mo, Jingwen Hou, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi
Lin. Topiq: A top-down approach from semantics to distortions for image quality assessment. IEEE
Transactions on Image Processing , 33:2404‚Äì2418, 2024.
[6]Xiangyu Chen, Xintao Wang, Jiantao Zhou, Yu Qiao, and Chao Dong. Activating more pixels in image
super-resolution transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 22367‚Äì22377, 2023.
[7]Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning
method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938 , 2021.
[8]Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional
diffusion models for inverse problems through stochastic contraction. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 12413‚Äì12422, 2022.
[9]Qinpeng Cui, Xinyi Zhang, Zongqing Lu, and Qingmin Liao. Elucidating the solution space of extended
reverse-time sde for diffusion models. arXiv preprint arXiv:2309.06169 , 2023.
[10] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for
single image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition , pages 11065‚Äì11074, 2019.
[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248‚Äì255.
Ieee, 2009.
[12] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for
image super-resolution. In Computer Vision‚ÄìECCV 2014: 13th European Conference, Zurich, Switzerland,
September 6-12, 2014, Proceedings, Part IV 13 , pages 184‚Äì199. Springer, 2014.
[13] Dario Fuoli, Luc Van Gool, and Radu Timofte. Fourier space losses for efficient perceptual image
super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages
2360‚Äì2369, 2021.
[14] Martin Gonzalez, Nelson Fernandez Pinto, Thuy Tran, Hatem Hajri, Nader Masmoudi, et al. Seeds:
Exponential sde solvers for fast high-quality sampling from diffusion models. Advances in Neural
Information Processing Systems , 36, 2024.
[15] Shuhang Gu, Andreas Lugmayr, Martin Danelljan, Manuel Fritsche, Julien Lamour, and Radu Timofte.
Div8k: Diverse 8k resolution image dataset. In 2019 IEEE/CVF International Conference on Computer
Vision Workshop (ICCVW) , pages 3512‚Äì3516. IEEE, 2019.
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural
information processing systems , 33:6840‚Äì6851, 2020.
[17] Huaibo Huang, Ran He, Zhenan Sun, and Tieniu Tan. Wavelet-srnet: A wavelet-based cnn for multi-scale
face super resolution. In Proceedings of the IEEE international conference on computer vision , pages
1689‚Äì1697, 2017.
[18] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration models.
Advances in Neural Information Processing Systems , 35:23593‚Äì23606, 2022.
11[19] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image
quality transformer. In Proceedings of the IEEE/CVF international conference on computer vision , pages
5148‚Äì5157, 2021.
[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
[21] Haoran Li, Long Ma, Yong Liao, Lechao Cheng, Yanbin Hao, and Pengyuan Zhou. 3d-goi: 3d gan
omni-inversion for multifaceted and multi-object editing. arXiv preprint arXiv:2311.12050 , 2023.
[22] Haoran Li, Haolin Shi, Wenli Zhang, Wenjun Wu, Yong Liao, Lin Wang, Lik-hang Lee, and Pengyuan
Zhou. Dreamscene: 3d gaussian-based text-to-3d scene generation via formation pattern sampling. arXiv
preprint arXiv:2404.03575 , 2024.
[23] Jie Liang, Hui Zeng, and Lei Zhang. Details or artifacts: A locally discriminative learning approach to
realistic image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5657‚Äì5666, 2022.
[24] Jie Liang, Hui Zeng, and Lei Zhang. Efficient and degradation-adaptive network for real-world image
super-resolution. In European Conference on Computer Vision , pages 574‚Äì591. Springer, 2022.
[25] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image
restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer
vision , pages 1833‚Äì1844, 2021.
[26] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual
networks for single image super-resolution. In Proceedings of the IEEE conference on computer vision
and pattern recognition workshops , pages 136‚Äì144, 2017.
[27] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, and
Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. arXiv preprint
arXiv:2308.15070 , 2023.
[28] Anran Liu, Yihao Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Blind image super-resolution: A survey and
beyond. IEEE transactions on pattern analysis and machine intelligence , 45(5):5461‚Äì5480, 2022.
[29] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer
data with rectified flow. In The Eleventh International Conference on Learning Representations (ICLR) ,
2023.
[30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode
solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information
Processing Systems , 35:5775‚Äì5787, 2022.
[31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver
for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095 , 2022.
[32] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing
high-resolution images with few-step inference, 2023.
[33] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In
International Conference on Machine Learning , pages 8162‚Äì8171. PMLR, 2021.
[34] Hannes Risken and Hannes Risken. Fokker-planck equation . Springer, 1996.
[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution
image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition , pages 10684‚Äì10695, 2022.
[36] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.
Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 45(4):4713‚Äì4726, 2022.
[37] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation.
arXiv preprint arXiv:2311.17042 , 2023.
[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International
Conference on Learning Representations , 2021.
12[39] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to
density and score estimation. In Uncertainty in Artificial Intelligence , pages 574‚Äì584. PMLR, 2020.
[40] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
Score-based generative modeling through stochastic differential equations. In International Conference on
Learning Representations , 2021.
[41] Stability.ai. https://stability.ai/stable-diffusion .
[42] Haoze Sun, Wenbo Li, Jianzhuang Liu, Haoyu Chen, Renjing Pei, Xueyi Zou, Youliang Yan, and Yujiu
Yang. Coser: Bridging image and language for cognitive super-resolution. In Proceedings of the IEEE
conference on computer vision and pattern recognition , 2024.
[43] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, and Lei Zhang. Ntire 2017 challenge
on single image super-resolution: Methods and results. In Proceedings of the IEEE conference on computer
vision and pattern recognition workshops , pages 114‚Äì125, 2017.
[44] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of
images. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 37, pages 2555‚Äì2563,
2023.
[45] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting
diffusion prior for real-world image super-resolution. arXiv preprint arXiv:2305.07015 , 2023.
[46] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-
resolution with pure synthetic data. In Proceedings of the IEEE/CVF international conference on computer
vision , pages 1905‚Äì1914, 2021.
[47] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-
resolution by deep spatial feature transform. In Proceedings of the IEEE conference on computer vision
and pattern recognition , pages 606‚Äì615, 2018.
[48] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao,
Alex C Kot, and Bihan Wen. Sinsr: Diffusion-based image super-resolution in a single step. In Proceedings
of the IEEE conference on computer vision and pattern recognition , 2024.
[49] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error
visibility to structural similarity. IEEE transactions on image processing , 13(4):600‚Äì612, 2004.
[50] Pengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo, and Liang Lin.
Component divide-and-conquer for real-world image super-resolution. In Computer Vision‚ÄìECCV 2020:
16th European Conference, Glasgow, UK, August 23‚Äì28, 2020, Proceedings, Part VIII 16 , pages 101‚Äì117.
Springer, 2020.
[51] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards
semantics-aware real-world image super-resolution. In Proceedings of the IEEE conference on computer
vision and pattern recognition , 2024.
[52] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and
Yujiu Yang. Maniqa: Multi-dimension attention network for no-reference image quality assessment. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1191‚Äì1200,
2022.
[53] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model for image
super-resolution by residual shifting. In Advances in Neural Information Processing Systems (NeurIPS) ,
2023.
[54] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model
for deep blind image super-resolution. In Proceedings of the IEEE/CVF International Conference on
Computer Vision , pages 4791‚Äì4800, 2021.
[55] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer
vision and pattern recognition , pages 586‚Äì595, 2018.
[56] Xinyi Zhang, Qinpeng Cui, Qiqi Bao, Wenming Yang, and Qingmin Liao. Geometry-guided diffusion
model with masked transformer for robust multi-view 3d human pose estimation. In ACM Multimedia
2024 , 2024.
13[57] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using
very deep residual channel attention networks. In Proceedings of the European conference on computer
vision (ECCV) , pages 286‚Äì301, 2018.
[58] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image
super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages
2472‚Äì2481, 2018.
[59] Yixuan Zhu, Wenliang Zhao, Ao Li, Yansong Tang, Jie Zhou, and Jiwen Lu. Flowie: Efficient image
enhancement via rectified flow. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 13‚Äì22, 2024.
14A Mathematical Details
A.1 Derivation of Eq.(3)
According to Eq. (2), we can express xtas a linear combination of x0,ÀÜx0and a noise variable œµ:
xt=Œ±t(Œ∑tÀÜx0+ (1‚àíŒ∑t)x0) +œÉtœµ,where œµ‚àº N(0,I). (13)
Subsequently, the relationship between xtandxt‚àí1is derived as follows:
xt=Œ±t(Œ∑t‚àí1ÀÜx0+ (1‚àíŒ∑t‚àí1)x0+ (Œ∑t‚àíŒ∑t‚àí1)(ÀÜx0‚àíx0)) +œÉtœµ
=Œ±t
Œ±t‚àí1
Œ±t‚àí1(Œ∑t‚àí1ÀÜx0+ (1‚àíŒ∑t‚àí1)x0) +œÉt‚àí1œµ1
+Œ±t(Œ∑t‚àíŒ∑t‚àí1)e0+s
œÉ2
t‚àíŒ±2
t
Œ±2
t‚àí1œÉ2
t‚àí1œµ2
=Œ±t
Œ±t‚àí1xt‚àí1+Œ±t(Œ∑t‚àíŒ∑t‚àí1)e0+s
œÉ2
t‚àíŒ±2
t
Œ±2
t‚àí1œÉ2
t‚àí1œµ2
where e0=ÀÜx0‚àíx0, andœµ,œµ1,œµ2‚àº N(0,I). Taking into account Œ±2
t+œÉ2
t= 1, the above equation
can be further simplified as follows:
xt=Œ±t
Œ±t‚àí1xt‚àí1+Œ±t(Œ∑t‚àíŒ∑t‚àí1)e0+s
1‚àíŒ±2
t
Œ±2
t‚àí1œµ2, (14)
Hence, the transition distribution between xtandxt‚àí1is as follows:
q(xt|xt‚àí1,ÀÜx0) =N(xt;Œ±t
Œ±t‚àí1xt‚àí1+Œ±t(Œ∑t‚àíŒ∑t‚àí1)e0,1‚àíŒ±2
t
Œ±2
t‚àí1I), t= 1,2,¬∑¬∑¬∑, T, (15)
A.2 Derivation of Eq.(6)
In this section, we derive the coefficients of the forward SDE. Discretizing Eq. (5) yields:
xt+‚àÜt‚àíxt=f(t)xt‚àÜt+h(t)ÀÜx0‚àÜt+g(t)‚àö
‚àÜtz1,where z1‚àº N(0,I)
xt+‚àÜt= (f(t)‚àÜt+ 1)xt+h(t)ÀÜx0‚àÜt+g(t)‚àö
‚àÜtz1. (16)
Substituting Eq. (13) into Eq. (16), we have
xt+‚àÜt= (f(t)‚àÜt+ 1)[Œ±t(Œ∑tÀÜx0+ (1‚àíŒ∑t)x0) +œÉtz2] +h(t)ÀÜx0‚àÜt+g(t)‚àö
‚àÜtz1
=Œ±t(f(t)‚àÜt+ 1)(1 ‚àíŒ∑t)x0+ [Œ±tŒ∑t(f(t)‚àÜt+ 1) + h(t)‚àÜt]ÀÜx0
+q
(f(t)‚àÜt+ 1)2œÉ2
t+g(t)2Àúz,(17)
where z2,Àúz‚àº N(0,I). For Eq. (13) at time t+ ‚àÜt, we have:
xt+‚àÜt=Œ±t+‚àÜt(Œ∑t+‚àÜtÀÜx0+ (1‚àíŒ∑t+‚àÜt)x0) +œÉt+‚àÜtœµ. (18)
Equating the corresponding parts of Eq. (17) and Eq. (18) yields:
Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥Œ±t+‚àÜt(1‚àíŒ∑t+‚àÜt) =Œ±t(f(t)‚àÜt+ 1)(1 ‚àíŒ∑t)
Œ±t+‚àÜtŒ∑t+‚àÜt=Œ±tŒ∑t(f(t)‚àÜt+ 1) + h(t)‚àÜt
œÉ2
t+‚àÜt= [f(t)‚àÜt+ 1]2œÉ2
t+g(t)2‚àÜt(19)
Then, letting ‚àÜt‚Üí0, the aforementioned three equations can be solved separately to yield:
Ô£±
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥f(t) =dlogŒ±t(1‚àíŒ∑t)
dt
h(t) =Œ±t
1‚àíŒ∑tdŒ∑t
dt
g(t) =r
dœÉ2
t
dt‚àí2dlogŒ±t(1‚àíŒ∑t)
dtœÉ2
t(20)
15A.3 Derivation of Eq.(7)
As outlined in Sec. 3.2, the forward process can be expressed as the SDE shown in Eq. (5). In
accordance with the Fokker-Plank Equation [34], we obtain:
‚àÇqt(xt)
‚àÇt=‚àí‚àáx{[f(t)xt+h(t)ÀÜx0]qt(xt)}+‚àÇ
‚àÇxi‚àÇxj[1
2g2(t)qt(xt)]
=‚àí‚àáx{[f(t)xt+h(t)ÀÜx0]qt(xt)}+‚àáx[1
2g2(t)‚àáxqt(xt)]
=‚àí‚àáx{[f(t)xt+h(t)ÀÜx0]qt(xt)}+‚àáx[1
2g2(t)qt(xt)‚àáxlogqt(xt)]
=‚àí‚àáx{[f(t)xt+h(t)ÀÜx0‚àí1
2g2(t)‚àáxlogqt(xt)]qt(xt)},
where q(xt)denotes the probability density function of state xt. Most process defined by a forward-
time or conventional diffusion equation model possess a corresponding reverse-time model [ 2], which
can be formulated as:
dxt=¬µ(t,xt)dt+œÉ(t,xt)dwt (21)
According to the backward Fokker-Plank Equation [34], we have:
‚àÇqt(xt)
‚àÇt=‚àí‚àáx[¬µ(t,xt)qt(xt)]‚àí‚àÇ
‚àÇxi‚àÇxj[1
2œÉ2(t,xt)qt(xt)]
=‚àí‚àáx[¬µ(t,xt)qt(xt)]‚àí ‚àáx[1
2œÉ2(t,xt)‚àáxqt(xt)]
=‚àí‚àáx[¬µ(t,xt)qt(xt)]‚àí ‚àáx[1
2œÉ2(t,xt)qt(xt)‚àáxlogqt(xt)]
=‚àí‚àáx{[¬µ(t,xt) +1
2œÉ2(t,xt)‚àáxlogqt(xt)]qt(xt)}.
Our goal is for the reverse process to have the same distribution as the forward process, specifically:
¬µ(t,xt) +1
2œÉ2(t,xt)‚àáxlogqt(xt) =f(t)xt+h(t)ÀÜx0‚àí1
2g2(t)‚àáxlogqt(xt). (22)
Typically, we set œÉ(t,xt) =g(t)[40], yielding:
¬µ(t,xt) =f(t)xt+h(t)ÀÜx0‚àíg2(t)‚àáxlogqt(xt). (23)
Therefore, the reverse-time SDE can be expressed as follows:
dxt=h
f(t)xt+h(t)ÀÜx0‚àíg2(t)‚àáxlogqt(xt)i
dt+g(t)dwt. (24)
A.4 Derivation of Eq.(9)
The data prediction model xŒ∏(xt,ÀÜx0, t)directly estimates the original target data x0from the noisy
samples, indicating that xŒ∏(xt,ÀÜx0, t)‚âàx0. Based on Eq. (2), the expression for qt(xt)can be
formulated as follows:
qt(xt) =1‚àö
2œÄœÉtexp(‚àí[xt‚àí(Œ±t(1‚àíŒ∑t)x0+Œ±tŒ∑tÀÜx0)]2
2œÉ2
t). (25)
Hence, score function is:
‚àáxlogqt(xt) =‚àíxt‚àí(Œ±t(1‚àíŒ∑t)x0+Œ±tŒ∑tÀÜx0)
œÉ2
t. (26)
Substituting xŒ∏(xt,ÀÜx0, t)‚âàx0, we can establish the relationship between the score function and the
data prediction model:
‚àáxlogqt(xt) =‚àíxt‚àí(Œ±t(1‚àíŒ∑t)xŒ∏(xt,ÀÜx0, t) +Œ±tŒ∑tÀÜx0)
œÉ2
t(27)
Furthermore, Eq. (8) shows that the noise prediction model is to estimate
œµŒ∏(xt,ÀÜx0, t)‚âà ‚àíœÉt‚àáxlogqt(xt). (28)
Hence, the relationship between the noise prediction model and the data prediction model is:
œµŒ∏(xt,ÀÜx0, t) =xt‚àí(Œ±t(1‚àíŒ∑t)xŒ∏(xt,ÀÜx0, t) +Œ±tŒ∑tÀÜx0)
œÉt, (29)
indicating that we easily use trained noise prediction model for data prediction through the equation.
16A.5 Proof of Proposition 3.1
In this section, we derive the solution to the equation Eq. (7). By substituting Eq. (9)and Eq. (20)
into Eq. (7), we obtain:
dxt=n1
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dtxt+Œ±t
1‚àíŒ∑tdŒ∑t
dtÀÜx0
‚àíh
2œÉtdœÉt
dt‚àí2œÉ2
td[Œ±t(1‚àíŒ∑t)]
dti
‚àíxt‚àí(Œ±t(1‚àíŒ∑t)xŒ∏(xt,ÀÜx0, t) +Œ±tŒ∑tÀÜx0)
œÉ2
to
dt
+r
dœÉ2
t
dt‚àí2dlogŒ±t(1‚àíŒ∑t)
dtœÉ2
tdwt.(30)
Combining like terms, we get:
dxt=h2
œÉtdœÉt
dt‚àí1
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dti
xt
+nŒ±t
1‚àíŒ∑tdŒ∑t
dt‚àíŒ±tŒ∑th2
œÉtdœÉt
dt‚àí2
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dtio
ÀÜx0
‚àí2Œ±t(1‚àíŒ∑t)h1
œÉtdœÉt
dt‚àí1
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dti
xŒ∏(xt,ÀÜx0, t)
+r
dœÉ2
t
dt‚àí2dlogŒ±t(1‚àíŒ∑t)
dtœÉ2
tdwt.(31)
Subsequently, dividing both sides by Œ±t(1‚àíŒ∑t)simultaneously, we have:
1
Œ±t(1‚àíŒ∑t)dxt=h2
œÉtdœÉt
dt‚àí1
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dtixt
Œ±t(1‚àíŒ∑t)
+n1
(1‚àíŒ∑t)2dŒ∑t
dt‚àíŒ∑t
(1‚àíŒ∑t)h2
œÉtdœÉt
dt‚àí2
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dtio
ÀÜx0
‚àí2h1
œÉtdœÉt
dt‚àí1
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dti
xŒ∏(xt,ÀÜx0, t)
+s
2œÉt
[Œ±t(1‚àíŒ∑t)]2dœÉt
dt‚àí2œÉ2
t
[Œ±t(1‚àíŒ∑t)]3d[Œ±t(1‚àíŒ∑t)]
dtdwt.(32)
LetŒªt=œÉt
Œ±t(1‚àíŒ∑t). Then Œªtis monotonically increasing, and we have:
dŒªt
dt=1
Œ±t(1‚àíŒ∑t)dœÉt
dt‚àíœÉt
[Œ±t(1‚àíŒ∑t)]2d[Œ±t(1‚àíŒ∑t)]
dt. (33)
Therefore, we have:
1
œÉtdœÉt
dt‚àí1
Œ±t(1‚àíŒ∑t)d[Œ±t(1‚àíŒ∑t)]
dt=1
ŒªtdŒªt
dt. (34)
By performing the variable substitution yt=xt
Œ±t(1‚àíŒ∑t)and then substituting Eq. (34) into Eq. (32),
we can simplify to obtain:
dyt=n2
ŒªtdŒªt
dtyt+h1
(1‚àíŒ∑t)2dŒ∑t
dt‚àíŒ∑t
1‚àíŒ∑t(2
ŒªtdŒªt
dt)i
ÀÜx0‚àí2
ŒªtdŒªt
dtxŒ∏(xt,ÀÜx0, t)o
dt+r
2ŒªtdŒªt
dtdwt.
(35)
Denoting dwŒªt:=q
dŒªt
dtdwt,xŒª:=xt(Œª),wŒª:=wŒªt, we rewrite the equation above w.r.t Œªas
dyŒª=2
ŒªyŒªdŒª+h1
(1‚àíŒ∑Œª)2dŒ∑Œª‚àíŒ∑Œª
1‚àíŒ∑Œª2
ŒªdŒªi
ÀÜx0‚àí2
ŒªxŒ∏(xŒª,ÀÜx0, Œª)dŒª+‚àö
2ŒªdwŒª.(36)
Utilizing the variation-of-constants formula to solve the equation above, we obtain
yt=eRŒªt
Œªs2
ŒªdŒªys+ZŒªt
ŒªseRŒªt
Œª2
œÑdœÑh1
(1‚àíŒ∑Œª)2dŒ∑Œª‚àíŒ∑Œª
1‚àíŒ∑Œª2
ŒªdŒªi
ÀÜx0
‚àíZŒªt
ŒªseRŒªt
Œª2
œÑdœÑ2
ŒªxŒ∏(xŒª,ÀÜx0, Œª)dŒª+ZŒªt
ŒªseRŒªt
Œª2
œÑdœÑ‚àö
2ŒªdwŒª.(37)
17Simplifying and substituting back xt=Œ±t(1‚àíŒ∑t)yt, we obtain
xt=Œ±t(1‚àíŒ∑t)
Œ±s(1‚àíŒ∑s)Œª2
t
Œª2sxs+Œ±t(1‚àíŒ∑t)(Œ∑t
1‚àíŒ∑t‚àíŒ∑s
1‚àíŒ∑sŒª2
t
Œª2s)ÀÜx0
‚àíŒ±t(1‚àíŒ∑t)ZŒªt
Œªs2Œª2
t
Œª3xŒ∏(xŒª,ÀÜx0, Œª)dŒª+Œ±t(1‚àíŒ∑t)s
Œª2
t‚àíŒª4
t
Œª2szs.(38)
Thus, we obtain the exact solution to the DoS-SDEs.
A.6 Derivation of Solvers for Diffusion DoS-SDEs
Denote x(n)
Œ∏(xŒª,ÀÜx0, Œª) :=dnxŒ∏(xŒª,ÀÜx0,Œª)
dŒªn as the n-th order total derivative of xŒ∏(xŒª, Œª)w.r.tŒª. For
k‚â•1, thek‚àí1-th order It√¥-Taylor expansion of xŒ∏(xŒª,ÀÜx0, Œª)w.r.tŒªatsis
xŒ∏(xŒª,ÀÜx0, Œª) =k‚àí1X
n=0(Œª‚àíŒªs)n
n!x(n)
Œ∏(xs,ÀÜx0, s) +Rk, (39)
where the residual Rkcomprises of deterministic iterated integrals of length greater than kand all
iterated with at least one stochastic component.
Substituting the above It√¥-Taylor expansion into Eq. (38) yields
xt=Œ±t(1‚àíŒ∑t)
Œ±s(1‚àíŒ∑s)Œª2
t
Œª2sxs+Œ±t(1‚àíŒ∑t)(Œ∑t
1‚àíŒ∑t‚àíŒ∑s
1‚àíŒ∑sŒª2
t
Œª2s)ÀÜx0
‚àíŒ±t(1‚àíŒ∑t)k‚àí1X
n=0x(n)
Œ∏(xs,ÀÜx0, s)ZŒªt
Œªs2Œª2
t
Œª3(Œª‚àíŒªs)n
n!dŒª+Œ±t(1‚àíŒ∑t)s
Œª2
t‚àíŒª4
t
Œª2szs+ÀúRk,
(40)
where ÀúRkcan be easily obtained from Rkand the integralRŒªt
Œªs2Œª2
t
Œª3(Œª‚àíŒªs)n
n!dŒªcan be analytically
computed by repeated applying ntimes of integration-by-parts. By dropping the Rkerror and
approximating the first k‚àí1-th total derivatives with forward differential method , we can derive
k-th order SDE solvers for diffusion DoS-SDEs. In fact, it is inaccurate to call it "order" when
k‚â•2, because the proposed algorithm has a global error of at least O(Œª‚àíŒªs)[14]. Thus, only
when k= 1, it is referred to as a first-order solver with a strong convergence guarantee, as stated in
[14]. Nevertheless, for practical convenience, we still refer to this approximation as k-th order. Here
we present the expressions for first-order as well as second and third-order solvers. We name such
solvers as DoS-SDE Solver overall, and DoS-SDE Solver-k for a specific order k.
DoS-SDE Solver-1 When k= 1, the integral becomes
‚àíZŒªt
Œªs2Œª2
t
Œª3xŒ∏(xŒª, Œª)dŒª‚âà ‚àíŒª2
tZŒªt
Œªs2
Œª3dŒªxŒ∏(xs,ÀÜx0, s) = (1 ‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s). (41)
Substituting into Eq.(38), we obtain first-order solver for DoS-SDEs
xt=Œ±t(1‚àíŒ∑t)
Œ±s(1‚àíŒ∑s)Œª2
t
Œª2sxs+Œ±t(1‚àíŒ∑t)(Œ∑t
1‚àíŒ∑t‚àíŒ∑s
1‚àíŒ∑sŒª2
t
Œª2s)ÀÜx0
+Œ±t(1‚àíŒ∑t)(1‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s) +Œ±t(1‚àíŒ∑t)s
Œª2
t‚àíŒª4
t
Œª2szs.(42)
DoS-SDE Solver-2 When k= 2, the integral in Eq.(40) becomes
‚àí1X
n=0x(n)
Œ∏(xs,ÀÜx0, s)ZŒªt
Œªs2Œª2
t
Œª3(Œª‚àíŒªs)n
n!dŒª
=‚àíZŒªt
Œªs2Œª2
t
Œª3dŒªxŒ∏(xs,ÀÜx0, s)‚àíZŒªt
Œªs2Œª2
t
Œª3(Œª‚àíŒªs)dŒªx(1)
Œ∏(xs,ÀÜx0, s)
= (1‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s)‚àí(Œªt‚àíŒªs)2
Œªsx(1)
Œ∏(xs,ÀÜx0, s)(43)
18Substituting into Eq.(40), we obtain 2-th order solver for DoS-SDEs
xt=Œ±t(1‚àíŒ∑t)
Œ±s(1‚àíŒ∑s)Œª2
t
Œª2sxs+Œ±t(1‚àíŒ∑t)(Œ∑t
1‚àíŒ∑t‚àíŒ∑s
1‚àíŒ∑sŒª2
t
Œª2s)ÀÜx0+Œ±t(1‚àíŒ∑t)s
Œª2
t‚àíŒª4
t
Œª2szs
+Œ±t(1‚àíŒ∑t)(1‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s)‚àíŒ±t(1‚àíŒ∑t)(Œªt‚àíŒªs)2
Œªsx(1)
Œ∏(xs,ÀÜx0, s),(44)
where x(1)
Œ∏(xs,ÀÜx0, s)can be estimated by forward differential method . We have
x(1)
Œ∏(xs,ÀÜx0, s)‚âàxŒ∏(xs,ÀÜx0, s)‚àíxŒ∏(xr,ÀÜx0, r)
Œªs‚àíŒªr, (45)
where time t < s < r andxŒ∏(xr,ÀÜx0, r)represents the output of the network at the previous time
step.
DoS-SDE Solver-3 Samely, when k= 3, the integral in Eq.(40) becomes
‚àí2X
n=0x(n)
Œ∏(xs,ÀÜx0, s)ZŒªt
Œªs2Œª2
t
Œª3(Œª‚àíŒªs)n
n!dŒª
= (1‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s)‚àí(Œªt‚àíŒªs)2
Œªsx(1)
Œ∏(xs,ÀÜx0, s)‚àíZŒªt
ŒªsŒª2
t
Œª3(Œª‚àíŒªs)2dŒªx(2)
Œ∏(xs,ÀÜx0, s)
= (1‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s)‚àí(Œªt‚àíŒªs)2
Œªsx(1)
Œ∏(xs,ÀÜx0, s)
+h(Œªs‚àí3Œªt)(Œªs‚àíŒªt)
2‚àíŒª2
tln(Œªt
Œªs)i
x(2)
Œ∏(xs,ÀÜx0, s)
(46)
Substituting into Eq.(40), we obtain 3-th order solver for DoS-SDEs
xt=Œ±t(1‚àíŒ∑t)
Œ±s(1‚àíŒ∑s)Œª2
t
Œª2sxs+Œ±t(1‚àíŒ∑t)(Œ∑t
1‚àíŒ∑t‚àíŒ∑s
1‚àíŒ∑sŒª2
t
Œª2s)ÀÜx0+Œ±t(1‚àíŒ∑t)s
Œª2
t‚àíŒª4
t
Œª2szs
+Œ±t(1‚àíŒ∑t)(1‚àíŒª2
t
Œª2s)xŒ∏(xs,ÀÜx0, s)‚àíŒ±t(1‚àíŒ∑t)(Œªt‚àíŒªs)2
Œªsx(1)
Œ∏(xs,ÀÜx0, s)
+Œ±t(1‚àíŒ∑t)h(Œªs‚àí3Œªt)(Œªs‚àíŒªt)
2‚àíŒª2
tln(Œªt
Œªs)i
x(2)
Œ∏(xs,ÀÜx0, s)(47)
where x(1)
Œ∏(xs,ÀÜx0, s)andx(2)
Œ∏(xs,ÀÜx0, s)can be estimated by forward differential method . We have
x(1)
Œ∏(xs,ÀÜx0, s)‚âàxŒ∏(xs,ÀÜx0, s)‚àíxŒ∏(xr,ÀÜx0, r)
Œªs‚àíŒªr, (48)
where time t < s < r andxŒ∏(xr,ÀÜx0, r)represents the output of the network at the previous time
step. And
x(2)
Œ∏(xs,ÀÜx0, s)‚âàx(1)
Œ∏(xs,ÀÜx0, s)‚àíx(1)
Œ∏(xr,ÀÜx0, r)
Œªs‚àíŒªq
2, (49)
where time t < s < r < q andx(1)
Œ∏(xs,ÀÜx0, s)andx(1)
Œ∏(xr,ÀÜx0, r)respectively represent the
approximations of the first-order derivatives at the current and previous steps.
Detailed algorithms for our solvers are proposed in Sec. B
A.7 Comparative Analysis of DoSSR and ResShift
Previous work has introduced a method called ResShift [ 53], which shortens the length of the
Markov chain in the diffusion process through residual shifting to achieve efficient super-resolution
in diffusion models. In this section, we theoretically elaborate on the similarities and differences
between our method and ResShift.
19ResShift expresses the forward diffusion process in the form of residual shifting, as shown in the
following equation:
q(xt|x0,y0) =N(xt;x0+Œ∑te0, k2Œ∑tI), t= 1,2,¬∑¬∑¬∑, T, (50)
where e0=y0‚àíx0respensents the residual between the LR image y0and the HR image x0. Hence,
it can be rewrite as,
q(xt|x0,y0) =N(xt;Œ∑ty0+ (1‚àíŒ∑t)x0, k2Œ∑tI), t= 1,2,¬∑¬∑¬∑, T. (51)
Therefore, essentially, the ResShift concept also represents a linear combination of the source domain
and the target domain. However, the crucial factor lies in our design of the diffusion equation,
which determines whether we can effectively utilize the diffusion prior . This is our most significant
differentiating point. Currently, the mainstream approach to leveraging diffusion prior involves
fine-tuning large-scale pretrained diffusion models to achieve SR. As we all know, Stable Diffusion, a
typical representative of large-scale diffusion models, employs the diffusion equation of DDPM [ 16].
Its forward process can be expressed as:
q(xt|x0) =N(xt;Œ±tx0, œÉ2
tI), t= 1,2,¬∑¬∑¬∑, T, (52)
where Œ±t, œÉt‚â•0andŒ±2
t+œÉ2
t= 1, referred to as noise schedule . If we consider Œ∑ty0+ (1‚àíŒ∑t)x0
as a whole, we can intuitively observe that Eq. (51) lacks a decay coefficient Œ±tcompared to Eq. (52).
Therefore, applying Eq. (51) to fine-tune a pretrained Stable Diffusion model poses significant
challenges. The equation proposed by our DoSSR is restated as follows:
q(xt|x0,ÀÜx0) =N(xt;Œ±t(Œ∑tÀÜx0+ (1‚àíŒ∑t)x0), œÉ2
tI), t= 1,2,¬∑¬∑¬∑, T, (53)
Our diffusion equation incorporates the noise schedule of Stable Diffusion, enabling seamless
compatibility with existing DDPM-type diffusion models. Therefore, in this sense, our method is
specifically designed as a diffusion equation tailored to adapt to Stable Diffusion.
In fact, Eq. (51) and Eq. (52) (or Eq. (53)) belong to two different types of diffusion models, which
correspond to the discretizations of two types of SDEs(VE SDE and VP SDE) [40], respectively.
The diffusion equation that satisfies the discretizations of VE SDEs typically takes the following
form:
q(xt|x0) =N(xt;x0, œÉ2
tI), t= 1,2,¬∑¬∑¬∑, T, (54)
where œÉtincreases with t, and œÉTis typically a very large number, ensuring that q(xT) =
N(x0;œÉ2
TI)‚âà N(0;œÉ2
TI). Therefore, Eq. (51) can be considered as a variant of the above equation.
By setting hyperparameters such that k2Œ∑t=œÉ2
t, it appears possible to fine-tune a diffusion model
pretrained with Eq. (54) as the diffusion equation, thereby leveraging the diffusion prior. However,
this contradicts its original intention, as the starting point of inference almost approximates Gaussian
noise, retaining little of the prior information of LR. Our design of shifting sequence {Œ∑t}T
t=1is
aimed at addressing this issue, which the ResShift lacks.
The second type of diffusion equation that satisfies the discretizations of VP SDEs is of the DDPM
type. In Eq. (52), the noise schedule satisfies the noise schedule 0‚â§œÉt‚â§1and it is typically
set to œÉT‚âà1, Œ±T‚âà0to ensure that q(xT) =N(Œ±Tx0;œÉ2
TI)‚âà N (0;I). To my knowledge,
most large-scale pretrained diffusion models, represented by Stable Diffusion, adopt the VP-type
(DDPM-type) diffusion equation. Therefore, our design is highly significant. Moreover, our theory is
not limited to the analysis of discrete cases but extends to more general continuous cases, expressed as
SDEs. Based on this, we have developed our fast samplers for our DoSSR. These are the distinctions
between our work and ResShift.
To further demonstrate the effect of the diffusion prior, we conducted an additional comparative
experiment with ResShift, as detailed in Appendix C.
20B Pseudocode
Here, algorithms for first, second, and third-order solvers for DoS-SDEs are presented as follows.
Algorithm 1 DoSSR Solver-1.
Require: starting point t1, used time steps {ti}N
i=1, nosie schedule Œ±tandœÉt, preprocessed LR
image ÀÜx0, data prediction model xŒ∏.
1:xt1‚ÜêŒ±t1ÀÜx0+œÉt1œµ ‚ñ∑initial value
2:fori‚Üê2toNdo
3: DoSG ‚ÜêŒ±ti(1‚àíŒ∑ti)(Œ∑ti
1‚àíŒ∑ti‚àíŒ∑ti‚àí1
1‚àíŒ∑ti‚àí1Œª2
ti
Œª2
ti‚àí1)ÀÜx0 ‚ñ∑domain shift guidance
4: Linear Term ‚ÜêŒ±ti(1‚àíŒ∑ti)
Œ±ti‚àí1(1‚àíŒ∑ti‚àí1)Œª2
ti
Œª2
ti‚àí1xti‚àí1
5: Noise Term ‚ÜêŒ±ti(1‚àíŒ∑ti)r
Œª2
ti‚àíŒª4
ti
Œª2
ti‚àí1ÀÜx0
6: PAT‚ÜêŒ±ti(1‚àíŒ∑ti)(1‚àíŒª2
ti
Œª2
ti‚àí1)xŒ∏(xti‚àí1,ÀÜx0, ti‚àí1) ‚ñ∑Prediction Approximation Term
7: xti‚ÜêLinear Term +DoSG +PAT+Noise Term
8:end for
9:Return: xtN
Algorithm 2 DoSSR Solver-2.
Require: starting point t1, used time steps {ti}N
i=1, noise schedule Œ±tandœÉt, preprocessed LR
image ÀÜx0, data prediction model xŒ∏.
1:xt1‚ÜêŒ±t1ÀÜx0+œÉt1œµ ‚ñ∑initial value
2:Q‚ÜêNone
3:fori‚Üê2toNdo
4: DoSG ‚ÜêŒ±ti(1‚àíŒ∑ti)(Œ∑ti
1‚àíŒ∑ti‚àíŒ∑ti‚àí1
1‚àíŒ∑ti‚àí1Œª2
ti
Œª2
ti‚àí1)ÀÜx0 ‚ñ∑domain shift guidance
5: Linear Term ‚ÜêŒ±ti(1‚àíŒ∑ti)
Œ±ti‚àí1(1‚àíŒ∑ti‚àí1)Œª2
ti
Œª2
ti‚àí1xti‚àí1
6: Noise Term ‚ÜêŒ±ti(1‚àíŒ∑ti)r
Œª2
ti‚àíŒª4
ti
Œª2
ti‚àí1ÀÜx0
7: ifQ=None then
8: PAT‚ÜêŒ±ti(1‚àíŒ∑ti)(1‚àíŒª2
ti
Œª2
ti‚àí1)xŒ∏(xti‚àí1,ÀÜx0, ti‚àí1)
9: else
10: Di‚ÜêxŒ∏(xti‚àí1,ÀÜx0,ti‚àí1)‚àíxŒ∏(xti‚àí2,ÀÜx0,ti‚àí2)
Œªti‚àí1‚àíŒªti‚àí2‚ñ∑first order derivative
11: PAT‚ÜêŒ±ti(1‚àíŒ∑ti)(1‚àíŒª2
ti
Œª2
ti‚àí1)xŒ∏(xti‚àí1,ÀÜx0, ti‚àí1)‚àíŒ±ti(1‚àíŒ∑ti)(Œªti‚àíŒªti‚àí1)2
Œªti‚àí1Di
12: end if
13: Q‚ÜêxŒ∏(xti‚àí1,ÀÜx0, ti‚àí1) ‚ñ∑save last network output
14: xti‚ÜêLinear Term +DoSG +PAT+Noise Term
15:end for
16:Return: xtN
21Algorithm 3 DoSSR Solver-3.
Require: starting point t1, used time steps {ti}N
i=1, noise schedule Œ±tandœÉt, preprocessed LR
image ÀÜx0, data prediction model xŒ∏.
1:xt1‚ÜêŒ±t1ÀÜx0+œÉt1œµ ‚ñ∑initial value
2:Q‚ÜêNone ,Qd‚ÜêNone
3:fori‚Üê2toNdo
4: DoSG ‚ÜêŒ±ti(1‚àíŒ∑ti)(Œ∑ti
1‚àíŒ∑ti‚àíŒ∑ti‚àí1
1‚àíŒ∑ti‚àí1Œª2
ti
Œª2
ti‚àí1)ÀÜx0 ‚ñ∑domain shift guidance
5: Linear Term ‚ÜêŒ±ti(1‚àíŒ∑ti)
Œ±ti‚àí1(1‚àíŒ∑ti‚àí1)Œª2
ti
Œª2
ti‚àí1xti‚àí1
6: Noise Term ‚ÜêŒ±ti(1‚àíŒ∑ti)r
Œª2
ti‚àíŒª4
ti
Œª2
ti‚àí1ÀÜx0
7: ifQ=None andQd=None then
8: PAT‚ÜêŒ±ti(1‚àíŒ∑ti)(1‚àíŒª2
ti
Œª2
ti‚àí1)xŒ∏(xti‚àí1,ÀÜx0, ti‚àí1)
9: else if QÃ∏=None andQd=None then
10: Di‚ÜêxŒ∏(xti‚àí1,ÀÜx0,ti‚àí1)‚àíxŒ∏(xti‚àí2,ÀÜx0,ti‚àí2)
Œªti‚àí1‚àíŒªti‚àí2‚ñ∑first order derivative
11: PAT‚ÜêŒ±ti(1‚àíŒ∑ti)(1‚àíŒª2
ti
Œª2
ti‚àí1)xŒ∏(xti‚àí1,ÀÜx0, ti‚àí1)‚àíŒ±ti(1‚àíŒ∑ti)(Œªti‚àíŒªti‚àí1)2
Œªti‚àí1Di
12: Qd‚ÜêDi
13: else
14: Di‚ÜêxŒ∏(xti‚àí1,ÀÜx0,ti‚àí1)‚àíxŒ∏(xti‚àí2,ÀÜx0,ti‚àí2)
Œªti‚àí1‚àíŒªti‚àí2‚ñ∑first order derivative
15: Ui‚ÜêDi‚àíDi‚àí1
Œªti‚àí1‚àíŒªti‚àí3
2‚ñ∑second order derivative
16: PAT‚ÜêŒ±ti(1‚àíŒ∑ti)(1‚àíŒª2
ti
Œª2
ti‚àí1)xŒ∏(xti‚àí1,ÀÜx0, ti‚àí1)‚àíŒ±ti(1‚àíŒ∑ti)(Œªti‚àíŒªti‚àí1)2
Œªti‚àí1Di+
Œ±ti(1‚àíŒ∑ti)h(Œªti‚àí1‚àí3Œªti)(Œªti‚àí1‚àíŒªti)
2‚àíŒª2
tiln(Œªti
Œªti‚àí1)i
Ui
17: Qd‚ÜêDi
18: end if
19: Q‚ÜêxŒ∏(xti‚àí1,ÀÜx0, ti‚àí1) ‚ñ∑save last network output
20: xti‚ÜêLinear Term +DoSG +PAT+Noise Term
21:end for
22:Return: xtN
C Additional Experiment Results
C.1 Ablation on Diffusion Prior
To demonstrate the impact of the diffusion prior, we conduct a comparative experiment with the
ResShift model [ 53], which does not leverage a pretrained diffusion model. Considering that ResShift
is trained on ImageNet [ 11] while our DoSSR model is trained on commonly used datasets for
super-resolution tasks (e.g., DIV2K [ 1]), to eliminate the influence of the training dataset, we retrain
our DoSSR model using ImageNet as well. To highlight the effect of the diffusion prior, we only
utilize a subset of ImageNet as our training data. This subset consists of randomly selected 10
images from each of the 1000 categories in ImageNet, totaling 10,000 images, as illustrated in
Table 4. It can be observed that our DoSSR achieves superior metrics compared to ResShift despite
utilizing significantly fewer training data and epochs for iteration. This indicates that leveraging
the diffusion prior is highly beneficial for super-resolution tasks, even without utilizing commonly
used high-resolution datasets for training, we can still achieve satisfactory results. We also perform
qualitative comparisons between our retrained DoSSR and ResShift, as illustrated in Fig. 5. It is
evident that the utilization of the diffusion prior significantly enhances the quality of the generated
images, both in terms of fidelity and realism.
22Table 4: Comparison of performance between our retrained DoSSR and ResShift models on the
DRealSR dataset. For fair comparison, we employ our first-order sampler for inference, running it 15
times to match ResShift‚Äôs default setting.
MethodDiffusion
PriorTraining Setup Evalution Metrics
Training Dataset Num of Iters Traing Method SSIM‚ÜëLPIPS ‚ÜìMUSIQ ‚ÜëMANIQA ‚Üë
ResShift % ImageNet( ‚àº1280k) 500k Train from scratch 0.7629 0.4036 49.73 0.3322
DoSSR ! sub-ImageNet(10k) 50k Finetune 0.7824 0.2943 53.94 0.3672
Table 5: Comparison of performance with other methods on the RealSRSet andRealSR datasets. NFE
represents the number of function evaluations in the inference.‚àóinvolves retraining using the same
training data and identical network architecture as our model.
Method Training DatasetRealSRSet RealSRNFE‚ÜìMUSIQ ‚ÜëMANIQA ‚ÜëMUSIQ ‚ÜëMANIQA ‚Üë
ColdDiff* DIV2K+DIV8K+Flickr2K+OST( ‚àº15k) 58.19 0.3194 47.42 0.2783 5
ResShift* DIV2K+DIV8K+Flickr2K+OST( ‚àº15k) 63.90 0.4505 56.01 0.4001 5
DoSSR DIV2K+DIV8K+Flickr2K+OST( ‚àº15k) 73.35 0.6169 69.42 0.5781 5
FlowIE ImageNet( ‚àº1280k) 61.63 0.3611 56.51 0.3284 1
FlowIE* DIV2K+DIV8K+Flickr2K+OST( ‚àº15k) 60.48 0.3644 50.82 0.3228 1
DoSSR DIV2K+DIV8K+Flickr2K+OST( ‚àº15k) 69.42 0.5554 62.69 0.5115 1
C.2 Compare with other formulations of diffusion process
Aside from ResShift, we also compare our method with other plausible alternative formulations of
diffusion processes, which encompass ColdDiff [ 3] and Rectified Flow [ 29]. ColdDiff is similar to
DDPM, but differs by employing alternative degradation methods, such as blurring and masking,
rather than additive Gaussian noise as used in DDPM. In our case for image super-resolusion, we
select the blur degradation. Note that ColdDiff is originally applied in the pixel space, while we have
to implement it in the latent space for fair comparison with our method. Rectified Flow defines the
forward process as xt=tÀÜx0+ (1‚àít)x0where ÀÜx0is the data sample and x0is Gaussian noise. In
our case for image super-resolusion, we choose FlowIE [ 59] as an implementation of rectified flow
which replaces x0with the LR image as the starting point.
For a fair comparison, we reimplemented the three methods‚ÄîResShift [ 53], ColdDiff [ 3], and
FlowIE [ 59]‚Äîusing the same network architecture, training dataset, and initialization as our method.
The results are shown in the Table 5. Results of ColdDiff is significantly worse than other methods.
The main reason can be summerized as two aspects. First, applying bluring kernels to latent features
is not equivalent to applying them to raw images, while ColdDiff is originally designed for the latter.
Second, the blurring kernel can only be designed in a hand-craft manner and may not represent
the real-world degradation, so when tested with real LR images there would be a domain gap.
The limilation of hand-crafted degradation is also well-known in many previous image restoration
works. In contrast, other methods, including our DoSSR, does not assume a fixed degradation, so
the performance is much better. Because FlowIE emphasizes single step evaluation, we follow its
setting and compare DoSSR with 1-setp evaluation with it. It can be see that FlowIE is also not
satisfactory and largely underperforms our DoSSR ( ‚àí11.87% MUSIQ). This is mainly attributed to
FlowIE‚Äôs bad adaptation from the DDPM pretrained T2I model, since the learning objective of flow
matching differs from score matching. By contrast, our design makes full use of the DDPM pretrained
weights so performs much better. ResShift also significantly underperforms our method, similar to
FlowIE, because its formulation does not account for adaptation from the pretrained diffusion prior,
as discussed in Appendix A.7 and C.1. To summarize, our formulation differs from other alternatives
especially in terms of better leverage and adaptation from DDPM pretrained T2I models .
C.3 Network Structure
The network structure of our model is illustrated in Fig. 6. In general, we adopt the same model
architecture as in DiffBIR [ 27], using LR images as conditional inputs for the ControlNet module.
The ControlNet is initialized from the Stable Diffusion 2.1 Unet encoder and trained to generate HR
images given LR inputs.
23Table 6: Comparison of performance: w/o DoSG vs. different accelerated samplers on the RealSR and
DRealSR datasets with same model(RealESRNet preprocessing + DiffBIR). In all setups, inference is
carried out over 5 steps.
Method Corr. SamplerRealSR Dataset DrealSR Dataset
CLIPIQA ‚ÜëMUSIQ ‚ÜëMANIQA ‚ÜëTOPIQ ‚ÜëCLIPIQA ‚ÜëMUSIQ ‚ÜëMANIQA ‚ÜëTOPIQ ‚Üë
w/o DoSG
(DDPM)DDIM( Œ∑= 1) 0.5176 57.95 0.4293 0.5286 0.4732 48.22 0.3518 0.4316
EDM 0.5351 62.08 0.4445 0.5789 0.5341 53.13 0.3917 0.5082
DPM-Solver++ -3 0.5323 62.67 0.4384 0.5807 0.5379 54.09 0.3932 0.5180
w/ DoSG
(DoSSR)DoS SDE-Solver -1 0.6874 66.55 0.5574 0.6588 0.5907 59.12 0.4686 0.5907
DoS SDE-Solver -2 0.7025 69.27 0.5794 0.6966 0.6749 64.09 0.5196 0.6571
DoS SDE-Solver -3 0.7025 69.42 0.5781 0.6985 0.6776 64.40 0.5214 0.6618
LR Image
 ResShift
 DoSSR
LR Image
 ResShift
 DoSSR
Figure 5: Qualitative comparisons between our retrained DoSSR and ResShift. The utilization of the
diffusion prior noticeably enhances the realism and visual appeal of the generated high-resolution
images. Please zoom in for a better view.
Figure 6: The overall framework of DoSSR. During training, we introduce noise to facilitate the
gradual transition from the HR to LR domain, integrating it with the standard diffusion process,
and incorporate preprocessed LR as a conditioning input for the denoising process, following the
ControlNet approach. During inference, we add noise to LR latent according to Eq. (2)and perform
inference starting from t1.
24LR Image
Zoomed LR
 BSRGAN
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 DiffBIR-50
 SeeSR-50
 DoSSR(ours)-5
LR Image
Zoomed LR
 BSRGAN
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 DiffBIR-50
 SeeSR-50
 DoSSR(ours)-5
LR Image
Zoomed LR
 BSRGAN
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 DiffBIR-50
 SeeSR-50
 DoSSR(ours)-5
LR Image
Zoomed LR
 BSRGAN
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 DiffBIR-50
 SeeSR-50
 DoSSR(ours)-5
LR Image
Zoomed LR
 BSRGAN
 RealESRGAN
 LDL
 DASR
StableSR-200
 ResShift-15
 DiffBIR-50
 SeeSR-50
 DoSSR(ours)-5Figure 7: Qualitative comparisons of different steps of our DoSSR and other diffusion-based SR
methods. The suffix "-N" appended to the method name indicates the number of inference steps.
Please zoom in for a better view.
25LR Image
 DiffBIR-50
 (seed = 512)
DiffBIR-50
 (seed = 13)
SeeSR-50
 (seed = 12)
SeeSR-50
 (seed = 42)
ResShift-15
 (seed = 512)
ResShift-15
 (seed = 42)
DoSSR-5
 (seed = 512)
DoSSR-5
 (seed = 321)Figure 8: Visualizing the impact of random seed on diffusion-based methods. The "-N" suffix denotes
inference steps. Please zoom in for a better view.
Zoomed LR
 DoSSR-onestep
 DoSSR-3
 DoSSR-5
 DoSSR-7
Figure 9: Qualitative comparisons of different inference steps of our DoSSR. The "-N" suffix denotes
inference steps. Please zoom in for a better view.
Zoomed LR
 order=1, step=5
 order=1, step=10
 order=2, step=5
 order=3, step=5
Figure 10: Qualitative comparisons of different sampler orders of our DoSSR. Please zoom in for a
better view.
26NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the
paper‚Äôs contributions and scope?
Answer: [Yes]
Justification: The abstract provides a concise summary of the main findings and contributions
of the paper, while the introduction elaborates on the problem statement and research
objectives, thereby clarifying the contributions.
Guidelines:
‚Ä¢The answer NA means that the abstract and introduction do not include the claims
made in the paper.
‚Ä¢The abstract and/or introduction should clearly state the claims made, including the
contributions made in the paper and important assumptions and limitations. A No or
NA answer to this question will not be perceived well by the reviewers.
‚Ä¢The claims made should match theoretical and experimental results, and reflect how
much the results can be expected to generalize to other settings.
‚Ä¢It is fine to include aspirational goals as motivation as long as it is clear that these goals
are not attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: In Appendix 5, we expound upon the limitations of the work conducted and
provide a brief discussion thereof.
Guidelines:
‚Ä¢The answer NA means that the paper has no limitation while the answer No means that
the paper has limitations, but those are not discussed in the paper.
‚Ä¢ The authors are encouraged to create a separate "Limitations" section in their paper.
‚Ä¢The paper should point out any strong assumptions and how robust the results are to
violations of these assumptions (e.g., independence assumptions, noiseless settings,
model well-specification, asymptotic approximations only holding locally). The authors
should reflect on how these assumptions might be violated in practice and what the
implications would be.
‚Ä¢The authors should reflect on the scope of the claims made, e.g., if the approach was
only tested on a few datasets or with a few runs. In general, empirical results often
depend on implicit assumptions, which should be articulated.
‚Ä¢The authors should reflect on the factors that influence the performance of the approach.
For example, a facial recognition algorithm may perform poorly when image resolution
is low or images are taken in low lighting. Or a speech-to-text system might not be
used reliably to provide closed captions for online lectures because it fails to handle
technical jargon.
‚Ä¢The authors should discuss the computational efficiency of the proposed algorithms
and how they scale with dataset size.
‚Ä¢If applicable, the authors should discuss possible limitations of their approach to
address problems of privacy and fairness.
‚Ä¢While the authors might fear that complete honesty about limitations might be used by
reviewers as grounds for rejection, a worse outcome might be that reviewers discover
limitations that aren‚Äôt acknowledged in the paper. The authors should use their best
judgment and recognize that individual actions in favor of transparency play an impor-
tant role in developing norms that preserve the integrity of the community. Reviewers
will be specifically instructed to not penalize honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and
a complete (and correct) proof?
27Answer: [Yes]
Justification: In Appendix A, we provide detailed mathematical derivations for all the
formulas appearing in the paper.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include theoretical results.
‚Ä¢All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
‚Ä¢All assumptions should be clearly stated or referenced in the statement of any theorems.
‚Ä¢The proofs can either appear in the main paper or the supplemental material, but if
they appear in the supplemental material, the authors are encouraged to provide a short
proof sketch to provide intuition.
‚Ä¢Inversely, any informal proof provided in the core of the paper should be complemented
by formal proofs provided in appendix or supplemental material.
‚Ä¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main ex-
perimental results of the paper to the extent that it affects the main claims and/or conclusions
of the paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: In Section 4.1, we introduced the details of experimental setup and model
training to ensure reproducibility.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢If the paper includes experiments, a No answer to this question will not be perceived
well by the reviewers: Making the paper reproducible is important, regardless of
whether the code and data are provided or not.
‚Ä¢If the contribution is a dataset and/or model, the authors should describe the steps taken
to make their results reproducible or verifiable.
‚Ä¢Depending on the contribution, reproducibility can be accomplished in various ways.
For example, if the contribution is a novel architecture, describing the architecture fully
might suffice, or if the contribution is a specific model and empirical evaluation, it may
be necessary to either make it possible for others to replicate the model with the same
dataset, or provide access to the model. In general. releasing code and data is often
one good way to accomplish this, but reproducibility can also be provided via detailed
instructions for how to replicate the results, access to a hosted model (e.g., in the case
of a large language model), releasing of a model checkpoint, or other means that are
appropriate to the research performed.
‚Ä¢While NeurIPS does not require releasing code, the conference does require all submis-
sions to provide some reasonable avenue for reproducibility, which may depend on the
nature of the contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how
to reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe
the architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should
either be a way to access this model for reproducing the results or a way to reproduce
the model (e.g., with an open-source dataset or instructions for how to construct
the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case
authors are welcome to describe the particular way they provide for reproducibility.
In the case of closed-source models, it may be that access to the model is limited in
some way (e.g., to registered users), but it should be possible for other researchers
to have some path to reproducing or verifying the results.
5.Open access to data and code
28Question: Does the paper provide open access to the data and code, with sufficient instruc-
tions to faithfully reproduce the main experimental results, as described in supplemental
material?
Answer: [No]
Justification: The code will be released once the submission is accepted.
Guidelines:
‚Ä¢ The answer NA means that paper does not include experiments requiring code.
‚Ä¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/
public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢While we encourage the release of code and data, we understand that this might not be
possible, so ‚ÄúNo‚Äù is an acceptable answer. Papers cannot be rejected simply for not
including code, unless this is central to the contribution (e.g., for a new open-source
benchmark).
‚Ä¢The instructions should contain the exact command and environment needed to run to
reproduce the results. See the NeurIPS code and data submission guidelines ( https:
//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.
‚Ä¢The authors should provide instructions on data access and preparation, including how
to access the raw data, preprocessed data, intermediate data, and generated data, etc.
‚Ä¢The authors should provide scripts to reproduce all experimental results for the new
proposed method and baselines. If only a subset of experiments are reproducible, they
should state which ones are omitted from the script and why.
‚Ä¢At submission time, to preserve anonymity, the authors should release anonymized
versions (if applicable).
‚Ä¢Providing as much information as possible in supplemental material (appended to the
paper) is recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: In Section 4.1, we introduced the details of experimental setup and model
training, and conducted ablation experiments in Section 4.3 to elucidate the selection of
hyperparameters.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The experimental setting should be presented in the core of the paper to a level of detail
that is necessary to appreciate the results and make sense of them.
‚Ä¢The full details can be provided either with the code, in appendix, or as supplemental
material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [NA]
Justification: The experiments conducted in our paper do not involve the use of error bars or
statistical significance analysis, thus this aspect is not applicable to our study.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The authors should answer "Yes" if the results are accompanied by error bars, confi-
dence intervals, or statistical significance tests, at least for the experiments that support
the main claims of the paper.
29‚Ä¢The factors of variability that the error bars are capturing should be clearly stated (for
example, train/test split, initialization, random drawing of some parameter, or overall
run with given experimental conditions).
‚Ä¢The method for calculating the error bars should be explained (closed form formula,
call to a library function, bootstrap, etc.)
‚Ä¢ The assumptions made should be given (e.g., Normally distributed errors).
‚Ä¢It should be clear whether the error bar is the standard deviation or the standard error
of the mean.
‚Ä¢It is OK to report 1-sigma error bars, but one should state it. The authors should
preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis
of Normality of errors is not verified.
‚Ä¢For asymmetric distributions, the authors should be careful not to show in tables or
figures symmetric error bars that would yield results that are out of range (e.g. negative
error rates).
‚Ä¢If error bars are reported in tables or plots, The authors should explain in the text how
they were calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the com-
puter resources (type of compute workers, memory, time of execution) needed to reproduce
the experiments?
Answer: [Yes]
Justification: For the latency testing experiments, we furnished detailed specifications of the
GPU models used along with their corresponding tasks. Furthermore, we included specific
information regarding the model training batch size and the number of training epochs.
Guidelines:
‚Ä¢ The answer NA means that the paper does not include experiments.
‚Ä¢The paper should indicate the type of compute workers CPU or GPU, internal cluster,
or cloud provider, including relevant memory and storage.
‚Ä¢The paper should provide the amount of compute required for each of the individual
experimental runs as well as estimate the total compute.
‚Ä¢The paper should disclose whether the full research project required more compute
than the experiments reported in the paper (e.g., preliminary or failed experiments that
didn‚Äôt make it into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the
NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
Justification: We have carefully reviewed the NeurIPS Code of Ethics and adhere to its
principles.
Guidelines:
‚Ä¢The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
‚Ä¢If the authors answer No, they should explain the special circumstances that require a
deviation from the Code of Ethics.
‚Ä¢The authors should make sure to preserve anonymity (e.g., if there is a special consid-
eration due to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative
societal impacts of the work performed?
Answer: [Yes]
Justification: We discuss both potential positive societal impacts and negative societal
impacts of the work performed in Appendix.
30Guidelines:
‚Ä¢ The answer NA means that there is no societal impact of the work performed.
‚Ä¢If the authors answer NA or No, they should explain why their work has no societal
impact or why the paper does not address societal impact.
‚Ä¢Examples of negative societal impacts include potential malicious or unintended uses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g., deployment of technologies that could make decisions that unfairly impact specific
groups), privacy considerations, and security considerations.
‚Ä¢The conference expects that many papers will be foundational research and not tied
to particular applications, let alone deployments. However, if there is a direct path to
any negative applications, the authors should point it out. For example, it is legitimate
to point out that an improvement in the quality of generative models could be used to
generate deepfakes for disinformation. On the other hand, it is not needed to point out
that a generic algorithm for optimizing neural networks could enable people to train
models that generate Deepfakes faster.
‚Ä¢The authors should consider possible harms that could arise when the technology is
being used as intended and functioning correctly, harms that could arise when the
technology is being used as intended but gives incorrect results, and harms following
from (intentional or unintentional) misuse of the technology.
‚Ä¢If there are negative societal impacts, the authors could also discuss possible mitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanisms for monitoring misuse, mechanisms to monitor how a system learns from
feedback over time, improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible
release of data or models that have a high risk for misuse (e.g., pretrained language models,
image generators, or scraped datasets)?
Answer: [NA]
Justification: Our paper poses no such risks.
Guidelines:
‚Ä¢ The answer NA means that the paper poses no such risks.
‚Ä¢Released models that have a high risk for misuse or dual-use should be released with
necessary safeguards to allow for controlled use of the model, for example by requiring
that users adhere to usage guidelines or restrictions to access the model or implementing
safety filters.
‚Ä¢Datasets that have been scraped from the Internet could pose safety risks. The authors
should describe how they avoided releasing unsafe images.
‚Ä¢We recognize that providing effective safeguards is challenging, and many papers do
not require this, but we encourage authors to take this into account and make a best
faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in
the paper, properly credited and are the license and terms of use explicitly mentioned and
properly respected?
Answer: [Yes]
Justification: The creators or original owners of assets, such as code, data, or models, used
in the paper, are properly credited. Additionally, the license and terms of use associated
with these assets are explicitly mentioned and respected in accordance with ethical and legal
standards.
Guidelines:
‚Ä¢ The answer NA means that the paper does not use existing assets.
‚Ä¢ The authors should cite the original paper that produced the code package or dataset.
31‚Ä¢The authors should state which version of the asset is used and, if possible, include a
URL.
‚Ä¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
‚Ä¢For scraped data from a particular source (e.g., website), the copyright and terms of
service of that source should be provided.
‚Ä¢If assets are released, the license, copyright information, and terms of use in the
package should be provided. For popular datasets, paperswithcode.com/datasets
has curated licenses for some datasets. Their licensing guide can help determine the
license of a dataset.
‚Ä¢For existing datasets that are re-packaged, both the original license and the license of
the derived asset (if it has changed) should be provided.
‚Ä¢If this information is not available online, the authors are encouraged to reach out to
the asset‚Äôs creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: Our paper does not release new assets.
Guidelines:
‚Ä¢ The answer NA means that the paper does not release new assets.
‚Ä¢Researchers should communicate the details of the dataset/code/model as part of their
submissions via structured templates. This includes details about training, license,
limitations, etc.
‚Ä¢The paper should discuss whether and how consent was obtained from people whose
asset is used.
‚Ä¢At submission time, remember to anonymize your assets (if applicable). You can either
create an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as
well as details about compensation (if any)?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Including this information in the supplemental material is fine, but if the main contribu-
tion of the paper involves human subjects, then as much detail as possible should be
included in the main paper.
‚Ä¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
Subjects
Question: Does the paper describe potential risks incurred by study participants, whether
such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)
approvals (or an equivalent approval/review based on the requirements of your country or
institution) were obtained?
Answer: [NA]
Justification: Our paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
32‚Ä¢The answer NA means that the paper does not involve crowdsourcing nor research with
human subjects.
‚Ä¢Depending on the country in which research is conducted, IRB approval (or equivalent)
may be required for any human subjects research. If you obtained IRB approval, you
should clearly state this in the paper.
‚Ä¢We recognize that the procedures for this may vary significantly between institutions
and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the
guidelines for their institution.
‚Ä¢For initial submissions, do not include any information that would break anonymity (if
applicable), such as the institution conducting the review.
33