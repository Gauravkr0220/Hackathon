Under review as submission to TMLR
Label Distribution Shift-Aware Prediction Refinement
for Test-Time Adaptation
Anonymous authors
Paper under double-blind review
Abstract
Test-time adaptation (TTA) is an effective approach to mitigate performance degradation of
trained models when encountering input distribution shifts at test time. However, existing
TTA methods often suffer significant performance drops when facing additional class distri-
bution shifts. We first analyze TTA methods under label distribution shifts and identify the
presence of class-wise confusion patterns commonly observed across different covariate shifts.
Based on this observation, we introduce label Distribution shift-Aware prediction Refinement
for Test-time adaptation (DART) , a novel TTA method that refines the predictions by focus-
ing on class-wise confusion patterns. DART trains a prediction refinement module during an
intermediate time by exposing it to several batches with diverse class distributions using
the training dataset. This module is then used during test time to detect and correct class
distribution shifts, significantly improving pseudo-label accuracy for test data. Our method
exhibits 5-18% gains in accuracy under label distribution shifts on CIFAR-10C, without any
performance degradation when there is no label distribution shift. Extensive experiments
on CIFAR, PACS, OfficeHome, and ImageNet benchmarks demonstrate DARTâ€™s ability to
correct inaccurate predictions caused by test-time distribution shifts. This improvement
leads to enhanced performance in existing TTA methods, making DART a valuable plug-in
tool.
1 Introduction
Deep learning has achieved remarkable success across various tasks, including image classification (Krizhevsky
et al., 2012; Radford et al., 2021; Simonyan & Zisserman, 2014) and natural language processing (Devlin et al.,
2018; Vaswani et al., 2017). However, these models often suffer from significant performance degradation
when there is a substantial shift between the training and test data distributions (Mendonca et al., 2020;
Saenko et al., 2010; Taori et al., 2020). Test-time adaptation (TTA) methods (Boudiaf et al., 2022; Goyal
et al., 2022; Jang et al., 2022; Wang et al., 2020; Zhao et al., 2022) have emerged as a prominent solution to
mitigate performance degradation due to distribution shifts. TTA methods allow trained models to adapt to
the test domains using only unlabeled test data. In particular, an approach known as BNAdapt (Nado et al.,
2020; Schneider et al., 2020), which substitutes the batch statistics in the batch normalization (BN) layers of
a trained classifier with those from the test batch, has proven to be effective in adapting to input distribution
shifts in scenarios such as image corruption. Consequently, numerous TTA methods (Lee, 2013; Wang et al.,
2020; Zhao et al., 2022; Zhou et al., 2023) are built upon the BNAdapt strategy.
However, recent studies (Gong et al., 2022; Niu et al., 2023; Zhou et al., 2023) have shown that the effectiveness
of the BNAdapt strategy diminishes or even becomes harmful when both the class distribution and the input
distribution in the test domain shift at test time. This reduction in effectiveness is due to BNAdaptâ€™s reliance
on batch-level data statistics, which are influenced not only by class-conditional input distributions but
also by the configuration of data within the test batches. For instance, when a test batch predominantly
contains samples from a few head classes, the batch statistics become biased towards these classes. To address
these limitations, some recent methods have been designed to lessen the dependency of test-time adaptation
strategies on batch-level statistics and to tackle class distribution shifts with additional adjustments. For
instance, NOTE (Gong et al., 2022) employs instance-aware batch normalization, which diverges from
1Under review as submission to TMLR
0 1 2 3 4 5 6 7 8 9
Class index0.00.10.20.30.40.5ptest(y)
0.404
0.242
0.145
0.087
0.052
0.031
0.019
0.011
0.006
0.004
 0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label63.2 0.1 2.8 3.3 3.7 2.0 3.4 4.7 10.9 5.9
0.0 83.3 0.2 0.2 0.0 0.5 1.8 0.5 2.5 11.0
0.0 0.0 82.5 3.1 5.0 1.9 4.5 2.5 0.6 0.0
0.0 0.0 0.0 77.2 2.8 14.4 4.2 0.5 0.5 0.5
0.0 0.0 0.0 1.6 86.8 4.7 3.1 3.1 0.8 0.0
0.0 0.0 0.0 1.3 2.6 92.2 1.3 2.6 0.0 0.0
0.0 0.0 0.0 0.0 2.2 0.0 97.8 0.0 0.0 0.0
0.0 0.0 0.0 0.0 3.7 0.0 0.0 96.3 0.0 0.0
6.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 93.8 0.0
0.0 0.0 10.0 0.0 0.0 0.0 0.0 0.0 0.0 90.0clean CIFAR-10
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label48.6 0.3 6.6 5.5 6.3 1.9 5.3 5.6 13.0 6.9
0.2 74.1 0.5 0.8 0.7 1.5 1.2 2.0 5.0 14.0
0.0 0.0 63.0 5.0 8.4 6.4 10.6 5.0 0.3 1.4
0.0 0.0 1.9 56.3 4.2 20.5 10.2 3.7 0.5 2.8
0.0 0.0 2.3 5.4 73.6 4.7 8.5 5.4 0.0 0.0
0.0 0.0 0.0 6.5 5.2 77.9 3.9 6.5 0.0 0.0
0.0 0.0 0.0 2.2 0.0 2.2 93.5 2.2 0.0 0.0
0.0 0.0 0.0 0.0 3.7 3.7 0.0 88.9 0.0 3.7
6.2 0.0 0.0 0.0 0.0 0.0 0.0 0.0 87.5 6.2
0.0 0.0 10.0 0.0 0.0 0.0 0.0 0.0 0.0 90.0CIFAR-10C (Gaussian noise)
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label58.9 0.2 4.0 4.1 4.0 2.3 3.7 5.4 11.2 6.2
0.3 79.1 0.5 0.8 0.3 1.0 1.8 1.2 2.5 12.4
0.0 0.0 78.6 2.8 5.6 2.2 7.2 2.8 0.8 0.0
0.0 0.0 0.0 69.8 5.1 16.3 3.7 3.7 0.0 1.4
0.0 0.0 0.0 1.6 86.8 5.4 3.1 2.3 0.0 0.8
0.0 0.0 1.3 1.3 2.6 92.2 1.3 1.3 0.0 0.0
0.0 0.0 0.0 0.0 2.2 0.0 97.8 0.0 0.0 0.0
0.0 0.0 0.0 0.0 3.7 0.0 0.0 96.3 0.0 0.0
0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0100.0 0.0
0.0 0.0 10.0 0.0 0.0 0.0 0.0 0.0 0.0 90.0CIFAR-10C (Defocus blur)
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label53.7 0.2 3.8 5.6 5.7 2.2 4.3 5.4 12.8 6.3
0.2 77.1 0.7 0.7 0.0 1.7 2.5 0.8 2.8 13.5
0.3 0.0 70.5 4.7 5.6 5.0 7.5 3.3 1.7 1.4
0.0 0.0 0.5 69.3 4.7 13.5 5.6 3.3 0.9 2.3
0.0 0.0 2.3 0.8 84.5 2.3 3.1 6.2 0.0 0.8
0.0 0.0 0.0 3.9 3.9 84.4 3.9 3.9 0.0 0.0
0.0 0.0 0.0 0.0 2.2 0.0 95.7 2.2 0.0 0.0
0.0 0.0 0.0 3.7 0.0 3.7 0.0 92.6 0.0 0.0
0.0 0.0 0.0 12.5 0.0 0.0 0.0 6.2 81.2 0.0
0.0 0.0 0.0 0.0 0.0 0.0 10.0 0.0 0.0 90.0CIFAR-10C (Snow)
0 1 2 3 4 5 6 7 8 9
Class index0.00.10.20.30.40.5ptest(y)
0.004
0.006
0.011
0.019
0.031
0.052
0.087
0.145
0.242
0.404
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=10)
1(n=16)
2(n=27)
3(n=46)
4(n=77)
5(n=129)
6(n=215)
7(n=359)
8(n=599)
9(n=1000)True label100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
0.0100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
0.0 0.0 88.9 0.0 11.1 0.0 0.0 0.0 0.0 0.0
0.0 0.0 0.0 89.1 6.5 2.2 2.2 0.0 0.0 0.0
0.0 0.0 0.0 2.6 96.1 0.0 1.3 0.0 0.0 0.0
1.6 0.0 1.6 8.5 2.3 84.5 0.8 0.8 0.0 0.0
0.9 0.0 3.7 2.8 0.0 0.0 92.6 0.0 0.0 0.0
0.8 0.0 3.1 0.3 5.8 2.5 0.0 87.5 0.0 0.0
10.5 1.2 4.2 2.3 0.5 0.8 1.7 0.0 78.8 0.0
8.7 12.2 3.3 5.4 1.0 1.8 1.5 0.5 0.3 65.3clean CIFAR-10
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=10)
1(n=16)
2(n=27)
3(n=46)
4(n=77)
5(n=129)
6(n=215)
7(n=359)
8(n=599)
9(n=1000)True label90.0 0.0 0.0 0.0 0.0 0.0 10.0 0.0 0.0 0.0
0.0 87.5 12.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0
3.7 0.0 77.8 3.7 11.1 3.7 0.0 0.0 0.0 0.0
0.0 0.0 10.9 73.9 6.5 6.5 2.2 0.0 0.0 0.0
0.0 0.0 5.2 2.6 87.0 2.6 2.6 0.0 0.0 0.0
2.3 0.0 3.9 13.2 8.5 68.2 2.3 1.6 0.0 0.0
1.4 0.0 5.1 4.2 4.2 1.4 83.7 0.0 0.0 0.0
1.1 0.0 5.8 2.8 10.6 5.6 0.8 73.3 0.0 0.0
13.0 2.0 7.5 4.8 5.7 1.0 2.3 0.3 63.3 0.0
8.8 15.9 3.5 4.7 1.9 3.8 2.3 1.7 1.6 55.8CIFAR-10C (Gaussian noise)
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=10)
1(n=16)
2(n=27)
3(n=46)
4(n=77)
5(n=129)
6(n=215)
7(n=359)
8(n=599)
9(n=1000)True label100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
0.0 93.8 0.0 6.2 0.0 0.0 0.0 0.0 0.0 0.0
0.0 0.0 88.9 0.0 11.1 0.0 0.0 0.0 0.0 0.0
2.2 0.0 2.2 84.8 6.5 2.2 2.2 0.0 0.0 0.0
0.0 0.0 0.0 0.0 98.7 0.0 1.3 0.0 0.0 0.0
0.8 0.0 2.3 10.9 0.0 85.3 0.8 0.0 0.0 0.0
0.5 0.0 4.7 3.3 1.9 0.0 89.8 0.0 0.0 0.0
1.1 0.0 2.8 0.8 7.0 3.3 0.0 85.0 0.0 0.0
10.5 2.0 4.5 4.3 1.7 1.0 2.0 0.2 73.8 0.0
8.2 12.3 4.5 6.1 1.6 2.3 1.6 0.7 0.6 62.1CIFAR-10C (Defocus blur)
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=10)
1(n=16)
2(n=27)
3(n=46)
4(n=77)
5(n=129)
6(n=215)
7(n=359)
8(n=599)
9(n=1000)True label90.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 10.0 0.0
0.0100.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
3.7 0.0 77.8 7.4 11.1 0.0 0.0 0.0 0.0 0.0
4.3 0.0 2.2 82.6 2.2 6.5 2.2 0.0 0.0 0.0
0.0 0.0 2.6 3.9 89.6 1.3 1.3 1.3 0.0 0.0
0.8 0.0 7.8 13.2 3.1 73.6 0.8 0.8 0.0 0.0
0.9 0.0 4.7 2.3 2.3 0.0 89.8 0.0 0.0 0.0
0.8 0.6 3.1 3.3 10.9 2.8 0.0 78.3 0.3 0.0
16.2 1.8 7.8 5.7 2.2 0.8 1.8 0.5 63.1 0.0
9.7 13.3 4.8 5.1 1.6 3.0 2.3 1.1 1.5 57.6CIFAR-10C (Snow)
Figure 1: Confusion patterns of BN-adapted classifier due to test-time label distribution shifts .
We present class-wise confusion matrices of BN-adapted classifiers, initially trained on class-balanced CIFAR-
10 and then tested on CIFAR-10C with two long-tailed distributions (first column). The second column shows
confusion patterns on the CIFAR-10 test dataset with only label shifts, while the third to fifth columns display
patterns on CIFAR-10C under three types of corruptions combined with label shifts. Class pairs where the
confusion rate exceeds 11% are highlighted in red. There is significant accuracy degradation in head classes
(e.g., class 0 in the 1st row and class 9 in the 2nd row). Additionally, similar class-wise confusion patterns are
observed across different types of corruption for each label distribution shift (each row). Confusion matrices
for other 15 types of corruption and class imbalance ratios of Ï= 1,10,100are also reported in Figure 9â€“11.
traditional batch normalization, and uses a prediction-balanced memory to simulate a class-balanced test
data stream. SAR (Niu et al., 2023) implements batch-agnostic normalization layers, such as group or layer
norm, complemented by techniques designed to drive the model toward a flat minimum, enhancing robustness
against noisy test samples. However, these methods still rely on pseudo labels of test samples, and their
effectiveness is fundamentally limited by the accuracy of these pseudo labels, which particularly deteriorates
under severe label distribution shifts.
In this work, we propose a more direct, simple yet effective method that can correct the inaccurate predictions
generated by the BNAdapt strategy under label distribution shifts. Our method can be integrated with a
variety of existing TTA methods (Boudiaf et al., 2022; Gong et al., 2022; Lee, 2013; Niu et al., 2023; Schneider
et al., 2020; Wang et al., 2020; Zhao et al., 2022; Zhou et al., 2023), significantly enhancing their accuracies by
5-18 percentage points under label distribution shifts, while having minimal impact when label distributions
remain unchanged. We achieve this by first identifying the existence of class-wise confusion patterns , i.e.,
specific misclassification trends between classes that the BNAdapt classifier exhibits under label distribution
shifts, regardless of different input corruption patterns (Fig. 1). In particular, we show that these confusion
patterns depend not only on the inter-class relationships but also on the magnitude and direction of the label
distribution shifts from a class-balanced training dataset. Building on this insight, it might seem natural to
correct the BN-adapted classifierâ€™s inaccurate predictions by applying a simple affine transformation that
reverses the effects of class-wise confusion as in learning with label noise (Natarajan et al., 2013; Patrini et al.,
2017; Zhu et al., 2021). However, a significant challenge arises because these class-wise confusion patterns are
difficult to deduce using only the unlabeled test data, which has unknown label distributions. Additionally,
these patterns can vary over time with online label distribution shifts, further complicating the adaptation
process.
To tackle these challenges, we propose a novel method, named label Distribution shift-Aware prediction
Refinement for Test-time adaptation (DART) , which detects test-time label distribution shifts and corrects
the inaccurate predictions of BN-adapted classifier through an effective affine transformation depending on
2Under review as submission to TMLR
ğ‘Šğ‘Šâ„¬âˆˆâ„ğ¾ğ¾Ã—ğ¾ğ¾
ğ‘ğ‘â„¬âˆˆâ„1Ã—ğ¾ğ¾Ì…ğ‘ğ‘â„¬âˆˆâ„ğ¾ğ¾
ğ‘”ğ‘”ğœ™ğœ™
Refined prediction: ğ‘ğ‘ğ‘–ğ‘–=softmax (ğ‘™ğ‘™ğ‘–ğ‘–ğ‘Šğ‘Šâ„¬+ğ‘ğ‘â„¬)
ğ‘¥ğ‘¥ğ‘–ğ‘–
ğ‘“ğ‘“ï¿½ğœƒğœƒ
ğ‘“ğ‘“ï¿½ğœƒğœƒğ‘™ğ‘™ğ‘–ğ‘–
Ì…ğ‘ğ‘â„¬,ğ‘‘ğ‘‘â„¬Logit
ğ‘¦ğ‘¦ğ‘–ğ‘–
â„’CEğ’Ÿğ’Ÿtrain
Shared
Prediction refinement module
Gradient ğ‘‘ğ‘‘â„¬âˆˆâ„
High: mild shift 
Low: severe shiftâ„¬
â‹®
Diverse label distributions 
via Dirichlet sampling
Figure 2: Intermediate time training of DART. At intermediate time, the period between the training
and test times, DART trains a prediction refinement module gÏ•to correct the inaccurate prediction caused
by the class distribution shifts. (left)By sampling the training data from Dirichlet distributions, we generate
batches with diverse class distributions. (right)The prediction refinement module gÏ•takes the averaged
pseudo label distribution Â¯pBand prediction deviation dBfor each batchB, and outputs a square matrix WB
and a vector bBof sizeK(class numbers). Using labels of the training data, we optimize gÏ•to minimize
the cross-entropy loss between labels yand the refined prediction q=softmax (fÂ¯Î¸(x)WB+bB)for samples
(x,y)âˆˆBfor the BN-adapted classifier fÂ¯Î¸.
the label distribution shifts. Our key insight is that the model can learn how to adjust inaccurate predictions
caused by label distribution shifts by experiencing several batches with diverse class distributions using the
labeled training dataset before the start of test time. DART trains a prediction refinement module during an
intermediate time , positioned between the end of training and the start of testing, by exposing multiple batches
of labeled training data with diverse class distributions, sampled from the Dirichlet distribution (Figure 2).
The module uses two inputs to detect label distribution shifts: the averaged pseudo-label distribution of each
batch generated by the BN-adapted classifier and a new metric called prediction deviation , which measures
average deviations of each sampleâ€™s soft pseudo-label from uniformity to gauge the confidence of predictions
made by the BN-adapted classifier. The module is trained to generate two outputs, a square matrix and a
vector of class dimensions, which together transform the logit vector for prediction refinement. Since this
module requires only soft pseudo-labels for test samples, it can be readily employed during test time using
the pre-trained BN-adapted model. This approach enables DART to dynamically adapt to label distribution
shifts at test time, enhancing the accuracy of predictions.
We evaluate the effectiveness of DART across a wide range of TTA benchmarks, including CIFAR-10/100C,
ImageNet-C, CIFAR-10.1, PACS, and OfficeHome. Our results consistently demonstrate that DART signifi-
cantly improves prediction accuracy across these benchmarks, particularly in scenarios involving test-time
distribution shifts in both input and label distributions. This enhancement also boosts the performance
of existing TTA methods, establishing DART as a valuable plug-in tool (Table 1). Specifically, DART
achieves notable improvements in test accuracy, enhancing the BNAdapt method by 5.7 %and 18.1 %on
CIFAR-10C-LT under class imbalance ratios of Ï= 10and100, respectively. Additionally, our ablation
studies highlight the critical role of the prediction refinement moduleâ€™s design, particularly its inputs and
outputs, in enhancing DARTâ€™s effectiveness.
2 Label Distribution Shifts on TTA
TTA methods are extensively investigated to mitigate input data distribution shifts, yet the impact of label
distribution shifts on these methods is less explored. In this section, we theoretically and experimentally
analyze the impact of label distribution shifts on BNAdapt (Nado et al., 2020; Schneider et al., 2020), a
foundational strategy for many TTA methods (Lee, 2013; Wang et al., 2020; Zhao et al., 2022; Zhou et al.,
2023). We also introduce a new metric designed to detect label distribution shifts at test time using only
unlabeled test samples.
3Under review as submission to TMLR
Impact of label distribution shifts on TTA We begin with a toy example to explore the impact
of test-time distribution shifts on a classifier trained for a four-class Gaussian mixture distribution. We
assume that the classifier is adapted at test time by a mean centering function, modeling the effect of
batch normalization in the BNAdapt. Consider the input distribution for class iasN(Âµi,Ïƒ2I2), where
Âµ1= (d,Î²d),Âµ2= (âˆ’d,Î²d),Âµ3= (d,âˆ’Î²d),andÂµ4= (âˆ’d,âˆ’Î²d)withdâˆˆR2andÎ² >1. Assume uniform
priorsptr= [1/4,1/4,1/4,1/4]for training. At test time, we consider shifts in both input and label
distributions. Consider the covariate shift by âˆ†âˆˆR2as studied in prior works (Stojanov et al., 2021; Yi
et al., 2023). This changes the input distribution for each class to N(Âµi+ âˆ†,Ïƒ2I2). Additionally, let the class
distribution shift to pte= [p,1/4,1/4,1/2âˆ’p]for somepâˆˆ[1/4,1/2). Leth(Â·)denote a Bayes classifier for the
training distribution, and Norm (Â·)represent a mean centering function, mimicking the batch normalization
at test time. When p= 1/4(i.e.,ptr=pte), the mean centering effectively mitigates the test-time input
distribution shift, restoring the original performance of h(Â·). However, when p>1/4(i.e.,ptrÌ¸=pte), the
BN-adapted classifier, modeled by Â¯h(Â·) :=h(Norm (Â·)), begins to exhibit performance degradation with a
distinct class-wise confusion pattern. This pattern reflects both the spatial distances between class means
and the severity of the label distribution shifts:
(#1)The misclassification probability from the major class (class 1) to a minor class is higher than the
reverse: Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2I2)[Â¯h(x) =i]>Prxâˆ¼N(Âµi+âˆ†,Ïƒ2I2)[Â¯h(x) = 1],âˆ€iÌ¸= 1.
(#2)The misclassification patterns are influenced by the spatial relationships between classes, e.g., the
probability of misclassifying from class 1 to other classes is higher for those that are spatially closer:
Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2I2)[Â¯h(x) = 2]>Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2I2)[Â¯h(x) = 3]>Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2I2)[Â¯h(x) = 4].
(#3)As the class distribution imbalance p>1/4increases, the rate at which misclassification towards spa-
tially closer classes increases is greater than towards more distant classes:âˆ‚
âˆ‚pPrxâˆ¼N(Âµ1+âˆ†,Ïƒ2I2)[Â¯h(x) =
2]>âˆ‚
âˆ‚pPrxâˆ¼N(Âµ1+âˆ†,Ïƒ2I2)[Â¯h(x) = 3].
More additional explanations for these confusion patterns including the symbol table, illustrations, and
detailed proofs are described in Appendix D. Through experiments with real datasets, we reaffirm the impact
of label distribution shifts on a BN-adapted classifier. We consider a BN-adapted classifier originally trained
on a class-balanced CIFAR-10 dataset (Krizhevsky & Hinton, 2009) and later tested using CIFAR-10-LT and
CIFAR-10C-LT test datasets, which include label distribution shifts. CIFAR-10C (Hendrycks & Dietterich,
2019) serves as a benchmark for assessing model robustness against 15 different predefined types of corruptions,
such as Gaussian noise. To analyze the effects of label distribution shifts, we define the number of samples
for classkasnk=n(1/Ï)k/(Kâˆ’1), wherenis the number of samples for the head class and Ïis the class
imbalance ratio. In Fig. 3, we observe substantial performance drops in test accuracy for BNAdapt (orange)
as the class imbalance ratio Ïincreases. The BNAdapt exhibits even worse performance than NoAdapt (blue)
whenÏ= 100.
In Fig. 1, we provide more detailed analysis for misclassification patterns by presenting confusion matrices
illustratingtheimpactoftwodistinctlong-taileddistributions(eachrow)acrossfourtypesofimagecorruptions
(each column, including clean, Gaussian noise, Defocus blur, and Snow). These matrices show the fraction of
test samples from class iclassified into class j. The trends observed in these real experiments are similar to
those analyzed in our toy example. Firstly, significant accuracy degradation is evident, especially for head
classes (with smaller/larger class indices in the 1st/2nd row, respectively). Secondly, the confusion patterns
remain consistent across different corruption types and reflect class-wise relationships; for example, more
frequent confusion occurs between classes with similar characteristics, such as airplane & ship (0 and 8),
automobile & truck (1 and 9), and cat & dog (3 and 5). Additionally, we present confusion matrices for
CIFAR-10C-LT with various levels of label distribution shifts ( Ï= 1,10,100) in Figures 9-11 of Appendix F,
revealing increasingly pronounced class-wise confusion patterns as the imbalance ratio Ïrises from 1 to 100.
These observations suggest the potential to correct the inaccurate predictions of the BN-adapted classifier by
reversing the effects of class-wise confusion. Similar problem has been considered in robust model training
with label noise (Natarajan et al., 2013; Patrini et al., 2017; Zhu et al., 2021), where attempts have been
made to adjust model outputs using an appropriate affine transformation, reversing the estimated label noise
4Under review as submission to TMLR
1 10 100
Class imbalance 
5060708090T est accuracy (%)NoAdapt
BNAdapt
BNAdapt+DART (ours)
Figure 3: We observe performance degra-
dation of BNAdapt (orange) as the class
imbalance ratio Ïincreases on long-tailed
CIFAR-10C. DART-applied BNAdapt
(green) shows consistently improved per-
formance regardless of class imbalance.
2.30 2.32 2.34 2.36 2.38
D(u,1
|B|
isoftmax(f(xi)))
0.20.30.40.50.60.70.80.9T est accuracy
4.50 4.75 5.00 5.25 5.50 5.75 6.00 6.25
1
|B|
iD(u,softmax(f(xi)))
0.20.30.40.50.60.70.80.9T est accuracy
IR1
IR5
IR20
IR50
IR5000Figure 4: We demonstrate the relationship between (left) test
accuracy and the difference between averaged pseudo label dis-
tribution,1
|B|/summationtext
xiâˆˆBsoftmax (fÂ¯Î¸(xi)), and uniform distribution,
u, and (right) the test accuracy and the prediction deviation
dB, for each batchBwith the BN-adapted classifier for CIFAR-
10C-imb under Gaussian noise of 5 different imbalance ratios.
Each point represents a single batch within the test dataset.
patterns in training dataset. However, a new challenge arises in TTA scenarios where only unlabeled test data
with unknown (online) label distributions are available, which hinders the accurate estimation of class-wise
confusion patterns. In particular, a commonly used label correction scheme in robust learning settings (Zhu
et al., 2021) fails to estimate the confusion matrix resulting from label distribution shifts in TTA scenarios, as
shown in Appendix H. To address these challenges, we next propose a new metric to detect label distribution
shifts and correct the inaccurate predictions from BN-adapted classifiers.
A new metric to detect label distribution shifts in TTA During test time, although we only have
access to unlabeled test samples, we can obtain pseudo soft labels for these samples using the BN-adapted
pre-trained classifier. A common metric for detecting label distribution shifts is the average pseudo-label
distribution of the test samples, as used in previous studies (Park et al., 2023; Zhao et al., 2022; Zhou
et al., 2023). However, we find that relying solely on this metric is insufficient for accurately detecting label
distribution shifts. To illustrate this, we use the CIFAR-10C-imb dataset (Niu et al., 2023), which models
online label distribution shifts with varying severity. CIFAR-10C-imb consists of 10 subsets, each with a
class distribution defined by [p1,p2,...,pK]wherepk=pmaxandpi=pmin= (1âˆ’pmax)/(Kâˆ’1)foriÌ¸=k.
The imbalance ratio (IR) is defined as IR=pmax/pmin. Each subset has 1,000 samples, totaling 10,000 for
the test set (detailed in Appendix A). If the average pseudo-label distribution accurately estimates the shift
from a uniform distribution, the distance D(u,Â¯pB)(using cross-entropy) between the uniform distribution u
and the average pseudo-label distribution Â¯pB:=1
|B|/summationtext
(x,Â·)âˆˆBsoftmax (fÂ¯Î¸(x)), calculated for each test batch B
using the BN-adapted classifier fÂ¯Î¸, should increase with IR. However, as shown in Figure 4 (left), D(u,Â¯pB)
increases from IR 1 (blue points) to 20 (green) but decreases again for more severe shifts (e.g., IR > 50), even
though test accuracy continues to drop. This indicates that the average pseudo-label distribution Â¯pBalone is
inadequate for distinguishing between no shifts (IR 1) and severe shifts (IR 5000). To address this limitation,
we introduce a new metric, prediction deviation (from uniformity) dB=1
|B|/summationtext
(x,Â·)âˆˆBD(u,softmax (fÂ¯Î¸(x))),
which measures the average confidence of predictions for each batch. Figure 4 (right) shows that as IR
increases, both prediction deviation and test accuracy decrease, effectively quantifying the severity of label
distribution shifts. Furthermore, we can now see that the decline in D(u,Â¯pB)under severe shifts is due to
the unconfident predictions, making Â¯pBappear closer to uniform. We use both the average pseudo-label
distribution Â¯pBand prediction variance dBto detect the magnitude and direction of label distribution shifts.
5Under review as submission to TMLR
3 Label Distribution Shift-Aware Prediction Refinement for TTA
We introduce a prediction refinement module that can detect test-time class distribution shifts and modify
the predictions of the trained classifiers to effectively reverse the class-wise confusion patterns.
Prediction refinement module gÏ•Our core idea is that if the prediction refinement module gÏ•experiences
batches with diverse class distributions before the test time, it can develop the ability to refine inaccurate
predictions resulting from label distribution shifts. Based on this insight, we train gÏ•during the intermediate
time, between the training and testing times, by exposing it to several batches with diverse class distributions
from the training datasets. The module gÏ•takes as inputs the average pseudo-label distribution Â¯pB=
1
|B|/summationtext
(x,Â·)âˆˆBsoftmax (fÂ¯Î¸(x))âˆˆRKand prediction variance dB=1
|B|/summationtext
(x,Â·)âˆˆBD(u,softmax (fÂ¯Î¸(x)))âˆˆRfor
each batch, to detect the label distribution shifts from uniformity. The module then outputs a square matrix
WBâˆˆRKÃ—Kand a vector bBâˆˆRK, which together transform the modelâ€™s logits fÂ¯Î¸(x)for a sample xâˆˆBto
fÂ¯Î¸(x)WB+bB.
Training of gÏ•During the intermediate time, we use the labeled training dataset Das the intermediate
datasetDint, assuming thatDis available while the test dataset Dtestremains unavailable, as is common in
previous TTA settings (Choi et al., 2022; Lim et al., 2022; Park et al., 2023). For example, we use CIFAR-10
dataset during the intermediate time on CIFAR-10C-imb benchmark. To create batches with diverse class
distributions during the intermediate time, we employ a Dirichlet distribution (Gong et al., 2022; Yurochkin
et al., 2019). Batches sampled through i.i.d. sampling typically have class distributions resembling a uniform
distribution. In contrast, batches sampled using the Dirichlet distribution exhibit a wide range of class
distributions, including long-tailed distributions, as illustrated in Fig. 6 of Appendix A. If the original training
dataset exhibits an imbalanced class distribution, as seen in datasets like PACS and OfficeHome (Fig. 5),
this imbalance can inadvertently influence the class distributions within batches. To mitigate this, we create
a class-balanced intermediate dataset Dintby uniformly sampling data from each class.
The training objective of gÏ•for a Dirichlet-sampled batch BDirâŠ‚D intis formulated as follows:
Limb(Ï•) =E(x,y)âˆˆBDir[CE(y,softmax (fÂ¯Î¸(x)Wimb(Ï•) +bimb(Ï•)))] (1)
forWimb(Ï•),bimb(Ï•) =gÏ•(Â¯pimb,dimb)where Â¯pimbis the averaged pseudo label distribution, dimbis the
prediction deviation of the batch BDir, andCEdenotes the cross-entropy loss. During the intermediate time,
gÏ•is optimized to minimize the CE loss between the ground truth labels of the training samples and the
modified softmax probability. During this time, the parameters of the trained classifier fÎ¸are not updated, but
the batch statistics in the classifiers are updated as in BNAdapt. Moreover, since gÏ•training is independent
of the test domains, e.g. test corruption types, we train gÏ•only once for each classifier. Therefore, it requires
negligible computation overhead (Appendix A).
Regularization for gÏ•When there is no label distribution shift, there are no significant class-wise confusion
patterns that the module needs to reverse (as shown in Fig. 9). However, when using only equation 1, the
module may become biased towards class-imbalanced situations due to exposure to batches with imbalanced
class configurations from Dirichlet sampling. To prevent this bias, we add a regularization term on the
training objective of gÏ•, encouraging it to produce outputs similar to an identity matrix and a zero vector
when exposed to batches i.i.d. sampled from the class-balanced intermediate dataset Dint. The regularization
for an i.i.d. sampled batch BIIDâŠ‚D intis formulated as
Lbal(Ï•) =E(x,y)âˆˆBIID[MSE (Wbal(Ï•),IKÃ—K) +MSE (bbal(Ï•),0K)] (2)
forWbal(Ï•),bbal(Ï•) =gÏ•(Â¯pbal,dbal).
Overall objective Combining these two losses, we propose a training objective for gÏ•:
L(Ï•) =Limb(Ï•) +Î±Lbal(Ï•), (3)
whereÎ±is a hyperparameter that balances the two losses for class-imbalanced and balanced intermediate
batches. We simply set Î±to 0.1 unless specified. The change in the outputs of gÏ•with respect to different Î±
can be observed in Appendix F. The pseudocode for DART is presented in Appendix B.
6Under review as submission to TMLR
UtilizinggÏ•at test-time Since the prediction refinement module gÏ•only requires the averaged pseudo
label distribution and the prediction deviation as inputs, it can be employed at test time when only unlabeled
test data is available. To generate the affine transformation for prediction refinement, gÏ•uses the outputs
of the BN-adapted pre-trained classifier fÂ¯Î¸0, not the classifier fÎ¸that is continually updated during test
time by some TTA method. This is because gÏ•has been trained to correct the inaccurate predictions of
the BN-adapted classifier caused by label distribution shifts during the intermediate time. Specifically, gÏ•
generatesWBandbBfor prediction refinement of a test batch B:
WB,bB=gÏ•(Â¯pB,dB)where Â¯pB=1
|B|/summationdisplay
Ë†xâˆˆBsoftmax (fÂ¯Î¸0(Ë†x)),dB=1
|B|/summationdisplay
Ë†xâˆˆBD(u,softmax (fÂ¯Î¸0(Ë†x))).
For a test data Ë†xâˆˆB, we modify the prediction from fÎ¸fromsoftmax (fÎ¸(Ë†x))tosoftmax (fÎ¸(Ë†x)WB+bB).
These modified predictions can then be used for the adaptation of fÎ¸. Consequently, our method can be
integrated as a plug-in with any TTA methods that rely on pseudo labels obtained from the classifier. Details
about the integration with existing TTA methods can be found in Appendix A.
4 Experiments
Benchmarks We consider two types of input data distribution shifts: synthetic and natural. Synthetic
distribution shifts are artificially created using data augmentation techniques, such as image corruption with
Gaussian noise. In contrast, natural distribution shifts arise from changes in image style, for instance, from
artistic to photographic styles. We evaluate synthetic distribution shifts on CIFAR-10/100C and ImageNet-C
(Hendrycks & Dietterich, 2019), and natural distribution shifts on CIFAR-10.1 (Recht et al., 2018), PACS (Li
et al., 2017), and OfficeHome (Venkateswara et al., 2017) benchmarks. For synthetic distribution shifts, we
apply 15 different types of common corruption, each at the highest severity level (i.e., level 5). To evaluate the
impact of class distribution shifts, we use long-tailed distributions for CIFAR-10C (class imbalance ratio Ï)
and online imbalanced distributions for CIFAR-100C and ImageNet-C (imbalance ratio IR). For datasets with
a large number of classes, such as CIFAR-100C and ImageNet-C, the test batch size is often smaller than the
number of classes, making the distribution of test batches significantly different from the assumed long-tailed
distributions. Therefore, to evaluate the impact of class distribution shifts, we consider test batches modeled
by online label distribution shifts, as described in Sec. 2 (with further details in Sec. A.1). For PACS and
OfficeHome, we use the original datasets, which inherently include natural label distribution shifts across
domains (Fig. 5).
Baselines Our method can be used as a plug-in for baseline methods that utilize test data predictions.
We test the efficacy of DART combined with following baselines: (1) BNAdapt (Schneider et al., 2020)
corrects batch statistics using test data; (2) TENT (Wang et al., 2020) fine-tunes BN layer parameters to
minimize the prediction entropy of test data; (3) PL (Lee, 2013) fine-tunes the trained classifier using confident
pseudo-labeled test samples; (4) NOTE (Gong et al., 2022) adapts classifiers while mitigating the effects of
non-i.i.d test data streams through instance-aware BN and prediction-balanced reservoir sampling; (5) DELTA
(Zhao et al., 2022) addresses incorrect BN statistics and prediction bias with test-time batch renormalization
and dynamic online reweighting; (6) ODS (Zhou et al., 2023) estimates test data label distribution via
Laplacian-regularized maximum likelihood estimation and adapts the model by weighting infrequent and
frequent classes differently. (7) LAME (Boudiaf et al., 2022) modifies predictions with Laplacian-regularized
maximum likelihood estimation using nearest neighbor information in the classifierâ€™s embedding space. (8)
SAR (Niu et al., 2023) adapts models to lie in a flat region on the entropy loss surface. More details about
baselines are available in Appendix A.
Experimental details We use ResNet-26 (He et al., 2016) for CIFAR and ResNet-50 for PACS, OfficeHome,
and ImageNet benchmarks. For the distribution shift-aware module gÏ•, we use a 2-layer MLP (Haykin,
1998) with a hidden dimension of 1,000. Fine-tuning layers, optimizers, and hyperparameters remain
consistent with those introduced in each baseline for fair comparison. Implementation details for pre-training,
intermediate-time training, and test-time adaptation are described in Appendix A.
7Under review as submission to TMLR
Table 1: Average accuracy on CIFAR-10C/10.1-LT, PACS, and OfficeHome. Boldindicates the best
performance for each benchmark. The prediction refinement of DART consistently contributes to effective
adaptation combined with existing entropy minimization-based TTA methods.
MethodCIFAR-10C-LT CIFAR-10.1-LTPACS OfficeHomeÏ= 1Ï= 10Ï= 100 Ï= 1Ï= 10Ï= 100
NoAdapt 71.7Â±0.0 71.3Â±0.1 71.4Â±0.287.7Â±0.087.3Â±0.7 88.3Â±0.760.7Â±0.060.8Â±0.0
BNAdapt (Schneider et al., 2020) 85.2Â±0.0 79.0Â±0.1 67.0Â±0.185.6Â±0.2 77.8Â±0.5 64.6Â±0.672.5Â±0.060.4Â±0.1
BNAdapt+DART (ours) 85.2Â±0.184.7Â±0.185.1Â±0.385.6Â±0.284.5Â±0.485.6Â±0.776.4Â±0.161.0Â±0.1
TENT (Wang et al., 2020) 86.3Â±0.1 82.9Â±0.4 70.4Â±0.185.8Â±0.3 77.6Â±1.5 64.6Â±1.176.8Â±0.660.9Â±0.2
TENT+DART (ours) 86.5Â±0.286.7Â±0.388.2Â±0.385.7Â±0.485.0Â±0.785.8Â±0.281.6Â±0.462.1Â±0.2
PL (Lee, 2013) 86.5Â±0.1 82.9Â±0.2 68.1Â±0.286.2Â±0.6 77.6Â±1.4 64.1Â±1.172.4Â±0.259.2Â±0.2
PL+DART (ours) 86.6Â±0.186.8Â±0.286.3Â±0.486.2Â±0.485.0Â±0.785.4Â±1.176.7Â±0.460.1Â±0.4
NOTE (Gong et al., 2022) 81.8Â±0.3 81.1Â±0.5 79.8Â±1.784.4Â±0.7 84.4Â±1.2 84.6Â±2.479.1Â±0.853.7Â±0.3
NOTE+DART (ours) 81.6Â±0.481.9Â±0.582.2Â±0.484.0Â±1.085.8Â±0.887.0Â±0.979.9Â±0.353.5Â±0.1
LAME (Boudiaf et al., 2022) 85.2Â±0.0 80.4Â±0.1 70.6Â±0.285.6Â±0.4 78.7Â±1.5 67.2Â±1.072.5Â±0.359.8Â±0.1
LAME+DART (ours) 85.2Â±0.082.8Â±0.181.2Â±0.285.6Â±0.581.9Â±1.080.7Â±0.576.0Â±0.260.2Â±0.2
DELTA (Zhao et al., 2022) 85.9Â±0.1 82.5Â±0.6 70.4Â±0.385.4Â±0.2 77.8Â±3.1 64.2Â±2.978.2Â±0.663.1Â±0.1
DELTA+DART (ours) 85.8Â±0.285.9Â±0.687.6Â±0.485.3Â±0.385.1Â±1.286.6Â±0.981.7Â±0.463.9Â±0.1
ODS (Zhou et al., 2023) 85.9Â±0.1 83.8Â±0.5 76.5Â±0.486.8Â±0.2 85.3Â±1.6 81.0Â±0.977.4Â±0.562.1Â±0.2
ODS+DART (ours) 85.9Â±0.185.0Â±0.484.9Â±0.286.7Â±0.387.4Â±1.488.5Â±1.279.7Â±0.462.5Â±0.2
SAR (Niu et al., 2023) 86.8Â±0.181.0Â±0.3 68.2Â±0.286.3Â±0.4 77.7Â±1.5 64.4Â±0.974.2Â±0.461.1Â±0.0
SAR+DART (ours) 86.7Â±0.186.4Â±0.387.0Â±0.486.1Â±0.384.9Â±0.885.6Â±0.478.5Â±0.261.7Â±0.2
4.1 Experimental Results
DART-applied TTA methods In Table 1, we compare the experimental results for the original vs. DART-
applied TTA methods across CIFAR-10C/10.1-LT, PACS and OfficeHome benchmarks. DART consistently
enhances the performance of existing TTA methods for both synthetic and natural distribution shifts. In
particular, DART achieves significant gains as the class imbalance ratio Ïincreases, while maintaining the
original performance for Ï= 1. For instance, on CIFAR-10C-LT with class imbalance ratios Ï= 10and
100, DART improves the test accuracy of BNAdapt by 5.7% and 18.1%, respectively. DARTâ€™s performance
improvement can vary depending on how the baseline method handles pseudo labels. The baseline methods
discussed in this paper can be categorized into two categories: those designed to mitigate the effects of label
distribution shift, such as NOTE, ODS, and DELTA, and the remaining baseline methods like TENT and
SAR. As shown in Table 1, methods like TENT and SAR suffer significant performance degradation due to
label distribution shifts because they do not consider the presence or intensity of test-time label distribution
shifts. And, DART successfully recovers the predictions significantly degraded by label distribution shift.
Conversely, methods such as NOTE, which builds a prediction balancing memory for adaptation, and DELTA
and ODS, which compute class frequencies in the test data and assign higher weights to infrequent class data,
show less performance degradation by label distribution shifts compared to other baseline methods. However,
since these methods still rely on pseudo-labeled test data for memory construction or weight computation,
the accuracy of pseudo labels significantly impacts their performance. Thus, as observed in Table 1, DARTâ€™s
prediction refinement scheme also contributes to performance improvement in these methods under label
distribution shifts. In conclusion, the degree of performance degradation due to label distribution shifts and
performance recovery by DART varies depending on the baseline methods used.
DART on large-scale benchmarks To apply DART to larger-scale benchmarks, such as CIFAR-100C-imb
and ImageNet-C-imb, we introduce a variant called DART-split. The main difference is that DART-split
divides the prediction refinement module gÏ•into two parts: gÏ•1for detecting the severity of label distribution
shifts andgÏ•2for generating an effective affine transformation for prediction refinement. Originally, DART
used a single module gÏ•to handle both roles, trained with the loss equation 1 for refining predictions on diverse
class-imbalanced batches, regularized by equation 2 using nearly class-balanced batches. However, with a large
number of classes, the module needs to experience numerous types of distribution shifts during intermediate
training to generate effective transformations at test time, regardless of the severity of class distribution
8Under review as submission to TMLR
Table 2: Average accuracy on CIFAR-100C, and ImageNet-C under online imbalance setup with several
imbalance ratios (IR). Both DART and DART-split effectively correct inaccurate predictions. On the large-
scale benchmarks such as CIFAR-100C and ImageNet-C, DART-split demonstrates superior performance.
Refer to Table 8 for the combined results with other TTA methods.
CIFAR-100C-imb ImageNet-C-imb
IR1 IR200 IR500 IR50000 IR1 IR1000 IR5000 IR500000
NoAdapt 41.5Â±0.3 41.5Â±0.3 41.4Â±0.3 41.5Â±0.318.0Â±0.0 18.0Â±0.1 18.0Â±0.0 18.1Â±0.1
BNAdapt 59.8Â±0.5 29.1Â±0.5 18.8Â±0.4 9.3Â±0.131.5Â±0.0 19.8Â±0.1 8.4Â±0.1 3.4Â±0.0
BNAdapt+ DART (ours) 59.9Â±0.3 41.8Â±0.8 32.2Â±0.7 22.7Â±0.6 - - - -
BNAdapt+ DART-split (ours) 59.7Â±0.5 45.8Â±0.2 50.2Â±0.3 49.1Â±0.731.5Â±0.0 20.0Â±0.2 11.5Â±0.6 8.2Â±0.5
Table3: Ablationstudieshighlightingthecriticalroleofthe prediction refinementmoduleâ€™s design, particularly
the significance of its inputs and outputs, in enhancing DARTâ€™s effectiveness.
gÏ•inputgÏ•output CIFAR-10C-LT
Â¯pBdBW b Ï= 1Ï= 10Ï= 100
BNAdapt 85.2Â±0.0 79.0Â±0.1 67.0Â±0.1
BNAdapt+DART (ours)âœ“ âœ“ âœ“ 81.8Â±1.6 78.0Â±1.0 81.9Â±0.5
âœ“ âœ“ âœ“ 85.0Â±0.1 84.0Â±0.2 83.0Â±0.5
âœ“ âœ“ âœ“85.4Â±0.083.6Â±0.2 80.7Â±0.1
âœ“ âœ“ âœ“ âœ“ 85.2Â±0.184.7Â±0.1 85.1Â±0.3
shifts. To address this, DART-split separates these tasks. gÏ•2is trained exclusively on Dirichlet-sampled
batches, focusing on severely class-imbalanced batches for prediction refinement. The entire module then
decides whether to apply gÏ•2â€™s refinement for each test batch based on the output of gÏ•1, which detects the
severity of label distribution shifts using the prediction deviation dBof each batch. gÏ•1outputs a severity
scoresBranging from 0 to 1, indicating the degree of label distribution shift. If sBexceeds 0.5, indicating a
severe shift, the batchâ€™s predictions are modified using the affine transformation WBandbBgenerated by
gÏ•2, i.e.,softmax (fÎ¸(x)WB+bB). Otherwise, the prediction is not modified. More detailed explanation of
DART-split is provided in Appendix B. In Table 2, we compare the experimental results for the original
vs. DART-split-applied BNAdapt on the CIFAR-100C-imb and ImageNet-C-imb benchmarks. DART-split
consistently improves performance, achieving a 16.7-39.8% increase on CIFAR-100C-imb with imbalance
ratios of 200-50000 and a 0.2-4.8% increase on ImageNet-C-imb with imbalance ratios of 1000-500000. It
notably outperforms BNAdapt under severe label distribution shifts and surpasses NoAdapt by over 4% across
all imbalance ratios on CIFAR-100C-imb. In Table 8, we also show the results of DART-split combined with
existing TTA methods demonstrating consistent performance gains, on CIFAR-100C-imb and ImageNet-C-imb.
Additionally, we report the performance of DART-split compared to DART on CIFAR-10C-LT in Table 7.
4.2 Ablation Studies
Importance of inputs and outputs of gÏ•for label distribution detection and prediction refinement
In Table 3, we present ablation studies demonstrating the effectiveness of the two inputsâ€“the averaged pseudo-
label distribution Â¯pBand prediction deviation dBâ€“and the two outputs, WâˆˆRKÃ—KandbâˆˆRK, of DART in
prediction refinement. First, we observe that using the prediction deviation dBas an input, in addition to the
average pseudo-label, results in a gain of 3.2-6.7% when gÏ•outputs both W&b. Additionally, using the square
matrixWfor prediction refinement outperforms using the bias vector bas label distribution shifts become
more severe, showing improvements of 0.4% and 2.3% on CIFAR-10C-LT with Ï= 10and 100, respectively.
The best performance for Ï= 10and 100 is achieved when both Wandbare used. These results align with
the analysis in Section 2, indicating that as label distribution shifts become more severe, class-wise confusion
patterns become clearer, making it crucial to reverse these patterns for improving BNAdaptâ€™s performance.
9Under review as submission to TMLR
Table 4: Average accuracy (%) on CIFAR-10C and CIFAR-10C-imb of DART-applied BNAdapt with different
sampling methods during intermediate time.
Sampling at int. time CIFAR-10C-LT CIFAR-10C-imb
Dirichelt (used) Unif&LT( Ï= 20)Ï= 1Ï= 10Ï= 100 IR1 IR20 IR50 IR5000
BNAdapt 85.2Â±0.0 79.0Â±0.1 67.0Â±0.185.3Â±0.1 50.0Â±0.4 34.6Â±0.4 20.3Â±0.1
BNAdapt+DART (ours)âœ“ 85.2Â±0.1 84.7Â±0.1 85.1Â±0.385.3Â±0.2 76.2Â±0.3 78.2Â±0.1 82.4Â±0.7
âœ“ 85.4Â±0.0 85.6Â±0.1 87.3Â±0.285.5Â±0.1 58.9Â±0.5 43.8Â±0.6 28.7Â±0.2
Importance of Dirichlet sampling during intermediate time We use Dirichlet sampling during the
intermediate time to make the prediction refinement module experience intermediate batches with diverse
label distributions. To evaluate the effectiveness of the Dirichlet sampling, we consider a scenario where the
module experiences only three types of batches during the intermediate time: uniform, long-tailed with a
class imbalance ratio Ï= 20, and inversely long-tailed class distributions, inspired by Park et al. (2023). In
Table 4, we report the performance of BNAdapt combined with this DART variant on CIFAR-10C-LT and
CIFAR-10C-imb. When experiencing only uniform and long-tailed distributions during the intermediate time,
the test accuracy of the variant of DART surpasses that of the original DART on CIFAR-10C-LT since it
experiences only similar label distribution shifts during the intermediate time. However, on CIFAR-10C-
imb, we can observe that the DART variantâ€™s performance is significantly lower since the test-time label
distributions become more diverse and severely imbalanced. Thus, using Dirichlet sampling to experience a
wide range of class distributions during the intermediate time is crucial for effectively addressing diverse test
time label distribution shifts.
Table 5: Average accuracy (%) on CIFAR-10C-LT of DART with different gÏ•outputs.
gÏ•output CIFAR-10C-LT
affine (used) additive params Ï= 1Ï= 10Ï= 100
BNAdapt 85.2Â±0.0 79.0Â±0.1 67.0Â±0.1
BNAdapt+DART (ours)âœ“ 85.2Â±0.1 84.7Â±0.1 85.1Â±0.3
âœ“ 82.0Â±2.2 79.9Â±2.0 82.3Â±1.6
Importance of DART prediction refinement scheme using affine transformation To test the
effectiveness of the DARTâ€™s prediction refinement scheme using the affine transformation, we consider a
variant of DART that modifies gÏ•to generate the additive parameters for a part of the classifier, including
affine parameters for the output of the feature extractor and the weight difference for the last linear classifier
weights, inspired by LSA (Park et al., 2023). For this case, the output dimension of gÏ•gets larger since it is
proportional to the feature dimension of the trained model (e.g. 512 for ResNet26). For a fair comparison,
we maintain the number of parameters of gÏ•. In Table 5, we report the performance of BNAdapt combined
with the DART variant on CIFAR-10C-LT. The variant of DART shows worse performance on all Ïs and
significant performance variability depending on different random seeds. We conjecture that this is because
gÏ•struggles to learn to generate additive parameters proportional to the feature dimensions in response to
various class distribution shifts.
We also present additional experiments in Appendix G. These experiments demonstrate the effectiveness
of DART in a more challenging TTA scenario, where test samples with multiple types of corruption are
encountered simultaneously. We also verify the robustness of DART against changes in hyperparameters,
such as the structure of gÏ•and intermediate batch size. Moreover, we introduce a variant of DART that
reduces the dependency on labeled training data during the intermediate training phase of DART by utilizing
the condensed training dataset as in Kang et al. (2023).
10Under review as submission to TMLR
5 Related Works
Methods to handle label distribution shifts The scenario where class distributions differ between
training and test datasets has been primarily studied in the context of long-tail learning. Long-tail learning
addresses class imbalance through two main approaches: re-sampling (Chawla et al., 2002; Liu et al., 2008) and
re-weighting (Hong et al., 2021; Menon et al., 2020). Recently, the impact of test-time label distribution shift
in domain adaptation has gained attention. Specifically, RLSbench (Garg et al., 2023) reveals performance
drops of TTA methods, BNAdapt and TENT, under label distribution shifts, and proposes a recovery
method through re-sampling and re-weighting. However, the method is unsuitable for TTA settings due to
the simultaneous use of training and test data. DART enhances the BN-adapted classifier predictions by
correcting errors through a prediction refinement module, without relying on training data during test time.
TTA method utilizing intermediate time Recent works (Choi et al., 2022; Lim et al., 2022) have
explored methods to prepare for unknown test-time distribution shifts by leveraging the training dataset
during an intermediate time. For instance, LSA (Park et al., 2023) conducts intermediate training to enhance
classifier performance on long-tailed training datasets when applied to test datasets with different long-tailed
distributions. Specifically, LSA trains a label shift adapter using batches with three types of class distributions
(uniform, imbalanced training distribution, and inversely imbalanced) to produce additive parameters for
the classifier. In contrast, DART exposes the prediction refinement module to a more diverse range of class
distributions through Dirichlet sampling and generates an effective affine transformation for logit refinement.
Additionally, while LSA uses only averaged pseudo-label distributions to estimate label distribution shifts,
DART also considers prediction deviation to more effectively detect label distribution shifts. In Section G, we
demonstrate the efficacy of DARTâ€™s sampling and prediction modification schemes compared to LSA. More
related works are reviewed in Appendix C.
6 Discussion and Conclusion
We proposed DART, a method to mitigate test-time class distribution shifts by accounting for class-wise
confusion patterns. DART trains a prediction refinement module during an intermediate time to detect
label distribution shifts and correct predictions of BN-adapted classifiers. Our experiments demonstrate
DARTâ€™s effectiveness across various benchmarks, including synthetic and natural distribution shifts. DARTâ€™s
intermediate-time training incurs additional costs compared to traditional TTA. However, these costs are
relatively minimal, especially considering the significant performance improvements it offers when combined
with traditional TTA methods (Table 1). The low cost is partly due to using the existing training dataset,
rather than requiring an auxiliary dataset, and its short runtime. This paper highlights and effectively
addresses the negative impact of test-time label distribution shifts on existing TTA methods. As label
distribution shifts are common in real-world scenarios, we expect future studies to build on our approach
to enhance real-world scalability by making models robust to test-time shifts in both input and label
distributions.
Broader Impact Statement
This paper highlights and effectively resolves the negative impact of test-time label distribution shifts to
existing TTA methods. Since the label distribution shifts between the training and test domains are common
in the real world, we expect further studies that are robust to test-time shifts on both input data and label
distributions to be built on our work to enhance real-world scalability.
11Under review as submission to TMLR
References
Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375 , 2018.
Malik Boudiaf, Romain Mueller, Ismail Ben Ayed, and Luca Bertinetto. Parameter-free online test-time
adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pp. 8344â€“8353, 2022.
George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distillation
by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pp. 4750â€“4759, 2022.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority
over-sampling technique. Journal of artificial intelligence research , 16:321â€“357, 2002.
Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers.
InProceedings of the IEEE/CVF international conference on computer vision , pp. 9640â€“9649, 2021.
Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-
agnostic weight regularization and nearest source prototypes. In European Conference on Computer Vision ,
pp. 440â€“458. Springer, 2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, and Zachary Chase
Lipton. Rlsbench: Domain adaptation under relaxed label shift. In International Conference on Machine
Learning , pp. 10879â€“10928. PMLR, 2023.
Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee. Note: Robust
continual test-time adaptation against temporal correlation. Advances in Neural Information Processing
Systems, 35:27253â€“27266, 2022.
Sachin Goyal, Mingjie Sun, Aditi Raghunathan, and J Zico Kolter. Test time adaptation via conjugate
pseudo-labels. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems , 2022.
Simon Haykin. Neural networks: a comprehensive foundation . Prentice Hall PTR, 1998.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In
2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770â€“778, 2016. doi:
10.1109/CVPR.2016.90.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. Proceedings of the International Conference on Learning Representations , 2019.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
Youngkyu Hong, Seungju Han, Kwanghee Choi, Seokjun Seo, Beomsu Kim, and Buru Chang. Disentangling
label distribution for long-tailed visual recognition. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pp. 6626â€“6636, 2021.
Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain
generalization. Advances in Neural Information Processing Systems , 34:2427â€“2440, 2021.
Minguk Jang, Sae-Young Chung, and Hye Won Chung. Test-time adaptation via self-training with nearest
neighbor information. In The Eleventh International Conference on Learning Representations , 2022.
Juwon Kang, Nayeong Kim, Kwon Donghyeon, Jungseul Ok, and Suha Kwak. Leveraging proxy of training
data for test-time adaptation. In International Conference on Machine Learning (ICML) , July 2023.
12Under review as submission to TMLR
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical
Report 0, University of Toronto, Toronto, Ontario, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. Advances in neural information processing systems , 25, 2012.
Dong-Hyun Lee. Pseudo-label : The simple and efficient semi-supervised learning method for deep neural
networks. In Workshop on challenges in representation learning, ICML , volume 3, 2013.
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain
generalization. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October
22-29, 2017 , pp. 5543â€“5551. IEEE Computer Society, 2017. doi: 10.1109/ICCV.2017.591.
Hyesu Lim, Byeonggeun Kim, Jaegul Choo, and Sungha Choi. Ttn: A domain-shift aware batch normalization
in test-time adaptation. In The Eleventh International Conference on Learning Representations , 2022.
Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. Exploratory undersampling for class-imbalance learning. IEEE
Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) , 39(2):539â€“550, 2008.
Robert A Marsden, Mario DÃ¶bler, and Bin Yang. Universal test-time adaptation through weight ensem-
bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision , pp. 2555â€“2565, 2024.
Russell Mendonca, Xinyang Geng, Chelsea Finn, and Sergey Levine. Meta-reinforcement learning robust to
distributional shift via model identification and experience relabeling. CoRR, abs/2006.07178, 2020.
Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and Sanjiv
Kumar. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314 , 2020.
Zachary Nado, Shreyas Padhy, D Sculley, Alexander Dâ€™Amour, Balaji Lakshminarayanan, and Jasper Snoek.
Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint
arXiv:2006.10963 , 2020.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisy
labels.Advances in neural information processing systems , 26, 2013.
Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan.
Towards stable test-time adaptation in dynamic wild world. In The Eleventh International Conference on
Learning Representations , 2023.
Sunghyun Park, Seunghan Yang, Jaegul Choo, and Sungrack Yun. Label shift adapter for test-time adaptation
under covariate and label shifts. arXiv preprint arXiv:2308.08810 , 2023.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep
learning library. Advances in neural information processing systems , 32, 2019.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep
neural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE conference
on computer vision and pattern recognition , pp. 1944â€“1952, 2017.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
language supervision. In International conference on machine learning , pp. 8748â€“8763. PMLR, 2021.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize
to cifar-10? arXiv preprint arXiv:1806.00451 , 2018.
13Under review as submission to TMLR
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains.
InProceedings of the 11th European Conference on Computer Vision: Part IV , ECCVâ€™10, pp. 213â€“226,
Berlin, Heidelberg, 2010. Springer-Verlag. ISBN 364215560X.
Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge.
Improving robustness against common corruptions by covariate shift adaptation. Advances in neural
information processing systems , 33:11539â€“11551, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556 , 2014.
Petar Stojanov, Zijian Li, Mingming Gong, Ruichu Cai, Jaime Carbonell, and Kun Zhang. Domain adaptation
with invariant representation learning: What transformations to learn? Advances in Neural Information
Processing Systems , 34:24791â€“24803, 2021.
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt.
Measuring robustness to natural distribution shifts in image classification. In H. Larochelle, M. Ranzato,
R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems , volume 33,
pp. 18583â€“18599. Curran Associates, Inc., 2020.
Antonio Torralba, Rob Fergus, and William T Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE transactions on pattern analysis and machine intelligence ,
30(11):1958â€“1970, 2008.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser,
and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30,
2017.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing
network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 5018â€“5027, 2017.
Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time
adaptation by entropy minimization. In International Conference on Learning Representations , 2020.
Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are
anchor points really indispensable in label-noise learning? Advances in neural information processing
systems, 32, 2019.
Yu Yao, Tongliang Liu, Bo Han, Mingming Gong, Jiankang Deng, Gang Niu, and Masashi Sugiyama. Dual t:
Reducing estimation error for transition matrix in label-noise learning. Advances in neural information
processing systems , 33:7260â€“7271, 2020.
Li Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, A Ian McLeod, and Boyu Wang. When
source-free domain adaptation meets learning with noisy labels. arXiv preprint arXiv:2301.13381 , 2023.
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and Yasaman
Khazaeni. Bayesian nonparametric federated learning of neural networks. In International conference on
machine learning , pp. 7252â€“7261. PMLR, 2019.
Yixin Zhang, Zilei Wang, and Weinan He. Class relationship embedded learning for source-free unsupervised
domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pp. 7619â€“7629, 2023.
Bowen Zhao, Chen Chen, and Shu-Tao Xia. Delta: Degradation-free fully test-time adaptation. In The
Eleventh International Conference on Learning Representations , 2022.
Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. On pitfalls of test-time adaptation. In International
Conference on Machine Learning (ICML) , 2023.
14Under review as submission to TMLR
Zhi Zhou, Lan-Zhe Guo, Lin-Han Jia, Dingchu Zhang, and Yu-Feng Li. Ods: test-time adaptation in the
presence of open-world data shift. In International Conference on Machine Learning . PMLR, 2023.
Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when learning with
noisy labels. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on
Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 12912â€“12923. PMLR,
18â€“24 Jul 2021.
15Under review as submission to TMLR
Appendix
A Implementation Details
A.1 Details about Dataset
We consider two types of input data distribution shifts: synthetic and natural distribution shifts. The
synthetic and natural distribution shifts differ in their generation process. Synthetic distribution shift is
artificially generated by data augmentation schemes including image corruption like Gaussian noise and glass
blur. On the other hand, the natural distribution shift occurs due to changes in image style transfer, for
example, the domain is shifted from artistic to photographic styles.
For the synthetic distribution shift, we test on CIFAR-10/100C and ImageNet-C, which is created by applying
15 types of common image corruptions (e.g. Gaussian noise and impulse noise) to the clean CIFAR-10/100
test and ImageNet validation datasets. We test on the highest severity ( i.e., level-5). CIFAR-10/100C
is composed of 10,000 generic images of size 32 by 32 from 10/100 classes, respectively. ImageNet-C is
composed of 50,000 generic images of size 224 by 224 from 1,000 classes. The class distributions of the
original CIFAR-10/100C and ImageNet-C are balanced. Thus, we consider two types of new test datasets to
change the label distributions between training and test domains. First, we consider CIFAR-10-LT, which
has long-tailed class distributions, as described in Section 2. We set the number of images per class to
decrease exponentially as the class index increases. Specifically, we set the number of samples for class kas
nk=n(1/Ï)k/(Kâˆ’1), wherenandÏdenote the number of samples of class 0 and the class imbalance ratio,
respectively. We also consider the test set of inversely long-tailed distribution in Figure 1. Specifically, we set
the number of samples for class kasnk=n(1/Ï)(Kâˆ’1âˆ’k)/(Kâˆ’1), wherenandÏdenote the number of samples
of classKâˆ’1and the class imbalance ratio, respectively. However, unlike CIFAR-10-LT, each test batch of
CIFAR-100C-LT and ImageNet-C-LT tend to not have imbalanced class distributions, since the test batch
size (e.g.,32 or 64) is set to be smaller than the number of classes (100 and 1,000). Thus, we also consider
CIFAR-100C-imb and ImageNet-C-imb, whose label distributions keep changing during test time, as described
in Section 2. These datasets are composed of Ksubsets, where Kis the number of classes. We assume a
class distribution of the k-th subset as [p1,p2,...,pK], wherepk=pmaxandpi=pmin= (1âˆ’pmax)/(Kâˆ’1)
foriÌ¸=k. LetIR=pmax/pminrepresent the imbalance ratio. Each subset consists of 100 samples from the
CIFAR-100C and ImageNet-C test set based on the above class distribution. Thus, the new test set for
CIFAR-100C/ImageNet-C is composed of 10,000/100,000 samples, respectively. Additionally, we shuffle the
subsets to prevent predictions based on their order.
For the natural distribution shift, we test on CIFAR-10.1-LT, PACS, and OfficeHome benchmarks. CIFAR-10.1
(Recht et al., 2018) is a newly collected test dataset for CIFAR-10 from the TinyImages dataset (Torralba
et al., 2008), and is known to exhibit a distribution shift from CIFAR-10 due to differences in data collection
process and timing. Since the CIFAR-10.1 has a balanced class distribution, we construct a test set having a
long-tailed class distribution, named CIFAR-10.1-LT, similar to CIFAR-10C-LT. PACS benchmark consists
of samples from seven classes including dogs and elephants in four domains: photo, art, cartoon, and sketch.
In PACS, we test the robustness of classifiers across 12 different scenarios, each using the four domains as
training and test domains, respectively. The OfficeHome (Venkateswara et al., 2017) benchmark is one of the
well-known large-scale domain adaptation benchmarks, comprising images from four domains (art, clipart,
product, and the real world). Each domain consists of images belonging to 65 different categories. The
number of data samples per class ranges from a minimum of 15 to a maximum of 99, with an average of
70. This ensures that the label distribution differences between domains are not substantial, as depicted in
Figure 5. The data generation/collection process of PACS and OfficeHome benchmarks is different across
domains, resulting in differently imbalanced class distribution, as illustrated in Figure 5.
A.2 Details about Pre-training
We use ResNet-26 for CIFAR datasets, and ResNet-50 for PACS, OfficeHome, and ImageNet benchmarks as
backbone networks. We use publicly released trained models and codes for a fair comparison. Specifically,
16Under review as submission to TMLR
0 1 2 3 4 5 6
Class index0.000.050.100.150.200.250.30Class probablity
0.185
0.125
0.139
0.090
0.098
0.144
0.219PACS-art
0 1 2 3 4 5 6
Class index0.000.050.100.150.200.250.30Class probablity
0.166
0.195
0.148
0.058
0.138
0.123
0.173PACS-cartoon
0 1 2 3 4 5 6
Class index0.000.050.100.150.200.250.30Class probablity
0.113
0.121
0.109
0.111
0.119
0.168
0.259PACS-photo
0 1 2 3 4 5 6
Class index0.000.050.100.150.200.250.30Class probablity
0.196
0.188
0.192
0.155
0.208
0.020
0.041PACS-sketch
0 10 20 30 40 50 60
Class index0.000.010.020.030.040.05Class probablityOfficeHome-art
0 10 20 30 40 50 60
Class index0.000.010.020.030.040.05Class probablityOfficeHome-clipart
0 10 20 30 40 50 60
Class index0.000.010.020.030.040.05Class probablityOfficeHome-product
0 10 20 30 40 50 60
Class index0.000.010.020.030.040.05Class probablityOfficeHome-realworld
Figure 5: Class distribution of PACS and OfficeHome
for CIFAR-10/1001, we train the model with 200 epochs, batch size 200, SGD optimizer, learning rate 0.1,
momentum 0.9, and weight decay 0.0005. For PACS and OfficeHome, we use released pre-trained models of
TTAB (Zhao et al., 2023)2. For ImageNet-C, we use the released pre-trained models in the PyTorch library
(Paszke et al., 2019) as described in (Niu et al., 2023).
A.3 Details about Intermediate Time Training
We use the labeled dataset in the training domain to train the prediction refinement module gÏ•for all
benchmarks. If the original training dataset exhibits an imbalanced class distribution, e.g. PACS and
OfficeHome, this imbalance can unintentionally influence the class distributions within the intermediate
batches. To mitigate this, we create a class-balanced dataset Dintby uniformly sampling data from each class.
For example, for the ImageNet benchmark, the intermediate dataset is a subset of the ImageNet training
dataset, composed of 50 samples randomly selected from each of the classes. For the intermediate dataset, we
use a simple augmentation such as normalization and center cropping.
We use a 2-layer MLP (Haykin, 1998) for the prediction refinement module gÏ•.gÏ•is composed of two fully
connected layers and ReLU (Agarap, 2018). The hidden dimension of the prediction refinement module
is set to 1,000. During the intermediate time, we train gÏ•by exposing it to several batches with diverse
class distributions using the labeled training dataset. We train gÏ•with Adam optimizer (Kingma & Ba,
2014), a learning rate of 0.001, and cosine annealing for 50 epochs. For large-scale benchmarks, such as
CIFAR-100 and ImageNet, gÏ•is trained for 100/200 epochs to expose it to diverse label distribution shifts of
many classes, respectively. To make intermediate batches having diverse class distributions, we use Dirichlet
sampling as illustrated in Figure 6 and 7 with two hyperparameters, the Dirichlet sampling concentration
parameterÎ´, and the number of chunks Ndir. As these two hyperparameters increase, the class distributions
of intermediate batches become similar to the uniform. Î´is set to 0.001 for ImageNet-C, 0.1 for CIFAR-100,
and 10 for other benchmarks. Ndiris set to 2000 for ImageNet-C, 1000 for CIFAR-100C, and 250 for other
benchmarks. The intermediate batch size is set to 32/64 for ImageNet and all other benchmarks, respectively.
On the other hand, we use a regularization on gÏ•to produce Wandbthat are similar to the identity matrix
of sizeKand the zero vector, respectively, for the case when there are no label distribution shifts. The
hyperparameter for the regularization Î±in equation 3 is generally set to 0.1. For PACS and OfficeHome, we
setÎ±to 10.
1https://github.com/locuslab/tta_conjugate
2https://github.com/LINs-lab/ttab
17Under review as submission to TMLR
Dirichlet sampling
 i.i.d. sampling
Class0 Class1 Class20.00.2=1
Class0 Class1 Class20.00.20.4=2
Class0 Class1 Class20.000.250.50=10
Figure 6: Example of the Dirichlet distribution sampling . IID (i.i.d.) sampling denotes standard
uniform sampling. The black dots indicate the class distribution of the sampled batches. The red, blue,
and yellow dots represent the class distributions of different class imbalance ratios Ï, namely 1,2, and 10,
respectively. By employing the Dirichlet distribution for batch sampling, we can expose the model to numerous
batches with diverse class distributions during the intermediate time, thereby enabling it to learn how to
mitigate performance degradation caused by class distribution shifts.
Figure 7: Class distributions of five batches sampled from CIFAR-10 using Dirichlet sampling. These sampled
batches demonstrate diverse and varied class distributions, highlighting the effectiveness of Dirichlet sampling
in generating a wide range of label distribution shifts.
A.4 Details about Test-Time Adaptation
During test time, we fine-tune only the Batch Normalization (BN) layer parameters as is common with most
TTA methods. We use the Adam optimizer with a learning rate of 0.001 for all TTA methods in every
experiment, except on ImageNet benchmarks, following the approach in TENT (Wang et al., 2020). On
ImageNet benchmarks, we use the SGD optimizer with a learning rate of 0.0005. We set the test batch size
to 64 for PACS, OfficeHome, and ImageNet-C, and to 200 for CIFAR benchmarks. We run on 4 different
random seeds for the intermediate-time and test-time training (0,1,2, and 3).
To generate the affine transformation for prediction refinement, gÏ•uses the outputs of the BN-adapted
pre-trained classifier fÂ¯Î¸0, not the classifier fÎ¸that is continually updated during test time. This is because gÏ•
has been trained to correct the inaccurate predictions of the BN-adapted classifier caused by label distribution
shifts during the intermediate time. Specifically, for a test batch B,
WB,bB=gÏ•(Â¯pB,dB)where Â¯pB=1
|B|/summationdisplay
Ë†xâˆˆBsoftmax (fÂ¯Î¸0(Ë†x)),dB=1
|B|/summationdisplay
Ë†xâˆˆBD(softmax (u,fÂ¯Î¸0(Ë†x))).(4)
For a test data Ë†xâˆˆB, we modify the prediction from fÎ¸fromsoftmax (fÎ¸(Ë†x))tosoftmax (fÎ¸(Ë†x)WB+bB).
These modified predictions can then be used for the adaptation of fÎ¸. Since the affine transforma-
tion bygÏ•focuses on prediction refinement, the confidence of predictions can be changed from the
original predictions, potentially causing negative effects during test-time training (Goyal et al., 2022).
Therefore, we use normalization to maintain the confidence level of predictions. For example, in the
18Under review as submission to TMLR
case of TENT (Wang et al., 2020), we adapt the classifier fÎ¸using a training objective LTENT (Î¸) =
EË†xâˆˆB[âˆ’/summationtext
ksoftmax (âˆ¥fÎ¸(Ë†x)âˆ¥2fÎ¸(Ë†x)WB+bB
âˆ¥fÎ¸(Ë†x)WB+bBâˆ¥2)klogsoftmax (âˆ¥fÎ¸(Ë†x)âˆ¥2fÎ¸(Ë†x)WB+bB
âˆ¥fÎ¸(Ë†x)WB+bBâˆ¥2)k].
During the test time, there often arises an issue where the logits/pseudo label distribution of unseen test
data exhibits unconfidence, making it challenging to apply prediction refinement through gÏ•directly. To
address this, we utilize the softmax temperature scaling. For OfficeHome and CIFAR100, we multiply logits
by 2 and 1.1 for test batches for softmax temperature scaling, respectively.
A.5 Details about Baseline Methods
BNAdapt (Schneider et al., 2020) BNAdapt does not update the parameters in the trained model, but
it corrects the BN statistics in the model using the BN statistics computed on the test batches.
TENT (Wang et al., 2020) TENT replaces the BN statistics of the trained classifier with the BN
statistics computed in each test batch during test time. And, TENT fine-tunes the BN layer parameters to
minimize the prediction entropy of the test data.
PL (Lee, 2013) PL regards the test data with confident predictions as reliable pseudo-labeled data and
fine-tunes the BN layer parameters to minimize cross-entropy loss using these pseudo-labeled data. We set
the confidence threshold to 0.95 for filtering out test data with unconfident predictions.
NOTE (Gong et al., 2022) NOTE aims to mitigate the negative effects of a non-i.i.d stream during test
time by instance-aware BN (IABN) and prediction-balanced reservoir sampling (PBRS). IABN first detects
whether a sample is from out-of-distribution or not, by comparing the instance normalization (IN) and BN
statistics for each sample. For in-distribution samples, IABN uses the standard BN statistics, while for
out-of-distribution samples, it corrects the BN statistics using the IN statistics. We set the hyperparameter to
determine the level of detecting out-of-distribution samples to 4 as used in NOTE (Gong et al., 2022). Due to
the non-i.i.d stream, class distribution within each batch is highly imbalanced. Thus, PBRS stores an equal
number of predicted test data for each class and does test-time adaptation using the stored data in memory.
We set the memory size same as the batch size, for example, 200 for CIFAR benchmarks. NOTE create
prediction-balanced batches and utilize them for adaptation. Thus, the batches for adaptation in NOTE
have different class distributions from the test dataset, unlike other baselines including TENT. Therefore,
in NOTE, DART is used exclusively to enhance the prediction accuracy of the examples stored in memory.
Since NOTE utilizes IABN layers instead of BN layers, we replace the BN layers of the pre-trained model
with IABN layers and fine-tune the model for 30 epochs using the labeled training dataset as described in
Gong et al. (2022).
DELTA (Zhao et al., 2022) DELTA aims to alleviate the negative effects such as wrong BN statistics
and prediction bias by test-time batch renormalization (TBR) and dynamic online reweighting (DOT). Since
the BN statistics computed in the test batch are mostly inaccurate, TBR corrects the BN statistics with
renormalization using test-time moving averaged BN statistics with a factor of 0.95. DOT computes the
class prediction frequency in exponential moving average with a factor of 0.95 during test time and uses the
estimated class prediction frequency to assign low/high weights to frequent/infrequent classes, respectively.
LAME (Boudiaf et al., 2022) LAME modifies the prediction by Laplacian regularized maximum
likelihood estimation considering nearest neighbor information in the embedding space of the trained classifier.
Note that LAME does not update network parameters of the classifiers. We compute the similarity among
samples for the nearest neighbor information with k-NN with k= 5.
ODS (Zhou et al., 2023) ODS consists of two modules: a distribution tracker and a prediction optimizer.
The distribution tracker estimates the label distribution wand modified label distribution zby LAME,
leveraging features of adapted classifier fÎ¸and predictions from a pre-trained classifier fÎ¸0. The prediction
optimizer produces refined predictions by taking an average of the modified label distribution zand the
19Under review as submission to TMLR
prediction of the adapted classifier fÎ¸, which is trained by a weighted loss that assigns high/low weights won
infrequent/frequent classes, respectively.
Since DELTA and ODS utilize predictions to estimate label distribution shifts, our prediction refinement
scheme can enhance the label distribution shift estimation, leading to more effective test-time adaptation.
SAR (Niu et al., 2023) SAR adapts the trained models to lie in a flat region on the entropy loss surface
using the test samples for which the entropy minimization loss does not exceed 0.4*log(number of classes).
We only reset the model for the experiments of online imbalance when the exponential moving average (EMA)
of the 2nd loss is smaller than 0.2. The authors of SAR observe a significant performance drop when label
distribution shifts or batch sizes decrease on BNAdapt, explain this drop by the BN layerâ€™s property that uses
the test batch statistics as shown in Section 2. Thus, they recommend the usage of other normalization layers
instead of BN layers. However, in this paper, we analyze the error patterns of BN-adapted classifiers under
label distribution shifts and propose a new method to refine the inaccurate prediction of the BN-adapted
classifiers. We expect that it can greatly enhance the scalability of BN-adapted classifiers to real-world
problems with shifted label distributions.
NOTE, ODS, and DELTA, which address class imbalances by re-weighting, the prediction-balancing memory,
and the modified BN layers, can be combined with any entropy minimization test-time adaptation methods
like TENT, PL, and SAR. For a fair comparison, we report the results of combining these methods with
TENT, specifically TENT+NOTE, TENT+ODS, and TENT+DELTA, in Table 1.
Table 6:# of parameters, FLOPs, wall clock at the test time. DART-applied BNAdapt does not
require additional cost except the memory for gÏ•. On the other hand, applying DART to other TTA methods
requires more computation costs due to the need for an additional BN-adapted classifier fÂ¯Î¸0to compute
affine transformations. However, the additional cost caused by fÂ¯Î¸0is negligible compared to the performance
increases.
Method # params (# trainable params) FLOPs Wall clock
NoAdapt 17.4 M (0 M) 0.86 G 2.24 sec
BNAdapt 17.4 M (0 M) 0.86 G 2.36 sec
BNAdapt+DART 17.6 M (0 M) 0.86 G 2.35 sec
TENT 17.4 M (0.013 M) 0.86 G 3.93 sec
TENT+DART 35.0 M (0.013 M) 1.72 G 4.81 sec
A.6 Additional Costs of DART
The additional cost incurred by DART during intermediate and test times is minimal, while it brings significant
performance gains. DARTâ€™s intermediate time training has a very low computation overhead. This is because
the prediction refinement module gÏ•is trained independently of the test domains, requiring training only once
for each classifier. The parameters of the pre-trained classifier, except for BN statistics, are kept frozen while
a shallowgÏ•(a 2-layer MLP) is trained. For instance, training gÏ•during the intermediate time for CIFAR-10
takes only 7 minutes and 9 seconds on RTX A100. Moreover, the DART-applied TTA methods also do
not require significant additional costs compared to naive TTA methods. We summarize the number of
parameters, FLOPs, and wall clocks on balanced CIFAR-10C in Table 6. When applying DART to BNAdapt,
there is almost no additional cost aside from the memory for gÏ•. This is because gÏ•remains fixed during the
test time and is shallow. On the other hand, applying DART to other TTA methods requires additional costs
due to the need for an additional classifier fÂ¯Î¸0to compute affine transformations, aside from the continuously
adapted classifier. The additional costs by the classifier are insignificant since its parameters are kept frozen
during the test time. The small additional cost is negligible (1-2 seconds) compared to the performance gains
of 5.7/18.1% for CIFAR-10C-LT with Ï= 10/100, respectively.
20Under review as submission to TMLR
B Comparison of DART and DART-split
Algorithm 1: Intermediate time training of DART
Input: pre-trained classifier fÎ¸, labeled training dataset D, hyperparameter Î±for balancing two losses, learning rate
Î²
Set class-balanced training dataset as an intermediate dataset Dint
Initialize 2-layer MLP gÏ•for logit refinement
for iterations
# (1) Class-imbalanced intermediate batch
Sample a batchBDir= (Ximb,Yimb)of sizeMfromDintusing Dirichlet distribution
Â¯Î¸â†BNAdapt (Î¸,BDir)# Adapt BN layer statistics using the current batch
limbâ†fÂ¯Î¸(Ximb)âˆˆRMÃ—K# logit of class-imbalanced intermediate batch
Wimb(Ï•),bimb(Ï•)â†LogitModifier-DART (limb,gÏ•)(Algorithm 2)
Limb(Ï•) =1
|BDir|/summationtext
iCE(Yimb,i,softmax (limb,iWimb(Ï•) +bimb(Ï•)))
# (2) Class-balanced intermediate batch
Sample a batchBIID= (Xbal,Ybal)of sizeMi.i.d. fromDintÂ¯Î¸â†BNAdapt (Î¸,BIID)# Adapt BN layer statistics using the current batch
lbalâ†fÂ¯Î¸(Xbal)âˆˆRMÃ—K# logit of class-balanced intermediate batch
Wbal,bbalâ†LogitModifier-DART (lbal,gÏ•)(Algorithm 2)
Lbal(Ï•) =MSE(Wbal(Ï•),IK) +MSE(bbal(Ï•),0K)
# UpdategÏ•
Ï•â†Ï•âˆ’Î²âˆ‡Ï•{Limb(Ï•) +Î±Lbal(Ï•)}
Output: trained gÏ•
Algorithm 2: LogitModifier-DART (l,gÏ•)
Input: BN-adapted classifierâ€™s logit lBfor current batch B, prediction refinement module gÏ•
pâ†softmax (lB)âˆˆRMÃ—K# softmax output
# Compute an averaged pseudo label distribution Â¯pâˆˆRKand prediction deviation dBâˆˆR
Â¯pBâ†1
|B|/summationtext
ipiâˆˆRK
dBâ†1
|B|/summationtext
iD(u,pi)âˆˆR, whereuis an uniform distribution of size K.
# ObtainWâˆˆRKÃ—KandbâˆˆR1Ã—Kby feeding Â¯pBanddBintogÏ•
WB,bBâ†gÏ•(Â¯pB,dB)
Output:WB,bB
Algorithm 3: Intermediate time training of DART-split
Input: pre-trained classifier fÎ¸, labeled training dataset D, learning rate Î²1andÎ²2
Set class-balanced training dataset as an intermediate dataset Dint
Initialize gÏ•1of a single layer and 2-layer MLP gÏ•2for logit refinement
for iterations
# (1) Class-imbalanced intermediate batch
Sample a batchBDir= (Ximb,Yimb)of sizeMfromDintusing Dirichlet distribution
Â¯Î¸â†BNAdapt (Î¸,BDir)# Adapt BN layer statistics using the current batch
limbâ†fÂ¯Î¸(Ximb)âˆˆRMÃ—K# logit of class-imbalanced intermediate batch
dimbâ†1
|BDir|/summationtext
iD(u,softmax (limb,i))âˆˆR, whereuis an uniform distribution of size K.
Wimb(Ï•2),bimb(Ï•2)â†LogitModifier-DART-split (limb,gÏ•2)(Algorithm 4)
Limb(Ï•2) =1
|BDir|/summationtext
iCE(Yimb,i,softmax (limb,iWimb(Ï•2) +bimb(Ï•2)))
# (2) Class-balanced intermediate batch
Sample a batchBIID= (Xbal,Ybal)of sizeMi.i.d. fromDintÂ¯Î¸â†BNAdapt (Î¸,BIID)# Adapt BN layer statistics using the current batch
lbalâ†fÂ¯Î¸(Xbal)âˆˆRMÃ—K# logit of class-balanced intermediate batch
dbalâ†1
|BIID|/summationtext
iD(u,softmax (lbal,i))âˆˆR, whereuis an uniform distribution of size K.
Ldetect(Ï•1) =BCE(0,Ïƒ(gÏ•1(dbal))) + BCE(1,Ïƒ(gÏ•1(dimb))), whereÏƒis a sigmoid function and BCE is binary cross entropy
loss
# UpdategÏ•
Ï•1â†Ï•1âˆ’Î²1âˆ‡Ï•1Ldetect(Ï•1)
Ï•2â†Ï•2âˆ’Î²2âˆ‡Ï•2Limb(Ï•2)
Output: trained gÏ•={gÏ•1,gÏ•2}
B.1 Detailed Explanation about DART-split
We propose a variant of DART, named DART-split, for large-scale benchmarks, which splits the prediction
refinement module gÏ•into two parts gÏ•1andgÏ•2. In this setup, gÏ•1takes the prediction deviation as an
input to detect label distribution shifts, while gÏ•2generates affine transformations using the averaged pseudo
21Under review as submission to TMLR
Algorithm 4: LogitModifier-DART-split (l,gÏ•2)
Input: BN-adapted classifierâ€™s logit lBfor current batch B, a part of prediction refinement module gÏ•2
pâ†softmax (lB)âˆˆRMÃ—K# softmax output
# Compute an averaged pseudo label distribution Â¯pâˆˆRK
Â¯pBâ†1
|B|/summationtext
ipiâˆˆRK
# ObtainWâˆˆRKÃ—KandbâˆˆR1Ã—Kby feeding Â¯pBintogÏ•2WB,bBâ†gÏ•2(Â¯pB)
Output:WB,bB
label distribution. Specifically, gÏ•1takes the prediction deviation of each batch Bas an input, and outputs a
severity score sB=Ïƒ(gÏ•1(dB))ranging from 0 to 1, with higher values indicating more severe shifts. On the
other hand, gÏ•2uses the averaged pseudo label distribution Â¯pBas an input to produce an affine transformation
WBandbB, effectively correcting predictions affected by the label distribution shifts. DART-split can address
the scaling challenges faced by the original DART as the number of classes, K, increases.
At the intermediate time, gÏ•1andgÏ•2are trained individually for detecting label distribution shifts and
generating affine transformation, respectively, as detailed in Algorithm 3. gÏ•1is trained to classify batches
to either severe label distribution shift (output close to 1) or no label distribution shift (output close to 0),
taking the prediction deviation as an input. Specifically, gÏ•1is trained to minimize BCE (0,Ïƒ(gÏ•1(dbal))) +
BCE (1,Ïƒ(gÏ•1(dimb))), whereÏƒis a sigmoid function, BCEis the binary cross entropy loss, and dbal,dimb
are the prediction deviation values of the batches i.i.d. sampled and Dirichlet-sampled, respectively. gÏ•1is
composed of a simple single layer, based on the observation in Fig. 4 that the prediction deviation almost
linearly decreases as the imbalance ratio increases. On the other hand, gÏ•2, is composed of a two-layer MLP
and focuses on generating affine transformations solely from the averaged pseudo label distribution, following
the training objectives similar to those of the original DARTâ€™s gÏ•.
During testing, each test batch Bis first evaluated by gÏ•1to determine whether there is a severe label
distribution shift or not. If sB=Ïƒ(gÏ•1(dB))exceeds 0.5, indicating a severe label distribution shift, the
predictions forBare adjusted using the affine transformations from gÏ•2, i.e., for a test data xâˆˆB, the
prediction is modified from softmax (fÎ¸(x))tosoftmax (fÎ¸(x)WB+bB). Otherwise, we do not modify the
prediction.
For ImageNet-C, the output dimension of gÏ•2increases as the number of classes increases, becoming more
challenging to learn and generate a higher-dimensional square matrix Wandbfor large-scale datasets. To
address this challenge, we modify the prediction refinement module to produce Wwith all off-diagonal entries
set to 0 and fix all entries of bto 0 for ImageNet benchmarks. Moreover, to alleviate the discrepancy in
the softmax prediction confidence between the training and test datasets, we store the confidence of the
pre-trained classifierâ€™s softmax probabilities and perform softmax temperature scaling to ensure that the
prediction confidence of the pre-trained classifier are maintained for the test dataset for each test corruption
for ImageNet-C. Specifically, for the pre-trained classifier fÎ¸0, training dataset D, and test dataset Dtest, we
aim to find Tthat achieves the following:
E(x,Â·)âˆ¼D[max
ksoftmax (fÎ¸0(x))k] =EË†xâˆ¼Dtest[max
ksoftmax (fÎ¸0(Ë†x)/T)k]. (5)
By Taylor series approximation, we can re-write equation 5 as in Hinton et al. (2015):
E(x,Â·)âˆ¼D/bracketleftbigg1 + maxkfÎ¸0(x)k/summationtext
kâ€²(1 +fÎ¸0(x)kâ€²)/bracketrightbigg
=EË†xâˆ¼Dtest/bracketleftbigg1 + maxkfÎ¸0(Ë†x)k/T/summationtext
kâ€²(1 +fÎ¸0(Ë†x)kâ€²/T)/bracketrightbigg
. (6)
With the assumption that the sum of logits,/summationtext
kâ€²fÎ¸0(Â·)kâ€², are constant ltr, sumandlte, sumfor any training
and test data, respectively, we can re-write equation 6 and compute Tas
1 +E(x,Â·)âˆ¼D[maxkfÎ¸0(x)k]
K+ltr, sum=1 +EË†xâˆ¼Dtest[maxkfÎ¸0(Ë†x)k/T]
K+lte, sum/T(7)
T=EË†xâˆ¼Dtest[maxkfÎ¸0(Ë†x)k]âˆ’lte, sum1+E(x,Â·)âˆ¼D[maxkfÎ¸0(x)k]
K+ltr, sum
K1+E(x,Â·)âˆ¼D[maxkfÎ¸0(x)k]
K+ltr, sumâˆ’1, (8)
22Under review as submission to TMLR
whereKis the number of classes. We store the average of the maximum logits and the sum of logits of the
pre-trained classifier on the training data, and use them to calculate the softmax temperature Tat test time.
For each test batch, we calculate Tby equation 8 for the test batch using the maximum logits and the sum
of logits of the test batch using the pre-trained classifier. For the (t+ 1)-th test batch, we perform softmax
temperature scaling using the average value of Tcalculated from the first to the (t+ 1)-th test batches.
Calculating the softmax temperature Tin equation 8 does not require the labels of the test data, ensuring
that it does not violate the test-time adaptation setup.
B.2 Comparison of DART and DART-split
Table 7: Test accuracy of DART-applied BNAdapt and DART-split-applied BNAdapt on CIFAR-10C-LT
Ï= 1Ï= 10Ï= 100
BNAdapt 85.2Â±0.0 79.0Â±0.1 67.0Â±0.1
BNAdapt+DART (ours) 85.2Â±0.1 84.7Â±0.1 85.1Â±0.3
BNAdapt+DART-split (ours) 85.0Â±0.0 78.4Â±0.2 76.1Â±0.4
Both DART and DART-split utilize the averaged pseudo label distribution and prediction deviation to detect
label distribution shifts and correct the inaccurate predictions of a BN-adapted classifier due to the label
distribution shifts, by considering class-wise confusion patterns. However, they differ in the structure of
the prediction refinement module gÏ•. The original DART fed two inputs into a single prediction refinement
module that detects shifts and generates the appropriate affine transformation for each batch. On the other
hand, DART-split divides this module into two distinct modules, each detecting the existence and severity of
the label distribution shift and generating the affine transformation. We use the same hyperparameters for
DART-split as much as possible to those of DART, but Î´for CIFAR-100C is set to 0.01, and the softmax
temperature for CIFAR-10C during testing is set to 1.1.
As shown in Table 1 and 2, we demonstrate that DART and DART-split show consistent performance
improvements under several label distribution shifts in small to mid-scale benchmarks and large-scale
benchmarks, respectively. A possible follow-up question is the efficiency of DART-split in benchmarks like
CIFAR-10C-LT, which are not large-scale benchmarks. As summarized in Table 7, DART-split shows about
a 10% improvement under severe label distribution shifts ( Ï= 100) in CIFAR-10C-LT, and it performs
comparably to naive BNAdapt at lower shifts ( Ï= 1,10). While original DART outperforms DART-split on
CIFAR-10C-LT for all Ïs, DART-splitâ€™s success in significantly enhancing predictions under severe shifts while
maintaining performance under milder conditions is notable. On the other hand, in Table 2, DART-split shows
superior performance on large-scale benchmarks compared to the original DART. Therefore, we recommend
using the original DART for small to mid-scale benchmarks and DART-split for large-scale benchmarks.
B.3 DART-split on Large-scale Benchmark
In Table 8, we present the experimental results on CIFAR-100C-imb and ImageNet-C-imb of several imbalance
ratios. We can observe that DART-split achieves consistent performance improvement of 16.7-39.8% on
CIFAR-100C-imb of imbalance ratios 200-50000, and 0.2-4.8% on ImageNet-C-imb of imbalance ratios
1000-500000, respectively, when combined with BNAdapt. Specifically, on CIFAR-100C-imb, BNAdaptâ€™s
performance significantly decreases as the imbalance ratio increases, falling well below that of NoAdapt.
However, due to the effective prediction refinement by DART-split, it consistently outperforms NoAdapt by
more than 4% across all imbalance ratios. DART-split can also be combined with existing TTA methods
as reported in Table 8. We confirm that our proposed method consistently contributes to performance
improvement in all scenarios. These experiments demonstrate that our proposed method effectively addresses
the performance degradation issues of the classifiers using the BN layer, which were significantly impacted by
label distribution shifts.
23Under review as submission to TMLR
Table 8: Average accuracy (%) of DART-split-applied TTA methods on CIFAR-100C-imb and ImageNet-C-imb
CIFAR-100C-imb ImageNet-C-imb
IR1 IR200 IR500 IR50000 IR1 IR1000 IR5000 IR500000
NoAdapt 41.5Â±0.3 41.5Â±0.3 41.4Â±0.3 41.5Â±0.318.0Â±0.0 18.0Â±0.1 18.0Â±0.0 18.1Â±0.1
BNAdapt 59.8Â±0.5 29.1Â±0.5 18.8Â±0.4 9.3Â±0.131.5Â±0.0 19.8Â±0.1 8.4Â±0.1 3.4Â±0.0
BNAdapt+DART-split (ours) 59.7Â±0.5 45.8Â±0.2 50.2Â±0.3 49.1Â±0.731.5Â±0.0 20.0Â±0.2 11.5Â±0.6 8.2Â±0.5
TENT 62.0Â±0.5 25.4Â±0.4 15.4Â±0.5 7.1Â±0.143.1Â±0.2 18.7Â±0.2 4.3Â±0.1 1.2Â±0.0
TENT+DART-split (ours) 61.9Â±0.5 43.9Â±0.4 48.7Â±0.5 47.7Â±0.743.1Â±0.2 19.2Â±0.2 9.0Â±0.7 6.1Â±0.5
SAR 61.0Â±0.6 26.7Â±0.4 16.5Â±0.4 7.8Â±0.140.5Â±0.1 24.3Â±0.1 9.6Â±0.1 3.5Â±0.0
SAR+DART-split (ours) 60.9Â±0.6 44.7Â±0.3 50.0Â±0.3 49.1Â±0.740.5Â±0.1 24.4Â±0.2 12.1Â±0.6 8.3Â±0.6
ODS 62.4Â±0.4 49.6Â±0.5 43.7Â±0.4 38.7Â±0.343.5Â±0.2 30.3Â±0.1 14.6Â±0.3 9.1Â±0.1
ODS+DART-split (ours) 62.4Â±0.4 52.5Â±0.7 50.7Â±0.7 49.3Â±0.343.5Â±0.2 30.3Â±0.2 17.1Â±0.6 13.3Â±0.6
C More Related Works
C.1 TTA method Utilizing Intermediate Time
Some recent works (Choi et al., 2022; Lim et al., 2022; Park et al., 2023) try to prepare an unknown test-time
distribution shift by utilizing the training dataset at the time after the training phase and before the test time,
called intermediate time. SWR (Choi et al., 2022) and TTN (Lim et al., 2022) compute the importance of
each layer in the trained model during intermediate time and prevent the important layers from significantly
changing during test time. SWR and TTN compute the importance of each layer by computing cosine
similarity between gradient vectors of training data and its augmented data. TTN additionally updates the
importance with subsequent optimization using cross-entropy. Layers with lower importance are encouraged to
change significantly during test time, while layers with higher importance are constrained to change minimally.
On the other hand, our method DART trains a prediction refinement module during intermediate time by
experiencing several batches with diverse class distributions and learning how to modify the predictions
generated by pre-trained classifiers to mitigate the negative effects caused by the class distribution shift of
each batch.
C.2 TTA methods considering Sample-wise Relationships
Some recent works (Boudiaf et al., 2022; Iwasawa & Matsuo, 2021; Jang et al., 2022) focus on prediction
modification using the nearest neighbor information based on the idea that nearest neighbors in the embedding
space of the trained classifier share the same label. T3A (Iwasawa & Matsuo, 2021) replaces the last linear
layer of the trained classifier with the prototypical classifier, which predicts the label of test data to the
nearest prototype representing each class in the embedding space. LAME (Boudiaf et al., 2022) modifies
the prediction of test data by Laplacian-regularized maximum likelihood estimation considering clustering
information.
C.3 TTA methods considering Class-wise Relationships
Some TTA methods (Iwasawa & Matsuo, 2021; Kang et al., 2023; Zhang et al., 2023) focus on preserving
class-wise relationships as domain-invariant information during test time. For instance, the method in (Kang
et al., 2023) aims to minimize differences between the class-wise relationships of the test domain and the
stored one from the training domain. CRS (Zhang et al., 2023) estimates the class-wise relationships and
embeds the source-domain class relationship in contrastive learning. While these methods utilize class-wise
relationships to prevent their deterioration during test-time adaptation, DART takes a different approach
by focusing on directly modifying the predictions. DART considers class-wise confusion patterns to refine
predictions, effectively addressing performance degradation due to label distribution shifts, without explicitly
enforcing preservation of class-wise relationships.
24Under review as submission to TMLR
C.4 Loss Correction Methods for Learning with Label Noise
In learning with label noise (LLN), it is assumed that there exists a noise transition matrix T, which
determines the label-flipping probability of a sample from one class to other classes. For LLN, two main
strategies have been widely used in estimating T: 1) using anchor points (Xia et al., 2019; Yao et al., 2020),
which are defined as the training examples that belong to a particular class almost surely, and 2) using the
clusterability of nearest neighbors of a training example belonging to the same true label class (Zhu et al.,
2021). LLN uses the empirical pseudo-label distribution of the anchor points or nearest neighbors to estimate
T.
For TTA, on the other hand, the misclassification occurs not based on a fixed label-flipping pattern, but
from the combination of covariate shift and label distribution shift. To adjust the pre-trained model against
the covariate shifts, most TTA methods apply the BN adaptation, which updates the Batch Norm statistics
using the test batches. However, when there exists a label distribution shift in addition to the covariate shift,
since the updated BN statistics follow the test label distribution, it induces bias in the classier (by pulling
the decision boundary closer to the head classes and pushing the boundary farther from the tail classes as in
Appendix 2). Thus, the resulting class-wise confusion pattern depends not only on the class-wise relationship
in the embedding space but also on the classifier bias originated from the label distribution shift and the
updated BN statistics. Such a classifier bias has not been a problem for LLN, where we do not modify the
BN statistics of the classifier at the test time.
Our proposed method, DART, focuses on this new class-wise confusion pattern and is built upon the idea
that if the module experiences various batches with diverse class distributions before the test time, it can
develop the ability to refine inaccurate predictions resulting from label distribution shifts.
D Theorectical Analysis
D.1 Motivating Toy Example - Confusion Patterns Reflecting the Class Relationship by BNAdapt
Table 9: Notation and definitions used in Section D.1
Symbol Meaning
N(Âµ,Ïƒ2I2)Multivariate Gaussian distribution with mean Âµand variance Ïƒ2
Âµi Mean vector of class i(i= 1,2,3,4)
Ïƒ2Variance shared across all classes
d,Î² Constants controlling class separation ( d>1,Î² > 1)
âˆ† Test-time distribution shift vector
h(Â·) Bayes classifier for prediction
Â¯h(Â·) Mean-centered Bayes classifier for prediction
ptr(y) Class prior probability during training (uniform: 1/4)
pte(y) Class prior probability during testing (imbalanced distribution)
Âµâ€²
i Shifted mean of class iafter test-time mean centering
Î¦(Â·) Cumulative distribution function (CDF) of the standard normal distribution
To understand the effects of test-time distribution shifts, we consider a Bayes classifier for four-class Gaussian
mixture distribution with mean centering, which mimics batch normalization. Let the distribution of class i
beN(Âµi,Ïƒ2I2)at training time for i= 1,2,3,and4, whereÂµiâˆˆR2is the mean of each class distribution.
We set the mean of each class as Âµ1= (d,Î²d),Âµ2= (âˆ’d,Î²d),Âµ3= (d,âˆ’Î²d),andÂµ4= (âˆ’d,âˆ’Î²d), whereÎ²
controls the distances between the classes, and we assume that Î² >1. Moreover, we assume that the four
classes have the same prior probability at training time, i.e.,ptr(y=i) = 1/4,i= 1,2,3,and 4. Since the
class priors for the training data are equal, the Bayes classifier hpredictsxto the class iwhen
ptr(x|y=i)>ptr(x|y=j), jÌ¸=i (9)
25Under review as submission to TMLR
due to Bayesâ€™ rule. Then, we have
h(x) =ï£±
ï£´ï£´ï£´ï£²
ï£´ï£´ï£´ï£³1,ifx1>0,x2>0;
2,ifx1<0,x2>0;
3,ifx1>0,x2<0;
4,ifx1<0,x2<0.(10)
Initially, we assume that the input data distribution is shifted by âˆ†âˆˆR2at test time as studied in prior
works such as Stojanov et al. (2021); Yi et al. (2023). Specifically, the distribution of class iis modeled as
N(Âµi+ âˆ†,Ïƒ2I2)fori= 1,2,3, and 4 at the test time. Employing mean centering moves the distribution
of classifromN(Âµi+ âˆ†,Ïƒ2I2)toN(Âµi,Ïƒ2I2). Consequently, we demonstrate that the mean centering
effectively mitigates the test-time input data distribution shift.
Furthermore, we consider a scenario where the class distribution is also shifted along with the input data
distribution. We assume that the class distribution is imbalanced, similar to the long-tailed distribution, as
pte(y= 1) =p, (11)
pte(y= 2) = 1/4, (12)
pte(y= 3) = 1/4, (13)
pte(y= 4) = 1/2âˆ’p. (14)
Assume that 1/4<p< 1/2. Due to the mean centering, the distribution of class iis shifted toN(Âµâ€²
i,Ïƒ2I2),
whereÂµâ€²
iis the shifted class mean as follows:
Âµâ€²
1= ((3/2âˆ’2p)d,(3/2âˆ’2p)Î²d), (15)
Âµâ€²
2= ((âˆ’1/2âˆ’2p)d,(3/2âˆ’2p)Î²d), (16)
Âµâ€²
3= ((3/2âˆ’2p)d,(âˆ’1/2âˆ’2p)Î²d), (17)
Âµâ€²
4= ((âˆ’1/2âˆ’2p)d,(âˆ’1/2âˆ’2p)Î²d). (18)
Then, the probability that the samples from class 1 are wrongly classified to class 2 can be computed as
Pr
xâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[h(Norm (x)) = 2] = Pr
xâˆ¼N(Âµâ€²
1,Ïƒ2Id)[h(x) = 2] (19)
= Pr
x=(x1,x2)âˆ¼N(Âµâ€²
1,Ïƒ2I2)[x1<0,x2>0] (20)
= Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)d
Ïƒ/parenrightbigg/braceleftbigg
1âˆ’Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightbigg/bracerightbigg
, (21)
whereNormis a mean centering function and Î¦is the standard normal cumulative density function. Similarly,
the probability that the samples from class 2 are wrongly classified to class 1 can be computed as
Pr
xâˆ¼N(Âµ2+âˆ†,Ïƒ2Id)[h(Norm (x)) = 1] =/braceleftbigg
1âˆ’Î¦/parenleftbigg
âˆ’(âˆ’1/2âˆ’2p)d
Ïƒ/parenrightbigg/bracerightbigg/braceleftbigg
1âˆ’Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightbigg/bracerightbigg
.(22)
Since 1/4< p < 1/2, we have Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 2]>Prxâˆ¼N(Âµ2+âˆ†,Ïƒ2Id)[Â¯h(x) = 1], where
Â¯h(Â·) :=h(Norm (Â·))is modeled BN-adapted classifier. With similar computations, we can obtain
Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) =i]>Prxâˆ¼N(Âµi+âˆ†,Ïƒ2Id)[Â¯h(x) = 1],âˆ€i= 2,3,and4. In other words, the proba-
bility that the samples from the class of a larger number of samples are confused to the rest of the classes is
greater than the inverse direction.
26Under review as submission to TMLR
Figure 8: Illustration of the 4-class Gaussian mixture model from Section D.1. The two lines indicate the
Bayes classifier decision boundaries, partitioning the feature space into regions corresponding to different
classes. The patterned areas highlight regions where misclassifications occur. Specifically, the red patterned
area denotes regions where samples from Class 1 are misclassified as Class 2, while the green patterned area
represents regions where samples from Class 1 are misclassified as Class 3. Two notable confusion patterns
can be observed when both input data and label distribution shifts occur, and mean centering is applied.
First, Class 1, which has a larger sample size, is more prone to misclassification into other classes compared
to smaller classes. Second, misclassifications are more likely to occur toward close classes in the feature space.
For instance, Class 1 is more frequently confused with the closer class (Class 2) than with other classes.
The probability that the samples from class 1 are wrongly classified by Â¯has class 2,3, and 4 can be calculated
as follows:
Pr
xâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 2] = Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)d
Ïƒ/parenrightbigg/braceleftbigg
1âˆ’Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightbigg/bracerightbigg
, (23)
Pr
xâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 3] = Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightbigg/braceleftbigg
1âˆ’Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)d
Ïƒ/parenrightbigg/bracerightbigg
, (24)
Pr
xâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 4] = Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightbigg
Î¦/parenleftbigg
âˆ’(3/2âˆ’2p)d
Ïƒ/parenrightbigg
. (25)
Note that Î¦/parenleftï£¬ig
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightï£¬ig
has the following properties: Since 1/4<p< 1/2,Î¦/parenleftï£¬ig
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightï£¬ig
<1/2; Since
Î² >1,Î¦/parenleftï£¬ig
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightï£¬ig
<Î¦/parenleftï£¬ig
âˆ’(3/2âˆ’2p)d
Ïƒ/parenrightï£¬ig
;âˆ‚
âˆ‚pÎ¦/parenleftï£¬ig
âˆ’(3/2âˆ’2p)Î²d
Ïƒ/parenrightï£¬ig
=C1Î²exp/parenleftï£¬ig
âˆ’(3/2âˆ’2p)2Î²2d2
2Ïƒ2/parenrightï£¬ig
, whereC1is a
positive constant, independent of pandÎ², which decreases as Î²grows forÎ² >Ïƒ
(3/2âˆ’2p)d.
Thus, we can say that
(1)Theprobabilitythatthesamplesfromtheheadclass(class1)areconfusedtotailclassesisgreaterthan
the reverse direction, specifically, Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) =i]>Prxâˆ¼N(Âµi+âˆ†,Ïƒ2Id)[Â¯h(x) = 1],âˆ€iÌ¸= 1,
where Â¯h(Â·) :=h(Norm (Â·))is a composition of mean-centering function and a Bayes classifier obtained
using training dataset, modeling the BN-adapted classifier.
(2)The probability that a sample from the head class is confused to the closer class is larger than that
to the farther classes. Specifically, Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 2]>Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 3]>
Prxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 4].
(3)As the class distribution imbalance p > 1/4increases, the rate at which misclassification to-
wards spatially closer classes increases is greater than towards more distant classes. Specifically,
âˆ‚
âˆ‚pPrxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 2]>âˆ‚
âˆ‚pPrxâˆ¼N(Âµ1+âˆ†,Ïƒ2Id)[Â¯h(x) = 3]when 2Ïƒ<d.
27Under review as submission to TMLR
The illustration of the class-wise confusion pattern caused by test-time input data and label distribution shift
in a 4-class Gaussian mixture model can be found in Figure 8. The effects of test-time label distribution
shift can be consistently observed not only in this toy example but also in real dataset experiments, e.g.,
CIFAR-10C-LT (Section 2).
D.2 Meaning of DARTâ€™s Logit Refinement Scheme
DART outputs a square matrix Wof sizeKand a vector bof sizeKto correct the inaccurate BN-adapted
predictions caused by the test time distribution shifts through an affine transformation of the classifier output
(logit). Since the affine transformation can be interpreted as a simple matrix multiplication, achieved by
modifying the input vector by adding 1 and including the vector into the square matrix, we focus on analyzing
the theoretical meaning of the square matrix Wgenerated by DART. To understand the theoretical meaning
ofWobtained by DARTâ€™s training, we consider K-class classification problem with mean centering, which
has a similar effect as the batch normalization as we discussed in Section D.1. As demonstrated in Section D.1,
the mean centering effectively mitigates a test-time input data distribution shift. Thus, we focus only on the
label distribution shifts in the following analysis. Let the distribution of class ibeN(Âµi,ÏƒId). Generally,
the classifier output (logit) is computed as the product of features and classifier weights. With the mean
centering, the logit for an example xfor classican be represented by the product of mean-centered features
and mean-centered class centroids, i.e.,
l(x)i= (xâˆ’/summationdisplay
aâˆˆ[K]paÂµa)(Âµiâˆ’/summationdisplay
bâˆˆ[K]pbÂµb)T,âˆ€iâˆˆ[K] (26)
wherep= [p1,p2,...,pK]âˆˆR1Ã—Kis the label distribution of training dataset. In the matrix form, the logit
forxcan be computed as
l(x) = (xâˆ’pÂµ)(Âµâˆ’1KÃ—1pÂµ)T= (xâˆ’pÂµ)ÂµT(IKâˆ’1KÃ—1p)T, (27)
where 1KÃ—1is a matrix of size KÃ—1for which all the elements are 1, and Âµ= [Âµ1,Âµ2,...,ÂµK]TâˆˆRKÃ—dis
the concatenated class centroids. We assume that the logits are robust to the changes in the class distribution
of the training dataset, i.e.,
l(x) = (xâˆ’pÂµ)ÂµT(IKâˆ’1KÃ—1p)T= (xâˆ’rÂµ)ÂµT(IKâˆ’1KÃ—1r)T, (28)
whenr= [r1,r2,...,rK]satisfiesriâ‰¥0and/summationtext
iri= 1.
At the test time, when the label distribution is shifted from p= [p1,p2,...,pK]toq= [q1,q2,...,qK], the
mean-centered logit for an example xafter the shift can be computed as
l(x) = (xâˆ’qÂµ)ÂµT(IKâˆ’1KÃ—1p)T, (29)
since only the features are newly mean-centered while the classifier weights are unchanged similar to BNAdapt
(Nado et al., 2020; Schneider et al., 2020).
DART learns the square matrix WâˆˆRKÃ—Kto refine the BN-adapted classifierâ€™s output to the original one
by multiplying it with W. Specifically, we obtain Wâˆ—which minimizes the below optimization problem when
we use the L2 norm,
Wâˆ—= arg min
WExâˆˆR1Ã—d[âˆ¥(xâˆ’pÂµ)ÂµT(IKâˆ’1KÃ—1p)Tâˆ’(xâˆ’qÂµ)ÂµT(IKâˆ’1KÃ—1p)TWâˆ¥2
2](30)
â‰ˆarg min
WExâˆˆR1Ã—d[âˆ¥(xâˆ’qÂµ)ÂµT(IKâˆ’1KÃ—1q)Tâˆ’(xâˆ’qÂµ)ÂµT(IKâˆ’1KÃ—1p)TWâˆ¥2
2](31)
= arg min
WExâˆˆR1Ã—d[âˆ¥(xâˆ’qÂµ){ÂµT(IKâˆ’1KÃ—1q)Tâˆ’ÂµT(IKâˆ’1KÃ—1p)TW}âˆ¥2
2]. (32)
Since we match these two logits for any x, we can rewrite the above equation as follows:
Wâˆ—â‰ˆarg min
Wâˆ¥ÂµT(IKâˆ’1KÃ—1q)Tâˆ’ÂµT(IKâˆ’1KÃ—1p)TWâˆ¥2
2. (33)
28Under review as submission to TMLR
Sincedis greater than Kgenerally, the least-square solution for the above equation is
Wâˆ—={(IKâˆ’1KÃ—1p)ÂµÂµT(IKâˆ’1KÃ—1p)T}âˆ’1(IKâˆ’1KÃ—1p)ÂµÂµT(IKâˆ’1KÃ—1q)T, (34)
with an assumption that (IKâˆ’1KÃ—1p)ÂµÂµT(IKâˆ’1KÃ—1p)Tis invertible. Then, we can observe that Wâˆ—is
determined by only three different components: the label distributions of training and test datasets ( pandq,
resp.) and the class-wise relationship, i.e., the relationship among the class centroids ( ÂµÂµTâˆˆRKÃ—K). In
particular, we can find that Wâˆ—is related to the relationships among class centroids, not the exact locations
of the class centroids.
In contrast to the re-weighting methods (Garg et al., 2023; Hong et al., 2021; Menon et al., 2020), which do
not take into account class relationships for prediction refinement, DART considers class relationships for
prediction refinement as evidenced by the above Wâˆ—. Thus, DART can address the errors with confusion
patterns arising from test-time distribution shifts, in which the label distribution is also shifted.
E Confusion Matrices of BN-adapted Classifiers on CIFAR-10C-LT
In Figure 1, we present confusion matrices of the BN-adapted classifiers for three different corruption types
(Gaussian noise, defocus blur, snow) from each of the three corruption categories (noise, blur, and weather)
among15pre-definedcorruptions. InFigure9-11, wepresentconfusionmatricesforall15corruptionsincluding
clean CIFAR-10 test dataset with various levels of label distribution shifts ( Ï= 1,10,100), respectively.
While test accuracy may not perfectly be matched across the corruptions, we can observe that (1) the
accuracy in head classes (with smaller class index) is significantly decreased and (2) confusion patterns tend
to be consistent across different corruptions as we described in Section 2. Additionally, Figure 9-11 reveal
increasingly pronounced class-wise confusion patterns as the imbalance ratio Ïrises from 1 to 100. Based on
these observations, we conjecture that the prediction refinement module trained only on the training data at
the intermediate time remains consistently effective in modifying predictions for the test datasets.
29Under review as submission to TMLR
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label94.80 0.40 1.60 0.60 0.30 0.00 0.30 0.10 1.60 0.30
0.80 96.80 0.00 0.00 0.00 0.00 0.00 0.00 0.40 2.00
1.10 0.00 91.00 1.80 2.50 1.00 2.00 0.50 0.10 0.00
0.90 0.20 2.30 86.00 1.90 5.70 1.40 0.90 0.30 0.40
0.30 0.10 0.60 1.40 95.20 0.60 0.80 0.50 0.20 0.30
0.20 0.00 1.50 6.00 1.80 88.80 0.50 0.90 0.10 0.20
0.70 0.00 1.20 1.60 0.90 0.40 95.10 0.00 0.00 0.10
0.50 0.00 0.50 0.40 1.50 0.70 0.10 95.80 0.30 0.20
2.20 0.50 0.40 0.30 0.00 0.00 0.00 0.00 96.20 0.40
1.20 2.00 0.10 0.10 0.00 0.00 0.10 0.10 1.10 95.30clean CIFAR-10
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label83.50 0.60 4.40 1.70 1.80 0.20 1.40 0.50 4.30 1.60
1.70 88.90 0.60 0.40 0.00 0.60 0.20 0.20 2.50 4.90
2.40 0.70 75.20 4.40 5.60 4.00 4.70 1.00 1.40 0.60
1.60 1.30 5.90 65.30 4.30 9.60 5.80 2.90 1.50 1.80
2.20 0.20 3.80 3.10 78.80 2.80 5.40 2.30 0.80 0.60
0.60 0.30 3.80 12.10 3.70 71.60 2.50 4.20 0.40 0.80
1.30 0.70 3.50 3.80 2.80 3.10 83.30 0.80 0.60 0.10
1.20 0.20 2.10 1.80 5.00 2.90 0.60 85.00 0.50 0.70
5.30 0.80 1.80 0.90 0.80 0.70 0.50 0.30 87.60 1.30
3.70 5.40 0.20 0.30 0.40 0.20 0.10 0.40 2.30 87.00gaussian_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label80.60 0.10 4.90 2.10 2.40 0.60 1.40 1.00 5.60 1.30
2.10 91.00 0.40 0.60 0.00 0.40 0.20 0.10 1.80 3.40
2.00 0.20 77.60 4.00 6.00 1.70 5.30 1.80 0.90 0.50
1.80 1.00 5.70 67.70 3.60 10.00 4.60 3.20 1.10 1.30
1.50 0.60 3.90 2.70 79.70 2.80 3.80 3.20 0.80 1.00
0.80 0.40 5.00 10.80 3.30 73.80 2.20 2.70 0.30 0.70
1.30 0.40 2.50 5.10 2.40 2.40 84.10 0.90 0.30 0.60
0.70 0.30 1.60 1.70 5.50 1.50 0.30 87.10 0.50 0.80
4.40 0.90 1.50 0.60 0.90 0.20 0.80 0.40 88.90 1.40
2.70 5.00 0.00 0.40 0.20 0.00 0.30 0.40 2.90 88.10shot_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label73.70 0.80 5.50 3.00 2.00 1.00 1.50 1.50 8.10 2.90
3.10 81.10 0.50 1.20 0.60 0.70 1.80 0.80 3.40 6.80
5.90 1.60 59.60 6.10 8.00 2.90 9.60 2.80 1.90 1.60
2.10 2.30 7.50 54.60 4.70 14.00 7.10 4.10 1.20 2.40
2.00 0.90 9.30 6.00 64.70 3.30 6.80 4.20 1.30 1.50
1.30 0.80 6.80 14.80 4.60 60.20 4.40 5.00 0.60 1.50
1.70 0.90 5.50 7.10 4.30 3.90 73.90 1.50 0.50 0.70
2.40 0.60 4.20 3.20 7.70 4.40 0.80 74.60 1.00 1.10
9.80 2.70 1.20 1.00 1.20 0.70 1.50 0.50 77.90 3.50
3.30 4.10 1.00 1.30 0.20 1.70 0.90 1.80 5.20 80.50impulse_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label92.50 0.30 1.50 0.60 0.50 0.10 0.60 0.50 2.20 1.20
1.00 95.10 0.10 0.10 0.00 0.00 0.20 0.00 0.50 3.00
1.50 0.10 88.80 2.30 2.80 1.10 2.50 0.40 0.40 0.10
0.60 0.30 3.00 81.10 3.00 7.60 1.70 1.30 0.50 0.90
0.40 0.10 0.80 2.20 93.20 0.80 1.10 0.70 0.20 0.50
0.30 0.10 1.60 7.30 2.20 86.10 0.60 1.40 0.10 0.30
0.80 0.20 2.20 1.70 1.00 1.10 92.90 0.00 0.00 0.10
0.50 0.00 0.20 0.50 2.30 0.70 0.10 94.80 0.60 0.30
2.50 0.70 0.40 0.30 0.10 0.00 0.10 0.00 95.50 0.40
1.80 2.20 0.10 0.20 0.00 0.10 0.10 0.20 1.00 94.30defocus_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label83.40 1.00 2.80 1.60 1.00 0.20 1.50 0.80 5.30 2.40
3.30 81.30 0.30 0.80 0.10 0.50 1.00 0.70 3.20 8.80
4.20 0.70 75.50 5.40 5.40 2.10 3.60 1.40 1.20 0.50
2.10 0.80 4.50 67.70 4.40 11.80 3.50 2.90 1.10 1.20
1.00 0.40 3.20 3.80 81.10 2.80 3.90 2.60 0.80 0.40
0.90 0.50 3.60 11.20 3.40 74.20 1.70 3.10 0.60 0.80
1.30 0.70 3.20 4.00 2.80 1.00 85.80 0.70 0.10 0.40
2.70 0.50 1.60 2.40 4.50 2.00 0.30 83.50 1.40 1.10
7.30 1.10 1.40 0.60 0.30 0.70 0.40 0.20 86.50 1.50
4.70 5.20 0.60 1.10 0.50 0.50 0.70 1.20 3.70 81.80glass_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label91.80 0.60 1.20 1.00 0.40 0.10 0.70 0.30 3.10 0.80
1.10 92.90 0.40 0.60 0.00 0.10 0.60 0.10 0.80 3.40
2.10 0.20 85.00 3.00 2.90 1.70 3.20 0.90 0.40 0.60
1.70 0.70 4.50 75.40 3.30 8.60 2.80 1.70 0.50 0.80
0.70 0.00 1.90 1.70 89.20 1.40 2.30 1.20 0.80 0.80
0.60 0.10 2.50 9.20 2.50 80.70 1.60 1.70 0.40 0.70
1.00 0.00 3.30 2.90 1.30 0.60 90.10 0.30 0.20 0.30
0.90 0.00 0.80 0.60 3.00 1.40 0.20 92.00 0.30 0.80
3.40 1.30 0.80 0.60 0.30 0.20 0.30 0.10 92.50 0.50
2.90 4.10 0.10 1.00 0.20 0.40 0.70 0.20 1.90 88.50motion_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label95.70 0.40 1.00 0.20 0.30 0.10 0.40 0.20 1.50 0.20
0.80 95.90 0.20 0.20 0.10 0.10 0.00 0.10 0.50 2.10
1.00 0.30 89.60 2.60 2.90 1.10 1.60 0.70 0.20 0.00
0.60 0.20 3.00 83.40 3.40 6.10 1.30 1.00 0.60 0.40
0.20 0.00 0.80 0.70 96.80 0.40 0.40 0.60 0.00 0.10
0.50 0.00 1.30 6.50 2.10 87.50 0.60 1.10 0.10 0.30
0.90 0.10 1.50 1.50 1.80 0.70 93.30 0.10 0.00 0.10
0.60 0.10 0.40 0.70 1.30 1.00 0.10 95.50 0.30 0.00
2.50 0.80 0.50 0.20 0.00 0.10 0.10 0.00 95.30 0.50
2.00 1.90 0.20 0.30 0.00 0.00 0.10 0.20 1.40 93.90zoom_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label87.00 0.80 2.20 1.30 1.30 0.40 0.50 0.40 4.50 1.60
1.30 92.60 0.30 0.00 0.10 0.20 0.40 0.00 1.20 3.90
3.70 0.00 80.90 4.20 3.40 2.30 2.90 1.20 0.90 0.50
1.70 0.40 3.10 77.30 3.50 8.00 2.30 1.50 0.80 1.40
1.00 0.20 2.30 3.00 87.30 0.70 2.50 1.80 0.60 0.60
0.70 0.00 3.80 8.20 2.30 80.70 1.40 1.90 0.20 0.80
1.20 0.00 1.50 2.60 2.20 0.90 90.40 0.20 0.50 0.50
1.20 0.10 1.50 1.20 3.20 1.80 0.10 90.30 0.50 0.10
7.00 0.50 1.30 1.10 0.20 0.20 0.10 0.40 87.50 1.70
3.20 3.90 0.30 0.60 0.20 0.30 0.40 0.10 2.60 88.40snow
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label87.30 0.40 2.10 1.90 1.60 0.10 0.60 0.50 4.40 1.10
1.10 93.50 0.00 0.20 0.10 0.40 0.50 0.20 1.10 2.90
1.80 0.10 86.00 2.80 3.20 1.50 3.40 0.60 0.40 0.20
1.40 0.10 3.50 78.60 3.20 7.60 2.10 1.60 0.90 1.00
1.30 0.00 1.90 1.80 89.70 0.90 1.90 1.50 0.70 0.30
0.70 0.00 2.80 8.30 1.60 82.70 1.40 1.90 0.20 0.40
0.90 0.20 2.60 2.00 1.50 1.10 91.20 0.10 0.10 0.30
0.50 0.10 0.70 1.40 2.60 1.30 0.40 92.50 0.20 0.30
3.80 0.90 1.30 0.40 0.20 0.10 0.80 0.00 91.50 1.00
1.60 3.70 0.10 0.20 0.10 0.10 0.10 0.30 2.00 91.80frost
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label80.10 0.70 4.40 3.00 1.60 1.10 0.90 0.60 5.00 2.60
1.70 90.00 0.20 0.50 0.00 0.10 0.10 0.20 2.20 5.00
2.70 0.50 80.70 4.10 3.20 2.20 4.60 0.60 1.00 0.40
2.10 0.60 4.50 73.30 3.50 8.80 3.30 1.70 0.80 1.40
1.40 0.50 2.80 3.20 83.40 1.70 3.70 1.70 1.00 0.60
0.90 0.40 2.90 9.90 3.20 77.70 2.00 2.10 0.30 0.60
1.30 0.10 2.20 3.50 2.10 1.00 89.20 0.00 0.10 0.50
1.20 0.30 1.40 1.60 4.40 2.20 0.40 86.80 1.00 0.70
5.10 0.60 1.60 1.40 1.20 0.60 0.80 0.70 86.40 1.60
2.00 3.60 0.20 0.30 0.20 0.30 0.20 0.50 4.30 88.40fog
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label93.80 0.30 1.60 0.60 0.40 0.10 0.60 0.10 2.00 0.50
0.60 95.20 0.00 0.00 0.00 0.00 0.20 0.10 0.80 3.10
1.60 0.00 88.90 1.70 2.80 1.00 2.80 0.80 0.10 0.30
1.40 0.40 2.40 81.60 1.80 8.20 1.70 1.40 0.50 0.60
0.50 0.20 1.10 1.60 93.10 1.00 1.20 0.70 0.30 0.30
0.10 0.00 1.80 6.40 2.10 86.60 0.90 1.60 0.20 0.30
0.80 0.10 1.80 2.50 1.40 0.60 92.40 0.00 0.20 0.20
0.70 0.00 0.60 0.60 2.60 0.90 0.10 93.90 0.50 0.10
5.10 0.70 0.70 0.60 0.00 0.10 0.10 0.00 91.90 0.80
1.40 2.30 0.00 0.20 0.00 0.00 0.20 0.20 1.70 94.00brightness
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label91.50 0.20 2.20 1.40 0.70 0.20 0.20 0.00 2.80 0.80
0.80 94.90 0.00 0.20 0.00 0.20 0.20 0.00 1.00 2.70
2.60 0.00 84.90 3.20 3.00 0.70 4.60 0.30 0.50 0.20
1.70 0.50 2.90 80.60 2.60 6.30 2.30 1.70 0.60 0.80
0.70 0.30 1.60 2.20 90.40 0.60 2.60 0.50 0.80 0.30
0.40 0.00 2.00 7.60 2.40 84.90 1.50 0.70 0.30 0.20
0.90 0.00 0.70 4.10 0.60 0.90 92.30 0.20 0.10 0.20
0.80 0.20 0.70 0.60 3.40 1.40 0.40 92.00 0.30 0.20
4.20 0.10 1.20 0.20 0.20 0.10 0.20 0.10 93.30 0.40
2.20 1.60 0.20 0.40 0.20 0.10 0.00 0.30 4.00 91.00contrast
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label87.60 1.10 2.70 1.20 0.50 0.10 0.70 0.70 4.30 1.10
2.70 87.00 0.50 0.50 0.00 1.10 0.80 0.10 1.80 5.50
3.10 0.20 79.60 3.10 5.40 2.20 3.70 1.50 0.40 0.80
1.70 0.60 5.60 71.30 3.90 9.80 2.20 2.80 0.40 1.70
0.60 0.30 2.70 2.30 86.80 1.80 1.60 2.40 1.00 0.50
0.40 0.40 2.80 12.10 2.80 76.20 0.90 3.20 0.70 0.50
1.00 0.20 2.30 3.40 2.40 1.20 88.00 0.40 0.30 0.80
1.10 0.00 1.30 2.40 4.20 2.40 0.10 87.50 0.70 0.30
6.50 1.20 1.20 0.60 0.30 0.20 0.50 0.20 88.30 1.00
3.30 4.40 0.30 1.20 0.20 0.60 0.50 1.00 1.90 86.60elastic_transform
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label92.00 0.50 1.50 0.80 0.30 0.20 0.60 0.60 2.70 0.80
1.00 95.00 0.00 0.10 0.00 0.00 0.10 0.10 0.60 3.10
1.60 0.20 87.70 2.20 3.00 1.40 2.60 0.50 0.50 0.30
1.00 0.10 2.20 80.20 3.50 7.60 2.10 1.50 0.50 1.30
0.70 0.10 1.00 2.40 91.50 1.00 1.60 0.90 0.40 0.40
0.40 0.10 1.90 8.50 2.20 83.20 0.90 1.70 0.40 0.70
0.90 0.00 1.60 1.60 1.30 0.90 93.40 0.10 0.00 0.20
0.60 0.00 0.50 0.80 2.40 0.50 0.30 93.90 0.50 0.50
3.30 0.80 0.70 0.10 0.40 0.00 0.20 0.00 93.90 0.60
1.60 2.70 0.10 0.10 0.00 0.10 0.10 0.20 1.60 93.50pixelate
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=1000)
2(n=1000)
3(n=1000)
4(n=1000)
5(n=1000)
6(n=1000)
7(n=1000)
8(n=1000)
9(n=1000)True label85.40 1.30 2.80 1.70 1.30 0.40 0.80 0.80 4.00 1.50
1.30 86.40 0.70 0.40 0.30 0.90 0.70 0.40 2.10 6.80
3.90 0.20 72.20 4.70 7.80 2.20 5.50 1.90 1.10 0.50
1.70 1.00 4.60 62.40 4.70 13.10 5.30 3.80 1.50 1.90
1.70 0.50 3.80 4.50 77.50 3.00 3.40 3.60 1.20 0.80
0.90 0.80 3.30 12.70 3.00 72.80 2.90 2.90 0.40 0.30
0.90 0.50 3.00 7.00 3.70 2.60 81.40 0.40 0.20 0.30
1.80 0.40 1.40 2.40 4.90 2.20 0.40 85.10 0.90 0.50
4.80 1.00 1.30 0.50 0.40 0.30 0.50 0.20 89.90 1.10
2.90 3.50 0.30 0.30 0.30 0.30 0.60 0.20 2.90 88.70jpeg_compression
Figure 9: Confusion matrices of BN-adapted classifiers on CIFAR-10C-LT with Ï= 1. We mark the cases
where the confusion rate exceeds 11% with red squares.
30Under review as submission to TMLR
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label82.60 0.10 2.00 1.10 0.80 0.70 1.60 1.30 6.80 3.00
0.26 89.92 0.13 0.00 0.00 0.13 0.00 0.13 1.94 7.49
0.33 0.00 87.15 1.84 3.17 1.50 3.84 1.17 0.50 0.50
0.00 0.00 0.22 83.62 1.94 9.27 2.16 1.72 0.22 0.86
0.00 0.28 0.28 0.56 96.10 0.56 0.84 0.56 0.56 0.28
0.00 0.00 0.00 4.32 1.80 91.01 1.44 1.44 0.00 0.00
0.47 0.00 0.47 0.93 0.93 0.47 96.74 0.00 0.00 0.00
0.60 0.00 0.00 0.00 0.00 0.00 0.00 98.80 0.00 0.60
0.00 0.00 0.78 0.00 0.00 0.00 0.00 0.00 99.22 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00clean CIFAR-10
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label68.30 0.20 4.70 3.70 3.00 0.80 2.30 2.10 10.50 4.40
0.78 81.78 0.39 0.26 0.00 0.90 0.39 0.90 5.30 9.30
0.50 0.83 68.78 3.84 6.68 5.51 7.51 3.51 1.50 1.34
0.22 0.43 2.37 62.93 4.53 13.36 6.90 3.45 1.94 3.88
0.28 0.00 3.06 2.51 79.94 3.06 5.57 3.62 1.39 0.56
0.00 0.36 0.72 10.79 2.88 74.10 1.08 9.35 0.72 0.00
0.00 0.00 0.93 3.72 1.40 5.12 84.19 1.40 1.40 1.86
0.60 0.60 0.60 1.20 1.20 1.81 0.60 92.77 0.00 0.60
1.55 0.78 1.55 0.00 0.00 0.00 0.78 0.00 93.80 1.55
0.00 1.00 0.00 1.00 0.00 0.00 0.00 0.00 3.00 95.00gaussian_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label65.60 0.20 4.90 3.30 3.80 1.10 2.50 2.10 11.70 4.80
0.39 85.01 0.26 0.90 0.13 1.03 0.39 0.78 3.75 7.36
0.67 0.00 70.78 3.34 6.84 3.34 8.01 3.51 1.50 2.00
0.22 0.22 1.94 64.87 3.66 13.36 5.82 5.82 1.51 2.59
0.28 0.28 3.34 3.06 77.16 3.62 5.01 4.74 1.95 0.56
0.00 0.72 1.44 7.19 3.24 75.90 3.24 5.76 0.72 1.80
0.47 0.00 0.47 3.26 3.26 3.26 86.05 2.33 0.47 0.47
0.00 0.00 0.60 1.20 3.61 0.00 0.00 93.37 0.60 0.60
1.55 0.00 0.00 0.78 0.78 0.78 0.78 0.00 94.57 0.78
0.00 2.00 0.00 0.00 0.00 0.00 0.00 0.00 2.00 96.00shot_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label59.10 0.80 5.40 4.90 4.00 2.20 2.70 3.00 12.60 5.30
2.20 72.74 0.65 1.03 0.78 1.29 2.20 1.55 5.17 12.40
2.50 0.50 57.93 6.68 8.35 4.01 11.35 4.17 1.67 2.84
0.65 0.86 4.96 53.45 3.66 17.24 7.76 5.60 1.72 4.09
0.28 0.56 5.85 5.29 65.74 4.46 8.36 6.13 1.67 1.67
0.36 0.00 4.32 13.31 5.76 63.67 4.32 6.12 0.72 1.44
0.47 0.47 3.72 5.58 3.72 5.58 76.74 2.79 0.47 0.47
0.00 0.00 1.81 2.41 4.82 2.41 0.00 86.14 1.20 1.20
6.20 0.00 0.00 1.55 1.55 0.78 1.55 0.78 82.95 4.65
0.00 0.00 0.00 1.00 0.00 1.00 0.00 3.00 7.00 88.00impulse_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label79.10 0.10 1.80 1.50 1.10 0.50 1.70 2.40 7.90 3.90
0.26 87.98 0.00 0.00 0.13 0.39 0.39 0.26 2.58 8.01
0.33 0.00 83.31 3.34 3.34 2.17 4.84 1.00 1.00 0.67
0.22 0.00 0.43 80.39 2.80 10.56 2.16 1.72 0.43 1.29
0.00 0.28 0.56 1.11 93.04 0.84 1.67 1.39 0.84 0.28
0.00 0.00 0.36 3.24 1.44 90.65 1.08 3.24 0.00 0.00
0.47 0.00 0.93 1.40 0.47 1.86 94.88 0.00 0.00 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 97.59 1.20 1.20
0.00 0.00 0.78 0.00 0.00 0.00 0.00 0.00 98.45 0.78
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00defocus_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label72.10 0.80 3.00 2.70 1.30 0.50 2.10 2.10 11.10 4.30
2.07 69.51 0.26 0.65 0.39 1.29 2.58 1.03 6.72 15.50
2.00 0.00 70.12 6.18 5.84 3.67 7.35 2.67 1.67 0.50
0.43 0.22 2.37 66.38 3.02 14.87 3.88 4.74 1.29 2.80
0.56 0.28 1.95 3.62 80.50 2.79 5.29 3.34 1.39 0.28
0.00 0.00 2.52 8.27 5.40 74.46 2.16 4.32 1.44 1.44
0.47 0.93 1.40 4.19 2.79 3.72 85.58 0.47 0.00 0.47
0.60 0.60 0.60 0.60 3.61 0.60 0.00 90.36 1.20 1.81
1.55 0.00 1.55 0.00 0.00 1.55 1.55 0.78 91.47 1.55
0.00 0.00 0.00 1.00 0.00 0.00 2.00 0.00 3.00 94.00glass_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label78.10 0.10 1.90 2.20 1.40 0.70 2.30 2.30 8.40 2.60
0.65 83.98 0.13 0.78 0.00 1.03 1.42 0.52 2.58 8.91
1.00 0.00 80.47 3.01 3.51 3.01 5.18 2.50 0.67 0.67
0.43 0.00 1.94 72.20 3.02 13.58 3.45 1.72 1.08 2.59
0.00 0.00 1.11 1.39 89.97 1.11 2.51 1.67 1.39 0.84
0.00 0.00 0.36 5.40 2.88 82.01 2.88 4.32 1.08 1.08
0.00 0.00 2.79 2.79 1.40 0.93 91.16 0.47 0.47 0.00
1.20 0.00 0.60 0.60 0.60 0.00 0.00 95.78 0.60 0.60
0.78 0.00 1.55 0.00 0.00 0.00 1.55 0.00 96.12 0.00
0.00 1.00 1.00 1.00 0.00 0.00 1.00 1.00 2.00 93.00motion_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label81.30 0.20 1.80 0.80 1.00 0.20 1.30 2.90 7.90 2.60
0.13 85.53 0.13 0.00 0.00 0.13 0.65 0.90 2.84 9.69
0.50 0.00 84.47 3.17 4.34 2.17 3.01 1.50 0.67 0.17
0.00 0.00 0.43 81.68 3.45 9.05 2.16 2.37 0.00 0.86
0.00 0.00 0.84 0.56 95.54 0.56 0.28 1.95 0.00 0.28
0.00 0.00 0.00 3.24 1.80 92.45 0.72 1.44 0.36 0.00
0.47 0.00 0.47 1.40 2.33 0.47 94.88 0.00 0.00 0.00
0.60 0.00 0.00 0.00 0.60 0.00 0.00 98.19 0.60 0.00
0.00 0.00 1.55 0.00 0.00 0.00 0.00 0.00 98.45 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 99.00zoom_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label73.90 0.50 1.90 2.50 2.50 0.60 2.20 2.20 9.10 4.60
0.52 83.85 0.26 0.26 0.00 0.78 0.90 0.26 3.62 9.56
1.34 0.00 75.79 4.34 3.84 2.84 6.51 3.01 1.17 1.17
0.86 0.00 1.51 75.00 2.80 10.34 2.80 2.59 0.86 3.23
0.28 0.28 1.67 1.95 86.35 0.28 3.34 2.23 1.95 1.67
0.00 0.00 2.16 6.47 3.24 82.01 1.80 3.24 0.36 0.72
0.47 0.00 0.00 2.79 2.79 0.00 91.16 2.33 0.47 0.00
0.00 0.00 0.60 0.60 1.81 0.60 0.00 95.78 0.00 0.60
2.33 0.00 0.00 0.00 0.00 0.00 0.00 0.78 93.80 3.10
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.00 98.00snow
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label72.10 0.30 2.50 2.90 3.70 0.60 2.30 2.10 10.20 3.30
0.78 85.92 0.00 0.39 0.13 0.52 0.65 0.13 2.45 9.04
0.67 0.00 80.13 2.67 4.34 2.50 5.51 1.84 1.17 1.17
0.43 0.22 1.51 75.43 2.80 10.78 2.59 2.80 1.08 2.37
0.28 0.28 1.95 1.39 89.42 1.67 0.84 2.79 1.11 0.28
0.00 0.00 1.44 5.40 1.80 86.33 1.80 2.52 0.36 0.36
0.00 0.00 1.40 1.86 1.86 1.40 92.56 0.93 0.00 0.00
0.00 0.00 0.00 0.00 1.81 0.60 0.00 96.39 0.60 0.60
1.55 0.00 0.78 0.78 0.00 0.78 0.78 0.00 94.57 0.78
1.00 0.00 0.00 1.00 0.00 0.00 0.00 1.00 0.00 97.00frost
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label64.20 0.20 3.70 4.60 2.80 2.00 1.60 3.30 11.80 5.80
0.78 82.82 0.13 0.39 0.00 0.78 0.39 0.52 3.36 10.85
1.00 0.17 75.29 3.01 3.01 4.34 8.51 2.34 1.34 1.00
0.22 0.43 2.37 70.26 3.45 12.07 5.17 2.80 1.29 1.94
0.28 0.00 1.67 1.67 84.12 4.18 4.74 1.39 0.84 1.11
0.00 0.36 2.16 6.12 3.96 78.78 3.96 4.32 0.00 0.36
0.93 0.00 0.47 3.72 1.40 1.86 91.63 0.00 0.00 0.00
0.60 0.00 1.81 0.00 3.01 1.20 0.00 91.57 1.20 0.60
0.78 0.00 1.55 1.55 0.00 0.78 0.78 1.55 92.25 0.78
0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 2.00 97.00fog
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label81.20 0.00 2.30 1.20 1.30 0.50 1.30 1.30 7.40 3.50
0.52 86.95 0.00 0.13 0.00 0.00 0.13 0.13 2.71 9.43
0.67 0.00 84.31 1.67 3.34 1.67 5.18 1.84 0.83 0.50
0.22 0.00 0.43 79.74 2.37 10.78 2.80 1.72 1.08 0.86
0.00 0.28 0.56 1.11 93.04 1.39 1.67 1.11 0.56 0.28
0.00 0.00 0.72 3.24 1.08 89.21 2.52 2.52 0.00 0.72
0.00 0.47 0.47 1.86 1.86 0.47 93.95 0.93 0.00 0.00
0.00 0.60 0.00 0.00 0.60 0.60 0.00 96.99 0.60 0.60
0.78 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.22 0.00
0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 1.00 98.00brightness
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label77.00 0.10 1.70 2.80 1.50 0.90 1.90 1.30 9.80 3.00
0.39 86.43 0.00 0.39 0.00 0.13 0.39 0.39 4.01 7.88
1.17 0.00 77.96 3.17 3.84 2.17 7.51 2.34 1.00 0.83
0.22 0.00 0.86 81.25 2.16 7.54 4.31 1.94 0.43 1.29
0.00 0.00 0.84 1.67 90.53 0.84 3.90 0.56 1.39 0.28
0.00 0.00 1.44 4.68 1.80 86.33 2.16 3.60 0.00 0.00
0.00 0.00 0.47 1.40 0.47 0.93 96.74 0.00 0.00 0.00
0.60 0.00 0.00 0.00 1.20 1.20 0.00 95.78 0.60 0.60
0.78 0.00 0.78 0.00 0.00 0.78 0.00 0.00 97.67 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 3.00 97.00contrast
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label72.90 0.50 3.20 2.60 1.70 0.70 1.90 2.60 9.90 4.00
1.29 75.97 0.13 0.90 0.13 1.29 1.42 0.52 4.78 13.57
1.34 0.33 74.79 3.84 5.51 3.51 5.34 3.01 1.00 1.34
0.22 0.22 1.29 69.83 3.45 14.01 3.45 4.31 0.86 2.37
0.84 0.00 2.23 3.34 82.73 2.51 2.79 4.46 0.84 0.28
0.00 0.00 0.72 6.83 4.68 80.94 1.80 3.24 0.72 1.08
0.47 0.00 1.40 3.72 1.40 2.33 90.23 0.00 0.00 0.47
1.20 0.00 0.00 0.60 0.60 0.60 0.00 95.78 0.60 0.60
0.78 0.00 0.78 0.78 0.78 1.55 2.33 0.78 90.70 1.55
0.00 1.00 0.00 1.00 0.00 0.00 0.00 1.00 1.00 96.00elastic_transform
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label78.20 0.20 2.00 1.60 0.70 0.60 1.70 2.00 8.90 4.10
0.52 84.63 0.00 0.52 0.00 0.39 0.78 0.26 3.49 9.43
0.50 0.00 80.80 3.34 4.01 2.34 5.34 1.50 1.00 1.17
0.22 0.00 0.22 76.94 2.37 12.07 3.45 2.37 0.43 1.94
0.00 0.00 0.84 1.95 90.53 1.95 1.95 1.11 1.11 0.56
0.00 0.00 0.72 4.68 1.80 87.05 2.52 2.16 0.72 0.36
0.00 0.47 0.47 1.40 0.93 0.93 95.81 0.00 0.00 0.00
0.00 0.00 0.00 0.00 0.60 0.00 0.00 97.59 0.60 1.20
0.78 0.00 0.78 0.00 0.78 0.78 0.78 0.00 96.12 0.00
0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 98.00pixelate
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=774)
2(n=599)
3(n=464)
4(n=359)
5(n=278)
6(n=215)
7(n=166)
8(n=129)
9(n=100)True label72.40 0.70 2.70 2.90 1.90 0.70 1.50 2.40 10.10 4.70
0.39 77.91 0.26 0.78 0.26 1.68 0.90 0.65 3.88 13.31
2.84 0.00 67.61 4.34 9.35 3.17 7.68 2.50 1.84 0.67
0.00 0.22 2.59 61.85 3.88 15.09 5.82 4.74 1.94 3.88
0.28 0.28 1.95 4.46 79.11 3.06 4.18 3.90 2.51 0.28
0.00 0.00 1.08 11.87 1.80 75.18 4.32 4.32 0.36 1.08
0.00 0.47 1.86 6.98 3.26 1.86 84.19 0.93 0.00 0.47
0.60 1.20 0.60 0.60 2.41 0.60 0.60 90.96 1.81 0.60
1.55 0.00 1.55 0.00 0.78 0.78 3.10 0.00 92.25 0.00
2.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 3.00 94.00jpeg_compression
Figure 10: Confusion matrices of BN-adapted classifiers on CIFAR-10C-LT with Ï= 10. We mark the cases
where the confusion rate exceeds 11% with red squares. We can observe notable accuracy decreases in classes
with large amounts of data, and similar confusing patterns regardless of the corruption types. Compared to
Ï= 1, we can observe more pronounced class-wise confusion patterns.
31Under review as submission to TMLR
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label63.20 0.10 2.80 3.30 3.70 2.00 3.40 4.70 10.90 5.90
0.00 83.31 0.17 0.17 0.00 0.50 1.84 0.50 2.50 11.02
0.00 0.00 82.45 3.06 5.01 1.95 4.46 2.51 0.56 0.00
0.00 0.00 0.00 77.21 2.79 14.42 4.19 0.47 0.47 0.47
0.00 0.00 0.00 1.55 86.82 4.65 3.10 3.10 0.78 0.00
0.00 0.00 0.00 1.30 2.60 92.21 1.30 2.60 0.00 0.00
0.00 0.00 0.00 0.00 2.17 0.00 97.83 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
6.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 93.75 0.00
0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 90.00clean CIFAR-10
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label48.60 0.30 6.60 5.50 6.30 1.90 5.30 5.60 13.00 6.90
0.17 74.12 0.50 0.83 0.67 1.50 1.17 2.00 5.01 14.02
0.00 0.00 62.95 5.01 8.36 6.41 10.58 5.01 0.28 1.39
0.00 0.00 1.86 56.28 4.19 20.47 10.23 3.72 0.47 2.79
0.00 0.00 2.33 5.43 73.64 4.65 8.53 5.43 0.00 0.00
0.00 0.00 0.00 6.49 5.19 77.92 3.90 6.49 0.00 0.00
0.00 0.00 0.00 2.17 0.00 2.17 93.48 2.17 0.00 0.00
0.00 0.00 0.00 0.00 3.70 3.70 0.00 88.89 0.00 3.70
6.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 87.50 6.25
0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 90.00gaussian_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label48.90 0.10 7.20 5.30 7.40 2.20 4.60 4.80 12.90 6.60
0.50 78.46 0.17 1.84 0.17 2.17 0.33 2.34 4.34 9.68
0.00 0.00 67.69 4.74 6.96 3.62 10.86 3.90 1.11 1.11
0.00 0.00 1.40 55.81 5.12 20.93 9.30 5.12 0.47 1.86
0.78 0.00 3.10 3.10 72.87 9.30 4.65 5.43 0.00 0.78
0.00 0.00 1.30 5.19 1.30 80.52 5.19 6.49 0.00 0.00
0.00 0.00 0.00 4.35 2.17 0.00 91.30 0.00 0.00 2.17
0.00 0.00 0.00 3.70 3.70 0.00 3.70 85.19 0.00 3.70
6.25 0.00 0.00 0.00 6.25 0.00 0.00 0.00 87.50 0.00
0.00 0.00 10.00 0.00 0.00 0.00 10.00 0.00 0.00 80.00shot_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label43.20 0.60 7.90 6.90 6.60 4.30 3.60 5.90 15.10 5.90
0.67 66.61 1.17 1.34 1.34 3.51 4.34 2.00 5.68 13.36
1.67 0.56 51.25 7.52 11.70 5.01 14.48 3.34 1.39 3.06
0.00 0.00 1.86 51.16 4.19 19.07 11.63 6.51 1.40 4.19
0.00 0.78 7.75 4.65 62.79 5.43 10.08 6.98 0.78 0.78
0.00 0.00 3.90 5.19 5.19 66.23 10.39 7.79 0.00 1.30
0.00 0.00 0.00 8.70 2.17 0.00 84.78 4.35 0.00 0.00
0.00 0.00 3.70 0.00 3.70 3.70 0.00 81.48 3.70 3.70
0.00 0.00 0.00 0.00 6.25 0.00 0.00 0.00 81.25 12.50
0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 10.00 80.00impulse_noise
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label58.90 0.20 4.00 4.10 4.00 2.30 3.70 5.40 11.20 6.20
0.33 79.13 0.50 0.83 0.33 1.00 1.84 1.17 2.50 12.35
0.00 0.00 78.55 2.79 5.57 2.23 7.24 2.79 0.84 0.00
0.00 0.00 0.00 69.77 5.12 16.28 3.72 3.72 0.00 1.40
0.00 0.00 0.00 1.55 86.82 5.43 3.10 2.33 0.00 0.78
0.00 0.00 1.30 1.30 2.60 92.21 1.30 1.30 0.00 0.00
0.00 0.00 0.00 0.00 2.17 0.00 97.83 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 0.00
0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 90.00defocus_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label54.20 0.40 5.10 5.70 5.10 2.50 5.00 3.70 12.90 5.40
1.17 58.76 0.50 1.17 1.00 2.34 5.18 2.84 5.51 21.54
0.56 0.00 67.13 6.13 6.41 3.90 9.19 3.34 2.51 0.84
0.00 0.47 1.40 57.67 6.51 22.33 4.65 5.12 0.47 1.40
0.00 0.78 0.00 4.65 75.97 6.98 5.43 5.43 0.78 0.00
0.00 0.00 1.30 7.79 6.49 71.43 3.90 9.09 0.00 0.00
0.00 0.00 0.00 0.00 4.35 2.17 93.48 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 7.41 0.00 88.89 0.00 0.00
0.00 0.00 0.00 6.25 6.25 0.00 0.00 0.00 87.50 0.00
0.00 10.00 0.00 0.00 0.00 0.00 0.00 10.00 10.00 70.00glass_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label58.10 0.10 4.00 4.70 4.60 2.30 4.50 5.40 11.40 4.90
0.33 74.96 0.33 1.34 0.33 1.67 2.67 1.67 3.51 13.19
0.00 0.00 76.32 3.62 4.74 1.67 8.36 4.46 0.28 0.56
0.00 0.00 0.93 69.77 3.26 16.74 4.65 2.33 0.47 1.86
0.00 0.00 0.00 1.55 83.72 4.65 5.43 3.10 0.78 0.78
0.00 0.00 1.30 6.49 2.60 81.82 2.60 5.19 0.00 0.00
0.00 0.00 0.00 0.00 0.00 2.17 97.83 0.00 0.00 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 0.00 0.00
0.00 0.00 0.00 0.00 6.25 0.00 0.00 0.00 93.75 0.00
0.00 0.00 10.00 0.00 0.00 0.00 0.00 0.00 0.00 90.00motion_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label61.90 0.10 3.90 2.50 3.50 1.90 3.60 6.10 11.30 5.20
0.17 79.13 0.00 0.17 0.83 0.67 1.34 1.34 3.67 12.69
0.00 0.00 80.50 2.79 8.08 1.95 4.18 2.51 0.00 0.00
0.00 0.00 0.00 72.56 5.12 16.74 2.33 2.79 0.00 0.47
0.00 0.00 0.00 1.55 92.25 1.55 1.55 3.10 0.00 0.00
0.00 0.00 0.00 1.30 3.90 93.51 0.00 1.30 0.00 0.00
0.00 0.00 0.00 0.00 2.17 0.00 97.83 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
6.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 93.75 0.00
0.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 10.00 80.00zoom_blur
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label53.70 0.20 3.80 5.60 5.70 2.20 4.30 5.40 12.80 6.30
0.17 77.13 0.67 0.67 0.00 1.67 2.50 0.83 2.84 13.52
0.28 0.00 70.47 4.74 5.57 5.01 7.52 3.34 1.67 1.39
0.00 0.00 0.47 69.30 4.65 13.49 5.58 3.26 0.93 2.33
0.00 0.00 2.33 0.78 84.50 2.33 3.10 6.20 0.00 0.78
0.00 0.00 0.00 3.90 3.90 84.42 3.90 3.90 0.00 0.00
0.00 0.00 0.00 0.00 2.17 0.00 95.65 2.17 0.00 0.00
0.00 0.00 0.00 3.70 0.00 3.70 0.00 92.59 0.00 0.00
0.00 0.00 0.00 12.50 0.00 0.00 0.00 6.25 81.25 0.00
0.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 90.00snow
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label53.80 0.20 4.10 5.40 6.90 1.70 4.20 5.20 13.40 5.10
0.50 78.46 0.33 0.50 0.17 1.34 2.50 0.50 2.34 13.36
0.00 0.00 77.99 2.51 4.46 3.90 6.69 2.79 1.11 0.56
0.00 0.00 0.47 69.30 2.33 17.67 5.58 2.79 0.47 1.40
0.00 0.00 0.00 1.55 82.17 3.88 4.65 6.20 1.55 0.00
0.00 0.00 1.30 3.90 1.30 87.01 6.49 0.00 0.00 0.00
0.00 0.00 0.00 0.00 0.00 0.00 100.00 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
0.00 0.00 0.00 0.00 0.00 0.00 6.25 0.00 93.75 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00frost
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label47.80 0.10 4.70 6.80 6.50 3.90 2.80 6.80 13.20 7.40
0.67 76.96 0.00 0.67 0.17 1.50 1.50 0.67 4.51 13.36
0.28 0.28 71.03 4.74 5.29 3.62 10.86 3.06 0.28 0.56
0.00 0.00 2.79 66.98 3.72 14.88 6.51 4.19 0.47 0.47
0.00 0.00 0.78 0.78 84.50 2.33 4.65 5.43 0.78 0.78
0.00 0.00 2.60 6.49 2.60 77.92 5.19 3.90 0.00 1.30
0.00 0.00 0.00 2.17 4.35 0.00 93.48 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 92.59 0.00 3.70
0.00 0.00 0.00 0.00 6.25 6.25 0.00 0.00 87.50 0.00
0.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 90.00fog
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label60.70 0.20 3.90 3.70 4.60 2.30 3.20 5.00 10.90 5.50
0.17 80.13 0.17 0.17 0.17 0.50 2.00 0.33 2.50 13.86
0.28 0.00 79.39 2.51 4.74 2.51 7.24 2.51 0.28 0.56
0.00 0.00 0.00 74.42 3.26 14.88 5.12 1.86 0.47 0.00
0.00 0.00 0.00 2.33 87.60 3.10 3.88 2.33 0.78 0.00
0.00 0.00 1.30 2.60 1.30 89.61 2.60 2.60 0.00 0.00
0.00 0.00 0.00 0.00 2.17 0.00 97.83 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
6.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 93.75 0.00
0.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 90.00brightness
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label59.10 0.00 3.90 4.80 5.30 2.80 3.90 3.80 12.20 4.20
0.17 79.63 0.17 1.17 0.00 1.67 1.84 0.83 3.51 11.02
0.28 0.00 74.65 3.90 3.90 1.39 12.81 2.51 0.00 0.56
0.00 0.00 0.93 78.60 2.33 10.23 6.05 1.40 0.47 0.00
0.00 0.00 0.00 1.55 82.17 4.65 6.20 3.10 1.55 0.78
0.00 0.00 0.00 5.19 1.30 89.61 0.00 3.90 0.00 0.00
0.00 0.00 0.00 2.17 2.17 2.17 93.48 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.25 93.75 0.00
0.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 0.00 90.00contrast
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label52.90 0.40 5.70 5.30 4.90 2.60 3.90 6.20 12.90 5.20
1.00 67.95 0.67 0.83 0.17 2.84 3.84 1.67 5.68 15.36
0.28 0.00 68.25 5.01 6.96 6.13 7.80 4.18 0.84 0.56
0.00 0.00 0.93 66.51 5.58 14.42 5.12 6.98 0.00 0.47
0.00 0.00 0.78 1.55 80.62 4.65 5.43 5.43 0.78 0.78
0.00 0.00 1.30 7.79 5.19 81.82 1.30 2.60 0.00 0.00
0.00 0.00 0.00 2.17 2.17 0.00 95.65 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
0.00 0.00 0.00 0.00 6.25 0.00 0.00 0.00 93.75 0.00
0.00 10.00 0.00 0.00 0.00 0.00 10.00 10.00 0.00 70.00elastic_transform
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label58.70 0.20 3.40 4.00 5.00 1.90 3.90 4.70 12.00 6.20
0.33 77.63 0.17 0.50 0.00 1.00 2.34 0.83 3.51 13.69
0.00 0.00 78.27 4.18 5.01 2.79 6.69 2.23 0.56 0.28
0.00 0.00 0.00 71.16 4.19 15.35 4.65 3.26 0.47 0.93
0.00 0.00 0.00 2.33 86.82 4.65 3.88 2.33 0.00 0.00
0.00 0.00 0.00 0.00 2.60 90.91 1.30 5.19 0.00 0.00
0.00 0.00 0.00 0.00 2.17 0.00 97.83 0.00 0.00 0.00
0.00 0.00 0.00 0.00 3.70 0.00 0.00 96.30 0.00 0.00
6.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 93.75 0.00
0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00pixelate
0 1 2 3 4 5 6 7 8 9
Predicted label0(n=1000)
1(n=599)
2(n=359)
3(n=215)
4(n=129)
5(n=77)
6(n=46)
7(n=27)
8(n=16)
9(n=10)True label54.20 0.50 4.50 6.40 5.50 2.00 3.00 5.20 12.40 6.30
0.17 70.95 0.83 0.83 0.33 2.84 3.17 0.83 3.51 16.53
1.11 0.00 63.79 4.74 10.58 3.90 10.86 4.18 0.28 0.56
0.00 0.00 2.33 57.21 4.65 19.07 7.44 6.51 0.93 1.86
0.00 0.78 0.78 3.88 72.87 6.20 6.98 6.20 0.78 1.55
0.00 0.00 0.00 7.79 3.90 79.22 1.30 7.79 0.00 0.00
0.00 0.00 0.00 2.17 0.00 4.35 93.48 0.00 0.00 0.00
0.00 0.00 0.00 0.00 0.00 3.70 0.00 96.30 0.00 0.00
6.25 0.00 0.00 0.00 0.00 0.00 0.00 0.00 93.75 0.00
0.00 0.00 0.00 0.00 0.00 0.00 10.00 10.00 0.00 80.00jpeg_compression
Figure 11: Confusion matrices of BN-adapted classifiers on CIFAR-10C-LT with Ï= 100. We mark the cases
where the confusion rate exceeds 11% with red squares. We can observe notable accuracy decreases in classes
with large amounts of data, and similar confusing patterns regardless of the corruption types under the class
distribution shift. Compared to Ï= 1,10, we can observe clearer class-wise confusion patterns.
32Under review as submission to TMLR
F Prediction Refinement Module Outputs on CIFAR-10C-LT
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 91.3 -0.3 -0.3 -0.3 -0.3 -0.2 -0.2 -0.3 -0.1 -0.1
-0.3 1.7 -0.2 0.0 -0.0 -0.2 -0.2 -0.1 -0.1 -0.2
-0.4 -0.4 1.5 -0.3 -0.3 -0.3 -0.2 -0.4 -0.2 -0.4
-0.4 -0.6 -0.5 1.4 -0.4 -0.4 -0.3 -0.4 -0.6 -0.7
-0.2 -0.3 -0.2 -0.3 1.6 -0.3 -0.4 -0.1 -0.4 -0.5
-0.4 -0.4 -0.5 -0.4 -0.5 1.5 -0.1 -0.5 -0.3 -0.1
-0.3 -0.2 -0.4 -0.3 -0.5 -0.3 1.5 -0.3 -0.1 0.0
-0.5 -0.2 -0.3 -0.2 -0.4 -0.1 0.0 1.9 -0.4 -0.2
-0.4 -0.3 -0.3 -0.3 -0.2 -0.2 -0.3 -0.1 1.6 -0.1
-0.4 -0.2 -0.0 -0.2 -0.1 -0.1 -0.2 -0.4 -0.3 1.6
00 1 2 3 4 5 6 7 8 9-3.3
-3.1
-4.3
-3.9
-3.5
-3.2
-2.0
-3.5
-2.7
-2.4trained W and b for Long-tailed CIFAR-10C (=1)
(a)Ï= 1
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 91.7 -0.3 -0.4 -0.5 -0.5 -0.3 -0.2 -0.4 -0.1 -0.0
-0.6 2.0 -0.3 0.0 -0.0 -0.2 -0.1 -0.0 -0.1 -0.1
-0.4 -0.5 1.5 -0.4 -0.4 -0.3 -0.2 -0.4 -0.0 -0.3
-0.3 -0.5 -0.4 1.2 -0.3 -0.3 -0.2 -0.3 -0.5 -0.5
-0.3 -0.5 -0.2 -0.3 1.3 -0.2 -0.2 -0.0 -0.2 -0.3
-0.5 -0.3 -0.5 -0.3 -0.4 1.1 0.0 -0.3 -0.1 -0.0
-0.4 -0.2 -0.3 -0.2 -0.3 -0.2 0.9 -0.0 -0.0 0.1
-0.4 -0.3 -0.3 -0.1 -0.2 -0.0 0.2 1.2 -0.2 -0.1
-0.4 -0.3 -0.3 -0.3 -0.0 -0.1 -0.2 0.1 1.0 0.0
-0.3 -0.2 0.1 -0.2 0.0 0.1 0.0 -0.1 -0.2 0.9
00 1 2 3 4 5 6 7 8 91.4
1.0
-2.2
-3.8
-3.8
-4.7
-3.4
-5.8
-3.8
-4.3trained W and b for Long-tailed CIFAR-10C (=10)
 (b)Ï= 10
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 91.7 -0.3 -0.5 -0.5 -0.5 -0.3 -0.2 -0.4 -0.1 -0.0
-0.8 1.9 -0.3 0.0 0.1 -0.1 -0.0 0.0 -0.1 -0.1
-0.3 -0.5 1.2 -0.3 -0.3 -0.2 -0.1 -0.4 0.0 -0.2
-0.2 -0.4 -0.3 0.8 -0.3 -0.2 -0.1 -0.2 -0.3 -0.3
-0.2 -0.4 -0.1 -0.1 1.0 -0.1 -0.1 -0.0 -0.1 -0.2
-0.5 -0.2 -0.3 -0.1 -0.2 0.7 0.0 -0.2 -0.1 0.0
-0.4 -0.1 -0.2 -0.1 -0.2 -0.1 0.6 0.1 0.0 0.1
-0.3 -0.2 -0.2 -0.1 -0.1 -0.0 0.2 0.8 -0.1 -0.0
-0.3 -0.2 -0.3 -0.2 0.0 -0.0 -0.1 0.2 0.8 0.1
-0.3 -0.2 0.1 -0.1 0.1 0.2 0.1 -0.1 -0.2 0.6
00 1 2 3 4 5 6 7 8 93.2
3.2
-2.1
-4.6
-4.4
-5.9
-4.1
-6.3
-2.5
-3.2trained W and b for Long-tailed CIFAR-10C (=100)
 (c)Ï= 100
Figure 12: Prediction refinement module outputs on CIFAR-10C-LT of Ï= 1,10,100without regularization
(i.e.,Î±= 0)
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 90.6 0.0 -0.1 -0.0 -0.0 0.0 0.0 -0.1 -0.0 0.0
-0.0 0.7 -0.0 0.1 -0.0 -0.0 -0.1 -0.1 -0.0 -0.0
-0.0 -0.0 0.7 -0.0 -0.1 -0.0 -0.0 -0.0 0.0 -0.0
-0.0 -0.0 -0.1 0.6 -0.0 -0.1 0.1 -0.0 -0.0 0.0
0.0 -0.0 -0.0 -0.0 0.7 0.0 -0.1 0.0 -0.0 -0.0
-0.0 -0.1 -0.1 -0.1 -0.1 0.7 0.1 -0.1 -0.0 0.0
0.1 -0.0 -0.0 0.1 -0.1 -0.0 0.6 -0.0 0.1 0.0
-0.0 -0.0 0.1 0.0 -0.0 0.0 0.1 0.7 -0.1 -0.1
-0.1 -0.0 0.0 -0.0 -0.0 -0.0 0.0 0.0 0.6 0.0
0.0 -0.0 0.1 0.0 0.0 0.0 -0.0 -0.1 -0.0 0.6
00 1 2 3 4 5 6 7 8 90.0
0.1
-0.2
-0.2
-0.0
0.0
-0.0
0.0
0.1
0.1trained W and b for Long-tailed CIFAR-10C (=1)
(a)Ï= 1
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 91.0 -0.2 -0.2 -0.1 -0.1 -0.0 0.0 -0.1 -0.0 0.0
-0.4 1.1 -0.1 0.1 0.0 -0.0 -0.1 0.0 0.1 0.1
-0.1 -0.2 0.8 -0.1 -0.1 -0.1 -0.0 -0.1 0.1 0.0
-0.0 -0.1 -0.1 0.6 -0.1 -0.1 0.0 -0.1 -0.1 -0.0
-0.0 -0.2 -0.0 -0.1 0.7 0.0 -0.0 0.0 -0.0 -0.0
-0.1 -0.1 -0.1 -0.1 -0.1 0.6 0.1 -0.1 0.0 0.1
-0.0 -0.0 -0.1 0.1 -0.1 -0.0 0.5 0.1 0.1 0.1
-0.1 -0.1 -0.0 -0.0 -0.0 -0.0 0.1 0.5 -0.1 -0.1
-0.1 -0.1 -0.0 -0.1 0.0 0.0 0.0 0.0 0.4 0.0
-0.0 0.0 0.1 -0.0 0.0 0.1 0.0 -0.1 -0.0 0.4
00 1 2 3 4 5 6 7 8 91.0
1.2
-0.2
-0.8
-0.6
-0.8
-0.5
-1.2
-0.4
-0.8trained W and b for Long-tailed CIFAR-10C (=10)
 (b)Ï= 10
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 91.3 -0.3 -0.3 -0.1 -0.1 0.0 0.0 -0.1 -0.0 0.0
-0.7 1.4 -0.1 0.1 0.1 0.1 0.0 0.1 0.0 0.1
-0.1 -0.3 0.9 -0.1 -0.1 -0.1 -0.0 -0.1 0.1 0.0
-0.0 -0.1 -0.1 0.6 -0.1 -0.1 0.0 -0.2 -0.1 -0.0
-0.0 -0.3 0.0 -0.1 0.6 -0.0 -0.0 -0.1 0.0 -0.0
-0.2 -0.1 -0.2 -0.1 -0.1 0.4 0.0 -0.1 0.0 0.1
-0.1 -0.0 -0.1 0.0 -0.1 -0.1 0.3 0.1 0.1 0.1
-0.1 -0.1 -0.1 -0.1 -0.1 -0.1 0.1 0.5 -0.1 -0.1
-0.1 -0.1 -0.1 -0.1 0.0 0.0 -0.0 0.1 0.5 0.0
-0.1 0.0 0.1 -0.0 0.0 0.1 0.1 -0.1 -0.0 0.3
00 1 2 3 4 5 6 7 8 92.2
2.4
-0.7
-1.8
-1.6
-2.3
-1.6
-2.7
-0.7
-1.4trained W and b for Long-tailed CIFAR-10C (=100)
 (c)Ï= 100
Figure 13: Prediction refinement module outputs on CIFAR-10C-LT of Ï= 1,10,100whenÎ±= 0.1
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 90.6 -0.0 -0.0 -0.0 -0.0 -0.0 0.0 -0.1 -0.0 0.0
-0.0 0.6 -0.0 0.0 0.0 -0.0 -0.0 -0.0 -0.0 -0.0
0.0 -0.0 0.6 -0.0 -0.1 0.0 -0.0 -0.0 0.0 -0.0
-0.0 -0.0 -0.1 0.6 -0.0 -0.0 0.0 0.0 -0.0 0.0
0.0 -0.0 -0.0 -0.0 0.6 0.0 -0.0 0.0 -0.0 -0.0
0.0 -0.0 -0.0 -0.1 -0.0 0.6 0.0 -0.1 0.0 0.0
0.0 -0.0 -0.0 0.0 -0.0 -0.0 0.5 -0.0 0.1 0.0
-0.0 -0.0 0.0 0.0 -0.0 -0.0 0.0 0.7 -0.0 -0.1
-0.1 -0.0 0.0 -0.0 0.0 -0.0 0.0 0.0 0.6 0.0
0.0 0.0 0.0 0.0 0.0 0.0 0.0 -0.0 0.0 0.6
00 1 2 3 4 5 6 7 8 90.0
0.1
0.0
0.0
0.0
0.0
0.0
0.0
0.1
0.1trained W and b for Long-tailed CIFAR-10C (=1)
(a)Ï= 1
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 90.7 -0.1 -0.1 -0.0 -0.1 0.0 -0.0 -0.0 -0.0 0.0
-0.3 0.8 -0.1 0.1 0.0 -0.0 -0.0 0.0 0.1 0.1
-0.0 -0.1 0.6 -0.1 -0.1 0.0 -0.0 -0.0 0.0 0.0
0.0 -0.1 -0.1 0.5 -0.0 -0.0 0.1 -0.1 -0.1 0.0
0.0 -0.1 0.0 -0.0 0.5 0.0 0.0 0.0 0.0 -0.0
-0.0 -0.1 -0.0 -0.0 -0.0 0.5 0.0 -0.1 0.0 0.1
-0.0 -0.0 -0.0 0.0 -0.0 -0.0 0.4 0.0 0.1 0.1
-0.0 -0.0 -0.0 -0.0 0.0 -0.0 0.0 0.5 -0.0 -0.1
-0.0 0.0 -0.0 -0.0 0.0 -0.0 0.0 0.0 0.4 0.0
0.0 0.1 0.1 0.0 0.0 0.0 0.0 -0.1 -0.0 0.4
00 1 2 3 4 5 6 7 8 90.6
0.7
-0.0
-0.4
-0.3
-0.4
-0.2
-0.5
-0.3
-0.5trained W and b for Long-tailed CIFAR-10C (=10)
 (b)Ï= 10
0 1 2 3 4 5 6 7 8 90 1 2 3 4 5 6 7 8 91.1 -0.3 -0.2 -0.1 -0.1 0.1 0.1 0.0 -0.0 0.0
-0.6 1.2 -0.1 0.2 0.1 0.1 0.1 0.2 0.1 0.1
-0.1 -0.2 0.7 -0.1 -0.1 -0.0 0.0 -0.1 0.1 0.0
0.0 -0.1 -0.0 0.5 -0.1 -0.1 0.0 -0.1 -0.1 0.0
0.0 -0.2 0.1 -0.1 0.5 -0.0 0.0 -0.0 0.0 -0.0
-0.1 -0.1 -0.1 -0.0 -0.1 0.3 0.0 -0.1 0.0 0.1
-0.1 0.0 -0.0 0.0 -0.1 -0.0 0.3 0.1 0.1 0.1
0.0 -0.1 -0.0 -0.1 -0.1 -0.0 0.0 0.4 -0.0 -0.0
-0.1 -0.1 -0.1 -0.1 0.0 -0.0 0.0 0.1 0.4 0.0
-0.1 0.1 0.1 0.0 0.0 0.1 0.1 -0.1 -0.0 0.3
00 1 2 3 4 5 6 7 8 91.7
2.0
-0.5
-1.3
-1.4
-1.9
-1.3
-2.0
-0.5
-1.1trained W and b for Long-tailed CIFAR-10C (=100)
 (c)Ï= 100
Figure 14: Prediction refinement module outputs on CIFAR-10C-LT of Ï= 1,10,100whenÎ±= 1
In Figures 12-14, we present the Wandb, generated by gÏ•when inputting the averaged pseudo label
distribution and prediction deviation computed on the whole test data for CIFAR-10C-LT under Gaussian
noise with different Î±values (0, 0.1, and 1) in equation 3. The (a), (b), and (c) of each figure show the
outputs ofgÏ•when class imbalance Ïis set to 1, 10, and 100, respectively. In Figures 12-14, we commonly
observe that both Wandbassign high values for the head classes (classes 0-1). This prediction refinement of
DART can correct the predictions of instances from the head classes that are misclassified to other classes,
recovering the reduced test accuracy of the head classes (shown in Figure 1). As shown in Figure 15 of
Appendix G, this prediction refinements scheme of DART can modify the averaged pseudo label distribution
to be closely matched with the ground truth label distribution, which is originally significantly disagreed.
Moreover, by comparing Figure 12 and Figure 13- 14, we can observe the impact of the regularization
of equation 2, which encourages gÏ•to output an identity matrix and a zero vector for nearly class-balanced
batches. Specifically, the values of the off-diagonal of Wandbbecome almost 0 for Ï= 1. The classifier logit
of each data point reflects not only the class to which the data point belongs but also the characteristics
of the individual data point (Hinton et al., 2015). For example, even for data points in the same class, the
classifier logit can vary depending on factors such as background or color, thereby containing information
33Under review as submission to TMLR
specific to each data point. Therefore, in the absence of a test-time class distribution shift, where there is no
class-wise confusion pattern to reverse, the classifier logit modified by the prediction refinement poses a risk
of damaging the information of each data point, even if the prediction itself remains unchanged. By adding
the regularization, we can prevent the unintended logit refinements for the nearly class-balanced batches
since the batches might have no common class-wise confusion patterns originated from label distribution
shifts (Figure 9).
We experimentally demonstrate the effect of regularization (equation 2) and present the results in Table 10.
Without regularization ( Î±= 0), DART-applied TENT shows approximately 2% lower performance under no
label distribution shift ( Ï= 1) compared to other Î±values. It indicates that DART without regularization
makes unnecessary adjustments, harming the subsequent test-time adaptation when there is no class distribu-
tion shift. On the other hand, the TTA performance remains stable across all Ïs whenÎ±is set between 0.01
and 1. This result demonstrates the effectiveness of regularization and the robustness of DART to changes in
Î±.
Table 10: Accuracy (%) of DART-applied BNAdapt and TENT with different Î±values on CIFAR-10C-LT.
Method Î±CIFAR-10C-LT
Ï= 1Ï= 10Ï= 100
BNAdapt + DART0 85.3 85.5 85.8
0.01 85.4 85.1 85.7
0.1 (used) 85.2 84.7 85.1
0.5 85.3 83.9 84.8
1 85.3 83.5 84.7
TENT + DART0 84.4 86.2 88.3
0.01 86.1 86.9 88.7
0.1 (used) 86.4 86.8 88.2
0.5 86.6 86.5 87.9
1 86.5 86.2 87.8
G More Experiments
Table 11: Average accuracy (%) on CIFAR-10C-LT using condensed training data. DART with
condensed training data achieves similar performance gains as with original training data, improving BN-
adapted classifiers under label distribution shifts without any degradation in performance when there is no
label distribution shift.
Method Avail. data at int. time Ï= 1Ï= 10Ï= 100
BNAdapt - 85.2Â±0.0 79.0Â±0.1 67.0Â±0.1
BNAdapt+DARTTraining data 85.2Â±0.1 84.7Â±0.1 85.1Â±0.3
Condensed tr. data 85.2Â±0.0 82.4Â±0.3 81.4Â±0.4
G.1Reducing the Dependency of Labeled Training Data in the Intermediate Time Training of DART
As described in Section 5 and Appendix C, some recent TTA methods utilize the training data to prepare
adaptations to unknown test-time distribution shifts during the intermediate time. DART also uses the
training data to construct batches with various label distributions to learn how to detect label distribution
shifts and correct the degraded predictions caused by the shifts. Under a strict situation in which training data
is unavailable due to privacy issues, we can use condensed training data, which is small synthetic data that
preserves essential classification features from the original training data while discarding private information.
34Under review as submission to TMLR
It could prevent privacy leakage (Kang et al., 2023). We report experimental results on CIFAR-10C-LT for
the case when using the condensed training data instead of the original one in Table 11. We use the publicly
released condensed dataset of CIFAR-10 by Cazenavette et al. (2022) consisting of only 50 images for each
class. Even when using the condensed data, DART achieves similar performance gains as when trained with
the original data. Specifically, DART improves the degraded performance caused by label distribution shifts
(+3.4/14.4% for Ï= 10/100) without any degradation in performance when there is no label distribution
shift (Ï= 1). This demonstrates that DART training is possible without the original training data.
Table 12: Average accuracy (%) on mixed CIFAR-10C. Even in the mixed CIFAR-10C dataset, DART
effectively mitigates performance degradation caused by label distribution shifts resulting from Dirichlet
distribution sampling.
Method No label dist. shifts Under label dist. shifts
NoAdapt 71.7Â±0.0 71.7Â±0.0
BNAdapt 75.7Â±0.1 20.4Â±0.1
BNAdapt+DART 75.8Â±0.1 81.5Â±0.8
ROID (Marsden et al., 2024) 80.3Â±0.2 12.9Â±1.2
ROID+DART 80.4Â±0.1 63.8Â±2.3
G.2 DART in Mixed Domain TTA Settings
To verify DARTâ€™s effectiveness in a complex scenario, we consider a "mixed domain" setting. Here, corrupted
test data from 15 types of corruptions are combined into a single test data, as in Marsden et al. (2024).
Under this mixed domain setting, we consider both no label distribution shifts and label distribution shifts,
where for label distribution shifts, the test data is non-i.i.d sampled using Dirichlet sampling with Î´= 0.1.
We summarize the results for the original v.s. DART-applied TTA methods in Table 12. The results indicate
that 1) DART effectively addresses the performance degradation of the BN-adapted classifier even in the
mixed domain setup (+61.1%), and 2) while ROID (Marsden et al., 2024), a state-of-the-art method in
mixed domains, shows a significant performance drop of 67.4% under label distribution shifts, DART leads
to an improvement of 50.9% in performance of ROID. DARTâ€™s prediction refinement could be effective in
mixed domains since it is based on the finding that the BN-adapted classifierâ€™s predictions exhibit consistent
class-wise confusion patterns due to label distribution shifts, regardless of corruption type (Fig. 1). We expect
that DARTâ€™s prediction refinement will also be effective in other challenging situations where both covariate
and label distribution shifts occur.
G.3 Robustness to Changes in Test Batch Size
Table 13: Experiments on balanced CIFAR-10C with different batch sizes.
MethodTest batch size
4 8 16 32 64 200
NoAdapt 71.7 Â±0.0 71.7Â±0.0 71.7Â±0.0 71.7Â±0.0 71.7Â±0.0 71.7Â±0.0
BNAdapt 71.5 Â±0.3 76.7Â±0.1 81.1Â±0.1 83.4Â±0.1 84.5Â±0.0 85.2Â±0.0
BNAdapt+DART (ours) 74.3 Â±0.1 80.4Â±0.1 82.0Â±0.1 83.5Â±0.1 84.6Â±0.1 85.2Â±0.1
TENT 19.7 Â±0.6 37.6Â±1.2 60.2Â±1.2 75.7Â±0.5 82.5Â±0.3 86.3Â±0.1
TENT+DART (ours) 51.3 Â±2.3 68.1Â±1.0 75.6Â±0.4 80.2Â±0.5 83.6Â±0.2 86.5Â±0.2
In Table 13, we evaluate the effectiveness of DART under small test batch sizes on balanced CIFAR-10C.
Even if the label distribution of the test dataset remains balanced, when the test batch size becomes small,
35Under review as submission to TMLR
class imbalance naturally occurs within the test batch as discussed in SAR (Niu et al., 2023). As the test
batch size gets smaller from 200 to 4, the performance of BNAdapt gets worse. For example, the performance
of BNAdapt is slightly worse than the one of NoAdapt when the test batch size is set to 4. However, DART
successfully improves the degraded performance of BNAdapt by about 3% under a small test batch size (e.g.
4 or 8).
G.4 Experimental Results using Different Hyperparameters on CIFAR-10C-LT
Table 14: Experimental results using different combinations of the hyperparameters on CIFAR-10C-LT
intermediate batch sizehidden dimension of gÏ•
250 500 1000 (used) 2000
Ï= 1Ï= 10Ï= 100Ï= 1Ï= 10Ï= 100Ï= 1Ï= 10Ï= 100Ï= 1Ï= 10Ï= 100
BNAdapt 85.2Â±0.0 79.0Â±0.1 67.0Â±0.185.2Â±0.0 79.0Â±0.1 67.0Â±0.185.2Â±0.0 79.0Â±0.1 67.0Â±0.185.2Â±0.0 79.0Â±0.1 67.0Â±0.1
BNAdapt+DART (ours)32 85.0Â±0.0 83.4Â±0.3 83.1Â±1.285.0Â±0.0 83.3Â±0.4 83.5Â±1.285.2Â±0.0 82.9Â±0.3 84.0Â±0.385.2Â±0.0 82.3Â±0.3 84.1Â±0.7
64 (used) 85.3Â±0.0 84.8Â±0.1 84.3Â±0.585.2Â±0.1 84.7Â±0.2 84.8Â±0.285.2Â±0.1 84.7Â±0.1 85.1Â±0.385.3Â±0.0 84.6Â±0.2 85.1Â±0.4
128 85.4Â±0.0 84.4Â±0.1 80.6Â±0.385.4Â±0.0 84.5Â±0.0 81.0Â±0.385.4Â±0.0 84.7Â±0.1 81.9Â±0.285.4Â±0.0 84.9Â±0.1 83.3Â±0.4
In Table 1, we report the experimental results when the intermediate batch size and the hidden dimension of
gÏ•are set to 64 and 1,000, respectively. In Table 14, we summarize the experimental results using different
combinations of the hyperparameters on CIFAR-10C-LT. We can observe that the performance gain of DART
is robust to changes in the hyperparameters for most cases. We observe that the performance improvement is
about 14% when the intermediate batch size is set to 128 for CIFAR-10C of Ï= 100, while it shows about
18% when the intermediate batch size is set to 32 or 64. This is because the prediction refinement module
gÏ•does not experience sufficiently severe class imbalance under the same Dirichlet distribution when the
intermediate batch size increases. For example, when there are two batches of the same size having class
distributions [0.9, 0.1] and [0.1, 0.9], the combined batch has a class distribution of [0.5, 0.5]. Therefore, this
can be addressed by modifying the Dirichlet distribution to experience severe class imbalance by reducing Î´
orNdir.
Table 15: Performance of DART-applied BNAdapt on CIFAR-10C-LT with different intermediate-time
training epochs
CIFAR-10C-LT
# of epochs Ï= 1Ï= 10Ï= 100
1 82.0 80.6 75.7
5 85.3 84.5 81.7
10 85.4 84.9 83.3
25 85.3 84.8 84.8
50 (used) 85.2 84.7 85.1
We summarize the experimental results on the effect of training epochs for the prediction refinement module
gÏ•during the intermediate-time training in Table 15. We set the training epochs for intermediate-time
training to 50 for CIFAR-10. To assess the convergence of gÏ•training, we conduct experiments by reducing
the number of training epochs from 1 to 25. Notably, training for only 10 epochs achieves comparable
results to 50 epochs, with just a 1.8% gap observed for CIFAR-10C-LT with Ï= 100. This demonstrates the
efficiency of DARTâ€™s intermediate-time training, significantly enhancing its scalability for practical use.
G.5 DART on CIFAR-10C-imb
We summarize the experimental results on CIFAR-10C under online imbalance setup in Table 16. We observe
that the DART-applied TTA methods consistently show improved performance than naive TTA methods as
shown in the experiments on CIFAR-10C-LT of Table 1. BNAdaptâ€™s performance decreases as the imbalance
ratio increases, and it shows worse performance than NoAdapt when the imbalance ratios are set to 20-5000.
However, DART-applied BNAdapt consistently outperforms NoAdapt across all imbalance ratios.
36Under review as submission to TMLR
Table 16: Average accuracy (%) of DART-applied TTA methods on CIFAR-10C-imb
CIFAR-10c-imb
IR1 IR5 IR20 IR50 IR5000
NoAdapt 71.6Â±0.1 71.7Â±0.1 71.7Â±0.1 71.6Â±0.1 71.7Â±0.1
BNAdapt 85.3Â±0.1 77.1Â±0.3 50.0Â±0.4 34.6Â±0.4 20.3Â±0.1
BNAdapt+DART (ours) 85.3Â±0.2 83.3Â±0.1 76.2Â±0.3 78.2Â±0.1 82.4Â±0.7
TENT 86.5Â±0.2 75.8Â±0.6 47.3Â±0.1 32.6Â±0.4 19.1Â±0.1
TENT+DART (ours) 86.5Â±0.3 83.2Â±0.1 73.6Â±0.3 71.4Â±0.6 71.5Â±1.3
Table 17: Average accuracy (%) on DomainNet-126 .X2Yrefers to a scenario where a pre-trained model,
trained in domain X, is tested in domain Y. We can observe that DART achieved consistent performance
improvements on DomainNet-126.
Method r2c r2p r2s c2r c2p c2s p2r p2c p2s s2r s2c s2p avg
BNAdapt 52.65 61.57 46.38 63.12 48.68 47.93 73.54 52.67 50.96 66.32 57.86 57.56 56.60
BNAdapt+DART 53.40 62.20 46.95 66.07 51.74 50.21 75.05 54.49 52.00 69.73 60.73 59.82 58.53 (+1.93%)
G.6 DART on DomainNet-126
We test the effectiveness of DART on DomainNet-126, and the experimental results are summarized in
Table 17. We follow the experimental setting described in Marsden et al. (2024) and set the test batch
size to 32. We can observe that DART achieved consistent performance improvements of 1.93% on average
on DomainNet-126. We would like to emphasize that DART consistently achieves performance gains in
large-scale benchmarks such as OfficeHome and DomainNet. In the large-scale benchmarks, the batch size is
generally significantly smaller than the number of classes. Then, even if the class distribution of the test
dataset differs greatly from that of the training dataset, the change in class distribution within each test batch
would be minimal. In such cases, the performance improvement due to DART seems to be less significant
compared to small-scale datasets like CIFAR-10C-LT in Table 1, but DART is still effectively adjusting
inaccurate predictions by label distribution shifts.
Table 18: Average accuracy (%) of MEMO on CIFAR-10C-LT . MEMO (stored BN) performs worse
than NoAdapt while it does not show a performance decline under label distribution shifts. On the other
hand, MEMO (adapted BN) exhibits performance degradation due to label distribution shifts, similar to
BNAdapt. DART can effectively address these performance degradations.
Method Ï= 1Ï= 10Ï= 100
NoAdapt 71.7Â±0.0 71.3Â±0.1 71.4Â±0.2
MEMO (stored BN) 67.6Â±0.1 68.9Â±0.3 71.4Â±0.1
MEMO (adapted BN) 87.0Â±0.1 85.0Â±0.2 73.6Â±0.1
MEMO (adapted BN) + DART 86.8Â±0.3 87.8Â±0.1 87.1Â±0.4
G.7 Applying DART to MEMO
We test the effectiveness of MEMO on CIFAR-10C-LT and try to combine MEMO and DART. MEMO fine-
tunes classifiers to minimize the marginal entropy of the classifier outputs across different data augmentations.
We consider two variants of MEMO introduced in (Goyal et al., 2022). The first variant, MEMO (stored
BN), uses stored BN statistics and resets the classifier for each test batch. The second variant, MEMO
(adapted BN), uses the BN statistics computed on each test batch and continuously adapts the classifier like
TENT. In Table 18, we can observe that MEMO (stored BN) does not show a performance decline under
37Under review as submission to TMLR
0 2 4 6 8
Class index0.000.050.100.150.200.250.300.350.40Class distributiongt label dist (=1)
avg pl dist (=1)
gt label dist (=10)
avg pl dist (=10)
gt label dist (=100)
avg pl dist (=100)
0 2 4 6 8
Class index0.000.050.100.150.200.250.300.350.40Class distributiongt label dist (=1)
avg refined pl dist (=1)
gt label dist (=10)
avg refined pl dist (=10)
gt label dist (=100)
avg refined pl dist (=100)
Figure 15: Visualizations of ground truth and averaged pseudo label distributions of (left)
BNAdapt and (right) DART-applied BNAdapt on CIFAR-10C-LT The averaged refined pseudo
label distribution (solid line) generated by the DART-applied BNAdapt closely matches the ground truth
label distribution (dashed line), but the pseudo label distribution of BNAdapt does not match.
label distribution shifts on CIFAR-10C as shown in Zhao et al. (2023), but it performs worse than NoAdapt.
On the other hand, MEMO (adapted BN) exhibits performance degradation due to label distribution shifts,
similar to BNAdapt (-2/13.4% for Ï= 10/100). However, we can observe that DART can effectively address
these performance degradations. Through the above experiments, we explored the potential for combining
DART and MEMO (adapted BN).
G.8 Comparison of Averaged Pseudo Label Distribution of BNAdapt and DART-applied BNAdapt
In Figure 15, we present the ground truth label distribution and the averaged pseudo label distributions
generated by (left) the BN-adapted classifier and (right) the DART-applied BN-adapted classifier for CIFAR-
10C-LT with different Ïs. In Figure 15 (left), we can observe that although the averaged pseudo-label
distribution of the BN-adapted classifier does not perfectly match the ground truth label distribution, it still
contains information about the direction and severity of the label distribution shift. However, except when Ï
is 1 (i.e.,balanced), the averaged pseudo label distribution differs significantly from the ground truth label
distribution. Specifically, the averaged pseudo-label distribution is slightly closer to uniform compared to the
ground truth label distribution. On the other hand, in Figure 15 (right), we observe that the averaged pseudo
label distribution generated by the DART-applied BN-adapted classifier almost perfectly matches the ground
truth label distribution. Through these experiments, we can see that the prediction refinement by DART can
accurately estimate the label distribution, which is advantageous for subsequent test-time adaptation.
H Transition Matrix Estimation by Noisy Label Learning Method
HOC (Zhu et al., 2021) estimates the noisy label transition matrix T, which represents the probabilities of
clean labels being flipped to noisy labels, for a given noisy label dataset under the intuition that the nearest
neighbor in the embedding space of a trained classifier fÎ¸shares the same ground truth label. In the case
of noisy labels caused by random transitions, the noisy transition matrix can be predicted using feature
clusterability or nearest neighbor information in the embedding space. However, as shown in Figure 16 (a)
left, the nearest neighbors in the embedding space already have the same predictions of BN-adapted classifiers,
making it difficult for HOC to estimate the noisy transition matrix using feature clusterability (Figure 16 (a)
right). Thus, the estimated Tby HOC is similar to the identity matrix (Figure 16 (b)). In this paper, we
theoretically and experimentally analyze the misclassification patterns and propose a TTA method DART to
address performance degradation by learning the prediction refinement module which modifies the inaccurate
BNAdapt predictions due to label distribution shifts.
38Under review as submission to TMLR
(a) T-SNE plots for CIFAR-10C-LT with Gaussian noise, Ï=100
0 2 4 6 80
2
4
6
80.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01
0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
0.00 0.00 0.98 0.00 0.01 0.00 0.00 0.00 0.00 0.00
0.01 0.00 0.01 0.93 0.00 0.01 0.02 0.01 0.02 0.00
0.01 0.00 0.01 0.01 0.94 0.01 0.01 0.00 0.01 0.00
0.00 0.01 0.01 0.01 0.01 0.94 0.01 0.00 0.01 0.01
0.01 0.00 0.01 0.00 0.01 0.02 0.93 0.01 0.01 0.00
0.02 0.00 0.01 0.02 0.01 0.01 0.01 0.89 0.02 0.01
0.03 0.00 0.01 0.01 0.01 0.00 0.00 0.00 0.92 0.02
0.01 0.01 0.00 0.00 0.00 0.01 0.01 0.00 0.01 0.94gaussian_noise_rho0.0100, estimated T by HOC (b) Estimated Tby HOC when Ï= 100
Figure 16: (a) T-SNE plots of test data with ground truth labels (left) and their predictions (right) for
CIFAR-10C-LT with Gaussian noise, Ï=100 (b) Estimated Tby HOC for CIFAR-10C-LT with Gaussian
noise,Ï=100
I Expansion to Vision Transformer (ViT)
Since DART can be applied to the adaptation of classifiers using batch normalization (BN), we believe that
DART is applicable to Vision Transformer (ViT) if it uses BN as well. ViTs typically use Layer Normalization
(LN) instead of BN. However, since BN generally leads to faster convergence and has shown strong performance
in vision tasks, there have been attempts to replace LN with BN in ViTs (Chen et al., 2021). For instance, it
was shown that replacing LN with BN in ViTs for the ImageNet classification task resulted in a 1% increase
in classification performance, evaluated by linear probing (Chen et al., 2021). For the ViTs replacing LN with
BN, we expect that applying DART will lead to significant improvements in the classification performance of
the classifiers under label distribution shifts.
39Under review as submission to TMLR
J Full Results
Table 19: Average accuracy (%) on CIFAR-10C-LT ( Ï= 1)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 46.4Â±0.0 51.6Â±0.0 27.1Â±0.0 90.7Â±0.0 67.9Â±0.0 82.1Â±0.0 91.9Â±0.0 83.8Â±0.0 80.2Â±0.0 68.8Â±0.0 91.2Â±0.0 51.6Â±0.0 82.8Â±0.0 81.2Â±0.0 77.9Â±0.0
BNAdapt 80.6Â±0.1 81.8Â±0.1 69.9Â±0.1 91.5Â±0.1 80.2Â±0.1 88.0Â±0.2 92.7Â±0.0 86.2Â±0.1 88.4Â±0.2 83.6Â±0.2 91.2Â±0.1 89.5Â±0.1 84.0Â±0.2 90.5Â±0.1 80.4Â±0.1
BNAdapt+DART (ours) 80.5Â±0.3 81.7Â±0.2 68.1Â±0.7 91.6Â±0.1 80.4Â±0.1 88.1Â±0.2 92.9Â±0.1 86.4Â±0.0 88.6Â±0.1 83.7Â±0.1 91.3Â±0.1 89.7Â±0.1 84.1Â±0.1 90.6Â±0.1 80.5Â±0.2
TENT 81.8Â±0.2 83.2Â±0.6 74.0Â±0.4 91.2Â±0.3 81.3Â±0.6 88.5Â±0.5 92.3Â±0.3 87.4Â±0.3 88.4Â±0.1 87.7Â±0.3 91.4Â±0.2 90.7Â±0.3 83.9Â±0.1 90.1Â±0.3 83.3Â±0.2
TENT+DART (ours) 82.0Â±0.5 83.5Â±0.5 71.8Â±1.5 91.6Â±0.4 81.9Â±0.3 88.6Â±0.6 92.5Â±0.2 87.6Â±0.4 89.0Â±0.1 87.6Â±0.8 91.8Â±0.2 91.1Â±0.2 84.4Â±0.0 90.5Â±0.3 83.2Â±0.6
PL 82.3Â±0.3 83.1Â±0.3 74.1Â±0.5 91.5Â±0.2 81.2Â±0.7 88.8Â±0.1 92.3Â±0.3 87.5Â±0.2 89.0Â±0.1 87.8Â±0.2 91.3Â±0.4 90.7Â±0.4 83.7Â±0.1 90.6Â±0.3 83.3Â±0.2
PL+DART (ours) 82.3Â±0.3 83.7Â±0.4 72.5Â±0.8 91.8Â±0.3 81.8Â±0.5 88.9Â±0.2 92.3Â±0.3 87.6Â±0.3 89.1Â±0.2 88.0Â±0.2 91.7Â±0.2 91.0Â±0.5 84.3Â±0.3 90.6Â±0.2 83.6Â±0.4
NOTE 73.1Â±2.2 76.5Â±0.4 65.9Â±0.5 88.6Â±0.5 74.0Â±1.8 85.9Â±0.4 89.2Â±0.7 83.7Â±0.4 85.2Â±0.7 83.3Â±1.1 89.0Â±0.8 88.6Â±0.2 78.6Â±1.1 87.3Â±0.5 77.8Â±0.9
NOTE+DART (ours) 74.2Â±1.4 76.5Â±2.0 63.0Â±2.9 88.0Â±0.7 73.6Â±1.2 85.5Â±0.2 89.2Â±0.7 84.0Â±0.3 84.9Â±0.6 83.3Â±0.8 89.0Â±0.8 88.7Â±0.5 79.4Â±0.4 86.6Â±0.5 78.1Â±0.7
LAME 80.6Â±0.1 81.8Â±0.0 70.1Â±0.1 91.5Â±0.1 80.1Â±0.1 88.0Â±0.2 92.7Â±0.1 86.3Â±0.1 88.4Â±0.1 83.6Â±0.2 91.3Â±0.1 89.5Â±0.1 84.0Â±0.2 90.5Â±0.1 80.3Â±0.1
LAME+DART (ours) 80.6Â±0.1 81.9Â±0.1 69.2Â±0.5 91.5Â±0.1 80.2Â±0.1 87.9Â±0.2 92.7Â±0.1 86.4Â±0.1 88.5Â±0.1 83.7Â±0.1 91.3Â±0.2 89.6Â±0.1 84.1Â±0.1 90.6Â±0.1 80.4Â±0.2
DELTA 81.0Â±0.2 82.7Â±0.4 72.4Â±1.0 91.3Â±0.3 80.7Â±0.5 88.2Â±0.3 92.0Â±0.3 86.9Â±0.3 88.3Â±0.5 87.4Â±0.6 91.2Â±0.4 90.5Â±0.4 83.3Â±0.4 90.1Â±0.2 82.4Â±0.5
DELTA+DART (ours) 80.8Â±0.8 82.6Â±0.2 71.3Â±1.0 91.3Â±0.1 80.8Â±0.7 88.1Â±0.3 91.8Â±0.2 87.4Â±0.1 88.2Â±0.1 87.2Â±0.4 91.0Â±0.8 90.2Â±0.4 83.6Â±0.5 90.2Â±0.4 82.8Â±0.6
ODS 80.4Â±0.4 82.1Â±0.6 72.4Â±0.5 91.5Â±0.1 80.8Â±0.6 88.4Â±0.7 92.5Â±0.2 87.7Â±0.3 88.5Â±0.2 87.1Â±0.4 91.7Â±0.3 88.7Â±0.4 83.9Â±0.2 90.0Â±0.3 83.0Â±0.2
ODS+DART (ours) 80.3Â±0.3 82.0Â±0.7 71.5Â±0.8 91.5Â±0.0 80.9Â±0.5 88.4Â±0.7 92.5Â±0.2 87.8Â±0.3 88.4Â±0.2 87.1Â±0.4 91.8Â±0.3 88.7Â±0.4 84.0Â±0.2 90.0Â±0.4 83.0Â±0.3
SAR 82.5Â±0.5 83.8Â±0.3 74.6Â±0.4 91.7Â±0.3 81.6Â±0.2 89.0Â±0.1 92.4Â±0.3 87.9Â±0.4 89.0Â±0.2 88.1Â±0.2 91.5Â±0.2 90.8Â±0.4 84.6Â±0.2 90.7Â±0.1 83.8Â±0.2
SAR+DART (ours) 82.5Â±0.2 83.7Â±0.5 72.8Â±1.0 91.9Â±0.1 81.7Â±0.6 89.2Â±0.2 92.4Â±0.3 87.9Â±0.1 89.1Â±0.1 88.0Â±0.3 91.6Â±0.1 90.9Â±0.4 84.5Â±0.3 90.8Â±0.3 83.6Â±0.4
Table 20: Average accuracy (%) on CIFAR-10C-LT ( Ï= 10)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 45.8Â±0.4 50.1Â±0.2 29.2Â±0.5 89.8Â±0.3 66.1Â±0.2 84.9Â±0.0 92.8Â±0.2 82.8Â±0.2 77.0Â±0.2 66.8Â±0.4 91.6Â±0.2 54.7Â±0.4 82.8Â±0.2 79.7Â±0.1 75.7Â±0.3
BNAdapt 73.9Â±0.3 75.0Â±0.5 63.9Â±0.5 85.1Â±0.3 73.8Â±0.2 82.5Â±0.2 87.0Â±0.1 80.2Â±0.4 81.8Â±0.3 76.4Â±0.6 85.8Â±0.1 83.5Â±0.4 77.5Â±0.4 84.3Â±0.1 74.0Â±0.6
BNAdapt+DART (ours) 80.1Â±0.5 80.5Â±0.5 67.2Â±0.4 90.5Â±0.4 80.3Â±0.4 88.4Â±0.2 92.3Â±0.2 86.5Â±0.4 87.5Â±0.1 82.8Â±0.5 90.7Â±0.2 88.8Â±0.2 84.5Â±0.3 89.7Â±0.4 80.0Â±0.6
TENT 77.6Â±2.0 79.3Â±0.8 68.4Â±1.0 88.9Â±0.9 77.2Â±1.0 85.8Â±0.6 89.9Â±0.6 84.3Â±0.8 86.3Â±0.3 82.8Â±0.5 88.9Â±0.7 87.6Â±1.1 81.4Â±0.9 87.3Â±0.6 78.5Â±1.0
TENT+DART (ours) 82.3Â±1.1 83.0Â±1.1 71.3Â±1.3 91.9Â±0.2 82.2Â±0.2 89.4Â±0.7 93.3Â±0.1 88.7Â±0.5 89.2Â±0.5 87.2Â±1.0 92.1Â±0.5 90.7Â±0.3 85.6Â±0.2 91.2Â±0.4 83.1Â±0.8
PL 77.9Â±1.0 79.1Â±0.7 68.9Â±0.8 88.8Â±0.5 77.1Â±0.5 85.4Â±0.5 90.0Â±0.2 84.7Â±0.8 85.3Â±0.7 82.6Â±0.7 88.7Â±0.4 86.8Â±0.6 81.4Â±1.1 87.4Â±0.5 79.5Â±0.7
PL+DART (ours) 82.3Â±0.6 82.8Â±0.8 71.7Â±0.7 92.3Â±0.4 82.7Â±0.4 89.7Â±0.3 93.2Â±0.0 88.7Â±0.6 89.2Â±0.4 87.8Â±0.4 91.6Â±0.6 90.9Â±0.0 85.6Â±0.1 90.9Â±0.5 83.0Â±0.4
NOTE 72.8Â±1.5 74.2Â±1.7 58.8Â±2.8 88.2Â±1.4 72.9Â±4.5 87.5Â±0.7 90.8Â±0.4 84.1Â±0.8 85.6Â±1.6 81.4Â±1.6 89.7Â±1.4 87.9Â±1.1 78.9Â±0.7 86.6Â±1.2 76.8Â±1.5
NOTE+DART (ours) 74.2Â±0.3 75.4Â±1.2 62.1Â±1.5 88.6Â±0.9 76.1Â±1.5 86.6Â±0.3 90.0Â±0.3 85.5Â±1.0 85.9Â±0.7 82.6Â±0.8 89.5Â±1.4 87.9Â±1.4 79.6Â±1.6 87.0Â±0.8 77.1Â±0.6
LAME 75.3Â±0.5 76.6Â±0.5 65.3Â±0.4 86.6Â±0.2 75.3Â±0.3 83.9Â±0.2 88.4Â±0.1 81.8Â±0.4 83.3Â±0.2 78.0Â±0.5 87.1Â±0.2 84.8Â±0.6 78.9Â±0.3 85.6Â±0.2 75.3Â±0.6
LAME+DART (ours) 78.1Â±0.7 78.6Â±0.5 66.2Â±0.6 88.6Â±0.2 78.1Â±0.2 86.2Â±0.3 90.7Â±0.3 84.5Â±0.4 85.7Â±0.2 80.7Â±0.3 89.1Â±0.2 87.0Â±0.3 82.2Â±0.4 87.9Â±0.4 78.1Â±0.4
DELTA 74.6Â±2.7 78.8Â±1.1 66.5Â±2.0 89.6Â±1.2 77.0Â±0.9 85.4Â±1.5 89.8Â±1.3 83.5Â±1.5 86.1Â±2.1 82.6Â±1.2 90.1Â±0.6 86.9Â±1.6 80.8Â±1.2 87.4Â±1.4 78.4Â±2.0
DELTA+DART (ours) 81.1Â±1.4 81.3Â±2.1 67.5Â±4.9 91.0Â±0.5 81.5Â±0.5 88.7Â±0.3 92.8Â±0.7 88.1Â±0.7 89.1Â±0.6 86.6Â±0.6 91.8Â±0.6 90.9Â±0.1 84.2Â±0.7 90.7Â±0.3 82.8Â±1.3
ODS 75.4Â±2.3 78.0Â±0.6 65.7Â±2.8 91.0Â±0.7 78.2Â±1.7 88.0Â±0.5 92.4Â±0.7 86.4Â±1.2 87.3Â±0.6 84.0Â±0.6 91.7Â±0.6 85.0Â±1.0 83.9Â±0.7 89.1Â±0.8 80.3Â±1.5
ODS+DART (ours) (ours)rs 78.0Â±1.3 79.6Â±0.4 68.6Â±1.7 91.6Â±0.5 80.2Â±1.3 88.7Â±0.6 92.9Â±0.5 87.4Â±1.0 88.2Â±0.5 85.2Â±0.7 92.0Â±0.5 85.9Â±0.9 85.1Â±0.8 89.9Â±0.6 82.0Â±1.2
SAR 76.1Â±1.8 77.4Â±0.9 67.2Â±0.9 86.6Â±0.7 75.5Â±1.7 83.8Â±0.7 87.9Â±0.3 82.2Â±1.1 82.8Â±0.4 80.8Â±0.4 87.3Â±0.5 85.2Â±0.5 79.4Â±1.0 85.6Â±0.3 77.4Â±1.1
SAR+DART (ours) 82.2Â±0.9 82.7Â±0.5 71.5Â±0.5 91.9Â±0.5 81.7Â±0.3 89.2Â±0.3 92.8Â±0.2 88.4Â±0.4 88.8Â±0.6 86.7Â±0.6 91.3Â±0.4 90.2Â±0.3 85.3Â±0.3 90.6Â±0.4 82.6Â±0.8
Table 21: Average accuracy (%) on CIFAR-10C-LT ( Ï= 100)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 43.4Â±0.8 46.7Â±0.4 27.8Â±0.4 90.4Â±0.7 66.8Â±0.9 86.9Â±0.3 93.6Â±0.5 82.6Â±0.5 75.2Â±0.4 63.6Â±0.6 92.8Â±0.1 59.6Â±0.3 83.1Â±0.0 82.1Â±0.9 76.6Â±0.7
BNAdapt 62.5Â±1.1 63.5Â±0.1 54.1Â±0.4 73.1Â±0.3 62.0Â±0.6 69.7Â±0.5 74.7Â±0.2 66.8Â±0.4 69.8Â±0.3 64.6Â±0.5 73.4Â±0.4 72.0Â±0.4 65.2Â±0.4 72.1Â±0.2 62.4Â±0.4
BNAdapt+DART (ours) 80.8Â±0.8 80.7Â±1.2 70.3Â±0.8 91.2Â±0.6 79.8Â±1.3 89.4Â±0.7 92.5Â±0.7 86.7Â±0.2 88.0Â±0.4 82.8Â±0.4 91.5Â±0.6 89.2Â±0.3 84.4Â±0.5 90.1Â±0.6 79.3Â±1.2
TENT 65.6Â±1.7 68.1Â±0.8 57.5Â±1.8 76.2Â±1.3 66.2Â±1.6 72.9Â±1.1 77.6Â±0.7 69.7Â±0.4 73.0Â±0.7 68.4Â±0.9 76.0Â±0.7 74.8Â±0.2 68.7Â±0.8 75.3Â±0.8 66.1Â±1.2
TENT+DART (ours) 84.1Â±1.0 85.0Â±1.4 73.3Â±2.1 93.2Â±0.6 84.6Â±1.0 91.6Â±0.8 93.9Â±0.4 89.8Â±0.8 90.5Â±0.6 87.8Â±0.4 93.2Â±0.2 91.8Â±0.8 87.5Â±0.5 92.1Â±0.6 84.0Â±0.7
PL 63.4Â±2.0 65.8Â±1.3 56.1Â±0.8 73.1Â±1.3 63.2Â±0.9 70.6Â±0.7 75.3Â±1.7 68.2Â±0.7 70.0Â±0.9 66.9Â±0.9 73.8Â±1.3 73.0Â±0.6 65.5Â±1.0 72.7Â±0.3 64.5Â±0.6
PL+DART (ours) 81.5Â±0.9 81.9Â±1.2 70.8Â±1.9 91.9Â±0.8 82.0Â±1.2 90.5Â±0.9 92.9Â±0.3 88.0Â±0.6 89.0Â±0.6 84.9Â±0.5 91.9Â±0.7 90.3Â±0.4 86.0Â±0.3 90.7Â±0.7 81.9Â±0.8
NOTE 68.3Â±6.1 67.2Â±3.1 56.2Â±5.8 88.4Â±1.4 72.8Â±3.8 86.5Â±2.3 90.7Â±2.2 83.6Â±3.7 81.8Â±6.3 79.6Â±1.5 90.2Â±0.9 87.2Â±2.8 80.5Â±1.4 86.3Â±1.6 77.3Â±4.4
NOTE+DART (ours) 76.8Â±1.5 78.7Â±1.7 66.7Â±2.0 88.1Â±0.4 74.0Â±1.9 86.4Â±0.3 88.6Â±1.0 84.5Â±0.3 86.6Â±0.3 83.7Â±1.0 88.7Â±2.0 86.4Â±1.4 81.4Â±0.6 85.7Â±1.1 76.0Â±1.3
LAME 66.0Â±1.3 67.4Â±0.2 57.4Â±0.5 76.3Â±0.9 65.4Â±1.0 73.6Â±0.8 78.6Â±0.5 70.5Â±0.7 73.3Â±0.5 68.0Â±0.5 77.2Â±0.3 75.4Â±0.6 68.8Â±0.6 75.5Â±0.4 66.1Â±0.6
LAME+DART (ours) 76.3Â±0.7 77.2Â±1.0 66.6Â±1.3 87.5Â±0.2 75.9Â±0.9 85.0Â±0.9 88.7Â±0.4 82.9Â±0.3 83.9Â±0.4 78.6Â±0.5 87.6Â±0.6 84.7Â±1.0 80.9Â±0.6 85.9Â±0.5 75.8Â±1.0
DELTA 63.8Â±4.5 67.2Â±2.6 56.4Â±5.8 77.1Â±2.3 70.5Â±4.6 71.1Â±4.1 76.4Â±2.7 71.4Â±2.5 73.2Â±0.4 69.3Â±5.1 76.4Â±3.1 73.7Â±2.2 65.3Â±4.7 75.9Â±4.9 68.0Â±4.7
DELTA+DART (ours) 83.3Â±0.7 84.0Â±0.8 70.1Â±2.1 93.4Â±0.5 83.3Â±1.9 91.3Â±0.9 94.2Â±0.6 89.9Â±0.9 90.4Â±0.6 87.4Â±0.7 93.5Â±0.5 92.8Â±0.6 85.5Â±0.8 92.5Â±0.9 82.0Â±2.9
ODS 64.7Â±4.1 68.4Â±2.8 57.0Â±2.8 85.0Â±1.6 73.6Â±3.4 83.0Â±1.6 86.9Â±0.5 79.1Â±0.7 79.1Â±1.2 71.4Â±2.1 86.9Â±1.0 77.6Â±1.9 77.5Â±0.9 82.4Â±0.7 74.8Â±1.2
ODS+DART (ours) 77.5Â±1.9 79.4Â±2.0 70.1Â±1.7 91.0Â±1.2 82.8Â±2.0 89.1Â±1.3 92.9Â±0.5 87.5Â±0.9 87.3Â±1.2 81.0Â±2.0 92.1Â±0.8 84.6Â±1.8 86.3Â±1.1 89.0Â±0.7 83.2Â±0.8
SAR 64.7Â±1.7 64.6Â±1.4 55.4Â±1.1 74.0Â±0.5 62.9Â±1.1 70.6Â±0.5 75.3Â±1.4 68.3Â±0.6 70.4Â±0.7 66.4Â±0.6 74.5Â±0.4 73.1Â±0.4 66.2Â±0.3 72.4Â±0.5 63.9Â±1.4
SAR+DART (ours) 82.9Â±1.0 83.2Â±1.4 72.9Â±1.8 92.1Â±0.4 82.5Â±1.0 91.1Â±0.9 93.3Â±0.8 88.6Â±0.4 89.7Â±0.4 85.7Â±0.6 92.4Â±0.5 90.6Â±0.6 86.3Â±0.6 91.1Â±0.6 82.5Â±1.2
Table 22: Average accuracy (%) on PACS. X2Yrefers to a scenario where a pre-trained model, trained in
domainX, is tested in domain Y. For example, a2c refers to a scenario where a pre-trained model, trained in
the art domain, is tested in the cartoon domain.
Method a2c a2p a2s c2a c2p c2s p2a p2c p2s s2a s2c s2p avg
NoAdapt 66.0Â±0.0 97.8Â±0.0 57.3Â±0.0 75.6Â±0.0 90.2Â±0.0 72.2Â±0.0 73.2Â±0.0 39.7Â±0.0 43.9Â±0.0 23.5Â±0.0 50.3Â±0.0 38.0Â±0.060.7Â±0.0
BNAdapt 75.8Â±0.3 97.0Â±0.2 70.0Â±0.2 82.5Â±0.3 95.1Â±0.4 73.5Â±0.7 77.8Â±0.2 65.0Â±0.4 46.9Â±0.2 59.4Â±0.5 69.0Â±0.4 57.8Â±0.272.5Â±0.0
BNAdapt+DART (ours) 76.0Â±0.3 96.8Â±0.1 72.1Â±0.5 84.4Â±0.6 95.4Â±0.2 74.7Â±0.3 76.1Â±0.3 62.0Â±0.4 55.0Â±0.7 74.1Â±0.2 76.7Â±0.2 74.0Â±0.676.4Â±0.1
TENT 78.6Â±0.9 97.8Â±0.5 77.1Â±1.9 88.0Â±0.3 97.1Â±0.5 79.1Â±1.2 81.7Â±0.6 75.5Â±1.0 59.2Â±2.5 60.1Â±1.1 71.0Â±1.0 56.8Â±1.676.8Â±0.6
TENT+DART (ours) 80.8Â±0.8 97.7Â±0.4 79.2Â±0.7 88.9Â±0.2 97.4Â±0.3 79.2Â±1.2 79.6Â±1.0 75.4Â±1.7 69.8Â±1.5 81.3Â±0.6 77.6Â±0.7 71.6Â±0.581.6Â±0.4
PL 77.6Â±0.4 96.9Â±0.6 70.0Â±0.3 83.8Â±1.2 95.4Â±0.6 68.9Â±1.5 78.0Â±0.5 66.3Â±0.8 46.9Â±0.2 59.9Â±0.4 68.6Â±1.4 56.8Â±0.272.4Â±0.2
PL+DART (ours) 78.0Â±0.8 96.8Â±0.2 71.7Â±0.8 85.2Â±0.7 96.0Â±0.4 72.9Â±1.3 76.5Â±0.3 63.5Â±1.1 55.0Â±0.7 74.1Â±0.2 76.9Â±0.6 74.0Â±2.676.7Â±0.4
NOTE 79.2Â±0.8 97.2Â±0.4 74.8Â±4.6 86.1Â±1.0 96.6Â±0.4 77.3Â±4.8 77.4Â±1.0 72.5Â±2.8 68.5Â±0.9 73.8Â±2.8 76.1Â±2.2 69.2Â±0.679.1Â±0.8
NOTE+DART (ours) 80.7Â±1.0 97.1Â±0.8 77.9Â±1.9 85.7Â±0.8 96.8Â±0.6 79.0Â±1.0 75.7Â±1.6 70.8Â±1.9 70.6Â±2.1 78.5Â±1.3 76.2Â±0.3 69.3Â±0.379.9Â±0.3
LAME 78.6Â±0.4 98.0Â±0.1 72.8Â±0.6 84.1Â±0.5 96.9Â±0.2 72.5Â±0.9 80.4Â±0.8 61.0Â±0.6 29.4Â±1.2 59.9Â±2.1 71.6Â±0.8 65.3Â±1.172.5Â±0.3
LAME+DART (ours) 78.7Â±0.7 97.8Â±0.2 70.7Â±0.9 84.8Â±0.5 96.5Â±0.7 71.0Â±1.1 78.5Â±1.1 59.8Â±0.4 41.3Â±1.9 78.6Â±0.7 76.0Â±0.7 78.8Â±3.476.0Â±0.2
DELTA 79.6Â±1.0 98.5Â±0.2 75.4Â±2.2 89.3Â±0.2 97.8Â±0.2 80.9Â±1.2 82.3Â±0.5 77.4Â±1.4 62.5Â±0.9 60.6Â±1.8 72.0Â±1.1 62.2Â±5.078.2Â±0.6
DELTA+DART (ours) 81.7Â±1.1 98.3Â±0.1 77.0Â±1.3 89.8Â±0.3 97.7Â±0.2 80.5Â±1.6 79.5Â±1.2 76.7Â±0.9 66.6Â±1.8 83.6Â±0.4 76.7Â±2.1 71.8Â±1.181.7Â±0.4
ODS 79.9Â±1.3 98.4Â±0.3 78.7Â±1.2 88.8Â±0.4 98.1Â±0.3 78.4Â±0.8 82.8Â±0.7 74.9Â±0.8 50.9Â±2.6 59.2Â±2.1 73.4Â±0.8 65.0Â±1.077.4Â±0.5
ODS+DART (ours) 80.1Â±1.3 98.4Â±0.2 78.6Â±0.7 88.8Â±0.2 97.8Â±0.3 77.8Â±0.8 81.4Â±0.5 74.0Â±0.2 58.6Â±2.0 73.3Â±1.6 74.9Â±0.3 73.2Â±1.979.7Â±0.4
SAR 79.6Â±0.8 97.3Â±0.4 70.3Â±1.6 85.4Â±0.4 95.8Â±0.6 74.9Â±2.1 78.0Â±0.7 67.8Â±0.3 46.4Â±0.8 62.6Â±0.5 72.5Â±0.5 59.7Â±0.674.2Â±0.4
SAR+DART (ours) 80.3Â±0.6 97.1Â±0.1 70.9Â±1.9 86.7Â±0.4 96.2Â±0.2 77.2Â±1.1 76.6Â±0.7 66.6Â±0.9 55.3Â±1.3 76.5Â±0.4 80.3Â±0.4 78.6Â±0.378.5Â±0.2
40Under review as submission to TMLR
Table 23: Average accuracy (%) on OfficeHome. X2Yrefers to a scenario where a pre-trained model, trained
in domain X, is tested in domain Y. For example, a2c refers to a scenario where a pre-trained model, trained
in the art domain, is tested in the clipart domain.
Method a2c a2p a2r c2a c2p c2r p2a p2c p2r r2a r2c r2p avg
NoAdapt 47.9Â±0.0 65.8Â±0.0 73.2Â±0.0 52.4Â±0.0 63.2Â±0.0 64.2Â±0.0 49.8Â±0.0 46.3Â±0.0 71.8Â±0.0 65.2Â±0.0 51.7Â±0.0 77.5Â±0.060.8Â±0.0
BNAdapt 48.2Â±0.1 62.4Â±0.2 71.4Â±0.2 51.2Â±0.3 62.6Â±0.2 62.9Â±0.1 51.3Â±0.1 48.3Â±0.4 72.3Â±0.4 64.5Â±0.4 53.6Â±0.5 76.3Â±0.260.4Â±0.1
BNAdapt+DART (ours) 47.8Â±0.3 63.5Â±0.1 72.4Â±0.2 52.7Â±0.4 63.2Â±0.2 63.7Â±0.2 51.9Â±0.3 48.9Â±0.2 72.8Â±0.4 64.5Â±0.4 53.8Â±0.4 76.8Â±0.261.0Â±0.1
TENT 48.1Â±0.4 62.0Â±0.8 69.5Â±0.6 54.6Â±0.4 63.7Â±0.7 64.3Â±0.8 52.2Â±0.3 47.7Â±0.7 72.3Â±0.5 65.5Â±0.7 54.0Â±0.6 76.6Â±0.460.9Â±0.2
TENT+DART (ours) 49.8Â±0.4 65.3Â±0.5 71.4Â±0.4 55.8Â±0.5 65.0Â±0.6 65.7Â±0.4 53.3Â±0.4 48.4Â±0.4 72.8Â±0.3 66.0Â±0.6 54.8Â±0.7 77.2Â±0.462.1Â±0.2
PL 46.3Â±0.8 60.3Â±0.1 68.7Â±0.6 51.5Â±0.4 61.7Â±0.5 62.8Â±0.2 51.1Â±0.1 46.1Â±1.2 71.2Â±0.4 64.7Â±0.5 51.7Â±0.4 75.0Â±0.559.2Â±0.2
PL+DART (ours) 46.8Â±0.4 62.5Â±0.7 71.0Â±0.3 51.9Â±1.1 62.1Â±0.3 63.7Â±0.6 51.8Â±0.5 46.8Â±1.7 72.0Â±0.6 64.7Â±0.5 52.1Â±1.5 75.1Â±0.460.1Â±0.4
NOTE 41.1Â±0.9 51.4Â±0.6 61.4Â±0.3 47.7Â±1.3 56.1Â±1.0 57.2Â±0.8 45.5Â±1.1 43.2Â±1.3 62.7Â±0.5 60.5Â±0.5 48.3Â±1.7 69.7Â±1.153.7Â±0.3
NOTE+DART (ours) 40.4Â±1.5 50.0Â±1.0 62.1Â±0.7 48.8Â±0.4 55.8Â±1.4 57.4Â±1.1 44.6Â±1.4 42.3Â±2.4 62.7Â±0.6 60.0Â±1.1 48.6Â±1.1 69.5Â±1.053.5Â±0.1
LAME 46.5Â±0.5 60.2Â±0.4 68.3Â±0.4 51.7Â±0.1 62.2Â±0.5 62.2Â±0.1 51.9Â±0.3 48.4Â±0.4 71.9Â±0.4 64.5Â±0.2 53.5Â±0.3 75.9Â±0.159.8Â±0.1
LAME+DART (ours) 45.7Â±0.2 61.3Â±0.2 68.4Â±0.4 53.2Â±0.6 62.8Â±0.2 62.9Â±0.1 52.7Â±0.4 49.0Â±0.4 72.3Â±0.4 64.7Â±0.1 53.8Â±0.4 76.0Â±0.360.2Â±0.2
DELTA 50.6Â±0.6 65.6Â±0.4 72.3Â±0.4 56.3Â±0.9 67.1Â±0.7 66.8Â±0.1 52.8Â±0.7 49.9Â±0.4 74.7Â±0.2 66.6Â±1.0 56.7Â±0.5 78.1Â±0.363.1Â±0.1
DELTA+DART (ours) 51.5Â±0.4 67.6Â±0.2 73.6Â±0.3 57.4Â±0.5 67.4Â±0.6 67.6Â±0.6 54.2Â±0.4 50.1Â±0.6 75.1Â±0.1 66.9Â±0.7 56.9Â±0.7 78.3Â±0.163.9Â±0.1
ODS 49.4Â±0.5 63.6Â±0.7 70.2Â±0.4 54.9Â±0.3 65.1Â±0.7 65.2Â±0.3 52.9Â±0.1 49.5Â±0.6 73.4Â±0.2 66.5Â±0.6 55.9Â±0.5 78.1Â±0.362.1Â±0.2
ODS+DART (ours) 49.9Â±0.4 64.8Â±0.9 71.0Â±0.4 55.4Â±0.4 65.6Â±0.6 65.7Â±0.2 53.4Â±0.2 49.8Â±0.6 73.7Â±0.2 66.7Â±0.5 56.0Â±0.5 78.3Â±0.362.5Â±0.2
SAR 49.8Â±0.3 62.8Â±0.5 71.2Â±0.2 52.5Â±0.6 64.1Â±0.3 64.9Â±0.1 51.2Â±0.5 49.1Â±0.3 72.2Â±0.4 65.0Â±0.3 54.1Â±0.5 76.6Â±0.261.1Â±0.0
SAR+DART (ours) 49.1Â±0.4 64.3Â±0.4 72.4Â±0.1 53.3Â±0.3 64.5Â±0.3 65.8Â±0.3 52.0Â±0.9 49.3Â±0.3 72.7Â±0.5 64.9Â±0.3 54.7Â±0.4 77.1Â±0.261.7Â±0.2
Table 24: Average accuracy (%) on CIFAR-100C-imb (IR1)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 16.3Â±0.1 17.4Â±0.2 7.6Â±0.2 67.3Â±0.7 27.8Â±0.5 55.9Â±0.3 70.4Â±0.4 50.5Â±0.4 42.4Â±0.7 33.6Â±0.4 63.7Â±0.2 18.8Â±0.5 52.6Â±0.5 50.9Â±0.2 47.8Â±0.6
BNAdapt 53.4Â±0.6 53.9Â±0.5 43.6Â±0.4 69.3Â±0.8 55.0Â±0.7 64.4Â±0.5 71.9Â±0.5 58.0Â±0.6 62.1Â±0.6 53.1Â±0.2 67.8Â±0.5 63.4Â±0.7 59.5Â±0.8 67.3Â±0.7 53.9Â±0.2
BNAdapt+DART-split (ours) 53.4Â±0.6 53.9Â±0.5 43.0Â±0.4 69.3Â±0.8 55.0Â±0.7 64.4Â±0.5 71.9Â±0.5 58.0Â±0.6 62.1Â±0.6 53.1Â±0.2 67.8Â±0.5 63.4Â±0.7 59.5Â±0.8 67.3Â±0.7 53.9Â±0.2
TENT 55.1Â±0.4 56.1Â±0.4 47.2Â±0.9 69.4Â±0.7 55.9Â±0.9 65.4Â±0.6 71.2Â±0.3 61.9Â±1.1 63.3Â±0.5 61.7Â±0.6 68.8Â±0.3 68.2Â±0.9 59.9Â±0.9 68.0Â±0.5 56.9Â±0.7
TENT+DART-split (ours) 55.1Â±0.4 56.1Â±0.4 46.6Â±1.0 69.4Â±0.7 55.9Â±0.9 65.4Â±0.6 71.2Â±0.3 61.9Â±1.1 63.3Â±0.5 61.7Â±0.6 68.8Â±0.3 68.2Â±0.9 59.9Â±0.9 68.0Â±0.5 56.9Â±0.7
SAR 53.9Â±1.3 54.8Â±0.7 46.0Â±1.5 69.1Â±0.4 54.9Â±1.2 65.3Â±0.5 70.7Â±0.6 60.3Â±0.4 62.4Â±0.7 60.2Â±0.4 67.9Â±0.5 66.9Â±0.3 58.6Â±0.7 67.2Â±0.8 56.1Â±0.9
SAR+DART-split (ours) 53.9Â±1.3 54.8Â±0.7 45.1Â±1.3 69.1Â±0.4 54.9Â±1.2 65.3Â±0.5 70.7Â±0.6 60.3Â±0.4 62.4Â±0.7 60.2Â±0.4 67.9Â±0.5 66.9Â±0.3 58.6Â±0.7 67.2Â±0.8 56.1Â±0.9
ODS 55.2Â±0.3 56.3Â±0.2 47.0Â±0.3 70.6Â±0.7 56.6Â±0.8 66.6Â±0.6 72.6Â±0.3 62.2Â±0.8 64.1Â±0.7 61.1Â±0.4 69.8Â±0.3 66.6Â±1.1 61.1Â±0.9 68.8Â±0.5 57.3Â±0.3
ODS+DART-split (ours) 55.2Â±0.3 56.3Â±0.2 46.8Â±0.2 70.6Â±0.7 56.6Â±0.8 66.6Â±0.6 72.6Â±0.3 62.2Â±0.8 64.1Â±0.7 61.1Â±0.4 69.8Â±0.3 66.6Â±1.1 61.1Â±0.9 68.8Â±0.5 57.3Â±0.3
Table 25: Average accuracy (%) on CIFAR-100C-imb (IR200)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 16.1Â±0.4 17.2Â±0.4 7.5Â±0.1 67.3Â±0.7 27.8Â±0.6 55.8Â±0.4 70.3Â±0.5 50.3Â±0.3 42.4Â±0.4 33.8Â±0.4 63.4Â±0.3 18.7Â±0.3 52.5Â±0.4 50.8Â±0.3 47.6Â±0.6
BNAdapt 26.6Â±0.4 26.8Â±0.5 22.7Â±0.5 33.7Â±0.6 26.4Â±0.5 31.2Â±0.4 34.6Â±0.8 28.0Â±0.3 30.1Â±0.5 26.3Â±0.5 32.8Â±0.8 30.0Â±0.6 28.5Â±0.6 32.5Â±0.5 26.3Â±0.3
BNAdapt+DART-split (ours) 45.9Â±0.7 46.1Â±0.4 41.2Â±1.6 44.7Â±1.4 45.5Â±1.2 46.6Â±1.1 46.0Â±0.5 45.6Â±1.1 47.8Â±1.2 45.7Â±1.2 44.4Â±1.8 47.2Â±0.7 47.5Â±0.9 46.0Â±1.3 46.2Â±0.6
TENT 23.3Â±0.6 23.9Â±0.7 20.2Â±0.3 29.1Â±0.7 23.2Â±0.6 26.8Â±0.5 29.7Â±0.7 24.5Â±0.7 25.7Â±0.4 24.9Â±0.1 28.2Â±0.8 25.5Â±0.8 24.6Â±0.4 28.4Â±0.4 23.7Â±0.7
TENT+DART-split (ours) 44.6Â±1.2 44.2Â±0.3 39.7Â±1.5 42.8Â±1.9 43.3Â±1.0 44.3Â±0.7 43.6Â±0.8 44.1Â±1.4 46.3Â±1.4 44.2Â±0.9 41.7Â±2.1 45.3Â±1.1 45.8Â±1.0 44.0Â±1.6 43.9Â±1.0
SAR 24.1Â±0.7 24.7Â±0.4 20.8Â±0.6 30.5Â±0.8 23.8Â±0.8 28.6Â±0.8 31.3Â±0.5 25.9Â±1.2 27.6Â±0.3 25.5Â±0.8 30.0Â±0.8 28.1Â±0.8 25.7Â±0.8 29.6Â±0.4 24.6Â±0.4
SAR+DART-split (ours) 45.3Â±0.9 45.9Â±0.6 40.5Â±2.0 42.9Â±2.0 44.5Â±0.7 44.9Â±0.9 44.0Â±0.6 44.9Â±1.3 46.8Â±1.6 45.7Â±1.6 42.9Â±1.9 45.8Â±1.0 46.2Â±0.4 44.6Â±1.5 45.8Â±0.5
ODS 43.3Â±1.7 44.2Â±0.9 36.0Â±0.7 58.0Â±0.6 44.9Â±0.6 52.7Â±0.8 60.1Â±1.1 48.7Â±0.4 51.0Â±0.3 48.5Â±0.7 57.3Â±0.5 47.5Â±0.7 48.8Â±1.1 56.0Â±0.8 46.6Â±0.5
ODS+DART-split (ours) 45.3Â±2.8 46.3Â±1.0 37.3Â±0.5 61.0Â±0.9 47.2Â±1.0 56.2Â±1.1 62.9Â±1.5 52.4Â±0.9 54.1Â±0.6 52.8Â±1.0 60.6Â±1.2 48.8Â±0.8 52.0Â±1.7 59.0Â±1.3 51.3Â±0.4
Table 26: Average accuracy (%) on CIFAR-100C-imb (IR500)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 16.1Â±0.3 17.2Â±0.3 7.5Â±0.2 67.2Â±0.6 27.7Â±0.5 55.8Â±0.5 70.1Â±0.6 50.2Â±0.6 42.3Â±0.6 33.5Â±0.5 63.4Â±0.4 18.6Â±0.4 52.5Â±0.6 50.8Â±0.3 47.8Â±0.6
BNAdapt 17.5Â±0.5 17.6Â±0.4 14.9Â±0.3 21.4Â±0.4 17.0Â±0.5 20.1Â±0.5 22.0Â±0.3 18.4Â±0.4 19.3Â±0.5 17.5Â±0.3 20.8Â±0.5 19.2Â±0.5 18.1Â±0.6 20.7Â±0.6 17.0Â±0.4
BNAdapt+DART-split (ours) 47.6Â±1.1 47.7Â±0.5 42.4Â±1.3 56.1Â±1.2 45.7Â±1.3 53.5Â±0.6 55.1Â±0.7 50.1Â±0.4 52.2Â±1.2 47.7Â±1.8 53.6Â±1.1 50.8Â±0.6 48.3Â±1.0 56.1Â±0.7 45.7Â±0.6
TENT 14.2Â±0.5 14.7Â±0.6 12.6Â±0.1 17.8Â±0.4 13.8Â±0.3 16.2Â±0.8 18.1Â±0.7 14.9Â±1.2 15.9Â±0.7 15.4Â±0.7 17.0Â±0.7 14.6Â±0.9 15.1Â±0.3 17.1Â±0.6 14.3Â±0.2
TENT+DART-split (ours) 46.2Â±1.6 46.1Â±0.8 41.7Â±1.0 54.4Â±1.7 44.1Â±1.6 51.9Â±0.7 53.5Â±0.3 48.8Â±1.4 50.3Â±1.5 46.4Â±2.3 52.5Â±1.6 49.2Â±1.5 46.6Â±1.1 54.2Â±0.4 44.1Â±1.4
SAR 15.3Â±0.9 15.5Â±0.5 12.7Â±0.3 19.1Â±0.5 14.9Â±0.5 17.4Â±0.4 19.2Â±0.1 16.3Â±0.3 16.9Â±0.8 16.1Â±0.6 18.4Â±0.6 16.6Â±0.5 15.9Â±0.6 18.1Â±0.6 14.9Â±0.4
SAR+DART-split (ours) 47.3Â±1.5 47.6Â±0.4 42.3Â±1.1 56.1Â±1.6 45.5Â±1.1 53.3Â±0.6 55.1Â±0.7 49.8Â±0.7 51.9Â±0.8 47.5Â±2.0 53.6Â±1.3 50.8Â±0.6 47.8Â±0.8 56.0Â±0.7 45.4Â±0.7
ODS 38.3Â±0.6 39.2Â±0.6 31.7Â±0.5 52.7Â±0.9 38.6Â±0.9 46.9Â±0.9 54.8Â±1.4 42.1Â±1.5 45.6Â±1.0 42.8Â±2.0 51.6Â±1.3 38.8Â±2.2 42.2Â±0.3 49.5Â±0.9 40.7Â±0.8
ODS+DART-split (ours) 43.9Â±1.0 44.6Â±1.0 35.8Â±0.8 61.5Â±1.1 44.7Â±0.5 54.7Â±1.1 64.1Â±1.8 49.9Â±2.4 53.5Â±1.8 49.8Â±2.7 61.1Â±1.4 42.7Â±3.0 49.5Â±1.1 57.4Â±1.3 47.8Â±1.0
Table 27: Average accuracy (%) on CIFAR-100C-imb (IR50000)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 16.1Â±0.5 17.3Â±0.2 7.4Â±0.2 67.3Â±0.6 27.8Â±0.5 55.9Â±0.2 70.3Â±0.4 50.3Â±0.5 42.5Â±0.4 33.4Â±0.3 63.7Â±0.3 18.7Â±0.3 52.5Â±0.3 51.0Â±0.3 47.9Â±0.4
BNAdapt 9.1Â±0.2 8.9Â±0.1 8.0Â±0.4 10.2Â±0.2 8.5Â±0.4 9.7Â±0.2 10.3Â±0.2 9.2Â±0.1 9.5Â±0.3 9.0Â±0.2 9.9Â±0.2 9.6Â±0.2 8.8Â±0.3 9.9Â±0.1 8.5Â±0.2
BNAdapt+DART-split (ours) 46.8Â±0.9 46.4Â±0.9 40.8Â±1.3 55.4Â±1.2 44.0Â±1.3 52.3Â±1.5 54.5Â±0.7 48.8Â±0.7 51.7Â±2.0 46.6Â±2.0 53.0Â±1.7 49.1Â±1.5 47.5Â±0.5 54.9Â±1.6 45.0Â±1.4
TENT 6.7Â±0.3 7.0Â±0.2 6.1Â±0.4 7.9Â±0.2 6.3Â±0.3 7.3Â±0.2 8.0Â±0.4 7.0Â±0.1 7.1Â±0.2 7.1Â±0.2 7.6Â±0.4 6.9Â±0.5 6.8Â±0.1 7.6Â±0.3 6.6Â±0.2
TENT+DART-split (ours) 45.9Â±0.6 45.3Â±1.1 40.5Â±1.5 54.5Â±0.9 42.2Â±1.6 50.4Â±1.8 53.2Â±0.6 47.3Â±0.4 50.3Â±2.9 45.5Â±1.7 51.6Â±1.7 47.0Â±1.7 45.8Â±0.6 53.3Â±1.6 43.6Â±0.5
SAR 7.7Â±0.4 7.7Â±0.5 6.6Â±0.2 8.6Â±0.3 7.0Â±0.2 7.9Â±0.3 8.8Â±0.4 7.9Â±0.3 8.2Â±0.1 7.6Â±0.3 8.3Â±0.2 7.8Â±0.3 7.4Â±0.3 8.5Â±0.4 7.4Â±0.3
SAR+DART-split (ours) 46.8Â±0.9 46.4Â±0.9 41.0Â±1.3 55.4Â±1.2 44.0Â±1.3 52.2Â±1.3 54.5Â±0.6 48.8Â±0.7 51.7Â±2.0 46.6Â±2.0 53.0Â±1.8 49.1Â±1.5 47.3Â±0.6 54.8Â±1.7 44.9Â±1.3
ODS 33.6Â±0.9 33.6Â±0.9 26.7Â±1.2 47.3Â±0.9 33.1Â±1.5 41.5Â±0.9 50.8Â±1.9 37.4Â±0.6 39.7Â±1.2 36.2Â±2.6 46.5Â±1.2 33.0Â±1.7 38.7Â±1.0 45.6Â±1.3 36.6Â±0.6
ODS+DART-split (ours) 42.6Â±1.3 43.3Â±1.4 33.6Â±1.5 60.4Â±1.5 42.5Â±1.6 53.4Â±0.7 63.3Â±1.6 49.1Â±1.0 51.2Â±1.4 46.8Â±4.1 58.7Â±1.6 41.2Â±2.8 49.5Â±1.4 57.4Â±1.3 46.7Â±0.3
Table 28: Average accuracy (%) on ImageNet-C-imb (IR1)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 2.2Â±0.0 2.9Â±0.0 1.8Â±0.0 18.0Â±0.1 9.8Â±0.1 14.8Â±0.1 22.5Â±0.1 17.0Â±0.1 23.4Â±0.1 24.5Â±0.0 59.0Â±0.2 5.4Â±0.1 17.0Â±0.1 20.6Â±0.1 31.7Â±0.1
BNAdapt 15.2Â±0.1 15.9Â±0.1 15.8Â±0.1 15.1Â±0.0 15.3Â±0.1 26.3Â±0.2 38.8Â±0.1 34.4Â±0.2 33.2Â±0.1 48.0Â±0.1 65.2Â±0.1 16.9Â±0.1 44.1Â±0.2 48.9Â±0.2 39.7Â±0.2
BNAdapt+DART-split (ours) 15.2Â±0.1 15.8Â±0.1 15.8Â±0.1 15.1Â±0.0 15.3Â±0.1 26.3Â±0.2 38.8Â±0.1 34.4Â±0.2 33.2Â±0.1 48.0Â±0.1 65.2Â±0.1 16.9Â±0.1 44.1Â±0.2 48.9Â±0.2 39.7Â±0.2
TENT 28.9Â±0.7 32.6Â±0.6 31.9Â±0.6 27.8Â±1.1 26.9Â±0.1 45.4Â±0.2 51.1Â±0.2 50.1Â±0.3 38.8Â±0.5 59.1Â±0.1 67.6Â±0.1 14.9Â±1.7 56.8Â±0.2 60.1Â±0.1 54.2Â±0.1
TENT+DART-split (ours) 28.9Â±0.7 32.2Â±0.7 31.9Â±0.6 27.8Â±1.1 26.9Â±0.1 45.4Â±0.2 51.1Â±0.2 50.1Â±0.3 38.8Â±0.5 59.1Â±0.1 67.6Â±0.1 14.9Â±1.7 56.8Â±0.2 60.1Â±0.1 54.2Â±0.1
SAR 26.5Â±0.1 25.8Â±0.5 27.2Â±0.1 25.4Â±0.2 24.7Â±0.2 37.2Â±0.1 46.6Â±0.1 43.5Â±0.2 39.8Â±0.1 55.3Â±0.1 66.7Â±0.1 32.6Â±0.2 51.4Â±0.2 56.1Â±0.1 49.5Â±0.1
SAR+DART-split (ours) 26.5Â±0.1 25.7Â±0.4 27.2Â±0.1 25.4Â±0.2 24.7Â±0.2 37.2Â±0.1 46.6Â±0.1 43.5Â±0.2 39.8Â±0.1 55.3Â±0.1 66.7Â±0.1 32.6Â±0.2 51.4Â±0.2 56.1Â±0.1 49.4Â±0.1
ODS 29.7Â±0.5 32.7Â±0.5 32.2Â±0.6 28.7Â±0.7 28.0Â±0.2 44.4Â±0.2 51.4Â±0.1 49.4Â±0.3 40.3Â±0.5 59.1Â±0.1 68.3Â±0.1 17.3Â±1.6 56.7Â±0.2 60.0Â±0.1 54.0Â±0.2
ODS+DART-split (ours) 29.7Â±0.5 32.6Â±0.5 32.2Â±0.6 28.7Â±0.7 28.0Â±0.2 44.4Â±0.2 51.4Â±0.1 49.4Â±0.3 40.3Â±0.5 59.1Â±0.1 68.3Â±0.1 17.3Â±1.6 56.7Â±0.2 60.0Â±0.1 54.0Â±0.2
41Under review as submission to TMLR
Table 29: Average accuracy (%) on ImageNet-C-imb (IR1000)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 2.2Â±0.0 3.0Â±0.0 1.8Â±0.0 17.9Â±0.1 9.8Â±0.1 14.8Â±0.1 22.6Â±0.2 17.0Â±0.1 23.3Â±0.1 24.5Â±0.1 59.0Â±0.2 5.4Â±0.0 17.0Â±0.2 20.6Â±0.2 31.8Â±0.2
BNAdapt 9.8Â±0.0 10.2Â±0.1 10.1Â±0.1 9.2Â±0.1 9.4Â±0.1 16.2Â±0.1 23.8Â±0.2 21.9Â±0.1 21.2Â±0.1 30.1Â±0.1 41.9Â±0.2 10.5Â±0.1 27.1Â±0.2 30.2Â±0.1 24.9Â±0.2
BNAdapt+DART-split (ours) 9.8Â±0.0 9.9Â±0.1 10.0Â±0.1 9.2Â±0.1 9.4Â±0.1 16.2Â±0.1 23.8Â±0.2 21.9Â±0.1 21.4Â±0.1 30.5Â±0.2 43.7Â±0.7 10.5Â±0.1 27.3Â±0.2 30.2Â±0.1 26.6Â±0.6
TENT 8.6Â±0.2 8.9Â±0.8 10.3Â±0.6 8.0Â±0.4 6.8Â±0.4 11.7Â±1.7 24.7Â±0.6 22.7Â±1.0 12.6Â±0.5 32.5Â±0.2 39.9Â±0.2 2.4Â±0.2 29.9Â±0.4 33.1Â±0.1 28.5Â±0.4
TENT+DART-split (ours) 8.7Â±0.1 10.0Â±0.7 10.3Â±0.8 8.0Â±0.4 6.8Â±0.4 11.8Â±1.7 24.9Â±0.6 22.8Â±0.8 13.5Â±0.9 32.9Â±0.2 42.4Â±0.7 2.4Â±0.2 30.0Â±0.4 33.1Â±0.1 30.5Â±0.7
SAR 15.9Â±0.1 13.4Â±0.3 16.2Â±0.3 14.3Â±0.3 14.2Â±0.2 22.2Â±0.1 28.2Â±0.2 26.9Â±0.2 24.9Â±0.1 34.5Â±0.1 42.6Â±0.2 14.5Â±1.8 31.6Â±0.3 34.6Â±0.1 30.6Â±0.1
SAR+DART-split (ours) 15.7Â±0.2 13.1Â±0.4 16.0Â±0.2 14.3Â±0.3 14.1Â±0.2 22.2Â±0.1 28.2Â±0.2 26.9Â±0.2 24.9Â±0.1 34.6Â±0.1 44.3Â±0.7 14.5Â±1.8 31.3Â±0.4 34.6Â±0.2 30.8Â±0.6
ODS 14.8Â±0.3 14.9Â±1.1 17.3Â±0.9 14.1Â±0.4 12.1Â±0.9 20.4Â±2.5 40.5Â±0.6 36.7Â±1.2 22.2Â±0.9 51.0Â±0.2 61.3Â±0.2 4.5Â±0.3 47.6Â±0.4 51.8Â±0.2 45.3Â±0.5
ODS+DART-split (ours) 14.8Â±0.3 15.2Â±1.2 17.3Â±0.9 14.1Â±0.4 12.1Â±0.9 20.5Â±2.5 40.5Â±0.6 36.8Â±1.2 22.4Â±1.0 50.9Â±0.2 61.2Â±0.2 4.5Â±0.3 47.3Â±0.4 51.8Â±0.2 45.2Â±0.6
Table 30: Average accuracy (%) on ImageNet-C-imb (IR5000)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 2.2Â±0.1 2.9Â±0.0 1.8Â±0.0 18.0Â±0.1 9.8Â±0.1 14.8Â±0.1 22.5Â±0.1 16.8Â±0.1 23.2Â±0.1 24.4Â±0.1 58.9Â±0.2 5.4Â±0.1 17.0Â±0.0 20.6Â±0.1 31.7Â±0.1
BNAdapt 4.4Â±0.0 4.6Â±0.0 4.5Â±0.1 3.9Â±0.1 3.9Â±0.1 6.8Â±0.1 9.7Â±0.1 9.5Â±0.1 9.7Â±0.0 12.8Â±0.1 17.6Â±0.1 4.5Â±0.1 11.1Â±0.1 12.3Â±0.1 10.6Â±0.1
BNAdapt+DART-split (ours) 4.2Â±0.1 4.9Â±0.3 4.2Â±0.2 3.9Â±0.1 3.9Â±0.1 7.2Â±0.2 12.7Â±0.7 12.5Â±0.9 14.2Â±0.8 20.1Â±1.2 31.5Â±1.9 4.5Â±0.1 16.5Â±0.9 14.0Â±0.4 18.0Â±1.3
TENT 2.1Â±0.1 2.2Â±0.2 2.4Â±0.2 1.8Â±0.2 1.6Â±0.1 1.9Â±0.0 5.3Â±0.3 2.6Â±0.1 2.9Â±0.2 7.9Â±0.4 11.9Â±0.3 0.6Â±0.1 6.1Â±0.4 7.7Â±0.3 6.9Â±0.2
TENT+DART-split (ours) 2.8Â±0.3 3.4Â±0.2 2.8Â±0.1 1.8Â±0.1 1.6Â±0.0 3.0Â±0.2 10.8Â±0.7 7.2Â±1.3 9.3Â±1.2 18.4Â±1.4 29.8Â±2.2 0.6Â±0.1 14.5Â±1.0 11.7Â±0.7 16.4Â±1.5
SAR 5.8Â±0.1 4.5Â±0.1 5.8Â±0.2 5.0Â±0.1 4.8Â±0.1 8.5Â±0.2 11.3Â±0.2 11.2Â±0.1 10.9Â±0.1 14.4Â±0.2 17.9Â±0.1 3.9Â±0.3 12.8Â±0.1 14.0Â±0.2 12.6Â±0.2
SAR+DART-split (ours) 5.3Â±0.1 5.4Â±0.4 5.1Â±0.1 4.9Â±0.1 4.7Â±0.1 8.4Â±0.3 13.4Â±0.7 13.1Â±0.8 14.6Â±0.9 20.9Â±1.2 31.6Â±1.9 3.9Â±0.3 17.0Â±0.9 15.1Â±0.6 18.6Â±1.3
ODS 6.4Â±0.3 6.5Â±0.4 7.2Â±0.3 5.7Â±0.4 4.8Â±0.2 6.3Â±0.1 18.9Â±1.2 8.2Â±0.4 9.1Â±0.5 28.0Â±1.1 41.9Â±0.7 1.8Â±0.2 22.4Â±1.1 28.1Â±0.8 24.0Â±0.9
ODS+DART-split (ours) 7.1Â±0.2 8.2Â±0.4 8.2Â±0.3 6.0Â±0.2 4.9Â±0.2 6.9Â±0.2 21.8Â±1.5 11.4Â±0.9 13.2Â±0.6 33.5Â±1.7 47.1Â±1.7 1.8Â±0.2 26.8Â±1.5 29.2Â±1.2 30.6Â±1.1
Table 31: Average accuracy (%) on ImageNet-C-imb (IR500000)
Method gaussian_noise shot_noise impulse_noise defocus_blur glass_blur motion_blur zoom_blur snow frost fog brightness contrast elastic_transform pixelate jpeg_compression
NoAdapt 2.2Â±0.1 2.9Â±0.1 1.9Â±0.0 18.0Â±0.0 9.8Â±0.1 14.9Â±0.1 22.6Â±0.2 17.0Â±0.1 23.4Â±0.1 24.5Â±0.1 59.1Â±0.1 5.5Â±0.1 16.9Â±0.1 20.7Â±0.1 31.9Â±0.1
BNAdapt 2.1Â±0.0 2.1Â±0.0 2.1Â±0.0 1.6Â±0.1 1.6Â±0.1 2.8Â±0.1 3.7Â±0.1 3.9Â±0.1 4.4Â±0.0 5.1Â±0.1 6.7Â±0.1 2.1Â±0.0 4.2Â±0.0 4.5Â±0.1 4.3Â±0.1
BNAdapt+DART-split (ours) 2.4Â±0.1 3.5Â±0.3 2.5Â±0.3 1.6Â±0.1 1.6Â±0.0 4.5Â±0.2 9.9Â±0.8 9.7Â±0.6 11.4Â±0.9 15.4Â±1.2 22.1Â±1.4 2.1Â±0.0 12.0Â±0.9 10.9Â±0.7 12.7Â±1.0
TENT 0.8Â±0.0 0.8Â±0.1 0.8Â±0.1 0.6Â±0.0 0.6Â±0.0 0.7Â±0.1 1.6Â±0.0 0.9Â±0.1 1.0Â±0.0 2.1Â±0.1 3.2Â±0.1 0.3Â±0.0 1.6Â±0.1 1.8Â±0.1 1.8Â±0.1
TENT+DART-split (ours) 1.4Â±0.1 2.2Â±0.3 1.5Â±0.2 0.7Â±0.0 0.7Â±0.0 2.4Â±0.2 8.0Â±0.7 6.1Â±0.7 7.2Â±1.1 13.0Â±1.3 19.5Â±1.5 0.3Â±0.0 9.8Â±1.0 8.7Â±0.8 10.3Â±1.0
SAR 2.2Â±0.0 2.0Â±0.0 2.2Â±0.0 1.7Â±0.1 1.7Â±0.1 2.9Â±0.0 3.9Â±0.1 4.1Â±0.1 4.0Â±0.3 5.4Â±0.1 6.7Â±0.1 1.5Â±0.2 4.4Â±0.1 4.7Â±0.0 4.5Â±0.1
SAR+DART-split (ours) 2.6Â±0.2 3.7Â±0.3 2.7Â±0.3 1.8Â±0.1 1.7Â±0.1 4.6Â±0.3 10.1Â±0.8 9.8Â±0.6 11.4Â±1.0 15.8Â±1.3 22.0Â±1.5 1.5Â±0.2 12.1Â±0.9 11.2Â±0.8 13.0Â±1.1
ODS 4.0Â±0.0 4.1Â±0.4 4.4Â±0.3 3.5Â±0.2 3.3Â±0.0 3.7Â±0.3 12.4Â±0.3 4.8Â±0.3 5.6Â±0.1 17.5Â±0.4 28.9Â±0.7 1.2Â±0.0 13.2Â±0.6 16.4Â±0.7 13.0Â±0.6
ODS+DART-split (ours) 5.4Â±0.3 6.4Â±0.5 6.3Â±0.7 3.9Â±0.2 3.7Â±0.2 5.2Â±0.5 18.6Â±1.1 9.3Â±0.7 10.8Â±0.6 26.9Â±1.4 37.5Â±1.6 1.2Â±0.0 19.9Â±1.2 22.5Â±1.1 21.3Â±1.5
42