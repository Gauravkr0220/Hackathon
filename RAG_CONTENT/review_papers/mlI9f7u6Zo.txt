Under review as submission to TMLR
Continuous Deep Equilibrium Models: Training Neural Odes
Faster by Integrating Them to Infinity
Anonymous authors
Paper under double-blind review
Abstract
Implicit models separate the definition of a layer from the description of its solution process.
While implicit layers allow features such as depth to adapt to new scenarios and inputs
automatically, this adaptivity makes its computational expense challenging to predict. In
this manuscript, we increase the â€œimplicitness" of the DEQ by redefining the method in
terms of an infinite time neural ODE , which paradoxically decreases the training cost over
a standard neural ODE by 2âˆ’4Ã—. Additionally, we address the question: is there a
way to simultaneously achieve the robustness of implicit layers while allowing the reduced
computational expense of an explicit layer? To solve this, we develop Skip and Skip Reg.
DEQ, an implicit-explicit (IMEX) layer that simultaneously trains an explicit prediction
followed by an implicit correction. We show that training this explicit predictor is free and
even decreases the training time by 1.11âˆ’3.19Ã—. Together, this manuscript shows how
bridging the dichotomy of implicit and explicit deep learning can combine the advantages of
both techniques.
1 Introduction
Implicit layer methods, such as Neural ODEs and Deep Equilibrium models Chen et al. (2018); Bai et al.
(2019); Ghaoui et al. (2020), have gained popularity due to their ability to automatically adapt model
depth based on the â€œcomplexityâ€ of new problems and inputs. The forward pass of these methods involves
solving steady-state problems, convex optimization problems, differential equations, etc., all defined by neural
networks, which can be expensive. However, training these more generalized models has empirically been
shown to take significantly more time than traditional explicit models such as recurrent neural networks and
transformers. Nothing within the problemâ€™s structure requires expensive training methods, so we asked, can
we reformulate continuous implicit models so that this is not the case ?
Grathwohl et al. (2018); Dupont et al. (2019); Kelly et al. (2020); Finlay et al. (2020) have identified several
problems with training implicit networks. These models grow in complexity as training progresses, and a
single forward pass can take over 100 iterations (Kelly et al., 2020) even for simple problems like MNIST.
Deep Equilibrium Models (Bai et al., 2019; 2020) have better scaling in the backward pass but are still
bottlenecked by slow steady-state convergence. Bai et al. (2021b) quantified several convergence and stability
problems with DEQs. They proposed a regularization technique by exploiting the â€œimplicitness" of DEQs to
stabilize their training. We marry the idea of faster backward pass for DEQs and continuous modeling from
Neural ODEs to create Infinite Time Neural ODEs which scale significantly better in the backward pass and
drastically reduce the training time .
Our main contributions include1
1.An improved DEQ architecture (Skip-DEQ) that uses an additional neural network to predict better
initial conditions.
1We provide an anonymous version of our codebase https://anonymous.4open.science/r/continuous_deqs_infinite_time_
neural_odes/ with the intent of public release after the review period
1Under review as submission to TMLR
CIFAR10 Small CIFAR10 Large ImageNe tTime Relative to Cont. DEQ
0123456Training
CIFAR10 Small C IFAR10 Large Image Net0123456Backwa rd PassSkip Cont. DEQ
Skip Reg. C ont. DE Q
Neural ODE
Figure 1: Relative Training and Backward Pass Timings against Continuous DEQs (lower is
better): In all scenarios, Neural ODEs take 4.7âˆ’6.182Ã—more time in the backward pass compared to
Vanilla Continuous DEQs. Whereas combining Skip (Reg.) with Continuous DEQs accelerates the backward
pass by 2.8âˆ’5.9Ã—.
2.A regularization scheme (Skip Regularized DEQ) incentivizes the DEQ to learn simpler dynamics and
leads to faster training and prediction. Notably, this does not require nested automatic differentiation
and thus is considerably less computationally expensive than other published techniques.
3.A continuous formulation for DEQs as an infinite time neural ODE, which paradoxically accelerates
the backward pass over standard neural ODEs by replacing the continuous adjoints with a simple
linear system.
4.We demonstrate the seamless combination of Continuous DEQs with Skip DEQs to create a drop-in
replacement for Neural ODEs without incurring a high training cost.
2 Background
Explicit Deep Learning Architectures specify a projection f:Xâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’Zby stacking multiple â€œlayers". Implicit
models, however, define a solution process instead of directly specifying the projection. These methods enforce
a constraint on the output space Zby learning g:XÃ—Zâˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’ Rn. By specifying a solution process, implicit
models can effectively vary features like depth to adapt automatically to new scenarios and inputs. Some
prominent implicit models include Neural ODEs (Chen et al., 2018), where the output zis defined by the
ODEdz
dt=gÏ•(x,t). Liu et al. (2019) generalized this framework to Stochastic Differential Equations (SDEs)
by stochastic noise injection, which regularizes the training of Neural ODEs, allowing them to be more robust
and achieve better generalization. Bai et al. (2019) designed equilibrium models where the output zwas
constrained to be a steady state, zâˆ—=fÎ¸(zâˆ—,x). Another example of implicit layer architectures is seen in
Amos & Kolter (2017); Agrawal et al. (2019) set zto be the solution of convex optimization problems.
Deep Implicit Models essentially removed the design bottleneck of choosing the â€œdepth" of neural networks.
Instead, these models use a â€œtolerance" to determine the accuracy to which the constraint needs to be satisfied.
Additionally, many of these models only require O(1)memory for backpropagation, thus alluding to potential
increased efficiency over their explicit layer counterparts. However, evaluating these models require solving
differential equations (Chen et al., 2018; Liu et al., 2019), non-linear equations (Bai et al., 2019), convex
optimization problems (Amos & Kolter, 2017; Agrawal et al., 2019), etc. Numerous authors (Dupont et al.,
2019; Grathwohl et al., 2018; Finlay et al., 2020; Kelly et al., 2020; Ghosh et al., 2020; Bai et al., 2021b) have
noted that this solution process makes implicit models significantly slower in practice during training and
prediction compared to explicit networks achieving similar accuracy.
2Under review as submission to TMLR
Figure 2:Discrete DEQ Formulation : Discrete DEQ Block where the input xis injected at every iteration
till the system (with initial state z0) converges to a steady zâˆ—. In Vanilla DEQ, z0= 0while in Skip DEQ, an
additional explicit model gÏ•(which can potentially share the weights of fÎ¸) is used to set the initial state
z0=gÏ•(x).
2.1 Neural Ordinary Differential Equations
Initial Value Problems (IVPs) are a class of ODEs that involve finding the state at a later time t1, given the
valuez0at timet0. Chen et al. (2018) proposed the Neural ODE framework, which uses neural networks to
model the ODE dynamics
dz(t)
dt=fÎ¸(z)
Using adaptive time stepping allows the model to operate at a variable continuous depth depending on the
inputs. Removing the fixed depth constraint of Residual Networks provides a more expressive framework and
offers several advantages in problems like density estimation Grathwohl et al. (2018), irregularly spaced time
series problems Rubanova et al. (2019), etc. Training Neural ODEs using continuous adjoints has the added
benefit of constant memory overhead. However, this benefit often leads to slower training since we need to
backsolve an ODE. We defer the exact details of the continuous adjoint equations to Chen et al. (2018).
2.2 Deep Equilibrium Models
Deep Equilibrium Networks (DEQs) (Bai et al., 2019) are implicit models where the output space represents a
steady-state solution. Intuitively, this represents infinitely deep neural networks with input injection, i.e., an
infinite composition of explicit layers zn+1=fÎ¸(zn,x)withz0= 0andnâ†’âˆ. In practice, it is equivalent to
evaluating a dynamical system until it reaches a steady state zâˆ—=fÎ¸(zâˆ—,x). Bai et al. (2019; 2020) perform
nonlinear fixed point iterations of the discrete dynamical system using Broydenâ€™s method (Broyden, 1965;
Bai et al., 2020) to reach this steady-state solution.
Evaluating DEQs requires solving a steady-state equation involving multiple evaluations of the explicit layer
slowing down the forward pass. However, driving the solution to steady-state makes the backward pass
very efficient (Johnson, 2006). Despite a potentially infinite number of evaluations of fÎ¸in the forward pass,
backpropagation only requires solving a linear equation.
zâˆ—=fÎ¸(zâˆ—,x)
=â‡’âˆ‚zâˆ—
âˆ‚Î¸=fÎ¸(zâˆ—,x)
âˆ‚zâˆ—Â·âˆ‚zâˆ—
âˆ‚Î¸+âˆ‚fÎ¸(zâˆ—,x)
âˆ‚Î¸
=â‡’/parenleftbigg
Iâˆ’âˆ‚fÎ¸(zâˆ—,x)
âˆ‚zâˆ—/parenrightbiggâˆ‚zâˆ—
âˆ‚Î¸=âˆ‚fÎ¸(zâˆ—,x)
âˆ‚Î¸
3Under review as submission to TMLR
For backpropagation, we need the Vector-Jacobian Product (VJP):
/parenleftbiggâˆ‚zâˆ—
âˆ‚Î¸/parenrightbiggT
v=/parenleftbiggâˆ‚fÎ¸(zâˆ—,x)
âˆ‚Î¸/parenrightbiggT/parenleftbigg
Iâˆ’âˆ‚fÎ¸(zâˆ—,x)
âˆ‚zâˆ—/parenrightbiggâˆ’T
v
=/parenleftbiggâˆ‚fÎ¸(zâˆ—,x)
âˆ‚Î¸/parenrightbiggT
g
wherevis the gradients from layers after the DEQ module. Computing/parenleftï£¬ig
Iâˆ’âˆ‚fÎ¸(zâˆ—,x)
âˆ‚zâˆ—/parenrightï£¬igâˆ’T
is expensive
and makes DEQs non-scalable to high-dimensional problems. Instead, we solve the linear equation g=/parenleftï£¬ig
âˆ‚fÎ¸(zâˆ—,x)
âˆ‚zâˆ—/parenrightï£¬igT
g+vusing Newton-Krylov Methods like GMRES (Saad & Schultz, 1986). To compute the final
VJP, we need to compute/parenleftï£¬ig
âˆ‚fÎ¸(zâˆ—,x)
âˆ‚Î¸/parenrightï£¬igT
g, which allows us to efficiently perform the backpropagation without
explicitly computing the Jacobian.
2.2.1 Multiscale Deep Equilibrium Network
Multiscale modeling (Burt & Adelson, 1987) has been the central theme for several deep computer vision
applications (Farabet et al., 2012; Yu & Koltun, 2015; Chen et al., 2016; 2017). The standard DEQ formulation
drives a single feature vector to a steady state. Bai et al. (2020) proposed Multiscale DEQ (MDEQ) to
learn coarse and fine-grained feature representations simultaneously. MDEQs operate at multiple feature
scalesz={z1,z2,...,zn}, with the new equilibrium state zâˆ—=fÎ¸(zâˆ—
1,zâˆ—
2,...,zâˆ—
n,x). All the feature vectors
in an MDEQ are interdependent and are simultaneously driven to a steady state. Bai et al. (2020) used a
Limited-Memory Broyden Solver (Broyden, 1965) to solve these large scale computer vision problems. We
use this MDEQ formulation for all our classification experiments.
2.2.2 Jacobian Stabilization
Infinite composition of a function fÎ¸does not necessarily lead to a steady-state â€“ chaos, periodicity, divergence,
etc., are other possible asymptotic behaviors. The Jacobian Matrix JfÎ¸(zâˆ—)controls the existence of a stable
steady-state and influences the convergence of DEQs in the forward and backward passes. Bai et al. (2021b)
describes how controlling the spectral radius of JfÎ¸(zâˆ—)would prevent simpler iterative solvers from diverging
or oscillating. Bai et al. (2021b) introduce a Jacobian term to the training objective to regularize the model
training. The authors use the Hutchinson estimator (Hutchinson, 1989) to compute and regularize the
Frobenius norm of the Jacobian.
Ljac=Î»jacâˆ¥ÏµTJfÎ¸(zâˆ—)âˆ¥2
2
d;Ïµâˆ¼N (0,Id)
While well-motivated, the disadvantage of this method is that the Hutchinson trace estimator requires
automatic differentiation in the loss function, thus requiring higher order differentiation in the training process
and greatly increasing the training costs. However, in return for the increased cost, it was demonstrated
that increased robustness followed, along with faster forward passes in the trained results. Our methods are
orthogonal to the Jacobian stabilization process. In Section 4, we provide empirical evidence on composing
our models with Jacobian Stabilization to achieve even more robust results.
3 Methods
3.1 Continuous Deep Equilibrium Networks
Deep Equilibrium Models have traditionally been formulated as steady-state problems for a discrete dynamical
system. However, discrete dynamical systems come with a variety of shortcomings. Consider the following
linear discrete dynamical system (See Figure 3):
un+1=Î±Â·un whereâˆ¥Î±âˆ¥<1andu0= 1
4Under review as submission to TMLR
This system converges to a steady state of uâˆ= 0. However, in many cases, this convergence can be relatively
slow. IfÎ±= 0.9, then after 10 steps, the value is u10= 0.35because a small amount only reduces each
successive step. Thus convergence could only be accelerated by taking many steps together. Even further,
ifÎ±=âˆ’0.9, the value ping-pongs over the steady state u1=âˆ’0.9, meaning that if we could take some
fractional step uÎ´tthen it would be possible to approach the steady state much faster.
t0 20 40un+1=ğ›¼un
-0.50.00.51.0
ğ›¼=âˆ’0.9
ğ›¼= 0.5
ğ›¼= 0.9
ğ›¼= 0.1
ğ›¼= 0.99
Figure 3: Slow Convergence of Simple Linear Discrete Dynamical Systems
Rico-Martinez et al. (1992); Bulsari (1995) describe several other shortcomings of using discrete steady-state
dynamics over continuous steady-state dynamics. These issues combined motivate changing from a discrete
description of the system (the fixed point or Broydenâ€™s method approach) to a continuous description of the
system that allows adaptivity to change the stepping behavior and accelerate convergence.
To this end, we propose an alternate formulation for DEQs by modeling a continuous dynamical system
(Continuous DEQ) where the forward pass is represented by an ODE which is solved from t0= 0tot1=âˆ:
dz
dt=fÎ¸(z,x)âˆ’z
wherefÎ¸is an explicit neural network. Continuous DEQs leverage fast adaptive ODE solvers, which terminate
automatically once the solution is close to a steady state, i.e.,dzâˆ—
dt= 0, which then satisfies fÎ¸(zâˆ—,x) =zâˆ—
and is thus the solution to the same implicit system as before.
The Continuous DEQ can be considered an infinite-time neural ODE in this form. However, almost
paradoxically, the infinite time version is cheaper to train than the finite time version as its solution is the
solution to the nonlinear system, meaning the same implicit differentiation formula of the original DEQ holds
for the derivative. This means that no backpropagation through the steps is required for the Continuous
DEQ, and only a linear system must be solved. In Section 4, we empirically demonstrate that Continuous
DEQs outperform Neural ODEs in terms of training time while achieving similar accuracies.
3.2 Skip Deep Equilibrium Networks
Bai et al. (2019; 2020) set the initial condition u0= 0while solving a DEQ. Assuming the existence of a
steady state, the solvers will converge given enough iterations. However, each iteration is expensive, and a
poor guess of the initial condition makes the convergence slower. To counteract these issues, we introduce an
alternate architecture for DEQ (Skip DEQ), where we use an explicit model gÏ•to predict the initial condition
for the steady-state problem u0=gÏ•(x)2. We jointly optimize for {Î¸,Ï•}by adding an auxiliary loss function:
Lskip =Î»skipâˆ¥fÎ¸(zâˆ—,x)âˆ’gÏ•(x)âˆ¥
2We note that the concurrent work Bai et al. (2021a) introduced a similar formulation as a part of HyperDEQ
5Under review as submission to TMLR
Model Jacobian Reg. # of Params Test Accuracy (%) Testing NFE Training Time (min) Prediction Time (s / batch)
Vanilla DEQ âœ— 138K 97.926Â±0.107 18.345Â±0.732 5.197Â±1.106 0 .038Â±0.009
âœ“ 98.123Â±0.025 5.034Â±0.059 7.321Â±0.454 0 .011Â±0.005
Skip DEQ âœ— 151K 97.759Â±0.080 4.001Â±0.001 1.711Â±0.202 0 .010Â±0.001
âœ“ 97.749Â±0.141 4.001Â±0.000 6.019Â±0.234 0 .012Â±0.001
Skip Reg. DEQ âœ— 138K 97.973Â±0.134 4.001Â±0.000 1.295Â±0.222 0 .010Â±0.001
âœ“ 98.016Â±0.049 4.001Â±0.000 5.128Â±0.241 0 .012Â±0.000
Table 1:MNIST Classification with Fully Connected Layers : Skip Reg. Continuous DEQ without
Jacobian Regularization takes 4Ã—less training time andspeeds up prediction time by 4Ã—compared to
Continuous DEQ. Continuous DEQ with Jacobian Regularization has a similar prediction time but takes 6Ã—
more training time than Skip Reg. Continuous DEQ. Using Skip variants speeds up training by 1.42Ã—âˆ’4Ã—.
Intuitively, our explicit model gÏ•better predicts a value closer to the steady-state (over the training iterations),
and hence we need to perform fewer iterations during the forward pass. Given that its prediction is relatively
free compared to the cost of the DEQ, this technique could decrease the cost of the DEQ by reducing the
total number of iterations required. However, this prediction-correction approach still uses the result of the
DEQ for its final predictions and thus should achieve robustness properties equal.
3.2.1 Skip Regularized DEQ: Regularization Scheme without Extra Parameters
One of the primary benefits of DEQs is the low memory footprint of these models (See Section 2). Introducing
an explicit model gÏ•increases the memory requirements for training. To alleviate this problem, we propose
a regularization term to minimize the L1 distance between the first prediction of fÎ¸and the steady-state
solution:
Lskip =Î»skipâˆ¥fÎ¸(zâˆ—,x)âˆ’fÎ¸(0,x)âˆ¥
This technique follows the same principle as the Skip DEQ where the DEQâ€™s internal neural network is now
treated as the prediction model. We hypothesize that this introduces an inductive bias in the model to learn
simpler training dynamics.
4 Experiments
In this section, we consider the effectiveness of our proposed methods â€“ Continuous DEQs and Skip DEQs â€“
on the training and prediction timings. We consider the following baselines:
1. Discrete DEQs with L-Broyden Solver.
2. Jacobian Regularization of DEQs.3
3.Multi-Scale Neural ODEs with Input Injection: A modified Continuous Multiscale DEQ without the
steady state convergence constaint.
Our primary metrics are classification accuracy, the number of function evaluations (NFEs), total training
time, time for the backward pass, and prediction time per batch. We showcase the performance of our
methods on â€“ MNIST (LeCun et al., 1998), CIFAR-10 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011),
& ImageNet (Deng et al., 2009). We use perform our experiments in Julia (Bezanson et al., 2017) using
Lux.jl (Pal, 2022) and DifferentialEquations.jl (Rackauckas & Nie, 2017; Rackauckas et al., 2018; 2020).
3We note that due to limitations of our Automatic Differentiation system, we cannot perform Jacobian Regularization for
Convolutional Models. However, our preliminary analysis suggests that the Skip DEQ and Continuous DEQ approaches are fully
composable with Jacobian Regularization and provide better performance compared to using only Jacobian Regularization (See
Table 1).
6Under review as submission to TMLR
Model Continuous # of Params Test Accuracy (%)Training Time
(s / batch)Backward Pass
(s / batch)Prediction Time
(s / batch)
Vanilla DEQ âœ— 163546 81.233Â±0.097 0.651Â±0.009 0.075Â±0.001 0.282Â±0.005
âœ“ 80.807Â±0.631 0.753Â±0.017 0.261Â±0.010 0.136Â±0.010
Skip DEQ âœ— 200122 82.013Â±0.306 0.717Â±0.022 0.115Â±0.004 0.274Â±0.005
âœ“ 80.807Â±0.230 0.806Â±0.010 0.293Â±0.004 0.154Â±0.002
Skip Reg. DEQ âœ— 163546 81.170Â±0.356 0.709Â±0.005 0.114Â±0.002 0.283Â±0.007
âœ“ 82.513Â±0.177 0.679Â±0.015 0.143Â±0.017 0.154Â±0.003
Neural ODE âœ“ 163546 83.543Â±0.393 1.608Â±0.026 1.240Â±0.021 0.207Â±0.006
Table 2:CIFAR10 Classification with Small Neural Network : Skip Reg. Continuous DEQ achieves
thehighest test accuracy among DEQs . Continuous DEQs are faster than Neural ODEs during training
by a factor of 2Ã—âˆ’2.36Ã—, with a speedup of 4.2Ã—âˆ’8.67Ã—in the backward pass. We also observe a
prediction speed-up for Continuous DEQs of 1.77Ã—âˆ’2.07Ã—against Discrete DEQs and 1.34Ã—âˆ’1.52Ã—
against Neural ODE.
0 5.00Ã—10Â³ 1. 00Ã—10â´1.50Ã—1 0â´2.0 0Ã—10â´Discrete D EQ
0.10.20.30.4Forward (Testing) Pass T ime (s)
0 5.00Ã—10Â³ 1.00Ã—10â´1.50Ã—10â´2. 00Ã—10â´0.10.20.30.4Backward (Tr aining) Pass Time ( s)
0 5.00Ã—10Â³ 1.00 Ã—10â´1.50Ã—10â´2.00Ã—1 0â´65707580859095Testing Accuracy (Top 1) (%)
Step0 5.00Ã—10Â³ 1. 00Ã—10â´1.50Ã—1 0â´2.0 0Ã—10â´Conti nuous DE Q
0.10.20.30.4
Step0 5.00Ã—10Â³ 1.00Ã—10â´1.50Ã—10â´2. 00Ã—10â´0.10.20.30.4
Step0 5.00Ã—10Â³ 1.00 Ã—10â´1.50Ã—10â´2.00Ã—1 0â´65707580859095Vanil la DEQ
Skip DEQ
Skip Reg. DEQ
Figure 4: CIFAR10 Classification with Small Neural Network
4.1 MNIST Image Classification
Training Details: Following Kelly et al. (2020), our Fully Connected Model consists of 3 layers â€“ a
downsampling layer R784âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R128, continuous DEQ layer fÎ¸:R128âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R128, and a linear classifier R128âˆâ‡•âŠ£âˆšâˆ«âŠ”â‰€â†’R10.
For regularization, we use Î»skip = 0.01and train the models for 25epochs with a batch size of 32. We
use Tsit5 (Tsitouras, 2011) with a relative tolerance for convergence of 0.005. For optimization, we use
Adam (Kingma & Ba, 2014) with a constant learning rate of 0.001.
Baselines: We use continuous DEQ and continuous DEQ with Jacobian Stabilization as our baselines. We
additionally compose Skip DEQs with Jacobian Stabilization in our benchmarks. For all experiments, we
keepÎ»jac= 1.0.
Results: We summarize our results in Table 1. Without Jacobian Stabilization, Skip Reg. Continuous DEQ
has the highest testing accuracy of 97.973%and has the lowest training and prediction timings overall . Using
Jacobian Regularization, DEQ outperforms Skip DEQ models by <0.4%, however, jacobian regularization
increases training time by 1.4âˆ’4Ã—. Skip DEQ models can obtain the lowest prediction time per batch of
âˆ¼0.01s.
4.2 CIFAR10 Image Classification
For all the baselines in this section, Vanilla DEQ is trained with the same training hyperparameters as the
corresponding Skip DEQs (taken from Bai et al. (2020)). Multiscale Neural ODE with Input Injection is
trained with the same hyperparameters as the corresponding Continuous DEQs.
7Under review as submission to TMLR
Model Continuous # of Params Test Accuracy (%)Training Time
(s / batch)Backward Pass
(s / batch)Prediction Time
(s / batch)
Vanilla DEQ âœ— 10.63M 88.913Â±0.287 0.625Â±0.165 0.111Â±0.021 0.414Â±0.222
âœ“ 89.367Â±0.832 1.284Â±0.011 0.739Â±0.003 0.606Â±0.010
Skip DEQ âœ— 11.19M 88.783Â±0.178 0.588Â±0.042 0.112Â±0.006 0.314Â±0.017
âœ“ 89.600Â±0.947 0.697Â±0.012 0.150Â±0.013 0.625Â±0.004
Skip Reg. DEQ âœ— 10.63M 88.773Â±0.115 0.613Â±0.048 0.109Â±0.008 0.268Â±0.031
âœ“ 90.107Â±0.837 0.660Â±0.019 0.125Â±0.003 0.634Â±0.019
Neural ODE âœ“ 10.63M 89.047Â±0.116 5.267Â±0.078 4.569Â±0.077 0.573Â±0.010
Table 3:CIFAR10 Classification with Large Neural Network : Skip Reg. Continuous DEQ achieves
thehighest test accuracy . Continuous DEQs are faster than Neural ODEs during training by a factor of
4.1Ã—âˆ’7.98Ã—, with a speedup of 6.18Ã—âˆ’36.552Ã—in the backward pass. However, we observe a prediction
slowdown for Continuous DEQs of 1.4Ã—âˆ’2.36Ã—against Discrete DEQs and 0.90Ã—âˆ’0.95Ã—against Neural
ODE.
0 2 .0Ã—10â´4.0Ã—10â´6.0Ã—10â´8.0Ã—10â´Discrete D EQ
0.00.51.0Forward (Testing) Pass T ime (s)
0 2.0Ã—10â´4. 0Ã—10â´6. 0Ã—10â´8.0Ã—10â´0.00.51.0Backward (Tr aining) Pass Time ( s)
0 2. 0Ã—10â´4. 0Ã—10â´6.0Ã—10â´8.0Ã—10â´65707580859095Testing Accuracy (Top 1) (%)
Step0 2 .0Ã—10â´4.0Ã—10â´6.0Ã—10â´8.0Ã—10â´Conti nuous DE Q
0.00.51.0
Step0 2.0Ã—10â´4. 0Ã—10â´6. 0Ã—10â´8.0Ã—10â´0.00.51.0
Step0 2. 0Ã—10â´4. 0Ã—10â´6.0Ã—10â´8.0Ã—10â´65707580859095Vanil la DEQ
Skip DEQ
Skip Reg. DEQ
Figure 5: CIFAR10 Classification with Large Neural Network
4.2.1 Architecture with 200K parameters
Training Details: Our Multiscale DEQ architecture is the same as MDEQ-small architecture used in Bai
et al. (2020). For the explicit network in Skip DEQ, we use the residual block and downsampling blocks from
Bai et al. (2020) which account for the additional 58K trainable parameters.
We use a fixed regularization weight of Î»skip = 0.01and the models are trained for 20000 steps. We use a
batch size of 128. For continuous models, we use VCAB3 (Wanner & Hairer, 1996) with a relative tolerance
for convergence of 0.05. We use AdamW (Loshchilov & Hutter, 2017) optimizer with a cosine scheduling on
the learning rate â€“ starting from 10âˆ’3and terminating at 10âˆ’6â€“ and a weight decay of 2.5Ã—10âˆ’6.
Results: We summarize our results in Table 2 and Figure 4. Continuous DEQs are faster than Neural ODEs
during training by a factor of 2Ã—âˆ’2.36Ã—, with a speedup of 4.2Ã—âˆ’8.67Ã—in the backward pass.
4.2.2 Architecture with 11M parameters
Training Details: Our Multiscale DEQ architecture is the same as MDEQ-large architecture used in Bai
et al. (2020). For the explicit network in Skip DEQ, we use the residual block and downsampling blocks from
Bai et al. (2020) which account for the additional 58K trainable parameters.
We use a fixed regularization weight of Î»skip = 0.01and the models are trained for 90000 steps. We use a
batch size of 128. For continuous models, we use VCAB3 (Wanner & Hairer, 1996) with a relative tolerance
for convergence of 0.05. We use Adam (Kingma & Ba, 2014) optimizer with a cosine scheduling on the
learning rate â€“ starting from 10âˆ’3and terminating at 10âˆ’6.
8Under review as submission to TMLR
Model Continuous # of ParamsTest Accuracy
(Top 5) (%)Training Time
(s / batch)Backward Pass
(s / batch)Prediction Time
(s / batch)
Vanilla DEQ âœ— 17.91M 81.809Â±0.115 2.057Â±0.138 0.195Â±0.007 1.963Â±0.189
âœ“ 81.329Â±0.516 3.131Â±0.027 1.873Â±0.015 1.506Â±0.027
Skip DEQ âœ— 18.47M 81.717Â±0.452 1.956Â±0.012 0.194Â±0.001 1.843Â±0.025
âœ“ 81.334Â±0.322 2.016Â±0.129 0.845Â±0.127 1.575Â±0.053
Skip Reg. DEQ âœ— 17.91M 81.611Â±0.369 1.996Â±0.035 0.539Â±0.023 1.752Â±0.093
âœ“ 81.813Â±0.350 1.607Â±0.044 0.444Â±0.026 1.560Â±0.021
Table 4:ImageNet Classification : All the variants attain comparable evaluation accuracies. Skip (Reg.)
accelerates the training of Continuous DEQ by 1.57Ã—âˆ’1.96Ã—, with a reduction of 2.2Ã—âˆ’4.2Ã—in the
backward pass timings. However, we observe a marginal increase of 4%in prediction timings for Skip (Reg.)
Continuous DEQ compared against Continuous DEQ. For Discrete DEQs, Skip (Reg.) variants reduce the
prediction timings by 6.5%âˆ’12%.
0 2.0Ã—10âµ 4.0Ã—10 âµDiscrete D EQ
01234Forward (Testing ) Pass Time ( s)
0 2.0Ã—10âµ 4.0Ã—10âµ01234Backward ( Training) Pass Time (s)
0 2.0Ã—10âµ4. 0Ã—10âµ65707580859095Testin g Accur acy (Top 5) (%)
Step0 2.0Ã—10âµ 4.0Ã—10 âµContinuous DE Q
01234
Step0 2.0Ã—10âµ 4.0Ã—10âµ01234
Step0 2.0Ã—10âµ4. 0Ã—10âµ65707580859095Vanilla DEQ
Skip DEQ
Skip Reg. DEQ
Figure 6: ImageNet Classification
Results: We summarize our results in Table 3 and Figure 5. Continuous DEQs are faster than Neural ODEs
during training by a factor of 4.1Ã—âˆ’7.98Ã—, with a speedup of 6.18Ã—âˆ’36.552Ã—in the backward pass.
4.3 ImageNet Image Classification
Training Details: Our Multiscale DEQ architecture is the same as MDEQ-small architecture used in Bai
et al. (2020). For the explicit network in Skip DEQ, we use the residual block and downsampling blocks from
Bai et al. (2020) which account for the additional 58K trainable parameters.
We use a fixed regularization weight of Î»skip = 0.01, and the models are trained for 500000 steps. We use a
batch size of 64. For continuous models, we use VCAB3 (Wanner & Hairer, 1996) with a relative tolerance
for convergence of 0.05. We use SGD with a momentum of 0.9and weight decay of 10âˆ’6. We use a step LR
scheduling reducing the learning rate from 0.05by a multiplicative factor of 0.1at steps 100000,150000, and
250000.
Baselines: Vanilla DEQ is trained with the same training hyperparameters as the corresponding Skip DEQs
(taken from (Bai et al., 2020))4.
Results: We summarize our results in Table 4 and Figure 6. Skip (Reg.) variants accelerate the training of
Continuous DEQ by 1.57Ã—âˆ’1.96Ã—, with a reduction of 2.2Ã—âˆ’4.2Ã—in the backward pass timings.
4When training MultiScale Neural ODE with the same configuration as Continuous DEQ, we observed a 8Ã—slower backward
pass which made the training of the baseline infeasible.
9Under review as submission to TMLR
5 Related Works
5.1 Implicit Models
Implicit Models have obtained competitive results in image processing (Bai et al., 2020), generative mod-
eling (Grathwohl et al., 2018), time-series prediction (Rubanova et al., 2019), etc, at a fraction of memory
requirements for explicit models. Additionally, Kawaguchi (2021) show that for a certain class of DEQs
convergence to global optima is guaranteed at a linear rate. However, the slow training and prediction
timings (Dupont et al., 2019; Kelly et al., 2020; Finlay et al., 2020; Ghosh et al., 2020; Pal et al., 2021; Bai
et al., 2021b) often overshadow these benefits.
5.2 Accelerating Neural ODEs
Finlay et al. (2020); Kelly et al. (2020) used higher-order regularization terms to constrain the space of
learnable dynamics for Neural ODEs. Despite speeding up predictions, these models often increase the
training time by 7x (Pal et al., 2021). Alternatively, Ghosh et al. (2020) randomized the endpoint of Neural
ODEs to incentivize simpler dynamics. Pal et al. (2021) used internal solver heuristics â€“ local error and
stiffness estimates â€“ to control the learned dynamics in a way that decreased both prediction and training
time. Xia et al. (2021) rewrite Neural ODEs as heavy ball ODEs to accelerate both forward and backward
passes. Djeumou et al. (2022) replace ODE solvers in the forward with a Taylor-Lagrange expansion and
report significantly better training and prediction times.
Regularized Neural ODEs can not be directly extended to discrete DEQs (Bai et al., 2019; 2020). Our
continuous formulation introduces the potential to extend Xia et al. (2021); Djeumou et al. (2022) to DEQs.
However, these methods benefit from the structure in the backward pass, which does not apply to DEQs.
Additionally, relying on discrete sensitivity analysis (Pal et al., 2021) nullifies the benefit of a cost-effective
backward pass.
5.3 Accelerating DEQs
Bai et al. (2021b) uses second-order derivatives to regularize the Jacobian, stabilizing the training and
prediction timings of DEQs. Fung et al. (2022) proposes a Jacobian-Free Backpropagation Model, which
accelerates solving the Linear Equation in the backward pass. Our work complements these models and
can be freely composed with them. We have shown that a poor initial condition harms convergence, and a
better estimate for the same leads to faster training and prediction. We hypothesize that combining these
methods would lead to more stable and faster convergence, demonstrating this possibility with the Jacobian
regularization Skip DEQ.
6 Discussion
We have empirically shown the effectiveness of Continuous DEQs as a faster alternative for Neural ODEs.
Consistent with the ablation studies in Bai et al. (2021a), we see that Skip DEQ in itself doesnâ€™t significantly
improve the prediction or training timings for Discrete DEQs. Skip Reg. DEQ does, however, speeds up the
inference for larger Discrete DEQs. However, combining Skip DEQ and Skip Reg. DEQ with Continuous
DEQs, enable a speedup in backward pass by over 2.8âˆ’5.9Ã—. We hypothesize that this improvement is due
to reduction in the condition number, which results in faster convergence of GMRES in the backward pass,
however, acertaining this would require furthur investigation. We have demonstrated that our improvements
to DEQs and Neural ODEs enable the drop-in replacement of Skip Continuous DEQs in any classical deep
learning problem where continuous implicit models were previously employed.
6.1 Limitations
We observe the following limitations for our proposed methods:
10Under review as submission to TMLR
â€¢Reformulating a Neural ODE as a Continuous DEQ is valid, when the actual dynamics of the system
doesnâ€™t matter. This holds true for all applications of Neural ODEs to classical Deep Learning
problems.
â€¢ContinuousDEQsareslowerthantheirDiscretecounterpartsforlargermodels(withoutanysignificant
improvement to accuracy), hence the authors recommend their usage only for cases where a continuous
model is truly needed.
References
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and Zico Kolter. Differen-
tiable convex optimization layers. arXiv preprint arXiv:1910.12430 , 2019.
Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks. In
International Conference on Machine Learning , pp. 136â€“145. PMLR, 2017.
Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep Equilibrium Models. arXiv:1909.01377 [cs, stat] ,
October 2019. URL http://arxiv.org/abs/1909.01377 . arXiv: 1909.01377.
Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale Deep Equilibrium Models. arXiv:2006.08656 [cs,
stat], November 2020. URL http://arxiv.org/abs/2006.08656 . arXiv: 2006.08656.
Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Neural deep equilibrium solvers. In International Conference
on Learning Representations , 2021a.
Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Stabilizing equilibrium models by jacobian regularization.
arXiv preprint arXiv:2106.14342 , 2021b.
Jeff Bezanson, Alan Edelman, Stefan Karpinski, and Viral B Shah. Julia: A fresh approach to numerical
computing. SIAM Review , 59(1):65â€“98, 2017. doi: 10.1137/141000671. URL https://epubs.siam.org/
doi/10.1137/141000671 .
Charles G Broyden. A class of methods for solving nonlinear simultaneous equations. Mathematics of
computation , 19(92):577â€“593, 1965.
A.B. Bulsari. Neural Networks for Chemical Engineers . Computer-aided chemical engineering. Elsevier, 1995.
ISBN 9780444820976. URL https://books.google.com/books?id=atBTAAAAMAAJ .
Peter J Burt and Edward H Adelson. The laplacian pyramid as a compact image code. In Readings in
computer vision , pp. 671â€“679. Elsevier, 1987.
Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L Yuille. Attention to scale: Scale-aware
semantic image segmentation. In Proceedings of the IEEE conference on computer vision and pattern
recognition , pp. 3640â€“3649, 2016.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:
Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
IEEE transactions on pattern analysis and machine intelligence , 40(4):834â€“848, 2017.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential
equations. arXiv preprint arXiv:1806.07366 , 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition , pp. 248â€“255. Ieee,
2009.
Franck Djeumou, Cyrus Neary, Eric Goubault, Sylvie Putot, and Ufuk Topcu. Taylor-lagrange neural ordinary
differential equations: Toward fast training and evaluation of neural odes, 2022.
11Under review as submission to TMLR
Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. arXiv preprint
arXiv:1904.01681 , 2019.
Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features for
scene labeling. IEEE transactions on pattern analysis and machine intelligence , 35(8):1915â€“1929, 2012.
Chris Finlay, JÃ¶rn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. How to train your neural
ode.arXiv preprint arXiv:2002.02798 , 2020.
Samy Wu Fung, Howard Heaton, Qiuwei Li, Daniel McKenzie, Stanley Osher, and Wotao Yin. Jfb: Jacobian-
free backpropagation for implicit networks. In Proceedings of the AAAI Conference on Artificial Intelligence ,
2022.
Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Y. Tsai. Implicit Deep
Learning. arXiv:1908.06315 [cs, math, stat] , August 2020. URL http://arxiv.org/abs/1908.06315 .
arXiv: 1908.06315.
Arnab Ghosh, Harkirat Singh Behl, Emilien Dupont, Philip HS Torr, and Vinay Namboodiri. Steer: Simple
temporal regularization for neural odes. arXiv preprint arXiv:2006.10711 , 2020.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form
continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367 , 2018.
Michael F Hutchinson. A stochastic estimator of the trace of the influence matrix for laplacian smoothing
splines.Communications in Statistics-Simulation and Computation , 18(3):1059â€“1076, 1989.
Steven G Johnson. Notes on adjoint methods for 18.335. Introduction to Numerical Methods , 2006.
Kenji Kawaguchi. On the theory of implicit deep learning: Global convergence with implicit layers. arXiv
preprint arXiv:2102.07346 , 2021.
Jacob Kelly, Jesse Bettencourt, Matthew James Johnson, and David Duvenaud. Learning differential equations
that are easy to solve. arXiv preprint arXiv:2007.04504 , 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE , 86(11):2278â€“2324, 1998.
Xuanqing Liu, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. Neural sde: Stabilizing neural ode networks
with stochastic noise. arXiv preprint arXiv:1906.02355 , 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 ,
2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.
Avik Pal. Lux: Explicit parameterization of deep neural networks in julia. https://github.com/avik-pal/
Lux.jl/, 2022.
Avik Pal, Yingbo Ma, Viral Shah, and Christopher V Rackauckas. Opening the blackbox: Accelerating neural
differential equations by regularizing internal solver heuristics. In Proceedings of the 38th International
Conference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 8325â€“8335.
PMLR, 18â€“24 Jul 2021. URL http://proceedings.mlr.press/v139/pal21a.html .
12Under review as submission to TMLR
Christopher Rackauckas and Qing Nie. Differentialequations.jl â€“ a performant and feature-rich ecosys-
tem for solving differential equations in julia. The Journal of Open Research Software , 5(1), 2017. doi:
10.5334/jors.151. URL https://app.dimensions.ai/details/publication/pub.1085583166andhttp:
//openresearchsoftware.metajnl.com/articles/10.5334/jors.151/galley/245/download/ . Ex-
ported from https://app.dimensions.ai on 2019/05/05.
Christopher Rackauckas, Yingbo Ma, Vaibhav Dixit, Xingjian Guo, Mike Innes, Jarrett Revels, Joakim
Nyberg, and Vijay Ivaturi. A comparison of automatic differentiation and continuous sensitivity analysis
for derivatives of differential equation solutions. arXiv preprint arXiv:1812.01892 , 2018.
Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, Kirill Zubov, Rohit Supekar, Dominic
Skinner, Ali Ramadhan, and Alan Edelman. Universal differential equations for scientific machine learning.
arXiv preprint arXiv:2001.04385 , 2020.
R Rico-Martinez, K Krischer, IG Kevrekidis, MC Kube, and JL Hudson. Discrete-vs. continuous-time
nonlinear signal processing of cu electrodissolution data. Chemical Engineering Communications , 118(1):
25â€“48, 1992.
Yulia Rubanova, Ricky TQ Chen, and David Duvenaud. Latent odes for irregularly-sampled time series.
arXiv preprint arXiv:1907.03907 , 2019.
Youcef Saad and Martin H Schultz. Gmres: A generalized minimal residual algorithm for solving nonsymmetric
linear systems. SIAM Journal on scientific and statistical computing , 7(3):856â€“869, 1986.
Ch Tsitouras. Rungeâ€“kutta pairs of order 5 (4) satisfying only the first column simplifying assumption.
Computers & Mathematics with Applications , 62(2):770â€“775, 2011.
Gerhard Wanner and Ernst Hairer. Solving ordinary differential equations II , volume 375. Springer Berlin
Heidelberg New York, 1996.
Hedi Xia, Vai Suliafu, Hangjie Ji, Tan Nguyen, Andrea Bertozzi, Stanley Osher, and Bao Wang. Heavy ball
neural ordinary differential equations. Advances in Neural Information Processing Systems , 34, 2021.
Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint
arXiv:1511.07122 , 2015.
13