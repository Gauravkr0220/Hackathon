UGC: Universal Graph Coarsening
Mohit Kataria1
Mohit.Kataria@scai.iitd.ac.inSandeep Kumar2,1,3
ksandeep@ee.iitd.ac.inJayadeva2,1
jayadeva@ee.iitd.ac.in
1Yardi School of Artificial Intelligence
2Department of Electrical Engineering
3Bharti School of Telecommunication Technology and Management
Indian Institute of Technology Delhi
Abstract
In the era of big data, graphs have emerged as a natural representation of intricate
relationships. However, graph sizes often become unwieldy, leading to storage,
computation, and analysis challenges. A crucial demand arises for methods that
can effectively downsize large graphs while retaining vital insights. Graph coars-
ening seeks to simplify large graphs while maintaining the basic statistics of the
graphs, such as spectral properties and Ïµ-similarity in the coarsened graph. This
ensures that downstream processes are more efficient and effective. Most published
methods are suitable for homophilic datasets, limiting their universal use. We
propose Universal Graph Coarsening (UGC), a framework equally suitable for
homophilic and heterophilic datasets. UGC integrates node attributes and adjacency
information, leveraging the datasetâ€™s heterophily factor. Results on benchmark
datasets demonstrate that UGC preserves spectral similarity while coarsening. In
comparison to existing methods, UGC is 4 Ã—to 15Ã—faster, has lower eigen-error,
and yields superior performance on downstream processing tasks even at 70%
coarsening ratios.1
1 Introduction
Graphs have emerged as highly expressive tools to represent diverse structures and knowledge
in various fields such as social networks, bio-informatics, transportation, and natural language
processing [ 1â€“3]. They are essential for tasks like community detection, drug discovery, route
optimization, and text analysis. With the growing importance of graph-based solutions, dealing with
large graphs has become a challenge. Graph Coarsening(GC), a widely used technique to simplify
graphs while retaining vital information, making them more manageable for analysis [ 4]. It has been
applied successfully in various tasks [ 5â€“10]. Preserving the structural information of the graph is
crucial in graph coarsening algorithms to ensure the fidelity of the coarsened graphs. A high-quality
coarsened graph retains essential features and relationships, enabling accurate results for downstream
tasks. Additionally, computational efficiency is equally vital for scalability, as large-scale graphs
are common in real-world applications. An efficient coarsening method should ensure that the
reduction in graph size does not come at the expense of excessive computation time but existing
graph coarsening methods often face trade-offs between scalability and the quality of the coarsened
graph. Our method draws inspiration from hashing techniques, which provide us with advantages
in terms of computational efficiency. As a result, our approach exhibits a linear time complexity,
making it highly efficient even for large graphs.
Graph datasets often exhibit a blend of homophilic and heterophilic traits [ 11,12].Graph
Coarsening(GC) has been widely explored on homophilic datasets, but, to the best of our knowledge,
has never been applied to heterophilic graphs. We propose Universal Graph Coarsening UGC , an
approach that works well on both. Figure 2 illustrates how UGC uses a graphâ€™s adjacency matrix as
1Code is available at UGC
38th Conference on Neural Information Processing Systems (NeurIPS 2024).91112131514123875461015
141291
1110432
5127681213121514
A d j a c e n t  V e c t o r  1F e a t u r e  1F e a t u r e  2
F e a t u r e  1 5A d j a c e n t  V e c t o r  2............................................................................................................................................................................................................................................................................................................A d j a c e n t  V e c t o r  1 5
1291
111
0432
5127668121
31
21
51
4
S u p e r  N o d e  f o r m a t i o n
ba
cdegf1234567a89101112131415bcdefg111111111111111000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000ab
cdefg
C o a r s e n i n g  M a t r i xabcdefg2
22213 ,4 , 83 ,43 ,410 , 1110 , 11116 , 76 , 75 , 9551 , 9 , 36 , 7 , 85 , 7 , 86 , 7 , 8
6 , 7 , 81 2 , 1 3 , 1 4 , 1 51 3 , 1 4 , 1 51 2 , 1 3 , 1 4 , 1 5
1 2 , 1 3 , 1 4 , 1 51 2 , 1 3 , 1 4 , 1 51 2 , 1 3 , 1 51 2 , 1 3 , 1 56 , 7 , 8 , 1 41 0 , 1 11 0 , 1 1 , 1 21 0 , 1 15 , 1 11 05651 ,21 , 9
1 , 91 , 93 ,43 ,43 ,441299 ,2p 1 . Fp 2 . Fp 3 . Fp 5 . Fp 4 . Fp 6 . Fp 7 . F
D o w n s t r e a m  T a s k s  u s i n g
A u g m e n t e d  F e a t u r e s
H a s h i n g
Figure 2: This figure illustrates our framework, UGC, which has three main modules a) Generation
of an augmented matrix by incorporating feature and adjacency matrices while using heterophily
measure Î±, b) Generation of coarsening matrix Cusing augmented features via Hashing, and c)
Generation of coarsened graph GcfromCfollowed by its utilization in downstream tasks.
well as the node feature matrix. UGC relies on hashing, lending computational efficiency. UGC
exhibits linear time complexity, enabling fast processing of large datasets. Figure 1 demonstrates the
computational time gains of UGC among graph coarsening methods. UGC surpasses the fastest
existing methods by about 6 Ã—on the Physics dataset and 9 Ã—on the Squirrel dataset. UGC enhances
the performance of Graph Neural Networks (GNN) models in classification tasks, indicating its
suitability for downstream processing. UGC coarsened graphs retain essential spectral properties
and show low eigen error, hyperbolic error, and Ïµ-similarity measure. In a nutshell, UGC is fast,
universally applicable, and information-preserving.
Figure 1: This figure illustrates the computational time com-
parison among graph coarsening methods to learn a coars-
ened graph over ten iterations. UGC outperforms the fastest
existing methods by approximately 6 Ã—on the Physics dataset
and 9Ã—on the Squirrel dataset.Key Contributions.
â€¢We proposed a novel framework
that is extremely fastcompared to
other existing methods for graph
coarsening. It is also shown to
be helpful and effective for graph-
based downstream tasks.
â€¢UGC is the first to handle het-
erophilic datasets for coarsening.
â€¢UGC can retain important spectral
properties, such as eigen error, hy-
perbolic error, and Ïµ-similarity mea-
sure, which ensures the preservation
of key characteristics and informa-
tion of the original graph during the
graph coarsening.
2 Background and Problem Formulation
A graph is represented using G(V, A, X )where V={v1,Â·Â·Â·, vN}denotes set of Nvertices,
AâˆˆRNÃ—Nis the adjacency matrix and Aij>0indicates an edge (vi, vj)between nodes viandvj.
XâˆˆRNÃ—eddenotes the feature matrix where ithrow of Xis a feature vector XiâˆˆRed, associated
with node vi. The degree matrix Dis a diagonal matrix, where Dii=P
jAij.LâˆˆRNÃ—Nis a
Laplacian matrix, L=Dâˆ’A[13], and it belongs to the set SL=
LâˆˆRNÃ—N|Lji=Lijâ‰¤
0,âˆ€iÌ¸=j;Lii=âˆ’P
jÌ¸=iLij	
as defined in [ 14,15]. The adjacency matrix Aand Laplacian
matrix Lassociated with the graph are related as follows: Aij=âˆ’LijforiÌ¸=jandAij= 0 for i=j.
2CT="1 1 1 1 0 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 0 1 1 1#
(a)
21345687ACB
 (b)
Figure 3: Graph coarsening toy example, a) Coarsening matrix, b) Original graph Gand corresponding
coarsened graph Gc
Both LandAcan represent the same graph. Hence, a graph G(V, A, X )can also be represented as
G(L, X), with either representation utilized as required within the paper.
Problem. The objective is to reduce an input graph G(V, A, X )withN-nodes into a new graph
Gc(eV ,eA,eX), with n-nodes and eXâˆˆRnÃ—edwhere nâ‰ªN. The Graph Coarsening(GC) problem
requires learning of a coarsening matrix C âˆˆRNÃ—n, which is a linear mapping from Vâ†’eV. A linear
mapping ensures that similar nodes in Gare mapped to the same super-node in Gc, s.t.eX=CTX.
Every non-zero entry Cijdenotes the mapping of the ithnode of Gto the jthsuper-node Gc. ThisC
matrix belongs to the following set:
S=
C âˆˆRNÃ—n,Cijâˆˆ {0,1},âˆ¥Ciâˆ¥= 1,âŸ¨CT
i,CT
jâŸ©= 0,âˆ€iÌ¸=j,âŸ¨Cl,ClâŸ©=deVl,âˆ¥CT
iâˆ¥0â‰¥1
(1)
where deVlmeans the number of nodes in the lth-supernode. The condition âŸ¨CT
i,CT
jâŸ©= 0ensures
that each node of Gis mapped to a unique super-node. The constraint âˆ¥CT
iâˆ¥0â‰¥1requires that each
super-node contains at least one node. Consider the 8-node graph in Figure 3b. Nodes 1, 2, 3, and
4 are mapped to super-node A, while nodes 6, 7, and 8 are mapped to super-node C. Hence, the
coarsening matrix Cis given in Figure 3a. The goal is to learn this Cmatrix such that GandGcare
similar. The Ïµâˆ’similarity is a widely used similarity measure for graphs with node features, as it
entails comparing the Laplacian norms of the respective feature matrices. The graphs G(V, A, X )
andGc(eV ,eA,eX)are said to be Ïµ-similar if there exist Ïµâ‰¥0such that
(1âˆ’Ïµ)âˆ¥Xâˆ¥Lâ‰¤ âˆ¥eXâˆ¥Lcâ‰¤(1 +Ïµ)âˆ¥Xâˆ¥L (2)
where LandLcare the Laplacian matrices of GandGcrespectively, âˆ¥Xâˆ¥L=p
tr(XTLX)and
âˆ¥eXâˆ¥Lc=q
tr(eXTLceX). The quantity tr(XTLX) =âˆ’P
i,jLijâˆ¥xiâˆ’xjâˆ¥2is known as Dirichlet
Energy (DE), which is employed to measure the smoothness of node features where xiandxjare the
node features of nodes iandj[14].
Goal : Given a graph G(V, A, X )ofNnodes, construct a coarsened graph Gc(eV ,eA,eX)withn
nodes, such that they are Ïµâˆ’similar.
Homophilic and Heterophilic datasets. Graph datasets may demonstrate homophily and heterophily
properties [ 16â€“19]. Homophily refers to the tendency of nodes to be connected to other nodes of
the same class or type, while heterophily signifies the tendency of nodes to connect with nodes of
different classes. A heterophily factor 0â‰¤Î±â‰¤1may be used to denote the degree of heterophily.
Î±is calculated as the fraction of edges between nodes of different classes to the total number of
edges. A strongly heterophilic graph ( Î±â†’1) has the most edges between nodes of different classes,
suggesting a diverse network with mixed interactions. Conversely, weak heterophily or strong
homophily ( Î±â†’0) occurs in networks where nodes predominantly connect with others of the same
class.
Locality Sensitive Hashing. Locality Sensitive Hashing (LSH) is a linear time, efficient similarity
search technique for high dimensional data [ 20â€“23]. It maps high-dimensional vectors to lower
dimensions while ensuring that similar vectors collide with high probability. LSH uses a family of
hash functions to map vectors to buckets, enabling fast retrieval and similarity search. It has found
applications in image retrieval [ 24], data mining [ 25], and similarity search algorithms [ 26]. LSH
family is defined as
Definition 2.1 Letdbe a distance measure, and let d1< d 2be two distances. A family of functions
Fis said to be (d1, d2, p1, p2)âˆ’sensitive if for every fâˆˆFthe following two conditions hold:
1. Ifd(x, y)â‰¤d1then probability [f(x) =f(y)]â‰¥p1
2. Ifd(x, y)â‰¥d2then probability [f(x) =f(y)]â‰¤p2
3UGC uses LSH with a set of random projectors to map similar nodes to the same super-node. The
projection is computed as<xÂ·wi>+bi
r
, where wiis a randomly selected dâˆ’dimensional projector
vector from a pâˆ’stable distribution (see Appendix A); xrepresents the dâˆ’dimensional data sample,
andris the width of each quantization bin.
Related Works. The literature is replete with graph reduction methods and their applications; they
may be broadly classified into three categories:
1.Optimization and Heuristics: Loukas [ 15] proposed advanced spectral graph coarsening algo-
rithms based on local variation to preserve the original graphâ€™s spectral properties. Two variants,
viz.edge-based (LVE) and neighborhood-based (LVN), select contraction sets with small local
variation in each stage but have limitations in achieving arbitrary coarsening levels. Heavy edge
matching (HE) [ 9,27], determines the contraction family by computing a maximum-weight
matching based on the weight of each contraction set. The Algebraic Distance method proposed
in [27,28] calculates the weight of each candidate set using an algebraic distance measure. The
affinity method [ 29], inspired by algebraic distance, uses the vertex proximity heuristic. The
Kron reduction method [ 30] was originally proposed for electrical networks but is too slow for
large networks. FGC [ 14,31] considers both the graph structure and the node attributes as the
input and, alternatively, optimizes C. The above-mentioned methods are computationally and
memory-intensive.
2.GNN based: GCond [ 32] and SFGC [ 33] are GNN-based graph condensation methods. These
works proposed the online gradient matching schema between the synthesized small-scale graph
and the large-scale graph. However, these methods have significant issues regarding computational
time and generalization ability. First, they require training GNN models on the original graph
to get a smaller graph as they imitate the GNN training trajectory on the original graph through
gradient matching. Due to this, these methods are extremely computationally demanding and may
not be suitable for the scalability of GNN models. However, these methods can be beneficial for
other tasks, like solving storage and visualization issues. Second, the condensed graph obtained
using GCond [ 32] shows poor generalization ability across different GNN models [ 33] because
different GNN models vary in their convolution operations along graph structures.
3.Scaling GNN viz. Graph Coarsening: SCAL [ 34] and GOREN [ 35] proposed to enhance the
scalability for training GNN models using graph coarsening. It is worth noting that SCAL and
GOREN are not standalone graph coarsening techniques. SCAL uses Loukaâ€™s [ 15] work to
coarsen the graph, then trains GNN models using the coarsened graph. While GOREN trying to
improve the coarsening quality of existing methods.
3 The Proposed Framework: Universal Graph Coarsening (UGC)
The proposed UGC framework comprises three main components: (a) First, obtaining an augmented
feature matrix Fcontaining both node feature and structural information, (b) Secondly, using locality-
sensitive hashing to derive the coarsening matrix C, (c) and Finally, obtaining the coarsened graph
adjacency matrix Acand coarsened features Fc.
Construction of Augmented Feature Matrix F.In order to create a universal GC framework
suitable for all, it is important to consider features at both i) the node level, i.e., features, and ii) the
structure-level, i.e., adjacency matrix, together. In this regard, we create an augmented feature matrix
F, where each nodeâ€™s feature vector Xiis augmented with its binary adjacency vector Ai. We use the
heterophily factor Î±discussed in Section 2 to balance the emphasis between node-level and structure-
level information. The augmented feature vector for node viis given using Fi=
(1âˆ’Î±)Â·XiâŠ•Î±Â·Ai	
where âŠ•andÂ·denote the concatenation and dot product operations, respectively. Figure 11 in
Appendix K illustrates a toy example of the process involved in calculating the augmented feature
vector. While larger graphs may result in long vectors, efficient implementations and sparse tensor
methods may alleviate this hurdle. A motivating example demonstrating the need for augmented
features while doing GC is discussed in Appendix K (Figure 12).
Construction of Coarsening Matrix C.LetFiâˆˆRdrepresent the augmented feature vector of node
vi. LetW âˆˆRdÃ—landbâˆˆRlbe the hashing matrices used in UGC, with ldenoting the number of
hash functions. The hash indices generated by kthhash/projector function for ithnode is given as
hk
i=âŒŠ1
râˆ—(WkÂ·Fi+bk)âŒ‹ (3)
4where ris a hyperparameter called bin-width. The hash index that has the maximum occurrence
among the hash indices generated by all lhash functions is the hash value assigned to the graph node
vi. Hence, the hash value for node viis given by
hi=maxOccured {h1
i, h2
i....hl
i} (4)
rcontrols the size of the coarsened graph Gc; empirically, we find that increasing rmeans reducing
the size of the coarsened graph Gc. All nodes assigned with the same hash value map to the same
super-node in Gc. The reader may like to refer to Algorithm 1 for the steps in UGC. The element of
coarsening matrix, Cijequals 1 if vertex viis associated with super-node evj. Crucially, every node is
assigned a unique hivalue, ensuring an exclusive mapping to a super-node. This constraint aligns
with the formulation of super-node and guarantees at least one node per super-node. Thus, each row
ofCcontains only one non-zero entry, leading to orthogonal columns. This matrix Csatisfies the
conditions specified in Equation 1.
Algorithm 1 UGC: Universal Graph Coarsening
Require: InputG(V, A, X ),lâ†Number of Projectors, râ†binWidth
1:Î±=|{(v,u)âˆˆE:yv=yu}|
|E|;Î±is heterophily factor, yiâˆˆRNis node labels, Edenotes edge list
2:F=
(1âˆ’Î±)Â·XâŠ•Î±Â·A	
3:W âˆ¼ D (.);W âˆˆRdÃ—ldenotes lprojectors, and Dis a p-stable distribution
4:bâˆ¼ D(.);bâˆˆRldenotes sampled bias
5:H=<FÂ·W>+b
r
;H âˆˆRNÃ—l
6:Ï€iâ†maxOccurence (Hi;iâˆˆ {1,2,3, ..., N}),Ï€âˆˆRN
7:forevery node v in V do
8:C[v, Ï€[v]]â†1
9:Ac(i, j)â†P
(uâˆˆÏ€âˆ’1(evi),vâˆˆÏ€âˆ’1(evj))Auv,âˆ€i, jâˆˆ {1,2, ..., n}
10:Fc(i)â†1
|Ï€âˆ’1(evi)|P
uâˆˆÏ€âˆ’1(evi)Fu,âˆ€iâˆˆ {1,2, ..., n}
11: return Gc(Vc, Ac, Fc),C
Construction of Coarsened Graph Gc.LetGc(eV ,eA,eF)represent the coarsened graph that is to
be built. A pair of super-nodes, say eviandevj, in the coarsened graph Gcare connected, if any of
the nodes, say uâˆˆÏ€âˆ’1(evi)has an edge to any of the nodes, say vâˆˆÏ€âˆ’1(evj)in the original graph,
i.e.,âˆƒuâˆˆÏ€âˆ’1(evi), vâˆˆÏ€âˆ’1(evj)such that AuvÌ¸= 0. The coarsened graph Gcis weighted, and the
weight assigned to the edge between nodes eviandevj, is given by eAij=P
(uâˆˆÏ€âˆ’1(evi),vâˆˆÏ€âˆ’1(evj))Auv
where Auvrefers to the element (u, v)in the adjacency matrix Aof graph G. The features of
super-nodes are taken to be the average of the features of the nodes in the super-node, i.e., eFi=
1
|Ï€âˆ’1(evi)|P
uâˆˆÏ€âˆ’1(evi)Fu. The super-nodeâ€™s label is chosen as the class that has the most instances.
From the Cmatrix, we can directly calculate the adjacency eAmatrix of GcusingeA=CTACwhich is
the same as eAij.eFcan also be obtained using eF=CTFwhere Cis the coarsening matrix discussed
earlier. Because each super-edge combines multiple edges from the original graph, the number
of edges in the coarse graph is also much less than m. In general, the adjacency matrix eAhas a
substantially smaller number of non-zero elements than A. The pseudocode for UGC is listed in
Algorithm 1. UGC gives a coarsened graph Gc(Lc,eF)which also satisfies Ïµâˆ’similarity ( Ïµâ‰¥0).
Theorem 3.1 The input graph G(L, F)and the coarsened graph Gc(Lc,eF)obtained using the
proposed UGC algorithm are Ïµ-similar with Ïµâ‰¥0, i.e.,
(1âˆ’Ïµ)âˆ¥Fâˆ¥Lâ‰¤ âˆ¥eFâˆ¥Lcâ‰¤(1 +Ïµ)âˆ¥Fâˆ¥L (5)
where LandLcare the laplacian matrices of GandGcrespectively.
Proof: The proof is deferred in Appendix I.
Universal Graph Coarsening with feature re-learning for Bounded Ïµ-similarity. The coarsened
graph Gcgenerated through UGC exhibits a high degree of similarity, within the range of Ïµ, to
the original graph G. It has also been empirically demonstrated that this coarsened representation
performs exceptionally well across various downstream tasks. Nonetheless, to achieve a tighter
Ïµ-bound, where (Ïµâ‰¤1), a potential step involves introducing modifications to the feature learning
procedure of the super-nodes Gc.
5It is important to note that the Ïµ-similarity measure introduced in [ 15] does not incorporate features.
Instead, it relies on the eigenvector of the laplacian matrix to compute similarity, which limits its
ability to capture the characteristics of the associated features along with the graph structure. Once we
get the loading matrix Cusing UGC as discussed in Section 3 we used eFi=1
|Ï€âˆ’1(evi)|P
uâˆˆÏ€âˆ’1(evi)Fu
to learn the feature-vectors of super-nodes. Using eFiwe can satisfy the Theorem 3.1. However, to
give a strict bound on the Ïµsimilarity we updated eFtobFby minimizing the term
min
bFf(bF) = tr( bFTCTLCbF) +Î±
2âˆ¥CbFâˆ’Fâˆ¥2
F (6)
We aim to enforce the Dirichlet smoothness condition in super-node features using Equation 6.
The above equation is a convex optimization problem from which we get a closed-form solution by
putting the gradient w.r.t to bFequal to zero. Update rule for bFcan be derived as:
2CTLCbF+Î±CT(CbFâˆ’F) = 0 = â‡’bF=2
Î±CTLC+CTCâˆ’1
CTF
We now have re-learnt features for super-nodes, please refer to Algorithm 2 in Appendix B which
we call as UGC-FL i,e UGC with feature learning. Using bFwe can give a more strict bound on
Ïµâˆ’similarity.
Theorem 3.2 The original graph G(L, F)and coarsened graph Gc(Lc,bF)obtained using the pro-
posed UGC-FL algorithm are Ïµsimilar with 0< Ïµâ‰¤1, i.e,
(1âˆ’Ïµ)||F||Lâ‰¤ ||bF||Lcâ‰¤(1 +Ïµ)||F||L (7)
where LandLcare the laplacian matrices of GandGcrespectively, and FandbFare features matrix
associated with original and coarsened graphs, respectively.
Proof: The proof is deferred in Appendix J.
Novelty: The majority of current techniques involve coarsening the original graph and simultaneously
learning the graph structure, which makes them computationally intensive. The UGC decouples this
process, making it incredibly fast, first learning the coarsening mapping Cby capturing the similarity
of features through hashing and then using the adjacency matrix only once as Ac=CTACfor
learning the coarsened graphâ€™s structure all at once. The UGC is easy to use, extremely fast, and
produces better results for tasks requiring downstream processing.
Time Complexity Analysis of UGC. We have three phases for our framework. For the first phase,
we can see Algorithm 1, Line 5 is driving the complexity of the algorithm, where we multiply two
FâˆˆRNÃ—dandW âˆˆRdÃ—lmatrices, which results in O(Nld). In the second pass, the super-nodes
for the coarsened graphs are constructed with the help of the accumulation of nodes in the bins. The
main contribution of UGC is up to these two phases i.e., Line 1-8. Till now, time-complexity is
O(Nld)â‰¡ O(NC)where Cis a constant.
In the third phase, Lines 10-11, we calculate the adjacency and features of the super-nodes of the
coarsened graph Gc. The computational cost of this operation is O(m), where mis the number of
edges in the original graph G, and this is a one-time step. Indeed, the overall time complexity of all
three phases combined is O(N+m)where mis the number of edges. However, itâ€™s important to
note that the primary contribution of UGC lies in the process of finding the coarsening matrix, whose
time complexity is O(N). We have compared the computational time for obtaining the coarsening
matrix via UGC with the existing methods.
4 Experiments
In this section, we conduct extensive experiments to evaluate the proposed UGC against the existing
graph coarsening algorithms. The conducted experiments establish the performance of UGC concern-
ing i) computational efficiency, ii) preservation of spectral properties, and iii) potential extensions of
the coarsened graph Gcinto real-world applications.
We compare our proposed algorithm with the following coarsening algorithms, as discussed in
Section 2. UGC (feat) represents a specific scenario within our framework, wherein only the feature
values are considered for hashing, thereby obtaining the mapping of super-nodes. To comprehend the
6significance of incorporating the adjacency vector, we have added the results for both UGC (feat) and
UGC (augmented feat).
Datasets. Our experiments cover widely adopted benchmarks, including Cora ,Citeseer, Pubmed
[36],CS, Physics [37],DBLP [38]. Additionally, UGC effectively coarsens large datasets like
Flickr, Yelp [39], and Reddit [40], previously challenging for existing techniques. We also present
datasets like Squirrel, Chameleon, Texas, Film, Wisconsin [11,12,16,17], characterized by dominant
heterophilic factors. Table 6 in Appendix G provides comprehensive dataset details.
Table 1: Summary of run-time in seconds averaged over 5 runs to reduce the graph to 50%.
Data/Method Cora Cite. CS PubMed DBLP Physics Flickr Reddit Yelp Squirrel Cham. Cor. Texas Film
Var. Neigh. 6.64 8 .72 23 .43 24 .38 22 .79 58 .0 OOM OOM OOM 33.26 12.2 1.34 0.63 27.67
Var. Edges 5.34 7 .37 16 .72 18 .69 20 .59 67 .16 OOM OOM OOM 46.45 12.65 1.31 0.76 26.6
Var. Cliq. 7.29 9 .8 24 .59 61 .85 38 .31 69 .80 OOM OOM OOM 28.91 10.55 1.56 1.14 33.04
Heavy Edge 0.7 1 .41 7 .50 12 .03 8 .39 39 .77 OOM OOM OOM 18.08 5.41 1.62 1.17 11.79
Alg. Dist 0.93 1 .55 9 .63 10 .48 9 .67 46 .42 OOM OOM OOM 18.03 5.24 1.58 0.81 12.65
Affinity GS 2.36 2 .53 169 .05 168 .3 110 .9 924 .7 OOM OOM OOM 20.00 5.83 1.81 1.24 20.65
Kron 0.63 1 .37 8 .72 5 .81 7 .09 34 .53 OOM OOM OOM 20.62 7.25 1.73 0.97 12.29
UGC 0.41 0.71 3.1 1.62 1.86 6.4 8.9 16.17 170.91 2.14 0.49 0.04 0.03 1.38
Run-Time Analysis. UGCâ€™s main contribution lies in its computational efficiency. The time required
to compute the coarsening matrix Cis summarized in Table 1. By referring to this Table, it becomes
evident that UGC exhibits a remarkable advantage, surpassing all existing methods across diverse
datasets. Our model outperforms existing methods by a substantial margin. While other methods
struggle at large datasets like Physics(34.4k nodes) , UGC is able to coarsen down massive datasets
likeYelp(716.8k nodes) , which was previously not possible. It should be emphasized that the time
taken by UGC on the Reddit(232.9k nodes) dataset, which has 7Ã—the number of nodes compared to
Physics is one-third the time taken by the fastest existing methods on Physics dataset.
Spectral Properties Preservation.
1.Relative Eigen Error (REE): , REE used in [ 14,15,41] gives the means to quantify the measure
of the eigen properties of the original graph Gthat are preserved in coarsened graph Gc.
Definition 4.1 REE is defined as follows: REE (L, L c, k) =1
kPk
i=1|eÎ»iâˆ’Î»i|
Î»iwhere Î»iandeÎ»i
are top keigenvalues of original graph Laplacian ( L) and coarsened graph Laplacian ( Lc) matrix,
respectively.
2.Hyperbolic error (HE): HE [ 42] indicates the structural similarity between GandGcwith the
help of a lifted matrix along with the feature matrix Xof the original graph.
Definition 4.2 HE is defined as follows: HE=arccosh (||(Lâˆ’Llift)X||2
F||X||2
F
2trace (XTLX)trace (XTLliftX)+1) where
Lis the Laplacian matrix and XâˆˆRNÃ—dis the feature matrix of the original input graph, Lliftis
the lifted Laplacian matrix defined in [ 41] asLlift=CLcCTwhere C âˆˆRNÃ—nis the coarsening
matrix and Lcis the Laplacian of Gc.
Figure 4: Top 100 eigenvalues of the original graph Gand coarsened graph Gcat different coarsening
ratios: 30%, 50%, and 70%.
Eigenvalue preservation can be seen in Figure 4 where we have plotted the top 100 eigenvalues of G
and of Gc. We can see that the spectral property is preserved even for 70% coarsened graphs. This
approximation is more accurate for a lower coarsening ratio, i.e., the smaller the graph, the bigger the
REE. The REE for all approaches across all datasets is shown in Table 2 for a fixed 50% coarsening
ratio. UGC stands out by giving the best REE values in 8 out of 12 datasets. Although we also have
coarsened graphs for large datasets like Yelp and Reddit , eigen error calculation for these datasets
was out of memory, so we have used EOOM while other methods fail to find even the coarsened
7Table 2: This table illustrates Relative Eigen Error at 50% coarsening ratio. UGC stands out by giving
the best REE values in 8 out of 12 datasets.
Data/Method Cora Cite. CS PubMed DBLP Physics Flickr Reddit Yelp Squirrel Cham. Cor. Texas Film
Var. Neigh. 0.121 0.180 0.248 0.108 0.117 0.273 OOM OOM OOM 0.871 0.657 0.501 0.391 32.87
Var. Edges 0.129 0.136 0.049 0.965 0.135 0.042 OOM OOM OOM 0.298 0.597 0.485 0.489 21.8
Var. Cli. 0.085 0.064 0.026 1.208 0.082 0.039 OOM OOM OOM 0.369 0.456 0.550 0.463 22.95
Hea. Edge 0.071 0.043 0.046 0.834 0.086 0.031 OOM OOM OOM 0.256 0.333 0.554 0.464 5.69
Alg. Dist. 0.107 0.111 0.087 0.403 0.047 0.117 OOM OOM OOM 0.245 0.413 0.552 0.465 5.71
Aff. GS 0.095 0.057 0.063 0.063 0.073 0.052 OOM OOM OOM 0.226 0.413 0.569 0.489 5.56
Kron 0.069 0.028 0.056 0.378 0.060 0.064 OOM OOM OOM 0.246 0.413 0.554 0.491 6.12
UGC(fea.) 0.224 0.340 0.208 0.179 0.145 0.016 0.014 EOOM EOOM 13.8 7.594 0.420 0.534 9.83
UGC(fea+Ad) 0.130 0.070 0.050 0.004 0.004 0.018 0.0153 EOOM EOOM 0.546 0.409 0.215 0.204 0.075
0.2 0.4 0.6 0.8 1.0
Coarsening Ratio R0.00.51.01.52.02.5REE ErrorUGC
V AN
V AE
VC
HE
aJC
aGS
Kron
0.2 0.4 0.6 0.8 1.0
Coarsening Ratio R12345678HE errorUGC
V AN
V AE
VC
HE
aJC
0.2 0.4 0.6 0.8 1.0
Coarsening Ratio R55606570758085GCN AccuracyUGC
VAN
VAE
VC
HE
aJC
aGS
Kron
Figure 5: This figure compares graph coarsening methods in terms of REE, HE, and GCN accuracy
on the Pubmed dataset.
graph, hence the term OOM. Figure 5 illustrates the trends for eigen error, hyperbolic error and GCN
accuracy for different methods as the coarsening ratio is altered.
LSH Similarity and Ïµ-Bounded Results The LSH family used in our framework is based on p-stable
distributions Dsee Appendix A. This ensures that the probability of two nodes going to the same
super-node is directly related to the distance between their features (augmented features Ffor UGC).
Theorem 4.1 As given in [ 43], the probability that two nodes vanduwill collide and go to
a super-node under a hash function drawn uniformly at random from a 2-stable distribution is
inversely proportional to c=||vâˆ’u||2and it is represented by p(c) =Prw,b[hw,b(v) =hw,b(u)] =Rr
01
cfp t
c 
1âˆ’t
r
dt.
In our experiments, we empirically validated the Theorem 4.1. We examined if the feature distance
between any node pair was below a specific threshold, and then using the coarsening matrix C
given by UGC, we verified if they shared the same super-node or not. Our evaluation involved
counting successful matches, where nodes belonged to the same super-node, and failures, where
they did not. We subsequently calculated a probability measure based on these counts. Figure 6a
and 6b plot this probabilistic function for two datasets, namely Cora andCiteseer as a function of
distance between two nodes. Re-visiting the Definition 2.1 for the Cora dataset, we denote our LSH
family as H(1,3,1,0.20). Suppose ddenotes the distance between the nodes {u, v}. In the notation
H(1,3,1,0.20), this implies that if dâ‰¤1, there is a 100% probability that u, vwill be grouped into
the same super-node. Conversely, if d >3, the probability of {u, v}being grouped into the same
super-node is 20%. Figure 6c plots different values of Ïµat different coarsening ratios. We used
Equation 6 for updating the augmented feature matrix Fgiven by UGC and as mentioned, we got
Ïµâ‰¤1similarity guarantees for the coarsened graph. Hence proving Theorem 3.2.
Scalable Training of Graph Neural Networks. Graph neural networks (GNNs), tailored for non-
Euclidean data [ 44â€“46], have shown promise in various applications [ 47,48]. However, scalability
remains a challenge. Building on [ 34], we investigate how our graph coarsening approach can
enhance GNN scalability for training, bridging the gap between GNNs and efficient processing of
large-scale data.
GNN parameter details. We employed a single hidden layer GCN model with standard hyper-
parameters values [ 13] see Appendix H for the node-classification task. Coarsened graph Gc
is used to train the GCN model, and all the predictions are made on test data from the orig-
inal graph. The relation between coarsening ratio and accuracy is evident from Table 9 in
Appendix H. Specifically, as we progressively coarsen the graph, a slight decrease in accu-
racy values becomes noticeable. Hence, there will always be a trade-off when it comes to the
80.5 1.0 1.5 2.0 2.5 3.0
Pairwise Distance(c)020406080100Probability of Same Supernode(a)
1 2 3 4 5 6 7
Pairwise Distance(c)020406080100Probability of Same Supernode (b)
30 40 50 60 70 80 90
Coarsening ratio0.50.60.70.80.91.0Epsilon V alue
Cora
Citeseer
CS (c)
Figure 6: a) Cora and b) Citeseer demonstrate the inverse relationship between the probability of two
nodes belonging to the same super-node and the distance between them. c) plots the Ïµvalues ( â‰¤1)
for Cora, Citeseer, and CS datasets.
Table 4: This table illustrates the accuracy of GCN model when trained with 50% coarsen graph.
UGC demonstrated superior performance compared to existing methods in 7 out of the 9 datasets.
Data/Method Cora DBLP PubMed Physics Squirrel Cham. Cor. Texas Film
Var.Neigh. 79.75 77 .05 77 .87 93 .74 19.67 20.03 52.49 34.51 15.67
Var.Edges 81.57 79.93 78.34 93 .86 20.22 29.95 55.32 30.59 21.8
Var.Clique 80.92 79 .15 73 .32 92 .94 19.54 31.92 58.8 33.92 20.35
Heavy Edge 79.90 77 .46 74 .66 93 .03 20.36 33.3 54.67 29.18 19.16
Alg. Dis. 79.83 74 .51 74 .59 93 .94 19.96 28.81 59.91 18.61 19.23
Aff. GS 80.20 78 .15 80 .53 93 .06 20.00 27.58 54.06 21.18 20.34
Kron 80.71 77 .79 74 .89 92 .26 18.03 29.1 55.02 31.14 17.41
UGC(fea.) 83.92 75.50 85.65 94.70 20.71 29.9 55.6 52.4 22.6
UGC(fea+Ad) 86.30 75.50 84.77 96.12 31.62 48.7 54.7 57.1 25.4
coarsening ratio and quality of the reduced graph. To emphasize the contribution of UGC in
terms of both computational time and node-classification accuracy, we have included Figure 7.
Citeseer Cora CS Pubmed Dblp Physics Squirrel Chameleon T exas Cornell F ilm020406080100120Accuracy  (%)
2.8%4.98%
3.59%6.25%
-3.79%2.79%56.13%
31.62%65.45%
-8.69%16.51%x% Accuracy Gain x% Accuracy Loss x% Computational gain
020406080100120
Computational Gain (%)61.7%94.56%
82.65%98.89%
71.34%91.21%
71.57%
68.39%91.11%97.78%
86.62%
Figure 7: Computational and accuracy gains of UGC. In
the bar plot, dashed bars represent the gain or loss in accu-
racy when compared to the existing best-performing method,
while plain bars indicate the computational gains. All
datasets are coarsened down by 50%.This figure illustrates the improve-
ments in computational time and the
corresponding changes in accuracy
values when compared to the currently
best-performing model across various
datasets. Table 4 compares the accu-
racy among all the approaches with
all datasets when they are coarsened
down by 50%. UGC demonstrated su-
perior performance compared to exist-
ing methods in 7 out of the 9 datasets.
We have used t-SNE [ 49] algorithm
for visualization of predicted node la-
bels shown in Figure 10 in Appendix
H. It is evident that even with highly
coarsened graph training, the GCN
model can maintain its accuracy.
Table 3: This table demonstrates UGCâ€™s
model-agnostic nature, as it doesnâ€™t rely
on any specific GNN model.
Model/Data Cora Pubmed Physics Squirrel
GCN 86.30 84.77 96.12 31.62
GraphSage 69.39 85.72 94.49 61.23
GIN 67.23 84.12 85.15 44.72
GAT 74.21 84.37 92.60 48.75UGC is Model-Agnostic. While our initial validation
utilized GCN to assess the quality of our coarsened graph
Gcour framework is not bound to any specific GNN ar-
chitecture. We extended our evaluations to include other
prominent graph neural network models. Results from
three diverse models, namely GCN [ 13], GraphSage [ 40],
GIN [ 50], and GAT [ 51], have been incorporated into our
analysis. All the models were trained using 50% coarsened
graphGc. Results from Table 3 demonstrate the robustness
and model-agnostic nature of UGC. Refer to Table 7 in
9Appendix H for a comprehensive analysis of node classification accuracy results for various GNN
models. We believe this flexibility further enhances the applicability and utility of our proposed
framework in various graph-based applications.
Gained Performance on Heterophilic Graphs. Existing work for GC is focused on homophilic
datasets. A notable contribution of our framework is its ability to generalize to all datasets, including
heterophilic datasets as well. Building upon the observations made in Table 2 and Table 4 our methods,
UGC (feat) and UGC (aug. feat.), showcase notable improvements in both node classification accuracy
and REE values when applied to heterophilic datasets. A comparison of these results reveals that
conventional approaches demonstrate poor node-classification accuracy on heterophilic graphs. In
contrast, our UGC (features) method achieves substantial accuracy enhancements, surpassing the
performance of these traditional approaches. Furthermore, the true potential of our approach becomes
evident with augmented features Fi.e., UGC (aug. feat.). This approach exhibits remarkable
accuracy gains, outperforming all other methods by a considerable margin, signifying the importance
of augmented features F.
5 Conclusion
In this paper, we present a framework UGC for reducing a larger graph to a smaller graph. We use
hashing of augmented node features inspired by Locality Sensitive Hashing (LSH). As expected,
the benefits of LSH are also reflected in the proposed coarsening algorithm. To the best of our
knowledge, it is the fastest algorithm for graph coarsening. Through extensive experiments, we have
also shown that our algorithm is not only fast but also preserves the properties of the original graph.
Furthermore, it is worth noting that UGC represents the first work in the domain of graph coarsening
for heterophilic datasets. This framework addresses the unique challenges posed by heterophilic
graphs and has demonstrated a significant increase in node classification accuracy following graph
coarsening. In conclusion, we believe that our framework is a major contribution to the field of graph
coarsening and offers a fast and effective solution for simplifying large networks. Our future research
goals include the exploration of different hash functions and novel applications for the framework.
6 Acknowledgement
Mohit Kataria acknowledges the generous grant received from Google Research India to sponsor
his travel to NeurIPS 2024. Additionally, this work is supported by DST INSPIRE faculty grant
MI02322G and Yardi-ScAI, IIT Delhi research fund.
References
[1]J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and M. Sun, â€œGraph neural networks:
A review of methods and applications,â€ ArXiv preprint , 2018.
[2]A. Fout, J. Byrd, B. Shariat, and A. Ben-Hur, â€œProtein interface prediction using graph convolu-
tional networks,â€ Advances in neural information processing systems , vol. 30, 2017.
[3]Y . Wu, D. Lian, Y . Xu, L. Wu, and E. Chen, â€œGraph convolutional networks with markov
random field reasoning for social spammer detection,â€ in Proceedings of the AAAI conference
on artificial intelligence , vol. 34, pp. 1054â€“1061, 2020.
[4]S. Kumar, J. Ying, J. V . de Miranda Cardoso, and D. P. Palomar, â€œA unified framework for
structured graph learning via spectral constraints.,â€ J. Mach. Learn. Res. , vol. 21, no. 22,
pp. 1â€“60, 2020.
[5]M. Kataria, A. Khandelwal, R. Das, S. Kumar, and J. Jayadeva, â€œLinear complexity framework
for feature-aware graph coarsening via hashing,â€ in NeurIPS 2023 Workshop: New Frontiers in
Graph Learning , 2023.
[6]B. Hendrickson and R. Leland, â€œA multilevel algorithm for partitioning graphs,â€ in Proceedings
of the 1995 ACM/IEEE Conference on Supercomputing , Supercomputing â€™95, (New York, NY ,
USA), p. 28â€“es, Association for Computing Machinery, 1995.
[7]G. Karypis and V . Kumar, â€œKumar, v.: A fast and high quality multilevel scheme for partition-
ing irregular graphs. siam journal on scientific computing 20(1), 359-392,â€ Siam Journal on
Scientific Computing , vol. 20, 01 1999.
10[8]D. Kushnir, M. Galun, and A. Brandt, â€œFast multiscale clustering and manifold identification,â€
Pattern Recognition , vol. 39, no. 10, pp. 1876â€“1891, 2006. Similarity-based Pattern Recognition.
[9]I. S. Dhillon, Y . Guan, and B. Kulis, â€œWeighted graph cuts without eigenvectors a multilevel
approach,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 29, no. 11,
pp. 1944â€“1957, 2007.
[10] L. Wang, Y . Xiao, B. Shao, and H. Wang, â€œHow to partition a billion-node graph,â€ in 2014 IEEE
30th International Conference on Data Engineering , (Chicago, IL, USA), pp. 568â€“579, IEEE,
2014.
[11] J. Zhu, Y . Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra, â€œBeyond homophily in graph
neural networks: Current limitations and effective designs,â€ in Advances in Neural Information
Processing Systems (H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds.),
vol. 33, pp. 7793â€“7804, Curran Associates, Inc., 2020.
[12] H. Pei, B. Wei, K. C.-C. Chang, Y . Lei, and B. Yang, â€œGeom-gcn: Geometric graph convolutional
networks,â€ arXiv preprint arXiv:2002.05287 , 2020.
[13] T. N. Kipf and M. Welling, â€œSemi-supervised classification with graph convolutional networks,â€
2016.
[14] M. Kumar, A. Sharma, and S. Kumar, â€œA unified framework for optimization-based graph
coarsening,â€ Journal of Machine Learning Research , vol. 24, no. 118, pp. 1â€“50, 2023.
[15] A. Loukas, â€œGraph reduction with spectral and cut guarantees,â€ Journal of Machine Learning
Research , vol. 20, no. 116, pp. 1â€“42, 2019.
[16] J. Zhu, R. A. Rossi, A. Rao, T. Mai, N. Lipka, N. K. Ahmed, and D. Koutra, â€œGraph neural
networks with heterophily,â€ in Proceedings of the AAAI conference on artificial intelligence ,
vol. 35, pp. 11168â€“11176, 2021.
[17] L. Du, X. Shi, Q. Fu, X. Ma, H. Liu, S. Han, and D. Zhang, â€œGbk-gnn: Gated bi-kernel graph
neural networks for modeling both homophily and heterophily,â€ in Proceedings of the ACM
Web Conference 2022 , pp. 1550â€“1558, 2022.
[18] M. McPherson, L. Smith-Lovin, and J. M. Cook, â€œBirds of a feather: Homophily in social
networks,â€ Annual review of sociology , no. 1, 2001.
[19] C. R. Shalizi and A. C. Thomas, â€œHomophily and contagion are generically confounded in
observational social network studies,â€ Sociological methods & research , no. 2, 2011.
[20] P. Indyk and R. Motwani, â€œApproximate nearest neighbors: Towards removing the curse of
dimensionality,â€ in Proceedings of the Thirtieth Annual ACM Symposium on Theory of Com-
puting , STOC â€™98, (New York, NY , USA), p. 604â€“613, Association for Computing Machinery,
1998.
[21] B. Kulis and K. Grauman, â€œKernelized locality-sensitive hashing for scalable image search,â€ in
2009 IEEE 12th international conference on computer vision , (Kyoto, Japan), pp. 2130â€“2137,
IEEE, IEEE, 2009.
[22] J. Buhler, â€œEfficient large-scale sequence comparison by locality-sensitive hashing,â€ Bioinfor-
matics , vol. 17, no. 5, pp. 419â€“428, 2001.
[23] V . Satuluri and S. Parthasarathy, â€œBayesian locality sensitive hashing for fast similarity search,â€
Proc. VLDB Endow. , vol. 5, p. 430â€“441, jan 2012.
[24] B. Kulis and K. Grauman, â€œKernelized locality-sensitive hashing for scalable image search,â€ in
2009 IEEE 12th international conference on computer vision , pp. 2130â€“2137, IEEE, 2009.
[25] D. Ravichandran, P. Pantel, and E. Hovy, â€œRandomized algorithms and nlp: Using locality
sensitive hash functions for high speed noun clustering,â€ in Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguistics (ACLâ€™05) , pp. 622â€“629, 2005.
11[26] O. Chum, J. Philbin, M. Isard, and A. Zisserman, â€œScalable near identical image and shot
detection,â€ in Proceedings of the 6th ACM international conference on Image and video retrieval ,
pp. 549â€“556, 2007.
[27] D. Ron, I. Safro, and A. Brandt, â€œRelaxation-based coarsening and multiscale graph organiza-
tion,â€ 2010.
[28] J. Chen and I. Safro, â€œAlgebraic distance on graphs,â€ SIAM J. Scientific Computing , vol. 33,
pp. 3468â€“3490, 12 2011.
[29] O. E. Livne and A. Brandt, â€œLean algebraic multigrid (lamg): Fast graph laplacian linear solver,â€
2011.
[30] F. Dorfler and F. Bullo, â€œKron reduction of graphs with applications to electrical networks,â€
IEEE Transactions on Circuits and Systems I: Regular Papers , vol. 60, no. 1, pp. 150â€“163,
2013.
[31] M. Kumar, A. Sharma, S. Saxena, and S. Kumar, â€œFeatured graph coarsening with similarity
guarantees,â€ in International Conference on Machine Learning , pp. 17953â€“17975, PMLR, 2023.
[32] W. Jin, L. Zhao, S. Zhang, Y . Liu, J. Tang, and N. Shah, â€œGraph condensation for graph neural
networks,â€ 2021.
[33] X. Zheng, M. Zhang, C. Chen, Q. V . H. Nguyen, X. Zhu, and S. Pan, â€œStructure-free graph
condensation: From large-scale graphs to condensed graph-free data,â€ Advances in Neural
Information Processing Systems , vol. 36, 2024.
[34] Z. Huang, S. Zhang, C. Xi, T. Liu, and M. Zhou, â€œScaling up graph neural networks via graph
coarsening,â€ 2021.
[35] C. Cai, D. Wang, and Y . Wang, â€œGraph coarsening with neural networks,â€ arXiv preprint
arXiv:2102.01350 , 2021.
[36] Z. Yang, W. W. Cohen, and R. Salakhutdinov, â€œRevisiting semi-supervised learning with graph
embeddings,â€ in Proceedings of the 33nd International Conference on Machine Learning, ICML
2016, New York City, NY, USA, June 19-24, 2016 , JMLR Workshop and Conference Proceedings,
2016.
[37] O. Shchur, M. Mumme, A. Bojchevski, and S. GÃ¼nnemann, â€œPitfalls of graph neural network
evaluation,â€ ArXiv preprint , 2018.
[38] X. Fu, J. Zhang, Z. Meng, and I. King, â€œMagnn: Metapath aggregated graph neural network for
heterogeneous graph embedding,â€ in Proceedings of The Web Conference 2020 , pp. 2331â€“2341,
2020.
[39] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V . Prasanna, â€œGraphsaint: Graph sampling
based inductive learning method,â€ arXiv preprint arXiv:1907.04931 , 2019.
[40] W. L. Hamilton, R. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€
2017.
[41] A. Loukas and P. Vandergheynst, â€œSpectrally approximating large graphs with smaller graphs,â€
inProceedings of the 35th International Conference on Machine Learning (J. Dy and A. Krause,
eds.), vol. 80 of Proceedings of Machine Learning Research , (PMLR 80:3237-3246, 2018),
pp. 3237â€“3246, PMLR, 10â€“15 Jul 2018.
[42] G. Bravo Hermsdorff and L. Gunderson, â€œA unifying framework for spectrum-preserving graph
sparsification and coarsening,â€ Advances in Neural Information Processing Systems , vol. 32,
p. 12, 2019.
[43] P. Indyk, â€œStable distributions, pseudorandom generators, embeddings, and data stream compu-
tation,â€ Journal of the ACM (JACM) , vol. 53, no. 3, pp. 307â€“323, 2006.
[44] J. Bruna, W. Zaremba, A. Szlam, and Y . LeCun, â€œSpectral networks and locally connected
networks on graphs,â€ 2013.
12[45] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y . Li, â€œSimple and deep graph convolutional networks,â€
2020.
[46] M. Defferrard, X. Bresson, and P. Vandergheynst, â€œConvolutional neural networks on graphs
with fast localized spectral filtering,â€ in Proceedings of the 30th International Conference on
Neural Information Processing Systems , NIPSâ€™16, (Red Hook, NY , USA), p. 3844â€“3852, Curran
Associates Inc., 2016.
[47] C. Li and D. Goldwasser, â€œEncoding social information with graph convolutional networks
forPolitical perspective detection in news media,â€ in Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics , (Florence, Italy), pp. 2594â€“2604, Association
for Computational Linguistics, July 2019.
[48] A. Paliwal, F. Gimeno, V . Nair, Y . Li, M. Lubin, P. Kohli, and O. Vinyals, â€œReinforced genetic
algorithm learning for optimizing computation graphs,â€ 2019.
[49] L. van der Maaten and G. Hinton, â€œVisualizing data using t-sne,â€ Journal of Machine Learning
Research , vol. 9, no. 86, pp. 2579â€“2605, 2008.
[50] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, â€œHow powerful are graph neural networks?,â€ 2018.
[51] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. LiÃ², and Y . Bengio, â€œGraph attention
networks,â€ in 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings , 2018.
[52] M. Datar, N. Immorlica, P. Indyk, and V . S. Mirrokni, â€œLocality-sensitive hashing scheme based
on p-stable distributions,â€ in Proceedings of the twentieth annual symposium on Computational
geometry , pp. 253â€“262, 2004.
[53] V . Zolatarev, â€œOne-dimensional stable distributions, vol. 65 of" translations of mathematical
monographs,",â€ American Mathematical Society. Translation from the original , 1983.
[54] J. M. Chambers, C. L. Mallows, and B. Stuck, â€œA method for simulating stable random variables,â€
Journal of the american statistical association , vol. 71, no. 354, pp. 340â€“344, 1976.
[55] J. Nolan, â€œAn introduction to stable distributions,â€ in on web , 2005.
13Appendix
A Stable Distribution
A distribution DoverRis called p-stable if there exists p â‰¥0 such that for any n real numbers
v1....vnand i.i.d. variables X1....X nwith distribution D, the random variableP
iviXihas the same
distribution as the variable (P
i|vi|p)1/pXwhere Xis a random variable with distribution D[52]. It
is known [53] that stable distributions exist for p âˆˆ(0,2].
â€¢Cauchy distribution Dc, defined by the density function c(x) =1
Ï€1
1+x2, is 1-stable.
â€¢Gaussian (normal) distribution Dg, defined by the density function g(x) =1âˆš
2Ï€eâˆ’x2
2is 2-stable.
However, it is known [ 54] that one can create p-stable random variables effectively from two
independent variables distributed uniformly across [0,1] despite the lack of closed-form density and
distribution functions.
Stable distributions have diverse applications across various fields (see survey [ 55] for details). In
computer science, they are utilized for "sketching" high-dimensional vectors, as demonstrated by
Indyk ([ 43]). The key property of p-stable distributions, mentioned in the definition, enables a
sketching technique for high-dimensional vectors. This technique involves generating a random
vector wof dimension d, with each entry independently chosen from a p-stable distribution. Given a
vector vof dimension d, the dot product wÂ·vis also a random variable. A small collection of such
dot products, corresponding to different wâ€™s, is termed as the sketch of the vector v and can be used
to estimate ||v||p[43]. However, instead of using the sketch to estimate the vector norm, we are using
it to assign hash values to each vector. These values map each vector to a point on the real line, which
is then split into equal-width segments to represent buckets. If two vectors v and If you are close,
they will have a small difference between lpnorms âˆ¥vâˆ’uâˆ¥p, and they should collide with a high
probability
B Algorithm for UGC-FL
Algorithm 2 UGC-FL: Universal Graph Coarsening with feature re-learning
Require: InputG(V, A, X ),lâ†Number of Projectors, râ†binWidth
1:Gc(eV ,eA,eF),C=UGC (G, l, r)
2:Î±=|{(v,u)âˆˆE:yv=yu}|
|E|;Î±is heterophily factor, yiâˆˆRNdenotes node labels, Edenotes edge
list
3:F=
(1âˆ’Î±)Â·XâŠ•Î±Â·A	
Augmented features
4:bF= 2
Î±CTLC+CTCâˆ’1CTF
5:returnGc(eV ,eA,bF),C
C Size of coarsened graph Controlled by Bin-Width
This section discusses the impact of bin-width on the coarsening ratio see Figure 8. Algorithm 3
outlines the procedure for determining the appropriate bin-width value that corresponds to a desired
coarsening ratio. The parameter bin-width rdecides the size of the coarsened graph Gc. For a
particular coarsening ratio R, we find the corresponding rby divide and conquer approach on the
real axis, which is similar to binary search. Algorithm 3 shows the method by which we find the rfor
any given RforGc. Figure 8 shows the relation of rwithRfor two datasets: a) Cora, and Coauthor
CS. It is observed that the Rincreases as the rincreases. For each dataset, ris a hyper-parameter
that needs to be calculated only once, and hence its computational cost is not included in the reported
time complexity.
14Figure 8: This figure shows the trend of coarsening ratio as the bin-width increases on two datasets:
Cora and Coauthor CS.
Algorithm 3 Bin-width Finder
Require: InputG(V, A, X ),Lâ†Number of Projectors, Râ†Coarsening Ratio, pâ†precision of
coarsening
Ensure: bin-width h
1:râ†1,ratioâ†1,Nâ† âˆ¥Vâˆ¥
2:while|Râˆ’ratio|> pdo
3: ifratio > R then
4: râ†râˆ—0.5
5: else
6: râ†râˆ—1.5
7: nâ†UGC (G, L, r );ndenotes number of supernodes in Gc
8: ratioâ†(1âˆ’n
N)
9:return r
D Proof of Theorem 4.1
Letfp(t)denote the probability density function of the absolute value of our stable distribu-
tion(Normal distribution), and let c=||vâˆ’u||pfor two node vectors v, u, and r is the bin-width.
Since we have a random vector w from our stable distribution, v.wâˆ’u.wis distributed as cX where
X is a random variable from our stable distribution. Therefore our probability function is
p(c) =Pra,b[ha,b(v) =ha,b(u)] =Zr
01
cfpt
c
1âˆ’t
r
dt (8)
For a fixed bin-width rthe probability of collision decreases monotonically with c=||vâˆ’u||2. For
Definition, 2.1 the hash family will be (r1, r2, p1, p2)-sensitive where p1=p(1)andp2=p(c)for
r2
r1=c.
For 2-stable distribution fp(x) =2âˆš
2Ï€eâˆ’x2/2. Equation 9 will be
p(c) =2âˆš
2Ï€Zr
01
ceâˆ’(1
c)2/2dtâˆ’2âˆš
2Ï€Zr
01
ceâˆ’(1
c)2/2t
rdt (9)
=S1(c)âˆ’S2(c)
Note that S1(c)â‰¤1.
S2(c) =2âˆš
2Ï€Â·c
rZr
0eâˆ’(t
c)2/2t
c2dt (10)
S2(c) =2âˆš
2Ï€Â·c
rZ R2
(2c2)
0eâˆ’ydy (11)
S2(c) =2âˆš
2Ï€Â·c
r[1âˆ’eâˆ’R2
(2c2)] (12)
15We have p(1) = S1(1)âˆ’S2(1)â‰¥1âˆ’eR2
2âˆ’2âˆš
2Ï€râ‰¥1âˆ’A
r, for some constant A >0. This implies
that the probability that u collides with v is at least (1âˆ’A
r)â‰ˆeâˆ’A. Thus the algorithm is correct
with the constant probability.
Ifc2â‰¤R2
2, then we have
p(c)â‰¤1âˆ’2âˆš
2Ï€c
r(1âˆ’1
e) (13)
E Additional experiments for LSH scheme
We have further validated our theoretical results through a secondary experiment. This LSH family
which we discussed above says as the distance between two nodes increases, the likelihood of them
being assigned to the same bin decreases, hence we will have more number of super-nodes now. By
increasing the bin-width, we can effectively reduce the number of super-nodes. This phenomenon
is evident when considering the average distance between node pairs in various graphs and the
corresponding bin-width required to achieve a 30% coarsening ratio. The table below illustrates these
findings:
Table 5: Average Distance and Bin-Width for 30% Coarsening
Dataset Average Distance Bin-Width
Citeseer 7.748 0.0029
Cora 5.810 0.0021
Dblp 3.168 0.000068
Pubmed 0.540 0.000025
The results in the table clearly demonstrate that as the average distance between nodes increases,
the required bin-width also increases when maintaining the same coarsening ratio. This observation
highlights the importance of considering the distance metric and bin-width selection during the graph
coarsening process to effectively control the number of super-nodes and achieve desired coarsening
ratios. Figure 8 shows the trend of coarsening ratio when we change bin-width.
F System Specifications:
All the experiments conducted for this work were performed on an Intel Xeon W-295 CPU and 64GB
of RAM desktop using the Python environment.
G Datasets
Table 6: Summary of the datasets. H.R shows heterophily factor.
Data Nodes Edges Features Class H.R( Î±)
Cora 2,708 5,429 1,433 7 0.19
Citeseer 3,327 9,104 3,703 6 0.26
DBLP 17,716 52,867 1,639 4 0.18
CS 18,333 163,788 6,805 15 0.20
PubMed 19,717 44,338 500 3 0.20
Phy. 34,493 247,962 8,415 5 0.07
Flickr 89,250 899,756 500 7 0.69
Reddit 232,965 114.61M 602 41 0.25
Yelp 716,847 13.95M 300 100
Texas 183 309 1703 5 0.91
Cornell 183 295 1703 5 0.70
Film 7600 33544 931 5 0.78
Squirrel 5201 217073 2089 5 0.78
Chameleon 2277 36101 2325 5 0.75
16H Application of coarsened graph for GNNs
This section contains additional results related to the scalable GNN training. Figure 9 shows the
GNN training pipeline. Figure 10 shows the visualization of GCN predicted nodes when training is
done using the coarsened graph.
We used four GNN models, namely GCN, GraphSage, GIN, and GAT. Table 7 contains node
classification accuracy results for across various methodologies employing different GNN models.
Table 8 contains parameter details used in UGC across different GNN models. We have used these
parameters across all methods.
Table 7: Evaluation of node classification accuracy of different GNN models when trained with 50%
coarsen graph.
Dataset Model Var.Neigh Var.Edges Var.Clique Heavy Edge Alg. Dis. Aff. GS Kron UGC
GCN 20.03 29.95 31.92 33.30 28.81 27.58 29.10 48.70
Chameleon GraphSage 20.03 20.02 22.05 23.03 19.88 20.02 27.62 58.86
GIN 20.22 19.53 25.25 19.98 18.20 18.06 21.50 54.92
GAT 22.94 19.33 26.44 21.95 23.72 18.06 21.95 55.58
GCN 19.67 20.22 19.54 20.36 19.96 20.00 18.03 31.62
Squirrel GraphSage 19.87 20.00 20.03 20.03 19.93 20.00 19.98 57.60
GIN 18.54 19.65 18.98 21.65 19.47 18.29 20.56 35.64
GAT 20.90 18.56 20.68 19.93 20.46 20.05 20.08 32.28
GCN 15.67 21.80 20.35 19.16 19.23 20.34 17.41 25.40
Film GraphSage 22.32 26.05 24.01 21.49 21.88 21.50 23.73 21.12
GIN 24.20 23.51 17.51 11.49 13.90 21.93 18.04 21.12
GAT 17.50 21.73 17.82 21.18 17.94 17.40 24.15 21.71
GCN 77.87 78.34 73.32 74.66 74.59 80.53 74.89 84.77
Pubmed GraphSage 78.85 62.73 67.18 60.11 63.09 71.25 62.00 83.76
GIN 74.77 39.29 46.19 35.97 32.13 49.63 39.29 76.36
GAT 75.22 72.63 74.81 60.04 69.47 59.76 71.92 83.56
GCN 93.74 93.86 92.94 93.03 93.94 93.06 92.26 96.12
Physics GraphSage OOM OOM OOM OOM OOM OOM OOM OOM
GIN OOM OOM OOM OOM OOM OOM OOM OOM
GAT 92.04 91.80 91.48 91.80 92.94 93.33 91.60 93.80
GCN 77.05 79.93 79.15 77.46 74.51 78.15 77.79 75.50
DBLP GraphSage 68.54 60.17 74.17 72.70 72.19 71.81 71.76 68.25
GIN 35.84 33.93 35.12 24.16 51.47 47.30 42.24 55.28
GAT 70.20 74.07 72.82 71.35 71.17 76.12 72.27 73.49
GCN 79.75 81.57 80.92 79.90 79.83 80.20 80.71 86.30
Cora GraphSage 70.49 68.48 70.16 69.17 72.26 67.77 73.20 69.39
GIN 47.65 35.03 52.91 34.00 63.05 23.49 48.56 67.23
GAT 69.26 74.02 75.92 68.95 73.09 73.83 73.24 74.21
911121315141238754610141291
1110432
5127681213121514ba
cdegf
Figure 9: GCN training pipeline
17(a) Original
 (b) 30% coarsen
 (c) 50% coarsen
 (d) 70% coarsen
Figure 10: Visualization of GCN predicted nodes when training is done using the coarsened graph of
Physics dataset.
Table 8: GNN model parameters.
MODEL HIDDEN LAYERS LEARNING RATE DECAY EPOCH
GCN {64,64} 0.003 0.0005 500
GRAPH SAGE {64,64} 0.003 0.0005 500
GIN {64,64} 0.003 0.0005 500
GAT {64,64} 0.003 0.0005 500
Table 9: We report the accuracy of GCN on node classification after coarsening by UGC at different
ratios.
Ratio/Data Cora DBLP Pub. Phy.
30 86.30 75 .50 85 .65 96 .70
50 86.30 75 .50 84 .77 96 .12
70 84.63 74 .82 80 .57 92 .43
We randomly split data in 60%, 20%, 20% for the training-validation-test.
I Proof of Theorem 3.1
Theorem I.1 Given a Graph Gand a coarsened graph Gcthey are said to be Ïµsimilar if there exists
some Ïµâ‰¥0such that:
(1âˆ’Ïµ)âˆ¥xâˆ¥Lâ‰¤ âˆ¥xâˆ¥Lcâ‰¤(1 +Ïµ)âˆ¥xâˆ¥L (14)
where LandLcare the Laplacian matrices of GandGcrespectively.
Proof: Let S be defined such that L = STS, by Choleskyâ€™s decomposition:
|âˆ¥xâˆ¥Lâˆ’ âˆ¥xcâˆ¥Lc|=|âˆ¥Sxâˆ¥2âˆ’ âˆ¥SP+Pxâˆ¥2| (15)
â‰¤ âˆ¥Sxâˆ’SP+Pxâˆ¥2=âˆ¥xâˆ’exâˆ¥Lâ‰¤ âˆ¥xâˆ¥L (16)
The conversion from Lthâˆ’norm to 2ndâˆ’norm or vice-versa is as follows:
âˆ¥xâˆ¥L=âˆš
xTLx=âˆš
xTSTSx=âˆ¥Sâˆ¥2
J Proof of Bounded Theorem 3.2
Theorem J.1 Given a graph G(L, F), a coarsened graph Gc(Lc, Fc), the enhanced features eF
obtained by enforcing smoothness condition. The original graph G(L, F)and a coarsened graph
Gc(Lc,eF), are said to be Ïµsimilar with 0< Ïµâ‰¤1
(1âˆ’Ïµ)âˆ¥Fâˆ¥Lâ‰¤ âˆ¥eFâˆ¥Lcâ‰¤(1 +Ïµ)âˆ¥Fâˆ¥L (17)
18where LandLcare the Laplacian matrices, FandFcare the augmented feature vector given by
UGC,eFis the relearned enhanced features by enforcing the smoothness condition discussed in
Equation 6.
Proof:
|âˆ¥Fâˆ¥Lâˆ’ âˆ¥eFâˆ¥Lc|=|q
tr(FTLF)âˆ’q
tr(eFTLceF)| (18)
AsLis a positive semi-definite matrix we can write L=STSusing Choleskyâ€™s decomposition and
by writing Lc=CTLCwe get,
=|q
tr(FTSTSF)âˆ’q
tr(eFTCTSTSCeF)| (19)
=|âˆ¥SFâˆ¥Fâˆ’ âˆ¥SPâ€ PFâˆ¥|F (20)
â‰¤ âˆ¥SFâˆ’SPâ€ PFâˆ¥F (21)
â‰¤Ïµâˆ¥Fâˆ¥L (22)
Using the new update rule of âˆ¥eFâˆ¥Lcwe have eFLcâ‰¤ âˆ¥Fâˆ¥L, we get
Ïµ=|âˆ¥Fâˆ¥Lâˆ’ âˆ¥eFâˆ¥Lc|
âˆ¥Fâˆ¥Lâ‰¤1 (23)
where Ïµâ‰¤1refer [ 14] for more details. See Figure 6 which plots different values of Ïµat different
coarsening ratios. As mentioned for fixed values of Î±we got Ïµâ‰¤1similarity guarantees for the
coarsened graph.
K Importance of Augmented Features
See Figure 12 which showcases the importance of considering the augmented feature vector. It
can be seen from the figure that when coarsened using Augmented features super-nodes have more
intra-node similarity.
Figure 11: A toy example illustrating the computation of augmented features of a given graph.
19Figure 12: This figure highlights the significance of the augmented vector and showcases coarsening
outcomes, specifically when coarsening is performed solely using the adjacency or feature matrix
compared to when the augmented matrix is taken into account.
20NeurIPS Paper Checklist
1.Claims
Question: Do the main claims made in the abstract and introduction accurately reflect the paperâ€™s
contributions and scope?
Answer: [Yes]
Justification: Yes, all the claims are reflected in paper. See Section 4 and Appendix.
Guidelines:
â€¢The answer NA means that the abstract and introduction do not include the claims made in the
paper.
â€¢The abstract and/or introduction should clearly state the claims made, including the contributions
made in the paper and important assumptions and limitations. A No or NA answer to this
question will not be perceived well by the reviewers.
â€¢The claims made should match theoretical and experimental results, and reflect how much the
results can be expected to generalize to other settings.
â€¢It is fine to include aspirational goals as motivation as long as it is clear that these goals are not
attained by the paper.
2.Limitations
Question: Does the paper discuss the limitations of the work performed by the authors?
Answer: [Yes]
Justification: See Section 3 (Need efficient implementations and sparse tensor methods to represent
theFmatrix) and Section 5 (Exploration of different hash functions and novel applications for
the framework).
Guidelines:
â€¢The answer NA means that the paper has no limitation while the answer No means that the
paper has limitations, but those are not discussed in the paper.
â€¢ The authors are encouraged to create a separate "Limitations" section in their paper.
â€¢The paper should point out any strong assumptions and how robust the results are to violations of
these assumptions (e.g., independence assumptions, noiseless settings, model well-specification,
asymptotic approximations only holding locally). The authors should reflect on how these
assumptions might be violated in practice and what the implications would be.
â€¢The authors should reflect on the scope of the claims made, e.g., if the approach was only tested
on a few datasets or with a few runs. In general, empirical results often depend on implicit
assumptions, which should be articulated.
â€¢The authors should reflect on the factors that influence the performance of the approach. For
example, a facial recognition algorithm may perform poorly when image resolution is low or
images are taken in low lighting. Or a speech-to-text system might not be used reliably to
provide closed captions for online lectures because it fails to handle technical jargon.
â€¢The authors should discuss the computational efficiency of the proposed algorithms and how
they scale with dataset size.
â€¢If applicable, the authors should discuss possible limitations of their approach to address
problems of privacy and fairness.
â€¢While the authors might fear that complete honesty about limitations might be used by reviewers
as grounds for rejection, a worse outcome might be that reviewers discover limitations that
arenâ€™t acknowledged in the paper. The authors should use their best judgment and recognize
that individual actions in favor of transparency play an important role in developing norms that
preserve the integrity of the community. Reviewers will be specifically instructed to not penalize
honesty concerning limitations.
3.Theory Assumptions and Proofs
Question: For each theoretical result, does the paper provide the full set of assumptions and a
complete (and correct) proof?
Answer: [Yes]
Justification: See Appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include theoretical results.
â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.
â€¢The proofs can either appear in the main paper or the supplemental material, but if they appear
in the supplemental material, the authors are encouraged to provide a short proof sketch to
provide intuition.
21â€¢Inversely, any informal proof provided in the core of the paper should be complemented by
formal proofs provided in appendix or supplemental material.
â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.
4.Experimental Result Reproducibility
Question: Does the paper fully disclose all the information needed to reproduce the main experi-
mental results of the paper to the extent that it affects the main claims and/or conclusions of the
paper (regardless of whether the code and data are provided or not)?
Answer: [Yes]
Justification: See Section 4 and Appendix
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢If the paper includes experiments, a No answer to this question will not be perceived well by the
reviewers: Making the paper reproducible is important, regardless of whether the code and data
are provided or not.
â€¢If the contribution is a dataset and/or model, the authors should describe the steps taken to make
their results reproducible or verifiable.
â€¢Depending on the contribution, reproducibility can be accomplished in various ways. For
example, if the contribution is a novel architecture, describing the architecture fully might
suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary
to either make it possible for others to replicate the model with the same dataset, or provide
access to the model. In general. releasing code and data is often one good way to accomplish
this, but reproducibility can also be provided via detailed instructions for how to replicate the
results, access to a hosted model (e.g., in the case of a large language model), releasing of a
model checkpoint, or other means that are appropriate to the research performed.
â€¢While NeurIPS does not require releasing code, the conference does require all submissions
to provide some reasonable avenue for reproducibility, which may depend on the nature of the
contribution. For example
(a)If the contribution is primarily a new algorithm, the paper should make it clear how to
reproduce that algorithm.
(b)If the contribution is primarily a new model architecture, the paper should describe the
architecture clearly and fully.
(c)If the contribution is a new model (e.g., a large language model), then there should either
be a way to access this model for reproducing the results or a way to reproduce the model
(e.g., with an open-source dataset or instructions for how to construct the dataset).
(d)We recognize that reproducibility may be tricky in some cases, in which case authors are
welcome to describe the particular way they provide for reproducibility. In the case of
closed-source models, it may be that access to the model is limited in some way (e.g.,
to registered users), but it should be possible for other researchers to have some path to
reproducing or verifying the results.
5.Open access to data and code
Question: Does the paper provide open access to the data and code, with sufficient instructions to
faithfully reproduce the main experimental results, as described in supplemental material?
Answer: [Yes]
Justification: All datasets used are publicly available. See Section 4 and link UGC-NeurIPS
Guidelines:
â€¢ The answer NA means that paper does not include experiments requiring code.
â€¢Please see the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
â€¢While we encourage the release of code and data, we understand that this might not be possible,
so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not including code, unless
this is central to the contribution (e.g., for a new open-source benchmark).
â€¢The instructions should contain the exact command and environment needed to run to reproduce
the results. See the NeurIPS code and data submission guidelines ( https://nips.cc/public/
guides/CodeSubmissionPolicy ) for more details.
â€¢The authors should provide instructions on data access and preparation, including how to access
the raw data, preprocessed data, intermediate data, and generated data, etc.
â€¢The authors should provide scripts to reproduce all experimental results for the new proposed
method and baselines. If only a subset of experiments are reproducible, they should state which
ones are omitted from the script and why.
22â€¢At submission time, to preserve anonymity, the authors should release anonymized versions (if
applicable).
â€¢Providing as much information as possible in supplemental material (appended to the paper) is
recommended, but including URLs to data and code is permitted.
6.Experimental Setting/Details
Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters,
how they were chosen, type of optimizer, etc.) necessary to understand the results?
Answer: [Yes]
Justification: See Section 4 and Appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The experimental setting should be presented in the core of the paper to a level of detail that is
necessary to appreciate the results and make sense of them.
â€¢ The full details can be provided either with the code, in appendix, or as supplemental material.
7.Experiment Statistical Significance
Question: Does the paper report error bars suitably and correctly defined or other appropriate
information about the statistical significance of the experiments?
Answer: [Yes]
Justification: See Section 4 and Appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The authors should answer "Yes" if the results are accompanied by error bars, confidence
intervals, or statistical significance tests, at least for the experiments that support the main claims
of the paper.
â€¢The factors of variability that the error bars are capturing should be clearly stated (for example,
train/test split, initialization, random drawing of some parameter, or overall run with given
experimental conditions).
â€¢The method for calculating the error bars should be explained (closed form formula, call to a
library function, bootstrap, etc.)
â€¢ The assumptions made should be given (e.g., Normally distributed errors).
â€¢It should be clear whether the error bar is the standard deviation or the standard error of the
mean.
â€¢It is OK to report 1-sigma error bars, but one should state it. The authors should preferably
report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of
errors is not verified.
â€¢For asymmetric distributions, the authors should be careful not to show in tables or figures
symmetric error bars that would yield results that are out of range (e.g. negative error rates).
â€¢If error bars are reported in tables or plots, The authors should explain in the text how they were
calculated and reference the corresponding figures or tables in the text.
8.Experiments Compute Resources
Question: For each experiment, does the paper provide sufficient information on the computer
resources (type of compute workers, memory, time of execution) needed to reproduce the experi-
ments?
Answer: [Yes]
Justification: See Appendix.
Guidelines:
â€¢ The answer NA means that the paper does not include experiments.
â€¢The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud
provider, including relevant memory and storage.
â€¢The paper should provide the amount of compute required for each of the individual experimental
runs as well as estimate the total compute.
â€¢The paper should disclose whether the full research project required more compute than the
experiments reported in the paper (e.g., preliminary or failed experiments that didnâ€™t make it
into the paper).
9.Code Of Ethics
Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS
Code of Ethics https://neurips.cc/public/EthicsGuidelines ?
Answer: [Yes]
23Justification: Research conducted in the paper conform, in every respect, with the NeurIPS Code
of Ethics.
Guidelines:
â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
â€¢If the authors answer No, they should explain the special circumstances that require a deviation
from the Code of Ethics.
â€¢The authors should make sure to preserve anonymity (e.g., if there is a special consideration due
to laws or regulations in their jurisdiction).
10.Broader Impacts
Question: Does the paper discuss both potential positive societal impacts and negative societal
impacts of the work performed?
Answer: [NA]
Justification: There is no societal impact of the work performed.
Guidelines:
â€¢ The answer NA means that there is no societal impact of the work performed.
â€¢If the authors answer NA or No, they should explain why their work has no societal impact or
why the paper does not address societal impact.
â€¢Examples of negative societal impacts include potential malicious or unintended uses (e.g.,
disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deploy-
ment of technologies that could make decisions that unfairly impact specific groups), privacy
considerations, and security considerations.
â€¢The conference expects that many papers will be foundational research and not tied to par-
ticular applications, let alone deployments. However, if there is a direct path to any negative
applications, the authors should point it out. For example, it is legitimate to point out that
an improvement in the quality of generative models could be used to generate deepfakes for
disinformation. On the other hand, it is not needed to point out that a generic algorithm for
optimizing neural networks could enable people to train models that generate Deepfakes faster.
â€¢The authors should consider possible harms that could arise when the technology is being used
as intended and functioning correctly, harms that could arise when the technology is being used
as intended but gives incorrect results, and harms following from (intentional or unintentional)
misuse of the technology.
â€¢If there are negative societal impacts, the authors could also discuss possible mitigation strategies
(e.g., gated release of models, providing defenses in addition to attacks, mechanisms for
monitoring misuse, mechanisms to monitor how a system learns from feedback over time,
improving the efficiency and accessibility of ML).
11.Safeguards
Question: Does the paper describe safeguards that have been put in place for responsible release of
data or models that have a high risk for misuse (e.g., pretrained language models, image generators,
or scraped datasets)?
Answer: [NA]
Justification: The paper poses no such risks
Guidelines:
â€¢ The answer NA means that the paper poses no such risks.
â€¢Released models that have a high risk for misuse or dual-use should be released with necessary
safeguards to allow for controlled use of the model, for example by requiring that users adhere
to usage guidelines or restrictions to access the model or implementing safety filters.
â€¢Datasets that have been scraped from the Internet could pose safety risks. The authors should
describe how they avoided releasing unsafe images.
â€¢We recognize that providing effective safeguards is challenging, and many papers do not require
this, but we encourage authors to take this into account and make a best faith effort.
12.Licenses for existing assets
Question: Are the creators or original owners of assets (e.g., code, data, models), used in the
paper, properly credited and are the license and terms of use explicitly mentioned and properly
respected?
Answer: [Yes]
Justification: Assets are properly credited and publicly available.
Guidelines:
â€¢ The answer NA means that the paper does not use existing assets.
â€¢ The authors should cite the original paper that produced the code package or dataset.
24â€¢ The authors should state which version of the asset is used and, if possible, include a URL.
â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.
â€¢For scraped data from a particular source (e.g., website), the copyright and terms of service of
that source should be provided.
â€¢If assets are released, the license, copyright information, and terms of use in the package should
be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for
some datasets. Their licensing guide can help determine the license of a dataset.
â€¢For existing datasets that are re-packaged, both the original license and the license of the derived
asset (if it has changed) should be provided.
â€¢If this information is not available online, the authors are encouraged to reach out to the assetâ€™s
creators.
13.New Assets
Question: Are new assets introduced in the paper well documented and is the documentation
provided alongside the assets?
Answer: [NA]
Justification: The paper does not release new assets.
Guidelines:
â€¢ The answer NA means that the paper does not release new assets.
â€¢Researchers should communicate the details of the dataset/code/model as part of their sub-
missions via structured templates. This includes details about training, license, limitations,
etc.
â€¢The paper should discuss whether and how consent was obtained from people whose asset is
used.
â€¢At submission time, remember to anonymize your assets (if applicable). You can either create
an anonymized URL or include an anonymized zip file.
14.Crowdsourcing and Research with Human Subjects
Question: For crowdsourcing experiments and research with human subjects, does the paper
include the full text of instructions given to participants and screenshots, if applicable, as well as
details about compensation (if any)?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
â€¢Including this information in the supplemental material is fine, but if the main contribution of
the paper involves human subjects, then as much detail as possible should be included in the
main paper.
â€¢According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other
labor should be paid at least the minimum wage in the country of the data collector.
15.Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Sub-
jects
Question: Does the paper describe potential risks incurred by study participants, whether such
risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals
(or an equivalent approval/review based on the requirements of your country or institution) were
obtained?
Answer: [NA]
Justification: The paper does not involve crowdsourcing nor research with human subjects.
Guidelines:
â€¢The answer NA means that the paper does not involve crowdsourcing nor research with human
subjects.
â€¢Depending on the country in which research is conducted, IRB approval (or equivalent) may be
required for any human subjects research. If you obtained IRB approval, you should clearly
state this in the paper.
â€¢We recognize that the procedures for this may vary significantly between institutions and
locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for
their institution.
â€¢For initial submissions, do not include any information that would break anonymity (if applica-
ble), such as the institution conducting the review.
25