Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems
with Solutions Exhibiting Weak Minty Properties
Abstract
This research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-
strate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has
already demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.
We establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within
this framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared
operator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified
version of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problemâ€™s
specific parameters.
1 Introduction
The recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,
have generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,
adversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed
that generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization
component and nonconcave in the maximization component remains limited, with some research even suggesting intractability in
certain cases.
A specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)
exhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of
the recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified
(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing
literature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of
weak Minty solutions was quickly investigated.
Assumption 1 (Weak Minty solution). For a given operator F:Rdâ†’Rd, there is a point uâˆ—âˆˆRdand a parameter Ï >0such that:
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥ âˆ’Ï
2âˆ¥F(u)âˆ¥2âˆ€uâˆˆRd. (1)
Moreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving
a complexity of O(Ïµâˆ’1)for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step
followed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant
of EG.
In a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-
Backward (FoRB). We address the following question with an affirmative answer:
Can OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?
Specifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0and a parameter 0< Î³â‰¤1
as follows:
uk= Â¯ukâˆ’aF(Â¯uk),
Â¯uk+1= Â¯ukâˆ’Î³aF(uk),âˆ€kâ‰¥0,
can achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.
It is worth noting that OGDA is most frequently expressed in a form where Î³= 1. However, two recent studies have examined
a more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of Î³becomes
apparent only when dealing with weak Minty solutions. In this context, we find that Î³must be greater than 1 to ensure convergence,
a phenomenon that is not observed in monotone problems.
When examining a general smooth min-max problem:
min
xmax
yf(x, y)
the operator Fmentioned in Assumption 1 naturally emerges as F(u) := [âˆ‡xf(x, y),âˆ’âˆ‡yf(x, y)]withu= (x, y). However,
by examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can
concurrently address more scenarios, such as certain equilibrium problems.
The parameter Ïin the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it
is essential that the step size exceeds a value proportional to Ï. Simultaneously, as is typical, the step size is limited from above
by the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than1
4L, their
convergence claim is valid only if Ï <1
4L. This condition was later improved to Ï <1
2Lfor the choice Î³= 1and to Ï <1
Lfor
even smaller values of Î³. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different
analysis, we are able to match the most general condition on the weak Minty parameter Ï <1
Lfor appropriate Î³anda.
1.1 Contribution
Our contributions are summarized as follows:
1.We establish a new convergence rate of O(1/k), measured by the squared operator norm, for a modified version of OGDA,
which we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to
the Minty variational inequality.
2.Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible
step sizes for OGDA+ and obtain the most favorable result known for the standard method ( Î³= 1).
3. We demonstrate a complexity bound of O(Ïµâˆ’2)for a stochastic variant of the OGDA+ method.
4.We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without
requiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in
areas with low curvature, enabling convergence where a fixed step size strategy might fail.
1.2 Related literature
We will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap
function or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,
or under the Polyak-Åojasiewicz assumption.
Weak Minty. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,
termed "weak Minty," without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.
Convergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as
the update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the
length of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is
also proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step
size we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps
per descent step, achieving the same O(1/k)rate as EG.
Minty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.
It was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing
the resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a
specific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While
the assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone
problems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point
by a factor proportional to the squared operator norm.
Negative comonotonicity. Although previously studied under the term "cohypomonotonicity," the concept of negative comono-
tonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty
solutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and
an improved convergence rate of O(1/k2)(in terms of the squared operator norm) was shown. Similarly, an accelerated version of
the reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty
solutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty
solution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient
norm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate
this class from problems with weak Minty solutions.
2
Interaction dominance. The concept of Î±-interaction dominance for nonconvex-nonconcave min-max problems was investigated,
and it was shown that the proximal-point method converges sublinearly if this condition is met in yand linearly if it is met in both
components. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively
comonotone.
Optimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the
attention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has
also been studied in the mathematical programming community.
2 Preliminaries
2.1 Notions of solution
We outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These
concepts are typically defined with respect to a constraint set CâŠ†Rd. A Stampacchia solution of the VI given by F:Rdâ†’Rdis a
point uâˆ—such that:
âŸ¨F(uâˆ—), uâˆ’uâˆ—âŸ© â‰¥0âˆ€uâˆˆC. (SVI)
In this work, we only consider the unconstrained case where C=Rd, and the above condition simplifies to F(uâˆ—) = 0 . Closely
related is the following concept: A Minty solution is a point uâˆ—âˆˆCsuch that:
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥0âˆ€uâˆˆC. (MVI)
For a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but
holds, for example, if the operator Fis monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but
without any Minty solutions.
2.2 Notions of monotonicity
This section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.
An operator Fis considered monotone if:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥0.
Such operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium
problems.
Two frequently studied notions that fall into this category are strongly monotone operators, which satisfy:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥Âµâˆ¥uâˆ’vâˆ¥2,
and cocoercive operators, which fulfill:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥Î²âˆ¥F(u)âˆ’F(v)âˆ¥2. (2)
Strongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max
problems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with Î²equal
to the inverse of the gradientâ€™s Lipschitz constant.
Departing from monotonicity. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring
the non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and
spurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and
foremost is the extensively studied setting of Î½-weak monotonicity:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥ âˆ’ Î½âˆ¥uâˆ’vâˆ¥2.
Such operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it
includes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this
property. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,
has received much less attention and is given by:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥ âˆ’ Î³âˆ¥F(u)âˆ’F(v)âˆ¥2.
Clearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.
Behavior with respect to the solution. While the above properties are standard assumptions in the literature, it is usually sufficient
to require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of
monotonicity, it is enough to ask for the operator Fto be star-monotone, i.e.,
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥0,
or star-cocoercive,
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥Î³âˆ¥F(u)âˆ¥2.
In this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the
operator Fto be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the
above star notions are sometimes required to hold for all solutions uâˆ—, in the following we only require it to hold for a single solution.
3
3 OGDA for problems with weak Minty solutions
The generalized version of OGDA, which we denote with a "+" to emphasize the presence of the additional parameter Î³, is given by:
Algorithm 1 OGDA+
Require: Starting point u0=uâˆ’1âˆˆRd, step size a >0and parameter 0< Î³ < 1.
fork= 0,1, ...do
uk+1=ukâˆ’a((1 + Î³)F(uk)âˆ’F(ukâˆ’1))
end for
Theorem 3.1. LetF:Rdâ†’RdbeL-Lipschitz continuous satisfying Assumption 1 with1
L> Ï, and let (uk)kâ‰¥0be the iterates
generated by Algorithm 1 with step size asatisfying a > Ï and
aLâ‰¤1âˆ’Î³
1 +Î³. (3)
Then, for all kâ‰¥0,
min
i=0,...,kâˆ’1âˆ¥F(ui)âˆ¥2â‰¤1
kaÎ³(aâˆ’Ï)âˆ¥u0+aF(u0)âˆ’uâˆ—âˆ¥2.
In particular, as long as Ï <1
L, we can find a Î³small enough such that the above bound holds.
The first observation is that we would like to choose aas large as possible, as this allows us to treat the largest class of problems
withÏ < a . To be able to choose a large step size a, we must decrease Î³, as evident from (3). However, this degrades the algorithmâ€™s
speed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could
derive an optimal Î³(i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on
Ï. In practice, the strategy of decreasing Î³until convergence is achieved, but not further, yields reasonable results.
Furthermore, we want to point out that the condition Ï <1
Lis precisely the best possible bound for EG+.
3.1 Improved bounds under monotonicity
While the above theorem also holds if the operator Fis monotone, we can modify the proof slightly to obtain a better dependence on
the parameters:
Theorem 3.2. LetF:Rdâ†’Rdbe monotone and L-Lipschitz. If aL=2âˆ’Î³
2+Î³âˆ’ÏµforÏµ >0, then the iterates generated by OGDA+
fulfill
min
i=0,...,kâˆ’1âˆ¥F(ui)âˆ¥2â‰¤2
ka2Î³2Ïµâˆ¥u0+aF(u0)âˆ’uâˆ—âˆ¥2.
In particular, we can choose Î³= 1anda <1
2L.
There are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a <1
2L. However, we
want to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as
ours for OGDA is shown, but requires the conservative step size bound aâ‰¤1
16L. This was later improved to aâ‰¤1
3L. All of these
only deal with the case Î³= 1. The only other reference that deals with a generalized (i.e., not necessarily Î³= 1) version of OGDA
is another work, where the resulting step size condition is aâ‰¤2âˆ’Î³
4L, which is strictly worse than ours for any Î³. To summarize, not
only do we show for the first time that the step size of a generalization of OGDA can go above1
2L, but we also provide the least
restrictive bound for any value of Î³.
3.2 OGDA+ stochastic
In this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent
estimators F(Â·, Î¾i)at every iteration. We assume here that the estimator Fis unbiased, i.e., E[F(uk, Î¾)|ukâˆ’1] =F(uk), and has
bounded variance E[âˆ¥F(uk, Î¾)âˆ’F(uk)âˆ¥2]â‰¤Ïƒ2. We show that we can still guarantee convergence by using batch sizes Bof order
O(Ïµâˆ’1).
Algorithm 2 stochastic OGDA+
Require: Starting point u0=uâˆ’1âˆˆRd, step size a >0, parameter 0< Î³â‰¤1and batch size B.
fork= 0,1, ...do
Sample i.i.d. (Î¾i)B
i=1and compute estimator Ëœgk=1
BPB
i=1F(uk, Î¾k
i)
uk+1=ukâˆ’a((1 + Î³)Ëœgkâˆ’Ëœgkâˆ’1)
end for
4
Theorem 3.3. LetF:Rdâ†’RdbeL-Lipschitz satisfying Assumption 1 with1
L> Ï, and let (uk)kâ‰¥0be the sequence of
iterates generated by stochastic OGDA+, with aandÎ³satisfying Ï < a <1âˆ’Î³
1+Î³1
L. Then, to visit an Ïµ-stationary point such that
mini=0,...,kâˆ’1E[âˆ¥F(ui)âˆ¥2]< Ïµ, we require
1
kaÎ³(aâˆ’Ï)âˆ¥u0+aËœg0âˆ’uâˆ—âˆ¥2max
1,4Ïƒ2
aLÏµ
calls to the stochastic oracle ËœF, with large batch sizes of order O(Ïµâˆ’1).
In practice, large batch sizes of order O(Ïµâˆ’1)are typically not desirable; instead, a small or decreasing step size is preferred. In the
weak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,
the current analysis does not allow for variable Î³.
4 EG+ with adaptive step sizes
In this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the
Lipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to
small step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in
choosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the
step size is chosen larger than a multiple of the weak Minty parameter Ïto guarantee convergence at all. For these reasons, we want
to outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried
out.
Since the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive
step size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,
especially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially
interesting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in
multiple gradient computations per iteration.
We use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone
decreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox
method, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.
A version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the
optimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and
are in terms of the gap function.
One of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which
results in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our
knowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch
is the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of Fat
all. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative
convergence result is still open.
Algorithm 3 EG+ with adaptive step size
Require: Starting points u0,Â¯u0âˆˆRd, initial step size a0and parameters Ï„âˆˆ(0,1)and0< Î³â‰¤1.
fork= 0,1, ...do
Find the step size:
ak= min
akâˆ’1,Ï„âˆ¥Â¯ukâˆ’Â¯ukâˆ’1âˆ¥
âˆ¥F(Â¯uk)âˆ’F(Â¯ukâˆ’1)âˆ¥
(4)
Compute next iterate:
uk= Â¯ukâˆ’akF(Â¯uk)
Â¯uk+1= Â¯ukâˆ’akÎ³F(uk).
end for
Clearly, akis monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that
akâ‰¥min{a0, Ï„/L}>0. The sequence therefore converges to a positive number, which we denote by aâˆ:= lim kak.
Theorem 4.1. LetF:Rdâ†’RdbeL-Lipschitz that satisfies Assumption 1, where uâˆ—denotes any weak Minty solution, with
aâˆ>2Ï, and let (uk)kâ‰¥0be the iterates generated by Algorithm 3 with Î³=1
2andÏ„âˆˆ(0,1). Then, there exists a k0âˆˆNsuch that
min
i=k0,...,kâˆ¥F(uk)âˆ¥2â‰¤1
kâˆ’k0L
Ï„(aâˆ/2âˆ’Ï)âˆ¥Â¯uk0âˆ’uâˆ—âˆ¥2.
5
Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the
Lipschitz constant of the operator Fdoes not need to be known. Moreover, the step size choice presented in (4) might allow us
to take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later
iterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage
that they allow us to solve a richer class of problems, as we are able to relax the condition Ï <1
4Lin the case of EG+ to Ï < a âˆ/2,
where aâˆ= lim kakâ‰¥Ï„/L.
On the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when
ak/ak+1â‰¤1
Ï„is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in
the starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient
ak/ak+1being too large. This drawback could be mitigated by choosing Ï„smaller. However, this will result in poor performance
due to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be
circumvented, and authors instead focused on the convergence of the iterates without any rate.
5 Numerical experiments
In the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see
Algorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification
of EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with
an initial guess made by second-order information, whose extra cost we ignore in the experiments.
5.1 Von Neumannâ€™s ratio game
We consider von Neumannâ€™s ratio game, which is given by:
min
xâˆˆâˆ†mmax
yâˆˆâˆ†nV(x, y) =âŸ¨x, RyâŸ©
âŸ¨x, SyâŸ©, (5)
where RâˆˆRmÃ—nandSâˆˆRmÃ—nwithâŸ¨x, SyâŸ©>0for all xâˆˆâˆ†m, yâˆˆâˆ†n, with âˆ† :={zâˆˆRd:zi>0,Pd
i=1zi= 1}denoting
the unit simplex. Expression (5) can be interpreted as the value V(x, y)for a stochastic game with a single state and mixed strategies.
We see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although
an estimated Ïis more than ten times larger than the estimated Lipschitz constant.
5.2 Forsaken
A particularly difficult min-max toy example with a "Forsaken" solution was proposed and is given by:
min
xâˆˆRmax
yâˆˆRx(yâˆ’0.45) + Ï•(x)âˆ’Ï•(y), (6)
where Ï•(z) =1
6z6âˆ’2
4z4+1
4z2âˆ’1
2z. This problem exhibits a Stampacchia solution at (xâˆ—, yâˆ—)â‰ˆ(0.08,0.4), but also two limit
cycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to
the solution repels possible trajectories of iterates, thus "shielding" the solution. Later, it was noticed that, restricted to the box
âˆ¥(x, y)âˆ¥âˆ<3, the above-mentioned solution is weak Minty with Ïâ‰¥2Â·0.477761 , which is much larger than1
2Lâ‰ˆ0.08. In line
with these observations, we can see that none of the fixed step size methods with a step size bounded by1
Lconverge. In light of this
observation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz
constant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling
limit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in
the backtracking procedure.
5.3 Lower bound example
The following min-max problem was introduced as a lower bound on the dependence between ÏandLfor EG+:
min
xâˆˆRmax
yâˆˆRÂµxy+Î¶
2(x2âˆ’y2). (7)
In particular, it was stated that EG+ (with any Î³) and constant step size a=1
Lconverges for this problem if and only if (0,0)is a
weak Minty solution with Ï <1âˆ’Î³
L, where ÏandLcan be computed explicitly in the above example and are given by:
L=p
Âµ2+Î¶2and Ï=Âµ2âˆ’Î¶2
2Âµ.
By choosing Âµ= 3andÎ¶=âˆ’1, we get exactly Ï=1
L, therefore predicting divergence of EG+ for any Î³, which is exactly what is
empirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case Ï <1
L, we
observe rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.
6
6 Conclusion
Many intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave
framework. Very recently, it was demonstrated that the O(1/k)bounds on the squared operator norm for EG and OGDA for the
last iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the
presence of merely weak Minty solutions remains an open question.
In general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the
majority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem
(7), which is not covered by theory, and OGDA+ is the only method capable of converging.
Finally, we note that the previous paradigm in pure minimization of "smaller step size ensures convergence" but "larger step size
gets there faster," where the latter is typically constrained by the reciprocal of the gradientâ€™s Lipschitz constant, does not appear
to hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates
that convergence can be lost if the step size is excessively small and sometimes needs to be larger than1
L, which one can typically
only hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a
backtracking linesearch.article graphicx
7
Examining the Convergence of Denoising Diffusion Probabilistic
Models: A Quantitative Analysis
Abstract
Deep generative models, particularly diffusion models, are a significant family within deep learning. This study
provides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion model
and the target distribution. In contrast to earlier research, this analysis does not rely on presumptions regarding
the learned score function. Furthermore, the findings are applicable to any data-generating distributions within
restricted instance spaces, even those lacking a density relative to the Lebesgue measure, and the upper limit is not
exponentially dependent on the ambient space dimension. The primary finding expands upon recent research by
Mbacke et al. (2023), and the proofs presented are fundamental.
1 Introduction
Diffusion models, alongside generative adversarial networks and variational autoencoders (V AEs), are among the most influential
families of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,
as well as in various other applications.
Two primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative
models (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while
simultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Conversely, SGMs
employ score-matching methods to approximate the score function of the data-generating distribution, subsequently generating new
samples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function, adding varying
noise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the score
function for all noise levels has been proposed.
Although DDPMs and SGMs may initially seem distinct, it has been demonstrated that DDPMs implicitly approximate the score
function, with the sampling process resembling Langevin dynamics. Moreover, a unified perspective of both methods using stochastic
differential equations (SDEs) has been derived. The SGM can be viewed as a discretization of Brownian motion, and the DDPM as a
discretization of an Ornstein-Uhlenbeck process. Consequently, both DDPMs and SGMs are commonly referred to as SGMs in the
literature. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-based
framework, necessitating assumptions about the effectiveness of the learned score function.
In this research, a different strategy is employed, applying methods created for V AEs to DDPMs, which can be viewed as hierarchical
V AEs with fixed encoders. This method enables the derivation of quantitative, Wasserstein-based upper bounds without making
assumptions about the data distribution or the learned score function, and with simple proofs that do not need the SDE toolkit.
Furthermore, the bounds presented here do not involve any complex discretization steps, as the forward and backward processes are
considered discrete-time from the beginning, rather than being viewed as discretizations of continuous-time processes.
1.1 Related Works
There has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. However,
these studies frequently depend on restrictive assumptions regarding the data-generating distribution, produce non-quantitative upper
bounds, or exhibit exponential dependencies on certain parameters. This work successfully circumvents all three of these limitations.
Some bounds are based on very restrictive assumptions about the data-generating distribution, such as log-Sobolev inequalities,
which are unrealistic for real-world data distributions. Furthermore, some studies establish upper bounds on the Kullback-Leibler
(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by the
diffusion model; however, unless strong assumptions are made about the support of the data-generating distribution, KL and TV
reach their maximum values. Such assumptions arguably do not hold for real-world data-generating distributions, which are widely
believed to satisfy the manifold hypothesis. Other work establishes conditions under which the support of the input distribution
is equal to the support of the learned distribution, and generalizes the bound to all f-divergences. Assuming L2 accurate score
estimation, some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, but
their Wasserstein-based bounds are not quantitative. Quantitative Wasserstein distance upper bounds under the manifold hypothesis
have been derived, but these bounds exhibit exponential dependencies on some of the problem parameters.
1.2 Our contributions
In this study, strong assumptions about the data-generating distribution are avoided, and a quantitative upper bound on the Wasserstein
distance is established without exponential dependencies on problem parameters, including the ambient space dimension. Moreover,
a common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According to
some, providing precise guarantees for the estimation of the score function is challenging, as it necessitates an understanding of the
non-convex training dynamics of neural network optimization, which is currently beyond reach. Therefore, upper bounds are derived
without making assumptions about the learned score function. Instead, the bound presented here is dependent on a reconstruction
loss calculated over a finite independent and identically distributed (i.i.d.) sample. Intuitively, a loss function is defined, which
quantifies the average Euclidean distance between a sample from the data-generating distribution and the reconstruction obtained by
sampling noise and passing it through the backward process (parameterized by Ë˜03b8). This method is inspired by previous work on
V AEs.
This approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution, avoids
exponential dependencies on the dimension, and provides a quantitative upper bound based on the Wasserstein distance. Furthermore,
this method benefits from utilizing very straightforward and basic proofs.
2 Preliminaries
Throughout this paper, lowercase letters are used to represent both probability measures and their densities with respect to the
Lebesgue measure, and variables are added in parentheses to enhance readability (e.g., q(xt|xtâˆ’1)to denote a time-dependent
conditional distribution). An instance space X, which is a subset of RDwith the Euclidean distance as the underlying metric, and
a target data-generating distribution ÂµâˆˆM+
1(X)are considered. Note that it is not assumed that Âµhas a density with respect to
the Lebesgue measure. Additionally, || Â· || represents the Euclidean (L2) norm, and Ep(x)is used as shorthand for Exâˆ¼p(x). Given
probability measures p, qâˆˆM+
1(X)and a real number k >1, the Wasserstein distance of order kis defined as (Villani, 2009):
Wk(p, q) = inf
Î³âˆˆÎ“(p,q)Z
XÃ—X||xâˆ’y||kdÎ³(x, y)1/k
,
where Î“(p, q)denotes the set of couplings of pandq, meaning the set of joint distributions on XÃ—Xwith respective marginals p
andq. The product measure pâŠ—qis referred to as the trivial coupling, and the Wasserstein distance of order 1 is simply referred to
as the Wasserstein distance.
2.1 Denoising Diffusion Models
Instead of employing the SDE framework, diffusion models are presented using the DDPM formulation with discrete-time processes.
A diffusion model consists of two discrete-time stochastic processes: a forward process and a backward process. Both processes are
indexed by time 0â‰¤tâ‰¤T, where the number of time steps Tis a predetermined choice.
**The forward process.** The forward process transforms a data point x0âˆ¼Âµinto a noise distribution q(xT|x0)through a sequence
of conditional distributions q(xt|xtâˆ’1)for1â‰¤tâ‰¤T. It is assumed that the forward process is defined such that for sufficiently
large T, the distribution q(xT|x0)is close to a simple noise distribution p(xT), which is referred to as the prior distribution. For
instance, p(xT) =N(xT; 0, I), the standard multivariate normal distribution, has been chosen in previous work.
**The backward process.** The backward process is a Markov process with parametric transition kernels. The objective of the
backward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samples
from the distribution Âµ. Following previous work, it is assumed that the backward process is defined by Gaussian distributions
pÎ¸(xtâˆ’1|xt)for2â‰¤tâ‰¤Tas
pÎ¸(xtâˆ’1|xt) =N(xtâˆ’1;gÎ¸
t(xt), Ïƒ2
tI),
and
pÎ¸(x0|x1) =gÎ¸
1(x1),
where the variance parameters Ïƒ2
tâˆˆRâ‰¥0are defined by a fixed schedule, the mean functions gÎ¸
t:RDâ†’RDare learned using a
neural network (with parameters Î¸) for2â‰¤tâ‰¤T, andgÎ¸
1:RDâ†’Xis a separate function dependent on Ïƒ1. In practice, the same
network has been used for the functions gÎ¸
tfor2â‰¤tâ‰¤T, and a separate discrete decoder for gÎ¸
1.
2
Generating new samples from a trained diffusion model is accomplished by sampling xtâˆ’1âˆ¼pÎ¸(xtâˆ’1|xt)for1â‰¤tâ‰¤T, starting
from a noise vector xTâˆ¼p(xT)sampled from the prior p(xT).
The following assumption is made regarding the backward process.
**Assumption 1.** It is assumed that for each 1â‰¤tâ‰¤T, there exists a constant KÎ¸
t>0such that for every x1, x2âˆˆX,
||gÎ¸
t(x1)âˆ’gÎ¸
t(x2)|| â‰¤KÎ¸
t||x1âˆ’x2||.
In other words, gÎ¸
tisKÎ¸
t-Lipschitz continuous. This assumption is discussed in Remark 3.2.
2.2 Additional Definitions
The distribution Ï€Î¸(Â·|x0)is defined as
Ï€Î¸(Â·|x0) =q(xT|x0)pÎ¸(xTâˆ’1|xT)pÎ¸(xTâˆ’2|xTâˆ’1). . . p Î¸(x1|x2)pÎ¸(Â·|x1).
Intuitively, for each x0âˆˆX,Ï€Î¸(Â·|x0)represents the distribution on Xobtained by reconstructing samples from q(xT|x0)through
the backward process. Another way to interpret this distribution is that for any function f:Xâ†’R, the following equation holds:
EÏ€Î¸(Ë†x0|x0)[f(Ë†x0)] =Eq(xT|x0)EpÎ¸(xTâˆ’1|xT). . . E pÎ¸(x1|x2)EpÎ¸(Ë†x0|x1)[f(Ë†x0)].
Given a finite set S={x1
0, . . . , xn
0}i.i.d.âˆ¼Âµ, the regenerated distribution is defined as the following mixture:
ÂµÎ¸
n=1
nnX
i=1Ï€Î¸(Â·|xi
0).
This definition is analogous to the empirical regenerated distribution defined for V AEs. The distribution on Xlearned by the
diffusion model is denoted as Ï€Î¸(Â·)and defined as
Ï€Î¸(Â·) =p(xT)pÎ¸(xTâˆ’1|xT)pÎ¸(xTâˆ’2|xTâˆ’1). . . p Î¸(x1|x2)pÎ¸(Â·|x1).
In other words, for any function f:Xâ†’R, the expectation of fwith respect to Ï€Î¸(Â·)is
EÏ€Î¸(Ë†x0)[f(Ë†x0)] =Ep(xT)EpÎ¸(xTâˆ’1|xT). . . E pÎ¸(x1|x2)EpÎ¸(Ë†x0|x1)[f(Ë†x0)].
Hence, both Ï€Î¸(Â·)andÏ€Î¸(Â·|x0)are defined using the backward process, with the difference that Ï€Î¸(Â·)starts with the prior
p(xT) =N(xT; 0, I), while Ï€Î¸(Â·|x0)starts with the noise distribution q(xT|x0).
Finally, the loss function lÎ¸:XÃ—Xâ†’Ris defined as
lÎ¸(xT, x0) =EpÎ¸(xTâˆ’1|xT)EpÎ¸(xTâˆ’2|xTâˆ’1). . . E pÎ¸(x1|x2)EpÎ¸(Ë†x0|x1)[||x0âˆ’Ë†x0||].
Hence, given a noise vector xTand a sample x0, the loss lÎ¸(xT, x0)represents the average Euclidean distance between x0and any
sample obtained by passing xTthrough the backward process.
2.3 Our Approach
The goal is to upper-bound the distance W1(Âµ, Ï€Î¸(Â·)). Since the triangle inequality implies
W1(Âµ, Ï€Î¸(Â·))â‰¤W1(Âµ, ÂµÎ¸
n) +W1(ÂµÎ¸
n, Ï€Î¸(Â·)),
the distance W1(Âµ, Ï€Î¸(Â·))can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The
upper bound on W1(Âµ, ÂµÎ¸
n)is obtained using a straightforward adaptation of a proof. First, W1(Âµ, ÂµÎ¸
n)is upper-bounded using the
expectation of the loss function lÎ¸, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependent
on the empirical risk and the prior-matching term.
The upper bound on the second term W1(ÂµÎ¸
n, Ï€Î¸(Â·))uses the definition of ÂµÎ¸
n. Intuitively, the difference between Ï€Î¸(Â·|xi
0)andÏ€Î¸(Â·)
is determined by the corresponding initial distributions: q(xT|xi
0)andp(xT)forÏ€Î¸(Â·). Hence, if the two initial distributions are
close, and if the steps of the backward process are smooth (see Assumption 1), then Ï€Î¸(Â·|xi
0)andÏ€Î¸(Â·)are close to each other.
3
3 Main Result
3.1 Theorem Statement
We are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generating
distribution Âµand the learned distribution Ï€Î¸(Â·).
**Theorem 3.1.** Assume the instance space Xhas finite diameter âˆ† = supx,xâ€²âˆˆX||xâˆ’xâ€²||<âˆ, and let Î» >0andÎ´âˆˆ(0,1)be
real numbers. Using the definitions and assumptions of the previous section, the following inequality holds with probability at least
1âˆ’Î´over the random draw of S={x1
0, . . . , xn
0}i.i.d.âˆ¼Âµ:
W1(Âµ, Ï€Î¸(Â·))â‰¤1
nnX
i=1Eq(xT|xi
0)[lÎ¸(xT, xi
0)] +1
Î»nnX
i=1KL(q(xT|xi
0)||p(xT)) +1
Î»nlogn
Î´+Î»âˆ†2
8n
+ TY
t=1KÎ¸
t!
Eq(xT|xi
0)Ep(yT)[||xTâˆ’yT||]
+TX
t=2 tâˆ’1Y
i=1KÎ¸
i!
ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I)are standard Gaussian vectors.
**Remark 3.1.** Before presenting the proof, let us discuss Theorem 3.1.
* Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. sample S, the bound holds with
high probability with respect to the randomness of S. This is the price we pay for having a quantitative upper bound with no
exponential dependencies on problem parameters and no assumptions on the data-generating distribution Âµ. * The first term of the
right-hand side is the average reconstruction loss computed over the sample S={x1
0, . . . , xn
0}. Note that for each 1â‰¤iâ‰¤n, the
expectation of lÎ¸(xT|xi
0)is only computed with respect to the noise distribution q(xT|xi
0)defined by xi
0itself. Hence, this term
measures how well a noise vector xTâˆ¼q(xT|xi
0)recovers the original sample xi
0using the backward process, and averages over
the set S={x1
0, . . . , xn
0}. * If the Lipschitz constants satisfy KÎ¸
t<1for all 1â‰¤tâ‰¤T, then the larger Tis, the smaller the upper
bound gets. This is because the product of KÎ¸
tâ€™s then converges to 0. In Remark 3.2 below, we show that the assumption that KÎ¸
t<1
for all tis a quite reasonable one. * The hyperparameter Î»controls the trade-off between the prior-matching (KL) term and the
diameter term âˆ†2. IfKÎ¸
t<1for all 1â‰¤tâ‰¤TandTâ†’ âˆ , then the convergence of the bound largely depends on the choice of Î».
In that case, Î»âˆn1/2leads to faster convergence, while Î»âˆnleads to slower convergence to a smaller quantity. This is because
the bound stems from PAC-Bayesian theory, where this trade-off is common. * The last term of the equation does not depend on the
sample size n. Hence, the upper bound given by Theorem 3.1 does not converge to 0 as nâ†’ âˆ . However, if the Lipschitz factors
(KÎ¸
t)1â‰¤tâ‰¤Tare all less than 1, then this term can be very small, especially in low-dimensional spaces.
3.2 Proof of the main theorem
The following result is an adaptation of a previous result.
**Lemma 3.2.** Let Î» >0andÎ´âˆˆ(0,1)be real numbers. With probability at least 1âˆ’Î´over the randomness of the sample
S={x1
0, . . . , xn
0}i.i.d.âˆ¼Âµ, the following holds:
W1(Âµ, ÂµÎ¸
n)â‰¤1
nnX
i=1Eq(xT|xi
0)[lÎ¸(xT, xi
0)] +1
Î»nnX
i=1KL(q(xT|xi
0)||p(xT)) +1
Î»nlogn
Î´+Î»âˆ†2
8n.
The proof of this result is a straightforward adaptation of a previous proof.
Now, let us focus our attention on the second term of the right-hand side of the equation, namely W1(ÂµÎ¸
n, Ï€Î¸(Â·)). This part is trickier
than for V AEs, for which the generative modelâ€™s distribution is simply a pushforward measure. Here, we have a non-deterministic
sampling process with Tsteps.
Assumption 1 leads to the following lemma on the backward process.
**Lemma 3.3.** For any given x1, y1âˆˆX, we have
EpÎ¸(x0|x1)EpÎ¸(y0|y1)[||x0âˆ’y0||]â‰¤KÎ¸
1||x1âˆ’y1||.
Moreover, if 2â‰¤tâ‰¤T, then for any given xt, ytâˆˆX, we have
4
EpÎ¸(xtâˆ’1|xt)EpÎ¸(ytâˆ’1|yt)[||xtâˆ’1âˆ’ytâˆ’1||]â‰¤KÎ¸
t||xtâˆ’yt||+ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I), meaning EÏµ,Ïµâ€²is a shorthand for EÏµ,Ïµâ€²âˆ¼N(0,I).
**Proof.** For the first part, let x1, y1âˆˆX. Since according to the equation pÎ¸(x0|x1) =Î´gÎ¸
1(x1)(x0)andpÎ¸(y0|y1) =Î´gÎ¸
1(y1)(y0),
then
EpÎ¸(x0|x1)EpÎ¸(y0|y1)[||x0âˆ’y0||] =||gÎ¸
1(x1)âˆ’gÎ¸
1(y1)|| â‰¤KÎ¸
1||x1âˆ’y1||.
For the second part, let 2â‰¤tâ‰¤Tandxt, ytâˆˆX. Since pÎ¸(xtâˆ’1|xt) =N(xtâˆ’1;gÎ¸
t(xt), Ïƒ2
tI), the reparameterization trick implies
that sampling xtâˆ’1âˆ¼pÎ¸(xtâˆ’1|xt)is equivalent to setting
xtâˆ’1=gÎ¸
t(xt) +ÏƒtÏµt,withÏµtâˆ¼N(0, I).
Using the above equation, the triangle inequality, and Assumption 1, we obtain
EpÎ¸(xtâˆ’1|xt)EpÎ¸(ytâˆ’1|yt)[||xtâˆ’1âˆ’ytâˆ’1||]
=EÏµt,Ïµâ€²
tâˆ¼N(0,I)[||gÎ¸
t(xt) +ÏƒtÏµtâˆ’gÎ¸
t(yt)âˆ’ÏƒtÏµâ€²
t||]
â‰¤EÏµt,Ïµâ€²
tâˆ¼N(0,I)[||gÎ¸
t(xt)âˆ’gÎ¸
t(yt)||] +ÏƒtEÏµt,Ïµâ€²
tâˆ¼N(0,I)[||Ïµtâˆ’Ïµâ€²
t||]
â‰¤KÎ¸
t||xtâˆ’yt||+ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I).
Next, we can use the inequalities of Lemma 3.3 to prove the following result.
**Lemma 3.4.** Let Tâ‰¥1. The following inequality holds:
EpÎ¸(xTâˆ’1|xT)EpÎ¸(yTâˆ’1|yT)EpÎ¸(xTâˆ’2|xTâˆ’1)EpÎ¸(yTâˆ’2|yTâˆ’1). . . E pÎ¸(x0|x1)EpÎ¸(y0|y1)[||x0âˆ’y0||]
â‰¤ TY
t=1KÎ¸
t!
||xTâˆ’yT||+TX
t=2 tâˆ’1Y
i=1KÎ¸
i!
ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I).
**Proof Idea.** Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step.
Using the two previous lemmas, we obtain the following upper bound on W1(ÂµÎ¸
n, Ï€Î¸(Â·)).
**Lemma 3.5.** The following inequality holds:
W1(ÂµÎ¸
n, Ï€Î¸(Â·))â‰¤1
nnX
i=1 TY
t=1KÎ¸
t!
Eq(xT|xi
0)Ep(yT)[||xTâˆ’yT||] +TX
t=2 tâˆ’1Y
i=1KÎ¸
i!
ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I).
**Proof.** Using the definition of W1, the trivial coupling, the definitions of ÂµÎ¸
nandÏ€Î¸(Â·), and Lemma 3.4, we get the desired result.
Combining Lemmas 3.2 and 3.5 with the triangle inequality yields Theorem 3.1.
3.3 Special case using the forward process of Ho et al. (2020)
Theorem 3.1 establishes a general upper bound that holds for any forward process, as long as the backward process satisfies
Assumption 1. In this section, we specialize the statement of the theorem to the particular case of the forward process defined in
previous work.
LetXâŠ†RD. The forward process is a Gauss-Markov process with transition densities defined as
q(xt|xtâˆ’1) =N(xt;âˆšÎ±txtâˆ’1,(1âˆ’Î±t)I),
where Î±1, . . . , Î± Tis a fixed noise schedule such that 0< Î±t<1for all t. This definition implies that at each time step 1â‰¤tâ‰¤T,
5
q(xt|x0) =N(xt;âˆšÂ¯Î±tx0,(1âˆ’Â¯Î±t)I),withÂ¯Î±t=tY
i=1Î±i.
The optimization objective to train the backward process ensures that for each time step t, the distribution pÎ¸(xtâˆ’1|xt)remains close
to the ground-truth distribution q(xtâˆ’1|xt, x0)given by
q(xtâˆ’1|xt, x0) =N(xtâˆ’1; ËœÂµq
t(xt, x0),ËœÏƒ2
tI),
where
ËœÂµq
t(xt, x0) =âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±txt+âˆšÂ¯Î±tâˆ’1(1âˆ’Î±t)
1âˆ’Â¯Î±tx0.
Now, we discuss Assumption 1 under these definitions.
**Remark 3.2.** We can get a glimpse at the range of KÎ¸
tfor a trained DDPM by looking at the distribution q(xtâˆ’1|xt, x0), since
pÎ¸(xtâˆ’1|xt)is optimized to be as close as possible to q(xtâˆ’1|xt, x0).
For a given x0âˆ¼Âµ, let us take a look at the Lipschitz norm of x7â†’ËœÂµq
t(x, x0). Using the above equation, we have
ËœÂµq
t(xt, x0)âˆ’ËœÂµq
t(yt, x0) =âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±t(xtâˆ’yt).
Hence, x7â†’ËœÂµq
t(x, x0)isKâ€²
t-Lipschitz continuous with
Kâ€²
t=âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±t.
Now, if Î±t<1for all 1â‰¤tâ‰¤T, then we have 1âˆ’Â¯Î±t>1âˆ’Â¯Î±tâˆ’1, which implies Kâ€²
t<1for all 1â‰¤tâ‰¤T.
Remark 3.2 shows that the Lipschitz norm of the mean function ËœÂµq
t(Â·, x0)does not depend on x0. Indeed, looking at the previous
equation, we can see that for any initial x0, the Lipschitz norm Kâ€²
t=âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±tonly depends on the noise schedule, not x0itself.
Since gÎ¸
t(Â·, x0)is optimized to match ËœÂµq
t(Â·, x0)for each x0in the training set, and all the functions ËœÂµq
t(Â·, x0)have the same Lipschitz
norm Kâ€²
t, we believe it is reasonable to assume gÎ¸
tis Lipschitz continuous as well. This is the intuition behind Assumption 1.
**The prior-matching term.** With the definitions of this section, the prior matching term KL(q(xT|x0)||p(xT))has the following
closed form:
KL(q(xT|x0)||p(xT)) =1
2
âˆ’Dlog(1âˆ’Â¯Î±T)âˆ’DÂ¯Î±T+ Â¯Î±T||x0||2
.
**Upper-bounds on the average distance between Gaussian vectors.** If Ïµ, Ïµâ€²are D-dimensional vectors sampled from N(0, I), then
EÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||]â‰¤âˆš
2D.
Moreover, since q(xT|x0) =N(xT;âˆšÂ¯Î±Tx0,(1âˆ’Â¯Î±T)I)and the prior p(yT) =N(yT; 0, I),
Eq(xT|x0)Ep(yT)[||xTâˆ’yT||]â‰¤p
Â¯Î±T||x0||2+ (2âˆ’Â¯Î±T)D.
**Special case of the main theorem.** With the definitions of this section, the inequality of Theorem 3.1 implies that with probability
at least 1âˆ’Î´over the randomness of {x1
0, . . . , x
6
Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems
with Solutions Exhibiting Weak Minty Properties
Abstract
This research examines a specific category of structured nonconvex-nonconcave min-max problems that demon-
strate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has
already demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time.
We establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within
this framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared
operator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified
version of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problemâ€™s
specific parameters.
1 Introduction
The recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems,
have generated significant interest in saddle point problems. Examples of these models include generative adversarial networks,
adversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed
that generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization
component and nonconcave in the maximization component remains limited, with some research even suggesting intractability in
certain cases.
A specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG)
exhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of
the recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified
(see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing
literature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of
weak Minty solutions was quickly investigated.
Assumption 1 (Weak Minty solution). For a given operator F:Rdâ†’Rd, there is a point uâˆ—âˆˆRdand a parameter Ï >0such that:
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥ âˆ’Ï
2âˆ¥F(u)âˆ¥2âˆ€uâˆˆRd. (1)
Moreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving
a complexity of O(Ïµâˆ’1)for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step
followed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant
of EG.
In a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected-
Backward (FoRB). We address the following question with an affirmative answer:
Can OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions?
Specifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0and a parameter 0< Î³â‰¤1
as follows:
uk= Â¯ukâˆ’aF(Â¯uk),
Â¯uk+1= Â¯ukâˆ’Î³aF(uk),âˆ€kâ‰¥0,
can achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration.
It is worth noting that OGDA is most frequently expressed in a form where Î³= 1. However, two recent studies have examined
a more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of Î³becomes
apparent only when dealing with weak Minty solutions. In this context, we find that Î³must be greater than 1 to ensure convergence,
a phenomenon that is not observed in monotone problems.
When examining a general smooth min-max problem:
min
xmax
yf(x, y)
the operator Fmentioned in Assumption 1 naturally emerges as F(u) := [âˆ‡xf(x, y),âˆ’âˆ‡yf(x, y)]withu= (x, y). However,
by examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can
concurrently address more scenarios, such as certain equilibrium problems.
The parameter Ïin the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it
is essential that the step size exceeds a value proportional to Ï. Simultaneously, as is typical, the step size is limited from above
by the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than1
4L, their
convergence claim is valid only if Ï <1
4L. This condition was later improved to Ï <1
2Lfor the choice Î³= 1and to Ï <1
Lfor
even smaller values of Î³. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different
analysis, we are able to match the most general condition on the weak Minty parameter Ï <1
Lfor appropriate Î³anda.
1.1 Contribution
Our contributions are summarized as follows:
1.We establish a new convergence rate of O(1/k), measured by the squared operator norm, for a modified version of OGDA,
which we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to
the Minty variational inequality.
2.Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible
step sizes for OGDA+ and obtain the most favorable result known for the standard method ( Î³= 1).
3. We demonstrate a complexity bound of O(Ïµâˆ’2)for a stochastic variant of the OGDA+ method.
4.We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without
requiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in
areas with low curvature, enabling convergence where a fixed step size strategy might fail.
1.2 Related literature
We will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap
function or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave,
or under the Polyak-Åojasiewicz assumption.
Weak Minty. It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution,
termed "weak Minty," without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions.
Convergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as
the update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the
length of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is
also proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step
size we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps
per descent step, achieving the same O(1/k)rate as EG.
Minty solutions. Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution.
It was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing
the resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a
specific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While
the assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone
problems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point
by a factor proportional to the squared operator norm.
Negative comonotonicity. Although previously studied under the term "cohypomonotonicity," the concept of negative comono-
tonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty
solutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and
an improved convergence rate of O(1/k2)(in terms of the squared operator norm) was shown. Similarly, an accelerated version of
the reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty
solutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty
solution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient
norm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate
this class from problems with weak Minty solutions.
2
Interaction dominance. The concept of Î±-interaction dominance for nonconvex-nonconcave min-max problems was investigated,
and it was shown that the proximal-point method converges sublinearly if this condition is met in yand linearly if it is met in both
components. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively
comonotone.
Optimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the
attention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has
also been studied in the mathematical programming community.
2 Preliminaries
2.1 Notions of solution
We outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These
concepts are typically defined with respect to a constraint set CâŠ†Rd. A Stampacchia solution of the VI given by F:Rdâ†’Rdis a
point uâˆ—such that:
âŸ¨F(uâˆ—), uâˆ’uâˆ—âŸ© â‰¥0âˆ€uâˆˆC. (SVI)
In this work, we only consider the unconstrained case where C=Rd, and the above condition simplifies to F(uâˆ—) = 0 . Closely
related is the following concept: A Minty solution is a point uâˆ—âˆˆCsuch that:
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥0âˆ€uâˆˆC. (MVI)
For a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but
holds, for example, if the operator Fis monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but
without any Minty solutions.
2.2 Notions of monotonicity
This section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them.
An operator Fis considered monotone if:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥0.
Such operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium
problems.
Two frequently studied notions that fall into this category are strongly monotone operators, which satisfy:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥Âµâˆ¥uâˆ’vâˆ¥2,
and cocoercive operators, which fulfill:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥Î²âˆ¥F(u)âˆ’F(v)âˆ¥2. (2)
Strongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max
problems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with Î²equal
to the inverse of the gradientâ€™s Lipschitz constant.
Departing from monotonicity. Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring
the non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and
spurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and
foremost is the extensively studied setting of Î½-weak monotonicity:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥ âˆ’ Î½âˆ¥uâˆ’vâˆ¥2.
Such operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it
includes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this
property. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity,
has received much less attention and is given by:
âŸ¨F(u)âˆ’F(v), uâˆ’vâŸ© â‰¥ âˆ’ Î³âˆ¥F(u)âˆ’F(v)âˆ¥2.
Clearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1.
Behavior with respect to the solution. While the above properties are standard assumptions in the literature, it is usually sufficient
to require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of
monotonicity, it is enough to ask for the operator Fto be star-monotone, i.e.,
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥0,
or star-cocoercive,
âŸ¨F(u), uâˆ’uâˆ—âŸ© â‰¥Î³âˆ¥F(u)âˆ¥2.
In this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the
operator Fto be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the
above star notions are sometimes required to hold for all solutions uâˆ—, in the following we only require it to hold for a single solution.
3
3 OGDA for problems with weak Minty solutions
The generalized version of OGDA, which we denote with a "+" to emphasize the presence of the additional parameter Î³, is given by:
Algorithm 1 OGDA+
Require: Starting point u0=uâˆ’1âˆˆRd, step size a >0and parameter 0< Î³ < 1.
fork= 0,1, ...do
uk+1=ukâˆ’a((1 + Î³)F(uk)âˆ’F(ukâˆ’1))
end for
Theorem 3.1. LetF:Rdâ†’RdbeL-Lipschitz continuous satisfying Assumption 1 with1
L> Ï, and let (uk)kâ‰¥0be the iterates
generated by Algorithm 1 with step size asatisfying a > Ï and
aLâ‰¤1âˆ’Î³
1 +Î³. (3)
Then, for all kâ‰¥0,
min
i=0,...,kâˆ’1âˆ¥F(ui)âˆ¥2â‰¤1
kaÎ³(aâˆ’Ï)âˆ¥u0+aF(u0)âˆ’uâˆ—âˆ¥2.
In particular, as long as Ï <1
L, we can find a Î³small enough such that the above bound holds.
The first observation is that we would like to choose aas large as possible, as this allows us to treat the largest class of problems
withÏ < a . To be able to choose a large step size a, we must decrease Î³, as evident from (3). However, this degrades the algorithmâ€™s
speed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could
derive an optimal Î³(i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on
Ï. In practice, the strategy of decreasing Î³until convergence is achieved, but not further, yields reasonable results.
Furthermore, we want to point out that the condition Ï <1
Lis precisely the best possible bound for EG+.
3.1 Improved bounds under monotonicity
While the above theorem also holds if the operator Fis monotone, we can modify the proof slightly to obtain a better dependence on
the parameters:
Theorem 3.2. LetF:Rdâ†’Rdbe monotone and L-Lipschitz. If aL=2âˆ’Î³
2+Î³âˆ’ÏµforÏµ >0, then the iterates generated by OGDA+
fulfill
min
i=0,...,kâˆ’1âˆ¥F(ui)âˆ¥2â‰¤2
ka2Î³2Ïµâˆ¥u0+aF(u0)âˆ’uâˆ—âˆ¥2.
In particular, we can choose Î³= 1anda <1
2L.
There are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a <1
2L. However, we
want to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as
ours for OGDA is shown, but requires the conservative step size bound aâ‰¤1
16L. This was later improved to aâ‰¤1
3L. All of these
only deal with the case Î³= 1. The only other reference that deals with a generalized (i.e., not necessarily Î³= 1) version of OGDA
is another work, where the resulting step size condition is aâ‰¤2âˆ’Î³
4L, which is strictly worse than ours for any Î³. To summarize, not
only do we show for the first time that the step size of a generalization of OGDA can go above1
2L, but we also provide the least
restrictive bound for any value of Î³.
3.2 OGDA+ stochastic
In this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent
estimators F(Â·, Î¾i)at every iteration. We assume here that the estimator Fis unbiased, i.e., E[F(uk, Î¾)|ukâˆ’1] =F(uk), and has
bounded variance E[âˆ¥F(uk, Î¾)âˆ’F(uk)âˆ¥2]â‰¤Ïƒ2. We show that we can still guarantee convergence by using batch sizes Bof order
O(Ïµâˆ’1).
Algorithm 2 stochastic OGDA+
Require: Starting point u0=uâˆ’1âˆˆRd, step size a >0, parameter 0< Î³â‰¤1and batch size B.
fork= 0,1, ...do
Sample i.i.d. (Î¾i)B
i=1and compute estimator Ëœgk=1
BPB
i=1F(uk, Î¾k
i)
uk+1=ukâˆ’a((1 + Î³)Ëœgkâˆ’Ëœgkâˆ’1)
end for
4
Theorem 3.3. LetF:Rdâ†’RdbeL-Lipschitz satisfying Assumption 1 with1
L> Ï, and let (uk)kâ‰¥0be the sequence of
iterates generated by stochastic OGDA+, with aandÎ³satisfying Ï < a <1âˆ’Î³
1+Î³1
L. Then, to visit an Ïµ-stationary point such that
mini=0,...,kâˆ’1E[âˆ¥F(ui)âˆ¥2]< Ïµ, we require
1
kaÎ³(aâˆ’Ï)âˆ¥u0+aËœg0âˆ’uâˆ—âˆ¥2max
1,4Ïƒ2
aLÏµ
calls to the stochastic oracle ËœF, with large batch sizes of order O(Ïµâˆ’1).
In practice, large batch sizes of order O(Ïµâˆ’1)are typically not desirable; instead, a small or decreasing step size is preferred. In the
weak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately,
the current analysis does not allow for variable Î³.
4 EG+ with adaptive step sizes
In this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the
Lipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to
small step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in
choosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the
step size is chosen larger than a multiple of the weak Minty parameter Ïto guarantee convergence at all. For these reasons, we want
to outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried
out.
Since the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive
step size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods,
especially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially
interesting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in
multiple gradient computations per iteration.
We use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone
decreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox
method, which corresponds to EG in the setting of (non-Euclidean) Bregman distances.
A version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the
optimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and
are in terms of the gap function.
One of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which
results in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our
knowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch
is the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of Fat
all. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative
convergence result is still open.
Algorithm 3 EG+ with adaptive step size
Require: Starting points u0,Â¯u0âˆˆRd, initial step size a0and parameters Ï„âˆˆ(0,1)and0< Î³â‰¤1.
fork= 0,1, ...do
Find the step size:
ak= min
akâˆ’1,Ï„âˆ¥Â¯ukâˆ’Â¯ukâˆ’1âˆ¥
âˆ¥F(Â¯uk)âˆ’F(Â¯ukâˆ’1)âˆ¥
(4)
Compute next iterate:
uk= Â¯ukâˆ’akF(Â¯uk)
Â¯uk+1= Â¯ukâˆ’akÎ³F(uk).
end for
Clearly, akis monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that
akâ‰¥min{a0, Ï„/L}>0. The sequence therefore converges to a positive number, which we denote by aâˆ:= lim kak.
Theorem 4.1. LetF:Rdâ†’RdbeL-Lipschitz that satisfies Assumption 1, where uâˆ—denotes any weak Minty solution, with
aâˆ>2Ï, and let (uk)kâ‰¥0be the iterates generated by Algorithm 3 with Î³=1
2andÏ„âˆˆ(0,1). Then, there exists a k0âˆˆNsuch that
min
i=k0,...,kâˆ¥F(uk)âˆ¥2â‰¤1
kâˆ’k0L
Ï„(aâˆ/2âˆ’Ï)âˆ¥Â¯uk0âˆ’uâˆ—âˆ¥2.
5
Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the
Lipschitz constant of the operator Fdoes not need to be known. Moreover, the step size choice presented in (4) might allow us
to take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later
iterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage
that they allow us to solve a richer class of problems, as we are able to relax the condition Ï <1
4Lin the case of EG+ to Ï < a âˆ/2,
where aâˆ= lim kakâ‰¥Ï„/L.
On the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when
ak/ak+1â‰¤1
Ï„is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in
the starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient
ak/ak+1being too large. This drawback could be mitigated by choosing Ï„smaller. However, this will result in poor performance
due to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be
circumvented, and authors instead focused on the convergence of the iterates without any rate.
5 Numerical experiments
In the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see
Algorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification
of EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with
an initial guess made by second-order information, whose extra cost we ignore in the experiments.
5.1 Von Neumannâ€™s ratio game
We consider von Neumannâ€™s ratio game, which is given by:
min
xâˆˆâˆ†mmax
yâˆˆâˆ†nV(x, y) =âŸ¨x, RyâŸ©
âŸ¨x, SyâŸ©, (5)
where RâˆˆRmÃ—nandSâˆˆRmÃ—nwithâŸ¨x, SyâŸ©>0for all xâˆˆâˆ†m, yâˆˆâˆ†n, with âˆ† :={zâˆˆRd:zi>0,Pd
i=1zi= 1}denoting
the unit simplex. Expression (5) can be interpreted as the value V(x, y)for a stochastic game with a single state and mixed strategies.
We see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although
an estimated Ïis more than ten times larger than the estimated Lipschitz constant.
5.2 Forsaken
A particularly difficult min-max toy example with a "Forsaken" solution was proposed and is given by:
min
xâˆˆRmax
yâˆˆRx(yâˆ’0.45) + Ï•(x)âˆ’Ï•(y), (6)
where Ï•(z) =1
6z6âˆ’2
4z4+1
4z2âˆ’1
2z. This problem exhibits a Stampacchia solution at (xâˆ—, yâˆ—)â‰ˆ(0.08,0.4), but also two limit
cycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to
the solution repels possible trajectories of iterates, thus "shielding" the solution. Later, it was noticed that, restricted to the box
âˆ¥(x, y)âˆ¥âˆ<3, the above-mentioned solution is weak Minty with Ïâ‰¥2Â·0.477761 , which is much larger than1
2Lâ‰ˆ0.08. In line
with these observations, we can see that none of the fixed step size methods with a step size bounded by1
Lconverge. In light of this
observation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz
constant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling
limit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in
the backtracking procedure.
5.3 Lower bound example
The following min-max problem was introduced as a lower bound on the dependence between ÏandLfor EG+:
min
xâˆˆRmax
yâˆˆRÂµxy+Î¶
2(x2âˆ’y2). (7)
In particular, it was stated that EG+ (with any Î³) and constant step size a=1
Lconverges for this problem if and only if (0,0)is a
weak Minty solution with Ï <1âˆ’Î³
L, where ÏandLcan be computed explicitly in the above example and are given by:
L=p
Âµ2+Î¶2and Ï=Âµ2âˆ’Î¶2
2Âµ.
By choosing Âµ= 3andÎ¶=âˆ’1, we get exactly Ï=1
L, therefore predicting divergence of EG+ for any Î³, which is exactly what is
empirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case Ï <1
L, we
observe rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios.
6
6 Conclusion
Many intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave
framework. Very recently, it was demonstrated that the O(1/k)bounds on the squared operator norm for EG and OGDA for the
last iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the
presence of merely weak Minty solutions remains an open question.
In general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the
majority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem
(7), which is not covered by theory, and OGDA+ is the only method capable of converging.
Finally, we note that the previous paradigm in pure minimization of "smaller step size ensures convergence" but "larger step size
gets there faster," where the latter is typically constrained by the reciprocal of the gradientâ€™s Lipschitz constant, does not appear
to hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates
that convergence can be lost if the step size is excessively small and sometimes needs to be larger than1
L, which one can typically
only hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a
backtracking linesearch.article graphicx
7
Examining the Convergence of Denoising Diffusion Probabilistic
Models: A Quantitative Analysis
Abstract
Deep generative models, particularly diffusion models, are a significant family within deep learning. This study
provides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion model
and the target distribution. In contrast to earlier research, this analysis does not rely on presumptions regarding
the learned score function. Furthermore, the findings are applicable to any data-generating distributions within
restricted instance spaces, even those lacking a density relative to the Lebesgue measure, and the upper limit is not
exponentially dependent on the ambient space dimension. The primary finding expands upon recent research by
Mbacke et al. (2023), and the proofs presented are fundamental.
1 Introduction
Diffusion models, alongside generative adversarial networks and variational autoencoders (V AEs), are among the most influential
families of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,
as well as in various other applications.
Two primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generative
models (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, while
simultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Conversely, SGMs
employ score-matching methods to approximate the score function of the data-generating distribution, subsequently generating new
samples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function, adding varying
noise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the score
function for all noise levels has been proposed.
Although DDPMs and SGMs may initially seem distinct, it has been demonstrated that DDPMs implicitly approximate the score
function, with the sampling process resembling Langevin dynamics. Moreover, a unified perspective of both methods using stochastic
differential equations (SDEs) has been derived. The SGM can be viewed as a discretization of Brownian motion, and the DDPM as a
discretization of an Ornstein-Uhlenbeck process. Consequently, both DDPMs and SGMs are commonly referred to as SGMs in the
literature. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-based
framework, necessitating assumptions about the effectiveness of the learned score function.
In this research, a different strategy is employed, applying methods created for V AEs to DDPMs, which can be viewed as hierarchical
V AEs with fixed encoders. This method enables the derivation of quantitative, Wasserstein-based upper bounds without making
assumptions about the data distribution or the learned score function, and with simple proofs that do not need the SDE toolkit.
Furthermore, the bounds presented here do not involve any complex discretization steps, as the forward and backward processes are
considered discrete-time from the beginning, rather than being viewed as discretizations of continuous-time processes.
1.1 Related Works
There has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. However,
these studies frequently depend on restrictive assumptions regarding the data-generating distribution, produce non-quantitative upper
bounds, or exhibit exponential dependencies on certain parameters. This work successfully circumvents all three of these limitations.
Some bounds are based on very restrictive assumptions about the data-generating distribution, such as log-Sobolev inequalities,
which are unrealistic for real-world data distributions. Furthermore, some studies establish upper bounds on the Kullback-Leibler
(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by the
diffusion model; however, unless strong assumptions are made about the support of the data-generating distribution, KL and TV
reach their maximum values. Such assumptions arguably do not hold for real-world data-generating distributions, which are widely
believed to satisfy the manifold hypothesis. Other work establishes conditions under which the support of the input distribution
is equal to the support of the learned distribution, and generalizes the bound to all f-divergences. Assuming L2 accurate score
estimation, some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, but
their Wasserstein-based bounds are not quantitative. Quantitative Wasserstein distance upper bounds under the manifold hypothesis
have been derived, but these bounds exhibit exponential dependencies on some of the problem parameters.
1.2 Our contributions
In this study, strong assumptions about the data-generating distribution are avoided, and a quantitative upper bound on the Wasserstein
distance is established without exponential dependencies on problem parameters, including the ambient space dimension. Moreover,
a common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According to
some, providing precise guarantees for the estimation of the score function is challenging, as it necessitates an understanding of the
non-convex training dynamics of neural network optimization, which is currently beyond reach. Therefore, upper bounds are derived
without making assumptions about the learned score function. Instead, the bound presented here is dependent on a reconstruction
loss calculated over a finite independent and identically distributed (i.i.d.) sample. Intuitively, a loss function is defined, which
quantifies the average Euclidean distance between a sample from the data-generating distribution and the reconstruction obtained by
sampling noise and passing it through the backward process (parameterized by Ë˜03b8). This method is inspired by previous work on
V AEs.
This approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution, avoids
exponential dependencies on the dimension, and provides a quantitative upper bound based on the Wasserstein distance. Furthermore,
this method benefits from utilizing very straightforward and basic proofs.
2 Preliminaries
Throughout this paper, lowercase letters are used to represent both probability measures and their densities with respect to the
Lebesgue measure, and variables are added in parentheses to enhance readability (e.g., q(xt|xtâˆ’1)to denote a time-dependent
conditional distribution). An instance space X, which is a subset of RDwith the Euclidean distance as the underlying metric, and
a target data-generating distribution ÂµâˆˆM+
1(X)are considered. Note that it is not assumed that Âµhas a density with respect to
the Lebesgue measure. Additionally, || Â· || represents the Euclidean (L2) norm, and Ep(x)is used as shorthand for Exâˆ¼p(x). Given
probability measures p, qâˆˆM+
1(X)and a real number k >1, the Wasserstein distance of order kis defined as (Villani, 2009):
Wk(p, q) = inf
Î³âˆˆÎ“(p,q)Z
XÃ—X||xâˆ’y||kdÎ³(x, y)1/k
,
where Î“(p, q)denotes the set of couplings of pandq, meaning the set of joint distributions on XÃ—Xwith respective marginals p
andq. The product measure pâŠ—qis referred to as the trivial coupling, and the Wasserstein distance of order 1 is simply referred to
as the Wasserstein distance.
2.1 Denoising Diffusion Models
Instead of employing the SDE framework, diffusion models are presented using the DDPM formulation with discrete-time processes.
A diffusion model consists of two discrete-time stochastic processes: a forward process and a backward process. Both processes are
indexed by time 0â‰¤tâ‰¤T, where the number of time steps Tis a predetermined choice.
**The forward process.** The forward process transforms a data point x0âˆ¼Âµinto a noise distribution q(xT|x0)through a sequence
of conditional distributions q(xt|xtâˆ’1)for1â‰¤tâ‰¤T. It is assumed that the forward process is defined such that for sufficiently
large T, the distribution q(xT|x0)is close to a simple noise distribution p(xT), which is referred to as the prior distribution. For
instance, p(xT) =N(xT; 0, I), the standard multivariate normal distribution, has been chosen in previous work.
**The backward process.** The backward process is a Markov process with parametric transition kernels. The objective of the
backward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samples
from the distribution Âµ. Following previous work, it is assumed that the backward process is defined by Gaussian distributions
pÎ¸(xtâˆ’1|xt)for2â‰¤tâ‰¤Tas
pÎ¸(xtâˆ’1|xt) =N(xtâˆ’1;gÎ¸
t(xt), Ïƒ2
tI),
and
pÎ¸(x0|x1) =gÎ¸
1(x1),
where the variance parameters Ïƒ2
tâˆˆRâ‰¥0are defined by a fixed schedule, the mean functions gÎ¸
t:RDâ†’RDare learned using a
neural network (with parameters Î¸) for2â‰¤tâ‰¤T, andgÎ¸
1:RDâ†’Xis a separate function dependent on Ïƒ1. In practice, the same
network has been used for the functions gÎ¸
tfor2â‰¤tâ‰¤T, and a separate discrete decoder for gÎ¸
1.
2
Generating new samples from a trained diffusion model is accomplished by sampling xtâˆ’1âˆ¼pÎ¸(xtâˆ’1|xt)for1â‰¤tâ‰¤T, starting
from a noise vector xTâˆ¼p(xT)sampled from the prior p(xT).
The following assumption is made regarding the backward process.
**Assumption 1.** It is assumed that for each 1â‰¤tâ‰¤T, there exists a constant KÎ¸
t>0such that for every x1, x2âˆˆX,
||gÎ¸
t(x1)âˆ’gÎ¸
t(x2)|| â‰¤KÎ¸
t||x1âˆ’x2||.
In other words, gÎ¸
tisKÎ¸
t-Lipschitz continuous. This assumption is discussed in Remark 3.2.
2.2 Additional Definitions
The distribution Ï€Î¸(Â·|x0)is defined as
Ï€Î¸(Â·|x0) =q(xT|x0)pÎ¸(xTâˆ’1|xT)pÎ¸(xTâˆ’2|xTâˆ’1). . . p Î¸(x1|x2)pÎ¸(Â·|x1).
Intuitively, for each x0âˆˆX,Ï€Î¸(Â·|x0)represents the distribution on Xobtained by reconstructing samples from q(xT|x0)through
the backward process. Another way to interpret this distribution is that for any function f:Xâ†’R, the following equation holds:
EÏ€Î¸(Ë†x0|x0)[f(Ë†x0)] =Eq(xT|x0)EpÎ¸(xTâˆ’1|xT). . . E pÎ¸(x1|x2)EpÎ¸(Ë†x0|x1)[f(Ë†x0)].
Given a finite set S={x1
0, . . . , xn
0}i.i.d.âˆ¼Âµ, the regenerated distribution is defined as the following mixture:
ÂµÎ¸
n=1
nnX
i=1Ï€Î¸(Â·|xi
0).
This definition is analogous to the empirical regenerated distribution defined for V AEs. The distribution on Xlearned by the
diffusion model is denoted as Ï€Î¸(Â·)and defined as
Ï€Î¸(Â·) =p(xT)pÎ¸(xTâˆ’1|xT)pÎ¸(xTâˆ’2|xTâˆ’1). . . p Î¸(x1|x2)pÎ¸(Â·|x1).
In other words, for any function f:Xâ†’R, the expectation of fwith respect to Ï€Î¸(Â·)is
EÏ€Î¸(Ë†x0)[f(Ë†x0)] =Ep(xT)EpÎ¸(xTâˆ’1|xT). . . E pÎ¸(x1|x2)EpÎ¸(Ë†x0|x1)[f(Ë†x0)].
Hence, both Ï€Î¸(Â·)andÏ€Î¸(Â·|x0)are defined using the backward process, with the difference that Ï€Î¸(Â·)starts with the prior
p(xT) =N(xT; 0, I), while Ï€Î¸(Â·|x0)starts with the noise distribution q(xT|x0).
Finally, the loss function lÎ¸:XÃ—Xâ†’Ris defined as
lÎ¸(xT, x0) =EpÎ¸(xTâˆ’1|xT)EpÎ¸(xTâˆ’2|xTâˆ’1). . . E pÎ¸(x1|x2)EpÎ¸(Ë†x0|x1)[||x0âˆ’Ë†x0||].
Hence, given a noise vector xTand a sample x0, the loss lÎ¸(xT, x0)represents the average Euclidean distance between x0and any
sample obtained by passing xTthrough the backward process.
2.3 Our Approach
The goal is to upper-bound the distance W1(Âµ, Ï€Î¸(Â·)). Since the triangle inequality implies
W1(Âµ, Ï€Î¸(Â·))â‰¤W1(Âµ, ÂµÎ¸
n) +W1(ÂµÎ¸
n, Ï€Î¸(Â·)),
the distance W1(Âµ, Ï€Î¸(Â·))can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. The
upper bound on W1(Âµ, ÂµÎ¸
n)is obtained using a straightforward adaptation of a proof. First, W1(Âµ, ÂµÎ¸
n)is upper-bounded using the
expectation of the loss function lÎ¸, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependent
on the empirical risk and the prior-matching term.
The upper bound on the second term W1(ÂµÎ¸
n, Ï€Î¸(Â·))uses the definition of ÂµÎ¸
n. Intuitively, the difference between Ï€Î¸(Â·|xi
0)andÏ€Î¸(Â·)
is determined by the corresponding initial distributions: q(xT|xi
0)andp(xT)forÏ€Î¸(Â·). Hence, if the two initial distributions are
close, and if the steps of the backward process are smooth (see Assumption 1), then Ï€Î¸(Â·|xi
0)andÏ€Î¸(Â·)are close to each other.
3
3 Main Result
3.1 Theorem Statement
We are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generating
distribution Âµand the learned distribution Ï€Î¸(Â·).
**Theorem 3.1.** Assume the instance space Xhas finite diameter âˆ† = supx,xâ€²âˆˆX||xâˆ’xâ€²||<âˆ, and let Î» >0andÎ´âˆˆ(0,1)be
real numbers. Using the definitions and assumptions of the previous section, the following inequality holds with probability at least
1âˆ’Î´over the random draw of S={x1
0, . . . , xn
0}i.i.d.âˆ¼Âµ:
W1(Âµ, Ï€Î¸(Â·))â‰¤1
nnX
i=1Eq(xT|xi
0)[lÎ¸(xT, xi
0)] +1
Î»nnX
i=1KL(q(xT|xi
0)||p(xT)) +1
Î»nlogn
Î´+Î»âˆ†2
8n
+ TY
t=1KÎ¸
t!
Eq(xT|xi
0)Ep(yT)[||xTâˆ’yT||]
+TX
t=2 tâˆ’1Y
i=1KÎ¸
i!
ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I)are standard Gaussian vectors.
**Remark 3.1.** Before presenting the proof, let us discuss Theorem 3.1.
* Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. sample S, the bound holds with
high probability with respect to the randomness of S. This is the price we pay for having a quantitative upper bound with no
exponential dependencies on problem parameters and no assumptions on the data-generating distribution Âµ. * The first term of the
right-hand side is the average reconstruction loss computed over the sample S={x1
0, . . . , xn
0}. Note that for each 1â‰¤iâ‰¤n, the
expectation of lÎ¸(xT|xi
0)is only computed with respect to the noise distribution q(xT|xi
0)defined by xi
0itself. Hence, this term
measures how well a noise vector xTâˆ¼q(xT|xi
0)recovers the original sample xi
0using the backward process, and averages over
the set S={x1
0, . . . , xn
0}. * If the Lipschitz constants satisfy KÎ¸
t<1for all 1â‰¤tâ‰¤T, then the larger Tis, the smaller the upper
bound gets. This is because the product of KÎ¸
tâ€™s then converges to 0. In Remark 3.2 below, we show that the assumption that KÎ¸
t<1
for all tis a quite reasonable one. * The hyperparameter Î»controls the trade-off between the prior-matching (KL) term and the
diameter term âˆ†2. IfKÎ¸
t<1for all 1â‰¤tâ‰¤TandTâ†’ âˆ , then the convergence of the bound largely depends on the choice of Î».
In that case, Î»âˆn1/2leads to faster convergence, while Î»âˆnleads to slower convergence to a smaller quantity. This is because
the bound stems from PAC-Bayesian theory, where this trade-off is common. * The last term of the equation does not depend on the
sample size n. Hence, the upper bound given by Theorem 3.1 does not converge to 0 as nâ†’ âˆ . However, if the Lipschitz factors
(KÎ¸
t)1â‰¤tâ‰¤Tare all less than 1, then this term can be very small, especially in low-dimensional spaces.
3.2 Proof of the main theorem
The following result is an adaptation of a previous result.
**Lemma 3.2.** Let Î» >0andÎ´âˆˆ(0,1)be real numbers. With probability at least 1âˆ’Î´over the randomness of the sample
S={x1
0, . . . , xn
0}i.i.d.âˆ¼Âµ, the following holds:
W1(Âµ, ÂµÎ¸
n)â‰¤1
nnX
i=1Eq(xT|xi
0)[lÎ¸(xT, xi
0)] +1
Î»nnX
i=1KL(q(xT|xi
0)||p(xT)) +1
Î»nlogn
Î´+Î»âˆ†2
8n.
The proof of this result is a straightforward adaptation of a previous proof.
Now, let us focus our attention on the second term of the right-hand side of the equation, namely W1(ÂµÎ¸
n, Ï€Î¸(Â·)). This part is trickier
than for V AEs, for which the generative modelâ€™s distribution is simply a pushforward measure. Here, we have a non-deterministic
sampling process with Tsteps.
Assumption 1 leads to the following lemma on the backward process.
**Lemma 3.3.** For any given x1, y1âˆˆX, we have
EpÎ¸(x0|x1)EpÎ¸(y0|y1)[||x0âˆ’y0||]â‰¤KÎ¸
1||x1âˆ’y1||.
Moreover, if 2â‰¤tâ‰¤T, then for any given xt, ytâˆˆX, we have
4
EpÎ¸(xtâˆ’1|xt)EpÎ¸(ytâˆ’1|yt)[||xtâˆ’1âˆ’ytâˆ’1||]â‰¤KÎ¸
t||xtâˆ’yt||+ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I), meaning EÏµ,Ïµâ€²is a shorthand for EÏµ,Ïµâ€²âˆ¼N(0,I).
**Proof.** For the first part, let x1, y1âˆˆX. Since according to the equation pÎ¸(x0|x1) =Î´gÎ¸
1(x1)(x0)andpÎ¸(y0|y1) =Î´gÎ¸
1(y1)(y0),
then
EpÎ¸(x0|x1)EpÎ¸(y0|y1)[||x0âˆ’y0||] =||gÎ¸
1(x1)âˆ’gÎ¸
1(y1)|| â‰¤KÎ¸
1||x1âˆ’y1||.
For the second part, let 2â‰¤tâ‰¤Tandxt, ytâˆˆX. Since pÎ¸(xtâˆ’1|xt) =N(xtâˆ’1;gÎ¸
t(xt), Ïƒ2
tI), the reparameterization trick implies
that sampling xtâˆ’1âˆ¼pÎ¸(xtâˆ’1|xt)is equivalent to setting
xtâˆ’1=gÎ¸
t(xt) +ÏƒtÏµt,withÏµtâˆ¼N(0, I).
Using the above equation, the triangle inequality, and Assumption 1, we obtain
EpÎ¸(xtâˆ’1|xt)EpÎ¸(ytâˆ’1|yt)[||xtâˆ’1âˆ’ytâˆ’1||]
=EÏµt,Ïµâ€²
tâˆ¼N(0,I)[||gÎ¸
t(xt) +ÏƒtÏµtâˆ’gÎ¸
t(yt)âˆ’ÏƒtÏµâ€²
t||]
â‰¤EÏµt,Ïµâ€²
tâˆ¼N(0,I)[||gÎ¸
t(xt)âˆ’gÎ¸
t(yt)||] +ÏƒtEÏµt,Ïµâ€²
tâˆ¼N(0,I)[||Ïµtâˆ’Ïµâ€²
t||]
â‰¤KÎ¸
t||xtâˆ’yt||+ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I).
Next, we can use the inequalities of Lemma 3.3 to prove the following result.
**Lemma 3.4.** Let Tâ‰¥1. The following inequality holds:
EpÎ¸(xTâˆ’1|xT)EpÎ¸(yTâˆ’1|yT)EpÎ¸(xTâˆ’2|xTâˆ’1)EpÎ¸(yTâˆ’2|yTâˆ’1). . . E pÎ¸(x0|x1)EpÎ¸(y0|y1)[||x0âˆ’y0||]
â‰¤ TY
t=1KÎ¸
t!
||xTâˆ’yT||+TX
t=2 tâˆ’1Y
i=1KÎ¸
i!
ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I).
**Proof Idea.** Lemma 3.4 is proven by induction using Lemma 3.3 in the induction step.
Using the two previous lemmas, we obtain the following upper bound on W1(ÂµÎ¸
n, Ï€Î¸(Â·)).
**Lemma 3.5.** The following inequality holds:
W1(ÂµÎ¸
n, Ï€Î¸(Â·))â‰¤1
nnX
i=1 TY
t=1KÎ¸
t!
Eq(xT|xi
0)Ep(yT)[||xTâˆ’yT||] +TX
t=2 tâˆ’1Y
i=1KÎ¸
i!
ÏƒtEÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||],
where Ïµ, Ïµâ€²âˆ¼N(0, I).
**Proof.** Using the definition of W1, the trivial coupling, the definitions of ÂµÎ¸
nandÏ€Î¸(Â·), and Lemma 3.4, we get the desired result.
Combining Lemmas 3.2 and 3.5 with the triangle inequality yields Theorem 3.1.
3.3 Special case using the forward process of Ho et al. (2020)
Theorem 3.1 establishes a general upper bound that holds for any forward process, as long as the backward process satisfies
Assumption 1. In this section, we specialize the statement of the theorem to the particular case of the forward process defined in
previous work.
LetXâŠ†RD. The forward process is a Gauss-Markov process with transition densities defined as
q(xt|xtâˆ’1) =N(xt;âˆšÎ±txtâˆ’1,(1âˆ’Î±t)I),
where Î±1, . . . , Î± Tis a fixed noise schedule such that 0< Î±t<1for all t. This definition implies that at each time step 1â‰¤tâ‰¤T,
5
q(xt|x0) =N(xt;âˆšÂ¯Î±tx0,(1âˆ’Â¯Î±t)I),withÂ¯Î±t=tY
i=1Î±i.
The optimization objective to train the backward process ensures that for each time step t, the distribution pÎ¸(xtâˆ’1|xt)remains close
to the ground-truth distribution q(xtâˆ’1|xt, x0)given by
q(xtâˆ’1|xt, x0) =N(xtâˆ’1; ËœÂµq
t(xt, x0),ËœÏƒ2
tI),
where
ËœÂµq
t(xt, x0) =âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±txt+âˆšÂ¯Î±tâˆ’1(1âˆ’Î±t)
1âˆ’Â¯Î±tx0.
Now, we discuss Assumption 1 under these definitions.
**Remark 3.2.** We can get a glimpse at the range of KÎ¸
tfor a trained DDPM by looking at the distribution q(xtâˆ’1|xt, x0), since
pÎ¸(xtâˆ’1|xt)is optimized to be as close as possible to q(xtâˆ’1|xt, x0).
For a given x0âˆ¼Âµ, let us take a look at the Lipschitz norm of x7â†’ËœÂµq
t(x, x0). Using the above equation, we have
ËœÂµq
t(xt, x0)âˆ’ËœÂµq
t(yt, x0) =âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±t(xtâˆ’yt).
Hence, x7â†’ËœÂµq
t(x, x0)isKâ€²
t-Lipschitz continuous with
Kâ€²
t=âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±t.
Now, if Î±t<1for all 1â‰¤tâ‰¤T, then we have 1âˆ’Â¯Î±t>1âˆ’Â¯Î±tâˆ’1, which implies Kâ€²
t<1for all 1â‰¤tâ‰¤T.
Remark 3.2 shows that the Lipschitz norm of the mean function ËœÂµq
t(Â·, x0)does not depend on x0. Indeed, looking at the previous
equation, we can see that for any initial x0, the Lipschitz norm Kâ€²
t=âˆšÎ±t(1âˆ’Â¯Î±tâˆ’1)
1âˆ’Â¯Î±tonly depends on the noise schedule, not x0itself.
Since gÎ¸
t(Â·, x0)is optimized to match ËœÂµq
t(Â·, x0)for each x0in the training set, and all the functions ËœÂµq
t(Â·, x0)have the same Lipschitz
norm Kâ€²
t, we believe it is reasonable to assume gÎ¸
tis Lipschitz continuous as well. This is the intuition behind Assumption 1.
**The prior-matching term.** With the definitions of this section, the prior matching term KL(q(xT|x0)||p(xT))has the following
closed form:
KL(q(xT|x0)||p(xT)) =1
2
âˆ’Dlog(1âˆ’Â¯Î±T)âˆ’DÂ¯Î±T+ Â¯Î±T||x0||2
.
**Upper-bounds on the average distance between Gaussian vectors.** If Ïµ, Ïµâ€²are D-dimensional vectors sampled from N(0, I), then
EÏµ,Ïµâ€²[||Ïµâˆ’Ïµâ€²||]â‰¤âˆš
2D.
Moreover, since q(xT|x0) =N(xT;âˆšÂ¯Î±Tx0,(1âˆ’Â¯Î±T)I)and the prior p(yT) =N(yT; 0, I),
Eq(xT|x0)Ep(yT)[||xTâˆ’yT||]â‰¤p
Â¯Î±T||x0||2+ (2âˆ’Â¯Î±T)D.
**Special case of the main theorem.** With the definitions of this section, the inequality of Theorem 3.1 implies that with probability
at least 1âˆ’Î´over the randomness of {x1
0, . . . , x
6
